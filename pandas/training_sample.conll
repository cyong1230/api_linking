How	O
to	O
check	O
if	O
the	O
signs	O
of	O
a	O
Series	B-API
conform	O
to	O
a	O
given	O
string	O
of	O
signs	O
?	O

For	O
example	O
I	O
have	O
a	O
Series	B-API
as	O
below	O
,	O
#CODE	O

To	O
check	O
the	O
whether	O
the	O
elements	O
of	O
the	O
Series	B-API
are	O
positive	O
you	O
could	O
create	O
a	O
Boolean	O
Series	B-API
like	O
this	O
:	O
#CODE	O

To	O
get	O
`	O
sign	O
`	O
into	O
a	O
similar	O
Boolean	O
series	B-API
,	O
you	O
can	O
need	O
to	O
interpret	O
the	O
strings	O
'	O
+	O
'	O
and	O
'	O
-	O
'	O
strings	O
as	O
Boolean	O
values	O
.	O

For	O
example	O
:	O
#CODE	O

Now	O
you	O
can	O
compare	O
the	O
two	O
series	B-API
and	O
use	O
`	O
all	B-API
`	O
.	O

In	O
one	O
line	O
,	O
the	O
whole	O
thing	O
looks	O
like	O
this	O
:	O
#CODE	O

This	O
solution	O
requires	O
some	O
numpy	O
functions	O
,	O
but	O
since	O
you	O
are	O
using	O
pandas	O
for	O
your	O
data	O
series	B-API
,	O
this	O
is	O
probably	O
not	O
an	O
issue	O
for	O
you	O
.	O

#CODE	O

Python	O
Pandas	O
Panel	B-API
counting	O
value	O
occurence	O

I	O
have	O
a	O
large	O
dataset	O
stored	O
as	O
a	O
pandas	O
panel	B-API
.	O

I	O
would	O
like	O
to	O
count	O
the	O
occurence	O
of	O
values	O
1.0	O
on	O
the	O
minor_axis	O
for	O
each	O
item	O
in	O
the	O
panel	B-API
.	O

What	O
I	O
have	O
so	O
far	O
:	O
#CODE	O

Having	O
never	O
dealt	O
with	O
panels	O
before	O
I	O
tried	O
this	O
:	O
`	O
P	O
[	O
P.minor_axis	O
==	O
'	O
a	O
']	O
.min()	B-API
`	O
does	O
it	O
do	O
what	O
you	O
want	O
?	O

To	O
count	O
all	O
the	O
"	O
b	O
"	O
values	O
1.0	O
,	O
I	O
would	O
first	O
isolate	O
b	O
in	O
its	O
own	O
DataFrame	B-API
by	O
swapping	O
the	O
minor	O
axis	O
and	O
the	O
items	O
.	O

#CODE	O

Thanks	O
for	O
thinking	O
with	O
me	O
guys	O
,	O
but	O
I	O
managed	O
to	O
figure	O
out	O
a	O
surprisingly	O
easy	O
solution	O
after	O
many	O
hours	O
of	O
attempting	O
.	O

I	O
thought	O
I	O
should	O
share	O
it	O
in	O
case	O
someone	O
else	O
is	O
looking	O
for	O
a	O
similar	O
solution	O
.	O

#CODE	O

Search	O
by	O
value	O
in	O
pandas	O
series	B-API

Give	O
a	O
pandas	O
series	B-API
#CODE	O

How	O
to	O
combine	O
pivot	B-API
with	O
cumulative	O
sum	O
in	O
Pandas	O

I	O
have	O
a	O
simple	O
dataframe	B-API
:	O
#CODE	O

Which	O
i	O
can	O
easily	O
pivot	O
with	O
the	O
dates	O
as	O
columns	O
using	O
the	O
following	O
function	O
:	O
#CODE	O

However	O
I	O
cant	O
find	O
a	O
way	O
to	O
use	O
a	O
cumulative	O
sum	O
in	O
place	O
of	O
the	O
np.sum	O
,	O
I	O
would	O
like	O
to	O
display	O
the	O
data	O
in	O
the	O
following	O
format	O
?	O

Is	O
this	O
possible	O
?	O

#CODE	O

How	O
to	O
convert	O
string	O
series	B-API
into	O
integer	O

Then	O
,	O
apply	O
`	O
get	B-API
`	O
to	O
the	O
column	O
,	O
returning	O
the	O
original	O
value	O
if	O
it	O
is	O
not	O
in	O
the	O
dictionary	O
:	O
#CODE	O

@USER	O
If	O
all	O
of	O
the	O
unique	O
items	O
contained	O
in	O
the	O
series	B-API
are	O
in	O
the	O
dictionary	O
keys	O
,	O
then	O
`	O
.map	B-API
(	O
d	O
)`	O
is	O
more	O
than	O
five	O
times	O
as	O
fast	O
.	O

However	O
,	O
any	O
missing	O
value	O
appears	O
as	O
a	O
`	O
NaN	O
`	O
.	O

Using	O
a	O
`	O
lambda	O
`	O
function	O
with	O
`	O
get	B-API
`	O
on	O
the	O
dictionary	O
appears	O
to	O
have	O
virtually	O
identical	O
performance	O
.	O

#CODE	O

Use	O
`	O
map	B-API
`	O
for	O
this	O
,	O
it's	O
cyython-ised	O
:	O
`	O
df	O
[	O
col	O
]	O
.map	B-API
(	O
d	O
)`	O

You	O
mean	O
your	O
df	O
somehow	O
has	O
no	O
name	O
?	O

It	O
is	O
probably	O
just	O
`''`	O
,	O
anyway	O
you	O
can	O
directly	O
assign	O
doing	O
`	O
df.columns	O
=	O
[	O
'	O
new_name	O
']`	O
,	O
the	O
normal	O
convention	O
is	O
to	O
call	O
`df.rename(columns={'':'new_name	O
'	O
}	O
,	O
inplace=True	O
)`	O

How	O
to	O
delete	O
some	O
of	O
the	O
mean	O
weekly	O
values	O
calculated	O
by	O
pandas	O
(	O
.resample	B-API
)	O
?	O

I	O
have	O
added	O
a	O
list	O
`	O
Wase	O
=	O
[	O
"	O
2013-12-22	O
"	O
,	O
"	O
2014-01-05	O
"]`	O
,	O
as	O
suggested	O
in	O
comments	O
and	O
used	O
`	O
Temp_plot1	O
=	O
Temp_plot.drop	O
(	O
Wase	O
)`	O
Now	O
I	O
got	O
any	O
error	O
,	O
which	O
says	O
`	O
ValueError	O
:	O
labels	O
[	O
'	O
2013-12-22	O
'	O
'	O
2014-01-05	O
']	O
not	O
contained	O
in	O
axis	O
`	O
.	O

If	O
you	O
have	O
a	O
container	O
(	O
e.g.	O
`	O
list	O
`)	O
of	O
dates	O
that	O
you	O
want	O
to	O
exclude	O
called	O
,	O
say	O
,	O
`	O
unwanted_dates	O
`	O
you	O
can	O
just	O
do	O
`	O
Temp_plot.drop	O
(	O
unwanted_dates	O
)`	O
.	O

Note	O
this	O
returns	O
a	O
view	O
with	O
the	O
desired	O
dates	O
excluded	O
and	O
doesn't	O
actually	O
alter	O
`	O
Temp_plot	O
`	O
.	O

To	O
drop	O
them	O
permanently	O
do	O
`	O
Temp_plot	O
=	O
Temp_plot.drop	O
(	O
unwanted_dates	O
)`	O
or	O
`	O
Temp_plot.drop	O
(	O
unwanted_dates	O
,	O
inplace=True	O
)`	O

Also	O
,	O
you	O
can	O
set	O
your	O
index	O
when	O
you	O
read	O
the	O
csv	O
file	O
:	O
`	O
pd.read_csv	B-API
(	O
"	O
Book1.csv	O
"	O
,	O
parse_dates=['date	O
']	O
,	O
index_col='date	O
')`	O

You	O
need	O
to	O
create	O
a	O
calendar	O
of	O
holidays	O
using	O
`	O
dt.date	B-API
(	O
year	O
,	O
month	O
,	O
day	O
)`	O
.	O

Then	O
you	O
filter	O
the	O
holidays	O
from	O
the	O
index	O
using	O
a	O
list	O
comprehension	O
structure	O
as	O
shown	O
below	O
.	O

Lastly	O
,	O
you	O
select	O
these	O
filtered	O
dates	O
using	O
`	O
.ix	B-API
`	O
which	O
selects	O
data	O
from	O
a	O
dataframe	B-API
based	O
on	O
the	O
index	O
value	O
.	O

#CODE	O

Thanks	O
for	O
your	O
time	O
but	O
it	O
is	O
not	O
working	O
.	O

I	O
have	O
added	O
`	O
holidays	O
=	O
[	O
dt.date	B-API
(	O
2013	O
,	O
12	O
,	O
17	O
)	O
,	O
(	O
2013	O
,	O
12	O
,	O
1	O
8)	O
,	O
(	O
2013	O
,	O
12	O
,	O
19	O
)	O
,	O
(	O
2013	O
,	O
12	O
,	O
20	O
)	O
,	O
(	O
2013	O
,	O
12	O
,	O
21	O
)	O
,	O
(	O
2013	O
,	O
12	O
,	O
22	O
)]`	O
`Temp=Temp.set_index('date	O
')`	O
and	O
same	O
lines	O
like	O
yours	O
`	O
idx=	O
...	O

`	O
and	O
`	O
Temp_plot=	O
...	O

`	O
but	O
it	O
gave	O
same	O
results	O
as	O
before	O
meaning	O
the	O
holidays	O
are	O
not	O
removed	O
.	O

I	O
have	O
also	O
tried	O
to	O
to	O
sampling	O
on	O
daily	O
basis	O
before	O
doing	O
it	O
on	O
weekly	O
basis	O
but	O
this	O
ain't	O
give	O
me	O
any	O
different	O
results	O
.	O

Your	O
help	O
will	O
be	O
appreciated	O
.	O

because	O
...?	O

What	O
is	O
the	O
error	O
.	O

My	O
guess	O
is	O
the	O
format	O
of	O
your	O
date	O
.	O

What	O
is	O
`	O
type	O
(	O
Temp.index.iat	O
[	O
0	O
])`	O
?	O

You	O
need	O
dt.date	B-API
(	O
...	O
)	O
for	O
each	O
date	O
in	O
your	O
holidays	O
.	O

Or	O
you	O
can	O
do	O
this	O
:	O
`	O
holidays	O
=	O
[	O
dt.date	B-API
(	O
d	O
[	O
0	O
]	O
,	O
d	O
[	O
1	O
]	O
,	O
d	O
[	O
2	O
])	O
for	O
d	O
in	O
[(	O
2013	O
,	O
12	O
,	O
1	O
8)	O
,	O
(	O
2013	O
,	O
12	O
,	O
19	O
)	O
,	O
(	O
2013	O
,	O
12	O
,	O
20	O
)	O
,	O
(	O
2013	O
,	O
12	O
,	O
21	O
)	O
,	O
(	O
2013	O
,	O
12	O
,	O
22	O
)]]`	O
.	O

Also	O
,	O
I	O
tried	O
the	O
code	O
using	O
your	O
sample	O
data	O
and	O
it	O
works	O
for	O
me	O
.	O

Creating	O
data	O
histograms	O
/	O
visualizations	O
using	O
ipython	O
and	O
filtering	O
out	O
some	O
values	O

but	O
get	O
an	O
error	O
due	O
to	O
unrecognized	O
dtypes	B-API
(	O
there	O
are	O
some	O
entries	O
that	O
aren't	O
in	O
datetime	O
format	O
)	O
.	O

How	O
can	O
I	O
exclude	O
these	O
in	O
the	O
original	O
command	O
?	O

Also	O
,	O
what	O
if	O
I	O
want	O
to	O
create	O
the	O
same	O
histogram	O
but	O
filtering	O
to	O
keep	O
only	O
records	O
that	O
have	O
id's	O
(	O
in	O
an	O
account	O
id	O
field	O
)	O
starting	O
with	O
the	O
integer	O
(	O
or	O
string	O
?	O
)	O
'	O
2	O
'	O
?	O

Ultimately	O
,	O
I	O
want	O
to	O
be	O
able	O
to	O
create	O
histograms	O
,	O
line	O
plots	O
,	O
box	O
plots	O
and	O
so	O
on	O
but	O
filtering	O
for	O
certain	O
months	O
,	O
user	O
id's	O
,	O
or	O
just	O
bad	O
'	O
dtypes	B-API
'	O
.	O

and	O
when	O
I	O
apply	O
the	O
command	O
pd.to_datetime()	B-API
to	O
these	O
columns	O
I	O
get	O
fields	O
resulting	O
that	O
look	O
like	O
:	O
#CODE	O

the	O
dtype	B-API
of	O
the	O
duration	O
column	O
fields	O
(	O
#URL	O
)	O
is	O
:	O
#CODE	O

Check	O
`	O
df.Duration.dtype	O
`	O
.	O

If	O
it	O
isn't	O
a	O
numeric	O
dtype	B-API
,	O
you	O
can	O
convert	O
using	O
`	O
df.Duration.astype	O
(	O
int	O
)`	O
or	O
`	O
(	O
float	O
)`	O
,	O
provided	O
that	O
you	O
only	O
have	O
strings	O
containing	O
numbers	O
,	O
and	O
no	O
NaNs	O
.	O

It	O
helps	O
if	O
you	O
post	O
a	O
sample	O
`	O
df.Duration.head	O
(	O
4	O
)`	O
.	O

oops-I	O
typed	O
df.Duration.dtype	O
and	O
get	O
dtype	B-API
(	O
'	O
<	O
m	O
8[	O
ns	O
]')	O
and	O
df.Duration.head	O
(	O
4	O
)	O
returns	O
0	O
00:14	O
:	O
00	O

Name	O
:	O
Duration	O
,	O
dtype	B-API
:	O
timedelta64	O
[	O
ns	O
]	O
so	O
obviously	O
it	O
is	O
not	O
an	O
integer	O
which	O
it	O
would	O
need	O
to	O
be-these	O
are	O
time	O
stamps	O
so	O
in	O
hours	O
minutes	O
seconds	O
,	O
will	O
df.Duration.astype	O
(	O
int	O
)	O
recognize	O
this	O
and	O
convert	O
accordingly	O
or	O
do	O
I	O
need	O
a	O
different	O
command	O
?	O

Thanks	O
!	O

I	O
read	O
somewhere	O
else	O
that	O
if	O
'	O
Duration	O
'	O
is	O
in	O
the	O
form	O
#URL	O
then	O
Duration.days	O
returns	O
days	O
,	O
Duration.minutes	O
the	O
minutes	O
,	O
Duration.seconds	O
the	O
seconds	O
I	O
could	O
then	O
just	O
define	O
a	O
function	O
which	O
inputs	O
the	O
duration	O
(	O
dur	O
)	O
and	O
outputs	O
Duration.days	O
*	O
3600	O
+	O
Duration.minutes	O
+	O
Duration.seconds	O
/	O
60	O
and	O
then	O
write	O
df.f	O
(	O
dur	O
)	O
.hist	B-API
(	O
bins=10	O
)	O
correct	O
?	O

I	O
do	O
have	O
a	O
duration-I	O
calculated	O
it	O
by	O
subtracting	O
two	O
dates	O
and	O
added	O
it	O
to	O
my	O
data	O
frame	O
with	O
:	O
df['Duration	O
']	O
=	O
pd.to_datetime(df['End	O
Time'])-pd.to_datetime(df['Start	O
Time	O
'])	O

I	O
think	O
you	O
need	O
to	O
convert	O
duration	O
(	O
timedelta64	O
)	O
to	O
int	O
(	O
assuming	O
you	O
have	O
a	O
duration	O
)	O
.	O

Then	O
the	O
.hist	B-API
method	O
will	O
work	O
.	O

#CODE	O

sorry	O
,	O
my	O
other	O
question	O
is	O
:	O
my	O
dtype	B-API
shows	O
as	O
:	O
dtype	B-API
(	O
'	O
<	O
m	O
8[	O
ns	O
]')	O
,	O
is	O
this	O
timedelta64	O
?	O

No	O
,	O
as	O
I	O
said	O
,	O
you	O
have	O
a	O
date	O
.	O

Look	O
at	O
my	O
output	O
above	O
,	O
a	O
duration	O
dtype	B-API
will	O
show	O
as	O
dtype	B-API
:	O
timedelta64	O
[	O
ns	O
]	O
(	O
if	O
the	O
base	O
is	O
nanosecond	O
)	O
or	O
something	O
similar	O
.	O

You	O
said	O
you	O
subtracted	O
your	O
dates	O
.	O

Did	O
you	O
actually	O
assign	O
that	O
result	O
to	O
a	O
column	O
?	O

Are	O
you	O
referencing	O
to	O
the	O
right	O
column	O
?	O

I	O
did	O
this	O
command	O
:	O
df['Duration	O
']	O
=	O
pd.to_datetime(df['End	O
Time'])-pd.to_datetime(df['Start	O
Time	O
'])	O
to	O
add	O
the	O
column	O
'	O
Duration	O
'	O
to	O
my	O
data	O
frame	O
'	O
df	O
'	O
where	O
the	O
'	O
End	O
Time	O
'	O
and	O
'	O
Start	O
Time	O
'	O
columns	O
are	O
of	O
the	O
format	O
:	O
2014	O
/	O
03	O
/	O
27	O
23:13	O
for	O
example-I	O
have	O
checked	O
and	O
the	O
result	O
of	O
this	O
was	O
to	O
give	O
fields	O
in	O
the	O
format	O
of	O
#URL	O
of	O
time	O
lapsed	O

I	O
am	O
trying	O
to	O
figure	O
out	O
how	O
to	O
get	O
a	O
slice	O
of	O
my	O
data	O
but	O
omitting	O
some	O
of	O
it	O
.	O

I	O
am	O
using	O
ipython	O
and	O
there	O
are	O
,	O
among	O
others	O
,	O
two	O
columns	O
in	O
my	O
data	O
frame	O
called	O
'	O
Start	O
Time	O
'	O
and	O
'	O
End	O
Time	O
'	O
.	O

All	O
I	O
did	O
was	O
use	O
the	O
command	O
I	O
wrote	O
above	O
to	O
subtract	O
these	O
two	O
columns	O
and	O
create	O
another	O
which	O
has	O
the	O
correct	O
time	O
lapse-I	O
will	O
edit	O
my	O
question	O
(	O
if	O
I	O
can	O
)	O
to	O
show	O
the	O
result	O
of	O
the	O
to_datetime()	B-API
transformation	O
.	O

To	O
convert	O
your	O
timedelta	O
to	O
minutes	O
,	O
you	O
just	O
have	O
to	O
divide	O
it	O
by	O
1	O
minute	O
,	O
hence	O
:	O
`df['minutes	O
']	O
=	O
df.Duration	O
/	O
timedelta	O
(	O
1	O
,	O
'	O
m	O
')`	O
.	O

Sorry	O
,	O
I	O
thought	O
you	O
had	O
read	O
the	O
last	O
line	O
of	O
my	O
answer	O
.	O

Then	O
you'll	O
be	O
able	O
to	O
do	O
`	O
df.minutes.hist()	O
`	O
without	O
problems	O
.	O

Thanks	O
!	O

I	O
ended	O
up	O
using	O
the	O
.str	B-API
somehow	O
but	O
this	O
is	O
good	O
to	O
know	O
!	O

no	O
,	O
.str	B-API
was	O
somethin	O
else	O
,	O
I	O
did	O
:	O
`df['Durationendminusstart	O
']	O
=	O
pd.to_timedelta	B-API
(	O
df.Duration	O
,	O
unit='ns').astype('timedelta64	O
[	O
m	O
]')`	O

Trouble	O
pivoting	O
/	O
reshaping	O
a	O
pandas	O
dataframe	B-API

I	O
have	O
the	O
following	O
dataframe	B-API
:	O
#CODE	O

I'd	O
like	O
to	O
pivot	O
the	O
data	O
so	O
that	O
I	O
end	O
up	O
with	O
:	O
#CODE	O

Fillna's	O
:	O
#CODE	O

does	O
your	O
data	O
get	O
correctly	O
parsed	O
with	O
read_csv	B-API
?	O

If	O
your	O
data's	O
correctly	O
parsed	O
by	O
`	O
read_csv	B-API
`	O
,	O
why	O
do	O
you	O
have	O
a	O
column	O
named	O
`'0	O
'`	O
?	O

I'd've	O
thought	O
that	O
it'd	O
be	O
`data[data['Value	O
']	O
==	O
0	O
]`	O
.	O

Truncating	O
column	O
width	O
in	O
pandas	O

I'm	O
reading	O
in	O
large	O
csv	O
files	O
into	O
pandas	O
some	O
of	O
them	O
with	O
String	O
columns	O
in	O
the	O
thousands	O
of	O
characters	O
.	O

Is	O
there	O
any	O
quick	O
way	O
to	O
limit	O
the	O
width	O
of	O
a	O
column	O
,	O
i.e.	O
only	O
keep	O
the	O
first	O
100	O
characters	O
?	O

Do	O
you	O
mean	O
after	O
the	O
fact	O
,	O
or	O
do	O
you	O
want	O
/	O
need	O
to	O
truncate	O
them	O
before	O
they're	O
even	O
stored	O
in	O
the	O
frame	O
?	O

Before	O
even	O
stored	O
would	O
be	O
great	O
.	O

Would	O
an	O
apply	B-API
be	O
the	O
fastest	O
way	O
for	O
after	O
the	O
fact	O
?	O

Writing	O
an	O
apply	B-API
right	O
now	O
.	O

Is	O
there	O
a	O
way	O
to	O
get	O
the	O
width	O
of	O
a	O
column	O
of	O
type	O
object	O
?	O

`df['string	O
col	O
']	O
.apply	B-API
(	O
lambda	O
x	O
:	O
x	O
[:	O
100	O
])`	O
will	O
keep	O
only	O
the	O
first	O
100	O
characters	O

Would	O
be	O
nice	O
to	O
check	O
if	O
it	O
needs	O
to	O
be	O
truncated	O
beforehand	O
.	O

Can't	O
seem	O
to	O
find	O
any	O
attribute	O
that	O
gives	O
the	O
width	O
.	O

If	O
you	O
can	O
read	O
the	O
whole	O
thing	O
into	O
memory	O
,	O
you	O
can	O
use	O
the	O
`	O
str	B-API
`	O
method	O
for	O
vector	O
operations	O
:	O
#CODE	O

Also	O
note	O
that	O
you	O
can	O
get	O
a	O
Series	B-API
with	O
lengths	O
using	O
#CODE	O

``	O
converters	O
``	O
are	O
called	O
row-by-row	O
(	O
by	O
a	O
cython	O
function	O
)	O
.	O

I	O
think	O
the	O
``	O
str	O
``	O
conversion	O
would	O
be	O
faster	O
(	O
your	O
first	O
example	O
)	O

Pandas	O
DataFrame	B-API
to	O
Excel	O
Problems	O

Does	O
it	O
have	O
to	O
output	O
as	O
an	O
excel	O
workbook	O
,	O
or	O
would	O
CSV	O
suffice	O
?	O

You	O
could	O
change	O
your	O
list	O
of	O
tuples	O
to	O
a	O
dataframe	B-API
then	O
output	O
as	O
a	O
csv	O
quite	O
easily	O
...	O

More	O
detailed	O
discussion	O
is	O
in	O
this	O
answer	O
too	O
.	O

Basically	O
,	O
openpyxl	O
version	O
2	O
deprecated	O
the	O
style-conversion	O
functions	O
that	O
map	O
dictionary	O
style	O
definitions	O
to	O
the	O
new	O
openpyxl	O
api	O
,	O
but	O
Pandas	O
still	O
uses	O
the	O
old	O
style	O
,	O
and	O
for	O
some	O
reason	O
the	O
deprecation	O
pass-through	O
function	O
errors	O
out	O
.	O

Using	O
this	O
map	O
of	O
NYC	O
I'd	O
like	O
to	O
change	O
Manhattan	O
to	O
be	O
bright	O
blue	O
.	O

But	O
when	O
I	O
change	O
the	O
individual	O
patch	O
color	O
of	O
Manhattan	O
all	O
the	O
other	O
patch	O
colors	O
change	O
too	O
.	O

This	O
was	O
unexpected	O
to	O
me	O
.	O

A	O
possible	O
solution	O
is	O
using	O
`	O
geopandas.plotting.plot_multipolygon	O
`	O
to	O
specifically	O
add	O
only	O
one	O
geometry	O
object	O
with	O
blue	O
colors	O
to	O
the	O
existing	O
figure	O
:	O
#CODE	O

I'm	O
trying	O
to	O
monkeypatch	O
how	O
`	O
pandas	O
`	O
Panel's	O
slicing	O
(	O
`	O
__getitem__	O
`)	O
.	O

This	O
is	O
straightforward	O
to	O
do	O
with	O
a	O
basic	O
function	O
,	O
foo	O
.	O

#CODE	O

Where	O
`	O
ORIGINAL_getitem	O
`	O
is	O
storing	O
the	O
original	O
Panel	B-API
method	O
.	O

I'm	O
trying	O
to	O
extend	O
to	O
the	O
case	O
where	O
`	O
foo()	O
`	O
is	O
not	O
a	O
function	O
,	O
but	O
an	O
instance	O
method	O
of	O
an	O
object	O
,	O
`	O
Foo	O
`	O
.	O

For	O
example	O
:	O
#CODE	O

`	O
Foo.foo()	O
`	O
must	O
access	O
the	O
attribute	O
`	O
self.name	O
`	O
.	O

Therefore	O
,	O
the	O
monkeypatched	O
function	O
would	O
need	O
a	O
reference	O
to	O
the	O
Foo	O
instance	O
somehow	O
,	O
in	O
addition	O
to	O
the	O
Panel	B-API
.	O

How	O
can	O
I	O
monkepatch	O
panel	B-API
with	O
`	O
Foo.foo()	O
`	O
and	O
make	O
self.name	O
accessible	O
?	O

But	O
this	O
doesn't	O
work	O
.	O

A	O
reference	O
to	O
self	O
is	O
passed	O
,	O
but	O
no	O
access	O
from	O
the	O
calling	O
Panel	B-API
possible	O
.	O

That	O
is	O
:	O
#CODE	O

Passes	O
a	O
reference	O
to	O
`	O
Foo	O
`	O
,	O
not	O
to	O
`	O
Panel	B-API
`	O
.	O

In	O
short	O
,	O
it	O
provides	O
a	O
cool	O
set	O
of	O
features	O
,	O
it	O
has	O
a	O
wide	O
range	O
of	O
unit	O
tests	O
,	O
and	O
it	O
comes	O
with	O
a	O
fancy	O
doc	O
that	O
should	O
cover	O
everything	O
you	O
need	O
to	O
get	O
started	O
.	O

Make	O
sure	O
to	O
also	O
check	O
the	O
FAQ	O
!	O

One	O
way	O
to	O
do	O
this	O
is	O
to	O
create	O
a	O
closure	O
(	O
a	O
function	O
with	O
reference	O
to	O
names	O
other	O
than	O
locals	O
or	O
globals	O
)	O
.	O

A	O
simple	O
closure	O
:	O
#CODE	O

append	O
pandas.DataFrame.GroupBy	B-API
results	O
into	O
another	O
dataframe	B-API

What	O
I'm	O
trying	O
to	O
do	O
is	O
make	O
a	O
dataframe	B-API
that	O
only	O
keeps	O
the	O
sum	O
of	O
the	O
counts	O
(	O
the	O
column	O
after	O
year	O
)	O
regardless	O
of	O
the	O
year	O
.	O

I	O
used	O
groupby	B-API
to	O
aggregate	O
my	O
dataframe	B-API
,	O
and	O
tried	O
to	O
save	O
the	O
results	O
in	O
another	O
dataframe	B-API
(	O
df_ngram	O
)	O
.	O

But	O
it	O
seems	O
like	O
it	O
isn't	O
appending	O
at	O
all	O
.	O

Below	O
is	O
what	O
I	O
get	O
when	O
I	O
run	O
this	O
.	O

I'm	O
not	O
sure	O
how	O
to	O
deal	O
with	O
the	O
groupby	B-API
results	O
.	O

How	O
can	O
I	O
aggregate	O
the	O
results	O
from	O
groupby	B-API
?	O

Or	O
can	O
I	O
get	O
what	O
I	O
want	O
without	O
using	O
groupby	B-API
?	O

#CODE	O

You	O
need	O
to	O
append	O
the	O
intermediate	O
DataFrames	O
to	O
a	O
list	O
and	O
then	O
concatenate	O
the	O
results	O
.	O

Instead	O
of	O
`	O
df_ngram.append	O
(	O
agg_chunk	O
)`	O
,	O
you	O
want	O
`	O
df_agg.append	O
(	O
agg_chunk	O
)`	O

I	O
did	O
`	O
df_ngram	O
=	O
pd.concat	B-API
(	O
df_agg	O
,	O
ignore_index=True	O
)`	O
but	O
then	O
`	O
print	O
df_ngram.head	O
(	O
10	O
)`	O
gives	O
me	O
`	O
0	O
279	O

Name	O
:	O
count	O
,	O
dtype	B-API
:	O
int64	O
`	O
which	O
isn't	O
what	O
I	O
wanted	O
.	O

I	O
want	O
the	O
dataframe	B-API
indexes	O
to	O
be	O
my	O
ngrams	O
not	O
index	O
numbers	O
.	O

Setting	O
`	O
ignore_index=False	O
`	O
also	O
doesn't	O
work	O
.	O

And	O
I	O
want	O
to	O
set	O
the	O
column	O
names	O
to	O
[	O
'	O
ngram	O
'	O
,	O
'	O
count	O
']	O
,	O
but	O
this	O
can	O
be	O
done	O
in	O
separate	O
line	O
.	O

I	O
am	O
completely	O
new	O
to	O
python	O
and	O
Pandas	O
of	O
course	O
.	O

I	O
am	O
trying	O
to	O
run	O
a	O
function	O
"	O
get	O
url	O
"	O
which	O
is	O
function	O
to	O
get	O
the	O
complete	O
extended	O
url	O
from	O
small	O
Url	O
.	O

I	O
have	O
a	O
data	O
frame	O
in	O
python	O
consists	O
all	O
the	O
short	O
URLs	O
.	O

Now	O
I	O
am	O
trying	O
to	O
do	O
with	O
following	O
ways	O
.	O

One	O
is	O
to	O
use	O
"	O
for	O
"	O
loop	O
which	O
loops	O
and	O
apply	O
function	O
on	O
all	O
the	O
elements	O
and	O
will	O
create	O
a	O
another	O
series	B-API
of	O
extended	O
URL	O
but	O
I	O
am	O
not	O
able	O
to	O
,	O
dont	O
know	O
why	O
,	O
I	O
tried	O
to	O
write	O
it	O
like	O
#CODE	O

again	O
second	O
solution	O
i	O
tried	O
was	O
passing	O
a	O
whole	O
array	O
to	O
applymap	B-API
fucntion	O
#CODE	O

can	O
you	O
dput	O
what	O
your	O
function	O
is	O
doing	O
?	O

Adding	O
the	O
same	O
suffix	O
to	O
each	O
url	O
or	O
particular	O
suffix	O
proper	O
to	O
each	O
base	O
url	O
?	O

expanded	O
(	O
i	O
)	O
=	O
get_real	O
(	O
df2	O
[[	O
i	O
]])`	O
doesn't	O
work	O
because	O
the	O
iterable	O
returned	O
is	O
the	O
column	O
not	O
each	O
element	O
,	O
so	O
unless	O
your	O
code	O
understands	O
how	O
to	O
operate	O
on	O
a	O
pandas	O
Series	B-API
then	O
your	O
code	O
won't	O
work	O

Hi	O
..	O
this	O
is	O
my	O
defined	O
function	O
..	O
def	O
myfunction	O
(	O
url	O
):	O
exturl=	O
urllib2.urlopen	O
(	O
HeadRequest	O
(	O
url	O
))	O
.geturl()	O

return	O
exturl	O
and	O
I	O
am	O
applying	O
it	O
on	O
my	O
list	O
df3['new']=df3['first_link	O
']	O
.apply	B-API
(	O
myfunction	O
)	O
...	O
but	O
because	O
or	O
errors	O
its	O
not	O
working	O
..	O
that	O
why	O
looking	O
for	O
for	O
loop	O

If	O
the	O
short	O
urls	O
are	O
a	O
column	O
in	O
the	O
pandas	O
dataFrame	B-API
,	O
you	O
can	O
use	O
the	O
`	O
apply	B-API
`	O
function	O
(	O
though	O
I	O
am	O
not	O
sure	O
if	O
they	O
would	O
resume	O
on	O
error	O
,	O
most	O
probably	O
not	O
)	O
.	O

Hi	O
..	O
this	O
is	O
my	O
defined	O
function	O
..	O
def	O
myfunction	O
(	O
url	O
):	O
exturl=	O
urllib2.urlopen	O
(	O
HeadRequest	O
(	O
url	O
))	O
.geturl()	O

return	O
exturl	O
and	O
I	O
am	O
applying	O
it	O
on	O
my	O
list	O
df3['new']=df3['first_link	O
']	O
.apply	B-API
(	O
myfunction	O
)	O
...	O
but	O
because	O
or	O
errors	O
its	O
not	O
working	O
..	O
that	O
why	O
looking	O
for	O
for	O
loop	O

Hi	O
Anand	O
-	O
Thanks	O
for	O
help	O
.	O
but	O
it	O
is	O
working	O
same	O
as	O
apply	B-API
function	O
.	O
as	O
soon	O
as	O
it	O
faces	O
a	O
error	O
it	O
pass	O
completely	O
.	O
error	O
is	O
like	O
HTTPError	O
:	O
HTTP	O
Error	O
404	O
:	O
Not	O
Found	O
....	O

ANd	O
i	O
am	O
using	O
the	O
formula	O
for	O
idx	O
in	O
df5.index	O
:	O

I	O
think	O
the	O
problem	O
you	O
are	O
having	O
is	O
treating	O
the	O
dataframe	B-API
just	O
like	O
a	O
dictionary	O
that	O
has	O
keys	O
and	O
values	O
.	O

I'm	O
trying	O
to	O
select	O
,	O
by	O
group	O
,	O
the	O
minimum	O
date	O
when	O
a	O
condition	O
is	O
met	O
and	O
assign	O
it	O
to	O
a	O
new	O
column	O
:	O
#CODE	O

I've	O
been	O
trying	O
to	O
use	O
groupby	B-API
in	O
combination	O
with	O
transform	O
,	O
but	O
don't	O
know	O
how	O
to	O
get	O
transform	O
to	O
take	O
account	O
of	O
conditions	O
based	O
on	O
other	O
columns	O
.	O

First	O
change	O
the	O
`	O
bool	B-API
`	O
column	O
to	O
actually	O
booleans	O
(	O
also	O
be	O
careful	O
with	O
your	O
names	O
.	O
DataFrame	B-API
has	O
a	O
`	O
bool	B-API
`	O
method	O
):	O
#CODE	O

Finding	O
the	O
minimum	O
dates	O
is	O
pretty	O
easy	O
.	O

Use	O
the	O
`	O
bool	B-API
`	O
column	O
to	O
index	O
into	O
`	O
df	O
`	O
:	O
#CODE	O

There	O
are	O
probably	O
a	O
bunch	O
of	O
ways	O
to	O
set	O
the	O
values	O
,	O
but	O
one	O
is	O
to	O
set	O
`	O
Group	O
`	O
as	O
the	O
index	O
and	O
`	O
join	B-API
`	O
the	O
`	O
dates	O
`	O
.	O

#CODE	O

Thanks	O
.	O

Worked	O
except	O
for	O
joining	O
bit	O
.	O

I	O
had	O
to	O
convert	O
the	O
series	B-API
to	O
a	O
dataframe	B-API
and	O
then	O
perform	O
a	O
left	O
merge	B-API
:	O
`	O
pd.merge	B-API
(	O
df	O
,	O
dates	O
,	O
how='left	O
'	O
,	O
on='Group	O
')`	O

I	O
have	O
a	O
pandas	O
data	O
frame	O
which	O
has	O
some	O
rows	O
and	O
columns	O
.	O

Each	O
column	O
has	O
a	O
header	O
.	O

Now	O
as	O
long	O
as	O
I	O
keep	O
doing	O
data	O
manipulation	O
operations	O
in	O
pandas	O
,	O
my	O
variable	O
headers	O
are	O
retained	O
.	O

But	O
if	O
I	O
try	O
some	O
data	O
pre-processing	O
feature	O
of	O
Sci-kit-learn	O
lib	O
,	O
I	O
end	O
up	O
losing	O
all	O
my	O
headers	O
and	O
the	O
frame	O
gets	O
converted	O
to	O
just	O
a	O
matrix	O
of	O
numbers	O
.	O

I	O
understand	O
why	O
it	O
happens	O
because	O
scikit-learn	O
gives	O
a	O
numpy	O
ndarray	O
as	O
output	O
.	O

And	O
numpy	O
ndarray	O
being	O
just	O
matrix	O
would	O
not	O
have	O
column	O
names	O
.	O

@USER	O
:	O
please	O
provide	O
a	O
very	O
simple	O
,	O
reproducible	O
example	O
!	O

A	O
three	O
row	O
dataframe	B-API
would	O
make	O
your	O
question	O
more	O
understandable	O
.	O

(	O
Maybe	O
just	O
copying	O
`	O
saved_cols	O
=	O
df.columns	O
`	O
and	O
then	O
reassigning	O
it	O
to	O
the	O
modified	O
`	O
df	O
`	O
would	O
do	O
the	O
trick	O
,	O
but	O
I'm	O
not	O
sure	O
that's	O
what	O
you	O
need	O
)	O

Indeed	O
,	O
as	O
@USER	O
says	O
,	O
copying	O
`	O
saved_cols	O
=	O
df.columns	O
`	O
and	O
then	O
when	O
you	O
got	O
the	O
series	B-API
,	O
doing	O
`	O
pandas.DataFrame	B-API
(	O
series	B-API
,	O
saved_cols	O
)`	O
you	O
get	O
your	O
dataframe	B-API
back	O
.	O

I	O
do	O
it	O
for	O
example	O
when	O
using	O
`	O
train_test_split	O
`	O
,	O
which	O
gives	O
back	O
a	O
`	O
numpy	O
ndarray	O
`	O
,	O
but	O
I	O
need	O
to	O
use	O
it	O
as	O
a	O
dataframe	B-API
.	O

It	O
is	O
not	O
something	O
to	O
be	O
particularly	O
proud	O
of	O
,	O
but	O
in	O
my	O
opinion	O
is	O
good	O
enough	O
.	O

scikit-learn	O
indeed	O
strips	O
the	O
column	O
headers	O
in	O
most	O
cases	O
,	O
so	O
just	O
add	O
them	O
back	O
on	O
afterward	O
.	O

In	O
your	O
example	O
,	O
with	O
`	O
X_imputed	O
`	O
as	O
the	O
`	O
sklearn.preprocessing	O
`	O
output	O
and	O
`	O
X_train	O
`	O
as	O
the	O
original	O
dataframe	B-API
,	O
you	O
can	O
put	O
the	O
column	O
headers	O
back	O
on	O
with	O
:	O
#CODE	O

For	O
example	O
,	O
I	O
would	O
like	O
to	O
create	O
a	O
new	O
column	O
titled	O
'	O
Rheum	O
'	O
,	O
which	O
takes	O
on	O
a	O
value	O
of	O
'	O
1	O
'	O
if	O
the	O
expression	O
'	O
391.1	O
'	O
appears	O
in	O
a	O
corresponding	O
column	O
'	O
ICD	O
'	O
per	O
row	O
.	O

In	O
some	O
rows	O
of	O
the	O
ICD	O
column	O
there	O
are	O
cells	O
which	O
have	O
a	O
variety	O
of	O
expressions	O
in	O
the	O
form	O
'	O
424.1	O
,	O
391.1	O
,	O
420.2	O
,	O
etc	O
'	O
.	O

Could	O
you	O
post	O
part	O
of	O
your	O
dataframe	B-API
?	O

Please	O
edit	O
your	O
question	O
with	O
your	O
data	O
as	O
dataframe	B-API

You	O
can	O
use	O
`	O
str.contains	B-API
`	O
:	O
#CODE	O

@USER	O
You	O
could	O
do	O
that	O
with	O
method	O
of	O
dataframe	B-API
and	O
series	B-API
`	O
astype	B-API
(	O
str	O
)`	O

Maybe	O
using	O
`df['ICD	O
']	O
.apply	B-API
(	O
str	O
)`	O
...	O
please	O
try	O
to	O
do	O
some	O
search	O
before	O

`	O
str.contains	B-API
`	O
return	O
bool	O
values	O
.	O

You	O
could	O
convert	O
it	O
to	O
integer	O
with	O
simple	O
add	O
0	O
:	O
#CODE	O

Yes	O
,	O
you	O
could	O
use	O
`	O
df	O
=	O
pd.read_csv('your_file	O
'	O
,	O
names=['patient	O
'	O
,	O
'	O
ICD9	O
'])`	O
and	O
then	O
operate	O
with	O
that	O
df	O

So	O
my	O
idea	O
was	O
,	O
I	O
will	O
create	O
an	O
excel-template	O
with	O
these	O
tricky	O
parts	O
and	O
then	O
from	O
python	O
just	O
insert	O
dynamic	O
data	O
to	O
this	O
template	O
.	O

@USER	O
:	O
That's	O
not	O
really	O
relevant	O
here	O
,	O
and	O
not	O
a	O
productive	O
comment	O
.	O

There	O
isn't	O
any	O
code	O
he	O
could	O
have	O
tried	O
,	O
given	O
the	O
tools	O
that	O
he's	O
mentioned	O
in	O
the	O
question	O
and	O
the	O
tags	O
.	O

If	O
you	O
don't	O
think	O
this	O
question	O
is	O
a	O
good	O
fit	O
for	O
Stack	O
Overflow	O
,	O
then	O
comment	O
on	O
why	O
you	O
believe	O
that	O
,	O
and	O
/	O
or	O
downvote	O
the	O
question	O
.	O

There	O
isn't	O
an	O
easy	O
way	O
to	O
do	O
this	O
with	O
any	O
of	O
the	O
usual	O
"	O
direct	O
file	O
manipulation	O
"	O
libraries	O
in	O
Python	O
(	O
xlrd	O
,	O
xlwt	O
,	O
XlsxWriter	O
,	O
OpenPyXL	O
;	O
these	O
are	O
what	O
pandas	O
uses	O
)	O
.	O

The	O
reason	O
is	O
that	O
the	O
structure	O
of	O
a	O
workbook	O
file	O
is	O
such	O
that	O
it's	O
impossible	O
or	O
prohibitively	O
difficult	O
(	O
depending	O
on	O
whether	O
you're	O
talking	O
about	O
.xls	O
or	O
.xlsx	O
)	O
to	O
do	O
anything	O
resembling	O
"	O
in-place	O
"	O
editing	O
,	O
short	O
of	O
re-implementing	O
Excel	O
itself	O
.	O

use	O
a	O
list	O
of	O
values	O
to	O
select	O
rows	O
from	O
a	O
pandas	O
dataframe	B-API

how	O
to	O
filter	O
the	O
dataframe	B-API
rows	O
of	O
pandas	O
by	O
within	O
/	O
in	O
?	O

Lets	O
say	O
I	O
have	O
the	O
following	O
pandas	O
dataframe	B-API
:	O
#CODE	O

This	O
is	O
indeed	O
a	O
duplicate	O
of	O
how	O
to	O
filter	O
the	O
dataframe	B-API
rows	O
of	O
pandas	O
by	O
"	O
within	O
"	O
/	O
"	O
in	O
"	O
?	O

,	O
translating	O
the	O
response	O
to	O
your	O
example	O
gives	O
:	O
#CODE	O

So	O
to	O
be	O
clear	O
,	O
you	O
want	O
to	O
do	O
basically	O
an	O
inner	O
join	O
on	O
the	O
`	O
DATE	B-API
`	O
key	O
in	O
df2	O
such	O
that	O
it	O
is	O
within	O
the	O
Start	O
/	O
End	O
Date	O
range	O
?	O

@USER	O
,	O
Correct	O
,	O
and	O
also	O
summing	O
the	O
values	O
of	O
the	O
dates	O
within	O
the	O
range	O
.	O

@USER	O
,	O
I	O
made	O
the	O
correction	O
,	O
I	O
removed	O
the	O
excel	O
tag	O
.	O

Thanks	O

hmm	O
..	O
works	O
for	O
me	O
.	O

They	O
key	O
step	O
is	O
the	O
second	O
last	O
.	O

First	O
check	O
that	O
the	O
index	O
on	O
both	O
frames	O
is	O
a	O
datetime	O
index	O
.	O

Then	O
check	O
the	O
output	O
of	O
`	O
map	B-API
(	O
df1.index.asof	O
,	O
df2.index	O
)`	O
.	O

This	O
is	O
the	O
array	O
indicating	O
the	O
groups	O
(	O
`	O
df1.index.asof	O
`	O
is	O
a	O
function	O
which	O
is	O
applied	O
to	O
the	O
index	O
of	O
`	O
df2	O
`)	O
.	O

For	O
each	O
date	O
in	O
`	O
df2.index	O
`	O
the	O
output	O
should	O
be	O
the	O
latest	O
date	O
from	O
`	O
df1.index	O
`	O
before	O
that	O
date	O
.	O

for	O
some	O
reason	O
`df2['DATE	O
']	O
=	O
pd.to_datetime	B-API
(	O
df2.DATE	O
)`	O
makes	O
the	O
df2	O
index	O
of	O
DATE	O
result	O
to	O
`	O
1970-01-01	O
00:00	O
:	O
00.0	O
20110706	O
`	O

found	O
the	O
issue	O
!	O

replacing	O
`df2['DATE	O
']	O
=	O
pd.to_datetime	B-API
(	O
df2.DATE	O
)`	O
with	O

`df2['DATE	O
']	O
=	O
pd.to_datetime	B-API
(	O
df	O
[	O
"	O
DATE	O
"]	O
,	O
format=	O
"	O
%Y%m%d	O
")`	O
resolves	O
it	O
!	O

Thanks	O
for	O
your	O
help	O
!	O

ahh	O
,	O
this	O
is	O
because	O
the	O
values	O
in	O
`'DATE	O
'`	O
are	O
`	O
ints	O
`	O
rather	O
than	O
strings	O
.	O

When	O
you	O
pass	O
an	O
`	O
ints	O
`	O
to	O
`	O
pd.to_datetime	B-API
`	O
it	O
treats	O
them	O
as	O
nanoseconds	O
from	O
epoch	O
(	O
unless	O
you	O
specify	O
the	O
unit	O
,	O
e.g.	O
`unit='ms	O
'`	O
.	O

If	O
you	O
were	O
to	O
pass	O
a	O
string	O
such	O
as	O
`'20110706	O
'`	O
you	O
wouldn't	O
need	O
to	O
specify	O
the	O
format	O
.	O

Python	O
--	O
Pandas	O
:	O
How	O
to	O
apply	O
aggfunc	B-API
to	O
data	O
in	O
currency	O
format	O
?	O

I	O
have	O
a	O
table	O
above	O
.	O

Want	O
to	O
apply	O
groupby	B-API
function	O
to	O
the	O
data	O
and	O
apply	O
sum	B-API
(	O
over	O
revenue_total	O
)	O
.	O

Pandas	O
gives	O
an	O
NA	O
value	O
since	O
revenue_total	O
is	O
an	O
object	O
data	O
type	O
.	O

Any	O
help	O
#CODE	O

df	O
=	O
pd.read_csv	B-API
(	O
path	O
)	O

df[['Product	O
ID	O
','Revenue	O
Total	O
']]	O
.head()	B-API

df.groupby(['Product	O
ID	O
'])	O
[[	O
'	O
Revenue	O
Total	O
']]	O
.sum()	B-API

Code	O
should	O
be	O
added	O
as	O
an	O
edit	O
to	O
the	O
question	O
(	O
and	O
put	O
in	O
a	O
code	O
block	O
)	O
.	O

I	O
don't	O
know	O
enough	O
Python	O
to	O
do	O
the	O
edit	O
for	O
you	O
,	O
but	O
as	O
is	O
it	O
is	O
not	O
very	O
helpful	O
to	O
potential	O
answerers	O
.	O

I	O
want	O
the	O
header	O
=0	O
.	O

The	O
issue	O
is	O
dtype	B-API
for	O
Revenue	O
Total	O
is	O
object	O
and	O
group	O
by	O
is	O
not	O
able	O
to	O
perform	O
an	O
agfunc	B-API
on	O
that	O
.	O

Then	O
you	O
could	O
remove	O
the	O
`	O
$	O
`	O
and	O
commas	O
and	O
parse	O
it	O
into	O
a	O
DataFrame	B-API
using	O
#CODE	O

to	O
your	O
call	O
to	O
`	O
pd.read_csv	B-API
`	O
.	O

That	O
is	O
what	O
is	O
stripping	O
the	O
`	O
$	O
`	O
and	O
commas	O
.	O

I	O
have	O
a	O
DataFrame	B-API
that	O
looks	O
like	O
this	O
:	O
#CODE	O

Assuming	O
the	O
format	O
is	O
consistent	O
throughout	O
the	O
column	O
,	O
you	O
could	O
use	O
`	O
str.split	B-API
`	O
to	O
extract	O
the	O
numerators	O
and	O
denominators	O
,	O
and	O
then	O
do	O
the	O
division	O
:	O
#CODE	O

I	O
might	O
do	O
this	O
with	O
apply	B-API
rather	O
than	O
eval	B-API
(	O
especially	O
if	O
I	O
didn't	O
trust	O
the	O
source	O
):	O
#CODE	O

Note	O
:	O
You	O
could	O
catch	O
the	O
error	O
if	O
the	O
entry	O
is	O
not	O
of	O
the	O
correct	O
form	O
.	O

pandas	O
0.13	O
read_excel	B-API
new	O
format	O

I'm	O
working	O
with	O
pandas	O
a	O
few	O
time	O
ago	O
.	O

In	O
0.12	O
version	O
,	O
I	O
read	O
excel	O
files	O
using	O
pandas.read_excel	B-API
(	O
filename	O
,	O
sheetname	O
,	O
index_col	O
)	O
,	O
the	O
read	O
file	O
was	O
in	O
the	O
next	O
format	O
,	O
with	O
the	O
header	O
in	O
the	O
first	O
row	O
:	O
#CODE	O

Now	O
,	O
when	O
I	O
save	O
a	O
dataframe	B-API
to	O
an	O
excel	O
file	O
,	O
the	O
header	O
format	O
changes	O
,	O
in	O
the	O
first	O
row	O
are	O
the	O
columns	O
names	O
,	O
and	O
in	O
the	O
second	O
row	O
the	O
index	O
name	O
,	O
as	O
show	O
in	O
the	O
next	O
table	O
:	O
#CODE	O

The	O
DataFrame	B-API
will	O
be	O
written	O
in	O
a	O
way	O
that	O
tries	O
to	O
mimic	O
the	O
REPL	O
output	O
.	O

One	O
difference	O
from	O
0.12.0	O
version	O
is	O
that	O
the	O
index_label	O
will	O
be	O
placed	O
in	O
the	O
second	O
row	O
instead	O
of	O
the	O
?	O

rst	O
.	O

You	O
can	O
get	O
the	O
previous	O
behaviour	O
by	O
setting	O
the	O
merge_cells	O
option	O
in	O
to_excel()	B-API
to	O
False	O
:	O
#CODE	O

How	O
to	O
save	O
a	O
dataframe	B-API
as	O
a	O
csv	O
file	O
with	O
'	O
/	O
'	O
in	O
the	O
file	O
name	O

I	O
want	O
to	O
save	O
a	O
`	O
dataframe	B-API
`	O
to	O
a	O
`	O
.csv	O
`	O
file	O
with	O
the	O
name	O
`'123	O
/	O
123	O
'`	O
,	O
but	O
it	O
will	O
split	O
it	O
in	O
to	O
two	O
strings	O
if	O
I	O
just	O
type	O
like	O
`df.to_csv('123	O
/	O
123.csv	O
')`	O
.	O

and	O
that	O
a	O
charmap	O
can	O
find	O
a	O
character	O
that	O
looks	O
like	O
it	O
,	O
which	O
is	O
legal	O
.	O

In	O
this	O
case	O
a	O
division	O
character	O

Python	O
2.7	O
with	O
Pandas	O
:	O
How	O
does	O
one	O
recover	O
the	O
non	O
intersecting	O
parts	O
of	O
two	O
dataframes	O
?	O

I	O
have	O
two	O
data	O
frames	O
and	O
the	O
second	O
is	O
a	O
subset	O
of	O
the	O
first	O
.	O

How	O
do	O
I	O
now	O
find	O
the	O
portion	O
of	O
the	O
first	O
dataframe	B-API
that	O
is	O
not	O
contained	O
in	O
the	O
second	O
one	O
?	O

For	O
example	O
:	O
#CODE	O

Well	O
,	O
one	O
way	O
to	O
do	O
this	O
is	O
using	O
`	O
isin	B-API
`	O
(	O
but	O
you	O
can	O
also	O
do	O
it	O
with	O
the	O
`	O
merge	B-API
`	O
command	O
...	O
I	O
show	O
examples	O
for	O
both	O
)	O
.	O

For	O
example	O
:	O
#CODE	O

Explanation	O
.	O

`	O
isin	B-API
`	O
can	O
check	O
using	O
multiple	O
columns	O
if	O
you	O
feed	O
it	O
a	O
dict	O
:	O
#CODE	O

And	O
then	O
`	O
isin	B-API
`	O
will	O
create	O
a	O
booleen	O
df	O
which	O
I	O
can	O
use	O
to	O
select	O
the	O
columns	O
we	O
want	O
(	O
in	O
this	O
case	O
require	O
all	O
the	O
columns	O
to	O
match	O
and	O
then	O
negate	O
with	O
`	O
~	O
`)	O
:	O
#CODE	O

In	O
the	O
specific	O
example	O
we	O
don't	O
need	O
to	O
feed	O
`	O
isin	B-API
`	O
a	O
dict	O
version	O
of	O
the	O
dataframe	B-API
because	O
we	O
can	O
identify	O
the	O
valid	O
rows	O
by	O
only	O
looking	O
at	O
column	O
A	O
:	O
#CODE	O

You	O
can	O
also	O
do	O
this	O
with	O
`	O
merge	B-API
`	O
.	O

Create	O
a	O
unique	O
column	O
in	O
the	O
subset	O
dataframe	B-API
.	O

When	O
you	O
merge	O
,	O
the	O
unique	O
rows	O
from	O
the	O
larger	O
dataframe	B-API
will	O
have	O
`	O
NaN	O
`	O
for	O
the	O
column	O
you	O
created	O
:	O
#CODE	O

Edit	O
:	O
@USER	O
notes	O
that	O
the	O
merge	B-API
method	O
performs	O
much	O
better	O
for	O
large	O
dataframes	O
.	O

Great	O
,	O
thanks	O
for	O
the	O
very	O
complete	O
answer	O
showing	O
several	O
options	O
.	O

I	O
appreciate	O
your	O
help	O
!	O

I	O
just	O
wanted	O
to	O
add	O
some	O
further	O
documentation	O
for	O
other	O
people	O
reading	O
this	O
...	O

The	O
latter	O
method	O
mentioned	O
above	O
is	O
MUCH	O
faster	O
than	O
the	O
former	O
for	O
big	O
dataframes	O
.	O

I	O
tried	O
both	O
and	O
didn't	O
get	O
a	O
solution	O
after	O
10	O
minutes	O
of	O
sorting	O
through	O
dataframe	B-API
with	O
80,000	O
rows	O
and	O
5	O
columns	O
,	O
but	O
it	O
only	O
takes	O
a	O
few	O
seconds	O
for	O
the	O
latter	O
merge	B-API
method	O
.	O

Thanks	O
again	O
!	O

Thanks	O
@USER	O
,	O
I	O
edited	O
the	O
answer	O
to	O
indicate	O
the	O
better	O
performance	O
of	O
the	O
`	O
merge	B-API
`	O
method	O
.	O

Groupby	B-API
values	O
start	O
with	O

is	O
there	O
a	O
way	O
to	O
compute	O
all	O
value	O
with	O
the	O
same	O
prefix	O
#CODE	O

No	O
,	O
you'd	O
have	O
to	O
build	O
a	O
list	O
where	O
the	O
prefix	O
is	O
matched	O
in	O
the	O
columns	O
but	O
there	O
is	O
no	O
built	O
in	O
method	O

Please	O
add	O
reproducible	O
code	O
and	O
well-formatted	O
tables	O
.	O

Just	O
group	O
by	O
`	O
Date	B-API
`	O
and	O
`	O
Income	O
`	O
and	O
get	O
the	O
sum	O
:	O
#CODE	O

in	O
order	O
to	O
filter	O
the	O
dataframe	B-API
before	O
getting	O
the	O
sum	O
,	O
just	O
set	O
the	O
filter	O
before	O
grouping	O
:	O
#CODE	O

Thanks	O
.	O

This	O
i	O
already	O
did	O
,	O
i	O
also	O
did	O
for	O
many	O
filters	O
(	O
combination	O
of	O
2	O
or	O
3	O
)	O
.	O

How	O
can	O
i	O
implement	O
below	O
conditions	O
:	O
1	O
)	O
If	O
only	O
one	O
filter	O
is	O
selected	O
e.g.	O
Male	O
then	O
all	O
other	O
filters	O
should	O
take	O
all	O
the	O
values	O
corresponding	O
to	O
Only	O
Male	O
filter	O
2	O
)	O
If	O
only	O
two	O
filters	O
are	O
selected	O
e.g.	O
Only	O
Male	O
and	O
IT	O
profession	O
are	O
selected	O
..	O
then	O
all	O
values	O
correspoding	O
to	O
only	O
these	O
two	O
filters	O
should	O
be	O
given	O
.	O

Actually	O
grouping	O
the	O
dataframe	B-API
lead	O
to	O
have	O
the	O
`	O
Date	B-API
`	O
as	O
index	O
of	O
the	O
new	O
dataframe	B-API
,	O
and	O
the	O
`	O
Income	O
`	O
as	O
values	O
.	O

So	O
you	O
can	O
access	O
them	O
through	O
`	O
grouped.index	O
`	O
and	O
`	O
grouped.values	O
`	O
.	O

Hope	O
that	O
helps	O
.	O

I	O
update	O
also	O
the	O
answer	O
with	O
a	O
new	O
way	O
to	O
do	O
it	O
.	O

Pandas	O
:	O
Replacing	O
column	O
values	O
in	O
dataframe	B-API

I'm	O
trying	O
to	O
replace	O
the	O
values	O
in	O
one	O
column	O
of	O
a	O
dataframe	B-API
.	O

The	O
column	O
(	O
'	O
female	O
')	O
only	O
contains	O
the	O
values	O
'	O
female	O
'	O
and	O
'	O
male	O
'	O
.	O

I	O
would	O
ideally	O
like	O
to	O
get	O
some	O
output	O
which	O
resembles	O
the	O
following	O
loop	O
element-wise	O
.	O

#CODE	O

Any	O
help	O
will	O
be	O
appreciated	O
.	O

The	O
reason	O
your	O
code	O
doesn't	O
work	O
is	O
because	O
using	O
`['female	O
']`	O
on	O
a	O
column	O
(	O
the	O
second	O
`'female	O
'`	O
in	O
your	O
`w['female']['female	O
']`)	O
doesn't	O
mean	O
"	O
select	O
rows	O
where	O
the	O
value	O
is	O
'	O
female	O
'"	O
.	O

It	O
means	O
to	O
select	O
rows	O
where	O
the	O
index	O
is	O
'	O
female	O
'	O
,	O
of	O
which	O
there	O
may	O
not	O
be	O
any	O
in	O
your	O
DataFrame	B-API
.	O

Thanks	O
.	O

Exactly	O
what	O
I	O
was	O
looking	O
for	O
.	O

If	O
I	O
were	O
to	O
map	O
'	O
female	O
'	O
to	O
1	O
and	O
anything	O
else	O
to	O
'	O
0	O
'	O
.	O

How	O
would	O
that	O
work	O
?	O

You	O
can	O
edit	O
a	O
subset	O
of	O
a	O
dataframe	B-API
by	O
using	O
loc	B-API
:	O
#CODE	O

Pandas	O
merge	B-API
not	O
giving	O
expected	O
output	O
with	O
datetime	O

I	O
have	O
two	O
dataframes	O
:	O
the	O
first	O
dataframe	B-API
"	O
fgblquotef	O
"	O
sample	O
is	O
:	O
#CODE	O

the	O
second	O
dataframe	B-API
"	O
df	O
"	O
has	O
column	O
df	O
[	O
"	O
DateTimesy	O
"]	O
created	O
using	O
:	O
#CODE	O

and	O
then	O
I	O
merge	O
using	O
:	O
#CODE	O

Which	O
is	O
wrong	O
because	O
there	O
should	O
be	O
"	O
fgblquotef	O
"	O
entries	O
mixed	O
up	O
in	O
there	O
as	O
well	O
and	O
not	O
just	O
"	O
df	O
"	O
entries	O
.	O

Can	O
anyone	O
explain	O
what	O
is	O
going	O
on	O
here	O
and	O
where	O
I	O
have	O
made	O
a	O
mistake	O
?	O

make	O
sure	O
both	O
dataframe	B-API
columns	O
have	O
the	O
same	O
data	O
type	O
.	O

I	O
know	O
Pandas	O
has	O
been	O
bugy	O
at	O
times	O
with	O
dates	O
but	O
hard	O
to	O
say	O
without	O
more	O
information	O
.	O

What	O
more	O
information	O
can	O
I	O
provide	O
:	O
I	O
use	O
fgbmquotef	O
[	O
"	O
DateTimes	O
"]	O
=	O
pd.to_datetime	B-API
(	O
fgbmquotef.dateTime	O
,	O
unit	O
=	O
"	O
s	O
")	O
and	O
the	O
same	O
for	O
df	O
to	O
create	O
dtype	B-API
:	O
datetime64	O
[	O
ns	O
]	O
.	O

df2	O
=	O
pd.merge	B-API
(	O
df	O
,	O
fgbmquotef	O
,	O
left_on	O
=	O
"	O
DateTimesy	O
"	O
,	O
right_on	O
=	O
"	O
DateTimesy	O
"	O
,	O
how	O
=	O
"	O
outer	O
")	O
#although	O
you	O
shouldn't	O
have	O
to	O
.	O

or	O
without	O
suffixes	O
:	O
#CODE	O

Finally	O
try	O
the	O
concatenate	O
function	O
:	O
#URL	O

I	O
am	O
getting	O
:	O
MergeError	O
:	O
Must	O
pass	O
right_on	O
or	O
right_index=True	O
.	O

Also	O
how	O
will	O
this	O
merge	O
using	O
DateTimesy	O
.	O

I	O
just	O
edited	O
the	O
answer	O
,	O
try	O
:	O
df2	O
=	O
pd.merge	B-API
(	O
df.set_index	B-API
(	O
"	O
DateTimesy	O
"	O
,	O
drop=False	O
)	O
,	O
fgbmquotef.set_index	O
(	O
"	O
DateTimesy	O
"	O
,	O
drop=False	O
)	O
,	O
left_index=True	O
,	O
right_index=True	O
,	O
how	O
=	O
"	O
outer	O
"	O
,	O
suffixes	O
=	O
('_df	O
'	O
,	O
'	O
_fgbmquotef	O
'))	O

or	O
maybe	O
:	O
df2	O
=	O
pd.merge	B-API
(	O
df.set_index	B-API
(	O
"	O
DateTimesy	O
")	O
,	O
fgbmquotef.set_index	O
(	O
"	O
DateTimesy	O
")	O
,	O
left_index=True	O
,	O
right_index=True	O
,	O
how	O
=	O
"	O
outer	O
")	O

I	O
have	O
ticked	O
this	O
as	O
the	O
right	O
answer	O
but	O
it	O
seems	O
to	O
be	O
a	O
bug	O
in	O
Pandas	O
,	O
unless	O
I	O
am	O
mistaken	O
.	O

Perhaps	O
I	O
should	O
post	O
it	O
as	O
a	O
bug	O
to	O
their	O
forum	O
.	O

BTW	O
i	O
used	O
df2	O
=	O
pd.merge	B-API
(	O
df.set_index	B-API
(	O
"	O
DateTimesy	O
"	O
,	O
drop=False	O
)	O
,	O
fgbmquotef.set_index	O
(	O
"	O
DateTimesy	O
"	O
,	O
drop=False	O
)	O
,	O
left_index=True	O
,	O
right_index=True	O
,	O
how	O
=	O
"	O
outer	O
")	O
.	O

Thanks	O

Well	O
,	O
one	O
approach	O
is	O
the	O
following	O
:	O
(	O
1	O
)	O
do	O
a	O
`	O
groupby	B-API
/	O
apply	B-API
`	O
with	O
'	O
id	O
'	O
as	O
grouping	O
variable	O
.	O

(	O
2	O
)	O
Within	O
the	O
apply	B-API
,	O
`	O
resample	B-API
`	O
the	O
group	O
to	O
a	O
daily	O
time	O
series	B-API
.	O

(	O
3	O
)	O
Then	O
just	O
using	O
`	O
rolling_sum	B-API
`	O
(	O
and	O
shift	O
so	O
you	O
don't	O
include	O
the	O
current	O
rows	O
'	O
x	O
'	O
value	O
)	O
to	O
compute	O
the	O
sum	O
of	O
your	O
70	O
day	O
lookback	O
periods	O
.	O

(	O
4	O
)	O
Reduce	O
the	O
group	O
back	O
to	O
only	O
the	O
original	O
observations	O
:	O
#CODE	O

You	O
are	O
going	O
to	O
need	O
your	O
data	O
sorted	O
by	O
`['id','dates	O
']`	O
.	O

Now	O
we	O
can	O
do	O
the	O
`	O
groupby	B-API
/	O
apply	B-API
`	O
:	O
#CODE	O

Thanks	O
,	O
this	O
seems	O
to	O
do	O
it	O
!	O

If	O
I	O
wanted	O
the	O
70	O
to	O
be	O
an	O
argument	O
of	O
the	O
past	O
function	O
(	O
i.e.	O
def	O
past	O
(	O
g	O
,	O
lookback	O
))	O
,	O
how	O
would	O
I	O
then	O
pass	O
that	O
argument	O
to	O
.apply	B-API
(	O
past	O
)	O
?	O

It	O
just	O
becomes	O
the	O
next	O
parameter	O
in	O
the	O
`	O
apply	B-API
`	O
.	O

See	O
me	O
edit	O
for	O
details	O
.	O

the	O
SOQL	O
docs	O
cover	O
all	O
these	O
.	O

There	O
are	O
also	O
tools	O
like	O
SoqlX	O
,	O
Workbench	O
etc	O
that	O
let	O
you	O
run	O
add-hoc	O
queries	O
,	O
these	O
are	O
useful	O
for	O
trying	O
things	O
out	O
without	O
having	O
to	O
run	O
your	O
full	O
integration	O
.	O

I	O
am	O
playing	O
with	O
the	O
excellent	O
Scikit-learn	O
today	O
.	O

I'm	O
forming	O
the	O
x's	O
out	O
of	O
panels	O
sliced	O
on	O
the	O
minor_axis	O
and	O
y's	O
out	O
of	O
DataFrame	B-API
sliced	O
on	O
columns	O
.	O

At	O
the	O
moment	O
I'm	O
doing	O
endless	O
iterations	O
,	O
does	O
any	O
.apply()	B-API
Masters	O
out	O
there	O
have	O
any	O
idea	O
how	O
to	O
speed	O
this	O
up	O
?	O

#CODE	O

My	O
idea	O
was	O
to	O
apply	B-API
this	O
function	O
(	O
or	O
similar	O
)	O
column	O
wise	O
.	O

Have	O
played	O
with	O
.apply()	B-API
but	O
because	O
its	O
a	O
double	O
(	O
or	O
triple	O
)	O
function	O
call	O
i.e.	O
f1	O
.	O

(	O
)	O
.f2	O
(	O
x	O
,	O
y	O
)	O
or	O
f1	O
.	O

(	O
)	O
.f2	O
(	O
x	O
,	O
y	O
)	O
.f3	O
(	O
x	O
,	O
y	O
)	O
it	O
gives	O
me	O
an	O
error	O
.	O

Any	O
ideas	O
would	O
be	O
greatly	O
appreciated	O
and	O
I	O
think	O
this	O
would	O
be	O
a	O
very	O
useful	O
bit	O
of	O
code	O
to	O
have	O
out	O
there	O
!	O

I	O
ran	O
a	O
million	O
regressions	O
and	O
as	O
you	O
can	O
see	O
it	O
took	O
~	O
2.5	O
minutes	O
.	O

This	O
will	O
vary	O
based	O
on	O
the	O
number	O
of	O
cores	O
you	O
have	O
.	O

`	O
result	O
`	O
will	O
be	O
a	O
list	O
of	O
your	O
scores	O
so	O
you	O
can	O
easily	O
reproduce	O
the	O
`	O
r2	O
`	O
`	O
Series	B-API
`	O
in	O
your	O
example	O
.	O

Good	O
Luck	O
!	O

python	O
pandas	O
:	O
how	O
to	O
use	O
concat()	B-API
to	O
do	O
a	O
SQL	O
union	O
?	O

I	O
understand	O
from	O
the	O
documentation	O
that	O
I	O
can	O
use	O
`	O
concat()	B-API
`	O
to	O
do	O
the	O
equivalent	O
of	O
a	O
SQL	O
UNION	O
.	O

Let's	O
say	O
I	O
have	O
a	O
table	O
with	O
3	O
fields	O
:	O
id	O
,	O
value1	O
,	O
value2	O
and	O
I	O
want	O
a	O
new	O
table	O
with	O
2	O
columns	O
,	O
where	O
id	O
is	O
repeated	O
,	O
and	O
value2	O
is	O
appended	O
below	O
value1	O
.	O

I	O
end	O
up	O
with	O
a	O
dataframe	B-API
with	O
3	O
,	O
not	O
2	O
columns	O
.	O

I	O
think	O
you	O
have	O
to	O
do	O
it	O
this	O
way	O
,	O
pandas	O
will	O
try	O
to	O
use	O
existing	O
indices	O
and	O
column	O
labels	O
for	O
alignment	O
without	O
renaming	O
you	O
can't	O
infer	O
how	O
the	O
values	O
should	O
align	O
without	O
renaming	O

You	O
could	O
try	O
the	O
melt	B-API
function	O
#CODE	O

You	O
didn't	O
show	O
exactly	O
what	O
the	O
data	O
looks	O
like	O
(	O
you	O
say	O
it's	O
delimited	O
by	O
semicolons	O
,	O
but	O
your	O
examples	O
don't	O
have	O
any	O
)	O
,	O
but	O
if	O
it	O
looks	O
like	O
#CODE	O

It	O
is	O
actually	O
much	O
faster	O
to	O
use	O
the	O
csv	O
lib	O
and	O
str.replace	B-API
:	O
#CODE	O

You	O
could	O
just	O
str.split	B-API
:	O
#CODE	O

p.s.	O

Background	O
info	O
:	O
The	O
data	O
is	O
generated	O
in	O
an	O
Android	O
app	O
as	O
a	O
the	O
java.util.Calendar	O
class	O
,	O
then	O
converted	O
to	O
a	O
string	O
in	O
Java	O
,	O
written	O
to	O
a	O
csv	O
and	O
then	O
sent	O
to	O
the	O
python	O
server	O
where	O
I	O
read	O
it	O
in	O
using	O
pandas	O
`	O
read_csv	B-API
`	O
.	O

`	O
datetime.strptime	O
(	O
x	O
,	O
"	O
%Y%m%d%H%M%S%f	O
")`	O

%b	O
:	O
Month	O
as	O
locale	O
s	O
abbreviated	O
name	O
.	O

Might	O
be	O
worth	O
highlighting	O
that	O
the	O
key	O
difference	O
here	O
is	O
the	O
use	O
of	O
`	O
%m	O
`	O
for	O
the	O
month	O
(	O
Month	O
as	O
a	O
zero-padded	O
decimal	O
number	O
)	O
instead	O
of	O
`	O
%b	O
`	O
(	O
Month	O
as	O
locale	O
s	O
abbreviated	O
name	O
)	O

`	O
%b	O
`	O
is	O
for	O
locale-based	O
month	O
name	O
abbreviations	O
like	O
`	O
Jan	O
`	O
,	O
`	O
Feb	O
`	O
,	O
etc	O
.	O

Pandas	O
groupby	B-API
:	O
compute	O
(	O
relative	O
)	O
sizes	O
and	O
save	O
in	O
original	O
dataframe	B-API

However	O
,	O
I	O
don't	O
know	O
how	O
to	O
put	O
them	O
back	O
into	O
the	O
old	O
dataframe	B-API
.	O

I	O
can	O
access	O
them	O
through	O
`	O
intensity	O
[	O
0	O
]`	O
,	O
but	O
`	O
intensity.loc()	O
`	O
gives	O
a	O
LocIndexer	O
not	O
callable	O
error	O
.	O

It's	O
a	O
multi-index	O
.	O

You	O
can	O
reset	O
the	O
index	O
by	O
calling	O
`	O
.reset_index()	B-API
`	O
to	O
your	O
resultant	O
dataframe	B-API
.	O

Or	O
you	O
can	O
disable	O
it	O
when	O
you	O
compute	O
the	O
group-by	O
operation	O
,	O
by	O
specifying	O
`	O
as_index=False	O
`	O
to	O
the	O
`	O
groupby()	B-API
`	O
,	O
like	O
:	O
#CODE	O

If	O
you	O
want	O
to	O
compute	O
`	O
intensity	O
/	O
mean	O
(	O
intensity	O
)`	O
,	O
where	O
`	O
mean	O
(	O
intensity	O
)`	O
is	O
based	O
only	O
on	O
the	O
`	O
year	B-API
`	O
and	O
not	O
`	O
year	O
/	O
groupid	O
`	O
subsets	O
,	O
then	O
you	O
first	O
have	O
to	O
create	O
the	O
`	O
mean	O
(	O
intensity	O
)`	O
based	O
on	O
the	O
`	O
year	B-API
`	O
only	O
,	O
like	O
:	O
#CODE	O

And	O
then	O
compute	O
the	O
`	O
intensity	O
/	O
mean	O
(	O
intensity	O
)`	O
for	O
all	O
`	O
year	O
/	O
groupid	O
`	O
subset	O
,	O
where	O
the	O
`	O
mean	O
(	O
intensity	O
)`	O
is	O
derived	O
only	O
from	O
`	O
year	B-API
`	O
subset	O
:	O
#CODE	O

Or	O
maybe	O
you	O
want	O
to	O
compute	O
it	O
based	O
on	O
the	O
`	O
year	B-API
`	O
only	O
,	O
not	O
on	O
`	O
year	O
/	O
groupid	O
`	O
combination	O
?	O

Actually	O
,	O
days	O
later	O
,	O
I	O
found	O
out	O
that	O
the	O
first	O
answer	O
to	O
this	O
double	O
question	O
was	O
wrong	O
.	O

Perhaps	O
someone	O
can	O
elaborate	O
to	O
what	O
`	O
.size()	B-API
`	O
actually	O
does	O
,	O
but	O
this	O
is	O
just	O
in	O
case	O
someone	O
googles	O
this	O
question	O
does	O
not	O
follow	O
my	O
wrong	O
path	O
.	O

It	O
turned	O
out	O
that	O
`	O
.size()	B-API
`	O
had	O
way	O
less	O
rows	O
than	O
the	O
original	O
object	O
(	O
also	O
if	O
I	O
used	O
`	O
reset_index()	B-API
`	O
,	O
and	O
however	O
I	O
tried	O
to	O
stack	O
the	O
sizes	O
back	O
into	O
the	O
original	O
object	O
,	O
there	O
were	O
a	O
lot	O
of	O
rows	O
left	O
with	O
`	O
NaN	O
`	O
.	O

The	O
following	O
,	O
however	O
,	O
works	O
#CODE	O

One	O
way	O
would	O
be	O
to	O
use	O
`	O
shift	B-API
`	O
to	O
move	O
the	O
relevant	O
column	O
down	O
`	O
n	O
`	O
rows	O
and	O
then	O
concatenate	O
the	O
entries	O
(	O
they	O
are	O
strings	O
so	O
we	O
can	O
use	O
`	O
+	O
`)	O
:	O
#CODE	O

This	O
creates	O
strings	O
of	O
the	O
previous	O
three	O
entries	O
separated	O
by	O
a	O
comma	O
and	O
space	O
(	O
not	O
lists	O
)	O
.	O

I'd	O
avoid	O
using	O
lists	O
in	O
DataFrames	O
if	O
possible	O
as	O
things	O
can	O
get	O
a	O
little	O
messy	O
.	O

What	O
do	O
you	O
want	O
to	O
do	O
with	O
those	O
lists	O
?	O

Storing	O
lists	O
inside	O
Series	B-API
/	O
DataFrames	O
is	O
not	O
usually	O
very	O
convenient	O
.	O

Anyway	O
,	O
this	O
would	O
get	O
you	O
close	O
.	O

You	O
have	O
to	O
handle	O
the	O
`	O
nans	O
`	O
,	O
and	O
then	O
you're	O
done	O
.	O

#CODE	O

Notice	O
that	O
we	O
have	O
to	O
convert	O
to	O
a	O
tuple	O
and	O
then	O
a	O
list	O
,	O
to	O
avoid	O
pandas	O
automatically	O
taking	O
our	O
list	O
and	O
making	O
it	O
back	O
into	O
a	O
Series	B-API
.	O

Try	O
this	O
and	O
you'll	O
see	O
why	O
it	O
doesn't	O
work	O
:	O
#CODE	O

This	O
solution	O
avoids	O
looping	O
,	O
but	O
I'm	O
not	O
sure	O
whether	O
it	O
really	O
counts	O
as	O
'	O
vectorized	O
'	O
,	O
since	O
once	O
you	O
start	O
using	O
`	O
apply()	B-API
`	O
I	O
think	O
you	O
start	O
losing	O
any	O
performance	O
benefits	O
granted	O
by	O
vectorization	O
:	O
#CODE	O

Memory	O
optimization	O
when	O
selecting	O
from	O
a	O
pandas	O
dataframe	B-API

I	O
have	O
a	O
rather	O
large	O
pandas	O
dataframe	B-API
(	O
1.7G	O
)	O
from	O
which	O
I	O
am	O
selecting	O
some	O
columns	O
to	O
do	O
some	O
computaton	O
(	O
find	O
maximum	O
value	O
of	O
the	O
three	O
selected	O
columns	O
)	O
.	O

It	O
seems	O
that	O
this	O
operation	O
is	O
memory	O
intensive	O
.	O

I	O
am	O
trying	O
to	O
find	O
a	O
way	O
to	O
avoid	O
this	O
memory	O
overhead	O
.	O

For	O
the	O
purpose	O
to	O
this	O
question	O
,	O
I	O
a	O
simplifying	O
the	O
dataframe	B-API
and	O
using	O
fake	O
data	O
.	O

My	O
code	O
and	O
the	O
memory	O
footprint	O
is	O
shown	O
below	O
,	O
#CODE	O

I	O
am	O
dealing	O
with	O
a	O
dataPanel	O
that	O
is	O
1.7G	O
.	O

When	O
I	O
do	O
this	O
selection	O
operation	O
at	O
multiple	O
place	O
in	O
my	O
program	O
,	O
I	O
end	O
up	O
with	O
an	O
overhead	O
that	O
causes	O
my	O
8G	O
machine	O
to	O
hang	O
and	O
freeze	O
.	O

you	O
could	O
do	O
that	O
,	O
or	O
work	O
with	O
the	O
data	O
in	O
a	O
HDF	O
file	O
.	O

Make	O
sure	O
your	O
dtypes	B-API
are	O
correct	O
(	O
e.g.	O
not	O
object	O
,	O
except	O
for	O
strings	O
)	O
.	O

Just	O
apply	O
the	O
function	O
directly	O
-	O
I	O
guess	O
this	O
will	O
take	O
more	O
CPU	O
as	O
it's	O
calculating	O
all	O
the	O
maxes	O
,	O
then	O
just	O
getting	O
the	O
ones	O
you	O
want	O
,	O
but	O
doesn't	O
create	O
a	O
new	O
variable	O
.	O

#CODE	O

I	O
have	O
a	O
large	O
(	O
~160	O
million	O
rows	O
)	O
dataframe	B-API
that	O
I've	O
stored	O
to	O
disk	O
with	O
something	O
like	O
this	O
:	O
#CODE	O

I	O
have	O
another	O
dataframe	B-API
containing	O
~18000	O
"	O
incidents	O
.	O

"	O
Each	O
incident	O
consists	O
of	O
some	O
(	O
as	O
few	O
as	O
hundreds	O
,	O
as	O
many	O
as	O
hundreds	O
of	O
thousands	O
)	O
individual	O
records	O
.	O

I	O
need	O
to	O
collect	O
some	O
simple	O
statistics	O
for	O
each	O
incident	O
and	O
store	O
them	O
in	O
order	O
to	O
collect	O
some	O
aggregate	O
statistics	O
.	O

Currently	O
I	O
do	O
this	O
like	O
so	O
:	O
#CODE	O

This	O
all	O
works	O
fine	O
except	O
for	O
the	O
fact	O
that	O
each	O
`	O
store.select()	O
`	O
statement	O
takes	O
roughly	O
5	O
seconds	O
which	O
means	O
that	O
processing	O
the	O
full	O
month's	O
worth	O
of	O
data	O
requires	O
somewhere	O
between	O
24-30	O
hours	O
of	O
processing	O
.	O

Meanwhile	O
,	O
the	O
actual	O
statistics	O
I	O
need	O
are	O
relatively	O
simple	O
:	O
#CODE	O

Would	O
I	O
benefit	O
from	O
using	O
store.select_as_index	O
?	O

If	O
I	O
receive	O
an	O
index	O
I'd	O
still	O
need	O
to	O
access	O
the	O
data	O
to	O
get	O
the	O
statistics	O
correct	O
?	O

what	O
is	O
the	O
relative	O
frequency	O
of	O
c_id	O
and	O
f_id	O
,	O
are	O
they	O
relatively	O
unique	O
or	O
very	O
common	O
,	O
how	O
big	O
is	O
the	O
range	O
that	O
you	O
are	O
selecting	O
each	O
time	O
(	O
e.g.	O
the	O
timestamp	O
range	O
)	O

Use	O
a	O
hierarchical	O
query	O
in	O
chunks	O
.	O

What	O
I	O
mean	O
is	O
this	O
.	O

Since	O
you	O
have	O
a	O
relatively	O
small	O
number	O
of	O
`	O
c_id	O
`	O
and	O
`	O
f_id	O
`	O
that	O
you	O
care	O
about	O
,	O
structure	O
a	O
single	O
query	O
something	O
like	O
this	O
.	O

This	O
is	O
kind	O
of	O
like	O
using	O
`	O
isin	B-API
`	O
.	O

#CODE	O

The	O
key	O
here	O
is	O
to	O
process	O
so	O
that	O
the	O
`	O
isin	B-API
`	O
DOES	O
not	O
have	O
more	O
that	O
32	O
members	O

If	O
you	O
exceed	O
this	O
,	O
the	O
query	O
will	O
work	O
,	O
but	O
it	O
will	O
drop	O
that	O
variable	O
and	O
do	O
a	O
reindex	B-API

So	O
the	O
query	O
scans	O
'	O
blocks	O
'	O
of	O
data	O
(	O
which	O
is	O
what	O
the	O
indexes	O
point	O
to	O
)	O
.	O

If	O
you	O
have	O
lots	O
of	O
hits	O
across	O
many	O
blocks	O
then	O
the	O
query	O
is	O
slower	O
.	O

How	O
can	O
I	O
rename	O
with	O
data.rename	O
(	O
index={	O
???	O
}	O
,	O
columns={	O
???	O
}	O
)	O
the	O
index	O
respectively	O
columns	O
?	O

The	O
current	O
one	O
(	O
Global	O
Data	O
Item	O
(	O
000s	O
)	O
2012	O
2011	O
2010	O
2009	O
2008	O
2007	O
2006	O
)	O
shall	O
be	O
replaced	O
by	O
(	O
10	O
Sub-Data	O
Item	O
2012	O
2011	O
2010	O
2009	O
2008	O
)	O

I'm	O
not	O
sure	O
that	O
this	O
is	O
your	O
task	O
,	O
but	O
from	O
comments	O
I	O
see	O
that	O
you	O
want	O
to	O
use	O
first	O
row	O
of	O
DataFrame	B-API
as	O
column	O
names	O
.	O

If	O
this	O
is	O
the	O
case	O
,	O
you	O
can	O
do	O
:	O
#CODE	O

I	O
have	O
a	O
column	O
'	O
datedif	O
'	O
in	O
my	O
dataframe	B-API
as	O
:	O
#CODE	O

data['datedif_day	O
']	O
=	O
data['datedif	O
']	O
.dt	B-API
.days	B-API

Error	O
:	O
AttributeError	O
:	O
'	O
Series	B-API
'	O
object	O
has	O
no	O
attribute	O
'	O
dt	B-API
'	O

Remove	O
or	O
comment	O
out	O
```	O
data.datedif	O
=	O
pd.to_datetime	B-API
(	O
data.datedif	O
)```	O
-	O
then	O
```	O
datedif	O
```	O
will	O
be	O
a	O
[	O
```	O
Timedelta	O
```	O
object	O
]	O
(	O
#URL	O
)	O
.	O

data['datedif	O
']	O
=	O
data['datedif	O
']	O
.astype	B-API
(	O
np.numpy64	O
)	O

I	O
create	O
a	O
DataFrame	B-API
?	O

#CODE	O

I	O
know	O
DataFrame	B-API
select	O
a	O
col	O
as	O
default.What	O
happened	O
when	O
I	O
run	O
`	O
data	O
[	O
se	O
]`	O
?	O

With	O
DataFrame	B-API
,	O
slicing	O
inside	O
of	O
[	O
]	O
slices	O
the	O
rows	O
.	O

This	O
is	O
provided	O
largely	O
as	O
a	O
convenience	O
since	O
it	O
is	O
such	O
a	O
common	O
operation	O
.	O

#CODE	O

In	O
your	O
example	O
where	O
you	O
use	O
`	O
range	O
(	O
2	O
)`	O
that	O
gives	O
you	O
`	O
[	O
0	O
,	O
1	O
]`	O
as	O
list	O
.	O

What	O
I	O
think	O
you	O
need	O
is	O
`	O
data	O
[	O
0:1	O
]`	O
to	O
slice	O
the	O
`	O
DataFrame	B-API
`	O
and	O
get	O
rows	O
0	O
and	O
1	O
which	O
is	O
the	O
same	O
as	O
`	O
data	O
[:	O
1	O
]`	O
omitting	O
the	O
zero	O
.	O

If	O
you	O
wanted	O
for	O
example	O
rows	O
3	O
,	O
4	O
and	O
5	O
that	O
would	O
be	O
`	O
data	O
[	O
3:5	O
]`	O
.	O

Thanks.If	O
I	O
want	O
to	O
select	O
these	O
row	O
[	O
2	O
,	O
4	O
,	O
3	O
,	O
5	O
,	O
1	O
]	O
,	O
I	O
run	O
df	O
[	O
slice	O
([	O
2	O
,	O
4	O
,	O
3	O
,	O
5	O
,	O
1	O
])]	O
,	O
there	O
is	O
an	O
error.I	O
just	O
used	O
df.ix	B-API
[	O
2	O
,	O
4	O
,	O
3	O
,	O
5	O
,	O
1	O
]	O
.Is	O
there	O
a	O
different	O
way	O
to	O
do	O
that	O
?	O

I	O
have	O
a	O
pandas	O
dataframe	B-API
that	O
contains	O
height	O
information	O
and	O
I	O
can't	O
seem	O
to	O
figure	O
out	O
how	O
to	O
convert	O
the	O
somewhat	O
unstructured	O
information	O
into	O
an	O
integer	O
.	O

I	O
figured	O
the	O
best	O
way	O
to	O
approach	O
this	O
was	O
to	O
use	O
regex	O
but	O
the	O
main	O
problem	O
I'm	O
having	O
is	O
that	O
when	O
I	O
attempt	O
to	O
simplify	O
a	O
problem	O
to	O
use	O
regex	O
I	O
usually	O
take	O
the	O
first	O
item	O
in	O
the	O
dataframe	B-API
(	O
7	O
'	O
5.5	O
'')	O
and	O
try	O
to	O
use	O
regex	O
specifically	O
on	O
it	O
.	O

It	O
seemed	O
impossible	O
for	O
me	O
to	O
put	O
this	O
data	O
in	O
a	O
string	O
because	O
of	O
the	O
quotes	O
.	O

So	O
,	O
I'm	O
really	O
confused	O
on	O
how	O
to	O
approach	O
this	O
problem	O
.	O

here	O
is	O
my	O
dataframe	B-API
:	O
#CODE	O

My	O
next	O
option	O
would	O
be	O
writing	O
this	O
to	O
csv	O
and	O
using	O
excel	O
,	O
but	O
I	O
would	O
prefer	O
to	O
learn	O
how	O
to	O
do	O
it	O
in	O
python	O
/	O
pandas	O
.	O
any	O
help	O
would	O
be	O
greatly	O
appreciated	O
.	O

How	O
about	O
using	O
string.find()	O
to	O
locate	O
the	O
'	O
and	O
the	O
"	O
and	O
then	O
cast	O
and	O
then	O
do	O
your	O
conversion	O
?	O

All	O
that	O
could	O
be	O
done	O
within	O
a	O
function	O
and	O
passed	O
to	O
an	O
Apply	B-API

One	O
possible	O
method	O
without	O
using	O
`	O
regex	O
`	O
is	O
to	O
write	O
your	O
own	O
function	O
and	O
just	O
`	O
apply	B-API
`	O
it	O
to	O
the	O
column	O
/	O
Series	B-API
of	O
your	O
choosing	O
.	O

You	O
could	O
apply	O
that	O
regular	O
expression	O
to	O
the	O
elements	O
in	O
the	O
data	O
.	O

However	O
,	O
the	O
solution	O
of	O
mapping	O
your	O
own	O
function	O
over	O
the	O
data	O
works	O
well	O
.	O

Thought	O
you	O
might	O
want	O
to	O
see	O
how	O
you	O
could	O
approach	O
this	O
using	O
your	O
original	O
idea	O
.	O

I	O
would	O
like	O
to	O
make	O
a	O
2-part	O
graphic	O
.	O

On	O
the	O
left	O
,	O
I	O
want	O
to	O
have	O
a	O
visualization	O
(	O
that	O
I	O
already	O
know	O
how	O
to	O
make	O
)	O
.	O

On	O
the	O
right	O
,	O
I	O
want	O
to	O
place	O
a	O
table	O
that	O
includes	O
more	O
numerical	O
data	O
about	O
the	O
same	O
subject	O
covered	O
by	O
the	O
graph	O
.	O

Say	O
I	O
set	O
things	O
up	O
with	O
`	O
axes	O
[	O
0	O
]`	O
for	O
the	O
visualization	O
and	O
`	O
axes	O
[	O
1	O
]`	O
as	O
the	O
place	O
where	O
I	O
want	O
the	O
table	O
.	O

I'm	O
using	O
Pandas	O
,	O
and	O
I	O
have	O
all	O
the	O
info	O
I'd	O
like	O
for	O
my	O
table	O
in	O
a	O
nice	O
neat	O
`	O
DataFrame	B-API
`	O
.	O

(	O
For	O
now	O
,	O
let's	O
assume	O
that	O
`	O
DataFrame	B-API
`	O
has	O
a	O
regular	O
`	O
Index	B-API
`	O
on	O
both	O
the	O
rows	O
and	O
the	O
columns	O
,	O
not	O
a	O
`	O
MultiIndex	O
`	O
,	O
but	O
I'm	O
curious	O
how	O
answers	O
would	O
change	O
if	O
we	O
dropped	O
that	O
assumption	O
.	O
)	O
Let's	O
call	O
that	O
`	O
tabledf	O
`	O
.	O

Thanks	O
for	O
the	O
suggestion	O
.	O

I'm	O
moderately	O
familiar	O
with	O
generating	O
tables	O
in	O
LaTeX	O
.	O

My	O
experience	O
is	O
that	O
it's	O
a	O
bit	O
of	O
a	O
pain	O
.	O

It	O
looks	O
like	O
`	O
pandas.DataFrame	B-API
`	O
has	O
a	O
`	O
to_latex	B-API
`	O
method	O
.	O

Seems	O
like	O
that's	O
the	O
next	O
place	O
I	O
should	O
look	O
.	O

@USER	O
:	O
If	O
you	O
do	O
not	O
like	O
`	O
tabular	O
`	O
,	O
you	O
may	O
use	O
any	O
LaTeX	O
package	O
.	O

The	O
output	O
of	O
`	O
to_latex	B-API
`	O
is	O
IIRC	O
for	O
the	O
`	O
booktab	O
`	O
package	O
.	O

However	O
,	O
beware	O
that	O
the	O
`	O
matplotlib	O
`	O
LaTeX	O
system	O
does	O
not	O
necessarily	O
like	O
line	O
breaks	O
,	O
so	O
you	O
may	O
need	O
to	O
replace	O
`	O
\n	O
`	O
by	O
spaces	O
in	O
the	O
string	O
given	O
by	O
`	O
to_latex	B-API
`	O
.	O

IMHO	O
well-designed	O
LaTeX	O
tables	O
look	O
very	O
good	O
.	O

Usually	O
the	O
trick	O
is	O
to	O
reduce	O
the	O
number	O
of	O
lines	O
(	O
horisontal	O
lines	O
are	O
often	O
unnecessary	O
)	O
,	O
but	O
this	O
is	O
a	O
matter	O
of	O
taste	O
.	O

How	O
do	O
I	O
combine	O
two	O
columns	O
within	O
a	O
dataframe	B-API
in	O
Pandas	O
?	O

Say	O
I	O
have	O
two	O
columns	O
,	O
A	O
and	O
B	O
,	O
in	O
my	O
dataframe	B-API
:	O
#CODE	O

I'm	O
sure	O
this	O
is	O
a	O
very	O
basic	O
question	O
,	O
but	O
as	O
I	O
am	O
new	O
to	O
Pandas	O
,	O
any	O
help	O
will	O
be	O
appreciated	O
!	O

You	O
can	O
use	O
`	O
where	B-API
`	O
which	O
is	O
a	O
vectorized	O
if	O
/	O
else	O
:	O
#CODE	O

you	O
can	O
simplify	O
to	O
this	O
:	O
`df['C	O
']	O
=	O
df.A.where	O
(	O
df.B.isnull()	O
,	O
df.B	O
)`	O
as	O
`	O
isnull	B-API
`	O
is	O
available	O
for	O
df	O
and	O
series	B-API
,	O
also	O
I	O
wouldn't	O
encourage	O
the	O
practice	O
of	O
accessing	O
columns	O
as	O
attributes	O
as	O
it	O
can	O
lead	O
to	O
strange	O
behaviour	O
,	O
better	O
to	O
do	O
this	O
`df['C	O
']	O
=	O
df['A'].where(df['B	O
']	O
.isnull()	O
,	O
df['B	O
'])`	O

You	O
can	O
use	O
`	O
combine_first	B-API
`	O
:	O
#CODE	O

shift	O
by	O
partition	O
using	O
groupby	B-API

I	O
have	O
a	O
dataframe	B-API
with	O
one	O
column	O
I	O
would	O
like	O
to	O
shift	O
,	O
but	O
over	O
partition	O
rather	O
than	O
the	O
whole	O
dataframe	B-API
.	O

For	O
example	O
,	O
I	O
would	O
like	O
to	O
go	O
from	O
this	O
dataframe	B-API
:	O
#CODE	O

to	O
this	O
dataframe	B-API
:	O
#CODE	O

I	O
believe	O
this	O
gives	O
me	O
what	O
I	O
want	O
for	O
each	O
State	O
,	O
but	O
then	O
I	O
don't	O
know	O
how	O
to	O
combine	O
it	O
back	O
together	O
(	O
basically	O
just	O
appending	O
each	O
dataframe	B-API
below	O
one	O
another	O
)	O
.	O

How	O
can	O
I	O
do	O
that	O
?	O

You	O
can	O
store	O
your	O
intermediate	O
DataFrames	O
in	O
a	O
list	O
and	O
use	O
`	O
pd.concat	B-API
`	O
to	O
join	O
them	O
together	O
:	O
#CODE	O

After	O
using	O
Pandas	O
to	O
read	O
a	O
json	O
object	O
into	O
a	O
`	O
Pandas.DataFrame	B-API
`	O
,	O
we	O
only	O
want	O
to	O
`	O
print	O
`	O
the	O
first	O
year	O
in	O
each	O
pandas	O
row	O
.	O

Eg	O
:	O
if	O
we	O
have	O
`	O
2013-2014	O
(	O
2015	O
)`	O
,	O
we	O
want	O
to	O
print	O
`	O
2013	O
`	O

Why	O
is	O
this	O
happening	O
?	O

How	O
can	O
we	O
fix	O
the	O
problem	O
?	O

Now	O
you	O
try	O
to	O
split	O
the	O
string	O
at	O
the	O
unicode	O
`	O
\u2031	O
`	O
(	O
EN	O
DASH	O
)	O
,	O
but	O
the	O
string	O
you	O
give	O
to	O
`	O
split	B-API
`	O
is	O
no	O
unicode	O
string	O
(	O
therefore	O
the	O
error	O
`'ascii	O
'	O
codec	O
can't	O
decode	O
byte	O
0xe2	O
`	O
-	O
the	O
EN	O
DASH	O
is	O
no	O
ASCII	O
character	O
)	O
.	O

Index	O
contains	O
duplicate	O
entries	O
after	O
drop_duplicates	B-API
called	O

I	O
have	O
a	O
pandas	O
dataframe	B-API
that	O
has	O
duplicate	O
entries	O
and	O
I	O
want	O
to	O
create	O
a	O
`	O
tsplot	O
`	O
using	O
`	O
seaborn	O
`	O
.	O

I	O
call	O
`	O
drop_duplicates	B-API
`	O
on	O
the	O
dataframe	B-API
(	O
and	O
even	O
call	O
`	O
reset_index()	B-API
`)	O
yet	O
when	O
I	O
got	O
to	O
do	O
the	O
plot	O
I	O
still	O
get	O
#CODE	O

Is	O
there	O
a	O
reason	O
why	O
`	O
drop_duplicates	B-API
`	O
wouldn't	O
solve	O
this	O
problem	O
?	O

EDIT	O
I've	O
even	O
checked	O
by	O
calling	O
`	O
duplicated	B-API
`	O
on	O
the	O
dataframe	B-API
after	O
the	O
drop	O
,	O
and	O
all	O
rows	O
show	O
`	O
False	O
`	O
.	O

As	O
I	O
would	O
expect	O
.	O

`	O
drop_duplicates	B-API
`	O
does	O
not	O
work	O
on	O
the	O
index	O
,	O
but	O
on	O
the	O
values	O
in	O
the	O
dataframe	B-API
!	O

(	O
so	O
it	O
looks	O
for	O
duplicate	O
rows	O
,	O
not	O
duplicate	O
indices	O
)	O
.	O

But	O
you	O
also	O
have	O
the	O
same	O
function	O
on	O
the	O
index	O
(	O
#URL	O
)	O

`	O
drop_duplicates	B-API
`	O
does	O
not	O
work	O
on	O
the	O
index	O
,	O
but	O
on	O
the	O
values	O
in	O
the	O
dataframe	B-API
!	O

(	O
so	O
it	O
looks	O
for	O
duplicate	O
rows	O
,	O
not	O
duplicate	O
indices	O
)	O
.	O

With	O
the	O
resulting	O
index	O
,	O
you	O
can	O
reindex	B-API
.	O

Another	O
option	O
is	O
to	O
add	O
the	O
index	O
as	O
a	O
column	O
and	O
use	O
`	O
DataFrame.drop_duplicates	B-API
`	O
on	O
that	O
column	O
.	O

Another	O
option	O
is	O
to	O
use	O
groupby	B-API
:	O
`	O
df.groupby	B-API
(	O
level=0	O
)	O
.first()	B-API
`	O
(	O
and	O
you	O
adapt	O
the	O
`	O
first	B-API
`	O
to	O
what	O
you	O
want	O
to	O
do	O
with	O
the	O
duplicate	O
rows	O
)	O

Starting	O
from	O
a	O
sample	O
dataframe	B-API
`	O
df	O
`	O
like	O
:	O
#CODE	O

Is	O
there	O
a	O
way	O
to	O
apply	O
a	O
`	O
math	O
`	O
function	O
to	O
a	O
whole	O
column	O
?	O

Well	O
`	O
math.exp	O
`	O
doesn't	O
understand	O
`	O
Series	B-API
`	O
datatype	O
,	O
use	O
numpy	O
`	O
np.exp	O
`	O
which	O
does	O
and	O
is	O
vectorised	O
so	O
operates	O
on	O
the	O
entire	O
column	O
:	O
#CODE	O

look	O
into	O
`	O
cubehelix	O
`	O

Cubehelix	O
is	O
awesome	O
.	O

I	O
read	O
the	O
paper	O
from	O
Dave	O
Green	O
.	O

Exactly	O
what	O
I	O
wanted	O
.	O

Got	O
excellent	O
looking	O
and	O
printing	O
graphs	O
on	O
the	O
first	O
try	O
.	O

If	O
your	O
comment	O
was	O
an	O
answer	O
I	O
would	O
accept	O
it	O
.	O

Colour-blindness	O
:	O
this	O
page	O
on	O
wikipedia	O
has	O
lots	O
of	O
good	O
info	O
about	O
choosing	O
colours	O
that	O
are	O
distinguishable	O
to	O
most	O
color-blind	O
people	O
.	O

If	O
you	O
notice	O
on	O
the	O
"	O
tips	O
for	O
editors	O
"	O
section	O
,	O
once	O
you	O
take	O
the	O
guidelines	O
into	O
account	O
there	O
are	O
only	O
a	O
few	O
sets	O
of	O
colours	O
available	O
.	O

(	O
A	O
good	O
rule	O
of	O
thumb	O
is	O
to	O
never	O
mix	O
red	O
and	O
green	O
!	O
)	O
You	O
can	O
also	O
use	O
the	O
linked	O
colour-blind	O
simulators	O
to	O
see	O
if	O
your	O
plot	O
would	O
be	O
well	O
visible	O
.	O

Luminance	O
:	O
most	O
of	O
the	O
journals	O
in	O
my	O
field	O
will	O
publish	O
in	O
B	O
W	O
by	O
default	O
.	O

Even	O
though	O
most	O
people	O
read	O
the	O
papers	O
online	O
,	O
I	O
still	O
like	O
to	O
make	O
sure	O
that	O
the	O
plots	O
can	O
be	O
understood	O
when	O
printed	O
in	O
grayscale	O
.	O

So	O
I	O
take	O
care	O
to	O
use	O
colours	O
that	O
have	O
different	O
luminances	O
.	O

To	O
test	O
,	O
a	O
good	O
way	O
is	O
to	O
just	O
desaturate	O
the	O
image	O
produced	O
,	O
and	O
you'll	O
have	O
a	O
good	O
idea	O
of	O
how	O
it	O
looks	O
when	O
printed	O
in	O
grayscale	O
.	O

In	O
many	O
cases	O
(	O
particularly	O
line	O
or	O
scatter	O
plots	O
)	O
,	O
I	O
also	O
use	O
other	O
things	O
than	O
colour	O
to	O
distinguish	O
between	O
sets	O
(	O
eg	O
.	O
line	O
styles	O
,	O
different	O
markers	O
)	O
.	O

In	O
1.5	O
matplotlib	O
will	O
ship	O
with	O
4	O
new	O
rationally	O
designed	O
color	O
maps	O
:	O

`'viridis	O
'`	O
(	O
will	O
be	O
default	O
color	O
map	O
in	O
2.0	O
)	O

The	O
process	O
of	O
designing	O
these	O
color	O
maps	O
is	O
presented	O
in	O
#URL	O
.	O

I	O
would	O
suggest	O
the	O
`	O
cubehelix	O
`	O
color	O
map	O
.	O

It	O
is	O
designed	O
to	O
have	O
correct	O
luminosity	O
ordering	O
in	O
both	O
color	O
and	O
gray-scale	O

So	O
your	O
requirements	O
are	O
"	O
lots	O
of	O
colors	O
"	O
and	O
"	O
no	O
two	O
colors	O
should	O
map	O
to	O
the	O
same	O
grayscale	O
value	O
when	O
printed	O
"	O
,	O
right	O
?	O

The	O
second	O
criteria	O
should	O
be	O
met	O
by	O
any	O
"	O
sequential	O
"	O
colormaps	O
(	O
which	O
increase	O
or	O
decrease	O
monotically	O
in	O
luminance	O
)	O
.	O

I	O
think	O
out	O
of	O
all	O
the	O
choices	O
in	O
matplotlib	O
,	O
you	O
are	O
left	O
with	O
`	O
cubehelix	O
`	O
(	O
already	O
mentioned	O
)	O
,	O
`	O
gnuplot	O
`	O
,	O
and	O
`	O
gnuplot2	O
`	O
:	O

The	O
white	O
line	O
is	O
the	O
luminance	O
of	O
each	O
color	O
,	O
so	O
you	O
can	O
see	O
that	O
each	O
color	O
will	O
map	O
to	O
a	O
different	O
grayscale	O
value	O
when	O
printed	O
.	O

The	O
black	O
line	O
is	O
hue	O
,	O
showing	O
they	O
cycle	O
through	O
a	O
variety	O
of	O
colors	O
.	O

Note	O
that	O
cubehelix	O
is	O
actually	O
a	O
function	O
(	O
`	O
from	O
matplotlib._cm	O
import	O
cubehelix	O
`)	O
,	O
and	O
you	O
can	O
adjust	O
the	O
parameters	O
of	O
the	O
helix	O
to	O
produce	O
more	O
widely-varying	O
colors	O
,	O
as	O
shown	O
here	O
.	O

In	O
other	O
words	O
,	O
cubehelix	O
is	O
not	O
a	O
colormap	O
,	O
it's	O
a	O
family	O
of	O
colormaps	O
.	O

Here	O
are	O
2	O
variations	O
:	O

For	O
less	O
wildly-varying	O
colors	O
(	O
more	O
pleasant	O
for	O
many	O
things	O
,	O
but	O
maybe	O
not	O
for	O
your	O
bar	O
graphs	O
)	O
,	O
maybe	O
try	O
the	O
ColorBrewer	O
3-color	O
maps	O
,	O
`	O
YlOrRd	O
`	O
,	O
`	O
PuBuGn	O
`	O
,	O
`	O
YlGnBu	O
`	O
:	O

I	O
wouldn't	O
recommend	O
using	O
only	O
this	O
color	O
to	O
identify	O
bar	O
graphs	O
,	O
though	O
.	O

You	O
should	O
always	O
use	O
text	O
labels	O
as	O
the	O
primary	O
identifier	O
.	O

Also	O
note	O
that	O
some	O
of	O
these	O
produce	O
white	O
bars	O
that	O
completely	O
blend	O
in	O
with	O
the	O
background	O
,	O
since	O
they	O
are	O
intended	O
for	O
heatmaps	O
,	O
not	O
chart	O
colors	O
:	O
#CODE	O

Can	O
you	O
add	O
a	O
note	O
to	O
your	O
answer	O
here	O
re	O
the	O
new	O
color	O
maps	O
?	O

I	O
know	O
I	O
can	O
just	O
edit	O
it	O
myself	O
,	O
but	O
that	O
seem	O
rude	O
.	O

Delete	O
a	O
group	O
after	O
pandas	O
groupby	B-API

Is	O
it	O
possible	O
to	O
delete	O
a	O
group	O
(	O
by	O
group	O
name	O
)	O
from	O
a	O
groupby	B-API
object	O
in	O
pandas	O
?	O

That	O
is	O
,	O
after	O
performing	O
a	O
groupby	B-API
,	O
delete	O
a	O
resulting	O
group	O
based	O
on	O
its	O
name	O
.	O

Seems	O
there's	O
no	O
direct	O
way	O
to	O
delete	O
a	O
group	O
from	O
a	O
groupby	B-API
object	O
.	O

I	O
think	O
you	O
can	O
filter	O
out	O
those	O
groupby	B-API
before	O
groupby	B-API
by	O
#CODE	O

Maybe	O
I	O
misunderstand	O
what	O
is	O
meant	O
by	O
the	O
variable	O
``	O
group	O
``	O
,	O
but	O
you	O
can't	O
index	O
a	O
DataFrame	B-API
by	O
a	O
GroupBy	B-API
object	O
.	O

Filtering	O
a	O
DataFrame	B-API
groupwise	O
has	O
been	O
discussed	O
.	O

And	O
a	O
future	O
release	O
of	O
pandas	O
may	O
include	O
a	O
more	O
convenient	O
way	O
to	O
do	O
it	O
.	O

But	O
currently	O
,	O
here	O
is	O
what	O
I	O
believe	O
to	O
be	O
the	O
most	O
succinct	O
way	O
to	O
filter	O
the	O
GroupBy	B-API
object	O
`	O
grouped	O
`	O
by	O
name	O
and	O
return	O
a	O
DataFrame	B-API
of	O
the	O
remaining	O
groups	O
.	O

#CODE	O

How	O
to	O
format	O
IPython	O
html	O
display	O
of	O
Pandas	O
dataframe	B-API
?	O

and	O
simalrly	O
for	O
other	O
data	O
types	O
.	O

But	O
IPython	O
does	O
not	O
pick	O
up	O
these	O
fromatting	O
options	O
when	O
displaying	O
dataframes	O
in	O
html	O
.	O

I	O
still	O
need	O
to	O
have	O
#CODE	O

You	O
can	O
also	O
specify	O
a	O
list	O
of	O
formatters	O
,	O
with	O
`	O
None	O
`	O
values	O
for	O
those	O
that	O
are	O
not	O
present	O
-	O
which	O
simplifies	O
the	O
`	O
frmt	O
`	O
creation	O
:	O
`	O
frmt	O
=	O
[	O
frmt_map.get	O
(	O
dtype	B-API
,	O
None	O
)	O
for	O
dtype	B-API
in	O
df.dtypes	B-API
]`	O
.	O

+1	O
for	O
the	O
research	O
.	O

HTML	O
receives	O
a	O
custom	O
string	O
of	O
html	O
data	O
.	O

Nobody	O
forbids	O
you	O
to	O
pass	O
in	O
a	O
style	O
tag	O
with	O
the	O
custom	O
CSS	O
style	O
for	O
the	O
`	O
.dataframe	O
`	O
class	O
(	O
which	O
the	O
`	O
to_html	B-API
`	O
method	O
adds	O
to	O
the	O
table	O
)	O
.	O

So	O
the	O
simplest	O
solution	O
would	O
be	O
to	O
just	O
add	O
a	O
style	O
and	O
concatenate	O
it	O
with	O
the	O
output	O
of	O
the	O
`	O
df.to_html	B-API
`	O
:	O
#CODE	O

But	O
I	O
would	O
suggest	O
to	O
define	O
a	O
custom	O
class	O
for	O
a	O
DataFrame	B-API
since	O
this	O
will	O
change	O
the	O
style	O
of	O
all	O
the	O
tables	O
in	O
your	O
notebook	O
(	O
style	O
is	O
"	O
global	O
")	O
.	O

#CODE	O

You	O
can	O
also	O
define	O
the	O
style	O
in	O
one	O
of	O
the	O
previous	O
cells	O
,	O
and	O
then	O
just	O
set	O
the	O
`	O
classes	O
`	O
parameter	O
of	O
the	O
`	O
to_html	B-API
`	O
method	O
:	O
#CODE	O

pandas	O
(	O
as	O
of	O
0.16.2	O
)	O
does	O
not	O
allow	O
overriding	O
the	O
default	O
integer	O
format	O
in	O
an	O
easy	O
way	O
.	O

It	O
is	O
hard	O
coded	O
in	O
`	O
pandas.core.format.IntArrayFormatter	O
`	O
(	O
the	O
`	O
labmda	O
`	O
function	O
):	O
#CODE	O

`	O
display.float_format	O
`	O
:	O
The	O
callable	O
should	O
accept	O
a	O
floating	O
point	O
number	O
and	O
return	O
a	O
string	O
with	O
the	O
desired	O
format	O
of	O
the	O
number	O
.	O

This	O
is	O
used	O
in	O
some	O
places	O
like	O
`	O
SeriesFormatter	O
`	O
.	O

See	O
`	O
core.format.EngFormatter	O
`	O
for	O
an	O
example	O
.	O

I	O
have	O
a	O
dataframe	B-API
like	O
the	O
following	O
:	O
#CODE	O

Efficient	O
matching	O
of	O
two	O
arrays	O
(	O
how	O
to	O
use	O
KDTree	O
)	O

I	O
have	O
two	O
2d	O
arrays	O
,	O
`	O
obs1	O
`	O
and	O
`	O
obs2	O
`	O
.	O

They	O
represent	O
two	O
independent	O
measurement	O
series	B-API
,	O
and	O
both	O
have	O
dim0	O
=	O
2	O
,	O
and	O
slightly	O
different	O
dim1	O
,	O
say	O
`	O
obs1.shape	O
=	O
(	O
2	O
,	O
250000	O
)`	O
,	O
and	O
`	O
obs2.shape	O
=	O
(	O
2	O
,	O
250050	O
)`	O
.	O

`	O
obs1	O
[	O
0	O
]`	O
and	O
`	O
obs2	O
[	O
0	O
]`	O
signify	O
time	O
,	O
and	O
`	O
obs1	O
[	O
1	O
]`	O
and	O
`	O
obs2	O
[	O
1	O
]`	O
signify	O
some	O
spatial	O
coordinate	O
.	O

Both	O
arrays	O
are	O
(	O
more	O
or	O
less	O
)	O
sorted	O
by	O
time	O
.	O

The	O
times	O
and	O
coordinates	O
should	O
be	O
identical	O
between	O
the	O
two	O
measurement	O
series	B-API
,	O
but	O
in	O
reality	O
they	O
aren't	O
.	O

Also	O
,	O
not	O
each	O
measurement	O
from	O
`	O
obs1	O
`	O
has	O
a	O
corresponding	O
value	O
in	O
`	O
obs2	O
`	O
and	O
vice-versa	O
.	O

Another	O
problem	O
is	O
that	O
there	O
might	O
be	O
a	O
slight	O
offset	O
in	O
the	O
times	O
.	O

I'm	O
looking	O
for	O
an	O
efficient	O
algorithm	O
to	O
associate	O
the	O
best	O
matching	O
value	O
from	O
`	O
obs2	O
`	O
to	O
each	O
measurement	O
in	O
`	O
obs1	O
`	O
.	O

Currently	O
,	O
I	O
do	O
it	O
like	O
this	O
:	O
#CODE	O

I	O
would	O
be	O
very	O
thankful	O
for	O
ideas	O
on	O
how	O
to	O
improve	O
this	O
algorithm	O
speed-wise	O
,	O
e.g.	O
using	O
KDtree	O
or	O
something	O
similar	O
.	O

resulting_i	O
=	O
i	O
+	O
argmin	B-API
(	O
abs	O
(	O
obs1	O
[	O
1	O
,	O
i	O
]	O
-	O
obs2	O
[	O
1	O
,	O
#URL	O
)`	O
?	O

Assuming	O
that	O
you	O
are	O
not	O
performing	O
another	O
loop	O
here	O
,	O
I	O
don't	O
believe	O
KDTrees	O
would	O
speed	O
things	O
up	O
too	O
much	O
.	O

Using	O
`	O
cKDTree	O
`	O
for	O
this	O
case	O
would	O
look	O
like	O
:	O
#CODE	O

where	O
`	O
indices	O
`	O
will	O
contain	O
the	O
column	O
indices	O
in	O
`	O
obs2	O
`	O
corresponding	O
to	O
each	O
observation	O
in	O
`	O
obs1	O
`	O
.	O

Note	O
that	O
I	O
had	O
to	O
transpose	O
`	O
obs1	O
`	O
and	O
`	O
obs2	O
`	O
.	O

In	O
my	O
specific	O
case	O
,	O
I	O
actually	O
have	O
a	O
dataframe	B-API
,	O
and	O
each	O
method	O
outputs	O
a	O
score	O
.	O

What	O
matters	O
is	O
not	O
the	O
difference	O
in	O
score	O
between	O
the	O
methods	O
and	O
the	O
true	O
scores	O
,	O
but	O
that	O
the	O
methods	O
get	O
the	O
ranking	O
right	O
(	O
higher	O
score	O
means	O
higher	O
ranking	O
for	O
all	O
columns	O
)	O
.	O

#CODE	O

Python	O
has	O
RankEval	O
based	O
module	O
that	O
implements	O
this	O
metric	O
(	O
and	O
some	O
others	O
if	O
you	O
want	O
to	O
try	O
them	O
)	O
.	O

The	O
repo	O
is	O
here	O
and	O
there	O
is	O
a	O
nice	O
IPython	O
NB	O
with	O
examples	O

I	O
am	O
producing	O
some	O
plots	O
in	O
matplotlib	O
and	O
would	O
like	O
to	O
add	O
explanatory	O
text	O
for	O
some	O
of	O
the	O
data	O
.	O

I	O
want	O
to	O
have	O
a	O
string	O
inside	O
my	O
legend	O
as	O
a	O
separate	O
legend	O
item	O
above	O
the	O
'	O
0-10	O
'	O
item	O
.	O

Does	O
anyone	O
know	O
if	O
there	O
is	O
a	O
possible	O
way	O
to	O
do	O
this	O
?	O

`ax.legend(['0-10','10-100','100-500','500+'],loc='best	O
')`	O

If	O
there	O
isn't	O
a	O
proper	O
way	O
of	O
doing	O
this	O
the	O
only	O
other	O
option	O
I	O
can	O
think	O
of	O
is	O
to	O
trick	O
the	O
graph	O
into	O
producing	O
it	O
by	O
plotting	O
some	O
empty	O
values	O

Why	O
not	O
simply	O
set	O
the	O
legends	O
`	O
title	B-API
`	O
?	O

I.e.	O

`ax.legend(['0-10','10-100','100-500','500	O
+	O
']	O
,	O
loc='best	O
'	O
,	O
title='Explanatory	O
text	O
')`	O
.	O

Sure	O
.	O

`	O
ax.legend()	O
`	O
has	O
a	O
two	O
argument	O
form	O
that	O
accepts	O
a	O
list	O
of	O
objects	O
(	O
handles	O
)	O
and	O
a	O
list	O
of	O
strings	O
(	O
labels	O
)	O
.	O

Use	O
a	O
dummy	O
object	O
(	O
aka	O
a	O
"	O
proxy	O
artist	O
"	O
)	O
for	O
your	O
extra	O
string	O
.	O

I	O
picked	O
a	O
`	O
matplotlib.patches.Rectangle	O
`	O
with	O
no	O
fill	O
and	O
0	O
linewdith	O
below	O
,	O
but	O
you	O
could	O
use	O
any	O
supported	O
artist	O
.	O

For	O
example	O
,	O
let's	O
say	O
you	O
have	O
4	O
bar	O
objects	O
(	O
since	O
you	O
didn't	O
post	O
the	O
code	O
used	O
to	O
generate	O
the	O
graph	O
,	O
I	O
can't	O
reproduce	O
it	O
exactly	O
)	O
.	O

#CODE	O

it's	O
actually	O
a	O
really	O
common	O
thing	O
.	O

I	O
bet	O
you've	O
done	O
something	O
like	O
:	O
`	O
line	O
=	O
ax.plot	O
(	O
x	O
,	O
y	O
)`	O
.	O

The	O
issue	O
is	O
that	O
`	O
plot	B-API
`	O
returns	O
a	O
*	O
list	O
*	O
of	O
lines	O
,	O
so	O
you	O
need	O
to	O
get	O
at	O
the	O
actual	O
artist	O
.	O

You	O
can	O
either	O
do	O
`	O
line	O
=	O
ax.plot	O
(	O
x	O
,	O
y	O
)	O
[	O
0	O
]`	O
or	O
do	O
`	O
line	O
,	O
=	O
ax.plot	O
(	O
x	O
,	O
y	O
)`	O
which	O
takes	O
advantage	O
of	O
parameter	O
unpacking	O
.	O

Handling	O
both	O
missing	O
and	O
duplicate	O
datatime	O
fields	O
in	O
a	O
dataframe	B-API

Observed	O
Output	O
:	O
Frame	O
contains	O
two	O
fields	O
i.e.	O
Timestamp	O
Active_Power	O
(	O
in	O
W	O
)	O
from	O
all	O
4	O
CSV	O
files	O
in	O
the	O
source	O
directory	O
aligned	O
side-by-side	O
.	O

So	O
far	O
so	O
good	O
.	O

How	O
to	O
remove	O
certain	O
rows	O
either	O
being	O
duplicated	O
and	O
/	O
or	O
missing	O
in	O
each	O
csv	O
files	O
rendering	O
dataframe	B-API
being	O
misaligned	O
in	O
timestamps	O
?	O

For	O
example	O
,	O
@USER	O
there	O
is	O
a	O
timestamp	O
value	O
missing	O
and	O
as	O
a	O
result	O
"	O
NaN	O
"	O
is	O
being	O
inserted	O
appropriately	O
.	O

But	O
@USER	O
a	O
timestamp	O
value	O
is	O
being	O
duplicated	O
twice	O
in	O
original	O
source	O
file	O
and	O
therefore	O
removed	O
resulting	O
in	O
"	O
NaN	O
"	O
again	O
,	O
as	O
a	O
result	O
the	O
timestamp	O
sequence	O
got	O
misaligned	O
.	O

How	O
to	O
deal	O
with	O
this	O
issue	O
?	O

How	O
can	O
I	O
improve	O
the	O
creation	O
time	O
of	O
a	O
pandas	O
DataFrame	B-API
?	O

I	O
am	O
having	O
a	O
dictionary	O
of	O
pandas	O
`	O
Series	B-API
`	O
,	O
each	O
with	O
their	O
own	O
index	O
and	O
all	O
containing	O
float	O
numbers	O
.	O

I	O
need	O
to	O
create	O
a	O
pandas	O
`	O
DataFrame	B-API
`	O
with	O
all	O
these	O
series	B-API
,	O
which	O
works	O
fine	O
by	O
just	O
doing	O
:	O
#CODE	O

I	O
thought	O
about	O
caching	O
the	O
result	O
but	O
unfortunately	O
the	O
`	O
dict_of_series	O
`	O
is	O
almost	O
all	O
the	O
time	O
different	O
.	O

are	O
the	O
series	B-API
aligned	O
(	O
i.e.	O
do	O
they	O
all	O
have	O
the	O
same	O
index	O
?	O
)	O

I	O
have	O
a	O
dataframe	B-API
df1	O
like	O
this	O
,	O
where	O
starttime	O
and	O
endtime	O
are	O
datetime	O
objects	O
.	O

StartTime	O
EndTime	O

If	O
endtime.hour	O
is	O
not	O
the	O
same	O
as	O
startime.hour	O
,	O
I	O
would	O
like	O
to	O
split	O
times	O
like	O
this	O

StartTime	O
EndTime	O

Essentially	O
insert	O
a	O
row	O
into	O
the	O
existing	O
dataframe	B-API
df1	O
.	O

I	O
have	O
looked	O
at	O
a	O
ton	O
of	O
examples	O
but	O
haven't	O
figured	O
out	O
how	O
to	O
do	O
this	O
.	O

If	O
my	O
question	O
isn't	O
clear	O
please	O
let	O
me	O
know	O
.	O

which	O
part	O
(	O
s	O
)	O
are	O
you	O
stuck	O
on	O
?	O

converting	O
to	O
datetime	O
and	O
extracting	O
the	O
hour	O
,	O
or	O
inserting	O
a	O
row	O
into	O
a	O
dataframe	B-API
?	O

I	O
think	O
you're	O
going	O
to	O
want	O
to	O
write	O
a	O
helper	O
function	O
or	O
two	O
,	O
and	O
you	O
may	O
have	O
to	O
reset	O
the	O
index	O
.	O

Creating	O
feature	O
(	O
row	O
)	O
vectors	O
,	O
for	O
SVM	O
,	O
from	O
Pandas	O
GroupBy	B-API
function	O
(	O
and	O
or	O
other	O
methods	O
suggested	O
)	O

I	O
am	O
trying	O
to	O
create	O
stacked	O
feature	O
vectors	O
for	O
an	O
SVM	O
classifier	O
.	O

I	O
have	O
all	O
my	O
data	O
in	O
a	O
large	O
matrix	O
.	O

The	O
problem	O
at	O
hand	O
is	O
a	O
multi-class	O
classification	O
problem	O
so	O
I	O
need	O
to	O
group	O
using	O
multi-index	O
.	O

I	O
use	O
the	O
following	O
groupby	B-API
to	O
get	O
the	O
appropriate	O
groups	O
for	O
my	O
application	O
:	O
#CODE	O

Now	O
I	O
would	O
like	O
to	O
create	O
feature	O
vectors	O
and	O
store	O
them	O
into	O
a	O
new	O
DataFrame	B-API
but	O
in	O
a	O
way	O
that	O
only	O
uses	O
two	O
rows	O
to	O
create	O
one	O
feature	O
vector	O
.	O

Meaning	O
that	O
in	O
the	O
test	O
example	O
the	O
`	O
test1	O
`	O
activity	O
is	O
performed	O
twice	O
with	O
each	O
iteration	O
having	O
the	O
same	O
label	O
so	O
in	O
this	O
case	O
it	O
has	O
two	O
labels	O
:	O
1	O
and	O
2	O
.	O

From	O
each	O
label	O
two	O
rows	O
should	O
be	O
stacked	O
to	O
create	O
the	O
desired	O
output	O
.	O

I	O
have	O
not	O
written	O
out	O
the	O
whole	O
thing	O
but	O
I	O
hope	O
it	O
is	O
obvious	O
what	O
I	O
would	O
like	O
to	O
achieve	O
.	O

Basically	O
;	O
two	O
rows	O
become	O
one	O
stacked	O
row	O
vector	O
(	O
with	O
the	O
label	O
on	O
top	O
)	O
,	O
the	O
same	O
vector	O
is	O
one	O
feature	O
vector	O
.	O

As	O
I	O
have	O
multiple	O
activities	O
I	O
need	O
multiple	O
feature	O
vectors	O
per	O
activity	O
to	O
train	O
the	O
SVM	O
.	O

For	O
this	O
example	O
I	O
would	O
ideally	O
get	O
one	O
pd.DataFrame	B-API
with	O
eight	O
feature	O
rows	O
vectors	O
in	O
it	O
so	O
the	O
data	O
frame	O
will	O
have	O
been	O
re-shaped	O
(	O
ignoring	O
everything	O
but	O
the	O
actual	O
data	O
contained	O
in	O
col_A	O
through	O
col_B	O
)	O
from	O
(	O
16	O
,	O
4	O
)	O
to	O
(	O
8	O
,	O
8)	O
.	O

You	O
need	O
to	O
pass	O
a	O
function	O
to	O
the	O
`	O
groupby	B-API
`	O
which	O
prepares	O
the	O
data	O
for	O
the	O
final	O
output	O
,	O
and	O
then	O
relabel	O
the	O
columns	O
,	O
just	O
like	O
this	O
:	O
#CODE	O

You	O
mention	O
list	O
but	O
tagged	O
this	O
as	O
pandas	O
,	O
for	O
a	O
series	B-API
by	O
default	O
calling	O
`	O
median	B-API
`	O
on	O
a	O
series	B-API
will	O
ignore	O
`	O
NaN	O
`	O
values	O
:	O
#URL	O

How	O
can	O
I	O
convert	O
Sqlalchemy	O
table	O
object	O
to	O
Pandas	O
DataFrame	B-API
?	O

Is	O
it	O
possible	O
to	O
convert	O
retrieved	O
SqlAlchemy	O
table	O
object	O
into	O
Pandas	O
DataFrame	B-API
or	O
do	O
I	O
need	O
to	O
write	O
a	O
particular	O
function	O
for	O
that	O
aim	O
?	O

Have	O
you	O
considered	O
using	O
[	O
pandas.read_sql	B-API
]	O
(	O
#URL	O
)	O
?	O

pandas.read_sql	B-API
can	O
use	O
an	O
SqlAlchemy	O
engine	O
.	O

Partial	O
query	O
results	O
(	O
NamedTuples	O
)	O
will	O
also	O
work	O
,	O
but	O
you	O
have	O
to	O
construct	O
the	O
DataFrame	B-API
`	O
columns	O
`	O
and	O
`	O
index	B-API
`	O
to	O
match	O
your	O
query	O
.	O

just	O
use	O
`	O
pandas.read_sql	B-API
`	O
with	O
an	O
SQLAlchemy	O
engine	O
.	O

it's	O
dead	O
simple	O
.	O

How	O
do	O
you	O
use	O
`	O
pandas.read_sql	B-API
`	O
on	O
an	O
ORM	O
query	O
,	O
like	O
:	O
`	O
session.query	O
(	O
MyORMTable	O
)	O
.limit	O
(	O
100	O
)	O
.all()	B-API
`	O
?	O

`pandas.read_sql_table('MyTable	O
'	O
,	O
MySQLEngine	O
)`	O
see	O
here	O
#URL	O

Well	O
,	O
you	O
can	O
avoid	O
the	O
apply	B-API
and	O
do	O
it	O
vectorized	O
(	O
I	O
think	O
that	O
makes	O
it	O
a	O
bit	O
nicer	O
):	O
#CODE	O

Edit	O
:	O
Jeff	O
points	O
out	O
that	O
a	O
more	O
pandonic	O
way	O
is	O
to	O
make	O
date	O
a	O
`	O
DatetimeIndex	B-API
`	O
and	O
use	O
a	O
Date	O
Offset	O
.	O

So	O
something	O
like	O
:	O
#CODE	O

This	O
is	O
great.Much	O
faster	O
than	O
`	O
apply()	B-API
`	O
Do	O
you	O
know	O
if	O
it	O
is	O
possible	O
to	O
use	O
`	O
datetime64	O
[	O
M	O
]`	O
to	O
find	O
the	O
end	O
of	O
the	O
month	O
instead	O
of	O
the	O
start	O
?	O

a	O
more	O
pandonic	O
way	O
is	O
to	O
treat	O
as	O
an	O
index	O
and	O
use	O
rollback	O
with	O
the	O
appropriate	O
offset	O
see	O
here	O
:	O
#URL	O

gr8	O
u	O
don't	O
actually	O
have	O
make	O
it	O
THE	O
index	O
just	O
set	O
box=False	O
I	O
think	O
on	O
pd.to_datetime	B-API
then	O
subtract	O
the	O
offset	O
(	O
but	O
what	O
u	O
r	O
doing	O
is	O
ok	O
too	O
)	O

[	O
@USER	O
]	O
(	O
#URL	O
)	O
,	O
got	O
it	O
.	O

I	O
had	O
to	O
add	O
`	O
pd.DatetimeIndex()	B-API
`	O
to	O
get	O
it	O
to	O
work	O
.	O

@USER	O
,	O
so	O
offsets	O
can	O
only	O
be	O
applied	O
to	O
an	O
`	O
Index	B-API
`	O
?	O

Offsets	O
work	O
with	O
Timestamp	O
/	O
Timedelta	O
&	O
DatetimeIndex	B-API
/	O
PeriodIndex	O
/	O
TimedeltaIndex	B-API

How	O
can	O
I	O
sum	O
column	O
values	O
that	O
corrispond	O
to	O
a	O
specific	O
value	O
of	O
another	O
column	O
in	O
a	O
pandas	O
DataFrame	B-API
?	O

I	O
have	O
a	O
python	O
pandas	O
DataFrame	B-API
with	O
(	O
item	O
,	O
feature	O
,	O
grade	O
)	O
#CODE	O

and	O
I	O
have	O
to	O
put	O
all	O
the	O
sum	O
in	O
a	O
new	O
DataFrame	B-API
with	O
(	O
item	O
,	O
sumGrade	O
):	O
#CODE	O

How	O
can	O
I	O
do	O
this	O
without	O
using	O
groupby	B-API
and	O
apply	B-API
function	O
?	O

Because	O
I	O
need	O
a	O
good	O
performance	O
in	O
computation	O
.	O

The	O
normal	O
op	O
here	O
is	O
to	O
`	O
groupby	B-API
`	O
on	O
'	O
item	O
'	O
and	O
call	O
`	O
sum	B-API
`	O
on	O
the	O
'	O
grade	O
'	O
column	O
no	O
need	O
to	O
call	O
`	O
apply	B-API
`	O
here	O

You	O
can	O
`	O
groupby	B-API
`	O
on	O
'	O
item	O
'	O
column	O
and	O
then	O
call	O
`	O
sum	B-API
`	O
on	O
the	O
'	O
grade	O
'	O
column	O
,	O
additionally	O
call	O
`	O
reset_index	B-API
`	O
to	O
restore	O
the	O
'	O
item	O
'	O
column	O
back	O
:	O
#CODE	O

Not	O
sure	O
why	O
you	O
don't	O
want	O
to	O
group	O
but	O
you	O
can	O
also	O
set	O
the	O
index	O
to	O
'	O
item	O
'	O
and	O
`	O
sum	B-API
`	O
on	O
the	O
index	O
level	O
:	O
#CODE	O

My	O
data	O
is	O
in	O
a	O
pandas	O
DataFrame	B-API
and	O
the	O
x	O
column	O
is	O
merged2	O
[:	O
-1	O
]	O
.lastqu	O

and	O
the	O
y	O
data	O
column	O
is	O
merged2	O
[:	O
-1	O
]	O
.Units	O

A	O
snippit	O
of	O
the	O
Dataframe	B-API
:	O
the	O
[:	O
-1	O
]	O
eliminates	O
the	O
current	O
period	O
from	O
the	O
data	O
which	O
will	O
subsequently	O
be	O
a	O
projection	O
#CODE	O

What's	O
the	O
problem	O
or	O
the	O
question	O
?	O

From	O
reading	O
the	O
example	O
everything	O
looks	O
correct	O
to	O
me	O
.	O

The	O
remaining	O
question	O
is	O
how	O
I	O
can	O
get	O
a	O
simple	O
regression	O
line	O
using	O
code	O
"	O
similar	O
to	O
the	O
x=	O
np.array	O
sequence	O
above	O
it	O
does	O
not	O
work	O
?	O

(	O
no	O
error	O
,	O
just	O
no	O
line	O
)	O
While	O
I	O
can	O
get	O
the	O
results	O
from	O
the	O
sm.graphics.plot	O

plt.plot	O
(	O
merged2	O
[:	O
-1	O
]	O
.lastqu	O
,	O
merged2	O
[:	O
-1	O
]	O
.Units	O
,	O
'	O
bo	O
')	O

x	O
=	O
np.array	O
([	O
merged2	O
[:	O
-1	O
]	O
.lastqu	O
.min()	B-API
,	O
merged2	O
[:	O
-1	O
]	O
.lastqu	O
.max()	B-API
])	O

plt.plot	O
(	O
x	O
,	O
y	O
,	O
'	O
r-	O
')	O

Merging	O
Pandas	O
DataFrames	O
(	O
LEFT	O
join	O
style	O
)	O
produces	O
strange	O
results	O

I'm	O
trying	O
to	O
merge	O
two	O
Pandas	O
DataFrames	O
with	O
a	O
many-to-one	O
relationship	O
.	O

One	O
DataFrame	B-API
contains	O
a	O
list	O
of	O
drug	O
names	O
and	O
their	O
ingredients	O
(	O
left	O
side	O
)	O
together	O
with	O
some	O
extra	O
information	O
(	O
an	O
irrelevant	O
ID	O
and	O
the	O
company	O
who	O
makes	O
the	O
drugs	O
)	O
,	O
the	O
other	O
contains	O
a	O
list	O
of	O
ingredients	O
and	O
their	O
numeric	O
IDs	O
(	O
right	O
side	O
)	O
.	O

Obviously	O
,	O
what	O
I	O
like	O
to	O
get	O
is	O
a	O
merged	O
DataFrame	B-API
where	O
the	O
ID	O
of	O
each	O
ingredient	O
is	O
listed	O
next	O
to	O
the	O
ingredient	O
name	O
and	O
,	O
considering	O
this	O
is	O
a	O
many-to-one	O
scenario	O
,	O
some	O
IDs	O
will	O
get	O
repeated	O
as	O
some	O
drugs	O
use	O
the	O
same	O
ingredient	O
.	O

A	O
contrived	O
example	O
of	O
both	O
DataFrames	O
(	O
index	O
omitted	O
for	O
brevity	O
):	O

To	O
do	O
the	O
merge	B-API
I	O
simply	O
do	O
the	O
following	O
:	O
#CODE	O

The	O
amount	O
of	O
rows	O
is	O
now	O
limited	O
to	O
how	O
many	O
rows	O
there	O
are	O
in	O
the	O
ingredients	O
(	O
df2	O
)	O
DataFrame	B-API
(	O
3354	O
)	O
.	O

The	O
`	O
info()	O
`	O
on	O
both	O
DataFrames	O
shows	O
me	O
that	O
both	O
`	O
ingredient	O
`	O
columns	O
are	O
of	O
the	O
same	O
type	O
(	O
just	O
to	O
exclude	O
comparison	O
problems	O
):	O

Now	O
for	O
the	O
interesting	O
part	O
.	O

In	O
an	O
attempt	O
to	O
find	O
out	O
where	O
this	O
might	O
be	O
going	O
wrong	O
,	O
I	O
tried	O
to	O
do	O
the	O
merge	B-API
on	O
only	O
a	O
subset	O
of	O
both	O
DataFrames	O
using	O
`	O
head()	O
`	O
:	O
#CODE	O

If	O
I	O
then	O
perform	O
the	O
merge	B-API
I	O
get	O
the	O
desired	O
output	O
without	O
a	O
problem	O
,	O
just	O
only	O
100	O
rows	O
.	O

If	O
I	O
keep	O
increasing	O
the	O
amount	O
of	O
rows	O
to	O
take	O
for	O
`	O
head()	O
`	O
,	O
even	O
up	O
to	O
the	O
point	O
that	O
it	O
is	O
way	O
more	O
than	O
the	O
number	O
of	O
rows	O
present	O
,	O
the	O
output	O
is	O
still	O
perfect	O
.	O

This	O
,	O
for	O
example	O
,	O
works	O
perfectly	O
:	O
#CODE	O

Notice	O
that	O
I	O
include	O
the	O
first	O
100.000	O
rows	O
(	O
only	O
53.000	O
in	O
df1	O
)	O
.	O

This	O
too	O
will	O
give	O
me	O
the	O
expected	O
merge	B-API
output	O
.	O

Interesting	O
bit	O
:	O
after	O
including	O
a	O
large	O
amount	O
of	O
rows	O
with	O
`	O
head	O
(	O
100000	O
)`	O
,	O
only	O
the	O
info	O
for	O
the	O
Drugs	O
DataFrame	B-API
(	O
df1	O
)	O
alters	O
slightly	O
:	O
#CODE	O

Notice	O
the	O
memory	O
usage	O
went	O
up	O
from	O
1.2	O
MB	O
to	O
2.0	O
MB	O
.	O

Yet	O
besides	O
that	O
,	O
they	O
both	O
still	O
are	O
DataFrame	B-API
objects	O
with	O
the	O
same	O
structure	O
as	O
before	O
I	O
pulled	O
them	O
through	O
that	O
operation	O
.	O

What	O
might	O
be	O
going	O
on	O
here	O
?	O

Why	O
does	O
the	O
merge	B-API
produce	O
such	O
strange	O
results	O
,	O
unless	O
I	O
pull	O
the	O
DataFrames	O
through	O
`	O
head()	O
`	O
first	O
?	O

To	O
summarize	O
:	O
Without	O
using	O
`	O
head()	O
`	O
the	O
`	O
merge()	B-API
`	O
output	O
is	O
showing	O
reduced	O
rows	O
(	O
#	O
rows	O
=	O
#	O
right	O
side	O
rows	O
)	O
and	O
each	O
ingredient	O
ID	O
is	O
simply	O
"	O
pasted	O
"	O
on	O
there	O
in	O
a	O
serial	O
manner	O
.	O

However	O
,	O
if	O
I	O
do	O
use	O
`	O
head()	O
`	O
,	O
even	O
with	O
a	O
great	O
enough	O
number	O
to	O
include	O
all	O
rows	O
on	O
both	O
sides	O
,	O
the	O
join	O
works	O
as	O
expected	O
.	O

Full	O
data	O
:	O
For	O
the	O
ones	O
wanting	O
to	O
try	O
with	O
the	O
exact	O
same	O
data	O
,	O
I	O
created	O
two	O
gists	O
on	O
Github	O
that	O
contain	O
the	O
full	O
data	O
(	O
dictionary	O
form	O
)	O
for	O
both	O
the	O
drugs	O
and	O
the	O
ingredients	O
DataFrame	B-API
mentioned	O
above	O
.	O

They	O
are	O
exported	O
using	O
`	O
DataFrame.to_dict()	B-API
`	O
and	O
can	O
be	O
imported	O
/	O
transformed	O
back	O
into	O
a	O
DataFrame	B-API
using	O
`	O
DataFrame.from_dict	B-API
(	O
data	O
)`	O
.	O

how	O
to	O
drop	O
dataframe	B-API
in	O
pandas	O
?	O

But	O
I	O
want	O
to	O
drop	O
the	O
whole	O
dataframe	B-API
created	O
in	O
pandas	O
.	O

like	O
in	O
R	O
:	O
rm	O
(	O
dataframe	B-API
)	O
or	O
in	O
SQL	O
:	O
drop	O
table	O

@USER	O
My	O
scenario	O
is	O
the	O
dataset	O
I	O
am	O
working	O
with	O
is	O
huge	O
.	O

So	O
once	O
loaded	O
(	O
with	O
pd.read_csv()	B-API
)	O
the	O
system	O
is	O
getting	O
slow	O
.	O

And	O
after	O
splitting	O
and	O
some	O
processing	O
,	O
I	O
want	O
to	O
remove	O
the	O
bigger	O
dataset	O
and	O
work	O
with	O
the	O
small	O
one.That's	O
why	O
I	O
need	O
.	O

How	O
do	O
you	O
clean	O
and	O
forward	O
fill	O
a	O
multiple	O
day	O
1	O
minute	O
time	O
series	B-API
with	O
pandas	O
?	O

Some	O
of	O
the	O
minutes	O
in	O
the	O
time	O
series	B-API
are	O
missing	O
:	O

With	O
pandas	O
,	O
how	O
do	O
I	O
forward	O
fill	O
the	O
series	B-API
so	O
every	O
minute	O
is	O
present	O
?	O

I	O
should	O
look	O
like	O
this	O
:	O
#CODE	O

So	O
I	O
copied	O
your	O
first	O
4	O
lines	O
into	O
a	O
dataframe	B-API
:	O
#CODE	O

The	O
key	O
thing	O
here	O
is	O
you	O
must	O
have	O
a	O
datetimeindex	B-API
,	O
if	O
you	O
want	O
to	O
keep	O
it	O
as	O
a	O
column	O
then	O
you	O
can	O
just	O
set	O
`	O
drop=False	O
`	O
in	O
`	O
set_index	B-API
`	O
.	O

Just	O
creating	O
the	O
dataframe	B-API
:	O
#CODE	O

Reindexing	O
and	O
filling	O
the	O
holes	O
,	O
then	O
forward	O
filling	O
the	O
resulting	O
NaN	O
values	O
,	O
then	O
dropping	O
all	O
times	O
outside	O
of	O
9:30	O
AM	O
to	O
4:00	O
PM	O
:	O
#CODE	O

First	O
,	O
reindex	B-API
the	O
dataframe	B-API
so	O
that	O
your	O
index	O
corresponds	O
to	O
your	O
starting	O
date	O
/	O
time	O
through	O
your	O
ending	O
date	O
/	O
time	O
with	O
a	O
frequency	O
of	O
1	O
minute	O
:	O
#CODE	O

This	O
will	O
create	O
a	O
lot	O
of	O
NaN	O
values	O
where	O
the	O
new	O
index	O
didn't	O
line	O
up	O
with	O
the	O
old	O
one	O
.	O

We	O
fill	O
this	O
with	O
ffill	B-API
(	O
forward	O
fill	O
)	O
,	O
though	O
there	O
are	O
other	O
options	O
out	O
there	O
:	O
#CODE	O

Because	O
.time()	B-API
doesn't	O
take	O
9.5	O
and	O
the	O
documentation	O
is	O
kind	O
of	O
sparse	O
,	O
I	O
just	O
created	O
a	O
datetime	O
object	O
with	O
the	O
time	O
value	O
set	O
to	O
9:30	O
AM	O
and	O
then	O
used	O
.time()	B-API
to	O
grab	O
this	O
.	O

There's	O
a	O
better	O
way	O
,	O
I'm	O
sure	O
.	O

I	O
am	O
pulling	O
this	O
data	O
from	O
a	O
Mongo	O
database	O
,	O
and	O
cleaning	O
it	O
using	O
pandas	O
(	O
converting	O
'	O
price	O
'	O
to	O
float	O
,	O
'	O
rank	O
'	O
to	O
int	O
,	O
string	O
date	O
to	O
datetime	O
,	O
etc	O
.	O
)	O
.	O

The	O
goal	O
I	O
had	O
in	O
mind	O
was	O
to	O
create	O
a	O
series	B-API
of	O
additional	O
dataframes	O
/	O
tables	O
(	O
one	O
for	O
each	O
'	O
rank	O
')	O
and	O
link	O
these	O
to	O
the	O
main	O
table	O
in	O
a	O
HDFStore	O
for	O
each	O
day	O
.	O

Something	O
like	O
:	O
#CODE	O

Before	O
it's	O
cleaned	O
(	O
and	O
to	O
some	O
extent	O
,	O
after	O
)	O
the	O
data	O
is	O
quite	O
large	O
.	O

Each	O
day	O
of	O
observation	O
is	O
~	O
1.5M	O
rows	O
,	O
and	O
uses	O
around	O
9GB	O
of	O
RAM	O
(	O
I	O
have	O
32GB	O
limit	O
)	O
when	O
imported	O
into	O
a	O
pandas	O
DataFrame	B-API
.	O

The	O
number	O
of	O
rows	O
in	O
the	O
total	O
data	O
set	O
is	O
~800M	O
.	O

Any	O
advice	O
or	O
help	O
will	O
be	O
greatly	O
appreciated	O
.	O

I	O
understand	O
this	O
is	O
a	O
broad	O
question	O
,	O
but	O
I	O
am	O
in	O
somewhat	O
unfamiliar	O
territory	O
.	O

Python	O
pandas	O
dataframe	B-API
max	O
in	O
a	O
group	O
based	O
on	O
conditions	O
on	O
other	O
columns	O

I'm	O
not	O
sure	O
if	O
this	O
question	O
has	O
been	O
asked	O
before	O
.	O

In	O
a	O
pandas	O
dataframe	B-API
I	O
have	O
data	O
like	O
#CODE	O

Then	O
,	O
groupby	B-API
the	O
device	O
type	O
,	O
and	O
using	O
the	O
cumulative	O
sum	O
of	O
the	O
`	O
new_sample	O
`	O
column	O
,	O
create	O
a	O
counter	O
for	O
which	O
trial	O
of	O
each	O
device	O
each	O
row	O
represents	O
.	O

#CODE	O

Using	O
Pandas	O
to	O
create	O
DataFrame	B-API
with	O
Series	B-API
,	O
resulting	O
in	O
memory	O
error	O

I'm	O
using	O
Pandas	O
library	O
for	O
remote	O
sensing	O
time	O
series	B-API
analysis	O
.	O

Eventually	O
I	O
would	O
like	O
to	O
save	O
my	O
DataFrame	B-API
to	O
csv	O
by	O
using	O
chunk-sizes	O
,	O
but	O
I	O
run	O
into	O
a	O
little	O
issue	O
.	O

My	O
code	O
generates	O
6	O
NumPy	O
arrays	O
that	O
I	O
convert	O
to	O
Pandas	O
Series	B-API
.	O

Each	O
of	O
these	O
Series	B-API
contains	O
a	O
lot	O
of	O
items	O
#CODE	O

I	O
would	O
like	O
to	O
add	O
the	O
Series	B-API
into	O
a	O
Pandas	O
DataFram	O
(	O
df	O
)	O
so	O
I	O
can	O
save	O
them	O
chunk	O
by	O
chunk	O
to	O
a	O
csv	O
file	O
.	O

#CODE	O

Any	O
suggestions	O
?	O

Is	O
it	O
possible	O
to	O
fill	O
the	O
Pandas	O
DataFrame	B-API
chunk	O
by	O
chunk	O
?	O

Can	O
you	O
make	O
a	O
DataFrame	B-API
from	O
a	O
single	O
column	O
:	O
pd.DataFrane({'tmax	O
'	O
:	O
pd.Series	B-API
(	O
tmaxSeries	O
)	O
}	O
)	O
?	O

create	O
a	O
frame	O
with	O
the	O
first	O
series	B-API
,	O
and	O
add	O
them	O
sequentially	O
,	O
e.g.	O
``	O
df	O
=	O
DataFrame({'prcp	O
'	O
:	O
pd.Series	B-API
(	O
prcpSeries	O
)	O
}	O
);	O
df['tmax	O
']	O
=	O
pd.Series	B-API
(	O
tmaxSeries	O
)``	O
.	O

You	O
should	O
probably	O
write	O
it	O
to	O
a	O
HDF5	O
in	O
any	O
event	O
,	O
see	O
:	O
#URL	O

When	O
you	O
are	O
passing	O
a	O
dict	O
(	O
even	O
if	O
the	O
values	O
are	O
Series	B-API
)	O
,	O
I	O
think	O
copies	O
are	O
made	O
.	O

If	O
you	O
do	O
iteratively	O
(	O
and	O
argument	O
is	O
a	O
series	B-API
)	O
,	O
then	O
no	O
copy	O

@USER	O
I've	O
cobbled	O
something	O
together	O
...	O

I	O
think	O
I	O
prefer	O
using	O
an	O
outer	O
concat	B-API
tbh	O
.	O

If	O
you	O
know	O
each	O
of	O
these	O
are	O
the	O
same	O
length	O
then	O
you	O
could	O
create	O
the	O
DataFrame	B-API
directly	O
from	O
the	O
array	O
and	O
then	O
append	O
each	O
column	O
:	O
#CODE	O

Note	O
:	O
you	O
can	O
also	O
use	O
the	O
`	O
to_frame	B-API
`	O
method	O
(	O
which	O
allows	O
you	O
to	O
(	O
optionally	O
)	O
pass	O
a	O
name	O
-	O
which	O
is	O
useful	O
if	O
the	O
Series	B-API
doesn't	O
have	O
one	O
):	O
#CODE	O

However	O
,	O
if	O
they	O
are	O
variable	O
length	O
then	O
this	O
will	O
lose	O
some	O
data	O
(	O
any	O
arrays	O
which	O
are	O
longer	O
than	O
`	O
prcpSeries	O
`)	O
.	O

An	O
alternative	O
here	O
is	O
to	O
create	O
each	O
as	O
a	O
DataFrame	B-API
and	O
then	O
perform	O
an	O
outer	O
join	O
(	O
using	O
`	O
concat	B-API
`	O
):	O
#CODE	O

Thanks	O
Andy	O
and	O
Jeff	O
!	O

I've	O
to	O
use	O
the	O
first	O
method	O
with	O
appending	O
each	O
column	O
,	O
since	O
the	O
second	O
approach	O
gets	O
an	O
Memory	O
Error	O
at	O
the	O
line	O
of	O
df	O
=	O
pd.concat	B-API
(	O
etc	O
.	O
)	O
.	O

I	O
know	O
the	O
series	B-API
with	O
the	O
longest	O
length	O
and	O
will	O
use	O
that	O
one	O
to	O
initialise	O
the	O
DataFrame	B-API
.	O

One	O
caveat	O
:	O
I'm	O
using	O
pandas	O
version	O
0.14.1	O
and	O
when	O
I	O
try	O
to	O
coerce	O
a	O
Series	B-API
object	O
to	O
a	O
DataFrame	B-API
object	O
,	O
if	O
I	O
specify	O
`	O
columns	O
=	O
[	O
'	O
my_column_name	O
']`	O
in	O
the	O
`	O
pandas.DataFrame()	B-API
`	O
call	O
,	O
the	O
resulting	O
object	O
is	O
an	O
empty	O
DataFrame	B-API
.	O

When	O
I	O
dropped	O
the	O
columns	O
argument	O
,	O
the	O
resulting	O
DataFrame	B-API
was	O
as	O
expected	O
.	O

@USER	O
Thanks	O
for	O
mentioning	O
this	O
,	O
perhaps	O
it's	O
cleaner	O
to	O
use	O
the	O
`	O
to_frame	B-API
`	O
method	O
here	O
(	O
I'm	O
not	O
sure	O
this	O
was	O
available	O
when	O
I	O
wrote	O
the	O
original	O
answer	O
)	O
-	O
I've	O
updated	O
this	O
answer	O
to	O
mention	O
that	O
.	O

I	O
will	O
have	O
a	O
look	O
to	O
see	O
if	O
this	O
no	O
longer	O
works	O
in	O
0.14	O
+	O
,	O
I	O
will	O
have	O
a	O
check	O
later	O
to	O
see	O
.	O

Python	O
pandas	O
,	O
how	O
to	O
only	O
plot	O
a	O
DataFrame	B-API
that	O
actually	O
have	O
the	O
datapoint	O
and	O
leave	O
the	O
gap	O
out	O

I	O
have	O
a	O
DataFrame	B-API
with	O
intraday	O
data	O
indexed	O
with	O
DatetimeIndex	B-API
#CODE	O

df3.plot()	O

One	O
way	O
to	O
do	O
this	O
is	O
to	O
`	O
resample	B-API
`	O
(	O
hourly	O
)	O
before	O
you	O
plot	O
:	O
#CODE	O

Pandas	O
Intraday	O
Time	O
Series	B-API
plots	O

Pandas	O
:	O
Efficient	O
spatial	O
analysis	O
using	O
adjacency	O
matrix	O

I	O
have	O
a	O
very	O
large	O
DataFrame	B-API
with	O
coordinates	O
.	O

Let's	O
take	O
the	O
following	O
example	O
:	O
#CODE	O

Based	O
on	O
this	O
DataFrame	B-API
I	O
need	O
to	O
calculate	O
the	O
distance	O
between	O
points	O
various	O
times	O
.	O

Often	O
the	O
points	O
which	O
need	O
to	O
be	O
compared	O
with	O
each	O
other	O
are	O
the	O
same	O
,	O
for	O
example	O
when	O
I	O
want	O
to	O
calculate	O
the	O
distance	O
from	O
Carl	O
to	O
all	O
other	O
Buyers	O
each	O
day	O
.	O

#CODE	O

To	O
do	O
this	O
efficiently	O
and	O
not	O
having	O
to	O
calculate	O
the	O
same	O
distances	O
multiple	O
times	O
,	O
I	O
am	O
hoping	O
to	O
build	O
an	O
adjacency	O
matrix	O
with	O
the	O
Buyers	O
and	O
their	O
distances	O
to	O
each	O
other	O
.	O

As	O
a	O
result	O
I	O
could	O
query	O
this	O
matrix	O
instead	O
of	O
the	O
doing	O
the	O
distance	O
calculations	O
.	O

What	O
would	O
you	O
recommend	O
as	O
data	O
structure	O
for	O
this	O
adjacency	O
matrix	O
and	O
how	O
would	O
you	O
implement	O
the	O
lookup	O
so	O
that	O
it	O
is	O
faster	O
than	O
a	O
dedicated	O
distance	O
computation	O
.	O

I	O
would	O
deeply	O
appreciate	O
any	O
help	O
.	O

It	O
looks	O
like	O
you	O
want	O
an	O
adjacency	O
matrix	O
for	O
each	O
day	O
.	O

I	O
suggest	O
a	O
dictionary	O
in	O
which	O
keys	O
are	O
dates	O
and	O
values	O
are	O
DataFrames	O
,	O
where	O
each	O
axis	O
lists	O
the	O
customers	O
.	O

Alternatively	O
you	O
could	O
look	O
into	O
the	O
pandas	O
'	O
Panel	B-API
objects	O
.	O

Hi	O
Dan	O
,	O
I	O
am	O
afraid	O
you	O
got	O
me	O
wrong	O
.	O

My	O
idea	O
is	O
rather	O
that	O
I	O
have	O
an	O
adjacency	O
matrix	O
between	O
the	O
Buyers	O
which	O
has	O
in	O
each	O
cell	O
the	O
distance	O
between	O
them	O
.	O

I	O
updated	O
my	O
problem	O
description	O
.	O

I	O
would	O
deeply	O
appreciate	O
to	O
get	O
your	O
opinion	O
about	O
that	O
.	O

Lets	O
say	O
that	O
I	O
have	O
a	O
dataframe	B-API
(	O
using	O
pandas	O
data	O
analysis	O
library	O
)	O
that	O
looks	O
like	O
so	O
:	O
#CODE	O

And	O
I	O
want	O
to	O
get	O
the	O
dataframe	B-API
to	O
look	O
like	O
this	O
:	O
#CODE	O

Note	O
that	O
`"	O
Unnamed	O
:	O
1	O
"`	O
sounds	O
suspiciously	O
like	O
the	O
name	O
`	O
pd.read_csv	B-API
`	O
assigns	O
to	O
the	O
column	O
if	O
the	O
header	O
row	O
lacked	O
a	O
column	O
name	O
.	O

In	O
that	O
case	O
,	O
instead	O
of	O
patching	O
up	O
the	O
result	O
as	O
shown	O
above	O
,	O
you	O
might	O
be	O
able	O
to	O
fix	O
the	O
problem	O
with	O
`	O
skiprows=1	O
`	O
or	O
`	O
header=1	O
`	O
instead	O
.	O

`	O
skiprows=1	O
`	O
would	O
cause	O
`	O
pd.read_csv	B-API
`	O
to	O
skip	O
the	O
first	O
row	O
and	O
thus	O
read	O
the	O
headers	O
(	O
column	O
names	O
)	O
from	O
the	O
second	O
row	O
automatically	O
.	O

I	O
am	O
new	O
to	O
python	O
/	O
pandas	O
,	O
and	O
encountering	O
an	O
issue	O
I	O
can't	O
really	O
make	O
sense	O
of	O
.	O

I	O
have	O
Twitter	O
data	O
in	O
csv	O
files	O
which	O
contain	O
1000	O
tweets	O
each	O
.	O

When	O
I	O
read	O
the	O
files	O
into	O
a	O
dataframe	B-API
,	O
all	O
works	O
fine	O
(	O
it	O
just	O
takes	O
a	O
lot	O
of	O
time	O
to	O
read	O
the	O
files	O
):	O
#CODE	O

This	O
dataframe	B-API
,	O
which	O
contains	O
around	O
6	O
million	O
tweets	O
,	O
can	O
be	O
manipulated	O
at	O
a	O
reasonable	O
speed	O
.	O

However	O
,	O
if	O
it	O
is	O
saved	O
#CODE	O

even	O
the	O
most	O
simple	O
operations	O
,	O
e.g.	O
finding	O
a	O
string	O
in	O
a	O
tweet	O
(	O
which	O
works	O
perfectly	O
fine	O
in	O
the	O
initial	O
dataframe	B-API
!	O
)	O
,	O
is	O
extremely	O
slow	O
(	O
1-2s	O
per	O
row	O
)	O
.	O

The	O
problem	O
seems	O
to	O
be	O
particularly	O
distinct	O
when	O
I	O
loop	O
through	O
all	O
rows	O
.	O

Is	O
there	O
any	O
reason	O
for	O
this	O
?	O

Would	O
it	O
help	O
dumping	O
all	O
data	O
into	O
a	O
database	O
and	O
reading	O
it	O
from	O
there	O
?	O

Here	O
is	O
the	O
df.info()	B-API
of	O
the	O
initial	O
file	O
(	O
stopped	O
after	O
50	O
files	O
):	O
#CODE	O

Is	O
there	O
reason	O
you	O
don't	O
just	O
store	O
the	O
dfs	O
in	O
a	O
list	O
and	O
then	O
concat	B-API
them	O
all	O
?	O

Also	O
this	O
line	O
is	O
unnecessary	O
:	O
`	O
dftemp2	O
=	O
pd.concat	B-API
([	O
dftemp1	O
[	O
"	O
source	O
"]	O
,	O
dftemp1	O
[	O
"	O
text	O
"]	O
,	O
dftemp1	O
[	O
"	O
timestamp_ms	O
"]]	O
,	O
axis=1	O
)`	O
you	O
can	O
specify	O
the	O
columns	O
of	O
interest	O
in	O
the	O
params	O
to	O
`	O
read_csv	B-API
`	O
,	O
`	O
usecols	O
`	O
:	O
#URL	O

Can	O
you	O
show	O
`	O
df.info()`	O
before	O
and	O
after	O
writing	O
/	O
reading	O
it	O
to	O
csv	O
?	O

@USER	O
added	O
df.info()	B-API

@USER	O
Thanks	O
,	O
changed	O
the	O
code	O
.	O

Aside	O
from	O
speeding	O
up	O
the	O
concat	B-API
process	O
from	O
2	O
hours	O
to	O
10	O
minutes	O
,	O
it	O
also	O
solved	O
the	O
problem	O
with	O
the	O
loaded	O
csv	O
-	O
it	O
runs	O
at	O
normal	O
speed	O
now	O
(	O
for	O
whatever	O
reason	O
)	O
:)	O

@USER	O
would	O
you	O
like	O
me	O
to	O
post	O
an	O
answer	O
,	O
I	O
can't	O
explain	O
everything	O
but	O
concatenating	O
repeatedly	O
was	O
always	O
going	O
to	O
be	O
slower	O
than	O
building	O
a	O
single	O
list	O
upfront	O
first	O
and	O
then	O
concatenating	O
all	O
at	O
once	O

I'm	O
pretty	O
sure	O
that	O
repeated	O
concatenations	O
make	O
a	O
copy	O
of	O
both	O
the	O
source	O
and	O
target	O
dataframes	O
each	O
time	O
.	O

This	O
gets	O
big	O
and	O
slow	O
as	O
the	O
target	O
gets	O
big	O
.	O

Concatenating	O
the	O
single	O
list	O
seems	O
to	O
have	O
some	O
deep	O
magic	O
that	O
makes	O
it	O
faster	O
.	O

I	O
suspect	O
it	O
has	O
to	O
do	O
with	O
allocating	O
memory	O
so	O
that	O
there's	O
fewer	O
copies	O
.	O

I	O
have	O
a	O
bunch	O
of	O
CSV	O
files	O
with	O
4	O
line	O
headers	O
.	O

In	O
these	O
files	O
,	O
I	O
want	O
to	O
change	O
the	O
values	O
in	O
the	O
sixth	O
column	O
based	O
on	O
the	O
values	O
in	O
the	O
second	O
column	O
.	O

For	O
example	O
,	O
if	O
the	O
second	O
column	O
,	O
under	O
the	O
name	O
`	O
PRODUCT	B-API
`	O
is	O
`	O
Banana	O
`	O
,	O
I	O
would	O
want	O
to	O
change	O
the	O
value	O
in	O
the	O
same	O
row	O
under	O
`	O
TIME	B-API
`	O
to	O
`	O
10m	O
`	O
.	O

If	O
the	O
the	O
product	O
was	O
`	O
Apple	O
`	O
I	O
would	O
want	O
the	O
time	O
to	O
be	O
`	O
15m	O
`	O
and	O
so	O
on	O
.	O

#CODE	O

Assuming	O
that	O
your	O
data	O
is	O
in	O
a	O
Pandas	O
DataFrame	B-API
and	O
looks	O
something	O
like	O
this	O
:	O
#CODE	O

The	O
code	O
first	O
loops	O
through	O
the	O
rows	O
of	O
the	O
dataframe	B-API
column	O
"	O
PRODUCT	O
"	O
,	O
with	O
the	O
row	O
value	O
stored	O
as	O
i	O
and	O
the	O
row-number	O
stored	O
as	O
numi	O
.	O

It	O
then	O
uses	O
if	O
statements	O
to	O
identify	O
the	O
different	O
levels	O
of	O
interest	O
in	O
the	O
Product	O
column	O
.	O

For	O
those	O
rows	O
with	O
the	O
levels	O
of	O
interest	O
(	O
eg	O
"	O
Banana	O
"	O
or	O
"	O
Apple	O
")	O
,	O
it	O
uses	O
the	O
row-numbers	O
to	O
change	O
the	O
value	O
of	O
another	O
column	O
in	O
the	O
same	O
row	O
.	O

There	O
are	O
lots	O
of	O
ways	O
to	O
do	O
this	O
,	O
and	O
depending	O
on	O
the	O
size	O
of	O
your	O
data	O
and	O
the	O
number	O
of	O
levels	O
(	O
in	O
this	O
case	O
"	O
Products	O
")	O
you	O
want	O
to	O
change	O
,	O
this	O
isn't	O
necessarily	O
the	O
most	O
efficient	O
way	O
to	O
do	O
this	O
.	O

But	O
since	O
you're	O
a	O
beginner	O
,	O
this	O
will	O
probably	O
be	O
a	O
good	O
basic	O
way	O
of	O
doing	O
it	O
for	O
you	O
to	O
start	O
with	O
.	O

fyi	O
:	O
you	O
shouldn't	O
do	O
this	O
type	O
of	O
chained	O
assignment	O
,	O
see	O
here	O
:	O
#URL	O
also	O
iterating	O
over	O
a	O
frame	O
is	O
not	O
efficient	O
,	O
this	O
is	O
an	O
easily	O
vectorized	O
problem	O

You	O
can	O
do	O
this	O
with	O
a	O
combination	O
of	O
`	O
groupby	B-API
`	O
,	O
`	O
replace	B-API
`	O
and	O
a	O
`	O
dict	O
`	O
#CODE	O

`df.groupby('fruits	O
')`	O
splits	O
the	O
`	O
DataFrame	B-API
`	O
into	O
subsets	O
(	O
which	O
are	O
`	O
DataFrame	B-API
`	O
s	O
or	O
`	O
Series	B-API
`	O
objects	O
)	O
using	O
the	O
values	O
of	O
the	O
`	O
fruits	O
`	O
column	O
.	O

The	O
`	O
apply	B-API
`	O
method	O
applies	O
a	O
function	O
to	O
each	O
of	O
the	O
aforementioned	O
subsets	O
and	O
concatenates	O
the	O
result	O
(	O
if	O
needed	O
)	O
.	O

`	O
replacer	O
`	O
is	O
where	O
the	O
"	O
magic	O
"	O
happens	O
:	O
each	O
group's	O
`	O
time	B-API
`	O
values	O
get	O
replaced	O
(	O
`	O
to_replace	O
`)	O
with	O
the	O
new	O
value	O
that's	O
defined	O
in	O
`	O
time_map	O
`	O
.	O

The	O
`	O
get	B-API
`	O
method	O
of	O
`	O
dict	O
`	O
s	O
allows	O
you	O
to	O
provide	O
a	O
default	O
value	O
if	O
the	O
key	O
you're	O
searching	O
for	O
(	O
the	O
fruit	O
name	O
in	O
this	O
case	O
)	O
is	O
not	O
there	O
.	O

`	O
nan	O
`	O
is	O
commonly	O
used	O
for	O
this	O
purpose	O
,	O
but	O
here	O
I'm	O
actually	O
just	O
using	O
the	O
time	O
that	O
was	O
already	O
there	O
if	O
there	O
isn't	O
a	O
new	O
one	O
defined	O
for	O
it	O
in	O
the	O
`	O
time_map	O
`	O
`	O
dict	O
`	O
.	O

One	O
thing	O
to	O
note	O
is	O
my	O
use	O
of	O
`	O
g.name	O
`	O
.	O

This	O
doesn't	O
normally	O
exist	O
as	O
an	O
attribute	O
on	O
`	O
DataFrame	B-API
`	O
s	O
(	O
you	O
can	O
of	O
course	O
define	O
it	O
yourself	O
if	O
you	O
want	O
to	O
)	O
,	O
but	O
is	O
there	O
so	O
you	O
can	O
perform	O
computations	O
that	O
may	O
require	O
the	O
group	O
name	O
.	O

In	O
this	O
case	O
that's	O
the	O
"	O
current	O
"	O
fruit	O
you're	O
looking	O
at	O
when	O
you	O
apply	O
your	O
function	O
.	O

How	O
to	O
sort	O
a	O
dataFrame	B-API
in	O
python	O
pandas	O
by	O
two	O
or	O
more	O
columns	O
?	O

As	O
commented	O
,	O
the	O
`	O
sort	B-API
`	O
method	O
is	O
now	O
deprecated	O
in	O
favor	O
of	O
`	O
sort_values	B-API
`	O
.	O

The	O
arguments	O
(	O
and	O
results	O
)	O
remain	O
the	O
same	O
:	O
#CODE	O

You	O
can	O
use	O
the	O
ascending	O
argument	O
of	O
`	O
sort	B-API
`	O
:	O
#CODE	O

Sort	O
isn't	O
in	O
place	O
by	O
default	O
!	O

So	O
you	O
should	O
assign	O
result	O
of	O
the	O
sort	B-API
method	O
to	O
a	O
variable	O
or	O
add	O
inplace=True	O
to	O
method	O
call	O
.	O

that	O
is	O
,	O
if	O
you	O
want	O
to	O
reuse	O
df1	O
as	O
a	O
sorted	O
DataFrame	B-API
:	O
#CODE	O

`	O
pd.DataFrame	B-API
(	O
randint	O
(	O
1	O
,	O
5	O
,	O
(	O
10	O
,	O
2	O
))	O
,	O
columns=['a','b	O
'])`	O
doesn't	O
seem	O
to	O
work	O
....	O

`	O
TypeError	O
:	O
randint()	O
takes	O
exactly	O
3	O
arguments	O
(	O
4	O
given	O
)`	O

Sort	O
isn't	O
in	O
place	O
by	O
default	O
!	O

So	O
you	O
should	O
assign	O
result	O
of	O
the	O
`	O
sort	B-API
`	O
method	O
to	O
a	O
variable	O
or	O
add	O
`	O
inplace=True	O
`	O
to	O
method	O
call	O
.	O

As	B-API
of	O
pandas	O
0.17.0	O
,	O
`	O
DataFrame.sort()	O
`	O
is	O
deprecated	O
,	O
and	O
set	O
to	O
be	O
removed	O
in	O
a	O
future	O
version	O
of	O
pandas	O
.	O

The	O
way	O
to	O
sort	O
a	O
dataframe	B-API
by	O
its	O
values	O
is	O
now	O
is	O
`	O
DataFrame.sort_values	B-API
`	O

I	O
had	O
the	O
same	O
problem	O
with	O
a	O
SSL	O
website	O
only	O
on	O
Linux	O
funny	O
enough	O
-on	O
Windows	O
the	O
same	O
code	O
parsed	O
the	O
tables	O
from	O
the	O
website	O
.	O

After	O
spending	O
some	O
time	O
comparing	O
and	O
updating	O
library	O
versions	O
on	O
Linux	O
with	O
no	O
result	O
,	O
I	O
just	O
added	O
some	O
extra	O
code	O
to	O
handle	O
the	O
SSL	O
certificate	O
before	O
using	O
read_html	B-API
:	O
#CODE	O

I	O
need	O
to	O
extract	O
any	O
lines	O
of	O
data	O
whose	O
time	O
is	O
within	O
certain	O
range	O
,	O
say	O
:	O
09:00	O
:	O
00	O
to	O
09:15	O
:	O
00	O
.	O

My	O
current	O
solution	O
is	O
simply	O
reading	O
in	O
each	O
data	O
file	O
to	O
a	O
data	O
frame	O
,	O
sorting	O
it	O
in	O
order	O
by	O
time	O
and	O
then	O
using	O
searchsorted	B-API
to	O
find	O
09:00	O
:	O
00	O
to	O
09:15	O
:	O
00	O
.	O

It	O
works	O
fine	O
if	O
performance	O
isn't	O
an	O
issue	O
and	O
I	O
don't	O
have	O
1000	O
files	O
waiting	O
to	O
be	O
processed	O
.	O

Any	O
suggestions	O
on	O
how	O
to	O
boost	O
the	O
speed	O
?	O

Thanks	O
for	O
help	O
in	O
advance	O
!!!	O

How	O
big	O
are	O
we	O
talking	O
here	O
?	O

you	O
could	O
read	O
all	O
the	O
csv's	O
into	O
a	O
list	O
of	O
df's	O
,	O
then	O
`	O
concat	B-API
`	O
them	O
all	O
set	O
the	O
index	O
to	O
be	O
the	O
time	O
and	O
then	O
filter	O
using	O
`	O
df.loc	B-API
[(	O
df.index.time	O
>	O
'	O
09:00	O
:	O
00	O
')	O
&	O
(	O
df.index.time	O
<	O
'	O
09:15	O
:	O
00	O
')]`	O
,	O
whilst	O
loading	O
them	O
you	O
could	O
sort	O
on	O
index	O
and	O
chuck	O
away	O
the	O
ones	O
that	O
don't	O
have	O
that	O
range	O
anyway	O

I	O
have	O
a	O
dataframe	B-API
that	O
includes	O
non-unique	O
time	O
stamps	O
,	O
and	O
I'd	O
like	O
to	O
group	O
them	O
by	O
time	O
windows	O
.	O

The	O
basic	O
logic	O
would	O
be	O
-	O

It	O
feels	O
like	O
a	O
df.groupby	B-API
(	O
pd.TimeGrouper	B-API
(	O
minutes=n	O
))	O
is	O
the	O
right	O
answer	O
,	O
but	O
I	O
don't	O
know	O
how	O
to	O
have	O
the	O
TimeGrouper	O
create	O
dynamic	O
time	O
ranges	O
when	O
it	O
sees	O
events	O
that	O
are	O
within	O
a	O
time	O
buffer	O
.	O

For	O
instance	O
,	O
if	O
I	O
try	O
a	O
TimeGrouper('20s	O
')	O
against	O
a	O
set	O
of	O
events	O
:	O
10:34	O
:	O
00	O
,	O
10:34	O
:	O
08	O
,	O
10:34	O
:	O
08	O
,	O
10:34	O
:	O
15	O
,	O
10:34	O
:	O
28	O
and	O
10:34	O
:	O
54	O
,	O
then	O
pandas	O
will	O
give	O
me	O
three	O
groups	O
(	O
events	O
falling	O
between	O
10:34	O
:	O
00	O
-	O
10:34	O
:	O
20	O
,	O
10:34	O
:	O
20	O
-	O
10:34	O
:	O
40	O
,	O
and	O
10:34	O
:	O
40-10	O
:	O
35:00	O
)	O
.	O

I	O
would	O
like	O
to	O
just	O
get	O
two	O
groups	O
back	O
,	O
10:34	O
:	O
00	O
-	O
10:34	O
:	O
28	O
,	O
since	O
there	O
is	O
no	O
more	O
than	O
a	O
20	O
second	O
gap	O
between	O
events	O
in	O
that	O
time	O
range	O
,	O
and	O
a	O
second	O
group	O
that	O
is	O
10:34	O
:	O
54	O
.	O

Given	O
a	O
Series	B-API
that	O
looks	O
something	O
like	O
-	O
#CODE	O

If	O
I	O
do	O
a	O
df.groupby(pd.TimeGrouper('20s	O
'))	O
on	O
that	O
Series	B-API
,	O
I	O
would	O
get	O
back	O
5	O
group	O
,	O
10:34	O
:	O
00-	O
:	O
20	O
,	O
:	O
20-	O
:	O
40	O
,	O
:	O
40-10	O
:	O
35:00	O
,	O
etc	O
.	O

What	O
I	O
want	O
to	O
do	O
is	O
have	O
some	O
function	O
that	O
creates	O
elastic	O
timeranges	O
..	O
as	O
long	O
as	O
events	O
are	O
within	O
20	O
seconds	O
,	O
expand	O
the	O
timerange	O
.	O

So	O
I	O
expect	O
to	O
get	O
back	O
-	O
#CODE	O

You	O
might	O
want	O
consider	O
using	O
apply	B-API
:	O
#CODE	O

It's	O
up	O
to	O
you	O
to	O
implement	O
just	O
any	O
grouping	O
logic	O
in	O
your	O
grouper	B-API
function	O
.	O

Btw	O
,	O
merging	O
overlapping	O
time	O
ranges	O
is	O
kind	O
of	O
iterative	O
task	O
:	O
for	O
example	O
,	O
A	O
=	O
(	O
0	O
,	O
10	O
)	O
,	O
B	O
=	O
(	O
20	O
,	O
30	O
)	O
,	O
C	O
=	O
(	O
10	O
,	O
20	O
)	O
.	O

After	O
C	O
appears	O
,	O
all	O
three	O
,	O
A	O
,	O
B	O
and	O
C	O
should	O
be	O
merged	O
.	O

Muzhig	O
,	O
thanks	O
for	O
the	O
response	O
!	O

I'm	O
not	O
sure	O
how	O
the	O
logic	O
plays	O
out	O
in	O
the	O
my_grouper	O
function	O
though	O
.	O

Can	O
you	O
show	O
me	O
an	O
example	O
of	O
what	O
my_grouper	O
would	O
look	O
like	O
if	O
you	O
were	O
just	O
finding	O
overlapping	O
tuples	O
(	O
as	O
your	O
A	O
,	O
B	O
,	O
and	O
C	O
are	O
in	O
your	O
post	O
)	O
?	O

Muzhig	O
,	O
I	O
appreciate	O
the	O
example	O
!	O

create	O
a	O
column	O
`	O
tsdiff	O
`	O
that	O
has	O
the	O
diffs	O
between	O
consecutive	O
times	O
(	O
using	O
`	O
shift	O
`)	O

`df['new_group	O
']	O
=	O
df.tsdiff	O
timedelta	O
`	O

`	O
fillna	B-API
`	O
on	O
the	O
`	O
new_group	O
`	O

`	O
groupby	B-API
`	O
that	O
column	O

This	O
is	O
how	O
to	O
use	O
to	O
create	O
a	O
custom	O
grouper	B-API
.	O

(	O
requires	O
pandas	O
>	O
=	O
0.13	O
)	O
for	O
the	O
timedelta	O
computations	O
,	O
but	O
otherwise	O
would	O
work	O
in	O
other	O
versions	O
.	O

Create	O
your	O
series	B-API
#CODE	O

Arbitrariy	O
assign	O
things	O
20s	O
to	O
group	O
0	O
,	O
else	O
to	O
group	O
1	O
.	O

This	O
could	O
also	O
be	O
more	O
arbitrary	O
.	O

if	O
the	O
diff	O
from	O
previous	O
is	O
0	O
BUT	O
the	O
total	O
diff	O
(	O
from	O
first	O
)	O
is	O
>	O
50	O
make	O
in	O
group	O
2	O
.	O

#CODE	O

Groupem	O
(	O
can	O
also	O
use	O
an	O
apply	B-API
here	O
)	O
#CODE	O

Jeff	O
,	O
I	O
definitely	O
like	O
where	O
you're	O
going	O
with	O
this	O
.	O

I'm	O
not	O
sure	O
how	O
to	O
take	O
that	O
out	O
to	O
scale	O
though	O
.	O

What	O
if	O
you	O
add	O
in	O
two	O
more	O
events	O
at	O
2013-01-01	O
10:34	O
:	O
55	O
and	O
10:35	O
:	O
12	O
.	O

You'd	O
end	O
up	O
with	O
the	O
indexer	O
dataframe	B-API
having	O
two	O
more	O
lines	O
:	O

The	O
indexer	O
could	O
be	O
as	O
big	O
as	O
the	O
original	O
series	B-API
if	O
you	O
want	O
.	O

I'll	O
update	O
the	O
example	O
to	O
do	O
what	O
I	O
think	O
you	O
want	O
.	O

This	O
is	O
just	O
an	O
example	O
,	O
I	O
am	O
not	O
sure	O
what	O
you	O
want	O
.	O

You	O
could	O
easily	O
have	O
a	O
function	O
do	O
this	O
if	O
you	O
want	O
.	O

Just	O
create	O
the	O
grouper	B-API
like	O
you	O
want	O
.	O

What	O
I	O
would	O
like	O
to	O
do	O
is	O
to	O
create	O
a	O
window	O
id	O
for	O
each	O
time	O
range	O
that	O
is	O
created	O
by	O
identifying	O
those	O
overlapping	O
events	O
.	O

In	O
your	O
example	O
(	O
grouper	B-API
dataframe	B-API
)	O
,	O
you	O
end	O
up	O
with	O
10:34	O
:	O
54	O
as	O
a	O
different	O
number	O
than	O
10:34	O
:	O
55	O
and	O
10:35	O
:	O
12	O
.	O

Maybe	O
I'm	O
framing	O
the	O
question	O
wrong	O
by	O
focusing	O
on	O
pandas	O
,	O
and	O
this	O
is	O
really	O
a	O
question	O
of	O
"	O
what's	O
the	O
best	O
way	O
to	O
create	O
elastic	O
ranges	O
of	O
overlapping	O
events	O
in	O
python	O
"	O
.	O

I	O
had	O
hoped	O
pandas	O
had	O
a	O
sort	O
of	O
built	O
in	O
TimeSeries	O
manipulation	O
function	O
in	O
here	O
already	O
.	O

I	O
need	O
to	O
create	O
res	O
a	O
new	O
dataframe	B-API
such	O
as	O
for	O
each	O
Media	O
modality	O
Budget	O
has	O
a	O
column	O
e.g.	O
:	O
#CODE	O

a	O
starting	O
point	O
might	O
be	O
`	O
pd.crosstab	B-API
(	O
dd.Budget	O
,	O
dd.Media	O
)`	O

Get	O
the	O
crosstab	B-API
on	O
`dd['Budget	O
']	O
,	O
dd['Media	O
']`	O
#CODE	O

And	O
,	O
then	O
merge	B-API
on	O
`	O
dd	O
`	O
and	O
fill	O
`NaN's	O
`	O
with	O
`	O
0	O
`	O
#CODE	O

Why	O
doesn't	O
this	O
function	O
"	O
take	O
"	O
after	O
I	O
iterrows	B-API
over	O
a	O
pandas	O
DataFrame	B-API
?	O

I	O
have	O
a	O
DataFrame	B-API
with	O
timestamped	O
temperature	O
and	O
wind	O
speed	O
values	O
,	O
and	O
a	O
function	O
to	O
convert	O
those	O
into	O
a	O
"	O
wind	O
chill	O
.	O

"	O
I'm	O
using	O
iterrows	B-API
to	O
run	O
the	O
function	O
on	O
each	O
row	O
,	O
and	O
hoping	O
to	O
get	O
a	O
DataFrame	B-API
out	O
with	O
a	O
nifty	O
"	O
Wind	O
Chill	O
"	O
column	O
.	O

However	O
,	O
while	O
it	O
seems	O
to	O
work	O
as	O
it's	O
going	O
through	O
,	O
and	O
has	O
actually	O
"	O
worked	O
"	O
at	O
least	O
once	O
,	O
I	O
can't	O
seem	O
to	O
replicate	O
it	O
consistently	O
.	O

I	O
feel	O
like	O
it's	O
something	O
I'm	O
missing	O
about	O
the	O
structure	O
of	O
DataFrames	O
,	O
in	O
general	O
,	O
but	O
I'm	O
hoping	O
someone	O
can	O
help	O
.	O

#CODE	O

But	O
,	O
when	O
I	O
look	O
at	O
the	O
DataFrame	B-API
again	O
,	O
the	O
NaN's	O
are	O
still	O
there	O
:	O
#CODE	O

for	O
the	O
whole	O
DataFrame	B-API
at	O
once	O
using	O
your	O
simple	O
`	O
windchill	O
`	O
function	O
.	O

You	O
can	O
use	O
`	O
apply	B-API
`	O
to	O
do	O
this	O
:	O
#CODE	O

Bu	O
they	O
don't	O
update	O
to	O
the	O
DataFrame	B-API
:	O
#CODE	O

@USER	O
are	O
you	O
saying	O
the	O
above	O
worked	O
on	O
newer	O
or	O
older	O
pandas	O
?	O

There	O
are	O
a	O
few	O
edge	O
cases	O
in	O
pandas	O
'	O
apply	B-API
which	O
have	O
been	O
tweaked	O
over	O
last	O
few	O
releases	O
so	O
this	O
could	O
be	O
one	O
of	O
them	O
!	O

@USER	O
,	O
yes	O
,	O
I	O
was	O
using	O
an	O
older	O
version	O
of	O
Anaconda	O
which	O
had	O
a	O
version	O
11	O
,	O
I	O
believe	O
,	O
edition	O
of	O
Pandas	O
,	O
and	O
was	O
using	O
iterrows	B-API
,	O
and	O
the	O
way	O
I	O
had	O
it	O
coded	O
worked	O
fine	O
,	O
with	O
references	O
to	O
updates	O
in	O
the	O
original	O
data	O
frame	O
via	O
row	O
.	O

But	O
that	O
didn't	O
work	O
(	O
apparently	O
row	O
referenced	O
a	O
copy	O
,	O
not	O
the	O
original	O
)	O
when	O
I	O
tried	O
on	O
two	O
later	O
versions	O
.	O

Fixed	O
it	O
in	O
my	O
code	O
with	O
direct	O
.loc	B-API
references	O
to	O
the	O
original	O
dataframe	B-API
.	O

Selecting	O
max	O
within	O
partition	O
for	O
pandas	O
dataframe	B-API

I	O
have	O
a	O
pandas	O
dataframe	B-API
.	O

My	O
goal	O
is	O
to	O
select	O
only	O
those	O
rows	O
where	O
column	O
C	O
has	O
the	O
largest	O
value	O
within	O
group	O
B	O
.	O

For	O
example	O
,	O
when	O
B	O
is	O
"	O
one	O
"	O
the	O
maximum	O
value	O
of	O
C	O
is	O
311	O
,	O
so	O
I	O
would	O
like	O
the	O
row	O
where	O
C	O
=	O
311	O
and	O
B	O
=	O
"	O
one	O
.	O

"	O
#CODE	O

Applying	O
`	O
Paul	O
H	O
`	O
solution	O
to	O
your	O
problem	O
yields	O
:	O
`df2.groupby('B	O
')	O
.apply	B-API
(	O
lambda	O
k	O
:	O
k[k['C	O
']	O
==	O
k['C	O
']	O
.max()	B-API
])`	O

You	O
can	O
use	O
`	O
idxmax()	B-API
`	O
,	O
which	O
returns	O
the	O
indices	O
of	O
the	O
max	O
values	O
:	O
#CODE	O

Python	O
:	O
Inserting	O
a	O
Row	O
into	O
a	O
Data	O
Frame	O

Is	O
there	O
a	O
more	O
pythonic	O
way	O
to	O
insert	O
a	O
row	O
into	O
a	O
data	O
frame	O
?	O

I	O
feel	O
like	O
this	O
has	O
to	O
be	O
a	O
functionality	O
of	O
pandas	O
but	O
can	O
not	O
find	O
it	O
.	O

Especially	O
,	O
is	O
there	O
a	O
way	O
to	O
'	O
reset	O
'	O
the	O
indices	O
?	O

DataFrames	O
have	O
a	O
`	O
reset_index	B-API
`	O
method	O
.	O

So	O
there's	O
that	O
.	O

Use	O
the	O
`	O
loc	B-API
`	O
attribute	O
to	O
assign	O
data	O
.	O

Syntax	O
is	O
`	O
df.loc	B-API
[	O
row_index	O
,	O
col_index	O
]`	O
.	O

An	O
example	O
:	O
#CODE	O

So	O
,	O
you're	O
saying	O
to	O
just	O
create	O
a	O
brand	O
new	O
row	O
,	O
and	O
then	O
re-index	O
the	O
rows	O
to	O
where	O
you	O
want	O
the	O
new	O
row	O
inserted	O
.	O

I	O
like	O
it	O
.	O

Still	O
hoping	O
there's	O
a	O
built-in	O
way	O
to	O
insert	O
a	O
row	O
in	O
a	O
desired	O
location	O
.	O

I	O
understand	O
you	O
as	O
to	O
what	O
.loc	B-API
does	O
.	O

I	O
don't	O
think	O
I'm	O
being	O
clear	O
on	O
what	O
i	O
mean	O
by	O
'	O
insert	B-API
'	O
row	O
.	O

I	O
don't	O
mean	O
append	O
a	O
row	O
.	O

I	O
mean	O
insert	O
a	O
row	O
at	O
a	O
certain	O
location	O
.	O

So	O
,	O
if	O
i	O
wanted	O
to	O
insert	O
row	O
'	O
e	O
'	O
between	O
rows	O
'	O
b	O
'	O
and	O
'	O
c	O
'	O
,	O
I	O
would	O
first	O
df.loc['e','E	O
']	O
then	O
df.reindex('a	O
b	O
e	O
c	O
d	O
'	O
.split()	B-API
)	O

If	O
your	O
current	O
function	O
works	O
well	O
enough	O
for	O
you	O
,	O
I	O
suggest	O
just	O
adding	O
`	O
reset_index	B-API
`	O
to	O
the	O
returned	O
result	O
.	O

See	O
something	O
like	O
below	O
:	O
#CODE	O

Thank	O
you	O
for	O
the	O
reset_index	B-API
option	O
.	O

That	O
certainly	O
improves	O
my	O
code	O
.	O

Multi-index	O
pivoting	O
in	O
Pandas	O

Consider	O
the	O
following	O
dataframe	B-API
:	O
#CODE	O

I	O
would	O
like	O
to	O
pivot	O
it	O
to	O
get	O
the	O
table	O
arranged	O
as	O
:	O
#CODE	O

If	O
I	O
understand	O
what	O
you	O
are	O
asking	O
I	O
think	O
what	O
you	O
want	O
is	O
`	O
pandas.pivot_table	B-API
(	O
...	O
)`	O
which	O
you	O
can	O
use	O
like	O
so	O
:	O
#CODE	O

produces	O
#CODE	O

Wow	O
.	O

OK	O
,	O
I	O
just	O
realized	O
that	O
`	O
pivot	B-API
`	O
and	O
`	O
pivot_table	B-API
`	O
are	O
two	O
different	O
methods	O
.	O

Yes	O
--	O
Thanks	O
,	O
I	O
was	O
reading	O
about	O
both	O
methods	O
and	O
testing	O
it	O
locally	O
.	O

Find	O
lowest	O
value	O
of	O
previous	O
3	O
days	O
in	O
pandas	O
DataFrame	B-API

I	O
am	O
trying	O
to	O
find	O
the	O
lowest	O
value	O
of	O
previous	O
3	O
days	O
in	O
a	O
time	O
series	B-API
.	O

I	O
tried	O
.shift()	B-API
,	O
.min()	B-API
etc	O
.	O
none	O
of	O
them	O
works	O
.	O

your	O
help	O
is	O
greatly	O
appreciated	O
!	O

Wonderful	O
!	O

quick	O
answer	O
and	O
thanks	O
for	O
fix	O
the	O
data	O
format	O
in	O
my	O
question	O
.	O

(	O
before	O
I	O
fixed	O
it	O
:)	O
)	O

More	O
generally	O
there	O
are	O
a	O
number	O
of	O
rolling-style	O
functions	O
to	O
handle	O
common	O
cases	O
and	O
a	O
`	O
rolling_apply	B-API
`	O
for	O
user	O
functions	O
.	O

Many	O
libraries	O
/	O
packages	O
have	O
these	O
sorts	O
of	O
functions	O
which	O
you	O
can	O
usually	O
find	O
by	O
searching	O
for	O
"	O
moving	O
"	O
or	O
"	O
rolling	O
"	O
.	O

Get	O
pandas.read_csv	B-API
to	O
read	O
empty	O
values	O
as	O
empty	O
string	O
instead	O
of	O
nan	O

It	O
correctly	O
reads	O
"	O
nan	O
"	O
as	O
the	O
string	O
"	O
nan	O
'	O
,	O
but	O
still	O
reads	O
the	O
empty	O
cells	O
as	O
NaN	O
.	O

I	O
tried	O
passing	O
in	O
`	O
str	B-API
`	O
in	O
the	O
`	O
converters	O
`	O
argument	O
to	O
read_csv	B-API
(	O
with	O
`converters={'One	O
'	O
:	O
str}	O
)`)	O
,	O
but	O
it	O
still	O
reads	O
the	O
empty	O
cells	O
as	O
NaN	O
.	O

I	O
realize	O
I	O
can	O
fill	O
the	O
values	O
after	O
reading	O
,	O
with	O
fillna	B-API
,	O
but	O
is	O
there	O
really	O
no	O
way	O
to	O
tell	O
pandas	O
that	O
an	O
empty	O
cell	O
in	O
a	O
particular	O
CSV	O
column	O
should	O
be	O
read	O
as	O
an	O
empty	O
string	O
instead	O
of	O
NaN	O
?	O

Use	O
the	O
fillna	B-API
method	O
,	O
but	O
use	O
it	O
twice	O
'	O
nan	O
'	O
=	O
'	O
nan	O
'	O
,	O
'	O
NaN	O
'	O
=	O
""	O
.	O

This	O
would	O
keep	O
comma's	O
lined	O
up	O
.	O

I	O
don't	O
understand	O
your	O
answer	O
.	O

As	O
I	O
said	O
,	O
I	O
don't	O
want	O
to	O
use	O
fillna	B-API
or	O
any	O
other	O
method	O
call	O
after	O
the	O
reading	O
.	O

I'm	O
asking	O
if	O
there's	O
a	O
way	O
to	O
make	O
the	O
conversion	O
take	O
place	O
during	O
the	O
CSV	O
reading	O
operation	O
.	O

In	O
the	O
meantime	O
,	O
`	O
result.fillna	O
(	O
'')`	O
should	O
do	O
what	O
you	O
want	O

[	O
Documentation	O
for	O
`	O
DataFrame.fillna	B-API
`	O
.	O
]	O
(	O
#URL	O
)	O
Try	O
`	O
result.fillna	O
(	O
''	O
,	O
inplace=True	O
)`	O
.	O

Otherwise	O
it	O
creates	O
a	O
copy	O
of	O
the	O
dataframe	B-API
.	O

sorry	O
to	O
resurrect	O
such	O
an	O
old	O
answer	O
,	O
but	O
did	O
this	O
ever	O
happen	O
?	O

As	O
far	O
as	O
I	O
can	O
tell	O
from	O
[	O
this	O
GitHub	O
PR	O
]	O
(	O
#URL	O
)	O
it	O
was	O
closed	O
without	O
ever	O
being	O
merged	O
,	O
and	O
I'm	O
not	O
seeing	O
the	O
requested	O
behavior	O
in	O
pandas	O
version	O
0.14.x	O

[	O
Documentation	O
]	O
(	O
#URL	O
)	O
for	O
read_csv	B-API
now	O
offers	O
both	O
`	O
na_values	O
`	O
(	O
list	O
or	O
dict	O
indexed	O
by	O
columns	O
)	O
and	O
`	O
keep_default_na	O
`	O
(	O
bool	O
)	O
.	O

The	O
`	O
keep_default_na	O
`	O
value	O
indicates	O
whether	O
pandas	O
'	O
default	O
NA	O
values	O
should	O
be	O
replaced	O
or	O
appended	O
to	O
.	O

The	O
OP's	O
code	O
doesn't	O
work	O
currently	O
just	O
because	O
it's	O
missing	O
this	O
flag	O
.	O

For	O
this	O
example	O
,	O
you	O
could	O
use	O
`pandas.read_csv('test.csv',na_values=['nan	O
']	O
,	O
keep_default_na=False	O
)`	O
.	O

Pandas	O
:	O
The	O
truth	O
value	O
of	O
a	O
Series	B-API
is	O
ambiguous	O

Oh	O
no	O
!	O

Iterating	O
over	O
just	O
`	O
readdata	O
`	O
gives	O
me	O
the	O
column	O
names	O
.	O

Well	O
that's	O
clearly	O
not	O
what	O
you	O
want	O
.	O

Poking	O
around	O
shows	O
me	O
that	O
there's	O
a	O
`	O
.iterrows()	B-API
`	O
method	O
on	O
`	O
readdata	O
`	O
,	O
so	O
let's	O
try	O
that	O
:	O
#CODE	O

Now	O
it's	O
time	O
to	O
dive	O
into	O
the	O
docs	O
to	O
figure	O
out	O
more	O
about	O
how	O
to	O
format	O
pandas	O
series	B-API
objects	O
to	O
get	O
the	O
output	O
the	O
way	O
you	O
want	O
.	O

As	O
a	O
hint	O
,	O
using	O
this	O
(	O
after	O
putting	O
`	O
import	O
sys	O
`	O
at	O
the	O
top	O
of	O
the	O
script	O
)	O
instead	O
of	O
`	O
print	O
line	O
`	O
might	O
be	O
what	O
you	O
want	O
:	O
#CODE	O

I	O
am	O
trying	O
to	O
get	O
the	O
Adj	O
Close	O
prices	O
from	O
Yahoo	O
Finance	O
into	O
a	O
DataFrame	B-API
.	O

I	O
have	O
all	O
the	O
stocks	O
I	O
want	O
but	O
I	O
am	O
not	O
able	O
to	O
sort	O
on	O
date	O
.	O

#CODE	O

Date	O
is	O
your	O
index	O
,	O
so	O
dataFrame.index	O
,	O
this	O
will	O
get	O
you	O
the	O
date	O
column	O

you	O
can	O
do	O
`	O
dataFrame	B-API
[	O
"	O
Date	O
"]	O
=	O
dataframe.index	O
`	O
if	O
you	O
want	O
to	O
add	O
a	O
column	O
called	O
'	O
Date	O
'	O
with	O
the	O
index	O
values	O
in	O
it	O

Use	O
dataFrame.index	O
to	O
directly	O
access	O
date	O
or	O
to	O
add	O
an	O
explicit	O
column	O
,	O
use	O
dataFrame	B-API
[	O
"	O
columnName	O
"]	O
=	O
dataframe.index	O

`	O
f	O
`	O
is	O
a	O
`	O
Panel	B-API
`	O

You	O
can	O
get	O
a	O
`	O
DataFrame	B-API
`	O
and	O
reset	O
index	O
(	O
Date	O
)	O
using	O
:	O
#CODE	O

but	O
I'm	O
not	O
sure	O
`	O
reset_index()	B-API
`	O
is	O
very	O
useful	O
as	O
you	O
can	O
get	O
Date	O
using	O
#CODE	O

Pandas	O
rolling_mean	B-API
with	O
center	O
keyword	O
leaves	O
last	O
values	O
as	O
NaN	O

I	O
noticed	O
behaviour	O
in	O
pandas	O
rolling_mean	B-API
function	O
I	O
did	O
not	O
expect	O
.	O

I	O
would	O
expect	O
the	O
last	O
value	O
to	O
be	O
3.5	O
as	O
in	O
(	O
3+4	O
)	O
/	O
2	O
which	O
would	O
be	O
consistent	O
with	O
the	O
first	O
value	O
in	O
the	O
series	B-API
.	O

Is	O
this	O
a	O
bug	O
or	O
a	O
feature	O
?	O

I	O
am	O
aware	O
that	O
"	O
financial	O
"	O
rolling	O
means	O
and	O
"	O
scientific	O
"	O
rolling	O
means	O
are	O
different	O
but	O
I	O
think	O
this	O
should	O
produce	O
a	O
series	B-API
without	O
NaN	O
values	O
.	O

python	O
pandas	O
transforming	O
dataframe	B-API

The	O
function	O
.stack()	B-API
almost	O
gets	O
it	O
done	O
but	O
in	O
the	O
wrong	O
format	O
.	O

close	O
but	O
.reset_index	B-API
(	O
level=0	O
)	O
gives	O
an	O
error	O
ValueError('cannot	O
insert	O
%s	O
,	O
already	O
exists	O
'	O
%	O
item	O
)	O

You	O
could	O
also	O
use	O
Pandas	O
`	O
get_dummies()	B-API
`	O
#CODE	O

Its	O
a	O
bit	O
obscure	O
in	O
one	O
line	O
.	O

`	O
df.unstack()	B-API
.dropna()	B-API
`	O
basically	O
flattens	O
your	O
DataFrame	B-API
to	O
a	O
series	B-API
and	O
drops	O
al	O
NaN's	O
.	O

The	O
`	O
get_dummies	B-API
`	O
gives	O
a	O
table	O
of	O
all	O
the	O
occurrences	O
of	O
the	O
letters	O
,	O
but	O
for	O
each	O
level	O
in	O
the	O
unstack	B-API
DataFrame	B-API
.	O

The	O
grouping	O
and	O
sum	O
then	O
combine	O
the	O
index	O
to	O
the	O
original	O
shape	O
.	O

Beautiful	O
.	O

Much	O
better	O
than	O
my	O
(	O
now-deleted	O
)	O
attempt	O
to	O
make	O
get_dummies	B-API
work	O
on	O
a	O
DataFrame	B-API
.	O

Really	O
like	O
the	O
``	O
unstack()	B-API
.dropna()	B-API
``	O
idiom	O
.	O

I	O
noticed	O
,	O
you	O
could	O
have	O
left	O
it	O
,	O
no	O
harm	O
in	O
having	O
some	O
options	O
.	O

I	O
still	O
agree	O
with	O
your	O
earlier	O
comment	O
that	O
its	O
less	O
pretty	O
(	O
and	O
readable	O
)	O
than	O
the	O
`	O
pivot	B-API
`	O
solution	O
from	O
Roman	O
.	O

The	O
concept	O
of	O
pivot	B-API
is	O
also	O
better	O
known	O
,	O
then	O
...	O

`	O
get_dummies	B-API
`	O
.	O

:)	O

Pandas	O
Dataseries	O
get	O
values	O
by	O
level	O

I	O
am	O
dealing	O
with	O
pandas	O
series	B-API
like	O
the	O
following	O

x	O
=p	O
d.Series	O
([	O
1	O
,	O
2	O
,	O
1	O
,	O
4	O
,	O
2	O
,	O
6	O
,	O
7	O
,	O
8	O
,	O
1	O
,	O
1	O
]	O
,	O
index=['a	O
'	O
,	O
'	O
b	O
'	O
,	O
'	O
a	O
'	O
,	O
'	O
c	O
'	O
,	O
'	O
b	O
'	O
,	O
'	O
d	O
'	O
,	O
'	O
e	O
'	O
,	O
'	O
f	O
'	O
,	O
'	O
g	O
'	O
,	O
'	O
g	O
'])	O

The	O
indices	O
are	O
non	O
unique	O
,	O
but	O
will	O
always	O
map	O
to	O
the	O
same	O
value	O
,	O
for	O
example	O
'	O
a	O
'	O
always	O
corresponds	O
to	O
'	O
1	O
'	O
in	O
my	O
sample	O
,	O
b	O
always	O
maps	O
to	O
'	O
2	O
'	O
etc	O
.	O

So	O
if	O
I	O
want	O
to	O
see	O
which	O
values	O
correspond	O
to	O
each	O
index	O
value	O
I	O
simply	O
need	O
to	O
write	O
#CODE	O

x=pd.Series(['1	O
'	O
,	O
'	O
2	O
'	O
,	O
'	O
1	O
'	O
,	O
'	O
4	O
'	O
,	O
'	O
2	O
'	O
,	O
'	O
6	O
'	O
,	O
'	O
7	O
'	O
,	O
'	O
8	O
'	O
,	O
'	O
1	O
'	O
,	O
'	O
1	O
']	O
,	O
index=['a	O
'	O
,	O
'	O
b	O
'	O
,	O
'	O
a	O
'	O
,	O
'	O
c	O
'	O
,	O
'	O
b	O
'	O
,	O
'	O
d	O
'	O
,	O
'	O
e	O
'	O
,	O
'	O
f	O
'	O
,	O
'	O
g	O
'	O
,	O
'	O
g	O
'])	O

So	O
long	O
as	O
your	O
indices	O
map	O
directly	O
to	O
the	O
values	O
then	O
you	O
can	O
simply	O
call	O
`	O
drop_duplicates	B-API
`	O
:	O
#CODE	O

This	O
works	O
as	O
long	O
as	O
any	O
other	O
index	O
is	O
not	O
mapped	O
to	O
XX	O
,	O
I	O
modified	O
my	O
question	O
to	O
reflect	O
this	O

`	O
pandas.Series.values	B-API
`	O
are	O
numpy	O
`	O
ndarray	O
`	O
s	O
.	O

Perhaps	O
doing	O
a	O
`	O
values.astype	O
(	O
int	O
)`	O
would	O
solve	O
your	O
problem	O
?	O

pandas	O
,	O
dataframe	B-API
,	O
groupby	B-API
,	O
std	O

New	O
to	O
pandas	O
here	O
.	O

A	O
(	O
trivial	O
)	O
problem	O
:	O
hosts	O
,	O
operations	O
,	O
execution	O
times	O
.	O

I	O
want	O
to	O
group	O
by	O
host	O
,	O
then	O
by	O
host+operation	O
,	O
calculate	O
std	O
deviation	O
for	O
execution	O
time	O
per	O
host	O
,	O
then	O
by	O
host+operation	O
pair	O
.	O

Seems	O
simple	O
?	O

how	O
do	O
I	O
calculate	O
std	O
deviation	O
on	O
`	O
dataframe.groupby	B-API
([	O
several	O
columns	O
])`	O
?	O

It's	O
important	O
to	O
know	O
your	O
version	O
of	O
Pandas	O
/	O
Python	O
.	O

Looks	O
like	O
this	O
exception	O
could	O
arise	O
in	O
Pandas	O
version	O
0.10	O
(	O
see	O
ValueError	O
:	O
Buffer	O
dtype	B-API
mismatch	O
,	O
expected	O
float64_t	O
but	O
got	O
float	O
)	O
.	O

To	O
avoid	O
this	O
,	O
you	O
can	O
cast	O
your	O
`	O
float	O
`	O
columns	O
to	O
`	O
float64	O
`	O
:	O
#CODE	O

As	O
far	O
as	O
it	O
goes	O
,	O
it	O
looks	O
like	O
`	O
std()	O
`	O
is	O
calling	O
`	O
aggregation()	O
`	O
on	O
the	O
`	O
groupby	B-API
`	O
result	O
,	O
and	O
a	O
subtle	O
bug	O
(	O
see	O
here	O
-	O
Python	O
Pandas	O
:	O
Using	O
Aggregate	B-API
vs	O
Apply	B-API
to	O
define	O
new	O
columns	O
)	O
.	O

To	O
avoid	O
this	O
,	O
you	O
can	O
use	O
`	O
apply()	B-API
`	O
:	O
#CODE	O

`byhostandop['time	O
']	O
.std()	B-API
`	O
again	O
raises	O
the	O
same	O
exception	O
.	O

just	O
out	O
of	O
curiosuty	O
,	O
have	O
you	O
tried	O
`byhostandop['time	O
']	O
.apply	B-API
(	O
lambda	O
x	O
:	O
x.std()	O
)`	O
?	O

`byhostandop['time	O
']	O
.apply	B-API
(	O
lambda	O
x	O
:	O
x.std()	O
)`	O
WORKS	O
.	O

Thank	O
you	O
!	O

In	O
`	O
df.astype	B-API
`	O
reply	O
,	O
did	O
you	O
mean	O
that	O
I	O
should	O
explicitly	O
convert	O
column	O
type	O
?	O

Like	O
this	O
works	O
:	O
`df['time	O
']	O
=	O
df['time'].astype('float64	O
')	O
;	O
byhostandop=df.groupby(['host	O
'	O
,	O
'	O
operation	O
'])	O
;	O
byhostandop['time	O
']	O
.std()	B-API
`	O
.	O

But	O
I'm	O
not	O
sure	O
if	O
this	O
is	O
idiomatic	O
pandas	O
operation	O
or	O
I	O
would	O
better	O
do	O
smth	O
else	O
to	O
get	O
correct	O
(	O
float64	O
)	O
type	O
in	O
column	O
for	O
std	O
dev	O
calculation	O
.	O

The	O
error	O
```	O
ValueError	O
:	O
Buffer	O
dtype	B-API
mismatch	O
,	O
expected	O
'	O
float64_t	O
'	O
but	O
got	O
'	O
float	O
'```	O
for	O
`	O
np.Float32	O
`	O
data	O
persists	O
in	O
pandas	O
0.13.0	O
.	O

The	O
temporary	O
solution	O
:	O
```	O
grouped	O
=	O
df.astype	B-API
(	O
np.float64	O
)	O
.groupby	B-API
(	O
...	O
)```	O
(	O
assuming	O
all	O
the	O
data	O
is	O
float	O
)	O

I'm	O
writing	O
some	O
data	O
analysis	O
pipelines	O
in	O
pandas	O
.	O

One	O
of	O
the	O
columns	O
in	O
the	O
dataframes	O
that	O
I've	O
been	O
working	O
with	O
is	O
made	O
up	O
of	O
objects	O
of	O
custom-written	O
classes	O
that	O
are	O
each	O
initialized	O
with	O
a	O
string	O
,	O
from	O
which	O
I	O
read	O
off	O
various	O
information	O
with	O
regular	O
expressions	O
and	O
store	O
in	O
the	O
object's	O
attributes	O
.	O

The	O
subclass	O
structure	O
is	O
similar	O
to	O
how	O
one	O
might	O
implement	O
a	O
tree	O
of	O
life	O
(	O
e.g.	O
a	O
Tiger	O
is	O
a	O
sublass	O
of	O
Cat	O
which	O
is	O
a	O
subclass	O
of	O
Animal	O
and	O
frequently	O
--	O
but	O
not	O
always	O
--	O
animals	O
with	O
the	O
same	O
superclass	O
will	O
share	O
methods	O
)	O
.	O

It	O
also	O
has	O
some	O
useful	O
methods	O
that	O
I	O
can	O
use	O
for	O
calculations	O
.	O

For	O
str	O
and	O
repr	O
methods	O
return	O
the	O
string	O
that	O
was	O
used	O
to	O
initialize	O
it	O
,	O
like	O
so	O
:	O
#CODE	O

This	O
means	O
that	O
when	O
I	O
want	O
to	O
view	O
my	O
data	O
frame	O
,	O
I	O
see	O
a	O
string	O
.	O

I	O
have	O
had	O
no	O
problems	O
using	O
the	O
to_csv()	B-API
method	O
on	O
data	O
frames	O
that	O
have	O
these	O
objects	O
in	O
them	O
,	O
but	O
when	O
I	O
use	O
the	O
to_excel()	B-API
method	O
of	O
the	O
pandas	O
data	O
frame	O
,	O
I	O
get	O
the	O
following	O
error	O
:	O
#CODE	O

Now	O
,	O
I	O
could	O
go	O
modify	O
the	O
offending	O
code	O
,	O
but	O
I	O
don't	O
want	O
to	O
have	O
to	O
deal	O
with	O
making	O
that	O
play	O
nicely	O
with	O
updates	O
to	O
xlsxwriter	O
.	O

I	O
could	O
simply	O
make	O
my	O
class	O
inherit	O
from	O
str	O
,	O
but	O
that	O
seems	O
like	O
unpythonic	O
given	O
that	O
I	O
don't	O
really	O
want	O
to	O
use	O
almost	O
any	O
of	O
the	O
string	O
methods	O
.	O

Lastly	O
,	O
I	O
could	O
sanitize	O
my	O
dataframe	B-API
by	O
taking	O
everything	O
in	O
it	O
that's	O
of	O
this	O
subclass	O
and	O
turning	O
it	O
back	O
into	O
a	O
string	O
,	O
but	O
that	O
would	O
mean	O
I	O
have	O
to	O
rewrite	O
a	O
lot	O
of	O
things	O
that	O
I	O
use	O
that	O
depend	O
on	O
being	O
able	O
to	O
use	O
the	O
DataFrame.to_excel	B-API
method	O
.	O

Is	O
there	O
anything	O
I	O
can	O
do	O
within	O
the	O
class	O
that	O
saves	O
me	O
from	O
having	O
to	O
inherit	O
everything	O
from	O
str	O
?	O

Trying	O
to	O
wrap	O
my	O
head	O
around	O
pandas	O
dataframe	B-API
(	O
with	O
no	O
looping	O
)	O

Use	O
`	O
where	B-API
`	O
to	O
conditionally	O
assign	O
a	O
value	O
where	O
true	O
and	O
return	O
a	O
different	O
value	O
if	O
false	O
:	O
#CODE	O

Just	O
return	O
that	O
using	O
`	O
np.where	O
`	O
:	O
`np.where(df['x	O
']	O
>	O
5	O
,	O
'	O
Bigger	O
'	O
,	O
'	O
Smaller	O
')`	O

My	O
script	O
uses	O
`	O
pandas.read_csv	B-API
`	O
to	O
directly	O
read	O
read	O
csv	O
files	O
into	O
data	O
frames	O
.	O

A	O
user	O
is	O
supposed	O
to	O
provide	O
a	O
configuration	O
file	O
that	O
contains	O
labels	O
of	O
columns	O
to	O
retain	O
along	O
with	O
corresponding	O
data	O
types	O
.	O

Let	O
me	O
illustrate	O
with	O
an	O
example	O
.	O

Here	O
is	O
a	O
csv	O
.	O

#CODE	O

This	O
fails	O
,	O
because	O
`	O
pandas.read_csv	B-API
`	O
requires	O
a	O
file	O
path	O
or	O
a	O
FileIO	O
stream	O
,	O
not	O
a	O
generator	O
.	O

Then	O
I	O
attempted	O
to	O
use	O
`	O
DataFrame.from_records	B-API
`	O
that	O
can	O
read	O
generators	O
,	O
but	O
doesn't	O
have	O
the	O
`	O
dtype	B-API
`	O
argument	O
of	O
`	O
pandas.read_csv	B-API
`	O
.	O

Finally	O
I	O
ended	O
up	O
mimicking	O
a	O
file	O
with	O
my	O
own	O
class	O
#CODE	O

I	O
know	O
that	O
I	O
can	O
read	O
in	O
all	O
lines	O
,	O
process	O
them	O
and	O
pass	O
to	O
`	O
pandas.DataFrame	B-API
`	O
,	O
but	O
I	O
don't	O
want	O
to	O
use	O
any	O
intermediate	O
Python	O
containers	O
,	O
because	O
the	O
files	O
are	O
huge	O
and	O
I	O
don't	O
want	O
to	O
run	O
into	O
memory	O
errors	O
.	O

Then	O
I	O
use	O
read	O
`	O
StringIO	O
`	O
by	O
chunks	O
using	O
parameter	O
`	O
chunksize=1024	O
`	O
and	O
`	O
iterator=True	O
`	O
and	O
concatenate	O
object	O
to	O
dataframe	B-API
`	O
df	O
`	O
.	O

Source	O
,	O
more	O
info	O
.	O

#CODE	O

That	O
was	O
supposed	O
to	O
be	O
`	O
dtype	B-API
`	O
not	O
`	O
dtypes	B-API
`	O
,	O
my	O
bad	O
,	O
sorry	O
.	O

As	O
for	O
your	O
solution	O
,	O
I've	O
clearly	O
stated	O
(	O
in	O
the	O
**	O
Note	O
**	O
section	O
)	O
that	O
the	O
lines	O
must	O
be	O
lazily	O
generated	O
.	O

In	O
your	O
case	O
,	O
you	O
preprocess	O
all	O
lines	O
in	O
memory	O
and	O
then	O
pass	O
them	O
to	O
`	O
StringIO	O
`	O

You	O
still	O
store	O
the	O
entire	O
file	O
here	O
`	O
lower_lines	O
=	O
u	O
""	O
.join	B-API
([	O
line.lower()	O
for	O
line	O
in	O
csv	O
])`	O
:)	O

`	O
DataFrame.append	B-API
`	O
is	O
highly	O
time	O
and	O
memory	O
inefficient	O
as	O
it	O
creates	O
a	O
new	O
data	O
frame	O
.	O

Memory-wise	O
it	O
might	O
be	O
even	O
more	O
inefficient	O
than	O
your	O
original	O
list-comprehension	O
based	O
approach	O
,	O
because	O
at	O
some	O
point	O
of	O
time	O
you	O
might	O
end	O
with	O
N	O
copies	O
of	O
a	O
data	O
frame	O
,	O
because	O
garbage	O
collection	O
is	O
not	O
happening	O
all	O
the	O
time	O
.	O

In	O
your	O
new	O
approach	O
you	O
still	O
hit	O
the	O
same	O
things	O
I've	O
already	O
pointed	O
out	O
.	O

`	O
str.join	B-API
`	O
stores	O
the	O
whole	O
file	O
in	O
memory	O
(	O
even	O
twice	O
in	O
fact	O
,	O
because	O
it	O
builds	O
a	O
temporary	O
array	O
out	O
of	O
an	O
iterable	O
and	O
then	O
builds	O
a	O
string	O
out	O
of	O
the	O
array	O
)	O
,	O
`	O
file.read	O
`	O
reads	O
the	O
entire	O
file	O
into	O
memory	O
by	O
definition	O
.	O

`	O
df.concat	O
`	O
is	O
inefficient	O
for	O
the	O
same	O
reason	O
`	O
df.append	B-API
`	O
is	O
.	O

How	O
to	O
fill	O
in	O
a	O
lot	O
of	O
data	O
utlizing	O
pandas	O
fillna	B-API
fast	O
?	O

I	O
have	O
two	O
Dataframes	O
one	O
large	O
one	O
with	O
a	O
lot	O
of	O
missing	O
values	O
and	O
a	O
second	O
one	O
with	O
data	O
to	O
fill	O
the	O
missing	O
data	O
in	O
the	O
first	O
one	O
.	O

Dataframe	B-API
examples	O
:	O
#CODE	O

You	O
can	O
use	O
try	O
`	O
groupby	B-API
`	O
-	O
each	O
group	O
is	O
transposing	O
with	O
`	O
T	O
`	O
and	O
then	O
`	O
fillna	B-API
`	O
`	O
df	O
`	O
by	O
values	O
of	O
`	O
df2	O
`	O
:	O
#CODE	O

Maybe	O
if	O
`	O
df	O
=	O
df.fillna	B-API
(	O
df2	O
)`	O
doesn't	O
work	O
,	O
can	O
be	O
use	O
`	O
df	O
=	O
df.combine_first	B-API
(	O
df2	O
)`	O
.	O

It	O
depends	O
on	O
index	O
.	O

How	O
does	O
it	O
work	O
?	O

I	O
cannot	O
compare	O
it	O
with	O
your	O
solution	O
,	O
because	O
error	O
:	O
`	O
NameError	O
:	O
global	O
name	O
'	O
toappend	O
'	O
is	O
not	O
defined	O
`	O

Thanks	O
for	O
your	O
answer	O
jezrael	O
!	O

I	O
fixed	O
the	O
cause	O
of	O
the	O
error	O
,	O
my	O
bad	O
sorry	O
.	O

I	O
have	O
quickly	O
checked	O
,	O
and	O
it	O
works	O
when	O
utilizing	O
`	O
df2	O
=	O
df2.groupby	O
(	O
df2.index	O
)	O
.apply	B-API
(	O
lambda	O
x	O
:	O
x.B.reset_index	O
(	O
drop=True	O
)	O
.T	O
)	O
`	O
instead	O
of	O
`	O
df2	O
=	O
df2.groupby('A	O
')	O
.apply	B-API
(	O
lambda	O
x	O
:	O
x.B.reset_index	O
(	O
drop=True	O
)	O
.T	O
)`	O
.	O

Need	O
to	O
test	O
on	O
the	O
large	O
dataset	O
when	O
I	O
get	O
home	O

But	O
I	O
get	O
the	O
following	O
warning	O
and	O
im	O
not	O
sure	O
how	O
to	O
fix	O
my	O
code	O
to	O
comply	O
?	O

#CODE	O

What	O
version	O
of	O
pandas	O
?	O

I	O
don't	O
see	O
this	O
on	O
0.17	O
,	O
so	O
perhaps	O
this	O
warnings	O
been	O
fixed	O
(	O
I	O
think	O
it	O
was	O
being	O
a	O
little	O
oversensitive	O
here	O
.	O
)	O
