How	O
to	O
check	O
if	O
the	O
signs	O
of	O
a	O
Series	B-API
conform	O
to	O
a	O
given	O
string	O
of	O
signs	O
?	O

For	O
example	O
I	O
have	O
a	O
Series	B-API
as	O
below	O
,	O
#CODE	O

To	O
check	O
the	O
whether	O
the	O
elements	O
of	O
the	O
Series	B-API
are	O
positive	O
you	O
could	O
create	O
a	O
Boolean	O
Series	B-API
like	O
this	O
:	O
#CODE	O

To	O
get	O
`	O
sign	O
`	O
into	O
a	O
similar	O
Boolean	O
series	B-API
,	O
you	O
can	O
need	O
to	O
interpret	O
the	O
strings	O
'	O
+	O
'	O
and	O
'	O
-	O
'	O
strings	O
as	O
Boolean	O
values	O
.	O

For	O
example	O
:	O
#CODE	O

Now	O
you	O
can	O
compare	O
the	O
two	O
series	B-API
and	O
use	O
`	O
all	B-API
`	O
.	O

In	O
one	O
line	O
,	O
the	O
whole	O
thing	O
looks	O
like	O
this	O
:	O
#CODE	O

This	O
solution	O
requires	O
some	O
numpy	O
functions	O
,	O
but	O
since	O
you	O
are	O
using	O
pandas	O
for	O
your	O
data	O
series	B-API
,	O
this	O
is	O
probably	O
not	O
an	O
issue	O
for	O
you	O
.	O

#CODE	O

Python	O
Pandas	O
Panel	B-API
counting	O
value	O
occurence	O

I	O
have	O
a	O
large	O
dataset	O
stored	O
as	O
a	O
pandas	O
panel	B-API
.	O

I	O
would	O
like	O
to	O
count	O
the	O
occurence	O
of	O
values	O
1.0	O
on	O
the	O
minor_axis	O
for	O
each	O
item	O
in	O
the	O
panel	B-API
.	O

What	O
I	O
have	O
so	O
far	O
:	O
#CODE	O

Having	O
never	O
dealt	O
with	O
panels	O
before	O
I	O
tried	O
this	O
:	O
`	O
P	O
[	O
P.minor_axis	O
==	O
'	O
a	O
']	O
.min()	B-API
`	O
does	O
it	O
do	O
what	O
you	O
want	O
?	O

To	O
count	O
all	O
the	O
"	O
b	O
"	O
values	O
1.0	O
,	O
I	O
would	O
first	O
isolate	O
b	O
in	O
its	O
own	O
DataFrame	B-API
by	O
swapping	O
the	O
minor	O
axis	O
and	O
the	O
items	O
.	O

#CODE	O

Thanks	O
for	O
thinking	O
with	O
me	O
guys	O
,	O
but	O
I	O
managed	O
to	O
figure	O
out	O
a	O
surprisingly	O
easy	O
solution	O
after	O
many	O
hours	O
of	O
attempting	O
.	O

I	O
thought	O
I	O
should	O
share	O
it	O
in	O
case	O
someone	O
else	O
is	O
looking	O
for	O
a	O
similar	O
solution	O
.	O

#CODE	O

Search	O
by	O
value	O
in	O
pandas	O
series	B-API

Give	O
a	O
pandas	O
series	B-API
#CODE	O

How	O
to	O
combine	O
pivot	B-API
with	O
cumulative	O
sum	O
in	O
Pandas	O

I	O
have	O
a	O
simple	O
dataframe	B-API
:	O
#CODE	O

Which	O
i	O
can	O
easily	O
pivot	O
with	O
the	O
dates	O
as	O
columns	O
using	O
the	O
following	O
function	O
:	O
#CODE	O

However	O
I	O
cant	O
find	O
a	O
way	O
to	O
use	O
a	O
cumulative	O
sum	O
in	O
place	O
of	O
the	O
np.sum	O
,	O
I	O
would	O
like	O
to	O
display	O
the	O
data	O
in	O
the	O
following	O
format	O
?	O

Is	O
this	O
possible	O
?	O

#CODE	O

How	O
to	O
convert	O
string	O
series	B-API
into	O
integer	O

Then	O
,	O
apply	O
`	O
get	B-API
`	O
to	O
the	O
column	O
,	O
returning	O
the	O
original	O
value	O
if	O
it	O
is	O
not	O
in	O
the	O
dictionary	O
:	O
#CODE	O

@USER	O
If	O
all	O
of	O
the	O
unique	O
items	O
contained	O
in	O
the	O
series	B-API
are	O
in	O
the	O
dictionary	O
keys	O
,	O
then	O
`	O
.map	B-API
(	O
d	O
)`	O
is	O
more	O
than	O
five	O
times	O
as	O
fast	O
.	O

However	O
,	O
any	O
missing	O
value	O
appears	O
as	O
a	O
`	O
NaN	O
`	O
.	O

Using	O
a	O
`	O
lambda	O
`	O
function	O
with	O
`	O
get	B-API
`	O
on	O
the	O
dictionary	O
appears	O
to	O
have	O
virtually	O
identical	O
performance	O
.	O

#CODE	O

Use	O
`	O
map	B-API
`	O
for	O
this	O
,	O
it's	O
cyython-ised	O
:	O
`	O
df	O
[	O
col	O
]	O
.map	B-API
(	O
d	O
)`	O

You	O
mean	O
your	O
df	O
somehow	O
has	O
no	O
name	O
?	O

It	O
is	O
probably	O
just	O
`''`	O
,	O
anyway	O
you	O
can	O
directly	O
assign	O
doing	O
`	O
df.columns	O
=	O
[	O
'	O
new_name	O
']`	O
,	O
the	O
normal	O
convention	O
is	O
to	O
call	O
`df.rename(columns={'':'new_name	O
'	O
}	O
,	O
inplace=True	O
)`	O

How	O
to	O
delete	O
some	O
of	O
the	O
mean	O
weekly	O
values	O
calculated	O
by	O
pandas	O
(	O
.resample	B-API
)	O
?	O

I	O
have	O
added	O
a	O
list	O
`	O
Wase	O
=	O
[	O
"	O
2013-12-22	O
"	O
,	O
"	O
2014-01-05	O
"]`	O
,	O
as	O
suggested	O
in	O
comments	O
and	O
used	O
`	O
Temp_plot1	O
=	O
Temp_plot.drop	O
(	O
Wase	O
)`	O
Now	O
I	O
got	O
any	O
error	O
,	O
which	O
says	O
`	O
ValueError	O
:	O
labels	O
[	O
'	O
2013-12-22	O
'	O
'	O
2014-01-05	O
']	O
not	O
contained	O
in	O
axis	O
`	O
.	O

If	O
you	O
have	O
a	O
container	O
(	O
e.g.	O
`	O
list	O
`)	O
of	O
dates	O
that	O
you	O
want	O
to	O
exclude	O
called	O
,	O
say	O
,	O
`	O
unwanted_dates	O
`	O
you	O
can	O
just	O
do	O
`	O
Temp_plot.drop	O
(	O
unwanted_dates	O
)`	O
.	O

Note	O
this	O
returns	O
a	O
view	O
with	O
the	O
desired	O
dates	O
excluded	O
and	O
doesn't	O
actually	O
alter	O
`	O
Temp_plot	O
`	O
.	O

To	O
drop	O
them	O
permanently	O
do	O
`	O
Temp_plot	O
=	O
Temp_plot.drop	O
(	O
unwanted_dates	O
)`	O
or	O
`	O
Temp_plot.drop	O
(	O
unwanted_dates	O
,	O
inplace=True	O
)`	O

Also	O
,	O
you	O
can	O
set	O
your	O
index	O
when	O
you	O
read	O
the	O
csv	O
file	O
:	O
`	O
pd.read_csv	B-API
(	O
"	O
Book1.csv	O
"	O
,	O
parse_dates=['date	O
']	O
,	O
index_col='date	O
')`	O

You	O
need	O
to	O
create	O
a	O
calendar	O
of	O
holidays	O
using	O
`	O
dt.date	B-API
(	O
year	O
,	O
month	O
,	O
day	O
)`	O
.	O

Then	O
you	O
filter	O
the	O
holidays	O
from	O
the	O
index	O
using	O
a	O
list	O
comprehension	O
structure	O
as	O
shown	O
below	O
.	O

Lastly	O
,	O
you	O
select	O
these	O
filtered	O
dates	O
using	O
`	O
.ix	B-API
`	O
which	O
selects	O
data	O
from	O
a	O
dataframe	B-API
based	O
on	O
the	O
index	O
value	O
.	O

#CODE	O

Thanks	O
for	O
your	O
time	O
but	O
it	O
is	O
not	O
working	O
.	O

I	O
have	O
added	O
`	O
holidays	O
=	O
[	O
dt.date	B-API
(	O
2013	O
,	O
12	O
,	O
17	O
)	O
,	O
(	O
2013	O
,	O
12	O
,	O
1	O
8)	O
,	O
(	O
2013	O
,	O
12	O
,	O
19	O
)	O
,	O
(	O
2013	O
,	O
12	O
,	O
20	O
)	O
,	O
(	O
2013	O
,	O
12	O
,	O
21	O
)	O
,	O
(	O
2013	O
,	O
12	O
,	O
22	O
)]`	O
`Temp=Temp.set_index('date	O
')`	O
and	O
same	O
lines	O
like	O
yours	O
`	O
idx=	O
...	O

`	O
and	O
`	O
Temp_plot=	O
...	O

`	O
but	O
it	O
gave	O
same	O
results	O
as	O
before	O
meaning	O
the	O
holidays	O
are	O
not	O
removed	O
.	O

I	O
have	O
also	O
tried	O
to	O
to	O
sampling	O
on	O
daily	O
basis	O
before	O
doing	O
it	O
on	O
weekly	O
basis	O
but	O
this	O
ain't	O
give	O
me	O
any	O
different	O
results	O
.	O

Your	O
help	O
will	O
be	O
appreciated	O
.	O

because	O
...?	O

What	O
is	O
the	O
error	O
.	O

My	O
guess	O
is	O
the	O
format	O
of	O
your	O
date	O
.	O

What	O
is	O
`	O
type	O
(	O
Temp.index.iat	O
[	O
0	O
])`	O
?	O

You	O
need	O
dt.date	B-API
(	O
...	O
)	O
for	O
each	O
date	O
in	O
your	O
holidays	O
.	O

Or	O
you	O
can	O
do	O
this	O
:	O
`	O
holidays	O
=	O
[	O
dt.date	B-API
(	O
d	O
[	O
0	O
]	O
,	O
d	O
[	O
1	O
]	O
,	O
d	O
[	O
2	O
])	O
for	O
d	O
in	O
[(	O
2013	O
,	O
12	O
,	O
1	O
8)	O
,	O
(	O
2013	O
,	O
12	O
,	O
19	O
)	O
,	O
(	O
2013	O
,	O
12	O
,	O
20	O
)	O
,	O
(	O
2013	O
,	O
12	O
,	O
21	O
)	O
,	O
(	O
2013	O
,	O
12	O
,	O
22	O
)]]`	O
.	O

Also	O
,	O
I	O
tried	O
the	O
code	O
using	O
your	O
sample	O
data	O
and	O
it	O
works	O
for	O
me	O
.	O

Creating	O
data	O
histograms	O
/	O
visualizations	O
using	O
ipython	O
and	O
filtering	O
out	O
some	O
values	O

but	O
get	O
an	O
error	O
due	O
to	O
unrecognized	O
dtypes	B-API
(	O
there	O
are	O
some	O
entries	O
that	O
aren't	O
in	O
datetime	O
format	O
)	O
.	O

How	O
can	O
I	O
exclude	O
these	O
in	O
the	O
original	O
command	O
?	O

Also	O
,	O
what	O
if	O
I	O
want	O
to	O
create	O
the	O
same	O
histogram	O
but	O
filtering	O
to	O
keep	O
only	O
records	O
that	O
have	O
id's	O
(	O
in	O
an	O
account	O
id	O
field	O
)	O
starting	O
with	O
the	O
integer	O
(	O
or	O
string	O
?	O
)	O
'	O
2	O
'	O
?	O

Ultimately	O
,	O
I	O
want	O
to	O
be	O
able	O
to	O
create	O
histograms	O
,	O
line	O
plots	O
,	O
box	O
plots	O
and	O
so	O
on	O
but	O
filtering	O
for	O
certain	O
months	O
,	O
user	O
id's	O
,	O
or	O
just	O
bad	O
'	O
dtypes	B-API
'	O
.	O

and	O
when	O
I	O
apply	O
the	O
command	O
pd.to_datetime()	B-API
to	O
these	O
columns	O
I	O
get	O
fields	O
resulting	O
that	O
look	O
like	O
:	O
#CODE	O

the	O
dtype	B-API
of	O
the	O
duration	O
column	O
fields	O
(	O
#URL	O
)	O
is	O
:	O
#CODE	O

Check	O
`	O
df.Duration.dtype	O
`	O
.	O

If	O
it	O
isn't	O
a	O
numeric	O
dtype	B-API
,	O
you	O
can	O
convert	O
using	O
`	O
df.Duration.astype	O
(	O
int	O
)`	O
or	O
`	O
(	O
float	O
)`	O
,	O
provided	O
that	O
you	O
only	O
have	O
strings	O
containing	O
numbers	O
,	O
and	O
no	O
NaNs	O
.	O

It	O
helps	O
if	O
you	O
post	O
a	O
sample	O
`	O
df.Duration.head	O
(	O
4	O
)`	O
.	O

oops-I	O
typed	O
df.Duration.dtype	O
and	O
get	O
dtype	B-API
(	O
'	O
<	O
m	O
8[	O
ns	O
]')	O
and	O
df.Duration.head	O
(	O
4	O
)	O
returns	O
0	O
00:14	O
:	O
00	O

Name	O
:	O
Duration	O
,	O
dtype	B-API
:	O
timedelta64	O
[	O
ns	O
]	O
so	O
obviously	O
it	O
is	O
not	O
an	O
integer	O
which	O
it	O
would	O
need	O
to	O
be-these	O
are	O
time	O
stamps	O
so	O
in	O
hours	O
minutes	O
seconds	O
,	O
will	O
df.Duration.astype	O
(	O
int	O
)	O
recognize	O
this	O
and	O
convert	O
accordingly	O
or	O
do	O
I	O
need	O
a	O
different	O
command	O
?	O

Thanks	O
!	O

I	O
read	O
somewhere	O
else	O
that	O
if	O
'	O
Duration	O
'	O
is	O
in	O
the	O
form	O
#URL	O
then	O
Duration.days	O
returns	O
days	O
,	O
Duration.minutes	O
the	O
minutes	O
,	O
Duration.seconds	O
the	O
seconds	O
I	O
could	O
then	O
just	O
define	O
a	O
function	O
which	O
inputs	O
the	O
duration	O
(	O
dur	O
)	O
and	O
outputs	O
Duration.days	O
*	O
3600	O
+	O
Duration.minutes	O
+	O
Duration.seconds	O
/	O
60	O
and	O
then	O
write	O
df.f	O
(	O
dur	O
)	O
.hist	B-API
(	O
bins=10	O
)	O
correct	O
?	O

I	O
do	O
have	O
a	O
duration-I	O
calculated	O
it	O
by	O
subtracting	O
two	O
dates	O
and	O
added	O
it	O
to	O
my	O
data	O
frame	O
with	O
:	O
df['Duration	O
']	O
=	O
pd.to_datetime(df['End	O
Time'])-pd.to_datetime(df['Start	O
Time	O
'])	O

I	O
think	O
you	O
need	O
to	O
convert	O
duration	O
(	O
timedelta64	O
)	O
to	O
int	O
(	O
assuming	O
you	O
have	O
a	O
duration	O
)	O
.	O

Then	O
the	O
.hist	B-API
method	O
will	O
work	O
.	O

#CODE	O

sorry	O
,	O
my	O
other	O
question	O
is	O
:	O
my	O
dtype	B-API
shows	O
as	O
:	O
dtype	B-API
(	O
'	O
<	O
m	O
8[	O
ns	O
]')	O
,	O
is	O
this	O
timedelta64	O
?	O

No	O
,	O
as	O
I	O
said	O
,	O
you	O
have	O
a	O
date	O
.	O

Look	O
at	O
my	O
output	O
above	O
,	O
a	O
duration	O
dtype	B-API
will	O
show	O
as	O
dtype	B-API
:	O
timedelta64	O
[	O
ns	O
]	O
(	O
if	O
the	O
base	O
is	O
nanosecond	O
)	O
or	O
something	O
similar	O
.	O

You	O
said	O
you	O
subtracted	O
your	O
dates	O
.	O

Did	O
you	O
actually	O
assign	O
that	O
result	O
to	O
a	O
column	O
?	O

Are	O
you	O
referencing	O
to	O
the	O
right	O
column	O
?	O

I	O
did	O
this	O
command	O
:	O
df['Duration	O
']	O
=	O
pd.to_datetime(df['End	O
Time'])-pd.to_datetime(df['Start	O
Time	O
'])	O
to	O
add	O
the	O
column	O
'	O
Duration	O
'	O
to	O
my	O
data	O
frame	O
'	O
df	O
'	O
where	O
the	O
'	O
End	O
Time	O
'	O
and	O
'	O
Start	O
Time	O
'	O
columns	O
are	O
of	O
the	O
format	O
:	O
2014	O
/	O
03	O
/	O
27	O
23:13	O
for	O
example-I	O
have	O
checked	O
and	O
the	O
result	O
of	O
this	O
was	O
to	O
give	O
fields	O
in	O
the	O
format	O
of	O
#URL	O
of	O
time	O
lapsed	O

I	O
am	O
trying	O
to	O
figure	O
out	O
how	O
to	O
get	O
a	O
slice	O
of	O
my	O
data	O
but	O
omitting	O
some	O
of	O
it	O
.	O

I	O
am	O
using	O
ipython	O
and	O
there	O
are	O
,	O
among	O
others	O
,	O
two	O
columns	O
in	O
my	O
data	O
frame	O
called	O
'	O
Start	O
Time	O
'	O
and	O
'	O
End	O
Time	O
'	O
.	O

All	O
I	O
did	O
was	O
use	O
the	O
command	O
I	O
wrote	O
above	O
to	O
subtract	O
these	O
two	O
columns	O
and	O
create	O
another	O
which	O
has	O
the	O
correct	O
time	O
lapse-I	O
will	O
edit	O
my	O
question	O
(	O
if	O
I	O
can	O
)	O
to	O
show	O
the	O
result	O
of	O
the	O
to_datetime()	B-API
transformation	O
.	O

To	O
convert	O
your	O
timedelta	O
to	O
minutes	O
,	O
you	O
just	O
have	O
to	O
divide	O
it	O
by	O
1	O
minute	O
,	O
hence	O
:	O
`df['minutes	O
']	O
=	O
df.Duration	O
/	O
timedelta	O
(	O
1	O
,	O
'	O
m	O
')`	O
.	O

Sorry	O
,	O
I	O
thought	O
you	O
had	O
read	O
the	O
last	O
line	O
of	O
my	O
answer	O
.	O

Then	O
you'll	O
be	O
able	O
to	O
do	O
`	O
df.minutes.hist()	O
`	O
without	O
problems	O
.	O

Thanks	O
!	O

I	O
ended	O
up	O
using	O
the	O
.str	B-API
somehow	O
but	O
this	O
is	O
good	O
to	O
know	O
!	O

no	O
,	O
.str	B-API
was	O
somethin	O
else	O
,	O
I	O
did	O
:	O
`df['Durationendminusstart	O
']	O
=	O
pd.to_timedelta	B-API
(	O
df.Duration	O
,	O
unit='ns').astype('timedelta64	O
[	O
m	O
]')`	O

Trouble	O
pivoting	O
/	O
reshaping	O
a	O
pandas	O
dataframe	B-API

I	O
have	O
the	O
following	O
dataframe	B-API
:	O
#CODE	O

I'd	O
like	O
to	O
pivot	O
the	O
data	O
so	O
that	O
I	O
end	O
up	O
with	O
:	O
#CODE	O

Fillna's	O
:	O
#CODE	O

does	O
your	O
data	O
get	O
correctly	O
parsed	O
with	O
read_csv	B-API
?	O

If	O
your	O
data's	O
correctly	O
parsed	O
by	O
`	O
read_csv	B-API
`	O
,	O
why	O
do	O
you	O
have	O
a	O
column	O
named	O
`'0	O
'`	O
?	O

I'd've	O
thought	O
that	O
it'd	O
be	O
`data[data['Value	O
']	O
==	O
0	O
]`	O
.	O

Truncating	O
column	O
width	O
in	O
pandas	O

I'm	O
reading	O
in	O
large	O
csv	O
files	O
into	O
pandas	O
some	O
of	O
them	O
with	O
String	O
columns	O
in	O
the	O
thousands	O
of	O
characters	O
.	O

Is	O
there	O
any	O
quick	O
way	O
to	O
limit	O
the	O
width	O
of	O
a	O
column	O
,	O
i.e.	O
only	O
keep	O
the	O
first	O
100	O
characters	O
?	O

Do	O
you	O
mean	O
after	O
the	O
fact	O
,	O
or	O
do	O
you	O
want	O
/	O
need	O
to	O
truncate	O
them	O
before	O
they're	O
even	O
stored	O
in	O
the	O
frame	O
?	O

Before	O
even	O
stored	O
would	O
be	O
great	O
.	O

Would	O
an	O
apply	B-API
be	O
the	O
fastest	O
way	O
for	O
after	O
the	O
fact	O
?	O

Writing	O
an	O
apply	B-API
right	O
now	O
.	O

Is	O
there	O
a	O
way	O
to	O
get	O
the	O
width	O
of	O
a	O
column	O
of	O
type	O
object	O
?	O

`df['string	O
col	O
']	O
.apply	B-API
(	O
lambda	O
x	O
:	O
x	O
[:	O
100	O
])`	O
will	O
keep	O
only	O
the	O
first	O
100	O
characters	O

Would	O
be	O
nice	O
to	O
check	O
if	O
it	O
needs	O
to	O
be	O
truncated	O
beforehand	O
.	O

Can't	O
seem	O
to	O
find	O
any	O
attribute	O
that	O
gives	O
the	O
width	O
.	O

If	O
you	O
can	O
read	O
the	O
whole	O
thing	O
into	O
memory	O
,	O
you	O
can	O
use	O
the	O
`	O
str	B-API
`	O
method	O
for	O
vector	O
operations	O
:	O
#CODE	O

Also	O
note	O
that	O
you	O
can	O
get	O
a	O
Series	B-API
with	O
lengths	O
using	O
#CODE	O

``	O
converters	O
``	O
are	O
called	O
row-by-row	O
(	O
by	O
a	O
cython	O
function	O
)	O
.	O

I	O
think	O
the	O
``	O
str	O
``	O
conversion	O
would	O
be	O
faster	O
(	O
your	O
first	O
example	O
)	O

Pandas	O
DataFrame	B-API
to	O
Excel	O
Problems	O

Does	O
it	O
have	O
to	O
output	O
as	O
an	O
excel	O
workbook	O
,	O
or	O
would	O
CSV	O
suffice	O
?	O

You	O
could	O
change	O
your	O
list	O
of	O
tuples	O
to	O
a	O
dataframe	B-API
then	O
output	O
as	O
a	O
csv	O
quite	O
easily	O
...	O

More	O
detailed	O
discussion	O
is	O
in	O
this	O
answer	O
too	O
.	O

Basically	O
,	O
openpyxl	O
version	O
2	O
deprecated	O
the	O
style-conversion	O
functions	O
that	O
map	O
dictionary	O
style	O
definitions	O
to	O
the	O
new	O
openpyxl	O
api	O
,	O
but	O
Pandas	O
still	O
uses	O
the	O
old	O
style	O
,	O
and	O
for	O
some	O
reason	O
the	O
deprecation	O
pass-through	O
function	O
errors	O
out	O
.	O

Using	O
this	O
map	O
of	O
NYC	O
I'd	O
like	O
to	O
change	O
Manhattan	O
to	O
be	O
bright	O
blue	O
.	O

But	O
when	O
I	O
change	O
the	O
individual	O
patch	O
color	O
of	O
Manhattan	O
all	O
the	O
other	O
patch	O
colors	O
change	O
too	O
.	O

This	O
was	O
unexpected	O
to	O
me	O
.	O

A	O
possible	O
solution	O
is	O
using	O
`	O
geopandas.plotting.plot_multipolygon	O
`	O
to	O
specifically	O
add	O
only	O
one	O
geometry	O
object	O
with	O
blue	O
colors	O
to	O
the	O
existing	O
figure	O
:	O
#CODE	O

I'm	O
trying	O
to	O
monkeypatch	O
how	O
`	O
pandas	O
`	O
Panel's	O
slicing	O
(	O
`	O
__getitem__	O
`)	O
.	O

This	O
is	O
straightforward	O
to	O
do	O
with	O
a	O
basic	O
function	O
,	O
foo	O
.	O

#CODE	O

Where	O
`	O
ORIGINAL_getitem	O
`	O
is	O
storing	O
the	O
original	O
Panel	B-API
method	O
.	O

I'm	O
trying	O
to	O
extend	O
to	O
the	O
case	O
where	O
`	O
foo()	O
`	O
is	O
not	O
a	O
function	O
,	O
but	O
an	O
instance	O
method	O
of	O
an	O
object	O
,	O
`	O
Foo	O
`	O
.	O

For	O
example	O
:	O
#CODE	O

`	O
Foo.foo()	O
`	O
must	O
access	O
the	O
attribute	O
`	O
self.name	O
`	O
.	O

Therefore	O
,	O
the	O
monkeypatched	O
function	O
would	O
need	O
a	O
reference	O
to	O
the	O
Foo	O
instance	O
somehow	O
,	O
in	O
addition	O
to	O
the	O
Panel	B-API
.	O

How	O
can	O
I	O
monkepatch	O
panel	B-API
with	O
`	O
Foo.foo()	O
`	O
and	O
make	O
self.name	O
accessible	O
?	O

But	O
this	O
doesn't	O
work	O
.	O

A	O
reference	O
to	O
self	O
is	O
passed	O
,	O
but	O
no	O
access	O
from	O
the	O
calling	O
Panel	B-API
possible	O
.	O

That	O
is	O
:	O
#CODE	O

Passes	O
a	O
reference	O
to	O
`	O
Foo	O
`	O
,	O
not	O
to	O
`	O
Panel	B-API
`	O
.	O

In	O
short	O
,	O
it	O
provides	O
a	O
cool	O
set	O
of	O
features	O
,	O
it	O
has	O
a	O
wide	O
range	O
of	O
unit	O
tests	O
,	O
and	O
it	O
comes	O
with	O
a	O
fancy	O
doc	O
that	O
should	O
cover	O
everything	O
you	O
need	O
to	O
get	O
started	O
.	O

Make	O
sure	O
to	O
also	O
check	O
the	O
FAQ	O
!	O

One	O
way	O
to	O
do	O
this	O
is	O
to	O
create	O
a	O
closure	O
(	O
a	O
function	O
with	O
reference	O
to	O
names	O
other	O
than	O
locals	O
or	O
globals	O
)	O
.	O

A	O
simple	O
closure	O
:	O
#CODE	O

append	O
pandas.DataFrame.GroupBy	B-API
results	O
into	O
another	O
dataframe	B-API

What	O
I'm	O
trying	O
to	O
do	O
is	O
make	O
a	O
dataframe	B-API
that	O
only	O
keeps	O
the	O
sum	O
of	O
the	O
counts	O
(	O
the	O
column	O
after	O
year	O
)	O
regardless	O
of	O
the	O
year	O
.	O

I	O
used	O
groupby	B-API
to	O
aggregate	O
my	O
dataframe	B-API
,	O
and	O
tried	O
to	O
save	O
the	O
results	O
in	O
another	O
dataframe	B-API
(	O
df_ngram	O
)	O
.	O

But	O
it	O
seems	O
like	O
it	O
isn't	O
appending	O
at	O
all	O
.	O

Below	O
is	O
what	O
I	O
get	O
when	O
I	O
run	O
this	O
.	O

I'm	O
not	O
sure	O
how	O
to	O
deal	O
with	O
the	O
groupby	B-API
results	O
.	O

How	O
can	O
I	O
aggregate	O
the	O
results	O
from	O
groupby	B-API
?	O

Or	O
can	O
I	O
get	O
what	O
I	O
want	O
without	O
using	O
groupby	B-API
?	O

#CODE	O

You	O
need	O
to	O
append	O
the	O
intermediate	O
DataFrames	O
to	O
a	O
list	O
and	O
then	O
concatenate	O
the	O
results	O
.	O

Instead	O
of	O
`	O
df_ngram.append	O
(	O
agg_chunk	O
)`	O
,	O
you	O
want	O
`	O
df_agg.append	O
(	O
agg_chunk	O
)`	O

I	O
did	O
`	O
df_ngram	O
=	O
pd.concat	B-API
(	O
df_agg	O
,	O
ignore_index=True	O
)`	O
but	O
then	O
`	O
print	O
df_ngram.head	O
(	O
10	O
)`	O
gives	O
me	O
`	O
0	O
279	O

Name	O
:	O
count	O
,	O
dtype	B-API
:	O
int64	O
`	O
which	O
isn't	O
what	O
I	O
wanted	O
.	O

I	O
want	O
the	O
dataframe	B-API
indexes	O
to	O
be	O
my	O
ngrams	O
not	O
index	O
numbers	O
.	O

Setting	O
`	O
ignore_index=False	O
`	O
also	O
doesn't	O
work	O
.	O

And	O
I	O
want	O
to	O
set	O
the	O
column	O
names	O
to	O
[	O
'	O
ngram	O
'	O
,	O
'	O
count	O
']	O
,	O
but	O
this	O
can	O
be	O
done	O
in	O
separate	O
line	O
.	O

I	O
am	O
completely	O
new	O
to	O
python	O
and	O
Pandas	O
of	O
course	O
.	O

I	O
am	O
trying	O
to	O
run	O
a	O
function	O
"	O
get	O
url	O
"	O
which	O
is	O
function	O
to	O
get	O
the	O
complete	O
extended	O
url	O
from	O
small	O
Url	O
.	O

I	O
have	O
a	O
data	O
frame	O
in	O
python	O
consists	O
all	O
the	O
short	O
URLs	O
.	O

Now	O
I	O
am	O
trying	O
to	O
do	O
with	O
following	O
ways	O
.	O

One	O
is	O
to	O
use	O
"	O
for	O
"	O
loop	O
which	O
loops	O
and	O
apply	O
function	O
on	O
all	O
the	O
elements	O
and	O
will	O
create	O
a	O
another	O
series	B-API
of	O
extended	O
URL	O
but	O
I	O
am	O
not	O
able	O
to	O
,	O
dont	O
know	O
why	O
,	O
I	O
tried	O
to	O
write	O
it	O
like	O
#CODE	O

again	O
second	O
solution	O
i	O
tried	O
was	O
passing	O
a	O
whole	O
array	O
to	O
applymap	B-API
fucntion	O
#CODE	O

can	O
you	O
dput	O
what	O
your	O
function	O
is	O
doing	O
?	O

Adding	O
the	O
same	O
suffix	O
to	O
each	O
url	O
or	O
particular	O
suffix	O
proper	O
to	O
each	O
base	O
url	O
?	O

expanded	O
(	O
i	O
)	O
=	O
get_real	O
(	O
df2	O
[[	O
i	O
]])`	O
doesn't	O
work	O
because	O
the	O
iterable	O
returned	O
is	O
the	O
column	O
not	O
each	O
element	O
,	O
so	O
unless	O
your	O
code	O
understands	O
how	O
to	O
operate	O
on	O
a	O
pandas	O
Series	B-API
then	O
your	O
code	O
won't	O
work	O

Hi	O
..	O
this	O
is	O
my	O
defined	O
function	O
..	O
def	O
myfunction	O
(	O
url	O
):	O
exturl=	O
urllib2.urlopen	O
(	O
HeadRequest	O
(	O
url	O
))	O
.geturl()	O

return	O
exturl	O
and	O
I	O
am	O
applying	O
it	O
on	O
my	O
list	O
df3['new']=df3['first_link	O
']	O
.apply	B-API
(	O
myfunction	O
)	O
...	O
but	O
because	O
or	O
errors	O
its	O
not	O
working	O
..	O
that	O
why	O
looking	O
for	O
for	O
loop	O

If	O
the	O
short	O
urls	O
are	O
a	O
column	O
in	O
the	O
pandas	O
dataFrame	B-API
,	O
you	O
can	O
use	O
the	O
`	O
apply	B-API
`	O
function	O
(	O
though	O
I	O
am	O
not	O
sure	O
if	O
they	O
would	O
resume	O
on	O
error	O
,	O
most	O
probably	O
not	O
)	O
.	O

Hi	O
..	O
this	O
is	O
my	O
defined	O
function	O
..	O
def	O
myfunction	O
(	O
url	O
):	O
exturl=	O
urllib2.urlopen	O
(	O
HeadRequest	O
(	O
url	O
))	O
.geturl()	O

return	O
exturl	O
and	O
I	O
am	O
applying	O
it	O
on	O
my	O
list	O
df3['new']=df3['first_link	O
']	O
.apply	B-API
(	O
myfunction	O
)	O
...	O
but	O
because	O
or	O
errors	O
its	O
not	O
working	O
..	O
that	O
why	O
looking	O
for	O
for	O
loop	O

Hi	O
Anand	O
-	O
Thanks	O
for	O
help	O
.	O
but	O
it	O
is	O
working	O
same	O
as	O
apply	B-API
function	O
.	O
as	O
soon	O
as	O
it	O
faces	O
a	O
error	O
it	O
pass	O
completely	O
.	O
error	O
is	O
like	O
HTTPError	O
:	O
HTTP	O
Error	O
404	O
:	O
Not	O
Found	O
....	O

ANd	O
i	O
am	O
using	O
the	O
formula	O
for	O
idx	O
in	O
df5.index	O
:	O

I	O
think	O
the	O
problem	O
you	O
are	O
having	O
is	O
treating	O
the	O
dataframe	B-API
just	O
like	O
a	O
dictionary	O
that	O
has	O
keys	O
and	O
values	O
.	O

I'm	O
trying	O
to	O
select	O
,	O
by	O
group	O
,	O
the	O
minimum	O
date	O
when	O
a	O
condition	O
is	O
met	O
and	O
assign	O
it	O
to	O
a	O
new	O
column	O
:	O
#CODE	O

I've	O
been	O
trying	O
to	O
use	O
groupby	B-API
in	O
combination	O
with	O
transform	O
,	O
but	O
don't	O
know	O
how	O
to	O
get	O
transform	O
to	O
take	O
account	O
of	O
conditions	O
based	O
on	O
other	O
columns	O
.	O

First	O
change	O
the	O
`	O
bool	B-API
`	O
column	O
to	O
actually	O
booleans	O
(	O
also	O
be	O
careful	O
with	O
your	O
names	O
.	O
DataFrame	B-API
has	O
a	O
`	O
bool	B-API
`	O
method	O
):	O
#CODE	O

Finding	O
the	O
minimum	O
dates	O
is	O
pretty	O
easy	O
.	O

Use	O
the	O
`	O
bool	B-API
`	O
column	O
to	O
index	O
into	O
`	O
df	O
`	O
:	O
#CODE	O

There	O
are	O
probably	O
a	O
bunch	O
of	O
ways	O
to	O
set	O
the	O
values	O
,	O
but	O
one	O
is	O
to	O
set	O
`	O
Group	O
`	O
as	O
the	O
index	O
and	O
`	O
join	B-API
`	O
the	O
`	O
dates	O
`	O
.	O

#CODE	O

Thanks	O
.	O

Worked	O
except	O
for	O
joining	O
bit	O
.	O

I	O
had	O
to	O
convert	O
the	O
series	B-API
to	O
a	O
dataframe	B-API
and	O
then	O
perform	O
a	O
left	O
merge	B-API
:	O
`	O
pd.merge	B-API
(	O
df	O
,	O
dates	O
,	O
how='left	O
'	O
,	O
on='Group	O
')`	O

I	O
have	O
a	O
pandas	O
data	O
frame	O
which	O
has	O
some	O
rows	O
and	O
columns	O
.	O

Each	O
column	O
has	O
a	O
header	O
.	O

Now	O
as	O
long	O
as	O
I	O
keep	O
doing	O
data	O
manipulation	O
operations	O
in	O
pandas	O
,	O
my	O
variable	O
headers	O
are	O
retained	O
.	O

But	O
if	O
I	O
try	O
some	O
data	O
pre-processing	O
feature	O
of	O
Sci-kit-learn	O
lib	O
,	O
I	O
end	O
up	O
losing	O
all	O
my	O
headers	O
and	O
the	O
frame	O
gets	O
converted	O
to	O
just	O
a	O
matrix	O
of	O
numbers	O
.	O

I	O
understand	O
why	O
it	O
happens	O
because	O
scikit-learn	O
gives	O
a	O
numpy	O
ndarray	O
as	O
output	O
.	O

And	O
numpy	O
ndarray	O
being	O
just	O
matrix	O
would	O
not	O
have	O
column	O
names	O
.	O

@USER	O
:	O
please	O
provide	O
a	O
very	O
simple	O
,	O
reproducible	O
example	O
!	O

A	O
three	O
row	O
dataframe	B-API
would	O
make	O
your	O
question	O
more	O
understandable	O
.	O

(	O
Maybe	O
just	O
copying	O
`	O
saved_cols	O
=	O
df.columns	O
`	O
and	O
then	O
reassigning	O
it	O
to	O
the	O
modified	O
`	O
df	O
`	O
would	O
do	O
the	O
trick	O
,	O
but	O
I'm	O
not	O
sure	O
that's	O
what	O
you	O
need	O
)	O

Indeed	O
,	O
as	O
@USER	O
says	O
,	O
copying	O
`	O
saved_cols	O
=	O
df.columns	O
`	O
and	O
then	O
when	O
you	O
got	O
the	O
series	B-API
,	O
doing	O
`	O
pandas.DataFrame	B-API
(	O
series	B-API
,	O
saved_cols	O
)`	O
you	O
get	O
your	O
dataframe	B-API
back	O
.	O

I	O
do	O
it	O
for	O
example	O
when	O
using	O
`	O
train_test_split	O
`	O
,	O
which	O
gives	O
back	O
a	O
`	O
numpy	O
ndarray	O
`	O
,	O
but	O
I	O
need	O
to	O
use	O
it	O
as	O
a	O
dataframe	B-API
.	O

It	O
is	O
not	O
something	O
to	O
be	O
particularly	O
proud	O
of	O
,	O
but	O
in	O
my	O
opinion	O
is	O
good	O
enough	O
.	O

scikit-learn	O
indeed	O
strips	O
the	O
column	O
headers	O
in	O
most	O
cases	O
,	O
so	O
just	O
add	O
them	O
back	O
on	O
afterward	O
.	O

In	O
your	O
example	O
,	O
with	O
`	O
X_imputed	O
`	O
as	O
the	O
`	O
sklearn.preprocessing	O
`	O
output	O
and	O
`	O
X_train	O
`	O
as	O
the	O
original	O
dataframe	B-API
,	O
you	O
can	O
put	O
the	O
column	O
headers	O
back	O
on	O
with	O
:	O
#CODE	O

For	O
example	O
,	O
I	O
would	O
like	O
to	O
create	O
a	O
new	O
column	O
titled	O
'	O
Rheum	O
'	O
,	O
which	O
takes	O
on	O
a	O
value	O
of	O
'	O
1	O
'	O
if	O
the	O
expression	O
'	O
391.1	O
'	O
appears	O
in	O
a	O
corresponding	O
column	O
'	O
ICD	O
'	O
per	O
row	O
.	O

In	O
some	O
rows	O
of	O
the	O
ICD	O
column	O
there	O
are	O
cells	O
which	O
have	O
a	O
variety	O
of	O
expressions	O
in	O
the	O
form	O
'	O
424.1	O
,	O
391.1	O
,	O
420.2	O
,	O
etc	O
'	O
.	O

Could	O
you	O
post	O
part	O
of	O
your	O
dataframe	B-API
?	O

Please	O
edit	O
your	O
question	O
with	O
your	O
data	O
as	O
dataframe	B-API

You	O
can	O
use	O
`	O
str.contains	B-API
`	O
:	O
#CODE	O

@USER	O
You	O
could	O
do	O
that	O
with	O
method	O
of	O
dataframe	B-API
and	O
series	B-API
`	O
astype	B-API
(	O
str	O
)`	O

Maybe	O
using	O
`df['ICD	O
']	O
.apply	B-API
(	O
str	O
)`	O
...	O
please	O
try	O
to	O
do	O
some	O
search	O
before	O

`	O
str.contains	B-API
`	O
return	O
bool	O
values	O
.	O

You	O
could	O
convert	O
it	O
to	O
integer	O
with	O
simple	O
add	O
0	O
:	O
#CODE	O

Yes	O
,	O
you	O
could	O
use	O
`	O
df	O
=	O
pd.read_csv('your_file	O
'	O
,	O
names=['patient	O
'	O
,	O
'	O
ICD9	O
'])`	O
and	O
then	O
operate	O
with	O
that	O
df	O

So	O
my	O
idea	O
was	O
,	O
I	O
will	O
create	O
an	O
excel-template	O
with	O
these	O
tricky	O
parts	O
and	O
then	O
from	O
python	O
just	O
insert	O
dynamic	O
data	O
to	O
this	O
template	O
.	O

@USER	O
:	O
That's	O
not	O
really	O
relevant	O
here	O
,	O
and	O
not	O
a	O
productive	O
comment	O
.	O

There	O
isn't	O
any	O
code	O
he	O
could	O
have	O
tried	O
,	O
given	O
the	O
tools	O
that	O
he's	O
mentioned	O
in	O
the	O
question	O
and	O
the	O
tags	O
.	O

If	O
you	O
don't	O
think	O
this	O
question	O
is	O
a	O
good	O
fit	O
for	O
Stack	O
Overflow	O
,	O
then	O
comment	O
on	O
why	O
you	O
believe	O
that	O
,	O
and	O
/	O
or	O
downvote	O
the	O
question	O
.	O

There	O
isn't	O
an	O
easy	O
way	O
to	O
do	O
this	O
with	O
any	O
of	O
the	O
usual	O
"	O
direct	O
file	O
manipulation	O
"	O
libraries	O
in	O
Python	O
(	O
xlrd	O
,	O
xlwt	O
,	O
XlsxWriter	O
,	O
OpenPyXL	O
;	O
these	O
are	O
what	O
pandas	O
uses	O
)	O
.	O

The	O
reason	O
is	O
that	O
the	O
structure	O
of	O
a	O
workbook	O
file	O
is	O
such	O
that	O
it's	O
impossible	O
or	O
prohibitively	O
difficult	O
(	O
depending	O
on	O
whether	O
you're	O
talking	O
about	O
.xls	O
or	O
.xlsx	O
)	O
to	O
do	O
anything	O
resembling	O
"	O
in-place	O
"	O
editing	O
,	O
short	O
of	O
re-implementing	O
Excel	O
itself	O
.	O

use	O
a	O
list	O
of	O
values	O
to	O
select	O
rows	O
from	O
a	O
pandas	O
dataframe	B-API

how	O
to	O
filter	O
the	O
dataframe	B-API
rows	O
of	O
pandas	O
by	O
within	O
/	O
in	O
?	O

Lets	O
say	O
I	O
have	O
the	O
following	O
pandas	O
dataframe	B-API
:	O
#CODE	O

This	O
is	O
indeed	O
a	O
duplicate	O
of	O
how	O
to	O
filter	O
the	O
dataframe	B-API
rows	O
of	O
pandas	O
by	O
"	O
within	O
"	O
/	O
"	O
in	O
"	O
?	O

,	O
translating	O
the	O
response	O
to	O
your	O
example	O
gives	O
:	O
#CODE	O

So	O
to	O
be	O
clear	O
,	O
you	O
want	O
to	O
do	O
basically	O
an	O
inner	O
join	O
on	O
the	O
`	O
DATE	B-API
`	O
key	O
in	O
df2	O
such	O
that	O
it	O
is	O
within	O
the	O
Start	O
/	O
End	O
Date	O
range	O
?	O

@USER	O
,	O
Correct	O
,	O
and	O
also	O
summing	O
the	O
values	O
of	O
the	O
dates	O
within	O
the	O
range	O
.	O

@USER	O
,	O
I	O
made	O
the	O
correction	O
,	O
I	O
removed	O
the	O
excel	O
tag	O
.	O

Thanks	O

hmm	O
..	O
works	O
for	O
me	O
.	O

They	O
key	O
step	O
is	O
the	O
second	O
last	O
.	O

First	O
check	O
that	O
the	O
index	O
on	O
both	O
frames	O
is	O
a	O
datetime	O
index	O
.	O

Then	O
check	O
the	O
output	O
of	O
`	O
map	B-API
(	O
df1.index.asof	O
,	O
df2.index	O
)`	O
.	O

This	O
is	O
the	O
array	O
indicating	O
the	O
groups	O
(	O
`	O
df1.index.asof	O
`	O
is	O
a	O
function	O
which	O
is	O
applied	O
to	O
the	O
index	O
of	O
`	O
df2	O
`)	O
.	O

For	O
each	O
date	O
in	O
`	O
df2.index	O
`	O
the	O
output	O
should	O
be	O
the	O
latest	O
date	O
from	O
`	O
df1.index	O
`	O
before	O
that	O
date	O
.	O

for	O
some	O
reason	O
`df2['DATE	O
']	O
=	O
pd.to_datetime	B-API
(	O
df2.DATE	O
)`	O
makes	O
the	O
df2	O
index	O
of	O
DATE	O
result	O
to	O
`	O
1970-01-01	O
00:00	O
:	O
00.0	O
20110706	O
`	O

found	O
the	O
issue	O
!	O

replacing	O
`df2['DATE	O
']	O
=	O
pd.to_datetime	B-API
(	O
df2.DATE	O
)`	O
with	O

`df2['DATE	O
']	O
=	O
pd.to_datetime	B-API
(	O
df	O
[	O
"	O
DATE	O
"]	O
,	O
format=	O
"	O
%Y%m%d	O
")`	O
resolves	O
it	O
!	O

Thanks	O
for	O
your	O
help	O
!	O

ahh	O
,	O
this	O
is	O
because	O
the	O
values	O
in	O
`'DATE	O
'`	O
are	O
`	O
ints	O
`	O
rather	O
than	O
strings	O
.	O

When	O
you	O
pass	O
an	O
`	O
ints	O
`	O
to	O
`	O
pd.to_datetime	B-API
`	O
it	O
treats	O
them	O
as	O
nanoseconds	O
from	O
epoch	O
(	O
unless	O
you	O
specify	O
the	O
unit	O
,	O
e.g.	O
`unit='ms	O
'`	O
.	O

If	O
you	O
were	O
to	O
pass	O
a	O
string	O
such	O
as	O
`'20110706	O
'`	O
you	O
wouldn't	O
need	O
to	O
specify	O
the	O
format	O
.	O

Python	O
--	O
Pandas	O
:	O
How	O
to	O
apply	O
aggfunc	B-API
to	O
data	O
in	O
currency	O
format	O
?	O

I	O
have	O
a	O
table	O
above	O
.	O

Want	O
to	O
apply	O
groupby	B-API
function	O
to	O
the	O
data	O
and	O
apply	O
sum	B-API
(	O
over	O
revenue_total	O
)	O
.	O

Pandas	O
gives	O
an	O
NA	O
value	O
since	O
revenue_total	O
is	O
an	O
object	O
data	O
type	O
.	O

Any	O
help	O
#CODE	O

df	O
=	O
pd.read_csv	B-API
(	O
path	O
)	O

df[['Product	O
ID	O
','Revenue	O
Total	O
']]	O
.head()	B-API

df.groupby(['Product	O
ID	O
'])	O
[[	O
'	O
Revenue	O
Total	O
']]	O
.sum()	B-API

Code	O
should	O
be	O
added	O
as	O
an	O
edit	O
to	O
the	O
question	O
(	O
and	O
put	O
in	O
a	O
code	O
block	O
)	O
.	O

I	O
don't	O
know	O
enough	O
Python	O
to	O
do	O
the	O
edit	O
for	O
you	O
,	O
but	O
as	O
is	O
it	O
is	O
not	O
very	O
helpful	O
to	O
potential	O
answerers	O
.	O

I	O
want	O
the	O
header	O
=0	O
.	O

The	O
issue	O
is	O
dtype	B-API
for	O
Revenue	O
Total	O
is	O
object	O
and	O
group	O
by	O
is	O
not	O
able	O
to	O
perform	O
an	O
agfunc	B-API
on	O
that	O
.	O

Then	O
you	O
could	O
remove	O
the	O
`	O
$	O
`	O
and	O
commas	O
and	O
parse	O
it	O
into	O
a	O
DataFrame	B-API
using	O
#CODE	O

to	O
your	O
call	O
to	O
`	O
pd.read_csv	B-API
`	O
.	O

That	O
is	O
what	O
is	O
stripping	O
the	O
`	O
$	O
`	O
and	O
commas	O
.	O

I	O
have	O
a	O
DataFrame	B-API
that	O
looks	O
like	O
this	O
:	O
#CODE	O

Assuming	O
the	O
format	O
is	O
consistent	O
throughout	O
the	O
column	O
,	O
you	O
could	O
use	O
`	O
str.split	B-API
`	O
to	O
extract	O
the	O
numerators	O
and	O
denominators	O
,	O
and	O
then	O
do	O
the	O
division	O
:	O
#CODE	O

I	O
might	O
do	O
this	O
with	O
apply	B-API
rather	O
than	O
eval	B-API
(	O
especially	O
if	O
I	O
didn't	O
trust	O
the	O
source	O
):	O
#CODE	O

Note	O
:	O
You	O
could	O
catch	O
the	O
error	O
if	O
the	O
entry	O
is	O
not	O
of	O
the	O
correct	O
form	O
.	O

pandas	O
0.13	O
read_excel	B-API
new	O
format	O

I'm	O
working	O
with	O
pandas	O
a	O
few	O
time	O
ago	O
.	O

In	O
0.12	O
version	O
,	O
I	O
read	O
excel	O
files	O
using	O
pandas.read_excel	B-API
(	O
filename	O
,	O
sheetname	O
,	O
index_col	O
)	O
,	O
the	O
read	O
file	O
was	O
in	O
the	O
next	O
format	O
,	O
with	O
the	O
header	O
in	O
the	O
first	O
row	O
:	O
#CODE	O

Now	O
,	O
when	O
I	O
save	O
a	O
dataframe	B-API
to	O
an	O
excel	O
file	O
,	O
the	O
header	O
format	O
changes	O
,	O
in	O
the	O
first	O
row	O
are	O
the	O
columns	O
names	O
,	O
and	O
in	O
the	O
second	O
row	O
the	O
index	O
name	O
,	O
as	O
show	O
in	O
the	O
next	O
table	O
:	O
#CODE	O

The	O
DataFrame	B-API
will	O
be	O
written	O
in	O
a	O
way	O
that	O
tries	O
to	O
mimic	O
the	O
REPL	O
output	O
.	O

One	O
difference	O
from	O
0.12.0	O
version	O
is	O
that	O
the	O
index_label	O
will	O
be	O
placed	O
in	O
the	O
second	O
row	O
instead	O
of	O
the	O
?	O

rst	O
.	O

You	O
can	O
get	O
the	O
previous	O
behaviour	O
by	O
setting	O
the	O
merge_cells	O
option	O
in	O
to_excel()	B-API
to	O
False	O
:	O
#CODE	O

How	O
to	O
save	O
a	O
dataframe	B-API
as	O
a	O
csv	O
file	O
with	O
'	O
/	O
'	O
in	O
the	O
file	O
name	O

I	O
want	O
to	O
save	O
a	O
`	O
dataframe	B-API
`	O
to	O
a	O
`	O
.csv	O
`	O
file	O
with	O
the	O
name	O
`'123	O
/	O
123	O
'`	O
,	O
but	O
it	O
will	O
split	O
it	O
in	O
to	O
two	O
strings	O
if	O
I	O
just	O
type	O
like	O
`df.to_csv('123	O
/	O
123.csv	O
')`	O
.	O

and	O
that	O
a	O
charmap	O
can	O
find	O
a	O
character	O
that	O
looks	O
like	O
it	O
,	O
which	O
is	O
legal	O
.	O

In	O
this	O
case	O
a	O
division	O
character	O

Python	O
2.7	O
with	O
Pandas	O
:	O
How	O
does	O
one	O
recover	O
the	O
non	O
intersecting	O
parts	O
of	O
two	O
dataframes	O
?	O

I	O
have	O
two	O
data	O
frames	O
and	O
the	O
second	O
is	O
a	O
subset	O
of	O
the	O
first	O
.	O

How	O
do	O
I	O
now	O
find	O
the	O
portion	O
of	O
the	O
first	O
dataframe	B-API
that	O
is	O
not	O
contained	O
in	O
the	O
second	O
one	O
?	O

For	O
example	O
:	O
#CODE	O

Well	O
,	O
one	O
way	O
to	O
do	O
this	O
is	O
using	O
`	O
isin	B-API
`	O
(	O
but	O
you	O
can	O
also	O
do	O
it	O
with	O
the	O
`	O
merge	B-API
`	O
command	O
...	O
I	O
show	O
examples	O
for	O
both	O
)	O
.	O

For	O
example	O
:	O
#CODE	O

Explanation	O
.	O

`	O
isin	B-API
`	O
can	O
check	O
using	O
multiple	O
columns	O
if	O
you	O
feed	O
it	O
a	O
dict	O
:	O
#CODE	O

And	O
then	O
`	O
isin	B-API
`	O
will	O
create	O
a	O
booleen	O
df	O
which	O
I	O
can	O
use	O
to	O
select	O
the	O
columns	O
we	O
want	O
(	O
in	O
this	O
case	O
require	O
all	O
the	O
columns	O
to	O
match	O
and	O
then	O
negate	O
with	O
`	O
~	O
`)	O
:	O
#CODE	O

In	O
the	O
specific	O
example	O
we	O
don't	O
need	O
to	O
feed	O
`	O
isin	B-API
`	O
a	O
dict	O
version	O
of	O
the	O
dataframe	B-API
because	O
we	O
can	O
identify	O
the	O
valid	O
rows	O
by	O
only	O
looking	O
at	O
column	O
A	O
:	O
#CODE	O

You	O
can	O
also	O
do	O
this	O
with	O
`	O
merge	B-API
`	O
.	O

Create	O
a	O
unique	O
column	O
in	O
the	O
subset	O
dataframe	B-API
.	O

When	O
you	O
merge	O
,	O
the	O
unique	O
rows	O
from	O
the	O
larger	O
dataframe	B-API
will	O
have	O
`	O
NaN	O
`	O
for	O
the	O
column	O
you	O
created	O
:	O
#CODE	O

Edit	O
:	O
@USER	O
notes	O
that	O
the	O
merge	B-API
method	O
performs	O
much	O
better	O
for	O
large	O
dataframes	O
.	O

Great	O
,	O
thanks	O
for	O
the	O
very	O
complete	O
answer	O
showing	O
several	O
options	O
.	O

I	O
appreciate	O
your	O
help	O
!	O

I	O
just	O
wanted	O
to	O
add	O
some	O
further	O
documentation	O
for	O
other	O
people	O
reading	O
this	O
...	O

The	O
latter	O
method	O
mentioned	O
above	O
is	O
MUCH	O
faster	O
than	O
the	O
former	O
for	O
big	O
dataframes	O
.	O

I	O
tried	O
both	O
and	O
didn't	O
get	O
a	O
solution	O
after	O
10	O
minutes	O
of	O
sorting	O
through	O
dataframe	B-API
with	O
80,000	O
rows	O
and	O
5	O
columns	O
,	O
but	O
it	O
only	O
takes	O
a	O
few	O
seconds	O
for	O
the	O
latter	O
merge	B-API
method	O
.	O

Thanks	O
again	O
!	O

Thanks	O
@USER	O
,	O
I	O
edited	O
the	O
answer	O
to	O
indicate	O
the	O
better	O
performance	O
of	O
the	O
`	O
merge	B-API
`	O
method	O
.	O

Groupby	B-API
values	O
start	O
with	O

is	O
there	O
a	O
way	O
to	O
compute	O
all	O
value	O
with	O
the	O
same	O
prefix	O
#CODE	O

No	O
,	O
you'd	O
have	O
to	O
build	O
a	O
list	O
where	O
the	O
prefix	O
is	O
matched	O
in	O
the	O
columns	O
but	O
there	O
is	O
no	O
built	O
in	O
method	O

Please	O
add	O
reproducible	O
code	O
and	O
well-formatted	O
tables	O
.	O

Just	O
group	O
by	O
`	O
Date	B-API
`	O
and	O
`	O
Income	O
`	O
and	O
get	O
the	O
sum	O
:	O
#CODE	O

in	O
order	O
to	O
filter	O
the	O
dataframe	B-API
before	O
getting	O
the	O
sum	O
,	O
just	O
set	O
the	O
filter	O
before	O
grouping	O
:	O
#CODE	O

Thanks	O
.	O

This	O
i	O
already	O
did	O
,	O
i	O
also	O
did	O
for	O
many	O
filters	O
(	O
combination	O
of	O
2	O
or	O
3	O
)	O
.	O

How	O
can	O
i	O
implement	O
below	O
conditions	O
:	O
1	O
)	O
If	O
only	O
one	O
filter	O
is	O
selected	O
e.g.	O
Male	O
then	O
all	O
other	O
filters	O
should	O
take	O
all	O
the	O
values	O
corresponding	O
to	O
Only	O
Male	O
filter	O
2	O
)	O
If	O
only	O
two	O
filters	O
are	O
selected	O
e.g.	O
Only	O
Male	O
and	O
IT	O
profession	O
are	O
selected	O
..	O
then	O
all	O
values	O
correspoding	O
to	O
only	O
these	O
two	O
filters	O
should	O
be	O
given	O
.	O

Actually	O
grouping	O
the	O
dataframe	B-API
lead	O
to	O
have	O
the	O
`	O
Date	B-API
`	O
as	O
index	O
of	O
the	O
new	O
dataframe	B-API
,	O
and	O
the	O
`	O
Income	O
`	O
as	O
values	O
.	O

So	O
you	O
can	O
access	O
them	O
through	O
`	O
grouped.index	O
`	O
and	O
`	O
grouped.values	O
`	O
.	O

Hope	O
that	O
helps	O
.	O

I	O
update	O
also	O
the	O
answer	O
with	O
a	O
new	O
way	O
to	O
do	O
it	O
.	O

Pandas	O
:	O
Replacing	O
column	O
values	O
in	O
dataframe	B-API

I'm	O
trying	O
to	O
replace	O
the	O
values	O
in	O
one	O
column	O
of	O
a	O
dataframe	B-API
.	O

The	O
column	O
(	O
'	O
female	O
')	O
only	O
contains	O
the	O
values	O
'	O
female	O
'	O
and	O
'	O
male	O
'	O
.	O

I	O
would	O
ideally	O
like	O
to	O
get	O
some	O
output	O
which	O
resembles	O
the	O
following	O
loop	O
element-wise	O
.	O

#CODE	O

Any	O
help	O
will	O
be	O
appreciated	O
.	O

The	O
reason	O
your	O
code	O
doesn't	O
work	O
is	O
because	O
using	O
`['female	O
']`	O
on	O
a	O
column	O
(	O
the	O
second	O
`'female	O
'`	O
in	O
your	O
`w['female']['female	O
']`)	O
doesn't	O
mean	O
"	O
select	O
rows	O
where	O
the	O
value	O
is	O
'	O
female	O
'"	O
.	O

It	O
means	O
to	O
select	O
rows	O
where	O
the	O
index	O
is	O
'	O
female	O
'	O
,	O
of	O
which	O
there	O
may	O
not	O
be	O
any	O
in	O
your	O
DataFrame	B-API
.	O

Thanks	O
.	O

Exactly	O
what	O
I	O
was	O
looking	O
for	O
.	O

If	O
I	O
were	O
to	O
map	O
'	O
female	O
'	O
to	O
1	O
and	O
anything	O
else	O
to	O
'	O
0	O
'	O
.	O

How	O
would	O
that	O
work	O
?	O

You	O
can	O
edit	O
a	O
subset	O
of	O
a	O
dataframe	B-API
by	O
using	O
loc	B-API
:	O
#CODE	O

Pandas	O
merge	B-API
not	O
giving	O
expected	O
output	O
with	O
datetime	O

I	O
have	O
two	O
dataframes	O
:	O
the	O
first	O
dataframe	B-API
"	O
fgblquotef	O
"	O
sample	O
is	O
:	O
#CODE	O

the	O
second	O
dataframe	B-API
"	O
df	O
"	O
has	O
column	O
df	O
[	O
"	O
DateTimesy	O
"]	O
created	O
using	O
:	O
#CODE	O

and	O
then	O
I	O
merge	O
using	O
:	O
#CODE	O

Which	O
is	O
wrong	O
because	O
there	O
should	O
be	O
"	O
fgblquotef	O
"	O
entries	O
mixed	O
up	O
in	O
there	O
as	O
well	O
and	O
not	O
just	O
"	O
df	O
"	O
entries	O
.	O

Can	O
anyone	O
explain	O
what	O
is	O
going	O
on	O
here	O
and	O
where	O
I	O
have	O
made	O
a	O
mistake	O
?	O

make	O
sure	O
both	O
dataframe	B-API
columns	O
have	O
the	O
same	O
data	O
type	O
.	O

I	O
know	O
Pandas	O
has	O
been	O
bugy	O
at	O
times	O
with	O
dates	O
but	O
hard	O
to	O
say	O
without	O
more	O
information	O
.	O

What	O
more	O
information	O
can	O
I	O
provide	O
:	O
I	O
use	O
fgbmquotef	O
[	O
"	O
DateTimes	O
"]	O
=	O
pd.to_datetime	B-API
(	O
fgbmquotef.dateTime	O
,	O
unit	O
=	O
"	O
s	O
")	O
and	O
the	O
same	O
for	O
df	O
to	O
create	O
dtype	B-API
:	O
datetime64	O
[	O
ns	O
]	O
.	O

df2	O
=	O
pd.merge	B-API
(	O
df	O
,	O
fgbmquotef	O
,	O
left_on	O
=	O
"	O
DateTimesy	O
"	O
,	O
right_on	O
=	O
"	O
DateTimesy	O
"	O
,	O
how	O
=	O
"	O
outer	O
")	O
#although	O
you	O
shouldn't	O
have	O
to	O
.	O

or	O
without	O
suffixes	O
:	O
#CODE	O

Finally	O
try	O
the	O
concatenate	O
function	O
:	O
#URL	O

I	O
am	O
getting	O
:	O
MergeError	O
:	O
Must	O
pass	O
right_on	O
or	O
right_index=True	O
.	O

Also	O
how	O
will	O
this	O
merge	O
using	O
DateTimesy	O
.	O

I	O
just	O
edited	O
the	O
answer	O
,	O
try	O
:	O
df2	O
=	O
pd.merge	B-API
(	O
df.set_index	B-API
(	O
"	O
DateTimesy	O
"	O
,	O
drop=False	O
)	O
,	O
fgbmquotef.set_index	O
(	O
"	O
DateTimesy	O
"	O
,	O
drop=False	O
)	O
,	O
left_index=True	O
,	O
right_index=True	O
,	O
how	O
=	O
"	O
outer	O
"	O
,	O
suffixes	O
=	O
('_df	O
'	O
,	O
'	O
_fgbmquotef	O
'))	O

or	O
maybe	O
:	O
df2	O
=	O
pd.merge	B-API
(	O
df.set_index	B-API
(	O
"	O
DateTimesy	O
")	O
,	O
fgbmquotef.set_index	O
(	O
"	O
DateTimesy	O
")	O
,	O
left_index=True	O
,	O
right_index=True	O
,	O
how	O
=	O
"	O
outer	O
")	O

I	O
have	O
ticked	O
this	O
as	O
the	O
right	O
answer	O
but	O
it	O
seems	O
to	O
be	O
a	O
bug	O
in	O
Pandas	O
,	O
unless	O
I	O
am	O
mistaken	O
.	O

Perhaps	O
I	O
should	O
post	O
it	O
as	O
a	O
bug	O
to	O
their	O
forum	O
.	O

BTW	O
i	O
used	O
df2	O
=	O
pd.merge	B-API
(	O
df.set_index	B-API
(	O
"	O
DateTimesy	O
"	O
,	O
drop=False	O
)	O
,	O
fgbmquotef.set_index	O
(	O
"	O
DateTimesy	O
"	O
,	O
drop=False	O
)	O
,	O
left_index=True	O
,	O
right_index=True	O
,	O
how	O
=	O
"	O
outer	O
")	O
.	O

Thanks	O

Well	O
,	O
one	O
approach	O
is	O
the	O
following	O
:	O
(	O
1	O
)	O
do	O
a	O
`	O
groupby	B-API
/	O
apply	B-API
`	O
with	O
'	O
id	O
'	O
as	O
grouping	O
variable	O
.	O

(	O
2	O
)	O
Within	O
the	O
apply	B-API
,	O
`	O
resample	B-API
`	O
the	O
group	O
to	O
a	O
daily	O
time	O
series	B-API
.	O

(	O
3	O
)	O
Then	O
just	O
using	O
`	O
rolling_sum	B-API
`	O
(	O
and	O
shift	O
so	O
you	O
don't	O
include	O
the	O
current	O
rows	O
'	O
x	O
'	O
value	O
)	O
to	O
compute	O
the	O
sum	O
of	O
your	O
70	O
day	O
lookback	O
periods	O
.	O

(	O
4	O
)	O
Reduce	O
the	O
group	O
back	O
to	O
only	O
the	O
original	O
observations	O
:	O
#CODE	O

You	O
are	O
going	O
to	O
need	O
your	O
data	O
sorted	O
by	O
`['id','dates	O
']`	O
.	O

Now	O
we	O
can	O
do	O
the	O
`	O
groupby	B-API
/	O
apply	B-API
`	O
:	O
#CODE	O

Thanks	O
,	O
this	O
seems	O
to	O
do	O
it	O
!	O

If	O
I	O
wanted	O
the	O
70	O
to	O
be	O
an	O
argument	O
of	O
the	O
past	O
function	O
(	O
i.e.	O
def	O
past	O
(	O
g	O
,	O
lookback	O
))	O
,	O
how	O
would	O
I	O
then	O
pass	O
that	O
argument	O
to	O
.apply	B-API
(	O
past	O
)	O
?	O

It	O
just	O
becomes	O
the	O
next	O
parameter	O
in	O
the	O
`	O
apply	B-API
`	O
.	O

See	O
me	O
edit	O
for	O
details	O
.	O

the	O
SOQL	O
docs	O
cover	O
all	O
these	O
.	O

There	O
are	O
also	O
tools	O
like	O
SoqlX	O
,	O
Workbench	O
etc	O
that	O
let	O
you	O
run	O
add-hoc	O
queries	O
,	O
these	O
are	O
useful	O
for	O
trying	O
things	O
out	O
without	O
having	O
to	O
run	O
your	O
full	O
integration	O
.	O

I	O
am	O
playing	O
with	O
the	O
excellent	O
Scikit-learn	O
today	O
.	O

I'm	O
forming	O
the	O
x's	O
out	O
of	O
panels	O
sliced	O
on	O
the	O
minor_axis	O
and	O
y's	O
out	O
of	O
DataFrame	B-API
sliced	O
on	O
columns	O
.	O

At	O
the	O
moment	O
I'm	O
doing	O
endless	O
iterations	O
,	O
does	O
any	O
.apply()	B-API
Masters	O
out	O
there	O
have	O
any	O
idea	O
how	O
to	O
speed	O
this	O
up	O
?	O

#CODE	O

My	O
idea	O
was	O
to	O
apply	B-API
this	O
function	O
(	O
or	O
similar	O
)	O
column	O
wise	O
.	O

Have	O
played	O
with	O
.apply()	B-API
but	O
because	O
its	O
a	O
double	O
(	O
or	O
triple	O
)	O
function	O
call	O
i.e.	O
f1	O
.	O

(	O
)	O
.f2	O
(	O
x	O
,	O
y	O
)	O
or	O
f1	O
.	O

(	O
)	O
.f2	O
(	O
x	O
,	O
y	O
)	O
.f3	O
(	O
x	O
,	O
y	O
)	O
it	O
gives	O
me	O
an	O
error	O
.	O

Any	O
ideas	O
would	O
be	O
greatly	O
appreciated	O
and	O
I	O
think	O
this	O
would	O
be	O
a	O
very	O
useful	O
bit	O
of	O
code	O
to	O
have	O
out	O
there	O
!	O

I	O
ran	O
a	O
million	O
regressions	O
and	O
as	O
you	O
can	O
see	O
it	O
took	O
~	O
2.5	O
minutes	O
.	O

This	O
will	O
vary	O
based	O
on	O
the	O
number	O
of	O
cores	O
you	O
have	O
.	O

`	O
result	O
`	O
will	O
be	O
a	O
list	O
of	O
your	O
scores	O
so	O
you	O
can	O
easily	O
reproduce	O
the	O
`	O
r2	O
`	O
`	O
Series	B-API
`	O
in	O
your	O
example	O
.	O

Good	O
Luck	O
!	O

python	O
pandas	O
:	O
how	O
to	O
use	O
concat()	B-API
to	O
do	O
a	O
SQL	O
union	O
?	O

I	O
understand	O
from	O
the	O
documentation	O
that	O
I	O
can	O
use	O
`	O
concat()	B-API
`	O
to	O
do	O
the	O
equivalent	O
of	O
a	O
SQL	O
UNION	O
.	O

Let's	O
say	O
I	O
have	O
a	O
table	O
with	O
3	O
fields	O
:	O
id	O
,	O
value1	O
,	O
value2	O
and	O
I	O
want	O
a	O
new	O
table	O
with	O
2	O
columns	O
,	O
where	O
id	O
is	O
repeated	O
,	O
and	O
value2	O
is	O
appended	O
below	O
value1	O
.	O

I	O
end	O
up	O
with	O
a	O
dataframe	B-API
with	O
3	O
,	O
not	O
2	O
columns	O
.	O

I	O
think	O
you	O
have	O
to	O
do	O
it	O
this	O
way	O
,	O
pandas	O
will	O
try	O
to	O
use	O
existing	O
indices	O
and	O
column	O
labels	O
for	O
alignment	O
without	O
renaming	O
you	O
can't	O
infer	O
how	O
the	O
values	O
should	O
align	O
without	O
renaming	O

You	O
could	O
try	O
the	O
melt	B-API
function	O
#CODE	O

You	O
didn't	O
show	O
exactly	O
what	O
the	O
data	O
looks	O
like	O
(	O
you	O
say	O
it's	O
delimited	O
by	O
semicolons	O
,	O
but	O
your	O
examples	O
don't	O
have	O
any	O
)	O
,	O
but	O
if	O
it	O
looks	O
like	O
#CODE	O

It	O
is	O
actually	O
much	O
faster	O
to	O
use	O
the	O
csv	O
lib	O
and	O
str.replace	B-API
:	O
#CODE	O

You	O
could	O
just	O
str.split	B-API
:	O
#CODE	O

p.s.	O

Background	O
info	O
:	O
The	O
data	O
is	O
generated	O
in	O
an	O
Android	O
app	O
as	O
a	O
the	O
java.util.Calendar	O
class	O
,	O
then	O
converted	O
to	O
a	O
string	O
in	O
Java	O
,	O
written	O
to	O
a	O
csv	O
and	O
then	O
sent	O
to	O
the	O
python	O
server	O
where	O
I	O
read	O
it	O
in	O
using	O
pandas	O
`	O
read_csv	B-API
`	O
.	O

`	O
datetime.strptime	O
(	O
x	O
,	O
"	O
%Y%m%d%H%M%S%f	O
")`	O

%b	O
:	O
Month	O
as	O
locale	O
s	O
abbreviated	O
name	O
.	O

Might	O
be	O
worth	O
highlighting	O
that	O
the	O
key	O
difference	O
here	O
is	O
the	O
use	O
of	O
`	O
%m	O
`	O
for	O
the	O
month	O
(	O
Month	O
as	O
a	O
zero-padded	O
decimal	O
number	O
)	O
instead	O
of	O
`	O
%b	O
`	O
(	O
Month	O
as	O
locale	O
s	O
abbreviated	O
name	O
)	O

`	O
%b	O
`	O
is	O
for	O
locale-based	O
month	O
name	O
abbreviations	O
like	O
`	O
Jan	O
`	O
,	O
`	O
Feb	O
`	O
,	O
etc	O
.	O

Pandas	O
groupby	B-API
:	O
compute	O
(	O
relative	O
)	O
sizes	O
and	O
save	O
in	O
original	O
dataframe	B-API

However	O
,	O
I	O
don't	O
know	O
how	O
to	O
put	O
them	O
back	O
into	O
the	O
old	O
dataframe	B-API
.	O

I	O
can	O
access	O
them	O
through	O
`	O
intensity	O
[	O
0	O
]`	O
,	O
but	O
`	O
intensity.loc()	O
`	O
gives	O
a	O
LocIndexer	O
not	O
callable	O
error	O
.	O

It's	O
a	O
multi-index	O
.	O

You	O
can	O
reset	O
the	O
index	O
by	O
calling	O
`	O
.reset_index()	B-API
`	O
to	O
your	O
resultant	O
dataframe	B-API
.	O

Or	O
you	O
can	O
disable	O
it	O
when	O
you	O
compute	O
the	O
group-by	O
operation	O
,	O
by	O
specifying	O
`	O
as_index=False	O
`	O
to	O
the	O
`	O
groupby()	B-API
`	O
,	O
like	O
:	O
#CODE	O

If	O
you	O
want	O
to	O
compute	O
`	O
intensity	O
/	O
mean	O
(	O
intensity	O
)`	O
,	O
where	O
`	O
mean	O
(	O
intensity	O
)`	O
is	O
based	O
only	O
on	O
the	O
`	O
year	B-API
`	O
and	O
not	O
`	O
year	O
/	O
groupid	O
`	O
subsets	O
,	O
then	O
you	O
first	O
have	O
to	O
create	O
the	O
`	O
mean	O
(	O
intensity	O
)`	O
based	O
on	O
the	O
`	O
year	B-API
`	O
only	O
,	O
like	O
:	O
#CODE	O

And	O
then	O
compute	O
the	O
`	O
intensity	O
/	O
mean	O
(	O
intensity	O
)`	O
for	O
all	O
`	O
year	O
/	O
groupid	O
`	O
subset	O
,	O
where	O
the	O
`	O
mean	O
(	O
intensity	O
)`	O
is	O
derived	O
only	O
from	O
`	O
year	B-API
`	O
subset	O
:	O
#CODE	O

Or	O
maybe	O
you	O
want	O
to	O
compute	O
it	O
based	O
on	O
the	O
`	O
year	B-API
`	O
only	O
,	O
not	O
on	O
`	O
year	O
/	O
groupid	O
`	O
combination	O
?	O

Actually	O
,	O
days	O
later	O
,	O
I	O
found	O
out	O
that	O
the	O
first	O
answer	O
to	O
this	O
double	O
question	O
was	O
wrong	O
.	O

Perhaps	O
someone	O
can	O
elaborate	O
to	O
what	O
`	O
.size()	B-API
`	O
actually	O
does	O
,	O
but	O
this	O
is	O
just	O
in	O
case	O
someone	O
googles	O
this	O
question	O
does	O
not	O
follow	O
my	O
wrong	O
path	O
.	O

It	O
turned	O
out	O
that	O
`	O
.size()	B-API
`	O
had	O
way	O
less	O
rows	O
than	O
the	O
original	O
object	O
(	O
also	O
if	O
I	O
used	O
`	O
reset_index()	B-API
`	O
,	O
and	O
however	O
I	O
tried	O
to	O
stack	O
the	O
sizes	O
back	O
into	O
the	O
original	O
object	O
,	O
there	O
were	O
a	O
lot	O
of	O
rows	O
left	O
with	O
`	O
NaN	O
`	O
.	O

The	O
following	O
,	O
however	O
,	O
works	O
#CODE	O

One	O
way	O
would	O
be	O
to	O
use	O
`	O
shift	B-API
`	O
to	O
move	O
the	O
relevant	O
column	O
down	O
`	O
n	O
`	O
rows	O
and	O
then	O
concatenate	O
the	O
entries	O
(	O
they	O
are	O
strings	O
so	O
we	O
can	O
use	O
`	O
+	O
`)	O
:	O
#CODE	O

This	O
creates	O
strings	O
of	O
the	O
previous	O
three	O
entries	O
separated	O
by	O
a	O
comma	O
and	O
space	O
(	O
not	O
lists	O
)	O
.	O

I'd	O
avoid	O
using	O
lists	O
in	O
DataFrames	O
if	O
possible	O
as	O
things	O
can	O
get	O
a	O
little	O
messy	O
.	O

What	O
do	O
you	O
want	O
to	O
do	O
with	O
those	O
lists	O
?	O

Storing	O
lists	O
inside	O
Series	B-API
/	O
DataFrames	O
is	O
not	O
usually	O
very	O
convenient	O
.	O

Anyway	O
,	O
this	O
would	O
get	O
you	O
close	O
.	O

You	O
have	O
to	O
handle	O
the	O
`	O
nans	O
`	O
,	O
and	O
then	O
you're	O
done	O
.	O

#CODE	O

Notice	O
that	O
we	O
have	O
to	O
convert	O
to	O
a	O
tuple	O
and	O
then	O
a	O
list	O
,	O
to	O
avoid	O
pandas	O
automatically	O
taking	O
our	O
list	O
and	O
making	O
it	O
back	O
into	O
a	O
Series	B-API
.	O

Try	O
this	O
and	O
you'll	O
see	O
why	O
it	O
doesn't	O
work	O
:	O
#CODE	O

This	O
solution	O
avoids	O
looping	O
,	O
but	O
I'm	O
not	O
sure	O
whether	O
it	O
really	O
counts	O
as	O
'	O
vectorized	O
'	O
,	O
since	O
once	O
you	O
start	O
using	O
`	O
apply()	B-API
`	O
I	O
think	O
you	O
start	O
losing	O
any	O
performance	O
benefits	O
granted	O
by	O
vectorization	O
:	O
#CODE	O

Memory	O
optimization	O
when	O
selecting	O
from	O
a	O
pandas	O
dataframe	B-API

I	O
have	O
a	O
rather	O
large	O
pandas	O
dataframe	B-API
(	O
1.7G	O
)	O
from	O
which	O
I	O
am	O
selecting	O
some	O
columns	O
to	O
do	O
some	O
computaton	O
(	O
find	O
maximum	O
value	O
of	O
the	O
three	O
selected	O
columns	O
)	O
.	O

It	O
seems	O
that	O
this	O
operation	O
is	O
memory	O
intensive	O
.	O

I	O
am	O
trying	O
to	O
find	O
a	O
way	O
to	O
avoid	O
this	O
memory	O
overhead	O
.	O

For	O
the	O
purpose	O
to	O
this	O
question	O
,	O
I	O
a	O
simplifying	O
the	O
dataframe	B-API
and	O
using	O
fake	O
data	O
.	O

My	O
code	O
and	O
the	O
memory	O
footprint	O
is	O
shown	O
below	O
,	O
#CODE	O

I	O
am	O
dealing	O
with	O
a	O
dataPanel	O
that	O
is	O
1.7G	O
.	O

When	O
I	O
do	O
this	O
selection	O
operation	O
at	O
multiple	O
place	O
in	O
my	O
program	O
,	O
I	O
end	O
up	O
with	O
an	O
overhead	O
that	O
causes	O
my	O
8G	O
machine	O
to	O
hang	O
and	O
freeze	O
.	O

you	O
could	O
do	O
that	O
,	O
or	O
work	O
with	O
the	O
data	O
in	O
a	O
HDF	O
file	O
.	O

Make	O
sure	O
your	O
dtypes	B-API
are	O
correct	O
(	O
e.g.	O
not	O
object	O
,	O
except	O
for	O
strings	O
)	O
.	O

Just	O
apply	O
the	O
function	O
directly	O
-	O
I	O
guess	O
this	O
will	O
take	O
more	O
CPU	O
as	O
it's	O
calculating	O
all	O
the	O
maxes	O
,	O
then	O
just	O
getting	O
the	O
ones	O
you	O
want	O
,	O
but	O
doesn't	O
create	O
a	O
new	O
variable	O
.	O

#CODE	O

I	O
have	O
a	O
large	O
(	O
~160	O
million	O
rows	O
)	O
dataframe	B-API
that	O
I've	O
stored	O
to	O
disk	O
with	O
something	O
like	O
this	O
:	O
#CODE	O

I	O
have	O
another	O
dataframe	B-API
containing	O
~18000	O
"	O
incidents	O
.	O

"	O
Each	O
incident	O
consists	O
of	O
some	O
(	O
as	O
few	O
as	O
hundreds	O
,	O
as	O
many	O
as	O
hundreds	O
of	O
thousands	O
)	O
individual	O
records	O
.	O

I	O
need	O
to	O
collect	O
some	O
simple	O
statistics	O
for	O
each	O
incident	O
and	O
store	O
them	O
in	O
order	O
to	O
collect	O
some	O
aggregate	O
statistics	O
.	O

Currently	O
I	O
do	O
this	O
like	O
so	O
:	O
#CODE	O

This	O
all	O
works	O
fine	O
except	O
for	O
the	O
fact	O
that	O
each	O
`	O
store.select()	O
`	O
statement	O
takes	O
roughly	O
5	O
seconds	O
which	O
means	O
that	O
processing	O
the	O
full	O
month's	O
worth	O
of	O
data	O
requires	O
somewhere	O
between	O
24-30	O
hours	O
of	O
processing	O
.	O

Meanwhile	O
,	O
the	O
actual	O
statistics	O
I	O
need	O
are	O
relatively	O
simple	O
:	O
#CODE	O

Would	O
I	O
benefit	O
from	O
using	O
store.select_as_index	O
?	O

If	O
I	O
receive	O
an	O
index	O
I'd	O
still	O
need	O
to	O
access	O
the	O
data	O
to	O
get	O
the	O
statistics	O
correct	O
?	O

what	O
is	O
the	O
relative	O
frequency	O
of	O
c_id	O
and	O
f_id	O
,	O
are	O
they	O
relatively	O
unique	O
or	O
very	O
common	O
,	O
how	O
big	O
is	O
the	O
range	O
that	O
you	O
are	O
selecting	O
each	O
time	O
(	O
e.g.	O
the	O
timestamp	O
range	O
)	O

Use	O
a	O
hierarchical	O
query	O
in	O
chunks	O
.	O

What	O
I	O
mean	O
is	O
this	O
.	O

Since	O
you	O
have	O
a	O
relatively	O
small	O
number	O
of	O
`	O
c_id	O
`	O
and	O
`	O
f_id	O
`	O
that	O
you	O
care	O
about	O
,	O
structure	O
a	O
single	O
query	O
something	O
like	O
this	O
.	O

This	O
is	O
kind	O
of	O
like	O
using	O
`	O
isin	B-API
`	O
.	O

#CODE	O

The	O
key	O
here	O
is	O
to	O
process	O
so	O
that	O
the	O
`	O
isin	B-API
`	O
DOES	O
not	O
have	O
more	O
that	O
32	O
members	O

If	O
you	O
exceed	O
this	O
,	O
the	O
query	O
will	O
work	O
,	O
but	O
it	O
will	O
drop	O
that	O
variable	O
and	O
do	O
a	O
reindex	B-API

So	O
the	O
query	O
scans	O
'	O
blocks	O
'	O
of	O
data	O
(	O
which	O
is	O
what	O
the	O
indexes	O
point	O
to	O
)	O
.	O

If	O
you	O
have	O
lots	O
of	O
hits	O
across	O
many	O
blocks	O
then	O
the	O
query	O
is	O
slower	O
.	O

How	O
can	O
I	O
rename	O
with	O
data.rename	O
(	O
index={	O
???	O
}	O
,	O
columns={	O
???	O
}	O
)	O
the	O
index	O
respectively	O
columns	O
?	O

The	O
current	O
one	O
(	O
Global	O
Data	O
Item	O
(	O
000s	O
)	O
2012	O
2011	O
2010	O
2009	O
2008	O
2007	O
2006	O
)	O
shall	O
be	O
replaced	O
by	O
(	O
10	O
Sub-Data	O
Item	O
2012	O
2011	O
2010	O
2009	O
2008	O
)	O

I'm	O
not	O
sure	O
that	O
this	O
is	O
your	O
task	O
,	O
but	O
from	O
comments	O
I	O
see	O
that	O
you	O
want	O
to	O
use	O
first	O
row	O
of	O
DataFrame	B-API
as	O
column	O
names	O
.	O

If	O
this	O
is	O
the	O
case	O
,	O
you	O
can	O
do	O
:	O
#CODE	O

I	O
have	O
a	O
column	O
'	O
datedif	O
'	O
in	O
my	O
dataframe	B-API
as	O
:	O
#CODE	O

data['datedif_day	O
']	O
=	O
data['datedif	O
']	O
.dt	B-API
.days	B-API

Error	O
:	O
AttributeError	O
:	O
'	O
Series	B-API
'	O
object	O
has	O
no	O
attribute	O
'	O
dt	B-API
'	O

Remove	O
or	O
comment	O
out	O
```	O
data.datedif	O
=	O
pd.to_datetime	B-API
(	O
data.datedif	O
)```	O
-	O
then	O
```	O
datedif	O
```	O
will	O
be	O
a	O
[	O
```	O
Timedelta	O
```	O
object	O
]	O
(	O
#URL	O
)	O
.	O

data['datedif	O
']	O
=	O
data['datedif	O
']	O
.astype	B-API
(	O
np.numpy64	O
)	O

I	O
create	O
a	O
DataFrame	B-API
?	O

#CODE	O

I	O
know	O
DataFrame	B-API
select	O
a	O
col	O
as	O
default.What	O
happened	O
when	O
I	O
run	O
`	O
data	O
[	O
se	O
]`	O
?	O

With	O
DataFrame	B-API
,	O
slicing	O
inside	O
of	O
[	O
]	O
slices	O
the	O
rows	O
.	O

This	O
is	O
provided	O
largely	O
as	O
a	O
convenience	O
since	O
it	O
is	O
such	O
a	O
common	O
operation	O
.	O

#CODE	O

In	O
your	O
example	O
where	O
you	O
use	O
`	O
range	O
(	O
2	O
)`	O
that	O
gives	O
you	O
`	O
[	O
0	O
,	O
1	O
]`	O
as	O
list	O
.	O

What	O
I	O
think	O
you	O
need	O
is	O
`	O
data	O
[	O
0:1	O
]`	O
to	O
slice	O
the	O
`	O
DataFrame	B-API
`	O
and	O
get	O
rows	O
0	O
and	O
1	O
which	O
is	O
the	O
same	O
as	O
`	O
data	O
[:	O
1	O
]`	O
omitting	O
the	O
zero	O
.	O

If	O
you	O
wanted	O
for	O
example	O
rows	O
3	O
,	O
4	O
and	O
5	O
that	O
would	O
be	O
`	O
data	O
[	O
3:5	O
]`	O
.	O

Thanks.If	O
I	O
want	O
to	O
select	O
these	O
row	O
[	O
2	O
,	O
4	O
,	O
3	O
,	O
5	O
,	O
1	O
]	O
,	O
I	O
run	O
df	O
[	O
slice	O
([	O
2	O
,	O
4	O
,	O
3	O
,	O
5	O
,	O
1	O
])]	O
,	O
there	O
is	O
an	O
error.I	O
just	O
used	O
df.ix	B-API
[	O
2	O
,	O
4	O
,	O
3	O
,	O
5	O
,	O
1	O
]	O
.Is	O
there	O
a	O
different	O
way	O
to	O
do	O
that	O
?	O

I	O
have	O
a	O
pandas	O
dataframe	B-API
that	O
contains	O
height	O
information	O
and	O
I	O
can't	O
seem	O
to	O
figure	O
out	O
how	O
to	O
convert	O
the	O
somewhat	O
unstructured	O
information	O
into	O
an	O
integer	O
.	O

I	O
figured	O
the	O
best	O
way	O
to	O
approach	O
this	O
was	O
to	O
use	O
regex	O
but	O
the	O
main	O
problem	O
I'm	O
having	O
is	O
that	O
when	O
I	O
attempt	O
to	O
simplify	O
a	O
problem	O
to	O
use	O
regex	O
I	O
usually	O
take	O
the	O
first	O
item	O
in	O
the	O
dataframe	B-API
(	O
7	O
'	O
5.5	O
'')	O
and	O
try	O
to	O
use	O
regex	O
specifically	O
on	O
it	O
.	O

It	O
seemed	O
impossible	O
for	O
me	O
to	O
put	O
this	O
data	O
in	O
a	O
string	O
because	O
of	O
the	O
quotes	O
.	O

So	O
,	O
I'm	O
really	O
confused	O
on	O
how	O
to	O
approach	O
this	O
problem	O
.	O

here	O
is	O
my	O
dataframe	B-API
:	O
#CODE	O

My	O
next	O
option	O
would	O
be	O
writing	O
this	O
to	O
csv	O
and	O
using	O
excel	O
,	O
but	O
I	O
would	O
prefer	O
to	O
learn	O
how	O
to	O
do	O
it	O
in	O
python	O
/	O
pandas	O
.	O
any	O
help	O
would	O
be	O
greatly	O
appreciated	O
.	O

How	O
about	O
using	O
string.find()	O
to	O
locate	O
the	O
'	O
and	O
the	O
"	O
and	O
then	O
cast	O
and	O
then	O
do	O
your	O
conversion	O
?	O

All	O
that	O
could	O
be	O
done	O
within	O
a	O
function	O
and	O
passed	O
to	O
an	O
Apply	B-API

One	O
possible	O
method	O
without	O
using	O
`	O
regex	O
`	O
is	O
to	O
write	O
your	O
own	O
function	O
and	O
just	O
`	O
apply	B-API
`	O
it	O
to	O
the	O
column	O
/	O
Series	B-API
of	O
your	O
choosing	O
.	O

You	O
could	O
apply	O
that	O
regular	O
expression	O
to	O
the	O
elements	O
in	O
the	O
data	O
.	O

However	O
,	O
the	O
solution	O
of	O
mapping	O
your	O
own	O
function	O
over	O
the	O
data	O
works	O
well	O
.	O

Thought	O
you	O
might	O
want	O
to	O
see	O
how	O
you	O
could	O
approach	O
this	O
using	O
your	O
original	O
idea	O
.	O

I	O
would	O
like	O
to	O
make	O
a	O
2-part	O
graphic	O
.	O

On	O
the	O
left	O
,	O
I	O
want	O
to	O
have	O
a	O
visualization	O
(	O
that	O
I	O
already	O
know	O
how	O
to	O
make	O
)	O
.	O

On	O
the	O
right	O
,	O
I	O
want	O
to	O
place	O
a	O
table	O
that	O
includes	O
more	O
numerical	O
data	O
about	O
the	O
same	O
subject	O
covered	O
by	O
the	O
graph	O
.	O

Say	O
I	O
set	O
things	O
up	O
with	O
`	O
axes	O
[	O
0	O
]`	O
for	O
the	O
visualization	O
and	O
`	O
axes	O
[	O
1	O
]`	O
as	O
the	O
place	O
where	O
I	O
want	O
the	O
table	O
.	O

I'm	O
using	O
Pandas	O
,	O
and	O
I	O
have	O
all	O
the	O
info	O
I'd	O
like	O
for	O
my	O
table	O
in	O
a	O
nice	O
neat	O
`	O
DataFrame	B-API
`	O
.	O

(	O
For	O
now	O
,	O
let's	O
assume	O
that	O
`	O
DataFrame	B-API
`	O
has	O
a	O
regular	O
`	O
Index	B-API
`	O
on	O
both	O
the	O
rows	O
and	O
the	O
columns	O
,	O
not	O
a	O
`	O
MultiIndex	O
`	O
,	O
but	O
I'm	O
curious	O
how	O
answers	O
would	O
change	O
if	O
we	O
dropped	O
that	O
assumption	O
.	O
)	O
Let's	O
call	O
that	O
`	O
tabledf	O
`	O
.	O

Thanks	O
for	O
the	O
suggestion	O
.	O

I'm	O
moderately	O
familiar	O
with	O
generating	O
tables	O
in	O
LaTeX	O
.	O

My	O
experience	O
is	O
that	O
it's	O
a	O
bit	O
of	O
a	O
pain	O
.	O

It	O
looks	O
like	O
`	O
pandas.DataFrame	B-API
`	O
has	O
a	O
`	O
to_latex	B-API
`	O
method	O
.	O

Seems	O
like	O
that's	O
the	O
next	O
place	O
I	O
should	O
look	O
.	O

@USER	O
:	O
If	O
you	O
do	O
not	O
like	O
`	O
tabular	O
`	O
,	O
you	O
may	O
use	O
any	O
LaTeX	O
package	O
.	O

The	O
output	O
of	O
`	O
to_latex	B-API
`	O
is	O
IIRC	O
for	O
the	O
`	O
booktab	O
`	O
package	O
.	O

However	O
,	O
beware	O
that	O
the	O
`	O
matplotlib	O
`	O
LaTeX	O
system	O
does	O
not	O
necessarily	O
like	O
line	O
breaks	O
,	O
so	O
you	O
may	O
need	O
to	O
replace	O
`	O
\n	O
`	O
by	O
spaces	O
in	O
the	O
string	O
given	O
by	O
`	O
to_latex	B-API
`	O
.	O

IMHO	O
well-designed	O
LaTeX	O
tables	O
look	O
very	O
good	O
.	O

Usually	O
the	O
trick	O
is	O
to	O
reduce	O
the	O
number	O
of	O
lines	O
(	O
horisontal	O
lines	O
are	O
often	O
unnecessary	O
)	O
,	O
but	O
this	O
is	O
a	O
matter	O
of	O
taste	O
.	O

How	O
do	O
I	O
combine	O
two	O
columns	O
within	O
a	O
dataframe	B-API
in	O
Pandas	O
?	O

Say	O
I	O
have	O
two	O
columns	O
,	O
A	O
and	O
B	O
,	O
in	O
my	O
dataframe	B-API
:	O
#CODE	O

I'm	O
sure	O
this	O
is	O
a	O
very	O
basic	O
question	O
,	O
but	O
as	O
I	O
am	O
new	O
to	O
Pandas	O
,	O
any	O
help	O
will	O
be	O
appreciated	O
!	O

You	O
can	O
use	O
`	O
where	B-API
`	O
which	O
is	O
a	O
vectorized	O
if	O
/	O
else	O
:	O
#CODE	O

you	O
can	O
simplify	O
to	O
this	O
:	O
`df['C	O
']	O
=	O
df.A.where	O
(	O
df.B.isnull()	O
,	O
df.B	O
)`	O
as	O
`	O
isnull	B-API
`	O
is	O
available	O
for	O
df	O
and	O
series	B-API
,	O
also	O
I	O
wouldn't	O
encourage	O
the	O
practice	O
of	O
accessing	O
columns	O
as	O
attributes	O
as	O
it	O
can	O
lead	O
to	O
strange	O
behaviour	O
,	O
better	O
to	O
do	O
this	O
`df['C	O
']	O
=	O
df['A'].where(df['B	O
']	O
.isnull()	O
,	O
df['B	O
'])`	O

You	O
can	O
use	O
`	O
combine_first	B-API
`	O
:	O
#CODE	O

shift	O
by	O
partition	O
using	O
groupby	B-API

I	O
have	O
a	O
dataframe	B-API
with	O
one	O
column	O
I	O
would	O
like	O
to	O
shift	O
,	O
but	O
over	O
partition	O
rather	O
than	O
the	O
whole	O
dataframe	B-API
.	O

For	O
example	O
,	O
I	O
would	O
like	O
to	O
go	O
from	O
this	O
dataframe	B-API
:	O
#CODE	O

to	O
this	O
dataframe	B-API
:	O
#CODE	O

I	O
believe	O
this	O
gives	O
me	O
what	O
I	O
want	O
for	O
each	O
State	O
,	O
but	O
then	O
I	O
don't	O
know	O
how	O
to	O
combine	O
it	O
back	O
together	O
(	O
basically	O
just	O
appending	O
each	O
dataframe	B-API
below	O
one	O
another	O
)	O
.	O

How	O
can	O
I	O
do	O
that	O
?	O

You	O
can	O
store	O
your	O
intermediate	O
DataFrames	O
in	O
a	O
list	O
and	O
use	O
`	O
pd.concat	B-API
`	O
to	O
join	O
them	O
together	O
:	O
#CODE	O

After	O
using	O
Pandas	O
to	O
read	O
a	O
json	O
object	O
into	O
a	O
`	O
Pandas.DataFrame	B-API
`	O
,	O
we	O
only	O
want	O
to	O
`	O
print	O
`	O
the	O
first	O
year	O
in	O
each	O
pandas	O
row	O
.	O

Eg	O
:	O
if	O
we	O
have	O
`	O
2013-2014	O
(	O
2015	O
)`	O
,	O
we	O
want	O
to	O
print	O
`	O
2013	O
`	O

Why	O
is	O
this	O
happening	O
?	O

How	O
can	O
we	O
fix	O
the	O
problem	O
?	O

Now	O
you	O
try	O
to	O
split	O
the	O
string	O
at	O
the	O
unicode	O
`	O
\u2031	O
`	O
(	O
EN	O
DASH	O
)	O
,	O
but	O
the	O
string	O
you	O
give	O
to	O
`	O
split	B-API
`	O
is	O
no	O
unicode	O
string	O
(	O
therefore	O
the	O
error	O
`'ascii	O
'	O
codec	O
can't	O
decode	O
byte	O
0xe2	O
`	O
-	O
the	O
EN	O
DASH	O
is	O
no	O
ASCII	O
character	O
)	O
.	O

Index	O
contains	O
duplicate	O
entries	O
after	O
drop_duplicates	B-API
called	O

I	O
have	O
a	O
pandas	O
dataframe	B-API
that	O
has	O
duplicate	O
entries	O
and	O
I	O
want	O
to	O
create	O
a	O
`	O
tsplot	O
`	O
using	O
`	O
seaborn	O
`	O
.	O

I	O
call	O
`	O
drop_duplicates	B-API
`	O
on	O
the	O
dataframe	B-API
(	O
and	O
even	O
call	O
`	O
reset_index()	B-API
`)	O
yet	O
when	O
I	O
got	O
to	O
do	O
the	O
plot	O
I	O
still	O
get	O
#CODE	O

Is	O
there	O
a	O
reason	O
why	O
`	O
drop_duplicates	B-API
`	O
wouldn't	O
solve	O
this	O
problem	O
?	O

EDIT	O
I've	O
even	O
checked	O
by	O
calling	O
`	O
duplicated	B-API
`	O
on	O
the	O
dataframe	B-API
after	O
the	O
drop	O
,	O
and	O
all	O
rows	O
show	O
`	O
False	O
`	O
.	O

As	O
I	O
would	O
expect	O
.	O

`	O
drop_duplicates	B-API
`	O
does	O
not	O
work	O
on	O
the	O
index	O
,	O
but	O
on	O
the	O
values	O
in	O
the	O
dataframe	B-API
!	O

(	O
so	O
it	O
looks	O
for	O
duplicate	O
rows	O
,	O
not	O
duplicate	O
indices	O
)	O
.	O

But	O
you	O
also	O
have	O
the	O
same	O
function	O
on	O
the	O
index	O
(	O
#URL	O
)	O

`	O
drop_duplicates	B-API
`	O
does	O
not	O
work	O
on	O
the	O
index	O
,	O
but	O
on	O
the	O
values	O
in	O
the	O
dataframe	B-API
!	O

(	O
so	O
it	O
looks	O
for	O
duplicate	O
rows	O
,	O
not	O
duplicate	O
indices	O
)	O
.	O

With	O
the	O
resulting	O
index	O
,	O
you	O
can	O
reindex	B-API
.	O

Another	O
option	O
is	O
to	O
add	O
the	O
index	O
as	O
a	O
column	O
and	O
use	O
`	O
DataFrame.drop_duplicates	B-API
`	O
on	O
that	O
column	O
.	O

Another	O
option	O
is	O
to	O
use	O
groupby	B-API
:	O
`	O
df.groupby	B-API
(	O
level=0	O
)	O
.first()	B-API
`	O
(	O
and	O
you	O
adapt	O
the	O
`	O
first	B-API
`	O
to	O
what	O
you	O
want	O
to	O
do	O
with	O
the	O
duplicate	O
rows	O
)	O

Starting	O
from	O
a	O
sample	O
dataframe	B-API
`	O
df	O
`	O
like	O
:	O
#CODE	O

Is	O
there	O
a	O
way	O
to	O
apply	O
a	O
`	O
math	O
`	O
function	O
to	O
a	O
whole	O
column	O
?	O

Well	O
`	O
math.exp	O
`	O
doesn't	O
understand	O
`	O
Series	B-API
`	O
datatype	O
,	O
use	O
numpy	O
`	O
np.exp	O
`	O
which	O
does	O
and	O
is	O
vectorised	O
so	O
operates	O
on	O
the	O
entire	O
column	O
:	O
#CODE	O

look	O
into	O
`	O
cubehelix	O
`	O

Cubehelix	O
is	O
awesome	O
.	O

I	O
read	O
the	O
paper	O
from	O
Dave	O
Green	O
.	O

Exactly	O
what	O
I	O
wanted	O
.	O

Got	O
excellent	O
looking	O
and	O
printing	O
graphs	O
on	O
the	O
first	O
try	O
.	O

If	O
your	O
comment	O
was	O
an	O
answer	O
I	O
would	O
accept	O
it	O
.	O

Colour-blindness	O
:	O
this	O
page	O
on	O
wikipedia	O
has	O
lots	O
of	O
good	O
info	O
about	O
choosing	O
colours	O
that	O
are	O
distinguishable	O
to	O
most	O
color-blind	O
people	O
.	O

If	O
you	O
notice	O
on	O
the	O
"	O
tips	O
for	O
editors	O
"	O
section	O
,	O
once	O
you	O
take	O
the	O
guidelines	O
into	O
account	O
there	O
are	O
only	O
a	O
few	O
sets	O
of	O
colours	O
available	O
.	O

(	O
A	O
good	O
rule	O
of	O
thumb	O
is	O
to	O
never	O
mix	O
red	O
and	O
green	O
!	O
)	O
You	O
can	O
also	O
use	O
the	O
linked	O
colour-blind	O
simulators	O
to	O
see	O
if	O
your	O
plot	O
would	O
be	O
well	O
visible	O
.	O

Luminance	O
:	O
most	O
of	O
the	O
journals	O
in	O
my	O
field	O
will	O
publish	O
in	O
B	O
W	O
by	O
default	O
.	O

Even	O
though	O
most	O
people	O
read	O
the	O
papers	O
online	O
,	O
I	O
still	O
like	O
to	O
make	O
sure	O
that	O
the	O
plots	O
can	O
be	O
understood	O
when	O
printed	O
in	O
grayscale	O
.	O

So	O
I	O
take	O
care	O
to	O
use	O
colours	O
that	O
have	O
different	O
luminances	O
.	O

To	O
test	O
,	O
a	O
good	O
way	O
is	O
to	O
just	O
desaturate	O
the	O
image	O
produced	O
,	O
and	O
you'll	O
have	O
a	O
good	O
idea	O
of	O
how	O
it	O
looks	O
when	O
printed	O
in	O
grayscale	O
.	O

In	O
many	O
cases	O
(	O
particularly	O
line	O
or	O
scatter	O
plots	O
)	O
,	O
I	O
also	O
use	O
other	O
things	O
than	O
colour	O
to	O
distinguish	O
between	O
sets	O
(	O
eg	O
.	O
line	O
styles	O
,	O
different	O
markers	O
)	O
.	O

In	O
1.5	O
matplotlib	O
will	O
ship	O
with	O
4	O
new	O
rationally	O
designed	O
color	O
maps	O
:	O

`'viridis	O
'`	O
(	O
will	O
be	O
default	O
color	O
map	O
in	O
2.0	O
)	O

The	O
process	O
of	O
designing	O
these	O
color	O
maps	O
is	O
presented	O
in	O
#URL	O
.	O

I	O
would	O
suggest	O
the	O
`	O
cubehelix	O
`	O
color	O
map	O
.	O

It	O
is	O
designed	O
to	O
have	O
correct	O
luminosity	O
ordering	O
in	O
both	O
color	O
and	O
gray-scale	O

So	O
your	O
requirements	O
are	O
"	O
lots	O
of	O
colors	O
"	O
and	O
"	O
no	O
two	O
colors	O
should	O
map	O
to	O
the	O
same	O
grayscale	O
value	O
when	O
printed	O
"	O
,	O
right	O
?	O

The	O
second	O
criteria	O
should	O
be	O
met	O
by	O
any	O
"	O
sequential	O
"	O
colormaps	O
(	O
which	O
increase	O
or	O
decrease	O
monotically	O
in	O
luminance	O
)	O
.	O

I	O
think	O
out	O
of	O
all	O
the	O
choices	O
in	O
matplotlib	O
,	O
you	O
are	O
left	O
with	O
`	O
cubehelix	O
`	O
(	O
already	O
mentioned	O
)	O
,	O
`	O
gnuplot	O
`	O
,	O
and	O
`	O
gnuplot2	O
`	O
:	O

The	O
white	O
line	O
is	O
the	O
luminance	O
of	O
each	O
color	O
,	O
so	O
you	O
can	O
see	O
that	O
each	O
color	O
will	O
map	O
to	O
a	O
different	O
grayscale	O
value	O
when	O
printed	O
.	O

The	O
black	O
line	O
is	O
hue	O
,	O
showing	O
they	O
cycle	O
through	O
a	O
variety	O
of	O
colors	O
.	O

Note	O
that	O
cubehelix	O
is	O
actually	O
a	O
function	O
(	O
`	O
from	O
matplotlib._cm	O
import	O
cubehelix	O
`)	O
,	O
and	O
you	O
can	O
adjust	O
the	O
parameters	O
of	O
the	O
helix	O
to	O
produce	O
more	O
widely-varying	O
colors	O
,	O
as	O
shown	O
here	O
.	O

In	O
other	O
words	O
,	O
cubehelix	O
is	O
not	O
a	O
colormap	O
,	O
it's	O
a	O
family	O
of	O
colormaps	O
.	O

Here	O
are	O
2	O
variations	O
:	O

For	O
less	O
wildly-varying	O
colors	O
(	O
more	O
pleasant	O
for	O
many	O
things	O
,	O
but	O
maybe	O
not	O
for	O
your	O
bar	O
graphs	O
)	O
,	O
maybe	O
try	O
the	O
ColorBrewer	O
3-color	O
maps	O
,	O
`	O
YlOrRd	O
`	O
,	O
`	O
PuBuGn	O
`	O
,	O
`	O
YlGnBu	O
`	O
:	O

I	O
wouldn't	O
recommend	O
using	O
only	O
this	O
color	O
to	O
identify	O
bar	O
graphs	O
,	O
though	O
.	O

You	O
should	O
always	O
use	O
text	O
labels	O
as	O
the	O
primary	O
identifier	O
.	O

Also	O
note	O
that	O
some	O
of	O
these	O
produce	O
white	O
bars	O
that	O
completely	O
blend	O
in	O
with	O
the	O
background	O
,	O
since	O
they	O
are	O
intended	O
for	O
heatmaps	O
,	O
not	O
chart	O
colors	O
:	O
#CODE	O

Can	O
you	O
add	O
a	O
note	O
to	O
your	O
answer	O
here	O
re	O
the	O
new	O
color	O
maps	O
?	O

I	O
know	O
I	O
can	O
just	O
edit	O
it	O
myself	O
,	O
but	O
that	O
seem	O
rude	O
.	O

Delete	O
a	O
group	O
after	O
pandas	O
groupby	B-API

Is	O
it	O
possible	O
to	O
delete	O
a	O
group	O
(	O
by	O
group	O
name	O
)	O
from	O
a	O
groupby	B-API
object	O
in	O
pandas	O
?	O

That	O
is	O
,	O
after	O
performing	O
a	O
groupby	B-API
,	O
delete	O
a	O
resulting	O
group	O
based	O
on	O
its	O
name	O
.	O

Seems	O
there's	O
no	O
direct	O
way	O
to	O
delete	O
a	O
group	O
from	O
a	O
groupby	B-API
object	O
.	O

I	O
think	O
you	O
can	O
filter	O
out	O
those	O
groupby	B-API
before	O
groupby	B-API
by	O
#CODE	O

Maybe	O
I	O
misunderstand	O
what	O
is	O
meant	O
by	O
the	O
variable	O
``	O
group	O
``	O
,	O
but	O
you	O
can't	O
index	O
a	O
DataFrame	B-API
by	O
a	O
GroupBy	B-API
object	O
.	O

Filtering	O
a	O
DataFrame	B-API
groupwise	O
has	O
been	O
discussed	O
.	O

And	O
a	O
future	O
release	O
of	O
pandas	O
may	O
include	O
a	O
more	O
convenient	O
way	O
to	O
do	O
it	O
.	O

But	O
currently	O
,	O
here	O
is	O
what	O
I	O
believe	O
to	O
be	O
the	O
most	O
succinct	O
way	O
to	O
filter	O
the	O
GroupBy	B-API
object	O
`	O
grouped	O
`	O
by	O
name	O
and	O
return	O
a	O
DataFrame	B-API
of	O
the	O
remaining	O
groups	O
.	O

#CODE	O

How	O
to	O
format	O
IPython	O
html	O
display	O
of	O
Pandas	O
dataframe	B-API
?	O

and	O
simalrly	O
for	O
other	O
data	O
types	O
.	O

But	O
IPython	O
does	O
not	O
pick	O
up	O
these	O
fromatting	O
options	O
when	O
displaying	O
dataframes	O
in	O
html	O
.	O

I	O
still	O
need	O
to	O
have	O
#CODE	O

You	O
can	O
also	O
specify	O
a	O
list	O
of	O
formatters	O
,	O
with	O
`	O
None	O
`	O
values	O
for	O
those	O
that	O
are	O
not	O
present	O
-	O
which	O
simplifies	O
the	O
`	O
frmt	O
`	O
creation	O
:	O
`	O
frmt	O
=	O
[	O
frmt_map.get	O
(	O
dtype	B-API
,	O
None	O
)	O
for	O
dtype	B-API
in	O
df.dtypes	B-API
]`	O
.	O

+1	O
for	O
the	O
research	O
.	O

HTML	O
receives	O
a	O
custom	O
string	O
of	O
html	O
data	O
.	O

Nobody	O
forbids	O
you	O
to	O
pass	O
in	O
a	O
style	O
tag	O
with	O
the	O
custom	O
CSS	O
style	O
for	O
the	O
`	O
.dataframe	O
`	O
class	O
(	O
which	O
the	O
`	O
to_html	B-API
`	O
method	O
adds	O
to	O
the	O
table	O
)	O
.	O

So	O
the	O
simplest	O
solution	O
would	O
be	O
to	O
just	O
add	O
a	O
style	O
and	O
concatenate	O
it	O
with	O
the	O
output	O
of	O
the	O
`	O
df.to_html	B-API
`	O
:	O
#CODE	O

But	O
I	O
would	O
suggest	O
to	O
define	O
a	O
custom	O
class	O
for	O
a	O
DataFrame	B-API
since	O
this	O
will	O
change	O
the	O
style	O
of	O
all	O
the	O
tables	O
in	O
your	O
notebook	O
(	O
style	O
is	O
"	O
global	O
")	O
.	O

#CODE	O

You	O
can	O
also	O
define	O
the	O
style	O
in	O
one	O
of	O
the	O
previous	O
cells	O
,	O
and	O
then	O
just	O
set	O
the	O
`	O
classes	O
`	O
parameter	O
of	O
the	O
`	O
to_html	B-API
`	O
method	O
:	O
#CODE	O

pandas	O
(	O
as	O
of	O
0.16.2	O
)	O
does	O
not	O
allow	O
overriding	O
the	O
default	O
integer	O
format	O
in	O
an	O
easy	O
way	O
.	O

It	O
is	O
hard	O
coded	O
in	O
`	O
pandas.core.format.IntArrayFormatter	O
`	O
(	O
the	O
`	O
labmda	O
`	O
function	O
):	O
#CODE	O

`	O
display.float_format	O
`	O
:	O
The	O
callable	O
should	O
accept	O
a	O
floating	O
point	O
number	O
and	O
return	O
a	O
string	O
with	O
the	O
desired	O
format	O
of	O
the	O
number	O
.	O

This	O
is	O
used	O
in	O
some	O
places	O
like	O
`	O
SeriesFormatter	O
`	O
.	O

See	O
`	O
core.format.EngFormatter	O
`	O
for	O
an	O
example	O
.	O

I	O
have	O
a	O
dataframe	B-API
like	O
the	O
following	O
:	O
#CODE	O

Efficient	O
matching	O
of	O
two	O
arrays	O
(	O
how	O
to	O
use	O
KDTree	O
)	O

I	O
have	O
two	O
2d	O
arrays	O
,	O
`	O
obs1	O
`	O
and	O
`	O
obs2	O
`	O
.	O

They	O
represent	O
two	O
independent	O
measurement	O
series	B-API
,	O
and	O
both	O
have	O
dim0	O
=	O
2	O
,	O
and	O
slightly	O
different	O
dim1	O
,	O
say	O
`	O
obs1.shape	O
=	O
(	O
2	O
,	O
250000	O
)`	O
,	O
and	O
`	O
obs2.shape	O
=	O
(	O
2	O
,	O
250050	O
)`	O
.	O

`	O
obs1	O
[	O
0	O
]`	O
and	O
`	O
obs2	O
[	O
0	O
]`	O
signify	O
time	O
,	O
and	O
`	O
obs1	O
[	O
1	O
]`	O
and	O
`	O
obs2	O
[	O
1	O
]`	O
signify	O
some	O
spatial	O
coordinate	O
.	O

Both	O
arrays	O
are	O
(	O
more	O
or	O
less	O
)	O
sorted	O
by	O
time	O
.	O

The	O
times	O
and	O
coordinates	O
should	O
be	O
identical	O
between	O
the	O
two	O
measurement	O
series	B-API
,	O
but	O
in	O
reality	O
they	O
aren't	O
.	O

Also	O
,	O
not	O
each	O
measurement	O
from	O
`	O
obs1	O
`	O
has	O
a	O
corresponding	O
value	O
in	O
`	O
obs2	O
`	O
and	O
vice-versa	O
.	O

Another	O
problem	O
is	O
that	O
there	O
might	O
be	O
a	O
slight	O
offset	O
in	O
the	O
times	O
.	O

I'm	O
looking	O
for	O
an	O
efficient	O
algorithm	O
to	O
associate	O
the	O
best	O
matching	O
value	O
from	O
`	O
obs2	O
`	O
to	O
each	O
measurement	O
in	O
`	O
obs1	O
`	O
.	O

Currently	O
,	O
I	O
do	O
it	O
like	O
this	O
:	O
#CODE	O

I	O
would	O
be	O
very	O
thankful	O
for	O
ideas	O
on	O
how	O
to	O
improve	O
this	O
algorithm	O
speed-wise	O
,	O
e.g.	O
using	O
KDtree	O
or	O
something	O
similar	O
.	O

resulting_i	O
=	O
i	O
+	O
argmin	B-API
(	O
abs	O
(	O
obs1	O
[	O
1	O
,	O
i	O
]	O
-	O
obs2	O
[	O
1	O
,	O
#URL	O
)`	O
?	O

Assuming	O
that	O
you	O
are	O
not	O
performing	O
another	O
loop	O
here	O
,	O
I	O
don't	O
believe	O
KDTrees	O
would	O
speed	O
things	O
up	O
too	O
much	O
.	O

Using	O
`	O
cKDTree	O
`	O
for	O
this	O
case	O
would	O
look	O
like	O
:	O
#CODE	O

where	O
`	O
indices	O
`	O
will	O
contain	O
the	O
column	O
indices	O
in	O
`	O
obs2	O
`	O
corresponding	O
to	O
each	O
observation	O
in	O
`	O
obs1	O
`	O
.	O

Note	O
that	O
I	O
had	O
to	O
transpose	O
`	O
obs1	O
`	O
and	O
`	O
obs2	O
`	O
.	O

In	O
my	O
specific	O
case	O
,	O
I	O
actually	O
have	O
a	O
dataframe	B-API
,	O
and	O
each	O
method	O
outputs	O
a	O
score	O
.	O

What	O
matters	O
is	O
not	O
the	O
difference	O
in	O
score	O
between	O
the	O
methods	O
and	O
the	O
true	O
scores	O
,	O
but	O
that	O
the	O
methods	O
get	O
the	O
ranking	O
right	O
(	O
higher	O
score	O
means	O
higher	O
ranking	O
for	O
all	O
columns	O
)	O
.	O

#CODE	O

Python	O
has	O
RankEval	O
based	O
module	O
that	O
implements	O
this	O
metric	O
(	O
and	O
some	O
others	O
if	O
you	O
want	O
to	O
try	O
them	O
)	O
.	O

The	O
repo	O
is	O
here	O
and	O
there	O
is	O
a	O
nice	O
IPython	O
NB	O
with	O
examples	O

I	O
am	O
producing	O
some	O
plots	O
in	O
matplotlib	O
and	O
would	O
like	O
to	O
add	O
explanatory	O
text	O
for	O
some	O
of	O
the	O
data	O
.	O

I	O
want	O
to	O
have	O
a	O
string	O
inside	O
my	O
legend	O
as	O
a	O
separate	O
legend	O
item	O
above	O
the	O
'	O
0-10	O
'	O
item	O
.	O

Does	O
anyone	O
know	O
if	O
there	O
is	O
a	O
possible	O
way	O
to	O
do	O
this	O
?	O

`ax.legend(['0-10','10-100','100-500','500+'],loc='best	O
')`	O

If	O
there	O
isn't	O
a	O
proper	O
way	O
of	O
doing	O
this	O
the	O
only	O
other	O
option	O
I	O
can	O
think	O
of	O
is	O
to	O
trick	O
the	O
graph	O
into	O
producing	O
it	O
by	O
plotting	O
some	O
empty	O
values	O

Why	O
not	O
simply	O
set	O
the	O
legends	O
`	O
title	B-API
`	O
?	O

I.e.	O

`ax.legend(['0-10','10-100','100-500','500	O
+	O
']	O
,	O
loc='best	O
'	O
,	O
title='Explanatory	O
text	O
')`	O
.	O

Sure	O
.	O

`	O
ax.legend()	O
`	O
has	O
a	O
two	O
argument	O
form	O
that	O
accepts	O
a	O
list	O
of	O
objects	O
(	O
handles	O
)	O
and	O
a	O
list	O
of	O
strings	O
(	O
labels	O
)	O
.	O

Use	O
a	O
dummy	O
object	O
(	O
aka	O
a	O
"	O
proxy	O
artist	O
"	O
)	O
for	O
your	O
extra	O
string	O
.	O

I	O
picked	O
a	O
`	O
matplotlib.patches.Rectangle	O
`	O
with	O
no	O
fill	O
and	O
0	O
linewdith	O
below	O
,	O
but	O
you	O
could	O
use	O
any	O
supported	O
artist	O
.	O

For	O
example	O
,	O
let's	O
say	O
you	O
have	O
4	O
bar	O
objects	O
(	O
since	O
you	O
didn't	O
post	O
the	O
code	O
used	O
to	O
generate	O
the	O
graph	O
,	O
I	O
can't	O
reproduce	O
it	O
exactly	O
)	O
.	O

#CODE	O

it's	O
actually	O
a	O
really	O
common	O
thing	O
.	O

I	O
bet	O
you've	O
done	O
something	O
like	O
:	O
`	O
line	O
=	O
ax.plot	O
(	O
x	O
,	O
y	O
)`	O
.	O

The	O
issue	O
is	O
that	O
`	O
plot	B-API
`	O
returns	O
a	O
*	O
list	O
*	O
of	O
lines	O
,	O
so	O
you	O
need	O
to	O
get	O
at	O
the	O
actual	O
artist	O
.	O

You	O
can	O
either	O
do	O
`	O
line	O
=	O
ax.plot	O
(	O
x	O
,	O
y	O
)	O
[	O
0	O
]`	O
or	O
do	O
`	O
line	O
,	O
=	O
ax.plot	O
(	O
x	O
,	O
y	O
)`	O
which	O
takes	O
advantage	O
of	O
parameter	O
unpacking	O
.	O

Handling	O
both	O
missing	O
and	O
duplicate	O
datatime	O
fields	O
in	O
a	O
dataframe	B-API

Observed	O
Output	O
:	O
Frame	O
contains	O
two	O
fields	O
i.e.	O
Timestamp	O
Active_Power	O
(	O
in	O
W	O
)	O
from	O
all	O
4	O
CSV	O
files	O
in	O
the	O
source	O
directory	O
aligned	O
side-by-side	O
.	O

So	O
far	O
so	O
good	O
.	O

How	O
to	O
remove	O
certain	O
rows	O
either	O
being	O
duplicated	O
and	O
/	O
or	O
missing	O
in	O
each	O
csv	O
files	O
rendering	O
dataframe	B-API
being	O
misaligned	O
in	O
timestamps	O
?	O

For	O
example	O
,	O
@USER	O
there	O
is	O
a	O
timestamp	O
value	O
missing	O
and	O
as	O
a	O
result	O
"	O
NaN	O
"	O
is	O
being	O
inserted	O
appropriately	O
.	O

But	O
@USER	O
a	O
timestamp	O
value	O
is	O
being	O
duplicated	O
twice	O
in	O
original	O
source	O
file	O
and	O
therefore	O
removed	O
resulting	O
in	O
"	O
NaN	O
"	O
again	O
,	O
as	O
a	O
result	O
the	O
timestamp	O
sequence	O
got	O
misaligned	O
.	O

How	O
to	O
deal	O
with	O
this	O
issue	O
?	O

How	O
can	O
I	O
improve	O
the	O
creation	O
time	O
of	O
a	O
pandas	O
DataFrame	B-API
?	O

I	O
am	O
having	O
a	O
dictionary	O
of	O
pandas	O
`	O
Series	B-API
`	O
,	O
each	O
with	O
their	O
own	O
index	O
and	O
all	O
containing	O
float	O
numbers	O
.	O

I	O
need	O
to	O
create	O
a	O
pandas	O
`	O
DataFrame	B-API
`	O
with	O
all	O
these	O
series	B-API
,	O
which	O
works	O
fine	O
by	O
just	O
doing	O
:	O
#CODE	O

I	O
thought	O
about	O
caching	O
the	O
result	O
but	O
unfortunately	O
the	O
`	O
dict_of_series	O
`	O
is	O
almost	O
all	O
the	O
time	O
different	O
.	O

are	O
the	O
series	B-API
aligned	O
(	O
i.e.	O
do	O
they	O
all	O
have	O
the	O
same	O
index	O
?	O
)	O

I	O
have	O
a	O
dataframe	B-API
df1	O
like	O
this	O
,	O
where	O
starttime	O
and	O
endtime	O
are	O
datetime	O
objects	O
.	O

StartTime	O
EndTime	O

If	O
endtime.hour	O
is	O
not	O
the	O
same	O
as	O
startime.hour	O
,	O
I	O
would	O
like	O
to	O
split	O
times	O
like	O
this	O

StartTime	O
EndTime	O

Essentially	O
insert	O
a	O
row	O
into	O
the	O
existing	O
dataframe	B-API
df1	O
.	O

I	O
have	O
looked	O
at	O
a	O
ton	O
of	O
examples	O
but	O
haven't	O
figured	O
out	O
how	O
to	O
do	O
this	O
.	O

If	O
my	O
question	O
isn't	O
clear	O
please	O
let	O
me	O
know	O
.	O

which	O
part	O
(	O
s	O
)	O
are	O
you	O
stuck	O
on	O
?	O

converting	O
to	O
datetime	O
and	O
extracting	O
the	O
hour	O
,	O
or	O
inserting	O
a	O
row	O
into	O
a	O
dataframe	B-API
?	O

I	O
think	O
you're	O
going	O
to	O
want	O
to	O
write	O
a	O
helper	O
function	O
or	O
two	O
,	O
and	O
you	O
may	O
have	O
to	O
reset	O
the	O
index	O
.	O

Creating	O
feature	O
(	O
row	O
)	O
vectors	O
,	O
for	O
SVM	O
,	O
from	O
Pandas	O
GroupBy	B-API
function	O
(	O
and	O
or	O
other	O
methods	O
suggested	O
)	O

I	O
am	O
trying	O
to	O
create	O
stacked	O
feature	O
vectors	O
for	O
an	O
SVM	O
classifier	O
.	O

I	O
have	O
all	O
my	O
data	O
in	O
a	O
large	O
matrix	O
.	O

The	O
problem	O
at	O
hand	O
is	O
a	O
multi-class	O
classification	O
problem	O
so	O
I	O
need	O
to	O
group	O
using	O
multi-index	O
.	O

I	O
use	O
the	O
following	O
groupby	B-API
to	O
get	O
the	O
appropriate	O
groups	O
for	O
my	O
application	O
:	O
#CODE	O

Now	O
I	O
would	O
like	O
to	O
create	O
feature	O
vectors	O
and	O
store	O
them	O
into	O
a	O
new	O
DataFrame	B-API
but	O
in	O
a	O
way	O
that	O
only	O
uses	O
two	O
rows	O
to	O
create	O
one	O
feature	O
vector	O
.	O

Meaning	O
that	O
in	O
the	O
test	O
example	O
the	O
`	O
test1	O
`	O
activity	O
is	O
performed	O
twice	O
with	O
each	O
iteration	O
having	O
the	O
same	O
label	O
so	O
in	O
this	O
case	O
it	O
has	O
two	O
labels	O
:	O
1	O
and	O
2	O
.	O

From	O
each	O
label	O
two	O
rows	O
should	O
be	O
stacked	O
to	O
create	O
the	O
desired	O
output	O
.	O

I	O
have	O
not	O
written	O
out	O
the	O
whole	O
thing	O
but	O
I	O
hope	O
it	O
is	O
obvious	O
what	O
I	O
would	O
like	O
to	O
achieve	O
.	O

Basically	O
;	O
two	O
rows	O
become	O
one	O
stacked	O
row	O
vector	O
(	O
with	O
the	O
label	O
on	O
top	O
)	O
,	O
the	O
same	O
vector	O
is	O
one	O
feature	O
vector	O
.	O

As	O
I	O
have	O
multiple	O
activities	O
I	O
need	O
multiple	O
feature	O
vectors	O
per	O
activity	O
to	O
train	O
the	O
SVM	O
.	O

For	O
this	O
example	O
I	O
would	O
ideally	O
get	O
one	O
pd.DataFrame	B-API
with	O
eight	O
feature	O
rows	O
vectors	O
in	O
it	O
so	O
the	O
data	O
frame	O
will	O
have	O
been	O
re-shaped	O
(	O
ignoring	O
everything	O
but	O
the	O
actual	O
data	O
contained	O
in	O
col_A	O
through	O
col_B	O
)	O
from	O
(	O
16	O
,	O
4	O
)	O
to	O
(	O
8	O
,	O
8)	O
.	O

You	O
need	O
to	O
pass	O
a	O
function	O
to	O
the	O
`	O
groupby	B-API
`	O
which	O
prepares	O
the	O
data	O
for	O
the	O
final	O
output	O
,	O
and	O
then	O
relabel	O
the	O
columns	O
,	O
just	O
like	O
this	O
:	O
#CODE	O

You	O
mention	O
list	O
but	O
tagged	O
this	O
as	O
pandas	O
,	O
for	O
a	O
series	B-API
by	O
default	O
calling	O
`	O
median	B-API
`	O
on	O
a	O
series	B-API
will	O
ignore	O
`	O
NaN	O
`	O
values	O
:	O
#URL	O

How	O
can	O
I	O
convert	O
Sqlalchemy	O
table	O
object	O
to	O
Pandas	O
DataFrame	B-API
?	O

Is	O
it	O
possible	O
to	O
convert	O
retrieved	O
SqlAlchemy	O
table	O
object	O
into	O
Pandas	O
DataFrame	B-API
or	O
do	O
I	O
need	O
to	O
write	O
a	O
particular	O
function	O
for	O
that	O
aim	O
?	O

Have	O
you	O
considered	O
using	O
[	O
pandas.read_sql	B-API
]	O
(	O
#URL	O
)	O
?	O

pandas.read_sql	B-API
can	O
use	O
an	O
SqlAlchemy	O
engine	O
.	O

Partial	O
query	O
results	O
(	O
NamedTuples	O
)	O
will	O
also	O
work	O
,	O
but	O
you	O
have	O
to	O
construct	O
the	O
DataFrame	B-API
`	O
columns	O
`	O
and	O
`	O
index	B-API
`	O
to	O
match	O
your	O
query	O
.	O

just	O
use	O
`	O
pandas.read_sql	B-API
`	O
with	O
an	O
SQLAlchemy	O
engine	O
.	O

it's	O
dead	O
simple	O
.	O
