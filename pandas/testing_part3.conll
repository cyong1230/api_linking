Unpacking	O
list	O
of	O
dictionary	O
elements	O
into	O
pandas	O
data	O
frame	O

I	O
am	O
trying	O
to	O
parse	O
my	O
itunes	O
playlist	O
which	O
is	O
in	O
xml	O
format	O
.	O

Here	O
is	O
the	O
sample	O
xml	O
which	O
i	O
am	O
trying	O
to	O
parse	O
and	O
put	O
the	O
end	O
result	O
in	O
pandas	O
data	O
frame	O
.	O

#CODE	O

Following	O
is	O
my	O
python	O
code	O
for	O
parsing	O
the	O
xml	O
#CODE	O

The	O
end	O
result	O
"	O
oddelements	O
"	O
object	O
is	O
a	O
list	O
of	O
element	O
dictionaries	O

Each	O
element	O
dictionary	O
in	O
this	O
list	O
contains	O
the	O
information	O
enclosed	O
in	O
"	O
dict	O
"	O
tag	O
in	O
the	O
sample	O
xml	O
which	O
i	O
have	O
pasted	O
above	O
.	O

How	O
do	O
i	O
parse	O
this	O
list	O
of	O
element	O
dictionaries	O
and	O
unpack	O
them	O
into	O
pandas	O
data	O
frame	O
for	O
further	O
analysis	O
?	O

Many	O
thanks	O
for	O
help	O

Something	O
like	O
that	O
should	O
work	O
:	O
#CODE	O

Thanks	O
Uri	O
.	O

However	O
if	O
i	O
was	O
to	O
go	O
by	O
my	O
way	O
of	O
doing	O
it	O
though	O
lxml	O
package	O
,	O
do	O
u	O
have	O
any	O
ideas	O
about	O
how	O
to	O
unpack	O
key	O
values	O
from	O
list	O
of	O
dictonary	O
elements	O
i.e	O
say	O
if	O
my	O
object	O
was	O
a	O
list	O
containing	O
dictionaries	O
like	O
[	O
dict	O
1	O
,	O
dict	O
2	O
....	O
dict	O
n	O
]	O
?	O

With	O
pandas	O
,	O
how	O
do	O
I	O
calculate	O
a	O
rolling	O
number	O
of	O
events	O
in	O
the	O
last	O
second	O
given	O
timestamp	O
data	O
?	O

I	O
have	O
dataset	O
where	O
I	O
calculate	O
service	O
times	O
based	O
on	O
request	O
and	O
response	O
times	O
.	O

I	O
would	O
like	O
to	O
add	O
a	O
calculation	O
of	O
requests	O
in	O
the	O
last	O
second	O
to	O
show	O
the	O
obvious	O
relationship	O
that	O
as	O
we	O
get	O
more	O
requests	O
per	O
second	O
the	O
system	O
slows	O
.	O

Here	O
is	O
the	O
data	O
that	O
I	O
have	O
,	O
for	O
example	O
:	O
#CODE	O

For	O
this	O
I	O
would	O
like	O
a	O
rolling	O
data	O
set	O
of	O
something	O
like	O
:	O
#CODE	O

I've	O
tried	O
rolling_sum	B-API
and	O
rolling_count	B-API
,	O
but	O
unless	O
I	O
am	O
using	O
them	O
wrong	O
or	O
not	O
understanding	O
the	O
period	O
function	O
,	O
it	O
is	O
not	O
working	O
for	O
me	O
.	O

For	O
your	O
problem	O
,	O
it	O
looks	O
like	O
you	O
want	O
to	O
summarize	O
your	O
data	O
set	O
using	O
a	O
split-apply-combine	O
approach	O
.	O

See	O
here	O
for	O
the	O
documentation	O
that	O
will	O
help	O
you	O
get	O
your	O
code	O
in	O
working	O
but	O
basically	O
,	O
you'll	O
want	O
to	O
do	O
the	O
following	O
:	O

Create	O
a	O
new	O
column	O
(	O
say	O
,	O
'	O
Req_Time_Sec	O
that	O
includes	O
`	O
Req_Time	O
`	O
down	O
to	O
only	O
second	O
resolution	O
(	O
e.g.	O
`	O
14:07	O
:	O
08.729000	O
`	O
becomes	O
`	O
14:07	O
:	O
08	O
`)	O

use	O
`	O
groups	O
=	O
serviceTimes.groupby	O
(	O
'	O
Req_Time_Sec	O
)`	O
to	O
separate	O
your	O
data	O
set	O
into	O
sub-groups	O
based	O
on	O
which	O
second	O
each	O
request	O
occurs	O
in	O
.	O

Finally	O
,	O
create	O
a	O
new	O
data	O
set	O
by	O
calculating	O
the	O
length	O
of	O
each	O
sub	O
group	O
(	O
which	O
represents	O
the	O
number	O
of	O
requests	O
in	O
that	O
second	O
)	O
and	O
aggregating	O
the	O
results	O
into	O
a	O
single	O
DataFrame	B-API
(	O
something	O
like	O
`	O
new_df	O
=	O
groups.aggregate	O
(	O
len	O
)`)	O

The	O
above	O
is	O
all	O
untested	O
pseudo-code	O
,	O
but	O
the	O
code	O
,	O
along	O
with	O
the	O
link	O
to	O
the	O
documentation	O
,	O
should	O
help	O
you	O
get	O
where	O
you	O
want	O
to	O
go	O
.	O

You	O
first	O
need	O
to	O
transform	O
the	O
timestamp	O
into	O
a	O
string	O
which	O
you	O
then	O
groupby	B-API
,	O
showing	O
the	O
count	O
and	O
average	O
service	O
times	O
:	O
#CODE	O

Alternatively	O
,	O
create	O
a	O
data	O
frame	O
of	O
the	O
request	O
time	O
in	O
the	O
appropriate	O
string	O
format	O
,	O
e.g.	O
15-13-15	O
17:27	O
,	O
then	O
count	O
the	O
occurrence	O
of	O
each	O
time	O
stamp	O
using	O
value_counts()	B-API
.	O

You	O
can	O
also	O
plot	O
the	O
results	O
quite	O
easily	O
.	O

#CODE	O

Replacing	O
duplicates	O
pandas	O
to_sql	B-API
(	O
sqlite	O
)	O

I	O
am	O
appending	O
pandas	O
dataframes	O
to	O
sqlite	O
.	O

My	O
primary	O
key	O
is	O
:	O
#CODE	O

My	O
issue	O
is	O
that	O
sometimes	O
I	O
get	O
a	O
new	O
file	O
with	O
old	O
data	O
that	O
I	O
want	O
to	O
append	B-API
to	O
the	O
existing	O
sqlite	O
table	O
.	O

I	O
am	O
not	O
reading	O
that	O
table	O
into	O
memory	O
so	O
I	O
can't	O
drop_duplicates	B-API
in	O
pandas	O
.	O

(	O
For	O
example	O
,	O
one	O
file	O
is	O
always	O
month-to-date	O
data	O
and	O
it	O
is	O
sent	O
to	O
me	O
everyday	O
)	O

How	O
can	O
I	O
ensure	O
that	O
I	O
am	O
only	O
appending	O
unique	O
values	O
based	O
on	O
my	O
primary	O
key	O
?	O

Is	O
there	O
a	O
pandas	O
to_sql	B-API
function	O
to	O
insert	B-API
or	O
replace	O
when	O
I	O
append	B-API
the	O
new	O
data	O
?	O

Also	O
,	O
should	O
I	O
specify	O
dtypes	B-API
in	O
pandas	O
before	O
writing	O
to	O
SQL	O
?	O

I	O
had	O
some	O
error	O
messages	O
when	O
I	O
tried	O
to	O
write	O
to	O
SQLite	O
and	O
I	O
had	O
categorical	B-API
dtypes	B-API
.	O

What	O
errors	O
are	O
you	O
getting	O
for	O
the	O
category	O
dtypes	B-API
?	O

If	O
you	O
attempt	O
to	O
insert	B-API
duplicate	O
data	O
you'll	O
get	O
a	O
`	O
sqlite3.IntegrityError	O
`	O
exception	O
.	O

You	O
can	O
catch	O
that	O
and	O
do	O
nothing	O
,	O
for	O
example	O
:	O
#CODE	O

Will	O
that	O
mean	O
the	O
data	O
will	O
be	O
duplicated	O
in	O
the	O
SQLite	O
table	O
?	O

Is	O
it	O
easier	O
to	O
just	O
drop	O
duplicates	O
within	O
SQLite	O
?	O

I	O
have	O
it	O
set	O
up	O
to	O
only	O
accept	O
unique	O
values	O
for	O
the	O
primary	O
key	O
(	O
the	O
three	O
columns	O
)	O
.	O

The	O
above	O
will	O
not	O
insert	B-API
duplicate	O
data	O
into	O
the	O
SQLite	O
table	O
.	O

Looping	O
through	O
a	O
groupby	B-API
and	O
adding	O
a	O
new	O
column	O

I	O
need	O
to	O
write	O
a	O
small	O
script	O
to	O
get	O
through	O
some	O
data	O
(	O
around	O
50k	O
rows	O
/	O
file	O
)	O
and	O
my	O
original	O
file	O
looks	O
like	O
this	O
:	O
#CODE	O

Its	O
a	O
rather	O
big	O
table	O
with	O
up	O
to	O
50k	O
rows	O
.	O

Now	O
not	O
all	O
the	O
data	O
is	O
important	O
to	O
me	O
,	O
I	O
mainly	O
need	O
the	O
Track_ID	O
and	O
the	O
X	O
and	O
Y	O
Position	O
.	O

So	O
I	O
create	O
a	O
dataframe	B-API
using	O
the	O
excel	O
file	O
and	O
only	O
access	O
the	O
corresponding	O
columns	O
#CODE	O

And	O
this	O
works	O
as	O
expected	O
.	O

Each	O
track_id	O
is	O
basically	O
one	O
set	O
of	O
data	O
that	O
needs	O
to	O
be	O
analyzed	O
.	O

So	O
the	O
straight	O
forward	O
way	O
is	O
to	O
group	O
the	O
dataframe	B-API
by	O
track_id	O
#CODE	O

Also	O
works	O
as	O
intended	O
.	O

Now	O
I	O
need	O
to	O
grab	O
the	O
first	O
POSITION_X	O
value	O
of	O
each	O
group	O
and	O
substract	O
them	O
from	O
the	O
other	O
POSITION_X	O
values	O
in	O
that	O
group	O
.	O

Now	O
,	O
I	O
already	O
read	O
that	O
looping	O
is	O
probably	O
not	O
the	O
best	O
way	O
to	O
go	O
about	O
it	O
,	O
but	O
I	O
have	O
no	O
idea	O
how	O
else	O
to	O
do	O
it	O
.	O

#CODE	O

This	O
stores	O
the	O
value	O
in	O
vect	O
,	O
which	O
,	O
if	O
I	O
print	O
it	O
out	O
,	O
gives	O
me	O
the	O
correct	O
value	O
.	O

However	O
,	O
I	O
have	O
the	O
problem	O
that	O
I	O
do	O
not	O
know	O
how	O
to	O
add	O
it	O
now	O
to	O
a	O
new	O
column	O
.	O

Maybe	O
someone	O
could	O
guide	O
me	O
into	O
the	O
correct	O
direction	O
.	O

Thanks	O
in	O
advance	O
.	O

EDIT	O

This	O
was	O
suggested	O
by	O
chappers	O
#CODE	O

So	O
expected	O
Output	O
would	O
be	O
that	O
the	O
very	O
first	O
calc	O
value	O
of	O
every	O
group	O
is	O
0	O

not	O
sure	O
of	O
another	O
method	O
other	O
then	O
a	O
loop	O
but	O
to	O
keep	O
track	O
of	O
the	O
values	O
just	O
append	B-API
them	O
to	O
a	O
new	O
list	O
.	O

`	O
new_list	O
=	O
[	O
]	O

loop	O
start	O
:	O

do	O
some	O
stuff	O

new_list.append	O
(	O
vect	O
)`	O

Here	O
is	O
one	O
way	O
of	O
approaching	O
it	O
,	O
using	O
the	O
apply	B-API
method	O
to	O
subtract	O
the	O
first	O
item	O
from	O
all	O
the	O
other	O
obs	O
.	O

#CODE	O

This	O
would	O
have	O
input	O
:	O
#CODE	O

and	O
output	O
#CODE	O

Thanks	O
a	O
bunch	O
,	O
in	O
principle	O
it	O
works	O
very	O
well	O
and	O
I	O
really	O
like	O
your	O
short	O
way	O
.	O

I	O
never	O
really	O
worked	O
with	O
lambda	O
operators	O
,	O
therefor	O
I	O
am	O
just	O
reading	O
up	O
on	O
them	O
now	O
.	O

One	O
problem	O
is	O
,	O
preferably	O
the	O
Value	O
for	O
the	O
0s	O
would	O
return	O
0	O
and	O
not	O
the	O
initial	O
X	O
value	O
.	O
any	O
idea	O
?	O

Could	O
you	O
provide	O
an	O
expected	O
input	O
/	O
output	O
?	O

I	O
don't	O
quite	O
understand	O
what	O
you	O
mean	O
.	O

I	O
don't	O
follow	O
,	O
could	O
you	O
post	O
a	O
minimal	O
data	O
set	O
that	O
I	O
can	O
reproduce	O
showing	O
your	O
expected	O
input	O
and	O
output	O
?	O

yes	O
,	O
sorry	O
,	O
I	O
edited	O
into	O
my	O
op	O
.	O

Hope	O
that	O
helps	O

Oh	O
right	O
in	O
that	O
case	O
you	O
don't	O
need	O
the	O
adjustment	O
thingy	O
I've	O
made	O
;	O
I'll	O
edit	O
my	O
answer	O
.	O

works	O
like	O
a	O
charm	O
,	O
thanks	O
so	O
much	O

Iterate	O
Through	O
Dictionary	O
using	O
Column	O

I	O
have	O
the	O
following	O
code	O
using	O
Pandas	O
and	O
a	O
for	O
loop	O
to	O
iterate	O
through	O
the	O
index	O
of	O
the	O
DF	O
and	O
produce	O
strings	O
of	O
each	O
row	O
in	O
a	O
dictionary	O
:	O
#CODE	O

Is	O
this	O
a	O
bad	O
way	O
of	O
doing	O
this	O
?	O

Is	O
there	O
a	O
better	O
way	O
to	O
be	O
able	O
to	O
dynamically	O
create	O
the	O
dictionary	O
object	O
,	O
event	O
,	O
and	O
pass	O
it	O
through	O
a	O
for	O
loop	O
function	O
?	O

Thanks	O
!	O

Chris	O

that	O
looks	O
quite	O
complex	O
-	O
and	O
you	O
are	O
overwriting	O
your	O
`	O
event	O
`	O
dictionary	O
on	O
every	O
iteration	O
of	O
the	O
loop	O
?	O

I'm	O
pretty	O
sure	O
this	O
code	O
won't	O
even	O
run	O
,	O
with	O
`	O
i	O
`	O
being	O
the	O
variable	O
of	O
the	O
for-loop	O
AND	O
some	O
index	O
in	O
the	O
`	O
zip	O
(	O
...	O
)`	O
.	O

I	O
think	O
you	O
should	O
try	O
to	O
specify	O
what	O
your	O
expected	O
outcome	O
is	O
,	O
and	O
then	O
somebody	O
might	O
be	O
able	O
to	O
help	O
you	O
.	O

I	O
am	O
wondering	O
if	O
there	O
is	O
a	O
way	O
to	O
iterate	O
through	O
columns	O
in	O
Pandas	O
producing	O
a	O
dictionary	O
with	O
the	O
iterated	O
variables	O
in	O
place	O
of	O
that	O
dictionary	O
.	O

I	O
am	O
not	O
even	O
sure	O
what	O
this	O
is	O
called	O
haha	O
.	O

Even	O
what	O
that	O
would	O
be	O
called	O
would	O
be	O
helpful	O
...	O

@USER	O
-sc	O

that	O
still	O
doesn't	O
sound	O
helpful	O
..	O

If	O
you	O
would	O
add	O
some	O
example	O
of	O
your	O
data	O
in	O
the	O
DF	O
and	O
your	O
expected	O
outcome	O
,	O
one	O
could	O
try	O
to	O
understand	O
what	O
you	O
want	O
to	O
achieve	O
.	O

But	O
not	O
with	O
this	O
confusing	O
and	O
not	O
working	O
code	O
example	O
.	O

Also	O
have	O
a	O
look	O
at	O
#URL	O

let	O
me	O
re	O
edit	O
the	O
question	O
...	O

@USER	O
-sc	O

Multiply	O
two	O
pandas	O
series	B-API
with	O
mismatched	O
indices	O

Created	O
two	O
series	B-API
:	O
`	O
s1	O
`	O
and	O
`	O
s2	O
`	O
from	O
`	O
df	O
`	O
.	O

Each	O
have	O
same	O
length	O
but	O
differing	O
indices	O
.	O

`	O
s1.multiply	O
(	O
s2	O
)`	O
unions	O
the	O
mismatched	O
indices	O
instead	O
of	O
multiplying	O
against	O
them	O
.	O

I	O
just	O
want	O
to	O
multiply	O
entrywise	O
`	O
s1	O
`	O
against	O
`	O
s2	O
`	O
ignoring	O
the	O
mismatched	O
indices	O
.	O

I	O
could	O
run	O
`	O
s1.reset_index()	O
`	O
and	O
`	O
s2.reset_index()	O
`	O
and	O
then	O
take	O
the	O
column	O
I	O
want	O
from	O
these	O
two	O
dfs	O
,	O
since	O
it	O
turns	O
the	O
original	O
index	O
into	O
a	O
separate	O
column	O
,	O
but	O
that's	O
tedious	O
and	O
I	O
thought	O
there	O
might	O
be	O
a	O
simpler	O
way	O
to	O
do	O
it	O
.	O

#CODE	O

doesn't	O
seem	O
to	O
work	O
either	O

you	O
can	O
convert	O
to	O
a	O
numpy	O
array	O
which	O
will	O
ignore	O
the	O
index	O
with	O
`	O
values	B-API
`	O
:	O
`	O
s1.values.mul	O
(	O
s2.values	O
)`	O
.	O

Thanks	O
John	O
,	O
that	O
does	O
indeed	O
work	O
to	O
multiply	O
the	O
values	O
of	O
the	O
series	B-API
.	O

Unfortunately	O
,	O
it	O
converts	O
the	O
series	B-API
to	O
a	O
numpy	O
array	O
.	O

Do	O
you	O
know	O
of	O
a	O
way	O
to	O
keep	O
the	O
whole	O
process	O
using	O
series	B-API
,	O
instead	O
of	O
moving	O
to	O
numpy	O
arrays	O
,	O
and	O
then	O
back	O
to	O
series	B-API
(	O
result	O
=	O
pandas.Series	B-API
(	O
s1.values	O
*	O
s2.values	O
)	O
)	O
?	O

`	O
s1	O
*	O
s2.values	O
`	O
should	O
work	O

It	O
depends	O
on	O
what	O
you	O
want	O
the	O
index	O
to	O
be	O
.	O

Your	O
suggestion	O
will	O
result	O
in	O
a	O
fresh	O
`	O
[	O
0	O
,	O
1	O
,	O
2	O
...	O
]`	O
index	O
whereas	O
Ed's	O
suggestion	O
will	O
use	O
the	O
index	O
from	O
`	O
s1	O
`	O

Ah	O
,	O
okay	O
.	O

Thank	O
you	O
both	O
John	O
and	O
Ed	O
.	O

Both	O
of	O
those	O
cover	O
the	O
solutions	O
I	O
needed	O
.	O

Could	O
someone	O
add	O
this	O
an	O
answer	O
so	O
we	O
can	O
close	O
the	O
question	O
?	O

I	O
think	O
going	O
with	O
`	O
reset_index()	B-API
`	O
is	O
the	O
way	O
,	O
but	O
there	O
is	O
an	O
option	O
to	O
drop	O
the	O
index	O
,	O
not	O
push	O
it	O
back	O
into	O
the	O
dataframe	B-API
.	O

Like	O
this	O
:	O
#CODE	O

The	O
reason	O
I	O
favour	O
the	O
`	O
reset_index()	B-API
`	O
approach	O
before	O
the	O
other	O
suggested	O
approach	O
with	O
simply	O
multiplying	O
by	O
values	O
#CODE	O

is	O
that	O
this	O
is	O
not	O
very	O
explicit	O
.	O

This	O
line	O
does	O
not	O
tell	O
me	O
that	O
there	O
is	O
an	O
index	O
problem	O
that	O
you	O
are	O
solving	O
.	O

While	O
this	O
line	O
tells	O
the	O
story	O
very	O
explicitly	O
that	O
you	O
are	O
solving	O
an	O
index	O
problem	O
:	O
#CODE	O

Or	O
break	O
it	O
down	O
to	O
multiple	O
rows	O
:	O
#CODE	O

Conditional	O
sum	O
across	O
rows	O
in	O
pandas	O
groupby	B-API
statement	O

I	O
have	O
a	O
dataframe	B-API
containing	O
weekly	O
sales	O
for	O
different	O
products	O
(	O
a	O
,	O
b	O
,	O
c	O
):	O
#CODE	O

I	O
would	O
like	O
to	O
create	O
a	O
new	O
column	O
containing	O
the	O
cumulative	O
sales	O
for	O
the	O
last	O
n	O
weeks	O
,	O
grouped	O
by	O
product	O
.	O

E.g.	O
for	O
`	O
n=2	O
`	O
it	O
should	O
be	O
like	O
`	O
last_2_weeks	O
`	O
:	O
#CODE	O

How	O
can	O
I	O
efficiently	O
calculate	O
such	O
an	O
cumulative	O
,	O
conditional	O
sum	O
in	O
pandas	O
?	O

The	O
solution	O
should	O
also	O
work	O
if	O
there	O
are	O
more	O
variables	O
to	O
group	O
by	O
,	O
e.g.	O
product	O
and	O
location	O
.	O

I	O
have	O
tried	O
creating	O
a	O
new	O
function	O
and	O
using	O
`	O
groupby	B-API
`	O
and	O
`	O
apply	B-API
`	O
,	O
but	O
this	O
works	O
only	O
if	O
rows	O
are	O
sorted	O
.	O

Also	O
it's	O
slow	O
and	O
ugly	O
.	O

#CODE	O

You	O
could	O
use	O
`	O
pd.rolling_sum	B-API
`	O
with	O
`	O
window=2	O
`	O
,	O
then	O
`	O
shift	B-API
`	O
once	O
and	O
fill	O
`	O
NaNs	O
`	O
with	O
`	O
0	O
`	O
#CODE	O

Thanks	O
.	O

Clear	O
solution	O
and	O
much	O
faster	O
compared	O
to	O
my	O
custom	O
function	O
.	O

Requires	O
sorting	O
the	O
data	O
frame	O
by	O
`	O
week	B-API
`	O
.	O

How	O
can	O
I	O
make	O
this	O
loop	O
more	O
efficient	O
?	O

I	O
have	O
a	O
historical	O
collection	O
of	O
~	O
500k	O
loans	O
,	O
some	O
of	O
which	O
have	O
defaulted	O
,	O
others	O
have	O
not	O
.	O

My	O
dataframe	B-API
is	O
`	O
lcd_temp	O
`	O
.	O

`	O
lcd_temp	O
`	O
has	O
information	O
on	O
the	O
loan	O
size	O
(	O
`	O
loan_amnt	O
`)	O
,	O
if	O
loan	O
has	O
defaulted	O
or	O
not	O
(	O
`	O
Total	O
Defaults	O
`)	O
,	O
annual	O
loan	O
rate	O
(	O
`	O
clean_rate	O
`)	O
,	O
term	O
of	O
loan	O
(	O
`	O
clean_term	O
`)	O
,	O
and	O
months	O
from	O
origination	O
to	O
default	O
(	O
`	O
mos_to_default	O
`)	O
.	O

`	O
mos_to_default	O
`	O
is	O
equal	O
to	O
`	O
clean_term	O
`	O
if	O
no	O
default	O
.	O

I	O
would	O
like	O
to	O
calculate	O
the	O
Cumulative	O
Cashflow	O
[	O
`	O
cum_cf	O
`]	O
for	O
each	O
loan	O
as	O
the	O
sum	O
of	O
all	O
coupons	O
paid	O
until	O
default	O
plus	O
(	O
1-severity	O
)	O
if	O
loan	O
defaults	O
,	O
and	O
simply	O
the	O
`	O
loan_amnt	O
`	O
if	O
it	O
pays	O
back	O
on	O
time	O
.	O

Here's	O
my	O
code	O
,	O
which	O
takes	O
an	O
awful	O
long	O
time	O
to	O
run	O
:	O
#CODE	O

Any	O
thoughts	O
or	O
suggestions	O
on	O
improving	O
the	O
speed	O
(	O
which	O
takes	O
over	O
an	O
hour	O
so	O
far	O
)	O
welcomed	O
!	O

Which	O
version	O
of	O
python	O
are	O
you	O
using	O
?	O

Did	O
you	O
profile	O
it	O
to	O
see	O
where	O
the	O
time	O
is	O
going	O
?	O

You	O
can	O
(	O
probably	O
should	O
,	O
if	O
it	O
takes	O
over	O
an	O
hour	O
)	O
use	O
numpy	O
for	O
this	O
,	O
and	O
then	O
select	O
between	O
the	O
two	O
use	O
cases	O
with	O
a	O
mask	O
,	O
e.g.	O
`	O
mask	O
=	O
lcd_temp.loc	O
[	O
...,	O
'	O
Total_Defaults	O
']	O
==	O
1	O
`	O
.	O

OP	O
:	O
are	O
you	O
using	O
Pandas	O
?	O

@USER	O
:	O
why	O
do	O
you	O
believe	O
the	O
`	O
[	O
pandas	O
]`	O
tag	O
is	O
not	O
appropriate	O
?	O

@USER	O
I	O
don't	O
see	O
any	O
indication	O
of	O
a	O
pandas	O
import	O
,	O
nor	O
any	O
use	O
of	O
a	O
pandas	O
namespace	O
.	O

For	O
all	O
I	O
know	O
,	O
the	O
OP	O
is	O
using	O
normal	O
classes	O
,	O
lists	O
and	O
dicts	O
.	O

I	O
think	O
,	O
if	O
you	O
are	O
using	O
python	O
2.6	O
or	O
above	O
,	O
then	O
`	O
multiprocessing	O
`	O
is	O
available	O
to	O
you	O
.	O

It	O
seems	O
to	O
me	O
that	O
you	O
could	O
split	O
your	O
data	O
set	O
into	O
a	O
number	O
of	O
chunks	O
,	O
one	O
per	O
core	O
,	O
and	O
pass	O
each	O
chunk	O
to	O
a	O
process	O
and	O
write	O
the	O
results	O
back	O
after	O
all	O
chunks	O
have	O
been	O
calculated	O
.	O

@USER	O
In	O
your	O
code	O
I	O
don't	O
see	O
anything	O
which	O
make	O
this	O
too	O
much	O
latent	O
..	O

`	O
1	O
hour	O
`	O
is	O
huge	O
,	O
you	O
might	O
have	O
to	O
add	O
more	O
information	O
here	O
like	O
..	O
what	O
is	O
your	O
data	O
frame	O
?	O

is	O
this	O
the	O
simple	O
calculation	O
or	O
any	O
other	O
operation	O
happening	O
?	O

@USER	O
the	O
question	O
mentions	O
a	O
dataframe	B-API
,	O
which	O
in	O
Python	O
means	O
Pandas	O
.	O

Things	O
like	O
`	O
lcd_temp.loc	O
`	O
in	O
the	O
question	O
also	O
show	O
that	O
OP	O
is	O
using	O
Pandas	O
,	O
not	O
standard	O
Python	O
data	O
structures	O
.	O

Sorry	O
for	O
delay	O
,	O
yes	O
,	O
using	O
Pandas	O
.	O

The	O
import	O
occured	O
in	O
another	O
cell	O
not	O
included	O
in	O
my	O
code	O
.	O

Apologies	O
.	O

And	O
thanks	O
to	O
unutbu	O
below	O
for	O
the	O
suggestion	O
.	O

Assuming	O
you	O
are	O
using	O
Pandas	O
/	O
NumPy	O
,	O
the	O
standard	O
way	O
to	O
replace	O
an	O
`	O
if-then	O
`	O
construction	O
such	O
as	O
the	O
one	O
you	O
are	O
using	O
is	O
to	O
use	O
`	O
np.where	O
(	O
mask	O
,	O
A	O
,	O
B	O
)`	O
.	O

The	O
`	O
mask	B-API
`	O
is	O
an	O
array	O
of	O
boolean	O
values	O
.	O

When	O
True	O
,	O
the	O
corresponding	O
value	O
from	O
`	O
A	O
`	O
is	O
returned	O
.	O

When	O
False	O
,	O
the	O
corresponding	O
value	O
from	O
`	O
B	O
`	O
is	O
returned	O
.	O

The	O
result	O
is	O
an	O
array	O
of	O
the	O
same	O
shape	O
as	O
`	O
mask	B-API
`	O
with	O
values	O
from	O
`	O
A	O
`	O
and	O
/	O
or	O
`	O
B	O
`	O
.	O

#CODE	O

Notice	O
that	O
this	O
performs	O
the	O
calculation	O
on	O
whole	O
columns	O
instead	O
of	O
row-by-row	O
.	O

This	O
improves	O
performance	O
greatly	O
because	O
it	O
gives	O
Pandas	O
/	O
NumPy	O
the	O
opportunity	O
to	O
pass	O
larger	O
arrays	O
of	O
values	O
to	O
fast	O
underlying	O
C	O
/	O
Fortran	O
functions	O
(	O
in	O
this	O
case	O
,	O
to	O
perform	O
the	O
arithmetic	O
)	O
.	O

When	O
you	O
work	O
row-by-row	O
,	O
you	O
are	O
performing	O
scalar	O
arithmetic	O
inside	O
a	O
Python	O
loop	O
,	O
which	O
gives	O
NumPy	O
zero	O
chance	O
to	O
shine	O
.	O

If	O
you	O
had	O
to	O
compute	O
row-by-row	O
,	O
you	O
would	O
be	O
just	O
as	O
well	O
(	O
and	O
maybe	O
better	O
)	O
off	O
using	O
plain	O
Python	O
.	O

Even	O
though	O
`	O
A	O
`	O
and	O
`	O
B	O
`	O
computes	O
the	O
values	O
for	O
the	O
entire	O
column	O
--	O
and	O
some	O
values	O
are	O
not	O
used	O
in	O
the	O
final	O
result	O
returned	O
by	O
`	O
np.where	O
`	O
--	O
this	O
is	O
still	O
faster	O
than	O
computing	O
row-by-row	O
assuming	O
there	O
are	O
more	O
than	O
a	O
trivial	O
number	O
of	O
rows	O
.	O

Tnx	O
,	O
I'll	O
check	O
this	O
out	O
and	O
revert	O
.	O

Didn't	O
know	O
about	O
mask	O
-	O
nor	O
about	O
the	O
efficiency	O
difference	O
with	O
Numpy	O
(	O
I'm	O
a	O
Newbie	O
)	O
.	O

Pandas	O
file	O
IO	O
Read	O
Error	O

New	O
to	O
pandas	O
,	O
running	O
into	O
an	O
error	O
consistently	O
with	O
WinXP	O
file	O
path	O
,	O
for	O
example	O
:	O
#CODE	O

Keep	O
getting	O
an	O
error	O
as	O
follows	O
:	O
#CODE	O

From	O
reading	O
thru	O
available	O
documentation	O
,	O
haven't	O
isolated	O
if	O
its	O
a	O
problem	O
with	O
my	O
syntax	O
or	O
a	O
parser	O
issue	O
.	O

Any	O
feedback	O
would	O
be	O
appreciated	O
.	O

You	O
need	O
to	O
include	O
the	O
entire	O
traceback	O
,	O
not	O
just	O
the	O
first	O
line	O
.	O

Also	O
use	O
raw-strings	O
or	O
forward-slashes	O
or	O
escape	O
your	O
backslashes	O
in	O
your	O
file	O
path	O
.	O

Unless	O
you	O
put	O
`	O
r	O
`	O
in	O
front	O
of	O
the	O
string	O
,	O
the	O
`	O
\n	O
`	O
is	O
being	O
interpreted	O
as	O
a	O
newline	O
:	O
#CODE	O

vs	O
#CODE	O

Python	O
Pandas	O
DataReader	O
vs	O
get_data_yahoo	O
which	O
is	O
better	O
?	O

I	O
understand	O
both	O
of	O
these	O
functions	O
do	O
the	O
exact	O
same	O
thing	O
.	O

Is	O
there	O
an	O
advantage	O
using	O
one	O
over	O
the	O
other	O
?	O

#CODE	O

Also	O
is	O
there	O
any	O
other	O
similar	O
functions	O
with	O
better	O
or	O
equal	O
performance	O
in	O
Pandas	O
?	O

Looking	O
at	O
the	O
source	O
code	O
,	O
`	O
DataReader	O
`	O
is	O
simply	O
a	O
wrapper	O
around	O
a	O
few	O
other	O
functions	O
including	O
`	O
get_data_yahoo	O
`	O
,	O
so	O
if	O
you're	O
definitely	O
going	O
to	O
use	O
Yahoo	O
as	O
your	O
data	O
source	O
,	O
I'd	O
say	O
just	O
stick	O
with	O
`	O
get_data_yahoo	O
`	O
.	O

But	O
it	O
really	O
doesn't	O
matter	O
.	O

I	O
don't	O
believe	O
there	O
are	O
other	O
functions	O
within	O
Pandas	O
that	O
do	O
this	O
task	O
.	O

Merge	B-API
a	O
csv	O
and	O
txt	O
file	O
,	O
then	O
alphabetize	O
and	O
eliminate	O
duplicates	O
using	O
python	O
and	O
pandas	O

I	O
am	O
trying	O
to	O
combine	O
two	O
csv	O
files	O
(	O
items.csv	O
and	O
prices.csv	O
)	O
to	O
create	O
combined_list.txt	O
.	O

The	O
result	O
(	O
combined_list.txt	O
)	O
should	O
be	O
a	O
list	O
sorted	O
in	O
alphabetical	O
order	O
in	O
the	O
format	O
:	O
item	O
(	O
quantity	O
):	O
$total_price_for_item	O
and	O
include	O
2	O
additional	O
lines	O
:	O
a	O
separator	O
line	O
with	O
10	O
equal	O
signs	O
and	O
a	O
line	O
with	O
the	O
total	O
amount	O
for	O
the	O
list	O
:	O
#CODE	O

items.csv	O
looks	O
like	O
#CODE	O

and	O
prices.txt	O
looks	O
like	O
#CODE	O

I	O
have	O
to	O
do	O
a	O
version	O
with	O
python	O
and	O
another	O
with	O
pandas	O
but	O
nothing	O
I	O
find	O
online	O
hits	O
the	O
mark	O
in	O
a	O
way	O
I	O
can	O
work	O
with	O
.	O

I	O
started	O
with	O
#CODE	O

But	O
I	O
am	O
having	O
trouble	O
putting	O
everything	O
together	O
.	O

Some	O
of	O
the	O
solutions	O
I	O
found	O
are	O
a	O
little	O
too	O
complex	O
for	O
me	O
or	O
don't	O
work	O
with	O
my	O
files	O
,	O
which	O
have	O
no	O
column	O
headers	O
.	O

I'm	O
still	O
trying	O
to	O
figure	O
it	O
out	O
but	O
I	O
know	O
that	O
for	O
some	O
of	O
you	O
this	O
is	O
super	O
easy	O
so	O
I	O
am	O
turning	O
to	O
the	O
collective	O
wisdom	O
instead	O
of	O
hitting	O
my	O
head	O
against	O
the	O
wall	O
alone	O
.	O

Pandas	O
has	O
a	O
built	O
in	O
method	O
for	O
reading	O
csv	O
files	O
.	O

Here	O
is	O
code	O
to	O
get	O
both	O
sets	O
of	O
data	O
into	O
one	O
dataframe	B-API
:	O
#CODE	O

To	O
sort	O
and	O
drop	O
duplicates	O
:	O
#CODE	O

Thanks	O
so	O
much	O
.	O

I	O
have	O
to	O
sort	O
out	O
the	O
column	O
headers	O
but	O
this	O
helps	O
get	O
started	O
.	O

subset	O
dataframe	B-API
pandas	O
timeseries	O

**	O
Updated	O
code	O
based	O
on	O
provided	O
answer	O
**	O

The	O
implemented	O
solution	O
isn't	O
subsetting	O
the	O
original	O
dataframe	B-API
.	O

#CODE	O

**	O
Original	O
problem	O
:	O
**	O

I'm	O
trying	O
to	O
subset	O
a	O
dataframe	B-API
based	O
on	O
a	O
comparison	O
between	O
its	O
datetime	O
index	O
,	O
and	O
the	O
datetime	O
index	O
of	O
another	O
data	O
frame	O
.	O

df1	O
is	O
a	O
dataframe	B-API
of	O
downsampled	O
timeseries	O
to	O
use	O
as	O
a	O
filter	O
.	O

df2	O
is	O
a	O
dataframe	B-API
of	O
records	O
to	O
be	O
filtered	O
,	O
which	O
has	O
higher	O
temporal	O
resolution	O
,	O
and	O
multiple	O
records	O
per	O
date	O
appearing	O
in	O
df1	O
:	O
#CODE	O

I'm	O
losing	O
the	O
contents	O
beyond	O
the	O
index	O
.	O

I'd	O
really	O
like	O
a	O
subset	O
of	O
records	O
from	O
df2	O
,	O
including	O
all	O
data	O
,	O
that	O
have	O
datetimes	O
matching	O
df1	O
at	O
the	O
day	O
frequency	O
,	O
like	O
:	O
#CODE	O

Any	O
help	O
would	O
be	O
appreciated	O
!	O

Use	O
the	O
`	O
isin	B-API
`	O
method	O
:	O
#CODE	O

You	O
can	O
check	O
that	O
the	O
result	O
only	O
contains	O
date	O
indices	O
where	O
they	O
overlap	O
on	O
the	O
day	O
frequency	O
by	O
comparing	O
#CODE	O

Thanks	O
for	O
your	O
input	O
.	O

I'm	O
attempting	O
to	O
implement	O
something	O
similar	O
to	O
this	O
,	O
but	O
am	O
getting	O
hung	O
up	O
at	O
df2	O
[	O
Index	O
(	O
df2.index.date	O
)	O
.isin	B-API
(	O
Index	O
(	O
df1.index.date	O
))]	O
,	O
which	O
throws	O
the	O
error	O
:	O
---------------------------------------------------------------------------	O

NameError	O
Traceback	O
(	O
most	O
recent	O
call	O
last	O
)	O

in	O
(	O
)	O

---->	O
1	O
df2	O
[	O
Index	O
(	O
df2.index.date	O
)	O
.isin	B-API
(	O
Index	O
(	O
df1.index.date	O
))]	O

NameError	O
:	O
name	O
'	O
Index	O
'	O
is	O
not	O
defined	O

You	O
need	O
to	O
do	O
`	O
from	O
pandas	O
import	O
Index	O
`	O
.	O

I'll	O
add	O
the	O
necessary	O
imports	O
to	O
the	O
answer	O
.	O

Thanks	O
.	O

This	O
looks	O
like	O
it	O
will	O
work	O
much	O
better	O
.	O

The	O
last	O
solution	O
I	O
came	O
up	O
with	O
prior	O
to	O
this	O
looks	O
like	O
:	O
`	O
df2_sub	O
=	O
[	O
i	O
for	O
i	O
,	O
item	O
in	O
enumerate	O
(	O
df2.index.date	O
)	O
if	O
item	O
in	O
df1.index.date	O
]`	O

`	O
new_df2=	O
pd.DataFrame	B-API
(	O
ais.ix	O
[	O
ais_sub	O
])`	O
Which	O
is	O
sloppy	O
code	O
and	O
fairly	O
inefficient	O
.	O

Each	O
of	O
my	O
files	O
to	O
be	O
filtered	O
is	O
about	O
1M	O
records	O
.	O

Last	O
comment	O
was	O
inaccurate	O
.	O
point	O
is	O
the	O
code	O
was	O
not	O
good	O
.	O

Can	O
you	O
be	O
a	O
bit	O
more	O
specific	O
?	O

I	O
had	O
made	O
cosmetic	O
changes	O
to	O
keep	O
code	O
in	O
df1	O
,	O
df2	O
context	O
.	O

I	O
missed	O
renaming	O
some	O
variables	O
:	O
new_df2	O
=p	O
d.DataFrame	O
(	O
ais.ix	O
[	O
ais_sub	O
])	O
.	O

ais_sub	O
is	O
a	O
subset	O
of	O
ais	O
(	O
df1	O
)	O
that	O
I	O
made	O
with	O
the	O
filter	O
(	O
df2	O
)	O
.	O

I	O
assigned	O
that	O
to	O
a	O
variable	O
,	O
and	O
then	O
cast	O
the	O
variable	O
as	O
a	O
dataframe	B-API
called	O
new_df2	O
.	O

It	O
works	O
,	O
but	O
it's	O
an	O
iterative	O
solution	O
,	O
and	O
takes	O
a	O
significant	O
amount	O
of	O
time	O
to	O
run	O
on	O
large	O
data	O
sets	O
.	O

Your	O
version	O
appears	O
to	O
be	O
array	O
wise	O
,	O
and	O
does	O
in	O
1	O
line	O
of	O
code	O
what	O
I	O
did	O
in	O
3	O
.	O

I	O
implemented	O
your	O
solution	O
but	O
it's	O
not	O
subsetting	O
.	O

Updated	O
code	O
in	O
the	O
original	O
problem	O
.	O

Can	O
you	O
show	O
the	O
output	O
of	O
what	O
you're	O
getting	O
from	O
what	O
you	O
implemented	O
?	O

Thx	O

That's	O
in	O
out	O
[	O
5	O
]	O
of	O
the	O
new	O
code	O
.	O

I	O
expanded	O
that	O
-	O
Output	O
is	O
now	O
in	O
out	O
[	O
7	O
]	O
.	O

Out	O
[8	O
]	O
and	O
out	O
[	O
9	O
]	O
are	O
for	O
verification	O
.	O

how	O
to	O
switch	O
columns	O
rows	O
in	O
a	O
pandas	O
dataframe	B-API

I	O
have	O
the	O
following	O
dataframe	B-API
:	O
#CODE	O

I	O
tried	O
with	O
pivot	B-API
table	O
#CODE	O

but	O
I	O
get	O
the	O
following	O
error	O
:	O
#CODE	O

any	O
alternative	O
to	O
pivot	B-API
table	O
to	O
do	O
this	O
?	O

What	O
is	O
the	O
desired	O
result	O
?	O

What	O
is	O
the	O
value	O
of	O
`	O
index	B-API
`	O
?	O

You	O
can	O
use	O
`	O
df	O
=	O
df.T	B-API
`	O
to	O
transpose	B-API
the	O
dataframe	B-API
-	O
if	O
I'm	O
understanding	O
the	O
question	O
correctly	O
.	O

This	O
switches	O
the	O
dataframe	B-API
round	O
so	O
that	O
the	O
rows	O
become	O
columns	O
.	O

Pandas	O
-	O
Get	O
Rank	O
on	O
a	O
Groupby	B-API
Object	O
without	O
running	O
out	O
of	O
memory	O

I	O
have	O
a	O
large	O
table	O
of	O
records	O
,	O
about	O
4	O
million	O
rows	O
.	O

I	O
need	O
to	O
add	O
an	O
index	O
that	O
counts	O
orders	O
by	O
email	O
address	O
based	O
on	O
the	O
orderID	O
(	O
ascending	O
)	O
.	O

#CODE	O

When	O
I	O
try	O
to	O
set	O
a	O
variable	O
called	O
rank	O
,	O
the	O
program	O
ran	O
for	O
90	O
minutes	O
and	O
took	O
about	O
5.5	O
gigs	O
of	O
RAM	O
,	O
but	O
never	O
returned	O
the	O
data	O
.	O

I	O
am	O
just	O
trying	O
to	O
add	O
a	O
column	O
so	O
that	O
for	O
each	O
email	O
(	O
my	O
customerID	O
)	O
,	O
I	O
get	O
the	O
order	O
rank	O
based	O
on	O
the	O
orderId	O
.	O

So	O
if	O
I	O
had	O
3	O
orders	O
,	O
my	O
first	O
order	O
would	O
have	O
the	O
lowest	O
orderID	O
,	O
etc	O
...	O
the	O
rank	O
restarts	O
for	O
every	O
email	O
.	O

Thanks	O
for	O
your	O
help	O
.	O

Jeff	O

sorting	O
is	O
slow	O
,	O
it's	O
O	O
(	O
n*log	O
(	O
n	O
))	O
.	O

I	O
think	O
this	O
may	O
also	O
be	O
doing	O
an	O
apply	B-API
,	O
which	O
is	O
also	O
slow	O
.	O

What's	O
the	O
reason	O
you	O
have	O
to	O
do	O
this	O
?	O

For	O
each	O
orderid	O
I	O
need	O
to	O
get	O
the	O
order	O
number	O
for	O
the	O
email	O
.	O

Many	O
emails	O
have	O
multiple	O
orders	O
.	O

Open	O
to	O
any	O
solution	O
that	O
will	O
do	O
that	O
.	O

try	O
`	O
df	O
=	O
df.sort	O
([	O
'	O
email	O
'	O
,	O
'	O
orderId	O
'])	O
;	O
df.groupby	B-API
(	O
'	O
email	O
')	O
.cumcount()	O
`	O
-	O
may	O
be	O
more	O
efficient	O
?	O

@USER	O
is	O
the	O
ranking	O
important	O
?	O

Can	O
you	O
get	O
away	O
with	O
just	O
enumerating	O
?	O

I	O
think	O
it	O
depends	O
what's	O
the	O
next	O
step	O
...	O

@USER	O
-	O
the	O
next	O
step	O
will	O
be	O
taking	O
the	O
categories	O
from	O
the	O
first	O
time	O
customers	O
,	O
then	O
calculating	O
total	O
lifetime	O
value	O
.	O

So	O
I	O
need	O
to	O
get	O
all	O
the	O
emails	O
in	O
the	O
set	O
of	O
rank=1	O
,	O
then	O
for	O
all	O
those	O
emails	O
,	O
given	O
their	O
starting	O
category	O
,	O
calculated	O
average	O
all	O
time	O
lifetime	O
value	O
.	O

@USER	O
-	O
the	O
sort	O
operation	O
was	O
super	O
fast	O
(	O
10	O
second	O
)	O
but	O
the	O
df.groupby	B-API
(	O
'	O
email	O
')	O
.cumcount()	O
wasn't	O
clear	O
on	O
what	O
it	O
was	O
supposed	O
to	O
return	O

[	O
cumcount	O
doc	O
]	O
(	O
#URL	O
)	O
.	O

When	O
you	O
say	O
"	O
rank	O
"	O
do	O
you	O
mean	O
size	O
,	O
i.e.	O
user	O
only	O
goes	O
once	O
,	O
or	O
the	O
first	O
entry	O
(	O
which	O
would	O
be	O
,	O
once	O
sorted	O
,	O
`	O
.groupby	B-API
(	O
"	O
email	O
")	O
.nth	O
(	O
0	O
)`	O

@USER	O
-	O
for	O
each	O
order	O
an	O
email	O
has	O
,	O
i	O
need	O
a	O
column	O
that	O
has	O
that	O
order	O
as	O
being	O
the	O
customer's	O
Nth	O
order	O
.	O

The	O
criteria	O
for	O
the	O
first	O
order	O
could	O
be	O
that	O
the	O
orderid	O
for	O
the	O
customer	O
is	O
the	O
lowest	O
possible	O
orderid	O
related	O
to	O
that	O
customer	O
.	O

By	O
customer	O
I	O
mean	O
'	O
email	O
'	O

Typically	O
in	O
large	O
memory	O
situations	O
you	O
would	O
chunk	O
your	O
data	O
and	O
run	O
each	O
chunk	O
serially	O
.	O

There's	O
lots	O
of	O
good	O
suggestions	O
for	O
doing	O
this	O
:	O

Large	O
data	O
work	O
flows	O
using	O
pandas	O

That's	O
not	O
going	O
to	O
work	O
with	O
rank	O
/	O
sort	O
however	O
.	O

Can	O
you	O
use	O
datetime.strptime	O
without	O
knowing	O
the	O
format	O
?	O

I	O
am	O
writing	O
a	O
function	O
that	O
takes	O
3	O
pandas	O
Series	B-API
,	O
one	O
of	O
which	O
is	O
dates	O
,	O
and	O
I	O
need	O
to	O
be	O
able	O
to	O
turn	O
it	O
into	O
a	O
dataframe	B-API
where	O
I	O
can	O
resample	B-API
by	O
them	O
.	O

The	O
issue	O
,	O
is	O
that	O
when	O
I	O
simply	O
do	O
the	O
following	O
:	O
#CODE	O

I	O
get	O
the	O
following	O
error	O
:	O
#CODE	O

I	O
know	O
this	O
is	O
because	O
even	O
though	O
the	O
index	O
type	O
is	O
a	O
datetime	O
object	O
,	O
when	O
going	O
through	O
with	O
resampling	O
,	O
unless	O
it	O
is	O
in	O
the	O
form	O
`	O
datetime	O
(	O
x	O
,	O
x	O
,	O
x	O
,	O
x	O
,	O
x	O
,	O
x	O
)`	O
,	O
It	O
wont	O
read	O
it	O
correctly	O
.	O

So	O
when	O
I	O
use	O
it	O
,	O
my	O
date	O
data	O
looks	O
like	O
this	O
:	O
`	O
2011-12-16	O
08:09	O
:	O
07	O
`	O
,	O
so	O
I	O
have	O
been	O
doing	O
the	O
following	O
:	O
#CODE	O

My	O
issue	O
is	O
that	O
I	O
am	O
using	O
this	O
for	O
open	O
source	O
and	O
I	O
cannot	O
know	O
what	O
format	O
the	O
dates	O
will	O
be	O
when	O
inputted	O
.	O

So	O
my	O
question	O
is	O
:	O
how	O
can	O
I	O
turn	O
a	O
string	O
with	O
a	O
date	O
and	O
a	O
time	O
to	O
a	O
datetime	O
object	O
WITHOUT	O
knowing	O
the	O
way	O
that	O
string	O
is	O
formatted	O
?	O

If	O
you	O
don't	O
know	O
how	O
the	O
string	O
is	O
formatted	O
,	O
how	O
do	O
you	O
know	O
if	O
`	O
02-01-2013	O
`	O
is	O
the	O
first	O
of	O
February	O
or	O
the	O
second	O
of	O
January	O
?	O

IOW	O
,	O
you	O
have	O
to	O
make	O
*	O
some	O
*	O
assumptions	O
about	O
the	O
format	O
.	O

You	O
can	O
use	O
the	O
`	O
dateutil	O
`	O
library	O
for	O
that	O
purpose	O
#CODE	O

thank	O
you	O
!!!	O

Worked	O
perfectly	O
.	O

it	O
did	O
and	O
it	O
was	O
great	O
,	O
but	O
it	O
did	O
cause	O
an	O
extra	O
import	O
so	O
I	O
consider	O
the	O
answer	O
I	O
accepted	O
now	O
better	O
for	O
that	O
reason	O

Pandas	O
has	O
a	O
`	O
to_datetime	B-API
`	O
function	O
for	O
this	O
purpose	O
,	O
and	O
when	O
applied	O
to	O
a	O
Series	B-API
it'll	O
convert	O
values	O
to	O
Timestamp	O
rather	O
than	O
datetime	O
:	O
#CODE	O

Where	O
:	O
#CODE	O

great	O
!	O

Quick	O
question	O
...	O

if	O
data.time	O
is	O
already	O
in	O
datetime	O
,	O
will	O
this	O
produce	O
an	O
error	O
or	O
still	O
work	O
?	O

@USER	O
It'll	O
still	O
work	O
just	O
fine	O
:)	O

pandas	O
-	O
Joining	O
CSV	O
time	O
series	B-API
into	O
a	O
single	O
dataframe	B-API

I'm	O
trying	O
to	O
get	O
4	O
CSV	O
files	O
into	O
one	O
dataframe	B-API
.	O

I've	O
looked	O
around	O
on	O
the	O
web	O
for	O
examples	O
and	O
tried	O
a	O
few	O
but	O
they	O
all	O
give	O
errors	O
.	O

Finally	O
I	O
think	O
I'm	O
onto	O
something	O
,	O
but	O
it	O
gives	O
unexpected	O
results	O
.	O

Can	O
anybody	O
tell	O
me	O
why	O
this	O
doesn't	O
work	O
?	O

#CODE	O

Expected	O
result	O
:	O
#CODE	O

Getting	O
this	O
:	O
#CODE	O

Did	O
you	O
mean	O
to	O
save	O
the	O
result	O
of	O
`	O
df.join	B-API
(	O
data	O
[	O
'	O
TempC	O
'])`	O
into	O
`	O
df	O
`	O
?	O

@USER	O
yes	O

You	O
need	O
to	O
save	O
the	O
result	O
of	O
your	O
join	O
:	O
#CODE	O

KDB+	O
like	O
asof	B-API
join	O
for	O
timeseries	O
data	O
in	O
pandas	O
?	O

kdb+	O
has	O
an	O
aj	O
function	O
that	O
is	O
usually	O
used	O
to	O
join	O
tables	O
along	O
time	O
columns	O
.	O

Here	O
is	O
an	O
example	O
where	O
I	O
have	O
trade	O
and	O
quote	O
tables	O
and	O
I	O
get	O
the	O
prevailing	O
quote	O
for	O
every	O
trade	O
.	O

#CODE	O

How	O
can	O
I	O
do	O
the	O
same	O
operation	O
using	O
pandas	O
?	O

I	O
am	O
working	O
with	O
trade	O
and	O
quote	O
dataframes	O
where	O
the	O
index	O
is	O
datetime64	O
.	O

#CODE	O

I	O
see	O
that	O
pandas	O
has	O
an	O
asof	B-API
function	O
but	O
that	O
is	O
not	O
defined	O
on	O
the	O
DataFrame	B-API
,	O
only	O
on	O
the	O
Series	B-API
object	O
.	O

I	O
guess	O
one	O
could	O
loop	O
through	O
each	O
of	O
the	O
Series	B-API
and	O
align	B-API
them	O
one	O
by	O
one	O
,	O
but	O
I	O
am	O
wondering	O
if	O
there	O
is	O
a	O
better	O
way	O
?	O

this	O
is	O
also	O
called	O
*	O
rolling	O
join	O
*	O

As	O
you	O
mentioned	O
in	O
the	O
question	O
,	O
looping	O
through	O
each	O
column	O
should	O
work	O
for	O
you	O
:	O
#CODE	O

We	O
could	O
potentially	O
create	O
a	O
faster	O
NaN-naive	O
version	O
of	O
DataFrame.asof	O
to	O
do	O
all	O
the	O
columns	O
in	O
one	O
shot	O
.	O

But	O
for	O
now	O
,	O
I	O
think	O
this	O
is	O
the	O
most	O
straightforward	O
way	O
.	O

Thanks	O
.	O

I	O
am	O
taking	O
this	O
approach	O
for	O
now	O
.	O

But	O
a	O
NaN-naive	O
version	O
would	O
be	O
very	O
welcome	O
!	O

#URL	O

I	O
wrote	O
an	O
under-advertised	O
`	O
ordered_merge	O
`	O
function	O
some	O
time	O
ago	O
:	O
#CODE	O

It	O
could	O
be	O
easily	O
(	O
well	O
,	O
for	O
someone	O
who	O
is	O
familiar	O
with	O
the	O
code	O
)	O
extended	O
to	O
be	O
a	O
"	O
left	O
join	O
"	O
mimicking	O
KDB	O
.	O

I	O
realize	O
in	O
this	O
case	O
that	O
forward-filling	O
the	O
trade	O
data	O
is	O
not	O
appropriate	O
;	O
just	O
illustrating	O
the	O
function	O
.	O

Thanks	O
,	O
this	O
is	O
very	O
good	O
to	O
know	O
.	O

This	O
is	O
essentially	O
uj	O
(	O
#URL	O
)	O
in	O
KDB	O
!.	O

For	O
the	O
aj	O
functionality	O
,	O
I	O
am	O
going	O
with	O
Chang's	O
approach	O
but	O
I	O
plan	O
to	O
take	O
a	O
serious	O
stab	O
at	O
the	O
code	O
later	O
.	O

Could	O
this	O
be	O
generalized	O
the	O
case	O
where	O
the	O
dataframe	B-API
contains	O
many	O
Series	B-API
together	O
,	O
for	O
example	O
if	O
the	O
data	O
,	O
in	O
addition	O
of	O
timestamps	O
,	O
also	O
had	O
a	O
stock	O
ID	O
column	O
?	O

(	O
Thus	O
we	O
may	O
have	O
thousands	O
of	O
groups	O
,	O
and	O
each	O
of	O
them	O
is	O
a	O
Series	B-API
)	O
.	O

I	O
suspect	O
we'll	O
need	O
a	O
mix	O
of	O
`	O
groupby()	B-API
`	O
and	O
`	O
ordered_merge	O
`	O
,	O
but	O
I'm	O
wrestling	O
about	O
how	O
to	O
do	O
it	O
...	O

For	O
sure	O
,	O
it	O
would	O
be	O
wrong	O
to	O
simply	O
`	O
ffill	B-API
`	O
on	O
the	O
overall	O
order	O
of	O
the	O
dataframe	B-API
(	O
I	O
don't	O
want	O
a	O
group	O
to	O
spill	O
into	O
the	O
next	O
group	O
by	O
virtue	O
of	O
forward	O
filling	O
)	O
.	O

bar	O
plot	O
with	O
different	O
colors	O
in	O
python	O

I	O
have	O
data	O
like	O
this	O
:	O
#CODE	O

I	O
want	O
to	O
plot	O
the	O
dataframe	B-API
in	O
horizontal	O
bars	O
with	O
different	O
colors	O
reffering	O
to	O
the	O
column	O
`'	O
typ	O
'`	O

Perhaps	O
the	O
accepted	O
answer	O
to	O
another	O
[	O
matplotlib	O
question	O
]	O
(	O
#URL	O
)	O
may	O
help	O
you	O
:	O
it	O
cycles	O
through	O
the	O
individual	O
bars	O
(	O
patches	O
)	O
and	O
assigns	O
a	O
color	O
to	O
each	O
patch	O
individually	O
.	O

You'll	O
need	O
to	O
adopt	O
it	O
for	O
a	O
barchart	O
plot	O
.	O

You	O
can	O
use	O
the	O
`	O
color	O
`	O
parameter	O
of	O
matplotlib's	O
`	O
barh	O
`	O
function	O
:	O
#CODE	O

Sub	O
Value	O
and	O
Add	O
new	O
column	O
pandas	O

I	O
am	O
trying	O
to	O
read	O
few	O
`	O
files	O
`	O
from	O
a	O
`	O
path	O
`	O
as	O
extension	O
to	O
my	O
previous	O
question	O
The	O
answer	O
given	O
by	O
Jianxun	O
Definitely	O
makes	O
sense	O
but	O
I	O
am	O
getting	O
a	O
key	O
error	O
.	O
very	O
very	O
new	O
to	O
pandas	O
and	O
not	O
able	O
to	O
fix	O
error	O
.	O

Note	O
:	O
I	O
use	O
Python	O
2.7	O
and	O
Pandas	O
0.16	O
#CODE	O

Programs	O
:	O
#CODE	O

Error	O
:	O
#CODE	O

#CODE	O

Still	O
the	O
same	O
error	O
:	O
`	O
File	O
"	O
distribute_lac.py	O
"	O
,	O
line	O
30	O
,	O
in	O

df_master	O
=	O
pd.read_csv	B-API
(	O
master_csv_file	O
,	O
index_col	O
=[	O
'	O
Ids	O
'])	O
.sort_index()	B-API

File	O
"	O
/	O
usr	O
/	O
lib	O
/	O
pymodules	O
/	O
python2.7	O
/	O
pandas	O
/	O
io	O
/	O
parsers.py	O
"	O
,	O
line	O
256	O
,	O
in	O
_read	O

return	O
parser.read()	O

File	O
"	O
/	O
usr	O
/	O
lib	O
/	O
pymodules	O
/	O
python2.7	O
/	O
pandas	O
/	O
io	O
/	O
parsers.py	O
"	O
,	O
line	O
715	O
,	O
in	O
read	O

ret	O
=	O
self._engine.read	O
(	O
nrows	O
)	O

File	O
"	O
/	O
usr	O
/	O
lib	O
/	O
pymodules	O
/	O
python2.7	O
/	O
pandas	O
/	O
io	O
/	O
parsers.py	O
"	O
,	O
line	O
1184	O
,	O
in	O
read	O

values	O
=	O
data.pop	O
(	O
self.index_col	O
[	O
i	O
])	O

KeyError	O
:	O
'	O
Ids	O
'`	O

@USER	O
Can	O
you	O
post	O
the	O
first	O
few	O
rows	O
of	O
the	O
original	O
`	O
Master1_Test.csv	O
`	O
file	O
?	O

it	O
seems	O
that	O
the	O
file	O
has	O
no	O
'	O
Ids	O
'	O
column	O
.	O

I	O
using	O
the	O
same	O
sample	O
file	O
and	O
I	O
am	O
getting	O
error	O
for	O
this	O
only	O
haven't	O
tried	O
with	O
real	O
data	O
yet	O
.	O

@USER	O
Why	O
in	O
master	O
csv	O
,	O
there	O
are	O
three	O
column	O
headers	O
`	O
Ids	O
,	O
00:00	O
:	O
00	O
,	O
00:30	O
:	O
00	O
`	O
,	O
but	O
4	O
columns	O
of	O
values	O
`	O
1234,100	O
0,500,100	O
`	O
?	O

Now	O
I	O
did	O
edit	O
the	O
Master	O
input	O
file	O
and	O
I	O
do	O
not	O
get	O
error	O
but	O
it	O
does	O
not	O
even	O
show	O
results	O
.	O

@USER	O
Could	O
you	O
please	O
double	O
check	O
whether	O
each	O
file	O
has	O
been	O
read	O
successfully	O
?	O
for	O
example	O
,	O
print	O
out	O
the	O
first	O
few	O
lines	O
of	O
each	O
`	O
df	O
`	O
see	O
whether	O
the	O
`	O
df	O
`	O
looks	O
normal	O
.	O

If	O
not	O
,	O
please	O
post	O
what	O
you	O
saw	O
.	O

Pls	O
find	O
the	O
Download	O
like	O
of	O
Dropbox	O
for	O
the	O
files	O
and	O
program	O
.	O

#URL	O

The	O
data	O
sets	O
are	O
for	O
two	O
programs	O
the	O
current	O
one	O
and	O
the	O
one	O
from	O
this	O
link	O
.	O

#URL	O

@USER	O
I've	O
replaced	O
the	O
code	O
with	O
the	O
real	O
file	O
path	O
and	O
it	O
works	O
on	O
my	O
PC	O
.	O

Give	O
a	O
check	O
and	O
see	O
whether	O
it	O
raises	O
any	O
error	O
on	O
your	O
side	O
.	O

Finally	O
!!	O

Awesome	O
..	O

Thank	O
you	O
so	O
much	O
...	O

so	O
now	O
if	O
i	O
write	O
a	O
schedule	O
program	O
to	O
auto	O
run	O
the	O
program	O
those	O
with	O
NAN	O
will	O
be	O
refilled	O
?	O
and	O
if	O
possible	O
do	O
help	O
with	O
second	O
program	O
also	O
.	O

@USER	O
I	O
am	O
certainly	O
happy	O
to	O
help	O
out	O
.	O

For	O
the	O
2nd	O
program	O
,	O
base	O
on	O
the	O
files	O
you	O
sent	O
,	O
I	O
didn't	O
see	O
the	O
`	O
Ids	O
`	O
columns	O
at	O
all	O
in	O
any	O
of	O
the	O
csv	O
files	O
.	O

Could	O
you	O
please	O
give	O
a	O
look	O
at	O
this	O
issue	O
and	O
re-upload	O
the	O
files	O
?	O

@USER	O
also	O
,	O
the	O
name	O
`	O
New	O
York	O
`	O
in	O
`	O
lat_lon_2.csv	O
`	O
seems	O
to	O
have	O
some	O
white	O
space	O
between	O
these	O
two	O
names	O
.	O

Is	O
this	O
natural	O
to	O
your	O
real	O
data	O
file	O
?	O

For	O
second	O
program	O
only	O
the	O
two	O
data	O
files	O
of	O
data_repository	O
are	O
to	O
be	O
considered	O
..	O

Not	O
from	O
Transition_Data	O
.	O

After	O
dataframe	B-API
append	B-API
I	O
have	O
the	O
same	O
amount	O
of	O
the	O
rows	O

I'm	O
trying	O
to	O
flatten	O
the	O
3rd	O
column	O
which	O
contains	O
an	O
array	O
:	O
#CODE	O

However	O
,	O
after	O
the	O
"	O
for	O
"	O
`	O
data2.id.count()	O
==	O
data.id.count()	O
`	O
and	O
`	O
data	O
[	O
'	O
Column	O
3	O
']	O
.count()	B-API
==	O
data2.my_array_items.count()	O
`	O
and	O
#CODE	O

which	O
is	O
the	O
same	O
as	O
#CODE	O

Why	O
?	O

The	O
pandas	O
`	O
DataFrame.append	B-API
`	O
method	O
returns	O
a	O
new	O
DataFrame	B-API
.	O

It	O
does	O
not	O

modify	O
the	O
original	O
.	O

Therefore	O
,	O
you	O
would	O
need	O
#CODE	O

However	O
,	O
this	O
would	O
be	O
terrible	O
slow	O
,	O
since	O
each	O
time	O
`	O
append	B-API
`	O
is	O
called	O
a	O
new	O
DataFrame	B-API
must	O
be	O
created	O
and	O
all	O
the	O
data	O
from	O
`	O
data2	O
`	O
and	O
`	O
x	O
`	O
would	O
need	O
to	O
be	O
copied	O
into	O
the	O
new	O
DataFrame	B-API
.	O

That's	O
on	O
the	O
order	O
of	O
`	O
n**2	O
`	O
copies	O
being	O
made	O
where	O
`	O
n	O
`	O
is	O
the	O
number	O
of	O
calls	O
to	O
`	O
append	B-API
`	O
.	O

A	O
faster	O
way	O
to	O
achieve	O
the	O
same	O
result	O
is	O
#CODE	O

For	O
example	O
,	O
#CODE	O

`	O
data2	O
=	O
pd.concat	B-API
([	O
data2	O
,	O
pd.DataFrame	B-API
(	O
series	B-API
)])	O

`	O
-	O
in	O
the	O
loop	O
?	O

Number	O
one	O
rule	O
of	O
programming	O
:	O
Try	O
first	O
,	O
ask	O
later	O
.	O

Therefore	O
,	O
try	O
it	O
in	O
a	O
loop	O
and	O
see	O
what	O
happens	O
.	O

If	O
it	O
works	O
,	O
you	O
have	O
your	O
answer	O
.	O

If	O
it	O
doesn't	O
,	O
again	O
you	O
have	O
your	O
answer	O
.	O

The	O
rule	O
number	O
two	O
:	O
strive	O
to	O
avoid	O
doing	O
the	O
things	O
you	O
don't	O
really	O
have	O
to	O
do	O
.	O

Your	O
code	O
from	O
the	O
1st	O
solution	O
doesn't	O
work	O
,	O
the	O
2nd	O
does	O
.	O

kde	O
density	O
plot	O
using	O
python	O
pandas	O

I	O
was	O
following	O
the	O
"	O
python	O
for	O
data	O
analysis	O
"	O
in	O
ipython	O
,	O
trying	O
to	O
do	O
a	O
kde	O
plot	O
using	O
pandas	O
(	O
chapter	O
8	O
,	O
Histograms	O
and	O
Density	O
Plots	O
)	O
.the	O
codes	O
are	O
simple	O
:	O
#CODE	O

the	O
error	O
is	O
#CODE	O

I	O
googled	O
around	O
and	O
I	O
think	O
it	O
is	O
a	O
local	O
global	O
variable	O
problem	O
and	O
may	O
be	O
due	O
to	O
a	O
wrong	O
way	O
to	O
import	O
the	O
modules	O
.	O

Anybody	O
can	O
suggest	O
any	O
ideas	O
?	O

Your	O
code	O
works	O
fine	O
for	O
me	O
.	O

Are	O
you	O
sure	O
you	O
are	O
running	O
%pylab	O
?	O

Which	O
version	O
of	O
`	O
pandas	O
`	O
are	O
you	O
running	O
?	O

`	O
print	O
(	O
pd.__version__	O
)`	O
.	O

I	O
suspect	O
it's	O
pre-0.8	O
,	O
in	O
which	O
case	O
you	O
should	O
take	O
this	O
opportunity	O
to	O
upgrade	O
to	O
`	O
0.10.1	O
`	O
.	O

The	O
problem	O
resolved	O
after	O
pandas	O
be	O
updated	O
.	O

Thank	O
you	O
very	O
much	O
!	O

Converting	O
string	O
objects	O
to	O
int	O
/	O
float	O
using	O
pandas	O

#CODE	O

The	O
csv	O
file	O
"	O
100	O
life_180_data.csv	O
"	O
contains	O
columns	O
like	O
age	O
,	O
bmi	O
,	O
Cigarettes	O
,	O
Alocohol	O
etc	O
.	O

#CODE	O

Cigarettes	O
column	O
contains	O
"	O
Never	O
"	O
"	O
1-5	O
Cigarettes	O
/	O
day	O
"	O
,	O
"	O
10-20	O
Cigarettes	O
/	O
day	O
"	O
.	O

I	O
want	O
to	O
assign	B-API
weights	O
to	O
these	O
object	O
(	O
Never	O
,	O
1-5	O
Cigarettes	O
/	O
day	O
,....	O
)	O

The	O
expected	O
output	O
is	O
new	O
column	O
CigarNum	O
appended	O
which	O
consists	O
only	O
numbers	O
0	O
,	O
1	O
,	O
2	O

CigarNum	O
is	O
as	O
expected	O
till	O
8	O
rows	O
and	O
then	O
shows	O
Nan	O
till	O
last	O
row	O
in	O
CigarNum	O
column	O
#CODE	O

The	O
output	O
I	O
get	O
shoudln't	O
give	O
NaN	O
after	O
few	O
first	O
rows	O
.	O

#CODE	O

Are	O
you	O
sure	O
row	O
10	O
and	O
11	O
actually	O
equals	O
'	O
Never	O
'	O
and	O
that	O
there	O
isn't	O
a	O
space	O
or	O
other	O
character	O
in	O
the	O
value	O
?	O

Yes	O
,	O
I	O
didn't	O
check	O
the	O
space	O
till	O
now.Really	O
thanks.Could	O
you	O
help	O
me	O
with	O
an	O
efficient	O
way	O
to	O
ignore	O
these	O
spaces.I	O
have	O
lot	O
more	O
columns	O
with	O
space	O
in	O
the	O
beginning.Thanks	O
in	O
advance	O
.	O

OK	O
,	O
first	O
problem	O
is	O
you	O
have	O
embedded	O
spaces	O
causing	O
the	O
function	O
to	O
incorrectly	O
apply	B-API
:	O

fix	O
this	O
using	O
vectorised	O
`	O
str	B-API
`	O
:	O
#CODE	O

now	O
create	O
your	O
new	O
column	O
should	O
just	O
work	O
:	O
#CODE	O

UPDATE	O

Thanks	O
to	O
@USER	O
as	O
always	O
for	O
pointing	O
out	O
superior	O
ways	O
to	O
do	O
things	O
:	O

So	O
you	O
can	O
call	O
`	O
replace	B-API
`	O
instead	O
of	O
calling	O
`	O
apply	B-API
`	O
:	O
#CODE	O

you	O
can	O
also	O
use	O
`	O
factorize	B-API
`	O
method	O
also	O
.	O

Thinking	O
about	O
it	O
why	O
not	O
just	O
set	O
the	O
dict	O
values	O
to	O
be	O
floats	O
anyway	O
and	O
then	O
you	O
avoid	O
the	O
type	O
conversion	O
?	O

So	O
:	O
#CODE	O

you	O
can	O
use	O
``	O
series.replace	B-API
(	O
dict	O
)``	O
I	O
believe	O
to	O
do	O
the	O
substitution	O
,	O
then	O
``	O
convert_objects	B-API
(	O
convert_numeric=True	O
)``	O
to	O
change	O
to	O
float	O
(	O
forcibly	O
);	O
you	O
can	O
also	O
``	O
factorize	B-API
``	O
to	O
make	O
categoricals	O
(	O
e.g.	O
map	B-API
the	O
strings	O
to	O
numbers	O
)	O

@USER	O
so	O
is	O
`	O
replace	B-API
`	O
faster	O
than	O
calling	O
`	O
map	B-API
`	O
or	O
`	O
apply	B-API
`	O
and	O
passing	O
a	O
dict	O
now	O
?	O

Wasn't	O
aware	O
of	O
`	O
factorize	B-API
`	O
also	O
,	O
when	O
was	O
this	O
introduced	O
?	O

replace	O
should	O
be	O
much	O
faster	O
;	O
``	O
factorize	B-API
``	O
has	O
been	O
their	O
quite	O
a	O
while	O
(	O
but	O
not	O
advertised	O
:))	O

Try	O
using	O
this	O
function	O
for	O
all	O
problems	O
of	O
this	O
kind	O
:	O
#CODE	O

Indexing	O
Row	O
Values	O
Based	O
on	O
Value	O
of	O
Each	O
Column	O

I	O
have	O
an	O
8r	O
x	O
10c	O
data	O
frame	O
and	O
I	O
want	O
to	O
duplicate	O
the	O
dataframe	B-API
by	O
dividing	O
the	O
values	O
in	O
each	O
row	O
by	O
the	O
first	O
value	O
in	O
its	O
column	O
(	O
i.e.	O
'	O
indexing	O
'	O
each	O
column	O
,	O
with	O
the	O
first	O
value	O
=	O
100	O
)	O
.	O

So	O
if	O
I	O
start	O
with	O
...	O

#CODE	O

It	O
would	O
return	O
...	O

#CODE	O

Is	O
there	O
a	O
simple	O
command	O
to	O
do	O
this	O
,	O
or	O
is	O
it	O
some	O
sort	O
of	O
loop	O
?	O

When	O
you	O
say	O
"	O
data	O
frame	O
"	O
,	O
do	O
you	O
specifically	O
mean	O
[	O
`	O
pandas.DataFrame	B-API
`]	O
(	O
#URL	O
)	O
?	O

If	O
you're	O
using	O
some	O
other	O
data	O
structure	O
,	O
please	O
tell	O
us	O
what	O
it	O
is	O
(	O
a	O
list	O
of	O
lists	O
,	O
a	O
NumPy	O
array	O
,	O
etc	O
)	O
.	O

Sorry	O
,	O
yes	O
--	O
a	O
Pandas	O
dataframe	B-API
.	O

The	O
first	O
column	O
in	O
your	O
example	O
has	O
been	O
divided	O
by	O
10	O
...	O
should	O
it	O
not	O
have	O
been	O
divided	O
by	O
1000	O
(	O
as	O
that's	O
the	O
first	O
value	O
in	O
the	O
column	O
)	O
?	O

I	O
want	O
the	O
first	O
value	O
in	O
each	O
column	O
to	O
be	O
100	O
and	O
everything	O
else	O
to	O
be	O
indexed	O
to	O
that	O
.	O

So	O
the	O
first	O
column	O
values	O
are	O
x	O
/	O
10	O
and	O
second	O
column	O
values	O
are	O
x	O
/	O
20	O
and	O
third	O
column	O
values	O
are	O
x	O
/	O
30	O
(	O
etc	O
)	O
.	O

You	O
could	O
do	O
the	O
following	O
:	O
#CODE	O

`	O
df.iloc	B-API
[	O
0	O
]`	O
selects	O
the	O
first	O
row	O
.	O

Divide	O
it	O
by	O
100	O
to	O
get	O
a	O
row	O
of	O
values	O
to	O
adjust	O
each	O
column	O
by	O
.	O

Lastly	O
we	O
divide	O
the	O
entire	O
DataFrame	B-API
by	O
this	O
new	O
row	O
of	O
values	O
.	O

Division	O
happens	O
along	O
axis	O
0	O
by	O
default	O
(	O
i.e.	O
downwards	O
along	O
each	O
column	O
)	O
.	O

An	O
equivalent	O
operation	O
would	O
be	O
`	O
df	O
/	O
df.iloc	B-API
[	O
0	O
]	O
*	O
100	O
`	O
.	O

Fantastic	O
--	O
thank	O
you	O
.	O

Knew	O
there	O
had	O
to	O
be	O
a	O
simple	O
way	O
to	O
do	O
it	O
.	O

iloc	B-API
works	O
to	O
select	O
the	O
first	O
row	O
,	O
but	O
the	O
division	O
fails	O
.	O

I'm	O
going	O
to	O
assume	O
this	O
is	O
my	O
lack	O
of	O
Python	O
knowledge	O
and	O
that	O
I'm	O
missing	O
something	O
obvious	O
,	O
but	O
if	O
anyone	O
knows	O
of	O
a	O
reason	O
it	O
would	O
not	O
work	O
as	O
presented	O
please	O
let	O
me	O
know	O
.	O

@USER	O
:	O
do	O
you	O
get	O
a	O
particular	O
error	O
message	O
when	O
you	O
try	O
the	O
division	O
?	O

Yields	O
a	O
long	O
strong	O
of	O
errors	O
...	O

OK	O
-	O
there	O
should	O
be	O
an	O
short	O
error	O
message	O
right	O
at	O
the	O
bottom	O
of	O
the	O
stack	O
trace	O
,	O
e.g.	O
beginning	O
with	O
`	O
ValueError	O
`	O
or	O
`	O
TypeError	O
`	O
or	O
something	O
similar	O
.	O

What	O
is	O
it	O
and	O
what	O
does	O
say	O
?	O

Turned	O
out	O
the	O
problem	O
was	O
a	O
hidden	O
index	O
column	O
.	O

Worked	O
around	O
the	O
problem	O
another	O
way	O
,	O
but	O
seems	O
like	O
.iloc	B-API
would	O
work	O
.	O

pandas	O
series	B-API
:	O
change	O
order	O
of	O
index	O

I	O
have	O
a	O
Pandas	O
series	B-API
,	O
for	O
example	O
like	O
this	O
#CODE	O

How	O
can	O
I	O
change	O
the	O
order	O
of	O
the	O
index	O
,	O
so	O
that	O
`	O
s	O
`	O
becomes	O
#CODE	O

I	O
have	O
tried	O
#CODE	O

but	O
that	O
will	O
give	O
me	O
a	O
key	O
error	O
.	O

(	O
In	O
this	O
particular	O
example	O
I	O
could	O
presumably	O
take	O
care	O
of	O
the	O
order	O
of	O
the	O
index	O
while	O
constructing	O
the	O
series	B-API
,	O
but	O
I	O
would	O
like	O
to	O
have	O
a	O
way	O
to	O
do	O
this	O
after	O
the	O
series	B-API
has	O
been	O
created	O
.	O
)	O

You	O
don't	O
have	O
enough	O
square	O
brackets	O
.	O

You	O
meant	O
to	O
do	O
this	O
:	O
`	O
s	O
[[	O
'	O
B	O
'	O
,	O
'	O
A	O
'	O
,	O
'	O
C	O
']]`	O
--	O
aka	O
advanced	O
or	O
fancy	O
indexing	O
.	O

The	O
error	O
message	O
is	O
b	O
/	O
c	O
pandas	O
thinks	O
you	O
are	O
asking	O
for	O
columns	O
'	O
B	O
'	O
,	O
'	O
A	O
'	O
,	O
and	O
'	O
C	O
'	O
.	O

Use	O
`	O
reindex	B-API
`	O
:	O
#CODE	O

Difference	O
Pivot_table	B-API
Pandas	O
and	O
excel	O

When	O
I	O
create	O
a	O
pivot	B-API
table	O
from	O
data	O
in	O
Pandas	O
(	O
python	O
)	O
,	O
I	O
get	O
an	O
other	O
result	O
than	O
when	O
I	O
create	O
it	O
with	O
Excel	O
.	O

I	O
think	O
this	O
is	O
due	O
to	O
the	O
fact	O
of	O
characters	O
.	O

Someone	O
knows	O
the	O
difference	O
between	O
the	O
pivot	B-API
table	O
in	O
Pandas	O
and	O
Excel	O
?	O

I've	O
made	O
this	O
example	O
.	O

I	O
have	O
the	O
excel	O
file	O
'	O
funds_steven	O
'	O
with	O
following	O
data	O
in	O
1	O
column	O
.	O

(	O
column	O
name	O
=	O
Steven_Funds	O
)	O
#CODE	O

How	O
can	O
I	O
read	O
this	O
in	O
and	O
calculate	O
the	O
sum	O
of	O
the	O
values	O
?	O

Always	O
helps	O
if	O
you	O
could	O
show	O
expected	O
result	O
and	O
pandas	O
result	O
to	O
debug	O
.	O

Seems	O
may	O
be	O
a	O
formatting	O
issue	O
.	O

Taking	O
the	O
sum	O
of	O
all	O
these	O
values	O
is	O
not	O
really	O
a	O
pivot	B-API
table	O
.	O

For	O
the	O
sum	O
you	O
can	O
just	O
use	O
the	O
`	O
sum()	O
`	O
method	O

Conditionally	O
concatenate	O
aggregated	O
columns	O
from	O
different	O
DataFrames	O
into	O
a	O
new	O
DataFrame	B-API

I	O
have	O
several	O
DataFrames	O
with	O
the	O
following	O
structure	O
:	O
#CODE	O

I	O
want	O
to	O
construct	O
the	O
following	O
DataFrame	B-API
.	O

`	O
max	B-API
`	O
is	O
the	O
maximum	O
value	O
in	O
the	O
column	O
'	O
0	O
'	O
subarray	O
;	O

`	O
nth	O
`	O
is	O
the	O
0-th	O
element	O
in	O
the	O
column	O
'	O
2	O
'	O
subarray	O
if	O
first-level	O
index	O
value	O
contains	O
'	O
1	O
'	O
and	O
0-th	O
element	O
in	O
the	O
column	O
'	O
3	O
'	O
subarray	O
otherwise	O
)	O
.	O

#CODE	O

I	O
tried	O
`	O
df	O
[	O
0	O
]	O
.groupby	B-API
(	O
level	O
=[	O
0	O
,	O
1	O
])	O
.max()	B-API
`	O
to	O
calculate	O
`	O
max	B-API
`	O
and	O
`	O
df	O
[	O
2	O
or	O
3	O
]	O
.groupby	B-API
(	O
level	O
=[	O
0	O
,	O
1	O
])	O
.nth	O
(	O
0	O
)`	O
to	O
calculate	O
`	O
nth	O
`	O
but	O
stuck	O
with	O
concatenation	O
using	O
index	O
values	O
as	O
a	O
condition	O
to	O
select	O
column	O
2	O
or	O
3	O
.	O

Here's	O
my	O
starting	O
point	O
(	O
same	O
code	O
as	O
yours	O
,	O
different	O
random	O
values	O
):	O
#CODE	O

#CODE	O

I	O
couldn't	O
find	O
a	O
way	O
to	O
directly	O
check	O
for	O
a	O
'	O
1	O
'	O
in	O
the	O
first	O
level	O
so	O
I	O
just	O
converted	O
it	O
to	O
a	O
colunn	O
with	O
`	O
reset_index	B-API
`	O
and	O
then	O
it's	O
fairly	O
easy	O
to	O
use	O
a	O
string	O
method	O
on	O
it	O
.	O

#CODE	O

Now	O
clean	O
things	O
up	O
(	O
some	O
of	O
which	O
could	O
be	O
done	O
earlier	O
but	O
I	O
thought	O
it	O
more	O
clear	O
to	O
wait	O
until	O
the	O
end	O
and	O
combine	O
it	O
all	O
):	O
#CODE	O

I'm	O
not	O
sure	O
if	O
you're	O
asking	O
about	O
concat	B-API
also	O
,	O
but	O
it's	O
pretty	O
straightforward	O
:	O
#CODE	O

I	O
found	O
that	O
we	O
can	O
say	O
`	O
df	O
[	O
'	O
one	O
']	O
=	O
df.index.get_level_values	O
(	O
0	O
)	O
.to_series()	B-API
.str	B-API
.contains	B-API
(	O
'	O
1	O
')	O
.values	B-API
`	O

Yeah	O
,	O
that's	O
a	O
good	O
way	O
.	O

I	O
managed	O
to	O
implement	O
the	O
solution	O
I	O
wanted	O
:	O
#CODE	O

Pandas	O
DataFrame	B-API
:	O
transforming	O
frame	O
using	O
unique	O
values	O
of	O
a	O
column	O

I	O
have	O
a	O
pandas	O
dataframe	B-API
/	O
csv	O
of	O
the	O
form	O
#CODE	O

I	O
want	O
to	O
convert	O
this	O
to	O
a	O
form	O
#CODE	O

In	O
general	O
I	O
am	O
looking	O
for	O
a	O
way	O
to	O
transform	O
a	O
table	O
using	O
unique	O
values	O
of	O
a	O
single	O
column	O
.	O

I	O
have	O
looked	O
at	O
`	O
pivot	B-API
`	O
and	O
`	O
groupby	B-API
`	O
but	O
didn't	O
get	O
the	O
exact	O
form	O
.	O

HINT	O
:	O
possibly	O
this	O
is	O
solvable	O
by	O
`	O
pivot	B-API
`	O
but	O
I	O
haven't	O
been	O
able	O
to	O
get	O
the	O
form	O

Probably	O
not	O
the	O
most	O
elegant	O
way	O
possible	O
,	O
but	O
using	O
unstack	B-API
:	O
#CODE	O

A	O
little	O
more	O
generally	O
,	O
and	O
removing	O
the	O
strange	O
hierarchical	O
columns	O
in	O
the	O
result	O
:	O
#CODE	O

wow	O
..	O
thanks	O
for	O
the	O
quick	O
response	O
..	O

I	O
am	O
guessing	O
a	O
mental	O
block	O
but	O
is	O
there	O
a	O
way	O
I	O
can	O
remove	O
the	O
index	O
name	O

If	O
you	O
mean	O
`	O
Type	O
`	O
there	O
,	O
it's	O
not	O
actually	O
`	O
df.index.name	O
`	O
but	O
instead	O
`	O
df.columns	O
`	O
is	O
hierarchical	O
and	O
has	O
the	O
name	O
`	O
Type	O
`	O
.	O

I	O
edited	O
in	O
how	O
to	O
get	O
rid	O
of	O
that	O
.	O

Thanks	O
figured	O
it	O
out	O
...	O
forgot	O
to	O
edit	O
..	O
but	O
it	O
seems	O
efficient	O
enough	O
..	O
accepting	O
the	O
answer	O
:)	O

I	O
cooked	O
up	O
my	O
own	O
pivot	B-API
based	O
solution	O
to	O
the	O
same	O
problem	O
before	O
finding	O
Dougal's	O
answer	O
,	O
thought	O
I	O
would	O
post	O
it	O
for	O
posterity	O
since	O
I	O
find	O
it	O
more	O
readable	O
:	O
#CODE	O

And	O
then	O
carry	O
on	O
with	O
Dougal's	O
cleanups	O
:	O
#CODE	O

Note	O
that	O
`	O
DataFrame.to_csv()	B-API
`	O
produces	O
your	O
requested	O
output	O
:	O
#CODE	O

Thanks	O
for	O
the	O
alternate	O
solution	O
!	O

convert	O
ceilometer	O
output	O
to	O
python	O
dataframe	B-API

I'm	O
a	O
little	O
new	O
to	O
Python	O
and	O
openstack	O
ceilometer	O
.	O

I'm	O
reading	O
ceilometer	O
data	O
using	O
the	O
following	O
code	O
:	O
#CODE	O

(	O
Please	O
see	O
picture	O
of	O
output	O
attached	O
)	O

Does	O
anyone	O
know	O
how	O
i	O
could	O
convert	O
this	O
into	O
a	O
dataframe	B-API
?	O

I	O
tried	O
:	O
`	O
ls2	O
=	O
pandas.DataFrame	B-API
(	O
ls	O
,	O
columns	O
=[	O
"	O
user_id	O
"	O
,	O
"	O
name	O
"	O
,	O
"	O
resource_id	O
"	O
,	O
"	O
source	O
"	O
,	O
"	O
meter_id	O
"	O
,	O
"	O
project_id	O
"	O
,	O
"	O
type	O
"	O
,	O
"	O
unit	O
"])`	O

but	O
get	O
the	O
following	O
error	O
:	O
#CODE	O

if	O
someone	O
could	O
help	O
it	O
would	O
really	O
be	O
much	O
much	O
appreciated	O
..	O

Thank	O
you	O

Best	O
Wishes	O

T	O

If	O
the	O
<	O
Meter	O
part	O
was	O
not	O
there	O
(	O
i.e.	O
if	O
you	O
had	O
a	O
list	O
of	O
dicts	O
)	O
,	O
your	O
code	O
would	O
work	O
.	O

I	O
tried	O
removing	O
it	O
with	O
ls	O
[	O
ls.index	O
(	O
"	O
[	O
<	O
Meter	O
")]	O
=	O
"'"	O
but	O
it	O
throws	O
and	O
error	O
:	O

what	O
is	O
type	O
(	O
ls	O
[	O
0	O
])	O
?	O

Also	O
,	O
what	O
happens	O
when	O
you	O
do	O
ls2	O
=	O
pandas.DataFrame	B-API
(	O
ls	O
)	O
?	O

(	O
This	O
will	O
give	O
you	O
an	O
insight	O
as	O
to	O
why	O
you	O
got	O
that	O
ValueError	O
)	O

it's	O
a	O
list	O
type	O
;	O
it	O
returns	O
this	O
when	O
i	O
try	O
to	O
convert	O
to	O
pandas.dataframe	B-API
:	O
0	O

0	O
<	O
Meter	O
{	O
u'user_id	O
'	O
:	O
u'f82bcc547ffd4bf0ae28c452	O
...	O

1	O
<	O
Meter	O
{	O
u'user_id	O
'	O
:	O
u'f82bcc547ffd4bf0ae28c452	O
...	O

not	O
sure	O
what	O
it	O
means	O
?	O

I'm	O
a	O
bit	O
busy	O
atm	O
but	O
it's	O
possible	O
to	O
use	O
`	O
DataFrame.from_items	B-API
`	O
with	O
a	O
generator	O
.	O

Will	O
post	O
an	O
answer	O
soon	O

With	O
the	O
help	O
of	O
a	O
colleague	O
we	O
managed	O
to	O
find	O
the	O
solution	O
.	O

I'm	O
posting	O
it	O
in	O
case	O
it's	O
useful	O
to	O
anyone	O
trying	O
to	O
convert	O
ceilometer	O
data	O
into	O
a	O
dataframe	B-API
.	O

#CODE	O

Replaced	O
characters	O
which	O
make	O
it	O
as	O
invalid	O
dictionary	O
and	O
converted	O
into	O
dataframe	B-API

#CODE	O
