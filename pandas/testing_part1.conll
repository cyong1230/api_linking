Combine	O
Columns	O
Pandas	O

I	O
have	O
problem	O
combining	O
two	O
pandas	O
dataframe	B-API
columns	O
.	O

I	O
have	O
tried	O
#CODE	O

but	O
gives	O
me	O
error	O
:	O
#CODE	O

My	O
Data	O
looks	O
like	O
this	O
:	O
#CODE	O

Note	O
that	O
I've	O
converted	O
everything	O
to	O
float	O
when	O
reading	O
in	O
the	O
data	O
.	O

I	O
am	O
reading	O
in	O
line	O
by	O
line	O
using	O
linecache.getline	O
which	O
returns	O
an	O
entire	O
string	O
of	O
each	O
line	O
.	O

I	O
then	O
use	O
`	O
.split	B-API
(	O
'	O
,	O
')`	O
to	O
fix	O
this	O
.	O

But	O
afterwards	O
can't	O
convert	O
to	O
datetime	O
.	O

Do	O
i	O
need	O
to	O
convert	O
the	O
dates	O
in	O
to	O
integers	O
?	O

Thanks	O

Is	O
there	O
a	O
reason	O
you're	O
reading	O
the	O
lines	O
in	O
like	O
that	O
?	O
with	O
`	O
read_csv()	B-API
`	O
you	O
can	O
pass	O
`	O
parse_dates	O
=[[	O
'	O
Date	O
'	O
,	O
'	O
Time	O
']]`	O
and	O
the	O
two	O
columns	O
will	O
be	O
combined	O
into	O
a	O
single	O
Datetime	O
column	O
.	O

You	O
may	O
need	O
to	O
write	O
a	O
custom	O
`	O
date_parser	O
`	O
function	O
.	O

I've	O
got	O
a	O
massive	O
csv	O
file	O
but	O
I	O
don't	O
need	O
all	O
the	O
lines	O
...	O

I	O
think	O
you	O
can	O
do	O
this	O
:	O
#CODE	O

If	O
your	O
`'	O
Date	O
'`	O
and	O
`'	O
Time	O
'`	O
are	O
both	O
`	O
float64	O
`	O
,	O
you	O
need	O
these	O
two	O
lines	O
first	O
:	O
#CODE	O

Pandas.DataFrame.read_sql	O
to	O
read	O
as	O
float32	O
instead	O
of	O
float64	O

I'm	O
using	O
`	O
pd.DataFrame.read_sql	O
`	O
to	O
retrieve	O
a	O
large	O
number	O
of	O
records	O
from	O
a	O
SQL	O
database	O
causing	O
a	O
huge	O
amount	O
of	O
memory	O
to	O
be	O
used	O
.	O

I	O
noticed	O
that	O
the	O
`	O
dtype	B-API
`	O
of	O
the	O
numerical	O
values	O
are	O
`	O
float64	O
`	O
,	O
but	O
I	O
only	O
require	O
`	O
float32	O
`	O
dtypes	B-API
on	O
most	O
of	O
the	O
numerical	O
values	O
.	O

Is	O
it	O
possible	O
to	O
retrieve	O
values	O
via	O
`	O
read_sql	B-API
`	O
as	O
`	O
float32	O
`	O
?	O

No	O
#URL	O
...	O

:/	O

You	O
could	O
convert	O
it	O
once	O
read	O
using	O
`	O
.astype	B-API
(	O
'	O
int32	O
')`	O

Eigenvalues	O
of	O
masked	O
array	O
(	O
NumPy	O
)	O

How	O
can	O
I	O
compute	O
the	O
eigenvalues	O
and	O
eigenvectors	O
of	O
a	O
masked	O
NumPy	O
array	O
(	O
for	O
an	O
unmasked	O
array	O
,	O
this	O
can	O
be	O
achieved	O
by	O
`	O
scipy.linalg.eig	O
`)	O
.	O

Edit	O
:	O

Turns	O
out	O
you	O
can	O
do	O
this	O
after	O
all	O
.	O

#CODE	O

that's	O
not	O
defined	O
--	O
what	O
is	O
the	O
shape	O
of	O
your	O
masked	O
array	O
??	O

If	O
you	O
have	O
answered	O
your	O
own	O
question	O
you	O
should	O
submit	O
it	O
as	O
an	O
answer	O
,	O
rather	O
than	O
edit	O
the	O
question	O
.	O

Create	O
subset	O
table	O
from	O
Pandas	O
dataframe	B-API
with	O
hierarchical	O
index	O
in	O
a	O
loop	O

I	O
want	O
to	O
iterate	O
through	O
a	O
hierarchical	O
index	O
panda	O
dataframe	B-API
and	O
print	O
a	O
subsets	O
based	O
on	O
"	O
group1	O
"	O
level	O
.	O

here	O
is	O
my	O
dataframe	B-API
#CODE	O

This	O
is	O
what	O
it	O
looks	O
like	O
:	O
#CODE	O

I	O
can	O
successfully	O
print	O
a	O
SINGLE	O
subset	O
based	O
on	O
group1	O
like	O
this	O
:	O
#CODE	O

But	O
how	O
can	O
I	O
put	O
this	O
in	O
a	O
loop	O
to	O
print	O
a	O
subset	O
for	O
each	O
group	O
of	O
elements	O
in	O
'	O
group1	O
'	O
?	O

My	O
failed	O
attempt	O
:	O
#CODE	O

This	O
printed	O
a	O
subset	O
for	O
each	O
index	O
instead	O
of	O
grouping	O
them	O
together	O
(	O
hope	O
that	O
makes	O
sense	O
)	O
.	O

Use	O
`	O
get_level_values	B-API
`	O
on	O
the	O
index	O
to	O
return	O
just	O
those	O
index	O
values	O
at	O
that	O
level	O
,	O
additionally	O
call	O
`	O
unique	B-API
`	O
as	O
what	O
is	O
returned	O
are	O
duplicate	O
index	O
values	O
for	O
each	O
sublevel	O
row	O
.	O

#CODE	O

Perfect	O
!	O

thanks	O
for	O
explaining	O
it	O
.	O

@USER	O
no	O
worries	O
,	O
you	O
can	O
upvote	O
too	O
;)	O

This	O
is	O
what	O
`	O
groupby	B-API
`	O
is	O
for	O
(	O
iteration	O
gives	O
your	O
the	O
groupname	O
,	O
group	O
):	O
#CODE	O

Most	O
of	O
the	O
time	O
you	O
actually	O
want	O
to	O
do	O
something	O
with	O
it	O
though	O
,	O
e.g.	O
#CODE	O

Ahh	O
right	O
,	O
thanks	O
a	O
lot	O
for	O
this	O
info	O
.	O

3D-plot	O
of	O
the	O
error	O
function	O
in	O
a	O
linear	O
regression	O

I	O
would	O
like	O
to	O
visually	O
plot	O
a	O
3D	O
graph	O
of	O
the	O
error	O
function	O
calculated	O
for	O
a	O
given	O
slope	O
and	O
y-intercept	O
for	O
a	O
linear	O
regression	O
.	O

This	O
graph	O
will	O
be	O
used	O
to	O
illustrate	O
a	O
gradient	O
descent	O
application	O
.	O

Let	O
s	O
suppose	O
we	O
want	O
to	O
model	O
a	O
set	O
of	O
points	O
with	O
a	O
line	O
.	O

To	O
do	O
this	O
we	O
ll	O
use	O
the	O
standard	O
y=mx+b	O
line	O
equation	O
where	O
m	O
is	O
the	O
line	O
s	O
slope	O
and	O
b	O
is	O
the	O
line	O
s	O
y-intercept	O
.	O

To	O
find	O
the	O
best	O
line	O
for	O
our	O
data	O
,	O
we	O
need	O
to	O
find	O
the	O
best	O
set	O
of	O
slope	O
m	O
and	O
y-intercept	O
b	O
values	O
.	O

A	O
standard	O
approach	O
to	O
solving	O
this	O
type	O
of	O
problem	O
is	O
to	O
define	O
an	O
error	O
function	O
(	O
also	O
called	O
a	O
cost	O
function	O
)	O
that	O
measures	O
how	O
good	O
a	O
given	O
line	O
is	O
.	O

This	O
function	O
will	O
take	O
in	O
a	O
(	O
m	O
,	O
b	O
)	O
pair	O
and	O
return	O
an	O
error	O
value	O
based	O
on	O
how	O
well	O
the	O
line	O
fits	O
the	O
data	O
.	O

To	O
compute	O
this	O
error	O
for	O
a	O
given	O
line	O
,	O
we	O
ll	O
iterate	O
through	O
each	O
(	O
x	O
,	O
y	O
)	O
point	O
in	O
the	O
data	O
set	O
and	O
sum	O
the	O
square	O
distances	O
between	O
each	O
point	O
s	O
y	O
value	O
and	O
the	O
candidate	O
line	O
s	O
y	O
value	O
(	O
computed	O
at	O
mx+b	O
)	O
.	O

It	O
s	O
conventional	O
to	O
square	O
this	O
distance	O
to	O
ensure	O
that	O
it	O
is	O
positive	O
and	O
to	O
make	O
our	O
error	O
function	O
differentiable	O
.	O

In	O
python	O
,	O
computing	O
the	O
error	O
for	O
a	O
given	O
line	O
will	O
look	O
like	O
:	O
#CODE	O

Since	O
the	O
error	O
function	O
consists	O
of	O
two	O
parameters	O
(	O
m	O
and	O
b	O
)	O
we	O
can	O
visualize	O
it	O
as	O
a	O
two-dimensional	O
surface	O
.	O

Now	O
my	O
question	O
,	O
how	O
can	O
we	O
plot	O
such	O
3D-graph	O
using	O
python	O
?	O

Here	O
is	O
a	O
skeleton	O
code	O
to	O
build	O
a	O
3D	O
plot	O
.	O

This	O
code	O
snippet	O
is	O
totally	O
out	O
of	O
the	O
question	O
context	O
but	O
it	O
show	O
the	O
basics	O
for	O
building	O
a	O
3D	O
plot	O
.	O

For	O
my	O
example	O
i	O
would	O
need	O
the	O
x-axis	O
being	O
the	O
slope	O
,	O
the	O
y-axis	O
being	O
the	O
y-intercept	O
and	O
the	O
z-axis	O
,	O
the	O
error	O
.	O

Can	O
someone	O
help	O
me	O
build	O
such	O
example	O
of	O
graph	O
?	O

#CODE	O

The	O
above	O
code	O
produce	O
the	O
following	O
plot	O
,	O
which	O
is	O
very	O
similar	O
to	O
what	O
i	O
am	O
looking	O
for	O
.	O

@USER	O
no	O
,	O
i	O
removed	O
the	O
R	O
tag	O
,	O
I	O
didn't	O
noticed	O
it	O
was	O
added	O
;	O
thanks	O

Simply	O
replace	O
`	O
fun	O
`	O
with	O
`	O
computeErrorForLineGivenPoints	O
`	O
:	O
#CODE	O

yields	O

Tip	O
:	O
I	O
renamed	O
`	O
computeErrorForLineGivenPoints	O
`	O
as	O
`	O
error	O
`	O
.	O

Generally	O
,	O
there	O
is	O
no	O
need	O
to	O
name	O
a	O
function	O
`	O
compute	O
...	O

`	O
since	O
almost	O
all	O
functions	O
compute	O
something	O
.	O

You	O
also	O
do	O
not	O
need	O
to	O
specify	O
"	O
GivenPoints	O
"	O
since	O
the	O
function	O
signature	O
shows	O
that	O
`	O
points	O
`	O
is	O
an	O
argument	O
.	O

If	O
you	O
have	O
other	O
error	O
functions	O
or	O
variables	O
in	O
your	O
program	O
,	O
`	O
line_error	O
`	O
or	O
`	O
total_error	O
`	O
might	O
be	O
a	O
better	O
name	O
for	O
this	O
function	O
.	O

Thanks	O
@USER	O
This	O
is	O
exactly	O
the	O
implementation	O
I	O
am	O
looking	O
for	O
.	O

I	O
will	O
change	O
the	O
function	O
signature	O
as	O
you	O
suggest	O
.	O

Thanks	O
!	O

Pandas	O
datetime	O
index	O
all	O
set	O
to	O
the	O
same	O
date	O
in	O
a	O
month	O
(	O
1st	O
)	O

I	O
have	O
a	O
dataframe	B-API
like	O
:	O
#CODE	O

I	O
want	O
all	O
the	O
dates	O
to	O
be	O
the	O
1st	O
of	O
the	O
month	O
like	O
:	O
#CODE	O

I	O
know	O
this	O
has	O
to	O
be	O
really	O
simple	O
,	O
but	O
I'm	O
new	O
at	O
pandas	O
/	O
python	O
and	O
looked	O
for	O
over	O
an	O
hour	O
at	O
this	O
point	O
.	O

Thanks	O
in	O
advance	O
.	O

Is	O
the	O
`	O
Date	B-API
`	O
the	O
`	O
index	B-API
`	O
,	O
or	O
a	O
`	O
column	O
'	O
?	O

You	O
may	O
just	O
use	O
simple	O
list	O
comprehension	O
to	O
"	O
convert	O
"	O
and	O
re-assign	O
your	O
index	O
:	O
#CODE	O

Repeat	O
data	O
frame	O
,	O
with	O
varying	O
column	O
value	O

I	O
have	O
the	O
following	O
data	O
frame	O
and	O
need	O
to	O
repeat	O
the	O
values	O
for	O
a	O
set	O
of	O
values	O
.	O

That	O
is	O
,	O
given	O
#CODE	O

I	O
need	O
to	O
do	O
something	O
like	O
this	O
,	O
but	O
more	O
performant	O
:	O
#CODE	O

That	O
is	O
,	O
the	O
expected	O
output	O
is	O
:	O
#CODE	O

This	O
looks	O
like	O
it	O
is	O
working	O
to	O
me	O
.	O

What	O
exactly	O
are	O
you	O
expecting	O
to	O
be	O
different	O
?	O

@USER	O
something	O
more	O
efficient	O
,	O
perhaps	O
vectorized	O
.	O

Plot	O
bar	O
graph	O
using	O
multiple	O
groupby	B-API
count	O
in	O
panda	O

I	O
am	O
trying	O
to	O
plot	O
bar	O
graph	O
using	O
pandas	O
.	O

DataTime	O
is	O
index	O
column	O
which	O
I	O
get	O
from	O
timestamp	O
.	O

Here	O
is	O
table	O
structure	O
:	O

So	O
far	O
i	O
have	O
written	O
this	O
:	O
#CODE	O

I	O
want	O
output	O
like	O
this	O
:	O

June	O
has	O
total	O
4	O
entry	O
and	O
one	O
entry	O
starts	O
with	O
u	O
.	O

so	O
X	O
has	O
4	O
y	O
has	O
1	O
.	O

Same	O
for	O
July	O
.	O

Also	O
i	O
want	O
to	O
plot	O
bar	O
graph	O
(	O
X	O
and	O
Y	O
entries	O
)	O
using	O
output	O
.	O

I	O
want	O
MONTH	O
vs	O
Values	O
bar	O
graph	O

This	O
is	O
a	O
near-exact	O
duplicate	O
of	O
#URL	O

Please	O
write	O
your	O
questions	O
so	O
they	O
make	O
sense	O
without	O
reference	O
to	O
links	O
to	O
external	O
sites	O
.	O

Questions	O
on	O
SO	O
should	O
be	O
helpful	O
to	O
future	O
readers	O
,	O
and	O
if	O
those	O
links	O
die	O
your	O
question	O
will	O
be	O
very	O
hard	O
to	O
understand	O
.	O

Also	O
,	O
please	O
do	O
not	O
post	O
links	O
to	O
images	O
of	O
text	O
(	O
or	O
embed	O
direct	O
images	O
of	O
text	O
)	O
-	O
post	O
the	O
text	O
itself	O
,	O
perhaps	O
in	O
a	O
Quote	O
block	O
,	O
or	O
in	O
a	O
Code	O
block	O
if	O
you	O
need	O
to	O
preserve	O
formatting	O
.	O

I	O
would	O
created	O
the	O
DataFrame	B-API
with	O
a	O
dict	O
:	O
#CODE	O

Now	O
you	O
can	O
use	O
the	O
plot	O
method	O
to	O
plot	O
months	O
vs	O
values	O
.	O

#CODE	O

Note	O
:	O
you	O
can	O
create	O
grouped	O
more	O
efficiently	O
:	O
#CODE	O

I	O
am	O
getting	O
an	O
error	O
when	O
i	O
use	O
your	O
groupby	B-API
code	O
.	O

This	O
is	O
error	O
"'	O
Series	B-API
'	O
object	O
has	O
no	O
attribute	O
'	O
dt	B-API
'"	O
.	O

This	O
is	O
sample	O
data	O
on	O
my	O
df	O
[	O
'	O
DateTime	O
']	O
=>	O
2013-06-25	O

@USER	O
you	O
need	O
to	O
ensure	O
your	O
Series	B-API
is	O
of	O
datetime64	O
type	O
,	O
rather	O
than	O
-	O
say	O
-	O
strings	O
...	O

e.g.	O
using	O
df	O
[	O
'	O
DateTime	O
']	O
=	O
pd.to_datetime	B-API
(	O
df	O
[	O
'	O
DateTime	O
'])	O
.	O

Also	O
ensure	O
you're	O
using	O
a	O
fairly	O
recent	O
version	O
of	O
pandas	O
>	O
=	O
0.15	O
.	O

I	O
am	O
using	O
panda	O
version	O
#	O
0.14.1	O
.	O

my	O
df	O
[	O
'	O
DateTime	O
']	O
.dtype	B-API
is	O
object	O
.	O

It	O
should	O
be	O
datetime64	O
as	O
per	O
your	O
last	O
comment	O
.	O

I	O
tried	O
to	O
convert	O
it	O
but	O
i	O
could	O
not	O
solve	O
the	O
problem	O
.	O

I	O
am	O
using	O
anaconda	O
with	O
2.7	O
python	O
version	O

I	O
use	O
pf	O
[	O
'	O
DateTime	O
']	O
=	O
pf	O
[	O
'	O
DATE	O
']	O
.apply	B-API
(	O
lambda	O
x	O
:	O
dt.date.fromtimestamp	O
(	O
x	O
))	O
to	O
convert	O
timestamp	O
to	O
date	O
.	O

Also	O
to	O
groupby	B-API
i	O
use	O
this	O
df1	O
[	O
'	O
DateTime	O
']	O
.groupby	B-API
(	O
lambda	O
x	O
:	O
x.month	O
)	O
.	O

In	O
my	O
final	O
output	O
i	O
want	O
Month-Year	O
on	O
x	O
axis	O
.	O

@USER	O
use	O
`	O
pd.to_datetime	B-API
`	O
rather	O
than	O
apply	B-API
.	O

If	O
you	O
use	O
the	O
apply	B-API
it	O
may	O
not	O
create	O
a	O
Datetime	O
column	O
.	O

I	O
think	O
you	O
may	O
have	O
to	O
update	O
to	O
0.15.X	O
for	O
the	O
dt	B-API
accessor	O
.	O

If	O
you	O
want	O
month-year	O
then	O
use	O
the	O
`	O
to_period	B-API
`	O
part	O
of	O
the	O
answer	O
above	O
?	O

result.plot()	O
does	O
not	O
short	O
the	O
month	O
and	O
year	O
together	O
.	O

it	O
short	O
by	O
only	O
month	O
,	O
thats	O
why	O
i	O
get	O
plot	O
for	O
Jan	O
2014	O
Jan	O
2015	O
together	O
.	O

how	O
to	O
short	O
result	O
data	O
frame	O

@USER	O
you	O
should	O
ask	O
that	O
as	O
a	O
new	O
question	O
:)	O

#URL	O

Pandas	O
:	O
Merge	B-API
or	O
join	O
dataframes	O
based	O
on	O
column	O
data	O
?	O

I	O
am	O
trying	O
to	O
add	O
several	O
columns	O
of	O
data	O
to	O
an	O
existing	O
dataframe	B-API
.	O

The	O
dataframe	B-API
itself	O
was	O
built	O
from	O
a	O
number	O
of	O
other	O
dataframes	O
,	O
which	O
I	O
successfully	O
joined	O
on	O
indices	O
,	O
which	O
were	O
identical	O
.	O

For	O
that	O
,	O
I	O
used	O
code	O
like	O
this	O
:	O
#CODE	O

I	O
actually	O
joined	O
these	O
on	O
a	O
multi-index	O
,	O
so	O
the	O
dataframe	B-API
looks	O
something	O
like	O
the	O
following	O
,	O
where	O
Name1	O
and	O
Name	O
2	O
are	O
indices	O
:	O
#CODE	O

So	O
the	O
Name1	O
index	O
does	O
not	O
repeat	O
data	O
,	O
but	O
the	O
Name2	O
index	O
does	O
(	O
I'm	O
using	O
this	O
to	O
keep	O
track	O
of	O
dyads	O
,	O
so	O
that	O
Name1	O
Name2	O
together	O
are	O
only	O
represented	O
once	O
)	O
.	O

What	O
I	O
now	O
want	O
to	O
add	O
are	O
4	O
columns	O
of	O
data	O
that	O
correspond	O
to	O
Name2	O
data	O
(	O
information	O
on	O
the	O
second	O
member	O
of	O
the	O
dyad	O
)	O
.	O

Unlike	O
the	O
"	O
present	O
"	O
"	O
r	O
"	O
and	O
"	O
behavior	O
"	O
data	O
,	O
these	O
data	O
are	O
per	O
individual	O
,	O
not	O
per	O
dyad	O
.	O

So	O
I	O
don't	O
need	O
to	O
consider	O
Name1	O
data	O
when	O
merging	O
.	O

The	O
problem	O
is	O
that	O
while	O
Name2	O
data	O
are	O
repeated	O
to	O
exhaust	O
the	O
dyad	O
combos	O
,	O
the	O
"	O
Name2	O
"	O
column	O
in	O
the	O
data	O
I	O
would	O
now	O
like	O
to	O
add	O
only	O
has	O
one	O
piece	O
of	O
data	O
per	O
Name2	O
individual	O
:	O
#CODE	O

What	O
I	O
would	O
like	O
the	O
output	O
to	O
look	O
like	O
:	O
#CODE	O

Despite	O
reading	O
the	O
documentation	O
,	O
I	O
am	O
not	O
clear	O
on	O
whether	O
I	O
can	O
use	O
join()	O
or	O
merge()	B-API
for	O
the	O
desired	O
outcome	O
.	O

If	O
I	O
try	O
a	O
join	O
to	O
the	O
existing	O
dataframe	B-API
like	O
the	O
simple	O
one	O
I've	O
used	O
previously	O
,	O
I	O
end	O
up	O
with	O
the	O
new	O
columns	O
but	O
they	O
are	O
full	O
of	O
NaN	O
values	O
.	O

I've	O
also	O
tried	O
various	O
combinations	O
using	O
Name1	O
and	O
Name2	O
as	O
either	O
columns	O
or	O
as	O
indices	O
,	O
with	O
either	O
join	O
or	O
merge	B-API
(	O
not	O
as	O
random	O
as	O
it	O
sounds	O
,	O
but	O
I'm	O
clearly	O
not	O
interpreting	O
the	O
documentation	O
correctly	O
!	O
)	O
.	O

Your	O
help	O
would	O
be	O
very	O
much	O
appreciated	O
,	O
as	O
I	O
am	O
presently	O
very	O
much	O
lost	O
.	O

I'm	O
not	O
sure	O
if	O
this	O
is	O
the	O
best	O
way	O
,	O
but	O
you	O
could	O
use	O
`	O
reset_index	B-API
`	O
to	O
temporarily	O
make	O
your	O
original	O
DataFrame	B-API
indexed	O
by	O
`	O
Name2	O
`	O
only	O
.	O

Then	O
you	O
could	O
perform	O
the	O
`	O
join	B-API
`	O
as	O
usual	O
.	O

Then	O
use	O
`	O
set_index	B-API
`	O
to	O
again	O
make	O
`	O
Name1	O
`	O
part	O
of	O
the	O
MultiIndex	O
:	O
#CODE	O

To	O
make	O
the	O
result	O
look	O
even	O
more	O
like	O
your	O
desired	O
DataFrame	B-API
,	O
you	O
could	O
reorder	O
and	O
sort	O
the	O
index	O
:	O
#CODE	O

This	O
works	O
-	O
I	O
must	O
have	O
been	O
setting	O
the	O
wrong	O
parameters	O
as	O
this	O
is	O
one	O
of	O
the	O
approaches	O
I	O
attempted	O
.	O

Good	O
to	O
know	O
I'm	O
not	O
wandering	O
in	O
the	O
entirely	O
wrong	O
direction	O
.	O

)	O

Export	O
Pandas	O
data	O
frame	O
with	O
text	O
column	O
containg	O
utf-8	O
text	O
and	O
URLs	O
to	O
Excel	O

My	O
Pandas	O
data	O
frame	O
consists	O
of	O
Tweets	O
and	O
meta	O
data	O
of	O
each	O
tweet	O
(	O
300.000	O
rows	O
)	O
.	O

Some	O
of	O
my	O
colleagues	O
need	O
to	O
work	O
with	O
this	O
data	O
in	O
Excel	O
which	O
is	O
why	O
I	O
need	O
to	O
export	O
it	O
.	O

I	O
wanted	O
to	O
use	O
either	O
`	O
.to_csv	B-API
`	O
or	O
`	O
.to_excel	B-API
`	O
which	O
are	O
both	O
provided	O
by	O
Pandas	O
but	O
I	O
can't	O
get	O
it	O
to	O
work	O
properly	O
.	O

When	O
I	O
use	O
`	O
.to_csv	B-API
`	O
my	O
problem	O
is	O
that	O
it	O
keeps	O
failing	O
in	O
the	O
text	O
part	O
of	O
the	O
data	O
frame	O
.	O

I've	O
played	O
around	O
with	O
different	O
separators	O
but	O
the	O
file	O
is	O
never	O
100%	O
aligned	O
.	O

The	O
text	O
column	O
seems	O
to	O
contain	O
tabs	O
,	O
pipe	O
characters	O
etc	O
.	O
which	O
confuses	O
Excel	O
.	O

#CODE	O

When	O
I	O
try	O
to	O
use	O
`	O
.to_excel	B-API
`	O
together	O
with	O
the	O
`	O
xlsxwriter	O
`	O
engine	O
I'm	O
confronted	O
with	O
a	O
different	O
problem	O
,	O
which	O
is	O
that	O
my	O
text	O
column	O
contains	O
to	O
many	O
URLs	O
(	O
I	O
think	O
)	O
.	O

`	O
xlswriter	O
`	O
tries	O
to	O
make	O
special	O
clickable	O
links	O
of	O
these	O
URLs	O
instead	O
of	O
just	O
handling	O
them	O
as	O
strings	O
.	O

I've	O
found	O
some	O
information	O
on	O
how	O
to	O
circumvent	O
this	O
but	O
,	O
again	O
,	O
I	O
can't	O
get	O
it	O
to	O
work	O
.	O

The	O
following	O
bit	O
of	O
code	O
should	O
be	O
used	O
to	O
disable	O
the	O
function	O
that	O
I	O
think	O
is	O
causing	O
trouble	O
:	O
#CODE	O

However	O
,	O
when	O
using	O
`	O
to_excel	B-API
`	O
I	O
can't	O
seem	O
to	O
adjust	O
this	O
setting	O
of	O
the	O
Workbook	O
object	O
before	O
I	O
load	O
the	O
data	O
frame	O
into	O
the	O
Excel	O
file	O
.	O

In	O
short	O
how	O
do	O
I	O
export	O
a	O
column	O
with	O
wildly	O
varying	O
text	O
from	O
a	O
Pandas	O
data	O
frame	O
to	O
something	O
that	O
Excel	O
understands	O
?	O

edit	O
:	O

example	O
:	O
#CODE	O

So	O
in	O
this	O
case	O
It	O
is	O
obviously	O
a	O
line	O
brake	O
that	O
is	O
my	O
data	O
.	O

I	O
will	O
try	O
to	O
find	O
some	O
more	O
examples	O
.	O

edit2	O
:	O
#CODE	O

Translation	O
of	O
Dutch	O
stuff	O
:	O

Errors	O
were	O
found	O
in	O
"	O
file	O
"	O
.	O

Here	O
follows	O
a	O
list	O
of	O
removed	O
records	O
:	O
removed	O
records	O
:	O
formula	O
of	O
the	O
part	O
/	O
xl	O
/	O
worksheets	O
/	O
sheet1.xml	O

can	O
you	O
give	O
us	O
a	O
sample	O
of	O
the	O
df	O
that	O
doesn't	O
work	O
?	O

@USER	O
I've	O
added	O
an	O
example	O
,	O
I	O
will	O
add	O
more	O
if	O
needed	O
.	O

I	O
don't	O
think	O
it	O
is	O
currently	O
possible	O
to	O
pass	O
XlsxWriter	O
constructor	O
options	O
via	O
the	O
Pandas	O
API	O
but	O
you	O
can	O
workaround	O
the	O
`	O
strings_to_url	O
`	O
issue	O
as	O
follows	O
:	O
#CODE	O

The	O
code	O
works	O
so	O
that	O
is	O
a	O
great	O
start	O
.	O

When	O
I	O
open	O
the	O
Excel	O
file	O
I	O
get	O
a	O
warning	O
that	O
it	O
needs	O
to	O
be	O
recovered	O
.	O

I've	O
put	O
the	O
log	O
file	O
in	O
my	O
post	O
for	O
better	O
formatting	O
.	O

You	O
get	O
a	O
warning	O
when	O
you	O
open	O
the	O
file	O
created	O
by	O
my	O
example	O
?	O

Yes	O
,	O
that	O
is	O
what	O
I	O
mean	O
.	O

I	O
don't	O
see	O
any	O
warning	O
when	O
I	O
open	O
the	O
file	O
and	O
there	O
really	O
isn't	O
any	O
reason	O
that	O
there	O
should	O
be	O
for	O
such	O
a	O
simple	O
test	O
case	O
.	O

Also	O
in	O
relation	O
to	O
the	O
warning	O
you	O
added	O
above	O
there	O
isn't	O
any	O
"	O
formula	O
"	O
part	O
in	O
`	O
/	O
xl	O
/	O
worksheets	O
/	O
sheet1.xml	O
`	O
.	O

Did	O
you	O
add	O
a	O
formula	O
or	O
other	O
data	O
to	O
the	O
dataframe	B-API
in	O
the	O
example	O
?	O

Well	O
I	O
was	O
testing	O
it	O
now	O
on	O
my	O
full	O
data	O
frame	O
.	O

I've	O
not	O
added	O
any	O
formulas	O
.	O

To	O
me	O
the	O
log	O
warning	O
seems	O
empty	O
.	O

Apparently	O
it	O
is	O
a	O
known	O
issue	O
/	O
bug	O
:	O
#URL	O
I	O
will	O
look	O
into	O
it	O
further	O
.	O

You	O
have	O
been	O
a	O
great	O
help	O
!!	O

Perhaps	O
there	O
is	O
some	O
data	O
in	O
the	O
dataframe	B-API
that	O
is	O
being	O
interpreted	O
as	O
a	O
formula	O
.	O

Try	O
adding	O
the	O
following	O
at	O
the	O
same	O
point	O
as	O
the	O
other	O
option	O
to	O
see	O
if	O
it	O
makes	O
a	O
difference	O
:	O
`	O
writer.book.strings_to_formulas	O
=	O
False	O
`	O
.	O

That	O
did	O
the	O
trick	O
!	O

SWEET	O
!	O

Running	O
alternative	O
version	O
of	O
Python	O
on	O
RedHat	O
6	O
produces	O
"	O
wrong	O
ELF	O
class	O
:	O
ELFCLASS64	O
"	O

I	O
inherited	O
some	O
python	O
code	O
that	O
I	O
need	O
to	O
modify	O
.	O

I	O
installed	O
python	O
2.7.10	O
on	O
my	O
RedHat	O
6	O
machine	O
and	O
ran	O
the	O
original	O
code	O
which	O
produced	O
results	O
different	O
from	O
those	O
that	O
were	O
generated	O
last	O
time	O
it	O
was	O
run	O
using	O
an	O
older	O
version	O
of	O
python	O
.	O

I	O
was	O
given	O
a	O
path	O
to	O
an	O
old	O
python	O
installation	O
and	O
tried	O
to	O
run	O
the	O
code	O
using	O
it	O
.	O

Got	O
errors	O
.	O

Here's	O
what	O
I've	O
done	O
so	O
far	O
:	O

This	O
is	O
the	O
path	O
to	O
the	O
old	O
python	O
install	O
(	O
2.7.8	O
):	O
#CODE	O

Tried	O
running	O
the	O
code	O
and	O
got	O
the	O
first	O
error	O
:	O
#CODE	O

Searched	O
for	O
`	O
libstdc++	O
.so	O
.6	O
`	O
and	O
found	O
it	O
in	O
`	O
/	O
usr	O
/	O
lib64	O
`	O
:	O
#CODE	O

So	O
I	O
modified	O
`	O
LD_LIBRARY_PATH	O
`	O
as	O
was	O
suggested	O
in	O
another	O
SO	O
post	O
:	O
#CODE	O

Tried	O
running	O
again	O
and	O
got	O
another	O
error	O
:	O
#CODE	O

How	O
can	O
I	O
get	O
rid	O
of	O
this	O
error	O
and	O
,	O
ultimately	O
,	O
be	O
able	O
to	O
run	O
the	O
old	O
python	O
version	O
with	O
`	O
pandas	O
`	O
?	O

Whatever	O
I	O
do	O
to	O
achieve	O
this	O
,	O
however	O
,	O
I	O
don't	O
want	O
to	O
mess	O
up	O
the	O
environment	O
for	O
the	O
current	O
version	O
of	O
python	O
I	O
am	O
using	O
(	O
which	O
,	O
in	O
case	O
this	O
is	O
relevant	O
,	O
was	O
installed	O
into	O
a	O
local	O
subdirectory	O
of	O
my	O
home	O
dir	O
using	O
`	O
miniconda	O
`)	O

The	O
version	O
of	O
python	O
you're	O
trying	O
to	O
use	O
wasn't	O
compiled	O
for	O
64	O
bit	O
,	O
so	O
it	O
isn't	O
linked	O
against	O
a	O
64	O
bit	O
`	O
libstdc++	O
`	O
.	O

The	O
proper	O
way	O
to	O
run	O
the	O
older	O
version	O
of	O
python	O
would	O
be	O
doing	O
an	O
install	O
on	O
the	O
machine	O
.	O

I'm	O
not	O
sure	O
how	O
Redhat	O
handles	O
multiple	O
package	O
versions	O
,	O
but	O
I'd	O
be	O
surprised	O
if	O
it	O
didn't	O
support	O
it	O
(	O
most	O
distributions	O
do	O
)	O
.	O

Other	O
than	O
that	O
,	O
you	O
would	O
need	O
to	O
find	O
a	O
host	O
that	O
the	O
older	O
version	O
is	O
installed	O
on	O
.	O

UnboundLocalError	O
in	O
ggplot	O
0.5	O

I	O
have	O
the	O
following	O
code	O
#CODE	O

and	O
I	O
get	O
the	O
following	O
error	O
:	O
#CODE	O

some	O
sample	O
data	O
#CODE	O

Hm	O
.	O

Can	O
you	O
try	O
:	O

```	O

import	O
ggplot	O

print	O
ggplot.__version__	O

```	O

And	O
perhaps	O
pasting	O
a	O
few	O
rows	O
of	O
your	O
data	O
would	O
also	O
help	O
reproduce	O
the	O
issue	O
you're	O
seeing	O
too	O
.	O

ggplot	O
version	O
0.5.0	O

It's	O
done	O
above	O
.	O

thanks	O

there's	O
a	O
space	O
in	O
your	O
x	O
variable	O
name	O
in	O
`	O
aes	O
`	O
but	O
not	O
in	O
your	O
data	O
frame	O
?	O

'	O
DHT	O
temp	O
'	O
vs	O
'	O
DHTtemp	O
'	O
.	O

Is	O
that	O
it	O
?	O

I	O
am	O
using	O
`	O
x	O
`	O
and	O
`	O
y	O
`	O
as	O
axis	O
indicators	O
.	O

Can	O
you	O
please	O
store	O
the	O
final	O
ggplot	O
object	O
(	O
`	O
p	O
=	O
p	O
+	O
...	O
`)	O
and	O
run	O
`	O
p.draw()	O
`	O
on	O
it	O
.	O

This	O
should	O
result	O
in	O
a	O
complete	O
stacktrace	O
insteed	O
of	O
only	O
the	O
error	O
message	O
.	O

I	O
think	O
this	O
is	O
a	O
bug	O
in	O
gpplots	O
code	O
,	O
so	O
opening	O
an	O
issue	O
at	O
#URL	O
would	O
be	O
nice	O
.	O

:-)	O

Great	O
.	O

Thx	O
for	O
posting	O
some	O
data	O
.	O

Unfortunately	O
,	O
I'm	O
not	O
able	O
to	O
recreate	O
the	O
issue	O
on	O
my	O
end	O
.	O

#CODE	O

The	O
plot	O
renders	O
for	O
me	O
without	O
the	O
exception	O
you're	O
seeing	O
.	O

Of	O
course	O
,	O
since	O
this	O
is	O
only	O
the	O
top	O
5	O
rows	O
,	O
the	O
plot	O
doesn't	O
really	O
describe	O
anything	O
interesting	O
.	O

Will	O
keep	O
digging	O
and	O
ask	O
a	O
few	O
people	O
on	O
my	O
team	O
if	O
they	O
know	O
what's	O
up	O
.	O

This	O
was	O
a	O
bug	O
in	O
ggplot	O
which	O
has	O
been	O
fixed	O
in	O
version	O
0.5.8	O
.	O

Pandas	O
for	O
Python	O
,	O
grouping	O

I	O
have	O
a	O
data	O
set	O
consisting	O
of	O
multiple	O
tuples	O
per	O
time	O
stamp	O
-	O
each	O
of	O
these	O
has	O
a	O
count	O
.	O

There	O
could	O
be	O
different	O
tuples	O
present	O
at	O
each	O
time	O
stamp	O
.	O

I	O
would	O
like	O
to	O
group	O
these	O
together	O
in	O
5	O
minute	O
bins	O
and	O
add	O
the	O
counts	O
for	O
each	O
unique	O
tuple	O
.	O

Is	O
there	O
a	O
nice	O
clean	O
way	O
to	O
do	O
this	O
using	O
Pandas	O
group-by	O
?	O

They	O
have	O
the	O
form	O
:	O

((	O
u'67	O
.163	O
.47	O
.231	O
'	O
,	O
u'8	O
.27	O
.82	O
.254	O
'	O
,	O
50186	O
,	O
80	O
,	O
6	O
,	O
1377565195000	O
)	O
,	O
2	O
)	O

This	O
is	O
currently	O
a	O
list	O
,	O
with	O
a	O
6-tuple	O
(	O
last	O
entry	O
is	O
time-stamp	O
)	O
,	O
and	O
then	O
count	O
.	O

There	O
will	O
be	O
a	O
collection	O
of	O
5-tuples	O
for	O
every	O
time	O
stamp	O
:	O

(	O
5-tuple	O
)	O
,	O
t-time-stamp	O
,	O
count	O
,	O
for	O
example	O
(	O
for	O
just	O
one	O
time	O
stamp	O
)	O
#CODE	O

or	O
converted	O
:	O
#CODE	O

Here's	O
what	O
seems	O
to	O
work	O
well	O
:	O
#CODE	O

Can	O
you	O
give	O
a	O
short	O
example	O
data	O
of	O
the	O
whole	O
dataframe	B-API
?	O

[((	O
u'71	O
.57	O
.43	O
.240	O
'	O
,	O
u'8	O
.27	O
.82	O
.254	O
'	O
,	O
33108	O
,	O
80	O
,	O
6	O
,	O
1377565195000	O
)	O
,	O
1	O
)	O
,	O

((	O
u'67	O
.163	O
.47	O
.231	O
'	O
,	O
u'8	O
.27	O
.82	O
.254	O
'	O
,	O
50186	O
,	O
80	O
,	O
6	O
,	O
1377565195000	O
)	O
,	O
2	O
)	O
,	O

((	O
u'8	O
.27	O
.82	O
.254	O
'	O
,	O
u'98	O
.206	O
.29	O
.242	O
'	O
,	O
25159	O
,	O
80	O
,	O
6	O
,	O
1377565195000	O
)	O
,	O
1	O
)	O
,	O

((	O
u'69	O
.66	O
.156	O
.250	O
'	O
,	O
u'8	O
.27	O
.82	O
.254	O
'	O
,	O
59274	O
,	O
80	O
,	O
6	O
,	O
1377565195000	O
)	O
,	O
3	O
)	O
,	O

((	O
u'76	O
.16	O
.235	O
.239	O
'	O
,	O
u'8	O
.27	O
.84	O
.126	O
'	O
,	O
48104	O
,	O
80	O
,	O
6	O
,	O
1377565195000	O
)	O
,	O
1	O
)	O
,	O

((	O
u'8	O
.27	O
.84	O
.254	O
'	O
,	O
u'98	O
.226	O
.117	O
.227	O
'	O
,	O
63795	O
,	O
80	O
,	O
6	O
,	O
1377565195000	O
)	O
,	O
2	O
)	O
,	O

((	O
u'24	O
.1	O
.153	O
.243	O
'	O
,	O
u'8	O
.27	O
.82	O
.126	O
'	O
,	O
18970	O
,	O
80	O
,	O
6	O
,	O
1377565195000	O
)	O
,	O
1	O
)	O
,	O

((	O
u'76	O
.16	O
.101	O
.243	O
'	O
,	O
u'8	O
.27	O
.82	O
.126	O
'	O
,	O
41329	O
,	O
80	O
,	O
6	O
,	O
1377565195000	O
)	O
,	O
1	O
)	O
,	O

You	O
can	O
edit	O
you	O
question	O
to	O
include	O
it	O
instead	O
of	O
as	O
a	O
comment	O
.	O

Where	O
is	O
the	O
timestamp	O
in	O
this	O
example	O
?	O

last	O
entry	O
in	O
the	O
6-tuple	O

Do	O
you	O
already	O
put	O
it	O
in	O
a	O
pandas	O
DataFrame	B-API
?	O

How	O
does	O
that	O
look	O
like	O
(	O
is	O
the	O
tuple	O
one	O
column	O
)	O
?	O

I've	O
not	O
put	O
these	O
into	O
a	O
DataFrame	B-API
yet	O
.	O

Given	O
this	O
is	O
a	O
list	O
(	O
or	O
a	O
dictionary	O
if	O
I	O
don't	O
sort	O
on	O
the	O
time	O
stamp	O
)	O
.	O

But	O
the	O
intent	O
would	O
be	O
to	O
have	O
the	O
tuple	O
as	O
a	O
column	O
.	O

But	O
then	O
use	O
the	O
time-stamp	O
as	O
a	O
time-series	O
-	O
to	O
really	O
the	O
first	O
5-tuple	O
is	O
the	O
unique	O
key	O
,	O
then	O
the	O
time-stamp	O
,	O
and	O
the	O
count	O
.	O

How	O
is	O
the	O
timestamp	O
defined	O
?	O

How	O
can	O
it	O
be	O
converted	O
to	O
a	O
datetime	O
?	O

The	O
timestamp	O
is	O
time	O
since	O
the	O
UNIX	O
epoch	O
-	O
can	O
be	O
converted	O
to	O
datetime	O

In	O
[	O
226	O
]:	O
df	O

Out	O
[	O
226	O
]:	O

data1	O
data2	O
key1	O

0	O
1	O
1377565195000	O
(	O
71.57.43.240	O
,	O
8.27.82.254	O
,	O
33108	O
,	O
80	O
,	O
6	O
)	O

1	O
2	O
1377565195000	O
(	O
67.163.47.231	O
,	O
8.27.82.254	O
,	O
50186	O
,	O
80	O
,	O
6	O
)	O

Do	O
you	O
only	O
have	O
timestamps	O
every	O
5	O
mins	O
?	O

Or	O
do	O
you	O
have	O
to	O
combine	O
several	O
minutes	O
in	O
one	O
5	O
min	O
bins	O
?	O

The	O
data	O
needs	O
to	O
be	O
grouped	O
(	O
binned	O
)	O
into	O
5min	O
bins	O
.	O

There	O
can	O
be	O
multiple	O
(	O
100's	O
of	O
5-tuples	O
or	O
keys	O
)	O
per	O
time	O
step	O
-	O
and	O
these	O
can	O
change	O
from	O
time	O
step	O
to	O
time	O
step	O
-	O
for	O
each	O
unique	O
key	O
or	O
tuple	O
,	O
want	O
to	O
add	O
together	O
the	O
counts	O
(	O
data1	O
)	O

Can	O
you	O
give	O
a	O
more	O
elaborate	O
example	O
dataset	O
,	O
and	O
also	O
give	O
an	O
example	O
of	O
the	O
desired	O
output	O

Here	O
is	O
a	O
simpler	O
example	O
that	O
illustrates	O
the	O
idea	O
:	O

time	O
count	O
city	O

00:00	O
:	O
00	O
1	O
Montreal	O

00:00	O
:	O
00	O
2	O
New	O
York	O

00:00	O
:	O
00	O
1	O
Chicago	O

00:01	O
:	O
00	O
2	O
Montreal	O

00:01	O
:	O
00	O
3	O
New	O
York	O

after	O
bin-ing	O
into	O
5min	O
bins	O

time	O
count	O
city	O

00:05	O
:	O
00	O
3	O
Montreal	O

00:05	O
:	O
00	O
5	O
New	O
York	O

00:05	O
:	O
00	O
1	O
Chicago	O

time	O
count	O
city	O

00:00	O
:	O
00	O
1	O
Montreal	O

00:00	O
:	O
00	O
2	O
New	O
York	O

00:00	O
:	O
00	O
1	O
Chicago	O

00:01	O
:	O
00	O
2	O
Montreal	O

00:01	O
:	O
00	O
3	O
New	O
York	O

after	O
bin-ing	O

If	O
you	O
just	O
want	O
to	O
add	O
together	O
the	O
counts	O
for	O
each	O
unique	O
tuple	O
,	O
just	O
groupby	B-API
`	O
key1	O
`	O
:	O
#CODE	O

If	O
you	O
want	O
to	O
do	O
this	O
for	O
each	O
time	O
step	O
and	O
each	O
unique	O
tuple	O
,	O
you	O
can	O
give	O
multiple	O
column	O
to	O
group	O
by	O
:	O
#CODE	O

If	O
different	O
timesteps	O
have	O
to	O
be	O
combined	O
in	O
one	O
5min	O
bin	O
,	O
a	O
possible	O
approach	O
is	O
to	O
round	O
your	O
timestamp	O
to	O
5	O
min	O
,	O
and	O
then	O
group	O
by	O
on	O
that	O
:	O
#CODE	O

If	O
you	O
want	O
to	O
preserve	O
some	O
of	O
the	O
original	O
timestamps	O
(	O
but	O
you	O
have	O
to	O
choose	O
which	O
if	O
you	O
are	O
binning	O
them	O
)	O
,	O
you	O
can	O
specify	O
a	O
function	O
to	O
apply	B-API
on	O
the	O
individual	O
columns	O
.	O

For	O
example	O
take	O
the	O
first	O
:	O
#CODE	O

If	O
you	O
just	O
want	O
to	O
resample	B-API
on	O
5	O
mins	O
and	O
add	O
the	O
counts	O
regardless	O
of	O
the	O
keys	O
,	O
you	O
can	O
simply	O
do	O
:	O
#CODE	O

The	O
keys	O
are	O
unique	O
(	O
but	O
may	O
not	O
appear	O
every	O
time	O
step	O
)	O
.	O

I	O
would	O
like	O
to	O
add	O
the	O
data1	O
(	O
counts	O
)	O
together	O
for	O
each	O
unique	O
key	O
-	O
and	O
then	O
bin	O
these	O
into	O
5min	O
bins	O
-	O
so	O
the	O
second	O
approach	O

So	O
the	O
second	O
approach	O
is	O
what	O
you	O
want	O
?	O

Or	O
not	O
yet	O
?	O

This	O
looks	O
way	O
nicer	O
than	O
mine	O
,	O
but	O
it	O
doesn't	O
bin	O
into	O
5	O
minute	O
intervals	O
(	O
I	O
guess	O
you	O
could	O
make	O
a	O
column	O
with	O
rounded	O
times	O
?	O
)	O

@USER	O
Yeah	O
,	O
that's	O
what	O
I	O
was	O
trying	O
(	O
rounded	O
times	O
)	O
,	O
but	O
it	O
is	O
not	O
clear	O
to	O
me	O
if	O
it	O
is	O
needed	O
(	O
maybe	O
all	O
the	O
times	O
are	O
already	O
5-mins	O
,	O
and	O
then	O
it	O
is	O
just	O
a	O
simple	O
groupby	B-API
)	O

@USER	O
Oops	O
,	O
mislooking	O
at	O
the	O
seconds	O
/	O
minutes	O
,	O
so	O
it	O
is	O
certainly	O
not	O
yet	O
in	O
5-min	O
times	O
..	O

Yeah	O
,	O
as	O
usual	O
here	O
example	O
not	O
the	O
best	O
...	O

Not	O
sure	O
how	O
you	O
do	O
the	O
rounded	O
times	O
(	O
will	O
upvote	O
when	O
you	O
get	O
it	O
:p	O
)	O
.	O

I	O
would	O
want	O
the	O
second	O
approach	O
-	O
but	O
it	O
groups	O
together	O
the	O
keys	O
?	O

ideally	O
would	O
want	O
to	O
keep	O
the	O
time-stamp	O
intact	O
,	O
for	O
each	O
unique	O
key	O
-	O
then	O
I	O
understood	O
that	O
the	O
time	O
stamp	O
resample()	B-API
could	O
handle	O
mutiple	O
keys	O
when	O
resampling	O
?	O

Yes	O
,	O
it	O
groups	O
together	O
the	O
*	O
unique	O
*	O
keys	O
(	O
the	O
tuples	O
)	O
.	O

Isn't	O
that	O
what	O
you	O
want	O
?	O

How	O
can	O
you	O
keep	O
the	O
time-stamp	O
intact	O
if	O
you	O
want	O
to	O
bin	O
your	O
data	O
every	O
5	O
min	O
?	O

OK	O
,	O
if	O
you	O
want	O
to	O
keep	O
the	O
time	O
stamp	O
intact	O
,	O
you	O
should	O
not	O
group	O
by	O
it	O
and	O
take	O
the	O
first	O
approach	O
.	O

I've	O
read	O
that	O
the	O
time	O
series	B-API
resample	B-API
can	O
work	O
with	O
repeated	O
time	O
stamps	O
-	O
I'd	O
really	O
just	O
like	O
to	O
aggregate	O
so	O
that	O
the	O
counts	O
(	O
data1	O
)	O
are	O
added	O
when	O
creating	O
a	O
5	O
min	O
time	O
bin	O
.	O

In	O
this	O
example	O
there	O
were	O
just	O
2	O
time	O
stamps	O
(	O
same	O
value	O
)	O
.	O

A	O
larger	O
data	O
set	O
would	O
have	O
1000's	O
or	O
more	O
time	O
stamps	O
for	O
keys	O
,	O
and	O
a	O
cound	O
for	O
each	O
.	O

Would	O
like	O
to	O
sum	O
the	O
counts	O
for	O
unique	O
keys	O
when	O
binned	O

OK	O
,	O
I	O
get	O
that	O
,	O
but	O
the	O
last	O
approach	O
does	O
this	O
?	O

What	O
do	O
you	O
mean	O
with	O
keep	O
the	O
timestamp	O
intact	O
?	O

If	O
you	O
have	O
two	O
rows	O
with	O
a	O
unique	O
key	O
but	O
different	O
timestamps	O
in	O
the	O
same	O
5min	O
bin	O
,	O
which	O
of	O
the	O
two	O
timestamps	O
you	O
want	O
to	O
preserve	O
?	O

by	O
time-stamp	O
intact	O
,	O
just	O
meant	O
that	O
it	O
is	O
included	O
in	O
the	O
time	O
series	B-API
or	O
dataframe	B-API
,	O
even	O
after	O
aggregation	O

If	O
you	O
"	O
aggregate	O
"	O
for	O
different	O
timestamps	O
,	O
you	O
cannot	O
just	O
include	O
it	O
(	O
because	O
for	O
each	O
aggregated	O
count	O
,	O
you	O
have	O
multiple	O
and	O
different	O
timestamps	O
)	O
.	O

How	O
do	O
you	O
want	O
it	O
included	O
?	O

Can	O
you	O
give	O
a	O
more	O
elaborate	O
example	O
dataset	O
(	O
with	O
different	O
timestamps	O
)	O
,	O
and	O
also	O
give	O
an	O
example	O
of	O
the	O
desired	O
output	O
.	O

would	O
resample()	B-API
be	O
able	O
to	O
create	O
the	O
5min	O
bins	O
?	O

even	O
when	O
there	O
are	O
repeated	O
time	O
stamps	O

yes	O
off	O
course	O
,	O
but	O
you	O
also	O
want	O
to	O
group	O
by	O
key	O
(	O
tuple	O
)	O
?	O

+1	O
goodness	O
,	O
that's	O
an	O
awful	O
hack	O
to	O
round	O
dates	O
:)	O

@USER	O
,	O
indeed	O
...	O

That's	O
certainly	O
something	O
that	O
can	O
be	O
improved	O
with	O
a	O
handy	O
function	O
:-)	O

I	O
want	O
add	O
the	O
counts	O
(	O
data1	O
)	O
associated	O
with	O
a	O
unique	O
key	O
-	O
so	O
if	O
a	O
(	O
count	O
,	O
key	O
)	O
appears	O
for	O
a	O
time	O
stamp	O
in	O
any	O
5	O
min	O
interval	O
,	O
the	O
counts	O
are	O
added	O
.	O

Do	O
not	O
want	O
to	O
add	O
counts	O
for	O
all	O
keys	O
in	O
the	O
5	O
min	O
bin	O
.	O

OK	O
,	O
but	O
in	O
that	O
case	O
,	O
I	O
think	O
my	O
answer	O
is	O
working	O
(	O
with	O
the	O
rounding	O
)	O
?	O

@USER	O
Lol	O
,	O
another	O
hack	O
to	O
round	O
:-)	O
`	O
df	O
[	O
'	O
data2	O
']	O
.map	B-API
(	O
lambda	O
x	O
:	O
pd.DataFrame	B-API
([	O
0	O
]	O
,	O
index	O
=p	O
d.DatetimeIndex	O
([	O
x	O
]))	O
.resample	B-API
(	O
'	O
5min	O
')	O
.index	B-API
[	O
0	O
])`	O

in	O
the	O
case	O
of	O
df.set_index	B-API
(	O
'	O
data2	O
'	O
,	O
inplace=True	O
)	O

df.resample	B-API
(	O
'	O
5min	O
'	O
,	O
'	O
sum	O
')	O
,	O
it	O
is	O
adding	O
toegther	O
counts	O
for	O
all	O
the	O
keys	O
-	O
want	O
to	O
keep	O
the	O
counts	O
for	O
the	O
unique	O
keys	O
separated	O

In	O
that	O
case	O
all	O
counts	O
are	O
added	O
,	O
also	O
for	O
non-unique	O
within	O
a	O
5	O
min	O
bin	O
.	O

So	O
that	O
is	O
not	O
what	O
you	O
want	O
.	O

But	O
my	O
third	O
code	O
block	O
is	O
doing	O
what	O
you	O
want	O
I	O
think	O
?	O

That	O
block	O
adds	O
together	O
the	O
counts	O
(	O
data1	O
)	O
for	O
all	O
keys	O
-	O
need	O
to	O
keep	O
the	O
unique	O
keys	O
separate	O
-	O
need	O
the	O
counts	O
added	O
for	O
each	O
unique	O
key	O
when	O
binned	O
-	O
so	O
in	O
this	O
case	O
would	O
not	O
change	O
-	O
need	O
to	O
have	O
more	O
time	O
stamps	O
that	O
span	O
several	O
minutes	O

This	O
``	O
df.groupby	B-API
([	O
'	O
data2_5min	O
'	O
,	O
'	O
key1	O
'])	O
.aggregate	O
(	O
'	O
sum	O
')``	O
keeps	O
the	O
keys	O
seperate	O
,	O
and	O
adds	O
the	O
counts	O
within	O
each	O
5	O
min	O
bin	O
together	O
for	O
the	O
unique	O
keys	O

Thanks	O
so	O
much	O
!	O

I	O
will	O
take	O
a	O
look	O

Would	O
it	O
be	O
possible	O
let	O
resample()	B-API
create	O
the	O
5min	O
bins	O
-	O
afterwards	O
and	O
preserve	O
the	O
unique	O
keys	O
(	O
along	O
with	O
their	O
counts	O
)	O
.	O

This	O
would	O
allow	O
for	O
example	O
,	O
to	O
create	O
2	O
min	O
,	O
10	O
min	O
bins	O
using	O
resample	B-API
-	O
and	O
look	O
at	O
the	O
data	O
over	O
different	O
bin	O
sizes	O
(	O
time	O
scales	O
)	O

I've	O
posted	O
a	O
much	O
simpler	O
example	O
(	O
based	O
on	O
cities	O
as	O
keys	O
)	O
above	O

If	O
you	O
set	O
the	O
date	O
as	O
the	O
index	O
you	O
can	O
use	O
TimeGrouper	O
(	O
which	O
allows	O
you	O
to	O
group	O
by	O
,	O
for	O
example	O
,	O
5	O
minute	O
intervals	O
):	O
#CODE	O

You	O
can	O
then	O
count	O
the	O
number	O
of	O
unique	O
items	O
in	O
each	O
5	O
minute	O
interval	O
using	O
nunique	B-API
:	O
#CODE	O

If	O
you're	O
looking	O
for	O
a	O
count	O
of	O
each	O
tuple	O
,	O
you	O
could	O
use	O
value_counts	B-API
:	O
#CODE	O

Note	O
:	O
this	O
is	O
a	O
Series	B-API
with	O
a	O
MultiIndex	O
(	O
use	O
reset_index	B-API
to	O
make	O
it	O
a	O
DataFrame	B-API
)	O
.	O

#CODE	O

You'll	O
probably	O
want	O
to	O
give	O
these	O
more	O
informative	O
column	O
names	O
:)	O
.	O

Update	O
:	O
previously	O
I	O
hacked	O
get	O
`	O
get_dummies	B-API
`	O
,	O
see	O
edit	O
history	O
.	O

I	O
?	O

get_dummies	B-API
,	O
but	O
there	O
is	O
probably	O
a	O
short-cut	O
for	O
`	O
get_dummies	B-API
(	O
x	O
)	O
.sum()	B-API
`	O
lol	O
it's	O
value_counts	B-API

Nice	O
trick	O
with	O
the	O
TimeGrouper	O
!	O

But	O
I	O
think	O
he	O
wants	O
to	O
'	O
add	O
the	O
counts	O
'	O
(	O
which	O
is	O
an	O
existing	O
column	O
)	O
,	O
and	O
not	O
to	O
'	O
count	O
the	O
unique	O
keys	O
'	O

Yeah	O
,	O
it	O
should	O
be	O
in	O
the	O
docs	O
!	O

(	O
Also	O
seems	O
a	O
shame	O
you	O
can't	O
do	O
it	O
to	O
a	O
column	O
...	O
)	O

(	O
you	O
could	O
be	O
right	O
:	O
maybe	O
I	O
don't	O
follow	O
the	O
question	O
...	O
)	O

Very	O
nice	O
,	O
thanks	O
.	O

But	O
for	O
sure	O
,	O
the	O
counts	O
need	O
to	O
be	O
added	O

@USER	O
I	O
don't	O
understand	O
what	O
you	O
mean	O
,	O
this	O
does	O
count	O
the	O
number	O
of	O
occurrences	O
.	O

If	O
this	O
isn't	O
what	O
you	O
want	O
could	O
you	O
include	O
the	O
desired	O
outcome	O
in	O
your	O
question	O
?	O

Sorry	O
-	O
I	O
want	O
to	O
add	O
the	O
counts	O
(	O
data1	O
column	O
)	O
for	O
unique	O
keys	O
-	O
when	O
binning	O
into	O
5	O
min	O
bins	O
-	O

in	O
this	O
example	O
there	O
was	O
just	O
one	O
time	O
stamp	O
-	O
but	O
there	O
will	O
be	O
1000's	O
of	O
time	O
stamps	O
across	O
several	O
minutes	O
up	O
to	O
an	O
hour	O
say	O
.	O

this	O
groups	O
these	O
timestamps	O
into	O
5	O
minute	O
bins	O

ah	O
you	O
want	O
to	O
groupby	B-API
data1	O
too	O
(	O
i.e.	O
my	O
solution	O
but	O
also	O
separated	O
by	O
data1	O
)	O

essentially	O
add	O
(	O
aggregate	O
)	O
the	O
data1	O
(	O
counts	O
)	O
for	O
each	O
unique	O
key	O
,	O
into	O
5	O
min	O
bins	O
.	O

so	O
when	O
there	O
are	O
enough	O
time	O
stamps	O
(	O
data2	O
)	O
spanning	O
more	O
than	O
5	O
minutes	O
(	O
e.g	O
an	O
hour	O
)	O
,	O
add	O
together	O
the	O
counts	O
(	O
data1	O
)	O
for	O
each	O
of	O
the	O
unique	O
keys	O
appearing	O
(	O
in	O
a	O
given	O
5min	O
bin	O
or	O
interval	O
)	O
.	O

The	O
difficulty	O
is	O
that	O
not	O
all	O
keys	O
will	O
appear	O
or	O
be	O
present	O
for	O
any	O
given	O
time	O
stamp	O

When	O
plotting	O
datetime	O
index	O
data	O
,	O
put	O
markers	O
in	O
the	O
plot	O
on	O
specific	O
days	O
(	O
e.g.	O
weekend	O
)	O

I	O
create	O
a	O
pandas	O
dataframe	B-API
with	O
a	O
DatetimeIndex	B-API
like	O
so	O
:	O
#CODE	O

Which	O
gives	O
#CODE	O

I	O
can	O
easily	O
plot	O
the	O
`	O
A	O
`	O
colum	O
with	O
pandas	O
by	O
doing	O
:	O
#CODE	O

which	O
plots	O
a	O
line	O
of	O
the	O
`	O
A	O
`	O
column	O
but	O
leaves	O
out	O
the	O
`	O
weekend	O
`	O
column	O
as	O
it	O
does	O
not	O
hold	O
numerical	O
data	O
.	O

How	O
can	O
I	O
put	O
a	O
"	O
marker	O
"	O
on	O
each	O
spot	O
of	O
the	O
`	O
A	O
`	O
column	O
where	O
the	O
`	O
weekend	O
`	O
column	O
has	O
the	O
value	O
`	O
yes	O
`	O
?	O

Meanwhile	O
I	O
found	O
out	O
,	O
it	O
is	O
as	O
simple	O
as	O
using	O
boolean	O
indexing	O
in	O
pandas	O
.	O

Doing	O
the	O
plot	O
directly	O
with	O
pyplot	O
instead	O
of	O
pandas	O
'	O
own	O
plot	O
wrapper	O
(	O
which	O
is	O
more	O
convenient	O
to	O
me	O
):	O
#CODE	O

Now	O
,	O
the	O
red	O
dots	O
mark	O
all	O
weekend	O
days	O
which	O
are	O
given	O
by	O
`	O
df.weekend	O
=	O
'	O
yes	O
'`	O
values	O
.	O

You	O
could	O
shorten	O
it	O
a	O
smidge	O
with	O
`	O
df	O
[	O
'	O
A	O
']	O
[	O
df	O
[	O
'	O
weekend	O
']	O
==	O
'	O
yes	O
']	O
.plot	B-API
(	O
style=	O
'	O
ro	O
')`	O
which	O
is	O
a	O
bit	O
more	O
readable	O
(	O
IMO	O
)	O
but	O
still	O
,	O
nice	O
self-answer	O
.	O

pandas	O
applying	O
regex	O
to	O
replace	O
values	O

I	O
have	O
read	O
some	O
pricing	O
data	O
into	O
a	O
pandas	O
dataframe	B-API
the	O
values	O
appear	O
as	O
:	O
#CODE	O

I	O
want	O
to	O
strip	O
it	O
down	O
to	O
just	O
the	O
numeric	O
values	O
.	O

I	O
know	O
I	O
can	O
loop	O
through	O
and	O
apply	B-API
regex	O
#CODE	O

to	O
each	O
field	O
then	O
join	O
the	O
resulting	O
list	O
back	O
together	O
but	O
is	O
there	O
a	O
not	O
loopy	O
way	O
?	O

Thanks	O

You	O
could	O
remove	O
all	O
the	O
non-digits	O
using	O
`	O
re.sub()	O
`	O
:	O
#CODE	O

regex101	O
demo	O

`	O
\D+	O
`	O
will	O
be	O
the	O
smallest	O
:-P	O

whats	O
the	O
best	O
way	O
to	O
apply	B-API
it	O
to	O
the	O
column	O
in	O
the	O
dataframe	B-API
?	O

so	O
I	O
have	O
df	O
[	O
'	O
pricing	O
']	O
do	O
I	O
just	O
loop	O
row	O
by	O
row	O
?	O

@USER	O
I	O
don't	O
have	O
much	O
experience	O
with	O
pandas	O
,	O
but	O
I	O
think	O
that	O
you	O
should	O
be	O
able	O
to	O
use	O
it	O
like	O
this	O
:	O
`	O
df	O
[	O
'	O
pricing	O
']	O
=	O
re.sub	O
(	O
r	O
"	O
[	O
^	O
0-9	O
]	O
+	O
"	O
,	O
""	O
,	O
df	O
[	O
'	O
pricing	O
'])`	O
.	O

@USER	O
Smaller	O
doesn't	O
necessarily	O
mean	O
better	O
.	O

`	O
\D	O
`	O
tends	O
to	O
be	O
slower	O
and	O
that's	O
not	O
better	O
at	O
all	O
.	O

ok	O
I	O
think	O
I	O
got	O
it	O
for	O
pandas	O
use	O
:	O
df	O
[	O
'	O
Pricing	O
']	O
.replace	B-API
(	O
to_replace=	O
'	O
[	O
^	O
0-9	O
]	O
+	O
'	O
,	O
value=	O
''	O
,	O
inplace	O
==	O
True	O
,	O
regex=True	O
)	O
the	O
.replace	B-API
method	O
uses	O
re.sub	O

@USER	O
Nice	O
,	O
thanks	O
for	O
letting	O
me	O
know	O
too	O
:)	O

You	O
could	O
use	O
`	O
Series.str.replace	B-API
`	O
:	O
#CODE	O

yields	O
#CODE	O

Pandas	O
groupby	B-API
:	O
How	O
do	O
I	O
use	O
shifted	O
values	O

I	O
have	O
a	O
dataset	O
that	O
represents	O
reoccurring	O
events	O
at	O
different	O
locations	O
.	O

#CODE	O

Each	O
location	O
can	O
have	O
8-10	O
events	O
that	O
repeat	O
.	O

What	O
I'm	O
trying	O
to	O
do	O
is	O
build	O
some	O
information	O
of	O
how	O
long	O
it	O
has	O
been	O
between	O
two	O
events	O
.	O

(	O
they	O
may	O
not	O
be	O
the	O
same	O
event	O
)	O

I	O
am	O
able	O
to	O
do	O
this	O
by	O
splitting	O
the	O
df	O
into	O
sub-dfs	O
and	O
processing	O
each	O
location	O
individually	O
.	O

But	O
it	O
would	O
seem	O
that	O
groupby	B-API
should	O
be	O
smarter	O
that	O
this	O
.	O

This	O
is	O
also	O
assuming	O
that	O
I	O
know	O
all	O
the	O
locations	O
which	O
may	O
vary	O
file	O
to	O
file	O
.	O

#CODE	O

What	O
I	O
would	O
like	O
to	O
do	O
is	O
groupBy	B-API
based	O
on	O
location	O
...	O

#CODE	O

Then	O
for	O
each	O
grouped	O
location	O

Add	O
a	O
delta	O
column	O

Shift	O
and	O
subtract	O
to	O
get	O
the	O
delta	O
time	O
between	O
events	O

Questions	O
:	O

Does	O
groupby	B-API
maintain	O
the	O
order	O
of	O
events	O
?	O

Would	O
a	O
for	O
loop	O
that	O
runs	O
over	O
the	O
DF	O
be	O
better	O
?	O

That	O
doesn't	O
seem	O
very	O
python	O
like	O
.	O

Also	O
once	O
you	O
have	O
a	O
grouped	O
df	O
is	O
there	O
a	O
way	O
to	O
transform	O
it	O
back	O
to	O
a	O
general	O
dataframe	B-API
.	O

I	O
don't	O
think	O
I	O
need	O
to	O
do	O
this	O
but	O
thought	O
it	O
may	O
be	O
helpful	O
in	O
the	O
future	O
.	O

Thank	O
you	O
for	O
any	O
support	O
you	O
can	O
offer	O
.	O

#URL	O
looks	O
like	O
it	O
provides	O
what	O
you	O
need	O
.	O

#CODE	O

or	O
#CODE	O

Will	O
split	O
it	O
into	O
groups	O
of	O
rows	O
with	O
matching	O
values	O
in	O
the	O
location	O
column	O
.	O

Then	O
you	O
can	O
sort	O
the	O
groups	O
based	O
on	O
the	O
time	O
value	O
and	O
iterate	O
through	O
to	O
create	O
the	O
deltas	O
.	O

JohnB	O
-	O
Yes	O
I	O
know	O
how	O
to	O
group-by	O
based	O
on	O
the	O
location	O
.	O

That	O
it	O
actually	O
included	O
as	O
part	O
of	O
the	O
question	O
.	O

But	O
when	O
you	O
group	O
by	O
you	O
end	O
up	O
with	O
a	O
group-by	O
object	O
which	O
you	O
can	O
iterate	O
over	O
.	O
for	O
a	O
specific	O
group	O
is	O
there	O
a	O
way	O
to	O
apply	B-API
a	O
shift	O
to	O
it	O
.	O

Or	O
do	O
I	O
need	O
to	O
iterate	O
over	O
it	O
.	O

Also	O
once	O
you	O
have	O
the	O
groups	O
there	O
seems	O
to	O
be	O
options	O
to	O
process	O
the	O
data	O
but	O
not	O
add	O
another	O
column	O
to	O
the	O
group	O
.	O

This	O
is	O
kind	O
of	O
where	O
I'm	O
stuck	O
.	O

It	O
appears	O
that	O
when	O
you	O
group-by	O
and	O
identify	O
a	O
column	O
to	O
act	O
on	O
the	O
data	O
is	O
returned	O
in	O
a	O
series	B-API
which	O
then	O
a	O
function	O
can	O
be	O
applied	O
to	O
.	O

#CODE	O

This	O
groups	O
by	O
location	O
and	O
returns	O
the	O
time	O
column	O
for	O
each	O
group	O
.	O

Each	O
sub-series	O
is	O
then	O
passed	O
to	O
the	O
function	O
deltaTime	O
.	O

Why	O
does	O
a	O
pandas	O
Series	B-API
of	O
DataFrame	B-API
mean	O
fail	O
,	O
but	O
sum	O
does	O
not	O
,	O
and	O
how	O
to	O
make	O
it	O
work	O

There	O
may	O
be	O
a	O
smarter	O
way	O
to	O
do	O
this	O
in	O
Python	O
Pandas	O
,	O
but	O
the	O
following	O
example	O
should	O
,	O
but	O
doesn't	O
work	O
:	O
#CODE	O

I	O
won't	O
post	O
the	O
whole	O
traceback	O
,	O
but	O
the	O
main	O
error	O
message	O
is	O
interesting	O
:	O
#CODE	O

It	O
looks	O
like	O
the	O
dataframe	B-API
was	O
successfully	O
summed	O
,	O
but	O
not	O
divided	O
by	O
the	O
length	O
of	O
the	O
series	B-API
.	O

However	O
,	O
we	O
can	O
take	O
the	O
sum	O
of	O
the	O
dataframes	O
in	O
the	O
series	B-API
:	O
#CODE	O

Returns	O
#CODE	O

Why	O
wouldn't	O
mean	O
work	O
when	O
sum	O
does	O
?	O

Is	O
this	O
a	O
bug	O
or	O
a	O
missing	O
feature	O
?	O

This	O
does	O
work	O
:	O
#CODE	O

And	O
so	O
does	O
this	O
:	O
#CODE	O

But	O
this	O
of	O
course	O
is	O
not	O
ideal	O
.	O

Unclear	O
if	O
it	O
should	O
work	O
at	O
all	O
but	O
you	O
could	O
apply	B-API
the	O
function	O
:	O
`	O
s.apply	O
(	O
pd.DataFrame.mean	B-API
)`	O

This	O
just	O
gives	O
the	O
mean	O
of	O
each	O
constituent	O
data	O
frame	O
.	O

When	O
you	O
define	O
`	O
s	O
`	O
with	O
#CODE	O

you	O
get	O
a	O
Series	B-API
with	O
DataFrames	O
as	O
items	O
:	O
#CODE	O

The	O
sum	O
of	O
the	O
items	O
is	O
a	O
DataFrame	B-API
:	O
#CODE	O

but	O
when	O
you	O
take	O
the	O
mean	O
,	O
`	O
nanops.nanmean	O
`	O
is	O
called	O
:	O
#CODE	O

Notice	O
that	O
`	O
_ensure_numeric	O
`	O
(	O
source	O
code	O
)	O
is	O
called	O
on	O
the	O
resultant	O
sum	O
.	O

An	O
error	O
is	O
raised	O
because	O
a	O
DataFrame	B-API
is	O
not	O
numeric	O
.	O

Here	O
is	O
a	O
workaround	O
.	O

Instead	O
of	O
making	O
a	O
Series	B-API
with	O
DataFrames	O
as	O
items	O
,	O

you	O
can	O
concatenate	O
the	O
DataFrames	O
into	O
a	O
new	O
DataFrame	B-API
with	O
a	O
hierarchical	O
index	O
:	O
#CODE	O

Now	O
you	O
can	O
take	O
the	O
`	O
sum	B-API
`	O
and	O
the	O
`	O
mean	B-API
`	O
:	O
#CODE	O

You	O
could	O
(	O
as	O
suggested	O
by	O
@USER	O
)	O
use	O
a	O
hierarchical	O
index	O
but	O
when	O
you	O
have	O
a	O
three	O
dimensional	O
array	O
you	O
should	O
consider	O
using	O
a	O
"	O
pandas	O
Panel	B-API
"	O
.	O

Especially	O
when	O
one	O
of	O
the	O
dimensions	O
represents	O
time	O
as	O
in	O
this	O
case	O
.	O

The	O
Panel	B-API
is	O
oft	O
overlooked	O
but	O
it	O
is	O
after	O
all	O
where	O
the	O
name	O
pandas	O
comes	O
from	O
.	O

(	O
Panel	B-API
Data	O
System	O
or	O
something	O
like	O
that	O
)	O
.	O

Data	O
slightly	O
different	O
from	O
your	O
original	O
so	O
there	O
are	O
not	O
two	O
dimensions	O
with	O
the	O
same	O
length	O
:	O
#CODE	O

Panels	O
can	O
be	O
created	O
a	O
couple	O
of	O
different	O
ways	O
but	O
one	O
is	O
from	O
a	O
dict	O
.	O

You	O
can	O
create	O
the	O
dict	O
from	O
your	O
index	O
and	O
the	O
dataframes	O
with	O
:	O
#CODE	O

The	O
mean	O
you	O
are	O
looking	O
for	O
is	O
simply	O
a	O
matter	O
of	O
operating	O
on	O
the	O
correct	O
axis	O
(	O
axis=0	O
in	O
this	O
case	O
):	O
#CODE	O

With	O
your	O
data	O
,	O
`	O
sum	O
(	O
axis=0	O
)`	O
returns	O
the	O
expected	O
result	O
.	O

EDIT	O
:	O
OK	O
too	O
late	O
for	O
panels	O
as	O
the	O
hierarchical	O
index	O
approach	O
is	O
already	O
"	O
accepted	O
"	O
.	O

I	O
will	O
say	O
that	O
that	O
approach	O
is	O
preferable	O
if	O
the	O
data	O
is	O
know	O
to	O
be	O
"	O
ragged	O
"	O
with	O
an	O
unknown	O
but	O
different	O
number	O
in	O
each	O
grouping	O
.	O

For	O
"	O
square	O
"	O
data	O
,	O
the	O
panel	B-API
is	O
absolutly	O
the	O
way	O
to	O
go	O
and	O
will	O
be	O
significantly	O
faster	O
with	O
more	O
built-in	O
operations	O
.	O

Pandas	O
0.15	O
has	O
many	O
improvements	O
for	O
multi-level	O
indexing	O
but	O
still	O
has	O
limitations	O
and	O
dark	O
edge	O
cases	O
in	O
real	O
world	O
apps	O
.	O

You're	O
right	O
that	O
computation	O
on	O
the	O
panel	B-API
is	O
significantly	O
faster	O
than	O
on	O
the	O
hierarchical	O
dataframe	B-API
.	O

But	O
what	O
are	O
the	O
extra	O
built-in	O
operations	O
that	O
panels	O
have	O
that	O
dataframes	O
don't	O
?	O

`	O
set	O
(	O
dir	O
(	O
pan	O
))	O
-set	O
(	O
dir	O
(	O
df	O
))`	O
did	O
not	O
turn	O
up	O
anything	O
interesting	O
.	O

Also	O
,	O
can	O
you	O
elaborate	O
on	O
the	O
"	O
limitations	O
and	O
dark	O
edges	O
cases	O
"	O
to	O
which	O
you	O
referred	O
?	O

This	O
is	O
my	O
fault	O
for	O
asking	O
two	O
questions	O
in	O
one	O
.	O

I	O
will	O
be	O
using	O
your	O
solution	O
for	O
my	O
problem	O
since	O
,	O
as	O
you	O
point	O
out	O
,	O
the	O
`	O
Panel	B-API
`	O
solution	O
is	O
the	O
correct	O
one	O
for	O
the	O
example	O
I	O
gave	O
.	O

But	O
I	O
think	O
@USER	O
addressed	O
the	O
full	O
question	O
of	O
what	O
exactly	O
is	O
keeping	O
my	O
original	O
series-based	O
code	O
from	O
working	O
.	O

I	O
very	O
much	O
appreciate	O
your	O
insight	O
!	O

Both	O
of	O
these	O
should	O
be	O
in	O
the	O
docs	O
!	O

I	O
want	O
to	O
also	O
add	O
,	O
that	O
unlike	O
the	O
hierarchical	O
index	O
version	O
,	O
you	O
can	O
now	O
also	O
do	O
`	O
s_mean	O
=	O
np.mean	O
(	O
s	O
,	O
axis=0	O
)`	O
,	O
or	O
sum	O
,	O
std	O
,	O
or	O
I'm	O
guessing	O
any	O
numpy	O
function	O
that	O
works	O
on	O
arrays	O
.	O
