Combine	O
Columns	O
Pandas	O

I	O
have	O
problem	O
combining	O
two	O
pandas	O
dataframe	B-API
columns	O
.	O

I	O
have	O
tried	O
#CODE	O

but	O
gives	O
me	O
error	O
:	O
#CODE	O

My	O
Data	O
looks	O
like	O
this	O
:	O
#CODE	O

Note	O
that	O
I've	O
converted	O
everything	O
to	O
float	O
when	O
reading	O
in	O
the	O
data	O
.	O

I	O
am	O
reading	O
in	O
line	O
by	O
line	O
using	O
linecache.getline	O
which	O
returns	O
an	O
entire	O
string	O
of	O
each	O
line	O
.	O

I	O
then	O
use	O
`	O
.split	B-API
(	O
'	O
,	O
')`	O
to	O
fix	O
this	O
.	O

But	O
afterwards	O
can't	O
convert	O
to	O
datetime	O
.	O

Do	O
i	O
need	O
to	O
convert	O
the	O
dates	O
in	O
to	O
integers	O
?	O

Thanks	O

Is	O
there	O
a	O
reason	O
you're	O
reading	O
the	O
lines	O
in	O
like	O
that	O
?	O
with	O
`	O
read_csv()	B-API
`	O
you	O
can	O
pass	O
`	O
parse_dates	O
=[[	O
'	O
Date	O
'	O
,	O
'	O
Time	O
']]`	O
and	O
the	O
two	O
columns	O
will	O
be	O
combined	O
into	O
a	O
single	O
Datetime	O
column	O
.	O

You	O
may	O
need	O
to	O
write	O
a	O
custom	O
`	O
date_parser	O
`	O
function	O
.	O

I've	O
got	O
a	O
massive	O
csv	O
file	O
but	O
I	O
don't	O
need	O
all	O
the	O
lines	O
...	O

I	O
think	O
you	O
can	O
do	O
this	O
:	O
#CODE	O

If	O
your	O
`'	O
Date	O
'`	O
and	O
`'	O
Time	O
'`	O
are	O
both	O
`	O
float64	O
`	O
,	O
you	O
need	O
these	O
two	O
lines	O
first	O
:	O
#CODE	O

Pandas.DataFrame.read_sql	O
to	O
read	O
as	O
float32	O
instead	O
of	O
float64	O

I'm	O
using	O
`	O
pd.DataFrame.read_sql	O
`	O
to	O
retrieve	O
a	O
large	O
number	O
of	O
records	O
from	O
a	O
SQL	O
database	O
causing	O
a	O
huge	O
amount	O
of	O
memory	O
to	O
be	O
used	O
.	O

I	O
noticed	O
that	O
the	O
`	O
dtype	B-API
`	O
of	O
the	O
numerical	O
values	O
are	O
`	O
float64	O
`	O
,	O
but	O
I	O
only	O
require	O
`	O
float32	O
`	O
dtypes	B-API
on	O
most	O
of	O
the	O
numerical	O
values	O
.	O

Is	O
it	O
possible	O
to	O
retrieve	O
values	O
via	O
`	O
read_sql	B-API
`	O
as	O
`	O
float32	O
`	O
?	O

No	O
#URL	O
...	O

:/	O

You	O
could	O
convert	O
it	O
once	O
read	O
using	O
`	O
.astype	B-API
(	O
'	O
int32	O
')`	O

Eigenvalues	O
of	O
masked	O
array	O
(	O
NumPy	O
)	O

How	O
can	O
I	O
compute	O
the	O
eigenvalues	O
and	O
eigenvectors	O
of	O
a	O
masked	O
NumPy	O
array	O
(	O
for	O
an	O
unmasked	O
array	O
,	O
this	O
can	O
be	O
achieved	O
by	O
`	O
scipy.linalg.eig	O
`)	O
.	O

Edit	O
:	O

Turns	O
out	O
you	O
can	O
do	O
this	O
after	O
all	O
.	O

#CODE	O

that's	O
not	O
defined	O
--	O
what	O
is	O
the	O
shape	O
of	O
your	O
masked	O
array	O
??	O

If	O
you	O
have	O
answered	O
your	O
own	O
question	O
you	O
should	O
submit	O
it	O
as	O
an	O
answer	O
,	O
rather	O
than	O
edit	O
the	O
question	O
.	O

Create	O
subset	O
table	O
from	O
Pandas	O
dataframe	B-API
with	O
hierarchical	O
index	O
in	O
a	O
loop	O

I	O
want	O
to	O
iterate	O
through	O
a	O
hierarchical	O
index	O
panda	O
dataframe	B-API
and	O
print	O
a	O
subsets	O
based	O
on	O
"	O
group1	O
"	O
level	O
.	O

here	O
is	O
my	O
dataframe	B-API
#CODE	O

This	O
is	O
what	O
it	O
looks	O
like	O
:	O
#CODE	O

I	O
can	O
successfully	O
print	O
a	O
SINGLE	O
subset	O
based	O
on	O
group1	O
like	O
this	O
:	O
#CODE	O

But	O
how	O
can	O
I	O
put	O
this	O
in	O
a	O
loop	O
to	O
print	O
a	O
subset	O
for	O
each	O
group	O
of	O
elements	O
in	O
'	O
group1	O
'	O
?	O

My	O
failed	O
attempt	O
:	O
#CODE	O

This	O
printed	O
a	O
subset	O
for	O
each	O
index	O
instead	O
of	O
grouping	O
them	O
together	O
(	O
hope	O
that	O
makes	O
sense	O
)	O
.	O

Use	O
`	O
get_level_values	B-API
`	O
on	O
the	O
index	O
to	O
return	O
just	O
those	O
index	O
values	O
at	O
that	O
level	O
,	O
additionally	O
call	O
`	O
unique	B-API
`	O
as	O
what	O
is	O
returned	O
are	O
duplicate	O
index	O
values	O
for	O
each	O
sublevel	O
row	O
.	O

#CODE	O

Perfect	O
!	O

thanks	O
for	O
explaining	O
it	O
.	O

@USER	O
no	O
worries	O
,	O
you	O
can	O
upvote	O
too	O
;)	O

This	O
is	O
what	O
`	O
groupby	B-API
`	O
is	O
for	O
(	O
iteration	O
gives	O
your	O
the	O
groupname	O
,	O
group	O
):	O
#CODE	O

Most	O
of	O
the	O
time	O
you	O
actually	O
want	O
to	O
do	O
something	O
with	O
it	O
though	O
,	O
e.g.	O
#CODE	O

Ahh	O
right	O
,	O
thanks	O
a	O
lot	O
for	O
this	O
info	O
.	O

3D-plot	O
of	O
the	O
error	O
function	O
in	O
a	O
linear	O
regression	O

I	O
would	O
like	O
to	O
visually	O
plot	O
a	O
3D	O
graph	O
of	O
the	O
error	O
function	O
calculated	O
for	O
a	O
given	O
slope	O
and	O
y-intercept	O
for	O
a	O
linear	O
regression	O
.	O

This	O
graph	O
will	O
be	O
used	O
to	O
illustrate	O
a	O
gradient	O
descent	O
application	O
.	O

Let	O
s	O
suppose	O
we	O
want	O
to	O
model	O
a	O
set	O
of	O
points	O
with	O
a	O
line	O
.	O

To	O
do	O
this	O
we	O
ll	O
use	O
the	O
standard	O
y=mx+b	O
line	O
equation	O
where	O
m	O
is	O
the	O
line	O
s	O
slope	O
and	O
b	O
is	O
the	O
line	O
s	O
y-intercept	O
.	O

To	O
find	O
the	O
best	O
line	O
for	O
our	O
data	O
,	O
we	O
need	O
to	O
find	O
the	O
best	O
set	O
of	O
slope	O
m	O
and	O
y-intercept	O
b	O
values	O
.	O

A	O
standard	O
approach	O
to	O
solving	O
this	O
type	O
of	O
problem	O
is	O
to	O
define	O
an	O
error	O
function	O
(	O
also	O
called	O
a	O
cost	O
function	O
)	O
that	O
measures	O
how	O
good	O
a	O
given	O
line	O
is	O
.	O

This	O
function	O
will	O
take	O
in	O
a	O
(	O
m	O
,	O
b	O
)	O
pair	O
and	O
return	O
an	O
error	O
value	O
based	O
on	O
how	O
well	O
the	O
line	O
fits	O
the	O
data	O
.	O

To	O
compute	O
this	O
error	O
for	O
a	O
given	O
line	O
,	O
we	O
ll	O
iterate	O
through	O
each	O
(	O
x	O
,	O
y	O
)	O
point	O
in	O
the	O
data	O
set	O
and	O
sum	O
the	O
square	O
distances	O
between	O
each	O
point	O
s	O
y	O
value	O
and	O
the	O
candidate	O
line	O
s	O
y	O
value	O
(	O
computed	O
at	O
mx+b	O
)	O
.	O

It	O
s	O
conventional	O
to	O
square	O
this	O
distance	O
to	O
ensure	O
that	O
it	O
is	O
positive	O
and	O
to	O
make	O
our	O
error	O
function	O
differentiable	O
.	O

In	O
python	O
,	O
computing	O
the	O
error	O
for	O
a	O
given	O
line	O
will	O
look	O
like	O
:	O
#CODE	O

Since	O
the	O
error	O
function	O
consists	O
of	O
two	O
parameters	O
(	O
m	O
and	O
b	O
)	O
we	O
can	O
visualize	O
it	O
as	O
a	O
two-dimensional	O
surface	O
.	O

Now	O
my	O
question	O
,	O
how	O
can	O
we	O
plot	O
such	O
3D-graph	O
using	O
python	O
?	O

Here	O
is	O
a	O
skeleton	O
code	O
to	O
build	O
a	O
3D	O
plot	O
.	O

This	O
code	O
snippet	O
is	O
totally	O
out	O
of	O
the	O
question	O
context	O
but	O
it	O
show	O
the	O
basics	O
for	O
building	O
a	O
3D	O
plot	O
.	O

For	O
my	O
example	O
i	O
would	O
need	O
the	O
x-axis	O
being	O
the	O
slope	O
,	O
the	O
y-axis	O
being	O
the	O
y-intercept	O
and	O
the	O
z-axis	O
,	O
the	O
error	O
.	O

Can	O
someone	O
help	O
me	O
build	O
such	O
example	O
of	O
graph	O
?	O

#CODE	O

The	O
above	O
code	O
produce	O
the	O
following	O
plot	O
,	O
which	O
is	O
very	O
similar	O
to	O
what	O
i	O
am	O
looking	O
for	O
.	O

@USER	O
no	O
,	O
i	O
removed	O
the	O
R	O
tag	O
,	O
I	O
didn't	O
noticed	O
it	O
was	O
added	O
;	O
thanks	O

Simply	O
replace	O
`	O
fun	O
`	O
with	O
`	O
computeErrorForLineGivenPoints	O
`	O
:	O
#CODE	O

yields	O

Tip	O
:	O
I	O
renamed	O
`	O
computeErrorForLineGivenPoints	O
`	O
as	O
`	O
error	O
`	O
.	O

Generally	O
,	O
there	O
is	O
no	O
need	O
to	O
name	O
a	O
function	O
`	O
compute	O
...	O

`	O
since	O
almost	O
all	O
functions	O
compute	O
something	O
.	O

You	O
also	O
do	O
not	O
need	O
to	O
specify	O
"	O
GivenPoints	O
"	O
since	O
the	O
function	O
signature	O
shows	O
that	O
`	O
points	O
`	O
is	O
an	O
argument	O
.	O

If	O
you	O
have	O
other	O
error	O
functions	O
or	O
variables	O
in	O
your	O
program	O
,	O
`	O
line_error	O
`	O
or	O
`	O
total_error	O
`	O
might	O
be	O
a	O
better	O
name	O
for	O
this	O
function	O
.	O

Thanks	O
@USER	O
This	O
is	O
exactly	O
the	O
implementation	O
I	O
am	O
looking	O
for	O
.	O

I	O
will	O
change	O
the	O
function	O
signature	O
as	O
you	O
suggest	O
.	O

Thanks	O
!	O

Pandas	O
datetime	O
index	O
all	O
set	O
to	O
the	O
same	O
date	O
in	O
a	O
month	O
(	O
1st	O
)	O

I	O
have	O
a	O
dataframe	B-API
like	O
:	O
#CODE	O

I	O
want	O
all	O
the	O
dates	O
to	O
be	O
the	O
1st	O
of	O
the	O
month	O
like	O
:	O
#CODE	O

I	O
know	O
this	O
has	O
to	O
be	O
really	O
simple	O
,	O
but	O
I'm	O
new	O
at	O
pandas	O
/	O
python	O
and	O
looked	O
for	O
over	O
an	O
hour	O
at	O
this	O
point	O
.	O

Thanks	O
in	O
advance	O
.	O

Is	O
the	O
`	O
Date	B-API
`	O
the	O
`	O
index	B-API
`	O
,	O
or	O
a	O
`	O
column	O
'	O
?	O

You	O
may	O
just	O
use	O
simple	O
list	O
comprehension	O
to	O
"	O
convert	O
"	O
and	O
re-assign	O
your	O
index	O
:	O
#CODE	O

Repeat	O
data	O
frame	O
,	O
with	O
varying	O
column	O
value	O

I	O
have	O
the	O
following	O
data	O
frame	O
and	O
need	O
to	O
repeat	O
the	O
values	O
for	O
a	O
set	O
of	O
values	O
.	O

That	O
is	O
,	O
given	O
#CODE	O

I	O
need	O
to	O
do	O
something	O
like	O
this	O
,	O
but	O
more	O
performant	O
:	O
#CODE	O

That	O
is	O
,	O
the	O
expected	O
output	O
is	O
:	O
#CODE	O

This	O
looks	O
like	O
it	O
is	O
working	O
to	O
me	O
.	O

What	O
exactly	O
are	O
you	O
expecting	O
to	O
be	O
different	O
?	O

@USER	O
something	O
more	O
efficient	O
,	O
perhaps	O
vectorized	O
.	O

Plot	O
bar	O
graph	O
using	O
multiple	O
groupby	B-API
count	O
in	O
panda	O

I	O
am	O
trying	O
to	O
plot	O
bar	O
graph	O
using	O
pandas	O
.	O

DataTime	O
is	O
index	O
column	O
which	O
I	O
get	O
from	O
timestamp	O
.	O

Here	O
is	O
table	O
structure	O
:	O

So	O
far	O
i	O
have	O
written	O
this	O
:	O
#CODE	O

I	O
want	O
output	O
like	O
this	O
:	O

June	O
has	O
total	O
4	O
entry	O
and	O
one	O
entry	O
starts	O
with	O
u	O
.	O

so	O
X	O
has	O
4	O
y	O
has	O
1	O
.	O

Same	O
for	O
July	O
.	O

Also	O
i	O
want	O
to	O
plot	O
bar	O
graph	O
(	O
X	O
and	O
Y	O
entries	O
)	O
using	O
output	O
.	O

I	O
want	O
MONTH	O
vs	O
Values	O
bar	O
graph	O

This	O
is	O
a	O
near-exact	O
duplicate	O
of	O
#URL	O

Please	O
write	O
your	O
questions	O
so	O
they	O
make	O
sense	O
without	O
reference	O
to	O
links	O
to	O
external	O
sites	O
.	O

Questions	O
on	O
SO	O
should	O
be	O
helpful	O
to	O
future	O
readers	O
,	O
and	O
if	O
those	O
links	O
die	O
your	O
question	O
will	O
be	O
very	O
hard	O
to	O
understand	O
.	O

Also	O
,	O
please	O
do	O
not	O
post	O
links	O
to	O
images	O
of	O
text	O
(	O
or	O
embed	O
direct	O
images	O
of	O
text	O
)	O
-	O
post	O
the	O
text	O
itself	O
,	O
perhaps	O
in	O
a	O
Quote	O
block	O
,	O
or	O
in	O
a	O
Code	O
block	O
if	O
you	O
need	O
to	O
preserve	O
formatting	O
.	O

I	O
would	O
created	O
the	O
DataFrame	B-API
with	O
a	O
dict	O
:	O
#CODE	O

Now	O
you	O
can	O
use	O
the	O
plot	O
method	O
to	O
plot	O
months	O
vs	O
values	O
.	O

#CODE	O

Note	O
:	O
you	O
can	O
create	O
grouped	O
more	O
efficiently	O
:	O
#CODE	O

I	O
am	O
getting	O
an	O
error	O
when	O
i	O
use	O
your	O
groupby	B-API
code	O
.	O

This	O
is	O
error	O
"'	O
Series	B-API
'	O
object	O
has	O
no	O
attribute	O
'	O
dt	B-API
'"	O
.	O

This	O
is	O
sample	O
data	O
on	O
my	O
df	O
[	O
'	O
DateTime	O
']	O
=>	O
2013-06-25	O

@USER	O
you	O
need	O
to	O
ensure	O
your	O
Series	B-API
is	O
of	O
datetime64	O
type	O
,	O
rather	O
than	O
-	O
say	O
-	O
strings	O
...	O

e.g.	O
using	O
df	O
[	O
'	O
DateTime	O
']	O
=	O
pd.to_datetime	B-API
(	O
df	O
[	O
'	O
DateTime	O
'])	O
.	O

Also	O
ensure	O
you're	O
using	O
a	O
fairly	O
recent	O
version	O
of	O
pandas	O
>	O
=	O
0.15	O
.	O

I	O
am	O
using	O
panda	O
version	O
#	O
0.14.1	O
.	O

my	O
df	O
[	O
'	O
DateTime	O
']	O
.dtype	B-API
is	O
object	O
.	O

It	O
should	O
be	O
datetime64	O
as	O
per	O
your	O
last	O
comment	O
.	O

I	O
tried	O
to	O
convert	O
it	O
but	O
i	O
could	O
not	O
solve	O
the	O
problem	O
.	O

I	O
am	O
using	O
anaconda	O
with	O
2.7	O
python	O
version	O

I	O
use	O
pf	O
[	O
'	O
DateTime	O
']	O
=	O
pf	O
[	O
'	O
DATE	O
']	O
.apply	B-API
(	O
lambda	O
x	O
:	O
dt.date.fromtimestamp	O
(	O
x	O
))	O
to	O
convert	O
timestamp	O
to	O
date	O
.	O

Also	O
to	O
groupby	B-API
i	O
use	O
this	O
df1	O
[	O
'	O
DateTime	O
']	O
.groupby	B-API
(	O
lambda	O
x	O
:	O
x.month	O
)	O
.	O

In	O
my	O
final	O
output	O
i	O
want	O
Month-Year	O
on	O
x	O
axis	O
.	O

@USER	O
use	O
`	O
pd.to_datetime	B-API
`	O
rather	O
than	O
apply	B-API
.	O

If	O
you	O
use	O
the	O
apply	B-API
it	O
may	O
not	O
create	O
a	O
Datetime	O
column	O
.	O

I	O
think	O
you	O
may	O
have	O
to	O
update	O
to	O
0.15.X	O
for	O
the	O
dt	B-API
accessor	O
.	O

If	O
you	O
want	O
month-year	O
then	O
use	O
the	O
`	O
to_period	B-API
`	O
part	O
of	O
the	O
answer	O
above	O
?	O

result.plot()	O
does	O
not	O
short	O
the	O
month	O
and	O
year	O
together	O
.	O

it	O
short	O
by	O
only	O
month	O
,	O
thats	O
why	O
i	O
get	O
plot	O
for	O
Jan	O
2014	O
Jan	O
2015	O
together	O
.	O

how	O
to	O
short	O
result	O
data	O
frame	O

@USER	O
you	O
should	O
ask	O
that	O
as	O
a	O
new	O
question	O
:)	O

#URL	O

Pandas	O
:	O
Merge	B-API
or	O
join	O
dataframes	O
based	O
on	O
column	O
data	O
?	O

I	O
am	O
trying	O
to	O
add	O
several	O
columns	O
of	O
data	O
to	O
an	O
existing	O
dataframe	B-API
.	O

The	O
dataframe	B-API
itself	O
was	O
built	O
from	O
a	O
number	O
of	O
other	O
dataframes	O
,	O
which	O
I	O
successfully	O
joined	O
on	O
indices	O
,	O
which	O
were	O
identical	O
.	O

For	O
that	O
,	O
I	O
used	O
code	O
like	O
this	O
:	O
#CODE	O

I	O
actually	O
joined	O
these	O
on	O
a	O
multi-index	O
,	O
so	O
the	O
dataframe	B-API
looks	O
something	O
like	O
the	O
following	O
,	O
where	O
Name1	O
and	O
Name	O
2	O
are	O
indices	O
:	O
#CODE	O

So	O
the	O
Name1	O
index	O
does	O
not	O
repeat	O
data	O
,	O
but	O
the	O
Name2	O
index	O
does	O
(	O
I'm	O
using	O
this	O
to	O
keep	O
track	O
of	O
dyads	O
,	O
so	O
that	O
Name1	O
Name2	O
together	O
are	O
only	O
represented	O
once	O
)	O
.	O

What	O
I	O
now	O
want	O
to	O
add	O
are	O
4	O
columns	O
of	O
data	O
that	O
correspond	O
to	O
Name2	O
data	O
(	O
information	O
on	O
the	O
second	O
member	O
of	O
the	O
dyad	O
)	O
.	O

Unlike	O
the	O
"	O
present	O
"	O
"	O
r	O
"	O
and	O
"	O
behavior	O
"	O
data	O
,	O
these	O
data	O
are	O
per	O
individual	O
,	O
not	O
per	O
dyad	O
.	O

So	O
I	O
don't	O
need	O
to	O
consider	O
Name1	O
data	O
when	O
merging	O
.	O

The	O
problem	O
is	O
that	O
while	O
Name2	O
data	O
are	O
repeated	O
to	O
exhaust	O
the	O
dyad	O
combos	O
,	O
the	O
"	O
Name2	O
"	O
column	O
in	O
the	O
data	O
I	O
would	O
now	O
like	O
to	O
add	O
only	O
has	O
one	O
piece	O
of	O
data	O
per	O
Name2	O
individual	O
:	O
#CODE	O

What	O
I	O
would	O
like	O
the	O
output	O
to	O
look	O
like	O
:	O
#CODE	O

Despite	O
reading	O
the	O
documentation	O
,	O
I	O
am	O
not	O
clear	O
on	O
whether	O
I	O
can	O
use	O
join()	O
or	O
merge()	B-API
for	O
the	O
desired	O
outcome	O
.	O

If	O
I	O
try	O
a	O
join	O
to	O
the	O
existing	O
dataframe	B-API
like	O
the	O
simple	O
one	O
I've	O
used	O
previously	O
,	O
I	O
end	O
up	O
with	O
the	O
new	O
columns	O
but	O
they	O
are	O
full	O
of	O
NaN	O
values	O
.	O

I've	O
also	O
tried	O
various	O
combinations	O
using	O
Name1	O
and	O
Name2	O
as	O
either	O
columns	O
or	O
as	O
indices	O
,	O
with	O
either	O
join	O
or	O
merge	B-API
(	O
not	O
as	O
random	O
as	O
it	O
sounds	O
,	O
but	O
I'm	O
clearly	O
not	O
interpreting	O
the	O
documentation	O
correctly	O
!	O
)	O
.	O

Your	O
help	O
would	O
be	O
very	O
much	O
appreciated	O
,	O
as	O
I	O
am	O
presently	O
very	O
much	O
lost	O
.	O

I'm	O
not	O
sure	O
if	O
this	O
is	O
the	O
best	O
way	O
,	O
but	O
you	O
could	O
use	O
`	O
reset_index	B-API
`	O
to	O
temporarily	O
make	O
your	O
original	O
DataFrame	B-API
indexed	O
by	O
`	O
Name2	O
`	O
only	O
.	O

Then	O
you	O
could	O
perform	O
the	O
`	O
join	B-API
`	O
as	O
usual	O
.	O

Then	O
use	O
`	O
set_index	B-API
`	O
to	O
again	O
make	O
`	O
Name1	O
`	O
part	O
of	O
the	O
MultiIndex	O
:	O
#CODE	O

To	O
make	O
the	O
result	O
look	O
even	O
more	O
like	O
your	O
desired	O
DataFrame	B-API
,	O
you	O
could	O
reorder	O
and	O
sort	O
the	O
index	O
:	O
#CODE	O

This	O
works	O
-	O
I	O
must	O
have	O
been	O
setting	O
the	O
wrong	O
parameters	O
as	O
this	O
is	O
one	O
of	O
the	O
approaches	O
I	O
attempted	O
.	O

Good	O
to	O
know	O
I'm	O
not	O
wandering	O
in	O
the	O
entirely	O
wrong	O
direction	O
.	O

)	O

Export	O
Pandas	O
data	O
frame	O
with	O
text	O
column	O
containg	O
utf-8	O
text	O
and	O
URLs	O
to	O
Excel	O

My	O
Pandas	O
data	O
frame	O
consists	O
of	O
Tweets	O
and	O
meta	O
data	O
of	O
each	O
tweet	O
(	O
300.000	O
rows	O
)	O
.	O

Some	O
of	O
my	O
colleagues	O
need	O
to	O
work	O
with	O
this	O
data	O
in	O
Excel	O
which	O
is	O
why	O
I	O
need	O
to	O
export	O
it	O
.	O

I	O
wanted	O
to	O
use	O
either	O
`	O
.to_csv	B-API
`	O
or	O
`	O
.to_excel	B-API
`	O
which	O
are	O
both	O
provided	O
by	O
Pandas	O
but	O
I	O
can't	O
get	O
it	O
to	O
work	O
properly	O
.	O

When	O
I	O
use	O
`	O
.to_csv	B-API
`	O
my	O
problem	O
is	O
that	O
it	O
keeps	O
failing	O
in	O
the	O
text	O
part	O
of	O
the	O
data	O
frame	O
.	O

I've	O
played	O
around	O
with	O
different	O
separators	O
but	O
the	O
file	O
is	O
never	O
100%	O
aligned	O
.	O

The	O
text	O
column	O
seems	O
to	O
contain	O
tabs	O
,	O
pipe	O
characters	O
etc	O
.	O
which	O
confuses	O
Excel	O
.	O

#CODE	O

When	O
I	O
try	O
to	O
use	O
`	O
.to_excel	B-API
`	O
together	O
with	O
the	O
`	O
xlsxwriter	O
`	O
engine	O
I'm	O
confronted	O
with	O
a	O
different	O
problem	O
,	O
which	O
is	O
that	O
my	O
text	O
column	O
contains	O
to	O
many	O
URLs	O
(	O
I	O
think	O
)	O
.	O

`	O
xlswriter	O
`	O
tries	O
to	O
make	O
special	O
clickable	O
links	O
of	O
these	O
URLs	O
instead	O
of	O
just	O
handling	O
them	O
as	O
strings	O
.	O

I've	O
found	O
some	O
information	O
on	O
how	O
to	O
circumvent	O
this	O
but	O
,	O
again	O
,	O
I	O
can't	O
get	O
it	O
to	O
work	O
.	O

The	O
following	O
bit	O
of	O
code	O
should	O
be	O
used	O
to	O
disable	O
the	O
function	O
that	O
I	O
think	O
is	O
causing	O
trouble	O
:	O
#CODE	O

However	O
,	O
when	O
using	O
`	O
to_excel	B-API
`	O
I	O
can't	O
seem	O
to	O
adjust	O
this	O
setting	O
of	O
the	O
Workbook	O
object	O
before	O
I	O
load	O
the	O
data	O
frame	O
into	O
the	O
Excel	O
file	O
.	O

In	O
short	O
how	O
do	O
I	O
export	O
a	O
column	O
with	O
wildly	O
varying	O
text	O
from	O
a	O
Pandas	O
data	O
frame	O
to	O
something	O
that	O
Excel	O
understands	O
?	O

edit	O
:	O

example	O
:	O
#CODE	O

So	O
in	O
this	O
case	O
It	O
is	O
obviously	O
a	O
line	O
brake	O
that	O
is	O
my	O
data	O
.	O

I	O
will	O
try	O
to	O
find	O
some	O
more	O
examples	O
.	O

edit2	O
:	O
#CODE	O

Translation	O
of	O
Dutch	O
stuff	O
:	O

Errors	O
were	O
found	O
in	O
"	O
file	O
"	O
.	O

Here	O
follows	O
a	O
list	O
of	O
removed	O
records	O
:	O
removed	O
records	O
:	O
formula	O
of	O
the	O
part	O
/	O
xl	O
/	O
worksheets	O
/	O
sheet1.xml	O

can	O
you	O
give	O
us	O
a	O
sample	O
of	O
the	O
df	O
that	O
doesn't	O
work	O
?	O

@USER	O
I've	O
added	O
an	O
example	O
,	O
I	O
will	O
add	O
more	O
if	O
needed	O
.	O

I	O
don't	O
think	O
it	O
is	O
currently	O
possible	O
to	O
pass	O
XlsxWriter	O
constructor	O
options	O
via	O
the	O
Pandas	O
API	O
but	O
you	O
can	O
workaround	O
the	O
`	O
strings_to_url	O
`	O
issue	O
as	O
follows	O
:	O
#CODE	O

The	O
code	O
works	O
so	O
that	O
is	O
a	O
great	O
start	O
.	O

When	O
I	O
open	O
the	O
Excel	O
file	O
I	O
get	O
a	O
warning	O
that	O
it	O
needs	O
to	O
be	O
recovered	O
.	O

I've	O
put	O
the	O
log	O
file	O
in	O
my	O
post	O
for	O
better	O
formatting	O
.	O

You	O
get	O
a	O
warning	O
when	O
you	O
open	O
the	O
file	O
created	O
by	O
my	O
example	O
?	O

Yes	O
,	O
that	O
is	O
what	O
I	O
mean	O
.	O

I	O
don't	O
see	O
any	O
warning	O
when	O
I	O
open	O
the	O
file	O
and	O
there	O
really	O
isn't	O
any	O
reason	O
that	O
there	O
should	O
be	O
for	O
such	O
a	O
simple	O
test	O
case	O
.	O

Also	O
in	O
relation	O
to	O
the	O
warning	O
you	O
added	O
above	O
there	O
isn't	O
any	O
"	O
formula	O
"	O
part	O
in	O
`	O
/	O
xl	O
/	O
worksheets	O
/	O
sheet1.xml	O
`	O
.	O

Did	O
you	O
add	O
a	O
formula	O
or	O
other	O
data	O
to	O
the	O
dataframe	B-API
in	O
the	O
example	O
?	O

Well	O
I	O
was	O
testing	O
it	O
now	O
on	O
my	O
full	O
data	O
frame	O
.	O

I've	O
not	O
added	O
any	O
formulas	O
.	O

To	O
me	O
the	O
log	O
warning	O
seems	O
empty	O
.	O

Apparently	O
it	O
is	O
a	O
known	O
issue	O
/	O
bug	O
:	O
#URL	O
I	O
will	O
look	O
into	O
it	O
further	O
.	O

You	O
have	O
been	O
a	O
great	O
help	O
!!	O

Perhaps	O
there	O
is	O
some	O
data	O
in	O
the	O
dataframe	B-API
that	O
is	O
being	O
interpreted	O
as	O
a	O
formula	O
.	O

Try	O
adding	O
the	O
following	O
at	O
the	O
same	O
point	O
as	O
the	O
other	O
option	O
to	O
see	O
if	O
it	O
makes	O
a	O
difference	O
:	O
`	O
writer.book.strings_to_formulas	O
=	O
False	O
`	O
.	O

That	O
did	O
the	O
trick	O
!	O

SWEET	O
!	O

Running	O
alternative	O
version	O
of	O
Python	O
on	O
RedHat	O
6	O
produces	O
"	O
wrong	O
ELF	O
class	O
:	O
ELFCLASS64	O
"	O

I	O
inherited	O
some	O
python	O
code	O
that	O
I	O
need	O
to	O
modify	O
.	O

I	O
installed	O
python	O
2.7.10	O
on	O
my	O
RedHat	O
6	O
machine	O
and	O
ran	O
the	O
original	O
code	O
which	O
produced	O
results	O
different	O
from	O
those	O
that	O
were	O
generated	O
last	O
time	O
it	O
was	O
run	O
using	O
an	O
older	O
version	O
of	O
python	O
.	O

I	O
was	O
given	O
a	O
path	O
to	O
an	O
old	O
python	O
installation	O
and	O
tried	O
to	O
run	O
the	O
code	O
using	O
it	O
.	O

Got	O
errors	O
.	O

Here's	O
what	O
I've	O
done	O
so	O
far	O
:	O

This	O
is	O
the	O
path	O
to	O
the	O
old	O
python	O
install	O
(	O
2.7.8	O
):	O
#CODE	O

Tried	O
running	O
the	O
code	O
and	O
got	O
the	O
first	O
error	O
:	O
#CODE	O

Searched	O
for	O
`	O
libstdc++	O
.so	O
.6	O
`	O
and	O
found	O
it	O
in	O
`	O
/	O
usr	O
/	O
lib64	O
`	O
:	O
#CODE	O

So	O
I	O
modified	O
`	O
LD_LIBRARY_PATH	O
`	O
as	O
was	O
suggested	O
in	O
another	O
SO	O
post	O
:	O
#CODE	O

Tried	O
running	O
again	O
and	O
got	O
another	O
error	O
:	O
#CODE	O

How	O
can	O
I	O
get	O
rid	O
of	O
this	O
error	O
and	O
,	O
ultimately	O
,	O
be	O
able	O
to	O
run	O
the	O
old	O
python	O
version	O
with	O
`	O
pandas	O
`	O
?	O

Whatever	O
I	O
do	O
to	O
achieve	O
this	O
,	O
however	O
,	O
I	O
don't	O
want	O
to	O
mess	O
up	O
the	O
environment	O
for	O
the	O
current	O
version	O
of	O
python	O
I	O
am	O
using	O
(	O
which	O
,	O
in	O
case	O
this	O
is	O
relevant	O
,	O
was	O
installed	O
into	O
a	O
local	O
subdirectory	O
of	O
my	O
home	O
dir	O
using	O
`	O
miniconda	O
`)	O

The	O
version	O
of	O
python	O
you're	O
trying	O
to	O
use	O
wasn't	O
compiled	O
for	O
64	O
bit	O
,	O
so	O
it	O
isn't	O
linked	O
against	O
a	O
64	O
bit	O
`	O
libstdc++	O
`	O
.	O

The	O
proper	O
way	O
to	O
run	O
the	O
older	O
version	O
of	O
python	O
would	O
be	O
doing	O
an	O
install	O
on	O
the	O
machine	O
.	O

I'm	O
not	O
sure	O
how	O
Redhat	O
handles	O
multiple	O
package	O
versions	O
,	O
but	O
I'd	O
be	O
surprised	O
if	O
it	O
didn't	O
support	O
it	O
(	O
most	O
distributions	O
do	O
)	O
.	O

Other	O
than	O
that	O
,	O
you	O
would	O
need	O
to	O
find	O
a	O
host	O
that	O
the	O
older	O
version	O
is	O
installed	O
on	O
.	O

UnboundLocalError	O
in	O
ggplot	O
0.5	O

I	O
have	O
the	O
following	O
code	O
#CODE	O

and	O
I	O
get	O
the	O
following	O
error	O
:	O
#CODE	O

some	O
sample	O
data	O
#CODE	O

Hm	O
.	O

Can	O
you	O
try	O
:	O

```	O

import	O
ggplot	O

print	O
ggplot.__version__	O

```	O

And	O
perhaps	O
pasting	O
a	O
few	O
rows	O
of	O
your	O
data	O
would	O
also	O
help	O
reproduce	O
the	O
issue	O
you're	O
seeing	O
too	O
.	O

ggplot	O
version	O
0.5.0	O

It's	O
done	O
above	O
.	O

thanks	O

there's	O
a	O
space	O
in	O
your	O
x	O
variable	O
name	O
in	O
`	O
aes	O
`	O
but	O
not	O
in	O
your	O
data	O
frame	O
?	O

'	O
DHT	O
temp	O
'	O
vs	O
'	O
DHTtemp	O
'	O
.	O

Is	O
that	O
it	O
?	O

I	O
am	O
using	O
`	O
x	O
`	O
and	O
`	O
y	O
`	O
as	O
axis	O
indicators	O
.	O

Can	O
you	O
please	O
store	O
the	O
final	O
ggplot	O
object	O
(	O
`	O
p	O
=	O
p	O
+	O
...	O
`)	O
and	O
run	O
`	O
p.draw()	O
`	O
on	O
it	O
.	O

This	O
should	O
result	O
in	O
a	O
complete	O
stacktrace	O
insteed	O
of	O
only	O
the	O
error	O
message	O
.	O

I	O
think	O
this	O
is	O
a	O
bug	O
in	O
gpplots	O
code	O
,	O
so	O
opening	O
an	O
issue	O
at	O
#URL	O
would	O
be	O
nice	O
.	O

:-)	O

Great	O
.	O

Thx	O
for	O
posting	O
some	O
data	O
.	O

Unfortunately	O
,	O
I'm	O
not	O
able	O
to	O
recreate	O
the	O
issue	O
on	O
my	O
end	O
.	O

#CODE	O

The	O
plot	O
renders	O
for	O
me	O
without	O
the	O
exception	O
you're	O
seeing	O
.	O

Of	O
course	O
,	O
since	O
this	O
is	O
only	O
the	O
top	O
5	O
rows	O
,	O
the	O
plot	O
doesn't	O
really	O
describe	O
anything	O
interesting	O
.	O

Will	O
keep	O
digging	O
and	O
ask	O
a	O
few	O
people	O
on	O
my	O
team	O
if	O
they	O
know	O
what's	O
up	O
.	O

This	O
was	O
a	O
bug	O
in	O
ggplot	O
which	O
has	O
been	O
fixed	O
in	O
version	O
0.5.8	O
.	O

Pandas	O
for	O
Python	O
,	O
grouping	O

I	O
have	O
a	O
data	O
set	O
consisting	O
of	O
multiple	O
tuples	O
per	O
time	O
stamp	O
-	O
each	O
of	O
these	O
has	O
a	O
count	O
.	O

There	O
could	O
be	O
different	O
tuples	O
present	O
at	O
each	O
time	O
stamp	O
.	O

I	O
would	O
like	O
to	O
group	O
these	O
together	O
in	O
5	O
minute	O
bins	O
and	O
add	O
the	O
counts	O
for	O
each	O
unique	O
tuple	O
.	O

Is	O
there	O
a	O
nice	O
clean	O
way	O
to	O
do	O
this	O
using	O
Pandas	O
group-by	O
?	O

They	O
have	O
the	O
form	O
:	O

((	O
u'67	O
.163	O
.47	O
.231	O
'	O
,	O
u'8	O
.27	O
.82	O
.254	O
'	O
,	O
50186	O
,	O
80	O
,	O
6	O
,	O
1377565195000	O
)	O
,	O
2	O
)	O

This	O
is	O
currently	O
a	O
list	O
,	O
with	O
a	O
6-tuple	O
(	O
last	O
entry	O
is	O
time-stamp	O
)	O
,	O
and	O
then	O
count	O
.	O

There	O
will	O
be	O
a	O
collection	O
of	O
5-tuples	O
for	O
every	O
time	O
stamp	O
:	O

(	O
5-tuple	O
)	O
,	O
t-time-stamp	O
,	O
count	O
,	O
for	O
example	O
(	O
for	O
just	O
one	O
time	O
stamp	O
)	O
#CODE	O

or	O
converted	O
:	O
#CODE	O

Here's	O
what	O
seems	O
to	O
work	O
well	O
:	O
#CODE	O

Can	O
you	O
give	O
a	O
short	O
example	O
data	O
of	O
the	O
whole	O
dataframe	B-API
?	O

[((	O
u'71	O
.57	O
.43	O
.240	O
'	O
,	O
u'8	O
.27	O
.82	O
.254	O
'	O
,	O
33108	O
,	O
80	O
,	O
6	O
,	O
1377565195000	O
)	O
,	O
1	O
)	O
,	O

((	O
u'67	O
.163	O
.47	O
.231	O
'	O
,	O
u'8	O
.27	O
.82	O
.254	O
'	O
,	O
50186	O
,	O
80	O
,	O
6	O
,	O
1377565195000	O
)	O
,	O
2	O
)	O
,	O

((	O
u'8	O
.27	O
.82	O
.254	O
'	O
,	O
u'98	O
.206	O
.29	O
.242	O
'	O
,	O
25159	O
,	O
80	O
,	O
6	O
,	O
1377565195000	O
)	O
,	O
1	O
)	O
,	O

((	O
u'69	O
.66	O
.156	O
.250	O
'	O
,	O
u'8	O
.27	O
.82	O
.254	O
'	O
,	O
59274	O
,	O
80	O
,	O
6	O
,	O
1377565195000	O
)	O
,	O
3	O
)	O
,	O

((	O
u'76	O
.16	O
.235	O
.239	O
'	O
,	O
u'8	O
.27	O
.84	O
.126	O
'	O
,	O
48104	O
,	O
80	O
,	O
6	O
,	O
1377565195000	O
)	O
,	O
1	O
)	O
,	O

((	O
u'8	O
.27	O
.84	O
.254	O
'	O
,	O
u'98	O
.226	O
.117	O
.227	O
'	O
,	O
63795	O
,	O
80	O
,	O
6	O
,	O
1377565195000	O
)	O
,	O
2	O
)	O
,	O

((	O
u'24	O
.1	O
.153	O
.243	O
'	O
,	O
u'8	O
.27	O
.82	O
.126	O
'	O
,	O
18970	O
,	O
80	O
,	O
6	O
,	O
1377565195000	O
)	O
,	O
1	O
)	O
,	O

((	O
u'76	O
.16	O
.101	O
.243	O
'	O
,	O
u'8	O
.27	O
.82	O
.126	O
'	O
,	O
41329	O
,	O
80	O
,	O
6	O
,	O
1377565195000	O
)	O
,	O
1	O
)	O
,	O

You	O
can	O
edit	O
you	O
question	O
to	O
include	O
it	O
instead	O
of	O
as	O
a	O
comment	O
.	O

Where	O
is	O
the	O
timestamp	O
in	O
this	O
example	O
?	O

last	O
entry	O
in	O
the	O
6-tuple	O

Do	O
you	O
already	O
put	O
it	O
in	O
a	O
pandas	O
DataFrame	B-API
?	O

How	O
does	O
that	O
look	O
like	O
(	O
is	O
the	O
tuple	O
one	O
column	O
)	O
?	O

I've	O
not	O
put	O
these	O
into	O
a	O
DataFrame	B-API
yet	O
.	O

Given	O
this	O
is	O
a	O
list	O
(	O
or	O
a	O
dictionary	O
if	O
I	O
don't	O
sort	O
on	O
the	O
time	O
stamp	O
)	O
.	O

But	O
the	O
intent	O
would	O
be	O
to	O
have	O
the	O
tuple	O
as	O
a	O
column	O
.	O

But	O
then	O
use	O
the	O
time-stamp	O
as	O
a	O
time-series	O
-	O
to	O
really	O
the	O
first	O
5-tuple	O
is	O
the	O
unique	O
key	O
,	O
then	O
the	O
time-stamp	O
,	O
and	O
the	O
count	O
.	O

How	O
is	O
the	O
timestamp	O
defined	O
?	O

How	O
can	O
it	O
be	O
converted	O
to	O
a	O
datetime	O
?	O

The	O
timestamp	O
is	O
time	O
since	O
the	O
UNIX	O
epoch	O
-	O
can	O
be	O
converted	O
to	O
datetime	O

In	O
[	O
226	O
]:	O
df	O

Out	O
[	O
226	O
]:	O

data1	O
data2	O
key1	O

0	O
1	O
1377565195000	O
(	O
71.57.43.240	O
,	O
8.27.82.254	O
,	O
33108	O
,	O
80	O
,	O
6	O
)	O

1	O
2	O
1377565195000	O
(	O
67.163.47.231	O
,	O
8.27.82.254	O
,	O
50186	O
,	O
80	O
,	O
6	O
)	O

Do	O
you	O
only	O
have	O
timestamps	O
every	O
5	O
mins	O
?	O

Or	O
do	O
you	O
have	O
to	O
combine	O
several	O
minutes	O
in	O
one	O
5	O
min	O
bins	O
?	O

The	O
data	O
needs	O
to	O
be	O
grouped	O
(	O
binned	O
)	O
into	O
5min	O
bins	O
.	O

There	O
can	O
be	O
multiple	O
(	O
100's	O
of	O
5-tuples	O
or	O
keys	O
)	O
per	O
time	O
step	O
-	O
and	O
these	O
can	O
change	O
from	O
time	O
step	O
to	O
time	O
step	O
-	O
for	O
each	O
unique	O
key	O
or	O
tuple	O
,	O
want	O
to	O
add	O
together	O
the	O
counts	O
(	O
data1	O
)	O

Can	O
you	O
give	O
a	O
more	O
elaborate	O
example	O
dataset	O
,	O
and	O
also	O
give	O
an	O
example	O
of	O
the	O
desired	O
output	O

Here	O
is	O
a	O
simpler	O
example	O
that	O
illustrates	O
the	O
idea	O
:	O

time	O
count	O
city	O

00:00	O
:	O
00	O
1	O
Montreal	O

00:00	O
:	O
00	O
2	O
New	O
York	O

00:00	O
:	O
00	O
1	O
Chicago	O

00:01	O
:	O
00	O
2	O
Montreal	O

00:01	O
:	O
00	O
3	O
New	O
York	O

after	O
bin-ing	O
into	O
5min	O
bins	O

time	O
count	O
city	O

00:05	O
:	O
00	O
3	O
Montreal	O

00:05	O
:	O
00	O
5	O
New	O
York	O

00:05	O
:	O
00	O
1	O
Chicago	O

time	O
count	O
city	O

00:00	O
:	O
00	O
1	O
Montreal	O

00:00	O
:	O
00	O
2	O
New	O
York	O

00:00	O
:	O
00	O
1	O
Chicago	O

00:01	O
:	O
00	O
2	O
Montreal	O

00:01	O
:	O
00	O
3	O
New	O
York	O

after	O
bin-ing	O

If	O
you	O
just	O
want	O
to	O
add	O
together	O
the	O
counts	O
for	O
each	O
unique	O
tuple	O
,	O
just	O
groupby	B-API
`	O
key1	O
`	O
:	O
#CODE	O

If	O
you	O
want	O
to	O
do	O
this	O
for	O
each	O
time	O
step	O
and	O
each	O
unique	O
tuple	O
,	O
you	O
can	O
give	O
multiple	O
column	O
to	O
group	O
by	O
:	O
#CODE	O

If	O
different	O
timesteps	O
have	O
to	O
be	O
combined	O
in	O
one	O
5min	O
bin	O
,	O
a	O
possible	O
approach	O
is	O
to	O
round	O
your	O
timestamp	O
to	O
5	O
min	O
,	O
and	O
then	O
group	O
by	O
on	O
that	O
:	O
#CODE	O

If	O
you	O
want	O
to	O
preserve	O
some	O
of	O
the	O
original	O
timestamps	O
(	O
but	O
you	O
have	O
to	O
choose	O
which	O
if	O
you	O
are	O
binning	O
them	O
)	O
,	O
you	O
can	O
specify	O
a	O
function	O
to	O
apply	B-API
on	O
the	O
individual	O
columns	O
.	O

For	O
example	O
take	O
the	O
first	O
:	O
#CODE	O

If	O
you	O
just	O
want	O
to	O
resample	B-API
on	O
5	O
mins	O
and	O
add	O
the	O
counts	O
regardless	O
of	O
the	O
keys	O
,	O
you	O
can	O
simply	O
do	O
:	O
#CODE	O

The	O
keys	O
are	O
unique	O
(	O
but	O
may	O
not	O
appear	O
every	O
time	O
step	O
)	O
.	O

I	O
would	O
like	O
to	O
add	O
the	O
data1	O
(	O
counts	O
)	O
together	O
for	O
each	O
unique	O
key	O
-	O
and	O
then	O
bin	O
these	O
into	O
5min	O
bins	O
-	O
so	O
the	O
second	O
approach	O

So	O
the	O
second	O
approach	O
is	O
what	O
you	O
want	O
?	O

Or	O
not	O
yet	O
?	O

This	O
looks	O
way	O
nicer	O
than	O
mine	O
,	O
but	O
it	O
doesn't	O
bin	O
into	O
5	O
minute	O
intervals	O
(	O
I	O
guess	O
you	O
could	O
make	O
a	O
column	O
with	O
rounded	O
times	O
?	O
)	O

@USER	O
Yeah	O
,	O
that's	O
what	O
I	O
was	O
trying	O
(	O
rounded	O
times	O
)	O
,	O
but	O
it	O
is	O
not	O
clear	O
to	O
me	O
if	O
it	O
is	O
needed	O
(	O
maybe	O
all	O
the	O
times	O
are	O
already	O
5-mins	O
,	O
and	O
then	O
it	O
is	O
just	O
a	O
simple	O
groupby	B-API
)	O

@USER	O
Oops	O
,	O
mislooking	O
at	O
the	O
seconds	O
/	O
minutes	O
,	O
so	O
it	O
is	O
certainly	O
not	O
yet	O
in	O
5-min	O
times	O
..	O

Yeah	O
,	O
as	O
usual	O
here	O
example	O
not	O
the	O
best	O
...	O

Not	O
sure	O
how	O
you	O
do	O
the	O
rounded	O
times	O
(	O
will	O
upvote	O
when	O
you	O
get	O
it	O
:p	O
)	O
.	O

I	O
would	O
want	O
the	O
second	O
approach	O
-	O
but	O
it	O
groups	O
together	O
the	O
keys	O
?	O

ideally	O
would	O
want	O
to	O
keep	O
the	O
time-stamp	O
intact	O
,	O
for	O
each	O
unique	O
key	O
-	O
then	O
I	O
understood	O
that	O
the	O
time	O
stamp	O
resample()	B-API
could	O
handle	O
mutiple	O
keys	O
when	O
resampling	O
?	O

Yes	O
,	O
it	O
groups	O
together	O
the	O
*	O
unique	O
*	O
keys	O
(	O
the	O
tuples	O
)	O
.	O

Isn't	O
that	O
what	O
you	O
want	O
?	O

How	O
can	O
you	O
keep	O
the	O
time-stamp	O
intact	O
if	O
you	O
want	O
to	O
bin	O
your	O
data	O
every	O
5	O
min	O
?	O

OK	O
,	O
if	O
you	O
want	O
to	O
keep	O
the	O
time	O
stamp	O
intact	O
,	O
you	O
should	O
not	O
group	O
by	O
it	O
and	O
take	O
the	O
first	O
approach	O
.	O

I've	O
read	O
that	O
the	O
time	O
series	B-API
resample	B-API
can	O
work	O
with	O
repeated	O
time	O
stamps	O
-	O
I'd	O
really	O
just	O
like	O
to	O
aggregate	O
so	O
that	O
the	O
counts	O
(	O
data1	O
)	O
are	O
added	O
when	O
creating	O
a	O
5	O
min	O
time	O
bin	O
.	O

In	O
this	O
example	O
there	O
were	O
just	O
2	O
time	O
stamps	O
(	O
same	O
value	O
)	O
.	O

A	O
larger	O
data	O
set	O
would	O
have	O
1000's	O
or	O
more	O
time	O
stamps	O
for	O
keys	O
,	O
and	O
a	O
cound	O
for	O
each	O
.	O

Would	O
like	O
to	O
sum	O
the	O
counts	O
for	O
unique	O
keys	O
when	O
binned	O

OK	O
,	O
I	O
get	O
that	O
,	O
but	O
the	O
last	O
approach	O
does	O
this	O
?	O

What	O
do	O
you	O
mean	O
with	O
keep	O
the	O
timestamp	O
intact	O
?	O

If	O
you	O
have	O
two	O
rows	O
with	O
a	O
unique	O
key	O
but	O
different	O
timestamps	O
in	O
the	O
same	O
5min	O
bin	O
,	O
which	O
of	O
the	O
two	O
timestamps	O
you	O
want	O
to	O
preserve	O
?	O

by	O
time-stamp	O
intact	O
,	O
just	O
meant	O
that	O
it	O
is	O
included	O
in	O
the	O
time	O
series	B-API
or	O
dataframe	B-API
,	O
even	O
after	O
aggregation	O

If	O
you	O
"	O
aggregate	O
"	O
for	O
different	O
timestamps	O
,	O
you	O
cannot	O
just	O
include	O
it	O
(	O
because	O
for	O
each	O
aggregated	O
count	O
,	O
you	O
have	O
multiple	O
and	O
different	O
timestamps	O
)	O
.	O

How	O
do	O
you	O
want	O
it	O
included	O
?	O

Can	O
you	O
give	O
a	O
more	O
elaborate	O
example	O
dataset	O
(	O
with	O
different	O
timestamps	O
)	O
,	O
and	O
also	O
give	O
an	O
example	O
of	O
the	O
desired	O
output	O
.	O

would	O
resample()	B-API
be	O
able	O
to	O
create	O
the	O
5min	O
bins	O
?	O

even	O
when	O
there	O
are	O
repeated	O
time	O
stamps	O

yes	O
off	O
course	O
,	O
but	O
you	O
also	O
want	O
to	O
group	O
by	O
key	O
(	O
tuple	O
)	O
?	O

+1	O
goodness	O
,	O
that's	O
an	O
awful	O
hack	O
to	O
round	O
dates	O
:)	O

@USER	O
,	O
indeed	O
...	O

That's	O
certainly	O
something	O
that	O
can	O
be	O
improved	O
with	O
a	O
handy	O
function	O
:-)	O

I	O
want	O
add	O
the	O
counts	O
(	O
data1	O
)	O
associated	O
with	O
a	O
unique	O
key	O
-	O
so	O
if	O
a	O
(	O
count	O
,	O
key	O
)	O
appears	O
for	O
a	O
time	O
stamp	O
in	O
any	O
5	O
min	O
interval	O
,	O
the	O
counts	O
are	O
added	O
.	O

Do	O
not	O
want	O
to	O
add	O
counts	O
for	O
all	O
keys	O
in	O
the	O
5	O
min	O
bin	O
.	O

OK	O
,	O
but	O
in	O
that	O
case	O
,	O
I	O
think	O
my	O
answer	O
is	O
working	O
(	O
with	O
the	O
rounding	O
)	O
?	O

@USER	O
Lol	O
,	O
another	O
hack	O
to	O
round	O
:-)	O
`	O
df	O
[	O
'	O
data2	O
']	O
.map	B-API
(	O
lambda	O
x	O
:	O
pd.DataFrame	B-API
([	O
0	O
]	O
,	O
index	O
=p	O
d.DatetimeIndex	O
([	O
x	O
]))	O
.resample	B-API
(	O
'	O
5min	O
')	O
.index	B-API
[	O
0	O
])`	O

in	O
the	O
case	O
of	O
df.set_index	B-API
(	O
'	O
data2	O
'	O
,	O
inplace=True	O
)	O

df.resample	B-API
(	O
'	O
5min	O
'	O
,	O
'	O
sum	O
')	O
,	O
it	O
is	O
adding	O
toegther	O
counts	O
for	O
all	O
the	O
keys	O
-	O
want	O
to	O
keep	O
the	O
counts	O
for	O
the	O
unique	O
keys	O
separated	O

In	O
that	O
case	O
all	O
counts	O
are	O
added	O
,	O
also	O
for	O
non-unique	O
within	O
a	O
5	O
min	O
bin	O
.	O

So	O
that	O
is	O
not	O
what	O
you	O
want	O
.	O

But	O
my	O
third	O
code	O
block	O
is	O
doing	O
what	O
you	O
want	O
I	O
think	O
?	O

That	O
block	O
adds	O
together	O
the	O
counts	O
(	O
data1	O
)	O
for	O
all	O
keys	O
-	O
need	O
to	O
keep	O
the	O
unique	O
keys	O
separate	O
-	O
need	O
the	O
counts	O
added	O
for	O
each	O
unique	O
key	O
when	O
binned	O
-	O
so	O
in	O
this	O
case	O
would	O
not	O
change	O
-	O
need	O
to	O
have	O
more	O
time	O
stamps	O
that	O
span	O
several	O
minutes	O

This	O
``	O
df.groupby	B-API
([	O
'	O
data2_5min	O
'	O
,	O
'	O
key1	O
'])	O
.aggregate	O
(	O
'	O
sum	O
')``	O
keeps	O
the	O
keys	O
seperate	O
,	O
and	O
adds	O
the	O
counts	O
within	O
each	O
5	O
min	O
bin	O
together	O
for	O
the	O
unique	O
keys	O

Thanks	O
so	O
much	O
!	O

I	O
will	O
take	O
a	O
look	O

Would	O
it	O
be	O
possible	O
let	O
resample()	B-API
create	O
the	O
5min	O
bins	O
-	O
afterwards	O
and	O
preserve	O
the	O
unique	O
keys	O
(	O
along	O
with	O
their	O
counts	O
)	O
.	O

This	O
would	O
allow	O
for	O
example	O
,	O
to	O
create	O
2	O
min	O
,	O
10	O
min	O
bins	O
using	O
resample	B-API
-	O
and	O
look	O
at	O
the	O
data	O
over	O
different	O
bin	O
sizes	O
(	O
time	O
scales	O
)	O

I've	O
posted	O
a	O
much	O
simpler	O
example	O
(	O
based	O
on	O
cities	O
as	O
keys	O
)	O
above	O

If	O
you	O
set	O
the	O
date	O
as	O
the	O
index	O
you	O
can	O
use	O
TimeGrouper	O
(	O
which	O
allows	O
you	O
to	O
group	O
by	O
,	O
for	O
example	O
,	O
5	O
minute	O
intervals	O
):	O
#CODE	O

You	O
can	O
then	O
count	O
the	O
number	O
of	O
unique	O
items	O
in	O
each	O
5	O
minute	O
interval	O
using	O
nunique	B-API
:	O
#CODE	O

If	O
you're	O
looking	O
for	O
a	O
count	O
of	O
each	O
tuple	O
,	O
you	O
could	O
use	O
value_counts	B-API
:	O
#CODE	O

Note	O
:	O
this	O
is	O
a	O
Series	B-API
with	O
a	O
MultiIndex	O
(	O
use	O
reset_index	B-API
to	O
make	O
it	O
a	O
DataFrame	B-API
)	O
.	O

#CODE	O

You'll	O
probably	O
want	O
to	O
give	O
these	O
more	O
informative	O
column	O
names	O
:)	O
.	O

Update	O
:	O
previously	O
I	O
hacked	O
get	O
`	O
get_dummies	B-API
`	O
,	O
see	O
edit	O
history	O
.	O

I	O
?	O

get_dummies	B-API
,	O
but	O
there	O
is	O
probably	O
a	O
short-cut	O
for	O
`	O
get_dummies	B-API
(	O
x	O
)	O
.sum()	B-API
`	O
lol	O
it's	O
value_counts	B-API

Nice	O
trick	O
with	O
the	O
TimeGrouper	O
!	O

But	O
I	O
think	O
he	O
wants	O
to	O
'	O
add	O
the	O
counts	O
'	O
(	O
which	O
is	O
an	O
existing	O
column	O
)	O
,	O
and	O
not	O
to	O
'	O
count	O
the	O
unique	O
keys	O
'	O

Yeah	O
,	O
it	O
should	O
be	O
in	O
the	O
docs	O
!	O

(	O
Also	O
seems	O
a	O
shame	O
you	O
can't	O
do	O
it	O
to	O
a	O
column	O
...	O
)	O

(	O
you	O
could	O
be	O
right	O
:	O
maybe	O
I	O
don't	O
follow	O
the	O
question	O
...	O
)	O

Very	O
nice	O
,	O
thanks	O
.	O

But	O
for	O
sure	O
,	O
the	O
counts	O
need	O
to	O
be	O
added	O

@USER	O
I	O
don't	O
understand	O
what	O
you	O
mean	O
,	O
this	O
does	O
count	O
the	O
number	O
of	O
occurrences	O
.	O

If	O
this	O
isn't	O
what	O
you	O
want	O
could	O
you	O
include	O
the	O
desired	O
outcome	O
in	O
your	O
question	O
?	O

Sorry	O
-	O
I	O
want	O
to	O
add	O
the	O
counts	O
(	O
data1	O
column	O
)	O
for	O
unique	O
keys	O
-	O
when	O
binning	O
into	O
5	O
min	O
bins	O
-	O

in	O
this	O
example	O
there	O
was	O
just	O
one	O
time	O
stamp	O
-	O
but	O
there	O
will	O
be	O
1000's	O
of	O
time	O
stamps	O
across	O
several	O
minutes	O
up	O
to	O
an	O
hour	O
say	O
.	O

this	O
groups	O
these	O
timestamps	O
into	O
5	O
minute	O
bins	O

ah	O
you	O
want	O
to	O
groupby	B-API
data1	O
too	O
(	O
i.e.	O
my	O
solution	O
but	O
also	O
separated	O
by	O
data1	O
)	O

essentially	O
add	O
(	O
aggregate	O
)	O
the	O
data1	O
(	O
counts	O
)	O
for	O
each	O
unique	O
key	O
,	O
into	O
5	O
min	O
bins	O
.	O

so	O
when	O
there	O
are	O
enough	O
time	O
stamps	O
(	O
data2	O
)	O
spanning	O
more	O
than	O
5	O
minutes	O
(	O
e.g	O
an	O
hour	O
)	O
,	O
add	O
together	O
the	O
counts	O
(	O
data1	O
)	O
for	O
each	O
of	O
the	O
unique	O
keys	O
appearing	O
(	O
in	O
a	O
given	O
5min	O
bin	O
or	O
interval	O
)	O
.	O

The	O
difficulty	O
is	O
that	O
not	O
all	O
keys	O
will	O
appear	O
or	O
be	O
present	O
for	O
any	O
given	O
time	O
stamp	O

When	O
plotting	O
datetime	O
index	O
data	O
,	O
put	O
markers	O
in	O
the	O
plot	O
on	O
specific	O
days	O
(	O
e.g.	O
weekend	O
)	O

I	O
create	O
a	O
pandas	O
dataframe	B-API
with	O
a	O
DatetimeIndex	B-API
like	O
so	O
:	O
#CODE	O

Which	O
gives	O
#CODE	O

I	O
can	O
easily	O
plot	O
the	O
`	O
A	O
`	O
colum	O
with	O
pandas	O
by	O
doing	O
:	O
#CODE	O

which	O
plots	O
a	O
line	O
of	O
the	O
`	O
A	O
`	O
column	O
but	O
leaves	O
out	O
the	O
`	O
weekend	O
`	O
column	O
as	O
it	O
does	O
not	O
hold	O
numerical	O
data	O
.	O

How	O
can	O
I	O
put	O
a	O
"	O
marker	O
"	O
on	O
each	O
spot	O
of	O
the	O
`	O
A	O
`	O
column	O
where	O
the	O
`	O
weekend	O
`	O
column	O
has	O
the	O
value	O
`	O
yes	O
`	O
?	O

Meanwhile	O
I	O
found	O
out	O
,	O
it	O
is	O
as	O
simple	O
as	O
using	O
boolean	O
indexing	O
in	O
pandas	O
.	O

Doing	O
the	O
plot	O
directly	O
with	O
pyplot	O
instead	O
of	O
pandas	O
'	O
own	O
plot	O
wrapper	O
(	O
which	O
is	O
more	O
convenient	O
to	O
me	O
):	O
#CODE	O

Now	O
,	O
the	O
red	O
dots	O
mark	O
all	O
weekend	O
days	O
which	O
are	O
given	O
by	O
`	O
df.weekend	O
=	O
'	O
yes	O
'`	O
values	O
.	O

You	O
could	O
shorten	O
it	O
a	O
smidge	O
with	O
`	O
df	O
[	O
'	O
A	O
']	O
[	O
df	O
[	O
'	O
weekend	O
']	O
==	O
'	O
yes	O
']	O
.plot	B-API
(	O
style=	O
'	O
ro	O
')`	O
which	O
is	O
a	O
bit	O
more	O
readable	O
(	O
IMO	O
)	O
but	O
still	O
,	O
nice	O
self-answer	O
.	O

pandas	O
applying	O
regex	O
to	O
replace	O
values	O

I	O
have	O
read	O
some	O
pricing	O
data	O
into	O
a	O
pandas	O
dataframe	B-API
the	O
values	O
appear	O
as	O
:	O
#CODE	O

I	O
want	O
to	O
strip	O
it	O
down	O
to	O
just	O
the	O
numeric	O
values	O
.	O

I	O
know	O
I	O
can	O
loop	O
through	O
and	O
apply	B-API
regex	O
#CODE	O

to	O
each	O
field	O
then	O
join	O
the	O
resulting	O
list	O
back	O
together	O
but	O
is	O
there	O
a	O
not	O
loopy	O
way	O
?	O

Thanks	O

You	O
could	O
remove	O
all	O
the	O
non-digits	O
using	O
`	O
re.sub()	O
`	O
:	O
#CODE	O

regex101	O
demo	O

`	O
\D+	O
`	O
will	O
be	O
the	O
smallest	O
:-P	O

whats	O
the	O
best	O
way	O
to	O
apply	B-API
it	O
to	O
the	O
column	O
in	O
the	O
dataframe	B-API
?	O

so	O
I	O
have	O
df	O
[	O
'	O
pricing	O
']	O
do	O
I	O
just	O
loop	O
row	O
by	O
row	O
?	O

@USER	O
I	O
don't	O
have	O
much	O
experience	O
with	O
pandas	O
,	O
but	O
I	O
think	O
that	O
you	O
should	O
be	O
able	O
to	O
use	O
it	O
like	O
this	O
:	O
`	O
df	O
[	O
'	O
pricing	O
']	O
=	O
re.sub	O
(	O
r	O
"	O
[	O
^	O
0-9	O
]	O
+	O
"	O
,	O
""	O
,	O
df	O
[	O
'	O
pricing	O
'])`	O
.	O

@USER	O
Smaller	O
doesn't	O
necessarily	O
mean	O
better	O
.	O

`	O
\D	O
`	O
tends	O
to	O
be	O
slower	O
and	O
that's	O
not	O
better	O
at	O
all	O
.	O

ok	O
I	O
think	O
I	O
got	O
it	O
for	O
pandas	O
use	O
:	O
df	O
[	O
'	O
Pricing	O
']	O
.replace	B-API
(	O
to_replace=	O
'	O
[	O
^	O
0-9	O
]	O
+	O
'	O
,	O
value=	O
''	O
,	O
inplace	O
==	O
True	O
,	O
regex=True	O
)	O
the	O
.replace	B-API
method	O
uses	O
re.sub	O

@USER	O
Nice	O
,	O
thanks	O
for	O
letting	O
me	O
know	O
too	O
:)	O

You	O
could	O
use	O
`	O
Series.str.replace	B-API
`	O
:	O
#CODE	O

yields	O
#CODE	O

Pandas	O
groupby	B-API
:	O
How	O
do	O
I	O
use	O
shifted	O
values	O

I	O
have	O
a	O
dataset	O
that	O
represents	O
reoccurring	O
events	O
at	O
different	O
locations	O
.	O

#CODE	O

Each	O
location	O
can	O
have	O
8-10	O
events	O
that	O
repeat	O
.	O

What	O
I'm	O
trying	O
to	O
do	O
is	O
build	O
some	O
information	O
of	O
how	O
long	O
it	O
has	O
been	O
between	O
two	O
events	O
.	O

(	O
they	O
may	O
not	O
be	O
the	O
same	O
event	O
)	O

I	O
am	O
able	O
to	O
do	O
this	O
by	O
splitting	O
the	O
df	O
into	O
sub-dfs	O
and	O
processing	O
each	O
location	O
individually	O
.	O

But	O
it	O
would	O
seem	O
that	O
groupby	B-API
should	O
be	O
smarter	O
that	O
this	O
.	O

This	O
is	O
also	O
assuming	O
that	O
I	O
know	O
all	O
the	O
locations	O
which	O
may	O
vary	O
file	O
to	O
file	O
.	O

#CODE	O

What	O
I	O
would	O
like	O
to	O
do	O
is	O
groupBy	B-API
based	O
on	O
location	O
...	O

#CODE	O

Then	O
for	O
each	O
grouped	O
location	O

Add	O
a	O
delta	O
column	O

Shift	O
and	O
subtract	O
to	O
get	O
the	O
delta	O
time	O
between	O
events	O

Questions	O
:	O

Does	O
groupby	B-API
maintain	O
the	O
order	O
of	O
events	O
?	O

Would	O
a	O
for	O
loop	O
that	O
runs	O
over	O
the	O
DF	O
be	O
better	O
?	O

That	O
doesn't	O
seem	O
very	O
python	O
like	O
.	O

Also	O
once	O
you	O
have	O
a	O
grouped	O
df	O
is	O
there	O
a	O
way	O
to	O
transform	O
it	O
back	O
to	O
a	O
general	O
dataframe	B-API
.	O

I	O
don't	O
think	O
I	O
need	O
to	O
do	O
this	O
but	O
thought	O
it	O
may	O
be	O
helpful	O
in	O
the	O
future	O
.	O

Thank	O
you	O
for	O
any	O
support	O
you	O
can	O
offer	O
.	O

#URL	O
looks	O
like	O
it	O
provides	O
what	O
you	O
need	O
.	O

#CODE	O

or	O
#CODE	O

Will	O
split	O
it	O
into	O
groups	O
of	O
rows	O
with	O
matching	O
values	O
in	O
the	O
location	O
column	O
.	O

Then	O
you	O
can	O
sort	O
the	O
groups	O
based	O
on	O
the	O
time	O
value	O
and	O
iterate	O
through	O
to	O
create	O
the	O
deltas	O
.	O

JohnB	O
-	O
Yes	O
I	O
know	O
how	O
to	O
group-by	O
based	O
on	O
the	O
location	O
.	O

That	O
it	O
actually	O
included	O
as	O
part	O
of	O
the	O
question	O
.	O

But	O
when	O
you	O
group	O
by	O
you	O
end	O
up	O
with	O
a	O
group-by	O
object	O
which	O
you	O
can	O
iterate	O
over	O
.	O
for	O
a	O
specific	O
group	O
is	O
there	O
a	O
way	O
to	O
apply	B-API
a	O
shift	O
to	O
it	O
.	O

Or	O
do	O
I	O
need	O
to	O
iterate	O
over	O
it	O
.	O

Also	O
once	O
you	O
have	O
the	O
groups	O
there	O
seems	O
to	O
be	O
options	O
to	O
process	O
the	O
data	O
but	O
not	O
add	O
another	O
column	O
to	O
the	O
group	O
.	O

This	O
is	O
kind	O
of	O
where	O
I'm	O
stuck	O
.	O

It	O
appears	O
that	O
when	O
you	O
group-by	O
and	O
identify	O
a	O
column	O
to	O
act	O
on	O
the	O
data	O
is	O
returned	O
in	O
a	O
series	B-API
which	O
then	O
a	O
function	O
can	O
be	O
applied	O
to	O
.	O

#CODE	O

This	O
groups	O
by	O
location	O
and	O
returns	O
the	O
time	O
column	O
for	O
each	O
group	O
.	O

Each	O
sub-series	O
is	O
then	O
passed	O
to	O
the	O
function	O
deltaTime	O
.	O

Why	O
does	O
a	O
pandas	O
Series	B-API
of	O
DataFrame	B-API
mean	O
fail	O
,	O
but	O
sum	O
does	O
not	O
,	O
and	O
how	O
to	O
make	O
it	O
work	O

There	O
may	O
be	O
a	O
smarter	O
way	O
to	O
do	O
this	O
in	O
Python	O
Pandas	O
,	O
but	O
the	O
following	O
example	O
should	O
,	O
but	O
doesn't	O
work	O
:	O
#CODE	O

I	O
won't	O
post	O
the	O
whole	O
traceback	O
,	O
but	O
the	O
main	O
error	O
message	O
is	O
interesting	O
:	O
#CODE	O

It	O
looks	O
like	O
the	O
dataframe	B-API
was	O
successfully	O
summed	O
,	O
but	O
not	O
divided	O
by	O
the	O
length	O
of	O
the	O
series	B-API
.	O

However	O
,	O
we	O
can	O
take	O
the	O
sum	O
of	O
the	O
dataframes	O
in	O
the	O
series	B-API
:	O
#CODE	O

Returns	O
#CODE	O

Why	O
wouldn't	O
mean	O
work	O
when	O
sum	O
does	O
?	O

Is	O
this	O
a	O
bug	O
or	O
a	O
missing	O
feature	O
?	O

This	O
does	O
work	O
:	O
#CODE	O

And	O
so	O
does	O
this	O
:	O
#CODE	O

But	O
this	O
of	O
course	O
is	O
not	O
ideal	O
.	O

Unclear	O
if	O
it	O
should	O
work	O
at	O
all	O
but	O
you	O
could	O
apply	B-API
the	O
function	O
:	O
`	O
s.apply	O
(	O
pd.DataFrame.mean	B-API
)`	O

This	O
just	O
gives	O
the	O
mean	O
of	O
each	O
constituent	O
data	O
frame	O
.	O

When	O
you	O
define	O
`	O
s	O
`	O
with	O
#CODE	O

you	O
get	O
a	O
Series	B-API
with	O
DataFrames	O
as	O
items	O
:	O
#CODE	O

The	O
sum	O
of	O
the	O
items	O
is	O
a	O
DataFrame	B-API
:	O
#CODE	O

but	O
when	O
you	O
take	O
the	O
mean	O
,	O
`	O
nanops.nanmean	O
`	O
is	O
called	O
:	O
#CODE	O

Notice	O
that	O
`	O
_ensure_numeric	O
`	O
(	O
source	O
code	O
)	O
is	O
called	O
on	O
the	O
resultant	O
sum	O
.	O

An	O
error	O
is	O
raised	O
because	O
a	O
DataFrame	B-API
is	O
not	O
numeric	O
.	O

Here	O
is	O
a	O
workaround	O
.	O

Instead	O
of	O
making	O
a	O
Series	B-API
with	O
DataFrames	O
as	O
items	O
,	O

you	O
can	O
concatenate	O
the	O
DataFrames	O
into	O
a	O
new	O
DataFrame	B-API
with	O
a	O
hierarchical	O
index	O
:	O
#CODE	O

Now	O
you	O
can	O
take	O
the	O
`	O
sum	B-API
`	O
and	O
the	O
`	O
mean	B-API
`	O
:	O
#CODE	O

You	O
could	O
(	O
as	O
suggested	O
by	O
@USER	O
)	O
use	O
a	O
hierarchical	O
index	O
but	O
when	O
you	O
have	O
a	O
three	O
dimensional	O
array	O
you	O
should	O
consider	O
using	O
a	O
"	O
pandas	O
Panel	B-API
"	O
.	O

Especially	O
when	O
one	O
of	O
the	O
dimensions	O
represents	O
time	O
as	O
in	O
this	O
case	O
.	O

The	O
Panel	B-API
is	O
oft	O
overlooked	O
but	O
it	O
is	O
after	O
all	O
where	O
the	O
name	O
pandas	O
comes	O
from	O
.	O

(	O
Panel	B-API
Data	O
System	O
or	O
something	O
like	O
that	O
)	O
.	O

Data	O
slightly	O
different	O
from	O
your	O
original	O
so	O
there	O
are	O
not	O
two	O
dimensions	O
with	O
the	O
same	O
length	O
:	O
#CODE	O

Panels	O
can	O
be	O
created	O
a	O
couple	O
of	O
different	O
ways	O
but	O
one	O
is	O
from	O
a	O
dict	O
.	O

You	O
can	O
create	O
the	O
dict	O
from	O
your	O
index	O
and	O
the	O
dataframes	O
with	O
:	O
#CODE	O

The	O
mean	O
you	O
are	O
looking	O
for	O
is	O
simply	O
a	O
matter	O
of	O
operating	O
on	O
the	O
correct	O
axis	O
(	O
axis=0	O
in	O
this	O
case	O
):	O
#CODE	O

With	O
your	O
data	O
,	O
`	O
sum	O
(	O
axis=0	O
)`	O
returns	O
the	O
expected	O
result	O
.	O

EDIT	O
:	O
OK	O
too	O
late	O
for	O
panels	O
as	O
the	O
hierarchical	O
index	O
approach	O
is	O
already	O
"	O
accepted	O
"	O
.	O

I	O
will	O
say	O
that	O
that	O
approach	O
is	O
preferable	O
if	O
the	O
data	O
is	O
know	O
to	O
be	O
"	O
ragged	O
"	O
with	O
an	O
unknown	O
but	O
different	O
number	O
in	O
each	O
grouping	O
.	O

For	O
"	O
square	O
"	O
data	O
,	O
the	O
panel	B-API
is	O
absolutly	O
the	O
way	O
to	O
go	O
and	O
will	O
be	O
significantly	O
faster	O
with	O
more	O
built-in	O
operations	O
.	O

Pandas	O
0.15	O
has	O
many	O
improvements	O
for	O
multi-level	O
indexing	O
but	O
still	O
has	O
limitations	O
and	O
dark	O
edge	O
cases	O
in	O
real	O
world	O
apps	O
.	O

You're	O
right	O
that	O
computation	O
on	O
the	O
panel	B-API
is	O
significantly	O
faster	O
than	O
on	O
the	O
hierarchical	O
dataframe	B-API
.	O

But	O
what	O
are	O
the	O
extra	O
built-in	O
operations	O
that	O
panels	O
have	O
that	O
dataframes	O
don't	O
?	O

`	O
set	O
(	O
dir	O
(	O
pan	O
))	O
-set	O
(	O
dir	O
(	O
df	O
))`	O
did	O
not	O
turn	O
up	O
anything	O
interesting	O
.	O

Also	O
,	O
can	O
you	O
elaborate	O
on	O
the	O
"	O
limitations	O
and	O
dark	O
edges	O
cases	O
"	O
to	O
which	O
you	O
referred	O
?	O

This	O
is	O
my	O
fault	O
for	O
asking	O
two	O
questions	O
in	O
one	O
.	O

I	O
will	O
be	O
using	O
your	O
solution	O
for	O
my	O
problem	O
since	O
,	O
as	O
you	O
point	O
out	O
,	O
the	O
`	O
Panel	B-API
`	O
solution	O
is	O
the	O
correct	O
one	O
for	O
the	O
example	O
I	O
gave	O
.	O

But	O
I	O
think	O
@USER	O
addressed	O
the	O
full	O
question	O
of	O
what	O
exactly	O
is	O
keeping	O
my	O
original	O
series-based	O
code	O
from	O
working	O
.	O

I	O
very	O
much	O
appreciate	O
your	O
insight	O
!	O

Both	O
of	O
these	O
should	O
be	O
in	O
the	O
docs	O
!	O

I	O
want	O
to	O
also	O
add	O
,	O
that	O
unlike	O
the	O
hierarchical	O
index	O
version	O
,	O
you	O
can	O
now	O
also	O
do	O
`	O
s_mean	O
=	O
np.mean	O
(	O
s	O
,	O
axis=0	O
)`	O
,	O
or	O
sum	O
,	O
std	O
,	O
or	O
I'm	O
guessing	O
any	O
numpy	O
function	O
that	O
works	O
on	O
arrays	O
.	O

Change	O
dataframe	B-API
index	O
values	O
while	O
keeping	O
other	O
column	O
data	O
same	O

I	O
have	O
a	O
DataFrame	B-API
with	O
4	O
columns	O
and	O
251	O
rows	O
and	O
an	O
index	O
that	O
is	O
a	O
progression	O
of	O
numbers	O
e.g.	O
1000	O
to	O
1250	O
.	O

The	O
index	O
was	O
initially	O
necessary	O
to	O
aid	O
in	O
joining	O
data	O
from	O
4	O
different	O
dataframes	O
.	O

However	O
,	O
once	O
i	O
get	O
the	O
4	O
columns	O
together	O
,	O
i	O
would	O
like	O
to	O
change	O
the	O
index	O
to	O
a	O
number	O
progression	O
from	O
250	O
to	O
0	O
.	O

This	O
is	O
because	O
i	O
would	O
be	O
performing	O
the	O
same	O
operation	O
on	O
different	O
sets	O
of	O
data	O
(	O
in	O
groups	O
of	O
4	O
)	O
that	O
would	O
have	O
different	O
indices	O
,	O
e.g.	O
2000	O
to	O
2250	O
or	O
500	O
to	O
750	O
,	O
but	O
would	O
all	O
have	O
the	O
same	O
number	O
of	O
rows	O
.	O

250	O
to	O
0	O
is	O
a	O
way	O
of	O
unifying	O
these	O
data	O
sets	O
,	O
but	O
i	O
can't	O
figure	O
out	O
how	O
to	O
do	O
this	O
.	O

i.e.	O
i'm	O
looking	O
for	O
something	O
that	O
replaces	O
any	O
existing	O
index	O
with	O
the	O
function	O
range	O
(	O
250	O
,	O
0	O
,	O
-1	O
)	O

I've	O
tried	O
using	O
set_index	B-API
below	O
and	O
a	O
whole	O
bunch	O
of	O
other	O
attempts	O
that	O
invariably	O
return	O
errors	O
,	O
#CODE	O

and	O
in	O
the	O
instance	O
when	O
i	O
am	O
able	O
to	O
set	O
the	O
index	O
of	O
the	O
df	O
to	O
the	O
range	O
,	O
the	O
data	O
in	O
the	O
4	O
columns	O
change	O
to	O
NaN	O
since	O
they	O
have	O
no	O
data	O
that	O
matches	O
the	O
new	O
index	O
.	O

I	O
apologize	O
if	O
this	O
is	O
rudimentary	O
,	O
but	O
i'm	O
a	O
week	O
old	O
in	O
the	O
world	O
of	O
python	O
/	O
pandas	O
,	O
haven't	O
programmed	O
in	O
+10yrs	O
,	O
and	O
have	O
taken	O
2	O
days	O
to	O
try	O
to	O
figure	O
this	O
out	O
for	O
myself	O
as	O
an	O
exercise	O
,	O
but	O
its	O
time	O
to	O
cry	O
...	O

Uncle	O
!!	O

Try	O
introducing	O
the	O
250:0	O
indices	O
as	O
a	O
column	O
first	O
,	O
then	O
setting	O
them	O
as	O
the	O
index	O
:	O
#CODE	O

Before	O
:	O
#CODE	O

After	O
:	O
#CODE	O

Thanks	O
for	O
the	O
quick	O
response	O
,	O
I	O
just	O
tried	O
it	O
,	O
and	O
i	O
believe	O
i'm	O
getting	O
a	O
datatype	O
conflict	O
of	O
some	O
sort	O
.	O

I	O
am	O
able	O
to	O
add	O
250:0	O
as	O
a	O
column	O
to	O
the	O
dataset	O
,	O
the	O
set_index	B-API
command	O
doesn't	O
throw	O
any	O
errors	O
,	O
but	O
when	O
i	O
look	O
at	O
the	O
dataframe	B-API
,	O
i	O
still	O
have	O
the	O
old	O
index	O
with	O
the	O
250:0	O
as	O
the	O
last	O
column	O
.	O

Below	O
is	O
the	O
output	O
i	O
got	O
on	O
executing	O
the	O
set_index	B-API
command	O

I	O
didn't	O
use	O
the	O
`	O
inplace=True	O
`	O
argument	O
in	O
my	O
code	O
like	O
you	O
did	O
,	O
so	O
it	O
doesn't	O
actually	O
modify	O
`	O
df	O
`	O
,	O
just	O
returns	O
a	O
new	O
dataframe	B-API
with	O
those	O
indexes	O
set	O
.	O

Add	O
that	O
argument	O
,	O
or	O
assign	B-API
the	O
result	O
to	O
a	O
new	O
variable	O
,	O
and	O
you	O
should	O
be	O
good	O
.	O

All	O
solved	O
.	O

Thanks	O
and	O
sorry	O
for	O
my	O
incomplete	O
response	O
earlier	O
.	O

You	O
can	O
just	O
do	O
#CODE	O

or	O
am	O
I	O
missing	O
something	O
?	O

how	O
to	O
concat	B-API
sets	O
when	O
using	O
groupby	B-API
in	O
pandas	O
dataframe	B-API
?	O

This	O
is	O
my	O
dataframe	B-API
:	O
#CODE	O

Now	O
when	O
I	O
`	O
groupby	B-API
`	O
,	O
I	O
want	O
to	O
update	O
sets	O
.	O

If	O
it	O
was	O
a	O
`	O
list	O
`	O
there	O
was	O
no	O
problem	O
.	O

But	O
the	O
output	O
of	O
my	O
command	O
is	O
:	O
#CODE	O

What	O
should	O
I	O
do	O
in	O
groupby	B-API
to	O
update	O
sets	O
?	O

The	O
output	O
I'm	O
looking	O
for	O
is	O
as	O
below	O
:	O
#CODE	O

This	O
might	O
be	O
close	O
to	O
what	O
you	O
want	O
#CODE	O

In	O
this	O
case	O
it	O
takes	O
the	O
union	O
of	O
the	O
sets	O
.	O

If	O
you	O
need	O
to	O
keep	O
the	O
column	O
names	O
you	O
could	O
use	O
:	O
#CODE	O

Result	O
:	O
#CODE	O

Thanks	O
,	O
It	O
solved	O
set	O
problem	O
,	O
but	O
column	O
name	O
renamed	O
to	O
0	O
.	O

Why	O
that	O
happened	O
?	O

It's	O
because	O
the	O
result	O
is	O
a	O
Series	B-API
so	O
no	O
column	O
name	O
.	O

I've	O
added	O
a	O
method	O
for	O
keeping	O
the	O
column	O
name	O
if	O
you	O
need	O
it	O
.	O

Take	O
a	O
Pandas	O
Series	B-API
where	O
each	O
element	O
is	O
a	O
DataFrame	B-API
and	O
combine	O
them	O
to	O
one	O
big	O
DataFrame	B-API

I	O
have	O
a	O
Pandas	O
Series	B-API
where	O
each	O
element	O
of	O
the	O
series	B-API
is	O
a	O
one	O
row	O
Pandas	O
DataFrame	B-API
which	O
I	O
would	O
like	O
to	O
append	B-API
together	O
into	O
one	O
big	O
DataFrame	B-API
.	O

For	O
example	O
:	O
#CODE	O

so	O
how	O
do	O
I	O
take	O
`	O
myResult	O
`	O
and	O
combine	O
all	O
the	O
little	O
dataframes	O
into	O
one	O
big	O
dataframe	B-API
?	O

#CODE	O

yields	O
#CODE	O

was	O
just	O
about	O
to	O
post	O
very	O
similar	O
....	O
fyi	O
,	O
you	O
function	O
could	O
return	O
the	O
column	O
labels	O
as	O
the	O
index	O
of	O
the	O
function	O
series	B-API
,	O
e.g.	O
``	O
pd.Series	B-API
([	O
val**2	O
,	O
val**3	O
,	O
index	O
=[	O
'	O
square	O
'	O
,	O
'	O
cube	O
'])``	O
will	O
work	O
as	O
well	O

@USER	O
:	O
Ah	O
,	O
much	O
better	O
.	O

Thank	O
you	O
.	O

Its	O
seems	O
overly	O
complicated	O
,	O
although	O
you	O
probably	O
posted	O
a	O
simplified	O
example	O
.	O

Creating	O
a	O
new	O
Series	B-API
for	O
each	O
row	O
creates	O
a	O
lot	O
of	O
overhead	O
.	O

This	O
for	O
example	O
is	O
over	O
200	O
times	O
faster	O
(	O
for	O
n=500	O
)	O
on	O
my	O
machine	O
:	O
#CODE	O

your	O
intuition	O
is	O
correct	O
.	O

My	O
example	O
is	O
a	O
bit	O
of	O
an	O
extreme	O
simplification	O
.	O

concat	B-API
them	O
:	O
#CODE	O

Since	O
the	O
original	O
indexes	O
are	O
all	O
0	O
,	O
I	O
also	O
reset	O
them	O
.	O

You	O
can	O
also	O
do	O
`	O
pd.concat	B-API
(	O
myResult	O
,	O
ignore_index=True	O
)`	O

pandas	O
:	O
Multiply	O
MultiIndex	O
DataFrame	B-API
with	O
Series	B-API

I	O
have	O
a	O
MultiIndex	O
DataFrame	B-API
that	O
contains	O
these	O
values	O
:	O
#CODE	O

--	O

and	O
a	O
Series	B-API
that	O
contains	O
these	O
values	O
:	O
#CODE	O

I'd	O
like	O
to	O
multiply	O
all	O
of	O
the	O
minor	O
index	O
CC	O
values	O
by	O
the	O
CC	O
value	O
in	O
the	O
Series	B-API
,	O
and	O
the	O
same	O
with	O
the	O
other	O
values	O
.	O

I	O
saw	O
another	O
question	O
on	O
here	O
that	O
gave	O
me	O
the	O
.mul	B-API
method	O
,	O
but	O
when	O
I	O
try	O
that	O
,	O
even	O
with	O
the	O
level=	O
'	O
minor	O
'	O
,	O
it	O
tells	O
me	O
:	O

TypeError	O
:	O
can	O
only	O
call	O
with	O
other	O
hierarchical	O
index	O
objects	O

I've	O
unstacked	O
the	O
minor	O
index	O
to	O
make	O
it	O
columns	O
,	O
and	O
specified	O
level=	O
'	O
minor	O
'	O
,	O
axis=	O
'	O
columns	O
'	O
with	O
the	O
same	O
result	O
.	O

Finally	O
,	O
the	O
end	O
result	O
is	O
to	O
be	O
able	O
to	O
run	O
this	O
same	O
calculation	O
on	O
a	O
DataFrame	B-API
where	O
the	O
major	O
columns	O
are	O
several	O
equities	O
--	O
in	O
that	O
instance	O
,	O
would	O
.mul()	B-API
work	O
against	O
each	O
equity	O
as	O
well	O
?	O

Thanks	O
for	O
your	O
assistance	O
!	O

If	O
you	O
add	O
the	O
output	O
of	O
the	O
DateFrame	O
and	O
Series	B-API
`	O
.to_dict()	B-API
`	O
then	O
it	O
is	O
much	O
easier	O
for	O
us	O
to	O
solve	O
these	O
type	O
of	O
questions	O
:)	O
What	O
code	O
are	O
you	O
using	O
to	O
multiplying	O
the	O
"	O
minor	O
index	O
CC	O
by	O
the	O
CC	O
value	O
"	O
?	O

I	O
updated	O
it	O
to	O
add	O
to_dict()	B-API
output	O
.	O

Series	B-API
based	O
it	O
works	O
with	O
`	O
level	O
`	O
:	O
#CODE	O

Then	O
you	O
can	O
insert	B-API
it	O
again	O
into	O
your	O
DataFrame	B-API
.	O

But	O
that	O
should	O
work	O
with	O
DataFrames	O
too	O
,	O
maybe	O
you	O
can	O
suggest	O
it	O
.	O

I	O
used	O
this	O
and	O
DataFrame.apply	B-API
to	O
apply	B-API
it	O
to	O
all	O
major	O
columns	O
in	O
the	O
dataframe	B-API
.	O

After	O
thinking	O
about	O
it	O
a	O
bit	O
more	O
,	O
I	O
think	O
this	O
is	O
the	O
intended	O
design	O
,	O
and	O
it	O
perfectly	O
accomplishes	O
the	O
goal	O
.	O

Merging	O
two	O
pandas	O
timeseries	O
shifted	O
by	O
1	O
second	O

I	O
have	O
two	O
sets	O
of	O
time	O
series	B-API
like	O
this	O
.	O

One	O
:	O
#CODE	O

The	O
other	O
:	O
#CODE	O

As	O
you	O
can	O
see	O
one	O
of	O
them	O
is	O
shifted	O
by	O
1	O
second	O
.	O

But	O
I	O
would	O
like	O
to	O
treat	O
them	O
as	O
same	O
bucket	O
.	O

For	O
instance	O
,	O
2014-03-17	O
13:25	O
:	O
01	O
should	O
be	O
same	O
as	O
2014-03-17	O
13:25	O
:	O
00	O
.	O

How	O
can	O
i	O
achieve	O
this	O
?	O

Nevermind	O
figured	O
out	O
one	O
way	O
to	O
do	O
it	O
.	O

#CODE	O

And	O
then	O
I	O
merge	B-API
the	O
two	O
..	O

If	O
there	O
are	O
better	O
ways	O
please	O
do	O
let	O
me	O
know	O
.	O

Seems	O
a	O
reasonable	O
way	O
.	O

You	O
could	O
also	O
do	O
:	O
`	O
df.index	O
=	O
df.index.values.astype	O
(	O
'	O
datetime64	O
[	O
m	O
]')`	O
.	O

you	O
can	O
also	O
round	O
the	O
as	O
well	O
,	O
see	O
here	O
:	O
#URL	O

Python	O
/	O
Pandas	O
-	O
GUI	O
for	O
viewing	O
a	O
DataFrame	B-API
or	O
Matrix	O

I'm	O
using	O
the	O
Pandas	O
package	O
and	O
it	O
creates	O
a	O
DataFrame	B-API
object	O
,	O
which	O
is	O
basically	O
a	O
labeled	O
matrix	O
.	O

Often	O
I	O
have	O
columns	O
that	O
have	O
long	O
string	O
fields	O
,	O
or	O
dataframes	O
with	O
many	O
columns	O
,	O
so	O
the	O
simple	O
print	O
command	O
doesn't	O
work	O
well	O
.	O

I've	O
written	O
some	O
text	O
output	O
functions	O
,	O
but	O
they	O
aren't	O
great	O
.	O

What	O
I'd	O
really	O
love	O
is	O
a	O
simple	O
GUI	O
that	O
lets	O
me	O
interact	O
with	O
a	O
dataframe	B-API
/	O
matrix	O
/	O
table	O
.	O

Just	O
like	O
you	O
would	O
find	O
in	O
a	O
SQL	O
tool	O
.	O

Basically	O
a	O
window	O
that	O
has	O
a	O
read-only	O
spreadsheet	O
like	O
view	O
into	O
the	O
data	O
.	O

I	O
can	O
expand	O
columns	O
,	O
page	O
up	O
and	O
down	O
through	O
long	O
tables	O
,	O
etc	O
.	O

I	O
would	O
suspect	O
something	O
like	O
this	O
exists	O
,	O
but	O
I	O
must	O
be	O
Googling	O
with	O
the	O
wrong	O
terms	O
.	O

It	O
would	O
be	O
great	O
if	O
it	O
is	O
pandas	O
specific	O
,	O
but	O
I	O
would	O
guess	O
I	O
could	O
use	O
any	O
matrix-accepting	O
tool	O
.	O

(	O
BTW	O
-	O
I'm	O
on	O
Windows	O
.	O
)	O

Any	O
pointers	O
?	O

Or	O
,	O
conversely	O
,	O
if	O
someone	O
knows	O
this	O
space	O
well	O
and	O
knows	O
this	O
probably	O
doesn't	O
exist	O
,	O
any	O
suggestions	O
on	O
if	O
there	O
is	O
a	O
simple	O
GUI	O
framework	O
/	O
widget	O
I	O
could	O
use	O
to	O
roll	O
my	O
own	O
?	O

(	O
But	O
since	O
my	O
needs	O
are	O
limited	O
,	O
I'm	O
reluctant	O
to	O
have	O
to	O
learn	O
a	O
big	O
GUI	O
framework	O
and	O
do	O
a	O
bunch	O
of	O
coding	O
for	O
this	O
one	O
piece	O
.	O
)	O

Thanks	O
.	O

Would	O
Pyspread	O
be	O
of	O
any	O
assistance	O
?	O

Looks	O
like	O
overkill	O
for	O
my	O
need	O
,	O
but	O
I'll	O
look	O
into	O
it	O
if	O
there's	O
nothing	O
easier	O
.	O

Thanks	O
.	O

can	O
this	O
be	O
done	O
in	O
spyder	O
(	O
#URL	O
)	O
?	O

I	O
have	O
been	O
using	O
Rstudio	O
with	O
R	O
and	O
I	O
like	O
being	O
able	O
to	O
see	O
the	O
data	O
with	O
a	O
single	O
click	O
.	O

I	O
totally	O
agree	O
that	O
a	O
comparable	O
tool	O
for	O
Python	O
/	O
Pandas	O
is	O
missing	O
and	O
iPython	O
is	O
great	O
but	O
not	O
in	O
this	O
area	O
.	O

i've	O
found	O
that	O
the	O
ipython	O
notebook	O
is	O
pretty	O
good	O
for	O
this	O
.	O

I'm	O
not	O
a	O
Pandas	O
user	O
myself	O
,	O
but	O
a	O
quick	O
search	O
for	O
"	O
pandas	O
gui	O
"	O
turns	O
up	O
the	O
Pandas	O
project's	O
GSOC	O
2012	O
proposal	O
:	O

Currently	O
the	O
only	O
way	O
to	O
interact	O
with	O
these	O
objects	O
is	O
through	O
the	O
API	O
.	O

This	O
project	O
proposes	O
to	O
add	O
a	O
simple	O
Qt	O
or	O
Tk	O
GUI	O
with	O
which	O
to	O
view	O
and	O
manipulate	O
these	O
objects	O
.	O

So	O
,	O
there's	O
no	O
GUI	O
,	O
but	O
if	O
you'd	O
write	O
one	O
using	O
Qt	O
or	O
Tk	O
,	O
the	O
project	O
might	O
be	O
interested	O
in	O
your	O
code	O
.	O

Thanks	O
,	O
but	O
I	O
think	O
building	O
a	O
generally	O
usable	O
tool	O
would	O
be	O
above	O
my	O
skill	O
level	O
!	O

It	O
seems	O
there	O
is	O
no	O
easy	O
solution	O
.	O

So	O
,	O
below	O
is	O
a	O
little	O
function	O
to	O
open	O
a	O
dataframe	B-API
in	O
Excel	O
.	O

It's	O
probably	O
not	O
production	O
quality	O
code	O
,	O
but	O
it	O
works	O
for	O
me	O
!	O

#CODE	O

I	O
use	O
`	O
QTableWidget	O
`	O
from	O
PyQt	O
to	O
display	O
a	O
`	O
DataFrame	B-API
`	O
.	O

I	O
create	O
a	O
`	O
QTableWidgetObject	O
`	O
and	O
then	O
populate	O
with	O
`	O
QTableWidgetItems	O
`	O
created	O
with	O
`	O
DataFrame	B-API
`	O
values	O
.	O

Following	O
is	O
the	O
snippet	O
of	O
code	O
that	O
reads	O
a	O
CSV	O
file	O
,	O
create	O
a	O
`	O
DataFrame	B-API
`	O
,	O
then	O
display	O
in	O
a	O
GUI	O
:	O
#CODE	O

That's	O
awesome	O
.	O

I	O
will	O
definitely	O
try	O
this	O
next	O
time	O
.	O

I	O
use	O
ipython	O
notebooks	O
to	O
drive	O
pandas	O
--	O
notebooks	O
provide	O
a	O
nice	O
clean	O
way	O
of	O
incrementally	O
building	O
and	O
interacting	O
with	O
pandas	O
data	O
structures	O
,	O
including	O
HTML-ized	O
display	O
of	O
dataframes	O
:	O
#URL	O

You	O
could	O
use	O
the	O
to_html()	B-API
dataframe	B-API
method	O
to	O
convert	O
the	O
dataframe	B-API
to	O
html	O
and	O
display	O
it	O
in	O
your	O
browser	O
.	O

Here	O
is	O
an	O
example	O
assuming	O
you	O
have	O
a	O
dataframe	B-API
called	O
df	O
.	O

You	O
should	O
check	O
the	O
documentation	O
to	O
see	O
what	O
other	O
options	O
are	O
available	O
in	O
the	O
to_html()	B-API
method	O
.	O

#CODE	O

If	O
you	O
want	O
to	O
get	O
the	O
table	O
to	O
be	O
nicely	O
formatted	O
and	O
scrollable	O
then	O
you	O
can	O
use	O
the	O
datatables	O
plug-in	O
for	O
jQuery	O
#URL	O
.	O

Here	O
is	O
the	O
javascript	O
I	O
use	O
to	O
display	O
a	O
table	O
the	O
scrolls	O
in	O
both	O
x	O
and	O
y	O
directiions	O
.	O

#CODE	O

Hey	O
-	O
this	O
looks	O
great	O
.	O

I'll	O
try	O
that	O
next	O
time	O
I	O
need	O
to	O
look	O
at	O
data	O
.	O

Pandas	O
0.13	O
provides	O
as	O
an	O
experimental	O
feature	O
:	O

PySide	O
support	O
for	O
the	O
qtpandas	O
`	O
DataFrameModel	O
`	O
and	O
`	O
DataFrameWidget	O
`	O

see	O
#URL	O

you	O
can	O
add	O
this	O
feature	O
using	O
#CODE	O

Thank	O
you	O
for	O
this	O
!	O

There's	O
now	O
a	O
working	O
sample	O
in	O
the	O
Pandas	O
docs	O
:	O
#URL	O

There's	O
tkintertable	O
for	O
python2.7	O
and	O
pandastable	O
for	O
python3	O
.	O

I've	O
been	O
working	O
on	O
a	O
PyQt	O
GUI	O
for	O
pandas	O
DataFrame	B-API
you	O
might	O
find	O
useful	O
.	O

It	O
includes	O
copying	O
,	O
filtering	O
,	O
and	O
sorting	O
.	O

#URL	O

The	O
nicest	O
solution	O
I've	O
found	O
is	O
using	O
`	O
qgrid	O
`	O
(	O
see	O
here	O
,	O
and	O
also	O
mentioned	O
in	O
the	O
pandas	O
docs	O
)	O
.	O

You	O
can	O
install	O
by	O
#CODE	O

and	O
then	O
you	O
need	O
to	O
do	O
a	O
further	O
install	O
(	O
just	O
once	O
)	O
in	O
your	O
`	O
IPython	O
`	O
notebook	O
#CODE	O

Afterwards	O
,	O
it's	O
as	O
easy	O
as	O
taking	O
your	O
`	O
pandas	O
`	O
`	O
df	O
`	O
and	O
running	O
#CODE	O

The	O
other	O
nice	O
thing	O
is	O
that	O
it	O
renders	O
in	O
`	O
nbviewer	O
`	O
too	O
.	O

See	O
it	O
in	O
action	O
here	O

Convert	O
one	O
row	O
of	O
a	O
pandas	O
dataframe	B-API
into	O
multiple	O
rows	O

I	O
want	O
to	O
turn	O
this	O
:	O
#CODE	O

Into	O
this	O
:	O
#CODE	O

Context	O
:	O
I	O
have	O
data	O
stored	O
with	O
one	O
value	O
coded	O
for	O
all	O
ages	O
(	O
age	O
=	O
99	O
)	O
.	O

However	O
,	O
the	O
application	O
I	O
am	O
developing	O
for	O
needs	O
the	O
value	O
explicitly	O
stated	O
for	O
every	O
id-age	O
pair	O
(	O
id	O
=1	O
,	O
age	O
=	O
25	O
,	O
50	O
,	O
and	O
75	O
)	O
.	O

There	O
are	O
simple	O
solutions	O
to	O
this	O
:	O
iterate	O
over	O
id's	O
and	O
append	B-API
a	O
bunch	O
of	O
dataframes	O
,	O
but	O
I'm	O
looking	O
for	O
something	O
elegant	O
.	O

I'd	O
like	O
to	O
do	O
a	O
#URL	O
merge	B-API
from	O
my	O
original	O
dataframe	B-API
to	O
a	O
template	O
containing	O
all	O
the	O
ages	O
,	O
but	O
I	O
would	O
still	O
have	O
to	O
loop	O
over	O
id's	O
to	O
create	O
the	O
template	O
.	O

Don't	O
know	O
,	O
may	O
be	O
there's	O
more	O
elegant	O
approach	O
,	O
but	O
you	O
can	O
do	O
something	O
like	O
cross	O
join	O
(	O
or	O
cartesian	O
product	O
):	O
#CODE	O

Thanks	O
!	O

This	O
is	O
exactly	O
what	O
I	O
was	O
looking	O
for	O
,	O
and	O
I	O
guess	O
I	O
even	O
said	O
the	O
words	O
many	O
to	O
one	O
in	O
my	O
question	O
,	O
but	O
I	O
didn't	O
understand	O
that	O
you	O
could	O
merge	B-API
like	O
that	O

@USER	O
I	O
think	O
code	O
could	O
be	O
cleaned	O
a	O
bit	O
,	O
but	O
you've	O
got	O
overall	O
idea	O

use	O
for	O
loop	O
to	O
concat	B-API
dataframe	B-API
to	O
a	O
larger	O
dataframe	B-API

my	O
question	O
is	O
for	O
every	O
step	O
of	O
for	O
loop	O
,	O
a	O
new	O
dataframe	B-API
will	O
be	O
generated	O
.	O

I	O
want	O
to	O
concat	B-API
the	O
data	O
frames	O
together	O
to	O
have	O
a	O
larger	O
one	O
but	O
somehow	O
my	O
function	O
will	O
only	O
return	O
the	O
last	O
step	O
of	O
the	O
result	O
rather	O
than	O
the	O
merged	O
result	O
#CODE	O

Thanks	O
!	O

maybe	O
result	O
=	O
pd.concat	B-API
(	O
[	O
result	O
,	O
rate	O
]	O
,	O
ignore_index=True	O
)	O
??	O

BTW	O
:	O
add	O
`	O
spaces	O
`	O
around	O
`	O
=	O
`	O
and	O
after	O
`	O
,	O
`	O
to	O
make	O
code	O
more	O
readable	O
-	O
see	O
:	O
[	O
PEP	O
8	O
--	O
Style	O
Guide	O
for	O
Python	O
Code	O
]	O
(	O
#URL	O
)	O

@USER	O
unfortunately	O
it	O
does	O
not	O
work	O
:(	O

@USER	O
thank	O
you	O
so	O
much	O
!	O

I	O
am	O
really	O
new	O
to	O
Python	O
will	O
def	O
pay	O
attention	O
to	O
that	O
next	O
time	O
.	O
bad	O
habit	O
in	O
R	O
lol	O

This	O
might	O
not	O
be	O
the	O
right	O
answer	O
,	O
it	O
is	O
more	O
readable	O
written	O
as	O
answer	O
.	O

I	O
think	O
the	O
right	O
logic	O
should	O
be	O
(	O
but	O
I	O
can	O
be	O
very	O
wrong	O
):	O
#CODE	O

Can	O
you	O
please	O
try	O
this	O
and	O
let	O
us	O
know	O
if	O
it	O
works	O
?	O

I	O
think	O
one	O
of	O
your	O
problem	O
is	O
the	O
way	O
you	O
use	O
pd.concat	B-API
(	O
obj	O
)	O
,	O
the	O
obj	O
should	O
be	O
a	O
list	O
of	O
item	O
or	O
a	O
dict	O
of	O
pd.Series	B-API
....	O

but	O
you	O
didn't	O
concat	B-API
rate	O
with	O
anything	O
else	O
.	O

and	O
the	O
use	O
of	O
variable	O
"	O
result	O
"	O
is	O
unnecessary	O
to	O
me	O
.	O

but	O
,	O
again	O
,	O
I	O
could	O
be	O
wrong	O
.	O

I	O
agree	O
-	O
`	O
result	O
`	O
is	O
unnecessary	O
.	O

`	O
cvResult	O
`	O
should	O
be	O
used	O
in	O
place	O
of	O
`	O
result	O
`	O
.	O

it	O
still	O
did	O
not	O
work	O
the	O
error	O
is	O
"	O
cannot	O
concatenate	O
a	O
non-NDFrame	O
object	O
"	O
but	O
thx	O
@USER	O
:)	O

oh	O
,	O
you	O
need	O
to	O
do	O
rate	O
=	O
pd.DataFrame	B-API
(	O
{	O
"	O
classfier	O
"	O
:	O
classifier_i	O
,	O
"	O
model	O
"	O
:	O
index_i	O
,	O
"	O
recall	O
"	O
:	O
recall_rate	O
,	O
"	O
precision	O
"	O
:p	O
recision_rate	O
,	O
"	O
accuracy	O
"	O
:	O
accuracy_rate	O
}	O
)	O
to	O
make	O
it	O
a	O
DataFrame	B-API
before	O
you	O
concat	B-API
it	O
.	O

pandas.to_html()	O
returning	O
None	O

I'm	O
trying	O
to	O
use	O
the	O
formatters	O
argument	O
to	O
add	O
a	O
html	O
property	O
to	O
a	O
specific	O
cell	O
/	O
text	O
.	O

I've	O
found	O
an	O
answer	O
that	O
fits	O
completely	O
my	O
request	O
.	O

However	O
trying	O
it	O
,	O
results	O
in	O
returning	O
None	O
.	O

The	O
example	O
that	O
I'm	O
trying	O
:	O
#CODE	O

This	O
question	O
is	O
from	O
2013	O
.	O

Does	O
the	O
syntax	O
changed	O
from	O
one	O
version	O
to	O
another	O
?	O

I'm	O
using	O
python	O
Python	O
2.7.9	O
pandas	O
0.16.0	O
.	O

Meybe	O
result	O
is	O
in	O
`	O
buf	O
`	O
?	O

As	O
above	O
:	O
the	O
to_x	O
methods	O
return	O
None	O
,	O
the	O
results	O
will	O
be	O
visible	O
in	O
whatever	O
file-like	O
you	O
pass	O
as	O
the	O
first	O
parameter	O

Ok	O
,	O
I	O
wasn't	O
understanding	O
the	O
stringIo	O
use	O
.	O

I	O
was	O
thinking	O
the	O
formatter	O
argument	O
required	O
a	O
stringIo	O
object	O
.	O

The	O
result	O
that	O
I	O
was	O
expecting	O
was	O
simply	O
:	O
df.to_html	B-API
(	O
formatters={	O
'	O
p_value	O
'	O
:	O
significant}	O
,	O
escape=False	O
)	O

The	O
df.to_html()	B-API
method	O
is	O
not	O
returning	O
the	O
output	O
,	O
the	O
output	O
is	O
returned	O
to	O
the	O
buf	O
stringIo	O
object	O
wich	O
can	O
be	O
conveniently	O
written	O
later	O
.	O

What	O
I	O
was	O
expecting	O
:	O
#CODE	O

You	O
have	O
given	O
the	O
DataFrame.to_html	B-API
function	O
a	O
StringIO	O
buffer	O
,	O
which	O
means	O
that	O
the	O
result	O
is	O
written	O
to	O
it	O
,	O
not	O
returned	O
as	O
a	O
string	O
.	O

This	O
has	O
not	O
changed	O
since	O
Pandas	O
version	O
0.10	O
of	O
DataFrame.to_html()	B-API
function	O

To	O
get	O
the	O
output	O
as	O
a	O
returned	O
string	O
,	O
simply	O
remove	O
the	O
buffer	O
:	O
#CODE	O

OR	O
,	O
if	O
you	O
want	O
to	O
use	O
a	O
buffer	O
,	O
this	O
is	O
how	O
to	O
print	O
the	O
value	O
of	O
it	O
:	O
#CODE	O

pandas	O
resample	B-API
MAX-VALUE	O
with	O
corresponding	O
ANGLE-VALUE	O

I	O
have	O
to	O
resample	B-API
wind_velocity	O
and	O
wind_angle	O
from	O
the	O
2	O
sec	O
period	O
to	O
2min	O
and	O
receive	O
the	O
maximum	O
wind_velocity	O
(=	O
MAX	O
)	O
with	O
the	O
corresponding	O
wind_angle	O
(	O
not	O
MAX	O
)	O
.	O

The	O
lines	O
below	O
give	O
me	O
the	O
maximum	O
of	O
both	O
columns	O
.	O

#CODE	O

the	O
data	O
looks	O
like	O
#CODE	O

Any	O
help	O
?	O

This	O
answer	O
has	O
a	O
couple	O
strategies	O
-	O
#URL	O

Thank	O
you	O
.	O

This	O
helped	O
.	O

lg	O
,	O
s	O

HDFStore	O
error	O
appending	O
-	O
"	O
Cannot	O
serialize	O
the	O
column	O
"	O

I	O
have	O
a	O
dataframe	B-API
,	O
df	O
:	O
#CODE	O

Trying	O
to	O
append	B-API
this	O
to	O
a	O
new	O
datastore	O
.	O

The	O
datastore	O
does	O
not	O
exist	O
so	O
I	O
use	O
the	O
following	O
to	O
create	O
and	O
append	B-API
the	O
data	O
;	O
#CODE	O

I	B-API
get	O
this	O
error	O
:	O
on	O
the	O
`	O
store.append	O
`	O
line	O
.	O

#CODE	O

How	O
do	O
I	O
get	O
the	O
data	O
to	O
store	O
properly	O
?	O

do	O
``	O
df.dtypes	B-API
``	O
.	O
you	O
have	O
``	O
object	O
``	O
dtypes	B-API
on	O
the	O
columns	O
(	O
the	O
message	O
indicates	O
that	O
they	O
look	O
like	O
``	O
float	O
``	O
type	O
,	O
but	O
they	O
are	O
not	O
typed	O
that	O
way	O
.	O

You	O
need	O
to	O
convert	O
as	O
@USER	O
Cloud	O
suggest	O
below	O
(	O
or	O
event	O
better	O
convert	O
when	O
you	O
are	O
reading	O
them	O
in	O
)	O
.	O

It	O
is	O
hard	O
to	O
create	O
object	O
dtyped	O
floating	O
values	O
unless	O
you	O
are	O
doing	O
it	O
on	O
purpose	O
(	O
that's	O
why	O
I	O
say	O
when	O
you	O
read	O
it	O
in	O
)	O

Call	O
`	O
DataFrame.convert_objects()	B-API
`	O
:	O
#CODE	O

It	O
might	O
be	O
worth	O
checking	O
to	O
see	O
if	O
you	O
can	O
get	O
your	O
data	O
in	O
the	O
correct	O
format	O
before	O
you	O
start	O
saving	O
to	O
HDF5	O
.	O

For	O
example	O
,	O
wherever	O
`	O
df	O
`	O
is	O
created	O
,	O
convert	O
the	O
objects	O
there	O
,	O
instead	O
of	O
converting	O
them	O
when	O
you	O
save	O
.	O

In	O
general	O
,	O
operations	O
in	O
pandas	O
will	O
be	O
very	O
cumbersome	O
with	O
a	O
`	O
Series	B-API
`	O
of	O
`	O
float	O
`	O
s	O
with	O
a	O
`	O
dtype	B-API
`	O
of	O
`	O
object	O
`	O
.	O

Your	O
life	O
will	O
be	O
much	O
easier	O
if	O
you	O
convert	O
your	O
object	O
arrays	O
(	O
where	O
possible	O
)	O
as	O
soon	O
as	O
you	O
need	O
to	O
do	O
anything	O
with	O
them	O
.	O

Thanks	O
.	O

What's	O
the	O
advantage	O
to	O
using	O
`	O
dtype	B-API
=o	O
bject	O
`	O
here	O
rather	O
than	O
,	O
say	O
,	O
`	O
dtype=	O
numpy.float64	O
`	O
?	O

Nothing	O
.	O

You	O
shouldn't	O
use	O
`	O
object	O
`	O
.	O

I	O
was	O
using	O
it	O
to	O
reproduce	O
the	O
issue	O
.	O

If	O
you	O
have	O
floating	O
point	O
values	O
,	O
avoid	O
the	O
`	O
object	O
`	O
dtype	B-API
.	O

When	O
facing	O
"	O
TypeError	O
:	O
Cannot	O
serialize	O
the	O
column	O
[	O
myvar	O
]	O
because	O
its	O
data	O
contents	O
are	O
[	O
mixed	O
]	O
object	O
dtype	B-API
"	O
,	O
I	O
tried	O
.convert_objects()	B-API
,	O
and	O
that	O
too	O
failed	O
,	O
with	O
"	O
***	O
AssertionError	O
:	O
Block	O
ref_items	O
must	O
be	O
BlockManager	O
items	O
"	O

That	O
might	O
be	O
a	O
bug	O
.	O

Can	O
you	O
post	O
a	O
reproducible	O
example	O
over	O
at	O
the	O
pandas	O
github	O
and	O
I'll	O
take	O
a	O
look	O
?	O

Pvalues	O
of	O
Coeffcients	O
in	O
Lasso	O
in	O
scikit-learn	O

I	O
ran	O
a	O
L1	O
regularisation	O
on	O
19	O
features	O
with	O
143	O
observations	O
.	O

While	O
Lasso	O
did	O
give	O
me	O
coefficients	O
with	O
zero	O
value	O
thereby	O
helping	O
in	O
further	O
reduce	O
features	O
(	O
I	O
had	O
initially	O
done	O
a	O
preliminary	O
feature	O
reduction	O
by	O
running	O
a	O
combination	O
of	O
random	O
forests	O
and	O
LARS	O
.	O

But	O
the	O
problem	O
is	O
while	O
it	O
does	O
tell	O
me	O
coefficient	O
estimates	O
and	O
I	O
can	O
get	O
a	O
regression	O
equation	O
,	O
if	O
I	O
have	O
to	O
explain	O
the	O
feature	O
importance	O
to	O
someone	O
,	O
they	O
would	O
want	O
to	O
see	O
how	O
much	O
confidence	O
is	O
there	O
on	O
those	O
coefficients	O
.	O

Like	O
without	O
showing	O
any	O
p	O
value	O
of	O
coefficient	O
ppl	O
are	O
sceptical	O
.	O

So	O
is	O
there	O
a	O
way	O
to	O
get	O
pvalue	O
for	O
Lasso	O
coefficients	O
from	O
scikit	O
learn	O
?	O

Or	O
in	O
R	O
.	O

I	O
guess	O
even	O
R	O
does	O
not	O
give	O
that	O
.	O

You	O
will	O
have	O
better	O
luck	O
with	O
this	O
question	O
on	O
#URL	O

Also	O
p-value	O
or	O
significance	O
with	O
respect	O
to	O
lasso	O
is	O
a	O
new	O
thing	O
#URL	O

How	O
about	O
bootstrapping	O
and	O
getting	O
the	O
frequency	O
at	O
which	O
each	O
variable	O
is	O
included	O
in	O
LASSO-ed	O
model	O
?	O

Don't	O
perpetuate	O
the	O
problems	O
with	O
using	O
p-values	O
to	O
determine	O
importance	O
.	O

The	O
coefficients	O
are	O
estimates	O
of	O
effect	O
size	O
.	O

If	O
you	O
standardise	O
your	O
features	O
so	O
they're	O
all	O
on	O
the	O
same	O
scale	O
,	O
coefficients	O
can	O
be	O
compared	O
to	O
one	O
another	O
for	O
"	O
importance	O
"	O
.	O

For	O
uncertainty	O
in	O
effect	O
,	O
use	O
bootstrapping	O
to	O
produce	O
intervals	O
for	O
the	O
coefficients	O
.	O

pandas	O
reindex	B-API
DataFrame	B-API
with	O
datetime	O
objects	O

Is	O
it	O
possible	O
to	O
reindex	B-API
a	O
pandas	O
DataFrame	B-API
using	O
a	O
column	O
made	O
up	O
of	O
datetime	O
objects	O
?	O

I	O
have	O
a	O
DataFrame	B-API
`	O
df	O
`	O
with	O
the	O
following	O
columns	O
:	O
#CODE	O

I	O
can	O
reindex	B-API
the	O
`	O
df	O
`	O
easily	O
along	O
`	O
DOYtimestamp	O
`	O
with	O
:	O
`	O
df.reindex	B-API
(	O
index	O
=d	O
f.dtstamp	O
)`	O

and	O
`	O
DOYtimestamp	O
`	O
has	O
the	O
following	O
values	O
:	O
#CODE	O

but	O
I'd	O
like	O
to	O
reindex	B-API
the	O
DataFrame	B-API
along	O
`	O
dtstamp	O
`	O
which	O
is	O
made	O
up	O
of	O
datetime	O
objects	O
so	O
that	O
I	O
generate	O
different	O
timestamps	O
directly	O
from	O
the	O
index	O
.	O

The	O
`	O
dtstamp	O
`	O
column	O
has	O
values	O
which	O
look	O
like	O
:	O
#CODE	O

When	O
I	O
try	O
and	O
reindex	B-API
`	O
df	O
`	O
along	O
`	O
dtstamp	O
`	O
I	O
get	O
the	O
following	O
:	O
#CODE	O

I'm	O
just	O
not	O
sure	O
what	O
I	O
need	O
to	O
do	O
get	O
the	O
index	O
to	O
be	O
of	O
a	O
datetime	O
type	O
.	O

Any	O
thoughts	O
?	O

It	O
sounds	O
like	O
you	O
don't	O
want	O
reindex	B-API
.	O

Somewhat	O
confusingly	O
`	O
reindex	B-API
`	O
is	O
not	O
for	O
defining	O
a	O
new	O
index	O
,	O
exactly	O
;	O
rather	O
,	O
it	O
looks	O
for	O
rows	O
that	O
have	O
the	O
specified	O
indices	O
.	O

So	O
if	O
you	O
have	O
a	O
DataFrame	B-API
with	O
index	O
`	O
[	O
0	O
,	O
1	O
,	O
2	O
]`	O
,	O
then	O
doing	O
a	O
`	O
reindex	B-API
([	O
2	O
,	O
1	O
,	O
0	O
])`	O
will	O
return	O
the	O
rows	O
in	O
reverse	O
order	O
.	O

Doing	O
something	O
like	O
`	O
reindex	B-API
([8	O
,	O
9	O
,	O
10	O
])`	O
does	O
not	O
make	O
a	O
new	O
index	O
for	O
the	O
rows	O
;	O
rather	O
,	O
it	O
will	O
return	O
a	O
DataFrame	B-API
with	O
`	O
NaN	O
`	O
values	O
,	O
since	O
there	O
are	O
no	O
rows	O
with	O
indices	O
8	O
,	O
9	O
,	O
or	O
10	O
.	O

It	O
seems	O
like	O
what	O
you	O
want	O
is	O
to	O
just	O
keep	O
the	O
same	O
rows	O
,	O
but	O
make	O
a	O
totally	O
new	O
index	O
for	O
them	O
.	O

For	O
that	O
you	O
can	O
just	O
assign	B-API
to	O
the	O
index	O
directly	O
.	O

So	O
try	O
doing	O
`	O
df.index	O
=	O
df	O
[	O
'	O
dtstamp	O
']`	O
.	O

Thanks	O
,	O
that	O
does	O
exactly	O
what	O
I	O
need	O
.	O

Somehow	O
it	O
wasn't	O
clear	O
to	O
me	O
that	O
I	O
could	O
assign	B-API
one	O
of	O
the	O
columns	O
to	O
the	O
index	O
.	O

You	O
can	O
also	O
use	O
the	O
`	O
set_index	B-API
`	O
method	O

autoscaling	O
in	O
matplotlib	O
,	O
plotting	O
different	O
time	O
series	B-API
in	O
same	O
chart	O

I	O
have	O
a	O
'	O
master	O
'	O
panda	O
dataframe	B-API
that	O
has	O
a	O
time	O
series	B-API
of	O
'	O
polarity	O
'	O
values	O
for	O
several	O
terms	O
.	O

I	O
want	O
to	O
work	O
with	O
4	O
of	O
them	O
,	O
so	O
I	O
extracted	O
4	O
separate	O
dataframes	O
,	O
containing	O
the	O
time	O
series	B-API
(	O
same	O
time	O
series	B-API
for	O
all	O
of	O
the	O
terms	O
,	O
but	O
different	O
polarity	O
values	O
.	O
)	O

I	O
plotted	O
them	O
in	O
4	O
separate	O
matplotlib	O
graphs	O
,	O
using	O
the	O
code	O
below	O
#CODE	O

Now	O
,	O
I	O
want	O
to	O
graph	O
them	O
all	O
in	O
the	O
same	O
graph	O
so	O
I	O
have	O
an	O
idea	O
of	O
the	O
magnitude	O
of	O
each	O
graph	O
,	O
because	O
the	O
auto	O
scaling	O
of	O
matplotlib	O
can	O
give	O
the	O
wrong	O
impression	O
about	O
the	O
magnitude	O
by	O
just	O
looking	O
at	O
the	O
graphs	O
.	O

Two	O
questions	O
:	O

1	O
)	O
Is	O
there	O
are	O
way	O
to	O
set	O
the	O
min	O
and	O
max	O
values	O
of	O
the	O
Y-axis	O
when	O
plotting	O
?	O

2	O
)	O
I	O
am	O
not	O
an	O
expert	O
in	O
matplotlib	O
,	O
so	O
I	O
am	O
not	O
sure	O
how	O
to	O
plot	O
the	O
4	O
variables	O
in	O
the	O
same	O
graph	O
using	O
different	O
colors	O
,	O
markers	O
,	O
labels	O
,	O
etc	O
.	O

I	O
tried	O
nrows	O
=	O
1	O
,	O
ncols	O
=	O
1	O
but	O
can't	O
plot	O
anything	O
.	O

Thank	O
you	O

did	O
you	O
check	O
the	O
approach	O
of	O
the	O
answer	O
below	O
?	O

`	O
axes	O
[	O
i	O
,	O
j	O
]	O
.set_ylim	O
([	O
min	O
,	O
max	O
]	O
,	O
auto=False	O
)`	O
will	O
set	O
the	O
y-limits	O
of	O
the	O
plot	O
in	O
the	O
`	O
i	O
,	O
j	O
`	O
th	O
plot	O
.	O

`	O
auto=False	O
`	O
keeps	O
it	O
from	O
clobbering	O
your	O
settings	O
.	O

You	O
can	O
plot	O
multiple	O
lines	O
on	O
the	O
same	O
graph	O
by	O
calling	O
`	O
plt.hold	O
(	O
True	O
)`	O
,	O
drawing	O
a	O
bunch	O
of	O
plots	O
,	O
and	O
then	O
calling	O
`	O
plt.show()	O
`	O
or	O
`	O
plt.savefig	O
(	O
filename	O
)`	O
.	O

You	O
can	O
pass	O
a	O
color	O
code	O
into	O
`	O
plt.plot()	O
`	O
as	O
a	O
third	O
positional	O
argument	O
.	O

The	O
syntax	O
is	O
a	O
little	O
byzantine	O
(	O
it's	O
inherited	O
from	O
MATLAB	O
);	O
it's	O
documented	O
in	O
the	O
matplotlib.pyplot.plot	O
documentation	O
.	O

You	O
can	O
pass	O
this	O
argument	O
to	O
`	O
DataFrame.plot	B-API
`	O
as	O
(	O
for	O
example	O
)	O
`	O
style=	O
'	O
k	O
--	O
'`	O
.	O

For	O
your	O
case	O
,	O
I	O
would	O
try	O
#CODE	O

You	O
can	O
perhaps	O
loop	O
into	O
your	O
`	O
AxesSubplot	O
`	O
objects	O
and	O
call	O
`	O
autoscale	O
`	O
passing	O
the	O
`	O
axis	O
`	O
parameter	O
:	O
#CODE	O

Thank	O
you	O
!	O

A	O
combination	O
/	O
hybrid	O
of	O
the	O
two	O
suggestions	O
worked	O
for	O
me	O
.	O

Thank	O
you	O
.	O

pandas	O
rearrange	O
dataframe	B-API
to	O
have	O
all	O
values	O
in	O
ascending	O
order	O
per	O
every	O
column	O
independently	O

The	O
title	O
should	O
say	O
it	O
all	O
,	O
I	O
want	O
to	O
turn	O
this	O
DataFrame	B-API
:	O
#CODE	O

into	O
this	O
DataFrame	B-API
:	O
#CODE	O

And	O
I	O
want	O
to	O
do	O
it	O
in	O
a	O
nice	O
manner	O
.	O

The	O
ugly	O
solution	O
would	O
be	O
to	O
take	O
every	O
column	O
and	O
form	O
a	O
new	O
DataFrame	B-API
.	O

To	O
test	O
,	O
use	O
:	O
#CODE	O

This	O
is	O
a	O
bit	O
tricky	O
due	O
to	O
the	O
presence	O
of	O
the	O
`	O
NaN	O
`	O
,	O
one	O
method	O
would	O
be	O
to	O
sort	O
the	O
columns	O
that	O
have	O
no	O
NaNs	O
and	O
then	O
sort	O
the	O
columns	O
with	O
NaN	O
and	O
concat	B-API
them	O
together	O
,	O
does	O
that	O
sound	O
reasonable	O
?	O

If	O
you	O
can	O
give	O
a	O
solution	O
that	O
is	O
better	O
than	O
using	O
df.values	B-API
in	O
np	O
(	O
that	O
is	O
without	O
leaving	O
pandas	O
)	O
then	O
go	O
ahead	O
.	O

The	O
desired	O
sort	O
ignores	O
the	O
index	O
values	O
,	O
so	O
the	O
operation	O
appears	O
to	O
be	O
more	O

like	O
a	O
NumPy	O
operation	O
than	O
a	O
Pandas	O
one	O
:	O
#CODE	O

yields	O
#CODE	O

awesome	O
,	O
thanks	O
!	O

Inconsistent	O
Nan	O
Key	O
Error	O
using	O
Pandas	O
Apply	B-API

I'm	O
recoding	O
multiple	O
columns	O
in	O
a	O
dataframe	B-API
and	O
have	O
come	O
across	O
a	O
strange	O
result	O
that	O
I	O
can't	O
quite	O
figure	O
out	O
.	O

I'm	O
probably	O
not	O
recoding	O
in	O
the	O
most	O
efficient	O
manner	O
possible	O
,	O
but	O
it's	O
mostly	O
the	O
error	O
that	O
I'm	O
hoping	O
someone	O
can	O
explain	O
.	O

#CODE	O

s1	O
works	O
fine	O
,	O
but	O
when	O
I	O
try	O
to	O
do	O
the	O
same	O
thing	O
with	O
a	O
list	O
of	O
integers	O
and	O
a	O
np.nan	O
,	O
I	O
get	O
`	O
KeyError	O
:	O
nan	O
`	O
which	O
is	O
confusing	O
.	O

Any	O
help	O
would	O
be	O
appreciated	O
.	O

A	O
workaround	O
is	O
to	O
use	O
the	O
get	O
dict	O
method	O
,	O
rather	O
than	O
the	O
lambda	O
:	O
#CODE	O

It's	O
not	O
clear	O
to	O
me	O
right	O
now	O
why	O
this	O
is	O
different	O
...	O

Note	O
:	O
the	O
dicts	O
can	O
be	O
accessed	O
by	O
nan	O
:	O
#CODE	O

and	O
`	O
hash	O
(	O
np.nan	O
)	O
==	O
0	O
`	O
so	O
it's	O
not	O
that	O
...	O

Update	O
:	O
Apparently	O
the	O
issue	O
is	O
with	O
`	O
np.nan	O
`	O
vs	O
`	O
np.float64	O
(	O
np.nan	O
)`	O
,	O
the	O
former	O
has	O
`	O
np.nan	O
is	O
np.nan	O
`	O
(	O
because	O
`	O
np.nan	O
`	O
is	O
bound	O
to	O
a	O
specific	O
instantiated	O
nan	O
object	O
)	O
whilst	O
`	O
float	O
(	O
'	O
nan	O
')	O
is	O
not	O
float	O
(	O
'	O
nan	O
')`	O
:	O

This	O
means	O
that	O
get	O
won't	O
find	O
`	O
float	O
(	O
'	O
nan	O
')`	O
:	O
#CODE	O

This	O
means	O
you	O
can	O
actually	O
retrieve	O
the	O
nans	O
from	O
a	O
dict	O
,	O
any	O
such	O
retrieval	O
would	O
be	O
implementation	O
specific	O
!	O

In	O
fact	O
,	O
as	O
the	O
dict	O
uses	O
the	O
id	O
of	O
these	O
nans	O
,	O
this	O
entire	O
behavior	O
above	O
may	O
be	O
implementation	O
specific	O
(	O
if	O
nan	O
shared	O
the	O
same	O
id	O
,	O
as	O
they	O
may	O
do	O
in	O
a	O
REPL	O
/	O
ipython	O
session	O
)	O
.	O

You	O
can	O
catch	O
the	O
nullness	O
beforehand	O
:	O
#CODE	O

But	O
I	O
think	O
the	O
original	O
suggestion	O
of	O
using	O
.get	B-API
is	O
a	O
better	O
option	O
.	O

I	O
think	O
this	O
may	O
be	O
a	O
bug	O
in	O
apply	B-API
/	O
map_infer	O
,	O
definitely	O
worth	O
a	O
github	O
issue	O
.	O

Thanks	O
for	O
the	O
sanity	O
check	O
.	O

I	O
opened	O
an	O
issue	O
on	O
github	O
.	O

Great	O
,	O
thanks	O
!	O

#URL	O

@USER	O
Ah	O
yes	O
,	O
I	O
meant	O
is	O
.	O

Thanks	O
!!	O

Pandas	O
DataFrame	B-API
index	O
by	O
belonging	O
to	O
a	O
set	O

I	O
have	O
a	O
Pandas	O
DataFrame	B-API
that	O
,	O
among	O
the	O
columns	O
,	O
has	O
one	O
called	O
Phone_Number	O
.	O

I	O
want	O
to	O
get	O
just	O
the	O
rows	O
that	O
have	O
a	O
phone	O
number	O
that	O
shows	O
50	O
times	O
or	O
more	O
.	O

My	O
best	O
attempt	O
was	O
this	O
:	O
#CODE	O

I	O
get	O
,	O
however	O
,	O
this	O
error	O
:	O
TypeError	O
:	O
'	O
Series	B-API
'	O
objects	O
are	O
mutable	O
,	O
thus	O
they	O
cannot	O
be	O
hashed	O

What	O
would	O
be	O
the	O
best	O
way	O
to	O
get	O
the	O
rows	O
in	O
the	O
data	O
frame	O
for	O
this	O
situation	O
?	O

Thank	O
you	O
very	O
much	O
!	O

Use	O
`	O
isin()	B-API
`	O
:	O
#URL	O

Thank	O
you	O
,	O
@USER	O
!	O

It	O
does	O
not	O
throw	O
an	O
error	O
,	O
but	O
I	O
get	O
an	O
empty	O
set	O
,	O
which	O
I	O
thought	O
wasn't	O
possible	O
(	O
the	O
counts	O
set	O
is	O
not	O
empty	O
,	O
and	O
was	O
generated	O
from	O
the	O
phone	O
numbers	O
contained	O
in	O
'	O
data	O
')	O

That's	O
odd	O
,	O
I	O
would	O
have	O
thought	O
`	O
data	O
[	O
data.Phone_Number.isin	O
(	O
counts.index	O
)]`	O
would	O
work	O
.	O

Are	O
you	O
able	O
to	O
post	O
a	O
small	O
sample	O
of	O
your	O
data	O
?	O

@USER	O
thanks	O
again	O
,	O
my	O
mistake	O
was	O
not	O
to	O
use	O
isin	B-API
(	O
counts.index	O
)	O
but	O
just	O
isin	B-API
(	O
counts	O
)	O
,	O
this	O
is	O
also	O
a	O
valid	O
solution	O
for	O
the	O
problem	O
.	O

You	O
can	O
use	O
`	O
groupby	B-API
`	O
with	O
`	O
filter	B-API
`	O
.	O

#CODE	O

Thank	O
you	O
,	O
@USER	O
-li	O
!	O

Python	O
Pandas	O
hdfstore's	O
select	O
(	O
where=	O
'')	O
return	O
unqualified	O
results	O

When	O
I	O
query	O
a	O
large	O
hdfstore	O
file	O
(	O
>	O
10G	O
)	O
like	O
this	O
:	O
#CODE	O

I	O
got	O
results	O
where	O
most	O
entries	O
'	O
node_id	O
is	O
1	O
,	O
but	O
some	O
entries	O
have	O
node_id	O
other	O
than	O
1	O
.	O

So	O
is	O
it	O
a	O
hdfstore	O
glitch	O
,	O
or	O
I	O
did	O
something	O
wrong	O
?	O

Here	O
is	O
part	O
of	O
the	O
results	O
you	O
can	O
see	O
there	O
are	O
some	O
entries	O
with	O
node_id	O
other	O
than	O
1	O
.	O

#CODE	O

Noticing	O
row	O
300002	O
is	O
an	O
unwanted	O
result	O
,	O
I	O
try	O
to	O
select	O
node	O
1	O
around	O
that	O
particular	O
area	O
like	O
this	O
:	O
#CODE	O

Only	O
node	O
3	O
is	O
returned	O
in	O
the	O
result	O
:	O
#CODE	O

Then	O
I	O
try	O
use	O
index	O
instead	O
of	O
start	O
/	O
stop	O
like	O
this	O
:	O
#CODE	O

And	O
this	O
time	O
it	O
returned	O
correct	O
results	O
:	O
#CODE	O

I	O
guess	O
I	O
might	O
walk	O
around	O
this	O
problem	O
with	O
selection	O
on	O
index	O
,	O
but	O
I	O
am	O
not	O
completely	O
sure	O
because	O
the	O
method	O
with	O
start	O
/	O
stop	O
also	O
get	O
the	O
correct	O
results	O
most	O
of	O
the	O
time	O
,	O
so	O
even	O
though	O
the	O
method	O
with	O
index	O
got	O
it	O
right	O
where	O
start	O
/	O
stop	O
failed	O
,	O
it	O
might	O
fail	O
somewhere	O
else	O
.	O

And	O
I	O
would	O
really	O
like	O
the	O
start	O
/	O
stop	O
method	O
to	O
work	O
,	O
because	O
it	O
is	O
much	O
faster	O
,	O
and	O
I	O
have	O
a	O
large	O
data	O
set	O
,	O
a	O
slow	O
method	O
is	O
really	O
time-consuming	O
.	O

BTW	O
,	O
In	O
case	O
you	O
are	O
wondering	O
,	O
I	O
cannot	O
use	O
'	O
chunksize	O
'	O
like	O
this	O
:	O
#CODE	O

Every	O
time	O
I	O
try	O
chunksize	O
I	O
got	O
a	O
MemoryError	O
like	O
this	O
.	O

Struggling	O
with	O
many	O
problems	O
,	O
Pandas	O
is	O
really	O
tough	O
for	O
a	O
newbie	O
like	O
me	O
.	O

Any	O
help	O
is	O
greatly	O
appreciated	O
.	O

This	O
was	O
a	O
recently	O
fixed	O
bug	O
in	O
`	O
PyTables	O
`	O
,	O
see	O
the	O
related	O
issue	O
here	O
.	O

In	O
effect	O
on	O
some	O
larger	O
stores	O
the	O
indexers	O
where	O
not	O
computed	O
correctly	O
when	O
using	O
a	O
`	O
where	B-API
`	O
and	O
`	O
start	O
/	O
stop	O
`	O
.	O

You	O
will	O
need	O
to	O
update	O
to	O
`	O
PyTables	O
`	O
3.2	O
,	O
then	O
re-write	O
the	O
store	O
itself	O
.	O

You	O
can	O
either	O
recreate	O
it	O
how	O
you	O
did	O
the	O
first	O
time	O
,	O
or	O
use	O
ptrepack	O

I	O
use	O
anaconda	O
,	O
and	O
I	O
cannot	O
upgrade	O
with	O
`	O
conda	O
update	O
pytables	O
`	O
,	O
it	O
says	O
"	O
already	O
installed	O
"	O
.	O

Then	O
I	O
tried	O
`	O
pip	O
install	O
--	O
upgrade	O
tables	O
`	O
,	O
it	O
says	O
`	O
LINK	O
:	O
fatal	O
error	O
LNK1181	O
:	O
cannot	O
open	O
input	O
file	O
'	O
hdf5dll.lib	O
'	O
...	O

ERROR	O
::	O
Could	O
not	O
find	O
a	O
local	O
HDF5	O
installation	O
.	O

`	O
I	O
searched	O
my	O
harddisk	O
,	O
and	O
couldn't	O
find	O
hdf5dll.lib	O
file	O
.	O

Pandas	O
:	O
Crash	O
when	O
dividing	O
one	O
column	O
by	O
the	O
other	O
with	O
index	O
set	O

I	O
have	O
a	O
larger	O
data	O
set	O
,	O
with	O
121232	O
rows	O
and	O
many	O
columns	O
:	O
#CODE	O

If	O
I	O
do	O
#CODE	O

pandas	O
,	O
python	O
,	O
my	O
editor	O
(	O
PyCharm	O
)	O
and	O
Windows	O
7	O
all	O
together	O
crash	O
,	O
one	O
after	O
the	O
other	O
.	O

I	O
guess	O
there	O
is	O
an	O
additional	O
issue	O
related	O
to	O
PyCharm	O
and	O
its	O
memory	O
usage	O
,	O
but	O
this	O
should	O
not	O
be	O
happening	O
inside	O
pandas	O
in	O
the	O
first	O
place	O
,	O
should	O
it	O
?	O

By	O
the	O
way	O
,	O
if	O
I	O
reset	O
the	O
index	O
before	O
the	O
division	O
,	O
`	O
pandas	O
`	O
does	O
not	O
crash	O
.	O

My	O
pandas	O
version	O
is	O
`	O
0.13.1	O
`	O
.	O

My	O
Python	O
system	O
:	O
`	O
2.7.3	O
|	O
64-bit	O
|	O
(	O
default	O
,	O
Aug	O
8	O
2013	O
,	O
05:30	O
:	O
12	O
)	O
[	O
MSC	O
v.1500	O
64	O
bit	O
(	O
AMD64	O
)`	O

How	O
many	O
rows	O
and	O
are	O
you	O
using	O
64-bit	O
python	O
?	O

updated	O
the	O
question	O

This	O
worked	O
fine	O
for	O
me	O
using	O
ipython	O
2.0	O
python	O
3.3	O
64-bit	O
pandas	O
0.13.1	O
,	O
maybe	O
a	O
problem	O
with	O
pycharm	O
,	O
I	O
have	O
16gb	O
on	O
my	O
machine	O
but	O
I	O
don't	O
think	O
that	O
should	O
make	O
a	O
difference	O
as	O
your	O
dataset	O
size	O
is	O
small	O
.	O

Not	O
sure	O
why	O
resetting	O
the	O
index	O
would	O
suddenly	O
make	O
it	O
work	O

Skip	O
last	O
row	O
when	O
importing	O
CSV	O
into	O
Pandas.DataFrame	B-API

`	O
pd.read_csv	B-API
(	O
f	O
,	O
skiprows=n	O
)`	O
skips	O
only	O
the	O
first	O
`	O
n	O
`	O
rows	O
of	O
the	O
CSV	O
.	O

Is	O
it	O
possible	O
to	O
skip	O
the	O
last	O
X	O
rows	O
of	O
a	O
CSV	O
file	O
when	O
importing	O
it	O
into	O
a	O
`	O
pandas.DataFrame	B-API
`	O
?	O

See	O
the	O
other	O
answer	O
for	O
the	O
question	O
just	O
pass	O
`	O
skipfooter=n	O
`	O
to	O
`	O
read_csv	B-API
`	O

How	O
to	O
remove	O
the	O
u	O
'	O
and	O
some	O
other	O
information	O
from	O
Ipython	O
/	O
Pandas	O
output	O

I've	O
been	O
reading	O
the	O
great	O
Python	O
for	O
Data	O
Analysis	O
book	O
and	O
following	O
its	O
exercises	O
along	O
,	O
but	O
my	O
outputs	O
are	O
not	O
the	O
same	O
that	O
the	O
outputs	O
shown	O
in	O
the	O
book	O
.	O

One	O
of	O
them	O
happens	O
when	O
I	O
want	O
to	O
print	O
the	O
indices	O
of	O
a	O
data	O
frame	O
object	O
.	O

For	O
example	O
:	O
#CODE	O

When	O
I	O
call	O
data.index	O
,	O
I	O
get	O
a	O
output	O
different	O
from	O
the	O
book	O
.	O

Here's	O
the	O
output	O
shown	O
in	O
the	O
book	O
:	O
#CODE	O

And	O
this	O
is	O
my	O
output	O
:	O
#CODE	O

How	O
do	O
I	O
configure	O
either	O
Ipython	O
or	O
Pandas	O
to	O
change	O
the	O
output	O
formatting	O
?	O

At	O
least	O
the	O
u	O
'	O
piece	O
of	O
string	O
.	O

Edit	O
:	O
I'm	O
using	O
Python	O
2.7	O
.	O

It	O
just	O
means	O
its	O
a	O
unicode	O
string	O
,	O
it	O
shouldn't	O
affect	O
any	O
operations	O
so	O
it's	O
a	O
display	O
thing	O
,	O
see	O
this	O
:	O
#URL	O

Also	O
,	O
that	O
book	O
is	O
based	O
on	O
a	O
somewhat	O
older	O
version	O
of	O
pandas	O
and	O
some	O
things	O
(	O
like	O
indexes	O
)	O
have	O
changed	O
in	O
the	O
meantime	O
.	O

It	O
shouldn't	O
be	O
a	O
problem	O
for	O
the	O
most	O
part	O
,	O
as	O
old	O
ways	O
of	O
doing	O
things	O
will	O
mostly	O
continue	O
to	O
work	O
even	O
as	O
better	O
ways	O
to	O
do	O
them	O
are	O
provided	O
.	O

You	O
can	O
look	O
at	O
the	O
release	O
notes	O
in	O
the	O
docs	O
if	O
you	O
want	O
to	O
see	O
how	O
things	O
have	O
changed	O

your	O
output	O
is	O
different	O
from	O
what	O
you	O
can	O
see	O
in	O
the	O
structure	O
itself	O
,	O
see	O
answer	O
below	O
.	O

EdChum	O
summarized	O
about	O
the	O
unicode	O
character	O
before	O
each	O
string	O
.	O

Using	O
Python	O
3	O
would	O
be	O
one	O
way	O
to	O
get	O
rid	O
of	O
the	O
`	O
u	O
`	O
prefixes	O
,	O
though	O
perhaps	O
a	O
bit	O
of	O
a	O
drastic	O
one	O
.	O

You	O
can	O
have	O
this	O
display	O
if	O
you	O
do	O
a	O
`	O
list	O
`	O
conversion	O
:	O
#CODE	O

How	O
to	O
get	O
the	O
reverse	O
percentile	O
for	O
a	O
list	O
of	O
scores	O
from	O
a	O
huge	O
txt	O
file	O
?	O

I	O
have	O
a	O
very	O
big	O
text	O
file	O
(	O
>	O
80Gb	O
)	O
.	O

It	O
contains	O
tab-delimited	O
values	O
.	O

I	O
am	O
interested	O
only	O
in	O
one	O
column	O
.	O

For	O
that	O
specific	O
column	O
,	O
I	O
want	O
to	O
get	O
the	O
reverse	O
percentile	O
for	O
~10	O
thresholds	O
.	O

So	O
basically	O
,	O
my	O
questions	O
look	O
like	O
this	O
:	O
"	O
What	O
is	O
the	O
percentage	O
of	O
rows	O
where	O
the	O
value	O
of	O
column	O
x	O
is	O
below	O
$threshold	O
?	O

"	O
.	O

Thresholds	O
are	O
roughly	O
1	O
,	O
5	O
,	O
10	O
,	O
100	O
,	O
500	O
,	O
1000	O
.	O

Sample	O
data	O
:	O
#CODE	O

In	O
the	O
above	O
case	O
,	O
I	O
would	O
like	O
to	O
ask	O
"	O
What	O
is	O
the	O
percentage	O
of	O
values	O
below	O
500	O
?	O

"	O
and	O
the	O
answer	O
would	O
be	O
80%	O
.	O

How	O
would	O
I	O
do	O
this	O
?	O

Notes	O
:	O

Using	O
awk	O
to	O
filter	O
the	O
file	O
first	O
for	O
the	O
interesting	O
column	O
took	O
~26mins	O
which	O
is	O
fine	O
speed-wise	O
(	O
ended	O
up	O
with	O
a	O
file	O
10Gb	O
)	O
.	O

Reading	O
the	O
resulting	O
file	O
into	O
a	O
pandas	O
data	O
frame	O
takes	O
~7	O
mins	O
;	O
but	O
the	O
calculation	O
(	O
`	O
df	O
[	O
df	O
threshold	O
]	O
.shape	B-API
(	O
0	O
)	O
/	O
total_length	O
`)	O
takes	O
way	O
too	O
long	O
.	O

I	O
stopped	O
calculations	O
after	O
a	O
couple	O
of	O
hours	O
.	O

I	O
guess	O
~1h	O
would	O
be	O
okay	O
.	O

`	O
wc	O
-l	O
filename	O
`	O
and	O
`	O
df	O
=	O
pd.read_csv	B-API
(	O
filename	O
,	O
sep=	O
'	O
\t	O
'	O
,	O
header=None	O
);	O
print	O
(	O
pandasdataframe	O
)`	O
yielded	O
a	O
different	O
number	O
of	O
rows	O
which	O
astonished	O
me	O
.	O

(	O
I'm	O
new	O
to	O
Pandas	O
,	O
though	O
)	O
.	O

I'd	O
prefer	O
a	O
solution	O
in	O
Python	O
/	O
Shell	O
but	O
I'm	O
open	O
for	O
any	O
ideas	O
.	O

EDIT	O
:	O

The	O
answer	O
below	O
is	O
correct	O
.	O

I	O
came	O
up	O
with	O
the	O
script	O
below	O
.	O

FYI	O
,	O
reading	O
the	O
prefiltered	O
file	O
(	O
one	O
column	O
only	O
,	O
10G	O
)	O
took	O
1h02	O
and	O
reading	O
the	O
original	O
file	O
(	O
5	O
cols	O
,	O
>	O
80G	O
)	O
took	O
1h16	O
.	O

For	O
the	O
sake	O
of	O
simplicity	O
,	O
I	O
won't	O
prefilter	O
the	O
file	O
then	O
.	O
mawk	O
was	O
2x	O
better	O
than	O
gawk	O
in	O
my	O
tests	O
.	O

I	O
used	O
`	O
NR	O
`	O
instead	O
of	O
`	O
(	O
NR-1	O
)`	O
as	O
there	O
is	O
no	O
header	O
row	O
.	O

#CODE	O

This	O
code	O
should	O
be	O
fast	O
enough	O
:	O
`	O
awk	O
'	O
BEGIN	O
{	O
n=0	O
;	O
getline}	O
;	O
{	O
if	O
(	O
$3	O
<	O
500	O
)	O
{	O
n+=1	O
}}	O
END	O
{	O
print	O
n	O
/(	O
NR-1	O
)	O
*100	O
}	O
'	O
x.txt	O
`	O

@USER	O
What	O
is	O
the	O
point	O
of	O
that	O
begin	O
block	O
?	O

@USER	O
,	O
to	O
skip	O
the	O
header	O
line	O

@USER	O
ahh	O
okay	O
,	O
don't	O
need	O
the	O
n=0	O
though	O
.	O

All	O
variables	O
are	O
initialised	O
at	O
0	O
anyway	O
in	O
awk	O
.	O

@USER	O
,	O
yes	O
,	O
that's	O
true	O
.	O

Explicit	O
is	O
better	O
than	O
implicit	O
...	O

No	O
,	O
explicit	O
is	O
NOT	O
better	O
than	O
implicit	O
.	O

If	O
it	O
were	O
we'd	O
all	O
be	O
writing	O
assembly	O
code	O
.	O

Conciseness	O
is	O
an	O
attribute	O
of	O
good	O
software	O
.	O

@USER	O
Thanks	O
,	O
that	O
helped	O
.	O

I	O
would	O
suggest	O
using	O
awk	O
to	O
do	O
this	O
:	O
#CODE	O

For	O
all	O
lines	O
after	O
the	O
first	O
one	O
where	O
the	O
third	O
field	O
is	O
less	O
than	O
500	O
,	O
increment	O
`	O
n	O
`	O
.	O

Once	O
the	O
file	O
has	O
been	O
processed	O
,	O
print	O
the	O
percentage	O
,	O
as	O
long	O
as	O
one	O
or	O
more	O
records	O
have	O
been	O
read	O
(	O
this	O
avoids	O
a	O
division	O
by	O
0	O
)	O
.	O

Pandas	O
FloatingPoint	O
Error	O

I'm	O
getting	O
a	O
floating	O
point	O
error	O
on	O
a	O
simple	O
time	O
series	B-API
in	O
pandas	O
.	O

I'm	O
trying	O
to	O
do	O
shift	O
operations	O
...	O
but	O
this	O
also	O
happens	O
with	O
the	O
window	O
functions	O
like	O
`	O
rolling_mean	B-API
`	O
.	O

EDIT	O
:	O
For	O
some	O
more	O
info	O
...	O

I	O
tried	O
to	O
actually	O
build	O
this	O
from	O
source	O
yesterday	O
prior	O
to	O
the	O
error	O
.	O

I'm	O
not	O
sure	O
if	O
the	O
error	O
would've	O
occurred	O
prior	O
the	O
build	O
attempt	O
,	O
as	O
I'd	O
never	O
messed	O
around	O
w	O
these	O
functions	O
.	O

EDIT2	O
:	O
I	O
thought	O
I'd	O
fixed	O
this	O
,	O
but	O
when	O
I	O
run	O
this	O
inside	O
python	O
it	O
works	O
,	O
but	O
when	O
it's	O
in	O
ipython	O
I	O
get	O
the	O
error	O
.	O

EDIT3	O
:	O
Numpy	O
1.7.0	O
,	O
iPython	O
0.13	O
,	O
pandas	O
0.7.3	O
#CODE	O

Works	O
fine	O
for	O
me	O
.	O

Python	O
2.7.3	O
,	O
pandas	O
0.7.0	O

Thanks	O
-	O
are	O
did	O
you	O
try	O
this	O
in	O
ipython	O
or	O
python	O
?	O

What	O
version	O
of	O
NumPy	O
?	O

1.7.0	O
,	O
installed	O
using	O
the	O
scipysuperpack	O

PS	O
,	O
just	O
bought	O
your	O
Python	O
for	O
Data	O
Analysis	O
early	O
release	O
,	O
which	O
is	O
great	O
,	O
but	O
also	O
how	O
I	O
ran	O
into	O
this	O
.	O

Yes	O
,	O
I	O
tried	O
with	O
ipython	O
without	O
any	O
problems	O
.	O

Though	O
mine	O
was	O
0.12.1	O
.	O

You	O
are	O
using	O
a	O
development	O
version	O
,	O
both	O
for	O
ipython	O
and	O
numpy	O
.	O

It's	O
highly	O
possible	O
that	O
those	O
are	O
unstable	O
.	O

That	O
might	O
be	O
the	O
problem	O
.	O

I	O
would	O
add	O
this	O
as	O
a	O
comment	O
,	O
but	O
I	O
don't	O
have	O
the	O
privilege	O
to	O
do	O
that	O
yet	O
:)	O

It	O
works	O
for	O
me	O
in	O
python	O
and	O
iPython	O
0.12	O
;	O
iPython	O
0.13	O
is	O
still	O
in	O
development	O
(	O
see	O
#URL	O
)	O
,	O
and	O
,	O
since	O
the	O
errors	O
you're	O
getting	O
seem	O
to	O
involve	O
formatting	O
in	O
the	O
iPython	O
0.13	O
egg	O
,	O
I	O
suspect	O
that	O
might	O
be	O
the	O
cause	O
.	O

Try	O
with	O
iPython	O
0.12	O
instead	O
--	O
if	O
it	O
works	O
,	O
file	O
a	O
bug	O
report	O
with	O
iPython	O
and	O
then	O
probably	O
stick	O
with	O
0.12	O
until	O
0.13	O
is	O
(	O
more	O
)	O
stable	O
.	O

Thanks	O
for	O
the	O
reply	O
,	O
I'm	O
still	O
getting	O
the	O
error	O
on	O
0.12	O
.	O

So	O
it	O
seems	O
to	O
be	O
more	O
pervasive	O
in	O
my	O
system	O
.	O

Oddly	O
works	O
in	O
bpython	O
.	O

Hm	O
.	O

Just	O
a	O
stab	O
,	O
but	O
,	O
what	O
happens	O
if	O
you	O
try	O
np.abs	O
([	O
np.nan	O
,	O
1	O
,	O
2	O
,	O
3	O
])	O
in	O
iPython	O
?	O

Good	O
catch	O
...	O
throws	O
the	O
same	O
error	O
.	O

Thought	O
so	O
.	O

Are	O
you	O
sure	O
your	O
iPython	O
numpy	O
is	O
the	O
same	O
as	O
the	O
other	O
installs	O
?	O

Try	O
np.__version__	O
in	O
the	O
working	O
vs	O
.	O
nonworking	O
environments	O
.	O

It	O
is	O
the	O
same	O
version	O
`	O
1.7.0.dev-3cb783e	O
`	O
.	O

I'm	O
guessing	O
your	O
iPython	O
for	O
some	O
reason	O
is	O
operating	O
with	O
a	O
lower	O
error	O
threshold	O
.	O

The	O
FloatingPointError	O
about	O
the	O
NaN	O
raises	O
an	O
"	O
invalid	O
"	O
flag	O
which	O
is	O
usually	O
ignored	O
,	O
but	O
can	O
be	O
raised	O
if	O
desired	O
.	O

Check	O
out	O
the	O
docs	O
here	O
:	O
#URL	O
,	O
and	O
play	O
around	O
with	O
resetting	O
the	O
numpy	O
errors	O
as	O
in	O
the	O
examples	O
.	O

Does	O
that	O
fix	O
it	O
?	O

If	O
so	O
,	O
there	O
must	O
be	O
some	O
way	O
to	O
configure	O
the	O
error	O
levels	O
for	O
iPython	O
...	O

I've	O
solved	O
this	O
...	O
even	O
though	O
the	O
version	O
numbers	O
were	O
some	O
build	O
issues	O
I	O
guess	O
.	O

I	O
cleaned	O
up	O
the	O
packages	O
I	O
had	O
and	O
it	O
works	O
now	O
...	O
thanks	O
for	O
all	O
them	O
help	O
.	O

Get	O
last	O
"	O
column	O
"	O
after	O
.str	B-API
.split()	B-API
operation	O
on	O
column	O
in	O
pandas	O
DataFrame	B-API

I	O
have	O
a	O
column	O
in	O
a	O
pandas	O
DataFrame	B-API
that	O
I	O
would	O
like	O
to	O
split	O
on	O
a	O
single	O
space	O
.	O

The	O
splitting	O
is	O
simple	O
enough	O
with	O
`	O
DataFrame.str.split	O
(	O
'	O
')`	O
,	O
but	O
I	O
can't	O
make	O
a	O
new	O
column	O
from	O
the	O
last	O
entry	O
.	O

When	O
I	O
`	O
.str	B-API
.split()	B-API
`	O
the	O
column	O
I	O
get	O
a	O
list	O
of	O
arrays	O
and	O
I	O
don't	O
know	O
how	O
to	O
manipulate	O
this	O
to	O
get	O
a	O
new	O
column	O
for	O
my	O
DataFrame	B-API
.	O

Here	O
is	O
an	O
example	O
.	O

Each	O
entry	O
in	O
the	O
column	O
contains	O
'	O
symbol	O
data	O
price	O
'	O
and	O
I	O
would	O
like	O
to	O
split	O
off	O
the	O
price	O
(	O
and	O
eventually	O
remove	O
the	O
"	O
p	O
"	O
...	O
or	O
"	O
c	O
"	O
in	O
half	O
the	O
cases	O
)	O
.	O

#CODE	O

which	O
yields	O
#CODE	O

But	O
`	O
temp2	O
[	O
0	O
]`	O
just	O
gives	O
one	O
list	O
entry's	O
array	O
and	O
`	O
temp2	O
[:	O
]	O
[	O
-1	O
]`	O
fails	O
.	O

How	O
can	O
I	O
convert	O
the	O
last	O
entry	O
in	O
each	O
array	O
to	O
a	O
new	O
column	O
?	O

Thanks	O
!	O

You	O
could	O
use	O
the	O
`	O
tolist	B-API
`	O
method	O
as	O
an	O
intermediary	O
:	O
#CODE	O

From	O
which	O
you	O
could	O
make	O
a	O
new	O
DataFrame	B-API
:	O
#CODE	O

For	O
good	O
measure	O
,	O
you	O
could	O
fix	O
the	O
price	O
:	O
#CODE	O

PS	O
:	O
but	O
if	O
you	O
really	O
just	O
want	O
the	O
last	O
column	O
,	O
`	O
apply	B-API
`	O
would	O
suffice	O
:	O
#CODE	O

Thanks	O
for	O
the	O
education	O
!	O

Do	O
this	O
:	O
#CODE	O

Love	O
the	O
clean	O
solution	O
!	O

from	O
the	O
author	O
of	O
"	O
Pandas	O
"	O
:)	O

Unpacking	O
list	O
of	O
dictionary	O
elements	O
into	O
pandas	O
data	O
frame	O

I	O
am	O
trying	O
to	O
parse	O
my	O
itunes	O
playlist	O
which	O
is	O
in	O
xml	O
format	O
.	O

Here	O
is	O
the	O
sample	O
xml	O
which	O
i	O
am	O
trying	O
to	O
parse	O
and	O
put	O
the	O
end	O
result	O
in	O
pandas	O
data	O
frame	O
.	O

#CODE	O

Following	O
is	O
my	O
python	O
code	O
for	O
parsing	O
the	O
xml	O
#CODE	O

The	O
end	O
result	O
"	O
oddelements	O
"	O
object	O
is	O
a	O
list	O
of	O
element	O
dictionaries	O

Each	O
element	O
dictionary	O
in	O
this	O
list	O
contains	O
the	O
information	O
enclosed	O
in	O
"	O
dict	O
"	O
tag	O
in	O
the	O
sample	O
xml	O
which	O
i	O
have	O
pasted	O
above	O
.	O

How	O
do	O
i	O
parse	O
this	O
list	O
of	O
element	O
dictionaries	O
and	O
unpack	O
them	O
into	O
pandas	O
data	O
frame	O
for	O
further	O
analysis	O
?	O

Many	O
thanks	O
for	O
help	O

Something	O
like	O
that	O
should	O
work	O
:	O
#CODE	O

Thanks	O
Uri	O
.	O

However	O
if	O
i	O
was	O
to	O
go	O
by	O
my	O
way	O
of	O
doing	O
it	O
though	O
lxml	O
package	O
,	O
do	O
u	O
have	O
any	O
ideas	O
about	O
how	O
to	O
unpack	O
key	O
values	O
from	O
list	O
of	O
dictonary	O
elements	O
i.e	O
say	O
if	O
my	O
object	O
was	O
a	O
list	O
containing	O
dictionaries	O
like	O
[	O
dict	O
1	O
,	O
dict	O
2	O
....	O
dict	O
n	O
]	O
?	O

With	O
pandas	O
,	O
how	O
do	O
I	O
calculate	O
a	O
rolling	O
number	O
of	O
events	O
in	O
the	O
last	O
second	O
given	O
timestamp	O
data	O
?	O

I	O
have	O
dataset	O
where	O
I	O
calculate	O
service	O
times	O
based	O
on	O
request	O
and	O
response	O
times	O
.	O

I	O
would	O
like	O
to	O
add	O
a	O
calculation	O
of	O
requests	O
in	O
the	O
last	O
second	O
to	O
show	O
the	O
obvious	O
relationship	O
that	O
as	O
we	O
get	O
more	O
requests	O
per	O
second	O
the	O
system	O
slows	O
.	O

Here	O
is	O
the	O
data	O
that	O
I	O
have	O
,	O
for	O
example	O
:	O
#CODE	O

For	O
this	O
I	O
would	O
like	O
a	O
rolling	O
data	O
set	O
of	O
something	O
like	O
:	O
#CODE	O

I've	O
tried	O
rolling_sum	B-API
and	O
rolling_count	B-API
,	O
but	O
unless	O
I	O
am	O
using	O
them	O
wrong	O
or	O
not	O
understanding	O
the	O
period	O
function	O
,	O
it	O
is	O
not	O
working	O
for	O
me	O
.	O

For	O
your	O
problem	O
,	O
it	O
looks	O
like	O
you	O
want	O
to	O
summarize	O
your	O
data	O
set	O
using	O
a	O
split-apply-combine	O
approach	O
.	O

See	O
here	O
for	O
the	O
documentation	O
that	O
will	O
help	O
you	O
get	O
your	O
code	O
in	O
working	O
but	O
basically	O
,	O
you'll	O
want	O
to	O
do	O
the	O
following	O
:	O

Create	O
a	O
new	O
column	O
(	O
say	O
,	O
'	O
Req_Time_Sec	O
that	O
includes	O
`	O
Req_Time	O
`	O
down	O
to	O
only	O
second	O
resolution	O
(	O
e.g.	O
`	O
14:07	O
:	O
08.729000	O
`	O
becomes	O
`	O
14:07	O
:	O
08	O
`)	O

use	O
`	O
groups	O
=	O
serviceTimes.groupby	O
(	O
'	O
Req_Time_Sec	O
)`	O
to	O
separate	O
your	O
data	O
set	O
into	O
sub-groups	O
based	O
on	O
which	O
second	O
each	O
request	O
occurs	O
in	O
.	O

Finally	O
,	O
create	O
a	O
new	O
data	O
set	O
by	O
calculating	O
the	O
length	O
of	O
each	O
sub	O
group	O
(	O
which	O
represents	O
the	O
number	O
of	O
requests	O
in	O
that	O
second	O
)	O
and	O
aggregating	O
the	O
results	O
into	O
a	O
single	O
DataFrame	B-API
(	O
something	O
like	O
`	O
new_df	O
=	O
groups.aggregate	O
(	O
len	O
)`)	O

The	O
above	O
is	O
all	O
untested	O
pseudo-code	O
,	O
but	O
the	O
code	O
,	O
along	O
with	O
the	O
link	O
to	O
the	O
documentation	O
,	O
should	O
help	O
you	O
get	O
where	O
you	O
want	O
to	O
go	O
.	O

You	O
first	O
need	O
to	O
transform	O
the	O
timestamp	O
into	O
a	O
string	O
which	O
you	O
then	O
groupby	B-API
,	O
showing	O
the	O
count	O
and	O
average	O
service	O
times	O
:	O
#CODE	O

Alternatively	O
,	O
create	O
a	O
data	O
frame	O
of	O
the	O
request	O
time	O
in	O
the	O
appropriate	O
string	O
format	O
,	O
e.g.	O
15-13-15	O
17:27	O
,	O
then	O
count	O
the	O
occurrence	O
of	O
each	O
time	O
stamp	O
using	O
value_counts()	B-API
.	O

You	O
can	O
also	O
plot	O
the	O
results	O
quite	O
easily	O
.	O

#CODE	O

Replacing	O
duplicates	O
pandas	O
to_sql	B-API
(	O
sqlite	O
)	O

I	O
am	O
appending	O
pandas	O
dataframes	O
to	O
sqlite	O
.	O

My	O
primary	O
key	O
is	O
:	O
#CODE	O

My	O
issue	O
is	O
that	O
sometimes	O
I	O
get	O
a	O
new	O
file	O
with	O
old	O
data	O
that	O
I	O
want	O
to	O
append	B-API
to	O
the	O
existing	O
sqlite	O
table	O
.	O

I	O
am	O
not	O
reading	O
that	O
table	O
into	O
memory	O
so	O
I	O
can't	O
drop_duplicates	B-API
in	O
pandas	O
.	O

(	O
For	O
example	O
,	O
one	O
file	O
is	O
always	O
month-to-date	O
data	O
and	O
it	O
is	O
sent	O
to	O
me	O
everyday	O
)	O

How	O
can	O
I	O
ensure	O
that	O
I	O
am	O
only	O
appending	O
unique	O
values	O
based	O
on	O
my	O
primary	O
key	O
?	O

Is	O
there	O
a	O
pandas	O
to_sql	B-API
function	O
to	O
insert	B-API
or	O
replace	O
when	O
I	O
append	B-API
the	O
new	O
data	O
?	O

Also	O
,	O
should	O
I	O
specify	O
dtypes	B-API
in	O
pandas	O
before	O
writing	O
to	O
SQL	O
?	O

I	O
had	O
some	O
error	O
messages	O
when	O
I	O
tried	O
to	O
write	O
to	O
SQLite	O
and	O
I	O
had	O
categorical	B-API
dtypes	B-API
.	O

What	O
errors	O
are	O
you	O
getting	O
for	O
the	O
category	O
dtypes	B-API
?	O

If	O
you	O
attempt	O
to	O
insert	B-API
duplicate	O
data	O
you'll	O
get	O
a	O
`	O
sqlite3.IntegrityError	O
`	O
exception	O
.	O

You	O
can	O
catch	O
that	O
and	O
do	O
nothing	O
,	O
for	O
example	O
:	O
#CODE	O

Will	O
that	O
mean	O
the	O
data	O
will	O
be	O
duplicated	O
in	O
the	O
SQLite	O
table	O
?	O

Is	O
it	O
easier	O
to	O
just	O
drop	O
duplicates	O
within	O
SQLite	O
?	O

I	O
have	O
it	O
set	O
up	O
to	O
only	O
accept	O
unique	O
values	O
for	O
the	O
primary	O
key	O
(	O
the	O
three	O
columns	O
)	O
.	O

The	O
above	O
will	O
not	O
insert	B-API
duplicate	O
data	O
into	O
the	O
SQLite	O
table	O
.	O

Looping	O
through	O
a	O
groupby	B-API
and	O
adding	O
a	O
new	O
column	O

I	O
need	O
to	O
write	O
a	O
small	O
script	O
to	O
get	O
through	O
some	O
data	O
(	O
around	O
50k	O
rows	O
/	O
file	O
)	O
and	O
my	O
original	O
file	O
looks	O
like	O
this	O
:	O
#CODE	O

Its	O
a	O
rather	O
big	O
table	O
with	O
up	O
to	O
50k	O
rows	O
.	O

Now	O
not	O
all	O
the	O
data	O
is	O
important	O
to	O
me	O
,	O
I	O
mainly	O
need	O
the	O
Track_ID	O
and	O
the	O
X	O
and	O
Y	O
Position	O
.	O

So	O
I	O
create	O
a	O
dataframe	B-API
using	O
the	O
excel	O
file	O
and	O
only	O
access	O
the	O
corresponding	O
columns	O
#CODE	O

And	O
this	O
works	O
as	O
expected	O
.	O

Each	O
track_id	O
is	O
basically	O
one	O
set	O
of	O
data	O
that	O
needs	O
to	O
be	O
analyzed	O
.	O

So	O
the	O
straight	O
forward	O
way	O
is	O
to	O
group	O
the	O
dataframe	B-API
by	O
track_id	O
#CODE	O

Also	O
works	O
as	O
intended	O
.	O

Now	O
I	O
need	O
to	O
grab	O
the	O
first	O
POSITION_X	O
value	O
of	O
each	O
group	O
and	O
substract	O
them	O
from	O
the	O
other	O
POSITION_X	O
values	O
in	O
that	O
group	O
.	O

Now	O
,	O
I	O
already	O
read	O
that	O
looping	O
is	O
probably	O
not	O
the	O
best	O
way	O
to	O
go	O
about	O
it	O
,	O
but	O
I	O
have	O
no	O
idea	O
how	O
else	O
to	O
do	O
it	O
.	O

#CODE	O

This	O
stores	O
the	O
value	O
in	O
vect	O
,	O
which	O
,	O
if	O
I	O
print	O
it	O
out	O
,	O
gives	O
me	O
the	O
correct	O
value	O
.	O

However	O
,	O
I	O
have	O
the	O
problem	O
that	O
I	O
do	O
not	O
know	O
how	O
to	O
add	O
it	O
now	O
to	O
a	O
new	O
column	O
.	O

Maybe	O
someone	O
could	O
guide	O
me	O
into	O
the	O
correct	O
direction	O
.	O

Thanks	O
in	O
advance	O
.	O

EDIT	O

This	O
was	O
suggested	O
by	O
chappers	O
#CODE	O

So	O
expected	O
Output	O
would	O
be	O
that	O
the	O
very	O
first	O
calc	O
value	O
of	O
every	O
group	O
is	O
0	O

not	O
sure	O
of	O
another	O
method	O
other	O
then	O
a	O
loop	O
but	O
to	O
keep	O
track	O
of	O
the	O
values	O
just	O
append	B-API
them	O
to	O
a	O
new	O
list	O
.	O

`	O
new_list	O
=	O
[	O
]	O

loop	O
start	O
:	O

do	O
some	O
stuff	O

new_list.append	O
(	O
vect	O
)`	O

Here	O
is	O
one	O
way	O
of	O
approaching	O
it	O
,	O
using	O
the	O
apply	B-API
method	O
to	O
subtract	O
the	O
first	O
item	O
from	O
all	O
the	O
other	O
obs	O
.	O

#CODE	O

This	O
would	O
have	O
input	O
:	O
#CODE	O

and	O
output	O
#CODE	O

Thanks	O
a	O
bunch	O
,	O
in	O
principle	O
it	O
works	O
very	O
well	O
and	O
I	O
really	O
like	O
your	O
short	O
way	O
.	O

I	O
never	O
really	O
worked	O
with	O
lambda	O
operators	O
,	O
therefor	O
I	O
am	O
just	O
reading	O
up	O
on	O
them	O
now	O
.	O

One	O
problem	O
is	O
,	O
preferably	O
the	O
Value	O
for	O
the	O
0s	O
would	O
return	O
0	O
and	O
not	O
the	O
initial	O
X	O
value	O
.	O
any	O
idea	O
?	O

Could	O
you	O
provide	O
an	O
expected	O
input	O
/	O
output	O
?	O

I	O
don't	O
quite	O
understand	O
what	O
you	O
mean	O
.	O

I	O
don't	O
follow	O
,	O
could	O
you	O
post	O
a	O
minimal	O
data	O
set	O
that	O
I	O
can	O
reproduce	O
showing	O
your	O
expected	O
input	O
and	O
output	O
?	O

yes	O
,	O
sorry	O
,	O
I	O
edited	O
into	O
my	O
op	O
.	O

Hope	O
that	O
helps	O

Oh	O
right	O
in	O
that	O
case	O
you	O
don't	O
need	O
the	O
adjustment	O
thingy	O
I've	O
made	O
;	O
I'll	O
edit	O
my	O
answer	O
.	O

works	O
like	O
a	O
charm	O
,	O
thanks	O
so	O
much	O

Iterate	O
Through	O
Dictionary	O
using	O
Column	O

I	O
have	O
the	O
following	O
code	O
using	O
Pandas	O
and	O
a	O
for	O
loop	O
to	O
iterate	O
through	O
the	O
index	O
of	O
the	O
DF	O
and	O
produce	O
strings	O
of	O
each	O
row	O
in	O
a	O
dictionary	O
:	O
#CODE	O

Is	O
this	O
a	O
bad	O
way	O
of	O
doing	O
this	O
?	O

Is	O
there	O
a	O
better	O
way	O
to	O
be	O
able	O
to	O
dynamically	O
create	O
the	O
dictionary	O
object	O
,	O
event	O
,	O
and	O
pass	O
it	O
through	O
a	O
for	O
loop	O
function	O
?	O

Thanks	O
!	O

Chris	O

that	O
looks	O
quite	O
complex	O
-	O
and	O
you	O
are	O
overwriting	O
your	O
`	O
event	O
`	O
dictionary	O
on	O
every	O
iteration	O
of	O
the	O
loop	O
?	O

I'm	O
pretty	O
sure	O
this	O
code	O
won't	O
even	O
run	O
,	O
with	O
`	O
i	O
`	O
being	O
the	O
variable	O
of	O
the	O
for-loop	O
AND	O
some	O
index	O
in	O
the	O
`	O
zip	O
(	O
...	O
)`	O
.	O

I	O
think	O
you	O
should	O
try	O
to	O
specify	O
what	O
your	O
expected	O
outcome	O
is	O
,	O
and	O
then	O
somebody	O
might	O
be	O
able	O
to	O
help	O
you	O
.	O

I	O
am	O
wondering	O
if	O
there	O
is	O
a	O
way	O
to	O
iterate	O
through	O
columns	O
in	O
Pandas	O
producing	O
a	O
dictionary	O
with	O
the	O
iterated	O
variables	O
in	O
place	O
of	O
that	O
dictionary	O
.	O

I	O
am	O
not	O
even	O
sure	O
what	O
this	O
is	O
called	O
haha	O
.	O

Even	O
what	O
that	O
would	O
be	O
called	O
would	O
be	O
helpful	O
...	O

@USER	O
-sc	O

that	O
still	O
doesn't	O
sound	O
helpful	O
..	O

If	O
you	O
would	O
add	O
some	O
example	O
of	O
your	O
data	O
in	O
the	O
DF	O
and	O
your	O
expected	O
outcome	O
,	O
one	O
could	O
try	O
to	O
understand	O
what	O
you	O
want	O
to	O
achieve	O
.	O

But	O
not	O
with	O
this	O
confusing	O
and	O
not	O
working	O
code	O
example	O
.	O

Also	O
have	O
a	O
look	O
at	O
#URL	O

let	O
me	O
re	O
edit	O
the	O
question	O
...	O

@USER	O
-sc	O

Multiply	O
two	O
pandas	O
series	B-API
with	O
mismatched	O
indices	O

Created	O
two	O
series	B-API
:	O
`	O
s1	O
`	O
and	O
`	O
s2	O
`	O
from	O
`	O
df	O
`	O
.	O

Each	O
have	O
same	O
length	O
but	O
differing	O
indices	O
.	O

`	O
s1.multiply	O
(	O
s2	O
)`	O
unions	O
the	O
mismatched	O
indices	O
instead	O
of	O
multiplying	O
against	O
them	O
.	O

I	O
just	O
want	O
to	O
multiply	O
entrywise	O
`	O
s1	O
`	O
against	O
`	O
s2	O
`	O
ignoring	O
the	O
mismatched	O
indices	O
.	O

I	O
could	O
run	O
`	O
s1.reset_index()	O
`	O
and	O
`	O
s2.reset_index()	O
`	O
and	O
then	O
take	O
the	O
column	O
I	O
want	O
from	O
these	O
two	O
dfs	O
,	O
since	O
it	O
turns	O
the	O
original	O
index	O
into	O
a	O
separate	O
column	O
,	O
but	O
that's	O
tedious	O
and	O
I	O
thought	O
there	O
might	O
be	O
a	O
simpler	O
way	O
to	O
do	O
it	O
.	O

#CODE	O

doesn't	O
seem	O
to	O
work	O
either	O

you	O
can	O
convert	O
to	O
a	O
numpy	O
array	O
which	O
will	O
ignore	O
the	O
index	O
with	O
`	O
values	B-API
`	O
:	O
`	O
s1.values.mul	O
(	O
s2.values	O
)`	O
.	O

Thanks	O
John	O
,	O
that	O
does	O
indeed	O
work	O
to	O
multiply	O
the	O
values	O
of	O
the	O
series	B-API
.	O

Unfortunately	O
,	O
it	O
converts	O
the	O
series	B-API
to	O
a	O
numpy	O
array	O
.	O

Do	O
you	O
know	O
of	O
a	O
way	O
to	O
keep	O
the	O
whole	O
process	O
using	O
series	B-API
,	O
instead	O
of	O
moving	O
to	O
numpy	O
arrays	O
,	O
and	O
then	O
back	O
to	O
series	B-API
(	O
result	O
=	O
pandas.Series	B-API
(	O
s1.values	O
*	O
s2.values	O
)	O
)	O
?	O

`	O
s1	O
*	O
s2.values	O
`	O
should	O
work	O

It	O
depends	O
on	O
what	O
you	O
want	O
the	O
index	O
to	O
be	O
.	O

Your	O
suggestion	O
will	O
result	O
in	O
a	O
fresh	O
`	O
[	O
0	O
,	O
1	O
,	O
2	O
...	O
]`	O
index	O
whereas	O
Ed's	O
suggestion	O
will	O
use	O
the	O
index	O
from	O
`	O
s1	O
`	O

Ah	O
,	O
okay	O
.	O

Thank	O
you	O
both	O
John	O
and	O
Ed	O
.	O

Both	O
of	O
those	O
cover	O
the	O
solutions	O
I	O
needed	O
.	O

Could	O
someone	O
add	O
this	O
an	O
answer	O
so	O
we	O
can	O
close	O
the	O
question	O
?	O

I	O
think	O
going	O
with	O
`	O
reset_index()	B-API
`	O
is	O
the	O
way	O
,	O
but	O
there	O
is	O
an	O
option	O
to	O
drop	O
the	O
index	O
,	O
not	O
push	O
it	O
back	O
into	O
the	O
dataframe	B-API
.	O

Like	O
this	O
:	O
#CODE	O

The	O
reason	O
I	O
favour	O
the	O
`	O
reset_index()	B-API
`	O
approach	O
before	O
the	O
other	O
suggested	O
approach	O
with	O
simply	O
multiplying	O
by	O
values	O
#CODE	O

is	O
that	O
this	O
is	O
not	O
very	O
explicit	O
.	O

This	O
line	O
does	O
not	O
tell	O
me	O
that	O
there	O
is	O
an	O
index	O
problem	O
that	O
you	O
are	O
solving	O
.	O

While	O
this	O
line	O
tells	O
the	O
story	O
very	O
explicitly	O
that	O
you	O
are	O
solving	O
an	O
index	O
problem	O
:	O
#CODE	O

Or	O
break	O
it	O
down	O
to	O
multiple	O
rows	O
:	O
#CODE	O

Conditional	O
sum	O
across	O
rows	O
in	O
pandas	O
groupby	B-API
statement	O

I	O
have	O
a	O
dataframe	B-API
containing	O
weekly	O
sales	O
for	O
different	O
products	O
(	O
a	O
,	O
b	O
,	O
c	O
):	O
#CODE	O

I	O
would	O
like	O
to	O
create	O
a	O
new	O
column	O
containing	O
the	O
cumulative	O
sales	O
for	O
the	O
last	O
n	O
weeks	O
,	O
grouped	O
by	O
product	O
.	O

E.g.	O
for	O
`	O
n=2	O
`	O
it	O
should	O
be	O
like	O
`	O
last_2_weeks	O
`	O
:	O
#CODE	O

How	O
can	O
I	O
efficiently	O
calculate	O
such	O
an	O
cumulative	O
,	O
conditional	O
sum	O
in	O
pandas	O
?	O

The	O
solution	O
should	O
also	O
work	O
if	O
there	O
are	O
more	O
variables	O
to	O
group	O
by	O
,	O
e.g.	O
product	O
and	O
location	O
.	O

I	O
have	O
tried	O
creating	O
a	O
new	O
function	O
and	O
using	O
`	O
groupby	B-API
`	O
and	O
`	O
apply	B-API
`	O
,	O
but	O
this	O
works	O
only	O
if	O
rows	O
are	O
sorted	O
.	O

Also	O
it's	O
slow	O
and	O
ugly	O
.	O

#CODE	O

You	O
could	O
use	O
`	O
pd.rolling_sum	B-API
`	O
with	O
`	O
window=2	O
`	O
,	O
then	O
`	O
shift	B-API
`	O
once	O
and	O
fill	O
`	O
NaNs	O
`	O
with	O
`	O
0	O
`	O
#CODE	O

Thanks	O
.	O

Clear	O
solution	O
and	O
much	O
faster	O
compared	O
to	O
my	O
custom	O
function	O
.	O

Requires	O
sorting	O
the	O
data	O
frame	O
by	O
`	O
week	B-API
`	O
.	O

How	O
can	O
I	O
make	O
this	O
loop	O
more	O
efficient	O
?	O

I	O
have	O
a	O
historical	O
collection	O
of	O
~	O
500k	O
loans	O
,	O
some	O
of	O
which	O
have	O
defaulted	O
,	O
others	O
have	O
not	O
.	O

My	O
dataframe	B-API
is	O
`	O
lcd_temp	O
`	O
.	O

`	O
lcd_temp	O
`	O
has	O
information	O
on	O
the	O
loan	O
size	O
(	O
`	O
loan_amnt	O
`)	O
,	O
if	O
loan	O
has	O
defaulted	O
or	O
not	O
(	O
`	O
Total	O
Defaults	O
`)	O
,	O
annual	O
loan	O
rate	O
(	O
`	O
clean_rate	O
`)	O
,	O
term	O
of	O
loan	O
(	O
`	O
clean_term	O
`)	O
,	O
and	O
months	O
from	O
origination	O
to	O
default	O
(	O
`	O
mos_to_default	O
`)	O
.	O

`	O
mos_to_default	O
`	O
is	O
equal	O
to	O
`	O
clean_term	O
`	O
if	O
no	O
default	O
.	O

I	O
would	O
like	O
to	O
calculate	O
the	O
Cumulative	O
Cashflow	O
[	O
`	O
cum_cf	O
`]	O
for	O
each	O
loan	O
as	O
the	O
sum	O
of	O
all	O
coupons	O
paid	O
until	O
default	O
plus	O
(	O
1-severity	O
)	O
if	O
loan	O
defaults	O
,	O
and	O
simply	O
the	O
`	O
loan_amnt	O
`	O
if	O
it	O
pays	O
back	O
on	O
time	O
.	O

Here's	O
my	O
code	O
,	O
which	O
takes	O
an	O
awful	O
long	O
time	O
to	O
run	O
:	O
#CODE	O

Any	O
thoughts	O
or	O
suggestions	O
on	O
improving	O
the	O
speed	O
(	O
which	O
takes	O
over	O
an	O
hour	O
so	O
far	O
)	O
welcomed	O
!	O

Which	O
version	O
of	O
python	O
are	O
you	O
using	O
?	O

Did	O
you	O
profile	O
it	O
to	O
see	O
where	O
the	O
time	O
is	O
going	O
?	O

You	O
can	O
(	O
probably	O
should	O
,	O
if	O
it	O
takes	O
over	O
an	O
hour	O
)	O
use	O
numpy	O
for	O
this	O
,	O
and	O
then	O
select	O
between	O
the	O
two	O
use	O
cases	O
with	O
a	O
mask	O
,	O
e.g.	O
`	O
mask	O
=	O
lcd_temp.loc	O
[	O
...,	O
'	O
Total_Defaults	O
']	O
==	O
1	O
`	O
.	O

OP	O
:	O
are	O
you	O
using	O
Pandas	O
?	O

@USER	O
:	O
why	O
do	O
you	O
believe	O
the	O
`	O
[	O
pandas	O
]`	O
tag	O
is	O
not	O
appropriate	O
?	O

@USER	O
I	O
don't	O
see	O
any	O
indication	O
of	O
a	O
pandas	O
import	O
,	O
nor	O
any	O
use	O
of	O
a	O
pandas	O
namespace	O
.	O

For	O
all	O
I	O
know	O
,	O
the	O
OP	O
is	O
using	O
normal	O
classes	O
,	O
lists	O
and	O
dicts	O
.	O

I	O
think	O
,	O
if	O
you	O
are	O
using	O
python	O
2.6	O
or	O
above	O
,	O
then	O
`	O
multiprocessing	O
`	O
is	O
available	O
to	O
you	O
.	O

It	O
seems	O
to	O
me	O
that	O
you	O
could	O
split	O
your	O
data	O
set	O
into	O
a	O
number	O
of	O
chunks	O
,	O
one	O
per	O
core	O
,	O
and	O
pass	O
each	O
chunk	O
to	O
a	O
process	O
and	O
write	O
the	O
results	O
back	O
after	O
all	O
chunks	O
have	O
been	O
calculated	O
.	O

@USER	O
In	O
your	O
code	O
I	O
don't	O
see	O
anything	O
which	O
make	O
this	O
too	O
much	O
latent	O
..	O

`	O
1	O
hour	O
`	O
is	O
huge	O
,	O
you	O
might	O
have	O
to	O
add	O
more	O
information	O
here	O
like	O
..	O
what	O
is	O
your	O
data	O
frame	O
?	O

is	O
this	O
the	O
simple	O
calculation	O
or	O
any	O
other	O
operation	O
happening	O
?	O

@USER	O
the	O
question	O
mentions	O
a	O
dataframe	B-API
,	O
which	O
in	O
Python	O
means	O
Pandas	O
.	O

Things	O
like	O
`	O
lcd_temp.loc	O
`	O
in	O
the	O
question	O
also	O
show	O
that	O
OP	O
is	O
using	O
Pandas	O
,	O
not	O
standard	O
Python	O
data	O
structures	O
.	O

Sorry	O
for	O
delay	O
,	O
yes	O
,	O
using	O
Pandas	O
.	O

The	O
import	O
occured	O
in	O
another	O
cell	O
not	O
included	O
in	O
my	O
code	O
.	O

Apologies	O
.	O

And	O
thanks	O
to	O
unutbu	O
below	O
for	O
the	O
suggestion	O
.	O

Assuming	O
you	O
are	O
using	O
Pandas	O
/	O
NumPy	O
,	O
the	O
standard	O
way	O
to	O
replace	O
an	O
`	O
if-then	O
`	O
construction	O
such	O
as	O
the	O
one	O
you	O
are	O
using	O
is	O
to	O
use	O
`	O
np.where	O
(	O
mask	O
,	O
A	O
,	O
B	O
)`	O
.	O

The	O
`	O
mask	B-API
`	O
is	O
an	O
array	O
of	O
boolean	O
values	O
.	O

When	O
True	O
,	O
the	O
corresponding	O
value	O
from	O
`	O
A	O
`	O
is	O
returned	O
.	O

When	O
False	O
,	O
the	O
corresponding	O
value	O
from	O
`	O
B	O
`	O
is	O
returned	O
.	O

The	O
result	O
is	O
an	O
array	O
of	O
the	O
same	O
shape	O
as	O
`	O
mask	B-API
`	O
with	O
values	O
from	O
`	O
A	O
`	O
and	O
/	O
or	O
`	O
B	O
`	O
.	O

#CODE	O

Notice	O
that	O
this	O
performs	O
the	O
calculation	O
on	O
whole	O
columns	O
instead	O
of	O
row-by-row	O
.	O

This	O
improves	O
performance	O
greatly	O
because	O
it	O
gives	O
Pandas	O
/	O
NumPy	O
the	O
opportunity	O
to	O
pass	O
larger	O
arrays	O
of	O
values	O
to	O
fast	O
underlying	O
C	O
/	O
Fortran	O
functions	O
(	O
in	O
this	O
case	O
,	O
to	O
perform	O
the	O
arithmetic	O
)	O
.	O

When	O
you	O
work	O
row-by-row	O
,	O
you	O
are	O
performing	O
scalar	O
arithmetic	O
inside	O
a	O
Python	O
loop	O
,	O
which	O
gives	O
NumPy	O
zero	O
chance	O
to	O
shine	O
.	O

If	O
you	O
had	O
to	O
compute	O
row-by-row	O
,	O
you	O
would	O
be	O
just	O
as	O
well	O
(	O
and	O
maybe	O
better	O
)	O
off	O
using	O
plain	O
Python	O
.	O

Even	O
though	O
`	O
A	O
`	O
and	O
`	O
B	O
`	O
computes	O
the	O
values	O
for	O
the	O
entire	O
column	O
--	O
and	O
some	O
values	O
are	O
not	O
used	O
in	O
the	O
final	O
result	O
returned	O
by	O
`	O
np.where	O
`	O
--	O
this	O
is	O
still	O
faster	O
than	O
computing	O
row-by-row	O
assuming	O
there	O
are	O
more	O
than	O
a	O
trivial	O
number	O
of	O
rows	O
.	O

Tnx	O
,	O
I'll	O
check	O
this	O
out	O
and	O
revert	O
.	O

Didn't	O
know	O
about	O
mask	O
-	O
nor	O
about	O
the	O
efficiency	O
difference	O
with	O
Numpy	O
(	O
I'm	O
a	O
Newbie	O
)	O
.	O

Pandas	O
file	O
IO	O
Read	O
Error	O

New	O
to	O
pandas	O
,	O
running	O
into	O
an	O
error	O
consistently	O
with	O
WinXP	O
file	O
path	O
,	O
for	O
example	O
:	O
#CODE	O

Keep	O
getting	O
an	O
error	O
as	O
follows	O
:	O
#CODE	O

From	O
reading	O
thru	O
available	O
documentation	O
,	O
haven't	O
isolated	O
if	O
its	O
a	O
problem	O
with	O
my	O
syntax	O
or	O
a	O
parser	O
issue	O
.	O

Any	O
feedback	O
would	O
be	O
appreciated	O
.	O

You	O
need	O
to	O
include	O
the	O
entire	O
traceback	O
,	O
not	O
just	O
the	O
first	O
line	O
.	O

Also	O
use	O
raw-strings	O
or	O
forward-slashes	O
or	O
escape	O
your	O
backslashes	O
in	O
your	O
file	O
path	O
.	O

Unless	O
you	O
put	O
`	O
r	O
`	O
in	O
front	O
of	O
the	O
string	O
,	O
the	O
`	O
\n	O
`	O
is	O
being	O
interpreted	O
as	O
a	O
newline	O
:	O
#CODE	O

vs	O
#CODE	O

Python	O
Pandas	O
DataReader	O
vs	O
get_data_yahoo	O
which	O
is	O
better	O
?	O

I	O
understand	O
both	O
of	O
these	O
functions	O
do	O
the	O
exact	O
same	O
thing	O
.	O

Is	O
there	O
an	O
advantage	O
using	O
one	O
over	O
the	O
other	O
?	O

#CODE	O

Also	O
is	O
there	O
any	O
other	O
similar	O
functions	O
with	O
better	O
or	O
equal	O
performance	O
in	O
Pandas	O
?	O

Looking	O
at	O
the	O
source	O
code	O
,	O
`	O
DataReader	O
`	O
is	O
simply	O
a	O
wrapper	O
around	O
a	O
few	O
other	O
functions	O
including	O
`	O
get_data_yahoo	O
`	O
,	O
so	O
if	O
you're	O
definitely	O
going	O
to	O
use	O
Yahoo	O
as	O
your	O
data	O
source	O
,	O
I'd	O
say	O
just	O
stick	O
with	O
`	O
get_data_yahoo	O
`	O
.	O

But	O
it	O
really	O
doesn't	O
matter	O
.	O

I	O
don't	O
believe	O
there	O
are	O
other	O
functions	O
within	O
Pandas	O
that	O
do	O
this	O
task	O
.	O

Merge	B-API
a	O
csv	O
and	O
txt	O
file	O
,	O
then	O
alphabetize	O
and	O
eliminate	O
duplicates	O
using	O
python	O
and	O
pandas	O

I	O
am	O
trying	O
to	O
combine	O
two	O
csv	O
files	O
(	O
items.csv	O
and	O
prices.csv	O
)	O
to	O
create	O
combined_list.txt	O
.	O

The	O
result	O
(	O
combined_list.txt	O
)	O
should	O
be	O
a	O
list	O
sorted	O
in	O
alphabetical	O
order	O
in	O
the	O
format	O
:	O
item	O
(	O
quantity	O
):	O
$total_price_for_item	O
and	O
include	O
2	O
additional	O
lines	O
:	O
a	O
separator	O
line	O
with	O
10	O
equal	O
signs	O
and	O
a	O
line	O
with	O
the	O
total	O
amount	O
for	O
the	O
list	O
:	O
#CODE	O

items.csv	O
looks	O
like	O
#CODE	O

and	O
prices.txt	O
looks	O
like	O
#CODE	O

I	O
have	O
to	O
do	O
a	O
version	O
with	O
python	O
and	O
another	O
with	O
pandas	O
but	O
nothing	O
I	O
find	O
online	O
hits	O
the	O
mark	O
in	O
a	O
way	O
I	O
can	O
work	O
with	O
.	O

I	O
started	O
with	O
#CODE	O

But	O
I	O
am	O
having	O
trouble	O
putting	O
everything	O
together	O
.	O

Some	O
of	O
the	O
solutions	O
I	O
found	O
are	O
a	O
little	O
too	O
complex	O
for	O
me	O
or	O
don't	O
work	O
with	O
my	O
files	O
,	O
which	O
have	O
no	O
column	O
headers	O
.	O

I'm	O
still	O
trying	O
to	O
figure	O
it	O
out	O
but	O
I	O
know	O
that	O
for	O
some	O
of	O
you	O
this	O
is	O
super	O
easy	O
so	O
I	O
am	O
turning	O
to	O
the	O
collective	O
wisdom	O
instead	O
of	O
hitting	O
my	O
head	O
against	O
the	O
wall	O
alone	O
.	O

Pandas	O
has	O
a	O
built	O
in	O
method	O
for	O
reading	O
csv	O
files	O
.	O

Here	O
is	O
code	O
to	O
get	O
both	O
sets	O
of	O
data	O
into	O
one	O
dataframe	B-API
:	O
#CODE	O

To	O
sort	O
and	O
drop	O
duplicates	O
:	O
#CODE	O

Thanks	O
so	O
much	O
.	O

I	O
have	O
to	O
sort	O
out	O
the	O
column	O
headers	O
but	O
this	O
helps	O
get	O
started	O
.	O

subset	O
dataframe	B-API
pandas	O
timeseries	O

**	O
Updated	O
code	O
based	O
on	O
provided	O
answer	O
**	O

The	O
implemented	O
solution	O
isn't	O
subsetting	O
the	O
original	O
dataframe	B-API
.	O

#CODE	O

**	O
Original	O
problem	O
:	O
**	O

I'm	O
trying	O
to	O
subset	O
a	O
dataframe	B-API
based	O
on	O
a	O
comparison	O
between	O
its	O
datetime	O
index	O
,	O
and	O
the	O
datetime	O
index	O
of	O
another	O
data	O
frame	O
.	O

df1	O
is	O
a	O
dataframe	B-API
of	O
downsampled	O
timeseries	O
to	O
use	O
as	O
a	O
filter	O
.	O

df2	O
is	O
a	O
dataframe	B-API
of	O
records	O
to	O
be	O
filtered	O
,	O
which	O
has	O
higher	O
temporal	O
resolution	O
,	O
and	O
multiple	O
records	O
per	O
date	O
appearing	O
in	O
df1	O
:	O
#CODE	O

I'm	O
losing	O
the	O
contents	O
beyond	O
the	O
index	O
.	O

I'd	O
really	O
like	O
a	O
subset	O
of	O
records	O
from	O
df2	O
,	O
including	O
all	O
data	O
,	O
that	O
have	O
datetimes	O
matching	O
df1	O
at	O
the	O
day	O
frequency	O
,	O
like	O
:	O
#CODE	O

Any	O
help	O
would	O
be	O
appreciated	O
!	O

Use	O
the	O
`	O
isin	B-API
`	O
method	O
:	O
#CODE	O

You	O
can	O
check	O
that	O
the	O
result	O
only	O
contains	O
date	O
indices	O
where	O
they	O
overlap	O
on	O
the	O
day	O
frequency	O
by	O
comparing	O
#CODE	O

Thanks	O
for	O
your	O
input	O
.	O

I'm	O
attempting	O
to	O
implement	O
something	O
similar	O
to	O
this	O
,	O
but	O
am	O
getting	O
hung	O
up	O
at	O
df2	O
[	O
Index	O
(	O
df2.index.date	O
)	O
.isin	B-API
(	O
Index	O
(	O
df1.index.date	O
))]	O
,	O
which	O
throws	O
the	O
error	O
:	O
---------------------------------------------------------------------------	O

NameError	O
Traceback	O
(	O
most	O
recent	O
call	O
last	O
)	O

in	O
(	O
)	O

---->	O
1	O
df2	O
[	O
Index	O
(	O
df2.index.date	O
)	O
.isin	B-API
(	O
Index	O
(	O
df1.index.date	O
))]	O

NameError	O
:	O
name	O
'	O
Index	O
'	O
is	O
not	O
defined	O

You	O
need	O
to	O
do	O
`	O
from	O
pandas	O
import	O
Index	O
`	O
.	O

I'll	O
add	O
the	O
necessary	O
imports	O
to	O
the	O
answer	O
.	O

Thanks	O
.	O

This	O
looks	O
like	O
it	O
will	O
work	O
much	O
better	O
.	O

The	O
last	O
solution	O
I	O
came	O
up	O
with	O
prior	O
to	O
this	O
looks	O
like	O
:	O
`	O
df2_sub	O
=	O
[	O
i	O
for	O
i	O
,	O
item	O
in	O
enumerate	O
(	O
df2.index.date	O
)	O
if	O
item	O
in	O
df1.index.date	O
]`	O

`	O
new_df2=	O
pd.DataFrame	B-API
(	O
ais.ix	O
[	O
ais_sub	O
])`	O
Which	O
is	O
sloppy	O
code	O
and	O
fairly	O
inefficient	O
.	O

Each	O
of	O
my	O
files	O
to	O
be	O
filtered	O
is	O
about	O
1M	O
records	O
.	O

Last	O
comment	O
was	O
inaccurate	O
.	O
point	O
is	O
the	O
code	O
was	O
not	O
good	O
.	O

Can	O
you	O
be	O
a	O
bit	O
more	O
specific	O
?	O

I	O
had	O
made	O
cosmetic	O
changes	O
to	O
keep	O
code	O
in	O
df1	O
,	O
df2	O
context	O
.	O

I	O
missed	O
renaming	O
some	O
variables	O
:	O
new_df2	O
=p	O
d.DataFrame	O
(	O
ais.ix	O
[	O
ais_sub	O
])	O
.	O

ais_sub	O
is	O
a	O
subset	O
of	O
ais	O
(	O
df1	O
)	O
that	O
I	O
made	O
with	O
the	O
filter	O
(	O
df2	O
)	O
.	O

I	O
assigned	O
that	O
to	O
a	O
variable	O
,	O
and	O
then	O
cast	O
the	O
variable	O
as	O
a	O
dataframe	B-API
called	O
new_df2	O
.	O

It	O
works	O
,	O
but	O
it's	O
an	O
iterative	O
solution	O
,	O
and	O
takes	O
a	O
significant	O
amount	O
of	O
time	O
to	O
run	O
on	O
large	O
data	O
sets	O
.	O

Your	O
version	O
appears	O
to	O
be	O
array	O
wise	O
,	O
and	O
does	O
in	O
1	O
line	O
of	O
code	O
what	O
I	O
did	O
in	O
3	O
.	O

I	O
implemented	O
your	O
solution	O
but	O
it's	O
not	O
subsetting	O
.	O

Updated	O
code	O
in	O
the	O
original	O
problem	O
.	O

Can	O
you	O
show	O
the	O
output	O
of	O
what	O
you're	O
getting	O
from	O
what	O
you	O
implemented	O
?	O

Thx	O

That's	O
in	O
out	O
[	O
5	O
]	O
of	O
the	O
new	O
code	O
.	O

I	O
expanded	O
that	O
-	O
Output	O
is	O
now	O
in	O
out	O
[	O
7	O
]	O
.	O

Out	O
[8	O
]	O
and	O
out	O
[	O
9	O
]	O
are	O
for	O
verification	O
.	O

how	O
to	O
switch	O
columns	O
rows	O
in	O
a	O
pandas	O
dataframe	B-API

I	O
have	O
the	O
following	O
dataframe	B-API
:	O
#CODE	O

I	O
tried	O
with	O
pivot	B-API
table	O
#CODE	O

but	O
I	O
get	O
the	O
following	O
error	O
:	O
#CODE	O

any	O
alternative	O
to	O
pivot	B-API
table	O
to	O
do	O
this	O
?	O

What	O
is	O
the	O
desired	O
result	O
?	O

What	O
is	O
the	O
value	O
of	O
`	O
index	B-API
`	O
?	O

You	O
can	O
use	O
`	O
df	O
=	O
df.T	B-API
`	O
to	O
transpose	B-API
the	O
dataframe	B-API
-	O
if	O
I'm	O
understanding	O
the	O
question	O
correctly	O
.	O

This	O
switches	O
the	O
dataframe	B-API
round	O
so	O
that	O
the	O
rows	O
become	O
columns	O
.	O

Pandas	O
-	O
Get	O
Rank	O
on	O
a	O
Groupby	B-API
Object	O
without	O
running	O
out	O
of	O
memory	O

I	O
have	O
a	O
large	O
table	O
of	O
records	O
,	O
about	O
4	O
million	O
rows	O
.	O

I	O
need	O
to	O
add	O
an	O
index	O
that	O
counts	O
orders	O
by	O
email	O
address	O
based	O
on	O
the	O
orderID	O
(	O
ascending	O
)	O
.	O

#CODE	O

When	O
I	O
try	O
to	O
set	O
a	O
variable	O
called	O
rank	O
,	O
the	O
program	O
ran	O
for	O
90	O
minutes	O
and	O
took	O
about	O
5.5	O
gigs	O
of	O
RAM	O
,	O
but	O
never	O
returned	O
the	O
data	O
.	O

I	O
am	O
just	O
trying	O
to	O
add	O
a	O
column	O
so	O
that	O
for	O
each	O
email	O
(	O
my	O
customerID	O
)	O
,	O
I	O
get	O
the	O
order	O
rank	O
based	O
on	O
the	O
orderId	O
.	O

So	O
if	O
I	O
had	O
3	O
orders	O
,	O
my	O
first	O
order	O
would	O
have	O
the	O
lowest	O
orderID	O
,	O
etc	O
...	O
the	O
rank	O
restarts	O
for	O
every	O
email	O
.	O

Thanks	O
for	O
your	O
help	O
.	O

Jeff	O

sorting	O
is	O
slow	O
,	O
it's	O
O	O
(	O
n*log	O
(	O
n	O
))	O
.	O

I	O
think	O
this	O
may	O
also	O
be	O
doing	O
an	O
apply	B-API
,	O
which	O
is	O
also	O
slow	O
.	O

What's	O
the	O
reason	O
you	O
have	O
to	O
do	O
this	O
?	O

For	O
each	O
orderid	O
I	O
need	O
to	O
get	O
the	O
order	O
number	O
for	O
the	O
email	O
.	O

Many	O
emails	O
have	O
multiple	O
orders	O
.	O

Open	O
to	O
any	O
solution	O
that	O
will	O
do	O
that	O
.	O

try	O
`	O
df	O
=	O
df.sort	O
([	O
'	O
email	O
'	O
,	O
'	O
orderId	O
'])	O
;	O
df.groupby	B-API
(	O
'	O
email	O
')	O
.cumcount()	O
`	O
-	O
may	O
be	O
more	O
efficient	O
?	O

@USER	O
is	O
the	O
ranking	O
important	O
?	O

Can	O
you	O
get	O
away	O
with	O
just	O
enumerating	O
?	O

I	O
think	O
it	O
depends	O
what's	O
the	O
next	O
step	O
...	O

@USER	O
-	O
the	O
next	O
step	O
will	O
be	O
taking	O
the	O
categories	O
from	O
the	O
first	O
time	O
customers	O
,	O
then	O
calculating	O
total	O
lifetime	O
value	O
.	O

So	O
I	O
need	O
to	O
get	O
all	O
the	O
emails	O
in	O
the	O
set	O
of	O
rank=1	O
,	O
then	O
for	O
all	O
those	O
emails	O
,	O
given	O
their	O
starting	O
category	O
,	O
calculated	O
average	O
all	O
time	O
lifetime	O
value	O
.	O

@USER	O
-	O
the	O
sort	O
operation	O
was	O
super	O
fast	O
(	O
10	O
second	O
)	O
but	O
the	O
df.groupby	B-API
(	O
'	O
email	O
')	O
.cumcount()	O
wasn't	O
clear	O
on	O
what	O
it	O
was	O
supposed	O
to	O
return	O

[	O
cumcount	O
doc	O
]	O
(	O
#URL	O
)	O
.	O

When	O
you	O
say	O
"	O
rank	O
"	O
do	O
you	O
mean	O
size	O
,	O
i.e.	O
user	O
only	O
goes	O
once	O
,	O
or	O
the	O
first	O
entry	O
(	O
which	O
would	O
be	O
,	O
once	O
sorted	O
,	O
`	O
.groupby	B-API
(	O
"	O
email	O
")	O
.nth	O
(	O
0	O
)`	O

@USER	O
-	O
for	O
each	O
order	O
an	O
email	O
has	O
,	O
i	O
need	O
a	O
column	O
that	O
has	O
that	O
order	O
as	O
being	O
the	O
customer's	O
Nth	O
order	O
.	O

The	O
criteria	O
for	O
the	O
first	O
order	O
could	O
be	O
that	O
the	O
orderid	O
for	O
the	O
customer	O
is	O
the	O
lowest	O
possible	O
orderid	O
related	O
to	O
that	O
customer	O
.	O

By	O
customer	O
I	O
mean	O
'	O
email	O
'	O

Typically	O
in	O
large	O
memory	O
situations	O
you	O
would	O
chunk	O
your	O
data	O
and	O
run	O
each	O
chunk	O
serially	O
.	O

There's	O
lots	O
of	O
good	O
suggestions	O
for	O
doing	O
this	O
:	O

Large	O
data	O
work	O
flows	O
using	O
pandas	O

That's	O
not	O
going	O
to	O
work	O
with	O
rank	O
/	O
sort	O
however	O
.	O

Can	O
you	O
use	O
datetime.strptime	O
without	O
knowing	O
the	O
format	O
?	O

I	O
am	O
writing	O
a	O
function	O
that	O
takes	O
3	O
pandas	O
Series	B-API
,	O
one	O
of	O
which	O
is	O
dates	O
,	O
and	O
I	O
need	O
to	O
be	O
able	O
to	O
turn	O
it	O
into	O
a	O
dataframe	B-API
where	O
I	O
can	O
resample	B-API
by	O
them	O
.	O

The	O
issue	O
,	O
is	O
that	O
when	O
I	O
simply	O
do	O
the	O
following	O
:	O
#CODE	O

I	O
get	O
the	O
following	O
error	O
:	O
#CODE	O

I	O
know	O
this	O
is	O
because	O
even	O
though	O
the	O
index	O
type	O
is	O
a	O
datetime	O
object	O
,	O
when	O
going	O
through	O
with	O
resampling	O
,	O
unless	O
it	O
is	O
in	O
the	O
form	O
`	O
datetime	O
(	O
x	O
,	O
x	O
,	O
x	O
,	O
x	O
,	O
x	O
,	O
x	O
)`	O
,	O
It	O
wont	O
read	O
it	O
correctly	O
.	O

So	O
when	O
I	O
use	O
it	O
,	O
my	O
date	O
data	O
looks	O
like	O
this	O
:	O
`	O
2011-12-16	O
08:09	O
:	O
07	O
`	O
,	O
so	O
I	O
have	O
been	O
doing	O
the	O
following	O
:	O
#CODE	O

My	O
issue	O
is	O
that	O
I	O
am	O
using	O
this	O
for	O
open	O
source	O
and	O
I	O
cannot	O
know	O
what	O
format	O
the	O
dates	O
will	O
be	O
when	O
inputted	O
.	O

So	O
my	O
question	O
is	O
:	O
how	O
can	O
I	O
turn	O
a	O
string	O
with	O
a	O
date	O
and	O
a	O
time	O
to	O
a	O
datetime	O
object	O
WITHOUT	O
knowing	O
the	O
way	O
that	O
string	O
is	O
formatted	O
?	O

If	O
you	O
don't	O
know	O
how	O
the	O
string	O
is	O
formatted	O
,	O
how	O
do	O
you	O
know	O
if	O
`	O
02-01-2013	O
`	O
is	O
the	O
first	O
of	O
February	O
or	O
the	O
second	O
of	O
January	O
?	O

IOW	O
,	O
you	O
have	O
to	O
make	O
*	O
some	O
*	O
assumptions	O
about	O
the	O
format	O
.	O

You	O
can	O
use	O
the	O
`	O
dateutil	O
`	O
library	O
for	O
that	O
purpose	O
#CODE	O

thank	O
you	O
!!!	O

Worked	O
perfectly	O
.	O

it	O
did	O
and	O
it	O
was	O
great	O
,	O
but	O
it	O
did	O
cause	O
an	O
extra	O
import	O
so	O
I	O
consider	O
the	O
answer	O
I	O
accepted	O
now	O
better	O
for	O
that	O
reason	O

Pandas	O
has	O
a	O
`	O
to_datetime	B-API
`	O
function	O
for	O
this	O
purpose	O
,	O
and	O
when	O
applied	O
to	O
a	O
Series	B-API
it'll	O
convert	O
values	O
to	O
Timestamp	O
rather	O
than	O
datetime	O
:	O
#CODE	O

Where	O
:	O
#CODE	O

great	O
!	O

Quick	O
question	O
...	O

if	O
data.time	O
is	O
already	O
in	O
datetime	O
,	O
will	O
this	O
produce	O
an	O
error	O
or	O
still	O
work	O
?	O

@USER	O
It'll	O
still	O
work	O
just	O
fine	O
:)	O

pandas	O
-	O
Joining	O
CSV	O
time	O
series	B-API
into	O
a	O
single	O
dataframe	B-API

I'm	O
trying	O
to	O
get	O
4	O
CSV	O
files	O
into	O
one	O
dataframe	B-API
.	O

I've	O
looked	O
around	O
on	O
the	O
web	O
for	O
examples	O
and	O
tried	O
a	O
few	O
but	O
they	O
all	O
give	O
errors	O
.	O

Finally	O
I	O
think	O
I'm	O
onto	O
something	O
,	O
but	O
it	O
gives	O
unexpected	O
results	O
.	O

Can	O
anybody	O
tell	O
me	O
why	O
this	O
doesn't	O
work	O
?	O

#CODE	O

Expected	O
result	O
:	O
#CODE	O

Getting	O
this	O
:	O
#CODE	O

Did	O
you	O
mean	O
to	O
save	O
the	O
result	O
of	O
`	O
df.join	B-API
(	O
data	O
[	O
'	O
TempC	O
'])`	O
into	O
`	O
df	O
`	O
?	O

@USER	O
yes	O

You	O
need	O
to	O
save	O
the	O
result	O
of	O
your	O
join	O
:	O
#CODE	O

KDB+	O
like	O
asof	B-API
join	O
for	O
timeseries	O
data	O
in	O
pandas	O
?	O

kdb+	O
has	O
an	O
aj	O
function	O
that	O
is	O
usually	O
used	O
to	O
join	O
tables	O
along	O
time	O
columns	O
.	O

Here	O
is	O
an	O
example	O
where	O
I	O
have	O
trade	O
and	O
quote	O
tables	O
and	O
I	O
get	O
the	O
prevailing	O
quote	O
for	O
every	O
trade	O
.	O

#CODE	O

How	O
can	O
I	O
do	O
the	O
same	O
operation	O
using	O
pandas	O
?	O

I	O
am	O
working	O
with	O
trade	O
and	O
quote	O
dataframes	O
where	O
the	O
index	O
is	O
datetime64	O
.	O

#CODE	O

I	O
see	O
that	O
pandas	O
has	O
an	O
asof	B-API
function	O
but	O
that	O
is	O
not	O
defined	O
on	O
the	O
DataFrame	B-API
,	O
only	O
on	O
the	O
Series	B-API
object	O
.	O

I	O
guess	O
one	O
could	O
loop	O
through	O
each	O
of	O
the	O
Series	B-API
and	O
align	B-API
them	O
one	O
by	O
one	O
,	O
but	O
I	O
am	O
wondering	O
if	O
there	O
is	O
a	O
better	O
way	O
?	O

this	O
is	O
also	O
called	O
*	O
rolling	O
join	O
*	O

As	O
you	O
mentioned	O
in	O
the	O
question	O
,	O
looping	O
through	O
each	O
column	O
should	O
work	O
for	O
you	O
:	O
#CODE	O

We	O
could	O
potentially	O
create	O
a	O
faster	O
NaN-naive	O
version	O
of	O
DataFrame.asof	O
to	O
do	O
all	O
the	O
columns	O
in	O
one	O
shot	O
.	O

But	O
for	O
now	O
,	O
I	O
think	O
this	O
is	O
the	O
most	O
straightforward	O
way	O
.	O

Thanks	O
.	O

I	O
am	O
taking	O
this	O
approach	O
for	O
now	O
.	O

But	O
a	O
NaN-naive	O
version	O
would	O
be	O
very	O
welcome	O
!	O

#URL	O

I	O
wrote	O
an	O
under-advertised	O
`	O
ordered_merge	O
`	O
function	O
some	O
time	O
ago	O
:	O
#CODE	O

It	O
could	O
be	O
easily	O
(	O
well	O
,	O
for	O
someone	O
who	O
is	O
familiar	O
with	O
the	O
code	O
)	O
extended	O
to	O
be	O
a	O
"	O
left	O
join	O
"	O
mimicking	O
KDB	O
.	O

I	O
realize	O
in	O
this	O
case	O
that	O
forward-filling	O
the	O
trade	O
data	O
is	O
not	O
appropriate	O
;	O
just	O
illustrating	O
the	O
function	O
.	O

Thanks	O
,	O
this	O
is	O
very	O
good	O
to	O
know	O
.	O

This	O
is	O
essentially	O
uj	O
(	O
#URL	O
)	O
in	O
KDB	O
!.	O

For	O
the	O
aj	O
functionality	O
,	O
I	O
am	O
going	O
with	O
Chang's	O
approach	O
but	O
I	O
plan	O
to	O
take	O
a	O
serious	O
stab	O
at	O
the	O
code	O
later	O
.	O

Could	O
this	O
be	O
generalized	O
the	O
case	O
where	O
the	O
dataframe	B-API
contains	O
many	O
Series	B-API
together	O
,	O
for	O
example	O
if	O
the	O
data	O
,	O
in	O
addition	O
of	O
timestamps	O
,	O
also	O
had	O
a	O
stock	O
ID	O
column	O
?	O

(	O
Thus	O
we	O
may	O
have	O
thousands	O
of	O
groups	O
,	O
and	O
each	O
of	O
them	O
is	O
a	O
Series	B-API
)	O
.	O

I	O
suspect	O
we'll	O
need	O
a	O
mix	O
of	O
`	O
groupby()	B-API
`	O
and	O
`	O
ordered_merge	O
`	O
,	O
but	O
I'm	O
wrestling	O
about	O
how	O
to	O
do	O
it	O
...	O

For	O
sure	O
,	O
it	O
would	O
be	O
wrong	O
to	O
simply	O
`	O
ffill	B-API
`	O
on	O
the	O
overall	O
order	O
of	O
the	O
dataframe	B-API
(	O
I	O
don't	O
want	O
a	O
group	O
to	O
spill	O
into	O
the	O
next	O
group	O
by	O
virtue	O
of	O
forward	O
filling	O
)	O
.	O

bar	O
plot	O
with	O
different	O
colors	O
in	O
python	O

I	O
have	O
data	O
like	O
this	O
:	O
#CODE	O

I	O
want	O
to	O
plot	O
the	O
dataframe	B-API
in	O
horizontal	O
bars	O
with	O
different	O
colors	O
reffering	O
to	O
the	O
column	O
`'	O
typ	O
'`	O

Perhaps	O
the	O
accepted	O
answer	O
to	O
another	O
[	O
matplotlib	O
question	O
]	O
(	O
#URL	O
)	O
may	O
help	O
you	O
:	O
it	O
cycles	O
through	O
the	O
individual	O
bars	O
(	O
patches	O
)	O
and	O
assigns	O
a	O
color	O
to	O
each	O
patch	O
individually	O
.	O

You'll	O
need	O
to	O
adopt	O
it	O
for	O
a	O
barchart	O
plot	O
.	O

You	O
can	O
use	O
the	O
`	O
color	O
`	O
parameter	O
of	O
matplotlib's	O
`	O
barh	O
`	O
function	O
:	O
#CODE	O

Sub	O
Value	O
and	O
Add	O
new	O
column	O
pandas	O

I	O
am	O
trying	O
to	O
read	O
few	O
`	O
files	O
`	O
from	O
a	O
`	O
path	O
`	O
as	O
extension	O
to	O
my	O
previous	O
question	O
The	O
answer	O
given	O
by	O
Jianxun	O
Definitely	O
makes	O
sense	O
but	O
I	O
am	O
getting	O
a	O
key	O
error	O
.	O
very	O
very	O
new	O
to	O
pandas	O
and	O
not	O
able	O
to	O
fix	O
error	O
.	O

Note	O
:	O
I	O
use	O
Python	O
2.7	O
and	O
Pandas	O
0.16	O
#CODE	O

Programs	O
:	O
#CODE	O

Error	O
:	O
#CODE	O

#CODE	O

Still	O
the	O
same	O
error	O
:	O
`	O
File	O
"	O
distribute_lac.py	O
"	O
,	O
line	O
30	O
,	O
in	O

df_master	O
=	O
pd.read_csv	B-API
(	O
master_csv_file	O
,	O
index_col	O
=[	O
'	O
Ids	O
'])	O
.sort_index()	B-API

File	O
"	O
/	O
usr	O
/	O
lib	O
/	O
pymodules	O
/	O
python2.7	O
/	O
pandas	O
/	O
io	O
/	O
parsers.py	O
"	O
,	O
line	O
256	O
,	O
in	O
_read	O

return	O
parser.read()	O

File	O
"	O
/	O
usr	O
/	O
lib	O
/	O
pymodules	O
/	O
python2.7	O
/	O
pandas	O
/	O
io	O
/	O
parsers.py	O
"	O
,	O
line	O
715	O
,	O
in	O
read	O

ret	O
=	O
self._engine.read	O
(	O
nrows	O
)	O

File	O
"	O
/	O
usr	O
/	O
lib	O
/	O
pymodules	O
/	O
python2.7	O
/	O
pandas	O
/	O
io	O
/	O
parsers.py	O
"	O
,	O
line	O
1184	O
,	O
in	O
read	O

values	O
=	O
data.pop	O
(	O
self.index_col	O
[	O
i	O
])	O

KeyError	O
:	O
'	O
Ids	O
'`	O

@USER	O
Can	O
you	O
post	O
the	O
first	O
few	O
rows	O
of	O
the	O
original	O
`	O
Master1_Test.csv	O
`	O
file	O
?	O

it	O
seems	O
that	O
the	O
file	O
has	O
no	O
'	O
Ids	O
'	O
column	O
.	O

I	O
using	O
the	O
same	O
sample	O
file	O
and	O
I	O
am	O
getting	O
error	O
for	O
this	O
only	O
haven't	O
tried	O
with	O
real	O
data	O
yet	O
.	O

@USER	O
Why	O
in	O
master	O
csv	O
,	O
there	O
are	O
three	O
column	O
headers	O
`	O
Ids	O
,	O
00:00	O
:	O
00	O
,	O
00:30	O
:	O
00	O
`	O
,	O
but	O
4	O
columns	O
of	O
values	O
`	O
1234,100	O
0,500,100	O
`	O
?	O

Now	O
I	O
did	O
edit	O
the	O
Master	O
input	O
file	O
and	O
I	O
do	O
not	O
get	O
error	O
but	O
it	O
does	O
not	O
even	O
show	O
results	O
.	O

@USER	O
Could	O
you	O
please	O
double	O
check	O
whether	O
each	O
file	O
has	O
been	O
read	O
successfully	O
?	O
for	O
example	O
,	O
print	O
out	O
the	O
first	O
few	O
lines	O
of	O
each	O
`	O
df	O
`	O
see	O
whether	O
the	O
`	O
df	O
`	O
looks	O
normal	O
.	O

If	O
not	O
,	O
please	O
post	O
what	O
you	O
saw	O
.	O

Pls	O
find	O
the	O
Download	O
like	O
of	O
Dropbox	O
for	O
the	O
files	O
and	O
program	O
.	O

#URL	O

The	O
data	O
sets	O
are	O
for	O
two	O
programs	O
the	O
current	O
one	O
and	O
the	O
one	O
from	O
this	O
link	O
.	O

#URL	O

@USER	O
I've	O
replaced	O
the	O
code	O
with	O
the	O
real	O
file	O
path	O
and	O
it	O
works	O
on	O
my	O
PC	O
.	O

Give	O
a	O
check	O
and	O
see	O
whether	O
it	O
raises	O
any	O
error	O
on	O
your	O
side	O
.	O

Finally	O
!!	O

Awesome	O
..	O

Thank	O
you	O
so	O
much	O
...	O

so	O
now	O
if	O
i	O
write	O
a	O
schedule	O
program	O
to	O
auto	O
run	O
the	O
program	O
those	O
with	O
NAN	O
will	O
be	O
refilled	O
?	O
and	O
if	O
possible	O
do	O
help	O
with	O
second	O
program	O
also	O
.	O

@USER	O
I	O
am	O
certainly	O
happy	O
to	O
help	O
out	O
.	O

For	O
the	O
2nd	O
program	O
,	O
base	O
on	O
the	O
files	O
you	O
sent	O
,	O
I	O
didn't	O
see	O
the	O
`	O
Ids	O
`	O
columns	O
at	O
all	O
in	O
any	O
of	O
the	O
csv	O
files	O
.	O

Could	O
you	O
please	O
give	O
a	O
look	O
at	O
this	O
issue	O
and	O
re-upload	O
the	O
files	O
?	O

@USER	O
also	O
,	O
the	O
name	O
`	O
New	O
York	O
`	O
in	O
`	O
lat_lon_2.csv	O
`	O
seems	O
to	O
have	O
some	O
white	O
space	O
between	O
these	O
two	O
names	O
.	O

Is	O
this	O
natural	O
to	O
your	O
real	O
data	O
file	O
?	O

For	O
second	O
program	O
only	O
the	O
two	O
data	O
files	O
of	O
data_repository	O
are	O
to	O
be	O
considered	O
..	O

Not	O
from	O
Transition_Data	O
.	O

After	O
dataframe	B-API
append	B-API
I	O
have	O
the	O
same	O
amount	O
of	O
the	O
rows	O

I'm	O
trying	O
to	O
flatten	O
the	O
3rd	O
column	O
which	O
contains	O
an	O
array	O
:	O
#CODE	O

However	O
,	O
after	O
the	O
"	O
for	O
"	O
`	O
data2.id.count()	O
==	O
data.id.count()	O
`	O
and	O
`	O
data	O
[	O
'	O
Column	O
3	O
']	O
.count()	B-API
==	O
data2.my_array_items.count()	O
`	O
and	O
#CODE	O

which	O
is	O
the	O
same	O
as	O
#CODE	O

Why	O
?	O

The	O
pandas	O
`	O
DataFrame.append	B-API
`	O
method	O
returns	O
a	O
new	O
DataFrame	B-API
.	O

It	O
does	O
not	O

modify	O
the	O
original	O
.	O

Therefore	O
,	O
you	O
would	O
need	O
#CODE	O

However	O
,	O
this	O
would	O
be	O
terrible	O
slow	O
,	O
since	O
each	O
time	O
`	O
append	B-API
`	O
is	O
called	O
a	O
new	O
DataFrame	B-API
must	O
be	O
created	O
and	O
all	O
the	O
data	O
from	O
`	O
data2	O
`	O
and	O
`	O
x	O
`	O
would	O
need	O
to	O
be	O
copied	O
into	O
the	O
new	O
DataFrame	B-API
.	O

That's	O
on	O
the	O
order	O
of	O
`	O
n**2	O
`	O
copies	O
being	O
made	O
where	O
`	O
n	O
`	O
is	O
the	O
number	O
of	O
calls	O
to	O
`	O
append	B-API
`	O
.	O

A	O
faster	O
way	O
to	O
achieve	O
the	O
same	O
result	O
is	O
#CODE	O

For	O
example	O
,	O
#CODE	O

`	O
data2	O
=	O
pd.concat	B-API
([	O
data2	O
,	O
pd.DataFrame	B-API
(	O
series	B-API
)])	O

`	O
-	O
in	O
the	O
loop	O
?	O

Number	O
one	O
rule	O
of	O
programming	O
:	O
Try	O
first	O
,	O
ask	O
later	O
.	O

Therefore	O
,	O
try	O
it	O
in	O
a	O
loop	O
and	O
see	O
what	O
happens	O
.	O

If	O
it	O
works	O
,	O
you	O
have	O
your	O
answer	O
.	O

If	O
it	O
doesn't	O
,	O
again	O
you	O
have	O
your	O
answer	O
.	O

The	O
rule	O
number	O
two	O
:	O
strive	O
to	O
avoid	O
doing	O
the	O
things	O
you	O
don't	O
really	O
have	O
to	O
do	O
.	O

Your	O
code	O
from	O
the	O
1st	O
solution	O
doesn't	O
work	O
,	O
the	O
2nd	O
does	O
.	O

kde	O
density	O
plot	O
using	O
python	O
pandas	O

I	O
was	O
following	O
the	O
"	O
python	O
for	O
data	O
analysis	O
"	O
in	O
ipython	O
,	O
trying	O
to	O
do	O
a	O
kde	O
plot	O
using	O
pandas	O
(	O
chapter	O
8	O
,	O
Histograms	O
and	O
Density	O
Plots	O
)	O
.the	O
codes	O
are	O
simple	O
:	O
#CODE	O

the	O
error	O
is	O
#CODE	O

I	O
googled	O
around	O
and	O
I	O
think	O
it	O
is	O
a	O
local	O
global	O
variable	O
problem	O
and	O
may	O
be	O
due	O
to	O
a	O
wrong	O
way	O
to	O
import	O
the	O
modules	O
.	O

Anybody	O
can	O
suggest	O
any	O
ideas	O
?	O

Your	O
code	O
works	O
fine	O
for	O
me	O
.	O

Are	O
you	O
sure	O
you	O
are	O
running	O
%pylab	O
?	O

Which	O
version	O
of	O
`	O
pandas	O
`	O
are	O
you	O
running	O
?	O

`	O
print	O
(	O
pd.__version__	O
)`	O
.	O

I	O
suspect	O
it's	O
pre-0.8	O
,	O
in	O
which	O
case	O
you	O
should	O
take	O
this	O
opportunity	O
to	O
upgrade	O
to	O
`	O
0.10.1	O
`	O
.	O

The	O
problem	O
resolved	O
after	O
pandas	O
be	O
updated	O
.	O

Thank	O
you	O
very	O
much	O
!	O

Converting	O
string	O
objects	O
to	O
int	O
/	O
float	O
using	O
pandas	O

#CODE	O

The	O
csv	O
file	O
"	O
100	O
life_180_data.csv	O
"	O
contains	O
columns	O
like	O
age	O
,	O
bmi	O
,	O
Cigarettes	O
,	O
Alocohol	O
etc	O
.	O

#CODE	O

Cigarettes	O
column	O
contains	O
"	O
Never	O
"	O
"	O
1-5	O
Cigarettes	O
/	O
day	O
"	O
,	O
"	O
10-20	O
Cigarettes	O
/	O
day	O
"	O
.	O

I	O
want	O
to	O
assign	B-API
weights	O
to	O
these	O
object	O
(	O
Never	O
,	O
1-5	O
Cigarettes	O
/	O
day	O
,....	O
)	O

The	O
expected	O
output	O
is	O
new	O
column	O
CigarNum	O
appended	O
which	O
consists	O
only	O
numbers	O
0	O
,	O
1	O
,	O
2	O

CigarNum	O
is	O
as	O
expected	O
till	O
8	O
rows	O
and	O
then	O
shows	O
Nan	O
till	O
last	O
row	O
in	O
CigarNum	O
column	O
#CODE	O

The	O
output	O
I	O
get	O
shoudln't	O
give	O
NaN	O
after	O
few	O
first	O
rows	O
.	O

#CODE	O

Are	O
you	O
sure	O
row	O
10	O
and	O
11	O
actually	O
equals	O
'	O
Never	O
'	O
and	O
that	O
there	O
isn't	O
a	O
space	O
or	O
other	O
character	O
in	O
the	O
value	O
?	O

Yes	O
,	O
I	O
didn't	O
check	O
the	O
space	O
till	O
now.Really	O
thanks.Could	O
you	O
help	O
me	O
with	O
an	O
efficient	O
way	O
to	O
ignore	O
these	O
spaces.I	O
have	O
lot	O
more	O
columns	O
with	O
space	O
in	O
the	O
beginning.Thanks	O
in	O
advance	O
.	O

OK	O
,	O
first	O
problem	O
is	O
you	O
have	O
embedded	O
spaces	O
causing	O
the	O
function	O
to	O
incorrectly	O
apply	B-API
:	O

fix	O
this	O
using	O
vectorised	O
`	O
str	B-API
`	O
:	O
#CODE	O

now	O
create	O
your	O
new	O
column	O
should	O
just	O
work	O
:	O
#CODE	O

UPDATE	O

Thanks	O
to	O
@USER	O
as	O
always	O
for	O
pointing	O
out	O
superior	O
ways	O
to	O
do	O
things	O
:	O

So	O
you	O
can	O
call	O
`	O
replace	B-API
`	O
instead	O
of	O
calling	O
`	O
apply	B-API
`	O
:	O
#CODE	O

you	O
can	O
also	O
use	O
`	O
factorize	B-API
`	O
method	O
also	O
.	O

Thinking	O
about	O
it	O
why	O
not	O
just	O
set	O
the	O
dict	O
values	O
to	O
be	O
floats	O
anyway	O
and	O
then	O
you	O
avoid	O
the	O
type	O
conversion	O
?	O

So	O
:	O
#CODE	O

you	O
can	O
use	O
``	O
series.replace	B-API
(	O
dict	O
)``	O
I	O
believe	O
to	O
do	O
the	O
substitution	O
,	O
then	O
``	O
convert_objects	B-API
(	O
convert_numeric=True	O
)``	O
to	O
change	O
to	O
float	O
(	O
forcibly	O
);	O
you	O
can	O
also	O
``	O
factorize	B-API
``	O
to	O
make	O
categoricals	O
(	O
e.g.	O
map	B-API
the	O
strings	O
to	O
numbers	O
)	O

@USER	O
so	O
is	O
`	O
replace	B-API
`	O
faster	O
than	O
calling	O
`	O
map	B-API
`	O
or	O
`	O
apply	B-API
`	O
and	O
passing	O
a	O
dict	O
now	O
?	O

Wasn't	O
aware	O
of	O
`	O
factorize	B-API
`	O
also	O
,	O
when	O
was	O
this	O
introduced	O
?	O

replace	O
should	O
be	O
much	O
faster	O
;	O
``	O
factorize	B-API
``	O
has	O
been	O
their	O
quite	O
a	O
while	O
(	O
but	O
not	O
advertised	O
:))	O

Try	O
using	O
this	O
function	O
for	O
all	O
problems	O
of	O
this	O
kind	O
:	O
#CODE	O

Indexing	O
Row	O
Values	O
Based	O
on	O
Value	O
of	O
Each	O
Column	O

I	O
have	O
an	O
8r	O
x	O
10c	O
data	O
frame	O
and	O
I	O
want	O
to	O
duplicate	O
the	O
dataframe	B-API
by	O
dividing	O
the	O
values	O
in	O
each	O
row	O
by	O
the	O
first	O
value	O
in	O
its	O
column	O
(	O
i.e.	O
'	O
indexing	O
'	O
each	O
column	O
,	O
with	O
the	O
first	O
value	O
=	O
100	O
)	O
.	O

So	O
if	O
I	O
start	O
with	O
...	O

#CODE	O

It	O
would	O
return	O
...	O

#CODE	O

Is	O
there	O
a	O
simple	O
command	O
to	O
do	O
this	O
,	O
or	O
is	O
it	O
some	O
sort	O
of	O
loop	O
?	O

When	O
you	O
say	O
"	O
data	O
frame	O
"	O
,	O
do	O
you	O
specifically	O
mean	O
[	O
`	O
pandas.DataFrame	B-API
`]	O
(	O
#URL	O
)	O
?	O

If	O
you're	O
using	O
some	O
other	O
data	O
structure	O
,	O
please	O
tell	O
us	O
what	O
it	O
is	O
(	O
a	O
list	O
of	O
lists	O
,	O
a	O
NumPy	O
array	O
,	O
etc	O
)	O
.	O

Sorry	O
,	O
yes	O
--	O
a	O
Pandas	O
dataframe	B-API
.	O

The	O
first	O
column	O
in	O
your	O
example	O
has	O
been	O
divided	O
by	O
10	O
...	O
should	O
it	O
not	O
have	O
been	O
divided	O
by	O
1000	O
(	O
as	O
that's	O
the	O
first	O
value	O
in	O
the	O
column	O
)	O
?	O

I	O
want	O
the	O
first	O
value	O
in	O
each	O
column	O
to	O
be	O
100	O
and	O
everything	O
else	O
to	O
be	O
indexed	O
to	O
that	O
.	O

So	O
the	O
first	O
column	O
values	O
are	O
x	O
/	O
10	O
and	O
second	O
column	O
values	O
are	O
x	O
/	O
20	O
and	O
third	O
column	O
values	O
are	O
x	O
/	O
30	O
(	O
etc	O
)	O
.	O

You	O
could	O
do	O
the	O
following	O
:	O
#CODE	O

`	O
df.iloc	B-API
[	O
0	O
]`	O
selects	O
the	O
first	O
row	O
.	O

Divide	O
it	O
by	O
100	O
to	O
get	O
a	O
row	O
of	O
values	O
to	O
adjust	O
each	O
column	O
by	O
.	O

Lastly	O
we	O
divide	O
the	O
entire	O
DataFrame	B-API
by	O
this	O
new	O
row	O
of	O
values	O
.	O

Division	O
happens	O
along	O
axis	O
0	O
by	O
default	O
(	O
i.e.	O
downwards	O
along	O
each	O
column	O
)	O
.	O

An	O
equivalent	O
operation	O
would	O
be	O
`	O
df	O
/	O
df.iloc	B-API
[	O
0	O
]	O
*	O
100	O
`	O
.	O

Fantastic	O
--	O
thank	O
you	O
.	O

Knew	O
there	O
had	O
to	O
be	O
a	O
simple	O
way	O
to	O
do	O
it	O
.	O

iloc	B-API
works	O
to	O
select	O
the	O
first	O
row	O
,	O
but	O
the	O
division	O
fails	O
.	O

I'm	O
going	O
to	O
assume	O
this	O
is	O
my	O
lack	O
of	O
Python	O
knowledge	O
and	O
that	O
I'm	O
missing	O
something	O
obvious	O
,	O
but	O
if	O
anyone	O
knows	O
of	O
a	O
reason	O
it	O
would	O
not	O
work	O
as	O
presented	O
please	O
let	O
me	O
know	O
.	O

@USER	O
:	O
do	O
you	O
get	O
a	O
particular	O
error	O
message	O
when	O
you	O
try	O
the	O
division	O
?	O

Yields	O
a	O
long	O
strong	O
of	O
errors	O
...	O

OK	O
-	O
there	O
should	O
be	O
an	O
short	O
error	O
message	O
right	O
at	O
the	O
bottom	O
of	O
the	O
stack	O
trace	O
,	O
e.g.	O
beginning	O
with	O
`	O
ValueError	O
`	O
or	O
`	O
TypeError	O
`	O
or	O
something	O
similar	O
.	O

What	O
is	O
it	O
and	O
what	O
does	O
say	O
?	O

Turned	O
out	O
the	O
problem	O
was	O
a	O
hidden	O
index	O
column	O
.	O

Worked	O
around	O
the	O
problem	O
another	O
way	O
,	O
but	O
seems	O
like	O
.iloc	B-API
would	O
work	O
.	O

pandas	O
series	B-API
:	O
change	O
order	O
of	O
index	O

I	O
have	O
a	O
Pandas	O
series	B-API
,	O
for	O
example	O
like	O
this	O
#CODE	O

How	O
can	O
I	O
change	O
the	O
order	O
of	O
the	O
index	O
,	O
so	O
that	O
`	O
s	O
`	O
becomes	O
#CODE	O

I	O
have	O
tried	O
#CODE	O

but	O
that	O
will	O
give	O
me	O
a	O
key	O
error	O
.	O

(	O
In	O
this	O
particular	O
example	O
I	O
could	O
presumably	O
take	O
care	O
of	O
the	O
order	O
of	O
the	O
index	O
while	O
constructing	O
the	O
series	B-API
,	O
but	O
I	O
would	O
like	O
to	O
have	O
a	O
way	O
to	O
do	O
this	O
after	O
the	O
series	B-API
has	O
been	O
created	O
.	O
)	O

You	O
don't	O
have	O
enough	O
square	O
brackets	O
.	O

You	O
meant	O
to	O
do	O
this	O
:	O
`	O
s	O
[[	O
'	O
B	O
'	O
,	O
'	O
A	O
'	O
,	O
'	O
C	O
']]`	O
--	O
aka	O
advanced	O
or	O
fancy	O
indexing	O
.	O

The	O
error	O
message	O
is	O
b	O
/	O
c	O
pandas	O
thinks	O
you	O
are	O
asking	O
for	O
columns	O
'	O
B	O
'	O
,	O
'	O
A	O
'	O
,	O
and	O
'	O
C	O
'	O
.	O

Use	O
`	O
reindex	B-API
`	O
:	O
#CODE	O

Difference	O
Pivot_table	B-API
Pandas	O
and	O
excel	O

When	O
I	O
create	O
a	O
pivot	B-API
table	O
from	O
data	O
in	O
Pandas	O
(	O
python	O
)	O
,	O
I	O
get	O
an	O
other	O
result	O
than	O
when	O
I	O
create	O
it	O
with	O
Excel	O
.	O

I	O
think	O
this	O
is	O
due	O
to	O
the	O
fact	O
of	O
characters	O
.	O

Someone	O
knows	O
the	O
difference	O
between	O
the	O
pivot	B-API
table	O
in	O
Pandas	O
and	O
Excel	O
?	O

I've	O
made	O
this	O
example	O
.	O

I	O
have	O
the	O
excel	O
file	O
'	O
funds_steven	O
'	O
with	O
following	O
data	O
in	O
1	O
column	O
.	O

(	O
column	O
name	O
=	O
Steven_Funds	O
)	O
#CODE	O

How	O
can	O
I	O
read	O
this	O
in	O
and	O
calculate	O
the	O
sum	O
of	O
the	O
values	O
?	O

Always	O
helps	O
if	O
you	O
could	O
show	O
expected	O
result	O
and	O
pandas	O
result	O
to	O
debug	O
.	O

Seems	O
may	O
be	O
a	O
formatting	O
issue	O
.	O

Taking	O
the	O
sum	O
of	O
all	O
these	O
values	O
is	O
not	O
really	O
a	O
pivot	B-API
table	O
.	O

For	O
the	O
sum	O
you	O
can	O
just	O
use	O
the	O
`	O
sum()	O
`	O
method	O

Conditionally	O
concatenate	O
aggregated	O
columns	O
from	O
different	O
DataFrames	O
into	O
a	O
new	O
DataFrame	B-API

I	O
have	O
several	O
DataFrames	O
with	O
the	O
following	O
structure	O
:	O
#CODE	O

I	O
want	O
to	O
construct	O
the	O
following	O
DataFrame	B-API
.	O

`	O
max	B-API
`	O
is	O
the	O
maximum	O
value	O
in	O
the	O
column	O
'	O
0	O
'	O
subarray	O
;	O

`	O
nth	O
`	O
is	O
the	O
0-th	O
element	O
in	O
the	O
column	O
'	O
2	O
'	O
subarray	O
if	O
first-level	O
index	O
value	O
contains	O
'	O
1	O
'	O
and	O
0-th	O
element	O
in	O
the	O
column	O
'	O
3	O
'	O
subarray	O
otherwise	O
)	O
.	O

#CODE	O

I	O
tried	O
`	O
df	O
[	O
0	O
]	O
.groupby	B-API
(	O
level	O
=[	O
0	O
,	O
1	O
])	O
.max()	B-API
`	O
to	O
calculate	O
`	O
max	B-API
`	O
and	O
`	O
df	O
[	O
2	O
or	O
3	O
]	O
.groupby	B-API
(	O
level	O
=[	O
0	O
,	O
1	O
])	O
.nth	O
(	O
0	O
)`	O
to	O
calculate	O
`	O
nth	O
`	O
but	O
stuck	O
with	O
concatenation	O
using	O
index	O
values	O
as	O
a	O
condition	O
to	O
select	O
column	O
2	O
or	O
3	O
.	O

Here's	O
my	O
starting	O
point	O
(	O
same	O
code	O
as	O
yours	O
,	O
different	O
random	O
values	O
):	O
#CODE	O

#CODE	O

I	O
couldn't	O
find	O
a	O
way	O
to	O
directly	O
check	O
for	O
a	O
'	O
1	O
'	O
in	O
the	O
first	O
level	O
so	O
I	O
just	O
converted	O
it	O
to	O
a	O
colunn	O
with	O
`	O
reset_index	B-API
`	O
and	O
then	O
it's	O
fairly	O
easy	O
to	O
use	O
a	O
string	O
method	O
on	O
it	O
.	O

#CODE	O

Now	O
clean	O
things	O
up	O
(	O
some	O
of	O
which	O
could	O
be	O
done	O
earlier	O
but	O
I	O
thought	O
it	O
more	O
clear	O
to	O
wait	O
until	O
the	O
end	O
and	O
combine	O
it	O
all	O
):	O
#CODE	O

I'm	O
not	O
sure	O
if	O
you're	O
asking	O
about	O
concat	B-API
also	O
,	O
but	O
it's	O
pretty	O
straightforward	O
:	O
#CODE	O

I	O
found	O
that	O
we	O
can	O
say	O
`	O
df	O
[	O
'	O
one	O
']	O
=	O
df.index.get_level_values	O
(	O
0	O
)	O
.to_series()	B-API
.str	B-API
.contains	B-API
(	O
'	O
1	O
')	O
.values	B-API
`	O

Yeah	O
,	O
that's	O
a	O
good	O
way	O
.	O

I	O
managed	O
to	O
implement	O
the	O
solution	O
I	O
wanted	O
:	O
#CODE	O

Pandas	O
DataFrame	B-API
:	O
transforming	O
frame	O
using	O
unique	O
values	O
of	O
a	O
column	O

I	O
have	O
a	O
pandas	O
dataframe	B-API
/	O
csv	O
of	O
the	O
form	O
#CODE	O

I	O
want	O
to	O
convert	O
this	O
to	O
a	O
form	O
#CODE	O

In	O
general	O
I	O
am	O
looking	O
for	O
a	O
way	O
to	O
transform	O
a	O
table	O
using	O
unique	O
values	O
of	O
a	O
single	O
column	O
.	O

I	O
have	O
looked	O
at	O
`	O
pivot	B-API
`	O
and	O
`	O
groupby	B-API
`	O
but	O
didn't	O
get	O
the	O
exact	O
form	O
.	O

HINT	O
:	O
possibly	O
this	O
is	O
solvable	O
by	O
`	O
pivot	B-API
`	O
but	O
I	O
haven't	O
been	O
able	O
to	O
get	O
the	O
form	O

Probably	O
not	O
the	O
most	O
elegant	O
way	O
possible	O
,	O
but	O
using	O
unstack	B-API
:	O
#CODE	O

A	O
little	O
more	O
generally	O
,	O
and	O
removing	O
the	O
strange	O
hierarchical	O
columns	O
in	O
the	O
result	O
:	O
#CODE	O

wow	O
..	O
thanks	O
for	O
the	O
quick	O
response	O
..	O

I	O
am	O
guessing	O
a	O
mental	O
block	O
but	O
is	O
there	O
a	O
way	O
I	O
can	O
remove	O
the	O
index	O
name	O

If	O
you	O
mean	O
`	O
Type	O
`	O
there	O
,	O
it's	O
not	O
actually	O
`	O
df.index.name	O
`	O
but	O
instead	O
`	O
df.columns	O
`	O
is	O
hierarchical	O
and	O
has	O
the	O
name	O
`	O
Type	O
`	O
.	O

I	O
edited	O
in	O
how	O
to	O
get	O
rid	O
of	O
that	O
.	O

Thanks	O
figured	O
it	O
out	O
...	O
forgot	O
to	O
edit	O
..	O
but	O
it	O
seems	O
efficient	O
enough	O
..	O
accepting	O
the	O
answer	O
:)	O

I	O
cooked	O
up	O
my	O
own	O
pivot	B-API
based	O
solution	O
to	O
the	O
same	O
problem	O
before	O
finding	O
Dougal's	O
answer	O
,	O
thought	O
I	O
would	O
post	O
it	O
for	O
posterity	O
since	O
I	O
find	O
it	O
more	O
readable	O
:	O
#CODE	O

And	O
then	O
carry	O
on	O
with	O
Dougal's	O
cleanups	O
:	O
#CODE	O

Note	O
that	O
`	O
DataFrame.to_csv()	B-API
`	O
produces	O
your	O
requested	O
output	O
:	O
#CODE	O

Thanks	O
for	O
the	O
alternate	O
solution	O
!	O

convert	O
ceilometer	O
output	O
to	O
python	O
dataframe	B-API

I'm	O
a	O
little	O
new	O
to	O
Python	O
and	O
openstack	O
ceilometer	O
.	O

I'm	O
reading	O
ceilometer	O
data	O
using	O
the	O
following	O
code	O
:	O
#CODE	O

(	O
Please	O
see	O
picture	O
of	O
output	O
attached	O
)	O

Does	O
anyone	O
know	O
how	O
i	O
could	O
convert	O
this	O
into	O
a	O
dataframe	B-API
?	O

I	O
tried	O
:	O
`	O
ls2	O
=	O
pandas.DataFrame	B-API
(	O
ls	O
,	O
columns	O
=[	O
"	O
user_id	O
"	O
,	O
"	O
name	O
"	O
,	O
"	O
resource_id	O
"	O
,	O
"	O
source	O
"	O
,	O
"	O
meter_id	O
"	O
,	O
"	O
project_id	O
"	O
,	O
"	O
type	O
"	O
,	O
"	O
unit	O
"])`	O

but	O
get	O
the	O
following	O
error	O
:	O
#CODE	O

if	O
someone	O
could	O
help	O
it	O
would	O
really	O
be	O
much	O
much	O
appreciated	O
..	O

Thank	O
you	O

Best	O
Wishes	O

T	O

If	O
the	O
<	O
Meter	O
part	O
was	O
not	O
there	O
(	O
i.e.	O
if	O
you	O
had	O
a	O
list	O
of	O
dicts	O
)	O
,	O
your	O
code	O
would	O
work	O
.	O

I	O
tried	O
removing	O
it	O
with	O
ls	O
[	O
ls.index	O
(	O
"	O
[	O
<	O
Meter	O
")]	O
=	O
"'"	O
but	O
it	O
throws	O
and	O
error	O
:	O

what	O
is	O
type	O
(	O
ls	O
[	O
0	O
])	O
?	O

Also	O
,	O
what	O
happens	O
when	O
you	O
do	O
ls2	O
=	O
pandas.DataFrame	B-API
(	O
ls	O
)	O
?	O

(	O
This	O
will	O
give	O
you	O
an	O
insight	O
as	O
to	O
why	O
you	O
got	O
that	O
ValueError	O
)	O

it's	O
a	O
list	O
type	O
;	O
it	O
returns	O
this	O
when	O
i	O
try	O
to	O
convert	O
to	O
pandas.dataframe	B-API
:	O
0	O

0	O
<	O
Meter	O
{	O
u'user_id	O
'	O
:	O
u'f82bcc547ffd4bf0ae28c452	O
...	O

1	O
<	O
Meter	O
{	O
u'user_id	O
'	O
:	O
u'f82bcc547ffd4bf0ae28c452	O
...	O

not	O
sure	O
what	O
it	O
means	O
?	O

I'm	O
a	O
bit	O
busy	O
atm	O
but	O
it's	O
possible	O
to	O
use	O
`	O
DataFrame.from_items	B-API
`	O
with	O
a	O
generator	O
.	O

Will	O
post	O
an	O
answer	O
soon	O

With	O
the	O
help	O
of	O
a	O
colleague	O
we	O
managed	O
to	O
find	O
the	O
solution	O
.	O

I'm	O
posting	O
it	O
in	O
case	O
it's	O
useful	O
to	O
anyone	O
trying	O
to	O
convert	O
ceilometer	O
data	O
into	O
a	O
dataframe	B-API
.	O

#CODE	O

Replaced	O
characters	O
which	O
make	O
it	O
as	O
invalid	O
dictionary	O
and	O
converted	O
into	O
dataframe	B-API

#CODE	O

Python	O
Pandas	O
:	O
dataframe	B-API
groupby	B-API
and	O
aggregation	O
UnicodeEncodeError	O

i	O
have	O
a	O
dataframe	B-API
y	O
that	O
i	O
read	O
it	O
from	O
a	O
csv	O
file	O
with	O
has	O
two	O
columns	O
one	O
for	O
text	O
and	O
other	O
for	O
votes	O
#CODE	O

what	O
i	O
need	O
to	O
is	O
group	O
by	O
unique	O
text	O
and	O
omit	O
remove	O
controversy	O
text	O
which	O
has	O
mixed	O
votes	O
,	O
(	O
only	O
keep	O
text	O
groups	O
which	O
all	O
of	O
them	O
has	O
same	O
vote	O
either	O
1	O
or	O
-1	O
)	O

there's	O
no	O
nans	O
in	O
the	O
data	O
so	O
far	O
.	O

the	O
"	O
text	O
"	O
column	O
values	O
are	O
in	O
arabic	O
utf-8	O
read	O
characters	O

what	O
i	O
have	O
tried	O
is	O
:	O
#CODE	O

unfortunately	O
i	O
have	O
this	O
error	O
:	O
#CODE	O

#CODE	O

the	O
first	O
line	O
give	O
me	O
the	O
same	O
UnicodeEncodeError	O
,	O
probably	O
also	O
it	O
wasn't	O
clear	O
(	O
edited	O
now	O
)	O
that	O
i	O
read	O
the	O
data	O
using	O
pandas.read_csv	B-API
function	O
which	O
AFAIK	O
fills	O
the	O
datatypes	O
automatically	O
in	O
the	O
dataframe	B-API

pandas.Series.interpolate()	B-API
does	O
nothing	O
.	O

Why	O
?	O

I	O
have	O
a	O
dataframe	B-API
with	O
DatetimeIndex	B-API
.	O

This	O
is	O
one	O
of	O
columns	O
:	O
#CODE	O

When	O
I'm	O
trying	O
to	O
use	O
`	O
interpolate()	B-API
`	O
on	O
function	O
I	O
get	O
absolutly	O
nothing	O
changes	O
:	O
#CODE	O

How	O
to	O
make	O
it	O
work	O
?	O

Update	O
:	O

the	O
code	O
for	O
generating	O
such	O
a	O
dataframe	B-API
.	O

#CODE	O

After	O
that	O
I	O
fill	O
cells	O
with	O
some	O
data	O
.	O

I	O
have	O
dataframe	B-API
`	O
field_data	O
`	O
with	O
survey	O
data	O
about	O
boarding	O
and	O
alighting	O
on	O
railroad	O
,	O
and	O
`	O
station	O
`	O
variable	O
.	O

I	O
also	O
have	O
`	O
interval_end	O
`	O
function	O
defined	O
like	O
this	O
:	O
#CODE	O

The	O
code	O
:	O
#CODE	O

Absolutly	O
the	O
same	O
results	O
I	O
achieve	O
with	O
`	O
linear	O
`	O
,	O
`	O
cubic	O
`	O
and	O
all	O
other	O
methods	O
.	O

Can	O
you	O
provide	O
a	O
[	O
SSCCE	O
]	O
(	O
#URL	O
)	O
?	O

Esp	O
.	O
some	O
code	O
to	O
construct	O
your	O
dataframe	B-API
,	O
that	O
would	O
be	O
nice	O

I	O
think	O
interpolate	B-API
needs	O
regular	O
spaced	O
time	O
series	B-API
.	O

Looks	O
like	O
you	O
need	O
to	O
resample	B-API
before	O
.	O

Try	O
converting	O
your	O
series	B-API
to	O
a	O
float	O
dtype	B-API
.	O

You	O
need	O
to	O
convert	O
your	O
`	O
Series	B-API
`	O
to	O
have	O
a	O
dtype	B-API
of	O
`	O
float64	O
`	O
instead	O
of	O
your	O
current	O
`	O
object	O
`	O
.	O

Here's	O
an	O
example	O
to	O
illustrate	O
the	O
difference	O
.	O

Note	O
that	O
in	O
general	O
`	O
object	O
`	O
dtype	B-API
`	O
Series	B-API
`	O
are	O
of	O
limited	O
use	O
,	O
the	O
most	O
common	O
case	O
being	O
a	O
`	O
Series	B-API
`	O
containing	O
strings	O
.	O

Other	O
than	O
that	O
they	O
are	O
very	O
slow	O
since	O
they	O
cannot	O
take	O
advantage	O
of	O
any	O
data	O
type	O
information	O
.	O

#CODE	O

Pandas	O
to_sql	B-API
returning	O
error	O
due	O
to	O
column	O
names	O
with	O
sqlite	O
?	O

I'm	O
trying	O
to	O
store	O
stock	O
screens	O
in	O
an	O
sqlite	O
database	O
but	O
when	O
I	O
use	O
screen.to_sql()	O
from	O
pandas	O
it	O
returns	O
:	O
#CODE	O

If	O
I	O
replace	O
all	O
the	O
slashes	O
with	O
'	O
over	O
'	O
then	O
I	O
get	O
the	O
following	O
:	O
#CODE	O

I'm	O
confused	O
because	O
this	O
code	O
is	O
nearly	O
identical	O
to	O
the	O
one	O
I	O
used	O
for	O
a	O
mysql	O
database	O
,	O
can	O
anyone	O
see	O
why	O
sqlite	O
does	O
not	O
like	O
this	O
?	O

Here	O
is	O
the	O
full	O
query	O
as	O
created	O
by	O
pandas	O
.	O

#CODE	O

is	O
this	O
the	O
cause	O
:	O
[	O
This_Yr	O
`	O
s_Est.d_Growth_	O
(	O
F	O
(	O
1	O
)	O
/	O
F	O
(	O
0	O
))]	O
TEXT	O
it	O
looks	O
like	O
this	O
maybe	O
escaping	O
the	O
string	O

Taking	O
care	O
of	O
both	O
the	O
/	O
and	O
the	O
`	O
we	O
end	O
up	O
with	O

[	O
This_Yr's_Est	O
.d_Growth_	O
(	O
F	O
(	O
1	O
)	O
_over_F	O
(	O
0	O
))]	O
TEXT	O
,	O

but	O
it	O
still	O
returns	O
the	O
error	O
concerning	O
the	O
]	O
.	O

Thanks	O
though	O
!	O

I'd	O
do	O
a	O
divide	O
and	O
conquer	O
to	O
find	O
the	O
offending	O
column	O
line	O
,	O
so	O
remove	O
half	O
the	O
columns	O
,	O
parse	O
again	O
,	O
if	O
it	O
succeeds	O
,	O
try	O
the	O
other	O
half	O
,	O
on	O
failure	O
keep	O
halving	O
until	O
you	O
find	O
the	O
line	O
.	O

Fix	O
it	O
and	O
restart	O
again	O

I'll	O
probably	O
just	O
make	O
a	O
quick	O
python	O
screen	O
to	O
try	O
to	O
make	O
a	O
table	O
with	O
each	O
line	O
individually	O
.	O

Thanks	O
!	O

even	O
`"`	O
may	O
break	O
now	O
depending	O
on	O
the	O
database	O
backend	O
,	O
I	O
have	O
created	O
an	O
issue	O
[	O
here	O
]	O
(	O
#URL	O
)	O
and	O
the	O
fix	O
is	O
to	O
properly	O
quote	O
and	O
escape	O
things	O
.	O

Actually	O
the	O
issue	O
is	O
about	O
table	O
names	O
,	O
but	O
similar	O
.	O

`	O
[	O
]`	O
quotes	O
cannot	O
be	O
nested	O
.	O

You	O
should	O
use	O
a	O
quote	O
character	O
that	O
can	O
be	O
escaped	O
,	O
such	O
as	O
`"`	O
(	O
which	O
would	O
have	O
to	O
be	O
doubled	O
):	O
#CODE	O

(	O
In	O
this	O
case	O
,	O
you	O
don't	O
even	O
have	O
`"`	O
in	O
the	O
names	O
.	O
)	O

Matplotlib	O
DateFormatter	O
for	O
axis	O
label	O
not	O
working	O

I'm	O
trying	O
to	O
adjust	O
the	O
formatting	O
of	O
the	O
date	O
tick	O
labels	O
of	O
the	O
x-axis	O
so	O
that	O
it	O
only	O
shows	O
the	O
Year	O
and	O
Month	O
values	O
.	O

From	O
what	O
I've	O
found	O
online	O
,	O
I	O
have	O
to	O
use	O
mdates.DateFormatter	O
,	O
but	O
it's	O
not	O
taking	O
effect	O
at	O
all	O
with	O
my	O
current	O
code	O
as	O
is	O
.	O

Anyone	O
see	O
where	O
the	O
issue	O
is	O
?	O

(	O
the	O
dates	O
are	O
the	O
index	O
of	O
the	O
pandas	O
DF	O
)	O
#CODE	O

thanks	O
guys	O
!	O

Reproducible	O
scenario	O
code	O
:	O
#CODE	O

Still	O
can't	O
get	O
just	O
the	O
year	O
and	O
month	O
to	O
show	O
up	O
...	O

what	O
happens	O
if	O
you	O
set	O
the	O
format	O
after	O
you	O
draw	O
the	O
bars	O
?	O

@USER	O
-	O
if	O
I	O
set	O
the	O
format	O
after	O
.plot	B-API
,	O
get	O
an	O
error	O
like	O
this	O
:	O
ValueError	O
:	O
DateFormatter	O
found	O
a	O
value	O
of	O
x=0	O
,	O
which	O
is	O
an	O
illegal	O
date	O
.	O

This	O
usually	O
occurs	O
because	O
you	O
have	O
not	O
informed	O
the	O
axis	O
that	O
it	O
is	O
plotting	O
dates	O
,	O
e.g.	O
,	O
with	O
ax.xaxis_date()	O
.	O

It's	O
the	O
same	O
for	O
if	O
I	O
put	O
it	O
before	O
ax.xaxis_date()	O
or	O
after	O
.	O

oof	O
--	O
this	O
is	O
why	O
i	O
hate	O
plotting	O
with	O
pandas	O
.	O
can	O
you	O
make	O
your	O
example	O
reproducible	O
(	O
i.e.	O
,	O
add	O
code	O
that	O
creates	O
`	O
basisDF	O
`	O
without	O
any	O
external	O
files	O
)	O
and	O
I'll	O
try	O
to	O
take	O
a	O
look	O
.	O

@USER	O
-	O
reproducible	O
code	O
up	O
!	O

pandas	O
just	O
doesn't	O
work	O
well	O
with	O
custom	O
date-time	O
formats	O
.	O

You	O
need	O
to	O
just	O
use	O
raw	O
matplotlib	O
in	O
cases	O
like	O
this	O
.	O

#CODE	O

And	O
that	O
gives	O
me	O
:	O

ah	O
gotcha	O
.	O
thanks	O
Paul	O
!	O

pandas	O
series	B-API
/	O
frame	O
quantile	B-API
function	O
taking	O
multiple	O
probabilities	O
?	O

Is	O
there	O
a	O
way	O
to	O
call	O
frame.quantile	O
(	O
or	O
series.quantile	B-API
)	O
and	O
provide	O
a	O
list	O
of	O
probabilities	O
?	O

It	O
looks	O
like	O
this	O
is	O
simply	O
an	O
omission	O
in	O
the	O
code	O
as	O
it	O
relies	O
on	O
scipy.stats.scoreatpercentile	O
which	O
takes	O
both	O
a	O
list	O
of	O
probabilities	O
and	O
an	O
axis	O
argument	O
.	O

#CODE	O

Compare	O
to	O
:	O
#CODE	O

You	O
could	O
write	O
one	O
:	O
#CODE	O

Example	O
use	O
:	O
#CODE	O

+1	O
for	O
``	O
type	O
(	O
df	O
)``	O
as	O
a	O
constructor	O
.	O

Wish	O
I'd	O
thought	O
of	O
that	O
or	O
seen	O
it	O
a	O
year	O
ago	O
!	O

Just	O
use	O
the	O
built	O
in	O
quantiles	O
in	O
scipy.stats.mstats	O
or	O
pencentile	O
in	O
numpy	O
.	O

@USER	O
lol	O
,	O
considering	O
the	O
OP	O
is	O
using	O
it	O
in	O
their	O
question	O
!!	O

Summarize	O
grouped	O
data	O
in	O
Pandas	O
while	O
filtering	O
after	O
the	O
group	O
part	O

I	O
am	O
stumbling	O
over	O
the	O
exact	O
(	O
or	O
at	O
least	O
most	O
elegant	O
)	O
steps	O
to	O
group	O
and	O
aggregate	O
some	O
data	O
in	O
Pandas	O
.	O

Let's	O
say	O
I	O
have	O
a	O
DataFrame	B-API
that	O
looks	O
something	O
like	O
this	O
-	O
#CODE	O

I'd	O
like	O
to	O
get	O
the	O
sum	O
of	O
datacount	O
while	O
grouping	O
by	O
system	O
and	O
sub_system	O
,	O
as	O
long	O
as	O
the	O
datatype	O
does	O
not	O
equal	O
bar	O
,	O
and	O
then	O
put	O
those	O
totals	O
back	O
into	O
the	O
original	O
dataframe	B-API
.	O

If	O
I	O
try	O
non_bar_totals	O
=	O
df	O
[	O
df.datatype	O
!	O
=	O
'	O
bar	O
']	O
.groupby	B-API
([	O
'	O
system	O
'	O
,	O
'	O
sub_sytem	O
'])	O
.agg	O
(	O
np.sum	O
)	O
,	O
it'll	O
get	O
me	O
something	O
like	O
-	O
#CODE	O

But	O
now	O
I'm	O
not	O
sure	O
how	O
to	O
push	O
that	O
count	O
value	O
back	O
into	O
the	O
original	O
DataFrame	B-API
.	O

What	O
is	O
the	O
right	O
syntax	O
to	O
get	O
those	O
counts	O
to	O
be	O
pushed	O
back	O
into	O
the	O
original	O
Dataframe	B-API
?	O

The	O
end	O
product	O
should	O
look	O
like	O
-	O
#CODE	O

Thanks	O
,	O
I	O
know	O
this	O
is	O
something	O
simple	O
,	O
I'm	O
just	O
missing	O
the	O
right	O
keyword	O
to	O
find	O
an	O
example	O
of	O
someone	O
doing	O
it	O
.	O

Feels	O
like	O
you	O
should	O
be	O
able	O
to	O
use	O
a	O
transform	O
like	O
`	O
g.transform	O
(	O
lambda	O
x	O
:	O
x	O
[	O
'	O
datacount	O
']	O
.where	B-API
(	O
x	O
[	O
'	O
datatype	O
']	O
!	O
=	O
'	O
bar	O
')	O
.sum()	B-API
)`	O
but	O
not	O
quite	O
.	O

You	O
can	O
go	O
by	O
using	O
the	O
power	O
of	O
apply	B-API
function	O
:	O
#CODE	O

Superb	O
!	O

Thanks	O
for	O
the	O
quick	O
and	O
simple	O
answer	O
!	O

Pandas	O
sparse	O
dataframe	B-API
larger	O
on	O
disk	O
than	O
dense	O
version	O

I	O
find	O
that	O
the	O
sparse	O
versions	O
of	O
a	O
dataframe	B-API
are	O
actually	O
much	O
larger	O
when	O
saved	O
to	O
disk	O
than	O
dense	O
versions	O
.	O

What	O
am	O
I	O
doing	O
wrong	O
?	O

#CODE	O

Using	O
version	O
0.12.0	O

I	O
would	O
ultimately	O
like	O
to	O
efficiently	O
store	O
10^7	O
by	O
60	O
arrays	O
,	O
with	O
about	O
10%	O
density	O
,	O
then	O
pull	O
them	O
into	O
Pandas	O
dataframes	O
and	O
play	O
with	O
them	O
.	O

Edit	O
:	O
Thanks	O
to	O
Jeff	O
for	O
answering	O
the	O
original	O
question	O
.	O

Follow-up	O
question	O
:	O
This	O
appears	O
to	O
only	O
give	O
savings	O
for	O
pickling	O
,	O
and	O
not	O
when	O
using	O
other	O
formats	O
like	O
HDF5	O
.	O

Is	O
pickling	O
my	O
best	O
route	O
?	O

#CODE	O

This	O
is	O
data	O
that	O
,	O
as	O
a	O
list	O
of	O
indices	O
in	O
a	O
Matlab	O
.mat	O
file	O
,	O
is	O
less	O
than	O
12M	O
.	O

I	O
was	O
eager	O
to	O
get	O
it	O
into	O
an	O
HDF5	O
/	O
Pytables	O
format	O
so	O
that	O
I	O
could	O
grab	O
just	O
specific	O
indices	O
(	O
other	O
files	O
are	O
much	O
larger	O
,	O
and	O
take	O
much	O
longer	O
to	O
load	O
into	O
memory	O
)	O
,	O
and	O
then	O
readily	O
do	O
Pandasy	O
things	O
to	O
them	O
.	O

Perhaps	O
I	O
am	O
not	O
going	O
about	O
this	O
the	O
right	O
way	O
?	O

add	O
a	O
compression	O
filter	O
,	O
see	O
here	O
:	O
#URL	O

With	O
a	O
dense	O
dataframe	B-API
and	O
complevel=9	O
and	O
complib=	O
'	O
blosc	O
'	O
,	O
that	O
drops	O
us	O
from	O
544M	O
to	O
26M	O
.	O

Far	O
better	O
,	O
but	O
still	O
not	O
keeping	O
up	O
with	O
12M	O
.	O

Trying	O
compression	O
with	O
the	O
sparse	O
dataframe	B-API
throws	O
a	O
TypeError	O
:	O

`	O
TypeError	O
:	O
cannot	O
properly	O
create	O
the	O
storer	O
for	O
:	O
[	O
_TABLE_MAP	O
]	O
[	O
group	O
->	O
/	O
test_sparse	O
(	O
Group	O
)	O
''	O
,	O
value	O
->	O
,	O
table	O
->	O
True	O
,	O
append	B-API
->	O
True	O
,	O
kwargs	O
->	O
{	O
'	O
encoding	O
'	O
:	O
None}	O
]`	O

hmm	O
....	O

that's	O
not	O
the	O
correct	O
format	O
;	O
it	O
should	O
save	O
it	O
with	O
table=False	O
;	O
but	O
that's	O
the	O
default	O
too	O
.	O
let	O
me	O
take	O
a	O
look	O
.	O

can	O
you	O
post	O
the	O
frame	O
that	O
you	O
saved	O
(	O
in	O
dense	O
format	O
is	O
ok	O
)	O
,	O
compressed	O
pls	O
!	O
on	O
say	O
a	O
dropbox	O
link	O
?	O

Here	O
you	O
go	O
:	O
#URL	O

you	O
are	O
creating	O
a	O
frame	O
that	O
has	O
4000	O
columns	O
,	O
and	O
only	O
4	O
rows	O
;	O
sparse	O
is	O
dealt	O
with	O
rows-wise	O
,	O
so	O
reverse	O
the	O
dimensions	O
.	O

#CODE	O

Followup	O
.	O

Your	O
store	O
that	O
you	O
supplied	O
was	O
written	O
in	O
`	O
table	O
`	O
format	O
,	O
and	O
as	O
a	O
result	O
saved	O
the	O
dense	O
version	O
(	O
Sparse	O
is	O
not	O
supported	O
for	O
table	O
format	O
which	O
are	O
very	O
flexible	O
and	O
queryable	O
,	O
see	O
docs	O
.	O

Furthermore	O
,	O
you	O
may	O
want	O
to	O
experiment	O
with	O
saving	O
your	O
file	O
using	O
2	O
different	O
representations	O
of	O
the	O
sparse	O
format	O
.	O

so	O
,	O
here's	O
a	O
sample	O
session	O
:	O
#CODE	O

IIRC	O
their	O
is	O
a	O
bug	O
in	O
0.12	O
in	O
that	O
`	O
to_hdf	B-API
`	O
doesn't	O
pass	O
all	O
the	O
arguments	O
thru	O
,	O
so	O
you	O
prob	O
want	O
to	O
use	O
:	O
#CODE	O

These	O
are	O
stored	O
basically	O
as	O
a	O
collection	O
of	O
`	O
SparseSeries	O
`	O
so	O
if	O
the	O
density	O
is	O
low	O
and	O
non-contiguous	O
then	O
it	O
will	O
not	O
be	O
as	O
minimal	O
as	O
far	O
as	O
size	O
goes	O
.	O

Pandas	O
sparse	O
suite	O
deals	O
better	O
with	O
a	O
smaller	O
number	O
of	O
contiguous	O
blocks	O
,	O
though	O
YMMV	O
.	O
scipy	O
provides	O
some	O
sparse	O
handling	O
tools	O
as	O
well	O
.	O

Though	O
IMHO	O
,	O
these	O
are	O
pretty	O
trivial	O
sizes	O
for	O
HDF5	O
files	O
anyhow	O
,	O
you	O
can	O
handle	O
gigantic	O
number	O
of	O
rows	O
;	O
and	O
files	O
sizes	O
into	O
the	O
10's	O
and	O
100's	O
of	O
gigabytes	O
can	O
easily	O
be	O
handled	O
(	O
though	O
recommend	O
)	O
.	O

Furthermore	O
you	O
might	O
to	O
consider	O
using	O
a	O
table	O
format	O
if	O
this	O
is	O
indeed	O
a	O
lookup	O
table	O
as	O
you	O
can	O
query	O
.	O

Difficulty	O
importing	O
.dat	O
file	O

I	O
am	O
somehow	O
having	O
difficulty	O
reading	O
in	O
this	O
file	O
into	O
python	O
with	O
pandas	O
read_table	B-API
function	O
.	O

#URL	O

This	O
is	O
my	O
code	O
:	O
#CODE	O

Which	O
yields	O
error	O
:	O
#CODE	O

Dont	O
know	O
about	O
read_table	B-API
,	O
but	O
you	O
can	O
read	O
this	O
file	O
directly	O
as	O
follows	O
:	O
#CODE	O

Prints	O
:	O
#CODE	O

The	O
same	O
can	O
be	O
obtained	O
as	O
follows	O
:	O
#CODE	O

Just	O
thought	O
it	O
might	O
be	O
more	O
efficient	O
to	O
do	O
so	O
with	O
built-in	O
functions	O
.	O

Do	O
you	O
know	O
of	O
anyway	O
to	O
do	O
it	O
with	O
built-in	O
functions	O
?	O

you	O
can	O
simply	O
use	O
``	O
skiprows=0	O
``	O

Thanks	O
a	O
ton	O
.	O

I	O
think	O
the	O
trick	O
is	O
to	O
use	O
regular	O
expression	O
in	O
the	O
sep	O
argument	O
.	O

Cause	O
when	O
I	O
use	O
"	O
\s+	O
"	O
,	O
even	O
with	O
read_table	B-API
,	O
it	O
works	O
.	O

Good	O
to	O
hear	O
its	O
ok	O
now	O
:-)	O

Pandas	O
Grouping	O
and	O
Reducing	O
DataFrame	B-API

I	O
am	O
rather	O
new	O
to	O
Python	O
,	O
and	O
VERY	O
new	O
to	O
Pandas	O
(	O
I	O
am	O
having	O
a	O
more	O
difficult	O
time	O
learning	O
Pandas	O
than	O
Python	O
)	O
.	O

I	O
am	O
trying	O
to	O
transform	O
a	O
large	O
dataset	O
,	O
and	O
I	O
am	O
stuck	O
.	O

I	O
upload	O
data	O
from	O
a	O
CSV	O
that	O
has	O
the	O
following	O
structure	O
.	O

#CODE	O

My	O
end	O
goal	O
is	O
find	O
a	O
way	O
to	O
group	O
this	O
into	O
form	O
where	O
I	O
can	O
get	O
the	O
series	B-API
of	O
categories	O
leading	O
up	O
to	O
a	O
success	O
flag	O
for	O
a	O
specific	O
ID	O
,	O
then	O
an	O
array	O
of	O
the	O
time	O
elapsed	O
during	O
from	O
the	O
previous	O
row	O
the	O
same	O
ID	O
.	O

So	O
a	O
result	O
would	O
something	O
like	O
:	O
#CODE	O

I	O
am	O
not	O
sure	O
if	O
Pandas	O
'	O
or	O
NumPy's	O
multidimensional	O
arrays	O
would	O
be	O
better	O
suited	O
for	O
the	O
task	O
.	O

I	O
am	O
also	O
not	O
sure	O
what	O
functions	O
to	O
play	O
around	O
with	O
more	O
in	O
Pandas	O
to	O
accomplish	O
this	O
.	O

A	O
point	O
in	O
the	O
right	O
direction	O
would	O
be	O
greatly	O
helpful	O
.	O

I	O
do	O
not	O
100%	O
understand	O
the	O
question	O
.	O

I	O
am	O
unsure	O
what	O
the	O
(	O
0	O
,	O
2	O
,	O
4	O
)	O
means	O
.	O

Ok	O
let's	O
make	O
a	O
start	O
.	O

This	O
is	O
a	O
non	O
pandas-esque	O
way	O
,	O
what	O
with	O
all	O
the	O
dataframe	B-API
looping	O
.	O

I	O
have	O
your	O
data	O
in	O
csv	O
so	O
load	O
it	O
as	O
follows	O
:	O
#CODE	O

and	O
looks	O
like	O
:	O
#CODE	O

And	O
now	O
the	O
code	O
:	O
#CODE	O

produces	O
#CODE	O

wow	O
,	O
that	O
worked	O
.	O

To	O
clarify	O
,	O
the	O
second	O
array	O
is	O
a	O
time	O
delta	O
.	O

I	O
updated	O
your	O
code	O
to	O
give	O
the	O
day	O
delta	O
from	O
the	O
first	O
instance	O
of	O
the	O
unique	O
ID	O
,	O
then	O
to	O
reset	O
after	O
a	O
success	O
.	O

Thanks	O
!	O

the	O
real	O
data	O
file	O
is	O
pretty	O
huge	O
,	O
so	O
I	O
have	O
to	O
figure	O
out	O
how	O
to	O
make	O
this	O
fit	O
into	O
memory	O
.	O

But	O
this	O
is	O
awesome	O
,	O
I	O
now	O
have	O
a	O
direction	O
to	O
keep	O
learning	O
toward	O
.	O

Pandas	O
Python	O
:	O
sort	O
dataframe	B-API
but	O
don't	O
include	O
given	O
row	O

I	O
have	O
df	O
which	O
looks	O
like	O
this	O
:	O
#CODE	O

I	O
would	O
like	O
to	O
sort	O
this	O
in	O
acsending	O
order	O
but	O
I	O
dont	O
want	O
"	O
A	O
gauche	O
"	O
and	O
"	O
A	O
droite	O
"	O
included	O
in	O
the	O
sorting	O
.	O

The	O
code	O
below	O
does	O
what	O
I	O
want	O
but	O
I'm	O
not	O
sure	O
how	O
to	O
exclude	O
"	O
A	O
gauche	O
"	O
and	O
"	O
A	O
droite	O
"	O
from	O
the	O
sorting	O
.	O

#CODE	O

expected	O
output	O
#CODE	O

Thanks	O

You	O
probably	O
want	O
to	O
filter	O
out	O
the	O
rows	O
you	O
don't	O
want	O
included	O
in	O
your	O
sort	O
operation	O
:	O
#CODE	O

Which	O
can	O
then	O
be	O
sorted	O
#CODE	O

And	O
if	O
you	O
want	O
the	O
excluded	O
rows	O
appended	O
to	O
the	O
end	O
of	O
your	O
data	O
frame	O
,	O
you	O
could	O
do	O
this	O
:	O
#CODE	O

Thanks	O
for	O
this	O
.	O

Q	O
-	O
What	O
would	O
I	O
do	O
if	O
the	O
rows	O
I	O
didn't	O
want	O
sorted	O
were	O
in	O
the	O
middle	O
,	O
say	O
row	O
3	O
and	O
5	O
?	O

You'd	O
either	O
have	O
to	O
do	O
more	O
splitting	O
and	O
concatenation	O
,	O
or	O
maybe	O
you	O
could	O
introduce	O
some	O
kind	O
of	O
weighting	O
function	O
.	O

It	O
would	O
probably	O
depend	O
on	O
whether	O
you	O
mean	O
'	O
put	O
this	O
row	O
at	O
position	O
3	O
when	O
I'm	O
done	O
sorting	O
'	O
or	O
'	O
put	O
this	O
row	O
after	O
this	O
other	O
row	O
when	O
I'm	O
done	O
sorting	O
'	O

Pandas	O
filter	O
rows	O
by	O
substring	O
within	O
text	O
column	O

I	O
have	O
a	O
list	O
of	O
keywords	O
as	O
well	O
as	O
a	O
DF	O
that	O
contains	O
a	O
text	O
column	O
.	O

I	O
am	O
trying	O
to	O
filter	O
out	O
every	O
row	O
where	O
the	O
text	O
in	O
the	O
text	O
field	O
contains	O
one	O
of	O
the	O
keywords	O
.	O

I	O
believe	O
what	O
am	O
I	O
looking	O
for	O
is	O
something	O
like	O
the	O
`	O
.isin	B-API
`	O
method	O
but	O
that	O
would	O
be	O
able	O
to	O
take	O
a	O
regex	O
argument	O
as	O
I	O
am	O
searching	O
for	O
substrings	O
within	O
the	O
text	O
not	O
exact	O
matches	O
.	O

What	O
I	O
have	O
:	O
#CODE	O

And	O
I	O
would	O
like	O
to	O
remove	O
any	O
rows	O
that	O
contain	O
a	O
key	O
in	O
the	O
text	O
so	O
I	O
would	O
end	O
up	O
with	O
:	O
#CODE	O

use	O
`	O
str.contains	B-API
`	O
and	O
join	O
the	O
keys	O
using	O
`	O
|	O
`	O
to	O
create	O
a	O
regex	O
pattern	O
and	O
negate	O
the	O
boolean	O
mask	O
`	O
~	O
`	O
to	O
filter	O
your	O
df	O
:	O
#CODE	O

Convert	O
pandas	O
multi-index	O
to	O
pandas	O
timestamp	O

I'm	O
trying	O
to	O
convert	O
an	O
unstacked	O
,	O
multi-indexed	O
data-frame	O
back	O
to	O
a	O
single	O
pandas	O
datetime	O
index	O
.	O

The	O
index	O
of	O
my	O
original	O
data-frame	O
,	O
i.e.	O
before	O
multi-indexing	O
and	O
unstacking	O
,	O
looks	O
like	O
this	O
:	O
#CODE	O

then	O
I	O
apply	B-API
the	O
multi-indexing	O
and	O
unstacking	O
so	O
I	O
can	O
plot	O
the	O
yearly	O
data	O
on	O
top	O
of	O
each	O
other	O
like	O
this	O
:	O
#CODE	O

My	O
new	O
data-frame	O
for	O
the	O
first	O
two	O
days	O
of	O
May	O
looks	O
like	O
this	O
:	O
#CODE	O

The	O
index	O
for	O
the	O
new	O
data	O
frame	O
shown	O
above	O
now	O
looks	O
like	O
this	O
:	O
#CODE	O

which	O
doesn't	O
produce	O
a	O
very	O
nice	O
plot	O
with	O
respect	O
to	O
the	O
xticks	O
(	O
major	O
and	O
minor	O
)	O
.	O

If	O
I	O
can	O
convert	O
this	O
multi-index	O
back	O
to	O
a	O
single	O
pandas	O
datetime	O
index	O
,	O
using	O
only	O
the	O
month	O
,	O
day	O
and	O
hour	O
data	O
,	O
then	O
the	O
major	O
/	O
minor	O
ticks	O
will	O
be	O
plotted	O
automagically	O
the	O
way	O
I	O
would	O
like	O
(	O
I	O
think	O
)	O
.	O

For	O
example	O
:	O

current	O
solution	O
:	O
#CODE	O

required	O
solution	O
:	O
#CODE	O

Even	O
a	O
little	O
hint	O
would	O
be	O
greatly	O
appreciated	O
.	O

How	O
do	O
I	O
go	O
about	O
bumping	O
this	O
up	O
for	O
some	O
support	O
?	O

There	O
are	O
some	O
questions	O
on	O
here	O
over	O
a	O
year	O
old	O
without	O
any	O
answers	O
.	O

Another	O
month	O
?	O

Anything	O
at	O
all	O
will	O
help	O
...	O

Is	O
there	O
a	O
reason	O
you	O
want	O
to	O
do	O
this	O
"	O
auto-magically	O
"	O
?	O

I	O
would	O
probably	O
just	O
write	O
a	O
function	O
to	O
custom	O
generate	O
the	O
x-labels	O
.	O

That	O
sounds	O
faster	O
than	O
what	O
you	O
want	O
.	O

Thanks	O
for	O
he	O
reply	O
.	O

Maybe	O
you're	O
right	O
,	O
it's	O
just	O
I	O
need	O
to	O
maintain	O
a	O
sensible	O
scale	O
when	O
zooming	O
in	O
.	O

I	O
know	O
this	O
would	O
be	O
taken	O
care	O
of	O
using	O
this	O
method	O
.	O

#CODE	O

I	O
think	O
this	O
is	O
a	O
better	O
way	O
to	O
accomplish	O
your	O
goal	O
than	O
re-indexing	O
.	O

What	O
do	O
you	O
think	O
?	O

Hey	O
!,	O
thanks	O
very	O
much	O
for	O
the	O
reply	O
.	O

Ok	O
,	O
I've	O
just	O
given	O
this	O
a	O
go	O
.	O

Yes	O
,	O
this	O
appears	O
to	O
be	O
a	O
much	O
easier	O
way	O
of	O
stacking	O
/	O
sorting	O
yearly	O
data	O
on	O
top	O
of	O
each	O
other	O
into	O
one	O
plot	O
,	O
so	O
thanks	O
for	O
that	O
.	O

However	O
,	O
it	O
isn't	O
a	O
solution	O
to	O
the	O
question	O
.	O

Instead	O
of	O
my	O
xticks	O
,	O
minor	O
/	O
major	O
,	O
being	O
yearly	O
coded	O
(	O
e.g.	O
day	O
month	O
hour	O
)	O
,	O
they	O
are	O
now	O
just	O
broken	O
down	O
into	O
arbitrary	O
chunks	O
of	O
single	O
data	O
points	O
,	O
scaling	O
from	O
0	O
to	O
n-1	O
,	O
where	O
n	O
is	O
the	O
number	O
datapoint	O
in	O
my	O
measurement	O
sample	O
set	O
.	O

Right	O
,	O
I	O
would	O
imagine	O
at	O
that	O
point	O
it's	O
an	O
x_axis	O
tick	O
manipulation	O
...	O
but	O
I	O
am	O
unable	O
to	O
figure	O
out	O
how	O
exactly	O
to	O
do	O
that	O
.	O

Could	O
you	O
perhaps	O
load	O
the	O
data	O
up	O
to	O
a	O
csv	O
somewhere	O
so	O
that	O
I	O
could	O
play	O
with	O
it	O
and	O
maybe	O
create	O
another	O
post	O
on	O
this	O
?	O

Would	O
hte	O
best	O
term	O
for	O
this	O
be	O
a	O
'	O
Seasonality	O
Plot	O
'	O
-	O
take	O
information	O
from	O
multiple	O
years	O
and	O
plot	O
them	O
on	O
one	O
Jan-Dec	O
axis	O
?	O

I	O
can't	O
find	O
any	O
documentation	O
on	O
how	O
to	O
do	O
this	O
which	O
is	O
surprising	O
to	O
me	O

Hey	O
!	O

I've	O
been	O
away	O
,	O
sorry	O
for	O
the	O
delay	O
.	O

Let	O
me	O
get	O
back	O
to	O
you	O
on	O
this	O
.	O

I'll	O
get	O
a	O
csv	O
to	O
you	O
also	O
.	O

Yes	O
to	O
your	O
question	O
also	O
.	O

That's	O
exactly	O
what	O
the	O
plot	O
is	O
all	O
about	O
.	O

Ok	O
,	O
here's	O
a	O
[	O
link	O
to	O
the	O
source	O
files	O
]	O
(	O
#URL	O
)	O
.	O

I've	O
included	O
a	O
small	O
program	O
too	O
,	O
so	O
that	O
you	O
can	O
have	O
a	O
real	O
example	O
to	O
play	O
with	O
.	O

If	O
you	O
launch	O
the	O
script	O
,	O
you	O
will	O
see	O
two	O
figures	O
.	O

Figure.1	O
is	O
the	O
requested	O
stacked	O
yearly	O
data	O
,	O
but	O
the	O
horrible	O
xticks	O
.	O

Figure.2	O
is	O
the	O
requested	O
xticks	O
but	O
without	O
the	O
yearly	O
stacking	O
.	O

Converting	O
data	O
back	O
and	O
forth	O
in	O
pandas	O
gets	O
messy	O
very	O
fast	O
,	O
as	O
you	O
seem	O
to	O
have	O
experienced	O
.	O

My	O
recommendation	O
in	O
general	O
concerning	O
pandas	O
and	O
indexing	O
,	O
is	O
to	O
never	O
just	O
set	O
the	O
index	O
,	O
but	O
to	O
copy	O
it	O
first	O
.	O

Make	O
sure	O
you	O
have	O
a	O
column	O
which	O
contains	O
the	O
index	O
,	O
since	O
pandas	O
does	O
not	O
allow	O
all	O
operations	O
on	O
the	O
index	O
,	O
and	O
intense	O
setting	O
and	O
resetting	O
of	O
the	O
index	O
can	O
cause	O
columns	O
to	O
dissapear	O
.	O

TLDR	O
;	O

Don't	O
convert	O
the	O
index	O
back	O
.	O

Keep	O
a	O
copy	O
.	O

This	O
also	O
ahears	O
to	O
the	O
open	O
/	O
closed	O
principle	O
:	O
#URL	O

Sort	O
Pandas	O
Dataframe	B-API
by	O
Date	O

I	O
have	O
a	O
pandas	O
dataframe	B-API
as	O
follows	O
:	O
#CODE	O

I	O
want	O
to	O
sort	O
it	O
by	O
`	O
Date	B-API
`	O
,	O
but	O
the	O
column	O
is	O
just	O
an	O
`	O
object	O
`	O
.	O

I	O
tried	O
to	O
make	O
the	O
column	O
a	O
date	O
object	O
,	O
but	O
I	O
ran	O
into	O
an	O
issue	O
where	O
that	O
format	O
is	O
not	O
the	O
format	O
needed	O
.	O

The	O
format	O
needed	O
is	O
`	O
2015-02-20	O
,	O
`	O
etc	O
.	O

So	O
now	O
I'm	O
trying	O
to	O
figure	O
out	O
how	O
to	O
have	O
numpy	O
convert	O
the	O
'	O
American	O
'	O
dates	O
into	O
the	O
ISO	O
standard	O
,	O
so	O
that	O
I	O
can	O
make	O
them	O
date	O
objects	O
,	O
so	O
that	O
I	O
can	O
sort	O
by	O
them	O
.	O

How	O
would	O
I	O
convert	O
these	O
american	O
dates	O
into	O
ISO	O
standard	O
,	O
or	O
is	O
there	O
a	O
more	O
straight	O
forward	O
method	O
I'm	O
missing	O
within	O
pandas	O
?	O

You	O
can	O
use	O
`	O
pd.to_datetime()	B-API
`	O
to	O
convert	O
to	O
a	O
datetime	O
object	O
.	O

It	O
takes	O
a	O
format	O
parameter	O
,	O
but	O
in	O
your	O
case	O
I	O
don't	O
think	O
you	O
need	O
it	O
.	O

#CODE	O

I	O
also	O
have	O
a	O
df	O
[	O
'	O
Date	O
']	O
.unique()	B-API
before	O
the	O
sort	O
,	O
which	O
returns	O
a	O
series	B-API
instead	O
of	O
a	O
Dataframe	B-API
.	O

This	O
makes	O
02	O
/	O
20	O
/	O
2015	O
into	O
2015-02-19T18	O
:	O
00:00	O
.000000000	O
-0600	O
which	O
then	O
gets	O
split	O
into	O
2015-02-19	O
.	O

Is	O
there	O
a	O
way	O
to	O
add	O
a	O
day	O
?	O

Or	O
a	O
more	O
formal	O
way	O
to	O
correct	O
this	O
?	O

`	O
df.Date.astype	O
(	O
np.int64	O
)`	O
should	O
work	O
for	O
epoch	O
time	O

Turns	O
out	O
that	O
epoch	O
would	O
be	O
wrong	O
since	O
its	O
assuming	O
times	O
of	O
18:00	O
hours	O
etc	O
.	O

I	O
need	O
them	O
to	O
be	O
00:00	O
hours	O
.	O

I	O
have	O
a	O
way	O
to	O
convert	O
to	O
epoch	O
if	O
I	O
could	O
just	O
get	O
the	O
date	O
objects	O
to	O
not	O
have	O
a	O
time	O
,	O
or	O
the	O
wrong	O
time	O
.	O

for	O
me	O
`	O
pd.to_datetime	B-API
(	O
df.Date	O
)	O
[	O
0	O
]`	O
returns	O
`	O
Timestamp	O
(	O
'	O
2015-02-20	O
00:00	O
:	O
00	O
')`	O

Starting	O
new	O
question	O
with	O
more	O
formal	O
description	O
of	O
issue	O

It	O
looks	O
like	O
there	O
may	O
actually	O
be	O
[	O
a	O
faster	O
way	O
]	O
(	O
#URL	O
)	O
of	O
converting	O
strings	O
into	O
dates	O
.	O

@USER	O
'	O
s	O
answer	O
is	O
fast	O
and	O
concise	O
.	O

But	O
it	O
changes	O
the	O
`	O
DataFrame	B-API
`	O
you	O
are	O
trying	O
to	O
sort	O
,	O
which	O
you	O
may	O
or	O
may	O
not	O
want	O
.	O

(	O
Note	O
:	O
You	O
almost	O
certainly	O
will	O
want	O
it	O
,	O
because	O
your	O
date	O
columns	O
should	O
be	O
dates	O
,	O
not	O
strings	O
!	O
)	O

In	O
the	O
unlikely	O
event	O
that	O
you	O
don't	O
want	O
to	O
change	O
the	O
dates	O
into	O
dates	O
,	O
you	O
can	O
also	O
do	O
it	O
a	O
different	O
way	O
.	O

First	O
,	O
get	O
the	O
index	O
from	O
your	O
sorted	O
`	O
Date	B-API
`	O
column	O
:	O
#CODE	O

Then	O
use	O
it	O
to	O
index	O
your	O
original	O
`	O
DataFrame	B-API
`	O
,	O
leaving	O
it	O
untouched	O
:	O
#CODE	O

Magic	O
!	O

Python	O
-	O
Pandas	O
'	O
.isin	B-API
'	O
on	O
a	O
list	O

I'm	O
using	O
Python	O
2.7	O
on	O
Mac	O
OSX	O
Lion	O
and	O
Pandas	O
0.11.0	O
with	O
the	O
IPython	O
shell	O
.	O

I	O
have	O
a	O
brief	O
issue	O
,	O
using	O
the	O
data	O
selection	O
method	O
`	O
.isin	B-API
`	O
.	O

The	O
issue	O
is	O
that	O
I	O
would	O
like	O
to	O
use	O
`	O
.isin	B-API
`	O
on	O
a	O
list	O
of	O
items	O
,	O
so	O
:	O
#CODE	O

I	O
get	O
the	O
following	O
error	O
when	O
I	O
do	O
this	O
:	O
`	O
KeyError	O
:	O
u'no	O
item	O
named	O
'`	O

I	O
generate	O
the	O
initial	O
list	O
by	O
calling	O
a	O
previously	O
developed	O
function	O
.	O

I	O
tried	O
using	O
`	O
eval	B-API
`	O
on	O
the	O
list	O
,	O
which	O
seems	O
to	O
solve	O
an	O
issue	O
that	O
comes	O
about	O
when	O
using	O
`	O
raw_input	O
`	O
and	O
iterating	O
over	O
items	O
within	O
it	O
-	O
kinda	O
trying	O
to	O
work	O
out	O
some	O
of	O
the	O
issues	O
I've	O
been	O
having	O
when	O
transitioning	O
to	O
`	O
IPython	O
`	O
and	O
`	O
Python	O
2.7	O
`	O
(	O
originally	O
used	O
`	O
Python	O
3.3	O
`)	O
.	O

I	O
also	O
tried	O
iterating	O
over	O
the	O
list	O
,	O
by	O
first	O
doing	O
:	O
#CODE	O

But	O
that	O
also	O
returns	O
:	O
`	O
KeyError	O
:	O
u'no	O
item	O
named	O
'`	O

UPDATE	O
:	O

Here	O
is	O
the	O
header	O
:	O
#CODE	O

Also	O
,	O
I	O
have	O
a	O
function	O
I	O
use	O
to	O
get	O
the	O
header	O
,	O
which	O
makes	O
things	O
easier	O
for	O
me	O
,	O
the	O
output	O
looks	O
like	O
this	O
:	O
#CODE	O

Which	O
,	O
actually	O
,	O
now	O
that	O
I	O
think	O
about	O
it	O
,	O
may	O
be	O
what	O
is	O
causing	O
the	O
problem-	O
although	O
`	O
eval	B-API
`	O
still	O
would	O
not	O
fix	O
it	O
.	O

UPDATE	O
2	O
:	O

So	O
,	O
initially	O
,	O
as	O
you	O
can	O
see	O
in	O
the	O
above	O
`	O
.isin	B-API
`	O
,	O
I	O
was	O
using	O
`	O
header	O
[	O
0	O
]`	O
,	O
which	O
was	O
not	O
right	O
.	O

I	O
tried	O
again	O
using	O
`	O
header	O
[	O
1	O
]`	O
,	O
which	O
is	O
appropriate	O
.	O

I	O
get	O
the	O
following	O
error	O
:	O
#CODE	O

I	O
also	O
tried	O
the	O
regular	O
list	O
again	O
and	O
got	O
this	O
error	O
:	O
#CODE	O

Which	O
,	O
I	O
guess	O
,	O
speaks	O
more	O
definitively	O
to	O
the	O
issue	O
....	O

Is	O
your	O
`	O
header	O
`	O
a	O
DataFrame	B-API
?,	O
What's	O
the	O
difference	O
between	O
your	O
`	O
df	O
`	O
and	O
`	O
header	O
`	O
?	O

Within	O
the	O
`	O
df	O
[	O
df	O
[	O
header	O
[	O
0	O
]]	O
.isin	B-API
(	O
list	O
)]`	O
,	O
header	O
is	O
not	O
a	O
`	O
DataFrame	B-API
`	O
.	O

It	O
is	O
a	O
list	O
of	O
the	O
column	O
names	O
,	O
which	O
compose	O
the	O
header	O
.	O

How	O
did	O
you	O
get	O
the	O
header	O
?	O

By	O
`	O
df.columns	O
`	O
?	O

I	O
got	O
the	O
header	O
by	O
calling	O
a	O
function	O
I	O
wrote	O
called	O
`	O
getHeader	O
`	O
,	O
which	O
literally	O
just	O
returns	O
the	O
name	O
of	O
the	O
columns	O
,	O
in	O
the	O
form	O
of	O
a	O
list	O
,	O
from	O
a	O
CSV	O
file	O
.	O

Although	O
,	O
I	O
did	O
also	O
put	O
the	O
result	O
of	O
calling	O
`	O
DataFrame.columns	O
`	O
,	O
which	O
is	O
right	O
below	O
where	O
it	O
says	O
"	O
UPDATE	O
"	O
...	O

let	O
us	O
[	O
continue	O
this	O
discussion	O
in	O
chat	O
]	O
(	O
#URL	O
)	O

Try	O
to	O
use	O
df.columns	O
as	O
your	O
header	O
instead	O
:	O
#CODE	O

Pandas	O
:	O
Quickly	O
joining	O
on	O
a	O
MultiIndex	O
or	O
reindexing	O

I	O
have	O
some	O
very	O
sparse	O
,	O
multidimensional	O
time	O
series	B-API
,	O
with	O
activity	O
at	O
certain	O
times	O
and	O
zero	O
activity	O
at	O
other	O
times	O
.	O

I'm	O
representing	O
the	O
data	O
in	O
Pandas	O
as	O
a	O
`	O
SparseDataframe	O
`	O
with	O
a	O
`	O
MultiIndex	O
`	O
.	O

The	O
work	O
flow	O
is	O
to	O
perform	O
calculations	O
on	O
the	O
small	O
set	O
of	O
data	O
that	O
are	O
non-zero	O
,	O
then	O
put	O
the	O
results	O
into	O
the	O
large	O
sparse	O
dataframe	B-API
.	O

Later	O
I	O
will	O
do	O
calculations	O
on	O
that	O
sparse	O
dataframe	B-API
(	O
namely	O
tracking	O
the	O
change	O
in	O
activity	O
over	O
time	O
,	O
including	O
the	O
zero	O
activity	O
areas	O
)	O
.	O

The	O
problem	O
is	O
in	O
putting	O
the	O
small	O
set	O
of	O
data	O
into	O
the	O
sparse	O
dataframe	B-API
.	O

Below	O
is	O
a	O
much	O
smaller	O
dataset	O
than	O
what	O
I	O
will	O
eventually	O
use	O
.	O

With	O
a	O
regular	O
index	O
it's	O
ok	O
:	O
#CODE	O

With	O
a	O
`	O
MultiIndex	O
`	O
it's	O
much	O
slower	O
!	O

#CODE	O

I	O
thought	O
the	O
issue	O
might	O
be	O
in	O
using	O
`	O
join	B-API
`	O
,	O
so	O
I	O
tried	O
another	O
route	O
.	O

It	O
was	O
faster	O
but	O
did	O
not	O
solve	O
the	O
problem	O
.	O

#CODE	O

It	O
is	O
indeed	O
faster	O
to	O
use	O
`	O
reindex	B-API
`	O
instead	O
of	O
`	O
join	B-API
`	O
,	O
but	O
now	O
`	O
MultiIndex	O
`	O
is	O
even	O
slower	O
,	O
comparatively	O
!	O

What	O
are	O
my	O
options	O
?	O

Is	O
there	O
a	O
way	O
to	O
achieve	O
the	O
speed	O
of	O
a	O
regular	O
index	O
with	O
the	O
`	O
MultiIndex	O
`	O
?	O

Is	O
there	O
an	O
elegant	O
way	O
to	O
make	O
a	O
regular	O
index	O
function	O
like	O
the	O
`	O
MultiIndex	O
`	O
?	O

Should	O
I	O
be	O
rethinking	O
how	O
to	O
do	O
all	O
of	O
this	O
?	O

Pandas	O
dataframe	B-API
:	O
representing	O
variable	O
length	O
3D	O
data	O
?	O

column	O
with	O
variable	O
length	O
list	O
?	O

There's	O
been	O
a	O
couple	O
times	O
where	O
I've	O
had	O
a	O
timeseries	O
,	O
where	O
one	O
of	O
the	O
columns	O
is	O
a	O
variable	O
length	O
array	O
of	O
information	O
.	O

One	O
example	O
would	O
be	O
image	O
statistics	O
,	O
where	O
the	O
index	O
is	O
a	O
datetime	O
,	O
and	O
the	O
columns	O
are	O
statistics	O
.	O

There	O
is	O
also	O
some	O
global	O
information	O
for	O
each	O
image	O
(	O
location	O
,	O
size	O
,	O
average	O
intensity	O
,	O
etc	O
)	O
,	O
but	O
also	O
a	O
variable	O
number	O
of	O
features	O
are	O
detected	O
,	O
and	O
is	O
some	O
information	O
for	O
each	O
of	O
those	O
features	O
(	O
e.g.	O
response	O
,	O
distance	O
,	O
angle	O
)	O
.	O

I'd	O
want	O
to	O
be	O
able	O
to	O
plot	O
and	O
explore	O
both	O
global	O
frame	O
stats	O
along	O
with	O
the	O
feature	O
stats	O
.	O

For	O
example	O
,	O
plotting	O
average	O
intensity	O
vs	O
location	O
.	O

However	O
,	O
I'd	O
also	O
want	O
to	O
be	O
able	O
to	O
do	O
histograms	O
or	O
violin	O
plots	O
across	O
all	O
of	O
the	O
features	O
:	O
e.g.	O
density	O
plots	O
of	O
feature	O
responses	O
based	O
on	O
time	O
of	O
day	O
.	O

Idealy	O
I	O
could	O
groupby	B-API
and	O
have	O
all	O
of	O
those	O
variable	O
length	O
lists	O
(	O
feature	O
responses	O
)	O
appended	O
to	O
the	O
same	O
group	O
.	O

I'd	O
love	O
to	O
be	O
able	O
to	O
represent	O
as	O
much	O
as	O
I	O
can	O
natively	O
pandas	O
.	O

Of	O
course	O
I	O
could	O
have	O
a	O
tuple	O
or	O
np.array	O
,	O
and	O
groupby	B-API
manually	O
,	O
but	O
then	O
I	O
couldn't	O
serialize	O
to	O
a	O
table	O
format	O
,	O
unless	O
I	O
convert	O
to	O
a	O
string	O
,	O
etc	O
.	O

did	O
you	O
consider	O
using	O
HDF	O
?	O

It	O
ties	O
in	O
well	O
with	O
numpy	O
and	O
pandas	O
,	O
allows	O
you	O
to	O
have	O
a	O
bunch	O
of	O
attributes	O
that	O
describe	O
the	O
timeseries	O
and	O
has	O
2	O
good	O
python	O
modules	O
H5PY	O
and	O
PyTables	O
.	O

Can	O
you	O
explain	O
how	O
to	O
tie	O
it	O
in	O
with	O
reference	O
to	O
my	O
question	O
?	O

I	O
use	O
HDF	O
to	O
store	O
the	O
entire	O
dataframe	B-API
(	O
in	O
fixed	O
format	O
,	O
not	O
table	O
format	O
)	O
,	O
but	O
are	O
you	O
saying	O
I	O
can	O
reference	O
a	O
HDF	O
as	O
a	O
column	O
for	O
a	O
row	O
?	O

pandas	O
handle	O
NaN	O
/	O
None	O
inserts	O
into	O
sybase	O

is	O
there	O
already	O
a	O
working	O
solution	O
to	O
insert	B-API
NaN	O
/	O
None	O
values	O
from	O
a	O
pandas	O
dataframe	B-API
into	O
sybase-ASE	O
tables	O
?	O

I	O
tried	O
the	O
suggested	O
solution	O
after	O
researching	O
online	O
(	O
for	O
pandas	O
side	O
):	O

df.where	B-API
(	O
pd.notnull	B-API
(	O
df	O
)	O
,	O
None	O
)	O

However	O
,	O
when	O
I	O
try	O
to	O
insert	B-API
this	O
into	O
sybase-ASE	O
using	O
bulk_copy	O
/	O
bcp	O
,	O
the	O
None	O
gets	O
inserted	O
as	O
'	O
nan.0	O
'	O
for	O
some	O
reason	O
.	O

Any	O
ideas	O
why	O
this	O
would	O
happen	O
or	O
anyone	O
knows	O
of	O
a	O
workaround	O
to	O
accomplish	O
these	O
insertions	O
of	O
NaN	O
/	O
None	O
as	O
NULL	O
into	O
sybase	O
?	O

[	O
sybase	O
supports	O
NULL	O
]	O

full	O
stacktrace	O
for	O
the	O
DatabaseError	O
:	O
#CODE	O

As	O
Sybase	O
is	O
supported	O
by	O
sqlalchemy	O
(	O
#URL	O
)	O
,	O
this	O
should	O
just	O
work	O
out	O
of	O
the	O
box	O
with	O
`	O
to_sql	B-API
`	O
(	O
no	O
need	O
to	O
convert	O
to	O
Nones	O
with	O
`	O
where	O
`)	O
.	O

What	O
version	O
of	O
pandas	O
are	O
you	O
using	O
?	O

And	O
can	O
you	O
provide	O
a	O
small	O
code	O
sample	O
that	O
you	O
use	O
to	O
insert	B-API
the	O
data	O
?	O

Thank	O
you	O
for	O
your	O
response	O
.	O

My	O
understanding	O
(	O
from	O
a	O
brief	O
previous	O
attempt	O
)	O
was	O
that	O
to_sql	B-API
or	O
pd.io.sql.write_frame	O
does	O
not	O
work	O
for	O
sybase	O
,	O
but	O
I	O
will	O
test	O
this	O
out	O
again	O
with	O
to_sql	B-API
.	O

I	O
am	O
using	O
0.12.0	O
version	O
of	O
pandas	O
.	O

Here	O
is	O
a	O
code	O
sample	O
of	O
how	O
I	O
convert	O
the	O
dataframe	B-API
to	O
a	O
list	O
and	O
then	O
do	O
the	O
bulk	O
copy	O
to	O
insert	B-API
this	O
data	O
into	O
sybase	O
ase	O
:	O
inList	O
=	O
df.values.tolist()	O
blk	O
=	O
self.sybase_conn.blkcursor()	O
blk.copy	O
(	O
table	O
,	O
direction=	O
'	O
in	O
')	O
blk.rowxfermany	O
(	O
inList	O
)	O

You	O
will	O
have	O
to	O
update	O
your	O
pandas	O
version	O
to	O
at	O
least	O
0.15	O
to	O
be	O
able	O
to	O
try	O
if	O
it	O
works	O
with	O
sybase	O
(	O
in	O
previous	O
versions	O
only	O
mysql	O
/	O
sqlite	O
were	O
supported	O
)	O

ok	O
,	O
thanks	O
.	O

I	O
am	O
trying	O
to	O
test	O
this	O
with	O
pandas	O
0.15.2-16	O
now	O
.	O

The	O
connection	O
to	O
sybase	O
and	O
sqlalchemy	O
engine	O
creation	O
looks	O
good	O
.	O

However	O
,	O
I	O
am	O
running	O
into	O
the	O
following	O
error	O
when	O
trying	O
to	O
execute	O
the	O
to_sql	B-API
:	O
dataframe.to_sql	B-API
(	O
'	O
sybase_table_name	O
'	O
,	O
engine	O
,	O
if_exists=	O
'	O
append	B-API
'	O
,	O
index=False	O
)	O
sqlalchemy.exc.DatabaseError	O
:	O
(	O
DatabaseError	O
)	O
Msg	O
102	O
,	O
Level	O
15	O
,	O
State	O
181	O
,	O
Line	O
1	O

Incorrect	O
syntax	O
near	O
'	O
,	O
'	O
.	O

can't	O
see	O
anything	O
obvious	O
as	O
to	O
the	O
root	O
cause	O
of	O
the	O
above	O
DatabaseError	O
.	O

Can	O
you	O
post	O
the	O
full	O
error	O
stacktrace	O
in	O
your	O
question	O
?	O

File	O
"	O
/	O
six2r9cj6gx8yhdpjygflakgnxyjxj2y-sybase-15.0-jump1.00-3	O
/	O
lib-python	O
/	O
Sybase.py	O
"	O
,	O
line	O
753	O
,	O
in	O
_raise_error	O

raise	O
exc	O

sqlalchemy.exc.DatabaseError	O
:	O
(	O
DatabaseError	O
)	O
Msg	O
102	O
,	O
Level	O
15	O
,	O
State	O
181	O
,	O
Line	O
1	O

Incorrect	O
syntax	O
near	O
'	O
,	O
'	O
.	O

Does	O
this	O
help	O
?	O

Unable	O
to	O
post	O
the	O
full	O
trace	O
due	O
to	O
size	O
limitation	O
here	O
.	O

you	O
can	O
update	O
the	O
original	O
question	O
to	O
include	O
it	O
.	O

Let	O
us	O
[	O
continue	O
this	O
discussion	O
in	O
chat	O
]	O
(	O
#URL	O
)	O
.	O

This	O
seems	O
a	O
bug	O
in	O
sqlalchemy	O
.	O

But	O
,	O
you	O
have	O
a	O
rather	O
old	O
version	O
of	O
sqlalchemy	O
(	O
0.7.9	O
,	O
while	O
the	O
most	O
recent	O
is	O
1.0.x	O
)	O
.	O

Can	O
you	O
first	O
try	O
to	O
update	O
sqlalchemy	O

ok	O
,	O
tried	O
with	O
the	O
latest	O
version	O
of	O
sqlalchemy	O
and	O
encountered	O
similar	O
error	O
:	O
In	O
[	O
9	O
]:	O
sqlalchemy.__version__	O

Out	O
[	O
9	O
]:	O
'	O
1.0.6	O
'	O
dataframe.to_sql	B-API
(	O
'	O
sybase_table_name	O
'	O
,	O
engine	O
,	O
if_exists=	O
'	O
append	B-API
'	O
,	O
index=False	O
)	O

DatabaseError	O
:	O
(	O
Sybase.DatabaseError	O
)	O
Msg	O
102	O
,	O
Level	O
15	O
,	O
State	O
181	O
,	O
Line	O
1	O

Can	O
you	O
try	O
with	O
only	O
a	O
couple	O
of	O
columns	O
first	O
?	O

Do	O
you	O
then	O
get	O
the	O
same	O
error	O
then	O
?	O

(	O
or	O
try	O
only	O
a	O
couple	O
of	O
rows	O
,	O
some	O
different	O
things	O
to	O
see	O
what	O
triggers	O
the	O
error	O
)	O

possible	O
bug	O
in	O
pandas	O
sort	O
with	O
NaN	O
values	O

If	O
I	O
make	O
a	O
dataframe	B-API
like	O
the	O
following	O
:	O
#CODE	O

basic	O
sorts	O
perform	O
as	O
expected	O
.	O

Sorting	O
on	O
column	O
c	O
appropriately	O
segregates	O
the	O
nan	O
values	O
.	O

Doing	O
a	O
multi-level	O
sort	O
on	O
columns	O
a	O
and	O
b	O
orders	O
them	O
as	O
expected	O
:	O
#CODE	O

But	O
doing	O
a	O
multi-level	O
sort	O
with	O
columns	O
b	O
and	O
c	O
does	O
not	O
give	O
the	O
expected	O
result	O
:	O
#CODE	O

And	O
,	O
in	O
fact	O
,	O
even	O
sorting	O
just	O
on	O
column	O
c	O
but	O
using	O
the	O
multi-level	O
sort	O
nomenclature	O
fails	O
:	O
#CODE	O

I	O
would	O
think	O
that	O
this	O
should	O
have	O
given	O
the	O
exact	O
same	O
result	O
as	O
line	O
133	O
above	O
.	O

Is	O
this	O
a	O
pandas	O
bug	O
or	O
is	O
there	O
something	O
I'm	O
not	O
getting	O
?	O

(	O
FYI	O
,	O
pandas	O
v0.11.0	O
,	O
numpy	O
v1.7.1	O
,	O
python	O
2.7.2.5	O
32bit	O
on	O
windows	O
7	O
)	O

possible	O
duplicate	O
of	O
[	O
Pandas	O
nested	O
sort	O
and	O
NaN	O
]	O
(	O
#URL	O
)	O

I	O
noticed	O
that	O
test.sort	O
(	O
columns=	O
'	O
c	O
'	O
,	O
ascending=False	O
)	O
.sort	B-API
(	O
columns=	O
'	O
b	O
'	O
,	O
ascending=False	O
)	O
does	O
give	O
the	O
correct	O
answer	O
in	O
this	O
case	O
.	O

But	O
I	O
don't	O
know	O
if	O
that's	O
a	O
robust	O
solution	O
.	O

Anyone	O
have	O
a	O
thought	O
?	O

That	O
will	O
only	O
work	O
if	O
pandas	O
sorting	O
algorithm	O
is	O
stable	O
.	O

I	O
didn't	O
find	O
anything	O
in	O
the	O
docs	O
(	O
and	O
numpy's	O
sorting	O
algorithm	O
is	O
not	O
stable	O
by	O
default	O
)	O
.	O

I'm	O
trying	O
to	O
find	O
the	O
source	O
now	O
....	O

@USER	O
checkout	O
this	O
[	O
bug	O
report	O
]	O
(	O
#URL	O
)	O
on	O
this	O

it	O
calls	O
`	O
k.argsort	O
`	O
...	O
where	O
`	O
k	O
`	O
is	O
a	O
column	O
of	O
the	O
dataframe	B-API
--	O
Presumably	O
this	O
is	O
a	O
numpy	O
array	O
which	O
gives	O
the	O
indices	O
to	O
tell	O
pandas	O
how	O
to	O
re-order	O
the	O
data	O
.	O

Unfortunatetly	O
,	O
np.argsort	O
uses	O
(	O
by	O
default	O
)	O
the	O
`	O
quicksort	O
`	O
algorithm	O
which	O
isn't	O
stable	O
,	O
so	O
your	O
solution	O
isn't	O
100%	O
robust	O
.	O

@USER	O
--	O
I'm	O
glad	O
unutbu's	O
on	O
it	O
.	O

:)	O

This	O
is	O
an	O
interesting	O
corner	O
case	O
.	O

Note	O
that	O
even	O
vanilla	O
python	O
doesn't	O
get	O
this	O
"	O
correct	O
"	O
:	O
#CODE	O

The	O
reason	O
here	O
is	O
because	O
`	O
NaN	O
`	O
is	O
neither	O
greater	O
nor	O
less	O
than	O
the	O
other	O
elements	O
--	O
So	O
there	O
is	O
no	O
strict	O
ordering	O
defined	O
.	O

Because	O
of	O
this	O
,	O
`	O
python	O
`	O
leaves	O
them	O
alone	O
.	O

#CODE	O

Pandas	O
must	O
make	O
an	O
explicit	O
check	O
in	O
the	O
single	O
column	O
case	O
--	O
probably	O
using	O
`	O
np.argsort	O
`	O
or	O
`	O
np.sort	O
`	O
as	O
starting	O
at	O
numpy	O
1.4	O
,	O
`	O
np.sort	O
`	O
puts	O
`	O
NaN	O
`	O
values	O
at	O
the	O
end	O
.	O

Thanks	O
for	O
the	O
heads	O
up	O
above	O
.	O

I	O
guess	O
this	O
is	O
already	O
a	O
known	O
issue	O
.	O

One	O
stopgap	O
solution	O
I	O
came	O
up	O
with	O
is	O
:	O
#CODE	O

This	O
method	O
wouldn't	O
work	O
in	O
regular	O
numpy	O
since	O
.min()	B-API
would	O
return	O
nan	O
,	O
but	O
in	O
pandas	O
it	O
works	O
fine	O
.	O

Numpy	O
does	O
have	O
a	O
`	O
nanmin	O
`	O
function	O
I	O
believe	O
:)	O
.	O

pandas	O
:	O
filter	O
intraday	O
df	O
by	O
non-consecutive	O
list	O
of	O
dates	O

I	O
have	O
dataframes	O
of	O
1	O
minute	O
bars	O
going	O
back	O
years	O
(	O
the	O
datetime	O
is	O
the	O
index	O
)	O
.	O

I	O
need	O
to	O
get	O
a	O
set	O
of	O
bars	O
covering	O
an	O
irregular	O
(	O
non-consecutive	O
)	O
long	O
list	O
of	O
dates	O
.	O

For	O
daily	O
bars	O
,	O
I	O
could	O
do	O
something	O
like	O
this	O
:	O
#CODE	O

However	O
if	O
I	O
try	O
that	O
on	O
1	O
minute	O
bar	O
data	O
,	O
it	O
only	O
gives	O
me	O
the	O
bars	O
with	O
time	O
00:00	O
:	O
00	O
,	O
e.g.	O
in	O
this	O
case	O
it	O
gives	O
me	O
two	O
bars	O
for	O
20140101	O
00:00	O
:	O
00	O
and	O
20140205	O
00:00	O
:	O
00	O
.	O

My	O
actual	O
source	O
df	O
will	O
look	O
something	O
like	O
:	O
#CODE	O

Is	O
there	O
any	O
better	O
way	O
to	O
get	O
all	O
the	O
bars	O
for	O
each	O
day	O
in	O
the	O
list	O
than	O
looping	O
over	O
the	O
list	O
?	O

Thanks	O
in	O
advance	O
.	O

One	O
way	O
is	O
to	O
add	O
a	O
date	O
column	O
based	O
on	O
the	O
index	O
#CODE	O

Then	O
use	O
that	O
column	O
when	O
filtering	O
#CODE	O

Thank	O
you	O
-	O
simple	O
and	O
effective	O
.	O

I	O
was	O
actually	O
able	O
to	O
do	O
without	O
the	O
extra	O
column	O
by	O
just	O
directly	O
doing	O
:	O
`	O
df1m	O
[	O
pd.to_datetime	B-API
(	O
df1m.index.date	O
)	O
.isin	B-API
(	O
datelist	O
)]`	O

Constructing	O
3D	O
Pandas	O
DataFrame	B-API

I'm	O
having	O
difficulty	O
constructing	O
a	O
3D	O
DataFrame	B-API
in	O
Pandas	O
.	O

I	O
want	O
something	O
like	O
this	O
#CODE	O

Where	O
`	O
A	O
`	O
,	O
`	O
B	O
`	O
,	O
etc	O
are	O
the	O
top-level	O
descriptors	O
and	O
`	O
start	O
`	O
and	O
`	O
end	O
`	O
are	O
subdescriptors	O
.	O

The	O
numbers	O
that	O
follow	O
are	O
in	O
pairs	O
and	O
there	O
aren't	O
the	O
same	O
number	O
of	O
pairs	O
for	O
`	O
A	O
`	O
,	O
`	O
B	O
`	O
etc	O
.	O

Observe	O
that	O
`	O
A	O
`	O
has	O
four	O
such	O
pairs	O
,	O
`	O
B	O
`	O
has	O
only	O
1	O
,	O
and	O
`	O
C	O
`	O
has	O
3	O
.	O

I'm	O
not	O
sure	O
how	O
to	O
proceed	O
in	O
constructing	O
this	O
DataFrame	B-API
.	O

Modifying	O
this	O
example	O
didn't	O
give	O
me	O
the	O
designed	O
output	O
:	O
#CODE	O

yielded	O
:	O
#CODE	O

Is	O
there	O
any	O
way	O
of	O
breaking	O
up	O
the	O
lists	O
in	O
C	O
into	O
their	O
own	O
columns	O
?	O

EDIT	O
:	O
The	O
structure	O
of	O
my	O
`	O
C	O
`	O
is	O
important	O
.	O

It	O
looks	O
like	O
the	O
following	O
:	O
#CODE	O

And	O
the	O
desired	O
output	O
is	O
the	O
one	O
at	O
the	O
top	O
.	O

It	O
represents	O
the	O
starting	O
and	O
ending	O
points	O
of	O
subsequences	O
within	O
a	O
certain	O
sequence	O
(	O
`	O
A	O
`	O
,	O
`	O
B	O
`	O
.	O
`	O
C	O
`	O
are	O
the	O
different	O
sequences	O
)	O
.	O

Depending	O
on	O
the	O
sequence	O
itself	O
,	O
there	O
are	O
a	O
differing	O
number	O
of	O
subsequences	O
that	O
satisfy	O
a	O
given	O
condition	O
I'm	O
looking	O
for	O
.	O

As	O
a	O
result	O
,	O
there	O
are	O
a	O
differing	O
number	O
of	O
#URL	O
pairs	O
for	O
`	O
A	O
`	O
,	O
`	O
B	O
`	O
,	O
etc	O

First	O
,	O
I	O
think	O
you	O
need	O
to	O
fill	O
C	O
to	O
represent	O
missing	O
values	O
#CODE	O

Then	O
,	O
convert	O
to	O
a	O
numpy	O
array	O
,	O
transpose	B-API
,	O
and	O
pass	O
to	O
the	O
DataFrame	B-API
constructor	O
along	O
with	O
the	O
columns	O
.	O

#CODE	O

My	O
data	O
is	O
organized	O
as	O
a	O
list	O
of	O
lists	O
so	O
that	O
`	O
C	O
=[[	O
...	O
]	O
,	O
[	O
...	O
]	O
,	O
[	O
...	O
]	O
...	O
]`	O
since	O
each	O
nested	O
list	O
has	O
a	O
different	O
length	O
.	O

How	O
could	O
I	O
handle	O
this	O
situation	O
?	O

This	O
implementation	O
is	O
giving	O
me	O
an	O
error	O
because	O
the	O
length	O
of	O
the	O
nested	O
lists	O
within	O
`	O
C	O
`	O
is	O
not	O
equal	O
to	O
length	O
of	O
`	O
A	O
`	O
and	O
`	O
B	O
`	O

What	O
does	O
each	O
list	O
represent	O
,	O
rows	O
or	O
columns	O
?	O

Why	O
are	O
they	O
different	O
lengths	O
?	O

Are	O
the	O
shorter	O
lists	O
supposed	O
to	O
be	O
missing	O
certain	O
elements	O
?	O

See	O
edited	O
answer	O
for	O
a	O
guess	O
.	O

The	O
values	O
in	O
each	O
nested	O
list	O
are	O
the	O
rows	O
and	O
the	O
nested	O
list	O
themselves	O
are	O
the	O
columns	O
.	O

The	O
length	O
of	O
the	O
columns	O
is	O
different	O
because	O
`	O
one	O
`	O
has	O
a	O
different	O
number	O
of	O
#URL	O
pairs	O
than	O
`	O
two	O
`	O

I	O
think	O
we're	O
getting	O
tangled	O
on	O
terminology	O
-	O
can	O
you	O
edit	O
your	O
question	O
to	O
provide	O
some	O
data	O
that	O
matches	O
what	O
you're	O
talking	O
about	O
,	O
and	O
then	O
show	O
what	O
output	O
you	O
want	O
?	O

I	O
added	O
the	O
structure	O
of	O
my	O
`	O
C	O
`	O
to	O
the	O
question	O
.	O

The	O
desired	O
output	O
was	O
what	O
was	O
shown	O
at	O
the	O
top	O
.	O

Thanks	O
for	O
the	O
help	O
!	O

I	O
added	O
an	O
extra	O
couple	O
clarifying	O
sentences	O
to	O
the	O
description	O
as	O
well	O
.	O

I'm	O
open	O
to	O
redesigning	O
`	O
C	O
`'	O
s	O
structure	O
,	O
but	O
I'm	O
not	O
aware	O
of	O
a	O
better	O
way	O
to	O
represent	O
the	O
data	O
.	O

Thanks	O
,	O
try	O
the	O
latest	O
edit	O
.	O

It	O
looks	O
like	O
it	O
worked	O
!	O

Is	O
there	O
no	O
better	O
way	O
of	O
making	O
a	O
2d	O
numpy	O
array	O
from	O
arrays	O
of	O
non-identical	O
lengths	O
?	O

Can't	O
you	O
just	O
use	O
a	O
panel	B-API
?	O

#CODE	O

It's	O
likely	O
that	O
my	O
dataset	O
will	O
be	O
higher	O
dimensional	O
in	O
the	O
future	O
.	O

Isn't	O
panel	B-API
limited	O
to	O
3	O
dimensions	O
?	O

Resample	B-API
error	O
:	O
ValueError	O
:	O
month	O
must	O
be	O
in	O
1	O
..	O

12	O

I	O
have	O
a	O
.csv	O
file	O
that	O
I	O
would	O
like	O
to	O
resample	B-API
at	O
1	O
minute	O
granularity	O
.	O

I	O
do	O
this	O
in	O
the	O
following	O
way	O
:	O
#CODE	O

But	O
I	O
get	O
the	O
following	O
error	O
:	O

ValueError	O
:	O
month	O
must	O
be	O
in	O
1	O
..	O

12	O

I	O
got	O
confused	O
,	O
because	O
when	O
I	O
print	O
main	O
to	O
see	O
how	O
it	O
looks	O
like	O
,	O
the	O
time	O
stamp	O
is	O
in	O
the	O
following	O
format	O
:	O

Timestamp	O

2014-04-15	O
00:00	O
:	O
00	O

Just	O
what	O
it	O
says	O
:	O
you	O
try	O
to	O
create	O
a	O
date	O
from	O
incorrect	O
input	O
.	O

The	O
traceback	O
should	O
tell	O
you	O
where	O
this	O
is	O
happening	O
.	O

With	O
little	O
information	O
to	O
go	O
on	O
,	O
but	O
a	O
decent	O
amount	O
of	O
experience	O
working	O
with	O
dates	O
in	O
programming	O
,	O
I	O
would	O
guess	O
that	O
you	O
likely	O
have	O
one	O
of	O
two	O
problems	O
:	O

Your	O
data	O
is	O
bad	O
and	O
you	O
are	O
going	O
to	O
have	O
to	O
find	O
out	O
where	O
your	O
month	O
is	O
out	O
of	O
bounds	O
.	O

Your	O
timestamp	O
format	O
needs	O
to	O
be	O
specified	O
when	O
you	O
parse	O
it	O
.	O

Your	O
program	O
is	O
reading	O
2014	O
and	O
trying	O
turn	O
it	O
into	O
a	O
month	O
.	O

infer_datetime_format	O
:	O
boolean	O
,	O
default	O
False	O

If	O
True	O
and	O
parse_dates	O
is	O
enabled	O
for	O
a	O
column	O
,	O
attempt	O
to	O
infer	O
the	O
datetime	O
format	O
to	O
speed	O
up	O
the	O
processing	O

Since	O
you	O
believe	O
you	O
have	O
eliminated	O
both	O
of	O
the	O
above	O
possibilities	O
,	O
maybe	O
this	O
will	O
help	O
.	O

All	O
of	O
the	O
examples	O
I	O
see	O
for	O
this	O
function	O
have	O
this	O
format	O
for	O
max	O
in	O
how	O
:	O
#CODE	O

Thank	O
you	O
for	O
your	O
answer	O
.	O

I'm	O
very	O
new	O
to	O
python	O
so	O
I	O
have	O
zero	O
experience	O
.	O

How	O
do	O
I	O
specify	O
the	O
timestamp	O
format	O
?	O

I	O
checked	O
the	O
data	O
and	O
it	O
seems	O
to	O
me	O
that	O
the	O
month	O
is	O
not	O
out	O
of	O
bounds	O
.	O

#URL	O
Try	O
using	O
the	O
intialization	O
part	O
of	O
the	O
documentation	O
examples	O
with	O
your	O
resample	B-API
method	O
.	O

Print	O
their	O
data	O
after	O
it	O
is	O
initialized	O
and	O
compare	O
it	O
to	O
yours	O
for	O
formatting	O
.	O

Thank	O
you	O
!	O

I	O
solved	O
it	O
!	O

No	O
problem	O
.	O

If	O
you	O
are	O
satisfied	O
with	O
an	O
answer	O
,	O
you	O
should	O
mark	O
it	O
as	O
accepted	O
though	O
.	O

Replacing	O
item	O
values	O
in	O
a	O
data	O
frame	O
on	O
certain	O
condition	O
in	O
other	O
columns	O

I	O
have	O
a	O
pandas	O
data	O
frame	O
like	O
this	O
one	O
:	O
#CODE	O

There	O
are	O
two	O
sets	O
of	O
columns	O
:	O
dx	O
and	O
dxpoa	O
.	O

Depending	O
on	O
certain	O
values	O
in	O
dxpoa	O
,	O
I	O
have	O
to	O
keep	O
values	O
in	O
dx	O
or	O
discard	O
it	O
.	O

Foe	O
each	O
value	O
in	O
dx	O
there	O
is	O
a	O
value	O
in	O
corresponding	O
dxpoa	O
in	O
that	O
row	O
.	O

For	O
ex	O
:	O
If	O
dxpoa	O
=	O
[	O
'	O
Y'or	O
'	O
W	O
'	O
or	O
'	O
1	O
'	O
or	O
'	O
E	O
']	O
then	O
keep	O
dx	O
value	O
in	O
corresponding	O
row	O
otherwise	O
discard	O
it	O
or	O
fill	O
it	O
with	O
0	O
.	O

Like	O
dxpoa1	O
,	O
in	O
first	O
row	O
,	O
is	O
'	O
Y	O
'	O
therefore	O
dx1	O
will	O
remain	O
as	O
it	O
is	O
.	O

But	O
dxpoa1	O
,	O
in	O
second	O
row	O
,	O
is	O
'	O
N	O
'	O
therefore	O
corresponding	O
value	O
of	O
dx1	O
,	O
of	O
second	O
row	O
,	O
will	O
become	O
0	O
.	O

Did	O
you	O
already	O
try	O
anything	O
?	O

Are	O
you	O
facing	O
any	O
issues	O
there	O
?	O

@USER	O
:	O
I	O
can	O
change	O
value	O
of	O
a	O
column	O
in	O
a	O
row	O
but	O
don't	O
know	O
how	O
to	O
iterate	O
over	O
row	O
or	O
column	O
.	O

I	O
am	O
trying	O
to	O
use	O
iterrow()	O
function	O
.	O

But	O
handicapped	O
with	O
little	O
knowledge	O
of	O
python	O
.	O

Given	O
a	O
dataframe	B-API
built	O
like	O
so	O
:	O
#CODE	O

Which	O
gives	O
:	O
#CODE	O

Define	O
a	O
function	O
that	O
implements	O
your	O
substitution	O
rules	O
.	O

This	O
is	O
replaces	O
the	O
target	O
column	O
with	O
zero	O
when	O
the	O
value	O
in	O
the	O
reference	O
column	O
is	O
not	O
'	O
Y	O
'	O
,	O
'	O
W	O
'	O
,	O
'	O
1	O
'	O
or	O
'	O
E	O
'	O
,	O
as	O
I	O
understood	O
from	O
your	O
description	O
:	O
#CODE	O

Then	O
iterate	O
over	O
the	O
column	O
names	O
applying	O
subfunc	O
over	O
each	O
row	O
:	O
#CODE	O

Results	O
in	O
the	O
dataframe	B-API
#CODE	O

Here's	O
a	O
vectorized	O
way	O
of	O
looking	O
at	O
it	O
(	O
using	O
@USER	O
'	O
s	O
handy	O
starting	O
frame	O
):	O
#CODE	O

What	O
this	O
does	O
is	O
make	O
an	O
array	O
of	O
True	O
and	O
False	O
for	O
the	O
last	O
N	O
//	O
2	O
columns	O
,	O
with	O
True	O
where	O
the	O
value	O
is	O
in	O
the	O
list	O
and	O
False	O
where	O
it's	O
not	O
(	O
note	O
also	O
that	O
I'm	O
assuming	O
1	O
is	O
the	O
string	O
`"	O
1	O
"`	O
and	O
not	O
the	O
integer	O
`	O
1	O
`)	O
:	O
#CODE	O

Then	O
we	O
can	O
use	O
`	O
where	B-API
`	O
to	O
set	O
the	O
value	O
of	O
the	O
first	O
N	O
//	O
2	O
columns	O
,	O
keeping	O
the	O
values	O
where	O
`	O
keep	O
`	O
is	O
True	O
and	O
otherwise	O
replacing	O
them	O
with	O
0	O
.	O

split	O
dataframe	B-API
into	O
multiple	O
dataframes	O
based	O
on	O
number	O
of	O
rows	O

I	O
have	O
a	O
dataframe	B-API
`	O
df	O
`	O
:	O
#CODE	O

that	O
I	O
need	O
to	O
split	O
into	O
multiple	O
dataframes	O
that	O
will	O
contain	O
every	O
10	O
rows	O
of	O
`	O
df	O
`	O
,	O
and	O
every	O
small	O
dataframe	B-API
I	O
will	O
write	O
to	O
separate	O
file	O
.	O

so	O
I	O
decided	O
create	O
multilevel	O
dataframe	B-API
,	O
and	O
for	O
this	O
first	O
assign	B-API
the	O
index	O
to	O
every	O
10	O
rows	O
in	O
my	O
`	O
df	O
`	O
with	O
this	O
method	O
:	O
#CODE	O

that	O
throws	O
out	O
#CODE	O

so	O
have	O
you	O
idea	O
how	O
to	O
fix	O
it	O
?	O

where	O
my	O
method	O
is	O
wrong	O
?	O

but	O
if	O
you	O
have	O
another	O
approache	O
to	O
split	O
my	O
dataframe	B-API
into	O
multiple	O
dataframes	O
every	O
of	O
which	O
contains	O
10	O
rows	O
of	O
`	O
df	O
`	O
,	O
you	O
are	O
also	O
welcome	O
,	O
cause	O
this	O
approach	O
was	O
just	O
the	O
first	O
I	O
thought	O
about	O
,	O
but	O
I'm	O
not	O
sure	O
that	O
it's	O
the	O
best	O
one	O

You	O
can	O
use	O
a	O
dictionary	O
comprehension	O
to	O
save	O
slices	O
of	O
the	O
dataframe	B-API
in	O
groups	O
of	O
ten	O
rows	O
:	O
#CODE	O

There	O
are	O
many	O
ways	O
to	O
do	O
what	O
you	O
want	O
,	O
your	O
method	O
looks	O
over-complicated	O
.	O

A	O
groupby	B-API
using	O
a	O
scaled	O
index	O
as	O
the	O
grouping	O
key	O
would	O
work	O
:	O
#CODE	O

thanks	O
@USER	O
,	O
it's	O
exactly	O
what	O
I	O
was	O
looking	O
for	O
!!	O

groupby	B-API
common	O
values	O
in	O
two	O
columns	O

I	O
need	O
to	O
extract	O
a	O
common	O
max	O
value	O
from	O
pairs	O
of	O
rows	O
that	O
have	O
common	O
values	O
in	O
two	O
columns	O
.	O

The	O
commonality	O
is	O
between	O
values	O
in	O
columns	O
A	O
and	O
B	O
.	O

Rows	O
0	O
and	O
1	O
are	O
common	O
,	O
2	O
and	O
3	O
,	O
and	O
4	O
is	O
on	O
its	O
own	O
.	O

#CODE	O

The	O
goal	O
is	O
to	O
extract	O
max	O
values	O
,	O
so	O
the	O
end	O
result	O
is	O
:	O
#CODE	O

I	O
could	O
do	O
this	O
if	O
there	O
is	O
a	O
way	O
to	O
assign	B-API
a	O
common	O
,	O
non-repeating	O
key	O
:	O
#CODE	O

Following	O
up	O
with	O
the	O
groupby	B-API
and	O
transform	O
:	O
#CODE	O

Question	O
1	O
:	O

How	O
would	O
one	O
assign	B-API
this	O
common	O
key	O
?	O

Question	O
2	O
:	O

Is	O
there	O
a	O
better	O
way	O
of	O
doing	O
this	O
,	O
skipping	O
the	O
common	O
key	O
step	O

Cheers	O
...	O

You	O
could	O
sort	O
the	O
values	O
in	O
columns	O
`	O
A	O
`	O
and	O
`	O
B	O
`	O
so	O
that	O
for	O
each	O
row	O
the	O
value	O
in	O
`	O
A	O
`	O
is	O
less	O
than	O
or	O
equal	O
to	O
the	O
value	O
in	O
`	O
B	O
`	O
.	O

Once	O
the	O
values	O
are	O
ordered	O
,	O
then	O
you	O
could	O
apply	B-API
`	O
groupby-transform-max	O
`	O
as	O
usual	O
:	O
#CODE	O

yields	O
#CODE	O

The	O
above	O
method	O
will	O
still	O
work	O
even	O
if	O
the	O
values	O
in	O
`	O
A	O
`	O
and	O
`	O
B	O
`	O
are	O
strings	O
.	O

For	O
example	O
,	O
#CODE	O

yields	O
#CODE	O

Thanks	O
@USER	O
.	O

Is	O
there	O
a	O
similar	O
approach	O
with	O
the	O
strings	O
?	O

df	O
=	O
DataFrame	B-API
([[	O
'	O
ab	O
'	O
,	O
'	O
ac	O
'	O
,	O
30	O
]	O
,	O
[	O
'	O
ac	O
'	O
,	O
'	O
ab	O
'	O
,	O
20	O
]	O
,	O

[	O
'	O
cb	O
'	O
,	O
'	O
ca	O
'	O
,	O
15	O
]	O
,	O
[	O
'	O
ca	O
'	O
,	O
'	O
cb	O
'	O
,	O
70	O
]	O
,	O

[	O
'	O
ff	O
'	O
,	O
'	O
zz	O
'	O
,	O
35	O
]]	O
,	O
columns	O
=[	O
'	O
A	O
'	O
,	O
'	O
B	O
'	O
,	O
'	O
Value	O
'])	O

Ah	O
,	O
in	O
that	O
case	O
,	O
geometry	O
is	O
not	O
going	O
to	O
help	O
--	O
unless	O
you	O
convert	O
the	O
values	O
to	O
factors	O
first	O
.	O

But	O
that	O
would	O
probably	O
negate	O
the	O
speed	O
advantage	O
.	O

Used	O
Categorical	B-API
and	O
get_indexer	B-API
.	O

Seem	O
to	O
work	O
.	O

Thanks	O
for	O
help	O
.	O

c	O
=	O
pd.Categorical.from_array	O
(	O
df.A	O
)	O

idx	O
=	O
c.levels	O

df	O
[	O
'	O
A1	O
']	O
=	O
idx.get_indexer	O
(	O
df.A	O
)	O

df	O
[	O
'	O
B1	O
']	O
=	O
idx.get_indexer	O
(	O
df.B	O
)	O

That	O
may	O
not	O
work	O
since	O
`	O
idx.get_indexer	O
(	O
df.B	O
)`	O
will	O
return	O
-1	O
wherever	O
the	O
value	O
in	O
`	O
B	O
`	O
is	O
not	O
in	O
`	O
A	O
`	O
.	O

Thus	O
,	O
`	O
(	O
ff	O
,	O
zz	O
)`	O
and	O
`	O
(	O
ff	O
,	O
qq	O
)`	O
would	O
both	O
be	O
mapped	O
to	O
something	O
like	O
`	O
(	O
4	O
,	O
-1	O
)`	O
.	O

@USER	O
:	O
The	O
first	O
method	O
still	O
works	O
even	O
if	O
the	O
values	O
are	O
strings	O
.	O

It's	O
not	O
the	O
most	O
performant	O
,	O
but	O
there's	O
also	O
`	O
df	O
[	O
"	O
Value	O
"]	O
.groupby	B-API
(	O
map	B-API
(	O
frozenset	O
,	O
df	O
[[	O
"	O
A	O
"	O
,	O
"	O
B	O
"]]	O
.values	B-API
)	O
,	O
sort=False	O
)	O
.transform	O
(	O
max	O
)`	O
.	O

Looking	O
up	O
values	O
from	O
one	O
csv-file	O
in	O
another	O
csv-file	O
,	O
using	O
a	O
third	O
csv-file	O
as	O
map	B-API

I	O
didn't	O
quite	O
figure	O
how	O
to	O
formulate	O
this	O
question	O
,	O
suggestions	O
to	O
improve	O
the	O
title	O
is	O
welcome	O
.	O

I	O
have	O
three	O
files	O
:	O
e_data.csv	O
,	O
t_data.csv	O
and	O
e2d.csv	O
.	O

I	O
want	O
to	O
merge	B-API
`	O
e_id	O
`	O
,	O
`	O
t_id	O
`	O
,	O
`	O
gene_name	O
`	O
and	O
`	O
value	O
`	O
into	O
one	O
file	O
,	O
as	O
represented	O
by	O
desired_result.csv	O
.	O

The	O
naive	O
approach	O
is	O
as	O
follows	O
:	O

For	O
each	O
row	O
in	O
e_data.csv	O
,	O
extract	O
`	O
e_id	O
`	O
and	O
`	O
value	O
`	O
.	O

Check	O
e2t.csv	O
for	O
which	O
`	O
t_id	O
`	O
that	O
corresponds	O
to	O
the	O
given	O
`	O
e_id	O
`	O
.	O

Check	O
t_data.csv	O
for	O
which	O
`	O
gene_name	O
`	O
that	O
corresponds	O
to	O
the	O
given	O
`	O
t_id	O
`	O
.	O

Merge	B-API
them	O
all	O
to	O
one	O
file	O
.	O

Please	O
see	O
the	O
following	O
example	O
for	O
what	O
I'm	O
trying	O
to	O
achieve	O
:	O

e_data.csv	O
:	O
#CODE	O

e2t.csv	O
:	O
#CODE	O

t_data.csv	O
:	O
#CODE	O

desired_result.csv	O
:	O
#CODE	O

There's	O
no	O
limitation	O
to	O
which	O
tools	O
or	O
language	O
to	O
use	O
,	O
but	O
I	O
would	O
prefer	O
to	O
use	O
Python	O
,	O
as	O
that's	O
what	O
I'm	O
most	O
familiar	O
with	O
.	O

R	O
could	O
also	O
be	O
an	O
option	O
.	O

I've	O
already	O
implemented	O
a	O
solution	O
in	O
pure	O
Python	O
,	O
but	O
the	O
datasets	O
are	O
rather	O
large	O
,	O
and	O
I'm	O
hoping	O
something	O
like	O
Pandas	O
or	O
Numpy	O
can	O
speed	O
things	O
up	O
a	O
bit	O
.	O

Thanks	O
!	O

After	O
you	O
load	O
all	O
the	O
csvs	O
using	O
`	O
read_csv	B-API
`	O
you	O
can	O
just	O
iteratively	O
`	O
merge	B-API
`	O
them	O
so	O
long	O
as	O
the	O
column	O
names	O
are	O
consistent	O
:	O
#CODE	O

The	O
above	O
works	O
as	O
by	O
default	O
it	O
will	O
try	O
to	O
merge	B-API
on	O
matching	O
column	O
names	O
and	O
perform	O
an	O
inner	O
merge	B-API
so	O
the	O
column	O
values	O
must	O
match	O
on	O
lhs	O
and	O
rhs	O
.	O

Thanks	O
for	O
the	O
quick	O
reply	O
!	O

I	B-API
get	O
an	O
empty	O
DataFrame	B-API
when	O
trying	O
to	O
do	O
t_data.merge	O
(	O
e2t.merge	O
(	O
e_data	O
))	O
.	O

But	O
the	O
column	O
names	O
are	O
the	O
same	O
as	O
in	O
your	O
answer	O
.	O

Merging	O
e2t	O
and	O
e_data	O
works	O
,	O
however	O
.	O

I'll	O
look	O
into	O
the	O
documentation	O
on	O
merge	B-API
,	O
perhaps	O
I'm	O
doing	O
something	O
wrong	O
.	O

I'd	O
look	O
at	O
the	O
output	O
of	O
each	O
merge	B-API
,	O
also	O
you	O
don't	O
necessarily	O
need	O
to	O
merge	B-API
the	O
entire	O
df	O
to	O
debug	O
this	O
,	O
ie	O
.	O

`	O
e2t.iloc	O
[:	O
5	O
]	O
.merge	O
(	O
e_data.iloc	O
[:	O
5	O
])`	O
will	O
merge	B-API
just	O
the	O
first	O
5	O
lines	O

For	O
some	O
reason	O
,	O
e2t	O
and	O
t_data	O
won't	O
merge	B-API
in	O
my	O
example	O
,	O
which	O
is	O
identical	O
to	O
the	O
data	O
in	O
the	O
question	O
.	O

I	O
shouldn't	O
matter	O
that	O
the	O
the	O
position	O
of	O
the	O
t_id	O
column	O
is	O
different	O
in	O
the	O
two	O
files	O
,	O
right	O
?	O

On	O
my	O
real	O
data	O
it	O
works	O
,	O
however	O
,	O
and	O
I	O
have	O
to	O
say	O
,	O
it	O
is	O
lightning	O
fast	O
compared	O
to	O
my	O
pure	O
python	O
solution	O
.	O

position	O
doesn't	O
care	O
,	O
I'd	O
check	O
the	O
column	O
names	O
to	O
see	O
if	O
the	O
spelling	O
is	O
identical	O
`	O
e2t.columns.tolist()	O
`	O
and	O
`	O
t_date.columns.tolist()	O
`	O

Do	O
all	O
the	O
entries	O
in	O
the	O
DataFrames	O
have	O
to	O
be	O
the	O
same	O
data	O
type	O
?	O

If	O
I'm	O
changing	O
"	O
Gene1	O
"	O
etc	O
to	O
integers	O
(	O
or	O
,	O
conversely	O
,	O
the	O
IDs	O
to	O
strings	O
)	O
,	O
the	O
merge	B-API
goes	O
fine	O
.	O

Not	O
sure	O
,	O
I'd	O
imagine	O
it	O
will	O
either	O
barf	O
or	O
produce	O
a	O
mixed	O
dtype	B-API
column	O

OK	O
,	O
I	O
just	O
tried	O
this	O
,	O
the	O
dtypes	B-API
have	O
to	O
match	O
otherwise	O
you	O
get	O
an	O
empty	O
df	O
,	O
so	O
that's	O
your	O
problem	O

Yes	O
,	O
good	O
to	O
know	O
.	O

Thanks	O
for	O
helping	O
me	O
out	O
!	O

Python	O
-	O
Using	O
a	O
List	O
,	O
Dict	O
Comprehension	O
,	O
and	O
Mapping	O
to	O
Change	O
Plot	O
Order	O

I	O
am	O
relatively	O
new	O
to	O
Python	O
,	O
Pandas	O
,	O
and	O
plotting	O
.	O

I	O
am	O
looking	O
to	O
make	O
a	O
custom	O
sort	O
order	O
in	O
a	O
pandas	O
plot	O
using	O
a	O
list	O
,	O
mapping	O
,	O
and	O
sending	O
them	O
through	O
to	O
the	O
plot	O
function	O
.	O

I	O
am	O
not	O
"	O
solid	O
"	O
on	O
mapping	O
or	O
dict	O
comprehensions	O
.	O

I've	O
looked	O
around	O
a	O
bit	O
on	O
Google	O
and	O
haven't	O
found	O
anything	O
really	O
clear	O
-	O
so	O
any	O
direction	O
to	O
helpful	O
references	O
would	O
be	O
much	O
appreciated	O
.	O

I	O
have	O
a	O
dataframe	B-API
that	O
is	O
the	O
result	O
of	O
a	O
groupby	B-API
:	O
#CODE	O

The	O
numerical	O
column	O
is	O
'	O
Symbol	O
'	O
and	O
the	O
exchange	O
listing	O
is	O
the	O
index	O

When	O
I	O
do	O
a	O
straightforward	O
pandas	O
plot	O
#CODE	O

I	O
get	O
this	O
:	O

The	O
columns	O
are	O
in	O
the	O
order	O
of	O
the	O
rows	O
in	O
the	O
dataframe	B-API
(	O
Amex	O
,	O
NYSE	O
,	O
Nasdaq	O
)	O
but	O
I	O
would	O
like	O
to	O
present	O
,	O
left	O
to	O
right	O
,	O
NYSE	O
,	O
Nasdaq	O
,	O
and	O
Amex	O
.	O

So	O
a	O
"	O
sort	O
"	O
won't	O
work	O
.	O

There	O
is	O
another	O
post	O
:	O

Sorting	O
the	O
Order	O
of	O
Bars	O

that	O
gets	O
at	O
this	O
-	O
but	O
I	O
just	O
couldn't	O
figure	O
it	O
out	O
.	O

I	O
feel	O
like	O
the	O
solution	O
is	O
one	O
step	O
out	O
of	O
my	O
reach	O
.	O

I	O
think	O
this	O
is	O
a	O
very	O
important	O
concept	O
to	O
get	O
down	O
as	O
it	O
would	O
help	O
me	O
considerably	O
in	O
visualizing	O
data	O
where	O
the	O
not-infrequent	O
case	O
of	O
a	O
custom	O
row	O
presentation	O
in	O
a	O
chart	O
is	O
needed	O
.	O

I'm	O
also	O
hoping	O
discussion	O
here	O
could	O
help	O
me	O
better	O
understand	O
mapping	O
as	O
that	O
seems	O
to	O
be	O
very	O
useful	O
in	O
many	O
instances	O
but	O
I	O
just	O
can't	O
seem	O
to	O
find	O
the	O
right	O
on-line	O
resource	O
to	O
explain	O
it	O
clearly	O
.	O

Thank	O
you	O
in	O
advance	O
.	O

The	O
solution	O
to	O
your	O
problem	O
is	O
putting	O
your	O
output	O
dataframe	B-API
into	O
desired	O
order	O
:	O
#CODE	O

As	O
soon	O
as	O
you	O
have	O
the	O
rightly	O
ordered	O
data	O
you	O
can	O
plot	O
it	O
:	O
#CODE	O

Thanks	O
for	O
this	O
-	O
so	O
easy	O
and	O
yet	O
it	O
escaped	O
me	O
.	O

The	O
real	O
lesson	O
here	O
for	O
me	O
was	O
the	O
use	O
of	O
iloc	B-API
.	O

I'm	O
familiar	O
with	O
using	O
it	O
to	O
iterate	O
in	O
two	O
dimensions	O
(	O
e.g.	O
,	O
df.iloc	B-API
[	O
row	O
,	O
column	O
])	O
.	O

I	O
didn't	O
know	O
I	O
could	O
iterate	O
this	O
way	O
where	O
a	O
column	O
is	O
specified	O
.	O

Makes	O
it	O
super	O
simple	O
.	O

Thanks	O
again	O
!	O

Finding	O
index	O
of	O
a	O
pandas	O
DataFrame	B-API
value	O

I	O
am	O
trying	O
to	O
process	O
some	O
.csv	O
data	O
using	O
pandas	O
,	O
and	O
I	O
am	O
struggling	O
with	O
something	O
that	O
I	O
am	O
sure	O
is	O
a	O
rookie	O
move	O
,	O
but	O
after	O
spending	O
a	O
lot	O
of	O
time	O
trying	O
to	O
make	O
this	O
work	O
,	O
I	O
need	O
your	O
help	O
.	O

Essentially	O
,	O
I	O
am	O
trying	O
to	O
find	O
the	O
index	O
of	O
a	O
value	O
within	O
a	O
dataframe	B-API
I	O
have	O
created	O
.	O

#CODE	O

The	O
maxindex	O
variable	O
gets	O
me	O
the	O
answer	O
using	O
idxmax()	B-API
,	O
but	O
what	O
if	O
I	O
am	O
not	O
looking	O
for	O
the	O
index	O
of	O
a	O
max	O
value	O
?	O

What	O
if	O
it	O
is	O
some	O
random	O
value's	O
index	O
that	O
I	O
am	O
looking	O
at	O
,	O
how	O
would	O
I	O
go	O
about	O
it	O
?	O

Clearly	O
.index	B-API
does	O
not	O
work	O
for	O
me	O
here	O
.	O

Thanks	O
in	O
advance	O
for	O
any	O
help	O
!	O

Does	O
this	O
dataframe	B-API
have	O
only	O
1	O
column	O
or	O
do	O
you	O
know	O
which	O
column	O
has	O
the	O
max	O
value	O
?	O

if	O
you	O
know	O
the	O
column	O
then	O
`	O
df.loc	B-API
[	O
df.col	O
==	O
max	O
]	O
.index	B-API
`	O
would	O
return	O
you	O
the	O
index	O

Hi	O
EdChum	O
,	O
thanks	O
for	O
your	O
answer	O
.	O

Doing	O
this	O
gives	O
me	O
the	O
following	O
error	O

`	O
Traceback	O
(	O
most	O
recent	O
call	O
last	O
):	O

File	O
"	O
psims2.py	O
"	O
,	O
line	O
81	O
,	O
in	O

print	O
cd_gross_revenue.loc	O
[	O
cd_gross_revenue.col	O
==	O
max	O
]	O
.index	B-API

File	O
"	O
C	O
:\	O
Python27\lib\	O
site-packages	O
\	O
pandas-0.14.1-py2.7-win32.egg	O
\pandas\core\	O
generic.py	O
"	O
,	O
line	O
18	O

43	O
,	O
in	O
__getattr__	O

(	O
type	O
(	O
self	O
)	O
.__name__	O
,	O
name	O
))	O

AttributeError	O
:	O
'	O
Series	B-API
'	O
object	O
has	O
no	O
attribute	O
'	O
col	O
'`	O

I	O
think	O
you	O
misunderstand	O
,	O
`	O
col	O
`	O
was	O
a	O
generic	O
name	O
for	O
your	O
column	O
of	O
interest	O
so	O
substitute	O
the	O
column	O
name	O
with	O
the	O
one	O
from	O
your	O
df	O
,	O
my	O
question	O
is	O
how	O
many	O
columns	O
does	O
this	O
df	O
have	O
and	O
is	O
there	O
only	O
1	O
or	O
do	O
you	O
know	O
which	O
column	O
has	O
the	O
max	O
value	O
,	O
if	O
so	O
the	O
subsitute	O
`	O
col	O
`	O
with	O
that	O
name	O

Use	O
a	O
boolean	O
mask	O
to	O
get	O
the	O
rows	O
where	O
the	O
value	O
is	O
equal	O
to	O
the	O
random	O
variable	O
.	O

Then	O
use	O
that	O
mask	O
to	O
index	O
the	O
dataframe	B-API
or	O
series	B-API
.	O

Then	O
you	O
would	O
use	O
the	O
`	O
.index	B-API
`	O
field	O
of	O
the	O
pandas	O
dataframe	B-API
or	O
series	B-API
.	O

An	O
example	O
is	O
:	O
#CODE	O

Hi	O
Daniel	O
,	O
thanks	O
for	O
your	O
response	O
,	O
this	O
worked	O
!	O

When	O
you	O
called	O
idxmax	B-API
it	O
returned	O
the	O
key	O
in	O
the	O
index	O
which	O
corresponded	O
to	O
the	O
max	O
value	O
.	O

You	O
need	O
to	O
pass	O
that	O
key	O
to	O
the	O
dataframe	B-API
to	O
get	O
that	O
value	O
.	O

#CODE	O

`	O
s	O
[	O
s	O
==	O
13	O
]`	O

Is	O
all	O
you	O
need	O
.	O

from	O
pandas	O
import	O
Series	B-API
#CODE	O

Yields	O
#CODE	O

filling	O
data	O
gaps	O
with	O
monthly	O
averages	O
(	O
Python	O
)	O

I	O
have	O
a	O
very	O
long	O
time	O
series	B-API
over	O
10	O
years	O
with	O
half-hourly	O
measurements	O
as	O
Csv	O
file	O
.	O

Every	O
now	O
and	O
then	O
the	O
measurement	O
device	O
break	O
down	O
.	O

I	O
want	O
to	O
interpolate	B-API
this	O
gaps	O
either	O
with	O
the	O
monthly	O
average	O
or	O
a	O
moving	O
average	O
(	O
which	O
neglect	O
missing	O
values	O
)	O
.	O

I	O
guess	O
I	O
need	O
a	O
for-loop	O
to	O
do	O
this	O
but	O
I	O
have	O
no	O
Idea	O
how	O
to	O
do	O
this	O
exactly	O
.	O

Could	O
anybody	O
help	O
me	O
?	O

My	O
data	O
look	O
like	O
this	O
:	O
#CODE	O

my	O
current	O
code	O
is	O
:	O
#CODE	O

So	O
i	O
get	O
the	O
daily	O
sum	O
of	O
my	O
evaporation	O
data	O
.	O

I	O
can	O
resample	B-API
the	O
monthly	O
daily	O
average	O
as	O
well	O
but	O
I	O
don't	O
know	O
how	O
to	O
tell	O
Python	O
it	O
need	O
to	O
use	O
for	O
each	O
gap	O
the	O
meanvalue	O
for	O
this	O
specific	O
month	O
.	O

I'd	O
propose	O
to	O
interpolate	B-API
the	O
missing	O
values	O
from	O
the	O
two	O
surrounding	O
values	O
;	O
that	O
should	O
be	O
closer	O
to	O
the	O
real	O
missing	O
value	O
than	O
a	O
monthly	O
average	O
.	O

In	O
case	O
you've	O
got	O
your	O
values	O
in	O
a	O
list	O
(	O
alas	O
,	O
you	O
didn't	O
state	O
your	O
data	O
structures	O
)	O
of	O
tuples	O
`	O
(	O
timestamp	O
,	O
value	O
)`	O
:	O
#CODE	O

This	O
will	O
print	O
#CODE	O

well	O
,	O
I	O
have	O
a	O
csv	O
file	O
with	O
160,000	O
entries	O
.	O

The	O
missing	O
values	O
are	O
somtimes	O
several	O
days	O
(	O
which	O
means	O
one	O
day	O
=	O
48	O
measurements	O
which	O
are	O
missing	O
)	O
,	O
sometimes	O
several	O
weeks	O
.	O

I	O
think	O
this	O
solution	O
is	O
only	O
for	O
a	O
short	O
series	B-API
,	O
isn't	O
it	O
?	O

I	O
don't	O
see	O
a	O
reason	O
why	O
even	O
a	O
million	O
entries	O
should	O
pose	O
a	O
problem	O
.	O

But	O
of	O
course	O
,	O
interpolation	O
won't	O
simulate	O
typical	O
daily	O
curves	O
.	O

You	O
will	O
get	O
a	O
straight	O
line	O
from	O
one	O
existing	O
point	O
to	O
the	O
next	O
only	O
.	O

Okay	O
thanks	O
for	O
your	O
effort	O
!	O

As	O
far	O
as	O
i	O
understand	O
it	O
,	O
it	O
build	O
averages	O
from	O
the	O
last	O
known	O
value	O
and	O
the	O
next	O
value	O
after	O
the	O
gap	O
,	O
correct	O
?	O

I	O
have	O
two	O
Questions	O
:	O
1	O
)	O
is	O
it	O
possible	O
to	O
fill	O
the	O
gap	O
with	O
the	O
monthly	O
average	O
(	O
or	O
30	O
day	O
moving	O
average	O
)	O
,	O
because	O
this	O
would	O
be	O
more	O
accurate	O
?	O

2	O
)	O
My	O
data	O
are	O
originally	O
in	O
a	O
csv	O
file	O
which	O
i	O
need	O
to	O
import	O
to	O
python	O
.	O

If	O
i	O
use	O
"	O
data	O
=	O
pd.read_csv	B-API
(	O
'	O
ET_T_2000.csv	O
'	O
,	O
sep=	O
'	O
;	O
'	O
,	O
parse_dates	O
=[[	O
'	O
date	O
'	O
,	O
'	O
time	O
']])	O
I	O
get	O
an	O
error	O
"	O
KeyError	O
:	O
u'no	O
item	O
named	O
187055	O
'"	O
.	O
the	O
csv	O
file	O
has	O
187057	O
rows	O
.	O

do	O
you	O
have	O
any	O
idea	O
how	O
to	O
fix	O
it	O
?	O

Of	O
course	O
you	O
can	O
compute	O
a	O
monthly	O
average	O
and	O
use	O
this	O
to	O
fill	O
any	O
gap	O
,	O
but	O
that's	O
more	O
complicated	O
(	O
and	O
this	O
is	O
not	O
a	O
site	O
for	O
developing	O
solutions	O
but	O
just	O
for	O
answers	O
to	O
questions	O
)	O
.	O

Concerning	O
the	O
KeyError	O
:	O
sounds	O
like	O
another	O
question	O
you	O
should	O
ask	O
here	O
(	O
do	O
not	O
mix	O
all	O
problems	O
into	O
one	O
question	O
;-)	O

Note	O
:	O
This	O
should	O
be	O
a	O
comment	O
but	O
I	O
don't	O
have	O
the	O
rep	O
for	O
it	O
:)	O

Pandas	O
has	O
a	O
nice	O
'	O
interpolate	B-API
'	O
function	O
on	O
both	O
series	B-API
and	O
dataframes	O
:	O
(	O
#URL	O
)	O
.	O

I'm	O
going	O
to	O
suggest	O
,	O
especially	O
if	O
you	O
have	O
'	O
several	O
days	O
'	O
of	O
missing	O
data	O
,	O
that	O
you	O
just	O
leave	O
the	O
values	O
as	O
NaNs	O
(	O
#URL	O
)	O
.	O

Pandas	O
has	O
really	O
nice	O
support	O
for	O
plots	O
with	O
NA	O
values	O
and	O
seeing	O
a	O
plot	O
that	O
has	O
the	O
correct	O
measurement	O
values	O
and	O
then	O
a	O
'	O
gap	O
'	O
is	O
easily	O
to	O
interpret	O
.	O

Also	O
that	O
approach	O
gives	O
additional	O
information	O
,	O
lets	O
say	O
that	O
you're	O
looking	O
at	O
the	O
plots	O
and	O
you	O
see	O
that	O
the	O
weekends	O
have	O
more	O
gaps	O
than	O
other	O
days	O
,	O
that	O
might	O
indicate	O
the	O
measure	O
device	O
is	O
less	O
stable	O
on	O
the	O
weekend	O
(	O
or	O
whatever	O
)	O
.	O

Yes	O
,	O
thank	O
you	O
.	O

I	O
know	O
this	O
interpolation	O
function	O
of	O
pandas	O
,	O
but	O
it	O
uses	O
only	O
some	O
values	O
before	O
and	O
after	O
the	O
gap	O
.	O
this	O
is	O
too	O
inaccurate	O
.	O
the	O
data	O
represant	O
evaporation	O
in	O
a	O
forest	O
.	O

If	O
for	O
some	O
days	O
the	O
device	O
broke	O
down	O
and	O
it	O
interpolate	B-API
just	O
the	O
values	O
before	O
and	O
after	O
the	O
gap	O
then	O
I	O
don't	O
get	O
the	O
course	O
over	O
the	O
day	O
and	O
it's	O
in	O
general	O
to	O
inaccurate	O
.	O

I	O
agree	O
that	O
for	O
your	O
use	O
case	O
interpolation	O
might	O
not	O
be	O
what	O
you	O
want	O
that's	O
why	O
I	O
suggested	O
just	O
leaving	O
the	O
data	O
as	O
NA	O
values	O
.	O

Pandas	O
has	O
great	O
support	O
for	O
it	O
and	O
functions	O
like	O
'	O
mean	O
,	O
min	O
,	O
max	O
,	O
std	O
'	O
will	O
all	O
work	O
well	O
(	O
do	O
the	O
right	O
thing	O
)	O
with	O
NA	O
values	O
.	O

If	O
you	O
replace	O
the	O
missing	O
values	O
with	O
'	O
monthly	O
average	O
'	O
than	O
all	O
of	O
those	O
stats	O
will	O
be	O
incorrect	O
.	O

With	O
such	O
big	O
gaps	O
of	O
missing	O
values	O
,	O
I	O
guess	O
you're	O
really	O
better	O
off	O
,	O
to	O
keep	O
them	O
as	O
NANs	O
,	O
and	O
adjust	O
your	O
calculations	O
,	O
that	O
they	O
can	O
deal	O
with	O
missing	O
data	O
.	O

Looks	O
like	O
you're	O
doing	O
financial	O
simulations	O
with	O
it	O
,	O
and	O
in	O
the	O
long	O
run	O
,	O
it	O
will	O
always	O
backfire	O
,	O
if	O
you	O
modify	O
the	O
actual	O
raw	O
data	O
.	O

In	O
case	O
you're	O
using	O
Numpy	O
for	O
the	O
calculations	O
,	O
Bottleneck	O
adds	O
a	O
bunch	O
of	O
modified	O
functions	O
,	O
that	O
skip	O
NAN	O
values	O
in	O
arrays	O
,	O
e.g.	O
when	O
calculating	O
means	O
and	O
so	O
on	O
.	O

Better	O
go	O
on	O
with	O
that	O
!	O

Don't	O
whatever	O
you	O
do	O
,	O
change	O
your	O
source	O
data	O
:	O
always	O
keep	O
original	O
data	O
as	O
it	O
is	O
.	O

Any	O
consumers	O
of	O
the	O
data	O
can	O
then	O
decide	O
on	O
a	O
particular	O
interpolation	O
scheme	O
relevant	O
to	O
what	O
the	O
data	O
are	O
showing	O
and	O
what	O
they	O
intend	O
to	O
do	O
with	O
it	O
.	O

In	O
particular	O
,	O
if	O
you	O
are	O
doing	O
some	O
volatility	O
analysis	O
of	O
the	O
data	O
then	O
your	O
putting	O
in	O
interpolated	O
values	O
will	O
have	O
the	O
effect	O
of	O
artificially	O
reducing	O
that	O
volatility	O
.	O

(	O
PS	O
you	O
say	O
you	O
have	O
a	O
Character	O
Separated	O
Value	O
file	O
,	O
but	O
what	O
is	O
the	O
delimiter	O
?	O
)	O

the	O
delimiter	O
is	O
"	O
;	O
"	O
.	O

I	O
need	O
to	O
interpolate	B-API
it	O
cause	O
I	O
want	O
to	O
sum	O
up	O
all	O
values	O
per	O
year	O
to	O
create	O
a	O
water	O
balance	O
.	O

This	O
values	O
are	O
evaporation	O
data	O
.	O

If	O
i	O
build	O
a	O
cumulative	O
sum	O
for	O
a	O
year	O
with	O
so	O
many	O
missing	O
values	O
,	O
the	O
sum	O
will	O
be	O
much	O
to	O
less	O
below	O
the	O
real	O
evaporation	O
.	O

So	O
I	O
need	O
to	O
interpolate	B-API
.	O

(	O
I	O
don't	O
see	O
any	O
delimiters	O
in	O
the	O
file	O
)	O
.	O

In	O
this	O
case	O
then	O
,	O
consider	O
computing	O
your	O
sum	O
(	O
`	O
s	O
`)	O
and	O
the	O
number	O
of	O
recorded	O
times	O
(	O
`	O
t	O
`)	O
and	O
scaling	O
that	O
to	O
the	O
whole	O
year	O
using	O
`	O
s	O
*	O
p	O
/	O
t	O
`	O
where	B-API
`	O
p	O
`	O
is	O
the	O
potential	O
number	O
of	O
records	O
.	O

Maybe	O
you	O
should	O
sum	O
the	O
existing	O
values	O
,	O
count	O
them	O
and	O
then	O
use	O
their	O
mean	O
value	O
to	O
estimate	O
what	O
the	O
sum	O
for	O
a	O
complete	O
year	O
would	O
have	O
been	O
.	O

Interpolating	O
here	O
could	O
grossly	O
falsify	O
the	O
value	O
if	O
data	O
dropout	O
is	O
correlated	O
to	O
extreme	O
values	O
(	O
e	O
.	O
g	O
.	O
if	O
the	O
sensor	O
always	O
breaks	O
when	O
the	O
water	O
is	O
extremely	O
high	O
)	O
because	O
then	O
the	O
interpolation	O
would	O
extend	O
the	O
extreme	O
value	O
to	O
a	O
longer	O
time	O
period	O
than	O
it	O
actually	O
was	O
.	O

how	O
to	O
Join	O
2	O
columns	O
in	O
numpy	O
when	O
they	O
are	O
list	O
of	O
lists	O
?	O

Dataframe	B-API
is	O
:	O
#CODE	O

The	O
convenient	O
way	O
,	O
but	O
slow	O
way	O
,	O
is	O
to	O
use	O
:	O
#CODE	O

I	O
want	O
to	O
achieve	O
this	O
method	O
by	O
`	O
numpy	O
`	O
,	O
for	O
now	O
it	O
is	O
very	O
slow	O
`	O
4	O
seconds	O
`	O
.	O

As	O
`	O
Pandas	O
`	O
use	O
numpy	O
I	O
think	O
I	O
should	O
use	O
numpy	O
without	O
using	O
`	O
Pandas	O
`	O
in	O
order	O
to	O
reduce	O
the	O
overhead	O
.	O

I	O
use	O
`	O
column_stack	O
`	O
but	O
the	O
output	O
is	O
:	O
#CODE	O

[	O
concatenate	O
]	O
(	O
#URL	O
)	O
?	O

This	O
may	O
no	O
be	O
fast	O
,	O
however	O
,	O
`	O
[	O
np.append	O
(	O
x	O
,	O
y	O
)	O
for	O
x	O
,	O
y	O
in	O
zip	O
(	O
a	O
,	O
b	O
)]`	O
works	O
?	O

@USER	O
I'm	O
looking	O
for	O
a	O
fast	O
way	O
as	O
`	O
df	O
[	O
'	O
something	O
']	O
+	O
df	O
[	O
'	O
another_thing	O
']`	O
would	O
solve	O
my	O
problem	O
without	O
code	O
complexity	O
.	O

@USER	O
`	O
concatenate	O
`	O
is	O
not	O
the	O
solution	O
as	O
it	O
merge	B-API
the	O
way	O
I	O
don't	O
want	O
to	O
.	O

I	O
want	O
something	O
similar	O
to	O
`	O
column_stack	O
`	O

The	O
problem	O
with	O
`	O
np.column_stack	O
`	O
is	O
that	O
in	O
`	O
b	O
`	O
you	O
don't	O
have	O
equal-length	O
columns	O
(	O
and	O
thus	O
a	O
`	O
dtype	B-API
`	O
of	O
`	O
object	O
`)	O
.	O

You	O
can	O
do	O
this	O
with	O
`	O
np.concatenate	O
`	O
(	O
or	O
as	O
@USER	O
Galt	O
said	O
in	O
comments	O
`	O
np.append	O
`)	O
;	O
e.g.	O
:	O
#CODE	O

pandas	O
merging	O
of	O
two	O
data-frames	O
using	O
conditions	O

Is	O
there	O
a	O
way	O
in	O
pandas	O
to	O
merge	B-API
two	O
data	O
frames	O
with	O
varying	O
lengths	O
by	O
using	O
a	O
conditional	O
statement	O
?	O

eg	O
:	O
#CODE	O

For	O
example	O
assume	O
there	O
are	O
two	O
data	O
frames	O
,	O
df1	O
and	O
df2	O
with	O
10,000	O
and	O
15,000	O
objects	O
respectively	O
.	O

I	O
want	O
to	O
match	O
common	O
objects	O
between	O
the	O
two	O
catalogues	O
using	O
their	O
x	O
and	O
y	O
position	O
.	O

Objects	O
should	O
be	O
matched	O
between	O
df1	O
and	O
df2	O
such	O
that	O
the	O
matched	O
objects	O
fall	O
within	O
1m	O
radius	O
of	O
each	O
other	O
.	O

Other	O
than	O
x	O
and	O
y	O
,	O
there	O
is	O
nothing	O
common	O
between	O
between	O
the	O
two	O
data	O
frames	O
.	O

The	O
best	O
I	O
can	O
think	O
so	O
far	O
involves	O
a	O
for	O
loop	O
.	O

I'm	O
sure	O
there	O
is	O
a	O
faster	O
and	O
better	O
way	O
to	O
do	O
this	O
?	O

#CODE	O

Do	O
you	O
need	O
to	O
compare	O
each	O
value	O
in	O
df2	O
with	O
each	O
in	O
df1	O
?	O

And	O
what	O
if	O
more	O
than	O
one	O
row	O
fulfil	O
the	O
condition	O
?	O

Would	O
that	O
be	O
possible	O
?	O

You	O
**	O
cannot	O
**	O
join	O
on	O
a	O
conditional	O
statement	O
with	O
any	O
pandas	O
functions	O
.	O

You	O
have	O
to	O
create	O
keys	O
in	O
each	O
dataframe	B-API
that	O
can	O
be	O
joined	O
on	O
.	O

@USER	O
-sc	O
yes	O
for	O
all	O
the	O
questions	O
.	O

so	O
this	O
should	O
be	O
able	O
to	O
identify	O
multiple	O
matches	O
and	O
return	O
them	O
as	O
well	O
.	O

@USER	O
can	O
you	O
please	O
explain	O
how	O
would	O
you	O
do	O
this	O
in	O
pandas	O
then	O
?	O

Apparently	O
there	O
is	O
a	O
non-pandas	O
function	O
that	O
does	O
similarly	O
using	O
a	O
kd-tree	O
:	O
#URL	O

When	O
you	O
need	O
to	O
check	O
each	O
value	O
permutation	O
,	O
this	O
is	O
of	O
O	O
(	O
m*n	O
)	O
complexity	O
(	O
with	O
m	O
and	O
n	O
the	O
number	O
of	O
values	O
per	O
df	O
)	O
.	O

The	O
problem	O
you	O
need	O
to	O
solve	O
is	O
how	O
to	O
get	O
your	O
loops	O
more	O
efficient	O
,	O
but	O
I	O
don't	O
think	O
there	O
is	O
a	O
straight	O
forward	O
way	O
(	O
aka	O
existing	O
function	O
)	O
to	O
do	O
that	O
in	O
pandas	O
.	O

Pandas	O
groupby	B-API
category	O
,	O
rating	O
,	O
get	O
top	O
value	O
from	O
each	O
category	O
?	O

First	O
question	O
on	O
SO	O
,	O
very	O
new	O
to	O
pandas	O
and	O
still	O
a	O
little	O
shaky	O
on	O
the	O
terminology	O
:	O
I'm	O
trying	O
to	O
figure	O
out	O
the	O
proper	O
syntax	O
/	O
sequence	O
of	O
operations	O
on	O
a	O
dataframe	B-API
to	O
be	O
able	O
to	O
group	O
by	O
column	O
B	O
,	O
find	O
the	O
max	O
(	O
or	O
min	O
)	O
corresponding	O
value	O
for	O
each	O
group	O
in	O
column	O
C	O
,	O
and	O
retrieve	O
the	O
corresponding	O
value	O
for	O
that	O
in	O
column	O
A	O
.	O

Suppose	O
this	O
is	O
my	O
dataframe	B-API
:	O
#CODE	O

Using	O
`	O
df.groupby	B-API
(	O
'	O
type	O
')	O
.votes	O
.agg	O
(	O
'	O
max	O
')`	O
returns	O
:	O
#CODE	O

So	O
far	O
,	O
so	O
good	O
.	O

However	O
,	O
I'd	O
like	O
to	O
figure	O
out	O
how	O
to	O
return	O
this	O
:	O
#CODE	O

I've	O
gotten	O
as	O
far	O
as	O
`	O
df.groupby	B-API
([	O
'	O
type	O
'	O
,	O
'	O
votes	O
'])	O
.name	B-API
.agg	O
(	O
'	O
max	O
')`	O
,	O
though	O
that	O
returns	O
#CODE	O

...	O
which	O
is	O
fine	O
for	O
this	O
pretend	O
dataframe	B-API
,	O
but	O
doesn't	O
quite	O
help	O
when	O
working	O
with	O
a	O
much	O
larger	O
one	O
.	O

Thanks	O
very	O
much	O
!	O

If	O
`	O
df	O
`	O
has	O
an	O
index	O
with	O
no	O
duplicate	O
values	O
,	O
then	O
you	O
can	O
use	O
`	O
idxmax	B-API
`	O
to	O
return	O
the	O
index	O
of	O
the	O
maximum	O
row	O
for	O
each	O
group	O
.	O

Then	O
use	O
`	O
df.loc	B-API
`	O
to	O
select	O
the	O
entire	O
row	O
:	O
#CODE	O

If	O
`	O
df.index	O
`	O
has	O
duplicate	O
values	O
,	O
i.e.	O
is	O
not	O
a	O
unique	O
index	O
,	O
then	O
make	O
the	O
index	O
unique	O
first	O
:	O
#CODE	O

then	O
use	O
`	O
idxmax	B-API
`	O
:	O
#CODE	O

If	O
you	O
really	O
need	O
to	O
,	O
you	O
can	O
return	O
`	O
df	O
`	O
to	O
its	O
original	O
state	O
:	O
#CODE	O

but	O
in	O
general	O
life	O
is	O
much	O
better	O
with	O
a	O
unique	O
index	O
.	O

Here	O
is	O
an	O
example	O
showing	O
what	O
goes	O
wrong	O
when	O
`	O
df	O
`	O
does	O
not	O
have	O
a	O
unique	O

index	O
.	O

Suppose	O
the	O
`	O
index	B-API
`	O
is	O
`	O
AABB	O
`	O
:	O
#CODE	O

`	O
idxmax	B-API
`	O
returns	O
the	O
index	O
values	O
`	O
A	O
`	O
and	O
`	O
B	O
`	O
:	O
#CODE	O

But	O
`	O
A	O
`	O
and	O
`	O
B	O
`	O
do	O
not	O
uniquely	O
specify	O
the	O
desired	O
rows	O
.	O

`	O
df.loc	B-API
[	O
...	O
]`	O

returns	O
all	O
rows	O
whose	O
index	O
value	O
is	O
`	O
A	O
`	O
or	O
`	O
B	O
`	O
:	O
#CODE	O

In	O
contrast	O
,	O
if	O
we	O
reset	O
the	O
index	O
:	O
#CODE	O

then	O
`	O
df.loc	B-API
`	O
can	O
be	O
used	O
to	O
select	O
the	O
desired	O
rows	O
:	O
#CODE	O

Thank	O
you	O
very	O
much	O
!	O

Still	O
trying	O
to	O
get	O
the	O
hang	O
of	O
indexes	O
,	O
will	O
read	O
through	O
documentation	O
much	O
more	O
thoroughly	O
.	O

Thanks	O
again	O
!	O
