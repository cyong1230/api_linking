How	O
do	O
you	O
use	O
`	O
pandas.read_sql	B-API
`	O
on	O
an	O
ORM	O
query	O
,	O
like	O
:	O
`	O
session.query	O
(	O
MyORMTable	O
)	O
.limit	O
(	O
100	O
)	O
.all()	B-API
`	O
?	O

`pandas.read_sql_table('MyTable	O
'	O
,	O
MySQLEngine	O
)`	O
see	O
here	O
#URL	O

Well	O
,	O
you	O
can	O
avoid	O
the	O
apply	B-API
and	O
do	O
it	O
vectorized	O
(	O
I	O
think	O
that	O
makes	O
it	O
a	O
bit	O
nicer	O
):	O
#CODE	O

Edit	O
:	O
Jeff	O
points	O
out	O
that	O
a	O
more	O
pandonic	O
way	O
is	O
to	O
make	O
date	O
a	O
`	O
DatetimeIndex	B-API
`	O
and	O
use	O
a	O
Date	O
Offset	O
.	O

So	O
something	O
like	O
:	O
#CODE	O

This	O
is	O
great.Much	O
faster	O
than	O
`	O
apply()	B-API
`	O
Do	O
you	O
know	O
if	O
it	O
is	O
possible	O
to	O
use	O
`	O
datetime64	O
[	O
M	O
]`	O
to	O
find	O
the	O
end	O
of	O
the	O
month	O
instead	O
of	O
the	O
start	O
?	O

a	O
more	O
pandonic	O
way	O
is	O
to	O
treat	O
as	O
an	O
index	O
and	O
use	O
rollback	O
with	O
the	O
appropriate	O
offset	O
see	O
here	O
:	O
#URL	O

gr8	O
u	O
don't	O
actually	O
have	O
make	O
it	O
THE	O
index	O
just	O
set	O
box=False	O
I	O
think	O
on	O
pd.to_datetime	B-API
then	O
subtract	O
the	O
offset	O
(	O
but	O
what	O
u	O
r	O
doing	O
is	O
ok	O
too	O
)	O

[	O
@USER	O
]	O
(	O
#URL	O
)	O
,	O
got	O
it	O
.	O

I	O
had	O
to	O
add	O
`	O
pd.DatetimeIndex()	B-API
`	O
to	O
get	O
it	O
to	O
work	O
.	O

@USER	O
,	O
so	O
offsets	O
can	O
only	O
be	O
applied	O
to	O
an	O
`	O
Index	B-API
`	O
?	O

Offsets	O
work	O
with	O
Timestamp	O
/	O
Timedelta	O
&	O
DatetimeIndex	B-API
/	O
PeriodIndex	O
/	O
TimedeltaIndex	B-API

How	O
can	O
I	O
sum	O
column	O
values	O
that	O
corrispond	O
to	O
a	O
specific	O
value	O
of	O
another	O
column	O
in	O
a	O
pandas	O
DataFrame	B-API
?	O

I	O
have	O
a	O
python	O
pandas	O
DataFrame	B-API
with	O
(	O
item	O
,	O
feature	O
,	O
grade	O
)	O
#CODE	O

and	O
I	O
have	O
to	O
put	O
all	O
the	O
sum	O
in	O
a	O
new	O
DataFrame	B-API
with	O
(	O
item	O
,	O
sumGrade	O
):	O
#CODE	O

How	O
can	O
I	O
do	O
this	O
without	O
using	O
groupby	B-API
and	O
apply	B-API
function	O
?	O

Because	O
I	O
need	O
a	O
good	O
performance	O
in	O
computation	O
.	O

The	O
normal	O
op	O
here	O
is	O
to	O
`	O
groupby	B-API
`	O
on	O
'	O
item	O
'	O
and	O
call	O
`	O
sum	B-API
`	O
on	O
the	O
'	O
grade	O
'	O
column	O
no	O
need	O
to	O
call	O
`	O
apply	B-API
`	O
here	O

You	O
can	O
`	O
groupby	B-API
`	O
on	O
'	O
item	O
'	O
column	O
and	O
then	O
call	O
`	O
sum	B-API
`	O
on	O
the	O
'	O
grade	O
'	O
column	O
,	O
additionally	O
call	O
`	O
reset_index	B-API
`	O
to	O
restore	O
the	O
'	O
item	O
'	O
column	O
back	O
:	O
#CODE	O

Not	O
sure	O
why	O
you	O
don't	O
want	O
to	O
group	O
but	O
you	O
can	O
also	O
set	O
the	O
index	O
to	O
'	O
item	O
'	O
and	O
`	O
sum	B-API
`	O
on	O
the	O
index	O
level	O
:	O
#CODE	O

My	O
data	O
is	O
in	O
a	O
pandas	O
DataFrame	B-API
and	O
the	O
x	O
column	O
is	O
merged2	O
[:	O
-1	O
]	O
.lastqu	O

and	O
the	O
y	O
data	O
column	O
is	O
merged2	O
[:	O
-1	O
]	O
.Units	O

A	O
snippit	O
of	O
the	O
Dataframe	B-API
:	O
the	O
[:	O
-1	O
]	O
eliminates	O
the	O
current	O
period	O
from	O
the	O
data	O
which	O
will	O
subsequently	O
be	O
a	O
projection	O
#CODE	O

What's	O
the	O
problem	O
or	O
the	O
question	O
?	O

From	O
reading	O
the	O
example	O
everything	O
looks	O
correct	O
to	O
me	O
.	O

The	O
remaining	O
question	O
is	O
how	O
I	O
can	O
get	O
a	O
simple	O
regression	O
line	O
using	O
code	O
"	O
similar	O
to	O
the	O
x=	O
np.array	O
sequence	O
above	O
it	O
does	O
not	O
work	O
?	O

(	O
no	O
error	O
,	O
just	O
no	O
line	O
)	O
While	O
I	O
can	O
get	O
the	O
results	O
from	O
the	O
sm.graphics.plot	O

plt.plot	O
(	O
merged2	O
[:	O
-1	O
]	O
.lastqu	O
,	O
merged2	O
[:	O
-1	O
]	O
.Units	O
,	O
'	O
bo	O
')	O

x	O
=	O
np.array	O
([	O
merged2	O
[:	O
-1	O
]	O
.lastqu	O
.min()	B-API
,	O
merged2	O
[:	O
-1	O
]	O
.lastqu	O
.max()	B-API
])	O

plt.plot	O
(	O
x	O
,	O
y	O
,	O
'	O
r-	O
')	O

Merging	O
Pandas	O
DataFrames	O
(	O
LEFT	O
join	O
style	O
)	O
produces	O
strange	O
results	O

I'm	O
trying	O
to	O
merge	O
two	O
Pandas	O
DataFrames	O
with	O
a	O
many-to-one	O
relationship	O
.	O

One	O
DataFrame	B-API
contains	O
a	O
list	O
of	O
drug	O
names	O
and	O
their	O
ingredients	O
(	O
left	O
side	O
)	O
together	O
with	O
some	O
extra	O
information	O
(	O
an	O
irrelevant	O
ID	O
and	O
the	O
company	O
who	O
makes	O
the	O
drugs	O
)	O
,	O
the	O
other	O
contains	O
a	O
list	O
of	O
ingredients	O
and	O
their	O
numeric	O
IDs	O
(	O
right	O
side	O
)	O
.	O

Obviously	O
,	O
what	O
I	O
like	O
to	O
get	O
is	O
a	O
merged	O
DataFrame	B-API
where	O
the	O
ID	O
of	O
each	O
ingredient	O
is	O
listed	O
next	O
to	O
the	O
ingredient	O
name	O
and	O
,	O
considering	O
this	O
is	O
a	O
many-to-one	O
scenario	O
,	O
some	O
IDs	O
will	O
get	O
repeated	O
as	O
some	O
drugs	O
use	O
the	O
same	O
ingredient	O
.	O

A	O
contrived	O
example	O
of	O
both	O
DataFrames	O
(	O
index	O
omitted	O
for	O
brevity	O
):	O

To	O
do	O
the	O
merge	B-API
I	O
simply	O
do	O
the	O
following	O
:	O
#CODE	O

The	O
amount	O
of	O
rows	O
is	O
now	O
limited	O
to	O
how	O
many	O
rows	O
there	O
are	O
in	O
the	O
ingredients	O
(	O
df2	O
)	O
DataFrame	B-API
(	O
3354	O
)	O
.	O

The	O
`	O
info()	O
`	O
on	O
both	O
DataFrames	O
shows	O
me	O
that	O
both	O
`	O
ingredient	O
`	O
columns	O
are	O
of	O
the	O
same	O
type	O
(	O
just	O
to	O
exclude	O
comparison	O
problems	O
):	O

Now	O
for	O
the	O
interesting	O
part	O
.	O

In	O
an	O
attempt	O
to	O
find	O
out	O
where	O
this	O
might	O
be	O
going	O
wrong	O
,	O
I	O
tried	O
to	O
do	O
the	O
merge	B-API
on	O
only	O
a	O
subset	O
of	O
both	O
DataFrames	O
using	O
`	O
head()	O
`	O
:	O
#CODE	O

If	O
I	O
then	O
perform	O
the	O
merge	B-API
I	O
get	O
the	O
desired	O
output	O
without	O
a	O
problem	O
,	O
just	O
only	O
100	O
rows	O
.	O

If	O
I	O
keep	O
increasing	O
the	O
amount	O
of	O
rows	O
to	O
take	O
for	O
`	O
head()	O
`	O
,	O
even	O
up	O
to	O
the	O
point	O
that	O
it	O
is	O
way	O
more	O
than	O
the	O
number	O
of	O
rows	O
present	O
,	O
the	O
output	O
is	O
still	O
perfect	O
.	O

This	O
,	O
for	O
example	O
,	O
works	O
perfectly	O
:	O
#CODE	O

Notice	O
that	O
I	O
include	O
the	O
first	O
100.000	O
rows	O
(	O
only	O
53.000	O
in	O
df1	O
)	O
.	O

This	O
too	O
will	O
give	O
me	O
the	O
expected	O
merge	B-API
output	O
.	O

Interesting	O
bit	O
:	O
after	O
including	O
a	O
large	O
amount	O
of	O
rows	O
with	O
`	O
head	O
(	O
100000	O
)`	O
,	O
only	O
the	O
info	O
for	O
the	O
Drugs	O
DataFrame	B-API
(	O
df1	O
)	O
alters	O
slightly	O
:	O
#CODE	O

Notice	O
the	O
memory	O
usage	O
went	O
up	O
from	O
1.2	O
MB	O
to	O
2.0	O
MB	O
.	O

Yet	O
besides	O
that	O
,	O
they	O
both	O
still	O
are	O
DataFrame	B-API
objects	O
with	O
the	O
same	O
structure	O
as	O
before	O
I	O
pulled	O
them	O
through	O
that	O
operation	O
.	O

What	O
might	O
be	O
going	O
on	O
here	O
?	O

Why	O
does	O
the	O
merge	B-API
produce	O
such	O
strange	O
results	O
,	O
unless	O
I	O
pull	O
the	O
DataFrames	O
through	O
`	O
head()	O
`	O
first	O
?	O

To	O
summarize	O
:	O
Without	O
using	O
`	O
head()	O
`	O
the	O
`	O
merge()	B-API
`	O
output	O
is	O
showing	O
reduced	O
rows	O
(	O
#	O
rows	O
=	O
#	O
right	O
side	O
rows	O
)	O
and	O
each	O
ingredient	O
ID	O
is	O
simply	O
"	O
pasted	O
"	O
on	O
there	O
in	O
a	O
serial	O
manner	O
.	O

However	O
,	O
if	O
I	O
do	O
use	O
`	O
head()	O
`	O
,	O
even	O
with	O
a	O
great	O
enough	O
number	O
to	O
include	O
all	O
rows	O
on	O
both	O
sides	O
,	O
the	O
join	O
works	O
as	O
expected	O
.	O

Full	O
data	O
:	O
For	O
the	O
ones	O
wanting	O
to	O
try	O
with	O
the	O
exact	O
same	O
data	O
,	O
I	O
created	O
two	O
gists	O
on	O
Github	O
that	O
contain	O
the	O
full	O
data	O
(	O
dictionary	O
form	O
)	O
for	O
both	O
the	O
drugs	O
and	O
the	O
ingredients	O
DataFrame	B-API
mentioned	O
above	O
.	O

They	O
are	O
exported	O
using	O
`	O
DataFrame.to_dict()	B-API
`	O
and	O
can	O
be	O
imported	O
/	O
transformed	O
back	O
into	O
a	O
DataFrame	B-API
using	O
`	O
DataFrame.from_dict	B-API
(	O
data	O
)`	O
.	O

how	O
to	O
drop	O
dataframe	B-API
in	O
pandas	O
?	O

But	O
I	O
want	O
to	O
drop	O
the	O
whole	O
dataframe	B-API
created	O
in	O
pandas	O
.	O

like	O
in	O
R	O
:	O
rm	O
(	O
dataframe	B-API
)	O
or	O
in	O
SQL	O
:	O
drop	O
table	O

@USER	O
My	O
scenario	O
is	O
the	O
dataset	O
I	O
am	O
working	O
with	O
is	O
huge	O
.	O

So	O
once	O
loaded	O
(	O
with	O
pd.read_csv()	B-API
)	O
the	O
system	O
is	O
getting	O
slow	O
.	O

And	O
after	O
splitting	O
and	O
some	O
processing	O
,	O
I	O
want	O
to	O
remove	O
the	O
bigger	O
dataset	O
and	O
work	O
with	O
the	O
small	O
one.That's	O
why	O
I	O
need	O
.	O

How	O
do	O
you	O
clean	O
and	O
forward	O
fill	O
a	O
multiple	O
day	O
1	O
minute	O
time	O
series	B-API
with	O
pandas	O
?	O

Some	O
of	O
the	O
minutes	O
in	O
the	O
time	O
series	B-API
are	O
missing	O
:	O

With	O
pandas	O
,	O
how	O
do	O
I	O
forward	O
fill	O
the	O
series	B-API
so	O
every	O
minute	O
is	O
present	O
?	O

I	O
should	O
look	O
like	O
this	O
:	O
#CODE	O

So	O
I	O
copied	O
your	O
first	O
4	O
lines	O
into	O
a	O
dataframe	B-API
:	O
#CODE	O

The	O
key	O
thing	O
here	O
is	O
you	O
must	O
have	O
a	O
datetimeindex	B-API
,	O
if	O
you	O
want	O
to	O
keep	O
it	O
as	O
a	O
column	O
then	O
you	O
can	O
just	O
set	O
`	O
drop=False	O
`	O
in	O
`	O
set_index	B-API
`	O
.	O

Just	O
creating	O
the	O
dataframe	B-API
:	O
#CODE	O

Reindexing	O
and	O
filling	O
the	O
holes	O
,	O
then	O
forward	O
filling	O
the	O
resulting	O
NaN	O
values	O
,	O
then	O
dropping	O
all	O
times	O
outside	O
of	O
9:30	O
AM	O
to	O
4:00	O
PM	O
:	O
#CODE	O

First	O
,	O
reindex	B-API
the	O
dataframe	B-API
so	O
that	O
your	O
index	O
corresponds	O
to	O
your	O
starting	O
date	O
/	O
time	O
through	O
your	O
ending	O
date	O
/	O
time	O
with	O
a	O
frequency	O
of	O
1	O
minute	O
:	O
#CODE	O

This	O
will	O
create	O
a	O
lot	O
of	O
NaN	O
values	O
where	O
the	O
new	O
index	O
didn't	O
line	O
up	O
with	O
the	O
old	O
one	O
.	O

We	O
fill	O
this	O
with	O
ffill	B-API
(	O
forward	O
fill	O
)	O
,	O
though	O
there	O
are	O
other	O
options	O
out	O
there	O
:	O
#CODE	O

Because	O
.time()	B-API
doesn't	O
take	O
9.5	O
and	O
the	O
documentation	O
is	O
kind	O
of	O
sparse	O
,	O
I	O
just	O
created	O
a	O
datetime	O
object	O
with	O
the	O
time	O
value	O
set	O
to	O
9:30	O
AM	O
and	O
then	O
used	O
.time()	B-API
to	O
grab	O
this	O
.	O

There's	O
a	O
better	O
way	O
,	O
I'm	O
sure	O
.	O

I	O
am	O
pulling	O
this	O
data	O
from	O
a	O
Mongo	O
database	O
,	O
and	O
cleaning	O
it	O
using	O
pandas	O
(	O
converting	O
'	O
price	O
'	O
to	O
float	O
,	O
'	O
rank	O
'	O
to	O
int	O
,	O
string	O
date	O
to	O
datetime	O
,	O
etc	O
.	O
)	O
.	O

The	O
goal	O
I	O
had	O
in	O
mind	O
was	O
to	O
create	O
a	O
series	B-API
of	O
additional	O
dataframes	O
/	O
tables	O
(	O
one	O
for	O
each	O
'	O
rank	O
')	O
and	O
link	O
these	O
to	O
the	O
main	O
table	O
in	O
a	O
HDFStore	O
for	O
each	O
day	O
.	O

Something	O
like	O
:	O
#CODE	O

Before	O
it's	O
cleaned	O
(	O
and	O
to	O
some	O
extent	O
,	O
after	O
)	O
the	O
data	O
is	O
quite	O
large	O
.	O

Each	O
day	O
of	O
observation	O
is	O
~	O
1.5M	O
rows	O
,	O
and	O
uses	O
around	O
9GB	O
of	O
RAM	O
(	O
I	O
have	O
32GB	O
limit	O
)	O
when	O
imported	O
into	O
a	O
pandas	O
DataFrame	B-API
.	O

The	O
number	O
of	O
rows	O
in	O
the	O
total	O
data	O
set	O
is	O
~800M	O
.	O

Any	O
advice	O
or	O
help	O
will	O
be	O
greatly	O
appreciated	O
.	O

I	O
understand	O
this	O
is	O
a	O
broad	O
question	O
,	O
but	O
I	O
am	O
in	O
somewhat	O
unfamiliar	O
territory	O
.	O

Python	O
pandas	O
dataframe	B-API
max	O
in	O
a	O
group	O
based	O
on	O
conditions	O
on	O
other	O
columns	O

I'm	O
not	O
sure	O
if	O
this	O
question	O
has	O
been	O
asked	O
before	O
.	O

In	O
a	O
pandas	O
dataframe	B-API
I	O
have	O
data	O
like	O
#CODE	O

Then	O
,	O
groupby	B-API
the	O
device	O
type	O
,	O
and	O
using	O
the	O
cumulative	O
sum	O
of	O
the	O
`	O
new_sample	O
`	O
column	O
,	O
create	O
a	O
counter	O
for	O
which	O
trial	O
of	O
each	O
device	O
each	O
row	O
represents	O
.	O

#CODE	O

Using	O
Pandas	O
to	O
create	O
DataFrame	B-API
with	O
Series	B-API
,	O
resulting	O
in	O
memory	O
error	O

I'm	O
using	O
Pandas	O
library	O
for	O
remote	O
sensing	O
time	O
series	B-API
analysis	O
.	O

Eventually	O
I	O
would	O
like	O
to	O
save	O
my	O
DataFrame	B-API
to	O
csv	O
by	O
using	O
chunk-sizes	O
,	O
but	O
I	O
run	O
into	O
a	O
little	O
issue	O
.	O

My	O
code	O
generates	O
6	O
NumPy	O
arrays	O
that	O
I	O
convert	O
to	O
Pandas	O
Series	B-API
.	O

Each	O
of	O
these	O
Series	B-API
contains	O
a	O
lot	O
of	O
items	O
#CODE	O

I	O
would	O
like	O
to	O
add	O
the	O
Series	B-API
into	O
a	O
Pandas	O
DataFram	O
(	O
df	O
)	O
so	O
I	O
can	O
save	O
them	O
chunk	O
by	O
chunk	O
to	O
a	O
csv	O
file	O
.	O

#CODE	O

Any	O
suggestions	O
?	O

Is	O
it	O
possible	O
to	O
fill	O
the	O
Pandas	O
DataFrame	B-API
chunk	O
by	O
chunk	O
?	O

Can	O
you	O
make	O
a	O
DataFrame	B-API
from	O
a	O
single	O
column	O
:	O
pd.DataFrane({'tmax	O
'	O
:	O
pd.Series	B-API
(	O
tmaxSeries	O
)	O
}	O
)	O
?	O

create	O
a	O
frame	O
with	O
the	O
first	O
series	B-API
,	O
and	O
add	O
them	O
sequentially	O
,	O
e.g.	O
``	O
df	O
=	O
DataFrame({'prcp	O
'	O
:	O
pd.Series	B-API
(	O
prcpSeries	O
)	O
}	O
);	O
df['tmax	O
']	O
=	O
pd.Series	B-API
(	O
tmaxSeries	O
)``	O
.	O

You	O
should	O
probably	O
write	O
it	O
to	O
a	O
HDF5	O
in	O
any	O
event	O
,	O
see	O
:	O
#URL	O

When	O
you	O
are	O
passing	O
a	O
dict	O
(	O
even	O
if	O
the	O
values	O
are	O
Series	B-API
)	O
,	O
I	O
think	O
copies	O
are	O
made	O
.	O

If	O
you	O
do	O
iteratively	O
(	O
and	O
argument	O
is	O
a	O
series	B-API
)	O
,	O
then	O
no	O
copy	O

@USER	O
I've	O
cobbled	O
something	O
together	O
...	O

I	O
think	O
I	O
prefer	O
using	O
an	O
outer	O
concat	B-API
tbh	O
.	O

If	O
you	O
know	O
each	O
of	O
these	O
are	O
the	O
same	O
length	O
then	O
you	O
could	O
create	O
the	O
DataFrame	B-API
directly	O
from	O
the	O
array	O
and	O
then	O
append	O
each	O
column	O
:	O
#CODE	O

Note	O
:	O
you	O
can	O
also	O
use	O
the	O
`	O
to_frame	B-API
`	O
method	O
(	O
which	O
allows	O
you	O
to	O
(	O
optionally	O
)	O
pass	O
a	O
name	O
-	O
which	O
is	O
useful	O
if	O
the	O
Series	B-API
doesn't	O
have	O
one	O
):	O
#CODE	O

However	O
,	O
if	O
they	O
are	O
variable	O
length	O
then	O
this	O
will	O
lose	O
some	O
data	O
(	O
any	O
arrays	O
which	O
are	O
longer	O
than	O
`	O
prcpSeries	O
`)	O
.	O

An	O
alternative	O
here	O
is	O
to	O
create	O
each	O
as	O
a	O
DataFrame	B-API
and	O
then	O
perform	O
an	O
outer	O
join	O
(	O
using	O
`	O
concat	B-API
`	O
):	O
#CODE	O

Thanks	O
Andy	O
and	O
Jeff	O
!	O

I've	O
to	O
use	O
the	O
first	O
method	O
with	O
appending	O
each	O
column	O
,	O
since	O
the	O
second	O
approach	O
gets	O
an	O
Memory	O
Error	O
at	O
the	O
line	O
of	O
df	O
=	O
pd.concat	B-API
(	O
etc	O
.	O
)	O
.	O

I	O
know	O
the	O
series	B-API
with	O
the	O
longest	O
length	O
and	O
will	O
use	O
that	O
one	O
to	O
initialise	O
the	O
DataFrame	B-API
.	O

One	O
caveat	O
:	O
I'm	O
using	O
pandas	O
version	O
0.14.1	O
and	O
when	O
I	O
try	O
to	O
coerce	O
a	O
Series	B-API
object	O
to	O
a	O
DataFrame	B-API
object	O
,	O
if	O
I	O
specify	O
`	O
columns	O
=	O
[	O
'	O
my_column_name	O
']`	O
in	O
the	O
`	O
pandas.DataFrame()	B-API
`	O
call	O
,	O
the	O
resulting	O
object	O
is	O
an	O
empty	O
DataFrame	B-API
.	O

When	O
I	O
dropped	O
the	O
columns	O
argument	O
,	O
the	O
resulting	O
DataFrame	B-API
was	O
as	O
expected	O
.	O

@USER	O
Thanks	O
for	O
mentioning	O
this	O
,	O
perhaps	O
it's	O
cleaner	O
to	O
use	O
the	O
`	O
to_frame	B-API
`	O
method	O
here	O
(	O
I'm	O
not	O
sure	O
this	O
was	O
available	O
when	O
I	O
wrote	O
the	O
original	O
answer	O
)	O
-	O
I've	O
updated	O
this	O
answer	O
to	O
mention	O
that	O
.	O

I	O
will	O
have	O
a	O
look	O
to	O
see	O
if	O
this	O
no	O
longer	O
works	O
in	O
0.14	O
+	O
,	O
I	O
will	O
have	O
a	O
check	O
later	O
to	O
see	O
.	O

Python	O
pandas	O
,	O
how	O
to	O
only	O
plot	O
a	O
DataFrame	B-API
that	O
actually	O
have	O
the	O
datapoint	O
and	O
leave	O
the	O
gap	O
out	O

I	O
have	O
a	O
DataFrame	B-API
with	O
intraday	O
data	O
indexed	O
with	O
DatetimeIndex	B-API
#CODE	O

df3.plot()	O

One	O
way	O
to	O
do	O
this	O
is	O
to	O
`	O
resample	B-API
`	O
(	O
hourly	O
)	O
before	O
you	O
plot	O
:	O
#CODE	O

Pandas	O
Intraday	O
Time	O
Series	B-API
plots	O

Pandas	O
:	O
Efficient	O
spatial	O
analysis	O
using	O
adjacency	O
matrix	O

I	O
have	O
a	O
very	O
large	O
DataFrame	B-API
with	O
coordinates	O
.	O

Let's	O
take	O
the	O
following	O
example	O
:	O
#CODE	O

Based	O
on	O
this	O
DataFrame	B-API
I	O
need	O
to	O
calculate	O
the	O
distance	O
between	O
points	O
various	O
times	O
.	O

Often	O
the	O
points	O
which	O
need	O
to	O
be	O
compared	O
with	O
each	O
other	O
are	O
the	O
same	O
,	O
for	O
example	O
when	O
I	O
want	O
to	O
calculate	O
the	O
distance	O
from	O
Carl	O
to	O
all	O
other	O
Buyers	O
each	O
day	O
.	O

#CODE	O

To	O
do	O
this	O
efficiently	O
and	O
not	O
having	O
to	O
calculate	O
the	O
same	O
distances	O
multiple	O
times	O
,	O
I	O
am	O
hoping	O
to	O
build	O
an	O
adjacency	O
matrix	O
with	O
the	O
Buyers	O
and	O
their	O
distances	O
to	O
each	O
other	O
.	O

As	O
a	O
result	O
I	O
could	O
query	O
this	O
matrix	O
instead	O
of	O
the	O
doing	O
the	O
distance	O
calculations	O
.	O

What	O
would	O
you	O
recommend	O
as	O
data	O
structure	O
for	O
this	O
adjacency	O
matrix	O
and	O
how	O
would	O
you	O
implement	O
the	O
lookup	O
so	O
that	O
it	O
is	O
faster	O
than	O
a	O
dedicated	O
distance	O
computation	O
.	O

I	O
would	O
deeply	O
appreciate	O
any	O
help	O
.	O

It	O
looks	O
like	O
you	O
want	O
an	O
adjacency	O
matrix	O
for	O
each	O
day	O
.	O

I	O
suggest	O
a	O
dictionary	O
in	O
which	O
keys	O
are	O
dates	O
and	O
values	O
are	O
DataFrames	O
,	O
where	O
each	O
axis	O
lists	O
the	O
customers	O
.	O

Alternatively	O
you	O
could	O
look	O
into	O
the	O
pandas	O
'	O
Panel	B-API
objects	O
.	O

Hi	O
Dan	O
,	O
I	O
am	O
afraid	O
you	O
got	O
me	O
wrong	O
.	O

My	O
idea	O
is	O
rather	O
that	O
I	O
have	O
an	O
adjacency	O
matrix	O
between	O
the	O
Buyers	O
which	O
has	O
in	O
each	O
cell	O
the	O
distance	O
between	O
them	O
.	O

I	O
updated	O
my	O
problem	O
description	O
.	O

I	O
would	O
deeply	O
appreciate	O
to	O
get	O
your	O
opinion	O
about	O
that	O
.	O

Lets	O
say	O
that	O
I	O
have	O
a	O
dataframe	B-API
(	O
using	O
pandas	O
data	O
analysis	O
library	O
)	O
that	O
looks	O
like	O
so	O
:	O
#CODE	O

And	O
I	O
want	O
to	O
get	O
the	O
dataframe	B-API
to	O
look	O
like	O
this	O
:	O
#CODE	O

Note	O
that	O
`"	O
Unnamed	O
:	O
1	O
"`	O
sounds	O
suspiciously	O
like	O
the	O
name	O
`	O
pd.read_csv	B-API
`	O
assigns	O
to	O
the	O
column	O
if	O
the	O
header	O
row	O
lacked	O
a	O
column	O
name	O
.	O

In	O
that	O
case	O
,	O
instead	O
of	O
patching	O
up	O
the	O
result	O
as	O
shown	O
above	O
,	O
you	O
might	O
be	O
able	O
to	O
fix	O
the	O
problem	O
with	O
`	O
skiprows=1	O
`	O
or	O
`	O
header=1	O
`	O
instead	O
.	O

`	O
skiprows=1	O
`	O
would	O
cause	O
`	O
pd.read_csv	B-API
`	O
to	O
skip	O
the	O
first	O
row	O
and	O
thus	O
read	O
the	O
headers	O
(	O
column	O
names	O
)	O
from	O
the	O
second	O
row	O
automatically	O
.	O

I	O
am	O
new	O
to	O
python	O
/	O
pandas	O
,	O
and	O
encountering	O
an	O
issue	O
I	O
can't	O
really	O
make	O
sense	O
of	O
.	O

I	O
have	O
Twitter	O
data	O
in	O
csv	O
files	O
which	O
contain	O
1000	O
tweets	O
each	O
.	O

When	O
I	O
read	O
the	O
files	O
into	O
a	O
dataframe	B-API
,	O
all	O
works	O
fine	O
(	O
it	O
just	O
takes	O
a	O
lot	O
of	O
time	O
to	O
read	O
the	O
files	O
):	O
#CODE	O

This	O
dataframe	B-API
,	O
which	O
contains	O
around	O
6	O
million	O
tweets	O
,	O
can	O
be	O
manipulated	O
at	O
a	O
reasonable	O
speed	O
.	O

However	O
,	O
if	O
it	O
is	O
saved	O
#CODE	O

even	O
the	O
most	O
simple	O
operations	O
,	O
e.g.	O
finding	O
a	O
string	O
in	O
a	O
tweet	O
(	O
which	O
works	O
perfectly	O
fine	O
in	O
the	O
initial	O
dataframe	B-API
!	O
)	O
,	O
is	O
extremely	O
slow	O
(	O
1-2s	O
per	O
row	O
)	O
.	O

The	O
problem	O
seems	O
to	O
be	O
particularly	O
distinct	O
when	O
I	O
loop	O
through	O
all	O
rows	O
.	O

Is	O
there	O
any	O
reason	O
for	O
this	O
?	O

Would	O
it	O
help	O
dumping	O
all	O
data	O
into	O
a	O
database	O
and	O
reading	O
it	O
from	O
there	O
?	O

Here	O
is	O
the	O
df.info()	B-API
of	O
the	O
initial	O
file	O
(	O
stopped	O
after	O
50	O
files	O
):	O
#CODE	O

Is	O
there	O
reason	O
you	O
don't	O
just	O
store	O
the	O
dfs	O
in	O
a	O
list	O
and	O
then	O
concat	B-API
them	O
all	O
?	O

Also	O
this	O
line	O
is	O
unnecessary	O
:	O
`	O
dftemp2	O
=	O
pd.concat	B-API
([	O
dftemp1	O
[	O
"	O
source	O
"]	O
,	O
dftemp1	O
[	O
"	O
text	O
"]	O
,	O
dftemp1	O
[	O
"	O
timestamp_ms	O
"]]	O
,	O
axis=1	O
)`	O
you	O
can	O
specify	O
the	O
columns	O
of	O
interest	O
in	O
the	O
params	O
to	O
`	O
read_csv	B-API
`	O
,	O
`	O
usecols	O
`	O
:	O
#URL	O

Can	O
you	O
show	O
`	O
df.info()`	O
before	O
and	O
after	O
writing	O
/	O
reading	O
it	O
to	O
csv	O
?	O

@USER	O
added	O
df.info()	B-API

@USER	O
Thanks	O
,	O
changed	O
the	O
code	O
.	O

Aside	O
from	O
speeding	O
up	O
the	O
concat	B-API
process	O
from	O
2	O
hours	O
to	O
10	O
minutes	O
,	O
it	O
also	O
solved	O
the	O
problem	O
with	O
the	O
loaded	O
csv	O
-	O
it	O
runs	O
at	O
normal	O
speed	O
now	O
(	O
for	O
whatever	O
reason	O
)	O
:)	O

@USER	O
would	O
you	O
like	O
me	O
to	O
post	O
an	O
answer	O
,	O
I	O
can't	O
explain	O
everything	O
but	O
concatenating	O
repeatedly	O
was	O
always	O
going	O
to	O
be	O
slower	O
than	O
building	O
a	O
single	O
list	O
upfront	O
first	O
and	O
then	O
concatenating	O
all	O
at	O
once	O

I'm	O
pretty	O
sure	O
that	O
repeated	O
concatenations	O
make	O
a	O
copy	O
of	O
both	O
the	O
source	O
and	O
target	O
dataframes	O
each	O
time	O
.	O

This	O
gets	O
big	O
and	O
slow	O
as	O
the	O
target	O
gets	O
big	O
.	O

Concatenating	O
the	O
single	O
list	O
seems	O
to	O
have	O
some	O
deep	O
magic	O
that	O
makes	O
it	O
faster	O
.	O

I	O
suspect	O
it	O
has	O
to	O
do	O
with	O
allocating	O
memory	O
so	O
that	O
there's	O
fewer	O
copies	O
.	O

I	O
have	O
a	O
bunch	O
of	O
CSV	O
files	O
with	O
4	O
line	O
headers	O
.	O

In	O
these	O
files	O
,	O
I	O
want	O
to	O
change	O
the	O
values	O
in	O
the	O
sixth	O
column	O
based	O
on	O
the	O
values	O
in	O
the	O
second	O
column	O
.	O

For	O
example	O
,	O
if	O
the	O
second	O
column	O
,	O
under	O
the	O
name	O
`	O
PRODUCT	B-API
`	O
is	O
`	O
Banana	O
`	O
,	O
I	O
would	O
want	O
to	O
change	O
the	O
value	O
in	O
the	O
same	O
row	O
under	O
`	O
TIME	B-API
`	O
to	O
`	O
10m	O
`	O
.	O

If	O
the	O
the	O
product	O
was	O
`	O
Apple	O
`	O
I	O
would	O
want	O
the	O
time	O
to	O
be	O
`	O
15m	O
`	O
and	O
so	O
on	O
.	O

#CODE	O

Assuming	O
that	O
your	O
data	O
is	O
in	O
a	O
Pandas	O
DataFrame	B-API
and	O
looks	O
something	O
like	O
this	O
:	O
#CODE	O

The	O
code	O
first	O
loops	O
through	O
the	O
rows	O
of	O
the	O
dataframe	B-API
column	O
"	O
PRODUCT	O
"	O
,	O
with	O
the	O
row	O
value	O
stored	O
as	O
i	O
and	O
the	O
row-number	O
stored	O
as	O
numi	O
.	O

It	O
then	O
uses	O
if	O
statements	O
to	O
identify	O
the	O
different	O
levels	O
of	O
interest	O
in	O
the	O
Product	O
column	O
.	O

For	O
those	O
rows	O
with	O
the	O
levels	O
of	O
interest	O
(	O
eg	O
"	O
Banana	O
"	O
or	O
"	O
Apple	O
")	O
,	O
it	O
uses	O
the	O
row-numbers	O
to	O
change	O
the	O
value	O
of	O
another	O
column	O
in	O
the	O
same	O
row	O
.	O

There	O
are	O
lots	O
of	O
ways	O
to	O
do	O
this	O
,	O
and	O
depending	O
on	O
the	O
size	O
of	O
your	O
data	O
and	O
the	O
number	O
of	O
levels	O
(	O
in	O
this	O
case	O
"	O
Products	O
")	O
you	O
want	O
to	O
change	O
,	O
this	O
isn't	O
necessarily	O
the	O
most	O
efficient	O
way	O
to	O
do	O
this	O
.	O

But	O
since	O
you're	O
a	O
beginner	O
,	O
this	O
will	O
probably	O
be	O
a	O
good	O
basic	O
way	O
of	O
doing	O
it	O
for	O
you	O
to	O
start	O
with	O
.	O

fyi	O
:	O
you	O
shouldn't	O
do	O
this	O
type	O
of	O
chained	O
assignment	O
,	O
see	O
here	O
:	O
#URL	O
also	O
iterating	O
over	O
a	O
frame	O
is	O
not	O
efficient	O
,	O
this	O
is	O
an	O
easily	O
vectorized	O
problem	O

You	O
can	O
do	O
this	O
with	O
a	O
combination	O
of	O
`	O
groupby	B-API
`	O
,	O
`	O
replace	B-API
`	O
and	O
a	O
`	O
dict	O
`	O
#CODE	O

`df.groupby('fruits	O
')`	O
splits	O
the	O
`	O
DataFrame	B-API
`	O
into	O
subsets	O
(	O
which	O
are	O
`	O
DataFrame	B-API
`	O
s	O
or	O
`	O
Series	B-API
`	O
objects	O
)	O
using	O
the	O
values	O
of	O
the	O
`	O
fruits	O
`	O
column	O
.	O

The	O
`	O
apply	B-API
`	O
method	O
applies	O
a	O
function	O
to	O
each	O
of	O
the	O
aforementioned	O
subsets	O
and	O
concatenates	O
the	O
result	O
(	O
if	O
needed	O
)	O
.	O

`	O
replacer	O
`	O
is	O
where	O
the	O
"	O
magic	O
"	O
happens	O
:	O
each	O
group's	O
`	O
time	B-API
`	O
values	O
get	O
replaced	O
(	O
`	O
to_replace	O
`)	O
with	O
the	O
new	O
value	O
that's	O
defined	O
in	O
`	O
time_map	O
`	O
.	O

The	O
`	O
get	B-API
`	O
method	O
of	O
`	O
dict	O
`	O
s	O
allows	O
you	O
to	O
provide	O
a	O
default	O
value	O
if	O
the	O
key	O
you're	O
searching	O
for	O
(	O
the	O
fruit	O
name	O
in	O
this	O
case	O
)	O
is	O
not	O
there	O
.	O

`	O
nan	O
`	O
is	O
commonly	O
used	O
for	O
this	O
purpose	O
,	O
but	O
here	O
I'm	O
actually	O
just	O
using	O
the	O
time	O
that	O
was	O
already	O
there	O
if	O
there	O
isn't	O
a	O
new	O
one	O
defined	O
for	O
it	O
in	O
the	O
`	O
time_map	O
`	O
`	O
dict	O
`	O
.	O

One	O
thing	O
to	O
note	O
is	O
my	O
use	O
of	O
`	O
g.name	O
`	O
.	O

This	O
doesn't	O
normally	O
exist	O
as	O
an	O
attribute	O
on	O
`	O
DataFrame	B-API
`	O
s	O
(	O
you	O
can	O
of	O
course	O
define	O
it	O
yourself	O
if	O
you	O
want	O
to	O
)	O
,	O
but	O
is	O
there	O
so	O
you	O
can	O
perform	O
computations	O
that	O
may	O
require	O
the	O
group	O
name	O
.	O

In	O
this	O
case	O
that's	O
the	O
"	O
current	O
"	O
fruit	O
you're	O
looking	O
at	O
when	O
you	O
apply	O
your	O
function	O
.	O

How	O
to	O
sort	O
a	O
dataFrame	B-API
in	O
python	O
pandas	O
by	O
two	O
or	O
more	O
columns	O
?	O

As	O
commented	O
,	O
the	O
`	O
sort	B-API
`	O
method	O
is	O
now	O
deprecated	O
in	O
favor	O
of	O
`	O
sort_values	B-API
`	O
.	O

The	O
arguments	O
(	O
and	O
results	O
)	O
remain	O
the	O
same	O
:	O
#CODE	O

You	O
can	O
use	O
the	O
ascending	O
argument	O
of	O
`	O
sort	B-API
`	O
:	O
#CODE	O

Sort	O
isn't	O
in	O
place	O
by	O
default	O
!	O

So	O
you	O
should	O
assign	O
result	O
of	O
the	O
sort	B-API
method	O
to	O
a	O
variable	O
or	O
add	O
inplace=True	O
to	O
method	O
call	O
.	O

that	O
is	O
,	O
if	O
you	O
want	O
to	O
reuse	O
df1	O
as	O
a	O
sorted	O
DataFrame	B-API
:	O
#CODE	O

`	O
pd.DataFrame	B-API
(	O
randint	O
(	O
1	O
,	O
5	O
,	O
(	O
10	O
,	O
2	O
))	O
,	O
columns=['a','b	O
'])`	O
doesn't	O
seem	O
to	O
work	O
....	O

`	O
TypeError	O
:	O
randint()	O
takes	O
exactly	O
3	O
arguments	O
(	O
4	O
given	O
)`	O

Sort	O
isn't	O
in	O
place	O
by	O
default	O
!	O

So	O
you	O
should	O
assign	O
result	O
of	O
the	O
`	O
sort	B-API
`	O
method	O
to	O
a	O
variable	O
or	O
add	O
`	O
inplace=True	O
`	O
to	O
method	O
call	O
.	O

As	B-API
of	O
pandas	O
0.17.0	O
,	O
`	O
DataFrame.sort()	O
`	O
is	O
deprecated	O
,	O
and	O
set	O
to	O
be	O
removed	O
in	O
a	O
future	O
version	O
of	O
pandas	O
.	O

The	O
way	O
to	O
sort	O
a	O
dataframe	B-API
by	O
its	O
values	O
is	O
now	O
is	O
`	O
DataFrame.sort_values	B-API
`	O

I	O
had	O
the	O
same	O
problem	O
with	O
a	O
SSL	O
website	O
only	O
on	O
Linux	O
funny	O
enough	O
-on	O
Windows	O
the	O
same	O
code	O
parsed	O
the	O
tables	O
from	O
the	O
website	O
.	O

After	O
spending	O
some	O
time	O
comparing	O
and	O
updating	O
library	O
versions	O
on	O
Linux	O
with	O
no	O
result	O
,	O
I	O
just	O
added	O
some	O
extra	O
code	O
to	O
handle	O
the	O
SSL	O
certificate	O
before	O
using	O
read_html	B-API
:	O
#CODE	O

I	O
need	O
to	O
extract	O
any	O
lines	O
of	O
data	O
whose	O
time	O
is	O
within	O
certain	O
range	O
,	O
say	O
:	O
09:00	O
:	O
00	O
to	O
09:15	O
:	O
00	O
.	O

My	O
current	O
solution	O
is	O
simply	O
reading	O
in	O
each	O
data	O
file	O
to	O
a	O
data	O
frame	O
,	O
sorting	O
it	O
in	O
order	O
by	O
time	O
and	O
then	O
using	O
searchsorted	B-API
to	O
find	O
09:00	O
:	O
00	O
to	O
09:15	O
:	O
00	O
.	O

It	O
works	O
fine	O
if	O
performance	O
isn't	O
an	O
issue	O
and	O
I	O
don't	O
have	O
1000	O
files	O
waiting	O
to	O
be	O
processed	O
.	O

Any	O
suggestions	O
on	O
how	O
to	O
boost	O
the	O
speed	O
?	O

Thanks	O
for	O
help	O
in	O
advance	O
!!!	O

How	O
big	O
are	O
we	O
talking	O
here	O
?	O

you	O
could	O
read	O
all	O
the	O
csv's	O
into	O
a	O
list	O
of	O
df's	O
,	O
then	O
`	O
concat	B-API
`	O
them	O
all	O
set	O
the	O
index	O
to	O
be	O
the	O
time	O
and	O
then	O
filter	O
using	O
`	O
df.loc	B-API
[(	O
df.index.time	O
>	O
'	O
09:00	O
:	O
00	O
')	O
&	O
(	O
df.index.time	O
<	O
'	O
09:15	O
:	O
00	O
')]`	O
,	O
whilst	O
loading	O
them	O
you	O
could	O
sort	O
on	O
index	O
and	O
chuck	O
away	O
the	O
ones	O
that	O
don't	O
have	O
that	O
range	O
anyway	O

I	O
have	O
a	O
dataframe	B-API
that	O
includes	O
non-unique	O
time	O
stamps	O
,	O
and	O
I'd	O
like	O
to	O
group	O
them	O
by	O
time	O
windows	O
.	O

The	O
basic	O
logic	O
would	O
be	O
-	O

It	O
feels	O
like	O
a	O
df.groupby	B-API
(	O
pd.TimeGrouper	B-API
(	O
minutes=n	O
))	O
is	O
the	O
right	O
answer	O
,	O
but	O
I	O
don't	O
know	O
how	O
to	O
have	O
the	O
TimeGrouper	O
create	O
dynamic	O
time	O
ranges	O
when	O
it	O
sees	O
events	O
that	O
are	O
within	O
a	O
time	O
buffer	O
.	O

For	O
instance	O
,	O
if	O
I	O
try	O
a	O
TimeGrouper('20s	O
')	O
against	O
a	O
set	O
of	O
events	O
:	O
10:34	O
:	O
00	O
,	O
10:34	O
:	O
08	O
,	O
10:34	O
:	O
08	O
,	O
10:34	O
:	O
15	O
,	O
10:34	O
:	O
28	O
and	O
10:34	O
:	O
54	O
,	O
then	O
pandas	O
will	O
give	O
me	O
three	O
groups	O
(	O
events	O
falling	O
between	O
10:34	O
:	O
00	O
-	O
10:34	O
:	O
20	O
,	O
10:34	O
:	O
20	O
-	O
10:34	O
:	O
40	O
,	O
and	O
10:34	O
:	O
40-10	O
:	O
35:00	O
)	O
.	O

I	O
would	O
like	O
to	O
just	O
get	O
two	O
groups	O
back	O
,	O
10:34	O
:	O
00	O
-	O
10:34	O
:	O
28	O
,	O
since	O
there	O
is	O
no	O
more	O
than	O
a	O
20	O
second	O
gap	O
between	O
events	O
in	O
that	O
time	O
range	O
,	O
and	O
a	O
second	O
group	O
that	O
is	O
10:34	O
:	O
54	O
.	O

Given	O
a	O
Series	B-API
that	O
looks	O
something	O
like	O
-	O
#CODE	O

If	O
I	O
do	O
a	O
df.groupby(pd.TimeGrouper('20s	O
'))	O
on	O
that	O
Series	B-API
,	O
I	O
would	O
get	O
back	O
5	O
group	O
,	O
10:34	O
:	O
00-	O
:	O
20	O
,	O
:	O
20-	O
:	O
40	O
,	O
:	O
40-10	O
:	O
35:00	O
,	O
etc	O
.	O

What	O
I	O
want	O
to	O
do	O
is	O
have	O
some	O
function	O
that	O
creates	O
elastic	O
timeranges	O
..	O
as	O
long	O
as	O
events	O
are	O
within	O
20	O
seconds	O
,	O
expand	O
the	O
timerange	O
.	O

So	O
I	O
expect	O
to	O
get	O
back	O
-	O
#CODE	O

You	O
might	O
want	O
consider	O
using	O
apply	B-API
:	O
#CODE	O

It's	O
up	O
to	O
you	O
to	O
implement	O
just	O
any	O
grouping	O
logic	O
in	O
your	O
grouper	B-API
function	O
.	O

Btw	O
,	O
merging	O
overlapping	O
time	O
ranges	O
is	O
kind	O
of	O
iterative	O
task	O
:	O
for	O
example	O
,	O
A	O
=	O
(	O
0	O
,	O
10	O
)	O
,	O
B	O
=	O
(	O
20	O
,	O
30	O
)	O
,	O
C	O
=	O
(	O
10	O
,	O
20	O
)	O
.	O

After	O
C	O
appears	O
,	O
all	O
three	O
,	O
A	O
,	O
B	O
and	O
C	O
should	O
be	O
merged	O
.	O

Muzhig	O
,	O
thanks	O
for	O
the	O
response	O
!	O

I'm	O
not	O
sure	O
how	O
the	O
logic	O
plays	O
out	O
in	O
the	O
my_grouper	O
function	O
though	O
.	O

Can	O
you	O
show	O
me	O
an	O
example	O
of	O
what	O
my_grouper	O
would	O
look	O
like	O
if	O
you	O
were	O
just	O
finding	O
overlapping	O
tuples	O
(	O
as	O
your	O
A	O
,	O
B	O
,	O
and	O
C	O
are	O
in	O
your	O
post	O
)	O
?	O

Muzhig	O
,	O
I	O
appreciate	O
the	O
example	O
!	O

create	O
a	O
column	O
`	O
tsdiff	O
`	O
that	O
has	O
the	O
diffs	O
between	O
consecutive	O
times	O
(	O
using	O
`	O
shift	O
`)	O

`df['new_group	O
']	O
=	O
df.tsdiff	O
timedelta	O
`	O

`	O
fillna	B-API
`	O
on	O
the	O
`	O
new_group	O
`	O

`	O
groupby	B-API
`	O
that	O
column	O

This	O
is	O
how	O
to	O
use	O
to	O
create	O
a	O
custom	O
grouper	B-API
.	O

(	O
requires	O
pandas	O
>	O
=	O
0.13	O
)	O
for	O
the	O
timedelta	O
computations	O
,	O
but	O
otherwise	O
would	O
work	O
in	O
other	O
versions	O
.	O

Create	O
your	O
series	B-API
#CODE	O

Arbitrariy	O
assign	O
things	O
20s	O
to	O
group	O
0	O
,	O
else	O
to	O
group	O
1	O
.	O

This	O
could	O
also	O
be	O
more	O
arbitrary	O
.	O

if	O
the	O
diff	O
from	O
previous	O
is	O
0	O
BUT	O
the	O
total	O
diff	O
(	O
from	O
first	O
)	O
is	O
>	O
50	O
make	O
in	O
group	O
2	O
.	O

#CODE	O

Groupem	O
(	O
can	O
also	O
use	O
an	O
apply	B-API
here	O
)	O
#CODE	O

Jeff	O
,	O
I	O
definitely	O
like	O
where	O
you're	O
going	O
with	O
this	O
.	O

I'm	O
not	O
sure	O
how	O
to	O
take	O
that	O
out	O
to	O
scale	O
though	O
.	O

What	O
if	O
you	O
add	O
in	O
two	O
more	O
events	O
at	O
2013-01-01	O
10:34	O
:	O
55	O
and	O
10:35	O
:	O
12	O
.	O

You'd	O
end	O
up	O
with	O
the	O
indexer	O
dataframe	B-API
having	O
two	O
more	O
lines	O
:	O

The	O
indexer	O
could	O
be	O
as	O
big	O
as	O
the	O
original	O
series	B-API
if	O
you	O
want	O
.	O

I'll	O
update	O
the	O
example	O
to	O
do	O
what	O
I	O
think	O
you	O
want	O
.	O

This	O
is	O
just	O
an	O
example	O
,	O
I	O
am	O
not	O
sure	O
what	O
you	O
want	O
.	O

You	O
could	O
easily	O
have	O
a	O
function	O
do	O
this	O
if	O
you	O
want	O
.	O

Just	O
create	O
the	O
grouper	B-API
like	O
you	O
want	O
.	O

What	O
I	O
would	O
like	O
to	O
do	O
is	O
to	O
create	O
a	O
window	O
id	O
for	O
each	O
time	O
range	O
that	O
is	O
created	O
by	O
identifying	O
those	O
overlapping	O
events	O
.	O

In	O
your	O
example	O
(	O
grouper	B-API
dataframe	B-API
)	O
,	O
you	O
end	O
up	O
with	O
10:34	O
:	O
54	O
as	O
a	O
different	O
number	O
than	O
10:34	O
:	O
55	O
and	O
10:35	O
:	O
12	O
.	O

Maybe	O
I'm	O
framing	O
the	O
question	O
wrong	O
by	O
focusing	O
on	O
pandas	O
,	O
and	O
this	O
is	O
really	O
a	O
question	O
of	O
"	O
what's	O
the	O
best	O
way	O
to	O
create	O
elastic	O
ranges	O
of	O
overlapping	O
events	O
in	O
python	O
"	O
.	O

I	O
had	O
hoped	O
pandas	O
had	O
a	O
sort	O
of	O
built	O
in	O
TimeSeries	O
manipulation	O
function	O
in	O
here	O
already	O
.	O

I	O
need	O
to	O
create	O
res	O
a	O
new	O
dataframe	B-API
such	O
as	O
for	O
each	O
Media	O
modality	O
Budget	O
has	O
a	O
column	O
e.g.	O
:	O
#CODE	O

a	O
starting	O
point	O
might	O
be	O
`	O
pd.crosstab	B-API
(	O
dd.Budget	O
,	O
dd.Media	O
)`	O

Get	O
the	O
crosstab	B-API
on	O
`dd['Budget	O
']	O
,	O
dd['Media	O
']`	O
#CODE	O

And	O
,	O
then	O
merge	B-API
on	O
`	O
dd	O
`	O
and	O
fill	O
`NaN's	O
`	O
with	O
`	O
0	O
`	O
#CODE	O

Why	O
doesn't	O
this	O
function	O
"	O
take	O
"	O
after	O
I	O
iterrows	B-API
over	O
a	O
pandas	O
DataFrame	B-API
?	O

I	O
have	O
a	O
DataFrame	B-API
with	O
timestamped	O
temperature	O
and	O
wind	O
speed	O
values	O
,	O
and	O
a	O
function	O
to	O
convert	O
those	O
into	O
a	O
"	O
wind	O
chill	O
.	O

"	O
I'm	O
using	O
iterrows	B-API
to	O
run	O
the	O
function	O
on	O
each	O
row	O
,	O
and	O
hoping	O
to	O
get	O
a	O
DataFrame	B-API
out	O
with	O
a	O
nifty	O
"	O
Wind	O
Chill	O
"	O
column	O
.	O

However	O
,	O
while	O
it	O
seems	O
to	O
work	O
as	O
it's	O
going	O
through	O
,	O
and	O
has	O
actually	O
"	O
worked	O
"	O
at	O
least	O
once	O
,	O
I	O
can't	O
seem	O
to	O
replicate	O
it	O
consistently	O
.	O

I	O
feel	O
like	O
it's	O
something	O
I'm	O
missing	O
about	O
the	O
structure	O
of	O
DataFrames	O
,	O
in	O
general	O
,	O
but	O
I'm	O
hoping	O
someone	O
can	O
help	O
.	O

#CODE	O

But	O
,	O
when	O
I	O
look	O
at	O
the	O
DataFrame	B-API
again	O
,	O
the	O
NaN's	O
are	O
still	O
there	O
:	O
#CODE	O

for	O
the	O
whole	O
DataFrame	B-API
at	O
once	O
using	O
your	O
simple	O
`	O
windchill	O
`	O
function	O
.	O

You	O
can	O
use	O
`	O
apply	B-API
`	O
to	O
do	O
this	O
:	O
#CODE	O

Bu	O
they	O
don't	O
update	O
to	O
the	O
DataFrame	B-API
:	O
#CODE	O

@USER	O
are	O
you	O
saying	O
the	O
above	O
worked	O
on	O
newer	O
or	O
older	O
pandas	O
?	O

There	O
are	O
a	O
few	O
edge	O
cases	O
in	O
pandas	O
'	O
apply	B-API
which	O
have	O
been	O
tweaked	O
over	O
last	O
few	O
releases	O
so	O
this	O
could	O
be	O
one	O
of	O
them	O
!	O

@USER	O
,	O
yes	O
,	O
I	O
was	O
using	O
an	O
older	O
version	O
of	O
Anaconda	O
which	O
had	O
a	O
version	O
11	O
,	O
I	O
believe	O
,	O
edition	O
of	O
Pandas	O
,	O
and	O
was	O
using	O
iterrows	B-API
,	O
and	O
the	O
way	O
I	O
had	O
it	O
coded	O
worked	O
fine	O
,	O
with	O
references	O
to	O
updates	O
in	O
the	O
original	O
data	O
frame	O
via	O
row	O
.	O

But	O
that	O
didn't	O
work	O
(	O
apparently	O
row	O
referenced	O
a	O
copy	O
,	O
not	O
the	O
original	O
)	O
when	O
I	O
tried	O
on	O
two	O
later	O
versions	O
.	O

Fixed	O
it	O
in	O
my	O
code	O
with	O
direct	O
.loc	B-API
references	O
to	O
the	O
original	O
dataframe	B-API
.	O

Selecting	O
max	O
within	O
partition	O
for	O
pandas	O
dataframe	B-API

I	O
have	O
a	O
pandas	O
dataframe	B-API
.	O

My	O
goal	O
is	O
to	O
select	O
only	O
those	O
rows	O
where	O
column	O
C	O
has	O
the	O
largest	O
value	O
within	O
group	O
B	O
.	O

For	O
example	O
,	O
when	O
B	O
is	O
"	O
one	O
"	O
the	O
maximum	O
value	O
of	O
C	O
is	O
311	O
,	O
so	O
I	O
would	O
like	O
the	O
row	O
where	O
C	O
=	O
311	O
and	O
B	O
=	O
"	O
one	O
.	O

"	O
#CODE	O

Applying	O
`	O
Paul	O
H	O
`	O
solution	O
to	O
your	O
problem	O
yields	O
:	O
`df2.groupby('B	O
')	O
.apply	B-API
(	O
lambda	O
k	O
:	O
k[k['C	O
']	O
==	O
k['C	O
']	O
.max()	B-API
])`	O

You	O
can	O
use	O
`	O
idxmax()	B-API
`	O
,	O
which	O
returns	O
the	O
indices	O
of	O
the	O
max	O
values	O
:	O
#CODE	O

Python	O
:	O
Inserting	O
a	O
Row	O
into	O
a	O
Data	O
Frame	O

Is	O
there	O
a	O
more	O
pythonic	O
way	O
to	O
insert	O
a	O
row	O
into	O
a	O
data	O
frame	O
?	O

I	O
feel	O
like	O
this	O
has	O
to	O
be	O
a	O
functionality	O
of	O
pandas	O
but	O
can	O
not	O
find	O
it	O
.	O

Especially	O
,	O
is	O
there	O
a	O
way	O
to	O
'	O
reset	O
'	O
the	O
indices	O
?	O

DataFrames	O
have	O
a	O
`	O
reset_index	B-API
`	O
method	O
.	O

So	O
there's	O
that	O
.	O

Use	O
the	O
`	O
loc	B-API
`	O
attribute	O
to	O
assign	O
data	O
.	O

Syntax	O
is	O
`	O
df.loc	B-API
[	O
row_index	O
,	O
col_index	O
]`	O
.	O

An	O
example	O
:	O
#CODE	O

So	O
,	O
you're	O
saying	O
to	O
just	O
create	O
a	O
brand	O
new	O
row	O
,	O
and	O
then	O
re-index	O
the	O
rows	O
to	O
where	O
you	O
want	O
the	O
new	O
row	O
inserted	O
.	O

I	O
like	O
it	O
.	O

Still	O
hoping	O
there's	O
a	O
built-in	O
way	O
to	O
insert	O
a	O
row	O
in	O
a	O
desired	O
location	O
.	O

I	O
understand	O
you	O
as	O
to	O
what	O
.loc	B-API
does	O
.	O

I	O
don't	O
think	O
I'm	O
being	O
clear	O
on	O
what	O
i	O
mean	O
by	O
'	O
insert	B-API
'	O
row	O
.	O

I	O
don't	O
mean	O
append	O
a	O
row	O
.	O

I	O
mean	O
insert	O
a	O
row	O
at	O
a	O
certain	O
location	O
.	O

So	O
,	O
if	O
i	O
wanted	O
to	O
insert	O
row	O
'	O
e	O
'	O
between	O
rows	O
'	O
b	O
'	O
and	O
'	O
c	O
'	O
,	O
I	O
would	O
first	O
df.loc['e','E	O
']	O
then	O
df.reindex('a	O
b	O
e	O
c	O
d	O
'	O
.split()	B-API
)	O

If	O
your	O
current	O
function	O
works	O
well	O
enough	O
for	O
you	O
,	O
I	O
suggest	O
just	O
adding	O
`	O
reset_index	B-API
`	O
to	O
the	O
returned	O
result	O
.	O

See	O
something	O
like	O
below	O
:	O
#CODE	O

Thank	O
you	O
for	O
the	O
reset_index	B-API
option	O
.	O

That	O
certainly	O
improves	O
my	O
code	O
.	O

Multi-index	O
pivoting	O
in	O
Pandas	O

Consider	O
the	O
following	O
dataframe	B-API
:	O
#CODE	O

I	O
would	O
like	O
to	O
pivot	O
it	O
to	O
get	O
the	O
table	O
arranged	O
as	O
:	O
#CODE	O

If	O
I	O
understand	O
what	O
you	O
are	O
asking	O
I	O
think	O
what	O
you	O
want	O
is	O
`	O
pandas.pivot_table	B-API
(	O
...	O
)`	O
which	O
you	O
can	O
use	O
like	O
so	O
:	O
#CODE	O

produces	O
#CODE	O

Wow	O
.	O

OK	O
,	O
I	O
just	O
realized	O
that	O
`	O
pivot	B-API
`	O
and	O
`	O
pivot_table	B-API
`	O
are	O
two	O
different	O
methods	O
.	O

Yes	O
--	O
Thanks	O
,	O
I	O
was	O
reading	O
about	O
both	O
methods	O
and	O
testing	O
it	O
locally	O
.	O

Find	O
lowest	O
value	O
of	O
previous	O
3	O
days	O
in	O
pandas	O
DataFrame	B-API

I	O
am	O
trying	O
to	O
find	O
the	O
lowest	O
value	O
of	O
previous	O
3	O
days	O
in	O
a	O
time	O
series	B-API
.	O

I	O
tried	O
.shift()	B-API
,	O
.min()	B-API
etc	O
.	O
none	O
of	O
them	O
works	O
.	O

your	O
help	O
is	O
greatly	O
appreciated	O
!	O

Wonderful	O
!	O

quick	O
answer	O
and	O
thanks	O
for	O
fix	O
the	O
data	O
format	O
in	O
my	O
question	O
.	O

(	O
before	O
I	O
fixed	O
it	O
:)	O
)	O

More	O
generally	O
there	O
are	O
a	O
number	O
of	O
rolling-style	O
functions	O
to	O
handle	O
common	O
cases	O
and	O
a	O
`	O
rolling_apply	B-API
`	O
for	O
user	O
functions	O
.	O

Many	O
libraries	O
/	O
packages	O
have	O
these	O
sorts	O
of	O
functions	O
which	O
you	O
can	O
usually	O
find	O
by	O
searching	O
for	O
"	O
moving	O
"	O
or	O
"	O
rolling	O
"	O
.	O

Get	O
pandas.read_csv	B-API
to	O
read	O
empty	O
values	O
as	O
empty	O
string	O
instead	O
of	O
nan	O

It	O
correctly	O
reads	O
"	O
nan	O
"	O
as	O
the	O
string	O
"	O
nan	O
'	O
,	O
but	O
still	O
reads	O
the	O
empty	O
cells	O
as	O
NaN	O
.	O

I	O
tried	O
passing	O
in	O
`	O
str	B-API
`	O
in	O
the	O
`	O
converters	O
`	O
argument	O
to	O
read_csv	B-API
(	O
with	O
`converters={'One	O
'	O
:	O
str}	O
)`)	O
,	O
but	O
it	O
still	O
reads	O
the	O
empty	O
cells	O
as	O
NaN	O
.	O

I	O
realize	O
I	O
can	O
fill	O
the	O
values	O
after	O
reading	O
,	O
with	O
fillna	B-API
,	O
but	O
is	O
there	O
really	O
no	O
way	O
to	O
tell	O
pandas	O
that	O
an	O
empty	O
cell	O
in	O
a	O
particular	O
CSV	O
column	O
should	O
be	O
read	O
as	O
an	O
empty	O
string	O
instead	O
of	O
NaN	O
?	O

Use	O
the	O
fillna	B-API
method	O
,	O
but	O
use	O
it	O
twice	O
'	O
nan	O
'	O
=	O
'	O
nan	O
'	O
,	O
'	O
NaN	O
'	O
=	O
""	O
.	O

This	O
would	O
keep	O
comma's	O
lined	O
up	O
.	O

I	O
don't	O
understand	O
your	O
answer	O
.	O

As	O
I	O
said	O
,	O
I	O
don't	O
want	O
to	O
use	O
fillna	B-API
or	O
any	O
other	O
method	O
call	O
after	O
the	O
reading	O
.	O

I'm	O
asking	O
if	O
there's	O
a	O
way	O
to	O
make	O
the	O
conversion	O
take	O
place	O
during	O
the	O
CSV	O
reading	O
operation	O
.	O

In	O
the	O
meantime	O
,	O
`	O
result.fillna	O
(	O
'')`	O
should	O
do	O
what	O
you	O
want	O

[	O
Documentation	O
for	O
`	O
DataFrame.fillna	B-API
`	O
.	O
]	O
(	O
#URL	O
)	O
Try	O
`	O
result.fillna	O
(	O
''	O
,	O
inplace=True	O
)`	O
.	O

Otherwise	O
it	O
creates	O
a	O
copy	O
of	O
the	O
dataframe	B-API
.	O

sorry	O
to	O
resurrect	O
such	O
an	O
old	O
answer	O
,	O
but	O
did	O
this	O
ever	O
happen	O
?	O

As	O
far	O
as	O
I	O
can	O
tell	O
from	O
[	O
this	O
GitHub	O
PR	O
]	O
(	O
#URL	O
)	O
it	O
was	O
closed	O
without	O
ever	O
being	O
merged	O
,	O
and	O
I'm	O
not	O
seeing	O
the	O
requested	O
behavior	O
in	O
pandas	O
version	O
0.14.x	O

[	O
Documentation	O
]	O
(	O
#URL	O
)	O
for	O
read_csv	B-API
now	O
offers	O
both	O
`	O
na_values	O
`	O
(	O
list	O
or	O
dict	O
indexed	O
by	O
columns	O
)	O
and	O
`	O
keep_default_na	O
`	O
(	O
bool	O
)	O
.	O

The	O
`	O
keep_default_na	O
`	O
value	O
indicates	O
whether	O
pandas	O
'	O
default	O
NA	O
values	O
should	O
be	O
replaced	O
or	O
appended	O
to	O
.	O

The	O
OP's	O
code	O
doesn't	O
work	O
currently	O
just	O
because	O
it's	O
missing	O
this	O
flag	O
.	O

For	O
this	O
example	O
,	O
you	O
could	O
use	O
`pandas.read_csv('test.csv',na_values=['nan	O
']	O
,	O
keep_default_na=False	O
)`	O
.	O

Pandas	O
:	O
The	O
truth	O
value	O
of	O
a	O
Series	B-API
is	O
ambiguous	O

Oh	O
no	O
!	O

Iterating	O
over	O
just	O
`	O
readdata	O
`	O
gives	O
me	O
the	O
column	O
names	O
.	O

Well	O
that's	O
clearly	O
not	O
what	O
you	O
want	O
.	O

Poking	O
around	O
shows	O
me	O
that	O
there's	O
a	O
`	O
.iterrows()	B-API
`	O
method	O
on	O
`	O
readdata	O
`	O
,	O
so	O
let's	O
try	O
that	O
:	O
#CODE	O

Now	O
it's	O
time	O
to	O
dive	O
into	O
the	O
docs	O
to	O
figure	O
out	O
more	O
about	O
how	O
to	O
format	O
pandas	O
series	B-API
objects	O
to	O
get	O
the	O
output	O
the	O
way	O
you	O
want	O
.	O

As	O
a	O
hint	O
,	O
using	O
this	O
(	O
after	O
putting	O
`	O
import	O
sys	O
`	O
at	O
the	O
top	O
of	O
the	O
script	O
)	O
instead	O
of	O
`	O
print	O
line	O
`	O
might	O
be	O
what	O
you	O
want	O
:	O
#CODE	O

I	O
am	O
trying	O
to	O
get	O
the	O
Adj	O
Close	O
prices	O
from	O
Yahoo	O
Finance	O
into	O
a	O
DataFrame	B-API
.	O

I	O
have	O
all	O
the	O
stocks	O
I	O
want	O
but	O
I	O
am	O
not	O
able	O
to	O
sort	O
on	O
date	O
.	O

#CODE	O

Date	O
is	O
your	O
index	O
,	O
so	O
dataFrame.index	O
,	O
this	O
will	O
get	O
you	O
the	O
date	O
column	O

you	O
can	O
do	O
`	O
dataFrame	B-API
[	O
"	O
Date	O
"]	O
=	O
dataframe.index	O
`	O
if	O
you	O
want	O
to	O
add	O
a	O
column	O
called	O
'	O
Date	O
'	O
with	O
the	O
index	O
values	O
in	O
it	O

Use	O
dataFrame.index	O
to	O
directly	O
access	O
date	O
or	O
to	O
add	O
an	O
explicit	O
column	O
,	O
use	O
dataFrame	B-API
[	O
"	O
columnName	O
"]	O
=	O
dataframe.index	O

`	O
f	O
`	O
is	O
a	O
`	O
Panel	B-API
`	O

You	O
can	O
get	O
a	O
`	O
DataFrame	B-API
`	O
and	O
reset	O
index	O
(	O
Date	O
)	O
using	O
:	O
#CODE	O

but	O
I'm	O
not	O
sure	O
`	O
reset_index()	B-API
`	O
is	O
very	O
useful	O
as	O
you	O
can	O
get	O
Date	O
using	O
#CODE	O

Pandas	O
rolling_mean	B-API
with	O
center	O
keyword	O
leaves	O
last	O
values	O
as	O
NaN	O

I	O
noticed	O
behaviour	O
in	O
pandas	O
rolling_mean	B-API
function	O
I	O
did	O
not	O
expect	O
.	O

I	O
would	O
expect	O
the	O
last	O
value	O
to	O
be	O
3.5	O
as	O
in	O
(	O
3+4	O
)	O
/	O
2	O
which	O
would	O
be	O
consistent	O
with	O
the	O
first	O
value	O
in	O
the	O
series	B-API
.	O

Is	O
this	O
a	O
bug	O
or	O
a	O
feature	O
?	O

I	O
am	O
aware	O
that	O
"	O
financial	O
"	O
rolling	O
means	O
and	O
"	O
scientific	O
"	O
rolling	O
means	O
are	O
different	O
but	O
I	O
think	O
this	O
should	O
produce	O
a	O
series	B-API
without	O
NaN	O
values	O
.	O

python	O
pandas	O
transforming	O
dataframe	B-API

The	O
function	O
.stack()	B-API
almost	O
gets	O
it	O
done	O
but	O
in	O
the	O
wrong	O
format	O
.	O

close	O
but	O
.reset_index	B-API
(	O
level=0	O
)	O
gives	O
an	O
error	O
ValueError('cannot	O
insert	O
%s	O
,	O
already	O
exists	O
'	O
%	O
item	O
)	O

You	O
could	O
also	O
use	O
Pandas	O
`	O
get_dummies()	B-API
`	O
#CODE	O

Its	O
a	O
bit	O
obscure	O
in	O
one	O
line	O
.	O

`	O
df.unstack()	B-API
.dropna()	B-API
`	O
basically	O
flattens	O
your	O
DataFrame	B-API
to	O
a	O
series	B-API
and	O
drops	O
al	O
NaN's	O
.	O

The	O
`	O
get_dummies	B-API
`	O
gives	O
a	O
table	O
of	O
all	O
the	O
occurrences	O
of	O
the	O
letters	O
,	O
but	O
for	O
each	O
level	O
in	O
the	O
unstack	B-API
DataFrame	B-API
.	O

The	O
grouping	O
and	O
sum	O
then	O
combine	O
the	O
index	O
to	O
the	O
original	O
shape	O
.	O

Beautiful	O
.	O

Much	O
better	O
than	O
my	O
(	O
now-deleted	O
)	O
attempt	O
to	O
make	O
get_dummies	B-API
work	O
on	O
a	O
DataFrame	B-API
.	O

Really	O
like	O
the	O
``	O
unstack()	B-API
.dropna()	B-API
``	O
idiom	O
.	O

I	O
noticed	O
,	O
you	O
could	O
have	O
left	O
it	O
,	O
no	O
harm	O
in	O
having	O
some	O
options	O
.	O

I	O
still	O
agree	O
with	O
your	O
earlier	O
comment	O
that	O
its	O
less	O
pretty	O
(	O
and	O
readable	O
)	O
than	O
the	O
`	O
pivot	B-API
`	O
solution	O
from	O
Roman	O
.	O

The	O
concept	O
of	O
pivot	B-API
is	O
also	O
better	O
known	O
,	O
then	O
...	O

`	O
get_dummies	B-API
`	O
.	O

:)	O

Pandas	O
Dataseries	O
get	O
values	O
by	O
level	O

I	O
am	O
dealing	O
with	O
pandas	O
series	B-API
like	O
the	O
following	O

x	O
=p	O
d.Series	O
([	O
1	O
,	O
2	O
,	O
1	O
,	O
4	O
,	O
2	O
,	O
6	O
,	O
7	O
,	O
8	O
,	O
1	O
,	O
1	O
]	O
,	O
index=['a	O
'	O
,	O
'	O
b	O
'	O
,	O
'	O
a	O
'	O
,	O
'	O
c	O
'	O
,	O
'	O
b	O
'	O
,	O
'	O
d	O
'	O
,	O
'	O
e	O
'	O
,	O
'	O
f	O
'	O
,	O
'	O
g	O
'	O
,	O
'	O
g	O
'])	O

The	O
indices	O
are	O
non	O
unique	O
,	O
but	O
will	O
always	O
map	O
to	O
the	O
same	O
value	O
,	O
for	O
example	O
'	O
a	O
'	O
always	O
corresponds	O
to	O
'	O
1	O
'	O
in	O
my	O
sample	O
,	O
b	O
always	O
maps	O
to	O
'	O
2	O
'	O
etc	O
.	O

So	O
if	O
I	O
want	O
to	O
see	O
which	O
values	O
correspond	O
to	O
each	O
index	O
value	O
I	O
simply	O
need	O
to	O
write	O
#CODE	O

x=pd.Series(['1	O
'	O
,	O
'	O
2	O
'	O
,	O
'	O
1	O
'	O
,	O
'	O
4	O
'	O
,	O
'	O
2	O
'	O
,	O
'	O
6	O
'	O
,	O
'	O
7	O
'	O
,	O
'	O
8	O
'	O
,	O
'	O
1	O
'	O
,	O
'	O
1	O
']	O
,	O
index=['a	O
'	O
,	O
'	O
b	O
'	O
,	O
'	O
a	O
'	O
,	O
'	O
c	O
'	O
,	O
'	O
b	O
'	O
,	O
'	O
d	O
'	O
,	O
'	O
e	O
'	O
,	O
'	O
f	O
'	O
,	O
'	O
g	O
'	O
,	O
'	O
g	O
'])	O

So	O
long	O
as	O
your	O
indices	O
map	O
directly	O
to	O
the	O
values	O
then	O
you	O
can	O
simply	O
call	O
`	O
drop_duplicates	B-API
`	O
:	O
#CODE	O

This	O
works	O
as	O
long	O
as	O
any	O
other	O
index	O
is	O
not	O
mapped	O
to	O
XX	O
,	O
I	O
modified	O
my	O
question	O
to	O
reflect	O
this	O

`	O
pandas.Series.values	B-API
`	O
are	O
numpy	O
`	O
ndarray	O
`	O
s	O
.	O

Perhaps	O
doing	O
a	O
`	O
values.astype	O
(	O
int	O
)`	O
would	O
solve	O
your	O
problem	O
?	O

pandas	O
,	O
dataframe	B-API
,	O
groupby	B-API
,	O
std	O

New	O
to	O
pandas	O
here	O
.	O

A	O
(	O
trivial	O
)	O
problem	O
:	O
hosts	O
,	O
operations	O
,	O
execution	O
times	O
.	O

I	O
want	O
to	O
group	O
by	O
host	O
,	O
then	O
by	O
host+operation	O
,	O
calculate	O
std	O
deviation	O
for	O
execution	O
time	O
per	O
host	O
,	O
then	O
by	O
host+operation	O
pair	O
.	O

Seems	O
simple	O
?	O

how	O
do	O
I	O
calculate	O
std	O
deviation	O
on	O
`	O
dataframe.groupby	B-API
([	O
several	O
columns	O
])`	O
?	O

It's	O
important	O
to	O
know	O
your	O
version	O
of	O
Pandas	O
/	O
Python	O
.	O

Looks	O
like	O
this	O
exception	O
could	O
arise	O
in	O
Pandas	O
version	O
0.10	O
(	O
see	O
ValueError	O
:	O
Buffer	O
dtype	B-API
mismatch	O
,	O
expected	O
float64_t	O
but	O
got	O
float	O
)	O
.	O

To	O
avoid	O
this	O
,	O
you	O
can	O
cast	O
your	O
`	O
float	O
`	O
columns	O
to	O
`	O
float64	O
`	O
:	O
#CODE	O

As	O
far	O
as	O
it	O
goes	O
,	O
it	O
looks	O
like	O
`	O
std()	O
`	O
is	O
calling	O
`	O
aggregation()	O
`	O
on	O
the	O
`	O
groupby	B-API
`	O
result	O
,	O
and	O
a	O
subtle	O
bug	O
(	O
see	O
here	O
-	O
Python	O
Pandas	O
:	O
Using	O
Aggregate	B-API
vs	O
Apply	B-API
to	O
define	O
new	O
columns	O
)	O
.	O

To	O
avoid	O
this	O
,	O
you	O
can	O
use	O
`	O
apply()	B-API
`	O
:	O
#CODE	O

`byhostandop['time	O
']	O
.std()	B-API
`	O
again	O
raises	O
the	O
same	O
exception	O
.	O

just	O
out	O
of	O
curiosuty	O
,	O
have	O
you	O
tried	O
`byhostandop['time	O
']	O
.apply	B-API
(	O
lambda	O
x	O
:	O
x.std()	O
)`	O
?	O

`byhostandop['time	O
']	O
.apply	B-API
(	O
lambda	O
x	O
:	O
x.std()	O
)`	O
WORKS	O
.	O

Thank	O
you	O
!	O

In	O
`	O
df.astype	B-API
`	O
reply	O
,	O
did	O
you	O
mean	O
that	O
I	O
should	O
explicitly	O
convert	O
column	O
type	O
?	O

Like	O
this	O
works	O
:	O
`df['time	O
']	O
=	O
df['time'].astype('float64	O
')	O
;	O
byhostandop=df.groupby(['host	O
'	O
,	O
'	O
operation	O
'])	O
;	O
byhostandop['time	O
']	O
.std()	B-API
`	O
.	O

But	O
I'm	O
not	O
sure	O
if	O
this	O
is	O
idiomatic	O
pandas	O
operation	O
or	O
I	O
would	O
better	O
do	O
smth	O
else	O
to	O
get	O
correct	O
(	O
float64	O
)	O
type	O
in	O
column	O
for	O
std	O
dev	O
calculation	O
.	O

The	O
error	O
```	O
ValueError	O
:	O
Buffer	O
dtype	B-API
mismatch	O
,	O
expected	O
'	O
float64_t	O
'	O
but	O
got	O
'	O
float	O
'```	O
for	O
`	O
np.Float32	O
`	O
data	O
persists	O
in	O
pandas	O
0.13.0	O
.	O

The	O
temporary	O
solution	O
:	O
```	O
grouped	O
=	O
df.astype	B-API
(	O
np.float64	O
)	O
.groupby	B-API
(	O
...	O
)```	O
(	O
assuming	O
all	O
the	O
data	O
is	O
float	O
)	O

I'm	O
writing	O
some	O
data	O
analysis	O
pipelines	O
in	O
pandas	O
.	O

One	O
of	O
the	O
columns	O
in	O
the	O
dataframes	O
that	O
I've	O
been	O
working	O
with	O
is	O
made	O
up	O
of	O
objects	O
of	O
custom-written	O
classes	O
that	O
are	O
each	O
initialized	O
with	O
a	O
string	O
,	O
from	O
which	O
I	O
read	O
off	O
various	O
information	O
with	O
regular	O
expressions	O
and	O
store	O
in	O
the	O
object's	O
attributes	O
.	O

The	O
subclass	O
structure	O
is	O
similar	O
to	O
how	O
one	O
might	O
implement	O
a	O
tree	O
of	O
life	O
(	O
e.g.	O
a	O
Tiger	O
is	O
a	O
sublass	O
of	O
Cat	O
which	O
is	O
a	O
subclass	O
of	O
Animal	O
and	O
frequently	O
--	O
but	O
not	O
always	O
--	O
animals	O
with	O
the	O
same	O
superclass	O
will	O
share	O
methods	O
)	O
.	O

It	O
also	O
has	O
some	O
useful	O
methods	O
that	O
I	O
can	O
use	O
for	O
calculations	O
.	O

For	O
str	O
and	O
repr	O
methods	O
return	O
the	O
string	O
that	O
was	O
used	O
to	O
initialize	O
it	O
,	O
like	O
so	O
:	O
#CODE	O

This	O
means	O
that	O
when	O
I	O
want	O
to	O
view	O
my	O
data	O
frame	O
,	O
I	O
see	O
a	O
string	O
.	O

I	O
have	O
had	O
no	O
problems	O
using	O
the	O
to_csv()	B-API
method	O
on	O
data	O
frames	O
that	O
have	O
these	O
objects	O
in	O
them	O
,	O
but	O
when	O
I	O
use	O
the	O
to_excel()	B-API
method	O
of	O
the	O
pandas	O
data	O
frame	O
,	O
I	O
get	O
the	O
following	O
error	O
:	O
#CODE	O

Now	O
,	O
I	O
could	O
go	O
modify	O
the	O
offending	O
code	O
,	O
but	O
I	O
don't	O
want	O
to	O
have	O
to	O
deal	O
with	O
making	O
that	O
play	O
nicely	O
with	O
updates	O
to	O
xlsxwriter	O
.	O

I	O
could	O
simply	O
make	O
my	O
class	O
inherit	O
from	O
str	O
,	O
but	O
that	O
seems	O
like	O
unpythonic	O
given	O
that	O
I	O
don't	O
really	O
want	O
to	O
use	O
almost	O
any	O
of	O
the	O
string	O
methods	O
.	O

Lastly	O
,	O
I	O
could	O
sanitize	O
my	O
dataframe	B-API
by	O
taking	O
everything	O
in	O
it	O
that's	O
of	O
this	O
subclass	O
and	O
turning	O
it	O
back	O
into	O
a	O
string	O
,	O
but	O
that	O
would	O
mean	O
I	O
have	O
to	O
rewrite	O
a	O
lot	O
of	O
things	O
that	O
I	O
use	O
that	O
depend	O
on	O
being	O
able	O
to	O
use	O
the	O
DataFrame.to_excel	B-API
method	O
.	O

Is	O
there	O
anything	O
I	O
can	O
do	O
within	O
the	O
class	O
that	O
saves	O
me	O
from	O
having	O
to	O
inherit	O
everything	O
from	O
str	O
?	O

Trying	O
to	O
wrap	O
my	O
head	O
around	O
pandas	O
dataframe	B-API
(	O
with	O
no	O
looping	O
)	O

Use	O
`	O
where	B-API
`	O
to	O
conditionally	O
assign	O
a	O
value	O
where	O
true	O
and	O
return	O
a	O
different	O
value	O
if	O
false	O
:	O
#CODE	O

Just	O
return	O
that	O
using	O
`	O
np.where	O
`	O
:	O
`np.where(df['x	O
']	O
>	O
5	O
,	O
'	O
Bigger	O
'	O
,	O
'	O
Smaller	O
')`	O

My	O
script	O
uses	O
`	O
pandas.read_csv	B-API
`	O
to	O
directly	O
read	O
read	O
csv	O
files	O
into	O
data	O
frames	O
.	O

A	O
user	O
is	O
supposed	O
to	O
provide	O
a	O
configuration	O
file	O
that	O
contains	O
labels	O
of	O
columns	O
to	O
retain	O
along	O
with	O
corresponding	O
data	O
types	O
.	O

Let	O
me	O
illustrate	O
with	O
an	O
example	O
.	O

Here	O
is	O
a	O
csv	O
.	O

#CODE	O

This	O
fails	O
,	O
because	O
`	O
pandas.read_csv	B-API
`	O
requires	O
a	O
file	O
path	O
or	O
a	O
FileIO	O
stream	O
,	O
not	O
a	O
generator	O
.	O

Then	O
I	O
attempted	O
to	O
use	O
`	O
DataFrame.from_records	B-API
`	O
that	O
can	O
read	O
generators	O
,	O
but	O
doesn't	O
have	O
the	O
`	O
dtype	B-API
`	O
argument	O
of	O
`	O
pandas.read_csv	B-API
`	O
.	O

Finally	O
I	O
ended	O
up	O
mimicking	O
a	O
file	O
with	O
my	O
own	O
class	O
#CODE	O

I	O
know	O
that	O
I	O
can	O
read	O
in	O
all	O
lines	O
,	O
process	O
them	O
and	O
pass	O
to	O
`	O
pandas.DataFrame	B-API
`	O
,	O
but	O
I	O
don't	O
want	O
to	O
use	O
any	O
intermediate	O
Python	O
containers	O
,	O
because	O
the	O
files	O
are	O
huge	O
and	O
I	O
don't	O
want	O
to	O
run	O
into	O
memory	O
errors	O
.	O

Then	O
I	O
use	O
read	O
`	O
StringIO	O
`	O
by	O
chunks	O
using	O
parameter	O
`	O
chunksize=1024	O
`	O
and	O
`	O
iterator=True	O
`	O
and	O
concatenate	O
object	O
to	O
dataframe	B-API
`	O
df	O
`	O
.	O

Source	O
,	O
more	O
info	O
.	O

#CODE	O

That	O
was	O
supposed	O
to	O
be	O
`	O
dtype	B-API
`	O
not	O
`	O
dtypes	B-API
`	O
,	O
my	O
bad	O
,	O
sorry	O
.	O

As	O
for	O
your	O
solution	O
,	O
I've	O
clearly	O
stated	O
(	O
in	O
the	O
**	O
Note	O
**	O
section	O
)	O
that	O
the	O
lines	O
must	O
be	O
lazily	O
generated	O
.	O

In	O
your	O
case	O
,	O
you	O
preprocess	O
all	O
lines	O
in	O
memory	O
and	O
then	O
pass	O
them	O
to	O
`	O
StringIO	O
`	O

You	O
still	O
store	O
the	O
entire	O
file	O
here	O
`	O
lower_lines	O
=	O
u	O
""	O
.join	B-API
([	O
line.lower()	O
for	O
line	O
in	O
csv	O
])`	O
:)	O

`	O
DataFrame.append	B-API
`	O
is	O
highly	O
time	O
and	O
memory	O
inefficient	O
as	O
it	O
creates	O
a	O
new	O
data	O
frame	O
.	O

Memory-wise	O
it	O
might	O
be	O
even	O
more	O
inefficient	O
than	O
your	O
original	O
list-comprehension	O
based	O
approach	O
,	O
because	O
at	O
some	O
point	O
of	O
time	O
you	O
might	O
end	O
with	O
N	O
copies	O
of	O
a	O
data	O
frame	O
,	O
because	O
garbage	O
collection	O
is	O
not	O
happening	O
all	O
the	O
time	O
.	O

In	O
your	O
new	O
approach	O
you	O
still	O
hit	O
the	O
same	O
things	O
I've	O
already	O
pointed	O
out	O
.	O

`	O
str.join	B-API
`	O
stores	O
the	O
whole	O
file	O
in	O
memory	O
(	O
even	O
twice	O
in	O
fact	O
,	O
because	O
it	O
builds	O
a	O
temporary	O
array	O
out	O
of	O
an	O
iterable	O
and	O
then	O
builds	O
a	O
string	O
out	O
of	O
the	O
array	O
)	O
,	O
`	O
file.read	O
`	O
reads	O
the	O
entire	O
file	O
into	O
memory	O
by	O
definition	O
.	O

`	O
df.concat	O
`	O
is	O
inefficient	O
for	O
the	O
same	O
reason	O
`	O
df.append	B-API
`	O
is	O
.	O

How	O
to	O
fill	O
in	O
a	O
lot	O
of	O
data	O
utlizing	O
pandas	O
fillna	B-API
fast	O
?	O

I	O
have	O
two	O
Dataframes	O
one	O
large	O
one	O
with	O
a	O
lot	O
of	O
missing	O
values	O
and	O
a	O
second	O
one	O
with	O
data	O
to	O
fill	O
the	O
missing	O
data	O
in	O
the	O
first	O
one	O
.	O

Dataframe	B-API
examples	O
:	O
#CODE	O

You	O
can	O
use	O
try	O
`	O
groupby	B-API
`	O
-	O
each	O
group	O
is	O
transposing	O
with	O
`	O
T	O
`	O
and	O
then	O
`	O
fillna	B-API
`	O
`	O
df	O
`	O
by	O
values	O
of	O
`	O
df2	O
`	O
:	O
#CODE	O

Maybe	O
if	O
`	O
df	O
=	O
df.fillna	B-API
(	O
df2	O
)`	O
doesn't	O
work	O
,	O
can	O
be	O
use	O
`	O
df	O
=	O
df.combine_first	B-API
(	O
df2	O
)`	O
.	O

It	O
depends	O
on	O
index	O
.	O

How	O
does	O
it	O
work	O
?	O

I	O
cannot	O
compare	O
it	O
with	O
your	O
solution	O
,	O
because	O
error	O
:	O
`	O
NameError	O
:	O
global	O
name	O
'	O
toappend	O
'	O
is	O
not	O
defined	O
`	O

Thanks	O
for	O
your	O
answer	O
jezrael	O
!	O

I	O
fixed	O
the	O
cause	O
of	O
the	O
error	O
,	O
my	O
bad	O
sorry	O
.	O

I	O
have	O
quickly	O
checked	O
,	O
and	O
it	O
works	O
when	O
utilizing	O
`	O
df2	O
=	O
df2.groupby	O
(	O
df2.index	O
)	O
.apply	B-API
(	O
lambda	O
x	O
:	O
x.B.reset_index	O
(	O
drop=True	O
)	O
.T	O
)	O
`	O
instead	O
of	O
`	O
df2	O
=	O
df2.groupby('A	O
')	O
.apply	B-API
(	O
lambda	O
x	O
:	O
x.B.reset_index	O
(	O
drop=True	O
)	O
.T	O
)`	O
.	O

Need	O
to	O
test	O
on	O
the	O
large	O
dataset	O
when	O
I	O
get	O
home	O

But	O
I	O
get	O
the	O
following	O
warning	O
and	O
im	O
not	O
sure	O
how	O
to	O
fix	O
my	O
code	O
to	O
comply	O
?	O

#CODE	O

What	O
version	O
of	O
pandas	O
?	O

I	O
don't	O
see	O
this	O
on	O
0.17	O
,	O
so	O
perhaps	O
this	O
warnings	O
been	O
fixed	O
(	O
I	O
think	O
it	O
was	O
being	O
a	O
little	O
oversensitive	O
here	O
.	O
)	O

