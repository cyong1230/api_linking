Minor : ` notnull ` is also a method of DataFrames .
I want to apply a function f ( lat1 , lon1 , lat2 , lon2 ) which calculates the distance between two points ( defined using lat1 , lon1 , lat2 , lon2 ) .
For all 110k+ records in ` df1 ` do you want to apply your distance function for every record in ` df2 ` ?
I chose to use map and list comprehensions because they will be faster than a standard ` for each `
However I took this into account and used map , and nested comprehensions which are going to be faster than a for loop .
Keep getting : KeyError : ' cannot use a single bool to index into setitem ' on this line of code in the second chunk I posted .
i have to merge them in to the same cell before applying this method .
Or is there are way to marge the columns in pandas ?
I have two TimeSeries with some overlapping dates / indices and I'd like to merge them .
I have an excel file ( .xls format ) with 5 sheets , I want to replace the contents of sheet 5 with contents of my pandas data frame .
So , I decided to do this task in VBA and drop python completely .
It could be I'm not using the right keywords , so if you have suggestions , that could also help .
plus the selected rows usage x 2 , which will happen when you concat the rows
after the concat the usage will go down to selected rows usage
See example here : #URL Not sure this will solve it , but that will do the query in chunks , and you can aggregate or merge them in pandas
Try a ` dropna ` or use ` missing= ' drop '` to Logit .
You might also check that the right hand side is full rank ` np.linalg.matrix_rank ( data [ train_cols ] .values )`
append pandas.DataFrame.GroupBy results into another dataframe
You need to append the intermediate DataFrames to a list and then concatenate the results .
I am taking the second dataframe and doing some calculations with it to append to the first dataframe .
However it does not appear that what I am appending to the first data frame is actually happening .
Could you use concat instead ?
` m= m.concat ([ a0 , a1 , a2 , a3 , a4 , a5 , a6 , a7 , a8 , a9 ] , ignore_index=True )`
I get an error trying to use this ...
AttributeError : ' DataFrame ' object has no attribute ' concat '
[ ` append `] ( #URL ) does * not * operate in place .
But for a start I would just be happy to get the first result .
I suspect that I need to use searchsort and asof , but I am not quite sure how to do that with .
You're looking for a near timestamp , where ` asof ` searches for the latest timestamp .
It is only applied to a time series , so you would have to apply ` reset_index ` to your ` DataFrame `
You're going to have to iterate over your list , get copies of them filtered and then concat them all together #CODE
A solution without loop but ` merge ` : #CODE
If there are no blanks some columns convert to ` TRUE / FALSE ` , others leave as ` Yes / No ` but dtype is bool .
` fhs = fhs.drop ([ 1002 ])` to drop that row and data types are still good .
first column comes into df as Yes , No , Yes , Yes type bool xxxx below
3rd column comes into df as FALSE , FALSE , TRUE , TRUE type bool
print ( len ( upregulated ) , end= ' \n ')
remove overlay text from pandas boxplot
I am trying to remove the overlay text on my boxplot I created using pandas .
The code to generate it is as follows ( minus a few other modifications ):
I just want to remove the " boxplot grouped by 0 ...
I know how to create a new column with ` apply ` or ` np.where ` based on the values of another column , but a way of selectively changing the values of an existing column is escaping me ; I suspect ` df.ix ` is involved ?
@USER For indexing with boolean vectors this is perfectly fine , if you want to add in other forms of indexing you would want ` loc ` .
For instance : ` df.loc [ df.name.str.contains ( ' e$ ') , ' flag '] = ' Blue '` .
use `` apply `` ONLY as a last resort ( e.g. you can't do vectorized things ) . even if you have a very complicated function to do , you can often do vectorized calculations on most of it , saving the last for `` apply `` , which is essentially a loop .
Using apply took 172ms versus 39ms using Jeff's method , I can also confirm that it made negligle difference whether the apply was called inside or outside the function but it does modify the df so you didn't need to return the df as it was being modified inside the function
And then sometimes different solutions ( in this case using ` apply `) come up on google / stackoverflow and yet again I can NOT verify that there is no better solution as I dont have the insight into the library .
I kindof disagree with using df as the variable name here , I also think I'd just use len : ` df.groupby ( " Name ") .filter ( lambda x : len ( x ) > 2 )`
Merge existing dataframe into fixed size new dataframe
Then I want merge these kinds of table into new dataframe
How could I merge them in that way ?
Inconsistent behavior of apply with operator.itemgetter v.s. applymap operator.itemgetter
` apply ` gives wrong result #CODE
apply is being passed an entire row which is a series of 2 elements which are lists ; the last list is returned and coerced to a series . embedded lists as elements are not a good idea in general .
The reason I am asking , is because I suspect ( ? ) it is faster to create a zero filled dataframe , and then replace each element as needed .
So it might be faster to create an empty dataframe with nxm dimensions and then replace elements as needed ( by copying a list to each column ) .
in general creating an empty frame , then filling it column by column is not very efficient ; use a dict / list instead , or create sub-frames and concat them
Are you trying to shift ends by one ( month ) ?
My initial suggestion was to do the shift after you've reindexed ( since you're about to do that anyway ): #CODE
the shift index looks like a better fix , still would like to know if there is a simple date add function , which is how I'd do it in sql , that could apply ?
I'd still like to know if there is a simple DateAdd type function that I could use that might also apply for use elsewhere if needed ?
Alternatively you could use ` apply ` ( but this will usually be slower ): #CODE
Since you are using the " trailing row " you are going to need to use ` shift ` : #CODE
thanks shift is what i was looking for . now i can find examples in the Pandas book
I have been searching for hours , literally the entire day on how to generate a pivot table in Python .
What I want is to take a csv file , extract the first column and generate a pivot table using the count or frequency of the numbers in that column , and sort descending #CODE
These columns all contain an identical kind of data , and I'd like to stack them into a single series , ergo : #CODE
From here , I can't quite figure out how to reindex my series such that the indexes go from 0 to ` len ( s )` .
But it could be an unexpected system difference -- I am using Python 2.7.3 on an Ubuntu machine .
An alternative you might try is to replace exit() with os._exit ( os.EX_OK ) .
I think it uses ` patsy ` in the backend to translate the formula expression , and intercept is added automatically .
Trying to append this to a new datastore .
The datastore does not exist so I use the following to create and append the data ; #CODE
I'm not looking to concatenate strings , just shift everything over .
I saw a method using " R " and melt , however I would like to stick with python / pandas if possible .
I cannot post real request for security reason .
By the way the code works without " append " within for loop .
At first I tried using pivot ( with timestamp as an index ) , but that didn't work because of those duplicates .
I don't want to drop them , since the other data is different and should not be lost .
Since index contains no duplicates , I thought maybe I can pivot over it and after that merge the result into the original DataFrame , but I was wondering if there is an easier more intuitive solution .
As your ` get_dummies ` returns a df this will be aligned already with your existing df so just ` concat ` column-wise : #CODE
You can drop the ' cat ' column by doing ` df.drop ( ' cat ' , axis=1 )`
You can see that the array is masked and that some of the first few rows show examples of ` -- ` in there .
So I drop the last field ( ` refGage `) and it works , so I think it's masked values which only appear in that field .
I used df.ix() to replace the filled-in tokens for what was masked out .
Next , you can use a dictionary comprehension together with ` loc ` to select the relevant ` group_no ` dataframe .
To get the last group number , I get the last value using ` iat ` for location based indexing .
Then apply your method : #CODE
Notice that if you unstack the ` id ` index level of ` df ` then you get : #CODE
I'm not used to working with ` lists ` in columns of Pandas and don't know how to get the intersection of ` lists ` from two columns in a ` dataframe ` , then get the index of where the words appear , then apply plus signs to the front of each found index .
Or maybe easier would be a string replacement on ` df [ ' Keyword ']` using the words from ` StemmedAG ` ?
You can use ` pivot ` #CODE
Cool I didn't know about pivot either ...
Instead of creating it , we can append it to initial StartDate .
However , the DptCityDptCountry might be different but if another ID matches with the StartDate and DptCityDptCountry , it will be added up i.e. #CODE
Then use apply and return a series indexed on the expanded set of dates for each row ( Series of Series = DataFrame ) .
So for each of the 7 rows in the DataFrame , I get a series indexed on the expanded date range .
Then its just clever stacking , naming , and reset_index .
Also , if you want to have the ticklabels / tickmarks of the x-axis connected to the " middle axis " ( also while panning / zooming ) , then it's easiest to insert an extra spine ; take a look at [ ` mpl_toolkits.axisartist `] ( #URL ) for some examples of this .
print ( ' Stock : ' , col , ' max diff : ' , sl.max() - sl.min() )`
Then merge back to the original dataframe to have your aggregates displayed against each row : #CODE
Unfortunately im getting an issue when trying to do the rename .
The true / false column does not have a column name , so how would I rename it and then merge it back into the original dataframe ?
I am using the below code which gives me the summary of count in the pivot table , #CODE
but what i want is the % of row calculation as in excel pivot when you right click the pivot and select " show value as -> % of Row Total " .
Since my Document is a non-numeric value i was not able to get it .
i am trying to manipulate the pivot data which will give me the row total , not the data from the dataframe and what i wanted is " % of row total " .
you can actually just pass ` aggfunc=len ` , since ` len ` is already a function :)
Hi maxymoo in the link you have given they are manipulating one of the column from the dataframe , but my question is different i am trying to manipulate the pivot data which will give me the row total and what i wanted is " % of row total " .
Then you can basically use the solution @USER linked to , but you need to use ` iloc ` or similar b / c the table columns are a little complicated now ( being a multi-indexed result of the pivot table ) .
Unfortunately , if I try to resample , I get an error #CODE
Are you ask for a process to interpolate , or a process to aggregate , or both ?
Firstly , prepare a function to map the day to week #CODE
Assume now your initialized new dataframe is ` result ` , you can now do a join #CODE
The ` Nan ` is what you need to interpolate .
Turns out the key is to resample a groupby object like so : #CODE
Then , I append a row of missing values .
Finally , I can insert values into this DataFrame one cell at a time .
This approach works perfectly fine , with the exception that the append statement inserts an additional column to my DataFrame .
The append is trying to append a column to your dataframe .
The column it is trying to append is not named and has two None / Nan elements in it which pandas will name ( by default ) as column named 0 .
In order to do this successfully , the column names coming into the append for the data frame must be consistent with the current data frame column names or else new columns will be created ( by default ) #CODE
have merged 2 dataframes with left join . works as I expected until I attempt to use the generated value in a simple string concatenation .
I am ultimately trying to merge two dataframes together , but I am running into an issue when I try to specify the column on which they should be merged .
Conform the index to another frequency .
Then its straightforward to resample to another frequency .
In the second chunk you are resampling and the result is a Series of monthly frequency so it would appear that the daily information is lost .
Then you resample and somehow the days are there ?
I want to use a combination of map & lambda functions to do this
the map function does not append to NN .
Have you tried using ` concat ` and a generator expression instead : #CODE
Can Pandas find all the lines that join any pair of dots and don't intersect any of the given lines without iteration ?
I'm a Stata user and in Stata , I'd be using replace command conditional on regexm .
I'm trying to learn Python and it's been a difficult journey !
We then apply another function to this that converts the str numbers to ints , puts these in a list and returns the smallest value : #CODE
this is an approach that I hadn't thought about and one that I'm likely to employ down the road . for age , I wanted the series [ 62 , 55 , 67 ] at the end , and the problem I'm having now is that I can't target just row2 when I apply split ( ' ') .
return min ( list ( map ( int , x )))` to ` def highest ( x ):
return max ( list ( map ( int , x )))`
I want to apply df [ ' age '] =d f [ ' e0 '] [( df [ ' e0 '] .str .match ( pattern7 )= =1 )] .apply ( lambda x : str ( x ) .split ( ' ') [ 1 ]) to only rows for which df [ ' e0 '] .str .match ( pattern7 )= =1 ) so as to not overwrite what was already in the age column ...
Suppose I have two DataFrames a b where a is larger than b and has all NaNs .
I wish to merge the values from b into a .
the w variable will not surpass len ( seq ) .
For example instead of looping trough every element in a numpy array to do some processing you can apply a numpy function directly on the array and get the results in seconds rather than hours . as an example : #CODE
Computing ` len ( seq )` inside the loop is not necessary , since its value is not changing .
You don't really need the ` if ` statement , since in your code it always evaluate to true ( ` w in range ( len ( seq ))` means ` w ` maximium value will be ` len ( seq ) -1 `) .
I tried pivot but it returns an error
Hmm My dataframe had 12 rows but when i tried the unstack operation the resulting dataframe has only 6 rows not exactly what i want.My resulting dataframe should also have 12 rows
For example , say ` Jul-03 ` data , row ` 0 , 6 , 9 ` are all records about the same ` snapDate ` with instance ` XX ` .
So doing a pivot would reshape these 3 rows to only one row because those data have been moved to columns .
Hi I went ahead and changed the datatype of AvgWaitInMs to int and the pivot worked
What I would like to do is slice each group down to 3 hours max and append something to the 6 and 9 length groups to denote that it is the same page like the following : #CODE
So , I truncated my data set in the question to make it easier to read and thinking that whatever solution came would also apply ..
If ' data ' is a pd.DataFrame and you iterate over range ( 0 , len ( data )) and then add data to your list ' all_info ' , you simply add the whole DataFrame ' data ' i times to the list .
Python pandas : retrieve the field associated to the min of another ( cross apply equivalent )
In SQL I was used to doing this with a cross apply .
PS other than calculating the min first , then doing a join on primary key and date
I can do this in two steps : 1 ) group by primary key and calculate min ( date ) 2 ) do an inner join between the starting table and the table calculated in the previous step , on primary key and date , to retrieve the amount
Call ` resample ` and pass the rule as ' 10Min ' : #CODE
The quickest way I know how to wrangle this thing into a long form dataframe is using ` stack ` and then ` reset_index ` : #CODE
Maybe my real question is " why isn't ` melt ` a DataFrame method ?
This works pretty well : ` pd.melt ( wide_df.reset_index() , " subject ")` , but it feels like it would be easier to read as chained method calls that can be read in linear order .
not sure why their isn't a `` melt `` on DataFrame , could / should be .
bool operator in for Timestamp in Series does not work
Is there a way to drop columns in a Dataframe with column names having a particular letter as I wasn't able to find any information on this ?
I want to drop all column headers having the letter ` F ` in them .
I was planning on doing it using ` df.drop ([ df.columns [[ column_names ]]] , axis=1 )` , but there are so many that I was wondering if there is an easier way to do this .
Rolling argmax in pandas
I have a pandas TimeSeries and would like to apply the argmax function to a rolling window .
However , due to casting to float from rolling_apply , if I apply ` numpy.argmax() ` , I only obtain the index of the slice of the ndarray .
Is there a way to apply a rolling argmax to a Series / DataFrame ?
Here is a work-around , essentially doing the apply ' manually ' , should be pretty efficient actually .
You could do a ` shift ` first : #CODE
Merge csv's with some common columns and fill in Nans
pandas - resample - upsampling before downsampling
My objective is to resample this data frame with a fixed time window ( e.g. : 1 second ) using last for regularization when upsampling and the mean for downsampling .
Is this possible at all using pandas resample function ?
You can't mix upsample / downsample in a single ` resample ` operation .
I'm not sure why the order of operations would matter to you as long as you get the desired results .
Thanks for your answer , it was not clear to me that you had to make multiple calls to resample .
You can then concat this back to get the ' I ' column back : #CODE
Actually setting index_col= ' I ' when reading allows to avoid the concat !
As a follow up to this post , I would like to concatenate a number of columns based on their index but I am encountering some problems .
In this example I get an Attribute error related to the map function .
Help around this error would be appreciated as would code that does the equivalent concatenation of columns .
note that support for ` filter ( None , iterable )` ceased in Python 3 , need to do ` filter ( bool , iterable )` there
I have found workaround which is extremely slow due to the " in python " apply : #CODE
How to drop extra copy of duplicate index of Pandas Series ?
So how to drop extra duplicate rows of series , keep the unique rows and only one copy of the duplicate rows in an efficient way ?
One way would be using ` drop ` and ` index.get_duplicates ` : #CODE
Not totally drop the duplicated ones .
You can groupby the index and apply a function that returns one value per index group .
@USER sorry , " arbitrary " of length len ( s ) :) .
Below is my snippet : import pandas as pd ; idx_tp = [( ' 600809 ' , ' 20061231 ') , ( ' 600809 ' , ' 20070331 ') , ( ' 600809 ' , ' 20070630 ') , ( ' 600809 ' , ' 20070331 ')] ; dt = [ ' demo ' , ' demo ' , ' demo ' , ' demo '] ; idx = pd.MultiIndex.from_tuples ( idx_tp , names = [ ' STK_ID ' , ' RPT_Date ']) ; s = pd.Series ( dt , index=idx ); # s.groupby ( s.index ) .first() will crash on my machine
Edit : another solution which is faster is to use ` value_counts ` ( and normalize ): #CODE
I had thought this was more concisely written as a ` resample ` , if you use a DatetimeIndex :
len ( Series.unique() ) might be even faster .
Interestingly , len ( Series.unique() ) is usually much faster than Series.nunique() .
Next , these 3 columns should be combined into one column - the mean of the order numbers - but I do know how to do that part ( with apply and axis=1 ) .
I would like to normalize my data by dividing every row by the first value of that very row .
I am just getting stuck on " setting with chained indexing " and setting with iloc / loc / ix .
I can't figure out how to represent this using iloc , loc and ix .
Python 2.7 & Pandas : How to replace values at 12:00 with values from 11:55 ?
How do I explicitly say ' replace the values at 19:40 : 00 with the values at 19:35 : 00 ?
Python merge excel documents with dynamic columns
However , since they are not 100% identical , I cannot simply merge them together and upload it into a database without messing up the data .
If a large proportion of them are similar , and this is a one-off operation it may be worth your while coding the solution for the majority and handling the other documents ( or groups of them if they are similar ) separately .
Any recommendations to a db that would allow me to dump a few thousand excel documents and then create join queries to the VIN column ?
I am doing a transformation on a variable from a pandas dataframe and then I would like to replace the column with my new values .
The problem seems to be that after the transformation , the length of the array is not the same as the length of my dataframe's index .
When I check the length , these lengths seem to disagree .
The len ( array ) says it is 2 but when I call the stats.boxcox it says it is 50000 .
Print out ` len ( df )` and ` len ( stats.boxcox ( df.variable ))` .
How to calculate the count of column values less than 95 on each row on pandas pivot table
I am new to pandas pivot tables , how to get the count of column values less than 95 for a row on pandas pivot table #CODE
My decorated DataFrames return new and similarly decorated DataFrames when I use methods such as copy and groupby.agg .
I.e. , how can I have my decorated DataFrames replace the stock DataFrames ?
Still not getting the hang of pandas , I am attempting to join two data frames in Pandas using merge .
I have read in the CSVs into two data frames ( named dropData and deosData in the code below ) .
The deosData file is an entire year s worth of observations that I am trying to match up with corresponding entries in dropData .
I have gone through the documentation for the merge function and have tried the following code in various iterations , so far I have only been able to have a blank data frame with correct header row , or have the two data frames merged on the 0 -- ( N-1 ) indexing that is assigned by default :
After searching on SE and the Doc s I have tried resetting the index , ignoring the index columns , copying the Date_Time column as a separate index and trying to merge on the new column , I have tried using on=None , left_on and right_on as permutations of Date_Time to no avail .
I have checked the column data types , Date_Time in both are dtype Objects , I do not know if this is the source of the error , since the only issues I could find searching revolved around matching different dtypes to each other .
What I am looking to do is have the two data frames merge where the two ' Date_Time ' columns intersect .
and then do your merge .
You can use ` join ` , but you first need to set the index : #CODE
You can also do ` groupby ( ..., as_index=False )` , though buggy with apply in 0.12 , fixed in 0.13 .
I've converted the last step to no longer be a loop and instead save directly to a list .
AFAIK , you would have to separate the two parts and append as lists since the columns of interest are different and converting to a dictionary would include the ` NaN ` s otherwise .
When using the pure XlsxWriter I can apply formats to cells what also works nice .
Basically how would I apply ` df [ ' col1 '] .str .contains ( ' ^ ')` to an entire dataframe at once and filter down to any rows that have records containing the match ?
Pandas : apply different functions to different columns
i am looking to apply multiply masks on each column of a pandas dataset ( respectively to it's properties ) in python .
how can i apply the concat_mask on df , so that i select rows , in which all Boolean criteria are matched ( are True ) ?
. Can You insert that into your answer ?
In the proper code i actually iterate throw all columns and apply various of diffenrent conditions to mask each column .
If you return a Series of the ( split ) location , you can merge ( ` join ` to merge on index ) the resulting DF directly with your value column .
If I'm not mistaken , it only works if ` df ` has index that is ` range ( len ( df ))` , right ?
` join ` is shorthand for merging on index with both frames , so the indices need only be consistent ( which it will be here as the apply and col selection don't affect it ) .
How to resample a dataframe with different functions applied to each column ?
You can also downsample using the ` asof ` method of ` pandas.DateRange ` objects .
@ Wes McKinney this should be ` resample ` in 0.8 , isn't it ?
Therefore , I join the index of ` count_df ` ( ` left_index=True `) with the ` CompanyName ` column of ` df ` ( ` right_on= " CompanyName "`) .
You can drop the extraneous column using ` df.drop ` : #CODE
( 3 ) save the header columns for concat later #CODE
( 5 ) output : concat [ header data ] . write output #CODE
groupby after concat , column missing in the group mean
concat two dataframe , then groupby ' type ' and calculate the mean , columns of second df , i.e. d1~d10 , showing in the concat'ed dataframe but not in the grouped mean .
I want to create a new DataFrame such that each row is created from the original df but rows with loc counts greater than 2 are excluded .
That is , the new df is created by looping through the old df , counting the number of loc rows that have come before , and including / excluding the row based on this count .
The output excludes the 4th row in the original df because its loc count is greater than 2 ( i.e. 3 ) .
Also , be careful with your column names , since ` loc ` clashes with the ` .loc ` method .
So you get a string back :) .
You can use eval ( "" [ 1.5 , 2.5 , 3.5 ]"") , but I hear it's bad practice .
You can map your lists to strings by using `" , " .join ( your_list )` given that you only use floats .
merge the dataframe on ID #CODE
The ` merge ` did the trick , but I thought it was more usefull to just do a ` dfMerged.dropna() ` after the merge and that will be the set with the difference .
yes , essentially , the answer was really about the ` merge ` method , which allows you to sql-like joins .
Instead , I get an error telling me that equiv is not a callable function .
Fair enough , it's a dictionary , but even if I wrap it in a function I still get frustration .
So I tried to use a map function that seems to work with other operations , but it also is defeated by use of a dictionary : #CODE
ok , revised the answer ; you can do almost anything inside the apply FYI
In order to normalize data in a pandas DataFrame I wrote the following functions : #CODE
If you want the values themselves , you can ` groupby ` ' Column1 ' and then call ` apply ` and pass the ` list ` method to apply to each group .
You could ` groupby ` on ` Column1 ` and then take ` Column3 ` to ` apply ( list )` and call ` to_dict ` ?
Pandas - How can I set rules for selecting which duplicates to drop
What I want to do is drop the values that have the same index ( date time ) , but I want to make a rule like :
I have tried using groupby and apply in several different ways but I cant get it to work .
You could use ` del df [ ' dist ']` to drop the dist column when you no longer need it .
Though I was wondering if you could do it immediately using lambda , apply and groupby .
I am sorry I am trying to insert code into comments I cant do it
All I am doing at the moment is loading the .csv as a dataframe and then writing it to the db using ` df.to_sql ( table_name , engine , index=False , if_exists= ' append ' , chunksize=1000 )`
I want to transform it into a single column data with index being year-month .
I try to stack my original data but it becomes a time series , which has the year mix with my values .
` set_index ` to ` Year ` first , and then ` stack ` .
