At	O
the	O
first	O
step	O
I	O
used	O
`	O
df.T	Y
`	O
to	O
transpose	O
the	O
dataframe	Y
,	O
and	O
tried	O
something	O
like	O
`	O
df.value_counts()	O
`	O
,	O
however	O
I'd	O

Replace	O
NaN	O
in	O
a	O
dataframe	Y
with	O
random	O
values	O

I	O
want	O
to	O
replace	O
all	O
the	O
NaN	O
with	O
some	O
random	O
values	O
like	O
.	O

#CODE	O

Resample	Y
function	O
throwing	O
error	O
with	O
Twitter	O
Data	O

I	O
then	O
try	O
to	O
resample	O
for	O
analysis	O
#CODE	O

I'm	O
writing	O
several	O
pivot	O
tables	O
using	O
pandas	O
.	O

For	O
many	O
of	O
them	O
,	O
I	O
need	O
to	O
return	O
unique	O
values	O
.	O

In	O
a	O
two-dimensional	O
pivot	O
table	O
,	O
the	O
below	O
code	O
works	O
as	O
it	O
should	O
.	O

When	O
I	O
add	O
a	O
third	O
dimension	O
,	O
the	O
code	O
returns	O
the	O
count	O
rather	O
than	O
the	O
unique	O
count	O
.	O

I	O
suspect	O
this	O
has	O
something	O
to	O
do	O
with	O
the	O
aggfunc	Y
,	O
but	O
can't	O
determine	O
to	O
what	O
it	O
should	O
be	O
changed	O
.	O

Use	O
a	O
groupby	Y
to	O
get	O
at	O
each	O
combination	O
of	O
`	O
col_1	O
`	O
and	O
`	O
col_3	O
`	O
,	O
then	O
unstack	Y
to	O
get	O
the	O
`	O
col_3	O
`	O
values	O
as	O
columns	O
:	O
#CODE	O

Python	O
pandas	O
merge	O
or	O
concat	O
dataframes	O

The	O
data	O
is	O
for	O
2	O
products	O
(	O
BBG.XAMS.UL.S_pnl_pos_cost	O
and	O
BBG.XAMS.UNA.S_pnl_pos_cost	O
)	O
by	O
date	O
,	O
in	O
the	O
future	O
there	O
will	O
be	O
more	O
products	O
.	O

I	O
want	O
to	O
concat	O
or	O
merge	O
(	O
not	O
sure	O
which	O
)	O
the	O
list	O
of	O
dataframes	O
into	O
one	O
data	O
frame	O
(	O
called	O
result	O
)	O
so	O
they	O
look	O
like	O
:	O
#CODE	O

where	O
axis	O
is	O
the	O
date	O
.	O

It	O
looks	O
like	O
the	O
data	O
is	O
merged	O
by	O
date	O
,	O
but	O
I	O
am	O
missing	O
the	O
data	O
for	O
the	O
week	O
beginning	O
2015-03-23	O
.	O

My	O
current	O
concat	O
result	O
dataframe	Y
looks	O
like	O
:	O
#CODE	O

Try	O
using	O
axis=0	O
.	O

This	O
should	O
concat	O
column-wise	O
,	O
assuming	O
each	O
dataframe	Y
has	O
the	O
same	O
column	O
names	O
.	O

possible	O
duplicate	O
of	O
[	O
Pandas	O
join	Y
/	O
merge	Y
/	O
concat	Y
two	O
dataframes	O
]	O
(	O
#URL	O
)	O

Also	O
,	O
how	O
do	O
you	O
join	O
this	O
back	O
to	O
original	O
dataframe	Y
?	O

We	O
can	O
resample	O
this	O
to	O
days	O
;	O
it'll	O
be	O
a	O
much	O
longer	O
timeseries	O
,	O
of	O
course	O
,	O
but	O
memory	O
is	O
cheap	O
and	O
I'm	O
lazy	O
:	O
#CODE	O

How	O
do	O
I	O
merge	O
the	O
birth	O
rate	O
back	O
to	O
the	O
original	O
table	O
?	O

Indexes	O
aren't	O
compatible	O
...	O

Turns	O
out	O
size	O
isn't	O
such	O
an	O
issue	O
.	O

Python	O
&	O
Pandas	O
:	O
Unable	O
to	O
drop	O
columns	O

I	O
try	O
to	O
drop	O
the	O
data	O
,	O
but	O
it	O
reports	O
some	O
column	O
does	O
not	O
exist	O
.	O

#CODE	O

@USER	O
,	O
that's	O
possible	O
,	O
but	O
I	O
don't	O
know	O
how	O
to	O
deal	O
with	O
it	O
.	O

In	O
my	O
previous	O
experience	O
with	O
pandas	O
,	O
it	O
will	O
automatically	O
turn	O
the	O
second	O
`	O
Q	O
`	O
into	O
`	O
Q.1	O
`	O
when	O
reading	O
the	O
data	O
.	O

However	O
,	O
in	O
my	O
case	O
,	O
it	O
failed	O
to	O
do	O
it	O
,	O
and	O
I	O
don't	O
know	O
why	O
.	O

However	O
,	O
This	O
it	O
cannot	O
`	O
drop	Y
`	O
`	O
NCDC	O
`	O
either	O
.	O

You	O
can	O
then	O
drop	O
your	O
columns	O
:	O
#CODE	O

I	O
would	O
like	O
to	O
take	O
a	O
given	O
row	O
from	O
a	O
DataFrame	Y
and	O
prepend	O
or	O
append	O
to	O
the	O
same	O
DataFrame	Y
.	O

Rather	O
than	O
concat	O
I	O
would	O
just	O
assign	O
directly	O
to	O
the	O
df	O
after	O
`	O
shift	O
`	O
ing	O
,	O
then	O
use	O
`	O
iloc	Y
`	O
to	O
reference	O
the	O
position	O
you	O
want	O
to	O
assign	O
the	O
row	O
,	O
you	O
have	O
to	O
call	O
`	O
squeeze	Y
`	O
so	O
that	O
you	O
assign	O
just	O
the	O
values	O
and	O
lose	O
the	O
original	O
index	O
value	O
otherwise	O
it'll	O
raise	O
a	O
`	O
ValueError	O
`	O
:	O
#CODE	O

To	O
insert	O
at	O
the	O
end	O
:	O
#CODE	O

I'm	O
not	O
sure	O
exactly	O
what	O
you're	O
expecting	O
,	O
but	O
you	O
could	O
replace	O
your	O
lists	O
with	O
numpy	O
arrays	O
(	O
I	O
don't	O
think	O
it'll	O
improve	O
your	O
specific	O
code	O
):	O
#CODE	O

What	O
is	O
the	O
best	O
way	O
for	O
me	O
to	O
get	O
this	O
data	O
into	O
Pandas	O
?	O

Is	O
there	O
a	O
standard	O
way	O
I	O
could	O
use	O
`	O
read_table	Y
`	O
or	O
some	O
similar	O
function	O
to	O
read	O
this	O
file	O
directly	O
?	O

Should	O
I	O
write	O
a	O
script	O
to	O
insert	O
commas	O
where	O
all	O
the	O
column	O
breaks	O
are	O
and	O
then	O
read	O
it	O
in	O
as	O
CSV	O
?	O

(	O
I'd	O
just	O
do	O
the	O
latter	O
,	O
but	O
I'm	O
also	O
interested	O
in	O
becoming	O
better	O
with	O
Pandas	O
so	O
if	O
there's	O
an	O
out-of-the-box	O
way	O
I'd	O
like	O
to	O
know	O
it	O
.	O
)	O

Any	O
ideas	O
on	O
how	O
to	O
get	O
this	O
file	O
to	O
load	O
?	O

Unfortunately	O
I	O
can't	O
just	O
strip	O
out	O
accents	O
,	O
as	O
I	O
have	O
to	O
interface	O
with	O
software	O
that	O
requires	O
the	O
proper	O
name	O
,	O
and	O
I	O
have	O
a	O
ton	O
of	O
files	O
to	O
format	O
(	O
not	O
just	O
the	O
one	O
)	O
.	O

Thanks	O
!	O

pls	O
show	O
your	O
input	O
and	O
what	O
is	O
the	O
expected	O
output	O
,	O
in	O
a	O
copy-pastable	O
form	O
.	O

What	O
you	O
are	O
doing	O
is	O
very	O
inefficient	O
.	O

A	O
groupby	Y
should	O
try	O
to	O
use	O
vectorized	O
functions	O
when	O
possible	O
.	O

Then	O
join	O
them	O
up	O
at	O
the	O
end	O
.	O

Hi	O
Tom	O
,	O
it	O
doesn't	O
look	O
like	O
this	O
works	O
.	O

It	O
outputs	O
just	O
one	O
array	O
and	O
is	O
equivalent	O
to	O
df2	O
[	O
'	O
array	O
']	O
.sum()	Y
.	O

But	O
you	O
have	O
given	O
me	O
an	O
idea	O
with	O
apply	Y
.	O

Let	O
me	O
see	O
if	O
I	O
can	O
figure	O
something	O
out	O
.	O

You	O
need	O
`	O
apply	Y
(	O
your_func	O
,	O
axis=1	O
)`	O
to	O
work	O
on	O
a	O
row-by-row	O
basis	O
.	O

#CODE	O

Another	O
way	O
would	O
be	O
to	O
call	O
`	O
unique	Y
`	O
on	O
the	O
transpose	O
of	O
your	O
df	O
:	O
#CODE	O

Drop	O
values	O
satisfying	O
condition	O
plus	O
arbitrary	O
number	O
of	O
next	O
values	O
in	O
a	O
pandas	O
DataFrame	Y

So	O
my	O
final	O
goal	O
is	O
to	O
drop	Y
values	O
in	O
one	O
column	O
of	O
a	O
`	O
pandas	O
`	O
`	O
DataFrame	Y
`	O
according	O
to	O
some	O
condition	O
on	O
another	O
column	O
of	O
the	O
same	O
`	O
DataFrame	Y
`	O
,	O
plus	O
several	O
next	O
values	O
e.g.	O
:	O
#CODE	O

So	O
this	O
will	O
drop	O
the	O
records	O
where	O
the	O
condition	O
is	O
satisfied	O
,	O
but	O
how	O
do	O
I	O
drop	O
the	O
next	O
3	O
records	O
after	O
the	O
condition	O
was	O
satisfied	O
too	O
?	O

My	O
desired	O
output	O
would	O
look	O
something	O
like	O
this	O
:	O
#CODE	O

We	O
can	O
use	O
the	O
boolean	O
condition	O
index	O
to	O
slice	O
the	O
df	O
using	O
`	O
loc	Y
`	O
and	O
set	O
the	O
following	O
values	O
:	O
#CODE	O

Panda's	O
boxplot	Y
but	O
not	O
showing	O
the	O
box	O

Note	O
,	O
`	O
showbox	O
`	O
and	O
`	O
whiskerprops	O
`	O
are	O
the	O
`	O
kwds	O
`	O
of	O
boxplot	Y
,	O
which	O
are	O
in	O
turn	O
passed	O
to	O
`	O
matplotlib.boxplot	Y
`	O
.	O

Applying	O
aggregate	Y
function	O
on	O
columns	O
of	O
Pandas	O
pivot	O
table	O

I	O
generated	O
the	O
following	O
pivot	O
table	O
via	O
taking	O
maximum	O
of	O
values	O
in	O
`	O
Z	O
`	O
column	O
:	O
#CODE	O

Here's	O
a	O
fairly	O
general	O
solution	O
you	O
can	O
apply	O
to	O
multiple	O
columns	O
.	O

The	O
'	O
To	O
'	O
column	O
doesn't	O
need	O
to	O
be	O
rounded	O
,	O
I	O
just	O
included	O
it	O
for	O
the	O
generality	O
of	O
two	O
columns	O
rather	O
than	O
one	O
:	O
#CODE	O

428	O
base	O
,	O
mult	O
=	O
_gfc	O
(	O
freq	O
)	O

-->	O
429	O
return	O
tslib.dt64arr_to_periodarr	O
(	O
data.view	O
(	O
'	O
i8	O
')	O
,	O
base	O
,	O
tz	O
)	O

I	O
could	O
do	O
a	O
left	O
merge	Y
,	O
but	O
I	O
would	O
end	O
up	O
with	O
a	O
huge	O
file	O
.	O

Is	O
there	O
any	O
way	O
to	O
add	O
specific	O
rows	O
from	O
df2	O
to	O
df1	O
using	O
merge	Y
?	O

Unclear	O
why	O
you	O
think	O
a	O
left	O
merge	Y
would	O
produce	O
a	O
huge	O
file	O
,	O
by	O
performing	O
a	O
left	O
merge	Y
on	O
the	O
product	O
id	O
you	O
are	O
stating	O
that	O
you	O
are	O
only	O
interested	O
in	O
matches	O
in	O
the	O
product_id	O
column	O
only	O

Just	O
perform	O
a	O
left	O
`	O
merge	Y
`	O
on	O
'	O
product_id	O
'	O
column	O
:	O
#CODE	O

What	O
would	O
be	O
the	O
Python	O
equivalent	O
?	O

I	O
cannot	O
think	O
of	O
a	O
way	O
to	O
translate	O
this	O
where	O
statement	O
into	O
pandas	O
syntax	O
.	O

The	O
only	O
way	O
I	O
can	O
think	O
of	O
is	O
to	O
add	O
an	O
arbitrary	O
field	O
to	O
people_usa	O
(	O
e.g.	O
`	O
people_usa	O
[	O
'	O
dummy	O
']	O
=1	O
`)	O
,	O
do	O
a	O
left	O
join	Y
,	O
then	O
take	O
only	O
the	O
records	O
where	O
'	O
dummy	O
'	O
is	O
nan	O
,	O
then	O
delete	O
the	O
dummy	O
field	O
-	O
which	O
seems	O
a	O
bit	O
convoluted	O
.	O

Does	O
this	O
work	O
only	O
on	O
the	O
index	O
of	O
the	O
dataframe	Y
?	O

I'd	O
like	O
the	O
option	O
to	O
specify	O
the	O
field	O
(	O
s	O
)	O
to	O
apply	O
this	O
to	O

Is	O
there	O
any	O
easy	O
way	O
to	O
do	O
this	O
if	O
you	O
have	O
multiple	O
columns	O
to	O
check	O
/	O
join	O
?	O

You	O
could	O
do	O
a	O
`	O
merge	Y
`	O
and	O
then	O
eliminate	O
the	O
rows	O
that	O
exist	O
in	O
the	O
merged	O
df	O
otherwise	O
you'd	O
have	O
to	O
build	O
a	O
boolean	O
condition	O
for	O
all	O
the	O
columns	O
you	O
want	O
to	O
compare	O
but	O
presumably	O
when	O
checking	O
the	O
multiple	O
columns	O
you're	O
stating	O
that	O
it's	O
unique	O
for	O
those	O
columns	O
,	O
correct	O
?	O

For	O
instance	O
it's	O
not	O
a	O
match	O
if	O
say	O
col1	O
and	O
col2	O
match	O
but	O
col3	O
does	O
not	O

Yes	O
merge	Y
is	O
what	O
I	O
have	O
been	O
doing	O
but	O
it	O
feels	O
like	O
a	O
hassle	O
.	O

I've	O
come	O
up	O
with	O
this	O
,	O
using	O
itertools	Y
,	O
to	O
find	O
mid-day	O
timestamps	O
and	O
group	O
them	O
by	O
date	O
,	O
and	O
now	O
I'm	O
coming	O
up	O
short	O
trying	O
to	O
apply	O
imap	O
to	O
find	O
the	O
means	O
.	O

#CODE	O

Since	O
not	O
sure	O
what	O
your	O
end	O
output	O
should	O
look	O
like	O
,	O
just	O
create	O
a	O
time-based	O
grouper	Y
manually	O
(	O
this	O
is	O
essentially	O
a	O
resample	Y
)	O
,	O
but	O
doesn't	O
do	O
anything	O
with	O
the	O
final	O
results	O
(	O
its	O
just	O
a	O
list	O
of	O
the	O
aggregated	O
values	O
)	O
#CODE	O

You	O
can	O
get	O
reasonable	O
fancy	O
here	O
and	O
say	O
return	O
a	O
pandas	O
object	O
(	O
and	O
potentially	O
`	O
concat	N
`	O
them	O
)	O
.	O

and	O
I	O
want	O
to	O
pivot	N
it	O
like	O
this	O
:	O
#CODE	O

I	O
am	O
calling	O
a	O
function	O
from	O
within	O
a	O
'	O
for	O
each	O
loop	O
'	O
which	O
attempts	O
to	O
insert	N
values	O
into	O
a	O
Pandas	O
DataFrame	Y
based	O
on	O
a	O
specified	O
column	O
start	O
and	O
end	O
location	O
.	O

The	O
function	O
is	O
this	O
:	O
#CODE	O

My	O
issue	O
is	O
that	O
despite	O
the	O
same	O
starting	O
conditions	O
when	O
I	O
call	O
this	O
function	O
it	O
seems	O
to	O
generate	O
a	O
list	O
of	O
inconsistent	O
length	O
.	O

e.g.	O
with	O
values	O
of	O
srowb	O
=	O
1	O
and	O
erowb	O
=	O
18	O
it	O
will	O
generate	O
a	O
list	O
(	O
tmp_brollb	O
)	O
which	O
has	O
either	O
len	Y
(	O
tmp_brollb	O
)	O
=	O
17	O
or	O
len	Y
(	O
tmp_brollb	O
)	O
=	O
18	O

Use	O
`	O
max	Y
`	O
and	O
check	O
for	O
equality	O
using	O
`	O
eq	Y
`	O
and	O
cast	O
the	O
boolean	O
df	O
to	O
int	O
using	O
`	O
astype	Y
`	O
,	O
this	O
will	O
convert	O
`	O
True	O
`	O
and	O
`	O
False	O
`	O
to	O
`	O
1	O
`	O
and	O
`	O
0	O
`	O
:	O
#CODE	O

Thanks	O
@USER	O
.	O

Did	O
you	O
try	O
my	O
original	O
post	O
?	O

I	O
would	O
be	O
interested	O
to	O
know	O
how	O
much	O
time	O
this	O
one	O
is	O
taking	O
compared	O
to	O
yours	O
?	O

`	O
for	O
i	O
in	O
range	O
(	O
len	Y
(	O
df	O
)):	O
...	O

df.loc	Y
[	O
i	O
]	O
[	O
df.loc	Y
[	O
i	O
]	O
.idxmax	Y
(	O
axis=1	O
)]	O
=	O
1	O
...	O

df.loc	Y
[	O
i	O
]	O
[	O
df.loc	Y
[	O
i	O
]	O
!	O
=	O
1	O
]	O
=	O
0	O
`	O

I	O
am	O
trying	O
to	O
normalize	N
the	O
missing	O
values	O
in	O
matrix	O
.	O

Here	O
is	O
the	O
code	O
.	O

#CODE	O

Last	O
line	O
should	O
replace	N
the	O
values	O
in	O
dataset1	O
by	O
mean	O
values	O
from	O
`	O
ds2_mean	O
[	O
1	O
]`	O
.	O

But	O
it	O
does	O
not	O
do	O
.	O

Anything	O
wrong	O
here	O
?	O

And	O
after	O
that	O
can	O
I	O
replace	N
NaN	O
with	O
the	O
average	O
value	O
of	O
it's	O
neighbours	O
in	O
dataset1	O
?	O

it	O
does	O
wrong	O
.	O

For	O
any	O
x	O
in	O
dataset2	O
it	O
has	O
mapped	O
value	O
in	O
col2	O
.	O

It	O
should	O
replace	N
all	O
values	O
of	O
x	O
in	O
ds1	O
by	O
mapped	O
value	O
.	O

But	O
this	O
also	O
does	O
not	O
do	O
it	O

Sorry	O
can	O
you	O
explain	O
clearer	O
,	O
what	O
are	O
you	O
mapping	O
from	O
what	O
to	O
what	O
exactly	O
?	O

By	O
default	O
fillna	Y
will	O
use	O
the	O
index	O
so	O
how	O
do	O
you	O
want	O
the	O
mapping	O
from	O
`	O
ds2	O
`	O
to	O
map	N
to	O
the	O
missing	O
values	O
in	O
`	O
ds1	O
`	O
?	O

Are	O
you	O
wanting	O
to	O
map	N
using	O
the	O
values	O
in	O
`	O
ds2	O
[	O
0	O
]`	O
as	O
the	O
index	O
lookup	O
?	O

So	O
use	O
the	O
index	O
from	O
`	O
ds1	O
`	O
find	O
value	O
in	O
`	O
ds2	O
[	O
0	O
]`	O
and	O
return	O
`	O
ds2	O
[	O
1	O
]`	O
?	O

yes	O
,	O
I	O
want	O
to	O
use	O
the	O
index	O
from	O
ds1	O
find	O
value	O
in	O
ds2	O
[	O
0	O
]	O
and	O
replace	N
it	O
with	O
ds2	O
[	O
1	O
]"	O
sorry	O
for	O
inconvenience	O

I	O
want	O
to	O
add	O
a	O
new	O
column	O
which	O
contains	O
values	O
based	O
on	O
df	O
[	O
'	O
diff	N
']	O

When	O
using	O
`	O
DataFrame.apply	Y
`	O
if	O
you	O
use	O
`	O
axis=0	O
`	O
it	O
applies	O
the	O
condition	O
through	O
columns	O
,	O
to	O
use	O
`	O
apply	Y
`	O
to	O
go	O
through	O
each	O
row	O
,	O
you	O
need	O
`	O
axis=1	O
`	O
.	O

But	O
given	O
that	O
,	O
you	O
can	O
use	O
`	O
Series.apply	Y
`	O
instead	O
of	O
`	O
DataFrame.apply	Y
`	O
on	O
the	O
`'	O
diff	Y
'`	O
series	O
.	O

Example	O
-	O
#CODE	O

You	O
can	O
just	O
set	O
all	O
the	O
values	O
that	O
meet	O
your	O
criteria	O
rather	O
than	O
looping	O
over	O
the	O
df	O
by	O
calling	O
`	O
apply	Y
`	O
so	O
the	O
following	O
should	O
work	O
and	O
as	O
it's	O
vectorised	O
will	O
scale	O
better	O
for	O
larger	O
datasets	O
:	O
#CODE	O

this	O
will	O
set	O
all	O
rows	O
that	O
meet	O
the	O
criteria	O
,	O
the	O
problem	O
using	O
`	O
apply	Y
`	O
is	O
that	O
it's	O
just	O
syntactic	O
sugar	O
for	O
a	O
`	O
for	O
`	O
loop	O
and	O
where	O
possible	O
this	O
should	O
be	O
avoided	O
where	O
a	O
vectorised	O
solution	O
exists	O
.	O

Then	O
you	O
can	O
`	O
stack	Y
`	O
(	O
first	O
by	O
`'	O
Marker	O
'`	O
then	O
by	O
`'	O
mrk	O
'`)	O
:	O
#CODE	O

Python	O
DataFrame	Y
-	O
apply	N
different	O
calculations	O
due	O
to	O
a	O
column's	O
value	O

You	O
could	O
do	O
this	O
using	O
2	O
`	O
loc	Y
`	O
calls	O
:	O
#CODE	O

There	O
are	O
two	O
reasons	O
whiskers	O
length	O
vary	O
from	O
one	O
boxplot	Y
to	O
any	O
other	O
boxplot	Y

Are	O
you	O
asking	O
why	O
the	O
top	O
whisker	O
isn't	O
the	O
same	O
length	O
as	O
the	O
bottom	O
?	O

I	O
think	O
the	O
whiskers	O
are	O
actually	O
the	O
lowest	O
or	O
highest	O
data	O
point	O
within	O
1.5	O
IQR	O
.	O

So	O
if	O
there	O
are	O
no	O
data	O
points	O
between	O
Q3	O
and	O
Q3	O
+	O
1.5	O
IQR	O
,	O
then	O
the	O
top	O
whisker	O
won't	O
show	O
up	O
.	O

For	O
the	O
one	O
boxplot	Y
where	O
the	O
are	O
outliers	O
beyond	O
the	O
whiskers	O
on	O
both	O
the	O
top	O
and	O
the	O
bottom	O
,	O
the	O
whiskers	O
do	O
look	O
about	O
the	O
same	O
size	O
.	O

``	O
hist	Y
``	O
->	O
``	O
histogram	O
``	O
(	O
``	O
hist	Y
``	O
is	O
pyplot	Y
or	O
something	O
)	O
.	O

There	O
is	O
a	O
pandas	O
equivalent	O
to	O
this	O
`	O
cut	Y
`	O
there	O
is	O
a	O
section	O
describing	O
this	O
here	O
.	O

`	O
cut	Y
`	O
returns	O
the	O
open	O
closed	O
intervals	O
for	O
each	O
value	O
:	O
#CODE	O

Pandas	O
Dataframe	Y
,	O
Apply	Y
Function	O
,	O
Return	O
Index	O

Then	O
I	O
can	O
apply	N
the	O
function	O
to	O
my	O
dataframe	Y
,	O
grouped	O
by	O
I	O
D:	O
#CODE	O

If	O
I	O
resample	N
this	O
DataField	O
by	O
any	O
frequency	O
,	O
the	O
timezone	O
is	O
kept	O
:	O
#CODE	O

their	O
are	O
a	O
couple	O
of	O
outstanding	O
bugs	O
w.r.t	O
to	O
resample	N
and	O
extra	O
binning	O
:	O
#URL	O
if	O
you	O
would	O
like	O
to	O
investigate	O
and	O
try	O
to	O
pinpoint	O
(	O
or	O
better	O
yet	O
fix	O
)	O
would	O
be	O
appreciated	O
!	O

you	O
can	O
comment	O
on	O
that	O
issue	O
directly	O

@USER	O
;	O
You	O
mean	O
to	O
the	O
stack	N
exchange	O
answer	O
?	O
