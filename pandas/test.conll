At	O
the	O
first	O
step	O
I	O
used	O
`	O
df.T	Y
`	O
to	O
transpose	O
the	O
dataframe	Y
,	O
and	O
tried	O
something	O
like	O
`	O
df.value_counts()	O
`	O
,	O
however	O
I'd	O

Replace	O
NaN	O
in	O
a	O
dataframe	Y
with	O
random	O
values	O

I	O
want	O
to	O
replace	O
all	O
the	O
NaN	O
with	O
some	O
random	O
values	O
like	O
.	O

#CODE	O

Resample	Y
function	O
throwing	O
error	O
with	O
Twitter	O
Data	O

I	O
then	O
try	O
to	O
resample	O
for	O
analysis	O
#CODE	O

I'm	O
writing	O
several	O
pivot	O
tables	O
using	O
pandas	O
.	O

For	O
many	O
of	O
them	O
,	O
I	O
need	O
to	O
return	O
unique	O
values	O
.	O

In	O
a	O
two-dimensional	O
pivot	O
table	O
,	O
the	O
below	O
code	O
works	O
as	O
it	O
should	O
.	O

When	O
I	O
add	O
a	O
third	O
dimension	O
,	O
the	O
code	O
returns	O
the	O
count	O
rather	O
than	O
the	O
unique	O
count	O
.	O

I	O
suspect	O
this	O
has	O
something	O
to	O
do	O
with	O
the	O
aggfunc	Y
,	O
but	O
can't	O
determine	O
to	O
what	O
it	O
should	O
be	O
changed	O
.	O

Use	O
a	O
groupby	Y
to	O
get	O
at	O
each	O
combination	O
of	O
`	O
col_1	O
`	O
and	O
`	O
col_3	O
`	O
,	O
then	O
unstack	Y
to	O
get	O
the	O
`	O
col_3	O
`	O
values	O
as	O
columns	O
:	O
#CODE	O

Python	O
pandas	O
merge	O
or	O
concat	O
dataframes	O

The	O
data	O
is	O
for	O
2	O
products	O
(	O
BBG.XAMS.UL.S_pnl_pos_cost	O
and	O
BBG.XAMS.UNA.S_pnl_pos_cost	O
)	O
by	O
date	O
,	O
in	O
the	O
future	O
there	O
will	O
be	O
more	O
products	O
.	O

I	O
want	O
to	O
concat	O
or	O
merge	O
(	O
not	O
sure	O
which	O
)	O
the	O
list	O
of	O
dataframes	O
into	O
one	O
data	O
frame	O
(	O
called	O
result	O
)	O
so	O
they	O
look	O
like	O
:	O
#CODE	O

where	O
axis	O
is	O
the	O
date	O
.	O

It	O
looks	O
like	O
the	O
data	O
is	O
merged	O
by	O
date	O
,	O
but	O
I	O
am	O
missing	O
the	O
data	O
for	O
the	O
week	O
beginning	O
2015-03-23	O
.	O

My	O
current	O
concat	O
result	O
dataframe	Y
looks	O
like	O
:	O
#CODE	O

Try	O
using	O
axis=0	O
.	O

This	O
should	O
concat	O
column-wise	O
,	O
assuming	O
each	O
dataframe	Y
has	O
the	O
same	O
column	O
names	O
.	O

possible	O
duplicate	O
of	O
[	O
Pandas	O
join	Y
/	O
merge	Y
/	O
concat	Y
two	O
dataframes	O
]	O
(	O
#URL	O
)	O

Also	O
,	O
how	O
do	O
you	O
join	O
this	O
back	O
to	O
original	O
dataframe	Y
?	O

We	O
can	O
resample	O
this	O
to	O
days	O
;	O
it'll	O
be	O
a	O
much	O
longer	O
timeseries	O
,	O
of	O
course	O
,	O
but	O
memory	O
is	O
cheap	O
and	O
I'm	O
lazy	O
:	O
#CODE	O

How	O
do	O
I	O
merge	O
the	O
birth	O
rate	O
back	O
to	O
the	O
original	O
table	O
?	O

Indexes	O
aren't	O
compatible	O
...	O

Turns	O
out	O
size	O
isn't	O
such	O
an	O
issue	O
.	O

Python	O
&	O
Pandas	O
:	O
Unable	O
to	O
drop	O
columns	O

I	O
try	O
to	O
drop	O
the	O
data	O
,	O
but	O
it	O
reports	O
some	O
column	O
does	O
not	O
exist	O
.	O

#CODE	O

@USER	O
,	O
that's	O
possible	O
,	O
but	O
I	O
don't	O
know	O
how	O
to	O
deal	O
with	O
it	O
.	O

In	O
my	O
previous	O
experience	O
with	O
pandas	O
,	O
it	O
will	O
automatically	O
turn	O
the	O
second	O
`	O
Q	O
`	O
into	O
`	O
Q.1	O
`	O
when	O
reading	O
the	O
data	O
.	O

However	O
,	O
in	O
my	O
case	O
,	O
it	O
failed	O
to	O
do	O
it	O
,	O
and	O
I	O
don't	O
know	O
why	O
.	O

However	O
,	O
This	O
it	O
cannot	O
`	O
drop	Y
`	O
`	O
NCDC	O
`	O
either	O
.	O

You	O
can	O
then	O
drop	O
your	O
columns	O
:	O
#CODE	O

I	O
would	O
like	O
to	O
take	O
a	O
given	O
row	O
from	O
a	O
DataFrame	Y
and	O
prepend	O
or	O
append	O
to	O
the	O
same	O
DataFrame	Y
.	O

Rather	O
than	O
concat	O
I	O
would	O
just	O
assign	O
directly	O
to	O
the	O
df	O
after	O
`	O
shift	O
`	O
ing	O
,	O
then	O
use	O
`	O
iloc	Y
`	O
to	O
reference	O
the	O
position	O
you	O
want	O
to	O
assign	O
the	O
row	O
,	O
you	O
have	O
to	O
call	O
`	O
squeeze	Y
`	O
so	O
that	O
you	O
assign	O
just	O
the	O
values	O
and	O
lose	O
the	O
original	O
index	O
value	O
otherwise	O
it'll	O
raise	O
a	O
`	O
ValueError	O
`	O
:	O
#CODE	O

To	O
insert	O
at	O
the	O
end	O
:	O
#CODE	O

I'm	O
not	O
sure	O
exactly	O
what	O
you're	O
expecting	O
,	O
but	O
you	O
could	O
replace	O
your	O
lists	O
with	O
numpy	O
arrays	O
(	O
I	O
don't	O
think	O
it'll	O
improve	O
your	O
specific	O
code	O
):	O
#CODE	O

What	O
is	O
the	O
best	O
way	O
for	O
me	O
to	O
get	O
this	O
data	O
into	O
Pandas	O
?	O

Is	O
there	O
a	O
standard	O
way	O
I	O
could	O
use	O
`	O
read_table	Y
`	O
or	O
some	O
similar	O
function	O
to	O
read	O
this	O
file	O
directly	O
?	O

Should	O
I	O
write	O
a	O
script	O
to	O
insert	O
commas	O
where	O
all	O
the	O
column	O
breaks	O
are	O
and	O
then	O
read	O
it	O
in	O
as	O
CSV	O
?	O

(	O
I'd	O
just	O
do	O
the	O
latter	O
,	O
but	O
I'm	O
also	O
interested	O
in	O
becoming	O
better	O
with	O
Pandas	O
so	O
if	O
there's	O
an	O
out-of-the-box	O
way	O
I'd	O
like	O
to	O
know	O
it	O
.	O
)	O

Any	O
ideas	O
on	O
how	O
to	O
get	O
this	O
file	O
to	O
load	O
?	O

Unfortunately	O
I	O
can't	O
just	O
strip	O
out	O
accents	O
,	O
as	O
I	O
have	O
to	O
interface	O
with	O
software	O
that	O
requires	O
the	O
proper	O
name	O
,	O
and	O
I	O
have	O
a	O
ton	O
of	O
files	O
to	O
format	O
(	O
not	O
just	O
the	O
one	O
)	O
.	O

Thanks	O
!	O

pls	O
show	O
your	O
input	O
and	O
what	O
is	O
the	O
expected	O
output	O
,	O
in	O
a	O
copy-pastable	O
form	O
.	O

What	O
you	O
are	O
doing	O
is	O
very	O
inefficient	O
.	O

A	O
groupby	Y
should	O
try	O
to	O
use	O
vectorized	O
functions	O
when	O
possible	O
.	O

Then	O
join	O
them	O
up	O
at	O
the	O
end	O
.	O

Hi	O
Tom	O
,	O
it	O
doesn't	O
look	O
like	O
this	O
works	O
.	O

It	O
outputs	O
just	O
one	O
array	O
and	O
is	O
equivalent	O
to	O
df2	O
[	O
'	O
array	O
']	O
.sum()	Y
.	O

But	O
you	O
have	O
given	O
me	O
an	O
idea	O
with	O
apply	Y
.	O

Let	O
me	O
see	O
if	O
I	O
can	O
figure	O
something	O
out	O
.	O

You	O
need	O
`	O
apply	Y
(	O
your_func	O
,	O
axis=1	O
)`	O
to	O
work	O
on	O
a	O
row-by-row	O
basis	O
.	O

#CODE	O

Another	O
way	O
would	O
be	O
to	O
call	O
`	O
unique	Y
`	O
on	O
the	O
transpose	O
of	O
your	O
df	O
:	O
#CODE	O

Drop	O
values	O
satisfying	O
condition	O
plus	O
arbitrary	O
number	O
of	O
next	O
values	O
in	O
a	O
pandas	O
DataFrame	Y

So	O
my	O
final	O
goal	O
is	O
to	O
drop	Y
values	O
in	O
one	O
column	O
of	O
a	O
`	O
pandas	O
`	O
`	O
DataFrame	Y
`	O
according	O
to	O
some	O
condition	O
on	O
another	O
column	O
of	O
the	O
same	O
`	O
DataFrame	Y
`	O
,	O
plus	O
several	O
next	O
values	O
e.g.	O
:	O
#CODE	O

So	O
this	O
will	O
drop	O
the	O
records	O
where	O
the	O
condition	O
is	O
satisfied	O
,	O
but	O
how	O
do	O
I	O
drop	O
the	O
next	O
3	O
records	O
after	O
the	O
condition	O
was	O
satisfied	O
too	O
?	O

My	O
desired	O
output	O
would	O
look	O
something	O
like	O
this	O
:	O
#CODE	O

We	O
can	O
use	O
the	O
boolean	O
condition	O
index	O
to	O
slice	O
the	O
df	O
using	O
`	O
loc	Y
`	O
and	O
set	O
the	O
following	O
values	O
:	O
#CODE	O

Panda's	O
boxplot	Y
but	O
not	O
showing	O
the	O
box	O

Note	O
,	O
`	O
showbox	O
`	O
and	O
`	O
whiskerprops	O
`	O
are	O
the	O
`	O
kwds	O
`	O
of	O
boxplot	Y
,	O
which	O
are	O
in	O
turn	O
passed	O
to	O
`	O
matplotlib.boxplot	Y
`	O
.	O

Applying	O
aggregate	Y
function	O
on	O
columns	O
of	O
Pandas	O
pivot	O
table	O

I	O
generated	O
the	O
following	O
pivot	O
table	O
via	O
taking	O
maximum	O
of	O
values	O
in	O
`	O
Z	O
`	O
column	O
:	O
#CODE	O

Here's	O
a	O
fairly	O
general	O
solution	O
you	O
can	O
apply	O
to	O
multiple	O
columns	O
.	O

The	O
'	O
To	O
'	O
column	O
doesn't	O
need	O
to	O
be	O
rounded	O
,	O
I	O
just	O
included	O
it	O
for	O
the	O
generality	O
of	O
two	O
columns	O
rather	O
than	O
one	O
:	O
#CODE	O

428	O
base	O
,	O
mult	O
=	O
_gfc	O
(	O
freq	O
)	O

-->	O
429	O
return	O
tslib.dt64arr_to_periodarr	O
(	O
data.view	O
(	O
'	O
i8	O
')	O
,	O
base	O
,	O
tz	O
)	O

I	O
could	O
do	O
a	O
left	O
merge	Y
,	O
but	O
I	O
would	O
end	O
up	O
with	O
a	O
huge	O
file	O
.	O

Is	O
there	O
any	O
way	O
to	O
add	O
specific	O
rows	O
from	O
df2	O
to	O
df1	O
using	O
merge	Y
?	O

Unclear	O
why	O
you	O
think	O
a	O
left	O
merge	Y
would	O
produce	O
a	O
huge	O
file	O
,	O
by	O
performing	O
a	O
left	O
merge	Y
on	O
the	O
product	O
id	O
you	O
are	O
stating	O
that	O
you	O
are	O
only	O
interested	O
in	O
matches	O
in	O
the	O
product_id	O
column	O
only	O

Just	O
perform	O
a	O
left	O
`	O
merge	Y
`	O
on	O
'	O
product_id	O
'	O
column	O
:	O
#CODE	O

What	O
would	O
be	O
the	O
Python	O
equivalent	O
?	O

I	O
cannot	O
think	O
of	O
a	O
way	O
to	O
translate	O
this	O
where	O
statement	O
into	O
pandas	O
syntax	O
.	O

The	O
only	O
way	O
I	O
can	O
think	O
of	O
is	O
to	O
add	O
an	O
arbitrary	O
field	O
to	O
people_usa	O
(	O
e.g.	O
`	O
people_usa	O
[	O
'	O
dummy	O
']	O
=1	O
`)	O
,	O
do	O
a	O
left	O
join	Y
,	O
then	O
take	O
only	O
the	O
records	O
where	O
'	O
dummy	O
'	O
is	O
nan	O
,	O
then	O
delete	O
the	O
dummy	O
field	O
-	O
which	O
seems	O
a	O
bit	O
convoluted	O
.	O

Does	O
this	O
work	O
only	O
on	O
the	O
index	O
of	O
the	O
dataframe	Y
?	O

I'd	O
like	O
the	O
option	O
to	O
specify	O
the	O
field	O
(	O
s	O
)	O
to	O
apply	O
this	O
to	O

Is	O
there	O
any	O
easy	O
way	O
to	O
do	O
this	O
if	O
you	O
have	O
multiple	O
columns	O
to	O
check	O
/	O
join	O
?	O

You	O
could	O
do	O
a	O
`	O
merge	Y
`	O
and	O
then	O
eliminate	O
the	O
rows	O
that	O
exist	O
in	O
the	O
merged	O
df	O
otherwise	O
you'd	O
have	O
to	O
build	O
a	O
boolean	O
condition	O
for	O
all	O
the	O
columns	O
you	O
want	O
to	O
compare	O
but	O
presumably	O
when	O
checking	O
the	O
multiple	O
columns	O
you're	O
stating	O
that	O
it's	O
unique	O
for	O
those	O
columns	O
,	O
correct	O
?	O

For	O
instance	O
it's	O
not	O
a	O
match	O
if	O
say	O
col1	O
and	O
col2	O
match	O
but	O
col3	O
does	O
not	O

Yes	O
merge	Y
is	O
what	O
I	O
have	O
been	O
doing	O
but	O
it	O
feels	O
like	O
a	O
hassle	O
.	O

I've	O
come	O
up	O
with	O
this	O
,	O
using	O
itertools	Y
,	O
to	O
find	O
mid-day	O
timestamps	O
and	O
group	O
them	O
by	O
date	O
,	O
and	O
now	O
I'm	O
coming	O
up	O
short	O
trying	O
to	O
apply	O
imap	O
to	O
find	O
the	O
means	O
.	O

#CODE	O

Since	O
not	O
sure	O
what	O
your	O
end	O
output	O
should	O
look	O
like	O
,	O
just	O
create	O
a	O
time-based	O
grouper	Y
manually	O
(	O
this	O
is	O
essentially	O
a	O
resample	Y
)	O
,	O
but	O
doesn't	O
do	O
anything	O
with	O
the	O
final	O
results	O
(	O
its	O
just	O
a	O
list	O
of	O
the	O
aggregated	O
values	O
)	O
#CODE	O

You	O
can	O
get	O
reasonable	O
fancy	O
here	O
and	O
say	O
return	O
a	O
pandas	O
object	O
(	O
and	O
potentially	O
`	O
concat	N
`	O
them	O
)	O
.	O

and	O
I	O
want	O
to	O
pivot	N
it	O
like	O
this	O
:	O
#CODE	O

I	O
am	O
calling	O
a	O
function	O
from	O
within	O
a	O
'	O
for	O
each	O
loop	O
'	O
which	O
attempts	O
to	O
insert	N
values	O
into	O
a	O
Pandas	O
DataFrame	Y
based	O
on	O
a	O
specified	O
column	O
start	O
and	O
end	O
location	O
.	O

The	O
function	O
is	O
this	O
:	O
#CODE	O

My	O
issue	O
is	O
that	O
despite	O
the	O
same	O
starting	O
conditions	O
when	O
I	O
call	O
this	O
function	O
it	O
seems	O
to	O
generate	O
a	O
list	O
of	O
inconsistent	O
length	O
.	O

e.g.	O
with	O
values	O
of	O
srowb	O
=	O
1	O
and	O
erowb	O
=	O
18	O
it	O
will	O
generate	O
a	O
list	O
(	O
tmp_brollb	O
)	O
which	O
has	O
either	O
len	Y
(	O
tmp_brollb	O
)	O
=	O
17	O
or	O
len	Y
(	O
tmp_brollb	O
)	O
=	O
18	O

Use	O
`	O
max	Y
`	O
and	O
check	O
for	O
equality	O
using	O
`	O
eq	Y
`	O
and	O
cast	O
the	O
boolean	O
df	O
to	O
int	O
using	O
`	O
astype	Y
`	O
,	O
this	O
will	O
convert	O
`	O
True	O
`	O
and	O
`	O
False	O
`	O
to	O
`	O
1	O
`	O
and	O
`	O
0	O
`	O
:	O
#CODE	O

Thanks	O
@USER	O
.	O

Did	O
you	O
try	O
my	O
original	O
post	O
?	O

I	O
would	O
be	O
interested	O
to	O
know	O
how	O
much	O
time	O
this	O
one	O
is	O
taking	O
compared	O
to	O
yours	O
?	O

`	O
for	O
i	O
in	O
range	O
(	O
len	Y
(	O
df	O
)):	O
...	O

df.loc	Y
[	O
i	O
]	O
[	O
df.loc	Y
[	O
i	O
]	O
.idxmax	Y
(	O
axis=1	O
)]	O
=	O
1	O
...	O

df.loc	Y
[	O
i	O
]	O
[	O
df.loc	Y
[	O
i	O
]	O
!	O
=	O
1	O
]	O
=	O
0	O
`	O

I	O
am	O
trying	O
to	O
normalize	N
the	O
missing	O
values	O
in	O
matrix	O
.	O

Here	O
is	O
the	O
code	O
.	O

#CODE	O

Last	O
line	O
should	O
replace	N
the	O
values	O
in	O
dataset1	O
by	O
mean	O
values	O
from	O
`	O
ds2_mean	O
[	O
1	O
]`	O
.	O

But	O
it	O
does	O
not	O
do	O
.	O

Anything	O
wrong	O
here	O
?	O

And	O
after	O
that	O
can	O
I	O
replace	N
NaN	O
with	O
the	O
average	O
value	O
of	O
it's	O
neighbours	O
in	O
dataset1	O
?	O

it	O
does	O
wrong	O
.	O

For	O
any	O
x	O
in	O
dataset2	O
it	O
has	O
mapped	O
value	O
in	O
col2	O
.	O

It	O
should	O
replace	N
all	O
values	O
of	O
x	O
in	O
ds1	O
by	O
mapped	O
value	O
.	O

But	O
this	O
also	O
does	O
not	O
do	O
it	O

Sorry	O
can	O
you	O
explain	O
clearer	O
,	O
what	O
are	O
you	O
mapping	O
from	O
what	O
to	O
what	O
exactly	O
?	O

By	O
default	O
fillna	Y
will	O
use	O
the	O
index	O
so	O
how	O
do	O
you	O
want	O
the	O
mapping	O
from	O
`	O
ds2	O
`	O
to	O
map	N
to	O
the	O
missing	O
values	O
in	O
`	O
ds1	O
`	O
?	O

Are	O
you	O
wanting	O
to	O
map	N
using	O
the	O
values	O
in	O
`	O
ds2	O
[	O
0	O
]`	O
as	O
the	O
index	O
lookup	O
?	O

So	O
use	O
the	O
index	O
from	O
`	O
ds1	O
`	O
find	O
value	O
in	O
`	O
ds2	O
[	O
0	O
]`	O
and	O
return	O
`	O
ds2	O
[	O
1	O
]`	O
?	O

yes	O
,	O
I	O
want	O
to	O
use	O
the	O
index	O
from	O
ds1	O
find	O
value	O
in	O
ds2	O
[	O
0	O
]	O
and	O
replace	N
it	O
with	O
ds2	O
[	O
1	O
]"	O
sorry	O
for	O
inconvenience	O

I	O
want	O
to	O
add	O
a	O
new	O
column	O
which	O
contains	O
values	O
based	O
on	O
df	O
[	O
'	O
diff	N
']	O

When	O
using	O
`	O
DataFrame.apply	Y
`	O
if	O
you	O
use	O
`	O
axis=0	O
`	O
it	O
applies	O
the	O
condition	O
through	O
columns	O
,	O
to	O
use	O
`	O
apply	Y
`	O
to	O
go	O
through	O
each	O
row	O
,	O
you	O
need	O
`	O
axis=1	O
`	O
.	O

But	O
given	O
that	O
,	O
you	O
can	O
use	O
`	O
Series.apply	Y
`	O
instead	O
of	O
`	O
DataFrame.apply	Y
`	O
on	O
the	O
`'	O
diff	Y
'`	O
series	O
.	O

Example	O
-	O
#CODE	O

You	O
can	O
just	O
set	O
all	O
the	O
values	O
that	O
meet	O
your	O
criteria	O
rather	O
than	O
looping	O
over	O
the	O
df	O
by	O
calling	O
`	O
apply	Y
`	O
so	O
the	O
following	O
should	O
work	O
and	O
as	O
it's	O
vectorised	O
will	O
scale	O
better	O
for	O
larger	O
datasets	O
:	O
#CODE	O

this	O
will	O
set	O
all	O
rows	O
that	O
meet	O
the	O
criteria	O
,	O
the	O
problem	O
using	O
`	O
apply	Y
`	O
is	O
that	O
it's	O
just	O
syntactic	O
sugar	O
for	O
a	O
`	O
for	O
`	O
loop	O
and	O
where	O
possible	O
this	O
should	O
be	O
avoided	O
where	O
a	O
vectorised	O
solution	O
exists	O
.	O

Then	O
you	O
can	O
`	O
stack	Y
`	O
(	O
first	O
by	O
`'	O
Marker	O
'`	O
then	O
by	O
`'	O
mrk	O
'`)	O
:	O
#CODE	O

Python	O
DataFrame	Y
-	O
apply	N
different	O
calculations	O
due	O
to	O
a	O
column's	O
value	O

You	O
could	O
do	O
this	O
using	O
2	O
`	O
loc	Y
`	O
calls	O
:	O
#CODE	O

There	O
are	O
two	O
reasons	O
whiskers	O
length	O
vary	O
from	O
one	O
boxplot	Y
to	O
any	O
other	O
boxplot	Y

Are	O
you	O
asking	O
why	O
the	O
top	O
whisker	O
isn't	O
the	O
same	O
length	O
as	O
the	O
bottom	O
?	O

I	O
think	O
the	O
whiskers	O
are	O
actually	O
the	O
lowest	O
or	O
highest	O
data	O
point	O
within	O
1.5	O
IQR	O
.	O

So	O
if	O
there	O
are	O
no	O
data	O
points	O
between	O
Q3	O
and	O
Q3	O
+	O
1.5	O
IQR	O
,	O
then	O
the	O
top	O
whisker	O
won't	O
show	O
up	O
.	O

For	O
the	O
one	O
boxplot	Y
where	O
the	O
are	O
outliers	O
beyond	O
the	O
whiskers	O
on	O
both	O
the	O
top	O
and	O
the	O
bottom	O
,	O
the	O
whiskers	O
do	O
look	O
about	O
the	O
same	O
size	O
.	O

``	O
hist	Y
``	O
->	O
``	O
histogram	O
``	O
(	O
``	O
hist	Y
``	O
is	O
pyplot	Y
or	O
something	O
)	O
.	O

There	O
is	O
a	O
pandas	O
equivalent	O
to	O
this	O
`	O
cut	Y
`	O
there	O
is	O
a	O
section	O
describing	O
this	O
here	O
.	O

`	O
cut	Y
`	O
returns	O
the	O
open	O
closed	O
intervals	O
for	O
each	O
value	O
:	O
#CODE	O

Pandas	O
Dataframe	Y
,	O
Apply	Y
Function	O
,	O
Return	O
Index	O

Then	O
I	O
can	O
apply	N
the	O
function	O
to	O
my	O
dataframe	Y
,	O
grouped	O
by	O
I	O
D:	O
#CODE	O

If	O
I	O
resample	N
this	O
DataField	O
by	O
any	O
frequency	O
,	O
the	O
timezone	O
is	O
kept	O
:	O
#CODE	O

their	O
are	O
a	O
couple	O
of	O
outstanding	O
bugs	O
w.r.t	O
to	O
resample	N
and	O
extra	O
binning	O
:	O
#URL	O
if	O
you	O
would	O
like	O
to	O
investigate	O
and	O
try	O
to	O
pinpoint	O
(	O
or	O
better	O
yet	O
fix	O
)	O
would	O
be	O
appreciated	O
!	O

you	O
can	O
comment	O
on	O
that	O
issue	O
directly	O

@USER	O
;	O
You	O
mean	O
to	O
the	O
stack	N
exchange	O
answer	O
?	O

I	O
think	O
that	O
I	O
understand	O
what's	O
going	O
on	O
:	O
create	O
a	O
frequency	O
table	O
of	O
ALL	O
words	O
.	O

After	O
each	O
operation	O
,	O
drop	N
all	O
relevant	O
columns	O
,	O
then	O
finally	O
count	O
all	O
remaining	O
columns	O
.	O

Also	O
,	O
I	O
quickly	O
tried	O
this	O
in	O
Python	O
3.4.3	O
and	O
I	O
got	O
the	O
error	O
that	O
freqDf	O
isn't	O
defined	O
.	O

Should	O
I	O
first	O
create	O
a	O
new	O
table	O
named	O
freqDf	O
?	O

`	O
df.precedingWord.isin	O
(	O
neuter	O
)`	O
is	O
just	O
a	O
Series	O
of	O
True	O
or	O
False	O
(	O
results	O
of	O
the	O
previous	O
test	O
`	O
isin	Y
`)	O
,	O
and	O
pandas	O
will	O
just	O
access	O
True	O
indexes	O
with	O
`	O
loc	Y
`	O

I	O
have	O
tried	O
a	O
some	O
join	Y
/	O
merge	Y
ideas	O
but	O
can't	O
seem	O
to	O
get	O
it	O
to	O
work	O
.	O

Just	O
`	O
concat	Y
`	O
them	O
and	O
pass	O
param	O
`	O
axis=1	O
`	O
:	O
#CODE	O

Or	O
`	O
merge	Y
`	O
on	O
'	O
Symbol	O
'	O
column	O
:	O
#CODE	O

Pandas	O
:	O
join	N
with	O
outer	O
product	O

How	O
to	O
join	N
/	O
multiply	O
the	O
DataFrames	O
`	O
areas	O
`	O
and	O
`	O
demand	O
`	O
together	O
in	O
a	O
decent	O
way	O
?	O

Now	O
`	O
apply	Y
`	O
needs	O
to	O
return	O
a	O
`	O
Series	Y
`	O
,	O
not	O
a	O
`	O
DataFrame	Y
`	O
.	O

One	O
way	O
to	O
turn	O
a	O
`	O
DataFrame	Y
`	O
into	O
a	O
`	O
Series	Y
`	O
is	O
to	O
use	O
`	O
stack	Y
`	O
.	O

`	O
stack	Y
`	O
this	O
DataFrame	Y
.	O

This	O
can	O
be	O
done	O
with	O
`	O
unstack	Y
`	O
:	O
#CODE	O

`	O
del	O
`	O
+	O
`	O
pivot	Y
`	O
turns	O
out	O
to	O
be	O
faster	O
than	O
`	O
pivot_table	Y
`	O
in	O
this	O
case	O
.	O

Maybe	O
the	O
reason	O
`	O
pivot	Y
`	O
exists	O
is	O
because	O
it	O
is	O
faster	O
than	O
`	O
pivot_table	Y
`	O
for	O
those	O
cases	O
where	O
it	O
is	O
applicable	O
(	O
such	O
as	O
when	O
you	O
don't	O
need	O
aggregation	O
)	O
.	O

`	O
apply	Y
`	O
is	O
now	O
among	O
my	O
top	O
5	O
functions	O
to	O
always	O
remember	O
.	O

Concerning	O
the	O
`	O
pivot_table	Y
`	O
solution	O
:	O
At	O
which	O
point	O
am	O
I	O
supposed	O
to	O
enter	O
the	O
line	O
?	O

No	O
matter	O
when	O
in	O
my	O
attempt	O
above	O
,	O
I	O
always	O
get	O
`	O
no	O
item	O
named	O
Edge	O
`	O
.	O

Or	O
pass	O
`	O
axis=0	O
`	O
to	O
`	O
loc	Y
`	O
:	O
#CODE	O

I've	O
got	O
2	O
pandas	O
dataframes	O
,	O
each	O
of	O
them	O
has	O
an	O
index	O
with	O
dtype	Y
`	O
object	O
`	O
,	O
and	O
in	O
both	O
of	O
them	O
I	O
can	O
see	O
the	O
value	O
`	O
533	O
`	O
.	O

However	O
,	O
when	O
I	O
join	Y
them	O
the	O
result	O
is	O
empty	O
,	O
as	O
one	O
of	O
them	O
is	O
the	O
number	O
`	O
533	O
`	O
and	O
the	O
other	O
is	O
a	O
string	O
`"	O
533	O
"`	O
.	O

Ideally	O
I	O
would	O
like	O
something	O
like	O
`	O
apply_chunk()	O
`	O
which	O
is	O
the	O
same	O
as	O
apply	Y
but	O
only	O
works	O
on	O
a	O
piece	O
of	O
the	O
dataframe	Y
.	O

This	O
has	O
to	O
be	O
a	O
common	O
problem	O
though	O
,	O
is	O
there	O
a	O
design	O
pattern	O
I	O
should	O
be	O
using	O
for	O
adding	O
columns	O
to	O
large	O
pandas	O
dataframes	O
?	O

whats	O
about	O
using	O
the	O
apply	Y
method	O
?	O

Anytime	O
you	O
find	O
yourself	O
using	O
`	O
apply	Y
`	O
or	O
`	O
iloc	Y
`	O
in	O
a	O
loop	O
it's	O
likely	O
that	O
Pandas	O
is	O
operating	O
much	O
slower	O
than	O
is	O
optimal	O
.	O

Convert	O
freq	N
string	O
to	O
DateOffset	Y
in	O
pandas	O

In	O
pandas	O
documentation	O
one	O
can	O
read	O
"	O
Under	O
the	O
hood	O
,	O
these	O
frequency	O
strings	O
are	O
being	O
translated	O
into	O
an	O
instance	O
of	O
pandas	O
DateOffset	Y
"	O
when	O
speaking	O
of	O
freq	N
string	O
such	O
as	O
"	O
W	O
"	O
or	O
"	O
W-SUN	O
"	O
.	O

stack	N
/	O
unstack	N
/	O
pivot	N
dataframe	Y
on	O
python	O
/	O
pandas	O

yes	O
,	O
`	O
isnull	Y
`	O
will	O
create	O
a	O
boolean	O
series	O
,	O
`	O
all	Y
`	O
returns	O
`	O
True	O
`	O
if	O
all	O
are	O
`	O
True	O
`	O

Then	O
merge	N
the	O
sub-tables	O
back	O
together	O
in	O
a	O
way	O
that	O
replaces	O
NaN	O
values	O
when	O
there	O
is	O
data	O
in	O
one	O
of	O
the	O
tables	O
.	O

I	O
regularly	O
work	O
with	O
very	O
large	O
data	O
sets	O
that	O
are	O
too	O
big	O
to	O
manipulate	O
in	O
memory	O
.	O

I	O
would	O
like	O
to	O
read	O
in	O
a	O
csv	O
file	O
iteratively	O
,	O
append	N
each	O
chunk	O
into	O
HDFStore	Y
object	O
,	O
and	O
then	O
work	O
with	O
subsets	O
of	O
the	O
data	O
.	O

If	O
you	O
replace	N
that	O
line	O
with	O
:	O

I	O
wanted	O
to	O
merge	N
these	O
files	O
so	O
that	O
i	O
have	O
something	O
like	O
this	O
#CODE	O

If	O
it's	O
six	O
,	O
then	O
you	O
can	O
use	O
join	Y
method	O
by	O
@USER	O
Hayden	O
.	O

Then	O
you	O
can	O
simply	O
`	O
join	Y
`	O
them	O
:	O
#CODE	O

@USER	O
when	O
you	O
do	O
a	O
join	Y
with	O
2x2	O
duplicates	O
you	O
get	O
4	O
in	O
the	O
joined	O
DataFrame	Y
.	O

It's	O
unclear	O
how	O
pandas	O
should	O
join	N
in	O
this	O
case	O
,	O
so	O
you	O
need	O
to	O
be	O
more	O
explicit	O
to	O
it	O
(	O
and	O
tell	O
it	O
what	O
do	O
you	O
want	O
)	O
.	O

On	O
the	O
similar	O
note	O
,	O
is	O
there	O
a	O
way	O
to	O
merge	N
values	O
based	O
on	O
index	O
.	O

For	O
example	O
,	O
instead	O
of	O
listing	O
Bact5	O
in	O
two	O
rows	O
,	O
can	O
we	O
merge	N
its	O
value	O
corresponding	O
to	O
file2	O
in	O
one	O
row	O
separated	O
by	O
a	O
delimeter	O
?	O

Pandas	O
dataframe	Y
insert	N
rows	O

I	O
want	O
to	O
insert	N
rows	O
in	O
DF	O
and	O
modify	O
its	O
related	O
values	O
:	O

The	O
code	O
can	O
only	O
append	N
rows	O
but	O
how	O
to	O
modify	O
its	O
values	O
in	O
a	O
faster	O
way	O
?	O

I	O
want	O
to	O
use	O
a	O
function	O
from	O
an	O
add-in	O
in	O
excel	O
and	O
apply	N
it	O
to	O
some	O
data	O
i	O
have	O
simulated	O
in	O
python	O
.	O

I	O
need	O
to	O
be	O
able	O
to	O
call	O
the	O
add-in	O
and	O
apply	N
my	O
data	O
indexes	O
there	O
...	O
something	O
along	O
these	O
lines	O
:	O
=	O
add-in_name	O
(	O
data_range1	O
,	O
data_range2	O
,	O
"	O
GGCV	O
")	O

After	O
reading	O
one	O
line	O
I	O
append	N
the	O
dictionary	O
to	O
a	O
list	O
(	O
so	O
,	O
the	O
number	O
of	O
dictionaries	O
in	O
the	O
list	O
is	O
equal	O
to	O
the	O
number	O
of	O
lines	O
in	O
the	O
file	O
)	O
.	O

I	O
can	O
easily	O
do	O
this	O
iteratively	O
with	O
loops	O
,	O
but	O
I've	O
read	O
that	O
you're	O
supposed	O
to	O
slice	Y
/	O
merge	Y
/	O
join	Y
data	O
frames	O
holistically	O
,	O
so	O
I'm	O
trying	O
to	O
see	O
if	O
I	O
can	O
find	O
a	O
better	O
way	O
of	O
doing	O
this	O
.	O

A	O
join	Y
will	O
give	O
me	O
all	O
the	O
stuff	O
that	O
matches	O
,	O
but	O
that's	O
not	O
exactly	O
what	O
I'm	O
looking	O
for	O
,	O
since	O
I	O
need	O
a	O
resulting	O
dataframe	Y
for	O
each	O
key	O
(	O
i.e.	O
for	O
every	O
row	O
)	O
in	O
A	O
.	O

You	O
then	O
want	O
to	O
apply	N
some	O
function	O
to	O
each	O
group	O
of	O
rows	O
in	O
`	O
b	O
`	O
where	O
the	O
`	O
b	O
[	O
"	O
key	O
"]`	O
is	O
one	O
of	O
the	O
values	O
in	O
`	O
keys	N
`	O
.	O

Under	O
the	O
covers	O
,	O
these	O
are	O
really	O
similar	O
uses	O
of	O
`	O
apply	Y
`	O
.	O

`	O
loop_iter	O
=	O
len	Y
(	O
A	O
)	O
/	O
max	Y
(	O
A	O
[	O
'	O
SEQ_NUM	O
'])	O

Easy	O
way	O
to	O
apply	N
transformation	O
from	O
`	O
pandas.get_dummies	Y
`	O
to	O
new	O
data	O
?	O

As	O
an	O
aside	O
that	O
may	O
help	O
you	O
in	O
the	O
meantime	O
,	O
with	O
datetime-indexed	O
data	O
,	O
[	O
resample	Y
]	O
(	O
#URL	O
)	O
is	O
usually	O
a	O
better	O
choice	O
than	O
reindex	Y
.	O

Call	O
`	O
transform	Y
`	O
on	O
the	O
'	O
measurement	O
'	O
column	O
and	O
pass	O
the	O
method	O
`	O
diff	Y
`	O
,	O
transform	Y
returns	O
a	O
series	O
with	O
an	O
index	O
aligned	O
to	O
the	O
original	O
df	O
:	O
#CODE	O

If	O
you	O
are	O
intending	O
to	O
apply	N
some	O
sorting	O
on	O
the	O
result	O
of	O
`	O
transform	Y
`	O
then	O
sort	O
the	O
df	O
first	O
:	O
#CODE	O

Or	O
you	O
can	O
slice	O
the	O
columns	O
and	O
pass	O
this	O
to	O
`	O
drop	Y
`	O
:	O
#CODE	O

These	O
values	O
are	O
median	N
values	O
I	O
calculated	O
from	O
elsewhere	O
,	O
and	O
I	O
have	O
also	O
their	O
variance	O
and	O
standard	O
deviation	O
(	O
and	O
standard	O
error	O
,	O
too	O
)	O
.	O

=	O
Hash	O
[	O
0	O
]	O
was	O
my	O
point	O
,	O
but	O
even	O
without	O
arithmetic	O
,	O
there	O
will	O
be	O
a	O
huge	O
range	O
values	O
for	O
the	O
keys	O
that	O
will	O
give	O
potentially	O
unfortunate	O
results	O
.	O

if	O
precision	O
is	O
to	O
decimal	O
place	O
,	O
I'd	O
multiply	O
it	O
by	O
10	O
and	O
truncate	N
maybe	O
.	O

the	O
documentation	O
to	O
concat	Y
is	O
impenetrable	O
and	O
its	O
hard	O
to	O
find	O
examples	O
of	O
this	O
relatively	O
simple	O
task	O
in	O
the	O
docs	O

If	O
you	O
had	O
not	O
called	O
`	O
apply	Y
`	O
on	O
the	O
`	O
groupby	Y
`	O
object	O
then	O
you	O
could	O
access	O
the	O
`	O
groups	Y
`	O
:	O
#CODE	O

pandas	O
groupby	Y
X	O
,	O
Y	O
and	O
select	O
last	O
week	O
of	O
X1	O
and	O
X2	O
(	O
which	O
have	O
diff	N
frequency	O
)	O

Then	O
you	O
can	O
select	O
the	O
rows	O
you	O
want	O
in	O
an	O
apply	Y
call	O
on	O
the	O
grouped	O
object	O
:	O
#CODE	O

If	O
you	O
can't	O
upgrade	O
or	O
don't	O
solve	O
the	O
issue	O
you	O
have	O
with	O
0.14	O
,	O
you	O
can	O
try	O
to	O
use	O
`	O
ix	Y
`	O
instead	O
of	O
`	O
iloc	Y
`	O

How	O
do	O
I	O
export	O
multiple	O
pivot	N
tables	O
from	O
python	O
using	O
pandas	O
to	O
a	O
single	O
csv	O
document	O
?	O

Say	O
I	O
have	O
a	O
function	O
pivots()	O
which	O
aggregates	O
pivot	N
tables	O
#CODE	O

I	O
know	O
how	O
to	O
export	O
a	O
single	O
pivot	N
table	O
#CODE	O

You	O
can	O
use	O
`	O
to_csv	Y
(	O
path	O
,	O
mode=	O
'	O
a	O
')`	O
to	O
append	N
files	O
.	O

Use	O
`	O
shift	Y
`	O
and	O
`	O
np.log	Y
`	O
:	O
#CODE	O

I'd	O
look	O
at	O
seeing	O
if	O
you	O
can	O
export	O
it	O
in	O
it's	O
raw	O
form	O
,	O
otherwise	O
this	O
must	O
be	O
a	O
common	O
problem	O
and	O
someone	O
somewhere	O
has	O
probably	O
coded	O
a	O
method	O
to	O
strip	N
the	O
emojis	O
out	O
of	O
the	O
text	O

Python	O
pandas	O
map	N
dict	O
keys	O
to	O
values	O

I	O
have	O
a	O
csv	O
for	O
input	O
,	O
whose	O
row	O
values	O
I'd	O
like	O
to	O
join	N
into	O
a	O
new	O
field	O
.	O

This	O
new	O
field	O
is	O
a	O
constructed	O
url	O
,	O
which	O
will	O
then	O
be	O
processed	O
by	O
the	O
requests.post()	Y
method	O
.	O

I	O
tried	O
to	O
map	N
values	O
to	O
keys	O
with	O
a	O
dict	O
comprehension	O
,	O
but	O
the	O
assignment	O
of	O
a	O
key	O
like	O
'	O
FIRST_NAME	O
'	O
could	O
end	O
up	O
mapping	O
to	O
values	O
from	O
an	O
arbitrary	O
field	O
like	O
test_df	O
[	O
'	O
CITY	O
']	O
.	O

which	O
will	O
give	O
you	O
output	O
as	O
follows	O
:	O
`	O
[	O
{	O
'	O
FIRST_NAME	O
'	O
:	O
...,	O
'	O
LAST_NAME	O
'	O
:	O
...	O
}	O
,	O
{	O
'	O
FIRST_NAME	O
'	O
:	O
...,	O
'	O
LAST_NAME	O
'	O
:	O
...	O
}	O
]`	O
(	O
which	O
will	O
give	O
you	O
a	O
list	O
that	O
has	O
equal	O
length	O
as	O
`	O
test_df	O
`)	O
.	O

This	O
might	O
be	O
one	O
possibility	O
to	O
easily	O
map	N
it	O
to	O
a	O
correct	O
row	O
.	O

Do	O
you	O
know	O
if	O
append	Y
returns	O
a	O
copy	O
/	O
view	O
/	O
reference	O
of	O
the	O
original	O
dataframe	Y
?	O

Right	O
now	O
,	O
I	O
am	O
trying	O
to	O
replace	N
a	O
stored	O
procedure	O
with	O
a	O
Python	O
service	O
,	O
and	O
the	O
temp	O
tables	O
with	O
Pandas	O
dataframes	O
.	O

You	O
could	O
pass	O
an	O
argument	O
to	O
`	O
apply	Y
`	O
:	O
#CODE	O

Originally	O
,	O
I	O
used	O
append	Y
api	O
to	O
create	O
a	O
single	O
table	O
'	O
impression	O
'	O
,	O
however	O
that	O
was	O
taking	O
80sec	O
per	O
dataframe	Y
and	O
given	O
that	O
I	O
have	O
almost	O
200	O
of	O
files	O
to	O
be	O
processed	O
,	O
the	O
'	O
append	Y
'	O
appeared	O
to	O
be	O
too	O
slow	O
.	O

Also	O
,	O
why	O
is	O
append	Y
so	O
much	O
slower	O
than	O
put	O
?	O

pandas	O
merge	Y
with	O
MultiIndex	Y
,	O
when	O
only	O
one	O
level	O
of	O
index	O
is	O
to	O
be	O
used	O
as	O
key	O

I	O
want	O
to	O
recover	O
the	O
values	O
in	O
the	O
column	O
'	O
_Cat	O
'	O
from	O
df2	O
and	O
merge	N
them	O
into	O
df1	O
for	O
the	O
appropriate	O
values	O
of	O
'	O
_ItemId	O
'	O
.	O

This	O
is	O
almost	O
(	O
I	O
think	O
?	O
)	O
a	O
standard	O
many-to-one	O
merge	N
,	O
except	O
that	O
the	O
appropriate	O
key	O
for	O
the	O
left	O
df	O
is	O
one	O
of	O
MultiIndex	Y
levels	O
.	O

Or	O
is	O
there	O
a	O
better	O
approach	O
to	O
this	O
merge	Y
?	O

loc	Y
will	O
not	O
attempt	O
to	O
use	O
a	O
number	O
(	O
eg	O
1	O
)	O
as	O
a	O
positional	O
argument	O
at	O
all	O
(	O
and	O
will	O
raise	O
instead	O
);	O
see	O
main	O
pandas	O
docs	O
/	O
selecting	O
data	O

I	O
have	O
the	O
following	O
boxplot	Y
:	O
#CODE	O

My	O
question	O
is	O
:	O
how	O
can	O
I	O
change	O
the	O
whiskers	O
/	O
quantiles	O
being	O
plotted	O
in	O
the	O
boxplot	Y
?	O

it'll	O
be	O
difficult	O
to	O
translate	N
those	O
`	O
ddply	O
`	O
calls	O
to	O
pandas	O
.	O

I	O
guess	O
`	O
groupby	Y
`	O
should	O
be	O
used	O
but	O
I	O
find	O
this	O
format	O
very	O
cryptic	O
so	O
it's	O
hard	O
to	O
translate	N
to	O
python	O

If	O
you	O
drop	N
the	O
"	O
%	O
"	O
sign	O
,	O
you	O
can	O
make	O
the	O
plot	O
without	O
ticks	O
.	O

Append	N
Two	O
Dataframes	O
Together	O
(	O
Pandas	O
,	O
Python3	O
)	O

I	O
am	O
trying	O
to	O
append	N
/	O
join	N
(	O
?	O
)	O
two	O
different	O
dataframes	O
together	O
that	O
don't	O
share	O
any	O
overlapping	O
data	O
.	O

I	O
am	O
trying	O
to	O
append	N
these	O
together	O
using	O
#CODE	O

EDIT	O
:	O
in	O
regards	O
to	O
Edchum's	O
answers	O
,	O
I	O
have	O
tried	O
merge	Y
and	O
join	Y
but	O
each	O
create	O
somewhat	O
strange	O
tables	O
.	O

OK	O
,	O
what	O
you	O
have	O
to	O
do	O
is	O
reindex	Y
or	O
reset	Y
the	O
index	O
so	O
they	O
align	N

Use	O
`	O
concat	Y
`	O
and	O
pass	O
param	O
`	O
axis=1	O
`	O
:	O
#CODE	O

`	O
join	Y
`	O
also	O
works	O
:	O
#CODE	O

As	O
does	O
`	O
merge	Y
`	O
:	O
#CODE	O

In	O
the	O
case	O
where	O
the	O
indices	O
do	O
not	O
align	N
where	O
for	O
example	O
your	O
first	O
df	O
has	O
index	O
`	O
[	O
0	O
,	O
1	O
,	O
2	O
,	O
3	O
]`	O
and	O
your	O
second	O
df	O
has	O
index	O
`	O
[	O
0	O
,	O
2	O
]`	O
this	O
will	O
mean	O
that	O
the	O
above	O
operations	O
will	O
naturally	O
align	N
against	O
the	O
first	O
df's	O
index	O
resulting	O
in	O
a	O
`	O
NaN	O
`	O
row	O
for	O
index	O
row	O
`	O
1	O
`	O
.	O

To	O
fix	O
this	O
you	O
can	O
reindex	N
the	O
second	O
df	O
either	O
by	O
calling	O
`	O
reset_index()	Y
`	O
or	O
assign	O
directly	O
like	O
so	O
:	O
`	O
df2.index	O
=[	O
0	O
,	O
1	O
]`	O
.	O

And	O
you	O
could	O
always	O
drop	N
back	O
to	O
numpy	O
operations	O
on	O
the	O
numpy	O
array	O
`	O
pan.values	O
`	O
if	O
need	O
be	O
,	O
though	O
,	O
hopefully	O
,	O
that	O
would	O
be	O
unnecessary	O
.	O

This	O
argument	O
is	O
new	O
in	O
1.9	O
...	O
but	O
there	O
is	O
a	O
workaround	O
,	O
try	O
`	O
np.linspace	Y
(	O
0	O
,	O
len	Y
(	O
pep_list	O
)	O
,	O
n+1	O
,	O
endpoint=True	O
)	O
.astype	Y
(	O
int	O
)`	O

Take	O
the	O
time	O
difference	O
(	O
using	O
`	O
shift	Y
`	O
)	O
til	O
the	O
next	O
value	O
,	O
and	O
multiply	O
(	O
value	O
*	O
seconds	O
):	O
#CODE	O

Then	O
do	O
the	O
resample	N
to	O
seconds	O
(	O
sum	O
the	O
value*seconds	O
):	O
#CODE	O

you	O
can	O
isnull	Y
(	O
df	O
[	O
'	O
difference	O
'])	O
will	O
give	O
True	O
on	O
NaT	O
,	O
so	O
you	O
could	O
subtract	O
then	O
use	O
mask	Y
I	O
think	O

After	O
they	O
are	O
done	O
,	O
merge	N
the	O
two	O
frames	O
together	O
:	O
#CODE	O

Another	O
solution	O
(	O
slightly	O
harder	O
):	O
Merge	N
the	O
columns	O
`	O
transcript_id	O
`	O
,	O
`	O
gene_id	O
`	O
and	O
`	O
gene_name	O
`	O
in	O
another	O
column	O
,	O
say	O
`	O
merged_id	O
`	O
and	O
`	O
groupby	Y
`	O
on	O
`	O
merged_id	O
`	O
.	O

Geo	O
Pandas	O
Data	O
Frame	O
/	O
Matrix	O
-	O
filter	O
/	O
drop	N
NaN	O
/	O
False	O
values	O

Then	O
I	O
stack	N
the	O
dataframe	Y
,	O
give	O
the	O
index	O
levels	O
the	O
desired	O
names	O
,	O
and	O
select	O
only	O
the	O
rows	O
where	O
we	O
have	O
'	O
True	O
'	O
values	O
:	O
#CODE	O

Can	O
you	O
enable	O
the	O
debugger	O
to	O
get	O
a	O
stack	N
trace	O
?	O

reshape	O
data	O
frame	O
in	O
pandas	O
with	O
pivot	N
table	O

With	O
pivot	N
table	O
you	O
can	O
get	O
a	O
matrix	O
showing	O
which	O
`	O
baz	O
`	O
corresponds	O
to	O
which	O
`	O
qux	O
`	O
:	O
#CODE	O

Rolling	O
apply	N
question	O

For	O
each	O
group	O
in	O
the	O
groupby	Y
object	O
,	O
we	O
will	O
want	O
to	O
apply	N
a	O
function	O
:	O
#CODE	O

We	O
want	O
to	O
take	O
the	O
Times	O
column	O
,	O
and	O
for	O
each	O
time	O
,	O
apply	N
a	O
function	O
.	O

That's	O
done	O
with	O
`	O
applymap	Y
`	O
:	O
#CODE	O

Given	O
a	O
time	O
`	O
t	O
`	O
,	O
we	O
can	O
select	O
the	O
`	O
Value	O
`	O
s	O
from	O
`	O
subf	O
`	O
whose	O
times	O
are	O
in	O
the	O
half-open	O
interval	O
`	O
(	O
t-60	O
,	O
t	O
]`	O
using	O
the	O
`	O
ix	Y
`	O
method	O
:	O
#CODE	O

pandas	O
join	N
data	O
frames	O
on	O
similar	O
but	O
not	O
identical	O
string	O
using	O
lower	O
case	O
only	O

I	O
need	O
to	O
join	N
data	O
frames	O
on	O
columns	O
that	O
are	O
similar	O
but	O
not	O
identical	O
.	O

So	O
I	O
am	O
trying	O
to	O
isolate	O
the	O
lowercase	O
letters	O
from	O
each	O
column	O
,	O
create	O
new	O
columns	O
to	O
join	N
on	O
.	O

Note	O
that	O
this	O
assumes	O
collecting	O
all	O
ASCII	O
characters	O
from	O
`	O
a	O
`	O
to	O
`	O
z	O
`	O
suffices	O
to	O
produce	O
values	O
on	O
which	O
to	O
join	N
.	O

You	O
can	O
of	O
course	O
extend	O
this	O
with	O
several	O
joins	O
,	O
the	O
join	N
solution	O
detects	O
common	O
indices	O
automatically	O
.	O

My	O
data	O
is	O
in	O
a	O
DataFrame	Y
of	O
about	O
10378	O
rows	O
and	O
`	O
len	Y
(	O
df	O
[	O
'	O
Full	O
name	O
'])`	O
is	O
10378	O
,	O
as	O
expected	O
.	O

But	O
`	O
len	Y
(	O
choices	O
)`	O
is	O
only	O
1695	O
.	O

I'm	O
fairly	O
certain	O
that	O
the	O
issue	O
is	O
in	O
the	O
first	O
line	O
,	O
with	O
the	O
`	O
to_dict()	Y
`	O
function	O
,	O
as	O
`	O
len	Y
(	O
df	O
[	O
'	O
Full	O
name	O
']	O
.astype	Y
(	O
str	O
)`	O
results	O
in	O
10378	O
and	O
`	O
len	Y
(	O
df	O
[	O
'	O
Full	O
name	O
']	O
.to_dict()	Y
)`	O
results	O
in	O
1695	O
.	O

what	O
is	O
`	O
len	Y
(	O
df.index.unique()	O
)`	O
?	O

@USER	O
using	O
`	O
choices	O
=	O
dict	O
(	O
zip	O
(	O
df	O
[	O
'	O
n	O
']	O
,	O
df	O
[	O
'	O
Full	O
name	O
']	O
.astype	Y
(	O
str	O
)))`	O
,	O
where	O
df	O
[	O
'	O
n	O
']	O
is	O
np.arange	Y
(	O
len	Y
(	O
df	O
))	O
,	O
worked	O
fine	O
and	O
got	O
what	O
I	O
needed	O
.	O

Had	O
some	O
indexing	O
issues	O
because	O
I	O
was	O
importing	O
the	O
data	O
from	O
different	O
Excel	O
spreadsheets	O
.	O

This	O
is	O
what	O
is	O
happening	O
in	O
your	O
case	O
,	O
and	O
noted	O
from	O
the	O
comments	O
,	O
since	O
the	O
amount	O
of	O
`	O
unique	N
`	O
values	O
for	O
the	O
index	O
are	O
only	O
`	O
1695	O
`	O
,	O
we	O
can	O
confirm	O
this	O
by	O
testing	O
the	O
value	O
of	O
`	O
len	Y
(	O
df.index.unique()	O
)`	O
.	O

what	O
do	O
you	O
mean	O
by	O
normalize	N
?	O

The	O
other	O
way	O
is	O
much	O
easier	O
and	O
involves	O
using	O
`	O
resample	Y
`	O
to	O
convert	O
to	O
daily	O
observations	O
and	O
backfill	O
daily	O
consumption	O
.	O

(	O
Note	O
that	O
the	O
first	O
and	O
last	O
months	O
are	O
based	O
on	O
partial	O
data	O
,	O
you	O
may	O
want	O
to	O
either	O
drop	N
them	O
or	O
pro-rate	O
the	O
daily	O
consumption	O
.	O
)	O
#CODE	O

Basically	O
,	O
after	O
calculating	O
the	O
daily	O
consumption	O
,	O
do	O
a	O
partial	O
resample	Y
by	O
adding	O
the	O
first	O
and	O
last	O
day	O
of	O
each	O
month	O
.	O

I	O
will	O
implement	O
it	O
and	O
see	O
how	O
it	O
goes	O
,	O
but	O
can	O
you	O
also	O
explain	O
what	O
'	O
1d	O
'	O
means	O
in	O
the	O
resample	Y
method	O
?	O

@USER	O
'	O
1d	O
'	O
just	O
means	O
1	O
day	O
for	O
the	O
frequency	O
of	O
the	O
resample	Y
.	O

So	O
I	O
want	O
something	O
that	O
will	O
drop	N
the	O
`	O
lob	O
`	O
group	O
,	O
but	O
keep	O
every	O
record	O
of	O
both	O
the	O
`	O
mol	O
`	O
and	O
`	O
thg	O
`	O
group	O
.	O

Pandas	O
Merge	N
2	O
data	O
frames	O
by	O
2	O
columns	O
each	O

In	O
each	O
data	O
frame	O
i	O
have	O
column	O
with	O
the	O
same	O
name	O
and	O
values	O
(	O
Key_Merge1	O
)	O
and	O
in	O
each	O
data	O
frame	O
i	O
have	O
2	O
different	O
column	O
names	O
with	O
same	O
values	O
(	O
Key_Merge2	O
)	O
.	O

How	O
can	O
i	O
merge	N
2	O
data	O
frames	O
by	O
2	O
columns	O
:	O

Can	O
you	O
post	O
an	O
example	O
data	O
and	O
df	O
,	O
your	O
text	O
description	O
is	O
not	O
clear	O
enough	O
but	O
generally	O
you	O
want	O
to	O
merge	N
and	O
pass	O
the	O
list	O
of	O
cols	O
to	O
merge	N
the	O
;	O
hs	O
and	O
rhs	O
on	O
:	O
`	O
pd.merge	Y
(	O
df1	O
,	O
df2	O
,	O
left_on	O
=[	O
'	O
Key_Merge1	O
'	O
,	O
'	O
Key_Merge21	O
']	O
,	O
right_on	O
=[	O
'	O
Key_Merge1	O
'	O
,	O
'	O
Key_merge22	O
'])`	O

OK	O
,	O
you	O
have	O
to	O
rename	O
'	O
PRODUCT_GROUP	O
'	O
in	O
DF2	O
in	O
order	O
for	O
the	O
`	O
merge	Y
`	O
to	O
work	O
:	O
#CODE	O

the	O
merge	Y
will	O
naturally	O
find	O
the	O
2	O
columns	O
that	O
match	O
and	O
perform	O
an	O
inner	O
merge	Y
as	O
desired	O

I	O
can	O
strip	N
out	O
the	O
rightmost	O
'	O
.csv	O
'	O
part	O
like	O
this	O
:	O
#CODE	O

How	O
to	O
merge	N
two	O
DataFrame	Y
columns	O
and	O
apply	N
pandas.to_datetime	Y
to	O
it	O
?	O

What	O
would	O
be	O
a	O
more	O
pythonic	O
way	O
to	O
merge	N
two	O
columns	O
,	O
and	O
apply	N
a	O
function	O
into	O
the	O
result	O
?	O

once	O
sorted	O
I	O
replace	N
the	O
df.index	O
with	O
a	O
numerical	O
index	O
#CODE	O

This	O
can	O
be	O
accomplished	O
with	O
a	O
one	O
line	O
solution	O
using	O
Pandas	O
'	O
boolean	O
indexing	O
.	O

The	O
one-liner	O
also	O
employs	O
some	O
other	O
tricks	O
:	O
Pandas	O
'	O
`	O
map	Y
`	O
and	O
`	O
diff	Y
`	O
methods	O
and	O
a	O
`	O
lambda	O
`	O
function	O
.	O

`	O
map	Y
`	O
is	O
used	O
to	O
apply	Y
the	O
`	O
lambda	O
`	O
function	O
to	O
all	O
rows	O
.	O

The	O
`	O
lambda	O
`	O
function	O
is	O
needed	O
to	O
create	O
a	O
custom	O
less-then	O
comparison	O
that	O
will	O
evaluate	O
NaN	O
values	O
to	O
True	O
.	O

There	O
is	O
a	O
built	O
in	O
method	O
for	O
this	O
`	O
diff	Y
`	O
:	O
#CODE	O

as	O
pointed	O
out	O
calling	O
`	O
diff	Y
`	O
here	O
will	O
lose	O
the	O
first	O
row	O
so	O
I'm	O
using	O
a	O
ugly	O
hack	O
where	O
I	O
concatenate	O
the	O
first	O
row	O
with	O
the	O
result	O
of	O
the	O
`	O
diff	Y
`	O
so	O
I	O
don't	O
lose	O
the	O
first	O
row	O

Using	O
`	O
diff	Y
`	O
like	O
this	O
drops	O
the	O
first	O
row	O
.	O

(	O
I	O
can	O
also	O
use	O
the	O
chunksize	O
option	O
and	O
concat	N
myself	O
,	O
but	O
that	O
seems	O
to	O
be	O
a	O
bit	O
of	O
a	O
hack	O
.	O
)	O

Jeff	O
,	O
I	O
updated	O
sec_id	O
and	O
dt	Y
in	O
the	O
dataframe	Y
.	O

Sorry	O
,	O
I	O
had	O
to	O
update	O
"	O
sec_id	O
"	O
and	O
"	O
dt	N
"	O
to	O
"	O
id	O
"	O
and	O
"	O
date	O
"	O
.	O

0.12	O
is	O
fine	O
;	O
FYI	O
the	O
format	O
keyword	O
doesn't	O
do	O
anything	O
with	O
append	Y
(	O
and	O
it's	O
for	O
0.13	O
anyhow	O
);	O
append	Y
always	O
is	O
a	O
table	O

I	O
would	O
like	O
to	O
get	O
every	O
,	O
let's	O
say	O
,	O
6	O
hours	O
of	O
data	O
and	O
independently	O
fit	O
a	O
curve	O
to	O
that	O
data	O
.	O

Since	O
pandas	O
'	O
`	O
resample	Y
`	O
function	O
has	O
a	O
`	O
how	O
`	O
keyword	O
that	O
is	O
supposed	O
to	O
be	O
any	O
numpy	O
array	O
function	O
,	O
I	O
thought	O
that	O
I	O
could	O
maybe	O
try	O
to	O
use	O
resample	Y
to	O
do	O
that	O
with	O
`	O
polyfit	Y
`	O
,	O
but	O
apparently	O
there	O
is	O
no	O
way	O
(	O
right	O
?	O
)	O
.	O

Why	O
does	O
the	O
second	O
block	O
of	O
code	O
not	O
work	O
?	O

Doesn't	O
DataFrame.apply()	Y
default	O
to	O
inplace	O
?	O

There	O
is	O
no	O
inplace	O
parameter	O
to	O
the	O
apply	Y
function	O
.	O

Even	O
if	O
it	O
doesn't	O
default	O
to	O
inplace	O
,	O
shouldn't	O
it	O
provide	O
an	O
inplace	O
parameter	O
the	O
way	O
replace()	Y
does	O
?	O

No	O
,	O
apply	Y
does	O
not	O
work	O
inplace*	O
.	O

In	O
general	O
apply	Y
is	O
slow	O
(	O
since	O
you	O
are	O
basically	O
iterating	O
through	O
each	O
row	O
in	O
python	O
)	O
,	O
and	O
the	O
"	O
game	O
"	O
is	O
to	O
rewrite	O
that	O
function	O
in	O
terms	O
of	O
pandas	O
/	O
numpy	O
native	O
functions	O
and	O
indexing	O
.	O

If	O
you	O
want	O
to	O
delve	O
into	O
more	O
details	O
about	O
the	O
internals	O
,	O
check	O
out	O
the	O
BlockManager	O
in	O
core	O
/	O
internals.py	O
,	O
this	O
is	O
the	O
object	O
which	O
holds	O
the	O
underlying	O
numpy	O
arrays	O
.	O

*	O
apply	Y
is	O
not	O
usually	O
going	O
to	O
make	O
sense	O
inplace	O
(	O
and	O
IMO	O
this	O
behaviour	O
would	O
rarely	O
be	O
desired	O
)	O
.	O

I	O
use	O
this	O
function	O
with	O
pandas	O
to	O
apply	N
it	O
to	O
each	O
month	O
of	O
a	O
historical	O
record	O
:	O
#CODE	O

I	O
am	O
trying	O
to	O
merge	N
tsv	O
files	O
using	O
pandas	O
but	O
cannot	O
get	O
pandas	O
to	O
return	O
the	O
file	O
contents	O
correctly	O
.	O

You	O
can	O
use	O
the	O
vectorised	O
`	O
str	Y
`	O
methods	O
to	O
replace	N
the	O
unwanted	O
characters	O
and	O
then	O
cast	O
the	O
type	O
to	O
int	O
:	O
#CODE	O

perhaps	O
`	O
reindex	Y
`	O
creates	O
a	O
new	O
dataframe	Y
,	O
`	O
ix	Y
`	O
returns	O
a	O
view	O

@USER	O
you	O
are	O
,	O
of	O
course	O
,	O
absolutely	O
right	O
.	O
what	O
do	O
`	O
loc	Y
`	O
and	O
`	O
iloc	Y
`	O
do	O
?	O

The	O
reason	O
for	O
the	O
seeming	O
redundancy	O
is	O
that	O
,	O
while	O
using	O
`	O
ix	Y
`	O
is	O
syntacticly	O
limiting	O
(	O
you	O
can	O
only	O
pass	O
a	O
single	O
argument	O
to	O
`	O
__getitem__	O
`)	O
,	O
`	O
reindex	Y
`	O
is	O
a	O
method	O
,	O
which	O
supports	O
taking	O
various	O
optional	O
parameters	O
.	O

I	O
am	O
getting	O
different	O
results	O
when	O
using	O
`	O
reindex	Y
`	O
with	O
`	O
inplace=True	O
`	O
vs	O
using	O
`	O
ix	Y
`	O
(	O
I	O
updated	O
the	O
OP	O
)	O

What	O
if	O
you	O
have	O
many	O
conditions	O
,	O
e.g.	O
you	O
want	O
to	O
split	O
up	O
the	O
scatters	O
into	O
4	O
types	O
of	O
points	O
or	O
even	O
more	O
,	O
plotting	O
each	O
in	O
different	O
shape	O
/	O
color	O
.	O

How	O
can	O
you	O
elegantly	O
apply	N
condition	O
a	O
,	O
b	O
,	O
c	O
,	O
etc	O
.	O
and	O
make	O
sure	O
you	O
then	O
plot	O
"	O
the	O
rest	O
"	O
(	O
things	O
not	O
in	O
any	O
of	O
these	O
conditions	O
)	O
as	O
the	O
last	O
step	O
?	O

To	O
find	O
points	O
skipped	O
due	O
to	O
NA	O
,	O
try	O
the	O
`	O
isnull	Y
`	O
method	O
:	O
`	O
df	O
[	O
df.col3.isnull()	O
]`	O

How	O
do	O
I	O
create	O
a	O
pivot	N
table	O
in	O
Pandas	O
where	O
one	O
column	O
is	O
the	O
mean	O
of	O
some	O
values	O
,	O
and	O
the	O
other	O
column	O
is	O
the	O
sum	O
of	O
others	O
?	O

Basically	O
,	O
how	O
would	O
I	O
create	O
a	O
pivot	N
table	O
that	O
consolidates	O
data	O
,	O
where	O
one	O
of	O
the	O
columns	O
of	O
data	O
it	O
represents	O
is	O
calculated	O
,	O
say	O
,	O
by	O
`	O
likelihood	O
percentage	O
`	O
(	O
0.0	O
-	O
1.0	O
)	O
by	O
taking	O
the	O
mean	O
,	O
and	O
another	O
is	O
calculated	O
by	O
`	O
number	O
ordered	O
`	O
which	O
sums	O
all	O
of	O
them	O
?	O
