Remove rows with duplicate indices ( Pandas DataFrame and TimeSeries )
I'm reading some automated weather data from the web .
The observations occur every 5 minutes and are compiled into monthly files for each weather station .
Once I'm done parsing a file , the DataFrame looks something like this :
@CODE
The problem I'm having is that sometimes a scientist goes back and corrects observations -- not by editing the erroneous rows , but by appending a duplicate row to the end of a file .
Simple example of such a case is illustrated below :
@CODE
And so I need ` df3 ` to evenutally become :
@CODE
I thought that adding a column of row numbers (`df3['rownum ' ] = range ( df3.shape [0] )`) would help me select out the bottom-most row for any value of the ` DatetimeIndex ` , but I am stuck on figuring out the ` group_by ` or ` pivot ` ( or ??? ) statements to make that work .
Oh my .
This is actually so simple !
@CODE
Follow up edit 2013-10-29
In the case where I have a fairly complex ` MultiIndex ` , I think I prefer the ` groupby ` approach .
Here's simple example for posterity :
@CODE
and here's the important part
@CODE
A simple solution is to use ` drop_duplicates `
@CODE
For me , this operated quickly on large data sets .
This requires that ' rownum ' be the column with duplicates .
In the modified example , ' rownum ' has no duplicates , therefore nothing gets eliminated .
What we really want is to have the ' cols ' be set to the index .
I've not found a way to tell drop_duplicates to only consider the index .
Here is a solution that adds the index as a dataframe column , drops duplicates on that , then removes the new column :
@CODE
And if you want things back in the proper order , just call ` sort ` on the dataframe .
@CODE
Unfortunately , I don't think Pandas allows one to drop dups off the indices .
I would suggest the following :
@CODE
