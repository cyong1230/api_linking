Memory error when using pandas read_csv
I am trying to do something fairly simple , reading a large csv file into a pandas dataframe .
@CODE
The code either fails with a ` MemoryError ` , or just never finishes .
Mem usage in the task manager stopped at 506 Mb and after 5 minutes of no change and no CPU activity in the process I stopped it .
I am using pandas version 0.11.0 .
I am aware that there used to be a memory problem with the file parser , but according to http://wesmckinney.com/blog/?p=543 this should have been fixed .
The file I am trying to read is 366 Mb , the code above works if I cut the file down to something short ( 25 Mb ) .
It has also happened that I get a pop up telling me that it can't write to address 0x1e0baf93 ...
Stacktrace :
@CODE
A bit of background - I am trying to convince people that Python can do the same as R .
For this I am trying to replicate an R script that does
@CODE
R not only manages to read the above file just fine , it even reads several of these files in a for loop ( and then does some stuff with the data ) .
If Python does have a problem with files of that size I might be fighting a loosing battle ...
Although this is a workaround not so much as a fix , I'd try converting that CSV to JSON ( should be trivial ) and using ` read_json ` method instead - I've been writing and reading sizable JSON / dataframes ( 100s of MB ) in Pandas this way without any problem at all .
There is no error for Pandas 0.12.0 and NumPy 1.8.0 .
I have managed to create a big DataFrame and save it to a csv file and then successfully read it .
Please see the example here .
The size of the file is 554 Mb ( It even worked for 1.1 Gb file , took longer , to generate 1.1Gb file use frequency of 30 seconds ) .
Though I have 4Gb of RAM available .
My suggestion is try updating Pandas .
Other thing that could be useful is try running your script from command line , because for R you are not using Visual Studio ( this already was suggested in the comments to your question ) , hence it has more resources available .
I encountered this issue as well when I was running in a virtual machine , or somewere else where the memory is stricktly limited .
It has nothing to do with pandas or numpy or csv , but will always happen if you try using more memory as you are alowed to use , not even only in python .
The only chance you have is what you already tried , try to chomp down the big thing into smaller pieces which fit into memory .
If you ever asked yourself what MapReduce is all about , you found out by yourself ...
MapReduce would try to distribute the chunks over many machines , you would try to process the chunke on one machine one after another .
What you found out with the concatenation of the chunk files might be an issue indeed , maybe there are some copy needed in this operation ... but in the end this maybe saves you in your current situation but if your csv gets a little bit larger you might run against that wall again ...
It also could be , that pandas is so smart , that it actually only loads the individual data chunks into memory if you do something with it , like concatenating to a big df ?
Several things you can try :
Don't load all the data at once , but split in in pieces
As far as I know , hdf5 is able to do these chunks automatically and only loads the part your program currently works on
Look if the types are ok , a string ' 0.111111 ' needs more memory than a float
What do you need actually , if there is the adress as a string , you might not need it for numerical analysis ...
A database can help acessing and loading only the parts you actually need ( e.g. only the 1% active users )
I use Pandas on my Linux box and faced many memory leaks that only got resolved after upgrading Pandas to the latest version after cloning it from github .
Memory errors happens a lot with python when using the 32bit version in Windows .
This is because 32bit processes only gets 2GB of memory to play with by default .
If you are not using 32bit python in windows but are looking to improve on your memory efficiency while reading csv files , there is a trick .
The pandas.read_csv function takes an option called ` dtype ` .
This lets pandas know what types exist inside your csv data .
By default , pandas will try to guess what dtypes your csv file has .
This is a very heavy operation because while it is determining the dtype , it has to keep all raw data as objects ( strings ) in memory .
Let's say your csv looks like this :
@CODE
This example is of course no problem to read into memory , but it's just an example .
If pandas were to read the above csv file without any dtype option , the age would be stored as strings in memory until pandas has read enough lines of the csv file to make a qualified guess .
I think the default in pandas is to read 1,000,000 rows before guessing the dtype .
By specifying `dtype={'age ' : int} ` as an option to the ` ` will let pandas know that age should be interpretted as a number .
This saves you lots of memory .
However , if your csv file would be corrupted , like this :
@CODE
Then specifying `dtype={'age ' : int} ` will break the ` ` command , because it cannot cast `" 40+ "` to int .
So sanitize your data carefully !
Here you can see how the memory usage of a pandas dataframe is a lot higher when floats are kept as strings :
@CODE
