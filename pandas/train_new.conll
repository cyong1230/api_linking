Minor	O
:	O
`	O
notnull	Y
`	O
is	O
also	O
a	O
method	O
of	O
DataFrames	O
.	O

I	O
want	O
to	O
apply	N
a	O
function	O
f	O
(	O
lat1	O
,	O
lon1	O
,	O
lat2	O
,	O
lon2	O
)	O
which	O
calculates	O
the	O
distance	O
between	O
two	O
points	O
(	O
defined	O
using	O
lat1	O
,	O
lon1	O
,	O
lat2	O
,	O
lon2	O
)	O
.	O

For	O
all	O
110k+	O
records	O
in	O
`	O
df1	O
`	O
do	O
you	O
want	O
to	O
apply	N
your	O
distance	O
function	O
for	O
every	O
record	O
in	O
`	O
df2	O
`	O
?	O

I	O
chose	O
to	O
use	O
map	Y
and	O
list	O
comprehensions	O
because	O
they	O
will	O
be	O
faster	O
than	O
a	O
standard	O
`	O
for	O
each	O
`	O

However	O
I	O
took	O
this	O
into	O
account	O
and	O
used	O
map	Y
,	O
and	O
nested	O
comprehensions	O
which	O
are	O
going	O
to	O
be	O
faster	O
than	O
a	O
for	O
loop	O
.	O

Keep	O
getting	O
:	O
KeyError	O
:	O
'	O
cannot	O
use	O
a	O
single	O
bool	N
to	O
index	O
into	O
setitem	O
'	O
on	O
this	O
line	O
of	O
code	O
in	O
the	O
second	O
chunk	O
I	O
posted	O
.	O

i	O
have	O
to	O
merge	N
them	O
in	O
to	O
the	O
same	O
cell	O
before	O
applying	O
this	O
method	O
.	O

Or	O
is	O
there	O
are	O
way	O
to	O
marge	O
the	O
columns	O
in	O
pandas	O
?	O

I	O
have	O
two	O
TimeSeries	O
with	O
some	O
overlapping	O
dates	O
/	O
indices	O
and	O
I'd	O
like	O
to	O
merge	N
them	O
.	O

I	O
have	O
an	O
excel	O
file	O
(	O
.xls	O
format	O
)	O
with	O
5	O
sheets	O
,	O
I	O
want	O
to	O
replace	N
the	O
contents	O
of	O
sheet	O
5	O
with	O
contents	O
of	O
my	O
pandas	O
data	O
frame	O
.	O

So	O
,	O
I	O
decided	O
to	O
do	O
this	O
task	O
in	O
VBA	O
and	O
drop	N
python	O
completely	O
.	O

It	O
could	O
be	O
I'm	O
not	O
using	O
the	O
right	O
keywords	O
,	O
so	O
if	O
you	O
have	O
suggestions	O
,	O
that	O
could	O
also	O
help	O
.	O

plus	O
the	O
selected	O
rows	O
usage	O
x	O
2	O
,	O
which	O
will	O
happen	O
when	O
you	O
concat	N
the	O
rows	O

after	O
the	O
concat	Y
the	O
usage	O
will	O
go	O
down	O
to	O
selected	O
rows	O
usage	O

See	O
example	O
here	O
:	O
#URL	O
Not	O
sure	O
this	O
will	O
solve	O
it	O
,	O
but	O
that	O
will	O
do	O
the	O
query	O
in	O
chunks	O
,	O
and	O
you	O
can	O
aggregate	N
or	O
merge	N
them	O
in	O
pandas	O

Try	O
a	O
`	O
dropna	Y
`	O
or	O
use	O
`	O
missing=	O
'	O
drop	N
'`	O
to	O
Logit	O
.	O

You	O
might	O
also	O
check	O
that	O
the	O
right	O
hand	O
side	O
is	O
full	O
rank	O
`	O
np.linalg.matrix_rank	Y
(	O
data	O
[	O
train_cols	O
]	O
.values	Y
)`	O

append	N
pandas.DataFrame.GroupBy	Y
results	O
into	O
another	O
dataframe	Y

You	O
need	O
to	O
append	N
the	O
intermediate	O
DataFrames	O
to	O
a	O
list	O
and	O
then	O
concatenate	O
the	O
results	O
.	O

I	O
am	O
taking	O
the	O
second	O
dataframe	Y
and	O
doing	O
some	O
calculations	O
with	O
it	O
to	O
append	N
to	O
the	O
first	O
dataframe	Y
.	O

However	O
it	O
does	O
not	O
appear	O
that	O
what	O
I	O
am	O
appending	O
to	O
the	O
first	O
data	O
frame	O
is	O
actually	O
happening	O
.	O

Could	O
you	O
use	O
concat	Y
instead	O
?	O

`	O
m=	O
m.concat	O
([	O
a0	O
,	O
a1	O
,	O
a2	O
,	O
a3	O
,	O
a4	O
,	O
a5	O
,	O
a6	O
,	O
a7	O
,	O
a8	O
,	O
a9	O
]	O
,	O
ignore_index=True	O
)`	O

I	O
get	O
an	O
error	O
trying	O
to	O
use	O
this	O
...	O

AttributeError	O
:	O
'	O
DataFrame	Y
'	O
object	O
has	O
no	O
attribute	O
'	O
concat	Y
'	O

[	O
`	O
append	Y
`]	O
(	O
#URL	O
)	O
does	O
*	O
not	O
*	O
operate	O
in	O
place	O
.	O

But	O
for	O
a	O
start	O
I	O
would	O
just	O
be	O
happy	O
to	O
get	O
the	O
first	O
result	O
.	O

I	O
suspect	O
that	O
I	O
need	O
to	O
use	O
searchsort	O
and	O
asof	Y
,	O
but	O
I	O
am	O
not	O
quite	O
sure	O
how	O
to	O
do	O
that	O
with	O
.	O

You're	O
looking	O
for	O
a	O
near	O
timestamp	O
,	O
where	O
`	O
asof	Y
`	O
searches	O
for	O
the	O
latest	O
timestamp	O
.	O

It	O
is	O
only	O
applied	O
to	O
a	O
time	O
series	O
,	O
so	O
you	O
would	O
have	O
to	O
apply	N
`	O
reset_index	Y
`	O
to	O
your	O
`	O
DataFrame	Y
`	O

You're	O
going	O
to	O
have	O
to	O
iterate	O
over	O
your	O
list	O
,	O
get	O
copies	O
of	O
them	O
filtered	O
and	O
then	O
concat	N
them	O
all	O
together	O
#CODE	O

A	O
solution	O
without	O
loop	O
but	O
`	O
merge	Y
`	O
:	O
#CODE	O

If	O
there	O
are	O
no	O
blanks	O
some	O
columns	O
convert	O
to	O
`	O
TRUE	O
/	O
FALSE	O
`	O
,	O
others	O
leave	O
as	O
`	O
Yes	O
/	O
No	O
`	O
but	O
dtype	Y
is	O
bool	Y
.	O

`	O
fhs	O
=	O
fhs.drop	O
([	O
1002	O
])`	O
to	O
drop	N
that	O
row	O
and	O
data	O
types	O
are	O
still	O
good	O
.	O

first	O
column	O
comes	O
into	O
df	O
as	O
Yes	O
,	O
No	O
,	O
Yes	O
,	O
Yes	O
type	O
bool	N
xxxx	O
below	O

3rd	O
column	O
comes	O
into	O
df	O
as	O
FALSE	O
,	O
FALSE	O
,	O
TRUE	O
,	O
TRUE	O
type	O
bool	N

print	O
(	O
len	Y
(	O
upregulated	O
)	O
,	O
end=	O
'	O
\n	O
')	O

remove	O
overlay	O
text	O
from	O
pandas	O
boxplot	Y

I	O
am	O
trying	O
to	O
remove	O
the	O
overlay	O
text	O
on	O
my	O
boxplot	N
I	O
created	O
using	O
pandas	O
.	O

The	O
code	O
to	O
generate	O
it	O
is	O
as	O
follows	O
(	O
minus	O
a	O
few	O
other	O
modifications	O
):	O

I	O
just	O
want	O
to	O
remove	O
the	O
"	O
boxplot	N
grouped	O
by	O
0	O
...	O

I	O
know	O
how	O
to	O
create	O
a	O
new	O
column	O
with	O
`	O
apply	Y
`	O
or	O
`	O
np.where	Y
`	O
based	O
on	O
the	O
values	O
of	O
another	O
column	O
,	O
but	O
a	O
way	O
of	O
selectively	O
changing	O
the	O
values	O
of	O
an	O
existing	O
column	O
is	O
escaping	O
me	O
;	O
I	O
suspect	O
`	O
df.ix	Y
`	O
is	O
involved	O
?	O

@USER	O
For	O
indexing	O
with	O
boolean	O
vectors	O
this	O
is	O
perfectly	O
fine	O
,	O
if	O
you	O
want	O
to	O
add	O
in	O
other	O
forms	O
of	O
indexing	O
you	O
would	O
want	O
`	O
loc	Y
`	O
.	O

For	O
instance	O
:	O
`	O
df.loc	Y
[	O
df.name.str.contains	O
(	O
'	O
e$	O
')	O
,	O
'	O
flag	O
']	O
=	O
'	O
Blue	O
'`	O
.	O

use	O
``	O
apply	Y
``	O
ONLY	O
as	O
a	O
last	O
resort	O
(	O
e.g.	O
you	O
can't	O
do	O
vectorized	O
things	O
)	O
.	O
even	O
if	O
you	O
have	O
a	O
very	O
complicated	O
function	O
to	O
do	O
,	O
you	O
can	O
often	O
do	O
vectorized	O
calculations	O
on	O
most	O
of	O
it	O
,	O
saving	O
the	O
last	O
for	O
``	O
apply	Y
``	O
,	O
which	O
is	O
essentially	O
a	O
loop	O
.	O

Using	O
apply	Y
took	O
172ms	O
versus	O
39ms	O
using	O
Jeff's	O
method	O
,	O
I	O
can	O
also	O
confirm	O
that	O
it	O
made	O
negligle	O
difference	O
whether	O
the	O
apply	Y
was	O
called	O
inside	O
or	O
outside	O
the	O
function	O
but	O
it	O
does	O
modify	O
the	O
df	O
so	O
you	O
didn't	O
need	O
to	O
return	O
the	O
df	O
as	O
it	O
was	O
being	O
modified	O
inside	O
the	O
function	O

And	O
then	O
sometimes	O
different	O
solutions	O
(	O
in	O
this	O
case	O
using	O
`	O
apply	Y
`)	O
come	O
up	O
on	O
google	O
/	O
stackoverflow	O
and	O
yet	O
again	O
I	O
can	O
NOT	O
verify	O
that	O
there	O
is	O
no	O
better	O
solution	O
as	O
I	O
dont	O
have	O
the	O
insight	O
into	O
the	O
library	O
.	O

I	O
kindof	O
disagree	O
with	O
using	O
df	O
as	O
the	O
variable	O
name	O
here	O
,	O
I	O
also	O
think	O
I'd	O
just	O
use	O
len	Y
:	O
`	O
df.groupby	Y
(	O
"	O
Name	O
")	O
.filter	Y
(	O
lambda	O
x	O
:	O
len	Y
(	O
x	O
)	O
>	O
2	O
)`	O

Merge	N
existing	O
dataframe	Y
into	O
fixed	O
size	O
new	O
dataframe	Y

Then	O
I	O
want	O
merge	Y
these	O
kinds	O
of	O
table	O
into	O
new	O
dataframe	Y

How	O
could	O
I	O
merge	N
them	O
in	O
that	O
way	O
?	O

Inconsistent	O
behavior	O
of	O
apply	Y
with	O
operator.itemgetter	O
v.s.	O
applymap	Y
operator.itemgetter	O

`	O
apply	Y
`	O
gives	O
wrong	O
result	O
#CODE	O

apply	Y
is	O
being	O
passed	O
an	O
entire	O
row	O
which	O
is	O
a	O
series	O
of	O
2	O
elements	O
which	O
are	O
lists	O
;	O
the	O
last	O
list	O
is	O
returned	O
and	O
coerced	O
to	O
a	O
series	O
.	O
embedded	O
lists	O
as	O
elements	O
are	O
not	O
a	O
good	O
idea	O
in	O
general	O
.	O

The	O
reason	O
I	O
am	O
asking	O
,	O
is	O
because	O
I	O
suspect	O
(	O
?	O
)	O
it	O
is	O
faster	O
to	O
create	O
a	O
zero	O
filled	O
dataframe	Y
,	O
and	O
then	O
replace	N
each	O
element	O
as	O
needed	O
.	O

So	O
it	O
might	O
be	O
faster	O
to	O
create	O
an	O
empty	O
dataframe	Y
with	O
nxm	O
dimensions	O
and	O
then	O
replace	N
elements	O
as	O
needed	O
(	O
by	O
copying	O
a	O
list	O
to	O
each	O
column	O
)	O
.	O

in	O
general	O
creating	O
an	O
empty	O
frame	O
,	O
then	O
filling	O
it	O
column	O
by	O
column	O
is	O
not	O
very	O
efficient	O
;	O
use	O
a	O
dict	O
/	O
list	O
instead	O
,	O
or	O
create	O
sub-frames	O
and	O
concat	N
them	O

Are	O
you	O
trying	O
to	O
shift	N
ends	O
by	O
one	O
(	O
month	O
)	O
?	O

My	O
initial	O
suggestion	O
was	O
to	O
do	O
the	O
shift	Y
after	O
you've	O
reindexed	O
(	O
since	O
you're	O
about	O
to	O
do	O
that	O
anyway	O
):	O
#CODE	O

the	O
shift	Y
index	O
looks	O
like	O
a	O
better	O
fix	O
,	O
still	O
would	O
like	O
to	O
know	O
if	O
there	O
is	O
a	O
simple	O
date	O
add	O
function	O
,	O
which	O
is	O
how	O
I'd	O
do	O
it	O
in	O
sql	O
,	O
that	O
could	O
apply	N
?	O

I'd	O
still	O
like	O
to	O
know	O
if	O
there	O
is	O
a	O
simple	O
DateAdd	O
type	O
function	O
that	O
I	O
could	O
use	O
that	O
might	O
also	O
apply	N
for	O
use	O
elsewhere	O
if	O
needed	O
?	O

Alternatively	O
you	O
could	O
use	O
`	O
apply	Y
`	O
(	O
but	O
this	O
will	O
usually	O
be	O
slower	O
):	O
#CODE	O

Since	O
you	O
are	O
using	O
the	O
"	O
trailing	O
row	O
"	O
you	O
are	O
going	O
to	O
need	O
to	O
use	O
`	O
shift	Y
`	O
:	O
#CODE	O

thanks	O
shift	Y
is	O
what	O
i	O
was	O
looking	O
for	O
.	O
now	O
i	O
can	O
find	O
examples	O
in	O
the	O
Pandas	O
book	O

I	O
have	O
been	O
searching	O
for	O
hours	O
,	O
literally	O
the	O
entire	O
day	O
on	O
how	O
to	O
generate	O
a	O
pivot	N
table	O
in	O
Python	O
.	O

What	O
I	O
want	O
is	O
to	O
take	O
a	O
csv	O
file	O
,	O
extract	O
the	O
first	O
column	O
and	O
generate	O
a	O
pivot	N
table	O
using	O
the	O
count	O
or	O
frequency	O
of	O
the	O
numbers	O
in	O
that	O
column	O
,	O
and	O
sort	O
descending	O
#CODE	O

These	O
columns	O
all	O
contain	O
an	O
identical	O
kind	O
of	O
data	O
,	O
and	O
I'd	O
like	O
to	O
stack	N
them	O
into	O
a	O
single	O
series	O
,	O
ergo	O
:	O
#CODE	O

From	O
here	O
,	O
I	O
can't	O
quite	O
figure	O
out	O
how	O
to	O
reindex	N
my	O
series	O
such	O
that	O
the	O
indexes	O
go	O
from	O
0	O
to	O
`	O
len	Y
(	O
s	O
)`	O
.	O

But	O
it	O
could	O
be	O
an	O
unexpected	O
system	O
difference	O
--	O
I	O
am	O
using	O
Python	O
2.7.3	O
on	O
an	O
Ubuntu	O
machine	O
.	O

An	O
alternative	O
you	O
might	O
try	O
is	O
to	O
replace	N
exit()	O
with	O
os._exit	O
(	O
os.EX_OK	O
)	O
.	O

I	O
think	O
it	O
uses	O
`	O
patsy	O
`	O
in	O
the	O
backend	O
to	O
translate	N
the	O
formula	O
expression	O
,	O
and	O
intercept	O
is	O
added	O
automatically	O
.	O

Trying	O
to	O
append	N
this	O
to	O
a	O
new	O
datastore	O
.	O

The	O
datastore	O
does	O
not	O
exist	O
so	O
I	O
use	O
the	O
following	O
to	O
create	O
and	O
append	N
the	O
data	O
;	O
#CODE	O

I'm	O
not	O
looking	O
to	O
concatenate	O
strings	O
,	O
just	O
shift	N
everything	O
over	O
.	O

I	O
saw	O
a	O
method	O
using	O
"	O
R	O
"	O
and	O
melt	Y
,	O
however	O
I	O
would	O
like	O
to	O
stick	O
with	O
python	O
/	O
pandas	O
if	O
possible	O
.	O

I	O
cannot	O
post	O
real	O
request	O
for	O
security	O
reason	O
.	O

By	O
the	O
way	O
the	O
code	O
works	O
without	O
"	O
append	Y
"	O
within	O
for	O
loop	O
.	O

At	O
first	O
I	O
tried	O
using	O
pivot	Y
(	O
with	O
timestamp	O
as	O
an	O
index	O
)	O
,	O
but	O
that	O
didn't	O
work	O
because	O
of	O
those	O
duplicates	O
.	O

I	O
don't	O
want	O
to	O
drop	N
them	O
,	O
since	O
the	O
other	O
data	O
is	O
different	O
and	O
should	O
not	O
be	O
lost	O
.	O

Since	O
index	O
contains	O
no	O
duplicates	O
,	O
I	O
thought	O
maybe	O
I	O
can	O
pivot	N
over	O
it	O
and	O
after	O
that	O
merge	N
the	O
result	O
into	O
the	O
original	O
DataFrame	Y
,	O
but	O
I	O
was	O
wondering	O
if	O
there	O
is	O
an	O
easier	O
more	O
intuitive	O
solution	O
.	O

As	O
your	O
`	O
get_dummies	Y
`	O
returns	O
a	O
df	O
this	O
will	O
be	O
aligned	O
already	O
with	O
your	O
existing	O
df	O
so	O
just	O
`	O
concat	Y
`	O
column-wise	O
:	O
#CODE	O

You	O
can	O
drop	N
the	O
'	O
cat	O
'	O
column	O
by	O
doing	O
`	O
df.drop	Y
(	O
'	O
cat	O
'	O
,	O
axis=1	O
)`	O

You	O
can	O
see	O
that	O
the	O
array	O
is	O
masked	O
and	O
that	O
some	O
of	O
the	O
first	O
few	O
rows	O
show	O
examples	O
of	O
`	O
--	O
`	O
in	O
there	O
.	O

So	O
I	O
drop	N
the	O
last	O
field	O
(	O
`	O
refGage	O
`)	O
and	O
it	O
works	O
,	O
so	O
I	O
think	O
it's	O
masked	O
values	O
which	O
only	O
appear	O
in	O
that	O
field	O
.	O

I	O
used	O
df.ix()	Y
to	O
replace	N
the	O
filled-in	O
tokens	O
for	O
what	O
was	O
masked	O
out	O
.	O

Next	O
,	O
you	O
can	O
use	O
a	O
dictionary	O
comprehension	O
together	O
with	O
`	O
loc	Y
`	O
to	O
select	O
the	O
relevant	O
`	O
group_no	O
`	O
dataframe	Y
.	O

To	O
get	O
the	O
last	O
group	O
number	O
,	O
I	O
get	O
the	O
last	O
value	O
using	O
`	O
iat	Y
`	O
for	O
location	O
based	O
indexing	O
.	O

Then	O
apply	N
your	O
method	O
:	O
#CODE	O

Notice	O
that	O
if	O
you	O
unstack	Y
the	O
`	O
id	O
`	O
index	O
level	O
of	O
`	O
df	O
`	O
then	O
you	O
get	O
:	O
#CODE	O

I'm	O
not	O
used	O
to	O
working	O
with	O
`	O
lists	O
`	O
in	O
columns	O
of	O
Pandas	O
and	O
don't	O
know	O
how	O
to	O
get	O
the	O
intersection	N
of	O
`	O
lists	O
`	O
from	O
two	O
columns	O
in	O
a	O
`	O
dataframe	Y
`	O
,	O
then	O
get	O
the	O
index	O
of	O
where	O
the	O
words	O
appear	O
,	O
then	O
apply	N
plus	O
signs	O
to	O
the	O
front	O
of	O
each	O
found	O
index	O
.	O

Or	O
maybe	O
easier	O
would	O
be	O
a	O
string	O
replacement	O
on	O
`	O
df	O
[	O
'	O
Keyword	O
']`	O
using	O
the	O
words	O
from	O
`	O
StemmedAG	O
`	O
?	O

You	O
can	O
use	O
`	O
pivot	Y
`	O
#CODE	O

Cool	O
I	O
didn't	O
know	O
about	O
pivot	Y
either	O
...	O

Instead	O
of	O
creating	O
it	O
,	O
we	O
can	O
append	N
it	O
to	O
initial	O
StartDate	O
.	O

However	O
,	O
the	O
DptCityDptCountry	O
might	O
be	O
different	O
but	O
if	O
another	O
ID	O
matches	O
with	O
the	O
StartDate	O
and	O
DptCityDptCountry	O
,	O
it	O
will	O
be	O
added	O
up	O
i.e.	O
#CODE	O

Then	O
use	O
apply	Y
and	O
return	O
a	O
series	O
indexed	O
on	O
the	O
expanded	O
set	O
of	O
dates	O
for	O
each	O
row	O
(	O
Series	O
of	O
Series	O
=	O
DataFrame	Y
)	O
.	O

So	O
for	O
each	O
of	O
the	O
7	O
rows	O
in	O
the	O
DataFrame	Y
,	O
I	O
get	O
a	O
series	O
indexed	O
on	O
the	O
expanded	O
date	O
range	O
.	O

Then	O
its	O
just	O
clever	O
stacking	O
,	O
naming	O
,	O
and	O
reset_index	Y
.	O

Also	O
,	O
if	O
you	O
want	O
to	O
have	O
the	O
ticklabels	O
/	O
tickmarks	O
of	O
the	O
x-axis	O
connected	O
to	O
the	O
"	O
middle	O
axis	O
"	O
(	O
also	O
while	O
panning	O
/	O
zooming	O
)	O
,	O
then	O
it's	O
easiest	O
to	O
insert	N
an	O
extra	O
spine	O
;	O
take	O
a	O
look	O
at	O
[	O
`	O
mpl_toolkits.axisartist	Y
`]	O
(	O
#URL	O
)	O
for	O
some	O
examples	O
of	O
this	O
.	O

print	O
(	O
'	O
Stock	O
:	O
'	O
,	O
col	O
,	O
'	O
max	O
diff	Y
:	O
'	O
,	O
sl.max()	O
-	O
sl.min()	O
)`	O

Then	O
merge	N
back	O
to	O
the	O
original	O
dataframe	Y
to	O
have	O
your	O
aggregates	O
displayed	O
against	O
each	O
row	O
:	O
#CODE	O

Unfortunately	O
im	O
getting	O
an	O
issue	O
when	O
trying	O
to	O
do	O
the	O
rename	O
.	O

The	O
true	O
/	O
false	O
column	O
does	O
not	O
have	O
a	O
column	O
name	O
,	O
so	O
how	O
would	O
I	O
rename	O
it	O
and	O
then	O
merge	N
it	O
back	O
into	O
the	O
original	O
dataframe	Y
?	O

I	O
am	O
using	O
the	O
below	O
code	O
which	O
gives	O
me	O
the	O
summary	O
of	O
count	O
in	O
the	O
pivot	N
table	O
,	O
#CODE	O

but	O
what	O
i	O
want	O
is	O
the	O
%	O
of	O
row	O
calculation	O
as	O
in	O
excel	O
pivot	N
when	O
you	O
right	O
click	O
the	O
pivot	N
and	O
select	O
"	O
show	O
value	O
as	O
->	O
%	O
of	O
Row	O
Total	O
"	O
.	O

Since	O
my	O
Document	O
is	O
a	O
non-numeric	O
value	O
i	O
was	O
not	O
able	O
to	O
get	O
it	O
.	O

i	O
am	O
trying	O
to	O
manipulate	O
the	O
pivot	N
data	O
which	O
will	O
give	O
me	O
the	O
row	O
total	O
,	O
not	O
the	O
data	O
from	O
the	O
dataframe	Y
and	O
what	O
i	O
wanted	O
is	O
"	O
%	O
of	O
row	O
total	O
"	O
.	O

you	O
can	O
actually	O
just	O
pass	O
`	O
aggfunc=len	O
`	O
,	O
since	O
`	O
len	Y
`	O
is	O
already	O
a	O
function	O
:)	O

Hi	O
maxymoo	O
in	O
the	O
link	O
you	O
have	O
given	O
they	O
are	O
manipulating	O
one	O
of	O
the	O
column	O
from	O
the	O
dataframe	Y
,	O
but	O
my	O
question	O
is	O
different	O
i	O
am	O
trying	O
to	O
manipulate	O
the	O
pivot	N
data	O
which	O
will	O
give	O
me	O
the	O
row	O
total	O
and	O
what	O
i	O
wanted	O
is	O
"	O
%	O
of	O
row	O
total	O
"	O
.	O

Then	O
you	O
can	O
basically	O
use	O
the	O
solution	O
@USER	O
linked	O
to	O
,	O
but	O
you	O
need	O
to	O
use	O
`	O
iloc	Y
`	O
or	O
similar	O
b	O
/	O
c	O
the	O
table	O
columns	O
are	O
a	O
little	O
complicated	O
now	O
(	O
being	O
a	O
multi-indexed	O
result	O
of	O
the	O
pivot	N
table	O
)	O
.	O

Unfortunately	O
,	O
if	O
I	O
try	O
to	O
resample	N
,	O
I	O
get	O
an	O
error	O
#CODE	O

Are	O
you	O
ask	O
for	O
a	O
process	O
to	O
interpolate	N
,	O
or	O
a	O
process	O
to	O
aggregate	N
,	O
or	O
both	O
?	O

Firstly	O
,	O
prepare	O
a	O
function	O
to	O
map	N
the	O
day	O
to	O
week	O
#CODE	O

Assume	O
now	O
your	O
initialized	O
new	O
dataframe	Y
is	O
`	O
result	O
`	O
,	O
you	O
can	O
now	O
do	O
a	O
join	Y
#CODE	O

The	O
`	O
Nan	O
`	O
is	O
what	O
you	O
need	O
to	O
interpolate	N
.	O

Turns	O
out	O
the	O
key	O
is	O
to	O
resample	N
a	O
groupby	Y
object	O
like	O
so	O
:	O
#CODE	O

Then	O
,	O
I	O
append	N
a	O
row	O
of	O
missing	O
values	O
.	O

Finally	O
,	O
I	O
can	O
insert	N
values	O
into	O
this	O
DataFrame	Y
one	O
cell	O
at	O
a	O
time	O
.	O

This	O
approach	O
works	O
perfectly	O
fine	O
,	O
with	O
the	O
exception	O
that	O
the	O
append	Y
statement	O
inserts	O
an	O
additional	O
column	O
to	O
my	O
DataFrame	Y
.	O

The	O
append	Y
is	O
trying	O
to	O
append	N
a	O
column	O
to	O
your	O
dataframe	Y
.	O

The	O
column	O
it	O
is	O
trying	O
to	O
append	Y
is	O
not	O
named	O
and	O
has	O
two	O
None	O
/	O
Nan	O
elements	O
in	O
it	O
which	O
pandas	O
will	O
name	O
(	O
by	O
default	O
)	O
as	O
column	O
named	O
0	O
.	O

In	O
order	O
to	O
do	O
this	O
successfully	O
,	O
the	O
column	O
names	O
coming	O
into	O
the	O
append	Y
for	O
the	O
data	O
frame	O
must	O
be	O
consistent	O
with	O
the	O
current	O
data	O
frame	O
column	O
names	O
or	O
else	O
new	O
columns	O
will	O
be	O
created	O
(	O
by	O
default	O
)	O
#CODE	O

have	O
merged	O
2	O
dataframes	O
with	O
left	O
join	Y
.	O
works	O
as	O
I	O
expected	O
until	O
I	O
attempt	O
to	O
use	O
the	O
generated	O
value	O
in	O
a	O
simple	O
string	O
concatenation	O
.	O

I	O
am	O
ultimately	O
trying	O
to	O
merge	N
two	O
dataframes	O
together	O
,	O
but	O
I	O
am	O
running	O
into	O
an	O
issue	O
when	O
I	O
try	O
to	O
specify	O
the	O
column	O
on	O
which	O
they	O
should	O
be	O
merged	O
.	O

Conform	N
the	O
index	O
to	O
another	O
frequency	O
.	O

Then	O
its	O
straightforward	O
to	O
resample	N
to	O
another	O
frequency	O
.	O

In	O
the	O
second	O
chunk	O
you	O
are	O
resampling	O
and	O
the	O
result	O
is	O
a	O
Series	O
of	O
monthly	O
frequency	O
so	O
it	O
would	O
appear	O
that	O
the	O
daily	O
information	O
is	O
lost	O
.	O

Then	O
you	O
resample	N
and	O
somehow	O
the	O
days	O
are	O
there	O
?	O

I	O
want	O
to	O
use	O
a	O
combination	O
of	O
map	Y
&	O
lambda	O
functions	O
to	O
do	O
this	O

the	O
map	Y
function	O
does	O
not	O
append	N
to	O
NN	O
.	O

Have	O
you	O
tried	O
using	O
`	O
concat	Y
`	O
and	O
a	O
generator	O
expression	O
instead	O
:	O
#CODE	O

Can	O
Pandas	O
find	O
all	O
the	O
lines	O
that	O
join	N
any	O
pair	O
of	O
dots	O
and	O
don't	O
intersect	O
any	O
of	O
the	O
given	O
lines	O
without	O
iteration	O
?	O

I'm	O
a	O
Stata	O
user	O
and	O
in	O
Stata	O
,	O
I'd	O
be	O
using	O
replace	Y
command	O
conditional	O
on	O
regexm	O
.	O

I'm	O
trying	O
to	O
learn	O
Python	O
and	O
it's	O
been	O
a	O
difficult	O
journey	O
!	O

We	O
then	O
apply	N
another	O
function	O
to	O
this	O
that	O
converts	O
the	O
str	O
numbers	O
to	O
ints	O
,	O
puts	O
these	O
in	O
a	O
list	O
and	O
returns	O
the	O
smallest	O
value	O
:	O
#CODE	O

this	O
is	O
an	O
approach	O
that	O
I	O
hadn't	O
thought	O
about	O
and	O
one	O
that	O
I'm	O
likely	O
to	O
employ	O
down	O
the	O
road	O
.	O
for	O
age	O
,	O
I	O
wanted	O
the	O
series	O
[	O
62	O
,	O
55	O
,	O
67	O
]	O
at	O
the	O
end	O
,	O
and	O
the	O
problem	O
I'm	O
having	O
now	O
is	O
that	O
I	O
can't	O
target	O
just	O
row2	O
when	O
I	O
apply	N
split	Y
(	O
'	O
')	O
.	O

return	O
min	Y
(	O
list	O
(	O
map	Y
(	O
int	O
,	O
x	O
)))`	O
to	O
`	O
def	O
highest	O
(	O
x	O
):	O

return	O
max	Y
(	O
list	O
(	O
map	Y
(	O
int	O
,	O
x	O
)))`	O

I	O
want	O
to	O
apply	N
df	O
[	O
'	O
age	O
']	O
=d	O
f	O
[	O
'	O
e0	O
']	O
[(	O
df	O
[	O
'	O
e0	O
']	O
.str	Y
.match	Y
(	O
pattern7	O
)=	O
=1	O
)]	O
.apply	Y
(	O
lambda	O
x	O
:	O
str	O
(	O
x	O
)	O
.split	Y
(	O
'	O
')	O
[	O
1	O
])	O
to	O
only	O
rows	O
for	O
which	O
df	O
[	O
'	O
e0	O
']	O
.str	Y
.match	Y
(	O
pattern7	O
)=	O
=1	O
)	O
so	O
as	O
to	O
not	O
overwrite	O
what	O
was	O
already	O
in	O
the	O
age	O
column	O
...	O

Suppose	O
I	O
have	O
two	O
DataFrames	O
a	O
b	O
where	O
a	O
is	O
larger	O
than	O
b	O
and	O
has	O
all	O
NaNs	O
.	O

I	O
wish	O
to	O
merge	N
the	O
values	O
from	O
b	O
into	O
a	O
.	O

the	O
w	O
variable	O
will	O
not	O
surpass	O
len	Y
(	O
seq	O
)	O
.	O

For	O
example	O
instead	O
of	O
looping	O
trough	O
every	O
element	O
in	O
a	O
numpy	O
array	O
to	O
do	O
some	O
processing	O
you	O
can	O
apply	N
a	O
numpy	O
function	O
directly	O
on	O
the	O
array	O
and	O
get	O
the	O
results	O
in	O
seconds	O
rather	O
than	O
hours	O
.	O
as	O
an	O
example	O
:	O
#CODE	O

Computing	O
`	O
len	Y
(	O
seq	O
)`	O
inside	O
the	O
loop	O
is	O
not	O
necessary	O
,	O
since	O
its	O
value	O
is	O
not	O
changing	O
.	O

You	O
don't	O
really	O
need	O
the	O
`	O
if	O
`	O
statement	O
,	O
since	O
in	O
your	O
code	O
it	O
always	O
evaluate	O
to	O
true	O
(	O
`	O
w	O
in	O
range	O
(	O
len	Y
(	O
seq	O
))`	O
means	O
`	O
w	O
`	O
maximium	O
value	O
will	O
be	O
`	O
len	Y
(	O
seq	O
)	O
-1	O
`)	O
.	O

I	O
tried	O
pivot	Y
but	O
it	O
returns	O
an	O
error	O

Hmm	O
My	O
dataframe	Y
had	O
12	O
rows	O
but	O
when	O
i	O
tried	O
the	O
unstack	Y
operation	O
the	O
resulting	O
dataframe	Y
has	O
only	O
6	O
rows	O
not	O
exactly	O
what	O
i	O
want.My	O
resulting	O
dataframe	Y
should	O
also	O
have	O
12	O
rows	O

For	O
example	O
,	O
say	O
`	O
Jul-03	O
`	O
data	O
,	O
row	O
`	O
0	O
,	O
6	O
,	O
9	O
`	O
are	O
all	O
records	O
about	O
the	O
same	O
`	O
snapDate	O
`	O
with	O
instance	O
`	O
XX	O
`	O
.	O

So	O
doing	O
a	O
pivot	Y
would	O
reshape	O
these	O
3	O
rows	O
to	O
only	O
one	O
row	O
because	O
those	O
data	O
have	O
been	O
moved	O
to	O
columns	O
.	O

Hi	O
I	O
went	O
ahead	O
and	O
changed	O
the	O
datatype	O
of	O
AvgWaitInMs	O
to	O
int	O
and	O
the	O
pivot	Y
worked	O

What	O
I	O
would	O
like	O
to	O
do	O
is	O
slice	O
each	O
group	O
down	O
to	O
3	O
hours	O
max	O
and	O
append	N
something	O
to	O
the	O
6	O
and	O
9	O
length	O
groups	O
to	O
denote	O
that	O
it	O
is	O
the	O
same	O
page	O
like	O
the	O
following	O
:	O
#CODE	O

So	O
,	O
I	O
truncated	O
my	O
data	O
set	O
in	O
the	O
question	O
to	O
make	O
it	O
easier	O
to	O
read	O
and	O
thinking	O
that	O
whatever	O
solution	O
came	O
would	O
also	O
apply	N
..	O

If	O
'	O
data	O
'	O
is	O
a	O
pd.DataFrame	Y
and	O
you	O
iterate	O
over	O
range	O
(	O
0	O
,	O
len	Y
(	O
data	O
))	O
and	O
then	O
add	O
data	O
to	O
your	O
list	O
'	O
all_info	O
'	O
,	O
you	O
simply	O
add	O
the	O
whole	O
DataFrame	Y
'	O
data	O
'	O
i	O
times	O
to	O
the	O
list	O
.	O

Python	O
pandas	O
:	O
retrieve	O
the	O
field	O
associated	O
to	O
the	O
min	O
of	O
another	O
(	O
cross	O
apply	N
equivalent	O
)	O

In	O
SQL	O
I	O
was	O
used	O
to	O
doing	O
this	O
with	O
a	O
cross	O
apply	N
.	O

PS	O
other	O
than	O
calculating	O
the	O
min	O
first	O
,	O
then	O
doing	O
a	O
join	Y
on	O
primary	O
key	O
and	O
date	O

I	O
can	O
do	O
this	O
in	O
two	O
steps	O
:	O
1	O
)	O
group	O
by	O
primary	O
key	O
and	O
calculate	O
min	O
(	O
date	O
)	O
2	O
)	O
do	O
an	O
inner	O
join	Y
between	O
the	O
starting	O
table	O
and	O
the	O
table	O
calculated	O
in	O
the	O
previous	O
step	O
,	O
on	O
primary	O
key	O
and	O
date	O
,	O
to	O
retrieve	O
the	O
amount	O

Call	O
`	O
resample	Y
`	O
and	O
pass	O
the	O
rule	O
as	O
'	O
10Min	O
'	O
:	O
#CODE	O

The	O
quickest	O
way	O
I	O
know	O
how	O
to	O
wrangle	O
this	O
thing	O
into	O
a	O
long	O
form	O
dataframe	Y
is	O
using	O
`	O
stack	Y
`	O
and	O
then	O
`	O
reset_index	Y
`	O
:	O
#CODE	O

Maybe	O
my	O
real	O
question	O
is	O
"	O
why	O
isn't	O
`	O
melt	Y
`	O
a	O
DataFrame	Y
method	O
?	O

This	O
works	O
pretty	O
well	O
:	O
`	O
pd.melt	Y
(	O
wide_df.reset_index()	O
,	O
"	O
subject	O
")`	O
,	O
but	O
it	O
feels	O
like	O
it	O
would	O
be	O
easier	O
to	O
read	O
as	O
chained	O
method	O
calls	O
that	O
can	O
be	O
read	O
in	O
linear	O
order	O
.	O

not	O
sure	O
why	O
their	O
isn't	O
a	O
``	O
melt	Y
``	O
on	O
DataFrame	Y
,	O
could	O
/	O
should	O
be	O
.	O

bool	N
operator	O
in	O
for	O
Timestamp	O
in	O
Series	O
does	O
not	O
work	O

Is	O
there	O
a	O
way	O
to	O
drop	N
columns	O
in	O
a	O
Dataframe	Y
with	O
column	O
names	O
having	O
a	O
particular	O
letter	O
as	O
I	O
wasn't	O
able	O
to	O
find	O
any	O
information	O
on	O
this	O
?	O

I	O
want	O
to	O
drop	N
all	O
column	O
headers	O
having	O
the	O
letter	O
`	O
F	O
`	O
in	O
them	O
.	O

I	O
was	O
planning	O
on	O
doing	O
it	O
using	O
`	O
df.drop	Y
([	O
df.columns	Y
[[	O
column_names	O
]]]	O
,	O
axis=1	O
)`	O
,	O
but	O
there	O
are	O
so	O
many	O
that	O
I	O
was	O
wondering	O
if	O
there	O
is	O
an	O
easier	O
way	O
to	O
do	O
this	O
.	O

Rolling	Y
argmax	Y
in	O
pandas	O

I	O
have	O
a	O
pandas	O
TimeSeries	O
and	O
would	O
like	O
to	O
apply	N
the	O
argmax	Y
function	O
to	O
a	O
rolling	Y
window	O
.	O

However	O
,	O
due	O
to	O
casting	O
to	O
float	O
from	O
rolling_apply	Y
,	O
if	O
I	O
apply	N
`	O
numpy.argmax()	Y
`	O
,	O
I	O
only	O
obtain	O
the	O
index	O
of	O
the	O
slice	O
of	O
the	O
ndarray	O
.	O

Is	O
there	O
a	O
way	O
to	O
apply	N
a	O
rolling	Y
argmax	Y
to	O
a	O
Series	Y
/	O
DataFrame	Y
?	O

Here	O
is	O
a	O
work-around	O
,	O
essentially	O
doing	O
the	O
apply	Y
'	O
manually	O
'	O
,	O
should	O
be	O
pretty	O
efficient	O
actually	O
.	O

You	O
could	O
do	O
a	O
`	O
shift	Y
`	O
first	O
:	O
#CODE	O

Merge	N
csv's	O
with	O
some	O
common	O
columns	O
and	O
fill	O
in	O
Nans	O

pandas	O
-	O
resample	Y
-	O
upsampling	O
before	O
downsampling	O

My	O
objective	O
is	O
to	O
resample	N
this	O
data	O
frame	O
with	O
a	O
fixed	O
time	O
window	O
(	O
e.g.	O
:	O
1	O
second	O
)	O
using	O
last	O
for	O
regularization	O
when	O
upsampling	O
and	O
the	O
mean	O
for	O
downsampling	O
.	O

Is	O
this	O
possible	O
at	O
all	O
using	O
pandas	O
resample	Y
function	O
?	O

You	O
can't	O
mix	O
upsample	O
/	O
downsample	O
in	O
a	O
single	O
`	O
resample	Y
`	O
operation	O
.	O

I'm	O
not	O
sure	O
why	O
the	O
order	O
of	O
operations	O
would	O
matter	O
to	O
you	O
as	O
long	O
as	O
you	O
get	O
the	O
desired	O
results	O
.	O

Thanks	O
for	O
your	O
answer	O
,	O
it	O
was	O
not	O
clear	O
to	O
me	O
that	O
you	O
had	O
to	O
make	O
multiple	O
calls	O
to	O
resample	Y
.	O

You	O
can	O
then	O
concat	N
this	O
back	O
to	O
get	O
the	O
'	O
I	O
'	O
column	O
back	O
:	O
#CODE	O

Actually	O
setting	O
index_col=	O
'	O
I	O
'	O
when	O
reading	O
allows	O
to	O
avoid	O
the	O
concat	Y
!	O

As	O
a	O
follow	O
up	O
to	O
this	O
post	O
,	O
I	O
would	O
like	O
to	O
concatenate	O
a	O
number	O
of	O
columns	O
based	O
on	O
their	O
index	O
but	O
I	O
am	O
encountering	O
some	O
problems	O
.	O

In	O
this	O
example	O
I	O
get	O
an	O
Attribute	O
error	O
related	O
to	O
the	O
map	Y
function	O
.	O

Help	O
around	O
this	O
error	O
would	O
be	O
appreciated	O
as	O
would	O
code	O
that	O
does	O
the	O
equivalent	O
concatenation	O
of	O
columns	O
.	O

note	O
that	O
support	O
for	O
`	O
filter	Y
(	O
None	O
,	O
iterable	O
)`	O
ceased	O
in	O
Python	O
3	O
,	O
need	O
to	O
do	O
`	O
filter	Y
(	O
bool	N
,	O
iterable	O
)`	O
there	O

I	O
have	O
found	O
workaround	O
which	O
is	O
extremely	O
slow	O
due	O
to	O
the	O
"	O
in	O
python	O
"	O
apply	Y
:	O
#CODE	O

How	O
to	O
drop	N
extra	O
copy	O
of	O
duplicate	O
index	O
of	O
Pandas	O
Series	O
?	O

So	O
how	O
to	O
drop	N
extra	O
duplicate	O
rows	O
of	O
series	O
,	O
keep	O
the	O
unique	O
rows	O
and	O
only	O
one	O
copy	O
of	O
the	O
duplicate	O
rows	O
in	O
an	O
efficient	O
way	O
?	O

One	O
way	O
would	O
be	O
using	O
`	O
drop	Y
`	O
and	O
`	O
index.get_duplicates	Y
`	O
:	O
#CODE	O

Not	O
totally	O
drop	N
the	O
duplicated	O
ones	O
.	O

You	O
can	O
groupby	Y
the	O
index	O
and	O
apply	N
a	O
function	O
that	O
returns	O
one	O
value	O
per	O
index	O
group	O
.	O

@USER	O
sorry	O
,	O
"	O
arbitrary	O
"	O
of	O
length	O
len	Y
(	O
s	O
)	O
:)	O
.	O

Below	O
is	O
my	O
snippet	O
:	O
import	O
pandas	O
as	O
pd	O
;	O
idx_tp	O
=	O
[(	O
'	O
600809	O
'	O
,	O
'	O
20061231	O
')	O
,	O
(	O
'	O
600809	O
'	O
,	O
'	O
20070331	O
')	O
,	O
(	O
'	O
600809	O
'	O
,	O
'	O
20070630	O
')	O
,	O
(	O
'	O
600809	O
'	O
,	O
'	O
20070331	O
')]	O
;	O
dt	Y
=	O
[	O
'	O
demo	O
'	O
,	O
'	O
demo	O
'	O
,	O
'	O
demo	O
'	O
,	O
'	O
demo	O
']	O
;	O
idx	O
=	O
pd.MultiIndex.from_tuples	Y
(	O
idx_tp	O
,	O
names	O
=	O
[	O
'	O
STK_ID	O
'	O
,	O
'	O
RPT_Date	O
'])	O
;	O
s	O
=	O
pd.Series	Y
(	O
dt	Y
,	O
index=idx	O
);	O
#	O
s.groupby	O
(	O
s.index	O
)	O
.first()	Y
will	O
crash	O
on	O
my	O
machine	O

Edit	O
:	O
another	O
solution	O
which	O
is	O
faster	O
is	O
to	O
use	O
`	O
value_counts	Y
`	O
(	O
and	O
normalize	Y
):	O
#CODE	O

I	O
had	O
thought	O
this	O
was	O
more	O
concisely	O
written	O
as	O
a	O
`	O
resample	Y
`	O
,	O
if	O
you	O
use	O
a	O
DatetimeIndex	Y
:	O

len	Y
(	O
Series.unique()	Y
)	O
might	O
be	O
even	O
faster	O
.	O

Interestingly	O
,	O
len	Y
(	O
Series.unique()	Y
)	O
is	O
usually	O
much	O
faster	O
than	O
Series.nunique()	Y
.	O

Next	O
,	O
these	O
3	O
columns	O
should	O
be	O
combined	O
into	O
one	O
column	O
-	O
the	O
mean	O
of	O
the	O
order	O
numbers	O
-	O
but	O
I	O
do	O
know	O
how	O
to	O
do	O
that	O
part	O
(	O
with	O
apply	Y
and	O
axis=1	O
)	O
.	O

I	O
would	O
like	O
to	O
normalize	N
my	O
data	O
by	O
dividing	O
every	O
row	O
by	O
the	O
first	O
value	O
of	O
that	O
very	O
row	O
.	O

I	O
am	O
just	O
getting	O
stuck	O
on	O
"	O
setting	O
with	O
chained	O
indexing	O
"	O
and	O
setting	O
with	O
iloc	Y
/	O
loc	Y
/	O
ix	Y
.	O

I	O
can't	O
figure	O
out	O
how	O
to	O
represent	O
this	O
using	O
iloc	Y
,	O
loc	Y
and	O
ix	Y
.	O

Python	O
2.7	O
&	O
Pandas	O
:	O
How	O
to	O
replace	N
values	O
at	O
12:00	O
with	O
values	O
from	O
11:55	O
?	O

How	O
do	O
I	O
explicitly	O
say	O
'	O
replace	N
the	O
values	O
at	O
19:40	O
:	O
00	O
with	O
the	O
values	O
at	O
19:35	O
:	O
00	O
?	O

Python	O
merge	N
excel	O
documents	O
with	O
dynamic	O
columns	O

However	O
,	O
since	O
they	O
are	O
not	O
100%	O
identical	O
,	O
I	O
cannot	O
simply	O
merge	N
them	O
together	O
and	O
upload	O
it	O
into	O
a	O
database	O
without	O
messing	O
up	O
the	O
data	O
.	O

If	O
a	O
large	O
proportion	O
of	O
them	O
are	O
similar	O
,	O
and	O
this	O
is	O
a	O
one-off	O
operation	O
it	O
may	O
be	O
worth	O
your	O
while	O
coding	O
the	O
solution	O
for	O
the	O
majority	O
and	O
handling	O
the	O
other	O
documents	O
(	O
or	O
groups	O
of	O
them	O
if	O
they	O
are	O
similar	O
)	O
separately	O
.	O

Any	O
recommendations	O
to	O
a	O
db	O
that	O
would	O
allow	O
me	O
to	O
dump	O
a	O
few	O
thousand	O
excel	O
documents	O
and	O
then	O
create	O
join	U
queries	O
to	O
the	O
VIN	O
column	O
?	O

I	O
am	O
doing	O
a	O
transformation	O
on	O
a	O
variable	O
from	O
a	O
pandas	O
dataframe	Y
and	O
then	O
I	O
would	O
like	O
to	O
replace	N
the	O
column	O
with	O
my	O
new	O
values	O
.	O

The	O
problem	O
seems	O
to	O
be	O
that	O
after	O
the	O
transformation	O
,	O
the	O
length	O
of	O
the	O
array	O
is	O
not	O
the	O
same	O
as	O
the	O
length	O
of	O
my	O
dataframe's	O
index	O
.	O

When	O
I	O
check	O
the	O
length	O
,	O
these	O
lengths	O
seem	O
to	O
disagree	O
.	O

The	O
len	Y
(	O
array	O
)	O
says	O
it	O
is	O
2	O
but	O
when	O
I	O
call	O
the	O
stats.boxcox	U
it	O
says	O
it	O
is	O
50000	O
.	O

Print	O
out	O
`	O
len	Y
(	O
df	O
)`	O
and	O
`	O
len	Y
(	O
stats.boxcox	U
(	O
df.variable	O
))`	O
.	O

How	O
to	O
calculate	O
the	O
count	O
of	O
column	O
values	O
less	O
than	O
95	O
on	O
each	O
row	O
on	O
pandas	O
pivot	N
table	O

I	O
am	O
new	O
to	O
pandas	O
pivot	N
tables	O
,	O
how	O
to	O
get	O
the	O
count	O
of	O
column	O
values	O
less	O
than	O
95	O
for	O
a	O
row	O
on	O
pandas	O
pivot	N
table	O
#CODE	O

My	O
decorated	O
DataFrames	O
return	O
new	O
and	O
similarly	O
decorated	O
DataFrames	O
when	O
I	O
use	O
methods	O
such	O
as	O
copy	Y
and	O
groupby.agg	Y
.	O

I.e.	O
,	O
how	O
can	O
I	O
have	O
my	O
decorated	O
DataFrames	O
replace	N
the	O
stock	O
DataFrames	O
?	O

Still	O
not	O
getting	O
the	O
hang	O
of	O
pandas	O
,	O
I	O
am	O
attempting	O
to	O
join	N
two	O
data	O
frames	O
in	O
Pandas	O
using	O
merge	Y
.	O

I	O
have	O
read	O
in	O
the	O
CSVs	O
into	O
two	O
data	O
frames	O
(	O
named	O
dropData	O
and	O
deosData	O
in	O
the	O
code	O
below	O
)	O
.	O

The	O
deosData	O
file	O
is	O
an	O
entire	O
year	O
s	O
worth	O
of	O
observations	O
that	O
I	O
am	O
trying	O
to	O
match	O
up	O
with	O
corresponding	O
entries	O
in	O
dropData	O
.	O

I	O
have	O
gone	O
through	O
the	O
documentation	O
for	O
the	O
merge	Y
function	O
and	O
have	O
tried	O
the	O
following	O
code	O
in	O
various	O
iterations	O
,	O
so	O
far	O
I	O
have	O
only	O
been	O
able	O
to	O
have	O
a	O
blank	O
data	O
frame	O
with	O
correct	O
header	O
row	O
,	O
or	O
have	O
the	O
two	O
data	O
frames	O
merged	O
on	O
the	O
0	O
--	O
(	O
N-1	O
)	O
indexing	O
that	O
is	O
assigned	O
by	O
default	O
:	O

After	O
searching	O
on	O
SE	O
and	O
the	O
Doc	O
s	O
I	O
have	O
tried	O
resetting	O
the	O
index	O
,	O
ignoring	O
the	O
index	O
columns	O
,	O
copying	O
the	O
Date_Time	O
column	O
as	O
a	O
separate	O
index	O
and	O
trying	O
to	O
merge	N
on	O
the	O
new	O
column	O
,	O
I	O
have	O
tried	O
using	O
on=None	O
,	O
left_on	O
and	O
right_on	O
as	O
permutations	O
of	O
Date_Time	O
to	O
no	O
avail	O
.	O

I	O
have	O
checked	O
the	O
column	O
data	O
types	O
,	O
Date_Time	O
in	O
both	O
are	O
dtype	Y
Objects	O
,	O
I	O
do	O
not	O
know	O
if	O
this	O
is	O
the	O
source	O
of	O
the	O
error	O
,	O
since	O
the	O
only	O
issues	O
I	O
could	O
find	O
searching	O
revolved	O
around	O
matching	O
different	O
dtypes	Y
to	O
each	O
other	O
.	O

What	O
I	O
am	O
looking	O
to	O
do	O
is	O
have	O
the	O
two	O
data	O
frames	O
merge	N
where	O
the	O
two	O
'	O
Date_Time	O
'	O
columns	O
intersect	O
.	O

and	O
then	O
do	O
your	O
merge	U
.	O

You	O
can	O
use	O
`	O
join	Y
`	O
,	O
but	O
you	O
first	O
need	O
to	O
set	O
the	O
index	O
:	O
#CODE	O

You	O
can	O
also	O
do	O
`	O
groupby	Y
(	O
...,	O
as_index=False	O
)`	O
,	O
though	O
buggy	O
with	O
apply	Y
in	O
0.12	O
,	O
fixed	O
in	O
0.13	O
.	O

I've	O
converted	O
the	O
last	O
step	O
to	O
no	O
longer	O
be	O
a	O
loop	O
and	O
instead	O
save	O
directly	O
to	O
a	O
list	O
.	O

AFAIK	O
,	O
you	O
would	O
have	O
to	O
separate	O
the	O
two	O
parts	O
and	O
append	N
as	O
lists	O
since	O
the	O
columns	O
of	O
interest	O
are	O
different	O
and	O
converting	O
to	O
a	O
dictionary	O
would	O
include	O
the	O
`	O
NaN	O
`	O
s	O
otherwise	O
.	O

When	O
using	O
the	O
pure	O
XlsxWriter	O
I	O
can	O
apply	N
formats	O
to	O
cells	O
what	O
also	O
works	O
nice	O
.	O

Basically	O
how	O
would	O
I	O
apply	N
`	O
df	O
[	O
'	O
col1	O
']	O
.str	Y
.contains	Y
(	O
'	O
^	O
')`	O
to	O
an	O
entire	O
dataframe	Y
at	O
once	O
and	O
filter	O
down	O
to	O
any	O
rows	O
that	O
have	O
records	O
containing	O
the	O
match	O
?	O

Pandas	O
:	O
apply	N
different	O
functions	O
to	O
different	O
columns	O

i	O
am	O
looking	O
to	O
apply	N
multiply	O
masks	O
on	O
each	O
column	O
of	O
a	O
pandas	O
dataset	O
(	O
respectively	O
to	O
it's	O
properties	O
)	O
in	O
python	O
.	O

how	O
can	O
i	O
apply	N
the	O
concat_mask	O
on	O
df	O
,	O
so	O
that	O
i	O
select	O
rows	O
,	O
in	O
which	O
all	O
Boolean	O
criteria	O
are	O
matched	O
(	O
are	O
True	O
)	O
?	O

.	O
Can	O
You	O
insert	N
that	O
into	O
your	O
answer	O
?	O

In	O
the	O
proper	O
code	O
i	O
actually	O
iterate	O
throw	O
all	O
columns	O
and	O
apply	N
various	O
of	O
diffenrent	O
conditions	O
to	O
mask	N
each	O
column	O
.	O

If	O
you	O
return	O
a	O
Series	O
of	O
the	O
(	O
split	O
)	O
location	O
,	O
you	O
can	O
merge	N
(	O
`	O
join	Y
`	O
to	O
merge	N
on	O
index	O
)	O
the	O
resulting	O
DF	O
directly	O
with	O
your	O
value	O
column	O
.	O

If	O
I'm	O
not	O
mistaken	O
,	O
it	O
only	O
works	O
if	O
`	O
df	O
`	O
has	O
index	O
that	O
is	O
`	O
range	O
(	O
len	Y
(	O
df	O
))`	O
,	O
right	O
?	O

`	O
join	Y
`	O
is	O
shorthand	O
for	O
merging	O
on	O
index	O
with	O
both	O
frames	O
,	O
so	O
the	O
indices	O
need	O
only	O
be	O
consistent	O
(	O
which	O
it	O
will	O
be	O
here	O
as	O
the	O
apply	Y
and	O
col	O
selection	O
don't	O
affect	O
it	O
)	O
.	O

How	O
to	O
resample	N
a	O
dataframe	Y
with	O
different	O
functions	O
applied	O
to	O
each	O
column	O
?	O

You	O
can	O
also	O
downsample	O
using	O
the	O
`	O
asof	Y
`	O
method	O
of	O
`	O
pandas.DateRange	U
`	O
objects	O
.	O

@	O
Wes	O
McKinney	O
this	O
should	O
be	O
`	O
resample	Y
`	O
in	O
0.8	O
,	O
isn't	O
it	O
?	O

Therefore	O
,	O
I	O
join	Y
the	O
index	O
of	O
`	O
count_df	O
`	O
(	O
`	O
left_index=True	O
`)	O
with	O
the	O
`	O
CompanyName	O
`	O
column	O
of	O
`	O
df	O
`	O
(	O
`	O
right_on=	O
"	O
CompanyName	O
"`)	O
.	O

You	O
can	O
drop	N
the	O
extraneous	O
column	O
using	O
`	O
df.drop	Y
`	O
:	O
#CODE	O

(	O
3	O
)	O
save	O
the	O
header	O
columns	O
for	O
concat	Y
later	O
#CODE	O

(	O
5	O
)	O
output	O
:	O
concat	Y
[	O
header	O
data	O
]	O
.	O
write	O
output	O
#CODE	O

groupby	Y
after	O
concat	Y
,	O
column	O
missing	O
in	O
the	O
group	O
mean	O

concat	Y
two	O
dataframe	Y
,	O
then	O
groupby	Y
'	O
type	O
'	O
and	O
calculate	O
the	O
mean	O
,	O
columns	O
of	O
second	O
df	O
,	O
i.e.	O
d1~d10	O
,	O
showing	O
in	O
the	O
concat'ed	O
dataframe	Y
but	O
not	O
in	O
the	O
grouped	O
mean	O
.	O

I	O
want	O
to	O
create	O
a	O
new	O
DataFrame	Y
such	O
that	O
each	O
row	O
is	O
created	O
from	O
the	O
original	O
df	O
but	O
rows	O
with	O
loc	N
counts	O
greater	O
than	O
2	O
are	O
excluded	O
.	O

That	O
is	O
,	O
the	O
new	O
df	O
is	O
created	O
by	O
looping	O
through	O
the	O
old	O
df	O
,	O
counting	O
the	O
number	O
of	O
loc	N
rows	O
that	O
have	O
come	O
before	O
,	O
and	O
including	O
/	O
excluding	O
the	O
row	O
based	O
on	O
this	O
count	O
.	O

The	O
output	O
excludes	O
the	O
4th	O
row	O
in	O
the	O
original	O
df	O
because	O
its	O
loc	N
count	O
is	O
greater	O
than	O
2	O
(	O
i.e.	O
3	O
)	O
.	O

Also	O
,	O
be	O
careful	O
with	O
your	O
column	O
names	O
,	O
since	O
`	O
loc	N
`	O
clashes	O
with	O
the	O
`	O
.loc	Y
`	O
method	O
.	O

So	O
you	O
get	O
a	O
string	O
back	O
:)	O
.	O

You	O
can	O
use	O
eval	Y
(	O
""	O
[	O
1.5	O
,	O
2.5	O
,	O
3.5	O
]"")	O
,	O
but	O
I	O
hear	O
it's	O
bad	O
practice	O
.	O

You	O
can	O
map	N
your	O
lists	O
to	O
strings	O
by	O
using	O
`"	O
,	O
"	O
.join	Y
(	O
your_list	O
)`	O
given	O
that	O
you	O
only	O
use	O
floats	O
.	O

merge	Y
the	O
dataframe	Y
on	O
ID	O
#CODE	O

The	O
`	O
merge	Y
`	O
did	O
the	O
trick	O
,	O
but	O
I	O
thought	O
it	O
was	O
more	O
usefull	O
to	O
just	O
do	O
a	O
`	O
dfMerged.dropna()	O
`	O
after	O
the	O
merge	Y
and	O
that	O
will	O
be	O
the	O
set	O
with	O
the	O
difference	O
.	O

yes	O
,	O
essentially	O
,	O
the	O
answer	O
was	O
really	O
about	O
the	O
`	O
merge	Y
`	O
method	O
,	O
which	O
allows	O
you	O
to	O
sql-like	O
joins	O
.	O

Instead	O
,	O
I	O
get	O
an	O
error	O
telling	O
me	O
that	O
equiv	O
is	O
not	O
a	O
callable	O
function	O
.	O

Fair	O
enough	O
,	O
it's	O
a	O
dictionary	O
,	O
but	O
even	O
if	O
I	O
wrap	O
it	O
in	O
a	O
function	O
I	O
still	O
get	O
frustration	O
.	O

So	O
I	O
tried	O
to	O
use	O
a	O
map	Y
function	O
that	O
seems	O
to	O
work	O
with	O
other	O
operations	O
,	O
but	O
it	O
also	O
is	O
defeated	O
by	O
use	O
of	O
a	O
dictionary	O
:	O
#CODE	O

ok	O
,	O
revised	O
the	O
answer	O
;	O
you	O
can	O
do	O
almost	O
anything	O
inside	O
the	O
apply	U
FYI	O

In	O
order	O
to	O
normalize	O
data	O
in	O
a	O
pandas	O
DataFrame	Y
I	O
wrote	O
the	O
following	O
functions	O
:	O
#CODE	O

If	O
you	O
want	O
the	O
values	O
themselves	O
,	O
you	O
can	O
`	O
groupby	Y
`	O
'	O
Column1	O
'	O
and	O
then	O
call	O
`	O
apply	Y
`	O
and	O
pass	O
the	O
`	O
list	U
`	O
method	O
to	O
apply	N
to	O
each	O
group	O
.	O

You	O
could	O
`	O
groupby	Y
`	O
on	O
`	O
Column1	O
`	O
and	O
then	O
take	O
`	O
Column3	O
`	O
to	O
`	O
apply	Y
(	O
list	O
)`	O
and	O
call	O
`	O
to_dict	Y
`	O
?	O

Pandas	O
-	O
How	O
can	O
I	O
set	O
rules	O
for	O
selecting	O
which	O
duplicates	O
to	O
drop	N

What	O
I	O
want	O
to	O
do	O
is	O
drop	N
the	O
values	O
that	O
have	O
the	O
same	O
index	O
(	O
date	O
time	O
)	O
,	O
but	O
I	O
want	O
to	O
make	O
a	O
rule	O
like	O
:	O

I	O
have	O
tried	O
using	O
groupby	Y
and	O
apply	Y
in	O
several	O
different	O
ways	O
but	O
I	O
cant	O
get	O
it	O
to	O
work	O
.	O

You	O
could	O
use	O
`	O
del	O
df	O
[	O
'	O
dist	O
']`	O
to	O
drop	N
the	O
dist	O
column	O
when	O
you	O
no	O
longer	O
need	O
it	O
.	O

Though	O
I	O
was	O
wondering	O
if	O
you	O
could	O
do	O
it	O
immediately	O
using	O
lambda	O
,	O
apply	Y
and	O
groupby	Y
.	O

I	O
am	O
sorry	O
I	O
am	O
trying	O
to	O
insert	N
code	O
into	O
comments	O
I	O
cant	O
do	O
it	O

All	O
I	O
am	O
doing	O
at	O
the	O
moment	O
is	O
loading	O
the	O
.csv	O
as	O
a	O
dataframe	Y
and	O
then	O
writing	O
it	O
to	O
the	O
db	O
using	O
`	O
df.to_sql	Y
(	O
table_name	O
,	O
engine	O
,	O
index=False	O
,	O
if_exists=	O
'	O
append	Y
'	O
,	O
chunksize=1000	O
)`	O

I	O
want	O
to	O
transform	N
it	O
into	O
a	O
single	O
column	O
data	O
with	O
index	O
being	O
year-month	O
.	O

I	O
try	O
to	O
stack	N
my	O
original	O
data	O
but	O
it	O
becomes	O
a	O
time	O
series	O
,	O
which	O
has	O
the	O
year	O
mix	O
with	O
my	O
values	O
.	O

`	O
set_index	Y
`	O
to	O
`	O
Year	N
`	O
first	O
,	O
and	O
then	O
`	O
stack	Y
`	O
.	O
