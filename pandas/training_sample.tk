How to check if the signs of a Series conform to a given string of signs ?
For example I have a Series as below , #CODE
To check the whether the elements of the Series are positive you could create a Boolean Series like this : #CODE
To get ` sign ` into a similar Boolean series , you can need to interpret the strings ' + ' and ' - ' strings as Boolean values . For example : #CODE
Now you can compare the two series and use ` all ` . In one line , the whole thing looks like this : #CODE
This solution requires some numpy functions , but since you are using pandas for your data series , this is probably not an issue for you . #CODE

Python Pandas Panel counting value occurence
I have a large dataset stored as a pandas panel . I would like to count the occurence of values 1.0 on the minor_axis for each item in the panel . What I have so far : #CODE
Having never dealt with panels before I tried this : ` P [ P.minor_axis == ' a '] .min() ` does it do what you want ?
To count all the " b " values 1.0 , I would first isolate b in its own DataFrame by swapping the minor axis and the items . #CODE
Thanks for thinking with me guys , but I managed to figure out a surprisingly easy solution after many hours of attempting . I thought I should share it in case someone else is looking for a similar solution . #CODE

Search by value in pandas series
Give a pandas series #CODE

How to combine pivot with cumulative sum in Pandas
I have a simple dataframe : #CODE
Which i can easily pivot with the dates as columns using the following function : #CODE
However I cant find a way to use a cumulative sum in place of the np.sum , I would like to display the data in the following format ? Is this possible ? #CODE

How to convert string series into integer
Then , apply ` get ` to the column , returning the original value if it is not in the dictionary : #CODE
@USER If all of the unique items contained in the series are in the dictionary keys , then ` .map ( d )` is more than five times as fast . However , any missing value appears as a ` NaN ` . Using a ` lambda ` function with ` get ` on the dictionary appears to have virtually identical performance . #CODE
Use ` map ` for this , it's cyython-ised : ` df [ col ] .map ( d )`

You mean your df somehow has no name ? It is probably just `''` , anyway you can directly assign doing ` df.columns = [ ' new_name ']` , the normal convention is to call `df.rename(columns={'':'new_name ' } , inplace=True )`

How to delete some of the mean weekly values calculated by pandas ( .resample ) ?
I have added a list ` Wase = [ " 2013-12-22 " , " 2014-01-05 "]` , as suggested in comments and used ` Temp_plot1 = Temp_plot.drop ( Wase )` Now I got any error , which says ` ValueError : labels [ ' 2013-12-22 ' ' 2014-01-05 '] not contained in axis ` .
If you have a container ( e.g. ` list `) of dates that you want to exclude called , say , ` unwanted_dates ` you can just do ` Temp_plot.drop ( unwanted_dates )` . Note this returns a view with the desired dates excluded and doesn't actually alter ` Temp_plot ` . To drop them permanently do ` Temp_plot = Temp_plot.drop ( unwanted_dates )` or ` Temp_plot.drop ( unwanted_dates , inplace=True )`
Also , you can set your index when you read the csv file : ` pd.read_csv ( " Book1.csv " , parse_dates=['date '] , index_col='date ')`
You need to create a calendar of holidays using ` dt.date ( year , month , day )` . Then you filter the holidays from the index using a list comprehension structure as shown below . Lastly , you select these filtered dates using ` .ix ` which selects data from a dataframe based on the index value . #CODE
Thanks for your time but it is not working . I have added ` holidays = [ dt.date ( 2013 , 12 , 17 ) , ( 2013 , 12 , 1 8) , ( 2013 , 12 , 19 ) , ( 2013 , 12 , 20 ) , ( 2013 , 12 , 21 ) , ( 2013 , 12 , 22 )]` `Temp=Temp.set_index('date ')` and same lines like yours ` idx= ... ` and ` Temp_plot= ... ` but it gave same results as before meaning the holidays are not removed . I have also tried to to sampling on daily basis before doing it on weekly basis but this ain't give me any different results . Your help will be appreciated .
because ...? What is the error . My guess is the format of your date . What is ` type ( Temp.index.iat [ 0 ])` ?
You need dt.date ( ... ) for each date in your holidays . Or you can do this : ` holidays = [ dt.date ( d [ 0 ] , d [ 1 ] , d [ 2 ]) for d in [( 2013 , 12 , 1 8) , ( 2013 , 12 , 19 ) , ( 2013 , 12 , 20 ) , ( 2013 , 12 , 21 ) , ( 2013 , 12 , 22 )]]` . Also , I tried the code using your sample data and it works for me .

Creating data histograms / visualizations using ipython and filtering out some values
but get an error due to unrecognized dtypes ( there are some entries that aren't in datetime format ) . How can I exclude these in the original command ?
Also , what if I want to create the same histogram but filtering to keep only records that have id's ( in an account id field ) starting with the integer ( or string ? ) ' 2 ' ?
Ultimately , I want to be able to create histograms , line plots , box plots and so on but filtering for certain months , user id's , or just bad ' dtypes ' .
and when I apply the command pd.to_datetime() to these columns I get fields resulting that look like : #CODE
the dtype of the duration column fields ( #URL ) is : #CODE
Check ` df.Duration.dtype ` . If it isn't a numeric dtype , you can convert using ` df.Duration.astype ( int )` or ` ( float )` , provided that you only have strings containing numbers , and no NaNs . It helps if you post a sample ` df.Duration.head ( 4 )` .
oops-I typed df.Duration.dtype and get dtype ( ' < m 8[ ns ]') and df.Duration.head ( 4 ) returns 0 00:14 : 00
Name : Duration , dtype : timedelta64 [ ns ] so obviously it is not an integer which it would need to be-these are time stamps so in hours minutes seconds , will df.Duration.astype ( int ) recognize this and convert accordingly or do I need a different command ? Thanks !
I read somewhere else that if ' Duration ' is in the form #URL then Duration.days returns days , Duration.minutes the minutes , Duration.seconds the seconds I could then just define a function which inputs the duration ( dur ) and outputs Duration.days * 3600 + Duration.minutes + Duration.seconds / 60 and then write df.f ( dur ) .hist ( bins=10 ) correct ?
I do have a duration-I calculated it by subtracting two dates and added it to my data frame with : df['Duration '] = pd.to_datetime(df['End Time'])-pd.to_datetime(df['Start Time '])
I think you need to convert duration ( timedelta64 ) to int ( assuming you have a duration ) . Then the .hist method will work . #CODE
sorry , my other question is : my dtype shows as : dtype ( ' < m 8[ ns ]') , is this timedelta64 ?
No , as I said , you have a date . Look at my output above , a duration dtype will show as dtype : timedelta64 [ ns ] ( if the base is nanosecond ) or something similar . You said you subtracted your dates . Did you actually assign that result to a column ? Are you referencing to the right column ?
I did this command : df['Duration '] = pd.to_datetime(df['End Time'])-pd.to_datetime(df['Start Time ']) to add the column ' Duration ' to my data frame ' df ' where the ' End Time ' and ' Start Time ' columns are of the format : 2014 / 03 / 27 23:13 for example-I have checked and the result of this was to give fields in the format of #URL of time lapsed
I am trying to figure out how to get a slice of my data but omitting some of it . I am using ipython and there are , among others , two columns in my data frame called ' Start Time ' and ' End Time ' . All I did was use the command I wrote above to subtract these two columns and create another which has the correct time lapse-I will edit my question ( if I can ) to show the result of the to_datetime() transformation .
To convert your timedelta to minutes , you just have to divide it by 1 minute , hence : `df['minutes '] = df.Duration / timedelta ( 1 , ' m ')` . Sorry , I thought you had read the last line of my answer . Then you'll be able to do ` df.minutes.hist() ` without problems .
Thanks ! I ended up using the .str somehow but this is good to know !
no , .str was somethin else , I did : `df['Durationendminusstart '] = pd.to_timedelta ( df.Duration , unit='ns').astype('timedelta64 [ m ]')`

Trouble pivoting / reshaping a pandas dataframe
I have the following dataframe : #CODE
I'd like to pivot the data so that I end up with : #CODE
Fillna's : #CODE

does your data get correctly parsed with read_csv ?
If your data's correctly parsed by ` read_csv ` , why do you have a column named `'0 '` ? I'd've thought that it'd be `data[data['Value '] == 0 ]` .

Truncating column width in pandas
I'm reading in large csv files into pandas some of them with String columns in the thousands of characters . Is there any quick way to limit the width of a column , i.e. only keep the first 100 characters ?
Do you mean after the fact , or do you want / need to truncate them before they're even stored in the frame ?
Before even stored would be great . Would an apply be the fastest way for after the fact ?
Writing an apply right now . Is there a way to get the width of a column of type object ?
`df['string col '] .apply ( lambda x : x [: 100 ])` will keep only the first 100 characters
Would be nice to check if it needs to be truncated beforehand . Can't seem to find any attribute that gives the width .
If you can read the whole thing into memory , you can use the ` str ` method for vector operations : #CODE
Also note that you can get a Series with lengths using #CODE
`` converters `` are called row-by-row ( by a cython function ) . I think the `` str `` conversion would be faster ( your first example )

Pandas DataFrame to Excel Problems
Does it have to output as an excel workbook , or would CSV suffice ? You could change your list of tuples to a dataframe then output as a csv quite easily ...
More detailed discussion is in this answer too . Basically , openpyxl version 2 deprecated the style-conversion functions that map dictionary style definitions to the new openpyxl api , but Pandas still uses the old style , and for some reason the deprecation pass-through function errors out .

Using this map of NYC I'd like to change Manhattan to be bright blue . But when I change the individual patch color of Manhattan all the other patch colors change too . This was unexpected to me .
A possible solution is using ` geopandas.plotting.plot_multipolygon ` to specifically add only one geometry object with blue colors to the existing figure : #CODE

I'm trying to monkeypatch how ` pandas ` Panel's slicing ( ` __getitem__ `) . This is straightforward to do with a basic function , foo . #CODE
Where ` ORIGINAL_getitem ` is storing the original Panel method . I'm trying to extend to the case where ` foo() ` is not a function , but an instance method of an object , ` Foo ` . For example : #CODE
` Foo.foo() ` must access the attribute ` self.name ` . Therefore , the monkeypatched function would need a reference to the Foo instance somehow , in addition to the Panel . How can I monkepatch panel with ` Foo.foo() ` and make self.name accessible ?
But this doesn't work . A reference to self is passed , but no access from the calling Panel possible . That is : #CODE
Passes a reference to ` Foo ` , not to ` Panel ` .
In short , it provides a cool set of features , it has a wide range of unit tests , and it comes with a fancy doc that should cover everything you need to get started . Make sure to also check the FAQ !
One way to do this is to create a closure ( a function with reference to names other than locals or globals ) . A simple closure : #CODE

append pandas.DataFrame.GroupBy results into another dataframe
What I'm trying to do is make a dataframe that only keeps the sum of the counts ( the column after year ) regardless of the year .
I used groupby to aggregate my dataframe , and tried to save the results in another dataframe ( df_ngram ) . But it seems like it isn't appending at all . Below is what I get when I run this . I'm not sure how to deal with the groupby results . How can I aggregate the results from groupby ? Or can I get what I want without using groupby ? #CODE
You need to append the intermediate DataFrames to a list and then concatenate the results .
Instead of ` df_ngram.append ( agg_chunk )` , you want ` df_agg.append ( agg_chunk )`
I did ` df_ngram = pd.concat ( df_agg , ignore_index=True )` but then ` print df_ngram.head ( 10 )` gives me ` 0 279
Name : count , dtype : int64 ` which isn't what I wanted . I want the dataframe indexes to be my ngrams not index numbers . Setting ` ignore_index=False ` also doesn't work . And I want to set the column names to [ ' ngram ' , ' count '] , but this can be done in separate line .

I am completely new to python and Pandas of course . I am trying to run a function " get url " which is function to get the complete extended url from small Url . I have a data frame in python consists all the short URLs . Now I am trying to do with following ways . One is to use " for " loop which loops and apply function on all the elements and will create a another series of extended URL but I am not able to , dont know why , I tried to write it like #CODE
again second solution i tried was passing a whole array to applymap fucntion #CODE
can you dput what your function is doing ? Adding the same suffix to each url or particular suffix proper to each base url ?
expanded ( i ) = get_real ( df2 [[ i ]])` doesn't work because the iterable returned is the column not each element , so unless your code understands how to operate on a pandas Series then your code won't work
Hi .. this is my defined function .. def myfunction ( url ): exturl= urllib2.urlopen ( HeadRequest ( url )) .geturl()
return exturl and I am applying it on my list df3['new']=df3['first_link '] .apply ( myfunction ) ... but because or errors its not working .. that why looking for for loop
If the short urls are a column in the pandas dataFrame , you can use the ` apply ` function ( though I am not sure if they would resume on error , most probably not ) .
Hi .. this is my defined function .. def myfunction ( url ): exturl= urllib2.urlopen ( HeadRequest ( url )) .geturl()
return exturl and I am applying it on my list df3['new']=df3['first_link '] .apply ( myfunction ) ... but because or errors its not working .. that why looking for for loop
Hi Anand - Thanks for help . but it is working same as apply function . as soon as it faces a error it pass completely . error is like HTTPError : HTTP Error 404 : Not Found ....
ANd i am using the formula for idx in df5.index :
I think the problem you are having is treating the dataframe just like a dictionary that has keys and values .

I'm trying to select , by group , the minimum date when a condition is met and assign it to a new column : #CODE
I've been trying to use groupby in combination with transform , but don't know how to get transform to take account of conditions based on other columns .
First change the ` bool ` column to actually booleans ( also be careful with your names . DataFrame has a ` bool ` method ): #CODE
Finding the minimum dates is pretty easy . Use the ` bool ` column to index into ` df ` : #CODE
There are probably a bunch of ways to set the values , but one is to set ` Group ` as the index and ` join ` the ` dates ` . #CODE
Thanks . Worked except for joining bit . I had to convert the series to a dataframe and then perform a left merge : ` pd.merge ( df , dates , how='left ' , on='Group ')`

I have a pandas data frame which has some rows and columns . Each column has a header . Now as long as I keep doing data manipulation operations in pandas , my variable headers are retained . But if I try some data pre-processing feature of Sci-kit-learn lib , I end up losing all my headers and the frame gets converted to just a matrix of numbers .
I understand why it happens because scikit-learn gives a numpy ndarray as output . And numpy ndarray being just matrix would not have column names .
@USER : please provide a very simple , reproducible example ! A three row dataframe would make your question more understandable . ( Maybe just copying ` saved_cols = df.columns ` and then reassigning it to the modified ` df ` would do the trick , but I'm not sure that's what you need )
Indeed , as @USER says , copying ` saved_cols = df.columns ` and then when you got the series , doing ` pandas.DataFrame ( series , saved_cols )` you get your dataframe back . I do it for example when using ` train_test_split ` , which gives back a ` numpy ndarray ` , but I need to use it as a dataframe . It is not something to be particularly proud of , but in my opinion is good enough .
scikit-learn indeed strips the column headers in most cases , so just add them back on afterward . In your example , with ` X_imputed ` as the ` sklearn.preprocessing ` output and ` X_train ` as the original dataframe , you can put the column headers back on with : #CODE

For example , I would like to create a new column titled ' Rheum ' , which takes on a value of ' 1 ' if the expression ' 391.1 ' appears in a corresponding column ' ICD ' per row . In some rows of the ICD column there are cells which have a variety of expressions in the form ' 424.1 , 391.1 , 420.2 , etc ' .
Could you post part of your dataframe ?
Please edit your question with your data as dataframe
You can use ` str.contains ` : #CODE
@USER You could do that with method of dataframe and series ` astype ( str )`
Maybe using `df['ICD '] .apply ( str )` ... please try to do some search before
` str.contains ` return bool values . You could convert it to integer with simple add 0 : #CODE
Yes , you could use ` df = pd.read_csv('your_file ' , names=['patient ' , ' ICD9 '])` and then operate with that df

So my idea was , I will create an excel-template with these tricky parts and then from python just insert dynamic data to this template .
@USER : That's not really relevant here , and not a productive comment . There isn't any code he could have tried , given the tools that he's mentioned in the question and the tags . If you don't think this question is a good fit for Stack Overflow , then comment on why you believe that , and / or downvote the question .
There isn't an easy way to do this with any of the usual " direct file manipulation " libraries in Python ( xlrd , xlwt , XlsxWriter , OpenPyXL ; these are what pandas uses ) . The reason is that the structure of a workbook file is such that it's impossible or prohibitively difficult ( depending on whether you're talking about .xls or .xlsx ) to do anything resembling " in-place " editing , short of re-implementing Excel itself .

use a list of values to select rows from a pandas dataframe
how to filter the dataframe rows of pandas by within / in ?
Lets say I have the following pandas dataframe : #CODE
This is indeed a duplicate of how to filter the dataframe rows of pandas by " within " / " in " ? , translating the response to your example gives : #CODE

So to be clear , you want to do basically an inner join on the ` DATE ` key in df2 such that it is within the Start / End Date range ?
@USER , Correct , and also summing the values of the dates within the range .
@USER , I made the correction , I removed the excel tag . Thanks
hmm .. works for me . They key step is the second last . First check that the index on both frames is a datetime index . Then check the output of ` map ( df1.index.asof , df2.index )` . This is the array indicating the groups ( ` df1.index.asof ` is a function which is applied to the index of ` df2 `) . For each date in ` df2.index ` the output should be the latest date from ` df1.index ` before that date .
for some reason `df2['DATE '] = pd.to_datetime ( df2.DATE )` makes the df2 index of DATE result to ` 1970-01-01 00:00 : 00.0 20110706 `
found the issue ! replacing `df2['DATE '] = pd.to_datetime ( df2.DATE )` with
`df2['DATE '] = pd.to_datetime ( df [ " DATE "] , format= " %Y%m%d ")` resolves it ! Thanks for your help !
ahh , this is because the values in `'DATE '` are ` ints ` rather than strings . When you pass an ` ints ` to ` pd.to_datetime ` it treats them as nanoseconds from epoch ( unless you specify the unit , e.g. `unit='ms '` . If you were to pass a string such as `'20110706 '` you wouldn't need to specify the format .

Python -- Pandas : How to apply aggfunc to data in currency format ?
I have a table above . Want to apply groupby function to the data and apply sum ( over revenue_total ) . Pandas gives an NA value since revenue_total is an object data type . Any help #CODE
df = pd.read_csv ( path )
df[['Product ID ','Revenue Total ']] .head()
df.groupby(['Product ID ']) [[ ' Revenue Total ']] .sum()
Code should be added as an edit to the question ( and put in a code block ) . I don't know enough Python to do the edit for you , but as is it is not very helpful to potential answerers .
I want the header =0 . The issue is dtype for Revenue Total is object and group by is not able to perform an agfunc on that .
Then you could remove the ` $ ` and commas and parse it into a DataFrame using #CODE
to your call to ` pd.read_csv ` . That is what is stripping the ` $ ` and commas .

I have a DataFrame that looks like this : #CODE
Assuming the format is consistent throughout the column , you could use ` str.split ` to extract the numerators and denominators , and then do the division : #CODE
I might do this with apply rather than eval ( especially if I didn't trust the source ): #CODE
Note : You could catch the error if the entry is not of the correct form .

pandas 0.13 read_excel new format
I'm working with pandas a few time ago . In 0.12 version , I read excel files using pandas.read_excel ( filename , sheetname , index_col ) , the read file was in the next format , with the header in the first row : #CODE
Now , when I save a dataframe to an excel file , the header format changes , in the first row are the columns names , and in the second row the index name , as show in the next table : #CODE
The DataFrame will be written in a way that tries to mimic the REPL output . One difference from 0.12.0 version is that the index_label will be placed in the second row instead of the ? rst . You can get the previous behaviour by setting the merge_cells option in to_excel() to False : #CODE

How to save a dataframe as a csv file with ' / ' in the file name
I want to save a ` dataframe ` to a ` .csv ` file with the name `'123 / 123 '` , but it will split it in to two strings if I just type like `df.to_csv('123 / 123.csv ')` .
and that a charmap can find a character that looks like it , which is legal . In this case a division character

Python 2.7 with Pandas : How does one recover the non intersecting parts of two dataframes ?
I have two data frames and the second is a subset of the first . How do I now find the portion of the first dataframe that is not contained in the second one ? For example : #CODE
Well , one way to do this is using ` isin ` ( but you can also do it with the ` merge ` command ... I show examples for both ) . For example : #CODE
Explanation . ` isin ` can check using multiple columns if you feed it a dict : #CODE
And then ` isin ` will create a booleen df which I can use to select the columns we want ( in this case require all the columns to match and then negate with ` ~ `) : #CODE
In the specific example we don't need to feed ` isin ` a dict version of the dataframe because we can identify the valid rows by only looking at column A : #CODE
You can also do this with ` merge ` . Create a unique column in the subset dataframe . When you merge , the unique rows from the larger dataframe will have ` NaN ` for the column you created : #CODE
Edit : @USER notes that the merge method performs much better for large dataframes .
Great , thanks for the very complete answer showing several options . I appreciate your help !
I just wanted to add some further documentation for other people reading this ... The latter method mentioned above is MUCH faster than the former for big dataframes . I tried both and didn't get a solution after 10 minutes of sorting through dataframe with 80,000 rows and 5 columns , but it only takes a few seconds for the latter merge method . Thanks again !
Thanks @USER , I edited the answer to indicate the better performance of the ` merge ` method .

Groupby values start with
is there a way to compute all value with the same prefix #CODE
No , you'd have to build a list where the prefix is matched in the columns but there is no built in method

Please add reproducible code and well-formatted tables .
Just group by ` Date ` and ` Income ` and get the sum : #CODE
in order to filter the dataframe before getting the sum , just set the filter before grouping : #CODE
Thanks . This i already did , i also did for many filters ( combination of 2 or 3 ) . How can i implement below conditions : 1 ) If only one filter is selected e.g. Male then all other filters should take all the values corresponding to Only Male filter 2 ) If only two filters are selected e.g. Only Male and IT profession are selected .. then all values correspoding to only these two filters should be given .
Actually grouping the dataframe lead to have the ` Date ` as index of the new dataframe , and the ` Income ` as values . So you can access them through ` grouped.index ` and ` grouped.values ` . Hope that helps . I update also the answer with a new way to do it .

Pandas : Replacing column values in dataframe
I'm trying to replace the values in one column of a dataframe . The column ( ' female ') only contains the values ' female ' and ' male ' .
I would ideally like to get some output which resembles the following loop element-wise . #CODE
Any help will be appreciated .
The reason your code doesn't work is because using `['female ']` on a column ( the second `'female '` in your `w['female']['female ']`) doesn't mean " select rows where the value is ' female '" . It means to select rows where the index is ' female ' , of which there may not be any in your DataFrame .
Thanks . Exactly what I was looking for . If I were to map ' female ' to 1 and anything else to ' 0 ' . How would that work ?
You can edit a subset of a dataframe by using loc : #CODE

Pandas merge not giving expected output with datetime
I have two dataframes : the first dataframe " fgblquotef " sample is : #CODE
the second dataframe " df " has column df [ " DateTimesy "] created using : #CODE
and then I merge using : #CODE
Which is wrong because there should be " fgblquotef " entries mixed up in there as well and not just " df " entries . Can anyone explain what is going on here and where I have made a mistake ?
make sure both dataframe columns have the same data type . I know Pandas has been bugy at times with dates but hard to say without more information .
What more information can I provide : I use fgbmquotef [ " DateTimes "] = pd.to_datetime ( fgbmquotef.dateTime , unit = " s ") and the same for df to create dtype : datetime64 [ ns ] .
df2 = pd.merge ( df , fgbmquotef , left_on = " DateTimesy " , right_on = " DateTimesy " , how = " outer ") #although you shouldn't have to .
or without suffixes : #CODE
Finally try the concatenate function : #URL
I am getting : MergeError : Must pass right_on or right_index=True . Also how will this merge using DateTimesy .
I just edited the answer , try : df2 = pd.merge ( df.set_index ( " DateTimesy " , drop=False ) , fgbmquotef.set_index ( " DateTimesy " , drop=False ) , left_index=True , right_index=True , how = " outer " , suffixes = ('_df ' , ' _fgbmquotef '))
or maybe : df2 = pd.merge ( df.set_index ( " DateTimesy ") , fgbmquotef.set_index ( " DateTimesy ") , left_index=True , right_index=True , how = " outer ")
I have ticked this as the right answer but it seems to be a bug in Pandas , unless I am mistaken . Perhaps I should post it as a bug to their forum . BTW i used df2 = pd.merge ( df.set_index ( " DateTimesy " , drop=False ) , fgbmquotef.set_index ( " DateTimesy " , drop=False ) , left_index=True , right_index=True , how = " outer ") . Thanks

Well , one approach is the following : ( 1 ) do a ` groupby / apply ` with ' id ' as grouping variable . ( 2 ) Within the apply , ` resample ` the group to a daily time series . ( 3 ) Then just using ` rolling_sum ` ( and shift so you don't include the current rows ' x ' value ) to compute the sum of your 70 day lookback periods . ( 4 ) Reduce the group back to only the original observations : #CODE
You are going to need your data sorted by `['id','dates ']` . Now we can do the ` groupby / apply ` : #CODE
Thanks , this seems to do it ! If I wanted the 70 to be an argument of the past function ( i.e. def past ( g , lookback )) , how would I then pass that argument to .apply ( past ) ?
It just becomes the next parameter in the ` apply ` . See me edit for details .

the SOQL docs cover all these . There are also tools like SoqlX , Workbench etc that let you run add-hoc queries , these are useful for trying things out without having to run your full integration .

I am playing with the excellent Scikit-learn today . I'm forming the x's out of panels sliced on the minor_axis and y's out of DataFrame sliced on columns . At the moment I'm doing endless iterations , does any .apply() Masters out there have any idea how to speed this up ? #CODE
My idea was to apply this function ( or similar ) column wise . Have played with .apply() but because its a double ( or triple ) function call i.e. f1 . ( ) .f2 ( x , y ) or f1 . ( ) .f2 ( x , y ) .f3 ( x , y ) it gives me an error . Any ideas would be greatly appreciated and I think this would be a very useful bit of code to have out there !
I ran a million regressions and as you can see it took ~ 2.5 minutes . This will vary based on the number of cores you have . ` result ` will be a list of your scores so you can easily reproduce the ` r2 ` ` Series ` in your example . Good Luck !

python pandas : how to use concat() to do a SQL union ?
I understand from the documentation that I can use ` concat() ` to do the equivalent of a SQL UNION .
Let's say I have a table with 3 fields : id , value1 , value2 and I want a new table with 2 columns , where id is repeated , and value2 is appended below value1 .
I end up with a dataframe with 3 , not 2 columns .
I think you have to do it this way , pandas will try to use existing indices and column labels for alignment without renaming you can't infer how the values should align without renaming
You could try the melt function #CODE

You didn't show exactly what the data looks like ( you say it's delimited by semicolons , but your examples don't have any ) , but if it looks like #CODE
It is actually much faster to use the csv lib and str.replace : #CODE
You could just str.split : #CODE

p.s. Background info : The data is generated in an Android app as a the java.util.Calendar class , then converted to a string in Java , written to a csv and then sent to the python server where I read it in using pandas ` read_csv ` .
` datetime.strptime ( x , " %Y%m%d%H%M%S%f ")`
%b : Month as locale s abbreviated name .
Might be worth highlighting that the key difference here is the use of ` %m ` for the month ( Month as a zero-padded decimal number ) instead of ` %b ` ( Month as locale s abbreviated name )
` %b ` is for locale-based month name abbreviations like ` Jan ` , ` Feb ` , etc .

Pandas groupby : compute ( relative ) sizes and save in original dataframe
However , I don't know how to put them back into the old dataframe . I can access them through ` intensity [ 0 ]` , but ` intensity.loc() ` gives a LocIndexer not callable error .
It's a multi-index . You can reset the index by calling ` .reset_index() ` to your resultant dataframe . Or you can disable it when you compute the group-by operation , by specifying ` as_index=False ` to the ` groupby() ` , like : #CODE
If you want to compute ` intensity / mean ( intensity )` , where ` mean ( intensity )` is based only on the ` year ` and not ` year / groupid ` subsets , then you first have to create the ` mean ( intensity )` based on the ` year ` only , like : #CODE
And then compute the ` intensity / mean ( intensity )` for all ` year / groupid ` subset , where the ` mean ( intensity )` is derived only from ` year ` subset : #CODE
Or maybe you want to compute it based on the ` year ` only , not on ` year / groupid ` combination ?
Actually , days later , I found out that the first answer to this double question was wrong . Perhaps someone can elaborate to what ` .size() ` actually does , but this is just in case someone googles this question does not follow my wrong path .
It turned out that ` .size() ` had way less rows than the original object ( also if I used ` reset_index() ` , and however I tried to stack the sizes back into the original object , there were a lot of rows left with ` NaN ` . The following , however , works #CODE

One way would be to use ` shift ` to move the relevant column down ` n ` rows and then concatenate the entries ( they are strings so we can use ` + `) : #CODE
This creates strings of the previous three entries separated by a comma and space ( not lists ) . I'd avoid using lists in DataFrames if possible as things can get a little messy .
What do you want to do with those lists ? Storing lists inside Series / DataFrames is not usually very convenient . Anyway , this would get you close . You have to handle the ` nans ` , and then you're done . #CODE
Notice that we have to convert to a tuple and then a list , to avoid pandas automatically taking our list and making it back into a Series . Try this and you'll see why it doesn't work : #CODE
This solution avoids looping , but I'm not sure whether it really counts as ' vectorized ' , since once you start using ` apply() ` I think you start losing any performance benefits granted by vectorization : #CODE

Memory optimization when selecting from a pandas dataframe
I have a rather large pandas dataframe ( 1.7G ) from which I am selecting some columns to do some computaton ( find maximum value of the three selected columns ) . It seems that this operation is memory intensive . I am trying to find a way to avoid this memory overhead .
For the purpose to this question , I a simplifying the dataframe and using fake data . My code and the memory footprint is shown below , #CODE
I am dealing with a dataPanel that is 1.7G . When I do this selection operation at multiple place in my program , I end up with an overhead that causes my 8G machine to hang and freeze .
you could do that , or work with the data in a HDF file . Make sure your dtypes are correct ( e.g. not object , except for strings ) .
Just apply the function directly - I guess this will take more CPU as it's calculating all the maxes , then just getting the ones you want , but doesn't create a new variable . #CODE

I have a large ( ~160 million rows ) dataframe that I've stored to disk with something like this : #CODE
I have another dataframe containing ~18000 " incidents . " Each incident consists of some ( as few as hundreds , as many as hundreds of thousands ) individual records . I need to collect some simple statistics for each incident and store them in order to collect some aggregate statistics . Currently I do this like so : #CODE
This all works fine except for the fact that each ` store.select() ` statement takes roughly 5 seconds which means that processing the full month's worth of data requires somewhere between 24-30 hours of processing . Meanwhile , the actual statistics I need are relatively simple : #CODE
Would I benefit from using store.select_as_index ? If I receive an index I'd still need to access the data to get the statistics correct ?
what is the relative frequency of c_id and f_id , are they relatively unique or very common , how big is the range that you are selecting each time ( e.g. the timestamp range )
Use a hierarchical query in chunks . What I mean is this . Since you have a relatively small number of ` c_id ` and ` f_id ` that you care about , structure a single query something like this . This is kind of like using ` isin ` . #CODE
The key here is to process so that the ` isin ` DOES not have more that 32 members
If you exceed this , the query will work , but it will drop that variable and do a reindex
So the query scans ' blocks ' of data ( which is what the indexes point to ) . If you have lots of hits across many blocks then the query is slower .

How can I rename with data.rename ( index={ ??? } , columns={ ??? } ) the index respectively columns ? The current one ( Global Data Item ( 000s ) 2012 2011 2010 2009 2008 2007 2006 ) shall be replaced by ( 10 Sub-Data Item 2012 2011 2010 2009 2008 )
I'm not sure that this is your task , but from comments I see that you want to use first row of DataFrame as column names . If this is the case , you can do : #CODE

I have a column ' datedif ' in my dataframe as : #CODE
data['datedif_day '] = data['datedif '] .dt .days
Error : AttributeError : ' Series ' object has no attribute ' dt '
Remove or comment out ``` data.datedif = pd.to_datetime ( data.datedif )``` - then ``` datedif ``` will be a [ ``` Timedelta ``` object ] ( #URL ) .
data['datedif '] = data['datedif '] .astype ( np.numpy64 )

I create a DataFrame ? #CODE
I know DataFrame select a col as default.What happened when I run ` data [ se ]` ?
With DataFrame , slicing inside of [ ] slices the rows . This is provided largely as a convenience since it is such a common operation . #CODE
In your example where you use ` range ( 2 )` that gives you ` [ 0 , 1 ]` as list . What I think you need is ` data [ 0:1 ]` to slice the ` DataFrame ` and get rows 0 and 1 which is the same as ` data [: 1 ]` omitting the zero . If you wanted for example rows 3 , 4 and 5 that would be ` data [ 3:5 ]` .
Thanks.If I want to select these row [ 2 , 4 , 3 , 5 , 1 ] , I run df [ slice ([ 2 , 4 , 3 , 5 , 1 ])] , there is an error.I just used df.ix [ 2 , 4 , 3 , 5 , 1 ] .Is there a different way to do that ?

I have a pandas dataframe that contains height information and I can't seem to figure out how to convert the somewhat unstructured information into an integer .
I figured the best way to approach this was to use regex but the main problem I'm having is that when I attempt to simplify a problem to use regex I usually take the first item in the dataframe ( 7 ' 5.5 '') and try to use regex specifically on it . It seemed impossible for me to put this data in a string because of the quotes . So , I'm really confused on how to approach this problem .
here is my dataframe : #CODE
My next option would be writing this to csv and using excel , but I would prefer to learn how to do it in python / pandas . any help would be greatly appreciated .
How about using string.find() to locate the ' and the " and then cast and then do your conversion ? All that could be done within a function and passed to an Apply
One possible method without using ` regex ` is to write your own function and just ` apply ` it to the column / Series of your choosing .
You could apply that regular expression to the elements in the data . However , the solution of mapping your own function over the data works well . Thought you might want to see how you could approach this using your original idea .

I would like to make a 2-part graphic . On the left , I want to have a visualization ( that I already know how to make ) . On the right , I want to place a table that includes more numerical data about the same subject covered by the graph . Say I set things up with ` axes [ 0 ]` for the visualization and ` axes [ 1 ]` as the place where I want the table .
I'm using Pandas , and I have all the info I'd like for my table in a nice neat ` DataFrame ` . ( For now , let's assume that ` DataFrame ` has a regular ` Index ` on both the rows and the columns , not a ` MultiIndex ` , but I'm curious how answers would change if we dropped that assumption . ) Let's call that ` tabledf ` .
Thanks for the suggestion . I'm moderately familiar with generating tables in LaTeX . My experience is that it's a bit of a pain . It looks like ` pandas.DataFrame ` has a ` to_latex ` method . Seems like that's the next place I should look .
@USER : If you do not like ` tabular ` , you may use any LaTeX package . The output of ` to_latex ` is IIRC for the ` booktab ` package . However , beware that the ` matplotlib ` LaTeX system does not necessarily like line breaks , so you may need to replace ` \n ` by spaces in the string given by ` to_latex ` . IMHO well-designed LaTeX tables look very good . Usually the trick is to reduce the number of lines ( horisontal lines are often unnecessary ) , but this is a matter of taste .

How do I combine two columns within a dataframe in Pandas ?
Say I have two columns , A and B , in my dataframe : #CODE
I'm sure this is a very basic question , but as I am new to Pandas , any help will be appreciated !
You can use ` where ` which is a vectorized if / else : #CODE
you can simplify to this : `df['C '] = df.A.where ( df.B.isnull() , df.B )` as ` isnull ` is available for df and series , also I wouldn't encourage the practice of accessing columns as attributes as it can lead to strange behaviour , better to do this `df['C '] = df['A'].where(df['B '] .isnull() , df['B '])`
You can use ` combine_first ` : #CODE

shift by partition using groupby
I have a dataframe with one column I would like to shift , but over partition rather than the whole dataframe .
For example , I would like to go from this dataframe : #CODE
to this dataframe : #CODE
I believe this gives me what I want for each State , but then I don't know how to combine it back together ( basically just appending each dataframe below one another ) . How can I do that ?
You can store your intermediate DataFrames in a list and use ` pd.concat ` to join them together : #CODE

After using Pandas to read a json object into a ` Pandas.DataFrame ` , we only want to ` print ` the first year in each pandas row . Eg : if we have ` 2013-2014 ( 2015 )` , we want to print ` 2013 `
Why is this happening ? How can we fix the problem ?
Now you try to split the string at the unicode ` \u2031 ` ( EN DASH ) , but the string you give to ` split ` is no unicode string ( therefore the error `'ascii ' codec can't decode byte 0xe2 ` - the EN DASH is no ASCII character ) .

Index contains duplicate entries after drop_duplicates called
I have a pandas dataframe that has duplicate entries and I want to create a ` tsplot ` using ` seaborn ` . I call ` drop_duplicates ` on the dataframe ( and even call ` reset_index() `) yet when I got to do the plot I still get #CODE
Is there a reason why ` drop_duplicates ` wouldn't solve this problem ?
EDIT I've even checked by calling ` duplicated ` on the dataframe after the drop , and all rows show ` False ` . As I would expect .
` drop_duplicates ` does not work on the index , but on the values in the dataframe ! ( so it looks for duplicate rows , not duplicate indices ) . But you also have the same function on the index ( #URL )
` drop_duplicates ` does not work on the index , but on the values in the dataframe ! ( so it looks for duplicate rows , not duplicate indices ) .
With the resulting index , you can reindex .
Another option is to add the index as a column and use ` DataFrame.drop_duplicates ` on that column .
Another option is to use groupby : ` df.groupby ( level=0 ) .first() ` ( and you adapt the ` first ` to what you want to do with the duplicate rows )

Starting from a sample dataframe ` df ` like : #CODE
Is there a way to apply a ` math ` function to a whole column ?
Well ` math.exp ` doesn't understand ` Series ` datatype , use numpy ` np.exp ` which does and is vectorised so operates on the entire column : #CODE

look into ` cubehelix `
Cubehelix is awesome . I read the paper from Dave Green . Exactly what I wanted . Got excellent looking and printing graphs on the first try . If your comment was an answer I would accept it .
Colour-blindness : this page on wikipedia has lots of good info about choosing colours that are distinguishable to most color-blind people . If you notice on the " tips for editors " section , once you take the guidelines into account there are only a few sets of colours available . ( A good rule of thumb is to never mix red and green ! ) You can also use the linked colour-blind simulators to see if your plot would be well visible .
Luminance : most of the journals in my field will publish in B W by default . Even though most people read the papers online , I still like to make sure that the plots can be understood when printed in grayscale . So I take care to use colours that have different luminances . To test , a good way is to just desaturate the image produced , and you'll have a good idea of how it looks when printed in grayscale . In many cases ( particularly line or scatter plots ) , I also use other things than colour to distinguish between sets ( eg . line styles , different markers ) .
In 1.5 matplotlib will ship with 4 new rationally designed color maps :
`'viridis '` ( will be default color map in 2.0 )
The process of designing these color maps is presented in #URL .
I would suggest the ` cubehelix ` color map . It is designed to have correct luminosity ordering in both color and gray-scale
So your requirements are " lots of colors " and " no two colors should map to the same grayscale value when printed " , right ? The second criteria should be met by any " sequential " colormaps ( which increase or decrease monotically in luminance ) . I think out of all the choices in matplotlib , you are left with ` cubehelix ` ( already mentioned ) , ` gnuplot ` , and ` gnuplot2 ` :
The white line is the luminance of each color , so you can see that each color will map to a different grayscale value when printed . The black line is hue , showing they cycle through a variety of colors .
Note that cubehelix is actually a function ( ` from matplotlib._cm import cubehelix `) , and you can adjust the parameters of the helix to produce more widely-varying colors , as shown here . In other words , cubehelix is not a colormap , it's a family of colormaps . Here are 2 variations :
For less wildly-varying colors ( more pleasant for many things , but maybe not for your bar graphs ) , maybe try the ColorBrewer 3-color maps , ` YlOrRd ` , ` PuBuGn ` , ` YlGnBu ` :
I wouldn't recommend using only this color to identify bar graphs , though . You should always use text labels as the primary identifier . Also note that some of these produce white bars that completely blend in with the background , since they are intended for heatmaps , not chart colors : #CODE
Can you add a note to your answer here re the new color maps ? I know I can just edit it myself , but that seem rude .

Delete a group after pandas groupby
Is it possible to delete a group ( by group name ) from a groupby object in pandas ? That is , after performing a groupby , delete a resulting group based on its name .
Seems there's no direct way to delete a group from a groupby object . I think you can filter out those groupby before groupby by #CODE
Maybe I misunderstand what is meant by the variable `` group `` , but you can't index a DataFrame by a GroupBy object .
Filtering a DataFrame groupwise has been discussed . And a future release of pandas may include a more convenient way to do it .
But currently , here is what I believe to be the most succinct way to filter the GroupBy object ` grouped ` by name and return a DataFrame of the remaining groups . #CODE

How to format IPython html display of Pandas dataframe ?
and simalrly for other data types . But IPython does not pick up these fromatting options when displaying dataframes in html . I still need to have #CODE
You can also specify a list of formatters , with ` None ` values for those that are not present - which simplifies the ` frmt ` creation : ` frmt = [ frmt_map.get ( dtype , None ) for dtype in df.dtypes ]` . +1 for the research .
HTML receives a custom string of html data . Nobody forbids you to pass in a style tag with the custom CSS style for the ` .dataframe ` class ( which the ` to_html ` method adds to the table ) .
So the simplest solution would be to just add a style and concatenate it with the output of the ` df.to_html ` : #CODE
But I would suggest to define a custom class for a DataFrame since this will change the style of all the tables in your notebook ( style is " global ") . #CODE
You can also define the style in one of the previous cells , and then just set the ` classes ` parameter of the ` to_html ` method : #CODE
pandas ( as of 0.16.2 ) does not allow overriding the default integer format in an easy way . It is hard coded in ` pandas.core.format.IntArrayFormatter ` ( the ` labmda ` function ): #CODE
` display.float_format ` : The callable should accept a floating point number and return a string with the desired format of the number . This is used in some places like ` SeriesFormatter ` . See ` core.format.EngFormatter ` for an example .

I have a dataframe like the following : #CODE

Efficient matching of two arrays ( how to use KDTree )
I have two 2d arrays , ` obs1 ` and ` obs2 ` . They represent two independent measurement series , and both have dim0 = 2 , and slightly different dim1 , say ` obs1.shape = ( 2 , 250000 )` , and ` obs2.shape = ( 2 , 250050 )` . ` obs1 [ 0 ]` and ` obs2 [ 0 ]` signify time , and ` obs1 [ 1 ]` and ` obs2 [ 1 ]` signify some spatial coordinate . Both arrays are ( more or less ) sorted by time . The times and coordinates should be identical between the two measurement series , but in reality they aren't . Also , not each measurement from ` obs1 ` has a corresponding value in ` obs2 ` and vice-versa . Another problem is that there might be a slight offset in the times .
I'm looking for an efficient algorithm to associate the best matching value from ` obs2 ` to each measurement in ` obs1 ` . Currently , I do it like this : #CODE
I would be very thankful for ideas on how to improve this algorithm speed-wise , e.g. using KDtree or something similar .
resulting_i = i + argmin ( abs ( obs1 [ 1 , i ] - obs2 [ 1 , #URL )` ? Assuming that you are not performing another loop here , I don't believe KDTrees would speed things up too much .
Using ` cKDTree ` for this case would look like : #CODE
where ` indices ` will contain the column indices in ` obs2 ` corresponding to each observation in ` obs1 ` . Note that I had to transpose ` obs1 ` and ` obs2 ` .

In my specific case , I actually have a dataframe , and each method outputs a score . What matters is not the difference in score between the methods and the true scores , but that the methods get the ranking right ( higher score means higher ranking for all columns ) . #CODE
Python has RankEval based module that implements this metric ( and some others if you want to try them ) . The repo is here and there is a nice IPython NB with examples

I am producing some plots in matplotlib and would like to add explanatory text for some of the data . I want to have a string inside my legend as a separate legend item above the ' 0-10 ' item . Does anyone know if there is a possible way to do this ?
`ax.legend(['0-10','10-100','100-500','500+'],loc='best ')`
If there isn't a proper way of doing this the only other option I can think of is to trick the graph into producing it by plotting some empty values
Why not simply set the legends ` title ` ? I.e. `ax.legend(['0-10','10-100','100-500','500 + '] , loc='best ' , title='Explanatory text ')` .
Sure . ` ax.legend() ` has a two argument form that accepts a list of objects ( handles ) and a list of strings ( labels ) . Use a dummy object ( aka a " proxy artist " ) for your extra string . I picked a ` matplotlib.patches.Rectangle ` with no fill and 0 linewdith below , but you could use any supported artist .
For example , let's say you have 4 bar objects ( since you didn't post the code used to generate the graph , I can't reproduce it exactly ) . #CODE
it's actually a really common thing . I bet you've done something like : ` line = ax.plot ( x , y )` . The issue is that ` plot ` returns a * list * of lines , so you need to get at the actual artist . You can either do ` line = ax.plot ( x , y ) [ 0 ]` or do ` line , = ax.plot ( x , y )` which takes advantage of parameter unpacking .

Handling both missing and duplicate datatime fields in a dataframe
Observed Output : Frame contains two fields i.e. Timestamp Active_Power ( in W ) from all 4 CSV files in the source directory aligned side-by-side . So far so good .
How to remove certain rows either being duplicated and / or missing in each csv files rendering dataframe being misaligned in timestamps ?
For example , @USER there is a timestamp value missing and as a result " NaN " is being inserted appropriately . But @USER a timestamp value is being duplicated twice in original source file and therefore removed resulting in " NaN " again , as a result the timestamp sequence got misaligned . How to deal with this issue ?

How can I improve the creation time of a pandas DataFrame ?
I am having a dictionary of pandas ` Series ` , each with their own index and all containing float numbers .
I need to create a pandas ` DataFrame ` with all these series , which works fine by just doing : #CODE
I thought about caching the result but unfortunately the ` dict_of_series ` is almost all the time different .
are the series aligned ( i.e. do they all have the same index ? )

I have a dataframe df1 like this , where starttime and endtime are datetime objects .
StartTime EndTime
If endtime.hour is not the same as startime.hour , I would like to split times like this
StartTime EndTime
Essentially insert a row into the existing dataframe df1 . I have looked at a ton of examples but haven't figured out how to do this . If my question isn't clear please let me know .
which part ( s ) are you stuck on ? converting to datetime and extracting the hour , or inserting a row into a dataframe ? I think you're going to want to write a helper function or two , and you may have to reset the index .

Creating feature ( row ) vectors , for SVM , from Pandas GroupBy function ( and or other methods suggested )
I am trying to create stacked feature vectors for an SVM classifier . I have all my data in a large matrix . The problem at hand is a multi-class classification problem so I need to group using multi-index .
I use the following groupby to get the appropriate groups for my application : #CODE
Now I would like to create feature vectors and store them into a new DataFrame but in a way that only uses two rows to create one feature vector . Meaning that in the test example the ` test1 ` activity is performed twice with each iteration having the same label so in this case it has two labels : 1 and 2 . From each label two rows should be stacked to create the desired output .
I have not written out the whole thing but I hope it is obvious what I would like to achieve . Basically ; two rows become one stacked row vector ( with the label on top ) , the same vector is one feature vector . As I have multiple activities I need multiple feature vectors per activity to train the SVM . For this example I would ideally get one pd.DataFrame with eight feature rows vectors in it so the data frame will have been re-shaped ( ignoring everything but the actual data contained in col_A through col_B ) from ( 16 , 4 ) to ( 8 , 8) .
You need to pass a function to the ` groupby ` which prepares the data for the final output , and then relabel the columns , just like this : #CODE

You mention list but tagged this as pandas , for a series by default calling ` median ` on a series will ignore ` NaN ` values : #URL

How can I convert Sqlalchemy table object to Pandas DataFrame ?
Is it possible to convert retrieved SqlAlchemy table object into Pandas DataFrame or do I need to write a particular function for that aim ?
Have you considered using [ pandas.read_sql ] ( #URL ) ?
pandas.read_sql can use an SqlAlchemy engine .
Partial query results ( NamedTuples ) will also work , but you have to construct the DataFrame ` columns ` and ` index ` to match your query .
just use ` pandas.read_sql ` with an SQLAlchemy engine . it's dead simple .
How do you use ` pandas.read_sql ` on an ORM query , like : ` session.query ( MyORMTable ) .limit ( 100 ) .all() ` ?
`pandas.read_sql_table('MyTable ' , MySQLEngine )` see here #URL

Well , you can avoid the apply and do it vectorized ( I think that makes it a bit nicer ): #CODE
Edit : Jeff points out that a more pandonic way is to make date a ` DatetimeIndex ` and use a Date Offset . So something like : #CODE
This is great.Much faster than ` apply() ` Do you know if it is possible to use ` datetime64 [ M ]` to find the end of the month instead of the start ?
a more pandonic way is to treat as an index and use rollback with the appropriate offset see here : #URL
gr8 u don't actually have make it THE index just set box=False I think on pd.to_datetime then subtract the offset ( but what u r doing is ok too )
[ @USER ] ( #URL ) , got it . I had to add ` pd.DatetimeIndex() ` to get it to work .
@USER , so offsets can only be applied to an ` Index ` ?
Offsets work with Timestamp / Timedelta & DatetimeIndex / PeriodIndex / TimedeltaIndex

How can I sum column values that corrispond to a specific value of another column in a pandas DataFrame ?
I have a python pandas DataFrame with ( item , feature , grade ) #CODE
and I have to put all the sum in a new DataFrame with ( item , sumGrade ): #CODE
How can I do this without using groupby and apply function ? Because I need a good performance in computation .
The normal op here is to ` groupby ` on ' item ' and call ` sum ` on the ' grade ' column no need to call ` apply ` here
You can ` groupby ` on ' item ' column and then call ` sum ` on the ' grade ' column , additionally call ` reset_index ` to restore the ' item ' column back : #CODE
Not sure why you don't want to group but you can also set the index to ' item ' and ` sum ` on the index level : #CODE

My data is in a pandas DataFrame and the x column is merged2 [: -1 ] .lastqu
and the y data column is merged2 [: -1 ] .Units
A snippit of the Dataframe : the [: -1 ] eliminates the current period from the data which will subsequently be a projection #CODE
What's the problem or the question ? From reading the example everything looks correct to me .
The remaining question is how I can get a simple regression line using code " similar to the x= np.array sequence above it does not work ? ( no error , just no line ) While I can get the results from the sm.graphics.plot
plt.plot ( merged2 [: -1 ] .lastqu , merged2 [: -1 ] .Units , ' bo ')
x = np.array ([ merged2 [: -1 ] .lastqu .min() , merged2 [: -1 ] .lastqu .max() ])
plt.plot ( x , y , ' r- ')

Merging Pandas DataFrames ( LEFT join style ) produces strange results
I'm trying to merge two Pandas DataFrames with a many-to-one relationship .
One DataFrame contains a list of drug names and their ingredients ( left side ) together with some extra information ( an irrelevant ID and the company who makes the drugs ) , the other contains a list of ingredients and their numeric IDs ( right side ) .
Obviously , what I like to get is a merged DataFrame where the ID of each ingredient is listed next to the ingredient name and , considering this is a many-to-one scenario , some IDs will get repeated as some drugs use the same ingredient .
A contrived example of both DataFrames ( index omitted for brevity ):
To do the merge I simply do the following : #CODE
The amount of rows is now limited to how many rows there are in the ingredients ( df2 ) DataFrame ( 3354 ) .
The ` info() ` on both DataFrames shows me that both ` ingredient ` columns are of the same type ( just to exclude comparison problems ):
Now for the interesting part . In an attempt to find out where this might be going wrong , I tried to do the merge on only a subset of both DataFrames using ` head() ` : #CODE
If I then perform the merge I get the desired output without a problem , just only 100 rows . If I keep increasing the amount of rows to take for ` head() ` , even up to the point that it is way more than the number of rows present , the output is still perfect . This , for example , works perfectly : #CODE
Notice that I include the first 100.000 rows ( only 53.000 in df1 ) . This too will give me the expected merge output .
Interesting bit : after including a large amount of rows with ` head ( 100000 )` , only the info for the Drugs DataFrame ( df1 ) alters slightly : #CODE
Notice the memory usage went up from 1.2 MB to 2.0 MB . Yet besides that , they both still are DataFrame objects with the same structure as before I pulled them through that operation .
What might be going on here ? Why does the merge produce such strange results , unless I pull the DataFrames through ` head() ` first ?
To summarize : Without using ` head() ` the ` merge() ` output is showing reduced rows ( # rows = # right side rows ) and each ingredient ID is simply " pasted " on there in a serial manner . However , if I do use ` head() ` , even with a great enough number to include all rows on both sides , the join works as expected .
Full data : For the ones wanting to try with the exact same data , I created two gists on Github that contain the full data ( dictionary form ) for both the drugs and the ingredients DataFrame mentioned above .
They are exported using ` DataFrame.to_dict() ` and can be imported / transformed back into a DataFrame using ` DataFrame.from_dict ( data )` .

how to drop dataframe in pandas ?
But I want to drop the whole dataframe created in pandas .
like in R : rm ( dataframe ) or in SQL : drop table
@USER My scenario is the dataset I am working with is huge . So once loaded ( with pd.read_csv() ) the system is getting slow . And after splitting and some processing , I want to remove the bigger dataset and work with the small one.That's why I need .

How do you clean and forward fill a multiple day 1 minute time series with pandas ?
Some of the minutes in the time series are missing :
With pandas , how do I forward fill the series so every minute is present ? I should look like this : #CODE
So I copied your first 4 lines into a dataframe : #CODE
The key thing here is you must have a datetimeindex , if you want to keep it as a column then you can just set ` drop=False ` in ` set_index ` .
Just creating the dataframe : #CODE
Reindexing and filling the holes , then forward filling the resulting NaN values , then dropping all times outside of 9:30 AM to 4:00 PM : #CODE
First , reindex the dataframe so that your index corresponds to your starting date / time through your ending date / time with a frequency of 1 minute : #CODE
This will create a lot of NaN values where the new index didn't line up with the old one . We fill this with ffill ( forward fill ) , though there are other options out there : #CODE
Because .time() doesn't take 9.5 and the documentation is kind of sparse , I just created a datetime object with the time value set to 9:30 AM and then used .time() to grab this . There's a better way , I'm sure .

I am pulling this data from a Mongo database , and cleaning it using pandas ( converting ' price ' to float , ' rank ' to int , string date to datetime , etc . ) . The goal I had in mind was to create a series of additional dataframes / tables ( one for each ' rank ') and link these to the main table in a HDFStore for each day . Something like : #CODE
Before it's cleaned ( and to some extent , after ) the data is quite large . Each day of observation is ~ 1.5M rows , and uses around 9GB of RAM ( I have 32GB limit ) when imported into a pandas DataFrame . The number of rows in the total data set is ~800M .
Any advice or help will be greatly appreciated . I understand this is a broad question , but I am in somewhat unfamiliar territory .

Python pandas dataframe max in a group based on conditions on other columns
I'm not sure if this question has been asked before . In a pandas dataframe I have data like #CODE
Then , groupby the device type , and using the cumulative sum of the ` new_sample ` column , create a counter for which trial of each device each row represents . #CODE

Using Pandas to create DataFrame with Series , resulting in memory error
I'm using Pandas library for remote sensing time series analysis . Eventually I would like to save my DataFrame to csv by using chunk-sizes , but I run into a little issue . My code generates 6 NumPy arrays that I convert to Pandas Series . Each of these Series contains a lot of items #CODE
I would like to add the Series into a Pandas DataFram ( df ) so I can save them chunk by chunk to a csv file . #CODE
Any suggestions ? Is it possible to fill the Pandas DataFrame chunk by chunk ?
Can you make a DataFrame from a single column : pd.DataFrane({'tmax ' : pd.Series ( tmaxSeries ) } ) ?
create a frame with the first series , and add them sequentially , e.g. `` df = DataFrame({'prcp ' : pd.Series ( prcpSeries ) } ); df['tmax '] = pd.Series ( tmaxSeries )`` . You should probably write it to a HDF5 in any event , see : #URL
When you are passing a dict ( even if the values are Series ) , I think copies are made . If you do iteratively ( and argument is a series ) , then no copy
@USER I've cobbled something together ... I think I prefer using an outer concat tbh .
If you know each of these are the same length then you could create the DataFrame directly from the array and then append each column : #CODE
Note : you can also use the ` to_frame ` method ( which allows you to ( optionally ) pass a name - which is useful if the Series doesn't have one ): #CODE
However , if they are variable length then this will lose some data ( any arrays which are longer than ` prcpSeries `) . An alternative here is to create each as a DataFrame and then perform an outer join ( using ` concat ` ): #CODE
Thanks Andy and Jeff ! I've to use the first method with appending each column , since the second approach gets an Memory Error at the line of df = pd.concat ( etc . ) . I know the series with the longest length and will use that one to initialise the DataFrame .
One caveat : I'm using pandas version 0.14.1 and when I try to coerce a Series object to a DataFrame object , if I specify ` columns = [ ' my_column_name ']` in the ` pandas.DataFrame() ` call , the resulting object is an empty DataFrame . When I dropped the columns argument , the resulting DataFrame was as expected .
@USER Thanks for mentioning this , perhaps it's cleaner to use the ` to_frame ` method here ( I'm not sure this was available when I wrote the original answer ) - I've updated this answer to mention that . I will have a look to see if this no longer works in 0.14 + , I will have a check later to see .

Python pandas , how to only plot a DataFrame that actually have the datapoint and leave the gap out
I have a DataFrame with intraday data indexed with DatetimeIndex #CODE
df3.plot()
One way to do this is to ` resample ` ( hourly ) before you plot : #CODE
Pandas Intraday Time Series plots

Pandas : Efficient spatial analysis using adjacency matrix
I have a very large DataFrame with coordinates . Let's take the following example : #CODE
Based on this DataFrame I need to calculate the distance between points various times . Often the points which need to be compared with each other are the same , for example when I want to calculate the distance from Carl to all other Buyers each day . #CODE
To do this efficiently and not having to calculate the same distances multiple times , I am hoping to build an adjacency matrix with the Buyers and their distances to each other . As a result I could query this matrix instead of the doing the distance calculations .
What would you recommend as data structure for this adjacency matrix and how would you implement the lookup so that it is faster than a dedicated distance computation .
I would deeply appreciate any help .
It looks like you want an adjacency matrix for each day . I suggest a dictionary in which keys are dates and values are DataFrames , where each axis lists the customers . Alternatively you could look into the pandas ' Panel objects .
Hi Dan , I am afraid you got me wrong . My idea is rather that I have an adjacency matrix between the Buyers which has in each cell the distance between them . I updated my problem description . I would deeply appreciate to get your opinion about that .

Lets say that I have a dataframe ( using pandas data analysis library ) that looks like so : #CODE
And I want to get the dataframe to look like this : #CODE
Note that `" Unnamed : 1 "` sounds suspiciously like the name ` pd.read_csv ` assigns to the column if the header row lacked a column name . In that case , instead of patching up the result as shown above , you might be able to fix the problem with ` skiprows=1 ` or ` header=1 ` instead . ` skiprows=1 ` would cause ` pd.read_csv ` to skip the first row and thus read the headers ( column names ) from the second row automatically .

I am new to python / pandas , and encountering an issue I can't really make sense of . I have Twitter data in csv files which contain 1000 tweets each . When I read the files into a dataframe , all works fine ( it just takes a lot of time to read the files ): #CODE
This dataframe , which contains around 6 million tweets , can be manipulated at a reasonable speed . However , if it is saved #CODE
even the most simple operations , e.g. finding a string in a tweet ( which works perfectly fine in the initial dataframe ! ) , is extremely slow ( 1-2s per row ) . The problem seems to be particularly distinct when I loop through all rows . Is there any reason for this ? Would it help dumping all data into a database and reading it from there ?
Here is the df.info() of the initial file ( stopped after 50 files ): #CODE
Is there reason you don't just store the dfs in a list and then concat them all ? Also this line is unnecessary : ` dftemp2 = pd.concat ([ dftemp1 [ " source "] , dftemp1 [ " text "] , dftemp1 [ " timestamp_ms "]] , axis=1 )` you can specify the columns of interest in the params to ` read_csv ` , ` usecols ` : #URL
Can you show ` df.info()` before and after writing / reading it to csv ?
@USER added df.info()
@USER Thanks , changed the code . Aside from speeding up the concat process from 2 hours to 10 minutes , it also solved the problem with the loaded csv - it runs at normal speed now ( for whatever reason ) :)
@USER would you like me to post an answer , I can't explain everything but concatenating repeatedly was always going to be slower than building a single list upfront first and then concatenating all at once
I'm pretty sure that repeated concatenations make a copy of both the source and target dataframes each time . This gets big and slow as the target gets big . Concatenating the single list seems to have some deep magic that makes it faster . I suspect it has to do with allocating memory so that there's fewer copies .

I have a bunch of CSV files with 4 line headers . In these files , I want to change the values in the sixth column based on the values in the second column . For example , if the second column , under the name ` PRODUCT ` is ` Banana ` , I would want to change the value in the same row under ` TIME ` to ` 10m ` . If the the product was ` Apple ` I would want the time to be ` 15m ` and so on . #CODE
Assuming that your data is in a Pandas DataFrame and looks something like this : #CODE
The code first loops through the rows of the dataframe column " PRODUCT " , with the row value stored as i and the row-number stored as numi . It then uses if statements to identify the different levels of interest in the Product column . For those rows with the levels of interest ( eg " Banana " or " Apple ") , it uses the row-numbers to change the value of another column in the same row .
There are lots of ways to do this , and depending on the size of your data and the number of levels ( in this case " Products ") you want to change , this isn't necessarily the most efficient way to do this . But since you're a beginner , this will probably be a good basic way of doing it for you to start with .
fyi : you shouldn't do this type of chained assignment , see here : #URL also iterating over a frame is not efficient , this is an easily vectorized problem
You can do this with a combination of ` groupby ` , ` replace ` and a ` dict ` #CODE
`df.groupby('fruits ')` splits the ` DataFrame ` into subsets ( which are ` DataFrame ` s or ` Series ` objects ) using the values of the ` fruits ` column .
The ` apply ` method applies a function to each of the aforementioned subsets and concatenates the result ( if needed ) .
` replacer ` is where the " magic " happens : each group's ` time ` values get replaced ( ` to_replace `) with the new value that's defined in ` time_map ` . The ` get ` method of ` dict ` s allows you to provide a default value if the key you're searching for ( the fruit name in this case ) is not there . ` nan ` is commonly used for this purpose , but here I'm actually just using the time that was already there if there isn't a new one defined for it in the ` time_map ` ` dict ` .
One thing to note is my use of ` g.name ` . This doesn't normally exist as an attribute on ` DataFrame ` s ( you can of course define it yourself if you want to ) , but is there so you can perform computations that may require the group name . In this case that's the " current " fruit you're looking at when you apply your function .

How to sort a dataFrame in python pandas by two or more columns ?
As commented , the ` sort ` method is now deprecated in favor of ` sort_values ` . The arguments ( and results ) remain the same : #CODE
You can use the ascending argument of ` sort ` : #CODE
Sort isn't in place by default ! So you should assign result of the sort method to a variable or add inplace=True to method call .
that is , if you want to reuse df1 as a sorted DataFrame : #CODE
` pd.DataFrame ( randint ( 1 , 5 , ( 10 , 2 )) , columns=['a','b '])` doesn't seem to work .... ` TypeError : randint() takes exactly 3 arguments ( 4 given )`
Sort isn't in place by default ! So you should assign result of the ` sort ` method to a variable or add ` inplace=True ` to method call .
As of pandas 0.17.0 , ` DataFrame.sort() ` is deprecated , and set to be removed in a future version of pandas . The way to sort a dataframe by its values is now is ` DataFrame.sort_values `

I had the same problem with a SSL website only on Linux funny enough -on Windows the same code parsed the tables from the website . After spending some time comparing and updating library versions on Linux with no result , I just added some extra code to handle the SSL certificate before using read_html : #CODE

I need to extract any lines of data whose time is within certain range , say : 09:00 : 00 to 09:15 : 00 . My current solution is simply reading in each data file to a data frame , sorting it in order by time and then using searchsorted to find 09:00 : 00 to 09:15 : 00 . It works fine if performance isn't an issue and I don't have 1000 files waiting to be processed . Any suggestions on how to boost the speed ? Thanks for help in advance !!!
How big are we talking here ? you could read all the csv's into a list of df's , then ` concat ` them all set the index to be the time and then filter using ` df.loc [( df.index.time > ' 09:00 : 00 ') & ( df.index.time < ' 09:15 : 00 ')]` , whilst loading them you could sort on index and chuck away the ones that don't have that range anyway

I have a dataframe that includes non-unique time stamps , and I'd like to group them by time windows . The basic logic would be -
It feels like a df.groupby ( pd.TimeGrouper ( minutes=n )) is the right answer , but I don't know how to have the TimeGrouper create dynamic time ranges when it sees events that are within a time buffer .
For instance , if I try a TimeGrouper('20s ') against a set of events : 10:34 : 00 , 10:34 : 08 , 10:34 : 08 , 10:34 : 15 , 10:34 : 28 and 10:34 : 54 , then pandas will give me three groups ( events falling between 10:34 : 00 - 10:34 : 20 , 10:34 : 20 - 10:34 : 40 , and 10:34 : 40-10 : 35:00 ) . I would like to just get two groups back , 10:34 : 00 - 10:34 : 28 , since there is no more than a 20 second gap between events in that time range , and a second group that is 10:34 : 54 .
Given a Series that looks something like - #CODE
If I do a df.groupby(pd.TimeGrouper('20s ')) on that Series , I would get back 5 group , 10:34 : 00- : 20 , : 20- : 40 , : 40-10 : 35:00 , etc . What I want to do is have some function that creates elastic timeranges .. as long as events are within 20 seconds , expand the timerange . So I expect to get back - #CODE
You might want consider using apply : #CODE
It's up to you to implement just any grouping logic in your grouper function . Btw , merging overlapping time ranges is kind of iterative task : for example , A = ( 0 , 10 ) , B = ( 20 , 30 ) , C = ( 10 , 20 ) . After C appears , all three , A , B and C should be merged .
Muzhig , thanks for the response ! I'm not sure how the logic plays out in the my_grouper function though . Can you show me an example of what my_grouper would look like if you were just finding overlapping tuples ( as your A , B , and C are in your post ) ?
Muzhig , I appreciate the example !
create a column ` tsdiff ` that has the diffs between consecutive times ( using ` shift `)
`df['new_group '] = df.tsdiff timedelta `
` fillna ` on the ` new_group `
` groupby ` that column
This is how to use to create a custom grouper . ( requires pandas > = 0.13 ) for the timedelta computations , but otherwise would work in other versions .
Create your series #CODE
Arbitrariy assign things 20s to group 0 , else to group 1 . This could also be more arbitrary . if the diff from previous is 0 BUT the total diff ( from first ) is > 50 make in group 2 . #CODE
Groupem ( can also use an apply here ) #CODE
Jeff , I definitely like where you're going with this . I'm not sure how to take that out to scale though . What if you add in two more events at 2013-01-01 10:34 : 55 and 10:35 : 12 . You'd end up with the indexer dataframe having two more lines :
The indexer could be as big as the original series if you want . I'll update the example to do what I think you want .
This is just an example , I am not sure what you want . You could easily have a function do this if you want . Just create the grouper like you want .
What I would like to do is to create a window id for each time range that is created by identifying those overlapping events . In your example ( grouper dataframe ) , you end up with 10:34 : 54 as a different number than 10:34 : 55 and 10:35 : 12 . Maybe I'm framing the question wrong by focusing on pandas , and this is really a question of " what's the best way to create elastic ranges of overlapping events in python " . I had hoped pandas had a sort of built in TimeSeries manipulation function in here already .

I need to create res a new dataframe such as for each Media modality Budget has a column e.g. : #CODE
a starting point might be ` pd.crosstab ( dd.Budget , dd.Media )`
Get the crosstab on `dd['Budget '] , dd['Media ']` #CODE
And , then merge on ` dd ` and fill `NaN's ` with ` 0 ` #CODE

Why doesn't this function " take " after I iterrows over a pandas DataFrame ?
I have a DataFrame with timestamped temperature and wind speed values , and a function to convert those into a " wind chill . " I'm using iterrows to run the function on each row , and hoping to get a DataFrame out with a nifty " Wind Chill " column .
However , while it seems to work as it's going through , and has actually " worked " at least once , I can't seem to replicate it consistently . I feel like it's something I'm missing about the structure of DataFrames , in general , but I'm hoping someone can help . #CODE
But , when I look at the DataFrame again , the NaN's are still there : #CODE
for the whole DataFrame at once using your simple ` windchill ` function .
You can use ` apply ` to do this : #CODE
Bu they don't update to the DataFrame : #CODE
@USER are you saying the above worked on newer or older pandas ? There are a few edge cases in pandas ' apply which have been tweaked over last few releases so this could be one of them !
@USER , yes , I was using an older version of Anaconda which had a version 11 , I believe , edition of Pandas , and was using iterrows , and the way I had it coded worked fine , with references to updates in the original data frame via row . But that didn't work ( apparently row referenced a copy , not the original ) when I tried on two later versions . Fixed it in my code with direct .loc references to the original dataframe .

Selecting max within partition for pandas dataframe
I have a pandas dataframe . My goal is to select only those rows where column C has the largest value within group B . For example , when B is " one " the maximum value of C is 311 , so I would like the row where C = 311 and B = " one . " #CODE
Applying ` Paul H ` solution to your problem yields : `df2.groupby('B ') .apply ( lambda k : k[k['C '] == k['C '] .max() ])`
You can use ` idxmax() ` , which returns the indices of the max values : #CODE

Python : Inserting a Row into a Data Frame
Is there a more pythonic way to insert a row into a data frame ? I feel like this has to be a functionality of pandas but can not find it . Especially , is there a way to ' reset ' the indices ?
DataFrames have a ` reset_index ` method . So there's that .
Use the ` loc ` attribute to assign data . Syntax is ` df.loc [ row_index , col_index ]` . An example : #CODE
So , you're saying to just create a brand new row , and then re-index the rows to where you want the new row inserted .
I like it . Still hoping there's a built-in way to insert a row in a desired location .
I understand you as to what .loc does . I don't think I'm being clear on what i mean by ' insert ' row . I don't mean append a row . I mean insert a row at a certain location . So , if i wanted to insert row ' e ' between rows ' b ' and ' c ' , I would first df.loc['e','E '] then df.reindex('a b e c d ' .split() )
If your current function works well enough for you , I suggest just adding ` reset_index ` to the returned result . See something like below : #CODE
Thank you for the reset_index option . That certainly improves my code .

Multi-index pivoting in Pandas
Consider the following dataframe : #CODE
I would like to pivot it to get the table arranged as : #CODE
If I understand what you are asking I think what you want is ` pandas.pivot_table ( ... )` which you can use like so : #CODE
produces #CODE
Wow . OK , I just realized that ` pivot ` and ` pivot_table ` are two different methods .
Yes -- Thanks , I was reading about both methods and testing it locally .

Find lowest value of previous 3 days in pandas DataFrame
I am trying to find the lowest value of previous 3 days in a time series .
I tried .shift() , .min() etc . none of them works .
your help is greatly appreciated !
Wonderful ! quick answer and thanks for fix the data format in my question . ( before I fixed it :) )
More generally there are a number of rolling-style functions to handle common cases and a ` rolling_apply ` for user functions . Many libraries / packages have these sorts of functions which you can usually find by searching for " moving " or " rolling " .

Get pandas.read_csv to read empty values as empty string instead of nan
It correctly reads " nan " as the string " nan ' , but still reads the empty cells as NaN . I tried passing in ` str ` in the ` converters ` argument to read_csv ( with `converters={'One ' : str} )`) , but it still reads the empty cells as NaN .
I realize I can fill the values after reading , with fillna , but is there really no way to tell pandas that an empty cell in a particular CSV column should be read as an empty string instead of NaN ?
Use the fillna method , but use it twice ' nan ' = ' nan ' , ' NaN ' = "" . This would keep comma's lined up .
I don't understand your answer . As I said , I don't want to use fillna or any other method call after the reading . I'm asking if there's a way to make the conversion take place during the CSV reading operation .
In the meantime , ` result.fillna ( '')` should do what you want
[ Documentation for ` DataFrame.fillna ` . ] ( #URL ) Try ` result.fillna ( '' , inplace=True )` . Otherwise it creates a copy of the dataframe .
sorry to resurrect such an old answer , but did this ever happen ? As far as I can tell from [ this GitHub PR ] ( #URL ) it was closed without ever being merged , and I'm not seeing the requested behavior in pandas version 0.14.x
[ Documentation ] ( #URL ) for read_csv now offers both ` na_values ` ( list or dict indexed by columns ) and ` keep_default_na ` ( bool ) . The ` keep_default_na ` value indicates whether pandas ' default NA values should be replaced or appended to . The OP's code doesn't work currently just because it's missing this flag . For this example , you could use `pandas.read_csv('test.csv',na_values=['nan '] , keep_default_na=False )` .

Pandas : The truth value of a Series is ambiguous
Oh no ! Iterating over just ` readdata ` gives me the column names . Well that's clearly not what you want . Poking around shows me that there's a ` .iterrows() ` method on ` readdata ` , so let's try that : #CODE
Now it's time to dive into the docs to figure out more about how to format pandas series objects to get the output the way you want . As a hint , using this ( after putting ` import sys ` at the top of the script ) instead of ` print line ` might be what you want : #CODE

I am trying to get the Adj Close prices from Yahoo Finance into a DataFrame . I have all the stocks I want but I am not able to sort on date . #CODE
Date is your index , so dataFrame.index , this will get you the date column
you can do ` dataFrame [ " Date "] = dataframe.index ` if you want to add a column called ' Date ' with the index values in it
Use dataFrame.index to directly access date or to add an explicit column , use dataFrame [ " columnName "] = dataframe.index
` f ` is a ` Panel `
You can get a ` DataFrame ` and reset index ( Date ) using : #CODE
but I'm not sure ` reset_index() ` is very useful as you can get Date using #CODE

Pandas rolling_mean with center keyword leaves last values as NaN
I noticed behaviour in pandas rolling_mean function I did not expect .
I would expect the last value to be 3.5 as in ( 3+4 ) / 2 which would be consistent with the first value in the series .
Is this a bug or a feature ? I am aware that " financial " rolling means and " scientific " rolling means are different but I think this should produce a series without NaN values .

python pandas transforming dataframe
The function .stack() almost gets it done but in the wrong format .
close but .reset_index ( level=0 ) gives an error ValueError('cannot insert %s , already exists ' % item )
You could also use Pandas ` get_dummies() ` #CODE
Its a bit obscure in one line . ` df.unstack() .dropna() ` basically flattens your DataFrame to a series and drops al NaN's . The ` get_dummies ` gives a table of all the occurrences of the letters , but for each level in the unstack DataFrame . The grouping and sum then combine the index to the original shape .
Beautiful . Much better than my ( now-deleted ) attempt to make get_dummies work on a DataFrame . Really like the `` unstack() .dropna() `` idiom .
I noticed , you could have left it , no harm in having some options . I still agree with your earlier comment that its less pretty ( and readable ) than the ` pivot ` solution from Roman . The concept of pivot is also better known , then ... ` get_dummies ` . :)

Pandas Dataseries get values by level
I am dealing with pandas series like the following
x =p d.Series ([ 1 , 2 , 1 , 4 , 2 , 6 , 7 , 8 , 1 , 1 ] , index=['a ' , ' b ' , ' a ' , ' c ' , ' b ' , ' d ' , ' e ' , ' f ' , ' g ' , ' g '])
The indices are non unique , but will always map to the same value , for example ' a ' always corresponds to ' 1 ' in my sample , b always maps to ' 2 ' etc . So if I want to see which values correspond to each index value I simply need to write #CODE
x=pd.Series(['1 ' , ' 2 ' , ' 1 ' , ' 4 ' , ' 2 ' , ' 6 ' , ' 7 ' , ' 8 ' , ' 1 ' , ' 1 '] , index=['a ' , ' b ' , ' a ' , ' c ' , ' b ' , ' d ' , ' e ' , ' f ' , ' g ' , ' g '])
So long as your indices map directly to the values then you can simply call ` drop_duplicates ` : #CODE
This works as long as any other index is not mapped to XX , I modified my question to reflect this
` pandas.Series.values ` are numpy ` ndarray ` s . Perhaps doing a ` values.astype ( int )` would solve your problem ?

pandas , dataframe , groupby , std
New to pandas here . A ( trivial ) problem : hosts , operations , execution times . I want to group by host , then by host+operation , calculate std deviation for execution time per host , then by host+operation pair . Seems simple ?
how do I calculate std deviation on ` dataframe.groupby ([ several columns ])` ?
It's important to know your version of Pandas / Python . Looks like this exception could arise in Pandas version 0.10 ( see ValueError : Buffer dtype mismatch , expected float64_t but got float ) . To avoid this , you can cast your ` float ` columns to ` float64 ` : #CODE
As far as it goes , it looks like ` std() ` is calling ` aggregation() ` on the ` groupby ` result , and a subtle bug ( see here - Python Pandas : Using Aggregate vs Apply to define new columns ) . To avoid this , you can use ` apply() ` : #CODE
`byhostandop['time '] .std() ` again raises the same exception .
just out of curiosuty , have you tried `byhostandop['time '] .apply ( lambda x : x.std() )` ?
`byhostandop['time '] .apply ( lambda x : x.std() )` WORKS . Thank you !
In ` df.astype ` reply , did you mean that I should explicitly convert column type ? Like this works : `df['time '] = df['time'].astype('float64 ') ; byhostandop=df.groupby(['host ' , ' operation ']) ; byhostandop['time '] .std() ` . But I'm not sure if this is idiomatic pandas operation or I would better do smth else to get correct ( float64 ) type in column for std dev calculation .
The error ``` ValueError : Buffer dtype mismatch , expected ' float64_t ' but got ' float '``` for ` np.Float32 ` data persists in pandas 0.13.0 . The temporary solution : ``` grouped = df.astype ( np.float64 ) .groupby ( ... )``` ( assuming all the data is float )

I'm writing some data analysis pipelines in pandas . One of the columns in the dataframes that I've been working with is made up of objects of custom-written classes that are each initialized with a string , from which I read off various information with regular expressions and store in the object's attributes . The subclass structure is similar to how one might implement a tree of life ( e.g. a Tiger is a sublass of Cat which is a subclass of Animal and frequently -- but not always -- animals with the same superclass will share methods ) . It also has some useful methods that I can use for calculations . For str and repr methods return the string that was used to initialize it , like so : #CODE
This means that when I want to view my data frame , I see a string . I have had no problems using the to_csv() method on data frames that have these objects in them , but when I use the to_excel() method of the pandas data frame , I get the following error : #CODE
Now , I could go modify the offending code , but I don't want to have to deal with making that play nicely with updates to xlsxwriter . I could simply make my class inherit from str , but that seems like unpythonic given that I don't really want to use almost any of the string methods . Lastly , I could sanitize my dataframe by taking everything in it that's of this subclass and turning it back into a string , but that would mean I have to rewrite a lot of things that I use that depend on being able to use the DataFrame.to_excel method . Is there anything I can do within the class that saves me from having to inherit everything from str ?

Trying to wrap my head around pandas dataframe ( with no looping )
Use ` where ` to conditionally assign a value where true and return a different value if false : #CODE
Just return that using ` np.where ` : `np.where(df['x '] > 5 , ' Bigger ' , ' Smaller ')`

My script uses ` pandas.read_csv ` to directly read read csv files into data frames . A user is supposed to provide a configuration file that contains labels of columns to retain along with corresponding data types . Let me illustrate with an example . Here is a csv . #CODE
This fails , because ` pandas.read_csv ` requires a file path or a FileIO stream , not a generator .
Then I attempted to use ` DataFrame.from_records ` that can read generators , but doesn't have the ` dtype ` argument of ` pandas.read_csv ` . Finally I ended up mimicking a file with my own class #CODE
I know that I can read in all lines , process them and pass to ` pandas.DataFrame ` , but I don't want to use any intermediate Python containers , because the files are huge and I don't want to run into memory errors .
Then I use read ` StringIO ` by chunks using parameter ` chunksize=1024 ` and ` iterator=True ` and concatenate object to dataframe ` df ` . Source , more info . #CODE
That was supposed to be ` dtype ` not ` dtypes ` , my bad , sorry . As for your solution , I've clearly stated ( in the ** Note ** section ) that the lines must be lazily generated . In your case , you preprocess all lines in memory and then pass them to ` StringIO `
You still store the entire file here ` lower_lines = u "" .join ([ line.lower() for line in csv ])` :)
` DataFrame.append ` is highly time and memory inefficient as it creates a new data frame . Memory-wise it might be even more inefficient than your original list-comprehension based approach , because at some point of time you might end with N copies of a data frame , because garbage collection is not happening all the time .
In your new approach you still hit the same things I've already pointed out . ` str.join ` stores the whole file in memory ( even twice in fact , because it builds a temporary array out of an iterable and then builds a string out of the array ) , ` file.read ` reads the entire file into memory by definition . ` df.concat ` is inefficient for the same reason ` df.append ` is .

How to fill in a lot of data utlizing pandas fillna fast ?
I have two Dataframes one large one with a lot of missing values and a second one with data to fill the missing data in the first one .
Dataframe examples : #CODE
You can use try ` groupby ` - each group is transposing with ` T ` and then ` fillna ` ` df ` by values of ` df2 ` : #CODE
Maybe if ` df = df.fillna ( df2 )` doesn't work , can be use ` df = df.combine_first ( df2 )` . It depends on index .
How does it work ? I cannot compare it with your solution , because error : ` NameError : global name ' toappend ' is not defined `
Thanks for your answer jezrael ! I fixed the cause of the error , my bad sorry . I have quickly checked , and it works when utilizing ` df2 = df2.groupby ( df2.index ) .apply ( lambda x : x.B.reset_index ( drop=True ) .T ) ` instead of ` df2 = df2.groupby('A ') .apply ( lambda x : x.B.reset_index ( drop=True ) .T )` . Need to test on the large dataset when I get home

But I get the following warning and im not sure how to fix my code to comply ? #CODE
What version of pandas ? I don't see this on 0.17 , so perhaps this warnings been fixed ( I think it was being a little oversensitive here . )

