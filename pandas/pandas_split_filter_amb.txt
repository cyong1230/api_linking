Rolling median in python
I need to be able to insert these values into a python list and get a median for the last 30 closes .
It would be more efficient to do the median of the last 29 values .
isn't the median just the middle value in a sorted range ?
For an list with an even number of values the median is the mean of the two middle values .
so you could grab daily close prices , their rolling means , etc ... then timestamp every calculation , and get all this stored in something similar to a python dictionary ( see the ` pandas.DataFrame ` class ) ... then you access slices of the data as simply as : #CODE
See the pandas rolling moments doc for more information on to calculate the rolling stdev ( it's a one-liner ) .
I was was thinking of using a lambda function in the apply method of DataMatrix but I'm having some challenges ...
@USER Pennington : couponded is an array in this case and has no shift method ...
@USER Pennington : Done , sorry , new to Stack :)
b ) strip the seconds out of python datetime objects ( Set seconds to 00 , without changing minutes ) .
You have a number of options using pandas , but you have to make a decision about how it makes sense to align the data given that they don't occur at the same instants .
The ` reindex ` function enables you to align data while filling forward values ( getting the " as of " value ): #CODE
I tested with apply , it seems that when there are many sub groups , it's very slow . the groups attribute of grouped is a dict , you can choice index directly from it : #CODE
append two data frame with pandas
I try to merge dataframes by rows doing : #CODE
Also post what keywords you used when you performed the merge .
The ` append ` function has an optional argument ` ignore_index ` which you should use here to join the records together , since the index isn't meaningful for your application .
Just as a small addition , you can also do an apply if you have a complex function that you apply to a single column :
probably x is a confusing name for the column name and the row variable , though I agree apply is easiest way to do it :)
just to add , ` apply ` can also be applied to multiple columns :
Can apply take in a function defined elsewhere in code ?
Is it possible create a matching panel that only has these columns and then somehow merge the two ?
Here's the 2nd panel I would like to append : #CODE
Could you have a look at the new concat function in pandas 0.7.0 and see if it meets your needs :
I recently spent a great deal of time on the join and concatenation methods .
I've moved on from this problem ( I hacked together a loop ) but looking at your concat documentation this does appear to solve the issue .
I realize the map function can only output lists .
How to apply slicing on pandas Series of strings
I'm playing with pandas and trying to apply string slicing on a Series of strings object .
I got it to work by using the map function instead , but I think I'm missing something about how it's supposed to work .
` apply ` first tries to apply the function to the whole series .
` apply `' s source code for reference : #CODE
also , how can I then reindex things so that the first indices are the column labels of each file and the second is the filename ?
Suppose there are many columns and you only want the ` any ` to apply to a subset of them ( you know the subset's labels ) .
There are at least a few approaches to shortening the syntax for this in Pandas , until it gets a full query API down the road ( perhaps I'll try to join the github project and do this is time permits and if no one else already has started ) .
Expanding the mini domain-specific language I made above for expressing logicals to have this option with simple syntax will probably be an uncomfortable chore .
Is there a way to perform inner and outer joins in ` data.table ` without resorting to ` merge ( X , Y , all=FALSE )` and ` merge ( X , Y , all=TRUE )` ?
And I think most of the pandas merge code is in Cython .
In this case ( database joins ) pandas ' DataFrame contains no pre-computed information that is being used for the merge , so to speak it's a " cold " merge .
If I had stored the factorized versions of the join keys , the join would be significantly faster - as factorizing is the biggest bottleneck for this algorithm .
Of course , now that you've figured it all out in python , it should be easy to translate into R ;)
This isn't really the join itself ( the algorithm ) , but a preliminary step .
Some benchmark results are already reported by ` test.data.table() ` but that code isn't hooked up yet to replace the levels to levels match .
Also , ` data.table ` has time series merge in mind .
Two aspects to that : i ) multi column ordered keys such as ( id , datetime ) ii ) fast prevailing join ( ` roll=TRUE `) a.k.a. last observation carried forward .
` data.table ` has time series merge in mind .
join ( ` roll=TRUE `) a.k.a. last observation carried forward .
So the Pandas equi join of two character columns is probably still faster than data.table .
I actually have not yet optimized the code for the integer join key case ( put that on my todo list ! ) , but you can expect significantly better performance than the string case given the hash table study in the linked presentation .
Here's some Rprof results #URL It looks like 20-40 % of the time is spent in sortedmatch depending on the join type .
The graph depicted there shows how different tools and packages compare in terms of aggregation and join speed .
I hope someone does a join benchmark soon too !
You can also use panels to help you do this pivot .
I am trying to do a pivot table of frequency counts using Panda .
Just replace ` rows =[ ' Y ']` with ` rows =[ ' X2 ']` #CODE
It doesn't have the GUI tools of Enthought but otherwise contains a full scientific python stack .
Debug build of Python ( python-dbg ) in tandem with gdb allow you right away debug your extensions while inspecting Python stack etc .
I would also be interested in aligning my irregular timestamp intervals to one second resolution , I would still wish to plot multiple events for a given second , but maybe I could introduce a unique index , then align my prices to it ?
I will join pystatsmodels -- if you are looking for stumbling noobs with use cases , I could be fertile territory .
I have a SAS background and was thinking it'd replace proc freq -- it looks like it'll scale to what I may want to do in the future .
However , I just can't seem to get my head around a simple task ( I'm not sure if I'm supposed to look at ` pivot / crosstab / indexing ` - whether I should have a ` Panel ` or ` DataFrames ` etc ... ) .
I can't work out whether I should be using ` pivot / crosstab / groupby / an index `
Then , using the ability to apply multiple aggregation functions following a groupby , you can say : #CODE
I think I will continue converting the dataframe after loading with the apply method .
Reason I put table=True is that I want to * append * to existing tables for very large data sets .
So it seems the combination of append and mixed-type is still on the todo-list ?
suspect with some unix magic you can transform a FWF file into a CSV
Pandas rolling median for duplicate time series data
I am wondering if there is a good way to apply rolling window means to a dataset with duplicate times by a multi-index tag / column
What I want to do is build and graph rolling means with varying ms windows , by event and event+tag .
join or merge with overwrite in pandas
I want to perform a join / merge / append operation on a dataframe with datetime index .
Try the ` truncate ` method : #CODE
I'm on #URL and I haven't found the truncate function .
@USER : here's the link to the description of truncate in the current documentation ( v0.7.2 ): #URL
See my answer below -- if someone would contribute some docs about truncate that would be helpful .
I am trying to insert a pandas ( pandas.pydata.org ) DataFrame into a Postgresql DB ( 9.1 ) in the most efficient way ( using Python 2.7 ) .
You can also perform aggregation on individual columns , in which case the aggregate function works on a Series object .
it uses numpys " argmax " function to find the rowindex in which the maximum appears .
i tested the speed on a dataframe with 24735 rows , grouped into 16 groups ( btw : dataset from planethunter.org ) and got 12.5 ms ( argmax ) vs 17.5 ms ( sort ) as a result of %timeit .
If the number of " obj_id " s is very high you'll want to sort the entire dataframe and then drop duplicates to get the last element .
This should be faster ( sorry I didn't test it ) because you don't have to do a custom agg function , which is slow when there is a large number of keys .
However mine uses the apply function of a dataframe instead of the aggregate .
Yes , just use truncate .
I have been using the scikits.statsmodels OLS predict function to forecast fitted data but would now like to shift to using Pandas .
I have tried using ` groupby ` and ` agg ` but to no avail .
and apply agg() with it : #CODE
I want to make a pivot_table on the dataframe using counting aggregate per month for each location .
to pivot the values .
It wants to apply the to strings , not series object .
I think you received a KeyError because `` df `` was indexed before joining , thus ' first ' was no longer a column to join " on " .
To put data in a DataFrame indexed by that , you should add the columns and reindex them to the date range above using ` method= ' ffill '` : #CODE
I am trying do use a pandas multiindex to select a partial slice at the top level index ( date ) , and apply a list to the second level index ( stock symbol ) .
which DOES work when passed to ix .
How to resample a dataframe with different functions applied to each column ?
You can also downsample using the ` asof ` method of ` pandas.DateRange ` objects .
@ Wes McKinney this should be ` resample ` in 0.8 , isn't it ?
Pandas : simple ' join ' not working ?
I can achieve the desired results using ' merge ' .
But I eventually need to join multiple ` pandas ` ` DataFrames ` so I need to get this method working .
Try using ` merge ` ( #URL ): #CODE
So it looks like in order to get what I want I'll have to perform successive merges , since ` merge ` only take two DataFrames ?
From the 0.16.2 docs : The related DataFrame.join method , uses merge internally for the index-on-index and index-on-column ( s ) joins , but joins on indexes by default rather than trying to join on common columns ( the default behavior for merge ) .
Best way to insert a new value
My question is , how can I group / transform the data in such a way that I have a MultiIndex with ( Z , A ) as indexes ( or MultiIndexes ) having into account that the data is not unique ?
Now use this as an auxiliary index variable and unstack : #CODE
Is there a way to index ` series ` by the mapping of result / frequency defined by ` freq ` ?
Yes , use the ` map ` Series method : #CODE
Pandas : trouble understading how merge works
I'm doing something wrong with merge and I can't understand what it is .
If I print ` hist ` and ` freq ` this is what I get : #CODE
They're both indexed by `" series "` but if I try to merge : #CODE
on : Columns ( names ) to join on .
are False , the intersection of the columns in the DataFrames will be
inferred to be the join keys
Alternatively and more simply , ` DataFrame ` has ` join ` method which does exactly what you want : #CODE
Time to improve the merge docstring !
I would like to use pandas to create a HLOC chart of data for every one minute starting with time zero being 9:46 using the asof method ....
Is there a add to table method .... thinking ..... take new data , process ( ts.convert ) . append table .. numpy add to array . any help here .
You can append data ( yielding a new object ) with df.append ( new_data ) but that's not especially efficient
" Parameters ---------- key : object Some label contained in the index , or partially in a MultiIndex axis : int , default 0 Axis to retrieve cross-section on copy : boolean , default True Whether to make a copy of the data "
Color each alternative cell with a specific color ( like a chess board : instead of black / white I will use some other color combination ) and insert value for each cell from a pandas data frame or python dictionary .
You can either truncate the data , or add an extra column .
This function was updated to the name ` idxmax ` in the Pandas API , though as of Pandas 0.16 , ` argmax ` still exists and performs the same function ( though appears to run more slowly than ` idxmax `) .
Previously ( as noted in the comments ) it appeared that ` argmax ` would exist as a separate function which provided the integer position within the index of the row location of the maximum element .
In general , I think the move to ` idxmax ` -like behavior for all three of the approaches ( ` argmax ` , which still exists , ` idxmax ` , and ` numpy.argmax `) is a bad thing , since it is very common to require the positional integer location of a maximum , perhaps even more common than desiring the label of that positional location within some index , especially in applications where duplicate row labels are common .
So here a naive use of ` idxmax ` is not sufficient , whereas the old form of ` argmax ` would correctly provide the positional location of the max row ( in this case , position 9 ) .
So you're left with hoping that your unit tests covered everything ( they didn't , or more likely no one wrote any tests ) -- otherwise ( most likely ) you're just left waiting to see if you happen to smack into this error at runtime , in which case you probably have to go drop many hours worth of work from the database you were outputting results to , bang your head against the wall in IPython trying to manually reproduce the problem , finally figuring out that it's because ` idxmax ` can only report the label of the max row , and then being disappointed that no standard function automatically gets the positions of the max row for you , writing a buggy implementation yourself , editing the code , and praying you don't run into the problem again .
Per #URL argmax is now idxmax .
Based on the second-to-last comment there , it looks like ` argmin ` and ` argmax ` will remain part of ` DataFrame ` and the difference is just whether you want the index or the label .
` argmax ` will give you the index integer itself .
Note that you need to be careful trying to use the output of ` idxmax ` as a feeder into ` ix ` or ` loc ` as a means to sub-slice the data and / or to obtain the positional location of the max-row .
Another way I did something similar was create a pivot table
In this case I want to convert this pivot table to 2d numpy array .
AttributeError : Cannot access callable attribute ' reset_index ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method
Aggregating functions are ones that reduce the dimension of the returned objects , for example : ` mean ` , ` sum ` , ` size ` , ` count ` , ` std ` , ` var ` , ` sem ` , ` describe ` , ` first ` , ` last ` , ` nth ` , ` min ` , ` max ` .
if you really need to replace the version provided by the system , uninstall it .
The ` reindex ` method can accomplish this when passed a reordered array of tuples matching the desired order .
let me try to answer this . basically i will pad or reindex with complete weekdays and sample every 5 days while drop missing data due to holiday or suspension
There may be cleaner way to perform the next step , but the goal is to change the index from an array of tuples to a MultiIndex object .
The final step is to " unstack " weekday from the
MultiIndex , creating columns for each weekday , and replace the weekday numbers with an abbreviation , to improve readability .
To create a line plot for each week , transpose the dataframe , so the columns are week numbers and rows are weekdays ( note this step can be avoided by unstacking week number , in place of weekday , in the previous step ) , and call ` plot ` .
I think the same concepts apply to an index of floats .
I can reindex but how do i deal with ` NaN ` ?
It would be nice to have a convenience function for this where you can pick the axes to interpolate over
Could also use DataFrame's interpolate method ?
but it would work if the csv file would be somehow transpose .
Obviously you may need to clean you data after import , e.g. you chould check for data types , remove empty fields ( or replace with None ) This version processes the entire dataset , but only returns one line , so you could use break at that point or perhaps append other interesting data .
I'm trying to align my index values between multiple DataFrames or Series and I'm using
Series.interpolate but it doesn't seem to interpolate correctly .
I don't think underlying mathematics apply that sum of interpolation equal to interpolation of sum .
It assumes they are equally spaced and just uses ` len ( serie )` for indexes .
I modified the ` Series.interpolate ` method and came up with this ` interpolate ` function .
I don't understand why the join has created a tuple .
When I export the csv -- it gives back the * original * data set df1 ( & vice versa for if df1 and df2 are swapped in the align command ) .
Is there a reason one shouldn't use align for this task ?
Using join works for what I needed .
I'm still curious about the align ...
@USER Align , I would imagine , simply arranges the data .
` align ` returns aligned versions of the left and right DataFrames ( as a tuple ):
Not sure whats the way to append to current data frame in pandas or is there a way for pandas to suck a list of files into a data frame .
Once you have read the files and save it in two dataframes , you could merge the two dataframes or add additional columns to one of the two dataframes ( assuming common index ) .
Why not use read_csv , to build two ( or more ) dataframes , then use join to put them together ?
The pandas ` concat ` command is your friend here .
` dr = pd.date_range ( dt ( 2009 , 1 , 1 ) , dt ( 2010 , 12 , 31 ) , freq= ' H ') ;
dt = pd.DataFrame ( rand ( len ( dr ) , 2 ) , dr );
data = dt [ selector ]`
there is conflict of dt package and dt variable
Python Pandas : Aggregate changed from 0.7.1 to 0.7.3
The problem seems to be with the aggregate method .
I'm using a dictionary of different aggregation methods to ` agg ` different columns ( ` np.mean ` , ` np.sum ` ... etc ) .
" No numeric values to aggregate "
Pandas : Sort pivot table
Just trying out Pandas for the first time , and I am trying to sort a pivot table first by an index , then by the values in a series .
What's the correct way to sort a pivot table by index then value ?
For example , just focus on the ix interface .
using the ix
using the reindex method
I'm trying to do shift operations ... but this also happens with the window functions like ` rolling_mean ` .
On an aside : does truncate need to be existing indexes in the data ?
Note this is a very inefficient way to build a large DataFrame ; new arrays have to be created ( copying over the existing data ) when you append a row .
For details and examples , see Merge , join , and concatenate .
I has a similar problem where if I created a data frame for each row and append it to the main data frame it took 30 mins .
Copying from pandas docs : ` It is worth noting however , that concat ( and therefore append ) makes a full copy of the data , and that constantly reusing this function can create a significant performance hit .
Add rows through ` loc ` on non existing index data .
` .loc ` is referencing the index column , so if you're working with a pre-existing DataFrame with an index that isn't a continous sequence of integers starting with 0 ( as in your example ) , ` .loc ` will overwrite existing rows , or insert rows , or create gaps in your index .
I would like to print the intersection between them removing all " NaN's " , but without loose alignment .
Do you mean you want to drop rows where there are NaNs in either of the S or JEXP columns only ?
numpy function to aggregate a signal for time ?
I know about apply , but sometimes it's more convenient to use a for loop .
Is apply more efficient than iterrows ?
Returning multiple values from pandas apply on a DataFrame
This is fairly trivial with pandas , using ` apply ` with axis=1 .
However , I can either return a DataFrame of the same shape if my function doesn't aggregate , or a Series if it aggregates .
Is this possible or I have to do two runs for the two calculations , then merge them together ?
Why are you using ` apply ` in the first place ?
You could just have ` t_test_and_mean ` accept your input dataframe ( and the columns to group by ) and return a 1-row-2-columns dataframe , without using ` apply ` .
Is there a really straight forward way to apply a css to an IPython Notebook and then have tables rendered using the style sheet ?
If you just stick that in one of your markdown cells , then it will apply to everything on the page .
b ) now do a left join on this on the data set A .
It also does a ` sort ` of the output index , so finally the complexity is something like O ( m lg m ) with m = len ( B.index ) ...
I have two pandas DataFrames and I want to join them together such that I get the outer join with the duplicates removed .
And then join all the pieces together with ` pandas.concat ` or similar .
Is there a way to perform something similar by a clever use of ` crosstab ` or ` pivot_table ` or ` stack ` or something similar ?
Note that I have implemented new ` cut ` and ` qcut ` functions for discretizing continuous data :
How to optimally apply a function on all items of a dataframe using inputs from another dataframe ?
I would like to apply the same function to each item of a given dataset but using a time-dependent parameter .
One way to do it is to use the ` map ` function , or ` numpy.vectorize ` ; it's also possible to do it with lambda functions .
Again , ` read_table ` can be used , for example : ` pandas.read_table ( buf , sep= ' ' , index_col =[ 0 , 1 ] , header=None )` will create a table with multiple columns and a ` multiindex ` made of 2 levels : first level the year-month-day , second level the time .
If you wish , you can then merge the multiindex in a normal index ( for example : ` df.index = [ ' %s %s ' % ( a , b ) for a , b in zip ( df.index.get_level_values ( 0 ) , df.index.get_level_values ( 1 ))]` .
One more thing : the grouping with multiindex works .
date_to = df.index [ len ( df.index ) -1 ] #last datetime entry in date frame
You can fix it with : ` df.index = map ( dateutil.parser.parse , df.index )` and then date-ranges are : ` pandas.DateRange ( df.index [ 0 ] , df.index [ -1 ])` .
You can use ` aggregate ` to define your aggregate function , which will just keep the first element of a column and drop the others .
Pandas : List of Column names in a pivot table
I got stuck trying to get the resulting names of a pivot table .
I'm having a bit of trouble altering a duplicated pandas DataFrame and not having the edits apply to both the duplicate and the original DataFrame .
Then I assign the ' d ' dataframe to variable ' e ' and apply some arbitrary math to column ' a ' using apply : #CODE
The problem arises in that the apply function apparently applies to both the duplicate DataFrame ' e ' and original DataFrame ' d ' , which I cannot for the life of me figure out : #CODE
can you specify the ' dropna ' value ? for example could you drop rows that are all zeros ?
[ Documentation ] ( #URL ) for read_csv now offers both ` na_values ` ( list or dict indexed by columns ) and ` keep_default_na ` ( bool ) .
Essentially my question would then boil down to : how to join several unaligned time series , where each series has a date column , and column for the series itself ( .CSV file exported from Excel )
the thing is you have to make sure that there is case like date exists but not rate . because dropna() will shift records and mismatch with index
Now , to join then together and align the data in a DataFrame , you can do : #CODE
This will form the union of the dates in ` ts1 ` and ` ts2 ` and align all of the data ( inserting NA values where appropriate ) .
So , apply this function to each of those 3 columns : #CODE
To transform to strings , use table.rename : #CODE
The ix [ , ] construct doesn't check if column exists .
pandas reindex DataFrame with datetime objects
Is it possible to reindex a pandas DataFrame using a column made up of datetime objects ?
I can reindex the ` df ` easily along ` DOYtimestamp ` with : ` df.reindex ( index =d f.dtstamp )`
but I'd like to reindex the DataFrame along ` dtstamp ` which is made up of datetime objects so that I generate different timestamps directly from the index .
When I try and reindex ` df ` along ` dtstamp ` I get the following : #CODE
It sounds like you don't want reindex .
Somewhat confusingly ` reindex ` is not for defining a new index , exactly ; rather , it looks for rows that have the specified indices .
So if you have a DataFrame with index ` [ 0 , 1 , 2 ]` , then doing a ` reindex ([ 2 , 1 , 0 ])` will return the rows in reverse order .
Doing something like ` reindex ([8 , 9 , 10 ])` does not make a new index for the rows ; rather , it will return a DataFrame with ` NaN ` values , since there are no rows with indices 8 , 9 , or 10 .
To do that i could loop through the file using beautiful soup and insert the values row by row or create lists to be inserted as columns .
Pandas DataFrame aggregate function using multiple columns
It may be more efficient to break this up into a few operations as follows : ( 1 ) create a column of weights , ( 2 ) normalize the observations by their weights , ( 3 ) compute grouped sum of weighted observations and a grouped sum of weights , ( 4 ) normalize weighted sum of observations by the sum of weights .
I am not seeing ` concat ` as a function in the pandas namespace ; I'm not sure what I am missing .
I was running pandas ver 0.6.1 which doesn't have the concat function included .
An upgrade to v 0.7.3 brings concat into the namespace .
How to shift a column in Pandas DataFrame
I would like to shift a column in a Pandas DataFrame , but I havent been able to find a method to do it from the documentation without rewriting the whole DF .
How to aggregate duplicate timestamps with pandas ?
@USER - you have to aggregate them somehow .
You want to use the apply function and a lambda : #CODE
You'll first need to split the data by timestamp , then from there if you were to treat it as an array shift an element from the array to use as comparison if the quote type is equal then continue and shift a new element repeat until !
i did groupby first but an array shift to compare is going to be extremely slow .
You can possibly duplicate the ` quote ` column twice shifting it by one each direction and apply it to your dataset to create a pivot table based on entries where ` quote ` is !
For example , when I try to aggregate ( using ' mean ') 10min values to monthly values , the function seems to use the last day of data from one month in the mean of the next month ...
Looking at the documentation , it may be that using the ` MultiIndex ` may solve my problem , but I'm not sure how to apply it to my situation - the documentation shows examples of creating MultiIndexes with random data and DataFrames , but not Series with pre-existing timeseries data .
When I iterate over these composite objects , I have an ` iterseries ` routine for the frame and ` iterframes ` routine for the panel that reconstruct the appropriate metadata / data pairing as I drop one dimension ( i.e. the series from the frame with lead time varying across the columns will have all the metadata of its parent plus the ` Lead Time ` field restored with the value taken from the column label ) .
I've tried the examples given in the documentation , but I'm still a little unclear how to apply it to my situation .
Once I have the frame given by this routine , I can easily apply the various operations suggested below - of particular utility is being able to use the ` names ` field when I
call ` concat ` - this eliminates the need to store the name of the column key internally
I might suggest using ` pandas.concat ` along with its ` keys ` argument to glue together Series DataFrames to create a MultiIndex in the columns : #CODE
We added an ` append ` option to ` set_index ` .
As I mentioned above , if you know beorehand that your TSV file header represents a ` MultiIndex ` then you can do the following to fix this : #CODE
How can I iterate and apply a function over a single level of a DataFrame with MultiIndex ?
However , I see that this doesn't drop the top level as you're looking for .
How do I apply it to a pandas DataFrame ?
` df.resample ( ' Min ')` is too high level and wants to aggregate .
NumPy by itself is a fairly low-level tool , and will be very much similar to using MATLAB . pandas on the other hand provides rich time series functionality , data alignment , NA-friendly statistics , groupby , merge and join methods , and lots of other conveniences .
Data alignment , join , etc all become * possible * due to this , but for people who don't grok that underlying difference it's not even clear what those mean ( e.g. , what is " data alignment " of two numpy arrays ? ) .
Other thing that is great in pandas is the Panel class that you can join series of layers with different properties and combine it using groupby function .
This isn't quite what's suggested in the documentation for pivot -- there , it shows results without the 1 and 0 in the upper-left-hand corner .
Is there a way to get pivot to give me a DataFrame object without that additional information in the upper-left corner ?
to build on @USER , it would be really helpful if you could convert the pandas output to either a list of lists or a list of dicts and then show the result of the conversion e.g. with ` map ( list , B_p )` .
I'm just confused because in the documentation ( help ( B_p.to_csv )) it doesn't show anything in the upper-left-hand corner when you pivot a table .
For making more general the answer ... first I will take the common index for synchronizing both dataframes , then I will join each of them to my pattern ( dates ) and I will sum the columns of the same name and finally join both dataframes ( deleting added columns in one of them ) ,
The solution I have come up with is to exclude dates for which valuation ( actually price ) data doesn't exist for a given holding and then aggregate on these dates where I have complete data .
It seems like some combination of resample and / or fillna is going to get you what you're looking for ( realize this is coming a little late ! ) .
We now need to drop the ` DataRange ` type of your ` Index ` and make it a list of ` str ` to simulate how you would parse in your data : #CODE
I would like to roll through my data by date and on each date take a time slice in the past apply a function to every time series so I get a result such as this where X is the output of the function of timeslice .
Also is there some other way to do the following.Using Apply function seems to be very slow for large dataset .
pandas row specific apply
Similar to this R question , I'd like to apply a function to each item in a Series ( or each row in a DataFrame ) using Pandas , but want to use as an argument to this function the index or id of that row .
In reality , I'm not worried ( in this case ) about returning anything meaningful , but more for the efficiency of something like ' apply ' .
If you use the apply method with a function what happens is that every item in the Series will be mapped with such a function .
A more complex usage of apply would be this one : #CODE
However you can include indices in your function by creating a new series ( apply wont give you any information about the current index ): #CODE
When you read in your files , you can use ` concat ` to join the resulting DataFrames into one , then just use normal pandas averaging techniques to average them .
The problem disappears if I change window_type to ` expanding ` and I get about 500 output points as expected , but I don't want to do an expanding regression .
issue with pandas and semilog for boxplot
For some reason , when I use semilogy and boxplot with the video series , I get the error #CODE
but when I do it on the ' link ' series I can draw the boxplot correctly .
and I am able to draw the boxplot .
You could create a semi-log boxplot , for example , by : #CODE
Or , more generally , modify / transform to you heart's content , and then boxplot .
Pandas pivot warning about repeated entries on index
On Pandas documentation of the ` pivot ` method , we have : #CODE
But when I run the ` pivot ` method , it is saying : #CODE
I'm using the ` name ` column as the index of the pivot , the first argument of the ` pivot ` method call .
Can you post the exact pivot method call you're using ?
If you have duplicates you may need to aggregate first .
It would be nice to add an option to pivot to take either the first or last observed entry : #URL
I fixed the parser bug shown in the stack trace that you pasted .
Find all elements in dataframe column that startswith string
I am currently rolling up numbers with the following code .
Try saving that to a temporary variable and using the temp variable inside ` startswith ` .
` df = df.index.drop ( ' L ')` removes L completely from the DataFrame ( unlike ` df= df.reset_index() ` which has a drop argument ) .
The bit about the copy was only for use of ` ix [ ]` if you * prefer * to use ` ix [ ]` for any reason .
` ix ` indexes rows , not columns .
` ix ` accepts slice arguments , so you can also get columns .
It is incorrect to say that ` ix ` indexes rows .
So , ` ix ` is perfectly general for this question .
Never knew about that feature of ` ix ` .
That said , you can easily convert a column-slicing problem into a row-slicing problem by just applying a transpose operation , ` df.T ` .
The ` drop ` method is documented here .
You're correct that this would be wrong for most types ; however ` pandas.DataFrame ` has special support for setting values using a Boolean mask ; it will select the corresponding values from the RHS with the corresponding time value .
My worry is that in the original code , the first N values of cap_level will be taken and used , where N is the number of True values in the Boolean mask .
I have several Series indexed by dates that I would like to concat into a single DataFrame , but the Series are of different lengths because of missing dates etc .
grouped pandas DataFrames : how do I apply scipy.stats.sem to them ?
I know that I can apply numpy methods by doing the following :
However , what if I want to compute the standard error of the mean ( sem ) ?
Oddly , the results of the method I used before to compute sem , ` std / sqrt ( # replicates )` , varies _slightly_ from ` sem() ` results , but we're only talking ~ 4e-11 difference max .
I'm not sure why Pandas thinks this is a " bool " type column .
Doesn't the true / false part of QRY explain the bool nature of i for your example ?
I have a data table using pandas and column labels that I need to edit to replace the original column labels .
I have the edited column names stored it in a list , but I don't know how to replace the column names .
This will put them into dictionary form , including the earlier defined class and subject variables , and append them to an outputList .
The trick here is to use the ` axis=1 ` option in the ` apply ` to pass elements to the lambda function row by row , as opposed to column by column .
once sorted I replace the df.index with a numerical index #CODE
Is it possible to customize Serie ( in a simple way , and DataFrame by the way :p ) from pandas to append extras informations on the display and in the plots ?
A great thing will be to have the possibility to append informations like " unit " , " origin " or anything relevant for the user that will not be lost during computations , like the " name " parameter .
We'd welcome any additional feedback you have ( see pandas on github ) and would love to accept a pull-request if you're interested in rolling your own .
Why map instead of apply ?
Using the dt option playing around with weekofyear , dayofweek etc . becomes a easier
Is there a way to " merge " multiple DataFrames , but what is done at dates that occur only in one of the underlying DataFrames ?
making things continuous : you should just be able to use resample directly
Basically , I'm trying to pivot on location to end up with a dataframe like : #CODE
Unfortunately when I pivot , the index , which is equivalent to the original dates column , does not change and I get : #CODE
because I have a # of data columns I want to pivot ( don't want to list each one as an argument ) .
I believe by default pivot pivots the rest of the columns in the dataframe .
I'm actually calling df.pivot without the third argument as in my actual data , i have a # of data columns and I want to pivot all of them .
If you have multiple data columns , calling pivot without the values columns should give you a pivoted frame with a MultiIndex as the columns : #CODE
Yeah I'm seeing the information come out as a multiindex , but again , I get the same issue where pandas seems to recognize all the dates as unique and I get a bunch of Nans .
Even if I set the pivot argument values to say column C , I still get the same # of rows as in my original table , just with Nans for all the repeated dates .
Hence the dataframe pivot treated each column value as unique in the index .
I'm trying to drop the last row in a dataframe created by pandas in python and seem to be having trouble .
I tried the drop method like this : #CODE
I also tried to drop by index name and it still doesn't seem to be working .
I can use sin and DataFrame.prod to create a boolean mask : #CODE
Then use the mask to select from the DataFrame : #CODE
better method to aggregate pandas dataframe by non matching criteria
I have a dataframe of species survey counts and need to aggregate the rows by multiple criteria .
Is there a vectorised way to aggregate data in this way ?
What's the right way for me to aggregate these stock prices into Monthly ?
The error message is " GroupByError ( ' No numeric types to aggregate ')" .
EDIT : I just realized that most of the other functions ( min , max , median , etc . ) work fine but not the mean function that i desperately need :-( .
Also , you don't really need the lambda here , just feeding ` np.mean ` would work too , but I left the lambda in to illustrate how you would solve this when more general functions that you want to apply aren't working in their default ways .
You can replace the ` [ 0:4 ]` with ` [ df.index.values [ i ]: df.index.values [ j ]]` or ` [ df.index.values [ i ] for i in range ( N )]` or even with logical values such as ` [ df [ ' a '] 5 ]` to only get rows where the ' a ' column exceeds 5 , for example .
I am trying to use the agg fn but without doing a groupby .
I think it uses ` patsy ` in the backend to translate the formula expression , and intercept is added automatically .
dalejung on GitHub has done quite a bit of work recently in creating a tighter pandas-xts interface with rpy2 , you might get in touch with him or join the PyData mailing list
Length : 3 , Freq : 3H , Timezone : None
Length : 3 , Freq : H , Timezone : None
We run into issues that join columns are converted into either ints or floats , based on the existence of a NA value in the original list .
( Creating issues later on when trying to merge these dataframes )
I haven't done time benchmarking , but I am skeptical of the following immediately obvious way that comes to mind ( and variants that might use ` map ` or ` filter `) .
First is for ` vtype ` above , then for the ` apply ` route .
Can you post a stack trace and / or what merged2 looks like ?
Collapse a Pandas multiindex or run OLS regression on a multiindexed dataframe
I used pivot to reshape my data and now have a column multiindex .
Collapse the multiindex .
Is there some way to run a regression with a multiindex ?
I can create a new dataframe , loop over both column indexes , and insert new columns into the new dataframe with the same name , but with names as strings instead of tuples .
I.e. how easy is it for me to cvs checkout the code , test my changes in iPython rather than with the prod version then creating a pull request ?
So in this case I would replace that missing value with the average rating given to that artist ( a bad first approximation , better to use the SVD )
It depends on the size of your DataFrame , but potentially you could repeat the mean rating so it's the same size as the ratings matrix and then use the NA mask to replace the missing values ?
I built the features by using the pandas group operations , then applied the mean() on the group , and finally did a merge back onto the main dataframe .
This supersedes the ` irow ` approach .
If you want to remove the old 10-based indices , you can insert the flag ` drop=True ` into the parenthesis of the reset_index function .
I think it is ` df [ ' A '] .iget ( 0 )` because ` df [ ' A ']` is a ` Series ` , which has no ` irow ` .
Say I can read the file and concat all of them together into one DataFrame .
what if the values are strings or categorical - i am getting the error : incompatible categories in categorical concat
Pandas join / merge / concat two dataframes
should I be able to join it with y on index with a simple join command where y = x except colnames have +2 .
I tried merge as well but I have the same issue .
If you are having issues with join , read Wes's answer below .
` merge ` and ` join ` do , well , joins , which means they will give you something based around the Cartesian product of the two inputs , but it sounds like you just want to paste them together into one big table .
Edit : did you try concat with ` axis=1 ` ?
here is what concat looks like :
Perhaps ` concat ([ x , y ] , axis=1 )` ?
Pandas : pivot a dataframe
The sensor timeseries data is then also rounded to the nearest minute and I use numpy.in1d and take the timestamps from the above ' minutes_array ' and the ' sensor_data ' array and create a mask for the records relating to that sensor .
I then wish to modify the records in minutes_array which are true for that mask and place the sensor_data value into the first column following the timestamp in minutes_array .
From my attempts it does not seem possible to alter the original ' minutes_array ' when a mask is applied to it , is there a way to achieve this outcome in numpy without using for loops and matching timestamps individually ?
concat pandas DataFrame along timeseries indexes
I have two largish ( snippets provided ) pandas DateFrames with unequal dates as indexes that I wish to concat into one : #CODE
Not sure what your ` concat ` line will do
Try to join on outer .
I often need to apply a function to the groups of a very large ` DataFrame ` ( of mixed data types ) and would like to take advantage of multiple cores .
I do something like that but using UWSGI , Flask and preforking : I load the pandas dataframe into a process , fork it x times ( making it a shared memory object ) and then call those processes from another python process where I concat the results . atm I use JSON as a communication process , but this is coming ( yet highly experimental still ): #URL
pandas concat ( ' outer ') not doing union ?
It looks pandas.concat is doing ' left outer ' join instead of just union the indexes .
How to resample a python pandas TimeSeries containing dytpe Decimal values ?
I'd like to use the new pandas 0.8 function to resample the decimal time series like this : #CODE
When trying this i get an " GroupByError : No numeric types to aggregate " error .
I assume the problem is that np.mean is used internaly to resample the values and np.mean expects floats instead of Decimals .
Thanks to the help of this forum i managed to solve a similar question using groupBy and the apply function but i would love to also use the cool resample function .
It is possible to provide a function to the ' how ' argument of resample : #CODE
Currently i'm using string replace which i consider to be a significant perfomance penalty .
Unexpected result when upsampling hourly values using the pandas resample function
I try to upsample daily TimeSeries values using the pandas resample function .
If it is a feature how can i set the resample arguments to achieve my goal ?
Given the " circular " dependency ( not really circular given the lags ) , I'm not sure how I could do either regular series math or use normal shift operations ( e.g as I do with ` Cash Return `) .
A combination of boolean indexing and apply can do the trick .
However , I think that you can get away with ` .max ( axis=1 )` instead of ` apply ( ... )` .
` max() ` is ok too of course , i think i got biased towards ` apply ` by the way you asked the question :-)
Simplest way is probably ` list ( dt.T.itertuples() )` ( where ` dt ` is your dataframe ) .
The problem in your code is that you want to apply the operation on every row .
Most operations in ` pandas ` can be accomplished with operator chaining ( ` groupby ` , ` aggregate ` , ` apply ` , etc ) , but the only way I've found to filter rows is via normal bracket indexing #CODE
If you want to chain methods , you can add your own mask method and use that one .
I would extend it by generalizing the mask function as : #CODE
If you would like to apply all of the common boolean masks as well as a general purpose mask you can chuck the following in a file and then simply assign them all as follows : #CODE
But I found that , if you wrap each condition in ` ( ... == True )` and join the criteria with a pipe , the criteria are combined in an OR condition , satisfied whenever either of them is true : #CODE
pandas : stacking DataFrames generated by apply
1 ) " join " the results back to the initial DataFrame
1 ) join this back to the original DataFrame ` df `
transform ( #URL ) and agg ( #URL ) can be used .
Just use the aggregate method of the groupby object : #CODE
If you want different operations on each column , according to the docs you can pass a ` dict ` to ` aggregate ` .
and than apply it by passing the function and the args to ` agg ` : #CODE
Excel considers 1900 a leap year , so be careful with exactly what you want to translate :
Minor bug : my_colors = [ cycle ([ ' b ' , ' r ' , ' g ' , ' y ' , ' k ']) .next() for i in range ( len ( df ))] will give ' b ' every time in python 2.7 .
You should use list ( islice ( cycle ([ ' b ' , ' r ' , ' g ' , ' y ' , ' k ']) , None , len ( df ))) instead .
it = cycle ([ ' b ' , ' r ' , ' g ' , ' y ' , ' k ']) ; my_colors =[ next ( it ) for i in xrange ( len ( df ))] would cut it as well ...
This creates a boolean mask which is then used for the subsetting .
I have a relatively simple python multiprocessing script that sets up a pool of workers that append output to a pandas dataframe by way of a custom manager .
The correct solution is to wait use the results objects to get the results and then append all of them in the main thread .
Where has ` unstack ` been hiding ???
Filtering and selecting from pivot tables made with python pandas
Is there a way to do this directly within the pivot table structure , or do I need to convert this back in to a panda data frame ?
Is there a way to get a list of values in a pivot table column by specifying the header ?
I can do this on the dataframe with ' df [ ' A '] .values ' but I'm struggling to obtain something similar from the pivot table
the result of the pivot table is a DataFrame .
How to keep index when using pandas merge
I would like to merge two data frames , and keep the index from the first frame as the index on the merged dataset .
However , when I do the merge , the resulting DataFrame has integer index .
But for many merge operations , the resulting frame has not the same number of rows than of the original ` a ` frame .
reset_index moves the index to a regular column and set_index from this column after merge also takes care when rows of a are duplicated / removed due to the merge operation .
There are some places where pandas is not as careful as it could be about memory usage when it comes to MultiIndex -- if you do find a case that reproduces the issue please do post it on the issue tracker .
I can loop over the list of TimeStamps and get the relevant rows from the ` DataFrame ` but this is a lengthy process and I thought that ` ix [ ]` should accept a list of values according to the docs ?
Ok , I can create an empty dict , insert values and create a DataFrame .
Just for completeness , though , you could -- inefficiently -- use ` join ` or ` concat ` to get a column-by-column approach to work : #CODE
In this example I get an Attribute error related to the map function .
note that support for ` filter ( None , iterable )` ceased in Python 3 , need to do ` filter ( bool , iterable )` there
I'm trying to unstack a dataframe perform operations on it ( over time only ) and then stack it back together like this : #CODE
The problem is that multiindex is getting reversed after the operation , is there any easy way to make this work ?
Perhaps I'm misusing the stack from the start ?
` stack ` and ` unstack ` add level ( s ) to the end of the MultiIndex , this is not controllable .
You can change the order of the levels in a MultiIndex with ` reorder_levels() ` : ` stacked.reorder_levels ([ 2 , 1 , 0 ])` will give you the same MultiIndex levels order as in ` df `
thanks almost works , you have to add an ` .sortlevel ( 0 )` to get a correct multiindex .
As noted below , pandas now uses SQLAlchemy to both read from ( read_sql ) and insert into ( to_sql ) a database .
How do I really use the ` ix ` method of a pandas DataFrame ?
Having read the docs one the ` ix ` method of DataFrames , I'm a bit confused by the following behavior with my MultiIndexed DataFrame ( specifying select columns of the index ) .
Why does the ` ix ` method behave like this ?
If you want to select rows / columns based on MultiIndex level values i suggest using the ' .xs() ' method .
Is there a direct way to do this on one level of a multiindex ( i.e. without the reset_index ? )
We can join these strings with the regex ' or ' character ` | ` and pass the string to ` str.contains ` to filter the DataFrame : #CODE
You mention in your question that the red line is the mean - it is actually the median .
with a line at the median .
The set_value approach also works for multiindex DataFrames by putting the multiple levels of the index in as a tuple ( e.g. replacing column with ( col , subcol ) )
I want to normalize this data , by splitting it into tables .
In case of fixed width file , no need to do anything special to strip white space , or handle missing fields .
Python pandas equivalent for replace
In R , there is a rather useful ` replace ` function .
` replace ( df$column , df$column == 1 , ' Type 1 ') ; `
Should I use a lambda with ` apply ` ?
` pandas ` has a ` replace ` method too : #CODE
In my defence , the replace function is not listed [ here ] ( #URL )
How to broadcast to a multiindex
If B is the one with a MultiIndex then you can do A.reindex ( B.index , level=0 ) , compute the result , and then do result.groupby ( level=0 ) to compute an aggregate result .
After this , I think I should insert a column in the dataframe that labels all my data with Monday through Friday -- for all the dates in the file ( there are 6 years of data ) .
And then , I need to join these .
Does there exist in pandas methods to perform a similar array calculation to obtain a datetime ( 64 ) date / time stamp with which I can replace the current sequential dataframe index ?
python pandas : apply a function with arguments to a series
I want to apply a function with arguments to a series in python pandas : #CODE
The documentation describes support for an apply method , but it doesn't accept any arguments .
The apply method accept a python function which should have a single parameter .
For a DataFrame apply method accepts ` args ` argument , which is a tuple holding additional positional arguments or ** kwds for named ones .
@USER : I notice that ` np.random.permutation ` would strip the column names from the DataFrame , because ` np.random.permutation ` .
pandas merge timeseries , concat / append / ...
I would like to merge the existing series with the new ones subsequently in every loop , while preserving their ( different ) indices .
I tried concat , but somehow I cannot add another series after the first one ...
so I really need to append the time series after every loop ...
I do something like this all the time but I use ` append ` like this : #CODE
I want to apply a groupby operation that computes cap-weighted average return across everything , per each date in the " yearmonth " column .
The join example at the bottom does work , but it's not presented clearly .
Then you reindex this result according to the original DataFrame , matching their indices on the 2 values in your example .
While I'm still exploring all of the incredibly smart ways that ` apply ` concatenates the pieces it's given , here's another way to add a new column in the parent after a groupby operation .
May I suggest the ` transform ` method ( instead of aggregate ) ?
My understanding was that transform produces an object that looks like the one it was passed .
So if you transform a DataFrame , you don't just get back a column , you get back a DataFrame .
Whereas in my case , I want to append a new result to the original data frame .
Or are you saying that I should write a separate function that takes a data frame , computes the new column , and appends the new column , and * then * transform with that function ?
I agree , transform is a better choice , df [ ' A-month-sum '] = df.groupby ( ' month ') [ ' A '] .transform ( sum )
IMHO , ` transform ` looks cleaner .
Very weird bug here : I'm using pandas to merge several dataframes .
As part of the merge , I have to call reset_index several times .
Inspecting frame.py , it looks like pandas tries to insert a column ' index ' or ' level_0 ' .
Fortunately , there's a " drop " option .
And now I want to replace the element of df_a by element of df_b which have the same ( index , column ) coordinate , and attach df_b's elements whose ( index , column ) coordinate beyond the scope of df_a .
They have 2 index columns and the reindex operation results in NaN values in strange places ( I'll post the dataframe contents if anyone is willing do debug it ) .
Is there an efficient way to apply this disaggregation map to get a new dataframe at a State level ?
Building on that , you can create a boolean array from a MultiIndex with df.index.map() and use the result to filter the frame .
And if I want to filter the Q1 , Q3 , Q4 together , which is " NOT endswith ( ' 0630 ')" , how to add the ' NOT ' to the command of " df [ df.index.map ( lambda x : x [ 1 ] .endswith ( " 0630 "))] " ?
I think the first thing I should do should be to ' unstack ' the values in the csv , in order to have an aligned index first , and then create a DataFrame , but really don't how ...
There might be a slick vectorized way to do this , but I'd just apply the obvious per-entry function to the values and get on with my day : #CODE
Reindex time-stamped data with date_range
If you would use the timestamps of the events as index of the series instead of the data , resample can do this .
resample ( this method can also be used on a DataFrame ) will give a new series with in this case 15min periods , the end time of a bucket ( period ) is used to refer to it ( you can control this with the label arg ) .
Definitely pay attention to the ` closed ` and ` label ` options to ` resample ` !
You can also use this to transform a subset of a column , e.g. : #CODE
However , it says [ here ] ( #URL ) that setting works with ix .
I didn't realize @USER B . was asking about ` ix ` in general .
The section after ' Assignment / setting values is possible when using ix : ' will explain exactly what you need !
KDB+ like asof join for timeseries data in pandas ?
kdb+ has an aj function that is usually used to join tables along time columns .
I see that pandas has an asof function but that is not defined on the DataFrame , only on the Series object .
I guess one could loop through each of the Series and align them one by one , but I am wondering if there is a better way ?
this is also called * rolling join *
It could be easily ( well , for someone who is familiar with the code ) extended to be a " left join " mimicking KDB .
I was getting ` ValueError : Index contains duplicate entries , cannot reshape ` when doing ` unstack ` on a MultIndex but this solution works for that only I had to do ` df_unique = df.groupby ( level =[ 0 , 1 ]) .first() `
You need to reset_index if you want to drop on index and values or just work with the index if you want to have a unique index .
Add parse_dates=True , otherwise your index will be plain strings and resample does not like that .
Date = range ( len ( df2 ))
Volume = np.zeros ( len ( df2 ))
EDIT : I have just read the help on ` pandas.Series.diff() ` , but still I'd like to " replace " the subtraction used on diff by another function , say ` euclidean_distance() ' .
I would suggest you just write a function to do what you're saying probably using ` drop ` ( to delete columns ) and ` insert ` to insert columns at a position .
I have a Pandas dataframe ' dt = myfunc() ' , and copy the screen output from IDLE as below : #CODE
The function to apply is like : #CODE
I would use transpose and the sort method ( which works on columns ): #CODE
Duplicate entries for index in pandas pivot function
My goal is to have data grouped by date , mat and strike ( I can drop the ' 3m ' and ' dataframe name ' columns since they're common to all data ) .
Can anyone help me with this issue , or propose an alternative approach to the pivot function ?
` pivot ` is a reshape operation : #CODE
Now for the strides .
Basic problem : how do I map the function to the column , specifically where I would like to reference more than one other column or the whole row or whatever ?
Then you can use map : #CODE
I don't suppose you have nny ideas on the second part , viz referencing neighbouring rows in the dataframe from within the map / apply function ?
The exact code will vary for each of the columns you want to do , but it's likely you'll want to use the ` map ` and ` apply ` functions .
If you need to use operations like max and min within a row , you can use ` apply ` with ` axis=1 ` to apply any function you like to each row .
For the second part of your question , you can also use ` shift ` , for example : #CODE
Then groupby the pattern and apply the appropriate function to each group .
Generating a boolean mask indexing one array into another array
Note that as suggested in a comment by @USER , you should use ` np.eye ( len ( x ))` instead of ` np.diag ([ 1 ] *len ( x ))` .
Now I really don't know why you want a boolean mask , these indices can be applied to z to give back x already and are more compact .
Python Pandas : how to add a totally new column to a data frame inside of a groupby / transform operation
Now , the real question is how to use ` transform ` to add a new column to the data .
Note that a simple ` apply ` will not work here , since it won't know how to make sense of the possibly differently-sized result arrays for each group .
can you not use ` map ` ?
What problems are you running into with ` apply ` ?
You need to construct your full index , and then use the ` reindex ` method of the dataframe .
In the current version of pandas , there is a function to build ` MultiIndex ` from the Cartesian product of iterables .
Python Pandas : How to broadcast an operation using apply without writing a secondary function
It seems logical to use the ` apply ` function for this , but it doesn't work like expected .
It does not even seem to be consistent with other uses of ` apply ` .
Based on this , it appears that ` apply ` does nothing but perform the NumPy equivalent of whatever is called inside .
That is , ` apply ` seems to execute the same thing as ` arr + " cat "` in the first example .
But this seems to break from what ` apply ` promises in the docs .
Is there some way of using ` apply ` that I am missing here ?
and I verified that this version does work with Pandas ` apply ` .
Isn't this specifically what ` apply ` is supposed to abstract away from the user ?
and use this in ` apply ` : #CODE
This works , but I consider it a workaround as well , since it doesn't address the fact that ` apply ` isn't working as promised .
Can you verify that ` map ` will work in all the same situations where ` apply ` will work ?
I also don't like the inconsistency in going from ` map ` for a Series to ` applymap ` for a DataFrame .
That contradicts the docs for ` apply ` , as well as its 0.8.1 behavior , in which it successfully performs the elementwise version of my example above , whereas version 0.7.3 seems to use the logic you describe .
Since ` apply ` should work in 0.7.3 as it does in 0.8.1 ( according to the docs ) , that's why I think it's a workaround .
` map ` is fine , but ` apply ` should work .
` apply ` is designed so that you can apply a ufunc and get back a Series with the index intact .
Though I do agree with you the docstring for apply is very unclear about this aspect .
We can improve the documentation for apply .
In fact , by saying that ` apply ` can take any function that expect a * single * argument , it's not just unclear , but plain misleading .
So to be clear , we should use ` apply ` whenever we have a vectorized / ufunc already , and ` map ` when we literally want to apply an elementwise operation to a series ?
Yup , that's exactly right on ` apply ` vs ` map ` .
Stepping the trace in the case of date column , shows that matplotlib tries to do x [ 0 ] on the dates to retrieve tz info , which throws a KeyError .
I decided to change date to first day of each month using a lamdba function that calls replace ( day=1 ) .
You're producing an aggregate r and s value per group , so you should be using ` Series ` here : #CODE
You'd have to map that to the columns using ` map ` or ` apply ` or something .
@USER : To avoid the error , replace ` int ( x )` with the expression ` int ( text ) if x.isdigit() else x ` .
Now I want to merge the data frame with the series , such that the values from the series are broadcasted along the second level index .
How to groupby the first level index and apply function to the second index in Pandas
And I want to apply a function ` func ` ( exp : `' lambda x : x*10 '`) to ` second ` , somewhat like : #CODE
This way , the index column is not dropped and still accessible for your ` apply ` .
I want to drop duplicates , keeping the row with the highest value in column B .
Wes has added some nice functionality to drop duplicates : #URL .
D'you know the best idiom to reindex this to look like the original DataFrame ?
There's some code to reindex the grouped dataframe .
PS : but if you really just want the last column , ` apply ` would suffice : #CODE
@USER It seems to work for me , with your example ( without the ` skiprows `) ... perhaps you need to ` myData.T ` ( transpose ) .
I've tried using rename to change the columns on one df first , but ( 1 ) I'd rather not do that and ( 2 ) my real data has a multiindex on the columns ( where only one layer of the multiindex is differently labeled ) , and rename seems tricky for that case ...
So to try and generalize my question , how can I get ` df1 * df2 ` using ` map ` to define the columns to multiply together ?
Assuming the index is already aligned , you probably just want to align the columns in both DataFrame in the right order and divide the ` .values ` of both DataFrames .
Suppose we want to multiply several columns with other serveral columns in the same dataframe and append these results into the original dataframe .
( I think it can be some problem with ` lambda ` When I want to apply my function to the column I have an error : ` TypeError : only length-1 arrays can be converted to Python scalars `)
On top of a dodgy converter , i think you apply the converter to the wrong column ( look at the exception you get ) .
Is there a way to do this if you want to normalize a subset ?
Say that row ` A ` and ` B ` are part of a larger grouping factor that you want to normalize separately from ` C ` and ` D ` .
You can use ` apply ` for this , and it's a bit neater : #CODE
If you want that done on every row in the dataframe , you can use apply ( with axis=1 to select rows instead of columns ): #CODE
I tried with various attempts of ' unstack ' , ' groupby ' and ' pivot ' but with no success .
At the moment for conversion I use as below , but need remove unwanted rows first to apply it to all df .
( The series always got the same length as a dataframe . ) I tried different versions of ` join ` , ` append ` , ` merge ` , but I did not get it as what I want , only errors at the most .
Note my original ( very old ) suggestion was to use ` map ` ( which is much slower ): #CODE
@USER if you already have ` e ` as a Series then you don't need to use ` map ` , use ` df [ ' e '] =e ` ( @USER answer ) .
this will effectively be a left join on the df1.index .
So if you want to have an outer join effect , my probably imperfect solution is to create a dataframe with index values covering the universe of your data , and then use the code above .
This worked fine to insert the column at the end .
Then , since you extend the base class , you have to replace the methods with a suitable descriptor : #CODE
I want to resample a TimeSeries in daily ( exactly 24 hours ) frequence starting at a certain hour .
Some weeks ago you could pass `' 24H '` to the ` freq ` argument and it worked totally fine .
is there an existing built-in way to apply two different aggregating functions to the same column , without having to call ` agg ` multiple times ?
Is there any other manner for expressing the input to ` agg ` ?
If you look at the doc string for ` aggregate ` it explicitly says that when a ` dict ` is passed , the keys must be column names .
So this is the Series version of aggregate ?
I'm looking to do the DataFrame version of aggregate , and I want to apply several different aggregations to each column all at once .
I would just use the transpose of the array because that's much faster , but then I would have a ` MultiIndex ` on the columns , and I haven't yet found any documentation in Pandas showing how to use ` MultiIndex ` s as columns .
To get all shank 1's ( i.e. where the first level of the MultiIndex is equal to 1 ) .
This is pretty flexible if you need to cross-section by a different level of the MultiIndex as well .
An alternative slightly more flexible way , might be to use ` apply ` ( or equivalently ` map ` ) to do this : #CODE
I think it wight be simpler to completely drop this column , and then add a new one with the year , or completely replace the values by the year .
First , I think you have to either specify named parameters or use ` args ` to pass additional arguments to ` apply ` .
because ` apply ` doesn't act elementwise , it acts on entire Series objects .
Is there a grep like built-in function in Pandas to drop a row if it has some string or value ?
Below example will drop all rows where column A holds ' a ' character and ' B ' equals 20 .
@USER : to drop the unmatched condition .
Generally , I find myself using boolean indexing and the tilde operator when obtaining the inverse of a selection , rather than df.drop() , though the same concept applies to df.drop when boolean indexing is used to form the array of labels to drop .
You can transform the tuple to list with ` list ( tup )` and do the switch .
One way to do this is to use apply : #CODE
If you want to change the values in only one column you can still use ` apply ` : #CODE
Note : since ` my_fun2 ` returns a single value , this time ` apply ` return a Series , so we need to slightly change the way we apply apply .
` agg ` is just a shorthand for ` aggregate ` , you are however forcing it to work on single columns always , which works around the issue .
median median
` agg ([ np.median ])` :) yes .
This means aggregate passes first the 2D series in .
Thank you , I suspected the problem was about pandas - numpy interface and numpy's array treatment , inspected ` aggregate ` docstring , but could not draw the conclusion you did ;)
For things like sum , mean , median , max , min , first , last , std , you can call the method directly and not have to worry about the apply-to-DataFrame-but-failover-to-each-column mechanism in the GroupBy engine .
( what is ` pd ` and what is ` dt `) ?
>>> import datetime as dt
I replace 2 by 1 in the isocalendar . the propriety week of TimeStamp is very strange .
In general , though , is there a prefered approach to Split-Apply-Combine where Apply returns a dataframe of arbitrary size ( but consistent for all chunks ) , and Combine just vstacks the returned DFs ?
Use ` join ` : #CODE
I do not want to join them )
I would like to drop all non-numeric columns in one fell swoop , without knowing their names or indices , since this could be doable reading their dtype .
pyPandas : mess with join / append / concat two dataframes
I would like to join them side by side resulting in a 21 cols dataframe with the same 624 number of rows .
I have tried several things join them by axis=1 ignoring index or not .
I also tried concat and append , but with no success .
One alternative is to merge on ' Name ' and ' L1 ' : #CODE
Another is to call DataFrame.reset_index first before you call merge : #CODE
I had realized that reset would work , however , why to reset index to concat dfs ignoring them ?
At least in ` concat ` , you have to declare axis .
Pandas DataFrame : apply function to all columns
Is there a more pythonic way to apply a function to all columns or the entire frame ( without a loop ) ?
Pandas transpose concat()
How to transpose a DataFrame returned by concat() ?
I have been attempting to use the merge , concat and join functions to no avail .
You can use the keys argument for concat , this will result in a MultiIndex and will allow you to uniquely select data : concat ( pieces , keys =[ ' left ' , ' middle ' , ' right '] .
Note that basic backfill and forward fill would not be sufficient to fill every daily observation in the month with the monthly value .
you can fill the resampled series sr as following : sr.groupby ( sr.index.month ) .transform ( lambda x : x.fillna ( method= ' backfill '))
Using your guidance , I was also able to implement the daily average I mentioned : sr.groupby ( sr.index.month ) .transform ( lambda x : x.fillna ( method= ' backfill ') / len ( x ))
I don't think you need the groupby here if you use resample , there is a ` fill_method ` parameter in ` resample ` that just fills the time bin .
@USER She - I was able to use s.resample ( ' D ' , fill_method= ' backfill ') to fill in data , however , I couldn't figure out how to use resample to get data starting from 2012-01-01 as opposed to 2012-01-31 .
Accessing pandas Multiindex Dataframe using integer indexes
No numeric types to aggregate - change in groupby() behaviour ?
On 0.9 , I get No numeric types to aggregate errors .
I can understand why this might happen , as the weekly dates don't exactly align with the monthly dates , and weeks can overlap months .
As for the resample example given , unfortunately it doesn't return quite what I was looking for .
I believe you can use the append #CODE
Python Pandas : pivot table with aggfunc = count unique distinct
How do I get a Pivot Table with counts of unique values of one DataFrame column for two other columns ?
I am aware of ' Series ' ` values_counts() ` however I need a pivot table .
Note that using ` len ` assumes you don't have ` NA ` s in your DataFrame .
You can do ` x.value_counts() .count() ` or ` len ( x.dropna() .unique() )` otherwise .
You can construct a pivot table for each distinct value of ` X ` .
will construct a pivot table for each value of ` X ` .
You could also do an inner join on stations.id :
the merge complains that there's ' no item named start_station_id ' .
It was my bad : " on " is only to be used when the columns occur in both DataFrames ( so my code was referring to a join on both id and start_station_id which is wrong here ) .
For the reindex : non-unique indices are rather new in pandas .
I filed an issue because of the reindex error : #URL
Can you load 2 separate data frames and do join / groupby on the datetime ?
Using the same basic loop as above , just append the set of every forth row starting at 0 to 3 after you run your code above .
If it is just formatting and the time is recorded to a precision that all entries are unique , then stack and merge would do it .
Here is a solution based on numpy's repeat and array indexing to build de-stacked values , and pandas ' merge to output the concatenated result .
Then build a de-stacked vector of TDRs and merge it with the original data frame #CODE
I have two time series I need to join :
How do I join these time series together in pandas as to express adjusted prices in expression #CODE
i had figured out that i need to pivot the first point-in-time timeseries for tickers go into the columns and date into rows and for the second timeseries expand the interval into daily granularity and also pivot it ( through dataframe.pivot function . by combining the two dataframes one can write function i need .
You can simply join the dataFrame with your daily bar and use fillna ( method= " ffill ") to forward fill the previous value . in your example you have adjustment factors for a range .
I was hoping to get this to work but a pyTable table where does not provide a len
` reindex ` realigns the existing index to the given index rather than changing the index .
If there are no blanks some columns convert to ` TRUE / FALSE ` , others leave as ` Yes / No ` but dtype is bool .
` fhs = fhs.drop ([ 1002 ])` to drop that row and data types are still good .
first column comes into df as Yes , No , Yes , Yes type bool xxxx below
3rd column comes into df as FALSE , FALSE , TRUE , TRUE type bool
first column comes into df as Yes , No , Yes , Yes type bool xxxx below
3rd column comes into df as FALSE , FALSE , TRUE , TRUE type bool
The apply or transform function on a group seems like the right way to go but after hours of trying i still do not succeed .
Using the same function with transform would work if the ' new ' column already exists in the df , but how do you add a new column at a specific level ' on the fly ' or before grouping ?
Creating a new column based on grouped values is a task for transform , but i ` m not aware if tranform can output multiple columns .
BTW under the hood , transform also creates a new frame for each group and concats them all at the end .
Having the apply / transform mechanism be able to output structured values and those broadcast into colums ( i.e. if a tuple is produced by the applied function , the components go in separate columns instead of the tuple becoming an atomic element in a single column ) would be a fantastic feature , even if it is only syntactic sugar .
Probably with another method name , to make intent clear ( applyfork or something like that , or a keyword splitseq=True in apply ) .
Pandas DataFrame reindex column issue
I want to reindex all the data frames according to totalColumns .
So I used the reindex method : #CODE
If yes , try to resolve that to have unique column names in the two frames , and execute the reindex again .
How can I join the two columns with the same label together and sum up their numbers according to the respective row index ?
After implementing a custom frequency in pandas by subclassing ` DateOffset ` , is it possible to " register " an offset alias for that frequency so that the alias can be used in built-in pandas functions such as ` date_range ` and ` resample ` ?
Given the df below I am trying to do a stack bar plot for ' stats_value ' and ' read1_length ' v / s ' lib_name ' .
Basically , I want to mimic R's melt / cast without getting into hierarchical indexing or stacked dataframes .
I can't seem to to get exactly the above playing with stack / unstack , melt , or pivot / pivot_table -- Is there a good way to do this ?
There is a ` melt ` in ` pandas.core.reshape ` : #CODE
The columns end up being a MultiIndex , but if that's a deal breaker for you just concat the names and make it a regular Index .
How might you concat the names , that is where I was confused ( " Basically , I want to mimic R's melt / cast without getting into hierarchical indexing or stacked dataframes " -- so was already aware of this , what I am confused on is how to get this into a flat structure with concatenated column names .
Following the official docs you can use loc #CODE
I have multiple ( more than 2 ) dataframes I would like to merge .
I read that join can handle multiple dataframes , however I get : #CODE
I have tried passing ` rsuffix =[ " %i " % ( i ) for i in range ( len ( data ))]` to join and still get the same error .
I am interested to see if the experts have a more algorithmic approach to merge a list of data frames .
I would try ` pandas.merge ` instead of ` join ` .
The ` pandas.concat() ` solution is _much_ better -- I thought ` concat ` gave the duplicated column name error when ` axis=1 ` , but I have a lot to learn .
As suggested in this post , I'm handling that with a MultiIndex .
Forcing dates to conform to a given frequency in pandas
Calculate diff of a numpy array using custom function instead of subtraction
This would give me either a list of distances with ` len ( distances ) == coord_array.shape [ 1 ]` , or maybe a third column in the same array .
It is important to say that I already have a function that returns a distance between two points ( two coordinate pairs ) , but I don't know how to apply it with a single array operation instead of looping through row pairs .
How to shift a pandas MultiIndex Series ?
In a regular time series you can shift it back or forward in time .
We can shift it with : #CODE
These values are median values I calculated from elsewhere , and I have also their variance and standard deviation ( and standard error , too ) .
but not sure how to proceed from here , and how to join the new column back to the original data frame .
Based on the values in two columns merge values in other columns
One way I am thinking of is to use nested loops : outer loop read the lines sequentially and the inner loop reads all lines from the begining and look for map .
All this does is ( 1 ) sort the first two columns so that ` e c ` becomes ` c e ` , ( 2 ) group the terms by ` col ` and ` col 2 ` , and then aggregate ( ` agg `) ` col3 ` and ` col4 ` by comma-joining the sorted set of the flattened terms .
You can compress to the zlib format instead using ` zlib.compress ` or ` zlib.compressobj ` , and then strip the zlib header and trailer and add a gzip header and trailer , since both the zlib and gzip formats use the same compressed data format .
The zlib header is fixed at two bytes and the trailer at four bytes , so those are easy to strip .
Then you can prepend a basic gzip header of ten bytes : `" \x1f\x 8b \x0 8\ 0\0\0\0\0\0\xff "` ( C string format ) and append a four-byte CRC in little-endian order .
I thought that adding a column of row numbers ( ` df3 [ ' rownum '] = range ( df3.shape [ 0 ])`) would help me select out the bottom-most row for any value of the ` DatetimeIndex ` , but I am stuck on figuring out the ` group_by ` or ` pivot ` ( or ??? ) statements to make that work .
Unfortunately , I don't think Pandas allows one to drop dups off the indices .
If you DO want a copy , you can ( in general ) use the copy method or , ( in this case ) use truncate : #CODE
pandas : apply function to DataFrame that can return multiple rows
I am trying to transform DataFrame , such that some of the rows will be replicated a given number of times .
One possibility might be to allow ` DataFrame.applymap ` function return multiple rows ( akin ` apply ` method of ` GroupBy `) .
How I do find median using pandas on a dataset ?
You could try applying your own median function to see if you can work around the cause of the error , something like : #CODE
Here is a different approach , you can add the median back to your original dataframe , the median for the metric column becomes : #CODE
Wether its useful to have the median of the group attached to each datapoint depends a bit what you want to do afterwards .
I created a DatetimeIndex and I want to resample the data with that index .
Any idea how to resample by an index ?
AFAIK you cannot pass in a DatetimeIndex to resample .
As a workaround , just resample by the freq alias ( ' 1Min ') and then reindex to your generated index ?
I started a github issue to maybe think about adding in additional parameters to resample .
Wes replied saying he plans to extend ` resample ` like this eventually .
Is there any work around I could apply before it is fixed ?
Or use a boolean mask : data.A [ data.index.get_level_values ( 1 ) == 2 ] = 0
I've played around with groupby and transpose to no avail , any tips would be great appreciated .
I suppose I could loop through and append to the DataFrame , however I feel like there should be a much smarter method to doing this .
You can use the ` pivot ` function : #CODE
Could you upload an example of when it saves with the graph cut off ?
Merge Columns within a DataFrame that have the Same Name
How do I stack two DataFrames next to each other in Pandas ?
and I would like to stack them next two each other in a single DataFrame so I can access and compare columns ( e.g. High ) across stocks ( GOOG vs . AAPL ) ?
Have a look at the ` join ` method of dataframes , use the ` lsuffix ` and ` rsuffix ` attributes to create new names for the joined columns .
And I want to do a rolling average for all columns , after groupby ` STK_ID ` , the rule expressed by pseudocode like : #CODE
not exactly as expanding_mean() , for the rolling window depends on the RPT_Date and is range from 2-5 periodically .
I notice Pandas can apply different function to different column by passing a dict .
But I have a long column list and just want parameters to set or tip to simply tell Pandas to bypass some columns and apply ` my_func() ` to rest of columns ?
One simple ( and general ) approach is to create a view of the dataframe with the subset you are interested in ( or , stated for your case , a view with all columns except the ones you want to ignore ) , and then use APPLY for that view .
I believe the appropriate way is to write a function that takes the current row , then figures out the previous row , and calculates the difference between them , the use the ` pandas ` ` apply ` function to update the dataframe with the value .
i.e. instead of diff ( 1 ) is there something like value ( 1 ) or value ( 1:3 ) .mean() .
To get the ultimate perf you'd want to drop down into C or Cython and build the raw byte string yourself using C string functions .
How to apply condition on level of pandas.multiindex ?
I.e. , I would like to apply np.mean over all counts of the detectors of 1 channel at each time separately .
Other aggregation functions can be passed via ` agg ` : #CODE
Be carefull with floats groupby ( this is independent of a MultiIndex or not ) , groups can differ due to numerical representation / accuracy-limitations related to floats .
( after diving in Pandas doc , I think ` cut ` function can help me because it's a discretization problem ... but I'm don't understand how to use it )
to plot the results you can use the matplotlib function hist , but if you are working in pandas each Series has its own handle to the hist function , and you can give it the chosen binning : #CODE
In the specific case of applying a ` diff ` function that could be vectorized ( applied like an array operation instead of an iterative pairwise loop ) , is there a way to do that idiomatically in pandas ?
Should I create a " coordinate " class which support the diff ( ` __sub__ `) operation so I could use ` dataframe.latlng.diff ` directly ?
As a follow-up question , how would you go about to apply the same function on groups ?
I suspect that I need to use searchsort and asof , but I am not quite sure how to do that with .
You're looking for a near timestamp , where ` asof ` searches for the latest timestamp .
It is only applied to a time series , so you would have to apply ` reset_index ` to your ` DataFrame `
This can be accomplished quite simply with the DataFrame method ` apply ` .
Now that we have our ` DataFrame ` and ` Series ` we need a function to pass to ` apply ` .
` df.apply ` acts column-wise by default , but it can can also act row-wise by passing ` axis=1 ` as an argument to ` apply ` .
This could be done more concisely by defining the anonymous function inside ` apply ` #CODE
In case of a multicolumn groupby these subgroups refer to several columns , but this is irrelevant as ` len ` counts by the rows in pandas objects .
to find a way to group the data in a way that I can aggregate the time column ` D_Time `
Most efficient way to shift MultiIndex time series
I would suggest you reshape the data and do a single shift versus the groupby approach : #CODE
You can verify it's been lagged by one period ( you want shift ( 1 ) instead of shift ( -1 )): #CODE
( And I did mean shift ( -1 ); it's a hazard rate calculation , so it's forward-looking . )
How do I resample / align a pandas timeseries to the closest calendar quarters ?
Also , is there any way I can get it to align to Dec / Mar ( which seem to be closer to the original dates ) with the timeseries functions ?
I know no easy solution to get to align to the closest and I find the current version quite logical .
But with ` label= ' left '` you can achieve what you want with the current data , still it doesn't align to the closest , so overall you probably have to figure out something else ( like using apply to change the dates so they would conform as you wish ) .
` lambda L : L.split ( ' , ')` - not join again ...
I rewrote the answer to remove the map function as it was more confusing than helpful . thank you for your answer
Using resample to align multiple timeseries in pandas
The goal is to align the data to calendar quarter markers so the 3 data sets can be compared .
just edited the question and added desired final output . the goal ( if it doesn't go without saying ) is to get this programmatically , so adjusting different resample params for each series in some non-automatically detectable way is unfortunately not helpful
To get the dtypes we'd need to transform this ndarray into a structured array using view : #CODE
The above gives me what I want , aggregate stats on julian day of the year BUT I would then like to reorder the group so the last half ( 183 days ) is placed in front of the 1st half .
Why not just reindex the result ?
I think the easiest way to do this is to ` join ` on index .
I've got some radar data that's in a bit of an odd format , and I can't figure out how to correctly pivot it using the pandas library .
Note that I did not set ` loc ` as the index yet so it uses an autoincrement integer index .
However , if your data frame is already using ` loc ` as the index , we will need to append the ` time ` column into it so that we have a loc-time hierarchal index .
This can be done using the new ` append ` option in the ` set_index ` method .
So I edited my answer to suggest that you use the ` append=True ` option while adding ' time ' into your existing ' loc ' index .
Now to extract a specific Series by the ` loc ` , since ` loc ` is an index in ` df_by_speed ` , it is as simple as ` df_by_speed.ix [ ' A ']` where A is the location name .
You can use the pivot method here : #CODE
If I transpose the input to model.predict , I do get a result but with a shape of ( 426,213 ) , so I suppose its wrong as well ( I expect one vector of 213 numbers as label predictions ): #CODE
I tried the separate date and time route , and created a multiindex , but when I did , I ended up with two index columns -- one of them containing the proper date with an incorrect time instead of just a date , and the second one containing an incorrect date , and then a correct time , instead of just a time .
The input data for my multiindex attempt looked like this : #CODE
Maybe the multiindex approach is totally wrong , but it's one thing I tried .
GroupBy objects also have an apply method , which is basically syntactic sugar around the " combine " step done with pd.concat() above .
Benefits of panda's multiindex ?
I found a way using ` join ` : #CODE
This is not so direct but I found it very intuitive ( the use of map to create new columns from another column ) and can be applied to many other cases : #CODE
Thanks , the map method seems pretty powerful .
I would like to apply a function to a dataframe and receive a single dictionary as a result . pandas.apply gives me a Series of dicts , and so currently I have to combine keys from each .
I had overlooked ` map ` completely , and my rewritten function is much cleaner now .
pandas pivot dataframe to 3d data
There seem to be a lot of possibilities to pivot flat table data into a 3d array but I'm somehow not finding one that works : Suppose I have some data with columns =[ ' name ' , ' type ' , ' date ' , ' value '] .
When I try to pivot via #CODE
` pivot ` only supports using a single column to generate your columns .
You probably want to use ` pivot_table ` to generate a pivot table using multiple columns e.g. #CODE
The hierarchical columns that are mentioned in the API reference and documentation for ` pivot ` relates to cases where you have multiple value fields rather than multiple categories .
However , if you want separate columns for different value fields for the same category ( e.g. ' type ') , then you should use ` pivot ` without specifying the value column and your category as the columns parameter .
Thanks , I guess the pandas philosophy is that you should probably work with your 3d data in a 2d multiindex array ... probably not a bad idea once you get used to indexing and slicing on multiindexes .
it is probably more intuitive / readable to stick to using stack / unstacks and groupby as in the other solution below .
` pandas.append() ` ( or ` concat() ` method ) can only append correctly if you have unique column names .
However , my goal is to be able to use a row-wise function in the ` DataFrame.apply() ` method ( so I can apply the desired functionality to other functions I build ) .
Row-wise functionality should be possible with apply .
If you have a key that is repeated for each row , then you can produce a cartesian product using merge ( like you would in SQL ) .
This won't win a code golf competition , and borrows from the previous answers - but clearly shows how the key is added , and how the join works .
So , I created a list of all the weeks I wanted to have , then a list of all the store IDs I wanted to map them against .
The merge I chose left , but would be semantically the same as inner in this setup .
` combine_first ` is not actually an ` append ` operation .
while ` append ` is #URL
What would be a way to read this file and align the date / values ?
Just replace `' / Users / spencerlyon2 / Desktop / test.csv '` with the path to your file
How can I replace all the NaN values with Zero's in a column of a pandas dataframe
I have also looked at this article How do I replace NA values with zeros in R ?
Also , this is a complex example ( though I really ran into it ) , but the same may apply to fewer levels of indexes depending on how you slice .
It's one line , reads reasonably well ( sort of ) and eliminates any unnecessary messing with intermediate variables or loops while allowing you to apply fillna to any multi-level slice you like !
Merge parameters for Pandas
I have a loop in Python which sequentially imports CSV files , assigns them to a temporary DataFrame object and then attempts to merge / concact them to a ' master ' DataFrame .
The MLS_Stats DF is initially empty , which is the reasoning for the if loop , since I don't think you can merge a DF with an empty DF .
For each merge , I want build the DataFrame by including any new uniquely indexed rows and new columns , but exclude overlapping columns .
You can filter duplicate rows with ` drop_duplicates ` , and select to join only columns that are not yet present .
You can use the append function to add another element to it .
Only , make a series of the new element , before you append it : #CODE
I believe append returns a new Series ( rather than doing it in place ) so you want ` test = test.append ( pd.Series ( 200 , index =[ 101 ]))`
How to apply a function to two columns of Pandas dataframe
Now I want to apply the ` f ` to ` df `' s two columns `' col_1 ' , ' col_2 '` to element-wise calculate a new column `' col_3 '` , somewhat like : #CODE
can you apply f directly to columns : df [ ' col_3 '] = f ( df [ ' col_1 '] , df [ ' col_2 '])
Here's an example using ` apply ` on the dataframe , which I am calling with ` axis = 1 ` .
Depending on your use case , it is sometimes helpful to create a pandas ` group ` object , and then use ` apply ` on the group .
Yes , i tried to use apply , but can't find the valid syntax expression .
How to use Pandas ' apply ' function to create ' col_3 ' ?
Use apply on the whole dataframe , passing in rows with df.apply ( f , axis=1 ) .
Since you haven't provided the body of f I can't help in anymore detail - but this should provide the way out without fundamentally changing your code or using some other methods rather than apply
pandas's resample with fill_method : Need to know data from which row was copied ?
I am trying to use resample method to fill the gaps in timeseries data .
With resample , I will get this #CODE
align edge ( default ) | center
For vertical bars , align = edge aligns bars by their left edges in left , while align = center interprets these values as the x coordinates of the bar centers .
So adding try adding the keyword align = ' center ' to you first plot call and that might get aligned your x-axis .
As a work-around , I'm probably going to simply import from my target sql and do a join .
Understanding ` MultiIndex ` dataframes in pandas - understanding MultiIndex and Benefits of pandas multiindex ?
If I ` reset_index ` on both frames I can now merge / join them and slice however I want , but how can I do it using the ( multi ) indexes ?
Note that you can use xs on a multiindex .
My approach was to groupby ' file ' and then aggregate on ' first ' : #CODE
I convert this to a sparse series / dataframe with censored observations that I would like to join or convert to a series / dataframe with continuous dates .
I feel as though I'm missing a more elegant solution involving a join .
You can just use reindex on a time series using your date range .
I used ` loffset= ' -1M '` to tell pandas to aggregate one period earlier than its default ( moved us to Jan-Jun ) .
Assuming you had a ` DateTimeIndex ` with regular frequency you could always use ` df.resample ` to aggregate the data at another regular frequency ( like every two months ) and then use ` df.pct_change() ` to get the returns .
Also there are various options for ` pct_change() ` [ see ` periods ` , ` freq `] that allow you to specify how many data points should be used to compute the returns ( ` periods ` defaults to 1 , which is why the solution gave the same answer as your function ) .
You might also want to looking into the rolling and window functions for other types of analysis of financial data .
Because you have a relatively small data set , the easiest way is to resample on the parameters that you need to calculate the data on then use the ` pct_change() ` function again .
However , after you've read it in , you could strip out the whitespace by doing , e.g. , ` df [ " Make "] = df [ " Make "] .map ( str.strip )` ( where ` df ` is your dataframe ) .
I don't have enough reputation to leave a comment , but the answer above suggesting using the map function along with strip won't work if you have NaN values , since strip only works on chars and NaN are floats .
To extract the Series ( and plot ) from the panel you can use ` ix ` with the following syntax : #CODE
pandas : slice a MultiIndex by range of secondary index
not sure if this is ideal but it works by creating a mask #CODE
Use ` ix ` : #CODE
( It's good practice to do in a single ix / loc / iloc since this version allows assignment . )
That said , I kinda disagree with the docs that ix is :
use loc for labels
use ix for both ( if you really have to )
It feels like there ought to be a way to do this in one pass ( using loc / without chaining ) , however assignment ( ` s [ ' b '] .ix [ 1:10 ]`) works so I guess it's ok .
Surprisingly ( for me at least ) , although comparable for small Series , this starts to become slower than using ` ix ` when the Series is longer than 250 .
you can also use a mask :
Using a mask is my fallback option , at this point =)
Pandas rolling apply with missing data
I want to do a rolling computation on missing data .
Sample Code : ( For sake of simplicity I'm giving an example of a rolling sum but I want to do something more generic . ) #CODE
I think that during the " rolling " , window with missing data is being ignored for computation .
I think a partial answer to this question is probably via using the keyword argument min_periods in the rolling apply function .
The best way to do this in pandas is to use drop : #CODE
Finally , to drop by index instead of by name , try this to delete , e.g. the 1st , 2nd and 4th columns : #CODE
@USER I don't know of any performance improvement , but readability-wise , ` drop ` is a more SQL-like description of the operation in question .
I think in version 0.16.2 drop by index doesn't work - do nothing .
How to drop rows of Pandas dataframe whose value of certain column is NaN
Don't ` drop ` .
` notnull ` is also what Wes ( author of Pandas ) suggested in his comment on another answer .
Though of course that will drop rows with negative numbers , too .
You could use dataframe method notnull or inverse of isnull , or numpy.isnan : #CODE
You can use groupby and then apply to achieve what you want : #CODE
pandas : slice a MultiIndex DataFrame by range of secondary index
I think you want to ` resample ` your dataframe , but I'm not sure .
My actual goal is to use ` groupby ` , ` crosstab ` and / or ` resample ` to calculate values for each period based on sums / means / etc of individual entries within the period .
In other words , I want to transform data from : #CODE
But you can use ` .shift ` to shift it by any number of days ( or any frequency for that matter ): #CODE
However , this doesn't help with resampling on a range , as resample will still use bins aligned to the beginning of the month AFAIK .
I don't have a simple workaround for you at the moment because ` resample ` requires passing a known frequency rule .
I want to find all values in a Pandas dataframe that contain whitespace ( any arbitrary amount ) and replace those values with NaNs .
I loop through each column and do boolean replacement against a column mask generated by applying a function that does a regex search of each value , matching on whitespace .
And finally , this code sets the target strings to None , which works with Pandas ' functions like fillna() , but it would be nice for completeness if I could actually insert a NaN directly instead of None .
What you really want is to be able to use [ ` replace `] ( #URL ) with a regex ...
I'm pretty new to Pandas , but maybe the map / apply function are what I need ?
Now the last step is to apply / map it back to the borough column ... how do I do that ?
Edit : If you've already created your dict as in your edited post , just use ` d [ ' Borough '] = d.City.map ( paired [ ' Borough '])` to map each city to the borough from your dict .
` map ` is a useful method to know about .
It can map values either with a Pandas series , with a dict , or with a function that returns the mapped value given the key .
There are cases when when the same city may be paired with different boroughs , for instance the city ' New York ' is mapped to the Borough Manhattan in like 97% of occurrences , but how does map handle that situation ?
Another way of doing this could be to join these data frames , it will remove the non-matching entries and I can drop count column afterwards .
i usually just use a mask and then select the cols i need
a cute one-liner : data [ data.groupby ( ' tag ') .pid .transform ( len ) > 1 ]
Ah ha , currently you can do : ` g.filter ( lambda x : len ( x ) > 1 , dropna=False ) .dropna() ` to keep the order .
Mind you , nor does transform .
Of course I can just manually replace the truncated words , but I'm curious to know what the cause is ?
I'm also puzzled why the ` apply ` version along ` axis=1 ` is so much slower .
This isn't exclusive to Pandas ; in general , any numpy operation will be much faster if you treat arrays and matrices in aggregate instead of calling Python functions or inner loops on individual items .
You'd need to override the ` apply ` and ` onOffset ` methods to take into account your holiday calendar .
If that's true , I guess I have to figured out how to properly convert my dataframe into a timeseries that I can resample .
I am able to get the desired result by converting my dataframe to a timeseries using this command : " ts = pd.TimeSeries ( df [ 0 ])" , and then I can resample the timeseries .
You can resample over an individual column ( since each of these is a timeseries ): #CODE
Update : A useful workaround is to just smash this with the DatetimeIndex constructor ( which is usually much faster than an apply ) , for example : #CODE
In 0.15 this will be vailable in the dt attribute ( along with other datetime methods ): #CODE
With more complicated selections like this one you can use ` apply ` : #CODE
problems with apply function in pandas after update
I tried to apply ' manually ' the function recursively to see if some of the dates passed as the x parameter in the lambda definition where wrong , but managed to get correct results any time .
But the ` apply ` method just seem not to work anymore , and cannot understand why .
If you unstack STK_ID , you can create side by side plots per RPT_Date .
I think the OP's primary concern is with the division , not the shift .
` shift ` realigns the data and takes an optional number of periods .
Aah , shift is what i needed .
Python pandas insert long integer
I'm trying to insert long integers in a Pandas Dataframe #CODE
If I used ` cmov_mean ` in ` scikits.timeseries ` , what should I use when I " resample " in pandas ?
When I " resample " my daily averages to monthly and then plot both , I notice a big time offset .
I need to join it with some reference tables that I have access via a pyodbc connection .
Merge of multiple data frames of different number of columns into one big data frame
Also what if instead two CSV files I had two data frames and wanted to do the same , for example if I loaded first csv to ` df1 ` and second one in ` df2 ` and then wanted to make a merge to ` df3 ` that would look like example above .
Why not try the ` concat ` function : #CODE
Note : the ` concat ` does have some additional options if you have slightly different requirements .
The ` and ` and ` or ` operators are special in Python and don't interact well with things like numpy and pandas that try to apply to them elementwise across a collection .
Using the transpose allows you to select rows as df [ " AA " : " AA "] which then return a MultiIndex DataFrame ( not losing information ) , however , df.xs ( " AA " , axis=1 ) returns a DataFrmae with a single level Index ( thus losing information ) .
However , if I do the same in a MultiIndex column DataFrame , then I get a crash .
build dataframes for each user and concat , very clever !
Finally , if you don't like the way the frame looks you can use the transpose function of panel to change the appearance before calling to_frame() see documentation here
How to keep MultiIndex when using pandas merge
A similar question was asked in How to keep index when using pandas merge , but it will not work with MultiIndexes , i.e , #CODE
How can one make the merge while preserving the MultiIndex in the left dataframe ?
( It would have to be exactly there , if the shift was detected a step later , wouldn't matter . )
Actually , many of DataFrameGroupBy object methods such as ( apply , transform , aggregate , head , first , last ) return a DataFrame object .
yes , df1 + df2 will try and align the columns .
you should convert your time steps to a ` DatetiemIndex ` and than resample R2
I would try to copy the original table , drop columns that differ ( ` result ` and ` run `) , reindex that , combine both things again with the new index as multi-index and then run the mean method for that outer multi-index level .
I know that the returned pivot is a dataframe so I can calculate it through other means , but just curious if there is a more efficient way .
Define the function you want to apply .
Then , apply it .
Was wondering if the pandas pivot had any similar built in functionality .
Efficient way to apply multiple filters to pandas DataFrame or Series
I have a scenario where a user wants to apply several filters to a Pandas DataFrame or Series object .
I want to take a dictionary of the following form and apply each operation to a given Series object and return a ' filtered ' Series object .
The input I receive is a dictionary defining what filters to apply .
My example could do something like ` df [( ge ( df [ ' col1 '] , 1 ) & le ( df [ ' col1 '] , 1 )]` .
Maybe I could add each intermediate boolean array to a big array and then just use ` map ` to apply the ` and ` operator to them ?
The call to ` reindex_like() ` inserts some NaN data into the series so the ` dtype ` of that series changes from ` bool ` to ` object ` .
is it possible to do fuzzy match merge with python pandas ?
I have two DataFrames which I want to merge based on a column .
However , due to alternate spellings , different number of spaces , absence / presence of diacritical marks , I would like to be able to merge as long as they are similar to one another .
I want to merge on similar values between two DataFrames
I would just do a separate step and use difflib getclosest_matches to create a new column in one of the 2 dataframes and the merge / join on the fuzzy matched column
Could you explain how to use ` difflib.get_closest_matches ` to create such a column and then merge on that ?
Similar to @USER suggestion , you can apply ` difflib ` ' s ` get_closest_matches ` to ` df2 `' s index and then apply a ` join ` : #CODE
If these were columns , in the same vein you could apply to the column then ` merge ` : #CODE
Instead of directly applying ` get_close_matches ` , I found it easier to apply the following function .
Then you can insert it again into your DataFrame .
I used this and DataFrame.apply to apply it to all major columns in the dataframe .
map ( lambda x : ( x [ 3 ] , [ int ( x [ 1 ]) , int ( x [ 2 ])]) , [ line.split() ])
You can either load the file and then filter using ` df [ df [ ' field '] constant ]` , or if you have a very large file and you are worried about memory running out , then use an iterator and apply the filter as you concatenate chunks of your file e.g. : #CODE
I realize Dataframe takes a map of { ' series_name ' : Series ( data , index ) } .
However , it automatically sorts that map even if the map is an OrderedDict() .
or just concat dataframes #CODE
I create a fresh dataframe with the appropriate index , drop the data to a dictionary , then populate the new dataframe based on the dictionary values ( skipping missing values ) .
Resample Searies / DataFrame with frequency anchored to specific time
Which I want to resample to say ' 5s ' .
I was pleased to see that this method also works with the replace function .
i'd use the pandas replace function , very simple and powerful as you can use regex .
numpy diff on a pandas Series
Pandas implements ` diff ` like so : #CODE
Because np is taking ` np.asanyarray() ` of the series before finding the ` diff ` .
I am attempting to left merge two dataframes , but I am running into an issue .
I noticed some strange behavior when using IX on large pandas dataframes .
Yes , ` ix ` caches results .
How do I do the equivalent of ` df [ df [ " B "] == 2 ]` , if ` B ` is the name of a level in a ` MultiIndex ` instead ?
Swapping the name I want to use to the start of the ` MultiIndex ` and then use lookup by index : ` df.swaplevel ( 0 , " B ") .ix [ 2 ]`
Note : it is not equal to ` dt ` because it's become " offset-aware " : #CODE
Think of np.datetime64 the same way you would about np.int8 , np.int16 , etc and apply the same methods to convert beetween Python objects such as int , datetime and corresponding numpy objects .
You can use the ` resample ` method ( #URL ) if you have a time series ( if the time is used as the index ): #CODE
Feels to me like the trick is to select all unique dates present , create 24 hour time samples for each of these dates and merge the two sets .
OK , then indeed , the easiest way will probably be to create an index object with the date range you want and to reindex the dataframe .
This gives me the error for serie_5 ( the second concat ): #CODE
Another way is to use join : #CODE
just create a Python list and append your Series into it and then provide it to pandas.concat as @USER was writing above .
The use of join looks generic enough !
I wish to merge the values from b into a .
Apply can also do aggregation and other things
Because my lists are large , is there any way not to load the full list into memory but , rather , efficiently append values one at a time ?
` append ` is a wrapper for ` concat ` , so ` concat ` would be marginally more efficient , but as @USER says Pandas is probably not appropriate for updating a HDF5 file every second .
If memory is no constraint just preallocate a huge zeros array and append at the end of the program removing any excess zeros .
Another option is to use messaging to transmit from one process to another ( and then append in memory ) , this avoids the serialization issue .
` tz ` means time zone and ` Not Windows ` and ` Windows ` are categories extracted from the User Agent in the original data , so we can see that there are 3 Windows users and 0 Non-windows users in Africa / Cairo from the data collected .
I need to merge these three columns and have the entire date in one column for all the rows .
You are looking for ` apply ` ( ` merge ` is like a database join . ): #CODE
merge two dataframe and create a new one with multiindex
I would like to merge the two dataframe in a new dataframe with a multi-index like :
I tried to reindex like this but it doesn't work : #CODE
Anybody knows how to reindex this dataframe to have the index levels in sorted order ?
I am trying to add a column of smaller ` len ` into a ` DataFrame ` where indexes of smaller item are a subset of a larger item .
You are looking for an outer ` join ` , here is a simple example : #CODE
The pandas implementation uses a rolling window of the previous n values , which is how it's usually done in finance ( see this Wikipedia entry for simple moving average ) .
` len ( np.arange ( 12 ))` and ` len ( pd.stats.moments.rolling_mean ( np.arange ( 12 ) , 6 ))` both equal 12 as I would have expected - what result were you expecting ?
I have faced this with rolling statistics in pandas , too .
I'd say for non-time-related measurements , such as an altitude vs . distance profile , a central-based moving window makes more sense , since it does not introduce lag or shift .
If you use a pandas Series rather than a list , you can use its ` diff ` method : #CODE
One " built-in " way to accomplish it might be accomplished using ` shift ` twice , but I think this is going to be somewhat messier ...
I assume what you are trying to do is change the frequency of a Time Series that contains data , in which case you can use ` resample ` ( documentation ) .
Then you can change the frequency to seconds using resample , specifying how you want to aggregate the values ( mean , sum etc . ): #CODE
Update : if you're doing this to a DatetimeIndex / datetime64 column a better way is to use ` np.round ` directly rather than via an apply / map : #CODE
Hence you can apply this to the entire index : #CODE
I have corrected this and added how to apply this to the entire dt_index .
Think I figured out the second part : sp500 [ " regression "] = exp ( sm.OLS ( log ( sp500 [ " Adj Close "]) , sm.add_constant ( range ( len ( sp500.index )) , prepend=True )) .fit() .fittedvalues )
You might also want to report bugs on github issues rather than stack overflow .
You are looking for a ` merge ` : #CODE
The keywords are the same as for ` join ` , but ` join ` uses only the index , see " Database-style DataFrame joining / merging " .
If you try and join on a column you get an error : #CODE
It is giving me an error when i run merge : - UnicodeDecodeError : ' ascii ' codec can't decode byte 0xc2 in position 268 : ordinal not in range ( 128 )
The ` for ` loops and ` append ` s will not be efficient and should be avoided .
Try rewrting these using numpy functions and / or the DataFrame ` apply ` method ...
Also , would you agree then , using your suggestion , if we want to apply a function / algorithm restricted every unique date in the file one should just groupby the ' datetime ' object ?
As commented , in newer pandas , Series has a ` replace ` method to do this more elegantly : #CODE
( been a while since I wrote this ! ) replace definitely best option , another is to use ` .apply ( { ' March ' : 0 , ' April ' : 1 , ' Dec ' : 3} .get )` :) In 0.15 we'll have Categorical Series / columns , so the best way will be to use that and then sort will just work .
@USER I've taken the liberty of replacing the second line with the ' replace ' method .
A bit late to the game , but here's a way to create a function that sorts pandas Series , DataFrame , and multiindex DataFrame objects using arbitrary functions .
This also works on multiindex DataFrames and Series objects : #CODE
some problem with ix and loc , the pandas documentation could be clearer
When you do ` len ( df [ ' column name '])` you are just getting one number , namely the number of rows in the DataFrame ( i.e. , the length of the column itself ) .
If you want to apply ` len ` to each element in the column , use ` df [ ' column name '] .map ( len )` .
I came up with a way using a list comprehension : ` df [[( len ( x ) < 2 ) for x in df [ ' column name ']]]` but yours is much nicer .
To directly answer this question's title ( which I understand is not necessarily the OP's problem but could help other users coming across this question ) one way to do this is to use the drop method :
Finally doing pivot table and putting the pivots into a dataframe and plotting the dataframe in bar mode created the necessary graphs .
I think the best way is to merge all dataframes together , then you could use all nice Panda functions to slice and mix-and-match anyway you want .
I would merge them like this : #CODE
You could of course use stack / unstack to structure your DataFrame a bit different depending on the amount of data and the way you will be using it most .
Pandas join grouped and normal dataframe
I need to join levels with lines ( atom , ion , level ): at first on atom , ion , level_number_upper and then atom , ion , level_number_lower .
Is there a way to precompute the join - memory is not an issue , but speed is .
To show what I want to join merge here a code snippet #CODE
and then I want to join / merge grouped data with lines on atomic_number and ion_number
Why not join / merge first then do the groupby ?
It would cost a lot of performance to do the join / merge before the groupby .
Just to confirm , are you are wanting to merge / join a groupby object with a dataframe ?
Well no - I want to join / merge the result .
pandas ' transform doesn't work sorting groupby output
Good enough , but then I wanted to use pandas ' transform to do the same like this : #CODE
I know that transform requires to return an array of the same dimensions that it accepts as input , so I thought I'd be complying with that requirement just sorting both slices ( smokers and non-smokers ) of the original DataFrame without changing their respective dimensions .
` transform ` is not that well documented , but it seems that the way it works is that what the transform function is passed is not the entire group as a dataframe , but a single column of a single group .
I don't think it's really meant for what you're trying to do , and your solution with ` apply ` is fine .
The transform does not call ` func ( group1 )` and ` func ( group2 )` .
This makes sense if you think about what transform is for .
It's meant for applying transform functions on the groups .
For instance , the example in the pandas docs is about z-standardizing using ` transform ` .
You have to z-standardize the age with respect to the mean age and the weight with respect to the mean weight , which means you want to transform separately for each column .
So basically , you don't need to use transform here .
` apply ` is the appropriate function here , because ` apply ` really does operate on each group as a single DataFrame , while ` transform ` operates on each column of each group .
Why is transform so poorly documented ?
I dont think you have it quite right though ( although I have no idea how it in fact is working ) as when you put print statments in you functions it seems clear that transform is actually passing columns as series and data frames .
Its really wierd and I want to understand exactly what is going on behind the scenes but can find no information on how transform is in fact implemented .
by setting ` index_col =[ 0 , 2 , 4 ]` you are creating a MultiIndex that's why you get that output .
Just read single and merge the dataframes
Try to convert the ' sales ' string to an ` int ` , if it is well formed then it goes on , if it is not it will raise a ` ValueError ` which we catch and replace with the place holder .
If it's already in the DataFrame you could use ` apply ` to convert those strings which are numbers into integers ( using ` str.isdigit ` ): #CODE
Although , ` apply ` is the important bit of my answer ( weirdly no other answers seem to use it ) .
Although Chang's answer explains how to plot multiple times on the same figure , in this case you might be better off in this case using a ` groupby ` and ` unstack ` ing :
In pandas it is called ' expanding ' instead of cumulative I think :
@USER only a multiindex has a levels attribute .
This will append the correct values but does not update the index properly and the graph is messed up .
If you want to plot the bars of all columns and the mean you can ` append ` the mean : #CODE
Have you seen / tried the built-in rolling mean function ?
For example , say you want to pivot the data so there are separate columns for
a function of your creation ) can easily be applied to the columns of ` pivot ` .
pd.rolling_mean() , like all rolling / moving functions in pandas , even accepts a ` center ` parameter for centered sliding windows .
Non standard interaction among two tables to avoid very large merge
However , standard solutions using merge / join would take too much RAM in the long run .
The last major improvement I can think of would be to replace df.apply() with a for loop to avoid calling any function 200M times ( or however large A is ) .
This is slightly faster than original , but there is till a lot of overhead ( for me 4ms for the merge and 2.5ms for the second line ) .
I was under the impression that apply was preferable to loops .
I would like to read in a csv file iteratively , append each chunk into HDFStore object , and then work with subsets of the data .
If you replace that line with :
or you can reindex afterwards #CODE
apply on group replicating complete MultiIndex
If I were to export to a list then I could use the numpy's ` interp1d ` function and apply this to the missing values .
I'm surprised you accepted the answer so fast ( no offense , hayden ;) because I thought you especially wanted to interpolate time series , but I guess you didn't mean exactly pandas.TimeSeries .
How do you apply that to one column only ( i.e. ' data1 ')
I'm a newbie to pandas dataframe , and I wanted to apply a function to each column so that it computes for each element x , x / max of column .
Pandas DataFrame : apply function to all columns
They also fail if I drop the index on both stacked DataFrames ( e.g. , do ` wstk.reset_index ( inplace=True )` before the join ) .
Trying to use the awfully useful pandas to deal with data as time series , I am now stumbling over the fact that there do not seem to exist libraries that can directly interpolate ( with a spline or similar ) over data that has DateTime as an x-axis ?
) as x-argument , interestingly , the Spline class does create an interpolator , but it still breaks when trying to interpolate / extrapolate to a larger DateTimeIndex ( which is my final goal here ) .
To simulate different consumption rates , replace all real outbound timestamps
How to apply function to date indexed DataFrame
Then ` apply ` this to each state in the DataFrame : #CODE
Using ` join ` or ` merge ` works too : #CODE
Pandas interpolate changed in version 0.10 ?
Calling the column's ` interpolate ` method as below is the correct way .
Dataframe merge creates duplicate records in pandas ( 0.7.3 )
When I merge two CSV files , of the format ( date , someValue ) , I see some duplicate records .
`' viridis '` ( will be default color map in 2.0 )
I would suggest the ` cubehelix ` color map .
So your requirements are " lots of colors " and " no two colors should map to the same grayscale value when printed " , right ?
The white line is the luminance of each color , so you can see that each color will map to a different grayscale value when printed .
Assuming Y is a column in your dataframe , one way is to use ` diff ` and cumsum : #CODE
diff returns timedelta objects now in pandas master .
But I do not see how to ' normalize ' fx and fy so that they have the same levels and ` fx.lables ` and ` fy.lables ` have the same coding .
For example , I know that every data point has to take one of the five values [ a , b , c , d , e ] and I already have an index ` Index ([ a , b , c , d , e ] , dtype =o bject )` and I want to factorize vector y =[ ' a ' , ' c ' , ' e '] into a Categoricial variable with ` Index ([ a , b , c , d , e ] , dtype =o bject )` as its levels .
In [ 6 ] , it should be range ( len ( fx.levels )) .
You can ` unstack ` the groupby : #CODE
I used map instead .
The problem is that no two TimeSeries have the exactly the same index , i.e. I would need to merge all the TimeSeries ' indexes .
However , an exception is thrown when attempting to aggregate the TimeSeries into a DataFrame and I believe it has to do with the duplicate index elements : #CODE
pd.concat() performs an ' outer ' join on the indexes by default and holes can be filled by padding forwards and / or backwards in time .
Python pandas resample added dates not present in the original data
For some reason ` resample ` added rows for days that were not present in the intraday data .
Prior to 0.10.0 , pandas labeled resample bins with the right-most edge , which for daily resampling , is the next day .
` resample ` converts to a regular time interval , so if there are no samples that day you get NaN .
Because I need to be able to rely on it that importing , say , a random number module , won't silently change , say , the pickle module to apply a random salt to everything it writes ..
Pandas slicing along multiindex and separate indices
I can slice on multiindex , but I give it a raw index and it gives me back a tuple .
Hopefully there is a better way than ` np.array ( map ( np.array , df.index.values ))` ( ! )
For now I think i will just do my_index = Series ( arange ( len ( df )) , index=myselectedindex )
Python - rolling functions for GroupBy object
Is there any way to apply rolling functions to ` groupby ` objects ?
How exactly do you expect rolling function to work on grouped objects ( I mean write out the math you want to do in symbols ) ?
Yes , ideally cumsum and any rolling function ( mean , sum , std ) .
I think you could apply any cumulative or " rolling " function in this manner and it should have the same result .
If you are creating a timeseries , you can use the ` tz ` argument of ` date_range ` : #CODE
( Note that I don't actually use ` date_range() ` so using its ` tz ` parameter is not an option . )
Update : In recent pandas , you can use the dt accessor to broadcast this : #CODE
Here's one way ( depending if tz is already set it might be a ` tz_convert ` rather than ` tz_localize ` ): #CODE
If you take the first and last duplicate value of each year and shift the data in-between by an hour , that should be the easiest way of correcting the issue .
I tried using merge and join but I am not sure how to go about getting the desired result .
If the dicts contain both numerical and string values , then you could combine them using a join , followed by a groupy and aggregation .
In that case , I would like to know if there is a way to add only the numbers and append the ' D ' values as a list
What do you want col1 and col2 to look like after you pivot ?
I am using the groupby and sum to quickly aggregate accros two data sets
However , I try using ` C = concat ([ A , B ])` and now find that I only have the column shares as an index and cannot group by sequence .
Pass the ` axis ` option to the ` apply ` function : #CODE
Does apply pass the columns including item1 , item2 when I use axis=0 ?
` ix `' s main purpose is to allow numpy like indexing with support for row and column labels .
I don't think ` ix ` supports negative indexing at all .
You can resample the data to business month .
If you don't want the mean price ( which is the default in ` resample `) you can use a custom resample method using the keyword argument ` how ` : #CODE
@USER Don't know which method is preferred : normally I would use ` resample ` .
` resample ` should work , not sure about advantages of ` asfreq ` .
The ix notation allows you to slice columns .
Now we have renamed the columns so they will align the way we want .
Since pandas in it's current form assumes time series data are arranged with time in the index , not the columns , transposing the DataFrame , at least temporarily , will enable the use of many built-in methods , such as ` shift ` / ` diff ` / ` pct_change ` / etc .
You can use the DataFrame ` apply ` method : #CODE
For each date , when there is an ' S ' in the signal column append the corresponding price at the time the ' S ' occurred .
That is , " give me the price at 1620 ; ( even when it doesn't give a " sell signal " , S ) so that I can diff . with the " extra B's " -- for the special case where B > S .
` from numpy import * ` will replace the builtin ` any ` with ` numpy `' s any , which doesn't handle genexps , and so ` any (( False for i in range ( 3 ))) == True ` ; ` from os import * ` will replace ` open ` with ` os.open ` , and so most ` open ` calls will return ` TypeError : an integer is required ` ; and so on .
I want to do it so that I can call reindex and fill in the dates between those listed in the table .
Do I have to write an algorithm which is then going to go back and say on User 2 , insert the following entry ( 6 , 35 ) .
You may find it faster to extract the index as a column and use ` apply ` and ` bfill ` .
I'm a bit confused , first you append something to cash then you reassign it to an empty series .
Update : in 0.15 you will have access to a dt attribute for datetimelike methods : #CODE
Here's one ( slow ! ) workaround to do it using ` apply ` , not ideal but it works : #CODE
It seems like a bug ( that you can't do ` apply ( lambda x : x.month )`) , perhaps worth adding as an issue on github .
This happens when using apply as well #CODE
How to apply quantile to pandas groupby object ?
Then we make a copy , and use tril_indices_from to get at the lower indices to mask them : #CODE
` map ` before ( or even after ) the ` zip ` ?
Indeed , they are all " numbers " . apply ( float ) for some reason was rejected w / ValueError : could not convert string to float : price .
Merge multi-indexed with single-indexed data frames in pandas
How can I merge the two data frames with only one of the multi-indexes , in this case the ' first ' index ?
Note : you are almost doing a ` join ` here ( except the df1 is MultiIndex ) ...
you can * nearly * merge like this : ` df1.merge ( df2 , left_on =d f1.index.get_level_values ( ' first ') , right_on =d f2.index.get_level_values ( ' first '))`
The mnemotechnic for what level you have to use in the reindex method :
According to the documentation , as of pandas 0.14 , you can simply join single-index and multiindex dataframes .
You can replace ` nan ` with ` None ` in your numpy array : #CODE
Unfortunately neither this , nor using ` replace ` , works with ` None ` see this ( closed ) issue .
The way I currently do it is that I reindex ` df ` with every second in the year and use forward filling like : #CODE
These are the lines I tried to replace the True and False , and got a dataframe filled with all True values : #CODE
` applymap() ` can be used to apply a function to every element of a ` dataframe ` #CODE
It seems to be because the type of the objects in the ` DataFrame ` is actually ` numpy.bool_ ` , not Python's ` bool ` .
I am having a real strange behaviour when trying to reindex a dataframe in pandas .
and then I try to reindex inside a larger date range : #CODE
I get strange behaviour when trying to reindex the dataframe .
If I reindex one larger part of the dataset I get this error : #CODE
Is there a way to replace the ' nan ' label with "" in the x-axis ?
It's good practice to do that first before you apply .
Is there a shorter way of dropping a column MultiIndex level ( in my case , basic_amt ) except transposing it twice ?
For the rows which have ' ' in front I want to cut that and move into column C before the ' = ' sign .
If I use normal slice it will cut values where there is no ' ' sign .
And ` startswith ` does not work on float values .
If you want to use ` startswith ` with a float , you can just first convert it to a str with str() .
Then you can use Series ` apply ` with this function : #CODE
I know I could resample the prices and fill in the details ( ` ffill ` , right ? ) , but that doesn't seem like such a nice solution , because I have to assume the frequency I'm going to be indexing it at and it reduces readability with too many unnecessary data points .
Check ` asof ` #CODE
I'm trying to merge a series of dataframes in pandas .
I have a list of dfs , ` dfs ` and a list of their corresponding labels ` labels ` and I want to merge all the dfs into 1 df in such that the common labels from a df get the suffix from its label in the ` labels ` list .
I'm trying to make a series of merges that at each merge grows at most by number of columns N , where N is the number of columns in the " next " df in the list .
Unionize the non-common column names ( as in outer join ) .
Here is my attempt at an implementation of this , which does not handle suffixes but illustrates the kind of merge I'm looking for : #CODE
New columns are added in an outer-join style , but columns that are common ( and not part of the index ) are used in the join via the ` on= ` keyword .
Is the above merge operation something that can be done more elegantly in pandas or that already exists as a builtin ?
The cardinality of the " join space " is the issue and may need to be worked around .
@USER : Yes , I want an outer join but I want it to use the indices of the left and right df .
I can explain what's unclear if you let me know what - I basically want an outer merge that adds columns together based on a unique index
On the other hand , you may want to look into an operation like ` pd.concat ([ df1 , df2 , df3 ] , keys =[ ' d1 ' , ' d2 ' , ' d3 '] , axis=1 )` , which produces a dataframe with MultiIndex columns .
Can you explain though why this does not cause combinatorial issues while merge does ?
From the documentation , this seems like an outer join on an index with merge , but they behave very differently ...
Good question -- it seems to be a limitation of the current merge implementation .
` Groupby ` the ID and apply the lambda function ` diff() .sum() ` to each group .
Use ` transform ` instead of ` apply ` because ` transform ` returns an indexed series which you can use to assign to a new column ' diff ' .
very smart , you changed diff ( 1 ) to diff ( -1 ) so that diff would be taken between i and i-1 , but then the signs were all negative , hence -diff ( -1 ) .
i.e. , what if the row with shift == -560 was bad ?
Here's a solution to separately aggregate each contiguous block of bad status ( part 2 of your question ? ) .
As @USER comments this fails for NaN's and isn't particularly robust either , in practise using something similar to @USER ' s answer is probably recommended ( Note : we want a bool rather than raise if there's an issue ): #CODE
@USER you're right you want to be using assert_frame_equal afterwards ( I don't think pandas exports a similar ) , although beware of using it from quant's answer as that can raise ( rather than return bool ) .
@USER I think you want to do something similar to quants but return a bool , have included a recipe .
Instead merge , join , or concatenate them into a single ` DataFrame ` beforehand as pandas gives you multiple ways of doing so .
I think you are going to be better doing a ` concat ` of these DataFrames before exporting ( via ` to_excel ` ) .
Unfortunately I think I should have to study a lot merge , join , concatenate functions in order to have usable and correctly formatted data on the sheet .
python pandas custom agg function
My agg function before integrating dataframes was u " | " .join ( sorted ( set ( x ))) .
Ideally I want to have any number of columns in the group and agg returns the u " | " .join ( sorted ( set() ) for each column item like two above .
I was hacking out the aweful ` grp2.agg ( lambda x : u " | " .join ( sorted ( set ( map ( str , x.tolist() )))))` .
I can create a mask explicitly : #CODE
[ Updated to adapt to modern ` pandas ` , which has ` isnull ` as a method of ` DataFrame ` s .. ]
You can use ` isnull ` and ` any ` to build a boolean Series and use that to index into your frame : #CODE
You could use the function ` isnull ` instead of the method : #CODE
I've also tried doing this with concat and I get the same results .
Are you trying to merge or concat these DataFrames somehow ?
You should be able to use ` concat ` and ` unstack ` .
If so , could you possibly append the output of ` s.head() .to_dict() ` for both Series to your question ?
If you have a list / Series for this ordering ( in this case ` ser [: 3 ]` will do ) you can ` reindex ` before plotting : #CODE
I suppose I could use ` pd.read_table ( " test.txt " , na_filter=False )` and subsequently replace ' NULL ' values with NaN and change the column dtype .
One day I hope to replace my use of SAS with python and pandas , but I currently lack an out-of-core workflow for large datasets .
I would then have to append these new columns into the database structure .
I rarely append rows , but I do perform many operations that create new columns .
Finally , I would like to append these new columns into the on-disk data structure .
At the very end of this process , I apply some learning techniques that create an equation out of those compound columns .
e.g. I have tables on disk that I read via queries , create data and append back .
The second link makes me a bit worried that I can't append new columns to the tables in HDFStore ?
You cannot append columns once a table is created .
Is there any reason I couldn't just transpose a dataframe , add it to an HDFStore , and then index it by the " column " names ?
This would allow me to access only the " columns " ( in the form of rows ) that I need , transpose them back , and then append any new ones I create .
I can also add columns ( though not drop or rename ) and append observations .
querying : gt = greater than ...
How about a join since I normally get 10 data sources to paste together : #CODE
then ( in my case sometimes I have to agg on ` aJoinDF ` first before its " mergeable " . ) #CODE
Finally you can read into pandas your 3 to memory max key indicators and do pivots / agg / data exploration .
You can also use the two methods built into MongoDB ( MapReduce and aggregate framework ) .
See here for more info about the aggregate framework , as it seems to be easier than MapReduce and looks handy for quick aggregate work .
Hi , I'm playing around with your example as well and I run into this error when trying to insert into a database : ` In [ 96 ]: test.insert (( a [ 1 ] .to_dict() for a in df.iterrows() )) ---------------
Then I " transpose " the row-oriented HDF5 file into a column-oriented HDF5 file .
The table transpose looks like : #CODE
I subsequently process each file separately and aggregate results at the end
And although the query language and pandas are different , it's usually not complicated to translate part of the logic from one to another .
I think type change is fine as long as the timestamp they map to is preserved #CODE
I want to do a map with a customized function where I'm expecting pandas.lib.Timestamp type .
How do I turn a row into a map ?
How can I turn a row into a map , or otherwise do simple concise custom printing of rows ?
This will get you a map : ` speeds.ix [ 3 ] .to_dict() `
This will convert to a map : ` speeds.ix [ 3 ] .to_dict() `
Trouble with pandas cut
I can then append this to my dataframe to have a new column .
Then , you can groupby by the new column ( here it's called index ) , and use ` transform ` with a lambda function .
Can you paste the entire stack trace ?
File " / misc / apps / linux / python-2.6.1 / lib / python2.6 / site-packages / pandas-0.10.0-py2.6-linux-x86_ #URL line 1817 , in transform
I pasted your stack trace into your original question .
I am getting a TypeError : Transform function invalid for data types .
Also , you shouldn't have to drop the NaNs .
How do I join two dataframes ( pandas ) with different indices ?
I'm working on a way to transform sequence / genotype data from a csv format to a genepop format .
I want to insert the values from ` df2 ` into ` df1 ` , keeping empty rows where ` df1.index = ' POP '` .
I tried ` join ` , ` combine ` , ` combine_first ` and ` concat ` , but they all seem to take the rows that exist in both df's .
` df1.join ( df2 )` should default to a ` left ` join which only preserves the columns from df1 .
It sounds like you want an ' outer ' ` join ` : #CODE
My current solution is to define a temporary dataframe w , based on the fancy boolean indexing , set the corresponding values in ' y ' to 0 in w , and then merge w back to d using the index .
And I'm trying to efficiently join where the keys match and the date is between the valid_from and valid_to .
I was wondering if anybody had a better idea for a join such as this .
( Note : the ` value ` column of ` df2 ` is accessed as ` value_y ` after the merge because it conflicts with a column of the same name in ` df ` and the default merge-conflict suffixes are ` _x , _y ` for the left and right frames , respectively . )
Thanks @USER , I didn't know that qcut / cut had a labels-attribute ( isn't showing in IPython autocompletion unfortunately ) .
2 , Use date as the primary index and time as the secondary index in a multiindex dataframe
multiindex dataframe #CODE
My naive inclination would be to prefer a single index over the multiindex .
However , I am not very experienced with Pandas , and there could be some advantage to having the multiindex when doing time-of-day analysis .
How can you elegantly apply condition a , b , c , etc . and make sure you then plot " the rest " ( things not in any of these conditions ) as the last step ?
To find points skipped due to NA , try the ` isnull ` method : ` df [ df.col3.isnull() ]`
pandas rolling computation with window based on values instead of counts
I'm looking for a way to do something like the various ` rolling_* ` functions of ` pandas ` , but I want the window of the rolling computation to be defined by a range of values ( say , a range of values of a column of the DataFrame ) , not by the number of rows in the window .
If I do something like ` rolling_sum ( d , 5 )` , I get a rolling sum in which each window contains 5 rows .
But what I want is a rolling sum in which each window contains a certain range of values of ` RollBasis ` .
I can't do it with the rolling functions , because their windows always roll by number of rows , not by values .
For this to work correctly ( at least in pandas 0.14 ) , I think you need to replace chunk = indexed_what [ indexer ] by chunk = indexed_what.iloc [ indexer ] .
" AssertionError when using apply after GroupBy " .
In some cases I can get ` apply ` working after ` groupby ` and in other cases not .
I am having difficulties understanding how to work with DataFrame with MultiIndex .
Pandas Merge ( pd.merge ) How to set the index and join
I tried the following merge : #CODE
I want it to merge based on both date and cusip / idc_id .
Reset the indices and then merge on multiple ( column- ) keys : #CODE
You could append `' cuspin '` and `' idc_id '` as a indices to your DataFrames before you ` join ` ( here's how it would work on the first couple of rows ): #CODE
` strip ` only removes the specified characters at the beginning and end of the string .
If you want to remove all ` \n ` , you need to use ` replace ` .
You could use ` regex ` parameter of ` replace ` method to achieve that : #CODE
Maybe a nice trick / slightly dirty way to get around the unicode issues is to convert unicode columns into string columns with the " xmlcharrefreplace " option ; later on you can translate this back into unicode if you want to .
Now groupby both columns and apply the lambda function : #CODE
I tried looking at various join , merge etc . operators in the docs , but couldn't find anything offering similar logic .
` alternating = big [( big.index.to_pydatetime() - start ) .total_seconds() / 17 % 2 == 0 ]` , but I can't seem to find a way to map the total_seconds() call to all elements .
Renaming a pandas pivot table without losing axis labels
When I invoke rename on a pivot table , I lose the axis labels : #CODE
When you pivot , the values of x and y are the labels , and that is expected behaviour .
You can use the ` DataFrame ` ` drop ` function to remove columns .
This does indeed work well , but in this instance I only need to keep about 5-6 out of 40-50 series of data , and the series I want to drop may fluctuate based on changes in the input data file .
I just had to do something similar to what you've done , and in my case , I've pre-computed the list of things I need to drop , and then passed in the list to the drop() function .
@USER Zelleke , what if i had about 50 columns i want to drop and 50 columns i want to keep . and the number of columns can change each instance i run it .
maybe replace the num with enumerate in the for loop ?
Pandas : Read Timestamp from CSV in GMT then resample
Then I would like the resample to one minute intervals , HOWEVER , I would like to be able to have it skip gaps which are larger than a user specified value .
If this is not possible , is there way to resample to to one minute , but in the gaps , put in an arbitrary value like 0.0 ?
The only thing I could figure out that might be the issue is in the dataframe dtypes were all object ( vs int64 ) but once I drop to the series , it went to longs .
I actually think it won't always make sense to apply ` reshape ` to a Series ( do you ignore the index ? ) , and that you're correct in thinking it's just numpy's reshape :
@USER the fact that it causes an exception when you try to do it is surely a bug , either it should let you do it to a DataFrame ( and reindex ) or the method shouldn't be available ?
How to drop extra copy of duplicate index of Pandas Series ?
So how to drop extra duplicate rows of series , keep the unique rows and only one copy of the duplicate rows in an efficient way ?
One way would be using ` drop ` and ` index.get_duplicates ` : #CODE
Not totally drop the duplicated ones .
You can groupby the index and apply a function that returns one value per index group .
@USER sorry , " arbitrary " of length len ( s ) :) .
Below is my snippet : import pandas as pd ; idx_tp = [( ' 600809 ' , ' 20061231 ') , ( ' 600809 ' , ' 20070331 ') , ( ' 600809 ' , ' 20070630 ') , ( ' 600809 ' , ' 20070331 ')] ; dt = [ ' demo ' , ' demo ' , ' demo ' , ' demo '] ; idx = pd.MultiIndex.from_tuples ( idx_tp , names = [ ' STK_ID ' , ' RPT_Date ']) ; s = pd.Series ( dt , index=idx ); # s.groupby ( s.index ) .first() will crash on my machine
@USER passing a MultiIndex to series.groupby and then applying a function also crashed for me .
How do I elegantly apply this in Pandas ?
Pandas has set logic for intersection and union , but nothing for disjoint .
The ` Target ` is just a constant , so instead of trying to find the root for ` f ( x ) = 0 ` , you'd define ` g ( x ) = f ( x ) - Target ` and apply ` newton ` to ` g ` .
I'm trying to identify the rows with unicode and strip the $ sign and comma , converting to float .
However when I use the apply function to my case I get an ' unhashable type ' error .
You are just printing these and not ` apply ` -ing them to the DataFrame , here's one way to do it :
If I understand you right , you're looking for the ` apply ` method : #CODE
Update : I now recommend installing the scientific python stack using Anaconda .
@USER it may be preferable to install pandas via the fedora package #URL I now recommend using conda ( much easier for the python data stack ) .
apply a function to a pandas Dataframe whose retuned value is based on other rows
I want to apply the same process to the whole quantity column .
Here , we groupby ` [ ' item ' , ' price ']` and apply the function above .
After building basic class with ` __str__ ` and plotData() methods I would like to apply some filters and build a new class where additional column is the filter .
To convert back to what we started with we could ` apply ` ` Timestamp ` to the column and ` set_index ` : #CODE
I'm currently trying to build a fairly simple script that will compare two DataFrames from a CSV and perform an inner merge to remove duplicates .
rank data over a rolling window in pandas DataFrame
I am trying to rank a Timeseries over a rolling window of N days .
I don't seem to be able to find a rolling rank function .
If I wanted to rank the data over a rolling window of 3 days , the answer should be : #CODE
To limit memory usage , simply replace the dict cache with something like a LRU .
I try to apply exactly the same logic to my original problem with large dataframe inside a class .
I found in here that there could be a problem with type of the columns but Depth is type ` numpy.float64 ` Hper is type ` float ` Vper is type ` float ` so I understand how it can apply to my problem .
No , ` reindex ` doesn't do any sorting .
You can use ` iget ` to retrieve by position :
I have a Panda Series and based on a random number I want to pick a row ( 5 in the code example below ) and drop that row .
I want to drop row " 5 NaN " and keep - 0.000052 with an index 0 to 8 .
Somewhat confusingly , ` reindex ` does not mean " create a new index " .
So at your last step just do ` sample_mean_series.index = range ( len ( sample_mean_series ))` .
Using ` reindex [ blah ]` just selects rows , basically like doing ` df.ix [ blah ]` , and like that it gives you NaN if the ones you ask for don't exist .
It does have some options for filling in the NaNs , but I've never really understood the point of reindex , let alone its name .
Not sure where to drop sample data .
In some circles this operation is known as the " asof " join .
What about using ` Series.searchsorted() ` to return the index of ` y ` where you would insert ` x ` .
If you want to combine ` join ` your MultiIndex into one Index ( assuming you have just string entries in your columns ) you could : #CODE
Note : we must ` strip ` the whitespace for when there is no second index .
And if you want to retain any of the aggregation info from the second level of the multiindex you can try this : #CODE
pandas : merge rows on timestamp
I'd like to merge the rows based on the first column and have the output look like this : #CODE
I am still struggling to get a combination of groupby and stack to recast the dataframe .
Construct the index as desired and apply it to the dataframe
Now create the desired index and apply it .
I'm trying to transform monthly returns data I have for thousands of stocks in postgres from the form : #CODE
My suggestion would be to first ` set_index ` as date and company name , then you can ` unstack ` the company name and ` resample ` .
@USER the column is a MultiIndex ( which is like a double header , and useful if there were more columns in the ` df2 ` , but not so useful here ) , you can " correct " this via ` df2.columns = df2.columns.get_level_values ( 1 )` .
For posterity : in my actual , messier dataset , I needed to also use ` groupby ( levels =[ 0 , 1 ]) .last() ` to remove duplicate indices so I could ` unstack ( level=1 )` the dataframe , and then , to get the final result , I had to call [ ' return '] on the dataframe : e.g. with Andy's ` df4 ` ,, ` df4 [ ' return ']` got me the DataFrame I needed .
The first resample starts at 2000-01-03 and the second resample starts at 2000-01-04
The docs show how to apply multiple functions on a groupby object at a time using a dict with the output column names as the keys : #CODE
What I want to do is apply multiple functions to several columns ( but certain columns will be operated on multiple times ) .
but as expected I get a KeyError ( since the keys have to be a column if ` agg ` is called from a DataFrame ) .
Because the aggregate function works on Series , references to the other column names are lost .
This is starting to get pretty messy , though -- I think for readability manual looping may be preferable , plus I'm not sure there's a way to give it my preferred name in the ` agg ` argument ( instead of ` `) .
How do I resample a time series in pandas to a weekly frequency where the weeks start on an arbitrary day ?
You can pass anchored offsets to ` resample ` , among other options they cover this case .
Pandas Drop Rows Outside of Time Range
I have been looking for solutions but none of them separate the Date from the Time , and all I want to do is drop the rows that are outside of a Time range .
Note : the same syntax ( using ` ix ` ) works for a DataFrame : #CODE
@USER -- My first thought was also that it worked the same , but I think DataFrame tries to apply it to the columns ( without the ix ) .
@USER I think it's good practice to use ix with Series as well , and that way the syntax is identical , so I updated the first part .
@USER thanks for mentioning that , I will append :)
I'm trying to transform a ( well , many ) column of return data to a column of closing prices .
If abs ( stuff ) > 1 , the result will be negative .
If you can't / don't want to replace your reading with ` pandas.read_csv ` , then probably my ` numpy.delete ` is easiest , but I think you're better off with his answer .
` read_csv ` is much simpler , harder to get wrong , and probably faster than what he has , and there's no good reason not to drop the column in ` pandas ` instead of post-deleting after ` to_records ` .
currently to create my time axis I'm using the following : ` x = mdates.num2date ( x , tz=None )` ` x = [ dt.replace ( tzinfo=None ) for dt in x ]`
You may be trying to force the use of ` hist ` ... consider taking a step back to construct a bar plot .
Pandas dataframe resample at every nth row
I am planning to resample the dataframe so that if the dataset passes certain size , I will resample it so there are ultimately only the SIZE_LIMIT number of rows .
I need to apply some function for every columns and create new columns in this DataFrame with special name .
You can use ` join ` to do the combining : #CODE
where you could replace ` df*2 ` with ` df.apply ( your_function )` if you liked .
I would skip the ` apply ` method and just define the columns directly .
But for whatever reason I avoid ` apply ` unless I really need it .
BY avoiding the ` join ` this also has the nice benefit of not having to reassign the dataframe .
In particular I want the inner join of the tables .
I could not load the table B on memory as a pandas dataframe to use the normal merge function on pandas .
straightfoward disk based merge , with all tables on disk .
See this answer for a comment on how doing a join operation will actually be an ' inner ' join .
For your merge_a_b operation I think you can use a standard pandas join
table ; instead of storing the merge results per se , store the row index ; later
I am avoiding to use join / concat / merge option of pandas since they all spit out a " segmentation fault : 11 " every now and then .
Which is probably like a inefficient " join " .
To avoid it replace last line with : #CODE
I basically replace the stars in my example code above with ` ` .
Pandas DatetimeIndex truncate error
` df ` was created by concatenating multiple dataframes together using the ` concat ` function .
Let's say I wanted to do the rolling sum over a 1ms window to get this : #CODE
add column with time rounded to millisec and groupby it , apply cumsum within each group
You need a way of handling NaNs and depending on your application , you may need the prevailing value asof the lagged time or not ( ie difference between using kdb+ bin vs np.searchsorted ) .
ugh , the second asof ( s.asof ( lag )) is wrong .
What you really need are the indices from the first asof .
I have tried concat : #CODE
but I need the output that adds values of existing indices and keeps new indices if they appear , so a mix of concat and ' + ' .
Or you could align the two first , and then simply add them with ` + ` : #CODE
Why not just take the [ transpose ] ( #URL ) ??
You could replace : #CODE
Is there a way to first compute the cumsums and then apply ' ohcl ' to the data ?
I wasn't able to get your resample suggestion to work .
Here's a way to aggregate the data at the business day level and compute the OHLC stats in one pass : #CODE
The outer key references the columns you want to apply the functions to .
The inner key contains the names of your aggregation functions and the inner values are the functions you want to apply : #CODE
boolean mask in pandas panel
1 ) I create a mask and mask the data as follows : #CODE
What I cannot figure out how to do is filter all of my data based on a mask without a for loop .
Assuming that Date is the index rather than a column then you can do an " outer " ` join ` : #CODE
I get : `` 275 # FIXME : This doesn't handle MultiIndex
How to drop a list of rows from Pandas dataframe ?
Then I want to drop rows with certain sequence numbers which indicated in a list , suppose here is ` [ 1 , 2 , 4 ] , ` then left : #CODE
This is because of using integer indices ( ` ix ` selects those by label over -3 rather than position , and this is by design : see integer indexing in pandas " gotchas " * ) .
* In newer versions of pandas prefer loc or iloc to remove the ambiguity of ix as position or label : #CODE
Note : Series has a similar ` iget ` method .
Using the ` map ` lets you put in any condition you want , in this case you can do this more simply ( as pointed out in the comments by DSM ) #CODE
Like the last non map solution from DSM , worked for me
Python pandas , how to truncate DatetimeIndex and fill missing data only in certain interval
for each date in the DataFrame truncate to only have data in the
the pandas truncate functions only allows me to truncate according to date , but I would like to truncate according to datetime.time here .
for each date in the DataFrame truncate to only have data in the range of 9:00 : 00AM - 11:30 : 00AM and 13:00 : 00 - 15:15 : 00
Merge this dataframe with the original one using outer join .
@USER You can probably create a two-level index ` [ ' date ' , ' time ']` and then apply time filtering for the second level , but that is beyond my current level of pandas-fu now .
So you can use ` map ` and a ` lambda ` : #CODE
Note , however , that while you can attach attributes to a DataFrame , operations performed on the DataFrame ( such as ` groupby ` , ` pivot ` , ` join ` or ` loc ` to name just a few ) may return a new DataFrame without the metadata attached .
When I do df [ ' tracking '] = pd.np.arange ( len ( df )) I get ' tracking not in this series !
To pick the last row using ` irow ` : #CODE
I've delved into the code to realize that the repr function on Series eventually calls ' _format_datetime64 ' , which checks ' isnull ' and will print out ' NaT ' That explains the difference between these two .
I suppose there may be other pandas functions that call ' isnull ' and act based on the answer , which might seem to partially work for NA timestamps in this case .
Pandas append data frames , add a field , and then flood the field with a default value ?
I want to append them into a master data frame .
I think that's not the best shape for your DataFrame -- I think columns like " letter " , " number " , " acc " , " rt " or something ( giving them more meaningful names ) would be easier to pivot .
Then we apply this to a slice of the ` _rt ` columns : #CODE
All the values which we're using are within 3 standard deviations , so this cut isn't very interesting , but we can apply it anyhow : #CODE
I'm new to pandas ( and python ) and have been slowly working my way trying to apply things learned to my own datasets .
Would you be okay with using a ` map ` on the columns ?
How to apply conditional logic to a Pandas DataFrame .
I could apply a loop and do re-construct the DataFrame ... but that would be ' un-pythonic '
You want to apply a function that conditionally returns a value based on the selected dataframe column .
` le ` tests whether elements are less than or equal 2.5 , similarly ` lt ` for less than , ` gt ` and ` ge ` .
And then use the ` .fillna ` method to replace the ` NaN ` s with 0 and then assign the columns : #CODE
python pandas : diff by user in a DataFrame containing multiple users
I'd like to get the diff of the timestamp within users .
` groupby ` the ` uid ` and then ` transform `
I'd ` groupby ` and then ` diff ` .
And then we select the column we're interested in along these groups and ` diff ` it : #CODE
Unfortunately , the Series returned from the grouped diff contains some strange values .
Running df [ " diff "] .value_counts() yields " TypeError : unhashable type : ' numpy.ndarray '" .
Still not sure where the arrays come from , but here's the fix : df [ " diff "] [ df [ " diff "] .apply ( type ) == numpy.ndarray ] = numpy.NaN
The shift function does exactly that , it shifts an array by a default of one index .
( Actually , this is not completely true since the mask has to be stored somewhere , but you still don't have to copy your table values somewhere else in memory )
` keep_default_na ` : bool , default True
But if I apply ( np.float64 ) it changes the numbers to what I need .
@USER must be a bug , works with ` test.TRAINER_MANAGING.str.startswith ( ' Han ' , na=True )` and ` endswith ` , which are the only ones tested against in the [ commit ] ( #URL ) you link to ) .
@USER that sounds like could be [ a good feature request ] ( #URL ) , I guess you can use loc : ` df.loc [( ' Group1 ' , ' C ')]`
And ` apply ` it ( row-wise ): #CODE
I'm not sure how big your dataframes are , but you could unstack ResultTable and access it with a multiindex tuple
You can first add it as a normal column and then append it to the current index , so : #CODE
Pandas resample not adding up
I then resample the dataframe as below #CODE
I just realised that if I replace NaN's with 0 , I don't have the problem .
But I would like to resample without filling in NaN's with 0 .
I don't know if it is a bug or just a constraint on the MultiIndex ( but in that case , the error message could be more clear I think ) .
but now , I need to apply this ( multiparameters ) function along 0-axis .
Maybe I'm wrong and using ` shift ` function is not a good idea
I would like to replace values 0 with the mean of the group that they are in .
Then use ` apply ` across each row , to replace each NaN with its groups mean : #CODE
Using @USER ' s example , you could use ` groupby ` / ` transform ` with ` replace ` : #CODE
I haven't seen the use of [ ~mask ] - does the tilda just mean ` not mask ` ?
@USER : ` mask ` is a ` Series ` , which is a subclass of numpy's ` ndarray ` class .
Since ` mask ` is of dtype ` bool ` ( i.e. a boolean array ) the inversion is done bit-wise for each element in the array .
` not mask ` has a different meaning .
This asks Python to reduce ` mask ` to its boolean value as a whole object and then take the negation .
` not mask ` raises a ` ValueError ` .
How to apply linregress in Pandas bygroup
I would like to apply a scipy.stats.linregress within Pandas ByGroup .
I had looked through the documentation but all I could see was how to apply something to a single column like #CODE
But how do I apply a linregress which has TWO inputs X and Y ?
and if using a groupby you can similarly ` apply ` ( to each group ): #CODE
Also I have a DataFram so how can I apply that using two columns in the DF ?
@USER This reminds me a lot of [ this answer ] ( #URL ) , drop the na first then do the calculation .
I was wondering if the ' mask ' feature in Pandas would work and was thinking along those lines .
I would like to resample / convert Daily ( ohlcv ) to Weekly ( ohlcv ) .
You can set ` ignore_index=True ` when ` append ` -ing : #CODE
One improvement on this is to use ` irow ` , which grabs only the rows in the specified integer positions : #CODE
irow is cool .
This is the closest I've gotten , by changing the dictionary value to a list of lists and using a transpose .
I would like to insert a new series into ` data ` #CODE
Then use ` concat ` : #CODE
Python : KeyError ' shift '
I assume the last option to be the most appropriate but then I have an error with panda ' shift ' attribute .
The easiest way to do this in zipline is to use the Returns transform which adds a returns field to the event-frame ( which is an ndict , not a pandas DataFrame as someone pointed out ) .
For this you have to add the transform to the initialize method :
And then pass that to the OLS transform .
Or do this computation inside of the OLS transform itself .
but when i apply a function from scikit-learn i loose the informations about columns : #CODE
Is there a way to apply scikit or numpy function to DataFrames without loosing the information ?
but can't figure out how to use this to replace my data values .
How best to replace these detection limit indicators with values that Pandas can handle ?
Yes , using na_values with pd.ExcelFile.parse worked to replace the < 0.1 values with NaN , but I don't really want NaN here , because below detection limit is very different than missing values ( Tufte argued that leaving out " no damage " values on the plot of o-ring damage played a significant role in the Challenger disaster ) .
I would like to replace the `' < 0.1 '` values with ` 0.1 ` to be conservative .
You are right that nowadays it should be ` to_list ` to conform with all the other Series and DataFrame ` to_* ` methods .
How can I insert the contents of the MultiIndex back into regular columns , such that I can select them like any other column ?
Note : the ` drop ` argument argument ( if set ` True `) will not add the index columns :
` drop ` : boolean , default False
Do not try to insert index into dataframe columns .
The align keyword behaves as the opposite of mpl and the log keyword is also weird .
How to Apply an equation to a Pandas dataframe ByGroup
Now my question is how do apply this back to the original data frame .
So I would like to apply that relationship back to all the Year=2010 and Month=1 to look like this .
Then for the rest of the DF apply the same approach for each month of each year .
Perhaps you want an ` apply ` which refers to ` Corr_grouped ` ( ? )
Your first step is to merge the grouped object with DF .
Now you can merge it : #CODE
One way to do this is to ` resample ` ( hourly ) before you plot : #CODE
I want to resample my data and use the ` describe ` function as the ` how ` parameter .
The question is : how do I implement my own ` describe ` function so that it works with ` resample ` ?
To this group you can apply a function of your choice , for example your directionAverage function .
How to drop columns by criteria ?
How do I drop all the columns of ` df ` that contain only ` None ` , empty strings , or white-space-only strings ?
would it be an option to loop through the columns and drop when the check is True ?
Apply a lambda testing for the conditions you want to drop : #CODE
Index df.columns with the boolean Series from above to get the columns you want to drop : #CODE
You can use ` applymap ` to apply your function to the elements of the ` DataFrame ` : #CODE
It failed when using just ` apply `
@USER Just to illustrate that this really has nothing to do with pandas , note that ` len ( set ( np.array ([ np.nan ] * 10 ))) = 10 ` .
Would converting the quantity to a string , i.e. , `` zip ( map ( str , i ) , map ( str , j ))`` , be an acceptable workaround ?
It has a constant , ` numpy.nan ` , which is predefined , but ` len ( set ( np.array ([ np.nan ] *10 ))) == 10 ` , and ` f = np.array ([ np.nan ] *2 ); print f [ 0 ] is f [ 1 ]` gives ` False ` .
I like your stack / unstack method .
If you know all the categories , maybe you could make a DF with all the dates / categories and merge it with your data-containing DF .
the version that iterates through the group does not throw a memoryerror for my local dataset ( the stack / unstack version does )
I'm looking to resample this data so I can get one 60-minute value and then calculate the range .
By default resample starts at the beggining of the hour .
You can use the ` base ` argument of ` resample ` : #CODE
pandas reindex a dataframe with duplicate keys
I wish to merge df2 with df .
What I ultimately want to do , is to merge df2 with the roughly closest index in df .
since you have 600k rows , create a mask array by ` df.sourceid == 10 ` maybe slow .
You can create Series objects that map value to the index of the DataFrame : #CODE
it's O ( log N ) , faster then create mask arrays which is O ( N ) .
You can use the module StringIo which you can use just like a file handle , so you can strip some data , dump the clean stuff into a virtual file handle , and your csv reader will be none the wiser
I can quickly make a scatterplot and apply color associated with a specific column and I would love to be able to do this with python / pandas / matplotlib .
I'm wondering if there are there any convenience functions that people use to map colors to values using pandas dataframes and Matplotlib ?
The ` join ` produces : #CODE
And then drop the rows without a utime : #CODE
You can pass a dictionary of functions to the ` resample ` method : #CODE
Pass the dictionary to the ` how ` parameter of ` resample ` : #CODE
A combined groupby / resample might work : #CODE
Inner join with MultiIndex fails if no overlap
I can't get " empty " inner joins to work with a MultiIndex .
I'd like to be able to tell in advance if the intersection is empty , so I can avoid the exception .
I'm not 100% on this , but doing an outer join and dropping the NAs is the same as an inner join .
And so after the outer join + ` dropna() ` , you can see how many rows ` d3 ` and go from there .
python pandas transform timeseries into multiple column DataFrame
I am hoping to transform the data into a DataFrame , with the columns as each date , and rows as the time in the date .
But how do I transform the groups into date columned DataFrame ? or is there any better way ?
I think the answer given by Chang She doesn't translate to dataframes that have a unique index , like this one : #CODE
In which case you can just turn the columns into a MultiIndex and use stack then reset_index .
Edit : use merge to join original ids back in #CODE
After the call to stack , " s1 s2 s1 s2 ...
If that's the case , use merge or set the name to be the index to begin with .
I have a ` dt ` : #CODE
So how can I transform ` matplotlib.axes.AxesSubplot object ` to ` matplotlib.figure.Figure object ` or directly return " Figure object " from Pandas plot ?
Maybe use [ drop ] ( #URL ) ?.
Use [ drop ] [ 1 ] as in this answer to another question : #URL [ 1 ]: #URL
I wondering if I can apply the ` pandas.ols ` model to a data frame of multiple response variables against one independent variable at one time .
+1 : I didn't know about ` icol / irow ` until someone else used it on SO about a month ago !
You could use ` unstack ` before sorting : #CODE
Pandas sort by group aggregate and column
One way to do this is to insert a dummy column with the sums in order to sort : #CODE
and maybe you would drop the dummy row : #CODE
Yes , it does seem silly using a dummy ( tbh I could've been more clever with my apply [ 12 ] to do it in one , and it may well be more efficient , but I decided I wouldn't like to be the person reading it ... ) .
@USER you can append ' C ' to the list of columns to sort by , so it's : ` df.sort ([ ' sum_B_over_A ' , ' A ' , ' B ' , ' C '])` ...
Within each group , sum over B and broadcast the values using transform .
This will re-order the A values by the aggregate sum of the B values : #CODE
So that's what transform does ... this is great , thanks a lot .
If you reset the index of your klmn1 dataframe to be that of the column L , then your dataframe will automatically align the indices with any series you subtract from it : #CODE
Predict number of rows in merge / join
In my code I use merge / join in numerous places .
Recently I bumped on an join probably just making a Cartesian product ( probably only out of 5000 files to process ) .
Since the code works on a 64 bit system / python , this join keeps running to fill all memory , blocking every process / user on this hardware node .
Is there a easy way to test the validity of a join / merge , which i could use in an assert statement ?
In my opinion the only way to do it is to understand the different join types ([ this ] ( #URL ) gives the equivalent to SQL )
merge / join creates a new dataframe , that the memory problem .
I think of a test to check the content of the join fields .
If the join columns do not match ( compare two set ( index_col.values ) of join columns ) this might be used to decide to merge or just return an error ...
you can then merge this new dataframe with the original one by using the merge function : #CODE
I want to replace df.ix [ 0 ] [ ' date2 '] with df.ix [ 1 ] [ ' date2 '] for each symbol -- the symbol changes through the dataframe so I can't just apply this through the whole dataframe .
I was going to replace the NaN with the date .
2 ) Basic question : How to replace an NaN ( or even another number ) in a pandas DF ?
I always forget about apply .
Pandas groupby indexing before and after transform
I think groupby's transform should do what I want : #CODE
Also , when I try to transform on 2 columns , it doesn't do what I expected : #CODE
The transform mechanic is a bit mysterious , agreed .
I think it lies in the ` transform ` method .
If you look at the documentation , it says that ` transform ` returns an object that is indexed the same ( same size ) as the one being grouped .
Pandas : Merge two Dataframes on a column , but keep only data for unlike Columns
Now I want to resample it as below : #CODE
... but using ` eval ` always leaves me feeling all yukky ' n ' stuff ...
1 E.g. , any approach I can think of involving the ` filter ` built-in is probably ineffiencient , since it would apply the criterion ( some lambda function ) by iterating , " in Python " , over the panda ( or numpy ) object ...
( with enough work we could lose the ` eval ` , this is simply proof of concept ) after which something like #CODE
Using append results in a horrible mess including NaNs and things for reasons I don't understand .
concat is more like rbind than append .
As @USER points out , we can generalise to drop the last ` k ` columns with : #CODE
python pandas ( horizontal ) concat error with timeseries of the same name
Vertical concat is fine #CODE
btw , the result is correct if you concat two DF with same column names
How to apply " first " and " last " functions to columns while using group by in pandas ?
To get a usual table ( i.e. a table in which every cell contains only one a single value ) I need to indicate what function I want to use to transform the sets of values in the cells into single values .
For example I can replace sets of values by their sum , or by their minimal or maximal value .
To replicate the behaviour of the groupby ` first ` method over a DataFrame using ` agg ` you could use ` iloc [ 0 ]` ( which gets the first row in each group ( DataFrame / Series ) by index ): #CODE
In older version of pandas you could would use the irow method ( e.g. ` x.irow ( 0 )` , see previous edits .
Just in case it's useful to anyone , according to [ the docs ] ( #URL ) , ` irow ` is now deprecated ( ` x.iloc [ 0 ]` does the trick instead )
I'm confused with [ the docs ] ( #URL ); it states : ` Aggregating functions are ones that reduce the dimension of the returned objects , for example : mean , sum , size , count , std , var , sem , describe , first , last , nth , min , max .
In some sense there's three types of mapping here : aggregation , apply and filter ( the above is kind of a filter , although it uses the agg verb ) .
This is complicated thing is that you can use ** either ** agg or apply to get the ` .iloc [ 0 ]` job done , not sure why I used agg , apply is probably a better description .
The essence of what I'm struggling with is how do I create a function which I apply to a groupby which returns both the result of the function AND the columns on which it was grouped ?
In the example you've appended , what's the purpose of the groupby ( it'll just find dupes ) , you can just do an apply to df itself and add that as a column : ` df [ ' func3 '] = df.apply ( lambda row : row [ ' col2 '] ** 2 , axis=1 )` .
I don't can't see an example where it makes sense to groupby all columns and apply , rather than just apply ( DataFrames apply can be very non-trivial and save to multiple columns ) .
It appears that when the apply is on every row it does not return the keys , but if the apply results in aggregation it does return the keys
The best way to understand how your apply is going to work is to inspect each action group-wise : #CODE
for an arbitrary function which I am applying , the groupby seems to drop my grouping columns from the result and returns only a Series of answers .
( to me , this seems a strange thing to do , but I agree the output is a little weird : not having the MultiIndex of the columns ) : s
Glue them back together and use the ` groupby ` function to aggregate the rows .
If we apply this operation to a data frame , as a result we do not get another data frame .
Can also use iloc ( rather than ix ) , which plays nicer with int / float labeled data .
How to do a data frame join with pandas ?
I believe this is what ` Pandas.DataFrame.join ` is for , but I cannot formulate the code needed to join the data frames ` A ` and ` B ` correctly .
I think I would use ` merge ` here : #CODE
How can I add a title to a histogram created using the pandas hist function on Series in case the " by " argument is used ?
Another strange observation I made is that when I create a figure first , and then use the hist function , in the case of the by argument , the first figure is ignored and the hist functions creates a new one , which I cannot find the gca of matplotlib .
For Example , I use ISO3 CountryCode's to merge from different data sources
Is the best solution to generate my own Dictionary() and then when it comes time to print or graph to then merge the country names in ?
After all of the reshaping I could merge the country names back in as a separate column .
The index option has a format method that lets you apply a formatter in the form of a function : #CODE
This works quite well to replace the Index Codes prior to printing etc ...
You can ` reindex ` by the index of the ( sorted ) Series : #CODE
a ) you specify the ` index_col ` rel . to the number of columns you really use -- so its three columns in this example , not four ( you drop ` dummy ` and start counting from then onwards )
Is there a way to concat , join or merge dataframes based on both the index and columns ?
where df.index should be the union of the indices in dfList ( ' outer ' join ) and df.columns should also be the union of the columns in dfList .
I think all of the concat , join and merge methods just do a join on either the index or the column .
I suppose a workaround is stack / unstack or reset_index ?
As a ' work flow ' t seems that working with a single MultiIndex during aggregation of data makes the most sense .
One way to fix it is to replace the newline characters in the latex string with spaces .
And then some dict replace nonsense : #CODE
My impression was that ` pivot ` is the right function to do this but it is not right : #CODE
If ` melt ` simply stacked the rows with ` value_vars ` SepalLength and SepalWidth , then you can manually create an index to pivot on ; and it looks like the result ends up the same as the original : #CODE
` len ( iris ) == 150 , len ( test ) == 300 ` .
My code ` range ( len ( test ) / 2 ) * 2 ` is two lists [ 0 .. 149 ] concatenated together , which can be seen in the output from ` test [ -10 :] ` ( the original and new indices don't match up ) .
One shortcut would be to replace ` df1 [ " label "] = len ( df1.index ) * [ " df1 "]` with ` df1 [ " label "] = " df1 "`
( rather than ` df1 [ " label "] = len ( df1.index ) * [ " df1 "]` . )
how to apply functions to grouped dataframes in Python pandas ?
I would like to apply a function per group that does something specific with a subset of the columns in ` grouped_iris ` .
How could I apply a function that for each group ( each value of ` Name `) sums ` PetalLength ` and ` PetalWidth ` and puts it in a new column called ` SumLengthWidth ` ?
I know that I can sum all the columns per group with ` agg ` like this : #CODE
Can't tell if you want the aggregate numbers ( in which case Andy's solution is what you want ) , or if you want it transformed back into the original dataframe .
If it's the latter , you can use ` transform ` #CODE
In general , if you want those two numbers projected back onto the original dataframe , you want to use ` transform ` : #CODE
Notice how these values are the exact same as the values produced by ` agg ` , but that it has the same dimensions as the original ` df ` .
Could you explain more what ` grouped_iris [ cols ] .transform ( sum )` is doing here exactly and how ` transform ` is generally used ?
There's a convenience function for not ` isnull ` , called ` notnull ` : #CODE
+1 for noticing the ` notnull ` .
I suspect this is partially / mainly because ` notnull ` supports more ` dtypes ` than ` float ` ?
Is there a way to optimize my solution ( finding starts and ends in a single pass of mask / where operations ) ?
( i.e. different solution than just applying mask / where on the DataFrame's values )
I need to create another series of lists which is an intersection of each of those lists with another list .
and apply set intersection : #CODE
A slightly more accurate answer : abs ( s - s.mean() ) > pd.rolling_std ( s , window=5 ) * 3
The method you suggested is good for selecting first and last ** rows ** of a dataframe for any given columns , however what I'm after is a way to keep / drop columns using combined ranges / lists of columns in a slice .
I'm interested in grouping by Name and Rank and possibly getting aggregate values #CODE
Each state varies in their business_code lengths ( IL - 6 len , OH - 8 len ) .
} ` means replace the following with an argument from ` format ` ,
I don't think this behaviour is available in ` to_html ` , but one way is to just insert it in manually : #CODE
append the filtered to a list
concat at the end
I was getting ' cannot append empty sequence ' exception .
i think you might have to do : if len ( filtered ): l.append ( filtered ); concat haandles Nones , prob bug if it doesn't handle empty sequences
In Pandas , there is a ` rolling ` option for the ` window_type ` argument to ` pandas.ols ` but it seems implicit that this requires some choice of a window size or use of the whole data sample as default .
I have written a function ( below ) that works with ` apply ` to perform this , but it is unacceptably slow .
Here is the function I am able to use with ` apply ` on the identifier-grouped object : #CODE
if you specify rolling_apply with the window size being the length of the entire set and min_periods equal to 1 , then you get the same expanding window behavior
pandas does offer cumsum ( cumulative sum ) and cumprod ( cumulative product ) that you could apply to a series .
Following on the advice in the comments , I created my own function that can be used with ` apply ` and which relies on ` cumsum ` to accumulate all the individual needed terms for expressing the coefficient from an OLS univariate regression vectorially .
It can melt your brain at first , but is extremely powerful and essential for taking advantage of the capabilities of ggplot2 .
When I drop some variables from a DataFrame , the returned dataframe is as I expect except the index.name is removed .
anyway to preserve the index.name during drop operations as the new
do you mean you want to drop the `' b '` row but keep the `' b '` label for the row with ` [ 5 , 6 , 7 ]` ?
No I would like to drop the ' b ' row but keep the entire Index in the new dataframe as the same name as before .
Looking at the source code it doesn't look like drop saves the index name when it reindexes the axis .
Set the index for use in the ` join ` function that is coming next .
Do an inner merge on ` df ` and ` dfm ` .
The keys are the intersection of the common columns of ` df ` and ` dfm ` -- namely , GENE , DIRECTION and SOURCE .
An " inner " merge keeps only those rows where both ` df ` and ` dfm ` share the same keys .
AttributeError : ' module ' object has no attribute ' merge '` was I supposed to import something else for pandas to work properly ?
[ The merge function ] ( #URL ) has been a part of pandas at least since version 0.6 .
AttributeError : ' module ' object has no attribute ' merge '` So should I try reinstalling my pandas and python programs ?
So merge definitely works , but it does not seem to recognize that the value of ' inner ' has been supplied ?
Is it possible to apply user defined functions to series in pandas ?
If we have two series ` s1 ` and ` s2 ` we can apply arithmetic operations to them : ` s1 + s2 ` or ` s1*s2 ` .
Now , I try to define my own operator and apply it to two series : #CODE
And I try to apply it to two series : ` f ( s1 , s2 )` .
However , just running ` abs ` or ` np.abs ` on it gives an error : #CODE
If so , how should I get ` abs ` to work , so that I can then select the minimum absolute time difference , and thus get the closest time .
For example , if I have data at 10:25 , 10:32 and call ` asof ` with 10:30 , it'll return 10:25 , rather than 10:32 which is actually closer .
When you do , can you please provide clearer details , i.e. , can you cut your dataset down to a small size and put up a self-contained reproducible example ?
I don't want to have to call the function multiple times ( eg . by doing two separate ` apply ` calls ) as it is rather computationally intensive .
Using ` apply ` :
Trying to apply this to the DataFrame gives an error : #CODE
I was then going to assign the values returned from ` apply ` to two new columns using the method shown in this question .
To make the first approach work , try returning a Series instead of a tuple ( apply is throwing an exception because it doesn't know how to glue the rows back together as the number of columns doesn't match the original frame ) .
The second approach should work if you replace : #CODE
Returning a series works , and I get a ' mini-df ' returned , but I can't seem to get the values returned from the ` apply ` function into the original dataframe .
The workaround I have is to reindex with a " clean " Multi-Index as follows : #CODE
Assuming your ratios are positive , if you take ` c = 1.0 ` then ` log2 ( c + x )` will map [ 0 , inf ) --> [ 0 , inf ) .
The ` ix ` attribute is a special thing that lets you do various kinds of advanced indexing by label choosing a list of labels , using an inclusive range of labels instead of a half-exclusive range of indices , and various other things .
What you've requested in your code is actually ambiguous between " all labels up to an including 3 " and " all indices up to but not including 3 " , and labels always win with ` ix ` ( because if you don't want label slicing , you don't have to use ` ix ` in the first place ) .
The general idea now is to apply these calibration data to the measurements .
You can drop them with ` df = df.dropna ( subset =[ ' item '])` .
Using ` irow ` is cumbersome , since it requires switching from ` ( )` notation to ` [ ]` notation ( ` irow ` returns a ` Series ` object )
And then you can use ` irow ` ( to get one location , or slice by integer location ): #CODE
To get the data in the shape you want you can use the unstack method : #CODE
For the example you give ( selecting ( ' B1 ' , 1 , 2 , ... )) you can use xs iso ix .
In contrast to ix , xs can return a view on the data when using labels ( more details on ix returning view / copy see docs ) .
Now merge will work .
( Consider the merge stage of a merge-sort ) .
Pass ` numpy.argsort ` to the ` apply ` method instead of using it directly .
@USER , The originals are datetime64 [ ns ] and the return index after the apply are objects .
continued - except you could mask yourself ( eg isnull ( df )) then just ignore the true values - eg they have nans
I'm assuming the ` AttributeError ` is because pandas will not automatically try to convert / truncate the float values into an integer ?
Write a function to replace the end conditions based on window size ?
you could use the shift function , like so , #CODE
For ` agg ` , the lambba function gets a ` Series ` , which does not have a `' Short name '` attribute .
You can index by SubID and then use ` join ` to get what you want : #CODE
or , you could use ` merge ` to accomplish without indexing : #CODE
This sounds like a job for ` transform ` .
@USER , often times these loops are implemented directly in C , thus reducing iteration time -- take ` map ` for example .
Pandas will automatically insert row identifiers .
Questions about pandas : expanding multivalued column , inverting and grouping
Next , we can join the created series into the original dataframe .
Finally , you can do your groupby operation on Gender , unstack the returned series and fill the NaN's with zeroes : #CODE
conditional sums for pandas aggregate
To complement unutbu's answer , here's an approach using ` apply ` on the groupby object .
Apply different functions to different items in group object : Python pandas
I want to group a duplicate data at time ` 14:42 : 10 ` and apply different functions to ` exe_price ` and ` exe_vol ` ( e.g. , sum the ` exe_vol ` and compute volume weighted average of ` exe_price `) .
Is there a way to group and then apply different ( written by me ) functions to values in different column ?
I wonder since my ' grouped ' is now a panda DataFrameGroupBy object , I cannot really apply your fucntion directly can I ?
@USER : It looks like I'm wrong -- waitingkuo shows a way to aggregate on multiple columns .
Can i use it replace all None objects in the dataframe without worrying about performance problem when storing dataframe into a hdf5 file ?
How to return max value of quantile cut range instead of quantile label
One of my goals is to precisely join different timeseries based on their millisecond counters .
I have a Pandas Dataframe with a DateTimeIndex and columns with hourly objects and I would like to transform and output a single column into a JSON file composed of an array of daily arrays of hourly values .
How to calculate rolling cumulative product on Pandas DataFrame
I have a time series of returns , rolling beta , and rolling alpha in a pandas DataFrame .
How can I calculate a rolling annualized alpha for the alpha column of the DataFrame ?
I was surprised to see that there was no " rolling " function built into pandas for this , but I was hoping somebody could help with a function that I can then apply to the df [ ' Alpha '] column using pd.rolling_apply .
I have used ` resample ` to get a second series with monthly averages .
I have the following boxplot : #CODE
My question is : how can I change the whiskers / quantiles being plotted in the boxplot ?
it'll be difficult to translate those ` ddply ` calls to pandas .
I guess ` groupby ` should be used but I find this format very cryptic so it's hard to translate to python
If you drop the " % " sign , you can make the plot without ticks .
We can apply column operations and get boolean Series objects : #CODE
More abstractly , what does the function in agg ( function ) see as its argument ?
` agg ` is the same as ` aggregate ` .
and then use ` loc ` to select those rows in the ` word ` and ` tag ` columns : #CODE
Alternative , you could use ` apply ` .
` apply `' s callable is passed a sub-DataFrame which gives you access to all the columns : #CODE
Using ` idxmax ` and ` loc ` is typically faster than ` apply ` , especially for large DataFrames .
Automatically does NOT aggregate on the string columns #CODE
More customizable and useful than startswith because it packs the versatility of regex in it .
I could map each of the unique classifications to a new field ( feature ) with a boolean mapping scheme , but then i'll generate an extremely large number of new features and I'm not sure if that will give a meaningful output .
I think I will need to resample my data ... but I don't know how !
I need to align data in order to have same ' x ' column for the 2 dataframes .
It seems that ` Series.interpolate() ` doesn't use index value to interpolate the NaN values .
I'm running into a strange issue ( or intended ? ) where ` combine_first ` or ` update ` are causing values stored as ` bool ` to be upcasted into ` float64 ` s if the argument supplied is not supplying the boolean columns .
If I insert one more command : ` test.update ([ ])` before running ` test.update ( b )` , boolean behavior works at the cost of numbers upcasted as ` objects ` .
Due to the very large nature of the problem , I'm using the pandas as the main Database API as its very easy to apply function to column .
Ideally using the optimized Pandas API , how can I transform the below " RawTable " into
Then stack the DataFrame , and drop our dummy columns index .
pandas join / merge ' Reindexing only valid with uniquely valued Index '
I get a very robust error on a join operation .
I attempted a merge ( left_index , right_index ) as well with the same result .
Length : 89 , Freq : None , Timezone : None
my code ( cut from larger code ) #CODE
I use merge and joins consecutive on a basic DataFrame .
At one point I start to get errors when attempting the next merge / join .
Once the error occurs I can not get any merge or join statement to be successful .
At first I did not see that the error was linked to a repeated merge / join action .
Any single merge / join of the latest set works now .
As soon I need another merge / join I get the same error .
Is this a bug , accidentely dupicated columns names , triggering an error when columns are reindexed ??? by a merge / join way after the duplicated columns enter the dataframe ???
I manually prevented the duplicated columns as a test and now the merge / join work !!!
I can convert a pandas string column to Categorical , but when I try to insert it as a new DataFrame column it seems to get converted right back to Series of str : #CODE
Guessing this is because Categorical doesn't map to any numpy dtype ; so do I have to convert it to some int type , and thus lose the factor labels -> levels association ?
2013-04-10 - 1.374779 Freq : D , Name : E
The transpose function is not fully documented for panels but very help ful advice .
pandas.DataFrame - how to reindex by group ?
Also - original DF can be big , and Im trying to figure how to make such reindex without inefficient forming whole new DF .
Unless you require the integer index for a specific reason , you can simply create a MultiIndex using columns A and B with ` df.set_index ([ " A " , " B "]) .sort() ` which will allow you to do all of the same selection and slicing the integer index would using ` xs ` and ` ix ` .
I understand this can be done by adding an extra column that contains values to allow grouping , for example you could join the above DataFrame to ` [ 1 , 1 , 1 , 2 , 2 , 2 , 3 , 3 , 3 ]` and groupby the added column .
How can I apply a function to calculate this in Pandas ?
` apply ` takes a function to apply to each value , not the series , and accepts kwargs .
If you want this result by using ` factorize ` , set ` sort ` argument to ` True ` : #CODE
I've split this into a list inside an ` apply ` and added it to the DataFrame .
The quickest way to get started with that is to ` stack ` your dataframe : #CODE
But A ) I'm not sure how to apply the conditional logic and B ) I have to apply the logic to each column iteratively rather than to the dataframe as a whole .
How can I apply conditional logic to the non-null values of a dataframe , preserving the nullity of the other fields ?
I have a dataset at a daily interval and want it to resample to what sometimes ( in scientific literature ) is named dekad's .
Interestingly enough , very often ` len ( unique() )` is a few times ( 3x-15x ) faster than ` nunique() ` .
` .CLIENTCODE .apply ( lambda x : len ( x.unique() ))` , from [ here ] ( #URL )
: ^ ) You can always replace the source data with equivalent random data .
If you replace those screenshots with short _typed_ samples , we can copy / paste the data try out your code .
or ( ix will work here as well , this is just more explicit ) #CODE
My question is : Is there a more efficient .map() which takes the Series and performs map on only unique values ?
There isn't , but if you want to only apply to unique values , just do that yourself .
Then use pandas ` map ` with the dictionary .
Even if you memoize ` guess_country ` , ` map ` will still call it for every single point , which means you suffer the extra function call overhead every time .
You would generate a new DatetimeIndex with hourly freq .
The missing days will be filled with ` np.nan ` which you can drop with ` s.dropna() ` .
The solution without Panel ` df = pd.concat ( map ( pd.DataFrame , d.itervalues() ) , keys =d .keys() ) .stack() .unstack ( 0 )` if ok ...
As we want a zero-based index , we set the first cell to ` 0 ` , and we append that column to the index of ` df ` to create the ` MultiIndex ` : #CODE
In this case , how can we execute the groupby on values of A , then apply this computation to each individual group , and finally plot the D values for the two groups ?
Could you touch on how to implement more customized computations for the D column , if say I wanted to make some specialized computation that isn't covered by a built-in " rolling " Pandas function ?
` rolling_mean ` is just one of [ many rolling functions in Pandas ] ( #URL ) .
To define a custom rolling function , use ` rolling_apply ` .
You could have provided data I can cut / paste in my terminal .
In col1 ( and all the other numerical columns ) , I want to put the median of all the values where col2 was equal .
Rolling apply question
For each group in the groupby object , we will want to apply a function : #CODE
We want to take the Times column , and for each time , apply a function .
Given a time ` t ` , we can select the ` Value ` s from ` subf ` whose times are in the half-open interval ` ( t-60 , t ]` using the ` ix ` method : #CODE
If I understand your problem statement correctly , you could probably skip ` rolling count ` if you use it only for the sake of computing the percentage .
However , when I drop this column from the pd.read_csv chunks and append to my HDFStore , I still get the same Exception .
When I try to append each column individually I get the following new exception : #CODE
My suggestions for this would be to either truncate the data in subsequent chunks ( with a warning ) or allow the user to specify maximum storage for the column and truncate anything that exceeds it .
I've tried to debug and inspected the items in the stack trace .
Once I have the dict of max string-column lengths , I try to pass it to ` append ` via the ` min_itemsize ` argument : #CODE
trouble shoot by trying the store the columns one at a time until u get a failure ; the error message is the last field . pass data_columns=True to append ( in practice you won't do this as u prob don't need to query on ALL the columns )
The next logical extension of what I'm trying to do is determine the max length of object fields so I can pass a dict to min_itemsize of the append method .
this is not valid . you are telling it a particular column should have a minimum length , but all of the strings are in a single block , as u r not specifying data_columns ( it is a bug that I let u specify this - and error msg should be better ) , you only can have a min per column if they are separate fields , otherwise the min is per column in a ( same for each one ) block . just use the min of the min ( note that you are essentially looking thru your data twice in order to do this ) . better to just pick a reasonable number ( separate issue is then we could allow u to truncate beyond that )
Also , you can use lib.max_len_string_array ( s.values ) for a quick max in your apply ( faster and you don't need to test for object type )
so with a chunksize of 10,000 the append operations failed because chunks 1 and 2 had
If the index isn't unique ` concat ` won't be able to align the columns properly back together since there will be multiple values for some indices , if that makes sense .
When you're suggesting to use resample , is it within the " handler " or before for the dataframe ?
I hope i've been asking something that make sense , as it not really clear what resample is actually doing ( need to get back home first ! )
` concat ` apparently aligns data with the same index in the order they appear .
Finally , is ` len ( reobsmaf ) == len ( remodmaf )` ?
Edit : As a workaround , you can just drop the DatetimeIndex from the DataFrame and pass it the numpy array .
How to index into a pandas multindex with ix
Can I acheive this with ix ?
As of Pandas 0.14 , you can pass a tuple of selectors to ` df.loc ` to slice a MultiIndex : #CODE
In this case is using reindex equivalent to using xs ?
is it possible in pandas to select a range of values from a MultiIndex object , e.g. the example DataFrame below I would like to select all of the values from the first level ( bar , baz , foo qux ) and the ' one ' and ' two ' from the second level for all the columns .
resulting_i = i + argmin ( abs ( obs1 [ 1 , i ] - obs2 [ 1 , #URL )` ?
Or maybe ask on a more CS stack exchange ..
Note that I had to transpose ` obs1 ` and ` obs2 ` .
I wanted to group the transactions by date , name and transaction-type and aggregate the tran ?.
Also , what is that ` apply ` method ?
and the " reshape " feature ( does not apply to 3-d case though ... ): #CODE
Well , I have to admit that I have found these solutions while writing this post ( and this is already very convenient ) , but it could be generalized to account for DataFrame ( or other ) with MultiIndex axes , do a search on all axes and labels ...
How to apply custom column order to boxplot ?
I can get a boxplot of a salary column in a pandas DataFrame ...
How can I apply my custom column order to the boxplot columns ?
A simple , brute-force way would be to add each boxplot one at a time .
I typically just use ` apply ` with a hard-coded lookup table . see my edited response for a different approach , though .
The resample function start from the earliest date to last date .
whereas i want the algorithm to resample in the reverse order .
My idea was to use ` cut ` and ` value_counts ` to first compute the number of pairs having a distance inside bins , which works fine : #CODE
My goal is to read EURUSD data ( daily ) into a time series object where I can easily slice-and-dice , aggregate , and resample the information based on irregular-ish time frames .
Pandas : use ` stack ` inplace
in pandas , I have a big ` DataFrame ` and I want to use the stack function without having to copy ( memory limit ): #CODE
this could be done if the Frame has a multi-index ( a simple index would yield a Series ) , but the stack operation by necessity almost always has to copy , see : #URL
FYI , there are various methods for getting around issues like this , here's one , slice into pieces perform your operations and concat , #URL
I would like to import the file using pd.read_csv and then immediately append the chunk into an HDFStore .
How can I get it to reindex sequentially without skipping ?
If you use the `` ix [ row , col ] = value `` syntax you can avoid this whole issue ( or in 0.11 , `` loc [ row , col ]`` and `` iloc [ row_number , col_number ]`` .
I tried with a pivot table but i only can have subtotals in columns #CODE
I can achieve this on excel , with a pivot table .
Then apply the groupby function and add a column City : #CODE
We can append the original data to the summed df by using append : #CODE
Note , I drop the second column because it's not relevant .
And I drop the first and last rows .
how to perform an inner or outer join of DataFrames with Pandas on non-simplistic criterion
we would like to produce a SQL-style join of both dataframes using a non-simplistic criteria , let's say " df_b.c > df_a.a " .
my current approach for inner join is to produce a cartesian product
for outer join , I'm not sure of the best way to go , so far
I've been playing with getting the inner join , then applying the negation
HYRY answered the specific question here but I needed something more generic and more within the Pandas API , as my join criterion could be anything , not just that one comparison .
For outerjoin , first I'm adding an extra index to the " left " side that will maintain itself after I do the inner join : #CODE
then we do the cartesian and get the inner join : #CODE
How would you produce a " join " of df_1 and df_2 on " c > a " ?
How would you produce the " left outer join " of same ?
Inner join , because this only calculate the cartesian product of ` c ` ` a ` , memory useage is less than cartesian product of the whole DataFrame : #CODE
to calculate the left outer join , use ` numpy.setdiff1d() ` to find all the rows of ` df_a ` that not in the inner join : #CODE
I have finally decided to use apply which I understand is more flexible .
Apparently you can pass a tuple of custom names to agg ( ' customname ' , ' nameoffunction ') but will it work here ?
I finally decided to use apply .
apply is more flexible than agg and transform because you can define your own function .
However the vertical x labels get cut off . when I try to change the bottom variable by saying
So the method is sending it's own value for bottom and I don't know how to replace that .
You could do a ` shift ` first : #CODE
The ` rename ` function should convert the the dictionary to a mapper and apply it to each index .
I want to resample a DataFrame with a multi-index containing both a datetime column and some other key .
And indeed it does , but i cant stack it again afterwards .
Strangely enough , i can stack the other column level with both : #CODE
Trying to use replace method with pandas
I am trying to do a simple replace with pandas : #CODE
The ` replace ` method was added in version 0.9.0 ( see release notes ) .
However , the example above requires the ` .names ` of the multiindex levels to be set in order to reorder them .
Of course , this makes it quite hard to use the names for reordering the levels of the multiindex .
Strangely , when trying a simpler example ( of both a single index and multiindex I can't recreate this on 0.10.1 - the names seem to propogate ) ...
if u r setting a lot of values like this it will be somewhat inefficient . oftentimes it's better to create a new frame and perform a join concat operation
I am thinking doing some kind of merge of our answers would provide the best one .
Should I apply the ` df.sortlevel ( level= ' Transition ')` after each ` df.set_value() ` call ?
How can I reindex a series created through the collapsing of a DataFrame ?
These columns all contain an identical kind of data , and I'd like to stack them into a single series , ergo : #CODE
From here , I can't quite figure out how to reindex my series such that the indexes go from 0 to ` len ( s )` .
Then one could run ` eval ( In [ 10 ])` to get the updated value for ` E ( x )` .
I was trying to create a pivot table and it gives me a ` MemoryError ` when trying to create the DataFrame .
I think at the end of the video Wes suggests this was mainly because of the bespoke date parser being used in read_csv , I don't see how this would help in a pivot .
How much memory in python using before the pivot ?
Once I start the assigning the columns and rows to the pivot table , then memory starts increasing and it fails once it reaches approximately 1.72GB of Memory .
Interesting ... could you share the code you are using to do the pivot ?
@USER : And I am using the following to create the pivot table LTable =p andas.tools.pivot.pivot_table ( df , values =[ ' Rat ' , ' B_AFLOW ' , ' B_VFLOW ' , ' B_PFLOW ' , ' C_AFLOW ' , ' C_VFLOW ' , ' C_PFLOW ' , ' Rat1 ' , ' Rat2 '] , rows =[ ' OverLine ' , ' Label '] , cols =[ ' Input_FileName '])
@USER but you are crashing before you pivot ?
What I really wanted was to do something like a boxplot : #CODE
but how can I transform the index to extract only the time from the date time ?
Calling ` map ` will be magnitudes slower .
I am having troubles understanding how pandas multiindex work .
how to merge two dataframes of different index level ( by row )
How can I merge t1 and t2 by rows ?
I have application that need to calculate the rolling sum of Pandas multilevel dataframe , and I want to find a way to shorten the processing time .
And use below command to calculate the rolling sum ( window size is 4 ) for each column data groupby ' IDX_1 ' : #CODE
How about shift it first and then add them together ?
pandas drops index index on merge in Python ?
I am merging two dataframes using ` merge ( ..., how= ' left ')` since I want to retain only entries that match up with the " left " dataframe .
The problem is that the merge operation seems to drop the index of my leftmost dataframe , shown here : #CODE
Before the merge , ` df1 ` was indexed by ` id ` .
After the merge operation , there's just the default numeric index for the merged dataframe and the ` id ` field was dropped .
How can I do this kind of merge operation but retain the index of the leftmost dataframe ?
edit : I could as a hack do : ` df1.reset_index() ` but merging and then set the index again , but I prefer not to if possible , it seems like merge shouldn't have to drop the index . thanks .
You could try indexing ` df1 ` and ` df2 ` by name instead of id , and then use ` join ` instead of ` merge ` like this : ` df1.join ( df2 )` which will preserve the index .
Why don't you set_index after the merge ?
I would but it drops the column that I want to be the index after the merge , namely ` id `
If you didn't call set_index before merge , it should still be around in df1
Preserve index after merge is a little bit strange while the method is " outer " .
You've already pointed out doing a reset_index before the merge and a set_index afterwards , which works .
The only way I know of to preserve indices across a merge is for the merge to involve an index on at least one of the data frames being merged .
to merge df2's index , which we've taken from its ' name ' column , against the ' name ' column on df1 .
Is there a better way to align pandas data in this way ?
The data will not be aligned so a simple concat will not be enough .
just reindex the frames before you concat in that case
Perhaps concat every file / frame and create a pivot table from the final DataFrame ?
I want read in weekday data and then reindex the data to fill the weekend with Friday's data .
I have tried the following code but it will not reindex the data .
I'm trying to join two dataframes in pandas to have the following behavior : I want to join on a specified column , but have it so redundant columns are not added to the dataframe .
If you are trying to merge columns from ` df2 ` into ` df1 ` while excluding any redundant columns , the following should work .
The whole point in this example is to ` append ` a ` DataFrame ` with some missing values to a ` HDFStore ` .
the condition column is a bool ) #CODE
The diff column are now timedelta64 [ ns ] , so you want integers in minutes
although you may want to play around with the ' shape ' and matching ` strides ` to get the exact windowed shape you are after .
I have seen that panda.read_table() has gone a long way , but it still at least takes at least as much memory ( in fact at least twice the memory ) as the original file size that we want to read to transform into a DataFrame .
HDF5 keeps a B-tree in memory that is used to map chunk structures on
Group by k1 , select column k2 and apply a lambda function .
PANDAS drop a range of rows from df
I want to drop m number of rows from the bottom of a data frame .
If you want to remove some middle rows , you can use ` drop ` : #CODE
It looks like I have to explicitly construct an array of size ` len ( df )` with nans and non-values ...
You can shorten the syntax -- using ` max ` , for example , or ` iget ` -- but other than that I'm not sure .
indeed this is much faster . transform seems to be slow for large dataset .
And then you can pivot ` df1 ` .
First using ` apply ` you could add a column with the signed shares ( positive for Buy negative for Sell ): #CODE
Use just those columns of interest to you and unstack them : #CODE
Reindex over whatever date range you like : #CODE
I use this ( somewhat kludgy ) R function to merge data.frames and keep the order of one of them : #CODE
In pandas a merge resets the index , but you can easily work around this by resetting the index before doing the merge .
Resetting the index will create a new column called " index " which you can then use to re-create your index after the merge .
Depending on DataFrame , sometimes you can use ` stack ` or ` set_index ` to make the index unique .
I've tried the pivot method , stack / unstack , etc . but these methods are not what I'm looking for .
Because you have a MultiIndex in place already , ` stack ` and ` unstack ` are what you want to use to move rows to cols and vice versa .
That being said , ` unstack ` should do exactly what you want to accomplish .
Or more simply , since by default ` stack ` / ` unstack ` use the innermost level , ` df2 = df.unstack() ` .
` pivot ` should work too , after resetting the index , i.e. ` df.reset_index() .pivot ( " major " , " minor ")` ( with or without a ` , " price ")` or ` [ " price "]` , depending on how important whether the name is still floating around is .
Ha nice , I was obviously not using unstack correctly , thanks .
pivot table subtotals
I get a pivot table with Grand Total row only ( " All ") .
Is there any efficient way to insert rows with subtotals into this table just as Excel pivot table with Field Setting > Layout Print > " Show item labels in tabular form " allows to do ?
what language do you want to perform the pivot in ?
I would demonstrate with a table but I have no idea how to insert a table on this website .
Grouping by ` Bool ` and ` Dir ` and applying an aggregation function ( ` sum ` , ` mean ` , etc ) would produce a DataFrame of a smaller size than you started with , as whatever function you used would aggregate values based on your group keys .
The way I looked at it was to examine the structure of data3.groupby ( level =[ 0 , 1 , 2 ]) .sum() which clarified why its necessary to append .groupby ( level =[ 0 , 1 ]) .cumsum() .
Stated differently , you actually want to group by [ Bool , Dir , Date ] , calculate a sum in each group , THEN return a cumsum on rows grouped by [ Bool , Dir ] .
This looks like a job for ` transform ` : #CODE
Pandas : rolling mean by time interval
I've got a bunch of polling data ; I want to compute a rolling mean to get an estimate for each day based on a three-day window .
First resample the data frame into 1D intervals .
Perhaps I'm mistaken here , but are you ignoring multiple readings from the same day ( when taking the rolling mean you'd expect two readings to carry more weight than one ... )
Hope my suffix conventions are readable : _s : string , _i : int , _b : bool , _ser : Series and _df : DataFrame .
Also can use ` df.iloc [ np.random.permutation ( np.arange ( len ( df )))]` if there's dupes and stuff ( and may be faster for mi ) .
Note that ` numpy.rollaxis ` brings the specified axis to the first dimension and then let's us iterate over arrays with the remaining dimensions , i.e. , if we want to shuffle along the first dimension ( columns ) , we need to roll the second dimension to the front , so that we apply the shuffling to views over the first dimension .
The ` bar ` method takes a parameter ` align ` .
` align ` aligns the bars on the center of the x values we give it , instead of aligning on the left side of the bar ( which is the default ) .
I get a ValueError ( " Currently , you need to give a freq if dates are used . ") , which according to [ #URL should be allowed to be ' None .
The freq issue should be fixed in master , but is not yet released yet .
how does boxplot in pandas / python work ?
I found this link and I'm trying to understand how boxplot works .
I would expect boxplot to want 4 values for each box however it seems to construct a box for each 10 points and I was wondering what is going on in the background ..
Each box depicts the median ( red line ) , 25th percentile ( lower edge of the box ) , 75th percentile ( upper edge of the box ) , and the most extreme observations ( the whiskers ) .
the feature is very useful , now i have to maintain one set of book-keeping code to hold and align the meta data .
You could also use ` ix ` if you set ` TYPE ` as the index first : #CODE
Pandas - finding the row with least value of one of the levels of a multiindex
So , I have a DataFrame with a multiindex which looks like this : #CODE
I must pass the columns to drop the duplicates , right ?
But the duplicates I wanna drop are in one of the levels of an index , not as a column .
df.reset_index() would insert the index values as columns .
which will drop the datetime field to a column before the groups are grouped by groupby , therefore it will remain in the dataframe in the result .
If you have a MultiIndex where level 0 has values like ' abc-8182 ' and level 1 has timeseries values , then the above code should drop that duplicate row and keep the first in your example .
Still not sure what you're asking , but if you have a function for one value , you can then use Series / DataFrame ` apply ` or index's ` map ` , e.g. ` df.index.map ( lambda t : t.value )` ?
For example to perform a 5 minutes mean you can use the ` resample ` function like this : #CODE
You can also use the standard time functions yearmon() or yearqtr() , or custom functions for both split and apply .
Then ` len ( v1 ) == len ( v2 )` and these can be plotted as CDFs of ` a , b ` on the same scale .
The way we use it in some goodness of fit tests is to stack the arrays , so they are defined on all points , points from both arrays .
I'm looking to aggregate the data into groups based on the value of a column " A " .
can u give a small example of what kind of functions u r going to apply with the group ? and a small example frame would be helpful .
is there some way stack and unstack might be able to help me ?
An alternative would be to unstack the State and City columns before resampling , but i doubt if thats more efficient .
An alternative using stack / unstack #CODE
Possible pandas bug - stack ( level =[ 2 , 1 ]) worked , but stack ( level =[ 1 , 2 ]) failed
For a series , you can do it with ` append ` , but you have to create a series from your value first : #CODE
For a DataFrame , you can also use ` append ` or ` concat ` , but it doesn't make sense to do this for a single cell only .
This means don't apply any automatic data conversions .
We can now merge these two ` DataArray ` on their common ` x ` dimension into a ` DataSet ` : #CODE
And we can finally access and aggregate data the way you wanted : #CODE
Pandas dataframe merge issue
One of the examples in Chapter 2 is a merge of MovieLens data on movie_id that is not working .
The merge returns an empty data frame .
Unfortunately , the type isn't changed and the merge still returns an empty set .
My ` movie_id ` Series in the movies dataframe is ` Name : movie_id , Length : 3883 , dtype : int64 ` , which seems right , and the merge behaves the way you'd expect .
See if your merge now works .
Pandas DataFrame concat vs append
I have a list of 4 pandas dataframe containing a day of tick data that I want to merge into a single data frame .
I find can't seem to understand the behavior of concat on my timestamps .
Using append I get : #CODE
Using concat I get : #CODE
Notice how the index changes when using concat .
Why is that and how would I go about using concat to reproduce the results obtained using append ?
( Since concat seems so much faster ; 24.6 ms per loop vs 3.02 s per loop )
So what are you doing is with append and concat is almost equivalent .
I almost always use concat ( though in this case they are equivalent , except for the empty frame );
+1 , it would be really nice to see the performance diff ...
I am getting an exception on the transform call because I have duplicate index values .
inner join A and B , then ffill on B #CODE
I'm not sure there's a vectorized hook , but you can use ` apply ` , anyhow : #CODE
You need to use ` concat ` or ` append ` to add rows to a data table .
See section " Merge , join , and concatenate " in the Pandas manual .
The idea for utilizing Pandas vs MySQL is to conduct this data import or append + stat analysis periodically throughout the day .
The ` append ` method on an instance of a DataFrame does not function the same as the ` append ` method on an instance of a list .
or you can use ` concat ` : #CODE
I need use this function to iterate over a bunch of files , grab a key piece of info , and merge it back into the current data structure on ' url ' .
I'd have to resample the data forst into weekly ohlc .
Converting hourly ohlc to weekly ohlc might take a little bit of coaxing as you would have to sample open , high , low , close differently ( ' how ' parameter of resample )
You could then pivot on this column to have columns Q1 ..
Once I get the years as integers , any subsequent group / transform operations run plenty fast ; at least , I do not think you can do substiantially better without digging into Cython .
" Once I get the years as integers , any subsequent group / transform operations run plenty fast " , i test it , yes , it is . great tip . thanks .
In essence you are flattening the blink frame to a series that you then can apply to each of the trial
If so , doesn't that mean I can just replace them with corresponding the columns from my ` trials ` dataframe ?
First , sort the ` blink ` by ` tstart ` before you check each row in ` trials ` , merge the overlapped ones , it takes ` O ( n log n )` , take a look at this
Use the binary search to insert the start in the sorted ` tstarts `
Use the binary search to insert the stop in the sorted ` tstarts `
Did you try using ` ix ` instead of ` xs ` ( e.g. , ` pd_price.ix [ pd_prices_index , s_symbol ] = size `) ?
You could use join , but it really depends on what sort of result your looking for .
Current fix : create a new time vector : time = [ t0 + dt.timedelta ( hours=i ) for i in range ( len ( nts.data ))]
I think you can do this using ` .shift() ` , which can shift a series forward or backward ( defaulting to one forward . ) You want to keep rows if they're not the same as the next ones , so something like : #CODE
pandas time series join
The keys are not under my control and not a perfect equal join where one and only one match is found .
My though is to do the join then remove negative durations because I know triggers must precede actions .
I know that my result may include some artificially short durations but I suspect due to the nature of the join key reuse that this will happen seldom and do not pose a problem for the rest of the analysis .
Previously I had been using arbitrarily selected max durations to toss out erroneous join data but tossing these outliers will be problematic because high outliers are what I am most interested in .
3 . order the result DF by the join key and the duration [ descending order ] ( will look into that )
4 . create a new Boolean field making use of the shift ( 1 ) comparing the current record to see next has a lower duration but the same key .
5 . select out all the records where my new bool field indicates that it is the lowest for its key .
you also might find some of these recipes useful : #URL e.g. you may actually need to look several periods ( and not just 1 on your shift ) , but you idea looks about right
Is it possible to insert a row at an arbitrary position in a dataframe using pandas ?
What I would like to do is insert a row at a position specified by some index value and update the following indices accordingly .
You could slice and use concat to get what you want .
As far as I'm aware , concat is the best method to achieve an insert type operation in pandas , but admittedly I'm by no means a pandas expert .
A series is a 1-dimensional object ; its transpose is ( vacuously
Wait : it indeed works for the example I've shown , but it doesn't exactly solve the whole question : whatever the orientation of t ( can be t or transpose t ) , it always outputs the same result !
why don't you use dot vs outer instead of doing transpose ?
Group them and then apply our customized function
How to join these two pandas data frames ?
I am unable to join Data Frame 1 with Data Frame 2 , I suspect this is due to one of them having an int64 index and the other having a string index .
If not , how to merge these two data frames .
This seems to have solved the problem and I am now able to join .
I eagerly await the addition of a ` truncate ` option .
As a workaround , just split your queries up into multiple ones and concat the results .
Prospectively I would like to normalize the values for each picture ( based on the 3 repetitions - so 3 numbers ) and run an ANOVA on the categories : first presentation , second presentation , third presentation ( for all pictures ) .
` groupby ` and ` pivot ` are good for performing aggregations over groups of data or when you have something specific you need to do to individual groups .
` stack ` and ` unstack ` help with reshaping your data , but move indexes to cols and vice versa .
You could try something like , ` meas_id = [ 1 , 2 , 3 ] * ( len ( df2.index ) / 3 )` and then do ` df3 = df2.reset_index() .set_index ([ " pic_name " , meas_id ])` .
Basically , read it in , and then do the pivot yourself , keeping every other element and then fixing the column names .
I would like to change this to a new dataframe , with the columns as the first level index of the MultiIndex dataframe ?
Pandas : reindex multiindex , broadcast results
I have a multiindex dataframe with sales data for different regions , sizes , and dates .
I thought I could group by size and date , then cumsum() and then take the answer and reindex it to the original dataframe , but that doesn't seem to work .
Instead of ( implicitly ) applying ` sum ` and ` cumsum ` , use the ` transform ` method on each group .
I'll take a crack at your bonus question : you could map ` Timestamp ` onto your list .
I get an assertion error when I try your transform suggestion .
Another tricky way is to transpose it first , divide it , and then transpose back : #CODE
Then you can ` apply ` this ( row-wise ): #CODE
I also worked out that ` df.ix [ np.random.random_integers ( 0 , len ( df ) , 10 )]` would also work .
Actually this will give you repeated indices ` np.random.random_integers ( 0 , len ( df ) , N )` where ` N ` is a large number .
apply changes to the column names of a dataframe
Worse it's not consistent within a column , so without dayfirst it looks up US-style dates first then UK-style , so if you're all UK-style it'll translate before the 12th : day-mon-year as mon-day-year , and 13th onwards correctly .
Take the time difference ( using ` shift ` ) til the next value , and multiply ( value * seconds ): #CODE
Then do the resample to seconds ( sum the value*seconds ): #CODE
you can isnull ( df [ ' difference ']) will give True on NaT , so you could subtract then use mask I think
These " MetaIndex " are basically a string + metadata and would replace the usual column labels in my_dataframe.columns The class definition looks something like that : #CODE
Concerning the propagation issue , I know that's what is making you sceptic about the feature , but what I am looking for is a system that works according to common sense in simple situations ( initializing a DataFrame with a dict of series , or join an extra column ) , and drops the metadata that anyway would not be trustable in not-obvious-situations .
Then I close the file , delete the data from memory , do the next 500 columns ( labelled by an increasing integer ) , open up the store , and try to append the new columns .
You will want to append on rows and make it your longest dimension .
If you want to use the result of list comprehension as an index , you should use : ` df [[ x for x in df [ ' shift '] if df [ " MA10 "] > df [ " MA100 "]]]` , but I think this will raise some exception .
Using ` apply ` : #CODE
You can use the ` .shape ` property or just ` len ( DataFrame.index )` as there are notable performance differences : #CODE
EDIT : As noted @USER Allen in the comments ` len ( df.index )` and ` df [ 0 ] .count() ` are not interchangeable as ` count ` excludes ` NaN ` s ,
Also , remember that `` len ( df.index )`` and `` df [ 0 ] .count() `` are not interchangeable : `` count `` excludes NaNs , which is probably helps explain why it is slower .
There's one good reason why to use ` shape ` in interactive work , instead of len ( df ): Trying out different filtering , I often need to know how many items remain .
Use ` len ( df )` .
Due to one additional function call it is a bit slower than calling ` len ( df.index )` directly , but this should not play any role in most use cases .
I am trying to get only the last row based on the first level of the multiindex , which is date in this case .
And select these rows with ` ix ` : #CODE
If I strip the tzinfo with ` .tz_convert ( None )` , the date gets converted to UTC : #CODE
Is there a TimeSeries method to apply ` .replace ( tzinfo=None )` to each date in the index ?
You can use ` apply ` to do this : #CODE
There are a few edge cases in pandas ' apply which have been tweaked over last few releases so this could be one of them !
You can convert this column to integers by ` apply ` -ing ` int ` : #CODE
Wondering if there is something as silly as it is failing because the df does not exist in the h5 ( and hence neither do any columns ) at the time of the first append ?
Simple workaround is to add ` data_columns=True ` to your append statement , but I have also updated the code to automatically create the data_columns if you pass a valid column name .
#URL just pass a dict of your keys to pieces to concat
Thanks I guess I just have to iterate over each of the hierarchy and apply pivot_table to get my desire output .
I wish to consolidate all organisms into one column , in the DataFrame below , based on the following rules .
I got lost when I tried to sequentially map AS201 -> AS101 , AS202 -> AS102 , AS203 -> AS103 etc . based on the condition .
` mask ` is then a boolean Series .
Note : Usually it would be enough to use ` df [ orgi ] .str .contains ( ' aureus ')` ( without the ` == True ` , but since ` df [ orgi ]` might contain ` NaN ` values , we need to also map the ` NaN ` s to False , so we use ` df [ orgi ] .str .contains ( ' aureus ') == True ` .
Rather than ` mask = ( df [ ' ORG1 '] ! = ' A ') & ( df [ orgi ] == ' A ')` , how would I mask with a regular expression on ORG1 and orgi in the mask ?
I tried ` mask = ( re.search ( ' aureus ' , x ) ! = None for x in df.ORG1 ) & ( re.search ( ' aureus ' , x ) ! = None for x in df [ orgi ] )` and got nowhere
Now that we have things set up we will be using the ` Panel ` method ` resample ` .
Note that the `' 24H '` parameter we gave the ` resample ` method was simply an extension of the example posted by the OA .
Notice that you don't have to pass starting and ending times that align perfectly with the frequency you passed in .
I am not sure what you mean by accumulation , but the ` how ` parameter in ` resample ` can take function names or numpy array functions that returns an aggregated form of the input array .
Which I would like to transform into a matrix where the new columns and indexes are unique values of two of the columns ( ` A ` and ` B `) and the cells are the join between these two unique values from a third column ( ` C `) .
I'm then trying to glue all of these Series together as rows in a new DataFrame , but can't get any of them to either ` append ` or ` concat ` into the new DataFrame ( following both the docs or similar questions on SO ) , e.g. #CODE
You could extend it for Multiindex dfs or automatic type detection using ` eval() ` if needed .
I have used pandas to concat all the dataset into a single dataframe .
now i want to extract all the notnull values from first few rows ( say from row 1 to row 6 ) .
As a heads up , ` irow ` will be deprecated in the next release of pandas .
New methods , with clearer usage , replace it .
But apparently the command you mentioned is not what i want :( :( in a row , i need to extract all notnull values . => for several rows , without iterating , can i do it in a more compact way is the question . Thank you so much for replying :)
Rolling record of first time changes ( & diffs ) between two columns in a DF
_ Keep a rolling record of the values ONLY at the first col change , for both columns .
Some combination of the `` shift `` and `` isnan `` methods should do it .
( With `` dropna `` and `` diff `` , ultimately . )
Is there a way to change diff so that we are always subtracting FROM column 2 ?
I wrongly thought you were referring to the iget used along with transform .
When you shift dates the first one is a NaT ( like a nan , but for datetimes / timedeltas ) #CODE
Confusion regarding Merge in Pandas
I am trying to merge two pandas dataframes without index : #CODE
Concatenate each row's identifer by applying ` join ` row-wise .
Unstack it , and join it to the original DataFrame on ' foo ' .
` lambda s : tuple ( s )` is the same as just ` tuple ` , just like ` lambda x : len ( x )` is the same as ` len ` .
It's less complicated and faster than using ` apply ` or ` map ` .
Using Python Pandas how do I convert ( transpose ) the above structure to the following ?
My attempts at using pivot have been futile .
I've tried merge and join ( obviously incorrectly ) but I am getting a bunch of NaNs when I do that .
` merge ` and ` join ` could both get you the result DataFrame you want .
Since one of your DataFrames is indexed ( by ID ) and the other has just a integer index , ` merge ` is the logical choice .
Merge : #CODE
The midpoint formula that I wish to apply is dependent on the bid / ask spread of the instrument .
If I'm reading the code right , the intent is to apply this function to corresponding elements of the four ` DataFrames ` .
I think you are understanding what I am aiming to do - essentially apply the midpoint formula for each symbol , and at each timestamp , in the same way that I could get a DataFrame of bid / ask spreads using ` spread = ap - bp ` .
Pandas : add a column to a multiindex column dataframe
I would like to add a column to the second level of a multiindex column dataframe .
pandas dataframes have their own boxplot method ( i.e. ` mydataframe.boxplot() `) .
I would like to plot them alongside something else , the pandas boxplot function creates a new figure for each boxplot set . also , apparently it won't let me customize color or position .
You need this odd apply at the end because not yet full support for timedelta64 [ ns ] scalars ( e.g. like how we use Timestamps now for datetime64 [ ns ] , coming in 0.12 )
How would I replace the names in the source and target files with the indexes from the nodes file ?
Then we make the ` dict ` we want to replace things with : #CODE
But we can ` stack ` and ` unstack ` to get around this : #CODE
PS : the same dict-based approach will work if you only want to apply the replace to certain columns , but you'd have to restrict the application .
The replace method only replaced the first column .
One possibility is that there's whitespace in one column not present in another , and so the replace isn't working because there's not actually a match .
Can stack / unstack be useful here ?
i.e if entry occurs 1st time i need to append 1 if it occurs 2nd time i need to append 2 and likewise i mean i need to count no of occurences of an email address in the file and if an email exists twice or more i want difference among dates and remember dates are not sorted so we have to sort them also against a particular email address and i am looking for a solution in python using numpy or pandas library or any other library that can handle this type of huge data without giving out of bound memory exception i have dual core processor with centos 6.3 and having ram of 4GB
Use the built-in sqlite3 database : you can insert the data , sort and group as necessary , and there's no problem using a file which is larger than available RAM .
The combiner function does not reduce , but instead calculates your function ( the diff in days ) between all elements in that chunk , eliminating duplicates as you go , and taking the latest data after each loop .
My first thought was also to put data in database but it wont do the trick i need to track 1st occurence of each email address also and count too and what about getting the diff of dates whats the soln for that ??
so the first time an email appears its email is then the reference date for subsequent appearances ? for the 2nd email it's easy , count is 1 and days is diff of days , what about 3rd email . does days get updated to be the diff between 3rd date and 1st date or is the number of days somehow involved ( maybe the 3rd days is max of current and 3rd date - reference date ? )
Merge columns of a dataframe into a list after merging by the first column
But , to answer your question , you can apply ` list ` .
and append each to a new df .
Depending on how complex my desired cut is , I often use a listcomp , like ` df [[ col for col in df.columns if some_complex_condition ( col )]]` .
So try it without passing the freq argument .
I don't have time to tease it out , but I was thinking something like ` df.sort ( " char ") .groupby ( np.arange ( len ( df )) % 2 )` .
Pass a function to ` apply ` and specify ` axis=1 ` .
The replace method itself replaced and returned the new values correctly , but the inplace option seems not to affect the original dataframe when applying a conditional .
How do I create custom rolling window functions ?
I think the na_values option does not apply to the columns that is being parsed as dates .
But I get the idea , I can extract indices , shift them and create another series from which I'll take delta's .
Adding two pandas series objects will automatically align the indexed data but if one object does not contain that index it is returned as NaN .
So I think I need to drop back to iterating with ` df.iterrows() ` , as per this ?
I think you'll have to ` merge ` your features into the dataset .
This ways better than user1827356 and Zelazny7 as it dosn't require an merge which is expensive
Pandas Data Frame join by grouping
drop a single tuple from a multi tuple column
Is there a way to transform this another way or is there a way to drop ' b_val ' from each of the column names ?
I think ` unstack ` is the correct way to do what you've done .
You could drop the first level from the the column names ( a MultiIndex ) using ` droplevel ` : #CODE
However , a cleaner option is just to ` unstack ` the column ( the series ): #CODE
If u have a zillionty-one data items , and the next few lines would have reasonably partioned , level-of-detailed , and clipped them , the whole shmegegge is still safe to drop in .
I've loaded my dataframe with read_csv and easily parsed , combined and indexed a date and a time column into one column but now I want to be able to just reshape and perform calculations based on hour and minute groupings similar to what you can do in excel pivot .
I know how to resample to hour or minute but it maintains the date portion associated with each hour / minute whereas I want to aggregate the data set ONLY to hour and minute similar to grouping in excel pivots and selecting " hour " and " minute " but not selecting anything else .
with the ultimate goal of doing a boxplot on the dataframe .
strange , I thought this fits very well with the ` pivot ` example in [ the doc ] ( #URL ) , but I keep on getting ` AssertionError ( ' Length of index , columns , and values must be the same ')` .
However an apply within an apply still isn't going to be particularly efficient ...
How to apply a function to several columns of a GroupBy object ?
If you dont specify a function per column , all columns will be passed to the function ( for both apply and agg ) .
You need to reshape with .reshape() or add an index by reindex by using np.newaxis .
if i could just strip out the minute part that would be good enough .
Take a loot at transpose #CODE
Swapping axes = transpose .
This can also be done using apply , no need to sort .
Thanks for you update , I originally thought you wanted the diff per buyer , not from the first buyer , but that's just a minor tweak .
However , you were right I am also trying to calculate the diff per buyer ( which would be for Kevin : 120 ( John-Kevin ) + 60 ( Mark-John ) = 180 ) and finally aggregated those daily sums to a monthly granularity ( which would be for John 180 ( 2012-01-01 ) + 60 ( 2012-01-02 ) = 240 ) .
Furthermore , you may need to put a Timegrouper on a monthly basis on the whole thing to aggregate per month , but should be a simple extension
I want to use groupby() .transform() to do a custom ( cumulative ) transform of each block of records in a ( sorted ) dataset .
but you can make it work if you transform the entire dataframe , like : #CODE
As a workaround , in earlier pandas you can use apply : #CODE
Is transform somehow more restrictive ??
@USER I'm wondering if it's a bug , it certainly looks like it should fit in the transform category ...
@USER tranform expects one result to all the things in the group , whereas apply expects a value for each row in the group .
For example [ here ] ( #URL ) it starts by describing transform as a form of apply , and later makes them sound almost equivalent : " ...
For these , use the apply function , which can be substituted for both aggregate and transform in many standard use cases .
However , apply can handle some exceptional use cases , for example ...
As far as it stands , this is not a well formulated question : you basically ask " how to import data into pandas series from a txt file , aggregate counts by day and make a plot " = " write program for me " .
NW , I reckon that link gives the answer - just replace the aggregate function with count .
Also simply having the terminology " aggregate " helped me break the back of this thing .
Is there an efficient way to merge two sorted dataframes in pandas , maintaing sortedness ?
If I have two dataframes ( or series ) that are already sorted on compatible keys , I'd like to be able to cheaply merge them together and maintain sortedness .
( Ironically when I started using pandas I assumed the " merge " operation did just this , instead of being a form of join . )
What's the equivalent of cut / qcut for pandas date fields ?
Pandas cut and qcut functions are great for ' bucketing ' continuous data for use in pivot tables and so forth , but I can't see an easy way to get datetime axes in the mix .
To bin by groups of price or quantity , I can use cut / qcut to bucket them : #CODE
It seems like resample() has all of the machinery to bucket into periods , but I can't figure out how to apply it here .
The buckets ( or levels ) in the ' date cut ' would be equivalent to a pandas.PeriodIndex , and then I want to label each value of df [ ' recd '] with the period it falls into ?
Just need to set the index of the field you'd like to resample by , here's some examples #CODE
This doesn't seem like a general solution , e.g. if I want to group on two different dates , or a date and a non-date ( via cut or category variable ) .
How about using ` Series ` and putting the parts of the ` DataFrame ` that you're interested into that , then calling ` cut ` on the series object ?
I have a Pandas Series where each element of the series is a one row Pandas DataFrame which I would like to append together into one big DataFrame .
concat them : #CODE
So all I wanted to do was reindex it with ` range ( len ( data ))` , but I get this error : #CODE
drop duplicates in Python Pandas DataFrame not removing duplicates
So drop duplicates is not removing anything .
Say two values are " nearly " equal ( say x1 and x2 ) , is there any way to replace them in a way that they are both equal ????
What I want is to replace x2 with x1 if they are " nearly " equal .
One workaround would be to round the data to however many decimal places are applicable with something like ` df.apply ( np.round , args =[ 4 ])` , then drop the duplicates .
The second line uses the ` apply ` method on groupby to replace the dataframe of near-duplicate rows , ` g ` , with a new dataframe ` g.apply ( lambda row : g.irow ( 0 ) , axis=1 )` .
That uses the ` apply ` method on dataframes to replace each row with the first row of the group .
Hopefully someone who knows pandas better than I do will drop by and show how to do this better .
This isn't a single process solution and will not map to mclapply .
This functionality is available from the map method in multiprocessing.Pool()
See this answer for more details : Is there a simple process-based parallel map for python ?
You could define a mask separately : #CODE
a mask is just a boolean array . using 2 terms is just anding ( or or-ring ) with another one , very efficient . and you are only doing this once ( the dataframe takes the boolean mask and applies it to the underlying numpy data )
A mask is efficient and works great here , I just missed the fact that I could combine it into one line before
no , what I meant was that the mask is computed only once ( per dataframe ) , and not for each row
Update : Dan Allan's ` resample ` suggestion worked , but now the the xticks are unreadable .
I think this task is more easily accomplished using ` resample ` , not ` group ` .
Calling unstack ( level=1 ) solved it .
I would like to replace each NAN ' x ' with the previous non-NAN ' x ' from a row with the same ' id ' value : #CODE
Concatenating one column to a string can be done using join .
You can create a column in your ` DataFrame ` based on your Days Late column by using the ` map ` or ` apply ` functions as follows .
Create a helper function to transform the data of the Days Late column and add a column called Code .
Lastly , you can use ` groupby ` to aggregate by the ID and Code columns , and get the counts of the groups as follows : #CODE
Can you tell me how I can pivot these results .
The ` groupby ` method generates a ` Series ` with a ` MultiIndex ` .
You can use ` unstack ` to pivot the lowest level index into columns , as shown in the edited answer above .
and use the ` unstack ` method to turn the ` status ` index into columns : #CODE
Pandas DataFrame from WB WDI data : combine year columns into " year " variable and merge rows
This seems like some complex pivot or opposite-of - " melt " , but I cannot figure it out .
Group your ` DataFrame ` by ` country ` and ` countrycode ` and then apply your own function : #CODE
There are sveral stack / unstack examples in the docs #URL ( same page also demos pivot and melt ) .
I'm suggesting that @USER may put this in his ( accepted ) answer , as it uses the actual names from the WDI data , and makes it more cut and paste for someone else using them .
Pandas : How to use apply function to multiple columns
I have some problems with the Pandas apply function , when using multiple columns with the following dataframe #CODE
When I try to apply this function with : #CODE
If you just want to compute ( column a ) % ( column b ) , you don't need ` apply ` , just do it directly : #CODE
@USER following [ 53-54 ] allow you to apply more complex functions .
The numpy array is not mapped and thus cannot align .
Pandas error : ' DataFrame ' object has no attribute ' loc '
` loc ` was introduced in 0.11 , so you'll need to upgrade your pandas to follow the 10minute introduction .
In fact , at this moment , it's the first new feature advertised on the front page : " New precision indexing fields loc , iloc , at , and iat , to reduce occasional ambiguity in the catch-all hitherto ix method .
I am finding it odd that ` loc ` isn't working on mine because I have pandas 0.11 , but here is something that will work for what you want , just use ` ix ` #CODE
` loc ` works for me with 0.11.0 .
So , in your case , when you try to assign ` df_a ` to ` df.ix [ ' a ']` , the indices do not match ( MultiIndex vs normal index ) , and nothing gets assigned ( or more exact : filled with NaN's ) .
( I suppose it has to be ' mask ') 2 ) What value do you want in the ' values ' column in the resulting dataframe ?
If you want the original columns in your result , you can first calculate the grouped and aggregated dataframe ( but you will have to aggregate in some way your original columns . I took the first occuring as an example ): #CODE
This is obviously using the data generated below , but you can easily apply to your example .
The ` loc ` then takes that boolean array and select the applicable columns #CODE
that's in 0.11 . you can use ix in place of iloc loc
I know I can us Pandas ' resample ( ' D ' , how= ' sum ') to calculate the daily sum of P ( DailyP ) but in the same step , I would like to use the daily P to calculate proportion of daily P in each hour ( so , P / DailyP ) to end up with an hourly time series ( i.e. , same frequency as original ) .
A possible approach is to reindex the daily sums back to the original hourly index ( ` reindex `) and filling the values forward ( so that every hour gets the value of the sum of that day , ` fillna `) : #CODE
Following the scikit-learn tutorial here , if we have a ` Pandas.DataFrame ` that has a column named ` colors ` , how can we create a loop to loop through all of the DataFrame's columns ( or a list containing the required columns ) so that all categorial variables ( eg . variable ` colors ` that can have values ` blue ` , ` red ` , ` purple `) will be replaced by ` len ( colors )` number of dummy variable columns ` colors #blue ` , ` colors #red ` , ` colors #purple ` ?
If I understand you , you don't actually want ` shift ` , you simply want to make a new column next to the existing ` DATE ` which is 180 days after .
How do strip I convert Unicode to strings ?
In the special case where you have a columnar MultiIndex , but a simple index , you can transpose the DataFrame and use ` index_label ` and ` index_col ` as follows : #CODE
If the dataframe has more than the two series , and you only want to plot those two , you'll need to replace ` df ` with ` df [[ ' korisnika ' , ' osiguranika ']]` .
You can use ` ix ` like you were , but applying a different slice ...
I think a more explicit way of doing this is to use drop .
` drop ` can even be calculated in-place ( without extra assignment ) .
428 base , mult = _gfc ( freq )
--> 429 return tslib.dt64arr_to_periodarr ( data.view ( ' i8 ') , base , tz )
Here's a way to create that pivot table with ' size ' as the aggregating function : #CODE
you should mask here !
You probably need ` apply ` , so something like : #CODE
Wrapping it in a Series in the apply returns a DataFrame : #CODE
Making it a multiindex , plotted all 4600 rows :)
I think I'd drop the first level of the multi-index ( the one with PARTS ) * : #CODE
But I found that i could not do the drop level df1.columns = df1.columns.droplevel ( 0 ) as you specified .
You could also ` apply ` ` np.prod ` , which is what I'd originally done , but usually when available the direct methods are faster .
Problem : the append traps this exception
I tried a df = df.reset_index ( drop = True ) but I still seem to have NotImplementedError : indexing 64-bit unsigned integer columns is not supported yet , sorry
I don't know much about plotting , but ISTM you can use ` groupby ` the way you want [ NB : this assumes your index consists of integers , not strings -- replace ` 0 ` by `' 0 '` if I'm wrong ]: #CODE
I've seen a few solutions which map / do list comprehension to ' manually ' put the dataframe together .
I was hoping pandas had some basic function to magically do this kind of thing ... apply ?
join ?
Try to use merge to achieve your goal .
And then append a new columns for ` index ` : #CODE
It just generate a new object but not replace the original one .
multi-column factorize in pandas
The pandas ` factorize ` function assigns each unique value in a series to a sequential , 0-based index , and calculates which index each series entry belongs to .
You can use ` drop_duplicates ` to drop those duplicated rows #CODE
To achieve your goal , you can join your original df to the drop_duplicated one : #CODE
I'm not looking to drop them , but to assign a unique index to each pair of distinct values ( i.e. I eventually want to add a new column to the data frame , with values [ 0 , 1 , 2 , 2 , 1 , 0 ]) .
The reason why the column names of ` x ` must match the index names of ` y ` is because the pandas ` dot ` method will reindex ` x ` and ` y ` so that if the column order of ` x ` and the index order of ` y ` do not naturally match , they will be made to match before the matrix product is performed : #CODE
Hello I have an hourly timeseries that I would like to mask if the timeseries index is outside business hours .
I have tried generating a BDay date range and then changing the freq to hourly but this doesn't work .
To restrict to Business days within business hours ( apply these in either order ) .
How to insert pandas dataframe via mysqldb into database ?
I can connect to my local mysql database from python , and I can create , select from , and insert individual rows .
My question is : can I directly instruct mysqldb to take an entire dataframe and insert it into an existing table , or do I need to iterate over the rows ?
I'm looking for a way to directly insert into mysql without the csv detour .
` if_exists : { ' fail ' , ' replace ' , ' append ' } ` , default `' fail '`
` replace ` : If table exists , drop it , recreate it , and insert data .
` append ` : If table exists , insert data .
Issue with Pandas boxplot within a subplot
I'm having an issue drawing a Pandas boxplot within a subplot .
Based on the two ways I'm trying , creating the boxplot either removes all the subplots that I've already created , or plots the boxplot after the subplot grid .
Any ideas how I can get my boxplot image to appear in the bottom right grid in the subplots ( the one that's empty in the first set of images ) ?
What is going on is that if you supply a ` by ` argument to ` boxplot ` , pandas issues its own ` subplots ` call , erasing any existing subplots .
It apparently does this so that , if you want to plot more than one value , it will create subplots for each value ( e.g. , one boxplot for Y1 by day , another for Y2 by day , etc . ) .
pandas dataframe with variable length multiindex replaces values with NaN
A MultiIndex can't have indices of different lengths .
In the mean time , I avoided using a value with NaNs in any multiindex , for instance by using some other flag value ( -99 ) for the NaNs in the column ' QC ' .
Then aggregate the dups : #CODE
You are looking for pandas pivot .
I don't know how I forgot about pivot tables
I'm looking for a way to transform this kind of data to #CODE
My first idea is to resample DataFrame
Although this answers how the OP ( or anyone else who stumbles here ) * could * do this ... they * really * shouldn't be rolling out their own ( less efficient ) numpy masking .
about concat in pandas : using row data to create new columns
I want to do a custom concat i.e using the rows in a group by object
You could resample ` df2 ` to 5 min and fill it .
In the end , I simply cut those rows from my data .
Also , if you want to have the ticklabels / tickmarks of the x-axis connected to the " middle axis " ( also while panning / zooming ) , then it's easiest to insert an extra spine ; take a look at [ ` mpl_toolkits.axisartist `] ( #URL ) for some examples of this .
I would just append the ` entry_frame ` dictionaries to a list and then create a DataFrame from a list of dicts after the whole file is read .
and when I go to resample it : #CODE
I wouldn't expect that bin to be there , as my 28 days are evenly divisible into the 7 day resample I'm performing .
Is there a more efficient way to map in a dynamic number of abc -type columns ?
@USER I'm not sure what you mean , so ApplyMap applies the function to every cell ( being every intersection of row and column ) so basically across the entire dataframe .
merge 2 dataframes in Pandas : join on some columns , sum up others
I want to merge two dataframes on specific columns ( key1 , key2 ) and sum up the values for another column ( value ) .
Here's a way with stacking , first clean up the columns to a ` MultiIndex ` : #CODE
Then you can ` stack ` ( first by `' Marker '` then by `' mrk '`) : #CODE
Perhaps you could get the best of both worlds by using a MultiIndex : #CODE
However , if you really want to drop the second level of the index , you could do this : #CODE
for the stupid hack solution , you can do search / replace in the csv and rename ` NA ` to something like ` NA_safe ` .
You didn't need column names not to have spaces , but you did need to join them by something you could separate .
Is it possible to append to an empty data frame that doesn't contain any indices or columns ?
but the ` append ` doesn't happen in-place , so you'll have to store the output if you want it : #CODE
actually that append doesn't happen in place is the most important info here ;)
As the next step , I would like to read the files into a single pandas dataframe in order to apply standard non-distributed algorithms .
plot column 1 where column 2 is notnull , but with different style .
In 11.0 all three methods work , the way suggested in the docs is simply to use ` df [ mask ]` .
However , this is not done on position , but purely using labels , so in my opinion ` loc ` best describes what's actually going on .
Update : I asked on github about this , the conclusion being that ` df.iloc [ msk ]` will give a ` NotImplementedError ` ( if integer indexed mask ) or ` ValueError ` ( if non-integer indexed ) in pandas ` 11.1 ` .
Perhaps worth noting that if you gave mask integer index it would throw an error : #CODE
To be honest I had actually forgotten that mask has an .index attribute in this case , I was thinking of it as purely a boolean numpy array .
It is very simple if I use simple operations ( ` + ` , ` - ` , ` * ` and even ` abs `) .
For example if I try ` abs ( df [ ' col1 '])` I get no complains but if I try ` asin ( df [ ' col1 '])` I got error message : #CODE
Is there a trick that will let me use ` asin ` ( or my own function ` my_func `) in the same way as ` abs ` or ` + ` are used ?
This is perhaps a little surprising , since ix works with the string ...
I am sampling at a freq of 880MHz , I want to do some calculations on the samples , and make use of the data in the 880 row of the .csv file .
I did this by using the freq colon as indexing , and then just use the sampling freq to get the data , but the tricky part is , if I sample with 900MHz I get an error .
Put an empty row in the middle and interpolate ( edit : the default interpolation would just take the average of the two , so we need to set ` method= ' values '`) : #CODE
A side comment , I don't know how the interpolate function work , but the way you are using it , it assumes that the 900MHz is in the middle of the two other frequencies , so the result is not correct .
A somewhat cheeky way could be to use ` loc ` : #CODE
Have you heard of the heapq module and merge sorting ?
For example , the following ` ix ` query is slow enough that it seems to be scanning the entire dataframe #CODE
( I realize the two ` ix ` queries don't return the same thing -- it's just an example that calls to ` ix ` on a non-unique index appear much slower )
When index is unique , pandas use a hashtable to map key to value O ( 1 ) .
Ah ha , it does , I didn't realise you could do that with ` replace ` .
@USER So actually what I want is the exact inverse of ` tz_localize ` which is what the ` replace ( tzinfo=None )` does for datetimes , but it is indeed not a very obvious way .
For reference , here is the ` replace ` method of ` Timestamp ` ( see tslib.pyx ): #CODE
Setting the ` tz ` attribute of the index explicitly seems to work : #CODE
You could just apply the Series constructor to that column : #CODE
Hmmm , a simple is to just apply something like ` make_series = lambda x : pd.Series ( x ) if x == nan else x ` , there's probably a more efficient way though .
How did the pivot happen ?
You probably want to append and create a smaller number of nodes .
You can just iterate thru your ` .csv ` and ` store / append ` them one by one .
By the way , What imports / packages do I need to use ` HDFStore() ` , append tables , and use ` read / write_hdf ` in Pandas ?
Would you mind expanding a bit on why having too many nodes in an ` .h5 ` is a bad idea ?
in general you want to have many more rows than columns ; hdf5 is row based . try storing the transpose of your frame
Starting in 0.11.1 ( coming out this week ) , replace has a new option to replace with a regex , so this becomes possible #CODE
Merge on single level of MultiIndex
Is there any way to merge on a single level of a MultiIndex without resetting the index ?
I'd like to join these tables together .
What if you created an additional column with the level of the MultiIndex you want to join on and then merged / joined on that on that ?
At that point , though , I think I might as well drop the index entirely , if it's not going to help speed up merges .
I get around this by reindexing the dataframe merging to have the full multiindex so that a left join is possible .
Do the join on the subindex by reindexing the newFactor dataframe to contain the index of the left data frame #CODE
Yes , since pandas 0.14.0 , it is now possible to merge a singly-indexed DataFrame with a level of a multi-indexed DataFrame using ` .join ` .
The docs also mention that ` .join ` can not be used to merge two multiindexed DataFrames on a single level and from the GitHub tracker discussion for the previous issue , it seems like this might not of priority to implement :
so I merged in the single join , see #6363 ; along with some docs on
how to do a multi-multi join .
I'm trying to create a horizontal stacked bar chart using ` matplotlib ` but I can't see how to make the bars actually stack rather than all start on the y-axis .
Thanks to @USER for the additional idea of using transpose .
FYI , sometimes you can transpose and access faster ( depended on your memory is aligned ) , in this case ( about 10% boost ): `` wpt = wp.transpose ( 1 , 0 , 2 ) .
possible duplicate of [ How to insert pandas dataframe via mysqldb into database ? ] ( #URL ) on seconds thoughts , it's perhaps the opposite , nevertheless related .
If you are appending to an existing table , use the keyword argument ` if_exists= ' append '` .
The above will raise : ` DataError : No numeric types to aggregate ` .
Thanks , I get `" No numeric types to aggregate "` , but all my columns ( except the first one , which holds ` object ` for strings ) hold ` float64 ` types .
Loop over lists and concat in pd.dataframe
pandas dataframe.drop ( col , axis=1 ) does not drop column from column.levels in multiindex dataframe
I have a multiindex dataframe from which I am dropping columns using df.drop ( col , axis=1 ) .
There might be more than one index map to your value , it make more sense to return a list : #CODE
You're changing a copy of df ( a boolean mask of the DataFrame is a copy , see docs ) .
Pandas stack unstack pivot hierarchical index - reshape dataframe
I would like to transform it , so it looks like this : #CODE
Can I transform " grouped " to a DataFrame ?
Have a look at #URL more specifically at the apply and transform sections
getting specific median from data
actually the * mode * is the most common value in a data set . the median is the value that half that values are less than and half are greater than . integral from -infinity to m where m is the median is 1 / 2
Your function return the median of the first 3 values ( or less if there are less ) #CODE
thanks to @USER Allen for resample , rather than groupby date
You can take the median start-of-day location in one line .
I find `` lambda d : d.date() `` is very slow compared to resample ( ' D ') .
File " / opt / local / Library / Frameworks / Python.framework / Versions / 2.7 / lib / python2.7 / site-packages / pandas / core / generic.py " , line 234 , in resample
File " / opt / local / Library / Frameworks / Python.framework / Versions / 2.7 / lib / python2.7 / site-packages / pandas / tseries / resample.py " , line 100 , in resample
@USER , you can only use resample when the index is time-based .
it could generate a `` nan `` in B ( for the first n elements actually , if not are abs > 1 ) , in this case I think `` nan `` is ok as its undefined by the OP
Repeated measures transform in Pandas
To analyze this data , I would want to transform it to stacked format , keeping gender as a covariate : #CODE
You could melt the DataFrame : #CODE
First , use ` isin ` and a mask : #CODE
Essentially you need to stack all the results yourself , whether in a list , numpy array or pandas DataFrame depends on what's more convenient for you .
But it might be better to align with pandas , depending on what structure you have across models .
A roundabout way is to ` reset_index ` of the transpose : #CODE
Instead of resampling the whole group , take the messages column only , and then resample , #CODE
Now I have two data frames ( ` df ` and ` df2 `) and I would like to merge them using ` col1 ` and ` col2 ` .
You can use ` left_index ` ( or ` right_index `) arguments of ` merge ` :
Use the index from the left DataFrame as the join key ( s ) .
and use ` right_on ` to determine which columns it should merge the index with .
Is there a way I can pivot the data using Pandas so that rows are daily dates ( e.g. 1-Jan , 2-Jan etc . ) and the corresponding 1 column to each date is the daily sum of hits ( sum of the hits for that day e.g. sum of hits for 1-Jan ) divided by the monthly sum of hits ( e.g. for the whole of Jan ) for that month ( i.e. the month normalised daily hit percentage for each day )
pandas resample doesn't work with numpy 1.7
Your parsing looks fine.In any event , ` pd.to_datetime ` will take what you posted and turn it into a ` DatetimeIndex ` , which is what you need for resample .
Please note that this does not work for columns ( at leadt multiindex ) , works just for values in the dataframe
Please show the full stack trace
After you've read in the DataFrame ( without restricting dtype ) you can then convert it ( using technique from this post ) with ` apply ` : #CODE
It's essentially the same trick in both cases ( just define a function which does it to a single string and then apply it the column ) .
Then append the new data , which automatically extends the columns : #CODE
I would like to append it to an existing DataFrame and then run some analysis on it .
Another option is to append to a `` HDFStore `` ; then select what you need ( you have synchronize the read / write a bit though )
If you don't care about the data and can always start over , then just aggregate the incoming data into chunks and append 100 or 1000 rows at a time .
If you absolutely need to append the rows asap , at least append 1000 empty rows to allocate memory and then manually insert the data as if you just had numpy arrays .
List append is O ( 1 ) , DataFrame append is O ( ` len ( df )`) .
Let's say I insert data today at 15:23 : 00 , 15:23 : 01 , 15:23 : 02 , 15:23 : 04 .
I would like each sensor to append the current value to a specific column in a panda data frame .
I was thinking you were trying to merge 4 columns into one .
I'm not sure you really need to use pandas to merge between threads , why not use something like SQLite or just use a simple numpy array ?
You could chunk your calls ( i.e. , generate 5+ values at a time ) , keep track of where you are in terms of observations , and then send update / insert commands on the fly to fill the columns .
I'll go with ` concat ` because I find it slightly simpler .
Finally , you could also do this with ` pd.merge ` , but I don't really see why you would need to ( unless you had columns with the same name on both parts , then you either need to use merge or pick one of the sensors to win ) .
To do that , you need a join condition ( so that's why we're using the unindexed sensors for this ): #CODE
Show only the n'th ticklabel in a pandas boxplot
I have a pandas boxplot with FICO score on the x-axis and interest rate on the y-axis .
How do I access the plot , subplot , axes objects from pandas boxplot .
While matplotlib is certainly more flexible , the learning curve is much higher and from my exploration - creating a boxplot was not as simple and transparent as in pandas .
This provides a correlation between 30 days of GE prices and the same 30 days of IBM prices , rolling it through the entire column of dates .
If you are sure that everything you want to drop is either the literal empty string ( `''` , ` None ` , ` np.NaN ` , or ` 0 `) and that you don't want to keep ` 0 ` , then you can just fill the ` NaN ` and convert to boolean and check whether the sum is 0 .
You can tweak depending on how you want to drop .
Here , we fill with False ( because ` bool ( float ( ' nan '))` evaluates to True ) , but you can fill with any value or with a variety of different methods .
So putting ` astype ( bool )` means that it converts the entire array to either ` True ` or ` False ` ( which are equivalent to ` 1 ` and ` 0 ` respectively ) and then you can just sum to find the number of ` True ` values in a row or column .
I understand ( I think ) the ` df.fillna ( False )` but what is the ` astype ( bool )` doing ?
To do so I am currently unstacking the trader column and resample the data using : #CODE
pivot " stacked " data with multiple indexes ?
And i want to pivot it to look like this : #CODE
You can set your multilevel index and then ` unstack ` the level within that index back to columns : #CODE
Then , to convert the date to the last day of the quarter , you could use ` map ` and the ` end_time ` attribute : #CODE
In this data frame , I want to replace all column names with that column name appended by ` x ` , so in this case , I can rename the column name by : #CODE
I would like to resample this DataFrame on a daily basis and compute the following complex statistics .
What went wrong with concat propsosed in the link above ?
The problem with the concat method is that it does not work for me when I try to unstack the Trader column .
* Trader * is a column level , if you want to pivot this you need to use stack and not unstack ( which if for pivoting a level on the index ) .
But how do I shift the trend column by 1 period ?
I do not know if it is possible to shift on a DateTime level of a MultiIndex ( operating directly on df4 ) .
So in function f : df [ ' Diff '] = abs ( df [( ' acctual ' , ' Quantity ')] - df [( ' trend ' , ' Quantity ')]) .
You do not need groupby / apply to compute the diff column .
df4 [ ' Diff '] = abs ( df4 [( ' acctual ' , ' Quantity ')] - df4 [( ' trend ' , ' Quantity ')]) .
The keys are not stored in the order you declare or append to it .
I'm not super-happy with the ` lookup ` line -- you could replace it with your original ` max ` call , of course -- but I think the ` idxmax ` approach isn't a bad one .
In [ 9 ]: %timeit df [ ' critic '] = abs ( df.lookup ( df.index , df.critic_vector ))
I think the easiest way is to apply a function of dividing by index1 values by 3 , but not sure how you apply a function to an index .
I tried to use ix to perform the same operation but it takes even more time .
transpose so that your dims are like : items ( low ) x major ( dates ) x minor ( ids ) , will give better perf
This could explain the diff .
As @USER mentions there are a few ways to do this , but I recommend using loc / iloc to be more explicit ( and raise errors early if your trying something ambiguous ): #CODE
The latter two choices remove ambiguity in the case of integer column names ( precisely why loc / iloc were created ) .
Can you only download the bits you want and then append to the existing data frame ?
I had been trying to guess if there was some canonical transform like " DateTime " or " Date_Time " happening .
in singular transformations ; automatically expanding .
Better still , we can use ` transform ` so that broadcasting will give us a Series of the group sums : #CODE
I want to append a calculated row that performs some math based on a given items index value , e.g. adding a row that sums the values of all items with an index value 2 , with the new row having an index label of ' Red ' .
My current solution involves transposing the DataFrame , applying a map function for each calculated column and then re-transposing , but I would imagine pandas has a more efficient way of doing this , likely using ` .append() ` .
The role of " transpose , " which you say you used in your unshown solution , might be played more naturally by the ` orient ` keyword argument , which is available when you construct a DataFrame from a dictionary .
I tried using ` eval ` on the list , which seems to solve an issue that comes about when using ` raw_input ` and iterating over items within it - kinda trying to work out some of the issues I've been having when transitioning to ` IPython ` and ` Python 2.7 ` ( originally used ` Python 3.3 `) .
Which , actually , now that I think about it , may be what is causing the problem- although ` eval ` still would not fix it .
How to create MultiIndex from joined MySQL tables ?
Due to your initial merge operations and multiple coaches per team there is indeed duplication of the playerID / Name , this has nothing to do with hierarchical indexing .
How to resample a TimeSeries in pandas with a fill_value ?
Is it possible to resample a ` TimeSeries ` using a ` fill_value ` for missing data like I can with ` reindex ( fill_value=0 )` ?
transpose it #CODE
e.g. take a single column for example , or apply a function across all the columns in a reduction operation , or concatenate the data
` unstack ` would return a series with a two-level index , and ` pd.Series ( df )` does not seem to work ( it's really odd what it does , since it splits the column title into characters and populates the Series with copies of this splitting )
Do you know if append returns a copy / view / reference of the original dataframe ?
same number of elements as you are trying to replace ) , and this MUST be sorted for this to work #CODE
Second question is once I am done with column wise comparison I want to be able to count for each row how many ' true ' are there , I am currently doing ` astype ( int32 )` to turn bool into int then do ` sum ` , does this sound reasonable ?
This seems like a well stated use case that may apply to others .
( They will be a ` MultiIndex ` . ) Then use ` unstack ` to pivot the index of Qs into columns .
pandas merge with dataframes of different frequency ( hourly and daily )
I'm trying to merge dataframes that are either daily or hourly .
It seems on the first iteration through my loop , I can merge the first daily with hourly values .
I am writing a function that takes 3 pandas Series , one of which is dates , and I need to be able to turn it into a dataframe where I can resample by them .
I seems that map / apply / lambda seems the way to go .
You can just use an apply with the same method suggested there : #CODE
Or perhaps you could use one of the Series string methods e.g. ` replace ` : #CODE
Hmm not too sure , think you may be better off just with the apply , but something using match could be possible : ` s.str.findall ( r ' ( ?
The issue came up with apply in fact .
You , you can join the dataframe to itself , and then some groupby tricks might achieve your goal .
If I append the dataframe to HDFStore the file is about 200 mb too .
I know that I could use the ` truncate ` method to extract the periods and create new timeseries to work with , but I am wondering whether I can define a custom ' period ' ( e.g. 1st of May through 1st of Sept ) .
Basically , I just want to ' mask ' the periods outside the seasons of interest .
to get a series of bool .
And I can do moving median or moving average to get an idea about the size of the data holes .
From that , I can make histograms of hole lengths , and of the ` and ` or ` or ` of isnull of multiple series ( that might be substitutes for eachother ) , and other nice things .
` Series ([ len ( list ( g )) for k , g in groupby ( a.isnull() ) if k ])` is probably slightly more efficient .
Is there an option not drop the the indices with ' nan ' in them ?
I think silently dropping these rows from the pivot will at some point cause someone serious pain .
Workaround to fill the index with a dummy , pivot , and replace #CODE
it must be align able or equal length to the index in the case of a list / numpy array
just to complete perigee's answer , it cost me quite some time to find a way to append the data .
This code uses the pandas read_csv method to get the new quote from yahoo , and it checks if the new quote is an update from the current date or a new date in order to update the last record in history or append a new record .
Is there any possibility to convert the resulting ' gr ' DataFrame into a DataFrame with a PeriodIndex so that I can use the resample function to calculate weekly and monthly averages ?
Important : I need to resample the ' gr ' Dataframe not the original ' df '
pandas - apply function to current row against all other rows
Let's assume for example the computation was a function to find the length of the intersection set , I'd like an output DataFrame with the ` len ( intersection ( Alice , Bob ))` , ` len ( intersection ( Alice , Carol ))` , ` len ( intersection ( Alice , Dave ))` in the first row , with each row following that format against the others .
I replaced my list comprehension with a slightly nicer nested apply .
It's not really that big of a deal because I can loop through the dataframe index and replace them with something like `" %06d " % i ` since they are always six digits , but you know ...
I'd replace the negative 1 but my reps not yet high enough .
I've used the map function on a dataframe column of postcodes to create a new Series of tuples which I can then manipulate into a new dataframe .
I've tried to use the answers at How to make a nested dictionary and dynamically append data but I'm getting lost .
When subsequently doing an apply / map , you'll usually want the function to return a Series ...
When I do ' g = df.groupby ( ' author_id ')' g is then just ' ' and I cant seem to be able to apply the function ...
You apply the function using ` g.apply ( some_function )` , whether you can apply it depends on the function ...
The output of the unique function is a numpy array , which doesn't provide the apply method .
You can create a ` Series ` by that array and then apply your function : #CODE
If they have different names you can ` drop_duplicates ` on the transpose : #CODE
I have two TimeSeries with some overlapping dates / indices and I'd like to merge them .
where ` id ` is an id for each point consisting of an ` a ` and ` b ` value , how can I bin ` a ` and ` b ` into a specified set of bins ( so that I can then take the median / average value of ` a ` and ` b ` in each bin ) ?
I am trying to append some of the words from a string into the Pandas Dataframe but after few trials it seems that I may not be succesful in it .
if you can't do that for some odd reason , you need to merge single dataframes using pandas concat :
Do you want to group and aggregate the data before applying the t-test ?
You should use ` isnull ` ( rather than ` == None `) to check for ` NaN ` : #CODE
If your date / time column were in the datetime format ( see dateutil.parser for automatic parsing options ) , you can use pandas resample as below : #CODE
I want to resample one of them to use the index of the other and forward fill data from the one on any dates in the index of the other in which there wasn't data for .
' b ' represents the data frame I'd like to resample and forward fill data .
To resample by a reference index , use ` reindex ` .
Error when trying to apply log method to pandas data frame column in Python
In the end , I want to collect all the results ( as a DataFrame ) from each grid job and aggregate them into a giant DataFrame .
Pandas : aggregate when column contains numpy arrays
However , since my real data frame has multiple numeric columns , arraydata is not chosen as the default column to aggregate on , and I have to select it manually .
It may just be that they want to preserve the separation of concerns between ` groupby ` and ` transform ` ?
This is very similar to what pandas is doing under the hood anyways [ groupby , then do some aggregation , then merge back in ] , so you aren't really losing out on much .
Since it's rare to have nested arrays in a DataFrame like that , it checks for ndarrays to make sure that you are actually using an aggregate function .
In my gut , this feels like a job for ` Panel ` , but I'm not sure how to transform it perfectly .
And if you want to convert back to array , just apply ` np.array ` to it .
How you want to resolve this issue really depends on why you have columns of ` ndarray ` and whether you want to aggregate anything else at the same time .
I need to develop a better feel for stack and unstack .
Then , with the list of dataframes , you can just use ` concat ` straight up .
If the index of A is non-consecutive integer index , how do I reindex A to be 1 ...
Is it best practice to reindex A and then add columns from B to it ?
You can combine the columns of two DataFrames using ` concat ` : #CODE
is Pandas concat an in-place function ?
I guess this question needs some insight into the implementation of concat .
if I do ' pd.concat ( list_of_pieces )' , does concat allocate another 30G ( or maybe 10G 15G ) in the heap and do some operations , or it run the concatation ' in-place ' without allocating new memory ?
Then stack it and add the Date and the Hour levels of the MultiIndex : #CODE
I think that `` read_csv `` should a ) raise on an invalid passed dtype , and b ) just translate `` str `` dtype to `` object `` , open an issue ?
snap @USER .
Then you're looking for a ` pivot ` : #CODE
then do the pivot .
If you use fruit and veg as the index , then you could use ` itertools.product ` * to create the ` MultiIndex ` to ` reindex ` by : #CODE
Then you can just reindex by these : #CODE
Looks like there is also a ` fill_value ` optional argument to reindex which can save a step ( e.g. ` df1.reindex ( fruit_x_veg , fill_value= ' UNKNOWN ')` .
How does one append large amounts of data to a Pandas HDFStore and get a natural unique index ?
Also : when using large amounts of small files , it would be better to first get the current number of rows and up nrows each time with the length of the current dataframe at the end of each append action ( much better for performance ) .
ahh ... yes if you split over multiple files that would be true . when you append LOTS of rows , for sure turn off indexing ( `` index=False ``) , and ptrepack occasionally ( then index at the very end )
pandas resample documentation
So I completely understand how to use resample , but the documentation does not do a good job explaining the options .
So most options in the ` resample ` function are pretty straight forward except for these two :
That's because you are using join incorrectly .
I have multiple data-frames with stock prices that I want to align into a single data-frame that contains only the close prices for all stocks .
Right now , I have 8 data-frames I need to align this way .
Is there any way to loop thru a list of data-frames and align them like above - instead of manually tying the data-frame names ( something like df [ 0 ] to df [ 7 ] figuratively speaking ) ?
The remaining question is : How do I join the dataframes into one by iterating thru my dict ?
I found workaround by adding new column and set levels as new MultiIndex , but it seems to be difficult ...
Is there any way to pass MultiIndex to crosstabs function to predefine output columns ?
How do I create pandas DataFrame ( with index or multiindex ) from list of namedtuple instances ?
Then just apply the aggregation function and boom you're done : #CODE
You can also provide different aggregation functions for different columns by passing ` agg ` a dict .
@USER yup , ` combine_it ` is the slow thing not agg .
Commenting on the first code-formatted section : for the my_columns can I just replace that with my list.txt file rather than type out the individual column names ?
Perhaps you could append a small example which exhibits this behaviour , and also post the traceback .
( Also , you can convert this to use the new ` loc ` , but results should be equivalent . )
I want to check if some data is missing in the PyTable for specific DatetimeIndexes ( value is NaN or a new Timestamp is available ) , replace this with new values from a given pandas DataFrame and append this to the Pytable .
just reindex and fillna !
You have answered my previous question not because of typing " reindex " but posting issue which tells me that by default without workaround by reindex such behaivor of crosstab and groupby is not possible in current version .
I'd like to consolidate the table to just show one row for each Id which has the most recent date .
which for each group grabs the row with largest ( latest ) date ( the argmax part ) .
Then use pandas ' groupby and / or resample functionality to do whatever you want to do .
I wonder whether ` to_datetime ` is faster that ` apply ( pd.Timestamp )` ?
`` pd.to_datetime `` should be faster if the cython doesn't raise ( in which case it essentially falls back on `` apply ( Timestamp )`` .
Then from the DataFrame , just ` stack ` and find the zeros : #CODE
Filtering based on the " rows " data after creating a pivot table in python pandas
Therefore , I'd like to pivot the table to give me a much much smaller table to work with ( few thousand rows ) .
Filtering and selecting from pivot tables made with python pandas
If you're dates aren't evenly spaced , you can ` resample ` ( by day ) first : #CODE
or reindex to equally spaced dates first ?
If I have datetime instead of dates and I do not want to resample as I want to keep my original index , is there a way ?
May be I can resample , then pad with limit and then extract only the values I want according to my original index ?
For now Andy , I think I will resample to the second and then use limit .
@USER Maybe have a column of originals=True and * don't * ffill that column after the resample .
1 resample the dataframe to the second
3 reindex my new dataframe with the index of the original one
Resample for every second could be a rather a large overhead though .
But I don't want to resample the data .
transpose a subset columns in dataframe ( not groupby , need to create new columns )
How can I transpose the second column ( 24 lines data per site ) into the line which contains 24+1 columns with site indice .
What is ` sitesData ` ( why can't you just transpose that ) ?
I cannot simply transpose the sitesData .
This may also be possible using ` unstack ` ...
Apply ` histogram ` to each group .
The histogram values are a " heat map .
storer format has its uses but since u cannot append data it is unsuitable for really large data sets . in addition u cannot query . you pay a small penalty for writing a table . what are u trying to accomplish ?
However , more important is the ability create the data in a straightforward manner , but possibily writing in several append operations ( so you do not have to keep all of the data in memory at once ) , m2c
However after trying ` set_values ` and ` concat ` ( which is even more difficult ) .
For me , ` concat ` is a last resort .
This will be considerably faster than using map ( especially python's builtin map ) .
Then you can grab out the difference using a ` shift ` : #CODE
The shift isn't inplace , you can set it to your column with ` df [ ' delta '] = ...
in singular transformations ; automatically expanding .
Its also more conform the preferred OO-style for using matplotlib .
I want to replace negative values in a pandas DataFrame column with zero .
Here is the canonical way of doing it , while not necessarily more concise , is more flexible ( in that you can apply this to arbitrary columns ) #CODE
Or are there arguments in favor of loc over ix ?
You could use the clip method : #CODE
True , but the ` clip ` method is different ( has default parameters for ` lower ` and ` upper `) , and it has additional methods such as ` clip_lower ` and ` clip_upper ` .
I want to reindex the frame so that the column values ( 10 , 20 , 30 , 70 ) become index values and the data becomes the column : #CODE
You're looking for the transpose ( ` T ` ) method : #CODE
I want to resample it for 20 minutes and get the hit count for certain 20 minute time slot .
It did align the column names with the values , but lost the row labels .
that you want to align the input ( for example you then don't have to to specify all of the elements ) #CODE
So the ` loc ` attribute of the data frame defines a special ` __setitem__ ` that does the magic I suppose .
This seems just like the transpose of OP's effort : s
stack columns in dataframe , and also stack index column
I have a problem stacking columns from a dataframe in pandas and in addition stack a column and making it an indexcolumn that not contains unique values .
I would like to stack the dummy variables ( aware , aware_2 , aware_3 ) , and also separately stack the resp variable and assosiate the stacked dummies with that variables .
... define your diff function ...
By the way , if you're in IPython Notebook , you may like to use a colored diff function
Also , I have seen some posts about wrapping a printed variable in a str() , but I'm not sure how that would apply here .
How to replace values with None in Pandas data frame in Python ?
Is there any method to replace values with ` None ` in Pandas in Python ?
You can use ` df.replace ( ' pre ' , ' post ')` and can replace a value with another , but this can't be done if you want to replace with ` None ` value , which if you try , you get a strange result .
@USER replace is actually a very feature-rich ( read complicated ) function , the [( dev ) docstring ] ( #URL ) is really good though .
when does ` agg ` pass single columns ( Series ) to ` myagg ` , and when does it pass
let's go write apply functions .
If you pass a ` dict ` or ` list ` to apply , you will have item-by-item agg , IOW , you will get a ` Series `
when having more than one key ( e.g. the 2nd example ) , the default is to aggregate with a series that has a multi-index , while a single key will do a data frame aggregation ( subject to my gotcha above )
I know i should do df.groupby ( ' channel ') and then apply function to each group .
In this case the apply value already returns the exact same df with only different cost values .
Try a ` dropna ` or use ` missing= ' drop '` to Logit .
( Some operating systems provide record-oriented files that have more complex internal structure than the common flat file . The above does not apply to them . )
Pandas : Filtering pivot table rows where count is fewer than specified value
I have a pandas pivot table that looks a little like this : #CODE
Is there anyway to use the mapping function or something better to replace values in an entire dataframe ?
I would like to replace the strings in the ' tesst ' and ' set ' column with a number
If there's a match , then my script would delete the row in the second list and append the pair of dates .
@USER give an index to the Series when you apply ; they will become column names
This is roughly 75MB , but when I apply the last solution verbatim , Python eats 2GB of my memory .
Or if I'm going to replace values I have to make sure I'm not steamrolling over something recklessly .
I see that pandas makes it easy peezy to replace values but in the beginning I just want to look .
Removing duplicates in a groupby using apply
Or of course if you have a list of the columns whose values you want to replace , you can pass that list in the second slot : #CODE
You can append using ` to_csv ` by passing a file which is open in append mode : #CODE
Use ` header=None ` , so as not to append the column names .
With regards to Pandas : df.merge() method , is their a convenient way to obtain the merge summary statistics ( such as number of matched , number of not matched etc . ) .
I know these stats depend on the how= ' inner ' flag , but it would be handy to know how much is being ' discarded ' when using an inner join etc .
Have I missed it ( i.e. something like report=True for merge which would return new_dataframe and a report series or dataframe )
Finding median with pandas transform
I needed to find the median for a pandas dataframe and used a piece of code from this previous SO answer : How I do find median using pandas on a dataset ?
It seemed to work well , so I'm happy about that , but I had a question : how is it that transform method took the argument ' median ' without any prior specification ?
I've been reading the documentation for transform but didn't find any mention of using it to find a median .
Basically , the fact that .transform ( ' median ') worked seems like magic to me , and while I have no problem with magic and fancy myself a young Tony Wonder , I'm curious about how it works .
When you pass the argument `' median '` to ` tranform ` pandas converts this behind the scenes via ` getattr ` to the appropriate method then behaves like you passed it a function .
You just need to find this character and either get rid of it or replace it with the one that is acceptable .
I want to merge the two together so that the ' NAME ' columns match up , basically just add the two Sdev_I1 / Sdev_I2 to the end of the first sample .
Seems you didn't strip the comma .
Seems you didn't strip the comma symbol in the second csv , you might try to use converters to convert them : #CODE
You can also use replace : ` df2.replace ( r ' ( . * ) , ' , r ' \1 ' , regex=True )`
Following advice in earlier posts , rather than append each new row to the dataframe I construct a dataframe with the historical file and append 5000 " blank " records to it with correct timestamps .
The conclusion is if efficiency is important , do not try to append to your arrays / DataFrames .
They have a fixed size , and when you " append " to one , you really are just creating an entirely new one with the new size , which is slow .
If you absolutely must append to them , you can try stuff like messing with types to ease the pain somewhat .
But ultimately you just have to accept that any time you try to append to a DataFrame ( or a numpy array in general ) , you will likely suffer a substantial performance hit .
In fact I only append every 5000 rows and the loop being timed is actually writing data into a row appended when the main file was read to create the dataframe .
I wouldn't expect this slowdown to be as much as the append , but it still does involve two separate numpy operations instead of one .
replace string / value in entire dataframe
I have a very large dataset were I want to replace strings with numbers .
( similar to the fillna method , but replace specific string with assosiated value ) .
Use replace #CODE
Can I resample somehow like I can do with TimeSeries ?
Since ` hist ` and ` value_counts ` don't use the Series ' index , you may as well treat the Series like an ordinary array and use ` np.histogram ` directly .
My current method is to create a list for the dates to be stored in and another list for the prices to be stored in , and to append a date and price to each list as they are read from the Bloomberg data request response .
I'm not 100% sure which your after , but you can ` concat ` a list of DataFrames : #CODE
or do an outer merge / join : #CODE
See the merge , join , concatenate section of the docs .
Timestamps have a ` replace ` method ( just like datetimes ): #CODE
Because read_csv set date to today and we replace date ... maybe we could do this using only one step ?
In your example , you can ` pivot ` the result : #CODE
Now you can ` reindex ` by this : #CODE
Do you know how to reindex if the row index is a MultiIndex ?
Does there exist a built in function in pandas that will Combine the two dataframes , using Col A as keys and updating value in Col B if it exists , otherwise append .
I've looked into merge and update and append but they don't seem to act the way I want , update updates by index instead of Col A value , merge doesn't overwrite , ect .
One way to do this is to ` concat ` then drop the duplicates : #CODE
And then you can append the label column to the index :
There are two problems that I'm encountering with this : 1 ) I can't call tz_localize or tz_convert on a MultiIndex ; 2 ) Accessing the hour field from a single Index still gives me the array ` [ 7 , 8 , 9 , 7 , 8 , 9 ]` when I'd like the Pacific values ( i.e. ` [ 0 , 1 , 2 , 0 , 1 , 2 ]`) .
Is there a way to create timezone aware Series and then insert them into a dataframe / make them an index ?
Struggling a lot with this issue , MultiIndex loses tz in many other conditions too .
I think I prefer using an outer concat tbh .
If you know each of these are the same length then you could create the DataFrame directly from the array and then append each column : #CODE
An alternative here is to create each as a DataFrame and then perform an outer join ( using ` concat ` ): #CODE
Basically I want do drop 1 column form a DataFrame and then " rebuild it " so that one column is summed and the rest of the fields ( which are the same ) stay in place .
replacing this in code just drop those whole rows ...
Now you can apply the respective ` fillna ` s , one cheeky way : #CODE
I think you're going to be much better off getting this into one DataFrame , so consider using a MultiIndex .
Here's a toy example , which I think will translate well to your code : #CODE
@USER No it should align it ( and put NaNs where data isn't present ) .
I'm trying to resample daily data to weekly data using pandas .
You can only resample by numeric columns : #CODE
you could do say median of the dates in the re sample interval if u really wanted
I don't get that when I resample using " 7D " .
Workaround is to do a fillna loop over the columns to replace missing strings with '' and missing numbers with zero solves the problem .
to aggregate the Position column .
yep .... kind of an `` ix `` artifact ...
now pivot that table so that the week_days are as columns ( could also change the needdays to string formats of days but leaving that for you .
I've noticied the same behavior implemented in Excel when building a pivot table with multiple columns .
You could do this once this pivot table has been created : #CODE
And now I use ` split ` and ` replace ` to split by `' , '` and remove all `' : '` #CODE
Do the regex replace * first* : ` s2.str.replace ( ' :\ d+ ' , '') .str .split ( " \s* , \s* ")` .
I've tried with to replace tzinfo but failed to find a proper solution .
I'm using pandas to do an ` outer ` merge on a set of about ~ 1000-2000 CSV files .
All I want to do is merge these together , bringing all the new columns together and using the ` id ` column as the merge index .
I do it using a simple ` merge ` call : #CODE
The problem is that with nearly 2000 CSV files , I get a ` MemoryError ` in the ` merge ` operation thrown by pandas .
I'm not sure if this is a limitation due to a problem in the merge operation ?
Is there something I can change in the ` merge ` operation to make it work on this scale ?
I think you'll get better performance using a ` concat ` ( which acts like an outer join ): #CODE
This means you are doing only one merge operation rather than one for each file .
( not sure about the inner workings of the ` concat ` though )
( tbh I didn't know concat would accept a generator ! )
is it that each individual merge takes up memory that is not garbage collected in time and then you run out of memory ?
@USER well , the main thing is that you are doing lots more merge operations , and also it only builds * one * DataFrame ( again this is an expensive operation ) .
If you enable logging in python , then when an exception is received you can use the method ` logging.exception ` to log when an exception has been caught - this method will print out a nicely formatted stack trace that shows you exactly in the code where the exception originated .
If you want to join a list of DataFrames like this you could use concat : #CODE
I'm note sure how you want to join the separate blocks together , but this leaves them as a list .
You can concat from there however you desire .
Assuming you've got the list of ` Dataframe ` s , which I've called ` res ` , you can use pandas ' concat to join them together into a single DataFrame with a MultiIndex ( also see the link Andy posted ) .
You get one string per line and the other cells are ` NaN ` , then the math to apply is to ask for the ` max ` value : #CODE
FWIW the ` isin ` method is about 10 times faster than using ` apply ` on my particular dataset .
Didn't realize you can specify an axis for concat , thanks !
then apply this function across each group : #CODE
` you couls use ` np.arange ( 1 , len ( x ) + 1 )` .
Knew it was going to be a groupby , but couldn't figure out how to apply it properly .
Use merge #CODE
Pandas crosstab double counting when using two aggregate functions ?
We group those two things by cols and then apply the aggregating function , which in this case is np.size .
How can I apply the testfunction on x which returns the two c [ 1 ] values ?
Do you mean to say , you need to apply testfunction on df1 and df2 individually and store the result somewhere ?
Like ` y = map ( testfunction , x )` ?
This will apply testfunction to every item of the iterable you pass and return a list of the results .
I thought that I need to apply a " for loop " ...
Use the built-in map function #CODE
This will apply the function ` testfunction ` to every item in ` x ` and return a list of the results .
melt is a reverse unstack #CODE
You can use the ` normalize ` argument of ` bdate_range ` ( which defaults to True ): #CODE
I did not know about normalize
I am trying to join ( vertically ) some of the tuples , better to say I am inserting these tuples in the dataframe .
Is it possible or do I have to iterate over each word in tuple to use insert
alternatively if you have many of these rows that you are going to append , just
Without it pandas generates a MultiIndex DataFrame , which includes proper timestamp .
Length : 372 , Freq : None , Timezone : None
Length : 371 , Freq : M , Timezone : None
IIUC you can ` filter ` : ` grouped2 = data.groupby ( lambda x : x.year ) .filter ( lambda x : len ( x ) > 11 ) .reset_index ( drop=True )`
Elaborating on answer using melt , both helped me understand pandas a bit more , but there was more unknowns for me in " melt " answer : #CODE
I figured the key is to use ` melt ` , and a bit of acrobatics afterwards .
I stepped through the large melt method and added it to the above question .
If you make ` rows ` a boolean array of length ` len ( df )` , then you can get the ` True ` rows with ` df [ rows ]` and get the ` False ` rows with ` df [ ~rows ]` : #CODE
Or rather , ` rows = np.random.binomial ( 1 , percentile*100 , size=len ( df_source )) .astype ( ' bool ')`
your can only save a Panel4D as a table ( append works ) . put an example of what your are trying to do
FYI - I've seen conditional sums for pandas aggregate but couldn't transform the answer provided there to work with sums rather than counts .
Is there any equivalent function like " apply " for rows .
Since apply seems to work on " columns " only .
@USER ` apply ` takes an ` axis ` kwarg to toggle between rows and columns
I would like to modify a pandas multiindex dataframe such that each group includes Dates between a specified range .
Once I've unstack'ed by [ ' A ' , ' B '] , I can then reindex .
Any thoughts on how I can ' reindex ' the unstack'ed dataframe , restack , and have the dataframe look to be in the same format as the original df ?
To get the unobserved values filled , we'll use the ` unstack ` and ` stack ` methods .
Unstacking will create the ` NaN ` s we're interested in , and then we'll stack them up to work with .
` stack ` does have a ` dropna ` argument .
The result you currently have is a ` Series ` with a ` MultiIndex ` , so all the usual rules will apply .
By reading the documentation I can't figure out how to store or change a MultiIndex in a HDFStore correctly .
Stack / Unstack dont seem to work .
' I suspect there's something wonky about your input data's alignment before using unstack .
Maybe you are looking for ` unstack ` : #CODE
However , when I apply the " unpack " function , all of those turn into a value 0.939085 , for which I'm not sure how it's happening .
Hence , when I wanted to stack the data ( as opposed to unstack it ) , what I ended up doing was doing : #CODE
Thus I suggested to use ` apply ` instead .
I see , I guess then you can split the DataFrame , apply ` value_counts() ` to each split , and then sum all the splits to get the final result .
I'm trying to join two datasets in Pandas .
Why not just join df1 and df2 to create a df with a MultiIndex of ' BuildingID ' and ' ItemID ' ?
That works fine with the simple data set here ( and is essentially what I want to do ) but when I try it on the full data set it tells me ` Exception : Reindexing only valid with uniquely valued Index objects ` at the ` merge ` step .
I'm trying to cut down the real dataset now to see what's the simplest version that produces the error .
In addition to this I want to apply some function to each group ?
I want to take the first group and apply the function , then the second group and apply function etc .
I would unstack the statistics from ` describe() ` , then you can simply use ` sort() ` , so : #CODE
You groupby two columns and therefore get a MultiIndex in return .
so len of the frame is the answer , unless I misunderstand what you are asking #CODE
In general I am looking for a way to transform a table using unique values of a single column .
I have looked at ` pivot ` and ` groupby ` but didn't get the exact form .
HINT : possibly this is solvable by ` pivot ` but I haven't been able to get the form
Probably not the most elegant way possible , but using unstack : #CODE
I cooked up my own pivot based solution to the same problem before finding Dougal's answer , thought I would post it for posterity since I find it more readable : #CODE
so I need to append the col names on top of existing data then change the col names ....
I am aware that " financial " rolling means and " scientific " rolling means are different but I think this should produce a series without NaN values .
subset before you apply #CODE
2nd part , row-wise apply , returning ' sum ' of elements in A and B columns .
pretty much what what you want in apply .
This is cool - how would I do a row-wise apply taking two values from different columns but in the same row into the apply function ?
Reindex both by the joint index , fill with 0 and add #CODE
Here is another way ( and allows various ways of joining if you specify ` join ` kw ) #CODE
Suppose there are 100 rows and the first level of multiindex has 3 groups .
This is a nice way to transform your data to a better repr to deal with ; uses some
neat apply tricks #CODE
Using pandas reindex with floats : interpolation
This is true even if I force the index to be a float before I try to interpolate .
Is this : #URL ie I have data for several years , and I want to interpolate column values to a ( finer ) grid in order to animate smoothly .
The reindex method only tries to for the data onto the new index that you provide .
I guess you were expecting the reindex method to do this though ( interpolation ): #CODE
No , I wasn't expecting reindex to do the interpolate .
As I said , ths was the first step / setup for interpolate .
I am trying to get a time delta ( in secs ) between dynamic ranges ( based on diff in values ) of a time series data frame .
I am trying to get the time delta between price diff of 4 .
Once the diff in price is reached , that price point become the ' starting point ' for the next calculation and so on .
Now , in one of the columns I have strings that I've been trying to map on integer values using ` unique_vals , int_representation = np.unique ( df.x , return_inverse = True )` but I found that for some reason the number of unique strings in my original column , and the number of unique integer values in ` int_representation ` is different , which doesn't make any sense .
I have also checked that when I don't drop , the problem disappears .
Before doing that , is it possible that the reason is that I didn't manually reindex the frame after dropping ?
Just wanted to map those strings to integers ( I'm calculating some stats on a text , and I need words in a number representation ) , and was hinted here that what I call keys in my code is an easy way of doing that , which is actually true .
@USER Take the leap and try ` eval ` !
I will try eval() after I figure out how to import the eval function .
Basically , it's using a mask to limit the rows in ` df ` to only those that return ` True ` , then it's only giving you columns : ` IP ` , ` Time ` , ` Resource ` .
I will analyse the map section of the statement , as this is the part most confusing to understand about this specific issue , and Pandas in general .
Name : dtu_topic_split , dtype : bool
To select rows with one item you're interested in , you can ` apply ` a ` lambda ` function .
here's the reason for this : since you are effectively masking certain values on a column and replace them ( with your updates ) , some values could become `` nan `
How to speed up Pandas multilevel dataframe shift by group ?
I am trying to shift the Pandas dataframe column data by group of first index .
the problem is that the ` shift ` operation is not cython optimized , so it involves callback to python .
How about shift the total DataFrame object and then set the first row of every group to NaN ?
similar question and added answer with that works for shift in either direction and magnitude : pandas : setting last N rows of multi-index to Nan for speeding up groupby with shift
Also the transformation is too complicated to fit a ` grouped ` then ` apply ` paradigm , it involves clustering and filling in multiple dataframes at once while going through each group .
In the master branch of pandas on GitHub the [ ` hist `] ( #URL ) method has a ` figsize ` parameter for this exact feature .
resample at the same frequency ( the additional day that we added makes this pad
It would be better if I didn't insert all these minute-rows .
I need to format the fields when creating the issue so the data I have can align properly with the appropriate fields in JIRA .
What does ` map ( type , df.Id )` return ?
Eg , on unix based OS , you could do something like the following before and after loading the object to get the diff .
The .dta files are composed of a lot of different variables , and I want to insert each variable into a separate " variable table " .
The above example also uses proper SQL parameters for the row data ; you generally want to let the database prepare a generic statement and reuse the prepared statement for each insert .
You could use ` apply ` , e.g. : #CODE
split a Pandas series without a multiindex
I cannot directly use the unstack command because it requires a multiindex and I only have a single-level index .
I call this the groupby via concat pattern .
Essentially an apply , but with control over how exactly its combined .
You can use ` groupby ` , ` apply ` , ` reset_index ` to create a multiindex Series , and then call ` unstack ` : #CODE
Instead of field being lang , now I want to look at the field which has the type bool .
` df.groupby ([ ' b ' , ' c ']) [ ' a '] .count() ` is a Series , but it has a multiindex , so it is still not clear how that could be assigned to ` df [ ' gr ']` , which has a single-level index .
I have been thinking about using the merge function but I'm not sure how to do this .
Consider separating the operations : first create DataFrame ( s ) from xml , maybe then merge / concat / reshape , then do computations .
I looked at ` groupby() ` but it wasn't immediately obvious how to apply it to this problem .
I have tried pandas ` BMonthEnd ` and ` BQuarterEnd ` with ` WeekOfMonth ( weekday = 4 , week =2 )` , I cant the rolling part .
If so , you can also apply the last groupby on the monthly df as well .
variable size rolling window regression
I have a series where it has variable number of observations per day and I have 10 years history of data , so I want to run rolling OLS on 1 year rolling window . loop through each date is a bit too slow , anyway to make it faster ?
all the rolling ols statistics are in model #CODE
to show rolling beta
I know this function and the function doesn't work the way I described since in my case rolling window size can be different between day to day ( more observations points in one day than the other ) .
The only problem is in order to ensure accuracy , the resampled data has to be equal to the mean of rolling window mean , this is slow process .
You don't seem to have the right terminology , in pandas you are looking for either [ merge ] ( #URL ) or [ join ] ( #URL ) .
thanks . merge does work .
Use ` merge ` function : #CODE
Turns out I need to create a matching MultiIndex with the higher levels fixed
in general you DO want to build these ( rather than replace ) , much more efficient , but in case you actually want to do what you are doing ( and there are valid use cases ) , e.g. say you just want to replace a part of the frame ...
prob better off using concat then : #URL building then modifying is not very efficient
How to join all rows of sng1 to nm .
values gives u back the underlying ndarray ; it allows you to map the values with a new index ( the events ); passing in the series to the Series constructor and a new index will reindex using the passed index , not what u want in this case
I created a multiindex .h5 file and use HDFStore to store the data .
I am unable to do this in using pandas merge function and I'm not sure where to even start .
If it fits in memory , you can ` merge ` the two dataframes with an ` outer ` method base only on ` chrom ` column , then filter your result by doing the range inclusion math : #CODE
This is how merge works : chromstart and chromend exist in both dataframe , but we merge on name only .
Have a look at merge documentation
Use the replace method on dataframes : #CODE
After reading one line I append the dictionary to a list ( so , the number of dictionaries in the list is equal to the number of lines in the file ) .
Ok it was simple . the len ( dataframe ) was the easy answer .
print len ( df3 )
I guess you could also apply the count to one particular Field ?
The problem is that I cannot reindex the ` Series ` from 2 to 3 levels : #CODE
@USER : This solution only works if the index is properly sorted * before * you drop one of the levels .
` unstack ` the problematic index level ( s ) away
` stack ` the problematic index level ( s ) back .
can you post the full stack trace ?
Assuming ` sequence.apply ` applies the lambda to each element in the sequence , ` sequence.apply ( lambda x : x 0 )` yields a sequence of Boolean values , and ` sequence.apply ( lambda x : x 0 ) .apply ( lambda x : sum ( x ))` attempts to sum each Boolean value , resulting in a `' bool ' object is not iterable ` -kinda error .
what is the quickest / simplest way to drop nan and inf / -inf values from a pandas DataFrame without resetting ` mode.use_inf_as_null ` ?
The simplest way would be to first ` replace ` infs to NaN : #CODE
Quick way to replace all occurences of a value in a pandas data frame with NA
How to apply to_datetime or with sort_index so that the dates are sorted ?
I don't know how to apply to_datetime , because the output of the dates are not sorted .
At the moment you can do this with an apply or the delta attribute : #CODE
Date and time are in two separate columns , but I want to merge in them into one column , " Datetime " , containing datetime objects .
You can do this with ` melt ` : #CODE
I'm having problems with MultiIndex and stack() .
Rather than do a ` pivot ` * , just ` set_index ` directly ( this works for both examples ): #CODE
* You're not really doing a pivot here anyway , it just happens to work in the above case .
I have a variable storing a string timestamp ( in Unix time ) that I'd like to append to an existing Python Pandas Data Frame as a column .
A simple way would be to copy the df and reverse the index yourself , and then append both together .
Using ` copy ` seems to only apply on the data , not the index .
See documentation for ` hist ` and the documentation for ` np.histogram ` .
Thinking about it , perhaps it makes more sense to do the pivot first : #CODE
I'm working with the current and previous time periods so I need to concat the two and work on them together .
Thanks Andy , This is just sample code and I'm doing a lot more then just getting aggregate stats like count .
@USER It's not really clear what you expect that to do , but using groupby ensures each groups is a DataFrame and then , for example , you can apply a function to that .
@USER if you really wanted to do it manually you could use : ` g = df1.groupy ( ' time ') ; [ g.get_group ( x ) for x in g.groups ]` , but I recommend apply .
this has been SOO helpful I can apply this to everything now !
I have been doing Pandas , in which I apply a ` .groupby ( ' Subtype ')` on the dataframe , but after I do that , I'm not sure how to proceed further .
You can append to a csv by opening the file in append mode : #CODE
If you read that and then append , for example , ` df + 6 ` : #CODE
For append it is ' a ' .
syntax error when attempting to insert data into postgresql
I am attempting to insert parsed dta data into a postgresql database with each row being a separate variable table , and it was working until I added in the second row " recodeid_fk " .
Eventually , I want to be able to parse multiple files at the same time and insert the data into the database , but if anyone could help me understand whats going on now it would be fantastic .
** NEVER interpolate values directly into SQL like this** , you leave yourself critically vulnerable to [ SQL injection ] ( bobby-tables.com) .
Maybe ` execute ` could fill in the table name too , and we could drop ` format ` entirely , but that would be an unusual usage , and I'm guessing ` execute ` might ( wrongly ) put quotes in the middle of the name .
That is , did you mean to insert the contents ` dr ` into nine different tables called ` tblv001 ` through ` tblv009 ` ?
read the column with no converter , then just replace on all at once , something like `` df [ ' column '] .replace ( ' . ' , '') .replace ( ' , ' , ' . ') .astype ( float )`` should be much faster as its done as array ops
The real case is a bit longer than the provided example and includes some stripping ( spaces ) , columns del , string concat , renaming / replace ....
The 30'000 x 150 DataFrame is processed in less than 15 seconds :-) :-) including all the other things I did not detailed here too much ( stripping , del , concat , .. ) .
You can just do it using ` cut ` on the groupby ( you can specify the bins if you want ) , or groupby however you want , using the data above ( that's why I am reading via ` StringIO `) #CODE
`` cut `` is pretty interesting here , can specify your own bins ( which is prob what the op wants to do ); easier than actually specifyng the groupby mapping , though could do that too
After you've constructed the timeSeries you can use the resample function : #CODE
Also , a lot of Pandas methods have a ` na ` argument , which let you decide which value you are going to use to replace not-available values
I usually read / translate NaN as " missing " .
The special value NaN ( Not-A-Number ) is used everywhere as the NA value , and there are API functions ` isnull ` and ` notnull ` which can be used across the dtypes to detect NA values .
` make_pivot_table ` just returns a pivot table using pandas ` pivot_table ` function .
I would do this using a ` shift ` and a ` cumsum ` ( here's a simple example , with numbers instead of times - but they would work exactly the same ): #CODE
Then you can use this in a groupby ` apply ` : #CODE
AttributeError : ' Timestamp ' object has no attribute ' shift '
@USER it looks like you've tried to apply shift to a Timestamp rather than a column / Series ( not sure how you did that though ) .
Basically the current excel workbook runs some vba on opening which refreshes a pivot table and does some other stuff .
Then I wish to import the results of the pivot table refresh into a dataframe in python for further analysis .
[ Using Python3 ] I'm using pandas to read a csv file , group the dataframe , apply a function to the grouped data and add these results back to the original dataframe .
Secondly I want to apply a flow statement to these results .
your func with `` percentileofscore `` needs 2 arguments so its not compaitable with transform ( which requires 1 ) . but I am not sure what `` score `` would be anyhow ...
so pandas doesn't know how to join the new sales figures back onto the old dataframe .
Another option is to use an apply : #CODE
Another option is promote the ' day ' level to a column and then use an apply .
Python 2.7 and Pandas merge 2 csv files with Forex Data
I want to merge the data based on the date time column which is the first column in both files . as you can see the second file does not have exact same records as the first file , so missing some data from second file , but thats ok .
And it should merge them using the same columns .
Matt , will the above merge both csv files based on common matching date time in the row ? and also will it just add the close or column #4 from the second csv file to the merged file ?
If I remember correctly it will just merge the common columns and add keep all the unique columns as they are .
You don't want to merge to two , you want to copy the column of one into the other .
Either avoid them from the beginning by not passing ` names =[ ]` to ` read_csv ` or replace them before writing with ` df.columns = range ( len ( df.columns ))` .
All I keep getting is index contains multiple values which it indeed does , but why wouldn't it understand that there are similar and map them to the same line as I did in my desired output ?
Does ` pivot ` give the correct output or not ?
@USER so i clearly say what the ` pivot ` is giving me index contains multiple values .
I don't know what you mean buy " understand that [ they ] are similar and map them to the same line " .
If list of functions passed , the resulting pivot table will have hierarchical columns
Regarding the difference between the two , I think ` pivot ` just does reshaping ( and throws an error if there is a problem ) , whereas ` pivot_table ` offers more advanced functionality , aka " spreadsheet-style pivot tables " .
can you explain the difference between ` pivot ` and ` pivot_table ` ?
I think it is purely that [ pivot_table ] ( #URL ) allows more advanced functionality , " spreadsheet-style pivot tables " , wheras [ pivot ] ( #URL ) just does * reshaping * ( and throws an error if there is a problem ) .
The file I am trying to read is 366 Mb , the code above works if I cut the file down to something short ( 25 Mb ) .
I can get it to work if I cut down the lines ( currently there are 900'000 lines , 71 columns ) .
Now I am encountering different out of memory errors where a table ( after merge ) is getting incrementally bigger to about 15M rows .
Also , what if I want to replace those values with a correctly shaped DataFrame ?
Use ` loc ` in an assignment expression ( the ` = ` means it's not relevant whether it is a view or a copy ! ): #CODE
This is an assignment expression ( see ' advanced indexing with ix ' section of the docs ) and doesn't return anything ( although there are assignment expressions which do return things , e.g. ` .at ` and ` .iat `) .
Bottom line : use ` ix ` , ` loc ` , ` iloc ` to set ( as above ) , and don't modify copies .
Definitely the docs need to extend more about when it's a view or a copy , I've said I would add some docs myself in the past ( just need to work out all the ins and outs ) , will make a github issue to get the ball rolling ...
Bottom line , is to use `` ix / loc / iloc `` to set , and don't modify copies .
and use this to mask the Series : #CODE
How can I generate a mask based on the index value ?
I want to generate a mask to only consider some rows where the index is in a certain range .
I was thinking of doing something like ` data [ ' index '] .time() datetime.time ( 1 , 15 )` to generate a mask .
How can you reference the index value for a row in a mask ?
You can mask using ` indexer_between_time ` : #CODE
Note : ` indexer_between_time ` by default both ` include_start ` and ` include_end ` are True , it also offers a ` tz ` argument to compare the time to a different timezone .
Note you can apply strftime directly from a Timestamp object e.g. ` rng.map ( lambda t : t.strftime ( ' %Y-%m-%d '))` .
Pass percentiles to pandas agg function
Then include this in your ` agg ` : #CODE
Being more specific , if you just want to aggregate your pandas groupby results using the percentile function , the python lambda function offers a pretty neat solution .
If I remove freq option in date_range , then this command will again not work for large series since the year will go well beyond pandas limit .
The problem is that this freq doesn't give back an offset from pandas for some reason , and we need an offset to be able to use the dates for anything .
I get ValueError : Can only append to Tables .
When you tried to do this with ` put ` it kept overwriting the store ( with the latest chunk ) , then you get the error when you append ( because you can't append to a storer / non-table ) .
` put ` writes a single , non-appendable fixed format ( called a ` storer `) , which is fast to write , but you cannot append , nor query ( only get it in its entirety ) .
` append ` creates a ` table ` format , which is what you want here ( and what a ` frame_table ` is ) .
Then append each DataFrame : #CODE
2 ) I tried filtering using the apply function : #CODE
I have extended it to what I need : Try ` new_ids = pd.Series ([ " a1 , c3 "])` and ` df = pd.DataFrame ( { ' vals ' : [ 1 , 2 , 3 , 4 ] , ' ids ' : [ ' a ' , ' b ' , ' c ' , ' f '] , ' stuff ' :[ " insert " , " whatever " , " you " , " fancy "] } )` .
I see what you mean by append now .
However , if anyone has a better way of doing this ( without creating an extra column in which I append the strings ) then please do let me know please .
Benefits of pandas multiindex ?
How to apply condition on level of pandas.multiindex ?
` concat ` , ` merge ` , ` join `
So far I've only been able to combine the values , ( ie . add the elements together , append the series to each other , or merge based on values ) .
If you're going to read tens of thousands of files , the fastest thing you can do is preprocess each file once and transform it to a regular format that you can easily read .
I have a nested dictionary object i want to convert to multiindex data frame , how can I achieve it .
you want an additional level which concat forms when passed a dict
I find it's easiest to get to a MultiIndex from a list of tuples , as described here .
As a side comment , if what you're trying to pickle is just a dict that maps strings to strings , you can just use a ` dbm ` instead of a ` dict ` , because that just is a map from strings to strings that's automatically persistent .
The ` dict ( {} .items() + function ( i , j ) .items() )` is supposed to merge both dict in one as ` dict() .update() ` does not return the merged dict .
I think you can directly call ` mean ` on ` GroupBy ` objects , and it will be faster than passing ` np.mean ` into ` agg ` and will handle ` NaN ` values , unless the name of the function maps to one bottleneck or numpy nan * functions
Pandas : How to merge Time Series with complementary Dates ?
I tried pd.concat in all variations ( or creating a series C with the orginial index and merging on it ) but it always just appends and does not merge as expected .
Why not just ` sort_index ` after doing the append / concat ?
Is there a way in Pandas to aggregate both columns at once to accomplish this without a loop ?
You can apply operations on those sparse matrices just like how you use a normal matrix .
Then replace the first line as indicated .
I want to merge A and B while keeping order , indexing and duplicates in A intact .
I tried doing a concat of the two columns and a groupby but that doesn't preserve column A values since duplicates are discarded .
And then you can append ( ` concat ` ) these with A , sort ( if you use ` order ` it returns the result rather than doing the operation in place ) and ` reset_index ` : #CODE
You can also resample if you are spanning multiple dates .
I have seen some of the slider examples but I am not able to translate that to pandas plots or what axes or arguments to pass to the Slider object .
Is there an easy way to find the disjoint set of records ( what would be left on each of the two original dataframes that is not included in the resulting inner join ) between two pandas dataframes based on a MultiIndex ?
My other option , which seems like it might be a bit easer is to add a dummy column of integers that can act as a different single index that is preserved even after I do the multiIndex merge so I that I can use the python set operators on this de facto single key .
[ Note that this is related to but slightly different than this question because this merge is not based on a MultiIndex object , but on the values in columns of the dataframe : How do I do a SQL style disjoint or set difference on two Pandas DataFrame objects ? ]
I'm not sure why there isn't a symmetric difference method on MultiIndex .
The only problem is with the MultiIndex and the size of my dataframes it ends up taking almost a minute to calculate the sym_diff .
` loc ` could be swapped out for ` ix ` here .
Pandas HDFStore of MultiIndex DataFrames : how to efficiently get all indexes
Plan B would be to keep a shorter MultiIndex DataFrame that just contains empty DataFrames appended every time I append the main one .
By way of explanation , not all possible combinations of my multiindex are filled , so just getting the levels of the multiindex are not enough .
I spent more than an hour implementing what you did with merge ...
switch your slashes ... change ` \ ` to ` / ` or double up on them ( replace ` \ ` with ` \\ `) ... your \r is being interpreted as a carriage return
Using a dictionary to replace column values on given index numbers on a pandas dataframe
Here's one way ( though it feels this should work in one go with an apply , I can't get it ) .
In good times and bad , `` transform `` is there .
You could also do a one liner using merge as follows : #CODE
You could also do a one liner using transform applied to the groupby : #CODE
I have a merge data frame ( mdf ) which the 2 data frames are retrieved from SQL .
I'm not sure what you mean by a " merge data frame , " but here's a sketch of what you might be after .
You can also try to replace those symbols and separator : #CODE
Not implemented error when i tried to resample a data frame with datetime index
When i try to resample , i got a not " implemented error " .
pandas normalize multiindex dataframe
How do I normalize a multiindex dataframe ?
I know how to normalize a basic dataframe : #CODE
but I'm not able to apply this on each " name " group of my dataframe
I tried groupby and a multiindex data frame but probably I'm not doing it in the right way
I'm assuming you want to normalize value1 and value2 separately .
Your desired result suggests that you actually want to normalize the values in each name group with reference to the elements in ` value1 ` and ` value2 ` .
For something like that , you can apply a function to each group individually , and reassemble the result .
But when I try to drop it into a dataframe :
loc will not attempt to use a number ( eg 1 ) as a positional argument at all ( and will raise instead ); see main pandas docs / selecting data
I read about the .stack method but couldn't figure out how to apply it for this case .
@USER : I meant if it was ever called because if you have a csv file with first column 1 , 2 , 3 ,... and you call ` set_index ` on that it will be indistinguishable from the default index , but it's still important for me to know if it was indexed or not because I am doing a merge operation that relies on an indexed dataframe
what do you mean by " indexed " and how does merge operation rely on it ?
@USER : I am using ` concat ` to concat columns of two dataframes together and I want to avoid concatenating together two dataframes that are not indexed that's all . indexed I mean dataframes where the user explicitly assigned a unique column identifying each row to the dataframe with ` set_index ` . maybe this is impossible to tell ?
Can't you just call set_index ( column ) again before doing the merge ?
@USER : good point but if you're writing a generic merge function you don't know what column the user wanted to merge
Unless of course someone called df.set_index ( np.arange ( 0 , len ( df ))) :) .
Can you apply the weighting first and then call describe on the resulting series ?
I'd expect it to apply to all of the metrics .
You'd want median revenue weighted by population to be 0 .
If you multiply revenue by weight and apply describe ... you probably get a median of 0.15 ( vector 0 , 0.3 ) which is irrelevant .
First , with stata's weight option ` [ w=wt ]` ( I've truncated the output , 50% here indicates the median ): #CODE
stack it first and then use value_counts : #CODE
Now I would like to merge these two nodes using this .
But now for the case where it has to merge chunk by chunk and it's giving me the following error : ` TypeError : Cannot serialize the column [ date ] , because its data contents are [ empty ] object dtype ` .
Check the len before appending #CODE
I added an issue about this , I think there is a subtle error here unrelated to HDFStore that is raising an error ( rather than just letting it proceed , which I think is correct ) , an empty frame will normally succeed on the append , except in the case you had ( and I illustrate ): #URL
One little thing that I notice from the results you provide : how come can the node ' mergev46 ' have a larger shape than ' var6 ' by doing an ' inner ' merge ?
You can use the ` pivot ` method for this .
I have a pandas series of booleans and was wondering what the best way is to apply " or " or " and " to the whole series .
will apply a function to each element in the series so doesn't seem to do what I need .
You'll be happy to discover ` replace ` : #CODE
Is there a difference with ` map ` in this case ?
It seems that something else ( not in the dift ) is just left with ` replace ` , but converted to ` NaN ` with ` map `
I think `` map `` is a better choice here , actually , because if a value isn't in `` d `` then the value is invalid and should be replaced with `` NaN `` .
` replace ` seems to apply to DataFrame not to a Serie
You can just use ` map ` : #CODE
Transpose so that we can use the ` diff ` method ( which doesn't take an axis argument ) .
Otherwise , using the resample function .
Try this , which uses ` apply ` with ` axis=1 ` instead of iterating through rows .
Thanks a lot , Dan , this works ( although I had to be a bit careful with argmax on an all-false array ) .
I tried to merge these values back to my original dataframe , but I the keys that I want to merge on are in the Index ( ix / loc ) .
Note that ` transform ( ' count ')` ignores NaNs .
If you want to count NaNs , use ` transform ( len )` .
To the anonymous editor : If you are getting an error while using ` transform ( ' count ')` it may be due to your version of Pandas being too old .
I've been trying to apply that to a larger DataFrame and keep on getting this error
Try selecting only one column for transform i.e. df.groupby ([ ' Color ']) [ ] .transform ( ' count ')
just for information , the groupby is dropping away the null values , so if you try to do a transform on a column which has null , the number of return value will be lower than expected .
@USER : Specify the column you wish to count before calling ` transform ` : ` df.groupby ([ ' Color ']) [ ' Value '] .transform ( ' count ')` .
My initial thought would be to use list comprehension as shown below but this as was pointed out in the comment , this is slower than the ` groupby ` and ` transform ` method .
If the criteria I apply return a single row , I'd expect to be able to set the value of a certain column in that row in an easy way , but my first attempt doesn't work : #CODE
I need to transform the data to have each individual index contain all the possible locations .
I can reindex in this way ( just showing individuals 1 to 3 ): #CODE
1 ) Using a loc_dict = { 301 : 1 , 302 : 7 , 303 : 3 } to replace loc_var and a ind_dict = { 1 : 4 , 2 : 8 , 3 : 10 , 4 : 15 } to replace ind_var
This can be done by ` stack / unstack ` and ` groupby ` very easily : #CODE
corrected string cast to bool with a comparison xxx == ' True ' #CODE
Is this line : ` checked = bool ( index.model() .data ( index , QtCore.Qt.DisplayRole ))` really gives a boolean ?
Here it is ( I had to create a class called ` Dataframe ` to simulate the panda Dataframe structure . Please replace all the ` if has_panda ` statements by yours ): #CODE
Ok , I did a little more research and this pretty much covers what i wanted to do . using matplotlib colormaps did the trick . here is a link to the options of map colours .
The files are hundreds of Mbs gzip compressed and several Gbs uncompresed , so will read line by line and append to the corresponding DataFrame .
A nested apply will do it #CODE
` x ` is a ` DataFrame ` containing rows which all have the same domain , and you transform the ` price ` and ` product ` columns in ` x ` into ` Series ` objects called ` y ` , one per column , then count number of times each distinct value appears in ` y ` .
Ah I think the problem is with my df's index , it is not a sequence like ` range ( len ( df ))` .
Pandas , numpy , scipy , and the rest of the scientific computing stack are going to look rather noisy as they are building packages .
Further update - I figured out how to conform to the pattern I wanted .
however , pandas seem to truncate the number to its integer part : #CODE
suppose I have a dataframe with index as monthy timestep , I know I can use ` dataframe.groupby ( lambda x : x.year )` to group monthly data into yearly and apply other operations .
Before solving classification or regression tasks I want to add new transformed columns i.e. to clean up source data or to normalize them .
There might be a cleaner method , but one approach is to use a mask ( boolean array ): #CODE
where I have `` df [ ' B ']`` you can put a scalar ( e.g. ' Close ') , though you should really do this in another coulumn ( e.g. columns you are selectin from , `` df [ ' A ']`` does not have to be the same as the mask `` df [ ' A '] > df [ ' B ']`` , otherwise you will get a mixed float / string column , generally not useful ( and not efficient for anything ) .
You can also have another column where I have `` df [ ' B ']`` as the replacement value ( and pandas will align it to the selector column ) .
The basic problem is that ` if / else ` doesn't play nicely with arrays , because ` if ( something )` always coerces the ` something ` into a single ` bool ` .
But if you want to do this in pandas , you can ` unstack ` and ` order ` the DataFrame : #CODE
Length : 91910 , Freq : None , Timezone : None
I have tried several variants on : ( I assume I'll need to apply the limits to each plot .. but since I can't get one working ...
Group it by value , and use the ` filter ` method on groups to replace any group with less than 3 values with ` NaN ` .
Then replace those ` NaNs ` with ` X ` .
And , to be sure , you can merge the original and the recoded frames just as you expressed it in your question : #CODE
You are using an indexing short-cut which doesn't apply , see here : #URL
First of all , get rid of that ` for i in range ( len ( filepaths ))` !
EDIT : the ` AttributeError : ' numpy.int64 ' object has no attribute ' replace '` error you get is due to the fact that you use integer column labels ( this is a bug ) .
I tried this after removing the table creation code ... sql.write_frame ( df , con =d b , name= ' TEST ' , if_exists= ' replace ' , flavor= ' mysql ') , It says : AttributeError : ' numpy.int64 ' object has no attribute ' replace '
Note , there seems to be a bug with the ' replace ' option ( see eg #URL ) .
Does it run without ` if_exists= ' replace '` ?
Without replace it generates this error ..
With replace gives the above error .
I followed what you said , now the problem is with this replace thing .
I think the best option at the moment is to delete the table yourself in MySQL if you want to overwrite it ( as ' replace ' is not working at the moment : #URL ) .
I am trying to merge a couple of dataframes that I create from csv files , and it is failing on one of the files with the following error : #CODE
By Googling the problem I got the suspicion that it might have to do with the uniqueness of the columns when trying to merge .
I did see this problem in an instance where the tuple provided as suffixes to the merge function consisted of two identical entries .
Hence my question : Can I debug the merge function in pandas , and if so , how ?
That's weird cos you can join / merge non-uniquely indexed on 0.11 ( which pandas version are you using ? ) Can you give a small example where this doesn't work , e.g. does it fail with ` df = df.head() ` ( or with just a couple of non unique index )
It looks like you want to first do a ` ffill ` then do a ` shift ` : #CODE
To do this over each group you have to groupby category first and then apply this function : #CODE
I want to do hierarchical indexing , so that the same e-mails are grouped together , so I need to convert a DataFrame Index into a MultiIndex ( e.g. for the entry above - ` ( name@USER.com , 2013-05-07 05:52 : 51 +0200 )`) .
You could drop these if you want .
Try storing as a table ( either use `` append `` or `` store.put ( ' df ' , df , table=True )`` which stores in the `` Table `` format ; better handling of things like `` nan `` certain dtypes ( that the `` Storer `` format will give you a PerfWarning .
Tables try to `` append `` ( while `` put `` always overwrites )
This will create a PeriodIndex of monthly freq .
Also , generally good practise to specify that you are indexing by the label ( with the loc ): #CODE
This is a surprising error in 0.8.1 ( but seems to be fixed in later versions ) , perhaps a workaround ( if the above doesn't work ) is to set the fancy index first ( ` df_A_gt_half = dfrm.A 0.5 `) and then do the assignment using that ... and are forced to use ` ix ` rather than ` loc ` .
The initial way I suggested ( which is clearly not optimal ) , once you've read it in as a DataFrame you can remove these rows using ` notnull ` ( you want to keep only those rows which are all ` notnull ` ): #CODE
using ` drop ` method ?
The same code works if I drop the chunksize bit : #CODE
When you apply your own function , there is not automatic exclusions of non-numeric columns .
How could I apply different functions on several columns in one go as well , e.g. sum on column " B " and set on column " C " ?
These are stacked ( row-wise ) to form the result frame at the very end of the apply .
just to point out though , you should still do the aggregation on A and B via a direct `` .sum() `` rather than apply because these are cythonized .
The apply going to be slower , so do only where you really need it .
A slightly more direct way to apply different functions on several columns is to use the ` agg ` function , so ` df.groupby ( ' A ') .agg ( dict ( A = ' sum ' , B = ' sum ' , C = lambda x : ' {%s} ' % ' , ' .join ( x )))`
Thanks a lot , I have been looking into the ` agg ` function .
Any ideas how that would compare performance-wise to apply ?
they are the same ; apply is a bit more flexible in how it looks at the output ( just slightly )
You can use the ` apply ` method to apply an arbitrary function to the grouped data .
So if you want a set , apply ` set ` .
If you want a list , apply ` list ` .
If you want something else , just write a function that does what you want and then ` apply ` that .
You may be able to use the ` aggregate ` ( or ` agg `) function to concatenate the values .
Whenever you are done calculating your result , you need to do ` output.set ( ... )` , and replace ` ...
If I put in a row for each team for each game , I will have two rows for each game , and it will be easy to find games by team , but it will have dupe rows if i try to aggregate across all games .
Then I select the argmin idx2 , and save somewhere that idx1 is matched with idx2 .
Works great , and the quadratic algorithm became linear when n >> len ( group_combination ) .
If you want the sum of the histogram to be 1 you can use Numpy's histogram() and normalize the results yourself .
If you want to avoid matplotlit altogether , join the two series first and call plot on the resulting DatFrame : #CODE
As an example of an application , I would like to drop rows in my dataframe where that condition is met .
I tried the suggestion from @USER to drop entries ( rows ) from my dataframe , but the following command : #CODE
The odd thing is that if I use that index object to drop rows from my dataframe I get : ` / opt / python / virtualenvs / work / lib / python2.7 / site-packages / pandas / core / config .
Applying a map elementwise to every element on a Series ( Pandas )
Why am I unable to apply my function elementwise on my ` Series ` object ?
Also , why do DataFrames and Series have different method names for elementwise operations ( i.e. ` map ` for Series vs ` applymap ` for DataFrames )
What happens when I append another dataframe to a store that has a full index on that column ?
I think when u append it will index to the new scheme . the only reason you actually need to sortby is to force it to reindex ( you can actually do this via a pytables function call as well , which is what sortby is doing )
However , you can also set the index to be CSI when you create it , so it will create the index as you append ( but I suspect that might be slower as it potentially may have to move data more than once )
Now use the ` resample ` method to get to the frequency you're looking for : I'm going to demonstart quarterly , but you can substitue the appropriate code .
To aggregate , we take the sum .
To do this , use the ` shift ` method , which shifts all the data down one row .
You could write a function and ` apply ` it to each DataFrame in the panel you get from Yahoo .
I am way I am trying to apply a lookup table that is based on the time and also the raw instrument reading through the use of a nested python dictionary .
is there a way to iterate through my dataframe to apply these calibration factors with the nested dict as a form of a lookup table ?
** Surely ** it's a bug that a shift ( by a month ) is actually going to the next month and not incrementing by a month ...
I agree with Andy ; that can't be the intended behavior of ` shift ` .
A cleaner way to shift times to the end of the month is this : #CODE
Finally reindex and fill the ` NaN ` s with 0 : #CODE
Till ` pandas ` is not officially implemented in ` plt.fill_between ` function , you can still apply ` pd.Series ` or ` pd.DataFrame ` as ` pd.Series() .values ` and ` pd.DataFrame() .values ` to make ` fill_between ` plots .
Then join theads and procces results .
If you don't need the indexes inside the function then you can often avoid this penalty ( by not instantiating the series , which value_counts does , and then gets discarded because all you need is the len of it )
I've been reading about hierarchical index and multiindex in a pandas dataframe but it seems these are all for ordered labels .
And I want to be able to group the data together based on the column label ie . aggregate all columns with ' d ' in row 3 together by averaging .
I tried ` g.filter ( lambda x : len ( x ) > 1 )` but it didn't work for me , I got an exception .
g.filter ( lambda x : len ( x ) > = threshold ) File " / var / tmp / SKL / lib / python2.7 / site-packages / pandas / core / groupby.py " , line 2094 , in filter
Instead of length ` len ` , I think you want to consider the number of unique values of Name in each group .
A general remark : Sometimes , of course , you do want to know the length of the group , but I find that ` size ` is a safer choice than ` len ` , which has been troublesome for me in some cases .
I always forget about transform !
People often do not discover it -- I think I answer transform questions at least weekly .
`` apply `` would return a shorter Series , with one entry per group .
Then we can use that boolean Series to mask the original Series .
You could first drop the duplicates : #CODE
And then use ` apply ` : #CODE
I think loc was introduce in 0.10 ( recommend updating to latest stable ) , difference is loc is label based vs ineger location based ix tries to infer your meaning
pandas select from Dataframe using startswith
And got AttributeError : ' float ' object has no attribute ' startswith '
Section 4 : List comprehensions and map method of Series can also be used to produce more complex criteria :
Could you give a small example which demonstrates this , I'm surprised that the list comprehension wouldn't raise in the same way as the map ...
and the boolean indexing will work just fine ( I prefer to use ` loc ` , but it works just the same without ): #CODE
It looks least one of your elements in the Series / column is a float , which doesn't have a startswith method hence the AttributeError , the list comprehension should raise the same error ...
Sorry the data is 27 cols long , kind of unwieldy even to post a clip .
Alternatively you could use ` apply ` ( but this will usually be slower ): #CODE
Since you are using the " trailing row " you are going to need to use ` shift ` : #CODE
thanks shift is what i was looking for . now i can find examples in the Pandas book
You can use the ` shift ` Series method ( note the DatetimeIndex method shifts by freq ): #CODE
I've isolated that column , and tried varies ways to drop the empty values . first , when im writing : #CODE
Now im trying to drop those entries .
You should use ` isnull ` and ` notnull ` to test for NaN ( these are more robust using pandas dtypes than numpy ) , see " values considered missing " in the docs .
The ` dropna ` DataFrame method has a subset argument ( to drop rows which have NaNs in specific columns ): #CODE
Could you do this easier using some kind of rolling sum ( of 3 Qs ) ?
How to apply a function to two columns of Pandas dataframe
Pandas : How to use apply function to multiple columns
And apply the function like this : #CODE
The kicker with my problem is that I needs be able to apply this function across several different column pairs , so hard coding the column names like the answer in 2nd question is undesirable .
The confusion stems from two different ( but equally named ) ` apply ` functions , one on Series / DataFrame and one on groupby .
The DataFrame apply method takes an axis argument : #CODE
The groupby apply doesn't , and the kwarg is passed to the function : #CODE
If I stick with the groupby apply and remove the axis param , I get a key error ` KeyError : u'no item named 0 '` for accessing the elements as ` row [ 0 ]` ect .
Is there a way to use the groupby apply and still use a notation that keeps it easy to apply to several differently named column pairs ?
@USER updated , you can use the groupby with axis=1 , you can apply pct_change to entire dataframe .
Or perhaps you want to do this one each group using an apply ( ` lambda x : x.pct_change() `) .
@USER you need to tweak delta a bit , see my last example ( assuming that's from the apply )
@USER not sure I have a good reference , but a good trick is to set up a break point in the function you're going to apply and then see how you can access things you want
pandas dataframe aggregate - why does it return column names ?
I have been trying to play around with the aggregate function , after grouping by CustID .
Why is the aggregate filling the data frame with the set of all column headers ?
The point of agg is to aggregate the passed frame to say a Series .
Pandas merge and join not working
On some pairs ` merge ( df1 , df2 )` is working correctly but ` df1.join ( df2 )` is not .
Now , to make things really strange , on some other pair of dataframes ` merge ( df1 , df2 )` is not woking but ` df1.join ( df2 )` is working .
I would recommend doing a concat here ( which works better if they are indexed , but doing have the repeated studentid column ) .
You could define a function to subtract the quarterly totals from the annual number , and then apply the function to each row , storing the result in a new column .
ReshapeError while trying to pivot pandas dataframe
Using pandas 0.11 on python 2.7.3 I am trying to pivot a simple dataframe with the following values : #CODE
The pivot results successfully in following : #CODE
What do you expect your pivot table to look like with the duplicate entries ?
I'm not sure it would make sense to have multiple elements for ( 1234 , bar ) in the pivot table .
Otherwise you can come up with some rule to determine which of the multiple entries to take for the pivot table , and avoid the Hierarchical index .
Therefore , I think the only way I can get is aggregate on the DateRecorded column by max() .
Instead of relying on Pandas ( which is better of course ) you could also aggregate your data manually .
Assuming these are just strings you could simply add them together ( with a space ) , allowing you to apply ` to_datetime ` : #CODE
Python Pandas merge only certain columns
Is it possible to only merge some columns ?
I want to merge the two dfs on x , but I only want to merge columns df2.a , df2.b - not the entire dataframe .
I could merge then delete the unwanted columns , but it seems like there is a better method .
You could merge the sub-DataFrame ( with just those columns ): #CODE
Then just drop rows where " InOtherDF " is ` True ` .
I think this is a cleaner way using ` merge ` #CODE
It feel like there ought to be a neat way to do this using an inner merge ...
Feels like ought to be a neat merge way
A purely pandas way might be to apply the Series constructor to put this into one DataFrame and stack into a Series ( so you can use value_counts ) ...
` Apply ` Series constructor to get a DataFrame and ` stack ` it into a Series : #CODE
Now when we stack , we see the names of the levels : #CODE
Pandas : boxplot of one column based on another column
I would like to boxplot the age of each Group ( A , B , C ) .
Misread 1st time so gave answer for histograms ... keeking that below . for boxplot the code is : #CODE
Do you know how to get rid of `" Boxplot grouped X "` in the figure title ?
Thanks Joop , unfortunately the title command just changes the part that says ` Age ` in the boxplot ( in your post ) , that's why I asked .
Is there any concern with calling transpose ?
To get a DataFrame back , you have to apply a function to each group , transform each element of a group , or filter the groups .
yay , transform !
I will then iterate over the groupbyObj and apply the calculation function , which will return the selected speaker and the files to be marked as used .
bcz I thought maybe instead of setting the files as indexed , grouping by a file , and the using that groupby obj to apply a changing function to all of its occurrences .
But I didn t find a way to approach a specific group and passing the group name as parameter and calling apply on all the groups and then acting only on one of them seemed not " right " to me .
If ` mask ` is a basic slice , then ` df.ix [ mask ]` returns a view of ` df ` .
If ` mask ` is something more complicated , such as an arbitrary sequence of indices , then ` df.ix [ mask ]` returns a copy of some rows in ` df ` .
How to compute rolling rank correlation using Pandas
I would like to compute rolling rank correlation between two columns in a data frame .
I tried to implement rolling rank correlation with ` rolling_apply ` , but did not have any success .
Is there a clever way to implement rolling rank correlation with ` rolling_apply ` or some other methods ?
I don't think ` rolling_apply ` can be used to do a rolling correlation , as it seems to break DataFrames into 1-d arrays .
I have also tried to read file line by line , create a series from ` row.split() ` and append the series to a dataframe , but it appears to crash due to memory .
I want to linearly interpolate the 30 minute data to the exact time values of the sparse data .
Subselect and join #CODE
Could also replace the 0s with ` NaNs ` ahead of time and call ` prod ` on that .
@USER or use the DataFrame prod method :)
Just to throw out an alternative ( you can use the ` prod ` DataFrame method ): #CODE
Is there an elegant way using ` transform ` ?
You can simply replace all of these occurences with NaN : #CODE
and perform ` get_dummies ` ( which I think ought to ignore the NaN , but there is a bug , hence the ` notnull ` hack ): #CODE
use ` value_counts() ` to do the frequency counting , and then create a mask for the rows that you want remain : #CODE
I am trying to append 3 columns from one dataFrame to the end of another , similar to the following : #CODE
If I write both df's to a file using to_csv , then read them back into new ones with read_csv , the concat process works fine .
If I try to take the originals , and convert them to numeric types by .astype ( str ) .convert_numeric , the types match the read versions , however the concat still fails .
All this seems to do is reverse the concat order ( ie :
I would like to point out this , which is ` True ` because ` bool ( np.nan )` is ` True `
and also ` bool ( float ( ' nan ')) is True ` , agree it's odd ... personally think it should be Falsey .
does the string in eval support functions , like , df [ ' c '] = df.eval ( ' a.diff() + b ') ?
Even python offer eval and exec , so the security is depend on how to use it . dataframe.eval only support arithmatic is too limited .
Quite easily with the ` shift ` method .
` apply ` on a series returns a DataFrame if the function wich is applied returns a Series for each element of the series , where the different elements of the returned Series become the values of the different columns of one row .
I need to be able to aggregate this table by customer ( without PartType ) and do a sum and count of customers in size bins by YEAR , as well as do the same exercise counting customers by PART_TYPE by year .
IIUC , you're looking to melt : #CODE
I'm not sure I follow exactly what aggregation you want , but you should be able to apply ` groupby ` however you like .
How to transform an SNP matrix in Tab-delimited format into numbers using python ?
I am attempting to transform the bases in the tab-delimited file into integers .
How to implement pandas ( moving ) rolling statistics functions performance in my own code
Also you can speed it up by extracting just the calculation part and using ` rolling_apply ` that takes an extra func argument and performs generic rolling computations .
You could use an aggregate with a ` join ` : #CODE
Also , * nearly * a transform : ` df.groupby ( ' B ') .transform ( ' , ' .join )` ( which would have been more elegant ) ...
I have two Series that I need to join in one DataFrame .
When I use concat I get a DataFrame that has one index ( good ) but two columns that have the same values ( bad ) .
Difference between asfreq and resample
Can some please explain the difference between the asfreq and resample methods in pandas ?
` resample ` is more general than ` asfreq ` .
For example , using ` resample ` I can pass an arbitrary function to perform binning over a ` Series ` or ` DataFrame ` object in bins of arbitrary size .
As the pandas documentation says , ` asfreq ` is a thin wrapper around a call to ` date_range ` + a call to ` reindex ` .
An example of ` resample ` that I use in my daily work is computing the number of spikes of a neuron in 1 second bins by resampling a large boolean array where ` True ` means " spike " and ` False ` means " no spike " .
Pandas will automatically align these passed in series and create the joint index
I think ` concat ` is a nice way to do this .
@USER hmmmm , I think it depends what you want the answer to be ... something with merge : ` pd.merge ( s1.reset_index() , s2.reset_index() , how= ' outer ')` ?
" It is worth noting however , that concat ( and therefore append ) makes a full copy of the data , and that constantly reusing this function can create a signifcant performance hit .
@USER what " constantly reusing this function " means is that you should prefer doing the concat ** once ** ` pd.concat ([ list_of_dataframes ])` vs concating many times ` new_df = pd.DataFrame() ; for df in list_of_dsf : new_df = pd.concat ([ new_df , df ])` or similar .
Using a Python dict to replace / clean data in a Pandas DataFrame
And I need to map / replace the names using a Dict , ( which I have in a Excel sheet )
When I read the translate table into Pandas I get a DF that looks like #CODE
I want to look up values in the 1st col of the xlate table match them to table2 [ ' SUBDIVISION '] and if found replace contents of SUBDIVISION with the values in xlate column 2 if not leave them alone ( bonus .. actually if col 2 is NAn I'd like to leave it alone as well ) for instance above finding INVERNESS POINT will be replaced by INVERNESS
And use this to ` replace ` the column : #CODE
Ahh I had tried to reassign the translate dict to a new variable which it did not like ..
Instead if I reindex ` a ` on the same range : #CODE
I would like to know if this is expected and if there is a workaround to reindex large datasets without significant memory overhead .
` reindex ` will be default create a copy , so if the old variable is still around you should get about double the memory usage .
Have you tried passing ` copy=False ` to ` reindex ` ?
Index uses a Hashtable to map labels to locations .
HYRY's comment below is the issue -- the reindex is creating a large hash table that isn't being garbage collected unless you take the approach you listed .
How can I apply formatting to the secondary Y-axis ( the one that displays on the right ) ?
Finding the intersection between two series in Pandas
I have two series s1 and s2 in pandas / python and want to compute the intersection i.e. where all of the values of the series are common .
How would I use the concat function to do this ?
I have been trying to work it out but have been unable to ( I don't want to compute the intersection on the indices of s1 and S2 , but on the values ) .
then use the set intersection method .
s1.intersection ( s2 ) and then transform back to list if needed .
Can translate back to that .
Have added the list ( .... ) to translate the set before going to pd.Series Pandas does not accept a set as direct input for a Series .
also , you can use ` & ` operator for set intersection .
Actually , you can't just apply Series to a set ( which is annoying ) ` TypeError : Set value is unordered ` , seems unnecessary restriction / not very duck .
You can count the NaN values , drop them , and append the same amount again at the end .
@USER exactly right ; apply DOES call things twice ( on purpose ) to see if there are modifies in place ( in which case slow path is taken ); otherwise a faster path can be taken .
The ` +1 ` in the function will mess up the shift in the subsequent rows though !
not sure about your i = 0 case , but if you care you can do a conditional , in the `` shift `` if needed
It takes an array , an integer number of places to shift and an ` axis ` argument so you can shift an ` ndarray ` along any of its axes .
You can use ` concat ` : #CODE
Note : if these were DataFrame ( rather than Series ) you could use ` merge ` : #CODE
I cant seem to get the merge to work though ... sorted now :)
The problem is that ` drop ` takes an array-like argument ` labels ` , and you are only passing it a timestamp .
Using a plain ` dict ` instead of ` defaultdict ` , and changing ` d [ parts [ 0 ]] .append ( parts [ 1 ])` to ` d [ parts [ 0 ]] = d.get ( parts [ 0 ] , [ ]) + [ parts [ 1 ]]` , cut the time by 10% .
Running in PyPy 1.9.0 instead of CPython 2.7.2 cut the time by 52% ; PyPy 2.0b by 55% .
If you can't use PyPy , CPython 3.3.0 cut the time by 9% .
So , that's one third of your time spent in ` string.split ` , 10% spent in ` append ` , and the rest spend it code that ` cProfile ` couldn't see , which here includes both iterating the file and the ` defaultdict ` method calls .
The profiling stack for the pandas version is quite long ( lot of c functions , it's highly optimized , and written in cython ) and I think it would be useless to post it .
I could ' cheat ' by reducing the hash size down to a very small number which for a multivalued dictionary would increase insert speed at the expense of lookup .
I do really think that the lookup speed is probably rather important too .. you can gear the insert speed and the expense of the lookup ( although I've intentionally not done this in my timings ) .
map ( hash , ( 0 , 1 , 2 , 3 )) [ 0 , 1 , 2 , 3 ]
map ( hash , ( " namea " , " nameb " , " namec " , " named ")) [ -1658398457 , -1658398460 , -1658398459 , -1658398462 ]
If you're running linux , try ` top ` and then ` Shift + M ` to sort my memory usage .
And then you can transform it , in this case reshape it with ` pivot ` to create columns for the different metrics : #CODE
Also my legend outside is cut off .
They are cut off .
How to merge two DataFrame columns and apply pandas.to_datetime to it ?
What would be a more pythonic way to merge two columns , and apply a function into the result ?
this is inefficient since it references the index ` ix ` of ` df ` for each row - is there a better way ?
@USER can just transpose do transpose it
Just apply the ` min ` function along the axis=1 .
Then use ` groupby ` and ` apply ` : #CODE
Sort the data by Customer and Date , then aggregate by taking the first for each Customer : #CODE
Multiple aggregate functions !
Create the mask ( which is essentially your conditions where you want to assign ; create the result and assign ) , e.g. `` df.loc [ mask , columns_i_want ] = result_df `` , see [ here ] ( #URL )
I'm trying to apply simple functions to groups in pandas .
I want to apply a function like ` np.log2 ` only to the groups before taking the mean of each group .
This does not work since ` apply ` is element wise and ` type ` ( as well as potentially other columns in ` df ` in a real scenario ) is not numeric : #CODE
is there a way to apply ` np.log2 ` only to the groups prior to taking the mean ?
I thought ` transform ` would be the answer but the problem is that it returns a dataframe that does not have the original ` type ` columns : #CODE
So can't think of how to use the second one to apply ` np.log2 ` to numeric data only and then group or group first and then apply only to groups
If there's another way to aggregate without putting a ` list ` inside of a ` Series ` you should consider using that approach instead .
A more general solution if you want to use ` agg ` : #CODE
I think you'll be able to ` apply ` a different function here ( rather than sum ) to achieve the desired result .
@USER then sum or more complicated analysis e.g. via apply :)
Pandas drop function : Unalignable boolean Series
I would like to iterate over df0 [ " MAPINFO "] and drop rows that don't match condition and append the means to another df .
My solution would be to make bool index which implements inclusion criteria then just use it : #CODE
I'm trying to modify your code to apply lambda function for each MAPINFO at once .
I'll open a github issue since passing ` align ` to the plot method doesn't do really do this ...
What do you think ` align ` should do as an argument to ` plot ` ?
The current behavior of ` align ` with ` s.plot ( kind= ' bar ' , align= ' center ')` is a bit off .
One of the issues I am having is that all of my NaNs are at the end of the 2013 values . matplotlib wants to drop these from the graph by default which causes issues when trying to layer these plots on top of each other .
I have the following ` DataFrame ` that I wish to apply some date range calculations to .
I then would use a list apply function to determine if the difference in date between the fist and last index within each dataframe to return the oldest if it was less than 8 weeks from the most recent date .
pandas groupby apply on multiple columns
I am trying to apply the same function to multiple columns of a groupby object , such as : #CODE
What is the correct way to apply the function to multiple columns at once ?
I think you are looking for transform - it applies a function to each group .
( Not that this syntax will be changing in 0.13 to allow a more direct ` df.loc [ mask , ' time '] += pd.offsets.Milli ( 5 )` #CODE
` melt ` is pretty useful once you get used to it .
This one is very similar to the ` melt ` solution provided by DSM : #CODE
Applying ` stack ` will pivot the column axis on a sublevel of the row axis already indexed by ` item ` .
Let me add that you can indicate on which column to drop duplicates , which would be the email column : ` df.drop_duplicates ([ ' Email '])`
I would suggest replacing the work_hours lambda and map with ` between_time ` which is direct and to the point : #URL
this bug is resolved in master , but concat is the right way to do this
This would work well in case the columns you want to merge on are index columns , which you can achieve using ` pandas.set_index() ` , possibly preceded by ` .reset_index() ` .
What I end up with is a stack of the three dataframes with blanks for all columns which aren't present in the other dataframe .
The ` axis=1 ` option allows you to concat horizontally using a common index for all DataFrames you want to merge .
The default ` axis = 0 ` will lead to a vertical stack .
data.replace ( ' , ' , '' , regex = True ) to replace , with nothing and easy to tranform into float type with the gentleman method !
Glad you like the ` replace ` method !
Iterrows a rolling sum
I want to perform a rolling sum over each 12 month period .
The ` axis=1 ` parameter causes Pandas to compute a rolling sum over rows of ` df ` .
Is there a ` drop_duplicates() ` functionality to drop every row involved in the duplication ?
CHeck documentation #URL Depending on how data is structured you should be able to do set operations too . can definately replace items in frame with items from another frame .
Ie do you merely want to find the unique items or do you need to merge them with some additional logic ?
Note have to transform set to list 1st as set cannot be used to construct dataframe : #CODE
Apply by the columns of the object you want to map ( df2 ); find the rows that are not in the set ( ` isin ` is like a set operator ) #CODE
I have a classical database which I have loaded as a dataframe , and I often have to do operations such as for each row , if value in column labeled ' A ' is greater than x then replace this value by column'C ' minus column ' D '
You can just use a boolean mask with either the ` .loc ` or ` .ix ` attributes of the DataFrame .
` apply ` should generally be much faster than a for loop .
This example you provided is : ` ( df [ ' A '] == 999 ) & ( df [ ' B '] == 999 )` , But if you have a branches with else statement also you should use ` apply ` along the asix .
I added an example to the answer that covers that case ( using ` apply `) .
I just need to interpolate ONE value , not the whole TimeSerie
I just want to interpolate data using a linear function between the previous index ( ` 2013-08-11 14:23 : 49 `) and the next index ( ` 2013-08-11 14:19 : 14 `)
Then use ` interpolate ` .
I'm afraid I don't have the reputation to comment but Dan Allan's interpolate function raises an exception if asked to interpolate a time already defined in the ts index .
I am attempting to use python insert the parsed data into an sql database based on the columns and rows .
How to restore\unfold multiindex in pandas DataFrame
That is great but I got stuck to " unfold " the multiindex so that my dataframe looks like this : #CODE
This is much harder when the columns are joined in an multiindex .
The ` dataframe.select ` method can very powerfully execute selections on multiply values of a ` MultiIndex ` ( e.g. , ` subset = df.select ( lambda x : x [ 0 ] == ' S019 ' and s [ 2 ] in [ 3 , 5 ] and s [ 3 ] <= 5 )`
In cases where you have multiple branching statements it's best to create a function that accepts a row and then apply it along the ` axis=1 ` .
I now want to merge ( 1 ) into ( 2 ) in such a way as to match the first 2 columns of ( 1 ) with the corresponding columns of ( 2 ) , pulling in the other column in ( 2 ) appropriately [ in the actually data set of ( 1 ) , ' id ' , ' revision ' and ' colour ' are not necessarily consecutive columns , and there are other columns ] .
Nice :) how would I have done this using a merge or something ?
Note Unlike list.append method , which appends to the original list and returns nothing , append here does not modify df1 and returns its copy with df2 appended .
How can I append to an existing DataFrame without making a copy ?
Why not use concat ?
Upcoming pandas 0.13 version will allow to add rows through ` loc ` on non existing index data .
but I don't know how to translate this line ` d = diff ([ 0 c ( n )]); `
If you aren't then you can cast the ` bool ` ` dtype ` to an ` int64 ` or ` float64 ` ` dtype ` : #CODE
#ts [ np.random.rand ( len ( ts )) > 0.5 ] = np.nan
c = valid.astype ( int ) .cumsum() # because of Pandas bug with bool and cumsum for pd < 0.12
The ` concatenate ` function is more general than ` hstack ` , in that you can stack on a dynamically-chosen axis ; I think ` hstack ` is more readable when you're specifically thinking in terms of " columnwise " instead of " axis 1 " , but really , that's a minor quibble either way .
Hi , it seems this algo will fail if we have ` l = [ 1 , 0 , 1 , 0 ]` , I think ` if len ( current_drop ) > len ( current_rise ): ` need to be changed to ` if len ( current_drop ) > = len ( current_rise ) and len ( current_drop ) > 1 and current_drop [ 0 ] <> current_drop [ -1 ]: ` same for the other ` if `
You can use concat #CODE
I have tried using ` pd.cut ` or ` np.digitize ` with different combinations of ` map ` , ` apply ` , but without success .
just FYI , about to merge into pandas master this functionaility : #URL ( you can do this on 0.12 and before by : `` s.apply ( lambda x : x / np.timedelta64 ( 1 , ' D '))``
You can do this in the same line where you compute column values by putting .apply ( lambda x : x / np.timedelta64 ( 1 , ' D ')) at the end to apply the conversion at the column level .
I am trying to use scipy.opt to give me the weights back that conform to individually being bound to ( 0 , 1 ) , then also have the grouped class weights to be bound by the limits I showed above .
for purposes of illustration , suppose I were to do this with constraints I would have in psuedo code : c_ = ( { ' type ' : ' eq ' , ' fun ' : lambda W : sum ( W ) - 1} , { ' type ' : ' eq ' , ' fun ' : lambda W : .08 <= sum ( equity weights ) <= .51 } , { ' type ' : ' eq ' , ' fun ' : lambda W : .05 <= sum ( intl_equity weights ) <= .21 } , etc .....
To map each of the level names to an interval do this : #CODE
Map each of your level names to an interval and add that to your original dataset as a column named ` range ` .
that's why `` resample `` is prob the right solution , unless she wants dups
What I want is a shift to the closest after the next second .
You can also move the ` mask [ i ]` access outside of the nested loop , which is a tiny optimization but might yield some performance gains for very large frames .
Im using groupby method to group data in a dataframe and then use the outcome to modify the dataframe ( for example , changing a bool value in one of its columns ) I've tried the modification in two ways :
The way I saw this , the whole idea behinds data analysis with pandas is to group and apply .
running groupby for each and every apply operation ? that seems redundant ..
Sorry , didn't get it completely .. what do you mean by " if you return NONE n your function then it uses the original " you mean that the underlying dataframe is determined according to my apply function returning or not returning a value ?
also , this implies regrouping the dataframe for each apply operation , which can be expensive and redundent ( see my edits .. )
Groupby is a cheap operation ; the apply can be more expensive .
If your process is iterative then just groupby ( and apply after each iteration )
Do an outer join : #CODE
If you have different column names that you want to merge on , in this case let's say that `' ID '` in ` Donors ` should be `' fundraiser ID '` you can do #CODE
would the merge be able to work if the column names were actually different ?
Note : this particular plot is rather messy , since there are a lot of datapoints , it may make sense to aggregate some of this data beforehand .
It's a little wasteful , though , in the sense that your mask array would need to have as many elements in it as ` my_list ` .
If you are using pandas do the above , if numpy use a mask , otherwise use list / dict / generator comprehensions .
Surprised no one mentioned the ` drop ` method in pandas : #CODE
Problem description : I have two dataframes ( ' Train ' and ' Test ') with nearly identical columns ( ' Test ' has two variables that don't appear in Train , and Train has one variable that doesn't appear in Test ; however , to produce " Test " , I needed to do some processing in R , because I couldn't figure out how to do the equivalent of PLYR's full join in Pandas .
I would suggest using ` rename ` with a function , e.g. to replace apostrophes and spaces with ` .
is a full-join different from an outer join ` ( df1.join ( df2 , how= ' outer ')` ?
which should work on any ` pandas ` version that has the ` map ` method on ` Series ` objects and any numpy version that is supported by ` pandas ` .
You can use the ` resample ` ( DataFrame or Series ) method : #CODE
I was writing up a version using ` groupby ` and ` apply ` , but this works too .
I wonder if there is an easier way to do it , I'm sure I've used / seen a neater solution than arange ( len ( x )) .
`` df.groupby ( ' col3 ') [ ' col1 '] .apply ( lambda x : Series ( np.arange ( len ( x )) , x.index ))`` works , but yours is cleaner
Another solution would be to use ` pandas.melt ` to avoid unnecessary creation of a ` MultiIndex ` , though this isn't that expensive if your frame is small and with my solution you still have to create a temporary for the " molten " data .
The guts of ` melt ` suggest that both ` id_vars ` and ` value ` are copied since ` id_vars ` creation uses ` tile ` and ` value ` creation uses ` df.values.ravel ( ' F ')` which I believe makes a copy if your data are not in Fortran order .
I would like to translate this into a wide ` DataFrame ` but be able to extend the time frame arbitrarily ( say , to 365 days ) .
You can use your MultiIndexed DataFrame , create a new index with ` itertools.product ` combining all the users from your DataFrame and all the days you want , and then just replace the index filling the missing values with 0 .
I'm trying to copy a pivot table in excel in python using the pandas ` pivot_table ` function , I'm having trouble with the output .
The former ouputs a pivot table as expected but this is the output for the latter : #CODE
Have you tried saving the pivot table to csv ?
Is the simple example the DataFrame that you are trying to pivot on ?
You might create a ` DataFrame ` for what you want to match , and then ` merge ` it : #CODE
As Phillip Cloud suggested , you can make a list of frames that you append with each loop .
Freq : D , Length : 1826 , dtype : float64 ----- I have data as shown and I tried using ts.groupby ([ lambda x : x.month , lambda x : x.month ]) .mean() for monthly Climatology .
Freq : D , Length : 1826 , dtype : float64 ----- I have data as shown and I tried using
Freq : D , Length : 1826 , dtype : float64 ----- I have data as shown and I tried using ts.groupby ([ lambda x : x.month , lambda x : x.month ]) .mean() for monthly Climatology .
An alternative approach is to use ` first_valid_index ` and a ` transform ` : #CODE
To cut to the chase I've been having some problems even doing the most basic inserts of dummy data into an HDF5 file .
using ipdb in ipython I've got a call stack to the hanging line as pasted below .
In stepping through the code I've noticed that ` BlockManagerStorer.write() ` method , which is about half way up the call stack above , is looping through 2 sets of data blocks ( lines 2002 to 2006 ) .
Further the ` GenericStorer.write_array() ` method that is then called in the next stack down has ` value.dtype.type == ' numpy.float64 '` in the first pass but ` value.dtype.type == ' numpy.object '` in the second pass leading to a different branch on line 1785 of io / pytables.py being taken .
Perhaps you want interpolate instead of resample .
That said , it seems like resample could be improved .
as an aside , if you do have lots of data , `` table `` format prob better for you , as you can `` append `` , e.g. do chunked reads and writes ( and queries ); `` storer `` cannot
- read all files in one directory and concat them into one data frame with one numerated index .
And now resample #CODE
But I get Errors : If I drop the " parse_dates =[[ ' date ' , ' time ']]" in the " pd.read.csv ...
If I don't drop it , the Error : AttributeError : ' SeriesGroupBy ' object has no attribute ' filter ' occurs .
Update your installation of pandas if you can ; otherwise dig through my old answers for a workaround using `` transform `` .
I realize this usage pattern circumvents most of the performance benefits of the numpy / Pandas stack .
I've much to learn about the numpy / scipy / Pandas stack , but it seems that for truly arbitrary logic , you may sometimes need to just use a slow pure Python architecture like the one above .
You should apply your function along the axis=1 .
As for the second part of the question : row wise operations , even optimised ones , using pandas ` apply ` , are not the fastest solution there is .
In that case , if the ` apply ` is too slow for you , I would suggest " Cython-izing " your code .
@USER I saw that you rarely use apply along ` axis=1 ` .
How can I drop or disable the indices in a pandas Data Frame ?
I am learning the pandas from the book " python for data analysis " and I already know I can use the dataframe.drop to drop one column or one row .
This will scale with ` n * m ` where ` n == df1.shape [ 1 ]` and ` m == df2.shape [ 1 ]` so you should transpose the axes of either ( you said above that you have the option to do this ) such that the smallest axis is the columns axis .
Well , I thought about something similar to your solution , however I didn't pursue it further because I want to apply it to millions of rows , so I'm afraid this won't scale well .
Wonder if there should be a native way to do apply across multiple dataframes ...
A little bit cleaner version would be ` map ( correlate , i , j )` .
When you call ` concat ` , ultimately ` np.concatenate ` gets called .
You easily generate two arrays with different strides sharing the same memory like so : #CODE
` merge ` has a ` copy ` parameter , so when I do ` df_concat = df.merge ( df1 , left_index=True , right_index=True , copy=False )` If the indexes of both DataFrames are the same ( which in this case is true , the Series objects don't have to be changed , why is pandas copying the data when I explicitly instructed it not to with ` copy=False ` ?
If that's the case then I'm just asking what does the ` copy ` parameter of the ` merge ` function does ?
No Errors with Transpose Function and No Change In Result
I am having difficulty making use of a transpose functions made available through pandas and scipy :
I have the same question for concat on a ` Series ` .
so my updated question is how to perform the rename / case insensitive concat with a Series ?
You can replace ` str.lower ` with whatever function you want to use to rename your ` index ` .
Note that you cannot use ` reindex ` in general here , because it tries to find values with the renamed index and thus it will return ` nan ` values , unless your ` rename ` results in no changes to the original ` index ` .
You cannot use ` reindex ` here unless no changes are made to the index by the renaming operation .
and I want to apply a function which uses the index of the row : #CODE
I don't believe ` apply ` has access to the index ; it treats each row as a numpy object , not a Series , as you can see : #CODE
To get around this limitation , promote the indexes to columns , apply your function , and recreate a Series with the original index .
+1 For getting rid of the ` MultiIndex ` .
In my case ( a dataframe , with axis=1 ) , x.name() returns the value of the index when I apply a function lambda x : x ...
so when calling ` apply ` on ` DataFrame ` its index will be accessible through ` name ` of each series ?
You may find it faster to use ` where ` rather than ` apply ` here : #CODE
I recommend testing for speed ( as efficiency against apply will depend on the function ) .
Although , I find that ` apply ` s are more readable ...
I want to replace the column index of my DataFrame with this MultiIndex , with each of the elements of the MultiIndex replacing the column index element that has the matching name .
Seems like this should be a simple join or matching concatenation , but can't find the method ( and / or keywords ) to do it .
I'm trying to iteratively read / append multiple csv files - that amount to about 8GB in total - into a HDF5 store based on this solution and this solution for creating a unique index .
pls show an example of the transform u want to do
If you can lower this it will cut your storage size
You need to thoroughly understand how HDF storage works ( e.g. you can append rows , but not columns ; likely you need to create a results table where you append transformed / filtered forws ) .
There is filter I apply that filters out about 90% of the data .
Like you say , there are better ways to do this in pandas , the obvious one is to use a mask : #CODE
pandas HDFStore.append data_columns after append operation
In my append operations , I am passing a list of data columns via the data_columns parameter so that I can perform out-of-core filtering via HDFStore.select passing conditions to the where parameter .
I am guessing that each time an append is made to HDFStore with new data , the index in data columns is recalculated .
Unfortunately , when ` nan ` is added to a ` bool ` Series , it is automatically converted to a ` float ` Series .
Is there any way to avoid this conversion , or at least coerce it to ` object ` dtype , so as to preserve the distinction between ` bool ` s and ` float ` s ?
Do you mean that for every Series ` foo ` , I should add a bool Series ` foo_missing_values ` , which is True for every missing value in ` foo ` ?
As @USER said , the best way is going to be to append a ` Series ` with ` object ` ` dtype `
I have changed the list of lists to a column stack according to suggestions below .
To get the ` index ` values as a ` list ` / ` list ` of ` tuple ` s for ` Index ` / ` MultiIndex ` do : #CODE
Pandas : interpolate missing rows and plot multiple series in dataframe
The above executes even if some data is missing , and the plot lines visually interpolate the data linearly .
I guess I could write a function using ` try ` and then use pandas ` apply ` or ` map ` , but that seems like an inelegant solution .
In fact , you can apply this to the entire DataFrame : #CODE
If you really wanted to do this you could use a groupby apply : #CODE
Pivot table gets you a more useful DataFrame : #CODE
Please can someone suggest how to replace ` 1 ` by ` i ` when using a ` for ` loop .
and apply the function to each group , and then ` unstack ` from a Series to a DataFrame : #CODE
@USER Not sure I do fully either , I think I should probably put it up as an issue on github to discuss / fix , suspect it is untested behaviour , but it seemed that you can only apply aggfunc to columns which were going to be in your pivot_table ( IIRC the rest are not passed to the function so couldn't be used ) , also I was seeing SNDArrays which was confusing ...
You can then drop the ` NaN ` s and the titles , which I think is described in the other answer : #CODE
Update : tested timing of the three solutions proposed below and the rolling mean seems faster : #CODE
Using a TimeGrouper in this way is equivalent of resample ( and that's what resample actually does )
Eg here is a 5s resample with a 30s interval .
what if you don't want to apply a rolling mean but an arbitrary function in rolling overlapped windows ?
you can use any rolling function of rolling_apply with an arbitrary function
#URL you also might be able to just tshift ( ' 15s ') then resample at 30s freq . alternatively there is a loffset argument u can pass to TimeGrouper
while trying the simple ( non date-specific ) numpy function ` ptp ` gives me an integer answer : #CODE
Assuming you have indexed by datetime can use groupby apply : #CODE
The `` .filter `` method seems to take the same time regardless of the number of records which meet the selection criteria unlike the `` .xs `` concat method .
I wanted to try ` concat , join , merge , append ` but those doesn't seem to be appropriate .
Personally , I end up using ` concat ` more than any of the others .
kind of feels like there could be a way to insert via iloc in one go , but I can't think of it .
Using ` concat ` got ' ValueError : Shape of passed values is ( 18 , 52760 ) , indices imply ( 18 , 52727 )' .
It's strange because df1 len is 52716 and df2 52555 .
Anyway ` join ` and ` concat ` doesn't work properly in case there are duplicated index entries ...
If you have duplicate indexes in ` df ` and then you're resetting the index of ` df2 ` to match that of ` df ` , then you'll necessarily have duplicates in the ` join ` ed index .
Note that you can't just use ` n ` , because you may have a group where ` len ( group ) 2 ` , therefore
` len ( x [: n ]) !
It's also present in apply though ( with both our answers ) .
For ` object ` data I can map two columns into a third , ( ` object `) column of tuples #CODE
( see also Pandas : How to use apply function to multiple columns ) .
Another option for me is to use ` collections.Counter ` and ` map ( lambda x , y : ( x , y ) , df [ 0 ] , df [ 1 ])` as in this use case I don't need index anymore ; I was curious if I can manage within ` pandas ` .
This means you can invert np.percentile using map , and then apply a shift and a subtract to get the " percentage if array in interval " you're after .
C4 : this is just a rolling mean with a period of 4
C1 : rolling mean can get another parameter for the minimum period .
I think creating rolling mean columns and grabbing the relevant one via direction is the way to do this ...
Also , Andy Hayden send me to create rolling mean , which helped me much as a direction .
This creates a boolean mask for when the rows of the column in question ( e.g. ` c2pos `)
Python pandas timeseries resample giving unexpected results
I want to resample the data to only use the end of day balance , so the last value given for a day .
pass ` sort=False ` to a groupby ( you can't do this with resample though ) #CODE
To be able to resample your data Pandas first have to sort it .
a sort on duplicates is arbitrary ( e.g. no guarantees from merge or quick sort ) , IIRC
( but not too resample )
Dataframe Merge in Pandas
For some reason , I cannot get this merge to work correctly .
When trying to join the above to a table based on the ` rsparid ` columns to this Dataframe ...
Note , the " datalines " column can have thousands more rows than the rspars , thus the left join .
I think you're misunderstanding ` merge ` .
If there are no * values * in ` datalines.rsparid ` that are equal to the values in ` rspars.rsparid ` then you'll get ` NaN ` s because there's nothing to join on .
It feels like there ought to be a neat way using stack ( essentially separating into away / home , enabling a groupby apply ) .
Create a ` defualtdict ` ( with default value 0 ) where you will keep the current scores of the teams , and apply along the ` axis=1 ` a function that updates this dictionary and returns a tuple of results .
Then just concatenate your DataFrame and the resulting DataFrame from the ` apply ` function along ` axis=1 ` .
stack / groupby / apply is * significantly * faster than this method ( once there's more than 40 or so rows ) !
But since we are comparing the methods , your is faster , but it takes at least twice as much memory since a stack cannot be a view and I don't know the implementation details of groupby so I cannot tell how much that costs ( assuming that I did the same thing of appending columns and not concat ) .
I'm not sure about the memory ( I would've thought it would be much less than the defaultdicts ... ) , not sure if stack creates a copy ( concat doesn't so it might not wither ) .
Would be ** shocked ** if memory was larger for stack approach tbh , but happy to be proved wrong !
So it * may * be twice the arrays ( I'm not sure either ) , but that doesn't mean twice the memory used overall . groupby / concat / cumsum all are very cheap , written in cython with numpy so fast and low memory .
If the original data frame takes N memory , defaultdict version takes 6xN more memory , and the stack version takes 32xN more memory .
@USER It's not * so * weird , the stack is ( pretty much ) indep of number of teams , the defaultdict is completely dep .
If there were just one team , defaultdict has to store one number ( hehe in which case both are answers get confused ) , stack stores same as if there were lots .
And the global can be passed as an argument to apply , but that is not the an issue :)
First we have to ` stack ` in such a way we can do this independently ( of away / home ): #CODE
Finally , we just have to insert these into the frame with the correctly named columns : #CODE
@USER sorry you do this * before * the stack , I was going to add a line at the end saying then you stack , whoops !
@USER Ah thought so ... the numbers don't match up with your original input ( ? ) , I think you can do this easier with value_counts , will append to answer .
In the code below I slice the first 10 rows and the first ten columns of the table using ` ix ` : #CODE
First replace all the string values with ` None ` , to mark them as missing values and then convert it to float .
You could use ` pd.to_numeric ` method and apply it for the dataframe with arg ' coerce ' .
The data looks like the above stream , after I did some find and replace in VIM ( I know i can just script this in python ) .
You can define minimal classes for ` Aggregate ` , ` DateAggregateDimensionValue ` , and ` StringAggregateDimensionValue ` , then ` eval ` each line in turn : #CODE
Use these minimal classes to ` eval ` the input strings : #CODE
Of course , this comes with all the usual caveats about using ` eval ` , such as beware of any possible injection of malicious code .
( There is still a little ` eval ` ing in this , cleaning that up is left as an exercise for the reader . )
When grouping data you usually aggregate it in some way if you want to plot it .
First check the NaNs are in the same place ( using isnull ): #CODE
Then check the values which aren't NaN are equal ( using notnull ): #CODE
Then merge columns from these three dataframe in this order :
reason user might * think * they want to do this is to write their own rolling functions ( rather than use pandas rolling functions , or a ` rolling_apply `) ...
But he explicitly stated that he wants a ** rolling reshape ** :-/ That's why I asked him why .
Pandas has a wealth of rolling computational functions which mean you shouldn't be doing this .
If it's something else , consider writing it as a generic rolling apply .
Note : you can also set the freq to be a DateOffset ( e.g days , minutes , hours , etc . ) , which would be more difficult to do with a reshape , and this gives you lots of flexibility .
Pandas stack columns in dataframe an make a histogram
However I want to stack the columns and do the histogram .
What I want to do is stack the columns ( VSPD1-VSPD7 ) , and make them the index column .
What I want to do is stack the columns ( VSPD1-VSPD7 ) , and make them the index column .
You can simply use an ` apply ` to do this : #CODE
It doesn't feel like a good solution , however , because I have to bust out ` itertools ` , build another MultiIndex by hand and then reindex ( and my actual code is even messier , since the column lists aren't so simple to fetch ) .
I am pretty sure there has to be some ` ix ` or ` xs ` way of doing this , but everything I tried resulted in errors .
You can use either , ` loc ` or ` ix ` I'll show an example with ` loc ` : #CODE
I would just use ` loc ` for clarity .
Getting pandas new indexing " loc " and " iloc " to work with PyInstaller
If I replace ` iloc ` with the older ` ix ` , everything works as it should so I'm suspecting that PyInstaller can't find / use the latest version of pandas .
However , I'm afraid I've mangled the problem through a haphazard application of various pivot / melt / groupby experiments .
Firstly to get the change from previous year , do a shift on each of the groups : #CODE
However , I think you may be better off keeping it as a pivot : #CODE
The issue is that agg is now a series of DICT .
Case 1 : I need to transform the DataFrame to the independent and dependent variables of a regression .
The transformation involves generating a 2D array and a 1D array group by group and then stack together the rows of the arrays from each group .
Case 2 : I want to transform the DataFrame to two arrays of different rows and columns group by group .
I am trying to transform the data points in a DataFrame to a two dimensional array group by group and then transform the same data to a different one dimensional array .
I could have written two functions , one for each array , and apply them separately , but that would be very slow .
I can then either use np.column_stack to stack them together or write a loop to slice the two dim array one column at a time to a dict .
You don't need to use apply here , and unless you are using a cythonized function which can operate on a frame / series , it doesn't make any difference in perf .
But when I try to use the append command , #CODE
I get something undesired , the MultiIndex that I set up in df is now gone , replaced with a simple index of type tuples ( ' NecS3Hs05 , Slice 0 ' etc ... ) .
I want to get an array / tuple of shape / len 6
Is the number of slices always the same from patient to patient ? in a MultiIndex that is somewhat required .
Your problem can be solved , I believe , if you drop the MultiIndex business .
As long as your new dataframe ( df2 ) defines the same index you can now join on that index quite simply : #CODE
You can easily move your ` MultiIndex ` to columns using some combination of ` stack ` / ` unstack ` / ` reset_index ` / ` melt ` .
It's quite possible that the same concepts and issues apply to the latter types I just don't know them that well so can't comment .
Obviously you need to transpose to read between these two constructs but this is a trivial transformation .
I've not really looked into it ( so could be way off base ) but the issue with the latter approach is that if I'm moving 100's of lines into C why am I bothering with Python in the first place , it doesn't feel like a simple lift and shift ?
Pandas aggregate values by years but keep original TimeSeries index
I'd like to sum aggregate the values by year but then keep the original TimeSeries index .
Use reindex instead : #CODE
The problem with ` resample ` is that when you first resampled ` a ` , the last entry become april 2053 .
( this comment suggests that it might be possible " using custom DateOffset classes with appropriately crafted onOffset , rollforward , rollback , and apply methods ")
It looks like a central assumption in ` PeriodIndex ` is that every Period has the same ` freq ` .
For that , i'm developping this code and i tried to apply what i found in this link #URL
NOTE : You don't necessarily have to ` concat ` the ` list ` of ` DataFrame ` s , but often that's a useful next step so that you don't have to keep operating on a ` list ` of ` DataFrame ` s .
If I replace the strings ' L ' and ' S ' with integers like -1 and 1 it works OK , so that's a work around .
I thought it might be due to Pandas somehow sharing metadata about Index objects and perhaps being allowed to truncate when inserting data if the data comes from an object that has the same index .
it's not misleading at all ; in fact it's the primary reason for having an index in general ; all operations align .
if you don't wnt it then use numpy directly which doesn't align
The purpose of pandas is to align by the index .
Then merge / combine at a later step .
In my case , the DataFrames do have a relationship , and the " proper " way to do this is to first calculate the column that I want in the second DataFrame ( the argument of ` np.where `) , then perform a ` merge ` or ` join ` based on other ( non-index ) columns .
But given that ` np.where ` would be faster than a merge , it would be nice if I could trust that was just inserting values positionally , not by alignment , unless the index objects are actually the same object .
Your problem is a bit ' unique ' in that sometimes you don't want to align .
Try to drop them from the call to ols .
drop the formula and data keywords did work .
What I want to do is to be able to create a boxplot with this data grouped according to different catagories in a specific line of the multiinex .
but it gives me a boxplot ( flat line ) for each point in the group rather than for the grouped set .
I don't have any problems grouping as I can aggregate the groups any which way I want , but I'm not sure what I'm doing wrong with this boxplot .
Basically groupby output needed to be transposed so that the boxplot showed the right grouping :
` matplotlib ` also offers the function ` boxplot ` to do vertical box plots .
matplotlib's ` boxplot ( ..., vert=False )` makes horizontal box plots .
First , ` ix ` will first try integers as labels then as indices , so it is immediate that 59 and 62 are the same .
Here are the docs on ` ix ` .
Notice that the sorted row indices are integers and they don't map to their locations .
Hi @USER , yes you are correct in that passing a masked array to a dataframe converts the mask to NaNs .
if you use a flat file , a dict structure is easy to map to a CSV file
reshape data frame in pandas with pivot table
With pivot table you can get a matrix showing which ` baz ` corresponds to which ` qux ` : #CODE
You need to decide how to compute time / bin and than translate to pandas .
I get the warnings below when building : ( for readability I use " PYINSTALLERDIR " to replace the full path , which is " C :\ Users\name\Downloads\ pyinstaller-pyinstaller-v2.0-544-g337ae69 \ pyinstaller-pyinstaller-337ae69 \ " .
But when I try to execute an INSERT query , MySQLdb says there's a syntax error .
The values of these rows converted to a string to append to the end of an INSERT statement ( note that all MySQL columns will be varchar , so all values are wrapped in single-quotes ) #CODE
First , the bug : I incorrectly used ` str.replace() ` to replace the single literal backslash with an escaped double : #CODE
Why does pandas groupby cut give different form of output with single record input ?
a complete hack is to check for ` len ( s ) == 1 ` and pass in ` pd.concat ([ s , s ])` and then divide the result by 2 .
You can do this with a combination of ` groupby ` , ` replace ` and a ` dict ` #CODE
The ` apply ` method applies a function to each of the aforementioned subsets and concatenates the result ( if needed ) .
In this case that's the " current " fruit you're looking at when you apply your function .
how to resample without skipping nan values in pandas
I am trying get the 10 days aggregate of my data which has NaN values .
When I apply the below code pandas is considering NaN as Zero and returing the sum of remaining days .
I can give you a diff that makes the unit-tests pass and works in this case and also in the case of the bug report but without duplicated indexes .
But the diff is super , extra ugly and I would never commit it :)
@USER It sounds like you're talking about lazy evaluation , which is a much broader topic than ` eval ` / ` query ` .
I would like to aggregate the dataframe along the rows with an arbitrary function that combines the columns , for example the norm : ` ( X^2 + Y^2 + Y^2 )` .
You are looking for apply .
Pandas resample dataframe
You can use ` reindex ` and choose one fill method #CODE
Then ` resample ` by second ( this is the mean value by default but can be changed to sum etc - e.g. add ` how= ' sum '` as parameter ): #CODE
The trouble is I can't make it work , if I apply your exact code ( plus my def of dates ) I get a Type Error : ` TypeError : ( " unsupported operand type ( s ) for /: ' buffer ' and ' int '" , u'occurred at index 2012-04-01 00:00 : 00 ')`
And you / are working on a ` DataFrame ` object inside of ` apply ` indeed .
Actually inside ` apply ` we seem to be working with a ` Series ` object , the name of which is the index of the ` DataFrame ` we're iterating through .
It seems ` apply ` collapses a " dimension " of the initiating object so this is why working on a ` TimeSeries ` directly only left me with a value object and no way to access the value of the index .
When I apply your solution I get ` s ` where ` s.index.month % 3 == 1 ` and ` s / 4 ` where ` s.index.month % 3 !
ok I think is easy to enable though , let me think about it . you can also do as a chained loc ( same idea )
Can you edit your question by adding the output of : ` print() ( type ( labels ))` , ` print ( len ( labels ))` , ` print ( list ( labels ) [ 0 ])` .
Just use the ` apply ` function along the ` axis=1 ` and pass the ` pattern ` parameter as an additional argument to the function .
I knew you'll gonna ` melt ` my mind again :)
Once I started to understand ` melt ` , I used it a lot more !
Like regexp ;) I agree , I just always give the simplest , predictable in speed solution to the OP , and live the exotic ones to you :) ` apply ` should always be ` O ( N )` if I'm not wrong .
I don't even know how to calculate how ` melt ` with ` groupby ` behaves ?
However , suppose we could replace the ` True ` values with their column index , we could just use normal ` max ` .
We need to make ` 0 ` come out as ` NaN ` and the rest of the columns to shift over .
I'm using Pandas to compare the outputs of two files loaded into two data frames ( uat , prod ):
You can pass a dictionary of replacement values into the Series replace method : #CODE
Is it possible to reshape the pivot table in a data frame suitable for analysis in R ?, something of the type : #CODE
I knew you'd eventually warm up to ` melt ` .
It's just that I hesitate to use it on SO since I don't want to melt the OP's brain :P But in this case it was really obvious ...
I agree with you , but ` melt ` is taken from R ( specifically , the ` reshape2 ` package by Hadley Wickham ) and it has been in ` pandas ` for awhile I believe , so we decided to stick with it .
` pivot ` in R is called ` dcast ` ( ` data.frame ` ` cast `) .
@USER I think we should * add * unpivot as a melt alias ( or vice versa ) .
It seems strange to use melt but not ( d ) cast terminology from R .
One way is to use ` stack ` : #CODE
Just throwing this out there , but if this is faster than ` melt ` it might be worth implementing ` melt ` in terms of this .
I am trying to merge two files using pandas ' ` pd.merge ` based on a common column named " JN " .
You cannot ` merge ` two strings .
You have to actually read in the ` DataFrame ` s from the files named ` JJ ` and ` WW ` , then perform the ` merge ` .
Pandas aggregate count distinct
I'm thinking I just need to provide a function that returns the count of distinct items of a Series object to the aggregate function , but I don't have a lot of exposure to the various libraries at my disposal .
Just apply the Timestamp ` time ` method to items in the date column : #CODE
Using the method Andy created on Index is faster than apply #CODE
And then apply the function along the ` axis=1 ` .
Pandas : join with outer product
How to join / multiply the DataFrames ` areas ` and ` demand ` together in a decent way ?
Now ` apply ` needs to return a ` Series ` , not a ` DataFrame ` .
One way to turn a ` DataFrame ` into a ` Series ` is to use ` stack ` .
` stack ` this DataFrame .
This can be done with ` unstack ` : #CODE
` del ` + ` pivot ` turns out to be faster than ` pivot_table ` in this case .
Maybe the reason ` pivot ` exists is because it is faster than ` pivot_table ` for those cases where it is applicable ( such as when you don't need aggregation ) .
` apply ` is now among my top 5 functions to always remember .
If you know that you want a few columns in a specific order , just do a set difference between all columns and the pre-ordered columns , then call ` reindex ` : #CODE
I am doing some analytics using Solr and specifically using the faceting and pivot functionality for a large set of log files .
To get the visits as a percentage ( to avoid double-counts ) I'd do a transform first : #CODE
Does ` H9 ` change with each iteration of the loop , so that you get ` len ( subsl )` different plots ?
ax2.set_title ( ' Percent change All Subdivisions ( rolling 4q avg )')
You can simply write them out with a boolean mask : #CODE
use `` loc `` don't chain !!!!
A different approach using ` map ` : #CODE
You also don't need to ` zip ` since you're just unpacking it in the argument list of the ` lambda ` : ` map ( lambda x , y , z : value_map.get (( x , y ) , z ) , df.l , df.n , df.v )` .
How to apply the logistic regression on the first part of my table ( the first sample ) , then apply predict on the second part ?
One solution could be to use get dummies ( which should be more efficient that apply ): #CODE
You could use an apply with a couple of locs : #CODE
But how to merge the result ?
The resample via ohlc ( here I've resampled by hour ): #CODE
You can apply to_json to any DataFrame : #CODE
@USER yup , just do the same but replace PRICE with VOLUME :)
@USER , do you mean i need to first resample on PRICE then resample on VOLUME ?
@USER , could you help to check how could i merge the VOLUME and OHLC DataFrames ?
You can just concat them , but I'm not sure this will make sense in this case ( since one is a resample ) , since it's quite different to this you should ask as a new question .
What is the most idiomatic way to normalize each row of a pandas DataFrame ?
I would suggest to use Scikit preprocessing libraries and transpose your dataframe as required : #CODE
You may need to resample first to fill in any missing values .
And now drop all the filled-in times we added , taking only values from the original Time index .
I also ran it without the resample operation in there and the same thing happened .
I think part of the trouble for me is that I don't really understand the resample operation ( and can't find any tutorials or explanations online ) so I don't know how to tweak the function to help me .
Can you post a ` Series ` object that reproduces the stack overflow ?
Your input file is big endian . see here to transform it : #URL
you can what I do in the apply , its operating on the numpy arrays directly .
I wish apply this algorithm in df [ ' X3 '] but I had 0 in all the values ?? of X3 columns .
You have to rewrite the entire file , you can't just append to each line .
I understand your code but when I try to apply it to my code it does not work !!
I tried to apply what you made but i have an error !!
A simple implementation would maybe look at the total number of data frame subsets upon which the ` apply ` function is working and report progress as the completed fraction of those subsets .
sometimes you can do operations on the whole frame before you apply to eliminate bottlenecks
You could even have it update with the progress #URL e.g. count / len as percentage .
If your using it for the apply it's not so important anyway .
Note : the apply progress percentage updates inline .
Thanks for expanding on Jeff's post .
@USER DataFrame's apply is row-wise so I don't * think * dropna can be written in terms of apply ( needed to use this answer ) .
To directly answer the original question , replace : #CODE
I came across what looks like the perfect solution on another SO answer ( #URL ) but when trying to apply to this series , I'm getting an exception : #CODE
I really want to be able to apply the changes within the dataframe to apply and reapply groupby conditions and perform the plots efficiently - and would love to learn more about how the .apply() method works .
As far as the ` apply ` method goes , it does slightly different things for different objects .
For example , ` DataFrame.apply() ` will apply the passed in callable across the columns by default , but you can pass ` axis=1 ` to apply it along the rows .
You can reproduce it with dupes in the findall ( see my answer ) , I think Jeff's apply solution should work fine with dupes in the DatetimeIndex .
Your explanation of how apply works as well is really useful , so even though it's not a solution to the original question it's really appreciated .
First convert your ` q_date ` column into a ` datetime64 [ ns ]` Series , then ` map ` over the column with a custom format string #CODE
Using pandas , is it possible to compute a single cross-tabulation ( or pivot table ) containing values calculated from two different functions ?
BTW , is it possible to make " len " and " mean " columns of their own when reshaping with melt ?
I tried : pd.melt ( table.reset_index() , id_vars =[ ' c1 ' , ' c2 ']) , but I get a column named " NaN " with " len " and " mean " as elements .
Do you want one column with " mean " and one column with " len " besided the columns " c1 " , " c2 " , " c3 " , " c4 " , as you would get with ` df.groupby ([ " c1 " , " c2 " , " c3 " , " c4 "]) .aggregate ([ len , np.mean ]) .reset_index() ` ?
In that case you can do it directly instead of via crosstab and melt .
Pandas reset index on series to remove multiindex
There are many ways to drop columns :
stack columns in pandas DataFrame CSV
I have a dataframe that is the result of a pivot table that has columns : #CODE
Python Pandas - Merge DataFrame , preserve index
What I'm trying to do is basically to merge two ` DataFrame ` objects in Pandas while preserving the index ( ` DateTimeIndex `) .
Neither the ` merge ` or ` ordered_merge ` support this .
Any sort of ` merge ` operation - so far as I can tell , simply doesn't support preserving indexes on many-to-many data
Finding indexes unique to ` aapl_new ` and ` append ` ing those to ` aapl_old ` - no idea how to find which rows in ` aapl_new ` don't appear in ` aapl_old `
Can you explain what you mean by " merge " ?
By " merge " I do mean to [ merge ] ( #URL ) like in the documentation .
The problem was when I would do a merge operation , pandas would destroy the index , meaning that I lost the DateTime the data came from .
First , to merge the two dataframes you can simply concatenate the two using ` concat ([ aapl_old , aapl_new ])` ( this will preserve the index ) .
1 ) You can only concat those from ` aapl_new ` that are unique ( do not appear in aapl_old ) .
And this you can then concat with aapl_old : #CODE
2 ) You could also just concat the dataframes , and then remove the duplicate rows : #CODE
Pandas truncate DataFrame after a column condition is met
If you try to return multiple values from the function that is passed to ` apply ` , and the DataFrame you call the ` apply ` on has the same number of item along the axis ( in this case columns ) as the number of values you returned , Pandas will create a DataFrame from the return values with the same labels as the original DataFrame .
It's easy to pivot to get this without the empty line of ` b 0 0 0 0 ` ; is that important ?
To fix the index , you could just add the X as this index , you could first apply set_index : #CODE
If you wanted to do this pivot ( you can also use pivot_table ): #CODE
Yeah sorry about not being clear - the pivot tables where all I was looking for ... forgot about those .
Now suppose I want to take rows A and B , and replace them both by a single row that is their sum ; and , moreover , that I want to assign a given index ( say ' sum ') to that replacement row ( note the order of indices doesn't matter ) .
Another option is to use concat : #CODE
and we want to concat with all the remaining rows : #CODE
If you want to preserve some of the original timestamps ( but you have to choose which if you are binning them ) , you can specify a function to apply on the individual columns .
If you just want to resample on 5 mins and add the counts regardless of the keys , you can simply do : #CODE
I've read that the time series resample can work with repeated time stamps - I'd really just like to aggregate so that the counts ( data1 ) are added when creating a 5 min time bin .
If you " aggregate " for different timestamps , you cannot just include it ( because for each aggregated count , you have multiple and different timestamps ) .
This would allow for example , to create 2 min , 10 min bins using resample - and look at the data over different bin sizes ( time scales )
essentially add ( aggregate ) the data1 ( counts ) for each unique key , into 5 min bins .
I want to retain the possibility of selecting the previous content of the matrix by a single column name after the merge .
I want to append a dataframe I have created into an HDF database .
Alternatively , pass ` as_index=False ` to ` groupby ` and use ` loc ` in the ` lambda ` , which is a bit more meaningful IMHO since you're indexing by name rather than integer location : #CODE
` as_index ` and ` apply ` will only work on pandas git master .
Not sure what the problem is , I am fairly new to python and I cannot currently find any solutions to this problem on stack overflow .
How to apply a complex formula using Pandas in Python ?
I need to apply a specially designed moving average filter on a traffic dataset ( NGSim ) .
Rolling average ?
If you don't like the way that looks , then you can play around with the other methods , like a rolling mean .
How to reindex_axis Pandas Panel to MultiIndex
I am unable to reindex it to a multi index along level 2 .
A workaround is not to reindex after , but to use the MultiIndex when creating dfclose : #CODE
my bad , the workaround still works in 0.12 , but was bug in reindex ?
In some cases you'll want to concat this to the DataFrame in place of the dict row : #CODE
Also needed to merge the statuses in , adding a suffix so name collisions got decent names .
The ` AttributeError : ' Series ' object has no attribute ' _data '` is caused because ` weather [ selector2 ] [ ' Dry_bulb_temperature ']` is a Series while concat expects a DataFrame which can not be concat-ed with a Dataframe , i.e. concat needs two similar types ( previous comment is wrong as pointed out by @USER below ) .
` concat ` expects objects the same * type* , not only ` DataFrame ` s .
You can ` concat ` two ` Series ` , for example .
I'm not sure what is happening in your concat .
Assuming the two dataframes have the same index , I would concat the whole thing , then do your filter : #CODE
The problem is that ` weather [ ' Dry_bulb_temperature ]` is a Series object , while concat expects a dataframe ...
It looks like you want to do a join ( which can merge a DataFrame and a Series on their index ): #CODE
See more in the merging , join and concating section of the docs .
You can create the Series / column using loc , avoiding chaining : #CODE
You could use stack here and then use a boolean mask ( for those values over 50 ): #CODE
In addition , with a ` DataFrame ` you have a whole world of fast operations at your fingertips which is not the case with ` MultiIndex ` : #CODE
You can produce the desired DataFrame using ` groupby ` and ` agg ` : #CODE
I was working with groupby and aggregate , but didn't know about the ' first ' option .
how do I insert a column at a specific column index in pandas ?
Can I insert a column at a specific column index in pandas ?
You could try to extract columns as list , massage this as you want , and reindex your dataframe : #CODE
using idx = 0 will insert at the beginning #CODE
Lastly , if you want to put your columns in a different order , use ` reindex ` : #CODE
I'm pretty sure the ' df [: 15 ]' in the map is the problem as I don't know how to pass a slice of the current future records to the function
rolling functions compute from that point on , so we timeshift ( which just changes the index )
so that the values align ( with the start point , rather than the end point ) #CODE
Hi low diff is just #CODE
I get an error when I try to do this : " ValueError : Freq was not given and was not set in the index " .
` df [ ' max_bidhigh '] = pd.rolling_max ( df [ ' bidhigh '] .tshift ( -14 ) , 15 )` gives an error of ` ValueError : Freq was not given and was not set in the index ` - I can't see how to set ' Freq ' in the documentation .
using ` shift ` rather than ` tshift ` seems to do what I need .
yep ... should have pointed u there in the first place . normally the rolling functions are ' backward ' looking in that the index is related the the future value ( and not the first value ) , that is why you need to shift
Then do the experiment and append the DataFrame with a new column of results for each row .
I want to look at the data by hour , to see how tickets are flowing through the Help Desk by shift - so an intermediate step could be something like this : #CODE
Now , iterate through each of the rows and concat : #CODE
But it is DatetimeIndex and I need to do some extra format transform , so how to do that in a neat way ?
or using ` map ` : #CODE
The only way I know of doing it is to reset_index on a multiindex and then set index again .
pandas DataFrame : replace nan values with average of columns
How can I replace the ` nan ` s with averages of columns where they are ?
This question is very similar to this one : numpy array : replace nan values with average of columns but , unfortunately , the solution given there doesn't work for a pandas DataFrame .
You'll also have to do a little hack here to allow us to use the eq DataFrame method , unfortunately this does a sort ( or you could keep track of the original unique index ) .
Select the columns you are interested in ( alternatively you could just drop the Service_Date column ): #CODE
Then you can use the eq DataFrame method : #CODE
pandas stack dataframe imported from excel
But when I tried to add more variables it was not able to drop thesuffixes anymore .
I have a function in Python , defined in an API from my broker , " getCalendar " which recieves a list of news announcements and expected impact on market . how can I transform this list which arrives as a JSON object to a pandas dataframe , so I can analyze it ?
how to replace values in a data frame that is already constructed ( pandas ) ?
Then , after reading the data frame ` df ` , I could tell pandas to insert the available elements in ` val ` in the appropriate rows of ` df_empty ` ( ` 0.277162 ` in row corresponding to ` a x ` , ` 0.324269 ` in row corresponding ` a y ` and ` 0.435033 ` in row corresponding to ` b y `) .
If so , you should make your data structure a Series with a MultiIndex and then use that to insert the values you need .
How do * you * know which row you want to replace ?
If the user provides an item in row ` x ` , you can use ` df.ix [ x , ' val ']` to insert a value at that position in the table .
@USER Are you looking to * append * rather than replace ?
@USER Hayden : How would you do it with append ?
If , when " df " is read , it happens to contain " a x G2 0.277162 " , then this should replace " a x G2 0 " in the empty data frame I'd created .
Would append have this effect ?
You could do a left merge between df_empty and df , then fillna with 0 : #CODE
As @USER Cloud points out , ` loc ` is better for this job .
I want to group by the year , then for each row , determine if the row is above the group's median by 20% or below the group's median by 20% .
A numpy ` ndarray ` is not a valid argument to ` bool ` unless its ` size ` is 0 or 1 .
actually one question : the answer above is based on one condition ( is the row-value greater than the median ? ) . how would the answer be modified to check if its above 1.2 *median OR below .8 *median ?
I'm trying to apply a case-statement-like function element wise that works on groups : if element above 1.2 * group_median , then assign ' h ' , if it's below .8 *group_median , then assign ' l ' , the third logical possibility ( it's within .2 of the group median ) is ignored or assigned ' n ' .
I realized that I was using wrong bracket ` ix = numpy.random.choice ( 10 , size=5 , replace=False , p=None )` and ` df = train1.loc ( ix )` :)
I have a rather basic question for pandas , but I've tried merge and join to no success
Many DataFrame methods rely on the indexes to automatically align data .
Now , ` join ` requires no extra parameters ; it gives you exactly what ( I think ) you want .
A potential solution is to use join , but that does not allow me to name it nor does it allow it be filled with a scalar values : #CODE
How to offset / shift a Pandas Dataframe into another year ?
I would like to offset / shift the data into the previous year so that ` 2014-01-01 00:15 : 00 1269.6 ` will be converted into ` 2013-01-01 00:15 : 00 1269.6 ` .
to shift the dataframe 15 mins into the past but would not like to offset / shift by the number of 15min intervals as this might cause errors in leap years and with clock changes .
This works but I agree with you , it would be great if one could shift / offset it by a year .
replace ` ( f1 , f2 , f3 , val )` with whatever you want to use to print the table .
I have a pandas.DataFrame that won't pivot the way I expect .
While ` pivot_table ` properly arranges everything , the fact that it uses aggregate functions to get there is off-putting .
Is there a way to get ` pivot ` to give essentially the same output as the ` pivot_table ` command did ( but hopefully flatter and neater ) ?
If you don't want the aggregation of ` pivot_table ` , you indeed need the ` pivot ` function .
However , ` pivot ` does not work with providing multiple index-columns ( actually I don't know why ) .
But , there is a similar function to pivot , ` unstack ` , which works the same but based on the ( multi ) index instead of columns .
and then unstack on the last level ( default ) , so on ' vehicle ' ( which become the column labels ): #CODE
Yeah , that's why you need to first drop the level , and only then reset the index .
In 0.13 , where can be an indexer ( e.g. an array of locations , so this is much easier , see ( Selecting using a where mask ) [ #URL then 2nd example down .
Pandas stack / groupby to make a new dataframe
I've been experimenting with Panels and pivot tables , but they don't seem to be quite right , and they often overflow the memory .
So , I've tried this with my table , and it quickly blows up with a MemoryError on the actual pivot .
I may have found a way to do this pivot table with much fewer rows in the original table .
You can do this manually using loc and mean : #CODE
By default this looks at 3 periods back , so we have to shift it two up : #CODE
and transpose to get of in the correct form : #CODE
what about the resample ( q ) function ...
You can join these columns to the original dataframe with #CODE
Using resample .
This is ultimately the most flexible as you can now resample by different frequencies .
good eval test case
Getting percentage of another Column in Pandas Pivot Table
To expand a little on what's going on here , during the apply the function is called on each group ( in this case there are two , one for B= ' c ' and one for B= ' d ') , here's the c group : #CODE
Apply then outputs this together with the B= ' d ' group to get the desired result .
You could groupby person and then apply a shift to the values : #CODE
shift() is shorthand for shift ( 1 ) , to go back two periods use shift ( 2 ) :)
pandas data frame transform INT64 columns to boolean
Is there a way to replace these values with boolean values ?
` location ` : an experimental condition , can be `" same "` , `" diff "` or ` None `
Full stack trace #CODE
You can see the previous vehicle class using shift : #CODE
In order to replace the value of END_DATE for those rows that have a S or P in STATUS
Or even simpler , try : ` print len ( ' A\B\C ')` .
Compare to ` print len ( ' C :\ b\a ')`
You can apply ` value_counts ` to the Series groupby : #CODE
Better to construct the frame column by column then transpose at the end .
Another option , if memory was an issue , is to do it chunkbychunk ( i.e. read it in several pieces and concat the result ) .
for i in xrange ( len ( df )):
And apply the code ,
Also , perhaps use a df.index.map , rather than apply .
As you can see from the example , I don't think the ` replace ` or ` where ` methods would work as the values of replacement are index location dependent and not input value dependent .
A faster way to copy a column's non-NaN values to another column is to use loc and boolean mask : #CODE
` where ` seems to be faster than ` loc ` : ` df.outts.where ( - pd.notnull ( df.orts ) , df.orts , inplace=True )`
@USER and even faster with isnull rather than -notnull , awesome !
And if you wan't a list of the prices per day then just do a groupby per day and return the list of all the prices from every group using the ` apply ` on the grouped object : #CODE
The problem with your data set is that you have some days without any data , so the function passed in as the resampler should handle those cases : #CODE
You need the ` argmax ` , which returns the position of the maximal value in the index .
Also try ` ix ` instead of ` iloc ` .
You can do it with map : #CODE
To compare today's values to yesterday's last value , take the last column and use shift to look " up " one row .
Pandas Combining 2 Data Frames ( join on a common column )
I would like to join these two DataFrames to make them into a single dataframe using the DataFrame.join() command in pandas .
I am very new to pandas and have no clue what I am doing wrong as far as executing the join statement is concerned .
Or , you could get around this by simply deleting the offending columns before you join .
it also says ' business_id ' column overlaps , isn't it supposed to overlap since that's the column I'm creating the join on ?
Hey @USER I tried the join method but all I get is 4503 entries in the restaurant_ids_dataframe and zero entries in the columns belonging to the restaurant_review_frame .
I have performed a left join as you suggested using your above statement but it doesn't seem to give me any items from the restaurant_review_frame for some reason .
You can use merge to combine two dataframes into one : #CODE
where on specifies field name that exists in both dataframes to join on , and how
defines whether its inner / outer / left / right join , with outer using ' union of keys from both frames ( SQL : full outer join ) .
As @USER mentioned for the join method , you can modify the suffixes for merge by passing it as a kwarg .
The only change I had to make was I did an inner merge instead of outer .
how=inner|outer|left|right , how to merge , intersection of keys left and right|union ( ALL ) keys left and right|left keys only|right keys only|
In case anyone needs to try and merge two dataframes together on the index ( instead of another column ) , this also works !
I had to use merge because append would fill NaNs in unnecessarily .
Trying to append this to a new datastore .
The datastore does not exist so I use the following to create and append the data ; #CODE
Second , I would like to ' melt ' the data into some new dataframe that counts the number of events for a given PictureID in a given period .
So that I can then stack ( ? ) this new data frame into something that provides period counts for all unique PictureIDs : #CODE
After this you can merge this with your DataFrame on index .
Note : A solution which only calculated the mean once might be preferable ... one option is to stack , groupby , mean , and unstack , but atm this is a little fiddly .
how to aggregate pandas Series values based on predicate statement ?
In R , it is easy to aggregate values and apply a function ( in this case , ` sum `) #CODE
You can use groupby , which can apply a function to the index values ( in this case looking at the first element ): #CODE
Pandas - possible to aggregate two columns using two different aggregations ?
I want to group by the columns : date , textA and textB - but want to apply " sum " to numberA , but " min " to numberB .
... but I cannot see how to then apply two different aggregate functions , to two different columns ?
The ` agg ` method can accept a dict , in which case the keys indicate the column to which the function is applied : #CODE
Merge a list of DataFrame's on a column ?
The problem is that , when you did the first merge , you changed the names of the columns ( adding suffixes ) and there won't be a name collision on the second merge , so the suffixes in the second merge will never be used .
The solution is to rename the columns manually after the merge .
@USER Oh I think I understand now , you want to chain the merge operations .
I have created a Gist containing a function to join a " list " of dataframes .
I would like to perform an aggregation on some data , but once done , link the aggregate back to the rows which made up the aggregate .
Say I want to display the aggregates , I can iterate through the above aggregate data .
However , I also want to be able to determine / display the rows which made up any given aggregate .
I need to be able to efficiently determine that the red car aggregate , is comprised of row 0 and row 3 in the original data set
Ultimately I'd like to persist this relationship to a file - but I'm unsure if this could be accomplished in one combined dataset , or if I'd need two separate data sets - with a way of linking any given aggregate back to the rows in the original data
Also you should link to the previous question , it's unclear exactly what's different here ( are you looking to transform rather than agg ) ?
You can apply a ` join ` operation between your original dataframe and the resulting aggregated data : #CODE
My intuition was to additionally group by the ' dstIP ' column ( which only has 5 unique values ) , but I get errors when I try to aggregate on srcIP .
Now that we have the grouping , we can apply the aggregator : #CODE
Oddly , I now get a ` TypeError : unhashable type ` when trying to aggregate .
I need to drop all instances of an ` appkey ` which occurs more than once .
Here's one way , using a transform with count : #CODE
Groupby the AppKey column and applying a transform count , means that each occurrence of AppKey is counted and the count is assigned to those rows where it appears : #CODE
You can then use this as a boolean mask to the original DataFrame ( leaving only those rows whose AppKey occurs precisely once ): #CODE
It does exactly what @USER ' s solution does using ` transform ` , but a little more succinctly and somewhat faster .
Just when I think " ah ha I can use transform " ...
Hahaha , I was about to congratulate you for joining Team Transform when I realized this one belonged to Team Filter .
Well , @USER , if upgrading is a hassle the transform trick is an option .
Or I can transpose and convert to a dictionary and Parse that Way .
If you want to select the 0 column in this way you should use loc : #CODE
To fix your solution you can append every iteration to a list : #CODE
I'm guessing they are a way to map a db onto objects .
If you want to override the format as desired by the OP , you can replace ( " monkey patch ") the ` IntArrayFormatter ` to print integer values with thousands separated by comma as follows : #CODE
The dataframe returns unsorted after I reindex .
My ultimate goal is to have a sorted dataframe with index numbers from 1 --> len ( dfm ) so if there's a better way to do that , I wouldn't mind ,
I think you're misunderstanding what ` reindex ` does .
Remember , if you want ` len ( df )` to be in the index you have to add 1 to the endpoint since Python doesn't include endpoints when constructing ranges .
Like most answers about ` reindex ` , this again shows how terrible a name ` reindex ` is for what this method does .
` reindex ` implies you are setting a new index for existing values .
The Pandas `` reindex `` function does something like Excel's ` VLOOKUP ` .
another option is to use ` reindex ` : #CODE
On a ( different ) dataframe I happened to have handy , it appears ` reindex ` might be the faster option : #CODE
And you can do an apply over the major axis : #CODE
I will have to look into how apply more carefully , I don't understand why you would have the function return 1 .
For example as a test I wanted to apply ` np.reshape ( x , ( 2 , 2 ))` on a panel of 4 dfs and it fails .
I guess I need to do all my work within the apply , and flatten it back out .
Use ` loc ` ( and avoid chaining ): #CODE
Assume we want to transform the following dataframe #CODE
There's an overview here , but basically you want to look at the ` asfreq ` and ` resample ` methods .
` fromtimestamp ` localizes the time if no tz is provided , you can use ` utcfromtimestamp ` to get what pandas does ( which does not localize here ) #CODE
If you want to localize the tz , you can do this #CODE
and what tz are these epoch seconds coming from ?
No tz is provided .
If you replace " do some plot " with a plt.plot ( data ) command , it will plot to the current axes .
This will work even if ` axs.size len ( subsl )` since ` StopIteration ` is raised when the shortest iterable runs out .
More generally , for ` axs.size - len ( subsl )` extra plots at the end of the grid do : #CODE
` subsl ` has ` len ( subsl )` elements and the same reasoning applies about the index of the last element .
I need to resample the data for daily averages .
Why not leave ` Date ` and ` time ` as separate columns , and then just perform a ` groupby ` on ` Date ` and aggregate each group using ` np.mean ` ?
( so you will get `` MultiIndex `` columns
To get out the array , use this as a boolean mask : #CODE
I wanted to merge these files so that i have something like this #CODE
If it's six , then you can use join method by @USER Hayden .
Then you can simply ` join ` them : #CODE
@USER when you do a join with 2x2 duplicates you get 4 in the joined DataFrame .
It's unclear how pandas should join in this case , so you need to be more explicit to it ( and tell it what do you want ) .
On the similar note , is there a way to merge values based on index .
For example , instead of listing Bact5 in two rows , can we merge its value corresponding to file2 in one row separated by a delimeter ?
Creating a weighted score column from Pandas pivot table
I have the following pivot table in pandas : #CODE
You can also adapt this to append rows to a DataFrame .
However , you don't have to materialize _all_ of it at once ; just consume part of the generator , then append the data to a DataFrame in chunks .
types to aggregate ') pandas.core.groupby.DataError : No numeric types
to aggregate
The parse doesn't support reading of complex directly , so do the following transform .
If you replace your zero values with ` NaN ` , ` cut() ` and ` qcut() ` behave as expected ; those rows will have a bin value ( from ` Categorical.labels `) of ` -1 ` : #CODE
Pandas dataframe merge
I want to consolidate this into a dataframe like this : #CODE
How can I merge them based on all.index ?
Just replace them with appropriate values .
Ok , I will give outer join a try , thanks .
I'm sorry for misleading you , I was not clear enough when I said merge :( I was not thinking about the ` merge ` function but a merge as a general term ...
First creating two diff data frames #CODE
But this is pretty redundant , and , as @USER mentioned , if you want the aggregate values , you should use groupby .
create a new key that is the combination of the fields you want to pivot .
I am sure the place to start is df.groupby ( ' time ') but then I can't seem to figure out the right way to use concat ( or other function ) to build the split data frame that I want .
Really it is just two indexed data , earlier code had them as separate columns , but I played with coverting it to using multiindex .
So the issue was that argmax found the 0th item if they were all False so I just special cased that ( I also used values so I'd get the position rather than the label ) .
python pandas : why map is faster ?
can anyone here explain a bit why the map approach is faster ?
I bet you ` from operator import methodcaller\\df2 [ ' a '] .map ( methodcaller ( " startswith " , " t "))` will be significantly faster yet .
@USER ; it's not using the built-in ` map ` ( which would be slower in this case ) .
@USER : I see ; I was just wondering where the second parameter to ` map ` was :)
For example , I find that ` map ` can actually be slower .
And now it seems like the ` map ` is winning when used as an index , although the difference is marginal .
This is probably also the section of the docs where Wes states that [ startswith is slower than slicing ] ( #URL ) !
Trying to shift back one row to get the previous value if the current ' Date ' is empty : #CODE
So then I tried applying with axis=0 ( per column basis ) and modifying the function so it only applies this to the ' Date ' column ( I can't see how to apply this to just one column ) #CODE
Also , ideally , I want to do this using ` apply ` as there are other things I want to do within that function which I removed to simplify this question .
then show what you actually want ; this is the efficient way of doing it , apply is essentially a loop in python space
Are you trying to shift ends by one ( month ) ?
My initial suggestion was to do the shift after you've reindexed ( since you're about to do that anyway ): #CODE
the shift index looks like a better fix , still would like to know if there is a simple date add function , which is how I'd do it in sql , that could apply ?
I'd still like to know if there is a simple DateAdd type function that I could use that might also apply for use elsewhere if needed ?
Use ` apply ` to do so : #CODE
Is it possible to merge it like following instead : #CODE
And then concat these into a DataFrame : #CODE
thank you , this helped me to merge my tables correctly .
My question is , how can I merge the columns ` Year ` , ` Month ` , ` Day ` into column ` NewDate `
If it's already in your DataFrame , you could use an apply : #CODE
I'm attempting to use the apply function to the panel and name the new columns based on the original column name appended with a ' p ' ( for easier indexing later ) .
The apply function I'm using operates on a Series object , which in this case would be a passed column .
My question is how can I use the apply function on a Panel such that it calculates new columns and appends them to each DataFrame ?
If you want to add a new column via ` apply ` simply assign the output of the apply operation to the column you desire : #CODE
Or , in this case , simply `` apply ( newCalculation )`` .
The first doesn't work because it actually creates a new DataFrame inside the panel and it doesn't append the column to each DataFrame .
There are some day by day calculations we need to perform , which is why it will be cleaner to not append all csv's into the same DataFrame .
Try using a double transpose : #CODE
So I want something that will drop the ` lob ` group , but keep every record of both the ` mol ` and ` thg ` group .
Unstacking to a MultiIndex of ( Index , ColumnName ) -> value works too but somehow doesn't feel as nice since the return structure would typically need to be pivoted back to index vs column view .
I am using the pandas library for creating pivot tables in csv files .
This is a portion of my original list.csv file i'm trying to convert to a pivot table .
right now my only problem is the fact that the values field cant take more than one value and wondering if anyone has any idea on how to use stack or reshape functions .
It looks like you can stack / reshape into the correct form .
I think you're looking to do some kind of stack / unstack afterwards ( most likely ` .stack ( 0 )`)
I got the pivot table right , but the rows were not in the format i wanted them to be .
This link explains that pandas has align functions and reindex functions to accomplish the above , but how to accomplish addition with automatic re-dimensioning and alignment , favoring the maximum dimensions ?
They do an outer join by default : #CODE
both the matrices have different label ordering , will it work , does it automatically align the labels following the #URL right , inner join for overlaps and conflicts ?
One way to workaround is to reindex using the same index : #CODE
Do a inner join on ` member ` and ` time ` columns : #CODE
But I feel pandas has this functionality somewhere in ` resample ` , ` reindex ` etc . but I can't quite get it .
Since the values in the second series are ` NaN ` you can ` interpolate ` and the just select out the values that represent the points from the second series : #CODE
You need to use method = ' values ' for the key arguments in interpolate to get the same answer as in numpy pd.concat ([ data , ts ]) .sort_index() .interpolate ( method = ' values ') [ ts.index ]
I then forward interpolate but feel free to apply more fancy methods #CODE
And several more , because I want to find out how many data points are there per user on average / median / etc ..
Merge Two DataFrames With Hierarchical Columns
I would like to merge two DataFrames while creating a multilevel column naming scheme denoting which dataframe the rows came from .
are you looking to transpose a matrix ?
So , I want to replace the markers in the Actor column with NaN values , and grab the special data from the behavior column to put in another column ( in this example , the empty Activity column ) .
The pandas pivot-table allows me to pivot easily to a particular subset of the data and plot it .
( please excuse the length of this post ! i wanted to explain throughly and i'm a total noob on this forum . also , stack overflow would not allow me to tag this with errorbars , error bars , or bars )
Then I can apply the function to my dataframe , grouped by I D: #CODE
( We could do it with ` transform ` , of course , but that feels like too much firepower . )
Now , ` loc ` throws an error because there is no index ` None ` : #CODE
If you wanted it to be NaN , you could backfill and then take the first element : #CODE
I'm a newbie to Pandas and I'm trying to apply it to a script that I have already written .
Don't use eval , use ` ast.literal_eval ` !
How to use crosstab / pivot with multi dimensions
I tried using pivot tables to have more than one values in the ' values ' field for the pivot_table function but it doesnt work , so im trying to see if i can do it with crosstabs .
In python , after converting it to a pivot table , the dtype is float64() and using #CODE
And i want to group them by date , after they are transformed into pivot table
Please suggest me a way to append unique values df =d ataframe [[ ' A ' , ' B ']]
for i in range ( len ( df [ ' A '])) :
Can we edit the question to get rid of the sparse stuff which isn't really relevant and change " override " in the title to " replace " or " update " ?
possible duplicate of [ How to remove relative shift in matplotlib axis ] ( #URL )
This is essentially the same problem as How to remove relative shift in matplotlib axis
I use pandas to join data .
Unfortunately you can't just do an apply ( since it fits it back to a DataFrame ): #CODE
What I would like to do is combine these into one data frame and then interpolate the missing values such that I have a complete dataframe .
Compare 2 columns of 2 different pandas dataframes , if the same insert 1 into the other in Python
So what I want to do is search df2 for all instances of a match from both columns of df1 ( noting the different data formats ) and insert the data from df2 into df1 .
You can use a plain ol ' merge to do this .
Now a simple merge will give you what you're after : #CODE
Could you try renaming the cols to replace the spaces with underscores
How to use pandas to group pivot table results by week ?
Below is a snippet of my pivot table output in .csv format after using pandas pivot_table function : #CODE
The tool you need is ` resample ` , which implicitly uses groupby over a time period / frequency and applies a function like mean or sum .
you can pass `` axis=1 `` to resample , to avoid the double transposing
For example , the date column remains as it is but all the data below the 2012-11-11 will shift to the left to be under 2012-11-04 ?
I can't apply a ` where ` filter to the ` read_hdf ` step .
This produces the following DataFrame with a MultiIndex based on the ` groupby ` call : #CODE
However , I want the ` agecat ` level of the MultiIndex to be naturally ordered , rather than alphabetical ordered , that is : ` [ ' child ' , ' adolescent ' , ' adult ' , ' senior ']` .
However , if I try using ` reindex ` to do this : #CODE
` melt ` it first in R , and then load it ...?
missing first row after resample
I'm taking a rolling average of all the data which corresponds to 1 , 2 , 3 , 4 , or 5 .
I have a pandas dataframe and would like to drop all columns save the index and a column named ' bob '
Pandas dataframe : drop columns whose name contains a specific string
I want to drop all the columns whose name contains the word " Test " .
And the op did not specify that a number had to follow ' Test ' : _I want to drop all the columns whose name contains the word " Test " _ .
But the thing you explained you want and the algorithm you showed , are more likely to be a rolling sum with a window size equals to 2 : #CODE
Naming returned columns in Pandas aggregate function ?
I've read the documentation , but I can't see to figure out how to apply aggregate functions to multiple columns and have custom names for those columns .
This will drop the outermost level from the hierarchical column index : #CODE
but since they have matching dimensions you need to transpose one of the DataFrames #CODE
Pandas Merge Error : MemoryError
I'm trying to two relatively small datasets together , but the merge raises a ` MemoryError ` .
I have two datasets of aggregates of country trade data , that I'm trying to merge on the keys year and country , so the data needs to be particularity placed .
This unfortunately makes the use of ` concat ` and its performance benefits impossible as seen in the answer to this question : MemoryError on large merges with pandas in Python .
The attempted merge : #CODE
You seem to have duplicates in your ` df ` what happens when you drop the duplicates and then merge ?
Also , the merge doesnt seem to be form lacking memory .
When I do the merge it I dont utilize over 50% of my memory .
No worries , another thing to check which caught me out is if you have any NaN ( null ) values in any columns you are merging with , up to you what to do but I would drop these also if you have any
In general , to apply functions that take two arguments , you can " curry " the function like so : #CODE
It's faster to use ` apply ` with ` functools.partial ` : #CODE
You could even make a function that returns a partial function that you can apply : #CODE
This should avoid overhead from ` apply ` -like things .
` map ` over the elements : #CODE
Merge identical index in pandas dataseries
I would like to merge these and then sum the values so it turns out to be : #CODE
Assuming your index are proper dates , you can resample to a daily interval .
I am busy writing code to translate this , but my guess is that it is such a simple use that there must be method to do this .
Assuming you have multiple entries like this and you want to put them into a table format , you should look at `` stack `` and `` unstack `` in the documentation .
I tried a pivot table but ended up with an ' Index contains duplicate entries error ' .
You can resample passing day as frequency , without specifying a ` fill_method ` parameter missing values will be ` NaN ` filled as you desired #CODE
This constructs a second time series and then we just append and call ` asfreq ( ' D ')` as before .
Basically I take a rolling average and a rolling standard deviation .
I plot the rolling average , but I would like to change the line colour if the standard deviation corresponding to that average is over the threshold I set .
This link is useful , although I cannot figure out how to apply it to my situation .
I am trying to calculate Volume Weighted Average Price on a rolling basis .
I want to drop std , min , max , etc ...
I use Pandas in IPython Notebook rather than IPython as a terminal shell , I don't see any options in ` set_option ` that supports the colouring , it maybe something that could be done as a plugin to apply some css or output formatting .
Pandas Merge - How to avoid duplicating columns
I am attempting a merge between two data frames .
What is the best way to merge these by index , but to not take two copies of currency and adj date .
Why not just select the columns you want to merge on like this : ` dfNew = merge ( df , df2 [[ ' data_col_2 ']] , left_index=True , right_index=True , how= ' outer ')` this avoids the duplicate columns and clash
@USER in that case there is nothing you can do other than define some method that drops all the columns that end in ` _y ` and rename the ` _x ` columns if you do not know upfront , after performing the merge , at least to the best of my knowledge
@USER actually thinking about it you can just take the complement of the columns that don't overlap e.g. ` cols_to_use = df2.columns-df.columns ` and then perform a merge using this list of columns that are in df2 and not in df ` dfNew = merge ( df , df2 [ cols_to_use.tolist() ] , left_index=True , right_index=True , how= ' outer ')` if htis works I will post as answer
You can work out the columns that are only in one dataframe and use this to select a subset of columns in the merge #CODE
then perform the merge using this ( note this is an index object but it has a handy ` tolist() ` method ) #CODE
This will avoid any columns clashing in the merge
Note that to use ` loc ` you must be working with Pandas 0.11 or newer .
But given that ` loc ` is not available until Pandas 0.11 , I think it's still fair to point out how to do it with chained assignment , rather than pretending like ` loc ` is the only thing that could possibly ever be the correct choice .
I dislike it that an entirely additional function , ` loc ` ( or even ` ix ` honestly ) , subsumes that functionality .
The obfuscation saved by not needing to reason step-wise about each iterative get or set is replaced by obfuscation that ` loc ` is not itself a data object but rather a function that pretends not to behave like a callable and acts like it is gettable or settable .
If there's more definitive examples for mixed datatype columns , I'm happy to endorse ` loc ` as the preferred method .
Using ` loc ` is not even possible for me except on our research system .
looking around I saw the pivot table function - ( df is the dataframe holding the original data ) #CODE
Would it be possible to map something like the following lambda function to mimic a SQL coalesce function ?
The whole point of ` map ` is that it passes each value , one by one , to your function .
The ` apply ` method is a very general way to apply a function across either the columns or rows of a ` DataFrame ` : #CODE
By default it applies a function to the columns , but by passing ` axis=1 ` you can apply a function to each row : #CODE
I'm trying to join two DataFrames in pandas on two fields , ' date_key ' and ' user_uuid ' , but when I do I only get an empty set , despite there being overlap when I search the tables for matches .
I made sure to strip out the index of definedRIDs so that it looks like this example from the docs .
Any ideas on why I'm having this mismatch on the join ?
if not (( len ( self.left_on ) == self.right.index.nlevels )):
you're correct , once the keys were the same type , the merge option with no index works !
The join appears to require an index , though .
The correct solution was in fact to join the frame with no index ( on the left ) to the frame with the multindex on the right : #CODE
genius , I from here learned something about multiIndex usage . the pandas tutorial just give too much info to digest and quick learning .
You can apply the swap_axes method after construction : #CODE
I think calling ` map ` is the fastest method as this will vectorize the operation without performing an expensive loop iteration .
` map ` * does * iterate .
@USER wasn't aware of that I knew that ` apply ` did iterate but thought ` map ` didn't
I would like to calculate Correlation and rolling Correlation of different Pairs .
This cross-computes all pairs with rolling , returns a Panel of the results .
this just calculates the correlation of a rolling 50 day window , and not just simple correlation for [ 0 : i ] rows
You could replace ` r*a [: , None ]` with ` np.outer ( a , r )` .
Is there a " pandas " way to identify which rows from a DataFrame are not present in a merge result ?
One " should " be a subset of the other , but when I perform a merge , the final product is smaller than the subset , so I want to look at the rows in the original ( as well as the ones in the subset ) to see why it's failing to merge properly .
I have GT , score and rsid in chip_raw that are tied uniquely to a CHROM and POS ( these two columns together identify uniqueness ) .
In some cases , simply inspecting the result of an ` outer ` merge for ` NaN ` will do it .
Using ` right ` or ` left ` can also help identify which side of the merge is creating the problem .
I hadn't thought about using an outer join for this , but that is perfect .
pandas.core.groupby.GroupByError : len ( index ) !
= len ( labels )
It's difficult to imagine a situation where a tuples nested in a MultiIndex is a good idea .
Dan , agree to a certain extent , but this particular messy multi-index came out of very natural operations : a df with a plain multiindex followed by a groupby on the multiindex itself and an additional column
Conform the results to 20ms , rolling 20 periods with no minimum #CODE
It is not a rolling sum with irregular times .
For instance , if I want to determine the number of times each user tagged something ( in descending freq . order ) , it's as simple as : #CODE
As a result of the ` groupby ` operation , both of these Series are indexed by date , so they will align automatically .
To average annotations per month across users , it is most convenient to use ` resample ` , a nice feature for grouping by different time frequencies .
@USER just pass a function to the resample then OP can do exactly what he wants
Best to put thin in a DataFrame , replace that value with a number and you will be good to go .
To not skip NaN values , you could use ` groupby / agg ` like this : #CODE
In the documentation , I can't find anything on how to transform the ` datetime ` coordinates to floats in the native transform of the plot I created by using the ` DataFrame.plot() ` method when I created the traces I'm trying to draw underneath .
Another option is to concat rather than merge : #CODE
Wouldn't it be best to strip out the whitespace in this example ?
The usual way you would do this in pandas is to use a boolean mask : #CODE
You can also use a ` ~ ` to negate the mask .
There error is raised because when you use numpy's where it returns the integer locations of the rows , rather than the labels , this means you can't use drop ( as this uses labels ) .
Iterating simultaneously / List comprehension issue ( in UDF to obtain Merge Report in Pandas )
Stata users will know that when merging data a ` _merge ` variable is produced that indicates by ` _merge ` being 1 that the ` merge ` was successful for that observation , by 2 that the observation is in the master dataset only , or by 3 that the observation is in the using dataset only .
The arguments are DataFrame1 , DataFrame2 , List of ' Keys ' ( i.e. columns to merge on ) and an optional argument HOW which is passed to the pd.merge argument how = HOW .
I have attempted creating the data frame and using ` resample ( ' h ' , how= ' count ')` to split into hourly counts , and using ` groupby ` , but can't quite put the timestamps into buckets and create the lists of values for each stream per bucket .
You can just use ` apply ` and assign direct to the column like so ` df [ ' start_time '] = df [ ' start_time '] .apply ( lambda x : dt.datetime.fromtimestamp ( x ))` , this is better than a list comprehension
Use ` apply ` #CODE
` datetime.fromtimestamp ` does a conversion to local tz ) .
To convert to the tz of Asia / Kolkata .
Pandas performance issue of dataframe column " rename " and " drop "
Why a simple dataframe column `" rename "` and `" drop "` operation costs that much percentage of time ( 8.9 % + 10.7 % ) ?
Anyway , the `" merge "` operation only costs 43.7 % , and " rename " / " drop " looks not like a calculation-intensive operation .
plt.legend ( loc = ' best ')
And then I append one result to another .
You can use loc : #CODE
Python using lambda to apply pd.DataFrame instead for nested loop is it possible ?
I'm trying to avoid nested loop in python here by using lambda apply to create a new column
Your apply doesn't work as by default it works column wise , plus you misunderstand what the lambda parameters actually represent so your lambda func does not map to the columns as you expected .
As @USER points out in the comment , the argument to the function in ` apply ` is a series , by default on axis ` 0 ` which are rows ( axis ` 1 ` means columns ): #CODE
Without ` DataFrame.isin ` you can create the ` mask ` with #CODE
I've added code above to show how ` mask ` can be defined without ` isin ` .
Why not reshape ( pivot ) after you've read in the DataFrame ?
Now , using concat / append and slicing , you can re-add the last row under a new date with : #CODE
I suspect I may need to change the index to MultiIndex on X , Y , but I'm not sure .
using rolling functions on multi-index dataframe in pandas
I want to be able to compute a time-series rolling sum of each ID but I cant seem to figure out how to do it without loops .
One possible way to get around this is to using groupby on IDs and then loop through that groupby and then apply a rolling_sum
I've been able to replace all the NaN data with 0's using the fillna function , which works like a dream !
I am hoping for something similar that will replace all string data with 1's , but leave the 0's in place .
The data look roughly like this , where I only want this to apply to columns starting with T_ : #CODE
I tried something using a T / F mask and a list of zeros , but the outcome was so wrong I won't bother to post the code here .
EDIT : Since you to replace all strings with the number ` 1 ` ( as per your comments below ) do : #CODE
Yet another way is to use ` filter ` and join the results together after replacement : #CODE
Right , so I understand how to use the replace() option , but as I said above I'm hoping to find some solution that will replace * any string* ...
This is because the above data is only a small chunk , I actually have upwards of about 50 strings to replace with 1's .
Basically I am looking for the equivalent to " search & replace all strings with 1's "
This gets the job done , except that I need to apply it to only 6 of the many columns in my dataframe .
I should have made it more clear above that there are additional columns containing strings that I don't need to replace .
Also , since I really want to understand this , would you suggest I check out the regex documentation and the replace documentation ?
I wrote the regex functionality of ` replace ` , so feel free to ask me qusetions
You * could * do this with ` DataFrame.filter() ` and then join the replaced columns with the not-replaced but that seems like it might be a bit inefficient and is also more complicated to understand ... want me to put it up ?
I was * really * hoping that I could outdo you with ` replace ` :( Guess not :)
Update : I've combined this solution with using a dict to apply the fillna method as suggested here .
Lookup value in dataframe using multiindex
If your df has a multiindex in columns ' key1 ' and ' key2 ' and you want to look up value xxx on key1 and yyy on key2 , try this
How can I access the ` name ` field of the resulting median ( in this case ` hello , foo `) ?
If you perform an operation on a single column the return will be a series with multiindex and you can simply apply ` pd.DataFrame ` to it and then reset_index .
Here is the rub - we do not yet know how we want to aggregate the data , and as such currently I just need to achieve a structure where we can try multiple aggregation methods .
Each aggregating function ( the functions you pass to ` agg `) is passed the column it aggregates over , as a Series .
Also , you might instead want to look at using ` apply ` , which lets you return an entire DataFrame .
Do it with ` loc ` : #CODE
I am using Pandas to plot rolling means , etc and would like to mark important positions with a vertical line .
Pandas will truncate the output if the result is likely to exceed the default settings , you can change this using ` set_option ( ' display.max_rows ' , 1000 )` or whatevery value , you can lookup the docstring and if you in IPython and perform a tab completion it will show you the default setting and the current settings .
I make a dataframe in ` pandas ` and aggregate the measure ' counter ' by the day created : #CODE
Transform your date index back into a simple data column with ` reset_index ` , and then generate your json object by using the ` orient= ' index '` property : #CODE
Can I use the resample function to look at end of the month snapshot of this data structure ?
Can I use the resample function call to help me out ?
Why do you need to resample ?
I would like to resample this serie in order to get a serie like that #CODE
You need to assign ` s ` to itself in order for the reindex to have an effect ` s = s.reindex ( index=range ( 1 , max_range ) , fill_value=0 )`
This question on Stack Overflow also has some good tips for profiling your memory-usage !
I am trying to apply Logistic Regression in Python using statsmodel.api.Logit .
To ignore rows with missing data and proceed with the rest , use ` missing= ' drop '` like so : #CODE
Note that you'll need ` pandas ` version 0.11 or newer to make use of ` loc ` for overwrite assignment operations .
You've even acknowledged this [ elsewhere on Stack Overflow ] ( #URL ) .
You can use ` map ` , it can map vales from a dictonairy or even a custom function .
And map : #CODE
You need to pass a ` list ` , ` dict ` , ` tuple ` , or generator of ` DataFrame ` or ` Series ` objects to ` concat ` .
How to resample a time series producing its geometric mean ?
I'm new to Python and I just came cross a tricky question when using ` pandas ` to resample some data .
When I want to resample my time series data , it is very straightforward to apply the arithmetic mean function .
I think I need to use ` datetime.date.fromtimestamp ` but I'm not quite sure how to apply this to the whole of df.date .
The rest is just standard applications of unstack , fillna , div and groupby .
That Series has its own index which I'd like to replace by ` x [ ' cat ']` .
If we use ` pd.Series ( x [ ' count '] .cumsum() , index=x [ ' cat '])` , then a ` ValueError : cannot reindex ...
` exception is raised because the original Series has duplicate entries in its index -- so it is unclear how to map from the old index to the new index .
If you are already going to allow the maximal st . dev ., why not just see what the profit is given your maximal shift allowed , and back-calculate the weight ?
But I imagine there would be a way to transform this into some type of Lagrangian optimization problem ...
When I found the combo I wanted , I would copy the commands , drop out of the interactive interpreter and modify the script .
This statement ( and similar others ) used to insert new data returns a ` NaturalNameWarning ` warning : ` store.append ( ' equity / bloomberg / 4615238QCN_Equity ' , df )` .
But , when I embed the same in a program and call with parameters , I get an error - the final lines are : ` code ` File " / usr / local / lib / python2.7 / dist-packages / pandas / core / series.py " , line 225 , in wrapper if len ( self ) !
= len ( other ): Type Error : len() of unsized object
I believe I need the transform method , and I think that the first argument passed to the transform function ( as specified ) is the DF column as a series as with the agg() method .
Also I am not seeing anything in the documentation about passing a Dict to transform , but I can't see any other way of performing the function on only one variable / column in the gorup .
The error message is long an confusing but ends in " Transform function invalid for data types "
I think you could use shift function to shift the column in grouped data by one row .
If you want general function to iterate through the series elements using transform , but without copying to list , try this : #CODE
I suspect that yemu's answer is good , but I prefer this as it was an exercise in finally learning how to apply my own functions : #CODE
You can call a function directly with transform , wrapping it in a ` lambda ` is not necessary : ` .transform ( WorkingHours )`
sorting pandas dataframes according to cut in python ?
Now how can you sort df according to the labels generated by ` cut ` ( the ` df [ " bins "]` column ) ?
` df.sort ( columns =[ ' a '] , inplace=True ) df [ " bins "] = pd.cut ( df.a , np.linspace ( -2 , 2 , 6 ))` will then order it prior to applying the cut for the new ' bins ' column
Or you could just sort ' a ' after applying ` cut ` would achieve the same thing
Do I need to convert every float to a string and replace the " .
Pandas interpolate data with units
( obviously there is not point to interpolate depth over depth , but it's a test before it gets more complicated ) .
But when I try to interpolate the missing values doing : #CODE
But if I try now to interpolate the missing values by dropping the units , it works : #CODE
Then pivot : #CODE
The basis for that pandas operation is called ` stack ` : #CODE
You can follow by resetting the index , split it into year month day columns , and apply the math on the non NaN data that are in a single column now .
Group hourly , and use ` transform ` with this answer like so : #CODE
Python : Pandas merge 2 dataframes using a common columm
How can I merge 2 dataframes df1 and df2 using a common column ' ADD ' into df3 ?
You have to do an outer merge ( join ): #CODE
So you need to apply an aggregation operation ( e.g. ` sum ` , or use ` apply `) to your grouped frame , which will then create a new frame , which you can ` to_excel ` .
I suppose I should be using code that just sorts if I want to organize by groups for the output , but not apply any aggregation operations .
if you truly do want to group ( outside of excel ) , then you should aggregate as well , otherwise , sorting is a good compromise
See this answer's edit history for a less direct way using groupby , mean and unstack .
However i'm fuzzy on exactly what the unstack operation does .
@USER It's basically a pivot to turn the test level a set of columns , which made me realise what you are doing is just a pivot !
I understand pivot tables from working with them in Excel years back .
The stack and unstack operations make a little more sense to me now in this context .
You can do this with the lesser-known ` align ` method and a little ` unstack ` magic : #CODE
Nice to see align used , also .
I'm trying to add two dataframes with Multiindex Columns and different index sizes together .
This will align the frames and fill the nan's with 0
yep .... that is shorter than `` align / + `` ...
So far , I have this ( where the " mask " gives the condition under which the data are true . checked this bit and it works ): #CODE
But I can't seem to figure out how to transform the output of the " mask " technique above to the correct format for this approach -- it has the same index info but is shorter than the dataframe I need to add it to .
Since the column names of the data frame and frame to mask with differ in this case , alignment can be disabled by masking with a raw , un-labeled ` numpy.ndarray ` ( aka . ` .values `) .
But transform reshapes the result to the original dimension .
Got it , then transform is what I need .
Then I'm trying to do a simple replace based on IDs :
` AttributeError : ' list ' object has no attribute ' loc '`
What you create there is a " mask " : an array with booleans that says which part of the index fulfilled your condition .
If you want to keep the entire dataframe and only want to replace specific values , there are methods such replace : Python pandas equivalent for replace .
But for a larger set of replaces you would want to use one of the two other methods or use " apply " with a lambda function ( for value transformations ) .
` df [ ' a '] .map ( df.groupby ( ' a ') .apply ( lambda x : len ( x )))`
First , reindex the dataframe so that your index corresponds to your starting date / time through your ending date / time with a frequency of 1 minute : #CODE
Then finally , strip out the times that are outside your 9:30 AM to 4:00 PM time range : #CODE
Now take the intersection of the sets of rows where those are true .
( DataError : No numeric types to aggregate )
I'm trying to do a pivot of a table containing strings as results .
But I get : ` DataError : No numeric types to aggregate ` .
I know I can map the strings to numerical values and then reverse the operation , but maybe there is a more elegant solution ?
I think the best compromise is to replace on / off with True / False , which will enable pandas to " understand " the data better and act in an intelligent , expected way .
My answer is , I don't think there's a better way , and you should replace ' on ' / ' off ' anyway for whatever comes next .
As Andy Hayden points out in the comments , you'll get better performance if you replace on / off with 1 / 0 .
Note that I changed the pivot to follow pandas 0.14.1 syntax ( thank you " future warning ") #CODE
So basically , you add lambda identity function as the aggfunc to the pivot magically works as you would expect .
There may be some performance difference for really big data , however , if you transform to numerically coded " results " first , you essentially still have to touch each cell in the table anyways .
The ` hist ` method of ` Series ` ( the type of ` values `) can be called with a ` bins ` keyword argument .
I think you're looking for the ` closed= ' right '` and ` label= ' right '` arguments of ` resample ` : #CODE
Another idea would be to use [ shift ] ( #URL )
@USER you can aggregate the different components of the date cols into a list of lists like so ` parse_dates =[[ ' s1_startdate ' , ' s1_starttime '] , [ ' s1_enddate ' , ' s1_endtime '] .... ]`
Python : append / merge dicts to pandas in a pythonic way
I'm new to python , I'm trying to append result from my iteration loop that give out different number of keys each time in the loop .. something like #CODE
You can either append each dict to your DataFrame as you loop : #CODE
How to create pivot with totals ( margins ) in Pandas ?
I have success when I try to make pivot without margins ( totals ): #CODE
but if I want create pivot with margins : #CODE
Read the docs on groupby and resample .
If you really want , you could strip the extra tabs at the end .
Updating a tri-match pandas script to update instead of append columns
I think you can achieve that by changing the merge to ` how= ' right '` .
Your question is a little unclear , but you seem to be trying to apply a function to each row of the DataFrame .
` timeit ` of the ( presumably ) desired single ` bool ` indicating whether the two ` DataFrame ` s are equal : #CODE
return bool ( asarray ( a1 == a2 ) .all() )`
Then perform an outer join operation and apply a function that does the comparison to the result .
One of the distance metrics ( jaro , perhaps ) in the ` jellyfish ` library would probably apply .
Do I need some sort of dynamic ` reindex ` function ?
I wasn't fully understanding how well reindex works .
` reindex ` is an amazing function .
It can ( 1 ) reorder existing data to match a new set of labels , ( 2 ) insert new rows where no label previously existed , ( 3 ) fill data for missing labels , ( including by forward / backward filling ) ( 4 ) select rows by label !
Is there a way to normalize all the possible column names to " Address " in pandas so I use the script on different files ?
Is there a way I can prevent pandas for ruining the data in the first place when I ix it ?
Then this give the intersection you need to group your data : #CODE
Iv'e tried to use the groupby mechanism , but with no success . using the simple apply mechanism is ok , but seems a little cumbersome ( I'll need to keep a dictionary containing a counter of appearances for each ID )
because i was trying to use groupby and apply and what i got back was a series with the ID as index and the modified ID's as lists for each index . what is going on here under the hood ?
` apply ` and ` transform ` do similar things .
` apply ` is a complicated beast because it behaves differently depending on the type of object the function returns .
In this case , since I knew transform is intended for changing a Series to another Series * of equal length* , I tried transform .
To better understand what my solution is doing , I suggest first looking at ` df.groupby ( ' ID ') [ ' ID '] .transform ( lambda x : x )` then ` df.groupby ( ' ID ') [ ' ID '] .transform ( lambda x : x.rank ( ' first '))` and ` map ( ' {}_{} ' .format , [ 1 , 2 , 3 ] , ' abc ')` .
Compute which rows you want to mask first ( otherwise they might change as you go )
How to pivot this data frame with pandas
I'm trying to pivot this DataFrame ' a ' : #CODE
by calling the pivot method #CODE
` target_period = source_period * float ( len ( df.index )) / 100 `
Build a MultiIndex from X and Y , and use unstack .
is there a way to append a new row to this newly created dataframe .
Currently I have to create a dictionary , populate it , then append the dictionary to the dataframe at the end -however is there a more direct way ?
Upcoming pandas 0.13 version will allow to add rows through ` loc ` on non existing index data .
enlargement only allowed thru `` loc `` ( `` iloc `` could add not-at-the-end so its a bit ambiguous )
I've found that this will drop any columns from the added Series that are not already in the DataFrame .
In this case , the documentation is misleading in that it states that " Setting With Enlargement " is like an " append " operation , even though " append " will add any new columns to the DataFrame .
Possibly useful , but this doesn't say how to apply the transformation to the date index ...
Thank you for expanding the explanation , this works perfectly .
You can also work with ( and resample ) as periods #CODE
Python Pandas business day range bdate_range doesn't take 1min freq ?
I am trying to use bdate_range with ' 1min ' freq to get minute by minute data on all business days .
I think the freq = ' 1min ' totally overwrites freq = ' B ' from function name bdate_range
I've tried using the stack / unstack and the transpose function , but I just can't figure it out .
THen you can transpose your resulting table : #CODE
replace values in pandas using regex that matches all values except the provided one
I want to use regex with pandas to replace values in a column to mark correct answer for the question .
and it doesn't replace values in pandas .
I'm not seeing where ` incorrect_dict ` is being passed to any replace / substitute call .
I can't check your answer right now , but the reason I chose to replace values is that I wanted a universal approach and other variables I have are in text format .
On ` groupby ` object , the ` agg ` function can take a list to apply several aggregation methods at once .
In this case we assign the output of ` groupby ` to a variable , and use it to , separately , aggregate the means and count the rows in the group .
I could like to test a few conditions for each row and if all conditions are passed I would like to append the row to list .
Instead , create a mask ( boolean array ) with True values for the rows you wish to select , and then call ` df [ mask ]` to select them : #CODE
It doesn't usually make sense to perform ` value_counts ` on a DataFrame , though I suppose you could apply it to every entry by flattening the underlying values array : #CODE
I know about the apply function but it is too easy in my case ..
I would like to map 1 , 2 and 3 to 1 and 4 , 40 , and 50 to 2 .
For example for three groups I would like to map 1 and 2 to 1 , 3 and 4 to 2 , 40 and 50 to 3 .
` heapq.nlargest ( len ( df [ ' ELEV ']) / 2 , df [ ' ELEV '])` but got this error :
In the dataframe above I would like to apply the qcut function to B while partitioning on A to return C .
To apply your custom function to each row , use ` apply ` with the keyword argument ` axis=1 ` .
So , if I try to insert into a new data frame I get all sorts of errors ( i.e. dimension mismatch , etc ) .
I want to selectively average slices of my original data frame , and insert these averaged values into a separate data frame .
`` df.mean() `` is a reduction operation , by definition it will aggregate across the specified dimensions , reducing the dims of the returned object by 1
So long as the result can be [ broadcasted ] ( #URL ) you can do this ` a [ ' mean '] = a.mean() ` , also do you really want to concat the results as new rows and not as a new column ?
@USER you can just pass the ` mean() ` into a ` DataFrame ` constructor for ` concat ` like so ` b = pd.concat ( b , DataFrame ( a ( a.A > 5 ) .mean() ))` , although this doesn't work I think you wanted ` a.loc [ a.A > 5 ]` ?
Since I can't easily construct a sample version with your exact column labels , there might be slight additional tinkering with getting rid of any index columns that are included in the ` diff ` and moved with the transposition .
The number of periods defined in your index is ` len ( x )` , which is the same as your ` duration ` variable .
You may wish to alter duration or maybe keep ` freq ` as a day to get more than 1 day worth data .
The following is the result of ` head ` on the CSV produced if I alter the ` freq .
You should use some kind of boolean mask to do this kind of operation .
Use this to create the mask : #CODE
And use loc to look at the subDataFrame where msk is True , and only the specified columns : #CODE
I apply a filter to the original_dataframe to create a new_dataframe_1 and another filter to create new_dataframe_2 .
If you want the columns from both DataFrames joined together , do an inner join : #CODE
do a ` reindex ` on the intersection of the indices : #CODE
I don't want to join the two dataframes .
I have been searching for hours , literally the entire day on how to generate a pivot table in Python .
What I want is to take a csv file , extract the first column and generate a pivot table using the count or frequency of the numbers in that column , and sort descending #CODE
I'm ultimately trying to get a pivot table of mean rating for usage by brand .
My approach was to merge the datasets like this : #CODE
And then attempt to create a pivot table using rating as the value , own as the rows and brand as the columns .
I have also attempted unstacking either the measure or brand levels , but I can't seem to use row index names as pivot keys .
I would like to replace an entire row in a data , subject to one column in that row satisfying the condition .
Use ` loc ` to filter out the part of the DataFrame you want to zero , and then assign the value to it .
stack / unstack / pivot dataframe on python / pandas
Pandas programming model for the rolling window indexing
If we could add an additional column , called trajectory_id , we'd be able later to reindex this DataFrame either by time ( creating sub-groups of the particles in single time instance and learn their spatial distributions ) or by the trajectory_id and then create trajectories ( or linked particles and learn about their time evolution in space , e.g. x ( t ) , y ( t ) , z ( t ) for the same trajectory_id ) .
The only problem is that I do not know how to have a rolling function through the rows of a DataFrame table , creating a new index column , trajectory_id .
I want to get a pivot table that combines all of the prices per UNSPSC code .
UNSPSC and then the aggregate price associate with the unique code .
I also tried to pivot by #CODE
How can I get a pivot table with that shows the sum of the prices to each unique UNSPSC ?
Not sure it's what you want , but try to combine groupby and aggregate : #CODE
possible duplicate of [ Pass percentiles to pandas agg function ] ( #URL ) .
You can apply value_counts to the SeriesGroupby ( for the column ): #CODE
I actually hoped this to return the following DataFrame , but you need to unstack it : #CODE
splitCompaniesSet = map ( lambda cmpnyName :
set ( map ( lambda name : name.split ( " ") , cmpnyName ) ) , dataFrame [ ' Company '] )
Then , starting with the first element , find the set intersection of every other element with that one .
For every non-empty intersection , change the name to whatever the simplest match was
among all the non-empty resulting sets , i.e. take one more set intersection with all the nonempty sets and set the result to be the company name for all those non-empty matches .
I was not sure if that would apply to this situation however .
I will take some time to check it out and see how I can apply it .
I get the error : ` KeyError : u'no item named name of first item I try to insert '`
I want to convert datetime to object because pandas have a bug ( see #URL ) in pivot totals calculation if column ( which used as header ) type is datetime .
the solution in that issue looks fine , just transpose the rows / columns .
yes , but only if pivot row column isn't datatime too .
So , you need to strip all of them off .
That's relatively simple to do , but because of the unicode issues I'm going to be crazy and suggest you read it in , strip the quotes and then write it to a file to use with ` read_csv ` ( because it'll simplify the encoding issues ) .
Here's how to write to a file and strip the quotes , write to a new file , then read in with read_csv : #CODE
Outer join in Python for thousands of large tables
So , I have some 4,000 CSV files and I need to outer join all of them .
Each file has two columns ( a string and a float ) and between 10,000 - 1,000,000 rows and I want to join by the first column ( i.e. , the string variable ) .
What other outer join implementations exist for Python ?
Would it be more efficient if I simply had Python call , say , sqlite3 , and then have sqlite3 do the join ?
Don't merge iteratively .
merge 1 and 2 to form 1_2
Then repeat , so then you merge 1_2 and 3_4 to form 1_2_3_4
This doesn't change the amount of work you are doing , but it makes each merge much simpler , which lowers the memory barrier and spent time ( as it has to go thru the cartesian product of the keys ) .
It may make sense to randomize the merge order .
The normal matplotlib boxplot command in Python returns a dictionary with keys for the boxes , median , whiskers , fliers , and caps .
The Pandas library has an " optimized " boxplot function for its grouped ( hierarchically indexed ) dataframes .
e.g , The 5th item is the median .
If we want to set the median to have ` linewidth ` of 2 : #CODE
This will return the boxplot properties directly in a dictionary , which is indexed by each column that was plotted in the boxplot .
when I call the function , both the hist and the scatter plot should appear
Pandas : Drop consecutive duplicates
What's the most efficient way to drop only consecutive duplicates in pandas ?
Use shift : #CODE
So the above uses boolean critieria , we compare the dataframe against the dataframe shifted by -1 rows to create the mask
Another method is to use ` diff ` : #CODE
Thanks to Bjarke Ebert for pointing out a subtle error , I should actually use ` shift ( 1 )` or just ` shift() ` as the default is a period of 1 , this returns the first consecutive value : #CODE
@USER no worries , glad to help , I noted your comment and you are correct , the resulting row values was what the OP wanted but the index values were the wrong rows as you correctly potined out , interestingly using ` diff ` was slower for a 50k series , probably due to the value comparison
Using ` diff ` only works for integers whereas ` shift ` works for floats , strings , etc .
How to sort a boxplot by the median values in pandas
I've got a dataframe ` outcome2 ` that I generate a grouped boxplot with in the following manner : #CODE
What I'd like to do is sort the plot by the median for each state , instead of alphabetically .
You can sort by the median or alphabetically , but you can't do both .
To sort by the median , just compute the median , then sort it and use the resulting ` Index ` to slice the ` DataFrame ` : #CODE
You can use ` loc ` too : #CODE
You'd need to select out all the columns that need leading zeros and apply the function ( with the correct width ) to each .
In other words , if the column names are an ` MultiIndex ` , how would I drop one of the levels ?
drop a column , or move into the index ( as a row ) ?
The idea being to move a given level of the column names's MultiIndex to the DataFrame as a new row ( or optionally just to drop it altogether ) .
Maybe a simpler question is how do I simply drop a level of hierarchy in the column names ?
You can build on that to get whatever you want , for example if you had a 2 level ` MultiIndex ` , to remove the outermost level , you could just do : #CODE
The result obtained , I need to apply the following conditions #CODE
You can make your own aggregate functions to apply to grouped data #URL .
Have you tried ` replace ` ?
please replace it with a list comprehension .
For my assignment I'm supposed to plot the tracks of 20 hurricanes on a map using matplotlib .
What I'm trying to do is put the longitude and latitude columns from the file into two seperate list so that I could plot them on the map .
What I'm trying to do is put the longitude and latitude columns from the file into two seperate list so that I could plot them on the map .
I think the problem is actually that the boolean array from the shift operation is one short of the the other conditional .
yes .... the issue is the Series vector ( e.g. `` df.D > 1 ``) * looks * like it should work , but its ambiguous how it should broadcast , e.g. should that Series named D apply to all of the other columns ( in which case what should it do ? ) , or should it effectively have no name which means it SHOULD broadcast .
All I simply want todo is cut this data into weeks and write it out to CSV files , I can already do this for conventional weeks ( Mon-Sun ) but I now want to cut into weeks that are defined as Sun-Fri .
I'm working with a 13 million row dataframe , and attempting to run the apply ( sequence_id ) bit ends maxing out the 20gb of ram I have available .
I'm trying to plot the path of 15 different storms on a map in 15 different colors .
For example if the storm's name is AUDREY , the color of the storm's path should be red on the map .
However the lines do not show up on my map .
Merge DataFrames in Pandas using the mean
I would like to merge them an take the mean if an index occurs in more than one DataFrame .
Now I would like to merge the DataFrames and take the mean for each index ( if applicable , i.e. if it occurs more than once ) .
One small thing : If I had more columns in the DataFrames , how would I define that I want to merge and average ' col ' and do another / no operation on the others ?
@USER : You could apply any of the above methods to the Series ` df1 [ ' col ']` and ` df2 [ ' col ']` .
you can use apply function : #CODE
@USER No , the line that throws an error is the one with ` reindex ` .
Then reindex that set to the full dates ( idx )
I'm having a similar problem at the moment , I think you shouldn't use reindex but something like asfreq or resample .
You should use loc to do this without chaining , which will garauntee that assignment works : #CODE
Assignment in loc is overridden so that the implementation detail of whether it's actually a view or a copy doesn't matter , it does matter if you chain so avoid / be very careful .
So if I can isolate the two-digit month , I can apply calendar.month_abbr [ ## ] to get that 3-letter month .
It may also work for a pivot table but I didn't test that nor did I look at the details .
Anyway I'm trying to plot the path of 20 hurricanes on a map ( graph ) .
The map itself and the legend show up perfectly but the paths of the hurricanes do not .
#Plot the points on the map
The goal here is to join one point to one person .
I've tried both merge and join but that doesn't give me the right output , way too many values .
I've tried massaging the merge output with drop_duplicates and unique methods , but no luck so far .
I've read through the merge documentation and I have this feeling there's a simple method to do this ... but so far I haven't found it .
As an aside , the background on my task is to generate the appropriate number of random points w / in each census block and join them back to the travel survey data so that it can be visualized in a dot visualizer .
If the indices of both dataframes match then what happens when you do this ` merge ( set1 , set2 , left_idnex=True , right_index=True )` ?
I think in your case as the two dataframes have the same number of rows , all you want is to add ` PLSAM ` to the other dataframe so the easiest thing to do is just ` set1 [ ' PLSAM '] = set2 [ ' PLSAM ']` , this will add the column from set2 to set1 , the merge and joins will not work in your case because you have no unique values even though the column values are identical , if this answers your question I will post as answer
The signature of merge from the pandas docs : #CODE
My suggestion would be to set the indices of each data frame to be the DTRACT column , and then continue with your merge .
I tried the merge method as you specified initially .
I would like to end up with a merge that gives me 141,947 values .
I know you said set 2 was derived from set 1 , but looking at the example data , for each of the indices provided , the DTRACT columns don't match up , which would cause issues similar to what you're experiencing when you try to merge .
Forget about ` merge ` .
Maybe it's because I use a lot of databases , but I prefer to the ` join ` method of the dataframe , and I greatly prefer to have indices defined for each dataframe .
I tried the join method and setting the indexes as you specified , but I still 25 values out instead of 5 .
@USER you're right . since the indices aren't unique , ` join ` won't work . glad you got it all worked out though .
I think this is far simpler than you think , the reason the merge and join do not work in your case is that although you have a common column , the values are not unique , this would not be a problem if the indices of both dataframes were the same but in your case it seems they are not .
I'm trying to use ` where ` on my Pandas DataFrame in replace all cells that don't meet my criteria with ` NaN ` .
This is close enough for me to accept , but it looks like you have to specify a particular column for this to work in cases where there are multiple columns , i.e. ` df.groupby ( " item_id ") .filter ( lambda x : len ( x.arbitraryColumn ) > 1 )` .
[ One reason I'm so puzzled is because the number of columns should have nothing to do with anything , the ` len ` here is giving the number of rows in the group . ]
I experimented a little bit with using a hierachical ` MultiIndex ` , to see if that was the issue , but I still couldn't reproduce the error .
Its counter intuitive for it to append today's date to it
`` to_datetime `` DOES NOT append today's date , nor does it assume anything .
My bad , I used the term append loosely .
Op wants to provide these locations back to the frame , just create a transform and assign .
Might be a bug , but it should be simple enough to ` apply ` something keep it all straight .
My problem is that I am unable to rename the aggregate variable count here .
As we see , it's very easy for me to rename the aggregate variable ' count ' to Total_Numbers in SQL .
Pandas division ( .div ) with multiindex
Could you please post the complete stack trace ?
So it would be drop the row if both the bid and the ask are the same as the row before it .
You can use ` .shift() ` to shift a column , and use ` any ` to check for differences .
Step-by-step , we shift down 1 : #CODE
are you looking for a join ?
I think what you want to do is do an outer merge , assuming you have the username in both ` start_df ` and ` friends_df ` then you perform the merge like so df = friends_df.merge ( start_df , left_on =[ ' username '] right_on =[ ' followee '] , how= ' outer ') .
What I want to do is reindex the dataframe such that each item has rows for each month in a specified range .
But with this method , I'd have to iterate through all the item_ids , then merge everything together , which seems woefully over-complicated .
So how can I apply this to the full dataframe , ffill-ing the observations ( but also the item_id index ) such that each item_id has properly filled rows for all the dates in baseDateRange ?
Essentially for each group you want to reindex and ffill .
The apply gets passed a data frame that has the item_id and date still in the index , so reset , then set and reindex with filling .
answer to your first part is yes that is reasonable , you can drop on the reset_index to not have a dup . for the second , I think the apply is a bit confused by the index because of the way you are aggregating ( so it's dropping the name of the index ) .
You can do another reset_index inside the apply , then at the very end ( after the apply , `` .reset_index ( drop=True ) .set_index ([ ' date ' , ' item_id '])`` , so reset the mi
Can pandas groupby aggregate into a list , rather than sum , mean , etc ?
I've had success using the groupby function to sum or average a given variable by groups , but is there a way to aggregate into a list of values , rather than to get a single result ?
A downvote for the question itself , cause " aggregate into a list " is very different than creating new columns ...
In other words I just want to shift the current header down and just add it to the dataframe as another record .
This is really cool and good to know but I meant how to replace the header ' A ' and ' B ' from the first df above but also just add the values ' A ' and ' B ' as another row , in other words move values ' A ' and ' B ' down to index 0 as the new first record in df .
It you want to apply these to all your pandas tables you can use css .
I need the apply function that returns several value from several complex calculations .
In general , The outcome of a groupby-apply operation would be a Series In the case apply returning 1 value .
In the case of apply returning 2 or more values , I would like the outcome to be a dataframe .
Returning a series in the apply call results being collated into a dataframe ( guessing that is what you are looking for )
Imagine that I need the apply function to retrieve few complex calculations .
In the case of apply returning 1 value , the outcome is a series .
In the case of apply returning 2 or more values , I would like the outcome to be a dataframe .
I appreciate a given column needing to have all the levels of the multiindex .
for this and your other questions , pls look at the docs : [ here ] ( #URL ) , you need to use `` ix / loc `` or `` xs ``
you cannot add a column that doesn't have all the levels of a multi-index . best to keep it in another frame then join / index as needed .
Pandas MultiIndex with integer labels
I have a MultiIndex with some levels labeled with strings , and others with integers : #CODE
The following code generates a Pandas series with a hierarchical MultiIndex : #CODE
I want to replace these by the corresponding values in ` column_2 ` .
you are modifying a copy , see [ here ] ( #URL ) , use `` mydf.loc [ ix , ' columns_1 '] = value ``
It is always better to set via the indexers ` ix / loc ` for multi-dimensional setting .
In this example , use ` mydf.loc [ ix , ' columns_1 '] = 45 `
so I'll try apply ( int ) I still don't get why the Dtype={ does not work on read_csv ?
as @USER pointed out , a quick way is to drop this value . run df [ df [ ' Credit_exp '] ! = ' \\N '] [ ' Credit_exp '] .astype ( int ) .
if no error occurs , you're good to go . otherwise , try numpy.unique ( df [ ' Credit_exp ']) and sense check string values . and of course drop them
I replaced map with apply and that didn't help matters .
Once I got rid of the rows with missing data , I could directly apply this to the new problem .
How can I drop duplicate data in a single column , group-wise in pandas ?
I've looked at the set_index and reindex documentation but I'm not making sense of it .
unstack doesn't work , because there are duplicate entries ...
What you could do is map your strings to ints / floats and map your column B to their dict lookup values into a new column C and then create the sparse dataframe like so : #CODE
Is there a simple way to calculate actual average or median time instead of strictly granular ?
If you must , you can then reindex your row labels and your column labels using these functions : #CODE
to add column with counts you can use transform : #CODE
Then ` pivot ` will take your data frame , collect all of the values ` N ` for each ` Letter ` and make them a column .
I'm on a roll , just found an even simpler way to do it using the by keyword in the hist method : #CODE
I'm exploring Pandas - trying to learn and apply it .
Now I want to convert the csv data into a pandas DataFrame object , so that date and time fields merge and become the DateTimeIndex of the DataFrame like this : #CODE
now apply the lambda function , doing what the parser should have done : #CODE
Could you see what happens when you drop the column names from the parameters to ` read_csv `
datetime.date creating many problems with set_index , groupby , and apply in Pandas 0.8.1
My goal : group a DataFrame by a categorical column " D " , and then for each group , sort by a date column " dt " , set the index to " dt " , perform a rolling OLS regression , and return the DataFrame ` beta ` of regression coefficients indexed by date .
Now here is what happens when I try to work with these using ` groupby ` and ` apply ` : #CODE
If I save the ` groupby ` object and attempt to apply ` foo ` myself , then in the straightforward way , this also fails : #CODE
I can simplify the problem just to the ` set_index ` call within the ` apply ` function .
Turns out this is based on something that happens in ` groupby ` which changes indices of the groups into a MultiIndex .
By adding a call to reset the index inside of the function to be applied with ` apply ` , it gets rid of the problem : #CODE
it is a bug : in pandas code len ( df1 ) is going to return 3 that is the length of the df1 index , but then the code iterates on df1 3 times through the for loop inside code .
@USER I'll join you on the crusade for Panels .
How to merge 2 columns in 1 within same dataframe ( Python , Pandas ) ?
After pd.read_csv ( ... ) he's using ' dt ' ( datetime ) column as index of dataframe .
I.e. merge two columns within one data frame , and then delete it .
You should be able to concat two columns using apply() and then use to_datetime() .
Id like to provide the row name , the column name , and a number of rows to shift back .
You'll want to use pandas shift method .
`` df.ix [ df.index.get_loc ( ' C ') -2 , ' b ']`` will be much faster ( as it doesn't shift anything , just an index lookup )
or ` left outer join ` with ` where ` clause : #CODE
Last one could be implemented in pandas with ` merge ` : #CODE
@USER see another solution with pandas merge .
Any suggestion that leads to a way of combining / stitching multiple ' hist ( by =) -plots ' in the same subplot grid is welcome .
In-line assignment via eval will be an additional feature in 0.13 , see here #CODE
Create a generator that evaluates a formula using ` eval ` ; assigns the result , then yields the result .
In theory ` numba ` is going to support this , so pandas possibly support this as a backend for ` eval ` ( which currently uses numexpr for immediate evaluation ) .
It is nice to have the ' formula ' and eval feature in coming update cersion .
you have duplicates ( the number 0 ) , you can't reindex with duplicates , reset_index first
How do you calculate expanding mean on time series using pandas ?
How would you create a column ( s ) in the below pandas DataFrame where the new columns are the expanding mean / median of ' val ' for each ' Mod_ID_x ' .
add title to collection of pandas hist plots
I've tried using the title keyword in the hist command ( i.e. title= ' My collection of histogram plots ') , but that didn't work .
Inspired by this question and @USER ' s answer , I wrote a parallel-version of map at github .
But all of this are encapsulated in a map function so that it is almost a drop-in replacement of the standard map function , with the help of pathos package .
I would have thought you could map a seasons dictionary to your data and then group based on that .
Finding common rows ( intersection ) in two Pandas dataframes
For example , we could find all the unique ` user_id ` s in each dataframe , create a set of each , find their intersection , filter the two dataframes with the resulting set and concatenate the two filtered dataframes .
I've looked at ` merge ` but I don't think that's what I need .
I don't think there's a way to use ` merge ` to have create the two separate rows .
+1 for merge , but looks like OP wants a bit different output .
or join and then unpivot ( possible in SQL server ) #CODE
Now I can easily demean that data etc . but what I can't seem to do properly is to transform data inside groups using multiple column values as parameters of the function .
For example if I wanted to add a column ' f ' that took the value a.mean() - b.mean() * c for each observation how can that be achived using the transform method .
1 ) What is being passed to the function f when it is called with apply ?
+1 , the main part of answer I think is to use apply instead of transform
Would it be right to say then that calling transform passes only the named column , or each column in the DF to the function individually and it is not possible to pass more than one column , whereas apply passes the whole data frame and then column values can be used within the function ?
ok , I think apply as in @USER answer is more appropriate here ?
How to do expanding functions using Pandas on Time Series
Try to use ` transform ` and ` shift ` combined together : #CODE
I am using some code to merge two csvs and sort these by two columns .
and replace the last line with : #CODE
If you really want to thank me , look up the functions I'm using there and figure out how each of those things is implemented , play around with them and apply it in your own code :)
This should be the case since you want to apply the function for each row ...
According to #URL axis=1 is to apply to each row .
So my suggestion would be to convert to Time Series after you apply the function , which obviously is contingent that you don't need your time variable in the function being applied .
* apply function not tested with pandas Time Spans .
You could ` apply ` the conversion on the appropriate column : #CODE
You can circumvent this to_datetime issue by resetting the index , manipulating the series via map / lambda / strptime , and then finally setting the index again .
This is a bug , because the index is not ordered monotonically , see here . but no reason to use ` TimeGrouper ` , this is somewhat internal ATM , use ` resample ` .
The standard grouping method of " resample " is " how= ' mean '" .
I have to apply a custom function .
a lot depends on how you are storing ( e.g. appending all at once is usually best ) , do you need to append , and compression .
and I'm trying to drop a column ' red_id'from a dataframe because i want class average for pass / fail in for loop as shown above .
Use of pandas.shift() to align datasets based on scipy.signal.correlate
with a positive shift of ` 410 ` .
I think I am just not understanding how ` pd.shift() ` works , but I was hoping that I could use ` pd.shift() ` to align my data sets .
As far as I understand , the return from ` correlate() ` tells me how far off my data sets are , so I should be able to use shift to overlap them .
` panda.shift() ` is not the correct method to shift curve along x-axis .
How to use argmin with groupby in pandas
I could , of course , come up with some horrible hacky thing in standard python where I iterate over all values of cat , then select the subset of my data corresponding to that value , perform the argmin operation then figure out where in the original dataframe that row was .
` argmin() ` is not an agg function , you can use apply to get the closest index of every group : #CODE
Pandas : Merge or join dataframes based on column data ?
If I try a join to the existing dataframe like the simple one I've used previously , I end up with the new columns but they are full of NaN values .
I've also tried various combinations using Name1 and Name2 as either columns or as indices , with either join or merge ( not as random as it sounds , but I'm clearly not interpreting the documentation correctly ! ) .
Then you could perform the ` join ` as usual .
I'm trying to append a level to a Pandas Series .
` series2 ` now has a multiindex with two levels .
Or just start with a ` MultiIndex ` altogether : #CODE
Are you trying to apply a function to each row by taking arguments from different columns ?
I add 4 columns in 4 difference apply steps , and then when I try to add another column I get this error .
using apply with a function that returns a list will try to coerce this to a Series , thus it needs the same length as the original lenght , OR a scalar ( including None ) .
Output in your question is not the one you get from apply .
@USER I think that the output is the output from apply because apply will run each row through func=random , and that func will print out [ 1 , 2 , 3 , 4 ] .
I had this issue , and my solution was just to join my list into a string ... then split it after the apply .
and then use apply : #CODE
I add the column ' E ' and then I do apply .
The problem that I am working on starts with a dataframe with one column and then I keep doing apply to the dataframe to add columns .
This is quite easy if I can just pass it a tpts x feats matrix , but right now it seems that the only way to do it is column-wise using " apply "
apply also works on entire dataframes .
So , it's not that I want to apply a function to every item in the dataframe , it's that I want to pass the entire contents of a dataframe to a function .
For example , the hilbert transform in scipy takes a timepoints x features array , and computes the transform along the first axis .
It is faster to pass a 10,000 by 100 matrix to this function than it is to pass 100 separate 10,000 length columns to this function , which is what would happen if I used " apply " .
You should transpose the dataframe first before setting the index : #CODE
Trying to drop NaN indexed row in dataframe
I want to drop the row with the NaN index so that I only have valid site_id values .
I've found that the easiest way is to reset the index , drop the NaNs , and then reset the index again .
` Index ` has no ` labels ` attribute , The suggested drop leads to ` AssertionError : axis must be a MultiIndex ` .
Indices don't have ` isnull ` , so we convert to a ` pandas.Series ` .
` isnull ` generates a boolean mask , e.g. ` [ False , False , True ]` , where True denotes the value at the corresponding position is null ( ` numpy.nan ` or ` None `) .
We then select the rows whose index corresponds to a true value in the mask by using ` df [ boolean_mask ]` .
I've also tried as many variations of merge , concat , and join as I can think of .
Your use of ` join ` is throwing an error because ` site_id ` isn't exactly a column in this case - it's the DataFrame's index .
By default , ` join ` assumes you want to join the two frames on their indexes .
Alternatively , you could use ` merge ` : #CODE
Merging two dataframe using ID and asof
I have two data frame that I wish to join together where the left dataframe have information index by ( date , ID ) and the right dataframe has information index by ( period , ID ) , where period is year-month .
I end up doing a group by ID for the left frame , iterating thru the groups , select the same group on the right frame , and then doing and asof operation for the index of the group from the left dataframe , like so : #CODE
Does anyone have a way to make this merge function faster and more efficient ?
Am I stuck with using a for loop to apply the boolean ' and ' operation on all columns ( i.e. from column A to column Z ) ?
plus the selected rows usage x 2 , which will happen when you concat the rows
after the concat the usage will go down to selected rows usage
I keep trying different things like ` merge ` , ` concat ` , ` dstack ` , etc ...
I'm getting ` RRuntimeError : Error in layout_base ( data , cols , drop = drop ) : At least one layer must contain all variables used for facetting `
The first error says that ` list ` object doesn't have attribute ` ix ` ( and it doesn't have , in fact ): #CODE
Lists and NumPy arrays do not have an ` ix ` method : #CODE
But Pandas Series and DataFrames do have an ` ix ` method : #CODE
Actually , it's strange that there's no possibility to pass ` how= ' cross '` to merge or join methods .
`` reindex `` , then `` fillna `` on individual columns as needed
I'm currently doing it with ` groupby ([ ' UserID ' , ' Date '])` and a for loop , where I drop users with only one result , but I feel like there is a much better way to do this .
` Series.nunique() ` is slower than ` len ( Series.unique() )` , can we consider this as a bug ?
The you can drop users with only one count .
Have you ever used the percentile numpy function when using the pandas function resample ??
When i resample : #CODE
As you see my initial date is 2009-11-17 14:14 : 00 but resample day start at 2009-01-12 .
my dataset is just 30min interval records and with resample i just want to make more readable my data ( 14:14 --> 14:00 , 14:44 --> 14:30 )
Hi @USER , I am getting ` OverflowError : Python int too large to convert to C long ` when I apply your solution .
I m trying to obtain the rolling mean ( window=2 ) , but without considering the NaNs , so , I use the nanmean function of scipy.stats : #CODE
Any reason you aren't using pandas ' rolling mean ?
I think that pandas rolling mean will cover the case you gave .
You can pass specific columns to drop from as an argument .
To get what I think you are after reindex for the entire period , select , then dropna .
That said , the loc behavior of raising is correct , while the [ ] indexing should work ( maybe a bug ) .
I think the problem is trying to perform the reindexing with the groupby / apply method , but I don't kow what the alternative is .
Then group the original ` DataFrame ` by ID and join on the date- ` DataFrame ` using ` outer ` , to pull in any missing dates ( and consequently , ` x ` and ` y ` will be ` NaN ` for the rows where the dates had to be pulled in with the join ) .
I had to modify the DIFF lines to add ( and change case on the " or " but in spite of that not working EG : quarter_score_diff = region [( region.Quradate == most_recent_date ) or ( region.Quradate == last_quarter )] .diff() getting ::
Firstly I would reindex so that region appears in the dataframe as columns , then I would do the following : #CODE
Basically all you need is for a diff from prior period #CODE
and for a diff from a year ago ( 4 quarters ) #CODE
Or you can use apply : #CODE
As of version 17.0 , you can format with the ` dt ` accessor : #CODE
I have a Pandas DataFrame that includes rows that I want to drop based on values in a column " population " : #CODE
here , I want to drop the two rows that have ` sibling ` as the value .
It does drop 2 rows , as you can see in the resulting value counts , but they were not the rows with the specified value .
` dataFrame.drop ` accepts an index ( list of labels ) as a parameter , not a mask .
To use ` drop ` you should do : #CODE
You can do a similar thing with a condition over multiple columns : ` data = data [( data [[ ' col1 ' , ' col2 ' , ' col3 ']] ! = 0 ) .all ( axis=1 )]` - to drop all rows with zeros in at least one of those columns .
I had this exact problem with a stalled computer during load ( 300 MB Stata file but only 8 GB system ram ) , and upgrading from v0.14 to v0.16.2 solved the issue in a snap .
But for some reason when I try do the total number of entries per day , it returns a a multiindex series instead of a dataframe : #CODE
I can fix the data so it's easily plotted with a simple unstack call , i.e. ` df.resample ( ' D ' , how= ' count ') .unstack() ` , but why does calling resample with ` how= ' count '` have a different behavior than with ` how= ' sum '` ?
It does appear the ` resample ` and ` count ` leads to some odd behavior in terms of how the resulting dataframe is structured ( Well , at least up to 0.13.1 ) .
You can fix the issue by specifying that ` count ` applies to the ` volume ` column with a dict in the ` resample ` call : #CODE
Read agg function documentation as a first step
Let me build up the expression ` dfg = df [[ ' bin ' , ' col7 ' , ' col11 ']] .groupby ( ' bin ') .agg ( { ' col11 ' : [ np.mean ] , ' col7 ' : [ count_ones , len ] } )`
I now want to apply some aggregate functions to the records in each of my bin groups ( An aggregate funcitn is something like sum , mean or count ) .
Now I want to apply three aggregate functions to the records in each of my bins : the mean of ' col11 ' , the number of records in each bin , and the number of records in each bin that have ' col7 ' equal to one .
The number of records is also easy ; python's ` len ` function ( It's not really a function but a property of lists etc . ) will give us the number of items in list .
So I now have ` dfg = df [[ ' bin ' , ' col7 ' , ' col11 ']] .groupby ( ' bin ') .agg ( { ' col11 ' : [ np.mean ] , ' col7 ' : [ len ] } )` .
Now that I have my ` count_ones ` function I apply it to ' col7 ' by : ` dfg = df [[ ' bin ' , ' col7 ' , ' col11 ']] .groupby ( ' bin ') .agg ( { ' col11 ' : [ np.mean ] , ' col7 ' : [ count_ones , len ] } )`
Remember way back when I did ` df.ix [ df.col11 == ' x ' , ' col11 '] =- 0.08 ` that was so all my invalid data was treated as a number and would be placed into the 1st bin . after applying group by and aggregate functions the mean of ' col11 ' values in my first bin will be - 0.08 ( because all such values are - 0.08 ) .
` dfg.ix [ ' x ' , ( ' col11 ' , ' mean ')] = ' N / A '` means in dfg where index ( or row ) is ' x ' and column is ' col11 mean ') set the value to ' N / A ' . the ` ( ' col11 ' , ' mean ')` I believe is how pandas comes up with the aggreagate column names i.e. when I did ` .agg ( { ' col11 ' : [ np.mean ] } )` , to refer to the resulting aggregate column i need ` ( ' column_name ' , ' aggregate_function_name ')`
Can't find a way to unstack a dataframe
So upon getting one dF with just current date and one with just last quarter's date the diff of the two dataframes would look like
I find this helpful to see what is actually passed to the apply , which in this case is a frame #CODE
For each region , show me the column-wise diff from 2 periods ago , among the scores #CODE
What I'm trying to create is R1 diff from prior quarter followed by R2 diff from prior quarter .
I don't get how the lambda knows to go one back for the diff , I also need one to go back 3 for the diff to get the diff from a year ago ...
I added some helpful logic , and show how to use diff .
Can you enable the debugger to get a stack trace ?
The ` drop_level=False ` solution in Pandas 0.13 will allow me to achieve the same result , but I am still wondering why I cannot get a view from the MultiIndex DataFrame .
I guess with the last question above I'm wondering what the current best solution is to index into arbitrary multiindex slices and cross-sections .
Based upon the syntax used with transform and apply it seems to me that the following should work just fine : #CODE
But then of course it could be better just to cut out the middle man function all together and simply use an array with list comprhension ....
Passing arguments to ` apply ` just happens to work , because ` apply ` passes on all arguments to the target function .
This is pivot .
Pivot requires an index , but I cannot tell it to keep the current ones ( I assume that whichever function did it would ask me what to put on the blanks- 0 vs . NA for example )
Thanks @USER for test case , you can pivot with : #CODE
@USER great , if I just had a bit more practice with pandas :) tried pivot , but didn't want to add index as column and decided that this is not possible to do with pivot
Comparison and join of two different data frames with strings ?
I would like to compare and then join two data frames with strings based on a common sequence within the strings .
and apply this custom converter #CODE
and apply this conversion function : #CODE
May be some conversion happens when you load data , try check what dt [ ' Time '] output is .
I am trying to normalize my timeperiods for each index in x .
` np.shape ( x ) = ( 814061 , 1 )` & ` len ( x.ix [8 498857 ] [ ' received ']) = 2 ` & ` len ( x.ix [8 536896 ] [ ' received ']) = 3 ` ** The index is only 100 unique numbers of different length timestamps ** If that helps .
` x.ix [8 536896 ] [ ' received '] = 2013-09-09 16:27 : 54 , 2013-09-09 17:56 : 15 , 2013-09-09 15:01 : 35 ` and I want to normalize to [ 0 , 1 ] start and stop for all indices .
Then I want to resample and I'm getting strange string concatenations since the numbers are still string #CODE
I apply some functions and generate a new column values to a existing column of Pandas dataframe .
Is it the wrong way and what is the accurate way to apply such operation ?
I don't know why the index method has inconsistent behavior while doing column-wise apply function .
And I want to apply lambda to the second columns , it it saying the Series object can not be apply ?
When you use ` apply ` on a series , your function is called on each element .
When you use ` apply ` on a DataFrame , your function is called on each column .
and then you can use apply ( you don't have to use ` lambda ` , BTW ): #CODE
If you want to apply ` upper ` to DataFrame , you can use pandas.applymap() : #CODE
I could add the actual code I am using to aggregate , but the functional parts are basically the same as what is presented here .
So I'd like a pivot table with a rows as schools , columns as subjects and values as fraction passing .
You can also use map function instead of generator : #CODE
What I intend to do is to split the dataframe ( data ) into smaller dataframes and append these to a list ( datalist ): #CODE
Here's a groupby way ( and you could do an arbitrary apply rather than sum ) #CODE
I would sort the dataframe by column `' name '` , set the index to be this and if required not drop the column .
I would like to extract the value from the measured data which is occurs closest to the hour mark to join with the modelled data .
Does anyone know of an easy way to align these without looping through all the data ?
Although , I just copied the data from your screen , re-created df from your data and tried to resample using ' ohlc ' .
how to replace elements in a column using pandas
I'd like to replace the elements of ` value ` with ` zero ` but only for the words that match those in ` b ` .
Or you can use ` ix ` selector : #CODE
Append is similar to one of python ` list ` , you will get both dataframes " stacked " together .
Whether a ` ValueError ` is raised in case of indexes with duplicates is controlled by ` verify_integrity ` param to ` append ` , defaulting to ` False ` .
For the reference , here is extensive guide on different merge and join means of dataframes
What I really need is a veryify_integrity=True except it skips those that are duplicates but it also checks every column to make sure it is a duplicate not only in index but also content and if it isn't then it will join the df as well
Not sure how else to describe it but what I want is : Merge two dataframes even if they have the same index , but if they do have the same index I want to not merge when all the resulting columns also match .
So merge all but those rows that are exact duplicates
Append wouldn't work as it keeps all the and I'm dealing with like 10m rows that are the same and 2k updated rows and 200 rows earlier than the end of the first df also being different .
Here is a way to create a json format , then literally eval it to create an actual dict #CODE
` map ( list , df.values ) == ast.literal_eval ( df.to_json ( orient= ' values '))` evals to ` True `
If you're sure your leafs won't overlap , replace last line #CODE
Difference between map , applymap and apply methods in Pandas
I see that ` map ` is a ` Series ` method whereas the rest are ` DataFrame ` methods .
I got confused about ` apply ` and ` applymap ` methods though .
DataFrame s apply method does exactly this :
so using apply is not necessary .
The reason for the name applymap is that Series has a map method for applying an element-wise function :
Summing up , ` apply ` works on a row / column basis of a DataFrame , ` applymap ` works element-wise on a DataFrame , and ` map ` works element-wise on a Series .
strictly speaking , applymap internally is implemented via apply with a little wrap-up over passed function parameter ( rougly speaking replacing ` func ` to ` lambda x : [ func ( y ) for y in x ]` , and applying column-wise )
Since ` map ` and ` applymap ` both work element-wise , I would expect a single method ( either ` map ` or ` applymap `) which would work both for a Series and a DataFrame .
@USER mentioned that apply works on row / columns , while applymap works element-wise .
But it seems you can still use apply for element-wise computation ....
So when apply pushes np.sqrt on each columns , np.sqrt works itself on each of the elements of the columns , so you are essentially getting the same result as applymap .
Adding to the other answers , in a ` Series ` there are also map and apply .
Apply can make a DataFrame out of a series ; however , map will just put a series in every cell of another series , which is probably not what you want .
Also if I had a function with side effects , such as " connect to a web server " , I'd probably use ` apply ` just for the sake of clarity .
You can compute it using ` map ` .
Python Pandas - how to do group by on a multiindex
I assume I want to drop the index , and create date , and category as a multiindex then do a groupby sum of the metrics .
My problem using the ` resample ( ' H ' , how= ' ohlc ')` method is that it provides values within that hour and I want the value closest to the hour .
It is not code but I think the logic is right ( or at least close .. after you find the mins ,, then you can select them and then do your resample ..
are you trying to implement inner join ?
Transpose a matrix - ` zip ( *matrix )`
Using ` pandas ` , you can view what you want as a corrupted pivot table .
Then we make a pivot table , using the identity function as the aggregation function because we're only reshaping .
This produces a ` DataFrame ` with a ` MultiIndex ` for the column index : #CODE
We can use ` transform ` to broadcast the size of each ` dateuse ` / ` station ` combination up to the length of the dataframe , and then select the rows in groups which have length > 1 .
Using pandas to merge data and output Json
I know I can do it without Pandas , but I'm wondering if I can do it in Pandas . for example can you do a join like ` iq between iq_start and iq_end ` in pandas ?
I can reproduce in 0.12.0-559-ga11e143 , and the problem goes away if I renumber the columns with ` q.columns = range ( len ( q.columns ))` .
Better way to aggregate timestamped data ?
I'm wondering what the best way to aggregate by time periods is .
This allows me to aggregate easily with ` groupby ` like so : #CODE
For simple statistics , you can use the resample method on a DataFrame / Series with a datetime index .
Alternatively you can use 2 dataframes , indexed the same , and combine / merge when needed .
A list of freq aliases is here : #URL
Create a series with ` [ 0 , 1 , ... ]` as the values and then call ` resample ` : #CODE
append columns of a data frame to a different data frame in pandas
I'd like to append the columns ` val1 ` and ` val2 ` of ` df2 ` to the data frame ` df1 ` , taking into account the elements in ` c1 ` .
@USER :) : You need to include the ` c1 ` column in ` df2 ` -- otherwise ` pd.merge ` will not know on what shared column to merge .
How can I do a hist ?
how to append every row of pandas dataframe to every row of another dataframe
I know I can use iteration to append columns of df2 to each row of df1 , but I am wondering whether there are some more efficient way to do this in pandas , like its concat functions .
You can emulate this with merge in pandas : #CODE
@USER I've opened an issue here #URL I think it's easy to add parameter value how= ' cross ' to merge and join .
In case it's not possible to have a shape like that , the following options for obtaining a good a x b shape would be acceptable for me : fill missing elements with NaN ; truncate the longer rows to the size of the shorter rows ; fill the shorter rows with repeats of their mean value .
I use len function .
It's much faster than empty() . len ( df.index ) is even faster .
You can do vectorized comparisons on the column index directly without using the ` map ` / ` lambda ` combination .
We can replace those ` Versions ` to a common name with : #CODE
You can use the ` diff ` method : #CODE
DataError : No numeric types to aggregate
What you are looking for is the ` melt ` function #CODE
I'm using a pretty complex pandas script to create a corrupted pivot table , but I'm still flailing on figuring out all of panda's shortcuts .
Have a look at the [ ` replace `] ( #URL ) method to standardize the naming .
The ` if ` tries to call ` bool ` on that series , basically , but there's no canonical way to decide whether an array of multiple bools should be considered True or not .
The first Series is very large and has a MultiIndex .
The speed difference is particularly noticeable when ( as here ) the Index is a MultiIndex .
A good way to think about this is that columns and index are the same type of object ( ` Index ` or ` MultiIndex `) , and you can interchange the two via transpose .
Note : this attribute is just a list , and you could do the renaming as a list comprehension / map .
You must first use ` Index.rename() ` to apply the new index level names to the Index and then use ` DataFrame.reindex() ` to apply the new index to the DataFrame .
I think `` apply ( ... )`` could be achieved more simply by `` replace ( dir_dict )`` .
I haven't tested that , but I think that's how replace works .
Also I'll check if replace could be modified to replace missing values to nan
The standard workaround is to do this using transform , which in this case would be something like : #CODE
Now I want to normalize ` x ` by row --- that is , divide each row by its sum .
You can apply a function that tests row-wise your ` DataFrame ` for the presence of strings , e.g. , say that ` df ` is your ` DataFrame ` #CODE
This will produce a mask for your DataFrame indicating which rows contain at least one string .
You can hence select the rows without strings through the opposite mask #CODE
You can take the transpose , call ``` convert_objects `` , which works columns-wise , and then compare the data types to get a boolean key like this : #CODE
I use groupby to identify my cartesian-product-like scenarios ( scenario = params ' A ' : ' E ' set / constant ) , then using aggregate to calculate the mean within this scenario ( param ' F ' is still a degree of freedom ) .
Pandas merge hierarchical dataframes and maintain hierarchy
and then you apply it row by row .
I was able to do this in the DataFrame using a lambda function with map ( lambda x : x.lower() ) .
Also when I try to isolate the column in series with something like series [ ' A '] should it return the index ( although I guess this makes sense ) because I get a float error even though the values that I want to apply the lower method to are strings .
Tbh I think the for loop could be the best / most efficient way from a dict , although join feels " cleaner " .
What you need is the ` join ` function : #CODE
Indices from the original dataframe will be used as if you had performed an in-place left join .
You can also do ` groupby ( ..., as_index=False )` , though buggy with apply in 0.12 , fixed in 0.13 .
You should use loc and do this without chaining : #CODE
See the docs on returning a view vs a copy , if you chain the assignment is made to the copy ( and thrown away ) but if you do it in one loc then pandas cleverly realises you want to assign to the original .
Equivalent of transform in R / ddply in Python / pandas ?
In R's ddply function , you can compute any new columns group-wise , and append the result to the original dataframe , such as : #CODE
In Python / pandas , I have computed it first , and then merge , such as : #CODE
You can add a column to a DataFrame by assigning the result of a groupby / transform operation to it : #CODE
To add multiple columns , you could use ` groupby / apply ` .
Make sure the function you apply returns a DataFrame with the same index as its input .
is it feasible to calculate multiple columns once , such as ` ddply ( mtcars , . ( cyl ) , transform , n=length ( cyl ) , total_wt=sum ( wt ))` ?
How to fix error in pandas groupby : No numeric types to aggregate
Why not use dictionary to map ** i ** to ** dir** ?
So the question finally : is there a way to pass ( name , function ) tuples when using agg with UDFs that have multiple arguments without wrapping the function in a lambda ...??
Formerly what I am looking for is to index by the argmax of a column over another column .
( The ` sort=False ` is there just to preserve the original horse order ; if you don't care about that , you can drop it . )
EDIT : I put some print statements in my function and also passed len as the grouping function to see how things changed .
I thought that transform would pass each column as a series to the transform function .
Is anyone able to explain to me what is actually be passed to the transform function ?
I think what this exercise has showed me is that I don't fully understand what is going on with the transform oepration .
When passing a function to agg , what if the aggreation function that is passed needs to consider values in columns other than those that are being aggregated .
Suppose you wanted to aggregate the data set into a single day / seller observations , and you wished to do so based upon which product sold the most volume .
To be clear , this would be simple for the volume measure , simply pass a max function to agg .
I am able to get the result I want by using the index values in the column that is passed to the function when agg is called and referencing the underlying data frame : #CODE
Something tells me no , as agg passes each column as a series to the aggregation function , rather than the dataframe itself .
Maybe this is not what agg was intended for .
Maybe I should be using apply ...
Also , am I right that passing a column to the agg func that references a column that is not being aggreated is not possible ?
I can't think of a good way to do what you want with ` agg ` , but people are always coming up with ways to do things I can't figure out , so that doesn't prove anything .
Use the string methods to truncate to the first 4 ( assuming this won't cause any false matches ) .
Loop through the each of the unique locations , check for matches ( the first 3 letters from the other frame ) and replace when there's a match .
@USER may be you can just cut column names to 4 letters in both DataFrames
You could use fuzzywuzzy to do the matching of town names , and append as a column to df2 : #CODE
Now you can merge ( on the overlapping columns ): #CODE
Note : to force the dtype to object ( and have mixed dtypes , ints and floats , rather than all floats ) you can use an apply .
Two notes : First , given how easy it is to use cdf1.Local.value_counts() or len ( cdf1.Local.value_counts() ) etc .
But now I have another error which says " ValueError : Currently , you need to give freq if dates are used .
" I suppose that " freq " is another argument that I need to include in AR() .
But what does " freq " mean ?
Currently there is a ` median ` method on the Pandas's ` GroupBy ` objects .
I am trying to merge to Dictionaries on a common key ( b ) .
Pandas resampling is really convenient if your indices use datetime indexing , but I haven't found an easy implementation to resample by an arbitrary factor .
E.g. , just treat each index as an arbitrary index , and resample the dataframe so that its resulting length is 4X shorter ( and being more intelligent about it than just taking every 4th datapoint ) .
For example , in my case I want to resample an audio vector from 44KHz to 11KHz .
You can use ` DatetimeIndex ` to resample high frequency data ( up to nanosecond precision , caveat : I believe this is only available in the upcoming 0.13 release ) .
I've successfully used pandas to resample electrophysiological data in the 24KHz range .
I'm still learning pandas , so some parts of this code may be not so elegant , but general idea is - get all notes columns , find all urls in there , combine it with ` URL ` column and then concat remaining notes into ` Notes1 ` column : #CODE
You can apply this per column , but much easier just to check the dtype . in any event pandas operations exclude non-numeric when needed . what are you trying to do ?
` ix ` index access and ` mean ` function handle this for you .
Fetch the two tuples from ` df.ix ` and apply the mean function to it : non existing keys are returned as nan values , and mean ignores nan values by default : #CODE
If I may suggestion a some improvements unrelated to your question : 1 ) use ` for i in range ( len ( df ))` .
Usually I ignore them , or truncate them as Tom suggested .
This is now pretty competetive with this PR : #URL ( going to merge for 0.13 shortly ) #CODE
This answer solves this toy example and will be enough for me to rewrite my actual function , but it does not address how to apply a previously defined function without rewriting it to reference columns .
But if you don't want to rewrite your function , you can wrap it into anonymous function inside apply , like this : #CODE
I would like to convert that into an hourly time series and interpolate missing values ( 15:00 ) so that I end up with : #CODE
How do I convert / map the dataframe data to a time series in Pandas ?
I need to replace the values in the dataframe with the corresponding values from the map , but keep the original value if the it doesn't map to anything .
If I then take the diff of this series : #CODE
instead of ` diff ( timeSeries )` , use ` timeSeries.diff() ` #CODE
In my smaller example for bheklilr if you look at the testing.txt the first line ( one of the 3 things it matches against testing.csv ) it's got ` Mdata ` where as in my csv it's got ` Mdata ( D )` , I want to strip the ` ( D )` parentheses data from the csv MATCH2 column and then match , it doesn't even need to be a regex to know what to strip as there are only 2-3 variations of parentheses data that I need to temporarily strip .
I often find I can map a step in a process to a sequence of vectrorized methods on a Series or DataFrame .
would calling pd.to_datetime from apply allow for easier parralelization after import or would you still have to manually split up the data frame into N / M parts ( N = num rows , M = num logical procs ) and execute afterward ?
Looping a merge for a amount of csv files using pandas
As of right now I'm able to get the files but I'm unable to determine a way to develop a way to iterate a data frame over each read csv and then merge all of these data frames together and push out a csv file .
ID's are different in two files , so do you want to join on ID or just add new columns without looking at ID column at all ?
How to align the bar and line in matplotlib two y-axes chart ?
How to shift the line to align with the bar at the same x-tickers ?
to align them , and the plot now looks ugly :
What I would like would be to have all multiindex combinations , even those that have no data for all column values .
I've found that reindex work well for this , you just have to create full index :)
I disagree -- I think you are just unfamiliar with the right way to stack data in a DataFrame .
You can pivot your DataFrame after creating : #CODE
Whereas , just using ` pivot ` , it's purely based on column convention as to what semantic entity it is that you are reshaping .
Generally , the main use case for doing what you're trying to do with ` pivot ` is when you're formatting some table so that it prints nicely to the screen , or is exported nicely to HTML , LaTeX , or .csv , or something .
They are essentialy the same , ` groupby ` method of a DataFrame and a ` aggregate ` method of ` groupby.SeriesGroupBy ` object .
Python Pandas fuzzy merge / match with duplicates
I tried following this thread : is it possible to do fuzzy match merge with python pandas ? but keep getting index out of range errors ( guessing it doesn't like the duplicated names in fundraisers ) :( So any ideas how I can match / merge these datasets ?
One way would be to use ` transform ` : #CODE
There's too much stuff to remember , but you pick up rules like " transform is for per-group operations which you want indexed like the original frame " and so on .
To return all the rows with the max value , you should use the mask : #CODE
I have tried with loc and map , but I don't manage to find a solution .
Use apply : #CODE
for some reason , when Im setting the index as shown , while to me it seems both layers are sorted , calling ` df.index.lexsort_depth ` command returns 1 , and i get the following error when trying to access with two keys : ` MultiIndex lexsort depth 1 , key was length 2 `
( e.g. Take the ` if ` out of the ` add_area_column ` and put it where you would call ` apply `)
these edge cases for apply are pretty tricky ... to fix your issue , don't use apply : df [ ' width '] * df [ ' height ']
I started by adding a date_order column this ` data [ ' date_order '] = 1 ` and then I tought I could increment over with an iterator using a lambda function and the map function .
Use ternary operator in apply function in pandas dataframe , without grouping columns
How can I use ternary operator in the lambda function within ` apply ` function of ` pandas ` dataframe ?
you can also replace ` ( x == 4 ) .astype ( float )` with ` ( x == 4 ) *1 ` which , for large arrays has been a bit faster in my tests but I'm not sure how generalizable it is .
` shift ` is much better than going down to ` values ` .
Be careful ; depending how your prices data is sorted by date ( asc , desc ) , the shift example would be wrong .
Python pandas time series resample function extends time index
I'm playing around with some financial time series data in pandas , and I'm trying to take a resample of some timestamp data .
And when I try a 40-minute resample , the time range is expanded : #CODE
Then you can use some regexp to replace the default column separators with your delimiter of choice .
select from hdf5 apply function ( e.g. mean )
You would have to drop down to pytables by using the ` store._handle ` to get at the data that is needed .
I want to combine ` MEETING DATE ` and ` MEETING TIME ` into one column . datetime.combine seems to do what I want , however , I need to apply this function column-wise somehow .
perhaps you could ` apply ` the function ( or anyfunction you want ) to MEETING DATE and MEETING TIME #URL
You can use apply method , and apply combine like this : #CODE
Get a bool series by compare the difference with 2 hours .
cumsum() the bool series to get a series that can split the dateframe .
call groupby and apply to get the begin and end datetime for every group .
One way is to do a merge : #CODE
( If you still want to do it then replace the NaN with 0 using ` .fillna ( 0 )`) .
Other ways is to use ` get_level_values ` to access ` MultiIndex ` by level #CODE
This was a bug and has been fixed in master ( 0.13 ) , a temporary workaround is to use ix ( ! ): #CODE
awesome , thanks , I searched tracker , but haven't found it . and forgot ix method too :)
join three pandas data frames into one ?
If you are trying to join Series objects rather than Dataframes , then you want #CODE
now I have the following error : AttributeError : ' Series ' object has no attribute ' join '
I see pandas has the rolling_std function which can calculate the rolling standard deviation but just on one column .
How do I get Python Pandas to calculate a rolling standard deviation on the ' High ' , ' Low ' , ' Open ' and ' Close ' column for say 20 periods ?
Then what I did was try and use you above suggestion to to put the rolling standard deviation into the new column that I created using : df [ ' 20SD '] = rolling_std ( df [[ ' Open ' , ' High ' , ' Low ' , ' Close ']] , 20 ) ... that didn't work and gave this long error ending with " ValueError : Wrong number of items passed 1 , indices imply 4 "
I'm trying yo interpolate the value ` value ` of column ` result_column ` using ` index_column ` .
Then i append a new row with the index ` value ` #CODE
I am not sure what you are trying to interpolate .
You can also try to strip the json from outputs , it will decrease notebook size .
What is the best way to apply a function over the index of a Pandas ` DataFrame ` ?
The only trick with this is that your index needs to have unique labels b / w different multiindex levels , but maybe someone more clever than me knows how to get around that .
I would like to append a string to the start of each value in a said column of a pandas dataframe ( elegantly ) .
I'd like to insert a link ( to a web page ) inside a pandas table , so when it is displayed in ipython notebook , I could press the link .
It's going to depend on the size of you DataFrames which is faster , but another option is to use join to smash your multi line " JSON " ( Note : it's not valid json ) , into valid json and use read_json : #CODE
Note : of that time the join is surprisingly fast .
I'm trying to reindex , with ` myFrame .set_index ( ' A ')` , so that I get #CODE
I'm not sure what the ` A ` is actually representing because it doesn't appear in ` myFrame .columns ` or ` myFrame .index ` and doing ` myFrame [ ' B '] [ 0 ]` gives me ` 232131 ` , so what is ` A ` in this reindexed data frame and how can I index correctly from the beginning or get rid of this strange ` A ` in the incorrectly reindex data frame .
The names describe the index , and give some meaning to the index , and also distinguishes between different levels in the index ( in a MultiIndex ) .
@USER ` TypeError : ' bool ' object is not callable `
is there a way to make pandas strip elements in CSVs ? that would seem like a fairly common task ( one I just expected to be built in ) .
construction_error ( len ( arrays ) , arrays [ 0 ] .shape [ 1 :] , axes )
tuple ( map ( int , [ len ( ax ) for ax in axes ]))))
I want to create a third dataframe that would be the equivalent of a minus in SQL : dropping all the observations in the intersection of the two dataframes .
Conditional merge for CSV files using python ( pandas )
I am trying to merge ` =2 ` files with the same schema .
- I need to merge multiple CSV files with same schema but with duplicate rows .
- Where the row level merge should have the logic to pick a specific value of a row based on its value .
One way to smash them together is to use merge ( on store_id and number , if these are the index then this would be a join rather than a merge ): #CODE
I get this error on merge : ` ValueError : ' array is too big .
I need to merge res with df3 ?
To get it with more than two frames use join ( passing list of frames and the on value )
How about use ` concat ` , ` groupby ` , ` agg ` , then you can write a agg function to choose the right value : #CODE
And then check for where extra1 or extra2 isnull , keep those rows , and then drop the extra rows .
If you want to drop the bad lines , you might be able to use ` error_bad_lines=False ` ( and ` warn_bad_lines = False ` if you want it to be quiet about it ): #CODE
Note : I would probably clean up the indentation to make this more readable ( you can also replace df.type with type in most cases : #CODE
There are simple solutions to this : iterate over id's and append a bunch of dataframes , but I'm looking for something elegant .
I'd like to do a #URL merge from my original dataframe to a template containing all the ages , but I would still have to loop over id's to create the template .
Don't know , may be there's more elegant approach , but you can do something like cross join ( or cartesian product ): #CODE
This is exactly what I was looking for , and I guess I even said the words many to one in my question , but I didn't understand that you could merge like that
And after that you can use pandas.DataFrame.apply function , with axis=1 ( means apply function to each row ): #CODE
Thanks but its not working , Its throwing a traceback and which says : AttributeError : DictReader instance has no attribute ' apply ' , is it because i am reading this as a dictonary ?
@USER oh , it will in 0.13 - there was a bug in apply maybe .
e.g. ` df.word.value_counts() [ ' myword ']` is about twice as fast as ` len ( df [ df.word == ' myword '])` .
This will be significantly more efficient than an apply or using lists ...
The following code works for me as PLOTs , but when I change to HISTs , they all stack on top of each other .
Is this a bug in Pandas or are plot and hist not interchangable ?
Plot and hist aren't interchangabe : hist is its own DataFrame and Series methods .
For any rows with a NaN in a Pandas DataFrame , shift by 1
What I'm trying to figure out is for the columns that have a NaN in the last row ( this being row i , I would like to shift those columns by 1 . )
First , find out which columns we need to shift , and then replace those columns by the shifted versions .
it seems that in your case a local disk is so fast that it's not worth the time to create a table , unless you are doing a lot of queries ( and don't append )
I am trying to merge time-course data from different participants .
It might be better to just add the ID as a column , then add to index when you concat ?
Maybe you can use ` keys ` argument of ` concat ` : #CODE
If you want to append other dataframes later : #CODE
This way another concat cannot be made afterwards , which the question seemed to imply would be necessary .
So , e.g. I don't see how you could easily append ` df4 = pd.DataFrame ( np.random.rand ( 3 , 2 ))` with a new key `" D "` after having done the above .
Is there a way to aggregate the number of days that occured per month : Year for each interval for each ID .
To break it down , for each row we are calculating the days in each month by creating a Series of 1s for every day between the start and end , then summing them up for each month using resample : #CODE
If you have an arbitrary number of places to split at , then you can use a boolean Series / shift / cumsum / groupby pattern , but if you can get away without it , so much the better .
If you use ` apply ` on the groupby , the function you pass is called on each group , passed as a DataFrame .
I'm not aware of a way to exclude the result of ` apply ` only for certain groups .
There is no built-in queue-behaviour as far as i know ( but you could write your own wrapper around a pandas data-frame using the concat method and / or using index-slices )
Could you possible replace the NULLs with a minimum date value or similar ?
The NULL values should translate to a NaT value in Pandas .
python pandas - replace number with string
You could also try to use a " map " function .
pandas group by n seconds and apply arbitrary rolling function
The accelerometer data is not uniformly sampled , and I want to group data by every 10 or 20 or 30 seconds and apply a custom function to the data group .
If the data was uniformly sampled , it would have been easy to apply a rolling function .
However , since it is not , I want to apply groupby using timestamp interval .
However , I cannot figure out how to group by an arbitary number of seconds and then apply a function to it .
Maybe you could apply a custom resampling-function instead of using the groupby-method .
python : shift column in pandas dataframe up by one
shift column gdp up : #CODE
Which will transform any value in the list to the boolean true / false .
In this particular case , since the returned result is a Series , you can make use of a trick using ` unstack ` : #CODE
I might have done something like ` agg_df.reset_index() .groupby ([ " id "]) .first() [ 0 ] .tolist() ` , but the ` unstack ` trick is nice .
Yes , for small data , reindex is much quicker than dict constructor , but i didnt test for the big data yet .
Thus I wish to cut R out of the loop completely .
This is pivot , if I get your idea right , with pandas it will be as follows .
pivot is based on this answer #CODE
Then perform the pivot ?
@USER It's not that you asked poorly , it's that I wasn't familiar with a pivot table until alko used the word and I was able to look it up :P
Reindex sublevel of pandas dataframe multiindex
I have a time series dataframe and I would like to reindex it by Trials and Measurements .
Strangely enough , after I run you code , df [ ' Trial '] no longer works :( which is a pity because I wanted to do this in order to better apply the same function to multiple trials ( I want to downsample all of the measurements in every trial to two - just two ) .
If you want to replace the index with simple sequential numbers , use ` df.reset_index() ` .
I want to resample my data so that every trial has just two measurements
I know pandas has a resample function , but I have no idea how to apply it to my second-level index while keeping the data in discrete categories based on the first-level index :(
What's your resample rule ?
and filter needed rows with ` ix ` : #CODE
@USER you can use sort=False param , and ix / idxmax instead of max . see updated code
If you copy the output of printing , most of the time answerers can use read_clipboard() ... except for MultiIndex : s .
if it's relevant that you have Timestamp columns , e.g. you're resampling or something , then be explicit and apply ` pd.to_datetime ` to them for good measure** .
If your DataFrame has a multiindex , you must first reset before calling ` to_dict ` .
Iteratively concatenate pandas dataframe with multiindex
I could be misreading the code , but why do you need to ` concat ` at each iteration of the loop ?
Can ` data_all ` just be a list , and you ` concat ` at the end of the for loop ?
Also keep in mind that pandas ' ` concat ` works with iterators .
I need this because I want to aggregate the data as instructed here , but I can't select my columns like that if they are in use as indices .
many thanks , I actually browsed around for this a lot , but " make multiindex to column " and similar queries always got me threads which wanted to pivot their dataframes ...
How to find out about what N the resample function in pandas did its job ?
I use the python module ` pandas ` and its function ` resample ` to calculate means of a dataset .
I'm currently storing this in an HDFStore with 2 tables : ` df_hist ` is a collection of histograms indexed by a ` MultiIndex ` where the first level labels the histogram and the second level labels the histogram bin .
I want to merge IDs with the same addresses in a dictionary #CODE
Now just apply usual pandas transformations and delete unneseccary columns : #CODE
define aggfunc for each values column in pandas pivot table
Was trying to generate a pivot table with multiple " values " columns .
I know I can use aggfunc to aggregate values the way I want to , but what if I don't want to sum or avg both columns but instead I want sum of one column while mean of the other one .
Now this will get a pivot table with sum :
You can concat two DataFrames : #CODE
or you can pass list of functions as ` aggfunc ` parameter and then reindex : #CODE
update Actually , in your case you can pivot by hand : #CODE
You can apply a specific function to a specific column by passing in a dict .
I'm trying to replace the location " Lobur " with " LON " as shown below : #CODE
If you want to chain the string methods , you have to insert another ` .str ` : #CODE
you can use apply : #CODE
append the output to the empty dataframe
I'm still looking at what the combination of apply , lambda , pd.Series and stack does , but it works exactly as intended . thanks !
It's not clear if you are trying to groupby and aggregate ( like in SQL ) or create an index with a date instead of a timestamp .
That will aggregate the timed column since it's the only numeric column .
This will give you a multiindex with the timestamp and a date .
If you don't want the index to be permanent , drop the inplace= argument .
I meant groupby and aggregate .
You can use the ` normalize ` DatetimeIndex method ( which takes it to midnight that day ): #CODE
In 0.15 you'll have access to the dt attribute , so can write this as : #CODE
Pandas groupby apply function that combines some groups but not others
I'm then calling an ` apply ` with my ` combine_function ` ( needs a better name ) on the groups like : #CODE
However it sometimes throws an error that is somehow related to a group that I don't want to aggregate has exactly 2 rows : #CODE
If I comment out the line ` group = group [( group.index == first )]` so update but don't aggregate or call my ` aggregate_function ` on all groups its fine .
Have you tried just using an apply ?
When you aggregate a group , it automatically reduces to one row ; you don't need to do it manually .
I think it has something to do with my handling of the other 8 columns where I just wanted to aggregate use the first value .
Resample Pandas Dataframe with " bin size " / " frequency "
9I have a multi-indexed dataframe which I would like to resample to reduce the frequency of datapoints by a factor of 3 ( meaning that every 3 rows become one ) .
I tried to turn my time column into a pandas datetime index like so , and then use resample : #CODE
Browsing arund stack overflow I found some similar quiestios which all had solutions NOT based on panda's resample - and also , sadly , not viable for my use case .
The trick here is to separate your question , you really have two problems : small ints for to 1970 something ( because that is the start of epoch time ) , and something about resample ( it not working is no suprise given the first issue ) .
So we can use helper function like this and apply it to each group to get desired results .
You could move ' col1 ' and ' col2 ' to the index and then unstack them .
Using ` pivot ` is probably more appropriate then the unstacking as shown above : #CODE
When apply a function to a list , it occurs " TypeError : ' Int64Index ' object is not callable "
But if I apply the function to the groupby , it returns " TypeError " .
maybe it's because when you apply this to a list ` ls [ x ]` returns an integer , when you apply this to a DataFrame , ` ls [ x ]` returns a Series .
Is there a way to drop those entries that do not belong to the respective months and provide a new time-index for each day ?
First I would replace the 0s with NaN ( to symbolise missing data ): #CODE
Use this with a groupby apply : #CODE
apply a function to a groupby function
But I can't apply the function on the groupby .
And also what's the difference between " apply " and " agg " ?
The ` apply ` method calls ` foo ` once for every group .
It is possible to use ` apply ` when ` foo ` returns an object such as a numerical value or string , but in such cases I think using ` agg ` is preferred .
A typical use case for using ` apply ` is when you want to , say , square every value in a group and thus need to return a new group of the same shape .
The ` transform ` method is also useful in this situation -- when you want to transform every value in the group and thus need to return something of the same shape -- but the result can be different than that with ` apply ` since a different object may be passed to ` foo ` ( for example , each column of a grouped dataframe would be passed to ` foo ` when using ` transform ` , while the entire group would be passed to ` foo ` when using ` apply ` . The easiest way to understand this is to experiment with a simple dataframe and the generic ` foo ` . )
The ` agg ` method calls ` foo ` once for every group , but unlike ` apply ` it should return a single number per group .
A typical use case for using ` agg ` is when you want to count the number of items in the group .
you could jus use lambda in apply like that :
if you try that with agg : #CODE
it won't work because agg needs to get one value for each group
if you subtract values of a particular cells , there's no difference between agg and apply , they both create a one value for each group #CODE
if you would like for example substract each row value from the previous row ( to get the increment for each row ) , you could use transform like that : #CODE
After that , you do a concat to merge with the original dataframe ( if needed ) .
Final result where we fill the nans to 0 , then astype to bool to make True / False .
When concat'ing two pandas Time Series object ` resample `' ed ` prices ` and ` amounts ` together , Python throws an error .
ohlcPrice is a frame ( this is what ohlc on a series does ) , so u r trying to concat a frame and a series ; try DataFrame ( ohlcVolume ) in the concat
ah , snap !
When I run this code on this platform ( RHEL 6.4 ) i get the following stack trace .
Thanks to alko , who asked for a smaller dataset , I found an issue with the dataset that the newer Fedora19 stack was ignoring .
For other folks , be aware of silent dataset repair that seems to take place on the newer stack on Fedora19
The stack I have on RHEL6.4 below works fine , so this is solved
What is the quickest way to insert a pandas DataFrame into mongodb using ` PyMongo ` ?
` db.myCollection.insert ( records )` should be replaced by ` db.myCollection.insert_many ( records )` see warning ` // anaconda / bin / #URL DeprecationWarning : insert is deprecated .
` df.index = np.arange ( 1 , len ( df ))`
You can shift this index by ` 1 ` with #CODE
You know to merge them and to update the names so that I don't have to remember which one is ` Id ` and which is ` id ` , a few lines earlier I had called #CODE
I was using the second answer found here : #URL It looks like this way of creating a cartesian join will not work in future ?
I would like to sum the amount column by person and itemCode and transform my results to a numpy array .
I can't pass a Pandas dataFrame to the kmeans or apriori algorithms I have , and no matter how I arrange the dataFrame , the df.values gets me to a multiIndex series ( which simplifies down to 1 long array ! ) .
I couldn't for the life of me get that pivot to work before .
get grouping level in agg function
I'm looking for a simple reliable way to obtain the grouping level inside an agg function for a pandas groupby object .
So , for example , for the following group object , and agg command : #CODE
Can't say what you want to get , but to get keys inside aggregate function you can take first element from ` arr ` : #CODE
It's a bit kludgy since I have to insert the name of the dataframe and the aggregation column into the function , which I would prefer to avoid .
The problem is that pd.Index does not call the eq method on the WrapStr instance but instead just checks if the two instances are the same .
or , very similarly , with `` filter `` like so : `` df.groupby ( level=0 ) .filter ( lambda x : len ( x ) > 1 ) [ ' type ']`` .
You can also use masking and transform for equivalent results , but this is faster , and a little more readable too .
The issue -- and a related issue with ` transform ` on Series -- was fixed for version 0.13 , which should be released any day now .
In the meantime , the ` transform ` workaround is the way to go .
There is no good reason why ` filter ` and ` transform ` should not be applied to nonunique indexes ; it was just poorly implemented at first .
How can a pandas merge preserve order ?
I have two DataFrames in pandas , trying to merge them .
Note we start out with the loans order ' a , b , c ' but after the merge , it's " a , c , b " .
For one thing , you need to pass ` sort=False ` or it will sort on the join key , which you don't want .
One easy workaround is to do ` x.merge ( x.merge ( y , how= ' left ' , on= ' state ' , sort=False ))` , which will merge each row in ` x ` with the corresponding for in the merge , which restores the original order of ` x ` .
First , you're asking it to sort based on the join keys .
Second , if you don't sort based on the join keys , the rows will end up grouped together , such that two rows that merged from the same source row end up next to each other , which means you're still going to get ` a ` , ` c ` , ` b ` .
Looks like one could use reindex to restore original order : #CODE
The fastest way I've found to merge and restore order - if you are merging " left " - is to include the original order as a column in the left dataframe before merging , then use that to restore the order after merging : #CODE
x [ " Order "] = np.arange ( len ( x ))
Pandas Resample Strange Zero Tolerance Behavior
I'm attempting to resample a time series in Pandas and I am getting some odd behavior : #CODE
Is this expected behavior for the resample function ?
What are some alternatives for overcoming a merge fail in Pandas that is to big to fit in memory ?
I am trying to merge a couple of DataFrames using pandas merge ( by left index ) and it fails because the machine is running out of RAM .
I just have one comment for future readers which I couldn't add to your referenced solution - the merge function would probably need to change , meaning a left join between two ' full ' dataframes would turn into an inner join , since not all indices would be available for each ' small ' merge .
I've looked at ` Series.asof() ` and the ` ordered_merge() ` functions but I can't see how to both join on the ` symbol == from_symbol ` clause , and use the ` end_date ` to find the first entry .
The ` end_date ` is inclusive for the join .
Don't know if there's more elegant way to do this , but at the moment I see 2 ways of doing it ( I'm mostly use SQL , so these approaches are taken from this background , since ` join ` is actually taken from relational databases , I'll add SQL syntax also ):
If there're could be duplicates , you can create DataFrame ` mapping2 ` like above and join on it .
SQL ( actually , SQL Server ) way would be to use ` outer apply ` : #CODE
Could you also reindex the mapping df and fill backward .
So you would then have all of the dates prior to the end date for which the mapping applied and simply merge on ' symbol ' and ' date ' ?
The first is running at around 50 seconds for 250,000 rows in the price table and 1,000 rows in the symbol map .
What I want to do is basically aggregate ( sum ) v1 by all three of the k * columns but only aggregate v2 by k1 and k2 , so I end up with something like this : #CODE
Then join .
For testing purposes you can add ` print line ` in a loop to check for skipped lines , or replace ` df = ...
Don't forget to replace things back when running your batch .
use ` drop ` method : #CODE
In fact dropping a column would delete the column if you assigned the result of drop to itself e.g. ` df = df.drop ( col , axis=1 )` .
You could use numpy to build a mask : #CODE
I am working to aggregate Json file in python
what if it has a multiindex ?
The itemsize is created on the first append ( and cannot be changed later ) .
is not specified it will be the max length of strings in that append .
As you manually drop table ( the same behaviour can be achived with ` recreate=True ` param ) , it seems that reason lies in creation statement , which for your case is generated as #CODE
and emptifying it not with ` drop ` statement ( this is a bad practice from dba point of view ) , but with ` delete from pattern ` statement .
To add missing days ( like holidays ) you need to reindex it : #CODE
Also , why do I get the error " File " / usr / local / lib / python2.7 / dist-packages / pandas / core / series.py " , line 3176 , in interpolate
TypeError : array cannot be safely cast to required type " on the interpolate line ?
Compute the usual rolling mean with a forward ( or backward ) window and then use the ` shift ` method to re-center it as you wish .
I would like to apply it to the " col1 " column of a dataframe similar to : #CODE
If the keys in ` di ` refer to ` df [ ' col1 ']` values , then @USER and @USER show how to achieve this with ` replace ` : #CODE
[ `` replace ``] ( #URL ) is equally good , and maybe a better word for what is happening here .
@USER : Stay away from ` apply ` ( especially with ` lambda `) if you can help it .
nope , none of the ` replace ` variants work and they all give me a column filled with ALL the values in the dict ( even if the keys to call some of them are not in that column ) .
pandas dataframe interpolate
So I see that there is a way to interpolate for a pandas series object #URL .
I'd like to interpolate every column .
Does this apply it to the columns or rows ?
Normally different columns in a pandas DataFrame contain different type of information , so an interpolation method may not apply or you may need different methods depending on the data .
You can interpolate NaN values of each column by doing : #CODE
More information about the interpolate function :
Pandas and HDF5 aggregate performance
bitwise operations ) , which then is used to mask or filter the corresponding elements .
In any event , what I recommend ( until openpxyl comes a tad further ) is to map the formulas to a new xlsxwriter.Workbook object .
I've had success using that module to create new xlsx workbooks ( with formatting and formulas ) , and without knowing how well the formats will translate from the openpyxl object to the xlsxwriter one , I believe it will be a viable solution for preserving at least the formulas .
How to resample timedeltas ?
I would now like to load this into Pandas to resample and plot the measurements .
My question is , is it possible to resample this data without subverting to epoch conversion ?
Is there a way to resample like this if the time column contains datetimes rather than ints ?
I'm sure this should be easy and something to do with resample .
You don't need to resample your data unless you want to aggregate into a daily value ( e.g. , sum , max , median )
You can use a combination of groupby and apply : #CODE
I drop ' Location ' at in the last line because groupby inserts the grouped levels into the first positions in the index .
Here is an example based on your data using the hist method of a pandas.Series
pandas dataframe shift dates
I'd like to shift just the dates , one business day forward ( Monday-Friday ) , without changing the size or anything else .
You can shift with ' B ' ( I think this requires numpy > = 1.7 ): #CODE
I know I didn't ask this in my original question , but how would you shift just 1 column along with the dates ( leaving all other columns as is ) ?
I think you'll want to shift the column / series , and then set that to a column in the DataFrame .
Grouping by two columns leads to an error when using aggregate : #CODE
But switching aggregate for apply seems to work .
Why does apply work and not aggregte ?
Actually , if you define some test function like ` def test ( x ): print x ; return x.sum() ` and call ` aggregate ` in both cases , you'll see that in first case ` x ` is a DataFrame and in second case ` x ` is a Series ( and when you call ` apply ` , it's always DataFrame ) .
As Roman points out , the first argument passed to agg is a series , therefore if you want to agg based on values in multiple columns you have to call the second column in the function based upon the index values of the series that is passed automatically . apply always passes as data frame as he points out .
If you want to see some really strange behaviour check out transform , it seems to pass series and dataframes as the first argument to the function .
@USER Hayden it sounds like the best approach is just to switch to apply or attempt to use cythonized functions when aggregate() fails ?
` toAdd ` and ` toRemove ` will then be used to insert and remove documents from the mongodb database .
Maybe you could replace all double whitespaces with a tab and use the tab as a delimiter .
Not yet , trying your approach i've trouble to understand how replace the separator , without looping trough the file line by line ( my real data can hundred thousands lines ) .
You could read the header line and replace lat / long with your approach above , `' LatD ' , ' LatM ' , ' LatS ' , ' LonD ' , ' LonM ' , ' LonS '` , read the file from just below the header , using whitespace as a delimiter and apply the previously read and amended header line as an index to the new dataframe .
pass ` header=0 ` to be able to replace existing names .
Currently I am grouping them using the merge function with the stack function in the following code : #CODE
Where are you getting concat from ?
You need a column multiindex to get the data in exactly the format you require : #CODE
The problem with join is that you end up with duplicate rows because of the non-unique index .
On the other hand , it allows you to control which records within a type align in a row .
What you tried was almost correct , you only needed a ` axis=1 ` in ` concat ` ( and no stack ) .
But the problem with your dataframe is that you have a non unique index , so ` concat ` cannot know how to concatenate the two dataframes along that axis ( you have eg multiple ' SS ') .
And then you can just concat the two with ` axis=1 ` and the ` keys ` you provided : #CODE
You can always drop the ` count ` again with ` merged.index = merged.index.droplevel ( 1 )` .
python dataframe pandas drop column using int
I understand that to drop a column you use df.drop ( ' column name ' , axis=1 ) .
Is there a way to drop a column using a numerical index instead of the column name ?
( df.shape vs len ( df.columns )) .
Truth be told , if you look at the pandas descriptor for shape , it calls len ( df.columns ) but numpy arrays and matricies have them as an attribute . most efficient vectorized operations can be done with regular python syntas as opposed to vectorized operations and is almost always wrong ( numba / jit operations excepted from that criticizm )
Wouldn't ` len ( df )` give you the rows ?
In this particular case : if you learn that the number of rows can be calculated with len ( df.index ) , next time you need the number of columns it comes natural to do len ( df.columns ) .
The problem is that I am calculating the distance among 83.803 boolean vectors , and the result is quite big , which makes difficult to go through all the cells of a 83.803 vs 83.803 matrix and insert its values .
I have attempted to use the pandas reindex functionality #CODE
merge multiple data frames ( more than 2 ) in Python
I would like to merge as runing below codes , however the output is not a data frame but the attributes of object , I am still fresh with Python .
Pandas clean column and apply optional multiplier
I can find some of this by using ` \d{3}k ` however I'm unsure how to replace the ' k ' part with the three ' 000 ' without affecting the first ` \d{3} ` .
You can use the string methods on pandas objects to strip the ` s `' s and replace the ` k `' s and ` K `' s with ` 000 ` .
Depending on the size of the file , one solution can be to pre-process the file in memory to replace all the occurrences of ` || ` with ` |NaN| ` using python ` io.StringIO ` #CODE
Interpolate on the fly to get previous valid entry from pandas DataFrame
use ` reindex ` : #CODE
You could just ` append ` if 2 isn't in the column : #CODE
So , you can apply a function to your data frame to do this ...
Pandas , multiindex , dates , HDFstore and frame_tables
I currently get an error : ` TypeError : [ date ] is not implemented as a table column ` and I was wondering if I am using the multiindex incorrectly , or this is indeed a limitation of Pandas .
You need to store as a ` table ` ( ` put ` stores in ` Fixed ` format , unless ` append ` is specified ) .
Insert or replace like operation for Pandas HDFStore ( table )
In such a case I can not append but must first destroy the old entries .
I will simply rewrite the files ( just read them in chunks , replace as needed , write as chunks ) .
I want to take the " z " column , and add more " z " columns from other places in the table where " c " = " c+1 " and " c-1 " and " d+1 " , " d-1 " , and just join those on the end .
Now I want to pivot my dataframe .
GroupByError : No numeric types to aggregate
well , I get an error , so the output would be something like : raise GroupByError ( ' No numeric types to aggregate ')
GroupByError : No numeric types to aggregate
You can group first and then pivot : #CODE
As far as it goes , it looks like ` std() ` is calling ` aggregation() ` on the ` groupby ` result , and a subtle bug ( see here - Python Pandas : Using Aggregate vs Apply to define new columns ) .
Why does function behavior used within pandas apply change ?
Now , using ` apply ` and ` to_integer ` with ` df1 ` : #CODE
But if I apply it to this ` df2 ` : #CODE
define what happens when you aggregate ( according to the way you sorted ) #CODE
make dict to apply same function to all columns #CODE
finally aggregate #CODE
Is there a way to pivot the matrix into a molten data table of the form : #CODE
My Dataframe looks like this and i have to insert a few rows
For dates where the accesstype 3 is missing i have to insert rows
Hi i tried it out in Python 2.6.6 the replace command does not work there , but in python 2.7 it works fine .. any help please feel free
@USER yeah , mean ` apply ` with ` count ` but you've already added that .
I can do a nasty work around by ` shift ` ing the data first , but it's unintuitive and even coming back to the same code after just a week I have problems wrapping my head around the shift direction !
I cannot figure out how to do it if the columns on which I want to join are not the index .
I was not sure from the question if you only wanted to merge if the key was in the left hand dataframe .
If that is the case then the following will do that ( the above will in effect do a many to many merge ) #CODE
right , but there is a [ bug ] ( #URL ) in panads that will break left join
for your information , in pandas left join breaks when the right frame has non unique values on the joining column . see this bug .
Partial merge in Pandas
E.g. , column to join on = ' number ' .
You can use ` numpy.where() ` after merge : #CODE
Summary : ` merge ` followed by a little DataFrame manipulation and then ` update ` .
Next merge the two data frames on `' number '` #CODE
If a more complicated set of rules for choosing values were desired then replace ` mdf.update ( pdf )` with ` mdf.combine ( pdf , function_of_two_variables_returning_preferred_value )` .
Anyway , to get a DataFrame every time , just pass a list to ` loc ` .
We would like to be able to insert and remove objects from these
Binary operation broadcasting across multiindex
Can you shed any light on why the join is reported as being ambiguous ?
I don't think it's ambiguous more that it just needs to be implemented ( their might be some ambiguity if the levels could match in multiple ways ; prob have to assume that the order of the levels is the join order ) . this is pretty analogous to doing a straight frame-frame join I think so could be straightforward .
another way to approach this is to reset the index on both frames , then join them , set the index and multiply
Move the non-matching index level ( s ) to columns using ` unstack `
Put the non-matching index level ( s ) back using ` stack `
Using pandas to drop all observations in a group after a condition has been met
All I want to do is , for each group , drop all observations after a condition has been met .
` .values ` get the raw ndarray values , I use this because there are some bugs in my pandas version ( 0.12 ) that can't cumsum bool Series .
I want to transform it to look like this : #CODE
Is there any way to insert a function into the agg method ?
There's ` fill_method ` parameter in the ` resample() ` function , but I don't know if it's possible to use it to replace ` NaN ` during resampling .
@USER there is with reindex , so that will probably be the most efficient ...
I think I would use a resample ( note if there are dupes it takes the mean by default ): #CODE
If you prefered dupes to raise , then use reindex : #CODE
+1 for ye fo reindex .
I would like to generate a pivot table using py-pandas from this kind of data #CODE
Simple way to append a pandas series with same index
Is there a simple way to append a pandas series to another and update its index ?
and i can append ` b ` to ` a ` by #CODE
you can also use ` concat ` with ` ignore_index=True ` : ( see docs ) #CODE
I want to wait with using ` dtype ` , because I have many columns , and I don't want to map types for all of them .
Apply SequenceMatcher to DataFrame
You have to apply a function , not a float which expression ` SequenceMatcher ( None , str ( m.ITEM_NAME_x ) , str ( m.ITEM_NAME_y )) .ratio() ` is .
Otherwise I would have to merge the results back in .
did not find a rolling function suitable for what I need to do .
One solution is to concat shifted Series together to make a DataFrame : #CODE
join two CSVs by IDs
Is is possible to join these two CSVs to make a CSV in similar format using pandas ?
Update : I realised what you were actually asking , and I think this ought to be an option in sortlevels , but for now I think you have to reset_index , groupby and apply : #CODE
If you're worried about duplicates ( join nth place ) you can take the head : #CODE
you should be able to use pivot to accomplish what do you want
I am trying to process the data column-by-column , then append the data to a frame in an HDF file .
But after that , I get a ValueError when trying to append a new column to the frame : #CODE
Stack trace and error message : #CODE
PyTables is row-oriented , so you can only append rows .
Read the csv chunk-by-chunk then append the entire frame as you go , something like this : #CODE
but it tells me there is no attribute ' first ' when I apply the same thing to the index .
You could use ` groupby / apply ` : #CODE
It makes sense to put the data into a multi-indexed dataframe ; however , I would like to selectively extract the data ( e.g. pull out data from station " A " , then truncate within specified date range , then calculate stats ) .
Is it possible to truncate via a tertiary index ?
I have considered converting the data to a pivot table , but I do not believe that is the best route to go .
you could use ` ix ` as in : #CODE
I have a dataframe ( ' frame ') on which I want to aggregate by Country and Date : #CODE
What is pythonic way to do dt [ , y : =myfun ( x ) , by=list ( a , b , c )] in R ?
Suppose I have a data frame which have column ` x , a , b , c ` And I would like to aggregate over ` a , b , c ` to get a value y from a list of x via a function ` myfun ` , then duplicate the value for all rows within each window / partition .
In R in ` data.table ` this is just 1 line : ` dt [ , y : =myfun ( x ) , by=list ( a , b , c )]` .
However if your function is written to accept a vector ( or a pandas ` Series `) , you can also select the column and ` apply ` your function to it : #CODE
I was wondering if there is a way to " reverse " of a colormap , for example from a blue extreme to a pink extreme to the opposite ( pink to blue ) for the ' cool ' color map .
In the example below , I chart baby names using the ' cool ' color map , that shows girls in blue color and boy in pink color .
To add a symbol , transpose , add and transpose back #CODE
these are all inherently slow operations because you are doing lots of little indexing operations . instead it is much more efficient to either construct a panel directly from a multi level dict or concat frames .
Erreur dans eval ( expr , envir , enclos ) : les valeurs de y doivent tre 0 = y = 1 #CODE
I can translate in english if it is not clear .
However , I think that it has more to do with missing that in R a data.frame will ( by default ) transform a vector of strings to a ` factor ` , and that in your Python structure this is just a vector of strings .
Unfortunately , it doesn't look like boxplot returns the axes : #CODE
I first thought about using sets which would return a list of all the elements in the intersection of the data sets .
Basically , I want to use Python with Numpy or Pandas to interpolate the dataset to achieve second by second interpolated data so that it is a much higher resolution .
So I want to linearly interpolate and produce new values between each of the real values I currently have while keeping the original values as well .
the way you are making the dataframe , the other column is index , pandas apply the functions to columns not the index
something like using df.str.split ( ' FLOATVALUE ' [ -1 ]) and then merge the data again by ID ?
Re : your second comment , it seems like you could hang onto the original ID column -- why discard it if you're just going to append the numbers back ?
You might need to take the transpose first , like ` df.values.T ` .
When using reindex with method= ' ffill ' , wouldn't one expect the following code to fill in the value on the 13th at 16:00 ?
If this is the case , what is the fill method in reindex actually doing .
As an aside that may help you in the meantime , with datetime-indexed data , [ resample ] ( #URL ) is usually a better choice than reindex .
@USER otherwise , how could you reindex the dataframe to some other frequency while still treating missing values as missing value ?
I will fill ` nan ` for those columns during the weekends . now if I ` reindex ` to finer index , say every minute , the reindex will pick the last value from Friday , and fill it out for the whole weekend .
I have no trouble making a boolean mask of the feature vector to find extreme outliers : #CODE
However , when I attempt to actually use this mask I get a memory explosion and have to force exit before a crash occurs .
There's always a slicker way to do things than the way I reach for , but I'd make a flat frame first and then pivot .
you will simply look it up in the pivot table : #CODE
to make life easier , we may re-index the pivot table : #CODE
Pandas ; transform column with MM : SS , decimals into number of seconds
Is there a way to automatically transform that column into seconds with Pandas , and without making that column the time index of the series ?
But then how do you transform each of the rows into seconds ?
You'll want to take the Series method and ` apply ` it to each column in ` data ` .
The first parts of the multiindex should be the names i grouped the dataframe by , followed by what looks to be the index .
I thought apply treated each group as a sub-dataframe , which i can then manipulate and then return .
After my apply function , I want to look at the same type of table , but it congests it down to one , with the rows being describe parameters , without the level of group indexing
I think there's some alignment magic that happens at the end ( rather than just a concat ) , often I find groupby apply a dark art .
That is , make a function that actually does the analysis you want done , and apply that with ` groupby ( ... ) .apply ( ... )` , so it just returns the results of your analysis .
Also it seems your stack trace is referring to another part of your script and may be pointing to another bug .
If you think about pivot tables in ` Excel ` , you can add additional columns and change from sum to mean to min or max .
Is it possible to get the multiple values in a ` pivot ` in ` Pandas ` ?
Here is a pivot example : #CODE
Is there a way I could take ` np.sum ` to the ` pivot ` example here ?
I could append the first pivot to add the second pivot data , but was wondering if it can be done in one swipe .
In Windows , this solution requires however that you align / streamline the installed software versions ( 32-bit or 64-bit ) of Excel , the ODBC driver and the software package from which you open the ODBC connection .
This was a very subtle bug in how freq reconversion on a queried DataFrame was reconstructed , resolved here .
to take to account that some periods span over one hour , i think one solution is to change the frequency to minute , backward fill all the nan values , and then resample hourly with mean calculation : #CODE
Thanks , this looks OK but I would also need the sum of traffic besides the speed , i.e. I should transform this dataset to a regular minute-level dataset having e.g a point at 12:00 with value 110+10*4 / 9 ( assuming even distribution of speed within the measure interval ) , how do I do that ?
The reason I ask , is because what you want to do would be easy with groupby passing a function , but you can't aggregate this data into bins of equal length .
So within the group you mention where x = 50 what value of y do you want to aggregate to ?
@USER Oh , I see , I can append the occurrences in chunks too , to the giant db .
Setting linewidth of Pandas boxplot fliers
Just want to chime in an say that I've submitted PR to matplotlib to allow full boxplot customization #URL
I am trying to align them using pandas DataFrame function and don't know what to do about the missing month data .
in 0.12 , you have to concat the result of selecting multiple criteria ( keeping in mind that you may generate duplicates ) #CODE
and merge over it #CODE
Replace column values in pandas multiindexed dataframe
I think I should be able to replace value in column via : #CODE
I'm still not sure why the ` groupby ` / ` apply ` is failing when I do it one way and not the other , but it seemed easier to go after the more general question .
I need to join the files based on the CO_Num column .
Basically , I need a way to join / merge these two csv files where the Operator column in plat1.csv is overwritten by what is in the COMPAC.csv file , and if there is no CO_Num , that line is simply empty in the Operator column ( all other columns remain untouched ) .
You shouldn't include the empty Operator column of c in the merge ( including it means merging on both CO_Num and Operator which means there are no shared keys ): #CODE
Can you append the head of the csvs ( or some example frames which show this ) to your question ...
You can't unpack a 1d structured array , since all that ` unpack=True ` does is to transpose your array so that columns vary along the first axis , and the transpose of a 1d array is itself .
` pandas.fillna() ` is mean to replace ` NaN ` values with something else , not insert ` NaN ` into null data slots .
How do we apply a function to an entire group in pandas and python ?
How can we apply a function to an entire group in pandas dataframe in python ?
Could you be more specific about how ` magic_apply ` will differ from ` apply ` ?
As @USER points out the " magic apply " is simply called ... apply .
You could just replace it with ` math.log ` , but there's almost certainly a better option with vectorized operations .
You can use numpy ( vectorized ) logarithm with panda's diff : #CODE
After conversion of numpy.float64 to Pandas timestamp I have 1 minute diff - why ?
or drop the column after importing , I prefer the former method ( why import data you are not interested in ? ) .
If the columns are varying length then you can just the header to get the columns and then read the csv again properly and drop the last column : #CODE
I would like to drop the last column of the cvs , but the cvs's are of varying column lenghts .
Is it possible to drop based on location ?
So why dont you resample / pivot your DF to this and plot it with ` plt.imshow ( df_all.values )` ?
I do not know what you mean by heat map for a time series , but for a dataframe you may do as below : #CODE
@USER if you want the heat map to be also based on daily intervals , then you need to first use pandas ` resample ` method , otherwise just modify ` set_xticks ` and ` set_xticklabels ` calls
The way aggregation works is that you give a key and a value , where the key is a pre existing column name and the value is a function to map on the column .
For this reason , typically people will add some data at the beginning , say 100 time steps , and then cut off the first 100 RSI values .
For this I think you can use ` transform ` and ` rolling_sum ` .
do you want aggregate or filter data ?
I thought something like ix index quarter existed just could not apply it !....
Renaming index values in multiindex dataframe
Can a dictionary be passed to replace only values in one index without touching ( or having to specify values for ) the other axes ?
I would like to reindex it , using the index of another time series : #CODE
that was my comment in the of the response . you might be able to reset these matched ones after the reindex with filling .
There have been some other similar issues reported with exceptions being shown in debugger when they shouldn't ( i.e. when they are caught up the stack , and the setting is to not report caught exceptions ) .
Depending on the number of items , it might be best to insert rows as you parse the json using sql commands .
Are you missing an ` abs ` ?
You can get the ` last_valid ` using shift and numpy's where : #CODE
@USER think I was just missing an abs in the second part , this should be correct now in your example !
But how do i apply that formatting to a dataframe ?
In addition , you are better off using a vectorized method or apply if you cannot vectorize .
Pandas - Replace Integers with Floats
Is there a pandas function to transform this data so it show the columns as a , b , c , d , e or whatever is inside the data field and the rows count how many of the letters there are .
close but .reset_index ( level=0 ) gives an error ValueError ( ' cannot insert %s , already exists ' % item )
You could replace the zeros with NaN's in you want to .
The ` get_dummies ` gives a table of all the occurrences of the letters , but for each level in the unstack DataFrame .
I still agree with your earlier comment that its less pretty ( and readable ) than the ` pivot ` solution from Roman .
The concept of pivot is also better known , then ...
I think this is a perfect use case for ` transform ` : #CODE
Typically you use ` transform ` when you want to broadcast the result across all entries of the group .
you can use ` ix ` to get the index in a different order and plot it using ` df.plot ` : #CODE
It's worth mentioning that you can often be explicit with loc / iloc : #CODE
To select with a position and a label use ix : #CODE
Note labels take precedence with ix .
It's worth emphasising that assignment is garaunteed to work with one loc / iloc / ix , but may fail when chaining : #CODE
When I insert this with support 3 #CODE
Your return is in the wrong place and for n in len ( pattern ) is wrong too ....
The above code works if I replace the np.NaN with a number , such as " 2.3 " .
I've tried to follow the stack trace , but after a while it gets complicated ( I'm not familiar with Pandas , Numpy and Matplotlib source code ) .
When one look at your data suggests it would be a simple ' group by ' in SQL , map reduce is probably going to unnecessary
How to make a boxplot where each row in my dataframe object is a box in the plot ?
But when I plot this data frame as a boxplot , I only get one box with the Open , High , Low , and Close values of the entire Volume column .
I just want a basic boxplot plot where each row from the DataFrame is a OHLC box in the plot .
I simply want to graph a boxplot where each box is a row of my DataFrame .
My first piece of code is graphing a boxplot where each box is a column of my DataFrame .
Do I need to transpose or unstack something ?
A couple of points : 1 ) boxplots and candlestick graphs , despite their similar appearance , are conceptually quite different ; 2 ) you actually are trying to make a candlestick graph ; 3 ) even if you transposed your data with ` SP.T ` , the ` boxplot ` method will not produce what you want it to and ; 4 ) the real change you face is figuring out how to take your dataframe and turn it into a format that ` matplotlib.finance.candlestick ` can use .
It's a simple linear algebra , you multiply matrix with its transpose ( your example contains strings , don't forget to convert them to integer ): #CODE
You just took the dot product of the matrix with its transpose .
Use apply : #CODE
you can also use ` map ` : #CODE
Is there a way to estimate how much memory a merge operation will require ?
Given two dataframes A and B , and X amount of shared columns to merge on ( B is merged to A ) , is there a way to pre-calculate / estimate in real-time how much memory the operation will require ?
you can always reindex to your desired frequency : #CODE
One option would be to mask on the day of week you want .
Another option would be to resample .
Using ` resample ` ( as @USER has shown ) is another possibility .
Use ` resample ` if you wish to aggregate groups of rows from the original data set ( for example , by taking a mean ) .
` reindex ` might introduce NaN values if the original data set does not have a value on the date specified by the reindex -- though ( as @USER .nouri points out ) you could use ` method =p ad ` to propagate last observations here as well .
Now , when you reindex with #CODE
When you reindex the original ` df ` with a string index , there are no matching index values , so no column values of the original ` df ` are retained .
When you reindex the second ` df ` ( after converting the index to a datetime index ) , there will be matching index values , so the column values on those indixes are retained .
If your are not certain you data are always nicely 1-minutly ( so the 5 rows are not always equal to 5 mins ) , you can set the ` freq ` keyword in rolling_max to ` 1min ` .
( I can also use the chunksize option and concat myself , but that seems to be a bit of a hack . )
Jeff , I updated sec_id and dt in the dataframe .
Sorry , I had to update " sec_id " and " dt " to " id " and " date " .
0.12 is fine ; FYI the format keyword doesn't do anything with append ( and it's for 0.13 anyhow ); append always is a table
How could I align the data between two dates ( EX : 2013-12-10 18:00 : 00 and 2013-12-11 00:00 : 00 at 20 minute intervals such that it appeared as below : #CODE
There are more columns in the data that are not shown above , and using this code causes the non-numeric columns to drop off .
@USER use ` reindex ` and ` date_range ` as in my edits above
@ behzed.nouri : I am receiving a " AssertionError : Block ref_items must be BlockManager items " from the reindex line
The Error I am getting is described in the following stack trace #URL
i will look into the stack trace once i get the chance
pandas merge columns to a single time series
it's a simple matrix operation , not sure how to map it to pandas ...
Strangely I get different results using this functionality compared to the numpy.std function applied to a rolling window over an array .
I've tried using the ` append ` method , but I get a cross join ( i.e. cartesian product ) .
Did you try the ` join ` method ?
It seems in general you're just looking for a join : #CODE
@USER Join and concat use a lot of the same code under the hood , so the " right " way probably only matters when you consider edge cases .
For instance here if both DataFrames had a ' data ' column the join would * fail* , whereas a concat would give you two columns named ' data ' .
I have recently been made aware of the dangers of chained assignment , and I am trying to use the proper method of indexing in pandas using loc [ rowindex , colindex ] .
Personally I think it's annoying to diagnose atm as there's too much noise ( e.g. drop the columns you're not using , some of the rows that aren't raising , make the line with the ands have fewer conditions ) Ideally smaller than a 5 by 5 :)
I have a data frame that I would like to transform into a numpy record array .
Now I apply the cut : ` up3 = up2 [: cut_loc-1 ]` , which should just shorten the ` DataFrame ` .
Resample while keeping last date from input file ( and not last day computed by ` resample `)
This is the true date from my input data , and not a date created by ` resample ` .
Perhaps not the most fancy way , but you can always ` groupby ` your time frequency and apply a custom function returning what you want .
Then groupby the month frequency and apply the function : #CODE
How to sort by some columns and some levels of a multi index simultaneously ( in this case let's say level 1 of the multiindex and then by column 0
` data.column_name ` would translate to ` data [ " column_name " ]` , which is not what you intend
If I have a time series that contains NaN values and want to resample to percentiles ( per this post ) #CODE
Download the the table , using a utility into a csv file , read the csv file chunk by chunk using pandas and append to HDF5 table using ` pandas.HDFStore ` .
Hence , when I append this DataFrame to an already existing ` HDFDatastore ` , there is a type mismatch for e.g. a float64 will maybe interpreted as int64 in one chunk .
maybe : DataFrame ([( i [ 1 ] , i [ 0 ]) for i in enumerate ( set ( x.index ))]) and then merge ?
Looking at [ the source code ] ( #URL ) , I see ` Categorical ` calls ` factorize ` .
@USER you could make the same argument for allowing ` bool ( arr )` on numpy ndarrays .
I * would * make that argument for allowing ` bool ( arr )` to have some implementation and not raise an error .
If you're using ` ndarray ` * you * should know what ` bool ` does on that input , rather than the library preventing it or warning because some people might not .
That is , ` bool ([ 1 , 2 , 3 ])` returns ` True ` , but ` bool ( np.asarray ([ 1 , 2 , 3 ]))` raises an error .
Another way to not have the warning would be to do the selection operation via ` reindex ` , e.g. #CODE
To resample , you can make it an index , and use resample #CODE
@USER there is builtin resample method , see example and link to docs in my answer .
Now I can ` concat ` the last 3 dataframes on a new minute passing and then calculate what I need .
You can calculate ` Mean ( M ) , Second Moment ( M2 ) and Std ( D )` for each dataframe and when you need to aggregate some of them you can use the properties of this statistics :
m_i = len ( X_i )
Complicated pandas merge operation
I would like to merge the three tables together , so each row in the new combined DataFrame has year , fruit , price , qty , weight ( possibly NaN ) and colour .
Isn't the syntax for join , df.join ( .. ) ?
If you were confident that there was only one weight of fruit ( i.e. independent of the year ) you could just drop the year column from df2 : #CODE
If so , I'd create a MultiIndex , and the unstack from that .
Now set that as the index and ` unstack ` .
The ` unstack ( 0 )` say to move the outer index labels to the columns .
Yes , this is the other option : ` unstack ` with a multi-index , or ` pivot ` with Series
You can also do it with ` unstack ` as @USER explained : #CODE
For example , let's say I'm looking to find and categorize transients using some moving window process ( e.g. wavelet analysis ) or apply a FIR filter .
Keep a map of the entire data set in memory so that I can address the data set as I would a regular pandas Series object , e.g. data [ #URL
I can then apply a succession of filters to the overlapping windows and then remove the overlaps at the end .
For instance , if you apply an operation that does a reduction that you can fit in memory , Results can simpley be a pandas.Series ( or Frame ) .
You might want to resample the data to lower frequencies , and work on these , loading the data in a particular slice as needed for more detailed work .
By overlapping the data chunks , I can apply a filter and then cast aside the padding at the end , thus preserving continuity .
Then , I am trying to reindex my irregularly timestamped data to the 20 minute interval series I created before , ` idx ` #CODE
You should be using resample rather than reindexing with a date_range : #CODE
@USER I guess that depends on what you are doing later , usually pandas is pretty clever with alignment ... have you tried the [ panel resample method ] ( #URL ) ?
I'll preface by saying I'm a programming n00b by stack standards .
Also have a look at the arrow library to replace the ` datetime ` part .
I think you're going to be able to wrap this with a concat , something like : pd.concat ([ series_chunk ( chunk ) for chunk in lines_per_n ( f , 5 )]) , where series_chunk is the function returning each row as a Series ( the bit in the try / except block ) .
I was expecting df.T to work but it didn't do transpose columns and rows .
More bugs from me , you need to use concat with axis=1 , updated my answer , sorry !
How would concat each dataframe it creates in the loop as one object ?
One way is to concat them together , tweaked ...
Are you recommending to use either append or concat ?
I get how the first concat we have will concat the chunks but it seems like now I have to wrap the whole thing in another concat .
Check out the IO part of the docs for several examples , arguments you can pass to this function , as well as ways to normalize less structured json .
If you have several json files you should concat the DataFrames together ( similar to in this answer ): #CODE
As mentioned in the comments you may be able to do this more directly by concat several Series together ...
I suspect it's possible for you to concat some objects together more directly , but difficult without a [ short example ] ( #URL )
Would love to be able to concat the object outputs into a dataframe somehow .
You can do this directly with an apply instead of last ( and get the -1th row of each group ): #CODE
But most likely you can do a transform or apply ( depending on what something is ): #CODE
Couldn't figure out the transform apply approach when I looked into it at first .
@USER you need something to be a function for transform / apply , worth checking out the docs : #URL
@USER completely depends on the something whether or not you can do apply / transform !
I want the earliest date for each ticker to wind up with an ` NaN ` in its diff column .
If so , would I need to do a merge to append the differences column ( currently in ` result [ ' current ']` to the original dataframe ` df ` ?
and then ` join ` the two dataframes at the end .
Would you just use your solution on a copy of the original dataframe and then merge ( keying on ` ticker ` and ` date `) ?
For example , I might want to add a column called " rolling average " to the original dataframe , where each row's value is the average of the previous N samples of the ` ticker ` specified in that row .
If you pivot as you suggested , and then compute diffs down the columns , you'll wind up with ` nan `' s everywhere .
Is there a way to combine my general approach ( ` set_index ` , ` groupby ` , ` transform ` , ` reset_index `) with the reindexing you mentioned to do the " realigning " at the end of the process ?
@USER i added an alternative way ; though this it is not using ` groupby ` it is easy to use this method to calculate rolling averages or such
For example , I get weird behavior if I take the original dataframe and try : ` df.sort ([ ' date ']) .groupby ([ ' ticker ']) .transform ( lambda x : x.diff() )` I would have hoped pandas would be able to figure out that it should ignore text columns and then apply the diff function to the numerical columns .
In general , is there a way to use a different function per column in ` transform ` ( like you can with ` agg `) ?
You can use ` pivot ` to convert the dataframe into date-ticker table , here is an example :
convert the dataframe by ` pivot ` format : #CODE
I'm curious what you think about the general ` set_index ` , ` groupby ` , ` transform ` , ` reset_index ` process I describe in my comment to behzad.nouri ' s answer above .
And what I really like is that it can be generalized to cases where you want to apply a function more intricate than ` diff ` .
In particular , you could do things like ` lambda x : pd.rolling_mean ( x , 20 , 20 )` to make a column of rolling means where you don't need to worry about each ticker's data being corrupted by that of any other ticker ( ` groupby ` takes care of that for you ... ) .
Then how do I apply clustering to this to determine a cut-off threshold ?
I think methods with apply are going to need some annoying sorting at the end ...
` len ( test_data [ 0 ] [ ' SomeGeneID '])
len ( test_data [ 0 ] [ ' DifferentgeneID '])
In case groups have length not greater than ` l ` , you can try something like ` pd.DataFrame ( map ( list , grouped [ person ] .values ))` to make it a DataFrame with ` l ` columns , with ` None `' s for missing values ( shorter rows ) .
File " C :\ Anaconda\lib\ site-packages \pandas\core\ generic.py " , line 348 , in drop
File " C :\ Anaconda\lib\ site-packages \pandas\core\ index.py " , line 1215 , in drop
raise ValueError ( ' labels %s not contained in axis ' % labels [ mask ])
Also if you need triangle matrix in result , reindex ts on each iteration
For something like this , you could use ` shift ` and a loop .
Yes , but then you are using pandas ` hist ` function , and not matplotlibs .
These NaN's are not handled well by the ` hist ` function of matplotlib .
Another option is to use pandas ` hist ` method on your series and providing the ` axes [ 0 ]` to the ` ax ` keyword : #CODE
Why order matters for multiindex selection using .ix
Compose a mask of the values that you want to change ( could be more complicated
If you have ` None ` values , it will replace them with whatever you pass into this field .
Of course you could always write your own function to output a csv , or simply replace the "" in the output file using : #CODE
I'm seeing this * only * after running map / reduce workers ( shepherd / mincemeat ) .
I've got a dataframe , and I'm trying to append a column of sequential differences to it .
If I replace the middle line above with the more concise code shown here , everything still works : #CODE
I know there's a ` diff ` method on the ` DataFrame ` class , but I couldn't figure out how to pass that to ` transform ` without the ` lambda ` function syntax I used to make ` data1 ` work .
How can I have call the Pandas ` diff ` method within ` transform ` without needing to write a ` lambda ` to do it ?
Just pass a lambda to transform ( this is tantamount to passing afuncton object , e.g. np.diff ( or Series.diff ) directly .
e.g. you can pass a time frequency to a datelike-index where it calculates how many n to actually diff .
Any feelings about style / stability / extensibility best practices that would lead you preferring either the ` transform ( lambda x : x.diff() )` method or the ` blahblah.diff() ` method to accomplish this task ?
Can you recommend a place I could look to see which methods ( ` diff ` , ` sum ` , whatever ) properly belong to ` Series ` vs ` DataFrame ` vs some other structure ?
lmao ( I also do the print trick , or append to a list , it works great ... although sometimes changes depending on the return type ! )
So is it correct to say that ` transform ` always operates on a series ( even if it means it has to work on multiple series in succession ) , while ` apply ` works on multiple series all at once as a DataFrame ?
is it the # of columns , or is it ` transform ` vs ` apply ` , or is it ` [ ' colname ']` vs ` [[ ' colname ']]` ?
I know I am late to this party , but is there any to know that this is running a ` diff ` in order ?
In my original post , I showed two methods that worked ( 1 : involved ` lambda ` and 2 : involved just using the ` diff ` method directly on a Pandas ` Series `) .
What Pandas data type is passed to transform or apply in a groupby
to inspect the calling stack : #CODE
@USER , you can print the calling stack , and analyze what's going on .
Is there any way to force transform to pass the multi-column dataframe ( i.e. not the individual series from the columns ) to the function ?
I basically want the same behavior of ` apply ( my_func , axis =1 )` but forcing it to return a result with the same index ( i.e. what transform is supposed to do , but rather than working column by column , I want to be able to access multiple columns at the same time ) .
In the more readable version a conversion of the mask to a pandas series is missing .
But indeed , then just ` A [ mask ]` is needed without ` ~ ` .
From here , we can use a little hack to directly append these columns in one step with the appropriate column names .
On the Right Hand Side , we break up the ' stats ' column using apply to make a data frame out of each key / value pair .
grouping by column and then doing a boxplot by the index in pandas
I have a large dataframe which I would like to group by some column and examine graphically the distribution per group using a boxplot .
The problem is that after a groupby operation , my data is all in one column with the group labels in the index , so i can't call boxplot on the result .
Now I want to group by column b and boxplot the distribution of both groups in one boxplot .
You can use the ` by ` argument of ` boxplot ` .
You can provide a function to replace default behavior , just like many other languages allow you to do ( through sub classing or other means ) .
After several weeks of refining this I have the following code thanks to awesome folks on SO which produces dataframes as I need but I'm not sure how to concat the dataframes in the program into one for the final dataframe object variable .
I just assign the concat statement to a variable then I end up getting the last dataframe only .
output from above program which I need to concat as one dataframe : #CODE
see here : #URL just append the df's to a list , them to a list , then concat at the end , e.g. `` result = pd.concat ([ list_of_frames ])``
Note : it could be that using axis=1 to the final concat means you can avoid T-ing earlier .
I just found this method ... which is neater ... but is there a way perhaps using ` resample ` and if using this method could I convert a minutely OHLC timeseries into a 15 minute timeseries OHLC ?
no , you would have to specifically do that ; easier IMHO to just ohlc on say last , them do a separate sum on volume and concat anyhow
If I build my own function , how can I apply it to the DataFrame columns ?
I cannot append a series to dataframe .
Pandas apply to data frame groupby
If I groupby ( g object below ) and then apply following function to first 1000 rows of df , it works .
But if I apply it to entire df , I get this exception : #CODE
First reset the index , then group and apply .
I'm trying to replace a list-of-dictionaries with a pandas-dataframe .
If you really want row-by-row ops , you could use ` iterrows ` and ` loc ` : #CODE
And there's always ` apply ` : #CODE
Merge / Join / Append two Pandas DataFrames with MultiIndex columns by both index and cols
I have tried several ways , including with Merge and Join , but can't get it to work .
In v0.13.1 , this does not support any other join method other than left ...
I have dropped the predictions for the time being , which reduced the complexity of my program significantly , instead I insert a standard sample for every non duplicate in my test set .
What is ` sum ( map ( len , new_text ))` , i.e. the total size of the strings you need to keep in memory ?
Also ` for item in xrange ( len ( test ))` should be ` for item in test ` .
If the order of your rows is not important you can use sets to find elements that are in train set but not in test set ( intersection trainset testset ) and add them first to your " result " and after that use set difference ( testset-trainset ) to add elements that are in your test set but not in the train set .
The ideal would be to read one line , transform it as we want , and store it into its final destination .
for your last question , freq I can't correctly define freq , so don't know what to suggest .
` p - median ( diff ( .index ( x )))`
Would a fourier transform possibly help ?
Sure median will be 24 hours , as most of days exist in list : #CODE
If you wish to automate the process of inferring the median frequency in days , then you could do this : #CODE
First question is how do I strip off the header with Python 2.7 .
With 3.3 the above code works fine but it doesn't strip off the header in 2.7 .
Once I download the data from the data source , one of many different sources I plan to use throughout the course of the program I'm starting to work on , I want to be able to strip off the unnecessary data and then do mathematical work with the remaining data and put the results into new columns right alongside the original data .
How do I strip off the row number from the entire head / tail line .
To support leap year , you can shift all date to year 2000 , here is my try : #CODE
But when i try to resample with lambda like
Id like to resample the data by quarter or week and get the difference from the start of the period .
My aim is to " unstack " the results such as I could have one row per ` name ` as long as ` unit ` and ` ref ` are the same .
so far , I tried to join the same table based on ' name ' -- for now , no check on ` unit ` neither ` ref ` -- : #CODE
Below is your example doing the most simplistic approach , which is just to JOIN a view of each one together .
I'm using SQL Server 2008+ to get the PIVOT command .
But here is the stack trace I'm getting #CODE
I'm using python 2.7.3 , and with pandas the stack trace above is from stable release 0.12.0 .
Please edit your question and include some sample code as well as the complete stack trace .
Instead of using concat , why not update the DataFrame in place ?
I have a legacy system and am running version 0.9.0 , can't upgrade till after project , so I will have to use a string replace or delete method many thanks .
My question is : how do I map these 7 rows to the original dataframe ?
I am not going to use this , but , out of curiosity , how do I map between slice and dataframe , in both directions ?
Then , I try to append the cumulative number calculation under the label " new " as suggested in the post #CODE
The problem is , as the Error message says , that the index of the calculated column you want to insert is incompatible with the index of ` df ` .
For this reason , you cannot insert it into the frame .
Remove the first level of the MultiIndex , so you get back the original index : #CODE
It is for this reason the ` as_index=False ` is used in the groupby call , so the column ` L1 ` ( fow which you group ) is not added to the index ( creating a MultiIndex ) , so the original index is retained and the result can be appended to the original frame .
But it seems the ` as_index ` keyword is ignored in 0.12 when using ` apply ` .
But with ` apply ` it doesn't .
Python pandas resample takes too much memory
I would like to resample it to get data with monthly frequency but the problem is that my code takes too much memory and too much time for very little amount of data .
Using ` resample ( ' M ' , how= ' last '` will be a little bit faster .
Caching the functions help somewhat if I am making repeated calls but it still takes too much time and I think it is because I unstack and stack ' Cusip ' .
I would like to pivot the data so that , of the 5 identifying columns , 3 become a hierarchical row index while the other 2 become a hierarchical column index .
I had initially tried making all the columns except for ` value ` into an index and then using ` unstack ([ ' observer ' , ' obstype '])` .
If I go with the " make a huge index , then unstack " method how do I kill the extra level in the column hierarchy ?
And in the case of ` unstack ` to get rid of the extra level , you can use ` droplevel ` : #CODE
I have searched stack overflow and the message board , but it seems like the Panel is not as widely used as the dataframe .
You still need to apply a function or aggregation on the ` groupby() ` call , in your case to sum all items in a group ` data.groupby ( .. ) .sum() ` .
Using boolean indexing for row and column MultiIndex in Pandas
@USER That's also fastest solution so far ( just ) , though loc seems * slow * here ...
The ix operator is the power indexing operator .
In general , to do a look up in a MultiIndex , you should pass in a tuple .
Hadoop ( Pydoop ) ( map / reduce )
can you provide output for your code , for something like ` data_2.reindex_axis ( sorted ( data_2.index , reverse=True ) , axis=0 , copy=False ) [: 3 ]` and for ` data_2 [: 3 ]` prior and after reindex .
I have a dataframe which I want to split into 5 chunks ( more generally n chunks ) , so that I can apply a groupby on the chunks .
I want to resample the datetime columns appropriately using pandas ` df.resample() ` .
Then I try and resample the ` DataFrame ` to daily using ` df.resample ( " 1D " , how= " sum ")` .
I tried creating a multiindex and then unstacking a few times : #CODE
The only work around we have found is to instead we truncate the dates using numpy datatypes .
So being able to truncate them to one day , two days , etc .
Set a timestamp index , sort the timestamp index ( the way I am doing it here , you don't need to sort , while a resample does need it sorted ) . and perform the resample in whatever frequency you want ( 1D in this example ); this is tantamount to a resample but it just ' groups ' and doesn't do the calculation ( yet ) .
then in the apply , do your calculation , which in this case is another groupby .
You almost never want to unstack twice in a row on a large set as its memory conducive .
Using a print statement to see the " Lines to be appended " section below , I then assign the variables to dict pointers and the append the dict to what started as a blank dataframe .
The first line / append seems to change the type for that first entry on the last column .
Return from line print prior to append to df : #CODE
pls make an example that is easily reproducible , e.g. can be cut and pasted .
This is a bug , noted here : #URL Merge / join / concat does not handle timedelta properly ATM .
Got around this by encapsulating the dict ' parms ' in a spawned dataframe per loop and append the single row df to the major dataframe .
For filling the NaNs , you can apply this on all columns in one line as follows : #CODE
For the resampling , normally you can do the following to resample with different functions on multiple columns at once : #CODE
You have then a MultiIndex ( or hierarchical ) column , and access it like : `` resampledData [( ' amount ' , ' sum ')]`` See here #URL
I kinda hate you but more myself because I've spent the past couple of hours not realizing DataFrame had resample too .
Most functions on Serieses you can also apply on a DataFrame , and if you can't , you can always apply a Series functions on all columns at once like this : ` df.apply ( lambda x : x.seriesmethod() )`
I am given daily rainfall data ( e.g. s , below ) which I can convert to a pd.Series and resample into monthly periods ( sum ; e.g. sm , below ) .
when you resample resample with both sum and mean ..
I misinterpreted the transform function .
It seems a little annoying to have to turn the ` Series ` object ( or in the answer above , ` dict `) back into a DataFrame and then append it , but it does work for my purpose .
I could append that data frame by getting the quarter for each entry ( or Month , or Year ) , then use ` Pandas ` ` groupby ` function , but that seems like it's extra work when I should be using ` TimeSeries ` .
I'm first doing a groupby , and then applying a resample on each group .
Ah , I didn't realize you could use ` resample ` directly on the groupby !
( I was thinking this would give problems for the resample with it then being a multi-index , but of course , the groups itself don't have the grouping value as an index yet )
@USER atm resample just passes to g.apply ( pd.DataFrame.resample ) , but perhaps in the future the groupby method could be written specifically and be more efficient ...
The command I use seems to apply the xlim only to the last of the facets .
Freq : T
Use resample to get each days maximum : #CODE
@USER I think you last step ( the resample by hour ) is not what is he asks .
Can anyone suggest a method where I don't need to do the ` zip ` ping to condense all the inputs for ` np.interp ` down to one column before using ` map ` ?
is there a Pandas function that acts like ` map ` does for Series , but takes in an entire row of a DataFrame as its input ( but doesn't involve ` groupby `) ?
Now consider 3 different ways to tack on a column which will interpolate between the given columns to an x-coordinate of 5 : #CODE
Pivot a pandas dataframe and get the non-axis columns as a series
If I pivot the table using D.pivot_table ( rows= " Category " , cols= " Period " , aggfunc= " sum ") I get a multi-indexed data frame looking like this #CODE
Where each ' Series ' is a pandas series of three numbers being the respective ( Projected , Actual , and Previous ) aggregate values .
I've looked at stack , unstack , various combination or rows , cols , and values arguments to pivot table and of the melt function in pandas.core.reshape , but none of them seem to do quite what I want .
Maybe you're looking for a groupby object instead of pivot ?
Instead of doing what you're doing with labeldict , you could make that information into a DataFrame and then join it with your original one : #CODE
You can use merge instead : #CODE
@USER apply answer from question then " 1 " the found rectangle and recurse .
The pandas object also performs ix addressing directly .
that's one of the oddities of ix in any event
@USER presumably there isn't an ` iloc ` either ( if it's [ pre 0.11 ] ( #URL )) , in which case you need to use ( the more ambiguous ) ` ix ` .
I have no problem using ` unstack ` to make the data wide , but then I end up with what looks like an index which I can't get rid of .
The purpose of column names perhaps becomes clearer when you look at what happens when you unstack twice : #CODE
Now ` dfUnstacked2.columns ` is a ` MultiIndex ` .
finds the median of these delays ;
presumes the first time period starts at half the median value before the first timestamp ( putting the measurement in the middle of the time period );
adds data to this array according to the correct ( timestamp - starttime ) / median element .
The median time between datapoints is 20 seconds , half that is 10 seconds .
The timestep is the median , 20 seconds .
median delay : ` df [ ' time '] .diff() .median() `
presumes the first time period starts at half the median value before the first timestamp ( putting the measurement in the middle of the time period ) ; I don't know what you mean here
Calc median #CODE
first , why not ` df [ ' diff '] = df [ ' time2 '] .diff() ` ?
`` diff `` is better hear ; I was trying to ' illustrate ' so didn't overwrite the columns
As of .13 , you should be able to use ` reindex ` and ` interpolate ` to do this ( as long as you have ` scipy `) .
So now the idea is to ` reindex ` ` df ` to ` many_idx ` and fill in the resulting ` NaN ` s with a spline ( separately for each column ) .
pd.scatter_matrix ( frame , alpha= 0.5 , figsize=None , ax=None , grid=False , diagonal= ' hist ' , marker= ' . ' , density_kwds={} , hist_kwds={} , ** kwds )
Your csv hasn't column names which you later use for pivot , spcify them expicetly .
Try replace #CODE
In statsmodels " freq " controls the units for the time series not the " data window " .
The closest I got was using merge , but that left me with different suffixes .
I want to keep the trials separate , and just merge the Times
IIUC , you can use ` groupby ` and then aggregate : #CODE
If you do not want to add a column to your original DataFrame , you could create an independent ` Series ` and apply the ` groupby ` method to the ` Series ` instead : #CODE
I have tried using ` df.groupby ` and ` df.filter ` to loop-over the columns but I cannot really get it to work , because I am not at all sure how I apply effectively the same function to chunks of the data-frame , all in one go ( as apparently one is to avoid looping over rows ) .
Your calculation is more NumPy-ish than Panda-ish , by which I mean the calculation can be expressed somewhat succinctly if you regard your DataFrame as merely a big array , whereas the solution ( at least the one I came up with ) is more complicated when you try to wrangle the DataFrame with melt , groupby , etc .
I just couldn't get ` groupby ` and ` apply ` to play nicely with ` axis=1 ` .
I am trying to replace the letter T or any other letter for that matter with the next highest integer for that table .
python pandas How to remove outliers from a dataframe and replace with an average value of preceding records
Now i need to do some data cleansing , manipulating , remove skews or outliers and replace it with a value based on certain rules .
i.e. on the below how could i identify the skewed points ( any value greater than 1 ) and replace them with the average of the next two records or previous record if there no later records .
So in the dataframe below I would like to replace Bill%4 for IT week1 of 1.21 with the average of week2 and week3 for IT so it is 0.81 .
i.e. i need to apply a groupby to your example in my case it would be by ' Country '
Pandas HDFStore Tables doesn't accept multiindex columns
I suspect this is a known limitation of using PyTables , but I couldn't find any reference in the Pandas docs that the multiindex is in fact restricted to the index , not the columns .
However , when I do ` apply ` , I'm getting dataframes only : #CODE
( I want to simply drop some groups from grouped-by df ) .
Internally , ` apply ` and ` filter ` try different ways of looping through the data : a " slow path " that is sure to work for any function , and a " fast path " that only works for some functions .
Which groups do you want to drop , and what criterion are you trying to use ?
I'm thinking about this sort of thing in the context of adding more useful marginals in pivot table type reports .
So we can insert another column ( i.e. row ) with the name of the additional level , which we name exactly like the one we summed over .
define a function to diff on that column ( and just return the rest of the group ) #CODE
the diff is now a timedelta64 [ ns ] , see here for how to convert / round to a specific frequency ( e.g. days ) .
The problem is , after the ` apply ( f )` , I cannot do something like ` nth ( 1 )` to return the 2nd value of each group .
in `` f `` you could do `` x.head ( 2 ) .tail ( 1 )`` to do that ( you could also do `` x.iloc [ 1 ]`` , but if you have a groupsize < 2 that will fail . alternatively , you can do a second group / apply ( `` nth `` is a groupby operation ) , e.g. `` dd.gropuby ( ' user_id ') .apply ( f ) .gropuby ( ' user_id ') .nth ( 1 )``
also , the function you suggest computes the diff between each subsequent time .
I need the diff computed from the ** first ** action to each subsequent action , so that the first time a user_id appears , that is time zero and each new action has its age computed relative to that first action .
easy , just do `` x [ ' diff '] = x [ ' date '] -x [ ' date '] .iloc [ 0 ]``
Note that you have to treat corner case , ` bins [ 0 ]` , separately , as it is not included by cut in leftmost bin .
Hence how do I get my function to apply to the data frame in a row-wise fashion ?
If you want to stick with ` pandas ` , based on answer from @USER you can replace the last line with , ` >>> u [ 1 :] = q >>> u= u.cumprod() `
Then I can groupby the two columns to aggregate : #CODE
To get it in the same format , I can ` unstack ` the ` dataframe ` : #CODE
The approach of @USER with ` groupby ` and ` unstack ` is certainly fine , but for completeness , you can also do this directly with ` pivot_table ` ( see doc ) [ version 0.13 and below ]: #CODE
your code does nothing because you are not appending to your ` dfs ` list , don't you want to replace the line ` data = pd.read_csv ( filename )` with ` dfs.append ( pd.read_csv ( filename )` .
You would then need to loop over the list and ` concat ` , I don't think ` concat ` will work on a list of ` df ` s .
also you are mixing an alias for the module with the module name in your last line , shouldn't it be ` big_frame = pd.concat ( dfs , ignore_index=True )` ?, anyway once you have a list of dataframes you will need to iterate over the list and concat to ` big_frame `
you need to loop over ` dfs ` now , so something like ` for df in dfs : big_frame.concat ( df , ignore_index=True )` should work , you could also try ` append ` instead of ` concat ` also .
Because ` concat ` should handle a list of DataFrames just fine like you did .
I stand corrected on the use of ` concat ` after reading the [ docs ] ( #URL ) , it should work like @USER says , can you edit your question and post the error message
Also , apparently ` join ` stands for ` os.path.join ` ( I might be wrong ) .
In any case , I replaced ` join ( directory , file )` with simply ` files ` and I got what I needed .
Your code in the other question was just fine , just replace ` read_csv ` with ` read_excel ` .
( 5 ) Loop over list of files to append to empty dataframe : #CODE
This is certainly OK , but I think the approach in the almost identical question #URL to append to a list and then ` pd.concat ( the_list )` is cleaner .
Instead of setting up a meshgrid , just unstack .
' module ' object has no attribute ' cut '
I am trying to use the pandas " cut " method , but am getting an ` object has no attribute ` error .
I'm guessing it doesn't have the method ` cut ` .
The ` cut ` function was introduced in pandas v0.8.0 .
how to do right join where key is null in python pandas
Is it possible to do a right join where key is null in python pandas .
That is , can I join DateFrames to produce only values from the right that do not match the left ?
You could make a dummy column of ones in each dataframe , use pd.merge() to right join the dataframes , and then use a mask to filter out the nulls .
I am looking for a vectorize way to do this but the closest thing I found on stack shows the problem can't be resolved because 2 values update depend on the i term see : #URL
I want to transform this data of the form .
I'm trying to use Python's Tfidf to transform a corpus of text .
Re-shaping pandas data frame using shape or pivot_table ( stack each row )
What I want is to stack it so that it takes the following form , where the columns identifiers have been changed ( to X and Y ) so that they are the same for all re-stacked values : #CODE
But instead of appending all columns to the end of the next , I just want to append a pairs ( or triplets of values actually ) of values from each row .
So I want to stack each individual row selectively ( e.g. by pairs of values or triplets ) , and then stack that row-stack , for the entire data frame , basically .
So on top of finding the ` inf ` s , we should stack the column index into the rows ( unpivot , if you will ) then drop all the ` NaN ` values .
my general advice is to create a hierarchical index on the columns and then simply use the ` stack ` method of the dataframe .
Then if you ` stack ` the dataframe , you get this : #CODE
In the future I may need to pre-process ` emission ` ( apply ` cumsum ` or other functions ) before computing ` counts ` .
I save ` emission ` as an Extensible array ( ` EArray `) , because I generate the data in chunks and I need to append each new chunk ( I know the final size though ) .
However , we don't know the arrays size before starting and a final ` hstack() ` call is needed to " merge " the different chunks stored in a list .
Python Pandas interpolate with new x-axis
I want to interpolate the data so that I have a new series to cover ` X =[ 23:392 : 1 ]` .
This can be done with ` pandas `' s ` reindex ` and ` interpolate ` : #CODE
The idea is the create the index you want ( ` s.index + idx `) , which is sorted automatically , reindex an that ( which makes a bunch of ` NaN ` s at the new points , and the interpolate to fill the ` NaN ` s , using the ` values ` method , which interpolates at the index points .
Let me know if thou think just passing an array to ` interpolate ` would be easier .
We decided to keep the API ( and implementation ) simple and use ` reindex ` ing instead .
Now I can loop through the entire thing and ` split ` , but can I do it with map ?
Something like - ` data [ ' i '] = map ( split_and_get_first_part , data [ ' interest '])` ?
Is there a specific reason to use map instead of a list comprehension , and did you try writing ` split_and_get_first_part ` already ?
You could use the ` map ` method : #CODE
I don't know why I didn't think about using the ` map ` method !
It's possible that ` map ` is a more efficient way of looping than ` for ` in this case , but you'd have to do some tests to see if that is the case .
It's worth noting that in Python 3 , ` map ` returns an iterator rather than a list , so , depending on what you're doing with it , it may well be more efficient in Py3 .
I just tested this on ` pd.concat ([ data ] *1000 )` and found ` str.contains ` to be 2x slower than ` apply ( lambda x : ' Fruit ' in x )` .
The difference is that these are actually slightly different operations . contains is more like this : `` reg = re.compile ( ' Fruit ') ; data [ data [ ' Type '] .apply ( lambda x : bool ( reg.search ( x )))`` ( also contains handles `` nan ``
And then selecting the relevant portions of the data frame with ` df [ mask ]` .
Merge Duplicates based on column ?
Instead of the ` map ` , you could use ` df [ ' fruit '] .str .lower() ` here .
Also , it looks like I did successfully drop the non ' M ' events , which was the goal .
You can do a monthly ` resample ` on each group .
And then set it as the index , groupby on ` [ ' Code ' , ' ID ']` and then apply a ` resample ` on each group : #CODE
But you can also work further with your results , ' unstack ' ( bring index levels to columns ) and then plot : #CODE
in a pandas dataframe how can I apply a sort of excel left ( ' state ' , 2 ) to only take the first two letters .
Don't know what exactly you want for middle , but you can apply arbitrary function to a column with ` apply ` method : #CODE
I have another example where i am try to apply the first two digits of an 8 digit number . then i get the error .
' invalid index to scalar variable ' how can i apply the above to take the last 2 numbers in ' year ' ?
The same is true if I replace ` np.min ([ 0.5 ])` ( which is a ` np.float64 `) with a float : ` 0.5 ` .
I would like to make it so that I have a table with the values of column 0 on the x axis and the values of column 1 on the y axis with the corresponding value of the pair at the intersection of the rows and columns such that it looks like :
To get you started , you're looking for the ` set_index ` method and the ` unstack ` method .
You want a [ pivot table ] ( #URL ) .
Python Pandas : Join on unique column values and concatenate
How can I join these dataframes based on ` id ` and then concatenate their columns together ?
There may be a better way with ` concat ` , but this should work : #CODE
I come pretty far by playing with the groupby and agg method .
I can't seem to find a way ( or the right place to look in the docs ) to properly group and aggregate the data in the way I want .
Basically , there's no need to apply the set of functions to the two groups separately and append the results together .
That's essentially what group by is doing : split , apply ( separately ) and combine .
I've also tried faking a value column of all 1's and pivoting , but almost immediately get a memory error , likely because pivot produces a dense DataFrame .
what for are you building this pivot ?
For basket analysis , it is a clustering method , and I am pretty sure all cluster methods can deal with your initial data without need of pivot .
I don't think you will a memory issue with this , as the final result won't be that big ( and so the unstack won't blow up ) #CODE
just do it iteratively then , e.g. do a section , repeat , do a section , then join the resulting frames .
I want to append pandas data frames to the ends of CSV files .
The tricky part is when I append rows , some of the times the columns may be different .
Your best bet is to join the DataFrames into one with all the columns represented .
How to do expanding window by group in pandas
In particular I want to do an expanding average of the difference between two dates in a series , by group .
To do this I need to aggregate all previous interactions whenever a point of interest ( either ` viewed_banner ` or ` viewed_and clicked_banner `) shows up in the feed : #CODE
I have tried to look at Pandas ` apply ` and ` groupby ` methods , but can not come up with something that generates the desired overlapping groups .
` cumsum() ` the counts and drop even rows will get the right counts .
thanks but how do I apply that to a dataframe .
I want to interpolate this gaps either with the monthly average or a moving average ( which neglect missing values ) .
I can resample the monthly daily average as well but I don't know how to tell Python it need to use for each gap the meanvalue for this specific month .
I'd propose to interpolate the missing values from the two surrounding values ; that should be closer to the real missing value than a monthly average .
Pandas has a nice ' interpolate ' function on both series and dataframes : ( #URL ) .
If for some days the device broke down and it interpolate just the values before and after the gap then I don't get the course over the day and it's in general to inaccurate .
If you replace the missing values with ' monthly average ' than all of those stats will be incorrect .
I need to interpolate it cause I want to sum up all values per year to create a water balance .
So I need to interpolate .
You could replace ` df [ " class "] == 1 ` by another condition .
I think " No numeric types to aggregate " is telling you that pandas is trying to take the average of a list of Series ( not the average of the numeric data they contain ) which is not defined .
when apply this concept to my larger working file I seem to lose the order of my columns .
If you want to just return all of the available listings when there are fewer than 3 , then you can just add an extra line ( ` if len ( x ) <= n : return x `) .
Pandas , concat Series to DF as rows
Since you can append two DataFrames by row
While not especially efficient ( since a new object must be created ) , you can append a
single row to a DataFrame by passing a Series or dict to append , which returns a new DataFrame as above .
and attempting to append " S1 " produces an error .
You were close , just transposed the result from ` concat ` #CODE
Tom , Using your answer , if I now want to concat another Series to the result I get : AttributeError : ' Series ' object has no attribute ' _data '
Can you edit your question to show the third Series you want to concat ?
Please see the edited version of the problem , the concat operations do not happen at the same time .
Once you have the DataFrame from my answer , you can add additional rows / columns with ` .loc [ 2 , :] ` or ` append ` or ` join ` , take a look at the [ docs ] ( #URL ) .
is to just convert the data to an array of lists and append the array ( s ) to the DataFrame .
So I want to replace #CODE
" Google or learn " is a generic attitude that could apply to all forums , especially Q& A-style forums .
I just needed to understand how to apply the string operations to the dataframes / series , but I guess that's as simple as it is for strings .
When I tried to insert the data through : #CODE
I have tried ` apply ( lambda x : set ( x ))` but it only works on individual lists as opposed to the entire column .
In that case you want to look at ` resample ` .
So if the DataFrames are in the right order , you can simply use ` concat ` like so : #CODE
this will raise starting in 0.13 , you need to `` apply `` to your groupby to get back a dataframe , see : #URL
My attempt : I tried to replicate ` df3.col1 ` ` len ( df.columns.values )` times to get a dataframe that is of the same dimension as ` df ` : #CODE
I know I can take the transpose with ` df3.T() ` to get what I need but I think this is not that the fastest way .
Another way is create list of columns and join them : #CODE
I would like to use the ` pandas.rolling_apply ` function to apply my own custom function on a rolling window basis .
is there a way to get both columns at the same time to say calculated an customised rolling correlation / regression ?
I would like to apply the following function to a pandas data frame .
You will need to [ index ] ( #URL ) properly , is this a pre-existing dataframe or are you populating and creating from scratch , you can index using integer or label based , if you were doing this from scratch then you would need to keep track of the row number as you append each entry so something like ` results.iloc [ row ] [ ' siteClean '] =host ` or similar .
If I understand your problem correctly , you want to look at a single column from your frame , and apply a function on each element of that column .
Then you can use ` map ` to generate a new ` Series ` whose entries are those of the input transformed by the specified function .
It can be a new column ( in your old ` DataFrame ` or elsewhere ) or it can replace the old column .
Note that ` map ` only works on a ` Series ` , so be sure to select down to one column before using it : #CODE
then you would replace the old column with the new one :
How can I use transform to extract a feature based on multiple conditions , using pandas
I've been trying to use groupby in combination with transform , but don't know how to get transform to take account of conditions based on other columns .
First change the ` bool ` column to actually booleans ( also be careful with your names . DataFrame has a ` bool ` method ): #CODE
Use the ` bool ` column to index into ` df ` : #CODE
There are probably a bunch of ways to set the values , but one is to set ` Group ` as the index and ` join ` the ` dates ` .
I had to convert the series to a dataframe and then perform a left merge : ` pd.merge ( df , dates , how= ' left ' , on= ' Group ')`
One way I could think of was to put the treated cell in a one-cell DataFrame and then doing a left merge on it , but this is much too inefficient for such big data .
You can then refer to the row , or specific entry , using loc : #CODE
How can i strip away those three unwanted lines , including \cf0 and \cell\row at the beginning and end of each text ?
If I replace the ` plt ` command with the pandas command , so I have : #CODE
Added ` bottom= 0.1 ` to ` hist ` call fixes the problem , I guess there is some kind of divide by zero thing , or something .
I'd recommend using the ` log=True ` parameter in the pyplot hist function : #CODE
I was doing something similar , where I added a diff column , set the values to 0 if it was below my cut , and then ` cumsum ` ed that .
If you are using UNIX type OS , please check if you care just about merging files how to merge two files consistently line by line
I'm sure one can invent some workarounds for passing 2d arrays , but in your case , you can simply precompute row-wise values for rolling evaluation : #CODE
gm still getting array of floats so i have to map them to ints to be used with iloc
And concat : #CODE
You might consider passing the families as the ` key ` parameter to ` concat ` if you want a hierarchical index for the columns with ( family , species ) pairs .
However if i insert a new column and repeat the same procedure the new attribute is stored but old attributes are lost #CODE
You can use the tilde ` ~ ` to flip the bool values : #CODE
And merge back ( adding 1 to start from 1 instead of 0 ): #CODE
Since in general , we don't know in advance all the items of the panels , I thought of using ` items = Index ( set ( np.concatenate ([ panel.items for panel in listOfPanels ])))` to get them , then reindex the panels afteward .
It looks like you want a ` rolling_max ` and ` rolling_min ` combined with a ` shift ` since you don't want to include the current value in the calculation : #CODE
It looks like what is passed to the aggregate function fn() is a data frame including the key .
In my understanding , the agg function is applied to each column separately and only one column is passed .
So how would you write a custom aggregate function ?
Compute * rolling * maximum drawdown of pandas Series
Now say I'm interested in computing the rolling drawdown of this Series .
I took a shot at writing something bespoke : it keeps track of all sorts of intermediate data ( locations of observed maxima , locations of previously found drawdowns ) to cut down on lots of redundant calculations .
Here's a numpy version of the rolling maximum drawdown function .
This is quite a complex problem if you want to solve this in a computationally efficient way for a rolling window .
here we compare to the results generated from my efficient rolling window algorithm where only the latest observation is added and then it does it's magic
Definitions are reusable for multiple rolling window sizes in the same script .
In a python pandas DataFrame , how do you shift row indexes up to fill empty rows ?
So I want to preserve the data , but shift the indexes together .
The ` drop=True ` says not to try to insert the original index into the DataFrame .
Basically create a repeating list just bigger than original dataframe , and then left join them .
You could use the map function .
If performance is a concern , you may want to run some tests : I'm not sure the map function is vectorized , so it may be faster to do a couple of passes as in the answer above rather than use map to do it in one line .
Create multiindex based on single index
Related #URL I guess the difference is how to separate out into multiple sql jobs and concat e.g. WHERE id is within a certain range ( but I suspect this will rarely be worth the effort in itself ) ...
You might want simply do ` df = df.reset_index ( drop=True )` to reindex your dataframe .
This required a standard Index ( Vincent can't parse a MultiIndex ) .
( Once I have the MultiIndex , I'm then resetting the index and merging the two columns into a single string value for the presentation stage . )
if len ( x.split ... )
And now , instead of ` value_counts() ` , use ` apply ( f )` .
in _save_chunk ( self , start_i , end_i ) 1094 ix =
-> 1096 lib.write_csv_rows ( self.data , ix , self.nlevels , self.cols , self.writer ) 1097 1098 # from collections import
So , say I want to perform this calculation on every ratio in the row , and then take the median of the three resulting values : #CODE
To apply a function to all groups , you can do that directly on the groupby result , eg ` df.groupby() .apply ( .. )` or ` df.groupby() .aggregate ( .. )` .
Can you give a more specific example of what kind of function you want to apply to the ratios ?
To calculate the median of the three ratio's for each sequence ( each row ) , you can do : #CODE
The ` axis=1 ` means that you do not want to take the median of one column ( over the rows ) , but for each row ( over the columns )
Another examle , to calculate the median of all Ratio1's for each ID , you can do : #CODE
Here you group by ` ID ` , select column ` Ratio1 ` and calculate the median value for each group .
To apply a function on each row of a dataframe , you can use ` apply ` : #CODE
I basically want a median of the three ratios for each sequence , and some other statistical calculations ( std , cv , etc . ) .
In the later stages I want the medians for all Ratio1 of ID1 , medians of Ratio2 , Ratio3 , and then another median of the medians from the ratios .
What if I wanted to perform some other , non-standard calculation on each ratio , and THEN calculate the median of the resulting three values ?
Maybe just calculating and assigning it to a new column , and then taking the median .
thanks It get the values of the array , but it don't include mask array .
How can I get the mask array if I use pandas directly ?
I need to compute some sort of a rolling mean for each team scores ( well , not exactly the rolling mean to the letter ) .
Now , I need to compute " rolling mean " for teams .
You can apply an ` expanding_mean ` ( see docs ) to each group : #CODE
I would like to replace every line that does not have " GE " in the " from " or " to " column , by two lines , one having " GE " in the " from " column and one having " GE " in the " to " column .
In the example above , I would replace the third line by the following two lines :
I tried using " apply " but I can't figure out how to return a correct data frame .
And concat together #CODE
Set the index to the exchtime , resample at 1second interval and take the max
concat part of string from groupby pandas python
How do i concat part of a date with a file name after doing groupby .
The problem is that you are concatenating a pandas Series ` Fx [ ' File ']` with the string representation of a pandas Series ` str ( Fx [ ' Date '])` , what you need to do is apply the ` str ` cast function to the elements of ` Fx [ ' Date ']` like this : #CODE
Pandas boxplot x-axis setting
I want to create a boxplot of data collected from four different sites over the past twenty years ( i.e. each site will have 20y of data ) .
Creating a boxplot is not a problem ; offsetting the boxes does seem to be an issue .
You can see which dates failed to parse by checking ` isnull ` : #CODE
What is wrong here , and why won't all the values map to the correct ID ?
Drop NAs if you haven't already , then explicitly convert whole columns to the appropriate dtype as needed .
I have also run into strange problems sometimes if my index wasn't continuous ( if you drop NAs , you may have to reindex ) , or if my x-axis values weren't pre-sorted .
Or drop the letters , as you suggested , and convert objects to numeric instead :
How can I transform the ` DataFrameGroupBy ` object displayed above to my desired format ?
First , groupby code and colour and then apply a customized function to format id and amount : #CODE
A dictionary map the code , colour pair to a dataframe contains id and amount ?
I have tried using numpy.ma to mask out the -1 values .
The simplest solution is to parse your ` Dataframe ` and replace your price of ` - 1.00 ` by ` np.nan ` with something like : #CODE
Apply read_csv instead of read_clipboard to handle your actual data : #CODE
@USER as ` df [[ ' code ' , ' colour ']]` builds a bunch of new objects ( a new dataframe included ) , this alone can lead to more performance loss than possible groupby performance gain from smaller dataframe ( not sure if any exists , as all computations on additional columns is done after grouping in transform or aggregate phase )
You can use ix method : #CODE
hey i can call specific column ( label ) s via that call also using loc .
Have you tried appending that to a list and ` concat ` enating the results ?
I would warn from personal experience that it may be easier for you to read in each type of data separately and then merge them after you have done some error checking / munging etc .
I'm trying to drop rows from a groupby object based on a condition : #CODE
With a dataframe , if I wanted to drop anything with a value less than 10 , I could just call ` df [ df [ ' value '] =10 ]` .
But you can do that in the resulting object , after you apply your aggregation function .
The same should apply for the density plots on the diagonal .
Pandas time series merge : result longer than components
Length : 2863 , Freq : None , Timezone : None
Length : 22992 , Freq : None , Timezone : None
Maybe I am missing something , but how can an outer merge / align ever yield more data points than the two constituents together ?
I have now managed to find the source of the above behavior by isolating the parts of the dataset that produce the " excess data " upon doing the outer merge .
As it turns out ( and is actually documented for Pandas - my bad here ) , the outer merge produces a Cartesian product for data points with the same index value ( in my case the same ms timestamp ) .
Now , I can resample this into OHLC data using ` df.resample ( freq , how={ ' price ' : ' ohlc ' } )` , which is fine , but I'd also like to include the volume .
When I try ` df.resample ( freq , how={ ' price ' : ' ohlc ' , ' volume ' : ' sum ' } )` , I get :
The problem isn't with the resampling , it's from trying to concat a MultiIndex ( from the price OHLC ) , with a regular index ( for the Volume sum ) .
I guess you could manually create a MultiIndex for ` ( volume , sum )` and then concat : #CODE
But it might be better if resample could handle this automatically .
Is the best way to fix this to convert it string , replace the null values , then convert it to datetime again ?
You can ` unstack ` the result : #CODE
Merge of multiple data frames of different number of columns into one big data frame
You need to decide in what axis you want to append your files .
If you need , you can transpose the resulting DataFrame back with ` d.T ` .
Can't you use axis=1 to concat to avoid .T ?
I must have been either using pandas 0.12 or doing something else wrong when I tried axis=1 with ` concat ` and failed .
How to apply hierarchy or multi-index to panda columns
You need to provide an example that's faithful to your context so that our solutions apply .
Groupby ` Group ` first and then apply your customized function : #CODE
Do you mean that when you use an apply on a groupby mapping , you can treat the object received by apply as a portion of the original df ?
Series.apply() will apply a function to every element of the Series .
Groupby will apply a function to every portion of the DataFrame that matches a value in the grouping Series ( es ) .
It must group so that each change in " loc " and / or each change in " name " is treated as a separate group .
The default grouping function ( correctly ) groups all the loc and name values so we are only left with 3 groups ( john / abc is 1 group ) .
Instead , compare consecutive rows by using ` shift ` .
While looping through the Data Frames , using a list to append the unique values throws a memory error ( The range mentioned in df1 , df2 , df3 is just an example . The actual range could be millions ) .
Having B column flattened , you can merge it back : #CODE
The docs for ` resample ` are a bit thin , unfortunately ( #URL ) ...
= 0 ` creates a boolean DataFrame where ` df ` is nonzero : #CODE
` ( df ! = 0 ) .any ( axis=0 )` returns a boolean Series indicating which columns have nonzero entries : #CODE
I currently have a series of 18 DataFrames ( each representing a different year ) consisting of 3 Columns and varying amounts of rows representing the normalize mutual information scores for amino acid residue positions like :
But when it comes to analyze daily datas if i change resample to " W " in day_min ( 4th line ) , and to " D " in 6th line it gives this error : KeyError : Timestamp ( ' 2013-01-01 00:00 : 00 ' , tz=None )
You can use TimeGrouper , by week ( and see whether the value is equal to the week's min ) and then resample by day : #CODE
If you wanted this as columns as days of each week , you could do the groupby within the apply : #CODE
You could use a groupby apply for this : #CODE
I'd like to pivot my data which results from a django queryset while maintaining the original ( non-alphabetical ) sort order on the index column .
I've hacked together my own code to do the job but it's a bit ugly and I was wondering if it could be done using a pandas DataFrame pivot .
Using pandas pivot produces the following results .
The pivot worked but the rows are in the wrong order .
I've investigated using google visualization pivot but that only seems to work on a query not on an existing DataTable .
now concat #CODE
Also worth mentioning I would like to apply this to datasets of up to 1 million rows .
In that case , try converting the mask to dtype ` int ` before applying ` cumsum ` : #CODE
np.putmask ( result , mask , pa.NA )
I want to append a ` reason ` column that gives a standard text + the column name of the minimum value of that row .
It seems like the transpose approach is twice as fast .
@USER : Using ` apply ` to do this with a ` lambda ` expression is a worse solution than using the builtin ` idxmin ` directly on the transposed data .
python pandas : apply a function with arguments to a series .
I would like to apply a function with argument to a pandas series : I have found two different solution of SO :
python pandas : apply a function with arguments to a series
Passing multiple arguments to apply ( Python )
I have a Date Time series from which I am trying to select certain elements , along with their date time index in order to append them to a new series .
( I don't just want to print it , I want to append some to a new series . For example , add all the numbers divisible by two to a Series , along with their index ) .
Would I have to use ` reindex ` ?
Don't completely get what you mean with " I want to append some to a new series " , but you can access index with ` index ` property : #CODE
But if anyone has a hint for a workaround I'd be glad to hear ( I tried passing a tz directly to the DateFormatter . That works , but the Locators don't seem to like it much ) .
Intersection of date ranges , calculation over all elements with an intersecting date range
if you're using pandas and assuming the date are in datetime format , you can group by on ' code ' and then apply a min , max function to that .
What I meant was you shouldn't create an empty DataFrame ** or ** concat with an iterrows ( just concat , or join / merge ) ...
I want to ` concat ` a row , because I'm validating it before putting it into the new ` DataFrame ` .
Now once you have your line of text insert it into a string and then split using commas .
` merge() ` can't do this kind of join , but you can use ` searchsorted() ` :
do a regular merge with outer join ( how= ' outer ') ;
drop all the rows you don't need from the outer join .
Python pandas group by aggregate statement print
Pandas sort by group aggregate and calculate sum of two colums
So ` apply ` gets called once for the column or once per element ?
FYI You can create those tab numbers like this : ` list ( map ( str , range ( 1 , 2 8))) ` .
If you want it as ` int ` you can map the list using ` map ( int , list1 )`
I think you can bring the concat out of all the indentation .
Note : This does more concats , but I think it's worth it for readability , you could do just one concat ...
I just can't make it , no matter what I try ( I tried ` apply ` with ` axis=1 ` and have it return a tuple , a list , a Series object .. neither worked ) .
How do I do it with ` apply ` ?
For various reasons , what I'd like to do is define the outliers by group : so , for example , drop the rows in the dataframe that are in the top xth percentile of their group , or the top n in their group .
I could iterate through the groups , I suppose , and for each group find out which rows to drop , and then go back to the original dataframe and drop them , but that seems terribly clumsy .
Is there a more general way of doing it , for example if I wanted to drop ( or keep ) only those rows that had a value above the group mean or mode ? or that were within the top xth percentile ?
@USER you could do a groupy apply to return just those rows ...
One way to get the desired result is to use an apply e.g. via the following function : #CODE
You could use set to create a list of the index locations that conform to your rule , and then use that list to slice the data frame .
My problem is that I cannot get how to merge the index properly .
Well , I'm not sure that merge would be the way to go .
You can use concat : #CODE
Hi , I have found the issue : I had a duplicate row index in my data that was making the concat function raise an exception .
Representing a matrix as a multiindex DataFrame is kinda strange , what use case do you have in mind ?
How to index into a pandas multindex with ix
It's worth mentioning that you can use ` loc ` ( which I think is slightly cleaner ): #CODE
I found this code in Stack ( show feature names after feature selection ) but don't totally understand it and have not been able to get it to work .
Assuming df1 is your first dataframe and df2 is your second one , you can merge on the country and perform a left join , you need to rename the country column on df2 first though : #CODE
Note : perhaps it makes more sense to concat rather than append : #CODE
You can also try this : merging two tables with millions of rows in python , where you use a merge function that is simply ` drop_duplicates() ` .
Basically you just have the function that does ` row / row.sum() ` , and you use ` apply ` with ` axis=1 ` to apply it by row .
Another option is to use div rather than apply : #CODE
If you're looking for a percentage of the total , you can divide by the len of the df instead of the row sum : #CODE
do I have to ` merge / join ` that info in later ?
This should truncate everything before the old date .
If the ` new_index ` ( combination ) already exists in the ` multi_df ` , I want to append the ` new_id ` to the existing index .
Here is my debug code , when you do indexing , Index object will create ` _tuples ` and ` engine map ` , I think the memory is used by this two cache object .
numpy diff doesn't deal properly with timedelata ( well it works but it's pretty raw ); you should have at least numpy 1.7
I wanted to use Pandas ' considerable resources for time series data to transform and analyse these coordinates as ` TimeSeries ` objects .
The problem with the first approach is that I have no way of accessing my categorical data ( i.e. the ` subject ` , ` stimuli ` , and ` resp ` columns , amongst others I've left out here ) , while the problem with the second is that I end up with a ` DataFrame ` thousands of columns wide ( and wider again for each transformation I apply : velocity at each step , angle at each step , etc ) , and no useful way of accessing specific time serieses ( i.e. what I've been currently calling as ` data.rx.mean() .plot() ` .
Python pandas concat intersect
I am struggling hard to get an intersection of two DataFrames in pandas .
All i want is to find an intersection of 2 DataFrames by index so only lines 8 through 10 appear in the result .
Why does concat over axis=0 doesnt work ?
You want to try a merge .
` concat ` is better for gluing multiple frames together , merge will handle detection of overlapping rows and all that : #CODE
Resample and align #CODE
I have a DataFrame where each element is a numpy array and I would like to apply to them numpy functions .
now let's try to apply ` np.dot ` along ` axis=1 `
First , use ` reindex ` to set the order of columns
Why doesn't my apply function return the length of the string ?
Just for the sake of trying something , does the same error happen if you replace your use of ` apply ` with ` map ` since you're looking to spray the operation onto a single ` Series ` object ?
What if you try ` csv [ ' text '] .apply ( lambda x : len ( str ( x )))` ?
and you can also use ` map ` instead of ` apply ` since you're operating along the values of a ` Series ` .
Is the issue that result of [ 11 ] is set not a list ( just apply set to it ) .
The only change I had to make is to replace datetime with datetime.datetime #CODE
Now , I ` apply ` it to the dataframe : #CODE
You should use bool rather than ' Y ' and ' N ' ...
The thing you have to realize about apply is you need to write functions that operate on scalar values and return the result that you want .
Just want to clarify that when using ` apply ` on a series , you should write function that accept scalar values .
When using ` apply ` on a DataFrame , however , the functions should accept either full columns ( when ` axis=0 ` -- the default ) or full rows ( when ` axis=1 `) .
OK - I think I just figured it out - to use functions on a dataframe , you have to use ( should use ) apply .
So , I can chain together functions by using apply inside of the main function .
It's worth noting that you can do this ( without using apply , so more efficiently ) using ` str.contains ` : #CODE
FYI , your method of using `` loc `` to convert doesn't work because pandas doesn't check whether the entire column is converted ( as you could have done something like `` df.loc [ 1:5 , ' one '] = df.loc [: , ' one '] .astype ( float )`` , so for efficiency we don't force this type of conversion . you could have done `` df [ ' one '] = df [ ' one '] .astype ( float )`` which would have worked ( but @USER soln is in general better for float conversion in any event ; `` astype ( float )`` would fail if their was a non-convertible entry ( e.g. a ' foo ') in your data ) .
I originally used df [ ' one '] but it did not work so I went and tried " loc " .
Step-by-step , first we stack : #CODE
If you need the data in one file , then at the end you can just stack them back together in order .
yeah , should be closed brackets around ix : ` stateDF.ix [ targetState , " Full Name "]` ...
I want to transform my data frame #CODE
If you reversed the keys and values in your dict then you can just use ` map ` : #CODE
then call map : #CODE
This assumes that your keys are present in the map , if not you will get a ` KeyError ` raised
now call map to perform the lookup and assign back to state : #CODE
If I apply map() , the corresponding values in the new series are missing .
Can I somehow specify that I want to apply identity function for the values not present in the dictionary , i.e. leave them as is ?
You could create a function that did a map lookup , if the key was not there then do nothing , or you could filter which values in the dataframe to assign to , only those which are present in the dict and convert those .
But since I'm only interested in the values in the multiIndex , I added to your solution >> ` c = pandas.Series ( df [ df == 100 ] .stack() .index .get_level_values ( 1 ) , index =d f [ df == 100 ] .stack() .index .get_level_values ( 0 ))` .
How to plot a pandas multiindex dataFrame with all xticks
This reports the ` dtypes ` as ` float64 ` as expected , any idea why ` append ` with a dict is not working as expected ?
This is fixed in master / 0.13.1 ( when creating the frame to append it was not converting the object dtypes , which is what a dict starts out as ) . see #URL
how to aggregate by month of datetime in pandas frame ?
Make your data into a TimeSeries object and then call resample : #CODE
You should also just do this to strip the whitespace : #CODE
But to compare two runs together I need to have a common X value to stack them on top of each other .
For the example below I have used the largest value in that dataset to " pivot " on .
My final aim is to have a dataset and methodology that means I can investigate the similarity and differences between different runs using different " pivot points " amd produce a graph of each one which I can then interrogate ( or at least tell which data set is which to interrogate the data directly ) but couldn ; t get past various errors with creating it .
Is it possible to specify more than one column for the merge in pandas ( python ) ?
I want to merge two tables , A and B .
if so , you can create a third column and use that to merge
You mean , merge B1 and B2 first ?
When you specify columns to join on , make sure to refer to them as strings .
If you need to refer to two columns in your join , use a list of strings .
I have multiple dataframes which I want to merge based on a string representation of several " integer " columns .
However , when one of those integer columns has a np.nan , the string casting produces a " .0 " , which throws off the merge .
Is this like a pivot table , where the values below the value are empty but implictly the same as the one above ?
If you can have a NaN in the FK column , then could you replace the NaN's with some other random number , let's say if all current FK's are positive integers , then use a negative int like -999 to distinguish empty values .
Then apply the filling on NaN values , and save this dataframe to a new variable for loading into the database .
using apply you can make new value and assign it to new column
for R folks : there is no ifelse in direct form ( but there are ways to nicely replace it ) .
The only tricky portion is the list comprehension ` [ m.append ( msg ( m )) for m in records ]` which iterates through all of the records , and modifies each to append your new field , created via a call to ` msg ` .
In case of two dimensional data , the simple map function will not work directly .
Just a quick comment , the original map function converts the 2d data to a 1d data ( sympy.latex works well with lists ) , hence it is required to take care of each cell individually -> ravel reshapes the 2d array into 1d and the subsequent reshape builds up the 2d version again .
Lastly when using cut is there a way to specify infinity so the last bucket is all values greater than say 60 ?
You can reindex by the categorical's levels : #CODE
I just wanted to know if they was a faster way to merge the data of the two DataFrame together .
I tried merging the data each time with the merge function but process took way to long that is way I have gone to creating a larger DataFrame that I update to improve the speed .
And concat everything together when you want ( they are kept in a list in the mean time , or see my comments below , these sub-frames could be moved to external storage when created , then read back before this concatenating step ) .
then at the end read them all in and concat ( in memory ) , and write out a gigantic new file .
The concat step could be done all at once in memory , or if truly a large task , then can be done iteratively .
The only dependent part is the concat .
The 2nd option of concat the DataFrame and then reindexing increased the run time by about half .
It takes about 10 seconds to run ` df.col1.map ( lambda x : len ( x )) .max() ` when timing it with IPython's ` %timit ` .
You might be able to save some time by simply using ` map ( len )` -- the ` lambda ` does nothing but waste time here .
I came to the same conclusion when DSM commented on ` map ( len )` .
Shaves off about 40% compared to ` len ( lambda x : len ( x ))` method .
I want to join them to receive : #CODE
In words , merge a single-level indexed dataframe into a hierarchical index dataframe .
This doesn't look that general to me , and I believe their is a way to do a merge directly , though maybe not w / o modifying the 2nd frame ( to make a merge key , e.g. this is kind of like cross merging ) .
I am not sure how to work the " for each city " -part into a custom aggregate function and get access to group-level summaries .
E.g. you could use groupby apply with ` def f ( x ): return ( 1 . * x [ ' weight '] * x [ ' jobs ']) .sum() / x [ ' jobs '] .sum() ` but it will probably be less efficient than the above .
I've looked at unpack / pivot / deaggregate ( couldn't find the right solution with any of these terms ... )
Also tried to aggregate with some smart lambda , but didn't succeed .
You could do this as a one line apply ( the first column being negative , the second positive ): #CODE
Is there another simple way to evaluate for the existence of a key without having to join strings ?
then use apply to get your " Type " #CODE
This will be faster than the apply soln ( and the looping soln )
How can I merge the ` round ` and ` square ` values under ` shape ` Series as ` other ` ?
( In R terminology , I want to merge the ` round ` and ` square ` levels of the ` shape ` factor into a new level labelled ` other ` . ) #CODE
This is a use case for the new ` query ` / ` eval ` functionality in pandas 0.13 #CODE
In the full operation , I split the dataframe into three , add three new columns using different equations , and append back into the final data frame .
This should be extremely memory efficient and fast , because we update by reference ( and no the entire join is not materialised ) .
` on = " crystal "` performs a join on that column and finds matching row indices in ` data ` corresponding to each row in ` key ` , and on those matching rows , we simultaneously update / create the necessary columns .
Make a Pandas MultiIndex from a product of iterables ?
You could create a dict that flips the values and call map , this would return a series and you can create a new dataframe and leave the original intact : #CODE
An easier way to describe your function is as x -> 1 - x , this will be more efficient that apply / map .
In Pandas , how do I append a sum of all columns of a dataframe to the bottow row ?
The reset_index ( drop=True ) is to fix up the index after the concat and dedupe .
Can also use ignore_index=True in the concat to avoid dupe indexes .
It is probably obvious that I am doing some cut and paste coding here :-( ( using this as a guide : #URL
But ` headers ` does not cut it .
Is there a simple way to either transpose the Series , or otherwise get it written as a row ?
If the ` category ` already exists , I want to get the ` description ` from the dataframe , append some text to it and then update the record .
But I get ` TypeError : Unhashable list ` I looked at several answers on Stack and can't find out where I passed a list into the loop .
I think this behaviour is deprecated then changing , it'll return bool if there is no groups ( and the things in brackets are considered groups ... : s ) .
To explain the behavior of the depreciated ( in 0.13 ) match : It now returns bool unless there are groups in the pattern ( here the parenthesis are groups , hence C is returned in one row ) ...
transpose the data frame ( takes ages )
If I resample this DataField by any frequency , the timezone is kept : #CODE
their are a couple of outstanding bugs w.r.t to resample and extra binning : #URL if you would like to investigate and try to pinpoint ( or better yet fix ) would be appreciated !
If you want to use counts as a DataFrame , I'd then transform it back #CODE
I changed the last line in your code like this : ` counts = df.groupby ([ df.day.to_period ( ' D ') , ' col_name ']) .agg ( len )` , as I need to group data by day and not by exact time from the index .
However I still cannot plot this data as I cannot ` unstack ` them .
But as ` unstack ` doesn't work , maybe there's another way to plot this data ?
Pandas rolling window appears to introduce offset to rolled data
However , so far all windows which I have tried to apply , over varying window lengths from 2 to 100 , appear to offset the smoothed data to lower values , e.g. :
To verify correctness apply the rolling window on a step function .
Take the intersection of their indices .
Then take the intersection of these as defined before .
Later you may wish to do something like the following to get the ` df ` at intersection index .
Say I have a few 2x2 " chunks " that I want to aggregate into a dataframe : #CODE
#URL I think you can do this by specifying `` levels `` and `` keys `` and not using a full multiindex on the constructing of the subframes .
Then apply an iterator to that where you select based on those rows .
This is the function you'll apply to each of the lists in ` groups ` .
You will need to convert to floats ( if necessary , it won't work for ints ) for this to work see the docs on ` replace ` and in particular the method used here is ` ffill ` which means forward fill : #CODE
A more " Panda-ic " way would be to use a groupby operation on the series and then aggregate the output by length .
Edit : another solution which is faster is to use ` value_counts ` ( and normalize ): #CODE
I had thought this was more concisely written as a ` resample ` , if you use a DatetimeIndex :
len ( Series.unique() ) might be even faster .
Interestingly , len ( Series.unique() ) is usually much faster than Series.nunique() .
And concat these Series together : #CODE
I've tried passing the different ` keys ` arguments to ` groupBy ` , but it always complains about being unable to reindex non-unique columns .
Pandas unstack doesn't work
So I grouped actions by date , using ` agg ` #CODE
But when tried ` unstack ` the series to get rid of the MultiIndex and plot my data , and got the error : #CODE
If you need to use ` .unstack() ` and it doesn't work with that multiindex , then starting from the non-indexed data #CODE
You don't need to if you simply pass a `` min_itemsize=40 `` ( or whatever number is ' big enough ') , this will apply to all object columns , alternatively , you can use : `` df.dtypes `` to see which are object ( the values are the dtype )
Can read_table do this or do I have to write a preprocessor to time align the data before giving it to pd.read_table ?
What exactly do you mean by time align the data ?
Why do pyplot methods apply instantly and subplot axes methods do not ?
Doing so , ` plt ` functions from ` matplotlib.pyplot ` apply instantly to my graphical output of pylab .
I have a weather data and I would need to apply a function to a specific number of rows .
Anyway I would like to apply any function independent from ` DateTime index ` .
I think slicing ` DF ` would be an option ` df [: 9 ]` but I don't know how to apply this to all rows ?
Also what is the function you want to apply ?
4 : I apply the transaction level criteria to the datafarme
I tried to join the final step of the two sequences to see which rows exist in Sequence 1 and not Sequence 2 , but a left merge between the two does not show any differences .
I'm pretty sure defining the criteria after adjusting the dataframe with the merge is the right way to go , but I want to understand why the results are different if nothing is changing in the underlying , transaction level dataframe other than the addition of a new column .
The docs mention that whenever joining columns on columns , the indexes will be ignored . after merging via a left join , the post-merge dataframe does not match the pandas-generated index of the left dataframe , despite being the same size .
since defining the boolean index before the join has the original index , applying that criteria to the post-merged dataframe results in mismatche errors .
My mistake was thinking the index of the left dataframe is maintained after a left join .
Calculating expanding mean on 2 columns simultaneously
I want to apply the expanding mean , such that ` ptsA ` and ` ptsB ` for each player get counted in ( and are not left ) to the net result .
and obviously , I need to do the same , and ` groupby ( ' plB ')` and then Im drawing a blank how to join these two results correctly .
Then create one new column with the expanding mean for each player : #CODE
For more than two names this is how you can build the formula for the expanding mean : #CODE
mask = st.scA * ( st.plA == name ) + st.scB * ( st.plB == name )
st [ ' mean ' + name ] = pd.expanding_mean ( mask [ mask > 0 ])` .
Merge pandas DataFrames based on irregular time intervals
I'm wondering how I can speed up a merge of two dataframes .
I'd like to merge these two dataframes more efficiently than the ` for ` loop below : #CODE
I am trying to loop through a pandas data frame and replace values in certain columns if they meet certain conditions .
When I try to replace text it does not replace it .
I was planning on approaching this problem by creating multiple df's like @USER said , and then just merge them on either name or unique ID , much like you would do with a db .
Calculate a new column in pandas with transform based on a subset of the group
` a.strip() ` results in ` AttributeError : ' Series ' object has no attribute ' strip '`
Just don't use ` join ` : #CODE
Then do a join on the original data : #CODE
Python pandas groupby object apply method duplicates first group
I am confused about this behavior of apply method of groupby in pandas ( 0.12.0-4 ) , it appears to apply the function TWICE to the first row of a data frame .
Then I try to do something similar using apply on the groupby object and I get the first row output twice : #CODE
I am dense and did not understand it immediately , so here is a simple example to show that despite the double printout of the first group in the example above , the apply method operates only once on the first group and does not mutate the original data frame : #CODE
This is checking whether you are mutating the data in the apply .
This might help if the function called by apply takes a long time ...
its actually tricky ; the fast-path is in cython ( usually ) , so right now it doesn't pass it back to python space ( it could I suppose ) . transform DOES do this however ( where it choses a path and then uses that result to move on ) .
The ` apply ` function needs to know the shape of the returned data to intelligently figure out how it will be combined .
Depending on your actual use case , you can replace the call to ` apply ` with ` aggregate ` , ` transform ` or ` filter ` , as described in detail here .
How to reindex a dataframe specifying center
How do I reindex the DataFrame by specifying the 0 index and let it auto fill the rest of the indices ?
Added another method , which shows that you're really just discarding the index and adding / subtracting as necessary to reindex at 0 at your desired row .
is there a reason you're making two axes when ` len ( area_tabs ) = 1 ` ?
Sometimes you want to get by label ( loc ) , sometimes by position ( iloc ) , sometimes both ( ix ) .
Sometimes ix is ambiguous . iloc is faster .
e.g. ` df = pd.DataFrame ([[ 1 , 2 ] , [ 3 , 4 ]] , [ 1 , 0 ] , [ 1 , 0 ])` try ` df.loc [ 0 , 0 ] , df.iloc [ 0 , 0 ] , df.ix [ 0 , 0 ]` , ` ix ` is ambiguous for this integer indexed DataFrame - which is why loc and iloc exist .
They also act differently when you have a non-unique index ( where loc / ix are type unstable ) .
This will return the cell located at the intersection of the third row with the fourth column .
However , if I replace the last line with #CODE
Alternatively you could create a map for the criteria to do boolean selection on month and year .
( you can ` reset_index() ` again here if you don't like the multiindex )
How to resample a time series Pandas dataframe ?
I am trying to resample 1 minute based data to day .
use `` data.set_index ([ ' DATE ' , ' TIME '] , inplace=True `` ; the returned copy now is just getting discarded so the resample will fail
Right now you're asking Python to pass the ` mean ` object to the ` resample ` method as the ` how ` argument , but you don't have one defined .
What can I do if I have several DataFrames and I want to apply the same set of operations to each with operations that do not support ` inplace=True ` ?
The gaps column holds the values I am trying to unstack .
I want to unstack the data so that with each neighborhood I can see the count for each gap .
Is it possible to unstack the gap values while preserving the groups of zone , city and date ?
When I try to use the function as unstack ( ' gaps ') , I get a key error that says ' Level gaps not found '
The zeroes for HH mm ss were added just to conform exactly to the examples .
dt = pd.datetime.strptime ( date_string , ' %d %m %Y ')
return dt
FYI I used ' and ' here but ' & ' is ok too ( query / eval have a somewhat relaxed parsing )
Second , let's try to replace with a scalar of a different data type : #CODE
Unless you transpose df.T [[ 4 , 5 , 6 ]] = df.T [[ 4 , 5 , 6 ]] , but this seems like cheating ...
But what you are actually saying is not replace this column with what is on the right hand side , but rather , overwrite using the row mask ( it happens to be null in this case ) .
no , you don't set new rows that way , use append .
Also , in case I wasn't clear , the intent of my df.loc [ 7:9 ] = val example was not to append rows , but to change the content of existing rows ( with index = 7 to 9 ) .
The goal is to take the 2x2 piece of df with index ( 4 , 5 ) and columns ( ' date ' , ' val ') and replace it with a same-shaped , same-typed 2x2 block .
I think a nicer way to do this , assuming you were planning on apply it to an entire column , is to use one of the vectorised string methods : ` str.split ` : #CODE
ewma or shift of MultiIndex
If I have a Pandas ` DataFrame ` with a ` MultiIndex ` where the first level is symbol , the second level is a date , what is the easiest way to perform an ` ewma ` or a ` shift ` operation on the data .
Python Pandas Not Getting Consistent Result using ix
Selecting with a scalar value ( like a 2 ) , will drop the dimension of the result ( e.g. you will get a Series ) .
I'd then drop the ` previous ` column as it's no longer needed .
Do you mind me asking why ix doesn't work - seems it did in smaller samples , but not in those with 5k rows
Am not sure why this join seems to be working , yet alas no rows can be seen - I cannot slice the DF .
Btw I do not join on the key , as I only want to really join on index .
It sounds like the join is working , but you are seeing a summarized view .
I want to merge them into one dataframe looking like #CODE
This looks like pivot table , which in Pandas is called ` unstack ` for some bizarre reason .
? The method to produce a pivot table isn't called ` unstack ` , it's called ` pivot_table ` .
I simply want to join such that the final DF will look like : #CODE
Also there is no join in a column .
You can use append for that #CODE
You can group and apply an user-defined function : #CODE
In other words , how to translate this pseudocode :
Are you trying to apply two different types of equations based on the value in serialNumber ?
After the merge between the object_list and percentages , you could " query " the dataframe based on the value in serialNumber and apply the correct formula ; #CODE
The apply function is similar to pythons builtin " map " .
You can ' apply ' the same function over the rows or columns ( where axis=1 is for row-wise [ top to bottom ] where the indexes will be the column names , and axis=0 is column-wise [ left to right ] where the row indexes are the indexes )
Create a summary Pandas DataFrame using concat / append via a for loop
If it has been created append the next DF ( last two rows ) for each additional tab .
and concat : #CODE
For each ID there is a date range with corresponding weight and roll_mean_weight ( which is a rolling 14 day average of weight ) .
I have tried a number of things but it seems to cut out rows .
I have a pandas Series and a function that I want to apply to each element of the Series .
python pandas : apply a function with arguments to a series .
I had to face this problem in my code and I have found a straightforward solution but it is quite specific and ( even worse ) do not use the apply method .
Here the function is quite simple and len ( t ) matches with len ( a [ ' x '] .index ) so I could just do : #CODE
Let's say len ( t )= 2 and len ( a [ ' x ']) =4 then t [ 0 ] would operate with a [ ' x '] [ 0 ] and a [ ' x '] [ 2 ] while t [ 1 ] on a [ ' x '] [ 1 ] and a [ ' x '] [ 3 ]
Your best bet would be to write a function that maps the ` t ` vector into a correctly-sized column in the data frame , using whatever mapping convention you'd like , and after that is created , * then * you can just use a simple ` apply ` or ` map ` or basic array function to operate on them .
What I would like to do is transform it into : #CODE
Sorry if I am doing something stupid , but I am very puzzled by this issue : I pass a DataFrame to a function , and inside that function I add a column and drop it .
I then intend to resample this to Daily and Monthly DataFrames .
I cant see how to post the entire stack trace here .
I would like to perform something similar to an SQL groupby operation or R's aggregate in Pandas .
I can't quite see how to use resample to do this
If you're indexing on timestamps , then you can use resample .
you should have `` float64 `` dtypes already . secondarily , you don't need the apply , you can do something like : `` data [ data [ ' currency '] ! = ' A ' , ' amount '] =d ata [ ' qty '] *data [ ' rate ']``
It may be cleaner to use and ` apply ` after grouping , instead of iterating over the groups .
Now I drop these rows from the initial DF for the training data : #CODE
Create a boolean mask of the values will index the rows where the 2 df's match , no need to iterate and much faster .
The default agg function is ` np.mean ` ( even though you didn't pass it explicitly this is what is being used ) , which doesn't make sense on strings , in fact it raises an AttributeError when passed an object array - so pandas complains when you try to do this .
Note : that this is the same as ` pivoted2 [ ' Value ']` , you can make this output the same as ` pivoted2 ` if you pass a list to values to aggregate : #CODE
@USER it's just that pivot_table doesn't label ( as the first row of the multiindex ) the values you are grabbing , * unless * you pass a list of value columns .
Calculations ( mainly ) , I'm now initialising like this : ( for example ) hourly_pred = pd.DataFrame ( { ' T ' : np.zeros ( len ( fpred )) , ' W ' : np.zeros ( len ( fpred )) } ,
Pandas - resample and standard deviation
I run resample ( " 10min " , how= " median ") .dropna() and I get : #CODE
Updated answer : Added how to resample by std .
The default as of v.0.13 is ' truncate ' .
To change the default value directly , line 229 in config_init.py should be changed from ` cf.register_option ( ' large_repr ' , ' truncate ' , pc_larg ...
Obviously you have to replace the strings ' xlabel ' and ' ylabel ' with what you want them to be .
And secondly , I do not now how to translate the dates into x position that are being used in ` cum_edits.plot() ` since the translation is invisible to me .
Convert every level of columns into a Series , then you can use boolean mask to select what you want .
I would strip them in python prior to using pandas
I strongly suspect that i need to use a lambda function , but i don't know how to apply it .
@USER : ` np.timedelta64 ( 1 , ' H ')` would work , but ` astype ( ' timedelta64 [ h ]')` -- I needed a lowercase ` h ` -- seems to truncate , so I wouldn't get 1.5 out .
I simply want to append values on axis=1 for the result of : #CODE
Yes , I tried to replace zeros with a list ( although I'd rather not do that because I need those zeros to stay zeros in my original df , and creating a new df may not be the best option ? ): #CODE
Why not just replace the zeros with [ 0 ] first ?
You would need to use ` applymap ` instead of ` apply ` to do it that way .
You can apply your values to your matrice by looping through the list with values that you
want to apply and append them to the matrice in the loop .
You need to do the other way around , you need to normalize the matrice and call the matrice again #CODE
It depends , you can convert your dataframe to an array , it might be easier to append values and normalize them that way , see thread #URL
No idea what you are trying to do nor how to translate that to valid code using Pandas .
is it possible to map to an item in a dictionary list , ie :
pandas column division ValueError ( putmask : mask and data must be the same size )
how to apply a function which takes the caller as its arugment
Following is the matrix which shows the intersection of pages accessed by common .
is it feasible to strip a redundant index returned by groupby in pandas ?
And note that I cannot strip it afterward , since the actual computation is much more convoluted such as : #CODE
When calling apply , add group keys to index to identify pieces ` .
What I'd like to strip away here is the redundant numerical index , not the grouping variable .
Trying to merge dataFrame
How can I insert a row into a dataframe , while preserving numerical order of row indexes ?
Now I want to insert an odd-indexed ttl , so I use ` DataFrame.loc ` : #CODE
How can I actually insert the " kowabunga " row such that it resides between index 2 and 4 ?
Reindexing error makes no sense does not seem to apply as my old index is unique .
Reindexing only valid with uniquely valued Index objects does not seem to apply
( I forgot that I need to drop a leading 1 from 11-digit numbers , too )
About your second question : You may want to look at the replace function .
Even via cut and paste ie output of a dataframe display ?
The dataframe data as below , was simply cut and pasted from the HTML rpr in IPython Notebook #CODE
It should return a ` bool ` instead of the error
The correct thing to return is a ` bool ` value
I want to unstack one column in my Pandas DataFrame .
The DataFrame is indexed by the ' Date ' and I want to unstack the ' Country ' column so each Country is its own column .
What I would like is to unstack the ' Country Column so only one Date would show for each month .
If you can drop ` Product ` , ` Unit ` , and ` Flow ` then it should be as easy as #CODE
Thanks Douglas , I've updated my answer assuming you can also drop ` Product ` .
Probably better to split your data using regexp and then apply some date parsing using strptime IMO , I can't think of an easier method
( you can omit the ` .reset_index() ` if you want , but in that case you'll need to transform ` mydf ` into a dataframe , like so : ` mydf = pd.DataFrame ( mydf )` , and only then rename the column )
Pandas align multiindex dataframe with other with regular index
In my case above I do not know where the value is in ` df.Matrix ` so ` str.contains ( ' Good ')` checks for a ` bool value ` before tagging it with whatever is in sch [ i ] and the part that check for the bool value causes the error .
Make a duplicate dataframe and modify that , modify an column of the same length and append it at the end , or just go with the suppressing of the error ?
Just create the Series as you need it ; pandas will align it ( filling the remaining values with nan )
Followup question : How can I implement the pandas shift with dataframes that are different lengths ?
The pandas answer is to use ` shift ` : ` first.shift() > second ` .
` shift ` shifts the labels .
In other words , I would like to replace the id's of df2 with the corresponding values in df1 .
then join the two columns #CODE
I am trying to merge some dataframes based on specific column .
I can do the merge for two of them .
But When i get the result and try to do the next merge I have problem with the suffixes :
The names of the columns of the dataframe were the same , so it was not possible to merge them .
I read these both is seperatly using pandas , and then append the 2010 file to the end of the 2009 dataframe .
I'm sure it is just a copy / paste error , but you have an erroneous bracket in your append statement
Use ` apply ` for row-wise methods : #CODE
concurrent reading works just fine in multi-threading / processes . the change in 3.1 was more to align PyTables with how the underlying HDF5 protocol works .
pandas concat is not possible got error why
I like to add a total row on the top of my pivot , but when I trie to concat I got an error .
Thats the pivot Table #CODE
The total of it I like to join #CODE
Try ` table = table [ ' Weight ']` and then do the concat .
I've included the full stack trace at the end of this question since it's so damn long .
Or was it just tough to translate what the docs were saying to your particular problem ?
Well , it does select the number of rows I want but I can't apply ` size() ` method on this new object .
@USER I'm not sure that is possible , once you assign ` NaN ` you get a dtype conversion to ` float ` s as ` bool ` s cannot represent ` NaN ` in any meaningful way .
@USER no worries , it seems there is an implicit dtype conversion after assigning ` NaN ` , it converts to float64 probably because ` NaN ` has no meanings for ` bool ` s .
If you don't care about the bool / float issue , I propose : #CODE
How can I merge one dataframe into another , inserting rows that do not exist ?
My goal is to merge the second dataframe into the first , such that each item in the ` msg ` column of dataframe2 gets placed into the ` event ` column of dataframe1 at the index indicated by column ` t ` .
Did you look at the ` join ` method ?
There is no ` t ` column in the first dataframe , so I would need to merge onto the index , it seems .
@USER your first df seems to have year as it's index it isn't clear how t relates to the first df's index , you need either indexes that mean the same thing or a common column in both df's in order to merge them in any meaningful way
The only problem is that it seems unwilling to insert rows that do not already exist .
@USER in which case what you do is ` df1.set_index ([ ' t '] , inplace=True )` and then merge : ` df =d f.merge ( df1 , left_index=True , right_index=True , how= ' left ')`
@USER just rename the column before merging ` df1.rename ( columns={ ' msg ' : ' event ' } )` , or you can just do it after merge by dropping the column from df : ` df.drop ( ' event ' , axis=1 )` then rename msg to event : ` df.rename ( { ' msg ' : ' event ' } )`
What you want to do is merge the other df to your first one on either an index ( which should represent the same thing ) or columns which should represent the same thing .
Now merge : #CODE
pandas join DataFrame force suffix ?
How can I force a suffix on a merge or join .
I understand it's possible to provide one if there is a collision but in my case I'm merging df1 with df2 which doesn't cause any collision but then merging again on df2 which uses the suffixes but I would prefer for each merge to have a suffix because it gets confusing if I do different combinations as you could imagine .
Why not just concat or append df1 and df2 together but rename the clashing columns in df2 so you can id where the original data came from ?
I guess in the end , it would be nice to save the step and provide a suffix to the join / merge functions .
this would be a nice addition to merge in pandas .
You can force the dtype to be int rather than bool : #CODE
I have a pandas dataframe that looks like the one below , and I want to drop several labels .
However , I have many labels that I want to drop , so I am looking for a command that works like : #CODE
How can I drop , or the reverse select , several rows with different labels in one command ?
You only need to use `` loc `` on the lhs if you are overwriting part of an existing column .
How can I strip the whitespace from Pandas DataFrame headers ?
My question , then , is how can I strip out the unwanted white space from the column headings ?
I have a dataframe and I want to replace the values in a particular column that exceed a value with zero .
I'd like to apply this elsewhere .
` TypeError : cannot properly create the storer for : [ _TABLE_MAP ] [ group -> / test_sparse ( Group ) '' , value -> , table -> True , append -> True , kwargs -> { ' encoding ' : None} ]`
unstack is not working .
Is is possible to write my own column names when writing a pivot table to a csv file ?
For each dataframe column you wish to convert to a list , you can transpose the values , and then convert it to a list as follows .
Then obtain the values using ` .values ` and transpose using ` .T ` , and convert to a list using ` .tolist() ` #CODE
There is a property ` dt ` specifically introduced to handle this problem .
I just noticed that the only apparent differences between my `` index `` and ones in `` working `` examples is the `` freq `` and `` Timezone `` .
`` df == DataFrame ( np.tile ( rowmax , len ( df )) .reshape ( df.shape ) .T , index =d f.index , columns =d f.columns )`` will get your boolean frame ( kind of like a broadcasted comparison operator ); faster , but prob not more clear than the `` apply ``
It looks like ` stack ` returns a copy , not a view , which is memory prohibitive in this case .
If we ignore the memory issues with ` stack ` and use it for now , how does one do the grouping correctly ?
The stacked DataFrame has a MultiIndex , with a length of some number less than ` n_rows*n_columns ` , because the ` nan ` s are removed .
It would be much better to just operate on the first level , but then I'm stuck on how to then apply the grouping I actually want .
I think you are doing a row / column-wise operation so can use ` apply ` : #CODE
Also , you were pretty close to getting this correct , but you'd need to stack and unstack : #CODE
Also , does stack return a copy instead of a view ?
It would be better to just operate without the ` stack ` , as you've done ; I just don't see a way to get the appropriate grouping in there .
@USER you groupby the level , stack is a copy .
Is there any disadvantage to this approach , or any particular reason you want to use stack and groupby ?
How far does the following statement apply to one but not the other select ?
The exporter insists that all data used respond to ` type ( x )` with either ` int ` , ` float ` , ` bool ` or a few others .
You can achieve an optimal chunksize in a ` Table ` format by passing ` expectedrows= ` ( and only doing a single append ) .
Why does pandas apply calculate twice
I'm using the apply method on a panda's DataFrame object .
Also , calling it four times when you apply on the column is normal .
` apply ` on a Series applies the function to each element .
I've been playing around with ` set_index ` and ` stack ` / ` unstack ` methods but with no success ...
stack and unstack to exactly reproduce result you posted : #CODE
( An alternative solution could probably be based on using ` unstack ` -- though , on second thought , it may not be very different since under the hood ` pivot ` uses ` unstack ` ... ) .
By looking at other examples of ` pivot ` in action , it became clear than ` something ` had to equal #CODE
You can use a combination of ` DataFrame.groupby ` , ` DataFrame.reset_index ` and ` DataFrame.T ` ( transpose ) #CODE
Or you can use ` concat ` : #CODE
doing a nested a nested apply / grouping like this is not the answer .
You prob want to groupby at the top level ( or construct a multi-index ) , select the values that you want to include , then use a cythonized function to apply it .
You only want to do 1 level of groupby and apply ( except in some very very rare cases ) .
You can do the apply and groupby by one multilevel groupby , here is the code : #CODE
FYI this is a completely parallelizable problem once u split the groups and you could say do them in separate processes , eg something like concat ( dict ([ ( g , group_process2 ( grp )) for g , grp in d.groupby ( grps ) ]))
` freq ` is a string .
I would like to define the same kind of DatetimeIndex using a timedelta as freq #CODE
A more robust way to get this is to use a DateOffset ( which you can pass as freq to ` date_range ` ) .
Pandas : how to apply function to only part of a dataframe and append result back to dataframe ?
I could then figure out how to append these lists back to the original list ( though I have no idea how to go from a dataframe to a list again to do this ) .
Is there more of a direct way to only apply the function to the numeric variables ?
To ' append ' the new one to the original dataframe , if needed , can be simply : ` df [[ ' c3 ' , ' c4 ']] =d f [[ ' week1 ' , ' week2 ']] .apply ( lambda x : ( x - x.mean() ) / x.std() )` .
pandas pivot table using index data of dataframe
I want to create a pivot table from a pandas dataframe
So pivot uses the index as values : #CODE
You can use ` apply ` and use a lambda to subtract the list values column-wise : #CODE
I was trying to remember what the correct / better method was and couldn't remember this one so i posted ` apply ` as an answer .
Ed , in this case , ' sub ' was what I was looking for , but I'm certainly keeping the ' apply lambda ' method in my back pocket -- the next problem in queue isn't a straight subtraction .
You want to combine the level names of the column multiindex , in reverse order , and join them with a ` _ ` , so it feels like you're going to have to specify at least three things .
How can I avoid repeated indices in pandas DataFrame after concat ?
Trying to improve performance , I realised that dropping the ` count ` method from the list passed to ` agg ` improves significantly .
Still - one cannot pass it , as with ` count ` ( and ` mean ` , ` sum ` ,... ) , in a list of functions to ` agg ` .
It looks to me you just want to join files of the same format , if it so there is no need to use pandas for this .
I am guessing this would work with a pivot table but since I was using a dataframe everything got mixed up .
For the fifth column I want the consolidate the number of unique values for each Address .
You have a pivot operation and a bunch of groupby operations .
Then we can pivot : #CODE
You can resample by month using sum as the aggregation function : #CODE
When you set this as the column , it'll align the index for you : #CODE
Given your desired solution it looks like you want to shift the results up by one too ...
I couldn't find a way to get ` .resample ` to work properly with this index , it always started near ` vaca_days [ 0 ]` even when this didn't align with my index .
is all the data text or are there any numbers ? for example , if someone types 3.14 as a string , do you really want to strip the period ?
It is returning me with an error that DataFrame does not have a ' translate ; attribute .
Use ` replace ` with correct regex would be easier : #CODE
@USER just read the docs , ` strip ` removes characters at beginning and end , hence the error , you should use ` replace `
But I am looking forward to store values that I continually need to replace in a list and run that command every time .
In any case concat produces a different output than subsequent append calls .
That said , you'll probably get better performance out of ` concat ` anyway .
my pd.version.version is ' 0.13.0-408-g464c1f9 ' , did you check also if it still keeps this the second time , when you append res2 ?
To drop the MultiIndex , you could use #CODE
Pandas - reindex so I can keep values
Eventually I want to melt down this data frame to look like the following .
I want to apply filters based on the following pattern from ' CAT1 ' ' CAT2 ' ;
You could just define a function and pass this to ` apply ` and set ` axis=1 ` would work , not sure I can think of an operation that would give you what you want
Then apply it to your dataframe passing in the ` axis=1 ` option : #CODE
pandas groupby agg function column / dtype error
In all these cases , it's pretty clearly trying to apply peak_to_peak() or np.mean() ( or whatever ) to the ( subsets of the ) ' key2 ' column from df , whereas for the built-in methods and predefined functions , it ( correctly ) ignores the ' key2 ' column subsets .
The documentation on ` agg ` doesn't mention this at all , and it should .
umm , wouldn't len ( yourFrame ) give you the number of rows ?
Is there any way to separate the dataframe into two dataframes and not actually " drop " rows ?
I want to set the entries of the column ' actual_12b1 ' that are 0.0 to NaN , but only if ' begdt ' ( the second level of the MultiIndex ) is in 1998 or before .
You have the right idea with the indexing but swapping values will be easier with the ` replace ` method of the dataframe .
I'm trying to merge some CSV files that I got as a result of some huge text files . the format is like this : #CODE
replace rows in a pandas data frame
I can even start with a 0 data frame ` data =p d.DataFrame ( np.zeros ( shape =( 10 , 2 )) , column =[ " a " , " b "])` and then replace one line each time .
I would recommend building lists with ` append ` and then converting to a dataframe when you've generated all the data , if possible .
Use ` concat ` to add a row #URL
I would probably ` append ` in your situation but that's just habit .
see here : #URL This is bascially an unsafe operation , modifying using chained indexing ( e.g. the `` df [ col ] .loc [ row ] = value `` ; you * can * instead do : `` df.loc [ row , col ] = value `` to avoid this issue ( even better is to use `` update / replace `` in any event .
Thanks , I changed to using replace and it works fine .
Is the best way to do a pivot table then implement the equation ?
Is this the best way to calculate percentage or is there a better way than pivot tables ?
Do you want the * first * index of each distinct entry in ` CAT1 ` or do you want to assign each distinct entry a number and replace the text with this number ?
Also check out #URL which covers ` apply ` , ` map ` as well as ` applymap ` .
In the df1 [ ' Flag '] = df1.applymap ( f ) .sum ( axis=1 ) .astype ( bool ) statement , when we sum it up by columns , then shouldnt first row would have value of 2 ( 1+1+0 ) ?.
Then bool of that should be True rite ?.
yes , sorry - I copied the result from the ` all ` function but wrote in the ` sum ` function ( bool ( sum ) gives True for all of the rows which isn't a good example ) .
The output of the above examples would be sent to a " map " or " reduce " type function .
If you roll back to version 0.12 it handles the Series replace correctly .
Python plotting points on a city map
And I want to plot them on a map of San Francisco with the ' Type ' column being in red for the data in ' G ' and blue for ' C ' .
How would I also include streets on the map ?
There are tons of ways to plot data in a map .
An easy way to obtain a map background is using a free tile service here are some useful urls :
Try using isin ( thanks to DSM for suggesting ` loc ` over ` ix ` here ): #CODE
`` loc `` will be faster than `` ix `` as it doesn't have to check whether things ' look ' like integers but are actually locations ; `` in1d `` is not well suited to `` datetime64 [ ns ]`` or `` timedelta64 [ ns ]`` types FYI
I've replaced ` ix ` with ` loc ` , above .
The initial error , it appears is that I included NaN values , I cut the frame to exclude NaN in both columns . and It ran for a bit but got some sort of Theano error , Which I am now admiring ....
A pivot table should do the trick .
i have a list of sentences and i need to replace each word by a word from a data frame column .
so now i need to replace words in z with these words ( correct ones ) in image i.e dataframe .. how do i do that ?
yes i did that .. now the problem is with this error - " replace takes atleast 3 arguments ( 2 given )" . check it out , i've edited my code ..
You should be invoking it on a string , and specifying a substring to replace and a new string to replace it with .
and converted it to a pivot table like this : #CODE
Conceptually , the dicts you say you want don't distinguish between what's in the index of your pivot table and what's in the columns ( you just want all index and column labels as keys in the dict ) , so you need to ` reset_index ` to remove the index / column distinction .
I don't think there's any need to create the pivot table differently ; just reset the index .
You just happen to want information in columns when the pivot table is giving it to you in the index .
processLine just creates an object and append this object to list .
I'd like to add distribution to boxplot when using it with pandas dataframe like this : #CODE
and now I would like to insert " kde " .
I would like to maintain the data types from the original data frame as I need to apply other operations to the total row , something like : #CODE
i need to iterate over each word of each address and replace it with the right one ..
The method ` replace ` is good for this .
so i will need to iterate over each word of sentence to replace it .. your approach is awesome .
You'll have to split your sentences into columns of words and then apply .
The ` mask ` is ` True ` on these rows .
` df [ ' date '] .where ( mask , np.nan )` returns a Series , equal to
` df [ ' date ']` where the mask is ` True ` , and ` np.nan ` otherwise .
Select only those rows where the mask is ` False ` , thus removing the header rows .
mask = data [ ' date '] .str .match ( ' ^\s*\D ')
i only get ( ) and [ ] as output for ' mask ' ...
Solved it by using mask = data [ ' date '] .str .CONTAINS ( ' ^\s*\D ') instead of MATCH .
The mask is True on the rows that have a " type " in the `' x '` column .
` df [ ' x '] .where ( mask , np.nan )` returns a Series , equal to ` df [ ' x ']` where
the mask is True , and np.nan otherwise .
Select only those rows where the mask is False
I have the following example data for performing a rolling OLS calculation ( here I am doing it from the debugger ): #CODE
When I attempt a rolling OLS : #CODE
Is this a known bug with ` pandas.ols ` in the case of trying a rolling regression ?
The data is small and obviously has no defects that should prevent a rolling 12-to-60 observation regression from working in this case .
That article used ` id ` s to achieve different formatting for each table but you could just apply the desired CSS styles directly to the appropriate HTML tags .
However , it looks like many of the missing ones are fairly clear because they just apply a mathematical function of the same name to each group ( e.g. , ` cummin `) .
these ultimately just call the same named DataFrame method ( or an optimized for groupby version ) ( asside from the specific methods `` transform / apply / agg / groups ``)
I'd say multiply by -1 , apply the sub and div , then multiply by -1 again .
If I find a more elegant way ( for example , using the column index : ( 0 or 1 ) mod 2 - 1 to select the sign in the apply operation so it can be done with just one apply command , I'll let you know .
I want to replace all the values whose magnitude is above a certain threshold with np.nan .
You can replace with nan + jnan explicitly with : ` df3.where ( df3 <= 5000 , nan+jnan )` , the default is nan .
Pandas uses ` datetime64 ` , ` timedelta64 ` , ` int64 ` , ` float64 ` , ` bool ` and ` object ` ( this includes strings and any other custom objects ) .
pandas : unexpected result when reindex ( ing ) a Panel
This is a follow-up on my reply to pandas pivot dataframe to 3d data , where later I needed to re-index ( get a different sorting of the Panel labels and of the elements accordingly , as well as filling in with NaNs of the missing labels ) .
reindex ( ing ) results in a " wrong " result when done on all axes of the Panel in one step .
Now let's examine the result of reindex ( including the filling of missing entries ) on the 4th item , which originally did not have values at all : #CODE
When reindex is split in two steps , the expected result is retrieved : #CODE
their was a bug that is fixed in 0.13 that doesn't reindex properly when specifying all 3 axes , see : #URL
What I would like to do is append either a series or a single column dataframe to a given column within the existing dataframe on the index based on that DataFrame column index location ( as opposed to the column name , which would overwrite values in columns with the same name ) .
So , why can't you append a row , as above and then fill it in with data as it comes ?
I aggregate the rows with duplicate token and years like so : #CODE
Alternative , but slower ways are to use the normalize method ( to bring it to the start of the day ) , or to use the date attribute : #CODE
* In 0.15 there will be a dt attribute so that you can access these as : #CODE
Running a groupby on a pivot table with Pandas
I have a pivot table that looks like this : #CODE
This is the pivot tables index : #CODE
( Using dataframes because it's critical the indexes align . )
In this case it looks like you can do a concat : #CODE
If you need to include source , the index information , within the DataFrames in SourceData , then I would do this before doing the concat .
You could also find the location of the first offender you could use argmin : #CODE
Already some great answers to this question , however here is a nice snippet that I use regularly to drop rows based on non-numeric values on some columns : #CODE
The way this works is we first ` drop ` the ` data_columns ` from the ` df ` altogether , and using a ` join ` we reimplant the ` data_columns ` as parsed by ` pd.to_numeric ` ( with option `' coerce '` , such that all non-numeric entries are converted to ` NaN `) .
Using the apply function , we can compare entire columns to the empty string and then aggregate down with the ` .all() ` method .
Now we can drop these columns #CODE
Pandas left outer join multiple dataframes on multiple columns
I am new to using DataFrame and I would like to know how to perform a SQL equivalent of left outer join on multiple columns on a series of tables
I have tried using merge and join but can't figure out how to do it on multiple tables and when there are multiple joints involved .
Merge them in two steps , ` df1 ` and ` df2 ` first , and then the result of that to ` df3 ` .
I dropped year from df3 since you don't need it for the last join .
As I have quite a few dataframes , I was looking for a quick way of joining the tables all together in one step - similar to using ' join ' as you can join more than two tables in one step .
However , I can't figure out how to do it on multiple join conditions .
dynamic function to insert or create table using pydobc to sql server
Ideally i want to send all variables without specifying anything except whether i want to replace or append a table .
Is it as simple as using ' create table ' instead of ' insert into ' , and somehow dynamically deriving the data type info e.g. ( varchar , int ( 20 )) .
Or you can use ` create or replace ` to always re-create it .
Can't figure out how to apply the solution to my code ,, when I try to just run the code as provided I get module not callable and when I try to place the code in the " with " clause it blows up ..
Generally speaking , ` for i in xrange ( len ( thing )): ... thing [ i ]` should be replaced with ` for item in thing : ... item ` .
Now change the index on ` vals ` to a ` MultiIndex ` #CODE
Then unstack and plot : #CODE
I would like to drop the dead space between 0-1 .
then you can select the rows using ` loc ` instead of ` iloc ` : #CODE
Note that ` loc ` can also accept boolean arrays : #CODE
Now if I try to replace those nan's with zerosby using #CODE
for icat in range ( len ( label )):
Conditional merge in pandas
I'm trying to merge each entry in the first dataset with the returns from the previous 12 months .
Is there an efficient way to do a conditional merge like this in Pandas ?
I don't think you want a conditional merge ( at least if I understand the situation correctly ) .
I think you can get your desired result by just merging header_mf and ret_mf on ` [ ' fundno ' , ' caldt ']` and then creating the columns of past returns using the ` shift ` operator in pandas .
But , if this basically captures the nature of your data then I think you can just merge on ` [ ' fundno ' , ' caldt ']` and then use ` shift ` : #CODE
From what I've seen , ` pandas ` index objects support the same membership methods as python ` set() ` objects , such as intersection , union , difference , and symmetric difference .
Try using the intersection operator ( ` `) on the two DatetimeIndex objects .
It seems that the histogram bars don't correctly align with the grids ( see first subplot ) .
Look for the ` align ` option in matplotlib hist .
You can align left , right , or center .
This is spelled out in the matplotlib hist docs : #URL
The ` converters ` parameter tells ` read_csv ` to apply the given
How to do a left join on a non unique column / index in Deedle
I am trying to do a left join between two data frames in Deedle .
I am thinking that I might have to flatten the second data frame to remove the duplicates then join and then unpivot / unstack it ?
I think using pivoting would be another option ( there is a nice pivot table function in the latest source code - not yet on NuGet ) .
Now we can join the two series ( because their work order codes are the keys ) .
However , then you get one or two data frames for each joined order code and there is quite a lot of work needed to outer join the rows of the two frames : #CODE
However , I seem to get conversion error when trying to apply the .loc .
Moreover , as this is just an example , do you think there is a way to apply the " *= -1 " to all columns , as ' 1Y ' : ' 1Y4M ' ?
File " C :\ anaconda\lib\ site-packages \pandas\core\ frame.py " , line 4416 , in apply
What is the difference between pandas agg and apply function ?
Using ` apply ` : #CODE
Using ` agg ` #CODE
` apply ` applies the function to each group ( your ` Species `) .
` agg ` aggregates each column ( feature ) for each group down to a single value .
So for all 4.000 locations you apply the distance function to all 11.000 towers ?
This whole function can be made into a ` map ` operation .
You're taking one structure and mapping it to another structure with a 1-1 correspondence between elements , which makes this a ` map ` .
I have a DataFrame from which I want to normalize some arbitrary columns using another arbitrary column : #CODE
I want to normalize all the columns ( attrib ) belonging to an arbitrary joint ( eg . ' head ') using another arbitrary joint ( eg . ' torso ') .
Perhaps using groupby and then and applying the normalize function to the selected DataFrame ?
In your first code block , you need to replace ` multiind_first ` with ` mi_level_one ` and ` multiind_second ` with ` mi_level_two ` .
Since indexes won't match it must join on ` First ` and ` Last ` columns .
I have an extra issue though , can you think of a way to merge only specific columns ?
I'd like to pivot the data so that I end up with : #CODE
I was trying to use multiindex but this is far simpler .
I'd like to calculate the median of all identically named columns across both index and columns , ideally getting a series : #CODE
Getting the median across one of these dimensions is trivial , but I can't seem to get it across both ( and doing one and then the other is not the same thing ) .
I've tried melt and index shenanigans , but I always seem to end up with the median long just one axis .
I thought melt was the key , but I neglected the [ " value "] .
I didn't like having to strip the index of its name , and transposing the values etc . seemed cumbersome .
What I am trying to do , and I might just be unclear on the shifting documents , is to shift all of the data down or up by the first level index .
So shift all of the data from the next date index up to the previous date index .
The idea is to shift all of the data up a period to compute change in prices .
Try using the [ ` shift `] ( #URL ) method as Wes suggests [ here ] ( #URL )
pandas.core.reshape.ReshapeError : Index contains duplicate entries , cannot reshape . this is raised on the unstack part of the function
Why don't you just shift the date and merge it ?
I needed to shift all of the information .
Maybe however , I can have a shifted index as you show , copy the dataframe , reindex by the shifted one and then join it and drop the new index ?
I did make an edit to the " unique value " multiplier as 0.1 was rolling over which caused problems so I used 0.00000001 instead .
so when I apply the filter it will filter against each of the words in my List2 ?
But thanks to your code , I now know that transform is the method I need to use .
I can unstack it now and get the years at the column level .
Pandas drops timestamp columns in resample
If I resample the DataFrame , the one remaining as a column gets dropped .
Now resample to daily : #CODE
Then resample them ; you need to view them as integers to compute on them ; Timestamp will handle this float input ( and essentially round to the nearest nanosecond ) #CODE
I suppose you could have a signature like : `` df.resamplee ( freq .... how={ columns : func ..... } , how_default = default_function )`` to have per-column overrides and a default .
Pandas , large file with varying number columns , in memory append
Normally as new data comes I would append to the existing table : #CODE
In these cases , I would like to get a behavior similar to the normal DataFrame append function
You could need to read it , append , and write it out .
You could need to read it in , append , and write it out .
` map_dtype() ` function , as you can see I have to manually map data types with there string representation in BigQuery
If I understand correctly , you have a CSV file and you are attempting to map the dtypes to the BigQuery types .
In this case , I feel like a dictionary is your best bet for the lookups , and you can replace the ` for ` loop with list comprehension : #CODE
/ len ( sq )` .
That should be ` len ( ser )` -- the length of the original list .
I have been trying to get a proper documentation for the freq arguments associated with pandas .
For example to resample a dataframe we can do something like #CODE
which will resample this weekly .
btw yup i do it by groupby for now , but am trying to understand what I can do by resample only
The types of the columns on that dataframe come out as object , object , int , float , bool , object .
( I don't want to have to tell pandas which columns are datetimes and timedeltas or tell it the formats , I want it to try and detect them automatically like it does for into , float and bool columns . )
R parameter DROP equivalent in Pandas
I would like to end up having the stacked bars clustered next to each other , so I thought I could just make them a little narrower and shift them right and left .
How can I sort a boxplot in pandas by the median values ?
I want to draw a boxplot of column ` Z ` in dataframe ` df ` by the categories ` X ` and ` Y ` .
How can I sort the boxplot by the median , in descending order ?
You can use the answer in How to sort a boxplot by the median values in pandas but first you need to group your data and create a new data frame : #CODE
You probably need to transpose ` data ` ( ` data.T `) so that the dot product makes sense .
How to replace all words in a series with a few specified words in Pandas , Python ?
I want to essentially find and replace using python .
However , I want to say if a cell contains something , then replace with what I want .
that can contain `' optiplex 9010 for classes and research '` which I just want to replace with `' optiplex 9010 '` .
I can either manually go into excel and filter by what contains ' optiplex 9010 ' and then replace everything with a simple description , doing the same for macbooks , etc .
You would probably be better off building a dict to hold your reduced / normalised key values with your desired replacement strings , then do a parse of your current string values by reducing / normalising them and then perform a lookup on the dict and replace the current value with your dict value .
However , you might be better off , especially if you have already have a complete list of topics , to normalize the names ( e.g. using fuzzywuzzy like in this question / answer ): #CODE
How would I represent everything before the ' macbook air 11 ' for example if the description was ' one computer-macbook air 11 ' and I wanted to replace this with ' macbook air 11 '
Then use a regex sub to replace the string value of all digits with ` 5 ` and lower case the string : #CODE
Of course if you want to just replace all , you can use a regex or string replace : #CODE
Interesting , How would I get it to replace everything in the cell ... essentially deleting the content and then putting back what I want .
I see that all of the 1's are replaced by 5 but what I would want would be to have everything that has a 1 replace it with only a five so 10 would become 5 and 11 would be come 5 only not 55 , etc
I want to bin that array into equal partitions of a given length ( it is fine to drop the last partition if it is not the same size ) and then calculate the mean of each of those bins .
More generally , we'd need to do something like this to drop the last bin when it's not an even multiple : #CODE
I usually read everything as string , then apply a helper function that includes try / except to convert each individual string .
My preferred way is Jeff's using loc ( it's generally good practice to avoid working on copies , especially if you might later do assignment ) .
You can eek some more performance by not creating a Series for the boolean mask , just a numpy array : #CODE
Thank you so much for the detailed analysis , In your last example , df.B.values [ df.A.values == 23 ] , is there any way I can replace ' B ' by a variable instead ?
Merge dataframes on index
I'd like to ' pivot ' the dataframe to end up with a single date index .
I think pandas ` melt ` provides the functionality you are looking for
Can Pandas find all the lines that join any pair of ** dots ** and don't intersect any of the given ** lines ** without iteration ?
Can Pandas find all the lines that join any pair of dots and don't intersect any of the given lines without iteration ?
As we pass by lines , there will be an expanding region of shadow ; but at any x value , we only need to know the 2 gradients of the shadow's edge to know if a y value is in shadow .
I have the following problem , I have a Panda data frame and I want to process each row ny using the apply method .
You can group by `' Currency '` and apply ` diff ` but first you need to convert the data to ` float ` , try this : #CODE
However , when applying " diff " to my groupby-function , it seems that Python mess up the original order of the currencys .
What's probably happening is that when I apply groupby currencies , Python also SORTS the data accordingly -> so e.g. if my original data is stored in order " EUR , CHF , DKK " the diff-command makes the data " CHF , DKK , EUR " , i.e. when putting back the currency-labels , they obviously will be mis-labeled .
If the value of column B is 1 , insert ' Y ' else insert ' N ' .
Avoid chained indexing by using ` loc ` .
So just enter % ( name ) s in the query , replace ' name ' with whatever name you want .
Simply , where column B = ' t3 ' , I want to replace the NaN value in column A with a new string .
Use ` loc ` see #URL #CODE
It could be an groupby's apply will do it , like I say in other question , depends what you're doing !
I'm trying to merge / join two dataframes , each with three keys ( Age , Gender and Signed_In ) .
It seems like the merge / join should be painless given the unique combined keys are shared across both dataframes .
Thinking there must be some simple error with my attempt at ' merge ' and ' join ' but can't for the life of me resolve it .
my guess is that since `' Age ' , ' Gender ' , ' Signed_In '` are indices , you don't need the ` on ` kwarg in the call to ` join `
You don't need to the ` on ` kwarg when joining on equivalently structured ` MultiIndex `
Note : most of the time you don't need to do this , apply , aggregate and transform are your friends !
i want to write something funny to let you know that i'm extremely grateful but , alas , my sense of humor is a little dark and i don't want to offend the people of stack overflow .
thanx .. you made my day . you could surely join in on the Python chat group some day and help
Have you thought about wrapping a csv reader so that it splits the date column and append day , month and year and create a pandas frame with that iterator .
good post to include new dt example !
Here is the basic , centered rolling sum ...
and then ` shift ` s the result by ` -offset ` , where #CODE
You need to use the merge operation in Pandas to get a better performance .
sorry I just fixed that It's the result of the first merge
Expanding on Bonlenfum's answer , here's a way to do it with a groupby clause : #CODE
I think it may be simpler to use an apply here : #CODE
@USER this seems to work with apply : s ( Thanks for editing , makes it much easier , if I hadn't already +1d , I would again ! ) :)
When you do an apply each column is realigned with the other results , since every value between 1 and 5 is seen it's aligned with ` range ( 1 , 6 )` : #CODE
When you do the apply , it concats the result of doing this for each column : #CODE
and a mapping that map pokemon name to the series consisting of its color and weight .
I would then like to apply some vba formatting to the results - but i'm not sure which dll or addon or something I would need to call excel vba using python to format headings as bold and add color etc .
The following example shows how to use the xlsxwriter python library to create a workbook , and insert worksheets , then insert data from pandas dataframes , ( and even charts based on the dataframes into excel ) .
I need to check if the ` dtype ` of this dataframe is ` bool ` .
The other thing to note that ` isinstance ( df , bool )` will not work as it is a pandas dataframe or more accurately : #CODE
I need to check if ` df.a.dtypes == bool ` .
I had to do this : ` if df1.v.dtype == ' bool ' : ` to check if it is a boolean DataFrame .
@USER ` dytpes ` is in fact a ` numpy.dtype ` see : #URL you could also do ` df.a.dtypes.name == ' bool '` would yield ` True `
I think I'd rather not use join or merge , as these return copies .
Matplotlib's ` .get_lines() ` return a list so you can merge them by simple addition .
Generate a mask of the types that are strings : #CODE
Doesn't ` apply ` call my lambda function , once for each column ?
Thus ` x.unique() / len ( x )` will be multiplying all of those elements , element-wise , by ` 1.0 / len ( x )` , and giving you back variably-lenghted arrays depending on the amount of unique entries per group .
Note that ` len ( x )` produces an integer , whereas ` x.unique() ` produces an array and the elements of that array will have whatever types they happened to have as entries in ` x ` .
So it won't necessarily be the case that ` x.unique() / len ( x )` is well-defined at all .
For example , if one of the entries of ` x ` is the integer ` 4 ` and if ` len ( x )` is the integer ` 5 ` , then the entry in the output of the expression ` x.unique() / len ( x )` corresponding to the unique value ` 4 ` will actually be ` 0 ` ( ! ) due to the specifics of integer division in Python .
Thus , even after correcting the mistake of computing the unique array instead of the length of the unique array , you must still be careful : ` len ( x.unique() ) / len ( x )` will also result in an integer being divided by another integer , and most of the time the numerator will be less than the denominator , yielding ` 0 ` .
Minor : you can use ` x.nunique() ` instead of ` len ( x.unique() )` .
how to append / insert an item at the beginning of a series ?
how can i append an item at the very beginning of this series ?
to do that kind of breaks the intent of pandas.Series ... you will likely see that it takes much much longer to insert at the begining of a series than at the end . are you sure that ` Pandas.Series ` is the right data structure for your problem ?
See the Merge , Join , and Concatenate documentation .
Similarly , you can use append with a list or tuple of series ( so long as you're using pandas version .13 or greater ) #CODE
How about reading the values in as ` NaN ` like ` df = pandas.read_csv ( " file.csv " , names =[ ' A ' , ' B ' , ' C ' , ' D ' , ' E ' , ' F ' , ' G ' , ' H ' , ' I ' , ' J ' , ' K '] , header=None , na_values =[ ' SUPP '])` this will replace ' SUPP ' with ` NaN ` which you should be able to replace
But you can replace ` NaN ` with 3.0 so it should achieve what you want no ?
Originally I thought using ` nonzero ` would make things simpler , but the best I could come up with was #CODE
The problem I'm having is that when I load this into a DataFrame and run resample with a 15min frequency , summing the values in-between , the DateTimeIndex labels are coerced to intra-hour 15 minute intervals ( i.e. 0 , 15 , 30 , 45 ) but what I want is to retain the original time series DateTimeIndex ( i.e. starting from ` datetime.datetime ( 2014 , 2 , 24 , 1 , 7 , tzinfo= UTC )`) .
I've tried using the resample ` loffset ` config argument which does affect the preferred behavior on the DateTimeIndex but the summed values aren't changed accordingly .
Use ` where ` instead of ` apply ` and add days with ` np.timedelta64 ` #CODE
To find the median values , you could use #CODE
but it is less clear which row should be associated with the median , since if there is an even number of rows in a group , the average of the middle rows is used to compute the median .
The median is thus not associated with one row , but rather two .
to find both the median value and an " associated " row .
How would I find something more generic , such as the ` median ` ( and its index ) as opposed to the maximum value ?
The median is more tricky .
and I want to pivot it like this : #CODE
If you load the spreadsheet into libreoffice , for example , you can see that the column headings are correctly parsed and appear in row 15 with drop down menus to let you select the items you want .
In my version there is a drop down menu in every field for row 15 .
Map categories into integer indexes
Merge df1 and df2 ( Note change in index for df2 )
df2 = DataFrame ( data , columns =[ ' f11 ' , ' f12 ' , ' f13 ' , ' f21 ' , ' f22 ' , ' f23 '] , index= np.arange ( len ( data ))
How to get the index value in pandas MultiIndex data frame ?
Is it correct to assume that ` df.merge ` can be used as a universal method to solve all merge / join problems that can be solved with the other methods ?
`` join `` is a convenience method that uses `` merge `` ; `` append `` is a convenience method that uses `` concat `` .
`` concat `` is for gluing together structures that doesn't need merging logic , while `` merge `` uses fuller logic ( and as a result is ' more complicated ')
Thanks @USER - would it be fair to say that ` merge ` uses a fuller logic than ` join ` ?
No the query language is mainly for selection . join is implemented using merge , just take a look at the code , see here : #URL
Just put your code in a function an use ` apply ` : #CODE
join or merge values calculated on grouped pandas dataframe
If I just group the object and apply the interval function , it looks like this : #CODE
Is there a standard way to join or merge values calculated from a grouped object , or should I be using ` transform ` differently ?
Try to transform on column like this - df [ ' bins '] = df.groupby ( df.hours ) .density .transform ( func )
Try to transform on column like this - #CODE
Pandas has a ` merge ` function .
` with ` Time ` value ( from the ` update ` df ) if match , or insert if a new record .
I think I would do this with a merge , and then update the columns with a where .
* keep_default_na * : bool , default True , If na_values are specified and keep_default_na is False the default NaN values are overridden , otherwise they re appended to .
That seems to drop the rows with NaN as the index ( good ) although the row seems to still exist ( just empty ); I'm not sure if that will be a problem in the future .
But I cannot work out how to use the group by functions ( transform , apply , etc ) to achieve the same result .
( Please note : In the full application this is not a sine wave but is a measured physical response to a signal sent out at the start of each period . So I am not looking for a robust way to align the signals or to detect frequencies . )
@USER it's the for loop which is slow , as is apply ( to a lesser extent ) .
If ` NaN ` indices are not supported by Pandas , we can drop the empty entries in the dictionaries above .
use ` concat ` : #CODE
How to efficiently join a list of values to a list of intervals ?
Weighted average using pivot tables in pandas
I have written some code to compute a weighted average using pivot tables in pandas .
I'd like to drop all values from a table if the rows = ` nan ` or ` 0 ` .
I am reading data from image files and I want to append this data into a single HDF file .
So , I am hoping someone can explain why this isn't working and how I can successfully append all the data to one file .
If you store a ` fixed ` format it will overwrite ( and if you try to append will give you the above error msg ); if you write a ` table ` format you can append .
default mode is to append ( to the file ); w creates a new file
In the excel file the value looks like 1 / 31 / 2014 5:47 : 00 AM In case this error message helps u out C :\ Miniconda\lib\ site-packages \pandas\tseries\ index.pyc in __new__ ( cls , data , freq , start , end , periods , copy , name , tz , verify_integrity , normalize , closed , ** kwds )
Here's a workaround , by using append .
so I chose to use pandas multiindex : #CODE
Does pandas have ` groupby after join ` similar functional ?
So I can right join the df with my list of tuple ( on the multi index ) and then ` groupby ` ?
The result of apply is a list of index values that represent rows with B == 1 if more than one row in the group else the default row for given A
The index values are then used to access the corresponding rows by ix operator
you have to use the `` * `` , you pass a list-of-dataframes to concat
I think the complaint is more that whenever any operation is done append , concat , ...
How to merge pandas dataframes bit by bit in python
I'm wondering how i can do the following merge ?
I don't want the merge to create new columns for each year-centrality merge , but rather add to the existing column .
Is the best way to deal with this to just create the complete eigenvector dataframe at once AND THEN merge with my newauthdf ?
instead of appending one-at-a-time , append the created dataframes to a list , then at the end `` df = pd.concat ( list_of_dataframes )`` will give you the result
#URL - unsupported at this time . unless a lot of interest in it ( these are done in python space anyhow , so no real advantage to using eval here ) .
that I can join on ` df1_keys ` and ` df2_keys ` .
Although these aren't supported directly , they can be achieved by tweaking with the indexes before attempting the join ...
and set union with the ` | ` and intersection with ` ` : #CODE
So if you have some DataFrames with these indexes , you can reindex before you join : #CODE
So now you can build up whatever join you want : #CODE
I thought I could simply do ` df.set_index ( columns_for_the_join )` before I join , but some of the entries in the columns are ` NaN ` , and when I convert this column to indices , it fails , since I can't later index my Dataframe with ` ind_diff = ind - ind2 ` if ` ind ` ( and therefore ` ind_diff `) have NaN values .
Not sure I understand what you're asking , you should be able to join on indexes with NaNs ...
Can I merge them into : #CODE
where the column is the userId , index is the title . isbn is dropped after the merge .
Once you ` merge ` these frames , you can use ` pivot_table ` : #CODE
I am gonna try to apply on a small set of data first .
I am working with multiindexing dataframe in pandas and am wondering whether I should multiindex the rows or the columns .
Here I choose to multiindex the column , with the rationale that pandas dataframe consists of series , and my data ultimately is a bunch of time series ( hence row-indexed by time here ) .
` loc , iloc , ix ` : row-first
For example , that ` [ ]` and ` loc / iloc / ix ` are two most common ways of indexing dataframes but one slices columns and the others slice rows seems a bit odd .
loc / iloc / ix are multi axis indexers able to index all axes at the same time ; [ ] only handles columns and is a dict like accessor ; these are pretty distinct and useful in their own right . the most common ops are probably [ ] accessing ; making it harder to do this would just make code more verbose
Ok , the ix trick is doing it , but there must be a deeper reason for this problem .
Why pandas xs doesn't drop levels even if drop_level = True
It doesn't drop for ` dfi.xs (( -1 , -1 , -1 ) , drop_level=True )` ( one 1 match )
then why ` dfi.xs (( 2 , 1 ) , drop_level=True )` can drop ?
And not drop for ` dfi.xs (( -1 , -1 , -1 ) , drop_level=True )` ( one 1 match )
When does Pandas xs drop dimensions and how can I force it to / not to ?
( may be related to Why pandas xs doesnt drop levels even if drop_level = True ) #CODE
the issue you filed was completely different than the issue of drop level not working with a fully specified indexer -that might be a bug - what you filed is not
Faster way to transform group with mean value in Pandas
I have a Pandas dataframe where I am trying to replace the values in each group by the mean of the group .
`` mean `` is cythonized , the transform has to go back and forth to python space
The result of ` df [ " signal "] .groupby ( g ) .mean() ` has length ` len ( g )` rather than ` len ( df )` .
Current method , using transform #CODE
Is there a vectorized way to apply that formatting command in either context ?
Still need to get my head around all the possibilities with ` replace ` .
Ok , then how would I apply that in my case ?
The way to get the previous is using the shift method : #CODE
You can just apply this to each case / group : #CODE
your last can just be : `` df.groupby ( ' case ') [ ' change '] .diff() `` ( though I don't think `` diff `` is cythonized so speed should be the same
One way is to ` replace ` the lower zeros with NaNs : #CODE
It seems using ` at ` ( or ` iat `) is about 10 times faster than ` loc ` ( or ` iloc `) .
it works now with using stock.iloc [ -1 ] [ ' Close '] , what is the diff between [ -1 :] and [ -1 ] , i thought [ -1 :] is syntactic rightful than [ -1 ] ...
You have to be very careful with `` ix `` , see here : #URL this has been true since beginning of pandas , but docs are newer
This is the key it tells loc which rows to set #CODE
The alignment happens only for the row mask and the column you specify #CODE
how to apply a function to multiple columns in a pandas dataframe at one time
Question : 1 - what if I have a dataframe with 50 columns , and want to apply that formatting to multiple columns , etc column 1 , 3 , 5 , 7 , 9 ,
Is there also any way to programatically create that string ( which would change depending on the number of columns you had ) and apply the format_number function ?
I.e. the above would work fine if I knew exactly how many columns were in the sheet every time , but If I didn't know the number of columns , and wanted to apply the same function to every column , is there a better way of doing it ?
@USER : If you just want to apply it to all the columns , just do ` df.applymap ( format_number )` .
You could use ` apply ` like this : #CODE
@USER ignoring my code example , if you perform ` apply ` to a dataframe then the dataframe itself is modified by any changes in your function so you would not need to assign to the column , you may still need to depending on what your function is doing .
If you have the ISIN , then you also have the the symbol , see the wikipedia page for information about how to translate ISIN to symbol :
Write a little bit of python code to translate the ISIN back to the symbol , then use the usual DataReader query .
Alternatively you could simply use the apply function on all rows of df .
and hence I can apply ` timedelta64 ` conversions .
A third alternative is to first get the unique members of ` xlab ` ( using e.g. ` set `) and then map each xlab to a position using the unique set for the mapping ; e.g. #CODE
@USER - I've added in two alternative schemes that both give uniform scaling ; one gives every point a new x / y position , while using e.g. ` set ` allows for the same label to map to the same x / y position .
` ax.set ( xticks=range ( len ( xuniques )) , xticklabels=xuniques , ...
pandas replace multiple values one column
In a column risklevels I want to replace Small with 1 , Medium with 5 and High with 15 .
You could define a dict and call ` map ` #CODE
yes that is correct ( but I realize that the issue is that the OP replace format is wrong )
Your replace format is off #CODE
I wasn't sure what was wrong with the ` replace ` format line so I suggested using ` map ` instead .
I am stumbling over the exact ( or at least most elegant ) steps to group and aggregate some data in Pandas .
Feels like you should be able to use a transform like ` g.transform ( lambda x : x [ ' datacount '] .where ( x [ ' datatype '] ! = ' bar ') .sum() )` but not quite .
You can go by using the power of apply function : #CODE
You could also replace the and / or idiom with if-else construct : #CODE
Instead use loc for this .
You can use ` apply ` and test your column like so : #CODE
You can now use this as a mask : #CODE
what would be the most efficient way to use groupby and in parallel apply a filter in pandas ?
You can write more complicated functions ( these are applied to each group ) , provided they return a plain ol ' bool : #CODE
@USER good call , looking at it , I wonder if should think about example of using agg * and * filter at the same time ( I don't think there is a way to do that easily without doing a second groupby ... ) that could be this question : s
just chain em ( but you do need a second groupby ): `` DataFrame ([[ 1 , 2 ] , [ 1 , 3 ] , [ 2 , 5 ] , [ 2 , 8] , [ 5 , 6 ]] , columns =[ ' A ' , ' B ']) .groupby ( ' A ') .filter ( lambda x : len ( x ) > 1 ) .groupby ( ' A ') .sum() ``
Basically I have just a datetime column and a price column in a pandas dataframe , and I've calculated returns , but I want to linearly interpolate the data so that I can get an estimate of prices every second , minute , day , etc ...
What I started to do was write code to interpolate my data linearly so that I could get the price every 10 seconds , minute , 10 minutes , hour , day , etc .
The problem is that concat function will work fine if X_chunks is DataFrame , but its type is TextParser here .
You can't use concat in the way you're trying to , the reason is that it demands the data be fully materialized , which makes sense , you can't concatenate something that isn't there yet .
Python -- Pandas : How to apply aggfunc to data in currency format ?
Want to apply groupby function to the data and apply sum ( over revenue_total ) .
Note : For large frames it may be worth making these columns an index ( to perform the join as discussed in the other question ) .
One way to merge on two or more columns is to use a dummy column : #CODE
Following your spec , we should drop those which are False : #CODE
Since your solution depends on the use of ` Series ` , what would happen if we are doing the difference ( i.e. join ) on ** multiple columns** ?
` AttributeError : ' DataFrame ' object has no attribute ' isnull '`
Also diff may have different index
But I've encountered cases where there are multiple records in the `' map '` row with the same value ( the max ) .
Do you mean : ` best_param = df.index [ ix ]` ?
So I need ` df.columns [ ix ]`
` dropna ` by default will drop rows where any values are ` NaN ` , what do you want to achieve ?
I have a time series and I want to apply different functions to the same column .
I know that ` read_csv ` has ` mangle_dup_cols ` but how can I do the same from a sql join in sqlalchemy after issuing : #CODE
Pandas append filtered row to another DataFrame
I apply some filters to ` df ` , which results in a single row of data , and I'd like to append that row to ` df_min ` .
I tried using a loop to traverse ` df ` , and tried using ` loc ` to append the row to ` df_min ` .
I keep getting a ` Incompatible indexer with DataFrame ` ValueError for the line where I use ` loc ` .
I guess I am not using ` loc ` correctly .
EDIT : I also tried the following instead of ` loc ` , but got the same error : #CODE
` combine_first ` is just matching the index , not the values ( like a merge would ) , and updating the NaNs in these rows .
A merge ?
If there both the same , use merge , if you want them to update in combine_first you have to index them with the unqiue index .
Note : replace works for the None but not the ` [ ]` ... this solution seems to be a little sensitive ( hence the use of negation ` ~ `) ...
Opposite of melt in python pandas
I cannot figure out how to do " reverse melt " using Pandas in python .
[ Docstring of melt ] ( #URL ): " Unpivots " a DataFrame ...
I have tried to change the dtype of the object with ` dt [ ' Date '] .apply ( str )` and I have tried a special dtype object and use that : #CODE
for i in range ( len ( tesla ) , 5 ):
Pandas how to apply multiple functions to dataframe
Is there a way to apply a list of functions to each column in a DataFrame like the DataFrameGroupBy.agg function does ?
This can be useful if you have a set of points where some areas are sparse and others are dense , but want an even distribution of displayed points ( such as on a map ) .
Finally , when the reduction / tallying is completed per grid partition , one can then visualize the tallied point ( e.g. , show marker on map at the single point with a tally indicator ): #CODE
Assuming these were datetime columns ( if they're not apply ` to_datetime `) you can just subtract them : #CODE
Now you can now set this as the index and resample : #CODE
But when I apply it to the DataFrame , I get an error .
I'd like to create a ' buy ' signal when the price of the stock is above the upper rolling mean for at least an X number of days and is below the rolling mean for at least an X number of days .
data2.AboveUpper is a column of Trues when the stock is above the upper rolling mean , and False when the stock is below the upper rolling mean .
if ` data2.AboveUpper ` is ` True ` when the stock is above the upper rolling mean , then #CODE
is ` True ` wherever the stock has been above the upper rolling mean for at least ` X ` consecutive days ; so all you need is : #CODE
Once the string is a date time object ( dt ) , I believe the code is : ` return dt.year , return dt.month , return dt.day ` .
Thanks to @USER for pointing out the sweet apply syntax to avoid the lambda .
The code currently has a bug , but the bigger issue is that I am simply unsure how to join the two features together and what is best practice here .
pandas DataFrame pivot table sum function not correct
After applying the pivot function to my df , I am returned with data that dont make sense :
For example , you can't sum a mix of strings and floats in pandas but Excel would silently drop the string value and sum the floats .
That returns a numpy array , you could just use the mask to perform the dataframe selection like ` df [ df > 2 ]` this returns a dataframe with ` NaN ` for values that do not satisfy the boolean criteria and the values that do as a dataframe .
Up to you what you then do with ` NaN ` values , either set to ` 0 ` or drop them using ` dropna `
How do I join columns from diferent Pandas DataFrames ?
Compute a sequential rolling mean in pandas as array function ?
I am trying to calculate a rolling mean on dataframe with NaNs in pandas , but pandas seems to reset the window when it meets a NaN , hears some code as an example ...
gives the rolling mean that resets the window after each NaN's , not just skipping out the NaNs #CODE
hello Jeff , was playing with min periods , when you set min_periods=1 it seems to perform like expanding_mean() after a NaN till it hits the length of the window . bar [ 0 , 7 ] on your example is the mean of 7 ( len 1 ) and bar [ 0 , 8 ] is ( 7+ 8) / 2 . think this is why you have values 0:3 and I do not
Then apply a lookup from ` df2 ` to each element of the lists in ` vals ` : #CODE
Finally concat : #CODE
you can use ` shift ` and ` | ` operator ; for example for + / - 2 days you can do #CODE
I keep getting this error : TypeError : unsupported operand type ( s ) for | : ' bool ' and ' float'for ----> 3 idx |= data2.buy.shift ( 1 ) | data2.buy.shift ( 2 )
If you look at a Panel as a stack of Dataframes , you cannot create it by stacking Dataframes of different sizes or with different indexes / columns .
You can indeed handle more non-homogeneous type of data with multiindex .
Ok , so relaxed homogeneity is one advantage of a MultiIndex over a Panel .
Note : that's a really bad way to iterate over the rows , either use iterrows or apply .
I think you should use ` apply ` : #CODE
I have been trying to use a mask or doing something like this : #CODE
I also tried something like this ( suggested on another stack overflow question ) #CODE
If you have a more complicated mask , ` rows_to_keep ` , you can do : #CODE
How can I set the index vertically , or transpose the dataframe ?
But as you suggest , you could alternatively transpose the DataFrame after reading it in : #CODE
` join ` the items with space : #CODE
Strange behavior when slicing Pandas MultiIndex
I am observing some really strange behavior while attempting to slice data using pandas MultiIndex .
KeyError : ' Key length ( 2 ) was greater than MultiIndex lexsort depth ( 1 )'
We could use ` ( df [ listvals ] ! = 0 ) .sum ( axis=1 )` instead if we only know that it's 0 or some nonzero value so that ` [ 0 , 3 , 0 ]` doesn't fool us .
This means that ` FrioAcum ` is not iterable , it is viewing it as a single value , it needs to be a list or other iterable object in order for the shapes to align , what exactly is ` FrioAcum ` ?
This looks like a job for pivot : #CODE
If you don't want a MultiIndex column , you can drop the ` col3 ` using : #CODE
add ` ~ ` to the beginning and drop ` regex=False ` ; i guess that is added in ` 13.1 ` ;
Now you unstack the second level ( type ) to make it a column and fill in the blanks : #CODE
I want to be able to assign a matrix of values to a matrix of locations using loc or , in this simplifed case , I just want to modify them by some simple operation like multiplication .
@USER I added the complete stack trace .
How do we resample above series with 0.1 interval ?
We need ` new_idx ` to be a union of the original index and the values we want to interpolate , so that the original index isn't dropped .
I think using cut is more stable , was hoping you / someone knew a better way !
One option is to use cut to bin this data ( much less elegant than a resample but here goes ): #CODE
Say if you wanted to resample with how= ' sum ' : #CODE
There's a lot of NaNs now , you can now , as Tom suggests , interpolate : #CODE
You can just resample by ' 2H ' : #CODE
I want to generate a single column of them with a unique index so that I can merge it with the columns from the other dataframes at a later point , which would looks somewhat like this , but with an unique index : #CODE
some kind of join ?
Do you want to stack AAPL / GOOG / MSFT on top of each other in one column , add the ' Returns ' & ' WinLose ' columns , and then add another column ' unique_id ' ( or something similar ) ?
But I want to stack e.g. the skew metric for all the stocks on top of each other but with a unique index so that in frame it can be joined with the other metrics calculated from the same period .
have you looked at resample , or this section of docs : #URL
resample data .
How to pass more than one parameter to map function in panda
I don't think so map operates on each element in a series , if you want to pass do something with multiple arguments on a row-wise basis then you could use ` apply ` and set ` axis=1 ` like so ` mn.apply ( lambda row : getTag ( row ) , axis=1 )` in ` getTag ` you can select the columns like so : ` row [ ' fld1 ']` and ` row [ ' fld2 ']` .
Instead of using map , you could use pd.cut ( Thanks to DSM and Jeff for pointing this out ): #CODE
And to answer the general question : Yes , there is a way to pass extra arguments -- use apply instead of map ( Thanks to Andy Hayden for pointing this out ): #CODE
Still , I don't recommend using ` apply ` for this particular problem since ` pd.cut ` is be faster , easier to use , and avoids the non-deterministic order of dict keys problem .
But knowing that ` apply ` can take additional positional arguments may help you in the future .
I think the trick is to extract start and end times and resample , I think there is a cookbook example for what's on and off at each period , this is a ( fiddly ) extension of those examples ...
One way to get the split times is to resample from Start and End , merge , and use diff : #CODE
To replace the NaTs with 0 it looks like you have to do some fiddling in 0.13.1 ( this may already be fixed up in master , otherwise is a bug ): #CODE
How to apply a long set of conditions on a pandas dataframe efficiently - stock backtesting
I'm attempting to apply a long set of conditions and operations onto a pandas dataframe ( see the dataframe below with VTI , upper , lower , etc ) .
I attempted to use apply , but I was having a lot of trouble doing so .
I append all these lists into a dataframe that I call the portfolio .
If a given row's output can be created based on * input * data in higher rows , you can save yourself some time and effort using something like ` apply ` .
yep ; you need to isolate setting / reading in a locked object ; note that ' continuously expanding with new rows / cols ' is in general a bad idea . you should simply create new frames with the new data , then `` concat ` them together to create new one .
Or even ` multiindex ` : #CODE
Pandas merge columns , but not the ' key ' column
Then I need to drop the ` id ` column since it's essentially a duplicate of the imp_type column .
Why does merge pull in the join key between the 2 dataframes by default ?
I would think there should at least be a param to set to False if you don't want to pull in the join key .
I know , but it's just frustrating since it shouldn't have been implemented that way from the beginning , and to have to do it after every merge adds up and feels hacky .
How do I group by Time by strip away microsecond , for example ,
I guess I forgot resample function , but how about a generic function ( or lambda ) .
Also , resample methods are limited , right ?
You need to make the datetime the index so that you can resample : #CODE
How do I apply a count function ?
You can [ set up a progress meter for apply ] ( #URL ) , but this obviously slows down whatever it is you're doing .
Generally a bad idea to return different types of data in an apply ( here a string or a Series ) , it's unclear what you want the apply to return ...
@USER Your comment suggests to me that I may not understand ` apply ` properly .
Ah wait , I see what you're saying , I mistook this for a DataFrame apply .
To overwrite the column with ' RARE_VALUE ' you can use transform ( which calculates the result once for each group , and spreads it around appropriately ): #CODE
Yeah ... there is something strange going on , I think maybe there's a bug here . the transform returned a DataFrame otherwise
@USER similarly for the astype bool ...
I'm v . shocked yours is so much faster than transform and the transform returns 2d data in this case ...
( also confused by transform ( ' count ') ! )
transform has to go back / forth to python space ; @USER soln is all indexing ( and value_counts which is in cython )
Are we talking Price-Is-Right rules or smallest ` abs ( event_time - consumer_time )` ?
Using ` agg ` function I got as many new columns as there are functions .
Thanks @USER I didn't know about ` loc `
Pandas : peculiar performance drop for inplace rename after dropna
that's near what i am guessing ... but even yahoo just put the parameters ( index as u guess ) , the length of data chunk to download is vastly diff , i am talking a about 1:200 dataframe ( there are 6~7 nos of column , e.g. open , high ... adj close , etc . ) time 1200+ nos of stocks !!!
shouldn't there be any performance diff ? and , if your guess is right , anything i could do to enhance the time to download ( performance ) ?
For example , ` groupby ` would be appropriate for determining the sum of all GDPs , or the median population , etc .
Or , if these are numpy arrays you need to apply tolist to each item first : #CODE
I am using ` pandas.rolling_apply ` to fit data to a distribution and get a value from it , but I need it also report a rolling goodness of fit ( specifically , p-value ) .
Ok , here's one way to attack it , using features from the ` matplotlib ` ` hist ` function itself : #CODE
I merging this data with other samples and my apply function that uses asof is similar to : #CODE
yes , use merge and vectorized operations instead .
Basically I'm finding the appropriate timestamp and assigning that col first to the destination table , then doing a merge on that .
not sure what you are going for here . looks like you might simply want to resample on both intervals to common times then join , seems easier .
@USER These aren't intervals they are event timestamps and since they don't align , I need to use indexof to find the " closest " match .
pandas groupby apply function that take N-column frame and returns object
Is there a ' transform ' method of something like that to apply a function to groups ( all columns at once ) and return an object ?
and suppose I do a groupby on Date and apply some function to the groups labeled by ( Term , Month , s ) .
I can obviously just iterate through the groups and aggregate the results but I imagine I'm just missing something obvious about how to use one of the transform methods .
You could apply the function and then aggregate each group manually .
Then we can apply the fit() on all the columns at once for each group of rows : #CODE
I should have mentioned that I can use the ' apply ' or ' agg ' function but it returns the same ( redundant ) object for each column .
Hmmm , I think you are correct for the usage of ` agg ` , but the ` apply ` method should normally receive all the columns at once for each group .
On integers the ` ` operator performs a bitwise mask #CODE
One more suggestion ` df [ ' data '] = df [ ' data '] .apply ( lambda x : x & mask )`
You can convert all elements of id to ` str ` using ` apply ` #CODE
How do I join the two data frames using the date index .
Or , if you wish to keep all the keys in both indexes , use an outer join : #CODE
Do you want the new dataframe to have as its index the union , or the intersection or some other combination of the indices of the building blocks ?
` concat ` should get you the union .
Various options for ` join ` and ` merge ` let you explore some of the other possibilities .
Note that you need to use ` axis=1 ` to stack things together side-by side and ` axis=0 ` ( which is the default ) to combine them one-over-the-other .
Seems like you want to join the dataframes ( works similar to SQL ): #CODE
If ` None ` , it'll join on the indices of the two dataframes .
You could drop the NaN and then strip the offset and then do the conversion : #CODE
Pandas : drop a level from a multi-level column index ?
How can I drop the " a " level of that index , so I end up with : #CODE
Reorder columns in pandas pivot table
One way is to create the MultiIndex and reindex by that : #CODE
You really want to return a bool to indicate rather than a string .
Also avoiding apply where possible .
You can do a transform and use it as a boolean mask : #CODE
` describe ` may give you everything you want otherwise you can perform aggregations using groupby and pass a list of agg functions : #URL #CODE
As you prefer you can transpose the result if you prefer : #CODE
I already know you can pull specific keys from dictionary objects in pandas if you already know the exact value for the key , but what if you wanted to pull the median key's values of a dictionary without knowing the value ( or in this case , author name ) ?
So I would want to get the median author name .
If you were taking about an integer / float column then you can just use the median method : #CODE
The best part of this is that I can adapt this code much more easily , and I could replace the " 4 " in sets with a variable that splits whatever I need .
Any idea how I can replace ' Consultation ' with blank and convert the column data type to int64 or float ?
It is my wish to remove the string part to apply aggregation functions on that columns .
I edited and added another question regarding the transform .
My basic question is can I efficiently apply this structure to HDF ?
How can I " reshape " x to merge both ?
So prob easiest to simply drop them and reindex to the new times you want .
You will have to either use a multi-index and loop on the duplicates or better yet just resample the data down ( e.g. say mean or something ) .
thanks but for ` df.set_index ( ' time ') .reindex ( new_range )` I get ` ValueError : cannot reindex from a duplicate axis ` and that's maybe due to the fact that ` df.Time ` is in ` dtype : datetime64 [ ns ]` since I have nanoseconds
try : `` ( Index ( df.Time ) + date_range ( ..... )) .unique() `` ; the goal is to create a combined ( unique ) index that you to reindex to ; nanosecons work fine ( just simpler for the example above )
sorry I forgot the ` ( )` but I still get an error : ` ValueError : cannot reindex from a duplicate axis `
`` resample ( .. )`` is your friend
You can't actually resample this at the nanosecond level it will blow up your memory , but see above for an easy way .
interestingly this is slower than the map method in the other answer ` 10000 loops , best of 3 : 96.4 s per loop ` versus ` 10000 loops , best of 3 : 125 s per loop `
.agg stands for aggregate .
You can then pass a dictionary of functions to the agg that tells it what to do with each column .
Now create your groupby object , create a dictionary of functions to be executed on each column , and pass that dictioanry to the .agg method ( aggregate ) like so : #CODE
apply hierarchy or multi-index to panda columns
TypeError when trying to join Pandas dataframe by index
I'm trying to join a column from one ` pandas dataframe ` to another using its date as the index .
I think you want to do this ` cln_df22 = cln_df2.append ( df3 [[ ' MeHg ( ng / L )']])` , what you attempted was to access an item from the dataframe of the append attribute which doesn't exist
I have some data which has multiindex called stdDF like so : #CODE
well it say `` df.ix [ ' bar ' , ' one ']`` , note that it uses `` ix `` ( its very similer to `` .loc ``) . but this is NOT the same as `` [ ]`` .
That example , has a COLUMN multi-index ( it IS a bit confusing as it then takes the transpose to make it a multi-index on the INDEX ) .
I am trying to join two numpy arrays .
As far as I can see I should be able to join these columns ?
Take a sequence of 1-D arrays and stack them as columns to make a
As ` X ` is a sparse array , instead of ` numpy.hstack ` , use ` scipy.sparse.hstack ` to join the arrays .
So , use ` scipy.sparse.hstack ` when you have a sparse array to stack .
How to aggregate multiple columns in pandas groupby
Filter out pandas pivot table rows
I have a pivot table like so : #CODE
If the cut off is 3000000 : #CODE
Meaning keeping the rows where at least one of the value are above the threshold , or drop the rows where all the values are below the threshold ?
Just before you transform ...
Do I need to append data from the original reviews back onto my sorted dataframe ?
Do I need to make a function to apply onto the groupby function ?
The columns ( like date ) aren't specific to the reviewer so it's not clear what it would mean to append them to the review counts .
Then , you apply some number of functions to each group and pandas will stitch the results together as best it can -- indexed by the original group identifiers .
There are a few ways to do this -- I'd look into ' agg ' and ' apply ' .
' Agg ' is for functions that return a single value for the whole group , whereas apply is much more flexible .
Since each of these is a single value per group , use ' agg ' :
We can use ' apply ' , which works with any function that outputs a pandas object .
Each familyid is represented twice in this data ( i.e they are siblings ) , and I wish to transform this dataframe to a 2*10 column dataframe where :
one way to do that is to ` tag ` each sibling as ` 1 ` ( first ) or ` 2 ` ( second ) and then pivot ; for example starting with #CODE
I get an error on implimentation of the the transform function : ... unsupported operand type ( s ) for - : ' str ' and ' int ' .
I am trying to run hstack to join a column of integer values to a list of columns created by a TF-IDF ( so I can eventually use all of these columns / features in a classifier ) .
Finally , I want to join them all together , and this is where our error occurs and the program cannot run , and also I am unsure whether I am using the StandardScaler appropriately here : #CODE
Another possibility is to use Pandas's ` concat ` .
Pandas resample
I am trying to resample the data in pandas , to a regular 100 milliS or a S frequency .
Is resample not meant to be used here ?
When you pass how = ' ffill ' to the resample method , it uses dataframe.ffill which fills in null values .
If you want to use resample and return the first observation within each group , use the ' first ' method : #CODE
So , are you really looking for a sampled series or do you just want to drop superfluous rows ?
Resample also takes an argument ` fill_method ` , which can be set to ` ffill ` which I think is what you were trying to get to before .
You should take a look at the [ resample documentation ] ( #URL ) which will give you more flexibility to customize .
Pandas has a lot of built-in functionality to apply functions in a vectorized way over Series and DataFrames .
When that fails , you can use ` map ` or ` apply ` .
Here ` map ` will applies a function element-wise on a Series .
For more on map and apply , see this answer .
Then , regarding your more general question you could use ` map ` ( as in this answer ) .
I want to check if there is minute without any data , so I did resample #CODE
This is better than having multiple DataFrames , because you can apply fast numpy / pandas operations to the entire DataFrame whereas , if you had a list of DataFrames you would be forced to use a Python loop to operate on the sub-DataFrames individually ( assuming you want to perform the same operation on each sub-DataFrame ) .
Is it possible to use cut on a collection of datetimes ?
Effectively you turn ` datetime ` or ` date ` into a numerical distance measure , then cut / qcut it .
Thanks a lot , tried and it works . not very familiar with reindex
Say I have a dataframe ` my_df ` with a column `' brand '` , I would like to drop any rows where brand is either ` toyota ` or ` bmw ` .
I can then ` join ` that to my original dataframe , and ` fillna ` with the ` method= ' ffill '` #CODE
Note : ( as pointed out by OP ) by default NaNs will propagate ( and hence cause an indexing error if you want to use the result as a boolean mask ) , we use this flag to say that NaN should map to False .
What I would like is to be able to do ` d.map ( s )` , so that each row of ` d ` should be taken as a tuple to use to index into the MultiIndex of ` s ` .
However , DataFrame , unlike Series , has no ` map ` method .
I know I can do it by converting the DataFrame to a list of lists , or by using a row-wise ` apply ` to grab each item one by one , but isn't there any way to do it without that amount of overhead ?
You could create a MultiIndex from the DataFrame and the ix / loc using that : #CODE
I get 900us for the MultiIndex solution and only 90us for your ` apply ` .
But that ` apply ` doesn't actually do the indexing ; timing ` d.apply ( lambda r : s.ix [ r ] , axis=1 )` gives a much slower result of about 5.5ms .
@USER answer gives a higher level reason while you should not rely on this behavior and just use loc always speciying all indexing dimensions .
Widening Pandas Data Frame , Similar to Pivot or Stack / Unstack
I am reading rows from the origin dataframe and trying to append it to target dataframe .
then i construct a dict to append to target dataframe .
What happens if you just append to a list and then append to the dataframe outside of the for loop ?
Series ( and DataFrames ) have a hist method for drawing histograms : #CODE
If it is a DatetimeIndex the apply won't work .
In this example , you can use a TimeGrouper to group by month ( which groups by year-month , like in a resample ): #CODE
I want to merge rows in csv files by matching the id with a given dictionary .
Literally , I should match : B.value , if ( B.Value == l.item() ) , I should insert the item from dict in the same row where we found the value in column B .
If you l were a DataFrame you could do a merge : #CODE
Merge on column A and the index of ` l_df ` : #CODE
What If I need to match with A column , how can I merge it ?
you can try append or concat . you are doing an assignment .
I'm worried that append and concat will be to slow .
I may try reindex the smaller dataframe and work for there .
It seems that if I reindex to match the st dataframe it works fine .
Merge two identical CSV from the same directory - Python
I want to read both CSV and merge them to create one bigger data frame .
concat takes a list or dict of series : #URL , so what you can do is make a list of the dataframes and concat them all together to make your big df : #CODE
Alternatively you can append : #CODE
I really want to merge 30 csv in one csv .
To find just the values of A where this is True you could use the sum agg method : #CODE
Sure , I guess I expected there to be a way to do this without using the custom aggregate to effectively just echo the values .
This behaviour isn't guaranteed , it's much better to do assignment using loc or assigning the column directly : #CODE
I should add that you can do this by assigning to the original frame ( using loc / ix ) , however you can ( and should ) usually avoid this by vectorising your solutions rather than iterating over each row : #CODE
transform " time " - which represents second elapsed from start time - in ` datetime64 ` datatype , and make it the index .
I am trying to append ` df1 [ ' B ']` based on ` df2 [ ' B ']` so that my desired output would be : #CODE
I believe you'll want a simple merge : #CODE
Now , you can do the groupby and use agg with the ` first ` and ` last ` groupby methods : #CODE
To get the complete path you will need to join ` targetdir ` and ` file ` ( bad variable name as it masks the ` file ` type ) .
I recommend using concat rather than append , as this way you don't make many intermediary frames : #CODE
One way is to merge twice .
and then merge with this result : #CODE
the date_format argument does not apply to timedelta dtypes .
Time diff is an integer given in nanoseconds , not a date .
The line ` df.mi.value_counts() > 2 ` produces a series of boolean values , the index is the value of mi , we therefore want to use this index which will produce some NaN values as some counts are not > 2 , we therefore should drop these .
Pandas ; tricky pivot table
I have a pandas dataframe that I need to reshape / pivot .
It looks like this requires more than one pivot or a combination of pivot and groupby , but I'm having no luck ...
As someone w / o any R experience , it's nice to see ` melt ` in action .
I hadnt come across the melt method yet , cheers .
As an alternative to ` melt ` , you can set a MultiIndex and chain the ` stack ` and ` unstack ` commands : #CODE
you can melt the categories : #CODE
You can do an apply on the LgRnk column : #CODE
But using the documentation I don't see how I can apply this to Raster data
Alternatively , take a look here #URL to see if any of the functions return what you want for your numpy array and replace the query in get_raster accordingly .
Using apply , as in ` df.x.apply ( tuple , axis=1 )` will work , but then I'd somehow need to iterate over the first level of the index .
Map string values in a Pandas Dataframe with integers
In Pandas ` DataFrame ` how to map strings in one column with integers .
I have around 500 strings in the ` DataFrame ` and need to replace them with integers starting with ' 1 ' .
No idea of how to map these Requests with 1-500 .
What will you do with this int , will this form an index or does it map to another column ?
I just need to map them in order to make it more easy for other processing .
So what you could do is construct a temporary dataframe and merge this back to your existing dataframe : #CODE
Now merge this back to your original dataframe #CODE
Ignore the fact that there is a redundant column , I can drop that easily .
Unfortunately , I cannot upgrade my stack just yet to 0.13 , I have to find a 0.12 solution .
I'm just curious , how does eval ( x [: -1 ] +D [ x [ -1 ]]) work ?
` eval ` basically runs the ` string ` provided to it as ` python ` code .
food_pd = pd.DataFrame ([ item for sublist in map ( lambda a : a.keys() , a.values() , food.itervalues() ) for item in sublist ] , columns =[ " Food "]) .
use ` ( a.keys() , a.values() )` ( with brackets ) inside ` map ` .
FYI : ` site2.index.values ` produces this ( I've cut out the middle part for brevity ): #CODE
Or align one of them vertically first : #CODE
pandas : how to use both name and dataframe of groupby object in an apply
I would like to apply ` func ` to each ` id , group ` in the groupby object .
Is there any better ( faster ) way of doing this using an apply or similar ?
You can use ` apply ` on the groupby object to apply a function to each group .
I would use the data for analysis , such as finding matches with other datasets , and visualisation , such as visualising profiles individually , or some statistic of each profile on a map .
See this ticket on GitHub for a follow-up on the topic : " Awkward " Integer-indexing in MultiIndex DataFrames
On this note - it surprises me that ` loc [ ' label '] ` retrieves ** all ** entries on subsequent levels , but ` iloc [ k ]` only returns ** one** .
doesn't work if N > len ( levels ) or K > len ( levels [ N ]) , works if you know these in advance ...
Using pandas built in functions you would have to apply ` notnull() ` over all series and then call a numpy function to the DataFrame anyway .
Edit : Apparently pandas has a ` notnull ` function for DataFrames in 0.13 , you can replace all ` ~ np.isnan ( df )` with ` df.notnull() ` if you wish .
` transform ` " broadcasts " the grouped values up to the full dataframe .
I didn't even know ` transform ` existed !
* Note : this should be fixed from 0.14 onwards i.e. you should be able to save a MultiIndex to sql .
See How to insert pandas dataframe via mysqldb into database ?
Thanks so much for the answer and the link to " how to insert pandas to mysqldb " .
And can you apply it to selected columns , instead of the whole dataframe ?
Panel truncate error : tuple object has no attribute ' year '
Ix / loc is very fast when setting big ranges / slices .
Given an ` opi ` and a ` new_value ` , I want to replace the first occurrence of Nan in the row with the ` new_value ` .
I was really looking to only replace the first occurrence of a NaN in a ** specific ** row .
Calculate median of the columns pepCN1 , pepCN2 and pepCN3 , respectively , for each gene
Calculate the median of the results of ( 1 )
The above data for gene ` CD38 ` would give two median values ( ` R1 ` and
another value for ` R3 ` based on the median of the two values in
The fact that the statistics are calculated after the first round of " reducing " the data ( i.e. calculating the first median ( s ) ) is intended : the three data columns represent technical replicates , and should be handled individually before " merging " them together in a final statistical value .
The script will here reduce the two ` pepCN1 ` values to a single median , disregarding the fact that there are no values ( i.e. no data from replicates 2 and 3 ) with which to calculate statistics with from the other data columns .
The script will function and give the correct ` CN ` value ( the median ) , but the statistics of standard deviation and coefficient of variation will be left out ( i.e. show as ` NaN `) .
In cases like this , I want the script to somehow see that reducing the data column to one value ( the first median ) is not the way to go .
Essentially , I want it to skip calculating the first median ( here : ` R1 `) and just calculate statistics on the two ` pepCN1 ` rows .
prob easiest to do something like : `` df.dropna ( subset =[ ' pepCN2 ' , ' pepCN3 '] , how= ' all ' , axis=1 )`` apriori to drop them from your main calculation and then use the return value of the drop to actually do what you need .
that's the beauty of doing it this way , you can change your mask really easily to whatever function you need .
don't start with an empty frame and try to append ; instead , append the results of the computation to a list , then do a `` pd.concat ( list_of_frames )`` at the end ( will be much faster too )
no , I just mean result part , e.g. when you are appending a frame to another frame ( and starting with an empty one ) . instead of starting with an empty one , append the resultant frame to a list , then when you are ready for the final frame , just do `` pd.concat ( list_of_frames )`` ; I think I fixed the issue you are seeing in upcoming 0.14 ( but not out yet ) .
no you cannot just group , you have to apply / transform .
The quickest way I know how to wrangle this thing into a long form dataframe is using ` stack ` and then ` reset_index ` : #CODE
Maybe my real question is " why isn't ` melt ` a DataFrame method ?
not sure why their isn't a `` melt `` on DataFrame , could / should be .
Otherwise you could just filter the NaN ( I'm assuming that is what you want to do , difficult to tell without sample data and code ) and apply the map : ` df [ ' flag '] = filtered.notnull() .map ( ' N ')`
What is the most elegant way to append np.nan to the arrays so that they match the sizes ?
@USER yes you can but it depends on how you want to filter the rows , you can do this by position indexing ` iloc ` and label ` loc ` , you can check the docs : #URL
If you wanted to add frequency back to the original dataframe use ` transform ` to return an aligned index : #CODE
" If you wanted to add frequency back to the original dataframe use transform to return an aligned index : " Does .transform ( ' count ') replace the original index , or did it never have an index until this ?
@USER No , what you see is that it creates a series that aligns with the original dataframe , unlike the other methods which display the unique values and their frequency , if you wanted to just add frequency count back to the dataframe you can use transform for this .
The " transform " part was really useful to me , as I grouped by 2 columns and counted , and didn't want the DataFrame to collapse .
One way is to use an apply : #CODE
One can replace it with format ( x [ ' Quantification '] , x [ ' Calibration ']) .
Pandas : merge miscellaneous keys into the " others " row
I want to merge everything else into an " others " row like this #CODE
Finally , append this column to the second df : #CODE
and just cannot figure out how to drop only a single ' sublevel ' , e.g. ` df.col1.a `
In 0.14 , you'll also be able to pass regex of what to drop ...
There's also a way to drop the entire level of a index / column : #CODE
Then generate the patterns using join .
Use a pivot table : #CODE
Then replace all the Nan's with zeros : #CODE
Then you might want to drop the multiindex column : #CODE
I need to combine different functions into one and use the apply function ( of those individual functions ) within the main function itself .
So I'd get different columns with the sum , diff , product , quotient etc .
is there a way to apply each one in such a way that they come under different columns ??
Join dataframe with different indices
I want to join the data from df2 into df1 and for the missing indices I want to have NAN .
In a second step I want to replace the NaN with the last available data like this : #CODE
First merge #CODE
or specify the columns from both left and right hand side to merge with : #CODE
Renaming time to ` start_time ` and adding a new column that computes the ` end_time ` is easy enough ( assuming that's the next step ) -- what I'm not quite sure I understand is how , afterwards , I can resample this data so I can chart concurrents .
A way to get the open events is to combine the start and end times , resample and cumsum : #CODE
Also possible to value_count with bins aka cut ( or qcut ) .
count = len ( my_variable [ my_variable <- 1.01 ] )
In fact this is the point of this type of index , to make a uniform indexing model with ` [ ] , ix , loc ` returning the same exact results .
You need to specify the agg function as len : #CODE
That is why I use the ` append ` since this is in a loop .
concat the list into one larger frame , and use ` to_csv ` : #CODE
iterate over each DataFrame and use ` to_csv ` with append ( ` mode= ' a '`) .
I know that correlation is a DataFrame but not if I append each dataframe in a loop .
@USER in that case you can either : concat to make one big DataFrame and use to_csv , or mode= ' a ' ( append ) whilst using to_csv .
Can also use df.filter ( regex= ' DUMMY ') rather than loc :)
Pandas will always try to align data based on the index .
Python Pandas - replace values with NAN in multiple columns based on mutliple dates ?
I want to use this event date as the maximum date -1 in the column and replace values after that date with ` NaN `
I understand how to replace the value is one column I think using a Boolean mask .
The ` apply ` returns a dataframe with True / False values ( the ` ` expression is evaluated for each column where ` x.name [ 2 ]` selects the third level of that column name ) , and the where replaces the False values with NaN .
Is there anyway to apply this simple function to each element in the data frame ?
So I insert the results into a Dataframe and would like to get 3 columns ( Experiment , first draw , second draw ) How do I efficiently split it so I can plot the results on to the results Dataframe as a number ( ie ) #CODE
To reindex have same rows and columns , I think you'l have to reindex : #CODE
In reality ` len ( multi_df )` and ` len ( look_up_list )` are quite large so I need to optimise this line : ` [ multi_df.ix [ idx ] **2 for idx in look_up_list if idx in multi_df.index ]` .
( I am trying to apply your solution and compare times . )
I'm not sure it is a bug , but it is definitely not right for this purpose , and I do think it is better to stick with the indexers , ` loc ` , ` iloc ` or ` ix ` when possible .
just try to select it via loc
Relabeling pivot tables
Lets say I have 2 pivot tables and I merge them , to make the case simple I will use binary digits .
found the answer to the second problem ` pd.merge ( df , df2 , how= ' inner ')` It is the method ' how ' in merge .
python pandas strange error when concat the return values of ' apply '
The apply function is trying to return a whole dataframe in your case .
You can use apply to map a column or a row of a frame to a row / column or scalar .
What you are trying to do is basically a join .
You want to join the tables with the names and the surnames of the employees with the rest of their details .
is load the whole HR database in a dataframe and join it with the new_employees dataframe .
In pandas , you can use ` apply ` to do similar thing #CODE
Since you are creating it after ( via join ) it is dependent on the underlying numpy view creation mechanisms .
don't ever ever ever assign like that , use ` loc ` #CODE
Why does my Pandas join shift rows of the joined data ?
In Pandas , when I ` join ` , the joined data is misaligned with respect to the original DataFrame : #CODE
I expected the joined column to align with all of the others .
That said , ` join ` doesn't blindly put two dataframes next to each other .
It behaves like an SQL join and aligns the data on the indices or specified columns .
Also why are you calling ` shift ( 2 )` ?
You call ` shift ( 2 )` and I see two rows with ` NaN ` values .
@USER : I expected the dataframe added in the final line to align , but it is shifted " up " two rows .
And ` pd.concat ([ flu_test , pd.DataFrame ( exp ( flu_trend_2.predict ( flu_test )) , columns =[ " ILIPred2X "])] , axis=1 )` works as ( I ) expected ` join ` to work .
@USER I think you called ` pd.DataFrame ( exp ( flu_trend_2.predict ( flu_test )) , columns =[ ' ILIPred2 ']) .index ` having already done the ` join ` .
Creating that seperate DF from scratch before the join gives me an index from 0 - 49 .
However , I have no luck after that as I tried writing to the database using to_sql() and it still doesn't work : ` df.to_sql ( ' TRORE.HW_TEST_PY ' , engine , flavor = ' oracle , if_exists = ' replace ')` and not ` engine.has_table ( ' MY_TABLE ' , ' MY_SCHEMA ')` either .
As I was only trying to insert into a specific schema , even though I specified it in the name ( eg . my_schema.table ) , it still created the table in the main schema of the database .
Count size of rolling intersection in pandas
To calculate the intersection between consecutive sets , make a shifted version ( I replace the NaN to an empty set for the next step ): #CODE
Combine both series and calculate the length of the intersection : #CODE
Another way to do the last step ( for sets ` ` means ` intersection `) : #CODE
The reason the rolling apply does not work is because 1 ) you provided it a GroupBy object and not a series , and 2 ) it only works with numerical values .
For set difference , ` s - s2 ` does work , but intersection as ` s & s2 ` seems not to work for pandas serieses .
I was looking for a way to apply a function on the elements of two serieses , but didn't find an obvious one .
If your categories can grow and the meaning can change i.e. you have dynamically expanding category where potentially more than 10 categories could be classified as low then you'd have to recategorise also which would make it slow
I think you're looking for cut : #CODE
And you either just map over the labels : #CODE
There hopefully will be some support for parallel ` apply ` in 0.14 , see here : #URL
You can use boolean mask on the dtypes attribute : #CODE
` random.choice ( range ( 0 , len ( x )))` is better written as ` np.random.randint ( 0 , len ( x ))`
This is wrong because the replace function only takes numbers from 0 to 59 as args for minutes .
also , there's no reason to use apply in this case .
Select and merge many statements like the following #CODE
Perform some sort of join
Since , in your actual use case , the values in ` df [ ' Name ']` are ` ints ` , you might be able to generate the boolean mask faster using NumPy indexing instead of ` Series.isin ` .
However , if ` N ` is too large , then forming ` idx = np.zeros ( N , dtype= ' bool ')` may not be feasible .
You could iterate over each column / bounds-list and apply the same filter .
Even after the mem usage swells in worker , a single ` ns.df.loc [ ix ]` call can take several seconds .
I apply a custom function on the DataFrame column ( convert_time ) #CODE
( apply a function to remove the unnecessary data )
It's very easy to interpolate NaN cells in a Pandas DataFrame : #CODE
consider using the series method hist rather than ... whichever one ( ? ) you are using .
It was more of a guess rather than an answer . marillion : IMO it's weird / unpythonic that hist cares about this and doesn't just iterate over it .
I assume the reason this does not work is because the matplotlib ` hist ` method tries to access the first ` 0 ` -index element of the input .
And indeed , as @USER says , you can also use the pandas ` hist ` method : #CODE
I think I will be using the pandas ` hist ` method .
Would this be faster than populating a dict [ col_name ] [ index_name ] and than using reindex after a from_dict ?
you can also initialize the data frame with the list of dicts without calling append , ie df = pd.DataFrame ([ dict1 , dict2 ,... ]) but I think there is no performance difference .
` map ` might be better for this simple case
Only better in terms of simpler syntax : ` df.rain_column.map ( d )` , and perhaps faster performance-wise , it depends on data size and type for a dataframe with 100 rows then ` apply ` is marginally faster ( apply 228 us vs map 287us ) , for one with 10000 rows then map is 26 times faster ( map is 512 us vs apply 13 ms )
Alright , this makes a lot of sense , since apply is more general purpose than map .
There's not need to use a ` map ` anymore .
How can I use apply with pandas rolling_corr()
Also how do we do the same using some applymap or apply / map method of df ?.
In your ` for-loop ` , if you replace ` df [ ' Prediction '] =1 ` with ` passenger [ ' Prediction '] =1 ` , then you * would * be adding a new " column " to the ` passenger ` row but this would not affect ` df ` since ` passenger ` is a * copy * of a row of ` df ` , not a * view * into the underlying data in ` df ` .
But in R , I could do a similar thing using apply method and then using the same if-else statement .
Or do you think we also have a similar way to do this in pandas using some applymap or apply method and if-else construct ?.
c$Flag -> apply ( c , 1 , function ( X ) {
TypeError : cannot compare a dtyped [ float64 ] array with a scalar of type [ bool ]
df [ ' Pclass '] == 1 & df.Age < 18 )) where it is taking df [ ' Pclass '] == 1 as Series ( array ) while df.Age < 18 is bool .
As each analysis looks at multiple securities , I'm wondering if it's preferable from an ease of use and efficiency perspective to store each time series in a separate df , each with date as the index , or to merge them all into a single df with a single date index , which would effectively be a 3d df .
Then , when you want to correlate some timeseries together , you can merge them and put suffixes in the columns of every different df to differentiate between them .
for these types of things i usually use a multiindex ( #URL ) with symbol as the first level and date as the second . then you can have just the columns OHLC and you're all set .
to access multiindex use the ` .xs ` function .
That doesn't work for me : I get ` np.sum ( test_mask ) + np.sum ( train_mask )` that's not the same as ` len ( quality )` .
Any thoughts on how to get the same rows ( other than generating the mask in R and transferring it over ) ?
And I want to resample to daily frequency , it is quite easy : #CODE
Is there a good pythonic way of using resample to both perform the specified " how " operation AND track number of data points going into each mean value , e.g. yielding #CODE
To get a simple DataFrame like the desired output in your question , you can drop the extra level like ` df1.columns = df1.columns.droplevel ( 0 )` or , better , you can do your resampling on ` df [ ' A ']` instead of ` df ` .
Use a Boolean mask : #CODE
Interpolate and eliminate the original values and show me the differential #CODE
Why not just use apply with these functions .
Anyway , like I commented usually it's better to use groupby methods e.g apply .
Using dateutil you could transform your timestamp columns to ' real ' timestamps :
You could index into ` columns ` and use ` drop ` , e.g. ` df2 = df1.drop ( df1.columns [[ 2 , 3 ]] , axis=1 )` , but I wonder if there's a cleaner way .. hmm .
yep , awesome , ` df1.iloc [: , ~ic ( len ( df1.columns )) .isin ( np.r_ [ 0:3 , 4 ])]` works like a charm .
hhm , i keep getting series object has no attribute stack ?
@USER that would be a sign that the apply did not create a dataframe .
Now comes the third part - Let's apply same reasoning .
` reindex ` is not defined in ` MyDataFrame ` .
Now ` reindex ` is indeed defined by this class and it returns a pandas.DataFrame object !
It is possible to implement a parent ` reindex ` method so the result is a child subclass : #CODE
I want do something like ` dat [ ' p + str ( type )'] = replace ` to get : #CODE
I can do it with list comprehension but I would like to understand if I can do that with the apply method of DataFrame .
Probably it is just because of my shallow knowledge of pandas , but when I use the apply method I imagine the serie as a list or so end hence I do not have any idea on how to " put " the index attribute .
Selecting rows from pandas by subset of multiindex
I have a multiindex dataframe in pandas , with 4 columns in the index , and some columns of data .
I've found how to generate a sort of mask for what I want : #CODE
Unfortunately , ` rdata.loc [ mask ]` fails with ` IndexingError : Unalignable boolean Series key provided ` .
however , both getting the count of distinct values using ` len ( set ( index.get_level_values ( ... )))` and building the boolean vector afterwards by iterating over every row feels more like I'm fighting the framework to achieve something that seems like a simple task in a multiindex setup .
There might be something better , but you could at least bypass defining ` mask ` by using groupby-filter : #CODE
` drop ` by default operates on rows , not columns .
` drop ` by default operates on rows , not columns .
groupby after concat , column missing in the group mean
concat two dataframe , then groupby ' type ' and calculate the mean , columns of second df , i.e. d1~d10 , showing in the concat'ed dataframe but not in the grouped mean .
I want to pivot on the log categories and group by day .
The precise position of the data points is vital , so I can't resample to an even grid , which would be much easier to deal with .
Pandas : peculiar performance drop for inplace rename after dropna
When you apply a new operation , this triggers a ` SettingWithCopy ` check because it could be a copy ( but often is not ) .
@USER fair point ; I usually just use a temporary variable , say `` mask `` , where the meaning is clear .
I get the following error : AttributeError : ( "' str ' object has no attribute ' loc '" , u'occurred at index 2010-12-31 00:00 : 00 ') .
Perhaps creating a duplicate dataframe and doing an intersection of ` id1 ` and ` id2 ` will do ?
To assess my need I have done the following after stack .
These are wind speeds : once I have similar data from a number of different met stations I want to be able to form a DataFrame with the bins as the index and the columns as the freq . distrs .
I tried to see if this could be done when doing the pivot table but I don't think it can be .
You can then use this mask to assign a new value if d [ s ] is below a certain threshold #CODE
I am following the docs but getting an error when using the ` if_exists : replace ` parameter .
Is there a threshold above which I should truncate ?
My solution has been to truncate the object columns while reading them in .
If I truncate to a width of 20 characters , the h5 file is only about twice as large as a csv file .
However , if I truncate to 100 characters , the h5 file is about 6 times larger .
I include my code below as an answer , but if anyone has any idea how to reduce this size disparity without having to truncate so much text , I'd be grateful .
slices aren't conditional , you'll have to apply a filter .
When I try to replicate the example here , my violin plots ( with my data ) don't show the median and median , along with the 25th and 75th percentile , but the original example does .
( even though the box plot can figure out the median , and percentiles )
What is the best way to change all the non zero values in the following pivot table into a pie chart using percentages of the total ( sum of all the data in the pivot table ) ?
So basically I want to create a pie chart from the pivot table where values are present and use labels such as ' Unix A ' with a value of 3 / sum=% of everything in the table .
First stack your data ( so the different columns become an index level , see stack docs ) , and then select only the positive ones : #CODE
I'm trying to replace a row in a dataframe with the row of another dataframe only if they share a common column .
The result of the inner merge between both dataframes returns the correct rows , but I'm having trouble inserting them at the correct index in the first dataframe
You could use ` apply ` , there is probably a better way than this : #CODE
One way is to merge index values as well - pd.merge ( df1.reset_index() , df2.reset_index() , ... ) .
Then run apply twice to return indices of df1 and df2 as appropriate .
I am trying to finish up a homework assignment and to do so I need to use categorical variables in statsmodels ( due to a refusal to conform to using stata like everyone else ) .
I am lost in a sea of ix , xs , MultiIndex , get_level_values and other Pandas .
I felt that I should be able to do something with xs , or ix , but the former seems to only let you filter by a specific value , and the latter only indexes on the position in the series ?
That was an issue related to boolean operations on bool arrays with a single element .
Just drop them : #CODE
this will drop all rows where there are at least two ` NaN `
then you could then drop where name is ` NaN ` : #CODE
Without setting threshold then it will drop any rows containing NaN which would remove the Graham row which is not what you want , you would need to define the criteria for dropping rows if you were to use ` dropna `
I have a dataframe to start with , with that dataframe I want to apply some function .
I want to repeat this many times and build / stack the reults from the operations in a new larger dataframe .
You could just append : ` large_df= large_df.append ( df_new )`
If you append one result at a time , the memory for the results has to be re-allocated each time .
Clearly the ` apply ` method isn't a disaster .
For what it's worth , I think my ` apply ` column-wise solution , above , is easier to read than the ` pd.concat ...
using `` apply `` will be much slower .
I was also thinking about approaching this using ` Series.map ` ( i.e. treat the ` ccy ` level of the index as a ` Series ` and use ` map ` to convert it to a series of the foreign-exchange rates and then multiply the original dataframe by that series ( which would be like-indexed ) .
also , What if I had two more columns ' C ' ' D ' but need to drop duplicates only from ' A ' ' B ' ?
What if I had two more columns ' C ' & ' D ' but need to drop duplicates only from ' A ' & ' B ' ?
Both options works very nice :) Do you know hot to apply your method to use secondary ` y_axis ` [ example ] ( #URL ) ?
Here is an example : ` meanTime = [ np.mean ( i ) for i in np.array_split ( time , len ( time ) / 60 )]`
When I replace a row of a df , it causes an existing column of dtype=int to become float .
see this question , here : #URL This is currently a bug , best to use `` append `` to avoid the dtype change
I had seen that link and did try append , but I'm replacing a row , not adding one , so I didn't think the enlargement problem applied .
I did try append though to see if it would let me overwrite , but it oddly seems to give me duplicate index entries , e.g. I get two rows with date 2014-01-01 .
I couldn't figure out how to get append to overwrite the existing row , which is what I'm trying to do .
You are much better off of creating the new data , selecting the data you want out of the existing frame , then `` concat `` together .
Your last method could also be written as ` df.loc [ testdate ] = map ( adddata.get , df.columns )` .
@USER in python 3 ` map ` returns a map object as opposed to a list , which causes ` TypeError : object of type ' map ' has no len() `
It is common to want to append the results of predictions to the dataset used to make the predictions , but the statsmodels ` predict ` function returns ( non-indexed ) results of a potentially different length than the dataset on which predictions are based .
@USER .nouri : So in ` 0.6.0.dev ` ` len ( train ) == len ( preds )` , regardless of missing values in ` train ` ?
Predict shouldn't drop any rows .
As you can see there is a problem because the reindexing enforce the backfill value until the 0 hour of the last day of the year but not after .
pandas applying regex to replace values
I want to strip it down to just the numeric values .
I know I can loop through and apply regex #CODE
to each field then join the resulting list back together but is there a not loopy way ?
whats the best way to apply it to the column in the dataframe ?
It is like a dataframe , but requires ` apply ` to generate a new structure ( either reduced or an actual DataFrame ) .
Doing something like : ` df.groupby ( ... ) .sum() ` is syntactic sugar for using ` apply ` .
The ` freq ` attributes of a datetimeindex are inferred when required ( via ` inferred_freq `) .
You code is very confusing , you are ` grouping ` , then ` resampling ` ; ` resample ` does this for you , so you don't need the former step at all .
` resample ` is de-facto equivalent of a groupby-apply ( but has special handling for the time-domain ) .
Their are ways to apply some operations to multiple slabs of a n-dim ( esp . via new ` apply ` in 0.13.1 , see here .
On the other hand , the ` OR ` operator requires both values to be equal to ` -1 ` to drop them .
values to be equal to -1 to drop them .
Remember that you're writing the condition in terms of what you want to keep , not in terms of what you want to drop .
I basically want to replace values only in column C .
For example , replace ` [ 2 , 3 , -2 , -3 ]` with ` [ 1 , 1 , -1 , -1 ]` .
Just one more thing , if I want to drop all ` date* ` columns , how should I do it ?
In the upcoming pandas , this functionality will also be provided in ` drop ` , so this will be easier .
Eg to drop all dates , you can provide a regex expression that says to not match a certain string : ` df.filter ( regex= " ^ ( ?! date ) . *$ ")`
In the upcoming pandas ( 0.14 ) , this functionality will also be provided in ` drop ` method , so this will be easier .
Using partial with groupby and apply in Pandas
I am having trouble using partial with groupby and apply in Pandas .
There is no need to use ` functools.partial ` here , as you can provide arguments to the function inside the ` apply ` call .
If your function has as first argument the group ( so switch the order of the arguments ) , then the other arguments in ` apply ` are passed to the function and in this way you can specify the ` columnName ` in the apply : #CODE
The reason it does not work with partial , is that ` functools.wraps ` does not seem to work together with ` functools.partial ` ( ` wraps ` is used inside the apply ) .
Fetching top n records in pandas pivot , based on multiple criteria and plotting them with matplotlib
Usecase : Extending the pivot functionality of Pandas .
f [ ' len '] [ ' click '] / sum ( f [ ' len '] [ ' click ']
except that the top n logic should be embedded ( head ( n ) does not work with n depends on my data-set - I guess I need to use " apply " ?
Store the actual occurances ( len of names ) , and find the fraction of a " name " in population
( 1 ) is > f [ ' len '] [ ' click '] / sum ( f [ ' len '] [ ' click ']) and
Have you tried ( f.astype ( ' float ') / f.sum() ) .sort ([ ' len ' , ' sum '] , ascending=False ) [: 3 ]
@USER After I run " =d f1.pivot_table ( rows= ' name ' , aggfunc =[ len , np.sum ]) and then your command this gives " ValueError : Cannot sort by duplicate column len " .After I remove " len " this gives , " % str ( by ))
I have several Dataframes with the same columns that I'd like to merge on their indices only .
I could also do a merge on two dataframes and use the suffixes parameter #CODE
Once I've suffixed the first merge , the names will no longer be in common with the third dataframe .
I figured the solution would be to append a level to the column index of each dataframe with the relevant information necessary to distinguish those columns .
Ultimately , I want a result to look a lot like the last concat statement .
Is there a better way to append a level to the column index ?
I you want a MultiIndex , you can do this directly in the ` concat ` function to get the same results , like : #CODE
I am trying to pivot this data frame so that I can identify the minimum and maximum trade date for a given contract , but this is not working intuitively : #CODE
What does work , however is nesting append and concatenate #CODE
Not great , but at least it doesn't care which things are 0d : ` np.concatenate ( map ( np.atleast_1d , [ tmp , id , freqs ]))`
Better way to compare all items in a dataframe and replace similar items with fuzzy matching python
I'm wondering if there's a better way to compare all items in a dataframe column to each other and replace those items if they have a high fuzzy set matching score .
it's more centered on how I might be able to efficiently run through all items in a list ( comparing each item to each other ) and replace them .
If you have 5 words which are close to each other how do you pick which is the true name ( i.e. the thing to replace them with ? )
Also , you don't need to apply list in definition of cn1 since you're iterating through it .
basically , i choose the name that has the shortest length and i replace only if the fuzzy partial ratio is 100
My approach is as follows , the concat step appears to be the bottleneck ( I tried append too ) .
Why you need to append a new row after a data frame ?
You should be able to modify and concat once for all groups
Thanks to user1827356 , I sped it up by a factor of 100 by taking the operation out of the apply .
so you can use `' | ' .join ( words )` as the pattern ; to be safe map to ` re.escape ` as well : #CODE
`' | ' .join ( map ( re.escape , words ))` would be the search pattern : #CODE
Merge many tsv files by common key using Python pandas
I would like to perform an outer join on all of the files using the field " CHR :P OS : REF : ALT " to form one giant matrix .
I got the output above using the following code , but I am having trouble looping over the hundreds of * tsv files in the directory ( path / to / testN.vmat ) . how can I modify this into something that will merge all the individual * tsv files from a directory into a single tsv file ?
If you make the ' CHROM :P OS : REF : ALT ' the index you join multiple frames : #CODE
In someways it's more honest to think of this as a concat rather than a join : #CODE
interesting , what do you mean by " more honest to think of this as a concat " ?
@USER good question , I say because each " frame " is really just a column in the final result , rather than a more complex merge ( where several columns are coming from each frame ) ... subjective thing to say though !
The data are originally daily , but I've used the ` groupby ` functionality to aggregate average monthly prices , and get the following ( a simplified sample ): #CODE
One thought I had would be to " unstack " such that every row represented a single time period , and then iterate that way .
Calling unstack on the df above gives this : #CODE
You could do this with a concat ( and itertools permuations ): #CODE
And you can just concat this result to the original frame ...
Then using the built-in apply function in pandas .
Freq : H
And you could always drop back to numpy operations on the numpy array ` pan.values ` if need be , though , hopefully , that would be unnecessary .
And now , we use the same ordering technique to order the rows of the pivot table ( instead of the rows created by groupby ) .
You can use ` get ` ( though it only works on the primary selection axis , e.g. in a frame its the columns ) , so you need to use the transpose to access .
Try doing what I suggegted AFTER a groupby ( e.g. in the apply function itself , which only has a time-index and only has the elements for which group you need )
Access two one index of multiindex DataFrame
Create the data you need , e.g. series or whatever , and just `` concat `` together or use the above method if you * really * need separate dtypes .
The other issue is that you don't want to append it to a list , you want to just set it as ` testdict [ list_instr_objects [ i ] .name ] = list_instr_objects [ i ] .data ` ( and it doesn't have to be a defaultdict , just a dictionary )
Second , use the ` shift ` and ` where ` commands to create the percentile3 column : #CODE
You could also use ` shift ` to accomplish this within a ` groupby ` / ` apply ` : #CODE
this is pretty awesome , never thought about shift .
I did try to use groupby and apply with a vlookup and couldn't quite crack it . this works perfectly , but just for my own educational use , if there were some years skipped and a vlookup was needed , would you know what to do ?
I think the easiest would be to use the ` resample ` command to add the skipped years ( your percentile variable would be missing in those years ) , and then you could just use ` shift ` and ` where ` in exactly the same way .
The way I would have thought it works is to group ` df ` into 3 dataframes ( for each machine ) and apply ` func ` on each of those grouped dataframes .
So , in this case ` .append ` should append only the first row from ` df2 ` to ` df1 ` .
This almost seems like something that ` concat ` should do .
* Why * should it only append that row ?
Sorry , I forgot to insert a row in df2 .
` col3 ` is a category variable , so is there is new ` name ` in the category , you would append .
To put it another way : you want to concatenate the two frames and then drop duplicates ?
Concat both dataframes , then drop duplicates
Using an outer join / merge , then drop duplicates
This method is possibly more memory intensive than an outer join because at one point you are holding ` df1 ` , ` df2 ` and the result of the concatination of both ` [ df1 , df2 ]` ( ` df3 `) in memory .
Outer join then Drop
Doing an ` outer ` join will make sure you get all entries from both dataframes , but ` df3 ` will be smaller than in the case where we use ` concat ` .
To get your desired output , you'll need to transpose the axes and reset the index afterwards .
In this example , we would like to drop the first 4 rows from the data frame .
No transpose needed : #CODE
Aw , I like my transpose , it golfs better :)
Replace the zeros with nan and then drop the rows with all entries as nan .
After that replace nan with zeros .
instead , use the indexers ` loc / ix ` to reliably set values .
So , let's keep using ` name ` and ` day ` as ` multiIndex ` , which make better sense ( ` reset_index() ` to get ` int ` index , if desired ) .
I am trying to append a date to a timeseries data .
I want to append to date series with ` 20140216 ` date .
Currently I am using append command to update series .
Can somebody suggest me any good ways to append date to timeseries data ?
Appending is usually best avoided ( it's usually preferable / more efficient to use concat or join ) .
@USER suggestion is for DataFrames or Series , pd.concat or join / merges : #URL It really depends what you're doing .
You can use join .
To join ` df2 ` and ` df1 ` , we need to set the column to join on , and transpose ` df1 ` so that we have stock name as index : #CODE
You can use loc directly to select some columns from your DataFrame ( to use @USER ' s example ): #CODE
The reason it returns a DataFrame with ` NaNs ` is because the ` DataFrame() ` call with a DataFrame as input will do a ` reindex ` operation with the provided input .
If you want to do any of these row wise , you may have to transpose the DataFrame first .
I need to be able to apply calculations on multiple values from a dataframe column , rather than a single value .
pandas do a " true " concat
It seems to me that pandas is returning a data frame with two series instead of one single serie when I do the concat .
Pandas is doing a true ` concat ` here in the sense that it concatenates ` df1 ` and ` df2 ` without dropping any rows ...
Python Pandas- how to unstack a pivot table with two values with each value becoming a new column ?
When I unstack the pivot and reset the index two new columns ' level_0 ' and 0 are created .
Is it possible to unstack the frame so each value ( C , D ) appears in a separate column or do I have to split and concatenate the frame to achieve this ?
You want to ` stack ` ( and not unstack ): #CODE
Although the ` unstack ` you used did a ' stack ' operation because you had no MultiIndex in the index axis ( only in the column axis ) .
Groupby works in this example , but in the real application I have non-unique dates so I am using pivot to put the dimensions in the horizontal axis and to group the dates so each date in the index is unique so they can be resampled .
You can always give the ` level ` keyword to stack to specify which level of the columns should go to the index ( default it is the last one ) .
I want to apply my_func ( a custom created function ) to each row of a dataframe .
You need to set ` axis=1 ` in ` apply ` : #CODE
` apply ( my_funx , axis=1 , args =( par1 , par2 ))` or ` apply ( lambda row : my_func ( row , par1 , par2 ) , axis=1 )`
they are equivalent in this case ( and generally they are as well ; more validation on `` loc `` is the difference )
I have a dataframe that I want to use to calculate rolling sums relative to an event date .
I now want to align the last event dates on the final row of the dataframe and set the index to 0 with each preceding row index -1 , -2 , -3 and so on .
Using Pandas , how do I drop the last row of each group ?
How can I drop the last row of each group instead ?
In R , I have always been able to merge dataframes very easily as follows : #CODE
However , I have been unable to reconstruct this in pandas with merge ( how= " left , right , inner , outer ") ..
The intersection should contain one row with the ` gene : HES4 ` .
As far as I know the columns are labelled so that they should merge fine , I only want to merge by the ` Gene ` column and keep all test rows : #CODE
Hey Andy , would ` join ` be more appropriate in this instance ?
I searched in the help document and it seems to most neat way is to use ` transform ` : #CODE
I looked around and found ` apply ` can do the work too : #CODE
This works much faster than the ` transform ` method .
With my larger dataset ( about 90k rows ) , the ` transform ` method takes about 44 secs on my computer , ` apply ` takes ~2 secs and the ` for loop ` takes only ~1 secs .
I need to work on much larger dataset so even the time difference between the ` apply ` and ` for loop ` makes a difference to me .
I think there should be a better API to append a level to a MultiIndex ...
On the other hand , ` pd.concat ` does accept multiple dataframes , but it doesn't take the argument ` on ` to specify the columns on which to join .
Why was ` merge ` constrained to only work with two dataframes ?
Also in SQL you have to do multiple joins to join multiple tables .
Is there a faster alternative than groupby transform ?
Here is the actual transform : #CODE
But is there a way to modify " records " directly , to somehow strip the single-element arrays into the elements themselves ?
One efficient hack is to replace the levels ( of the MultiIndex ) inplace , sort , then put them back : #CODE
A potentially more robust way anyway is to get to transform , use sort passing a list to ascending : #CODE
and use this result to reindex : #CODE
If you have to do two on level=0 then your going to have to do a little more work ( something like : ` df.groupby ( level=0 , as_index=False ) .agg ( { ' D ' : ' min ' , ' E ' : ' median ' } ) .sort ([ ' D ' , ' E ']) .index `)
Also I think in your comment with ` agg ` you shouldn't use single quotes around the function names ` min ` and ` median ` .
@USER actually using quotes is a trick to use the pandas version of min / median rather than defined elsewhere .
@USER ah cripes , my second answer is totally wrong ( I should be using transform ) .
Above " apply_by_multiprocessing " can execute Pandas Dataframe apply in parallel .
periodindex not well supported at lower than day freq - just use datetime index - this usage might be a bug
Or is there a way to transform the index in code 2 back to multi-index easily ?
It only pops up when I use multiindex .
Pandas - The difference between join and merge
I want to merge them , so I try something like this : #CODE
But I'm trying to use the join method , which I've been lead to believe is pretty similar .
I always use ` join ` on indices : #CODE
First , ` join ` expects a single , common column .
Specifying an outer join is even more confusing : #CODE
My thoughts are that ` merge ` is for columns and ` join ` is for indices .
I was using ` df.to_sql ( con=con_mysql , name= ' testdata ' , if_exists= ' replace ' , flavor= ' mysql ')` to export a data frame into mysql .
So when I save csv from STATA , some characters were cut in half , the encoding of which cannot be recognized by python .
@USER , maybe this is too late already , but Stata prints out the script version of any manual editing you do ( something like ` replace var2 = 28 in 4 `) .
If you have the log of your manual edit saved somewhere , it shouldn't be difficult to automatically convert ` replace var2 = 28 in 4 ` and the like into pandas code - just parse this string for the arguments to your custom pandas replacement function .
If your script is just ` replace ` done hundreds of times , then it shouldn't be so difficult to do , since you just need to parse a couple of arguments ( I had to do something similar a while ago ) .
using ` transform ` and ` drop_duplicates ` : #CODE
Pandas boxplot order not correctly sorted
That the boxplot is sorted alphabetically by ' Character ' , but that the ' Numbers ' are not always sorted .
Do I need to sort the dataframe before doing boxplot or can I do something when calling the boxplot _ _ ?
You can apply ` strip ` to each element in a column this way : #CODE
Like the idea , but I'm missing the ` inplace=True ` like the ` replace ` method has ( #URL ) - otherwise it will just return a Series and not a DataFrame
The problem is that the Series don't do boxplot - as I recall it .
You can use ` shift ` to nudge your data up or down a row .
If you really prefer ` 1 `' s and ` 0 `' s replace the last line with : #CODE
You must ` apply ` , ` transform ` , or ` filter ` to get back to a DataFrame or Series .
I'm very new to projections and struggling to convert a map dataset from OSGB36 ( Eastings and Northings ) to WGS84 ( lat / longs ) .
Now I want to export it as GeoJSON so I can then use Folium to map it on leaflet.js .
Please forgive me for not expanding on your answer , but I'm not too familiar with pandas , so I opted to use the csv module .
mean = sum ([ float ( row [ 1 ]) for row in data ]) / len ( data )
I was running into the same kind of issue and found that the resample method can be used to do that just using the parameter 3M ( for 3 months ) .
If you have a dataframe with index as pandas datetime object then all you need to do is ask to resample on 3 months basis .
Why am I getting an empty row in my dataframe after using pandas apply ?
The problem I am having is that I am getting an blank row at the top of all the dataframes I'm getting back from Pandas ' apply function and I'm not sure why .
The groupby / apply operation returns is a new DataFrame , with a named index .
Ah , okay , so the reason it is there is that the groupby / apply operation _replaces_ the usual indexing with indexing by the grouping value ?
Yes , the result of the groupby / apply is a new DataFrame , with a named index , and the name corresponds to the column name by which the original DataFrame was grouped .
You can access a dtype of a column with ` agg [ y ] .dtype ` : #CODE
As @USER Hayden points out that ` normalize ` sets the times to 0 so that you can compare directly to a Timestamp with 0 for the time .
I was reading about normalize but couldnt see how to use it in this instance .
So I have to be more efficient as I can because after , I have to stach columns and do an apply to do a relativedelta on the datetime ( which is take actually 2 min more ) ...
I have done one for this typical problem { too long dataframe apply row functions} .
I think the cleanest way is to check all columns against the first column using eq : #CODE
too long dataframe apply row functions
I do a read_csv , then a stack and the following function but it's more time and memory consumming .
The command you're looking for is ` stack ` .
Basically , you're looking at using stack and then converting ` Hxx ` to ` #URL which should be straightforward .
When I append a record containing the long userID , pytables raises an error because it is expecting a 13 characters field .
For each column , I would like to replace the date and time by the time since the last post happened .
How to join two dataframes on datetime index autofill non matched rows with nan
As you don't have a common column you need to specify to use the indices of both dataframes and that you want to perform an ' outer ' merge : #CODE
Pandas merge not working
However , when I try to merge the two using : #CODE
Can you show part of the dataframe whith rows that should merge ?
If not then the default type of merge is ' inner ' and so this could explain why you have no rows , if you want to combine all data from both then do ` result = pandas.merge ( step1_merge , transp_merged , on =[ u'type_str ' , ' GRID '] , how= ' outer ')`
There are several points that do not join / merge .
I'm trying to transform this DataFrame into something like this : #CODE
TBH I think this is better , I know which one I'd prefer to read ( stack / unstack are indecipherable )
Which levels to stack / unstack is not very readable ...
I've made it work with ` records.groupby ( ' product_name ') .filter ( lambda x : len ( x [ ' url ']) == 1 )` .
Note that simply using ` len ( x )` doesn't work .
Pandas Left Outer Join results in table larger than left table
From what I understand about a left outer join , the resulting table should never have more rows than the left table ...
Therefore I merge them as such : #CODE
To avoid this behaviour drop the duplicates in df2 : #CODE
@USER yup , drop the duplicates , edited answer to reflect that .
Rather than do a apply here , I would probably check each column for whether it's numeric with a simple list comprehension and separate these paths and then concat them back .
Now you can concat these and potentially reindex : #CODE
I have tried " where " , " isin " , " join " , " merge " , and I am not able to replicate this in Pandas .
I then want to do a self join with a specific criteria shown below .
I can't think any simple method that can do the join less than O ( N**2 ) .
You can use the diff groupby method directly : #CODE
which does the same thing as using shift and subtract : #CODE
I prefer using ` apply ` as suggested by Karl .
@USER true , it's not very pythonic ... you can always do the fillna on the result on the next line . shame shift doesn't have a default value arg .
@USER remembered the diff method does this , much nicer !
Or you could use shift within a ` groupby ` / ` apply ` : #CODE
I like the ` apply ` solution !
concateneted with original series and rolling max values : #CODE
Merge existing dataframe into fixed size new dataframe
Then I want merge these kinds of table into new dataframe
How could I merge them in that way ?
using zip() on the shortest amount of data , I get the following error ''' % ( len ( value ) , len ( cur_axis )))
Is there a way to combine ( append ) h5 files with the same table format more efficiently ?
( SQL Union like functionality ) .I tried this SO but couldn't get it to append tables .
i am thinking about writing a map / combine function that will look in all the parts of a table for select from where queries .
This brings me to the second part of my questions , what if i don't merge the files and use the pre-merge files to be processed in map / combine way , could that be a reasonable way to approach this ?
I do a very similar , split-process-combine method , using multiple processes to create intermediate files , then use a single process to merge the resulting files .
I believe that ` PyTables ` incrementally updates the index , which in this case is completely unecessary ( as you are going to merge them afterwards ) .
I merge the files in the indexed order , basically by reading a subset of the pre-merge files into memory ( a constant number to use only a constant amount of memory ) , then append them one-by-one to the final file .
Pandas Groupby apply function to count values greater than zero
Pandas Groupby apply function to count values greater than zero
I am using groupby and agg in the following manner : #CODE
you could complete the dict by replace missing values to some invalid value like -1
You could simply take the transpose ( ` .T `) to get your desired result .
Merge specific column into dataframe
If I replace values of two rows using ` .loc ` indexer in the following way : #CODE
However , if I replace values of all rows in the following way : #CODE
Pandas groupby columns without multiindex
But that simply returns a multiindex , with all rows ...
Getting a error numpy.ndarray ' object has no attribute ' append ' in python
' numpy.ndarray ' object has no attribute ' append ' when i run the code of groupby in python The command is
Rather than 2 you can insert first the Series ( in this example it happens to be the same ): #CODE
Brilliant , this might sound really stupid but I never knew about the lt function ( or the corresponding gt , le , ge , etc ) .
Here's the full stack : #CODE
You might want consider using apply : #CODE
create a column ` tsdiff ` that has the diffs between consecutive times ( using ` shift `)
if the diff from previous is 0 BUT the total diff ( from first ) is > 50 make in group 2 .
Groupem ( can also use an apply here ) #CODE
@USER the resample worth w / o the sort , not the Grouper ....
Sorry , but in fact it still not working : I used pivot table instead of this method but presently I still need to resolve the above issue.Because there is now an additional column with dates all at 1970-01-01 .
How do I convert a pandas pivot table to a dataframe
I want to use a pivot table to summarise a dataset and then be able to access the information in the pivot table as if it were a DataFrame .
Now I want to summarise using a pivot table , simply counting the number of patients in each hospital : #CODE
First up , this isn't a pivot table job , it's a ` groupby ` job .
Pivot tables are for re-shaping your data when you haven't set an index ( see this doc article ) , ` stack ` and ` unstack ` are for reshaping when you have set an index , and ` groupby ` is for aggregating ( which is what this is ) and split-apply-combine operations .
To select certain rows in a multi-index , I usually use ` ix ` as follows : #CODE
But I don't quite understand the difference between a pivot table and group by .
I need to collect some simple statistics for each incident and store them in order to collect some aggregate statistics .
If you exceed this , the query will work , but it will drop that variable and do a reindex
replace ` timestamp ` with ` date ` .
unstack multiindex dataframe to flat data frame in pandas
it returns a map which causes a " TypeError : object of type ' map ' has no len() " .
Solved it by changing to " return list ( map ( ' \n ' .join , np.fliplr ( Blank_ar )))" 3 .
Average value for each user from pivot table ( dataframe )
You probably want to use the resample method
you can convert your timestamp column to datetime type before using the pivot table .
Thanks @USER . another quick question is how do i resample between two given timestamps to arrive at daily rainfall.For example I would like to calculate daily rainfall by summing all not NaN values between 7:30 given day to 7:30 next day.NOTE there could be more than 24 values in the timeseries as the frequency is not hourly .
time and memory lambda apply
Merge DataFrames with NaN
Then I tried to concat the two dataset first : #CODE
Is there anything that can confuse the use of loc ?
see the docs #URL You need to specify how you want the resample done eg sum or mean .. you can specify by field ...
Aggregate Dataframe into weighted average series
I want to aggregate the dataframe as follows :
How to get all fields for only a specic user_id from a pivot dataframe indexed by two fields ' timestamps ' and ' user_id ' ?
I want to find and graph a rolling correlation of the A and B values with time in , say , a 50 minute window .
Do you mean after the fact , or do you want / need to truncate them before they're even stored in the frame ?
Would an apply be the fastest way for after the fact ?
Writing an apply right now .
Then resample : #CODE
My question is : How do I transform the original dataframe to get the desired serialization ?
I am using pandas to join several huge csv files using HDFStore .
Right now I create a new table in the HDFStore for the output of each merge , which I call ` temp ` .
Finally , I copy ` temp ` to ` base ` and start the process over again on the next table I need to join .
Luke , I'm curious why you wouldn't just append additional csv's directly to the base table , rather than have the intermediate ( slow ) step of creating a new table ?
Merging two DataFrames using an aggregate on a column on one of the DataFrames
Does the 2nd line merge A1 into B , or do I need to say ` B = pd.merge ( B , A1 , on= ' key1 ')`
@USER See the docs on [ merge ] ( #URL ) .
Also , setting ` how= ' left '` in merge will cause an SQL-style left-join ; i.e. , the values in ` B ` will persist whether they exist in ` A ` or not .
I have data in long format and am trying to reshape to wide , but there doesn't seem to be a straightforward way to do this using melt / stack / unstack : #CODE
hmm could be a pivot then
A simple pivot might be sufficient for your needs but this is what I did to reproduce your desired output : #CODE
Yeah , that's basically what I ended up doing , although you also have to separate out the columns that don't change , like height , drop duplicates and then concat those later .
In Pandas , I can use ` df.dropna() ` to drop any ` NaN ` entries .
Is there anything similar in Pandas to drop non-finite ( e.g. ` Inf `) entries ?
You can use ` .dropna() ` after a ` DF [ DF == np.inf ]= np.nan ` , ( unless you still want to keep the ` NAN ` s and only drop the ` inf ` s )
Just apply ` to_datetime ` : #CODE
you can pass the format directly into the to_datetime , no need for apply
How to get sum of same field for ALL user_id from a pivot dataframe indexed by two fields ' timestamps ' and ' user_id ' ?
i think the cut function is what I am looking for
You can simply ` map ` the list , like so : #CODE
or if you're using pandas ( as your question suggests ) use ` astype ` method or ` map ` : #CODE
Cannot interpolate on Pandas Dataframes
I am trying to interpolate missing values on my Pandas Dataframe : #CODE
If you still want to use interpolate without upgrading pandas use can do this : #CODE
is_retweet 506 non-null bool
is_truncated 506 non-null bool
dtypes : bool ( 2 ) , float64 ( 2 ) , int64 ( 11 ) , object ( 16 )`
I have ` z = pa.read_json ( ' C :\ Users\Admin\JSON files\ file1.JSON ')` as before , but I wish to use sample of 100 / 200 of the retweet_count so I can find mean / max / etc of diff size samples .
Dataframe exportation to excel ( shift of rows )
I want to cut the first down to only the matches , so #CODE
You can just ( inner ) merge : #CODE
Or if you really want to add a column to your original dataframe , you can use ` transform ` : #CODE
If I use one of the rolling_* functions , for instance rolling_sum , I can get the behavior I want for backward looking rolling calculations : #CODE
Could you resample first ( to get consistent freq ) and then rolling_ in reverse ?
I think that should work , but only if you resample in the frequency that you want to aggregate on .
I look for drop / add the hour which correspond to a daylight .
I process as follow but I obtain only shift on datetime since also want a shift for values .
You can drop the timezone and subtract the first index .
There is no shift !
A temporary solution is to change the above described function in nanops.py , and replace x=float ( x ) to x= x.astype ( np.complex ) , so the part of the code now is : #CODE
Presumably you want to real part of the complex numbers ( numpy.real ) , unfortunately this doesn't play super nicely with pandas Series / DataFrame , so you need to apply back the indexes : #CODE
It sounds like you could be after a resample : #CODE
Pandas resample with " seasonal " frequency
Using pandas , I am trying to resample daily data into seasons and depending on the start of the daily index , I seem to get different results .
Basically if the start is in Q2 or Q4 , resample works as expected , but not if index starts in Q1 or Q3 .
Note that the same does not happen with the end date as resample seems to behave correctly there .
I want to group my DataFrame by row names so that I can then perform aggregate operations by group ( e.g. , count , mean ) .
However , since I presently have some issue with this version ( no possibility to really calculate the mean using resample with DataFrame ) I would like to know how I can quickly upgrade my version to 0.14 .
I'm trying to aggregate a Pandas DataFrame by date , and then index into the result by date , but I can't seem to index it at all .
a more usual way to do this would be to resample : #CODE
With regards to accessing the rows , you should use loc , though I like to access with a Timestamp : #CODE
Pandas resample with dataframe returns just two fields instead of four
You can do a resample of a groupby , provided you have a DatetimeIndex : #CODE
@USER yeah , that could definitely break the resample ( which uses mean under the hood ) , you'll notice my blank values are NaN , I strongly urge you to use those ( float dtype rather than object ) . see : #URL
@USER oh , I see what you mean ... what do you expect the output to be with a resample of strings ( e.g. what is the mean of a list of strings ) ?
pandas can't resample non-numerics with a numeric agg like mean , so you have to pass it a different agg functions ( which can handle strings ) .
If i want to convert column B to int values and drop values that can't be converted I have to do : #CODE
I want to cut the first down to only the matches , so #CODE
if you want to remove matches , just drop ` index-1 ` from ` df1 ` and ` index-2 ` from ` df2 ` .
I have it printing after the merge and it's just df1 .
Would the merge be something like ` pd.merge ( df1 , df2 , on =[ ' Qty ' , ' Price '] , how= ' inner ' , suffixes =( ' -1 ' , ' -2 '))` ?
I suspect the answer lies in a better comprehension of groupby and the apply method , but I'm having trouble getting this off the ground .
This ' normalized ' time field we can map to the 5-minute interval : #CODE
[ Edit ] Update - I think you can use the apply function to subtract the right min time #CODE
So , I can't just do step 1 across all my ' epoch ' data - I need to apply it separately for each ' observation ' group .
So I think the plan of action will be to use the sort of approach you've provided , but apply it via groupby ?
Based on the dataframe you created you can get the right min value and apply to the epoch column
A few caveats apply :
Sorry it still works fine for me , you can drop the last row by just doing ` df1_clean = df1_clean [: -1 ]`
No that still remains but your df1_clean prior to the pivot does not have the erroneous date you mentioned
You can use shift to align the previous row with current row and then you can do your operation .
In ` Pandas ` we can drop duplicates by using ` dataframe.drop_duplicates() ` which keeps the first row of the duplicate data by default .
How can we keep any random row and drop the duplicate rows using pandas ` drop_duplicate ` ?
If you want to drop duplicates based on specific columns , you can use the ` cols ` argument in ` drop_duplicates ` : #CODE
Edit : I tried assigning using both loc and iloc but it doesn't seem to work :
loc : #CODE
I like to group a DataFrame according to their date and get the mean of each group then merge them into a single DataFrame .
sub is somekind of format of DataFrame , but how can I merge then together to a single DataFrame df2 ?
I have two dataframes that i'm trying to merge on key ' Time ' .
I'm trying to merge both these table to get the count next to events table using the below pandas merge function however it throwing a unhashable instance error - can someone advise ?
If this was a 1:1 merge ( let's say ) , then the merge would only ever happen if the combination of ' Time ' and ' Event ' in the Events dataframe , exactly matches the combination of ' Time ' and ' count ' in the other data frame .
It looks to me like you are just wanting to merge on ' Time ' in both sets .
I'm prettys sure you don't want to merge on those variables , you just want to merge on Time .
Tried to just merge on Time but getting the same error :
or provide eg a function to replace all periods with underscores : #CODE
this will append your csv name to your destination path correctly
You'll usually see the SettingWithCopy warning if you use consecutive [ ] in your code , and the are best combined into one [ ] e.g. using loc : #CODE
Aside : as mentioned in comments resample offers ` how= ' ohlc '` , so you may be best of doing this , padding , filling and then joining with the resampled volumes .
If you're alright with linearly averaging entries and exits , you can take each time period and calculate how many entries and exits occur per hour on average , then , given an hour , you could iterate through all data points , find how much a data point overlaps with that hour ( i.e. 15 minutes or the whole hour ) , and apply the data point's average entries / exits per hour modified by the percentage of overlap to an accumulator .
ix [ ... ]
I am doing a transformation on a variable from a pandas dataframe and then I would like to replace the column with my new values .
The len ( array ) says it is 2 but when I call the stats.boxcox it says it is 50000 .
Print out ` len ( df )` and ` len ( stats.boxcox ( df.variable ))` .
I have 2 data frames created by pivot tables #CODE
you want to reindex your first Dataframe before you call update
Python pandas time series w hierarchical indexing and rolling shifting
Struggling with pandas ' rolling and shifting concept .
There are many good suggestions including in this forum but I failed miserably to apply these to my scenario .
Got 2 months data to process it probably won't finish after I come back from a sabbatical , not mentioning risk of electricity cut off after which I'd have to start over again this time no sabbatical while waiting .
I did a long research seems like the solution would be a combination of rolling and shifting but I can't figure out how to put it together .
Thanks to Jeff's tips , after swapping and sorting ix level I am able to get the 12sma right with rolling_mean() and with a effort managed to insert the first 12ema value copied from 12sma at the same timestamp : #CODE
I tried apply() with a function but shift gave an error : #CODE
If you manage to get the data in ine single Dataframe it should be possible using ' apply ' .
and then you can group by the ' i ' column and apply an arbitrary function to the subgroup .
from which I want to retrieve all the users ( userXY ) from one of the experiments ( exp0Z ) and append them into a single big DataFrame .
I can retrieve a single user by calling ` store.get ( ' exp03 / user01 ')` , so I guess it is possible to iterate the ` store.keys() ` and append manually the retrieved dataframes , but I wonder if it is possible to do so in a single call to ` store.get() ` or other similar method .
Group pandas multiindex dataframe by all the indices
A faster way would be to create a mask for all your categories , assuming you have a smallish number : #CODE
I'd like to ( perhaps visually ) check what these duplicated rows are like and then decide which one to drop .
or ` apply ` : #CODE
groupby.aggregate and modification then cast / reindex
I firstly do a groupby and aggregate .
Then I reindex and take NaN .
You're looking for ` resample ` followed by ` fillna ` with ` method= ' bfill '` : #CODE
This is due by the resample method which do 01 / 01 / 2015 00:00 : 00 , 01 / 02 / 2015 00:00 : 00 , ...
You may have to actually read the documentation and play around with some of the arguments to ` resample ` .
In essence I want to insert new rows in between existing rows based on a calculation that uses values from the 2 rows straddling the new row .
In my example you can see we insert a row which is the mid point value of the row both before and after .
You could use a merge like this : #CODE
You can replace ` dict ` with ` OrderedDict ` if you want to preserve the column orders .
Then , I append a row of missing values .
Finally , I can insert values into this DataFrame one cell at a time .
This approach works perfectly fine , with the exception that the append statement inserts an additional column to my DataFrame .
The append is trying to append a column to your dataframe .
The column it is trying to append is not named and has two None / Nan elements in it which pandas will name ( by default ) as column named 0 .
In order to do this successfully , the column names coming into the append for the data frame must be consistent with the current data frame column names or else new columns will be created ( by default ) #CODE
If I want to drop duplicated index in a dataframe the following doesn't work for obvious reasons : #CODE
If I want to drop an index I have to do : #CODE
This would drop all duplicates though ?
In my real scenario , ` mul ` also has a multi-index and I always get ` Join on level between two MultiIndex objects us ambiguous `
I've updated the question to reflect the fact that it's really about MultiIndexed Series on both sides of the join .
Simply merge #CODE
when I try to perform the shift for ` departure hour ` I get " cannot shift with no offset " .
When you concat ,
do ` concat ( list_of_frames , ignore_index=True )` in this case because I'll bet that the read in dfs's have an index starting with 0 , so you want a unique consecutive index .
Set the index and shift #CODE
Also , the set index and shift part confuses me a bit : I actually want the ` departure time ` column to be the previous row's index timestamp , so I'm doing a ` .shift ( 1 )` to get that but it blows up when I set timestamp as an index as part of the parse .
I think you can ` unstack ` it first , generate the ` T_3 ` and ` stack ` it back again .
I need to pass by an unstack .
For my PC , even the number of observation is unchanged , I have a lot of memory trouble when I use stack / unstack and concat functions .
when I try add new column diff with to find the difference between two date using #CODE
I get the diff column in days if more than 24 hours .
I could just loop through the first level of the MultiIndex and concat each column , but I feel that I should be able to do this without a loop .
Since ` indicator ` is really just a boolean indexer , drop its ` NaN ` s and convert it to bool dtype #CODE
Interesting use of melt / merge to get it in a form where you can pivot .
Pandas dataframe transpose , to_csv
In the code below , in line 4 I can transpose the dataframe , but in line 5 , when I use to_csv , the new CSV file is created , it remains the original version and not the transposed one .
I have a MultiIndex pandas DataFrame in which I want to apply a function to one of its columns and assign the result to that same column .
I managed to apply the function slicing the dataframe with ` .loc ` as the warning recommended : #CODE
pandas pytables append : performance and increase in file size
I would like to merge these files into a big store , using pandas ` append ` as in the code below .
The append operation is very slow ( it is taking up to 10 minutes to append a single store to ` merged_store `) , and strangely the file size of ` merged_store ` seems to be increasing by 1Gb for each appended store .
You might try tweaking the ` chunksize ` parameter to ` append ` ( this is the writing chunksize ) to a larger number as well .
However , I still get very large file sizes : for every 300Mb table that I append to the merged store , I get an increase in size of 1Gb which eventually is going to fill in my disk .
This should not be due to compression , as I did not apply compression to the 300Mb files .
When you concat them , the strings will all be of the resulting table string-sizes ( which could be bigger depending on what the minimum is )
I have parsed the data so that the size of long strings in every table is restricted to fixed values ( to avoid errors in the ` append ` operation ) .
Because all the tables were generated with the same ` min_itemsize ` parameters ( and contained strings of fixed maximum length ) , I assumed that the fields of the merged table would automatically be created with the right size without explicitly passing the ` min_itemsize ` parameter to the append function .
It * might * pay to keep that in a separate table ( with an index key ) , then effectively join it when selecting it .
( One approach would be to reset the index , step through to find the values and replace NaNs with values , pass in the list of days , then set the index to the column . But it seems like there should be a simpler approach . )
For example , ` df.loc [ mask ] = df.loc [ mask ] .fillna ( method= ' ffill ' , axis=1 )` .
I want the columns to be grouped by month in a pivot table .
When I pivot the data a column for each day is being created .
any reason not to use a pivot table within Excel ?
with ` colors ` being an list of size ` len ( df )` containing colors
` test ` is the result of a groupby / agg operation .
Since the ` groupby ` operation is grouping by ` office ` values , the unique ` office ` values becomes the * index * of the DataFrame returned by the ` agg ` operation .
( I'm using mysql ( mariadb if that makes a diff .. )
Insert data into Pandas DataFrame without index or column overhead ( so not concat or append )
How can I just append this structure to the top of the DataFrame without building a temporary structure that has the same column / index naming .
The overhead of creating your small DataFrame to insert is probably small compared ot the overhead of creating the new large DataFrame to hold the result .
The data is never large enough that inserting rows and generating a new object would matter , but it makes the code vastly harder to read if you have to lace in all these calls to helper functions that try to understand the hierarchical indices and make the insertable data into a little DataFrame with matching indices / columns so that ` concat ` can work .
@USER is exactly right ; much more efficient to append even small frames ( or big ones too ) to a list then concat all in 1 go . inserting is simply not efficient row-wise in a column based structure . that said what IS efficient is appending to a HDFStore which IS row based
Very much what ` concat ` must already do , except not forcing the user to pre-arrange for the indices to all make sense with each other other than positionally .
unstack to columns via panels or set_index not working in pandas
Ive tried a bunch of approaches , such as panel , or sort , set_index , unstack , reindex .
If I try to set_index , unstack ...
If I set_index of item then unstack , ` AttributeError : ' Int64Index ' object has no attribute ' levels '` .
You can't unstack a single level index ...
@USER , I set_index then unstack .
However , I got pivot ( time , item , value ) to work ; although pivot is just a set_index and unstack under-the-hood .
I tried to apply these methods as reduction function , but couldn't make it work .
So it will be up to the user to first drop a level or flatten the multi-index .
Pandas : join ' on ' failing
... which I want to join using ` ID ` , with a result that looks like this : #CODE
You can just do a merge to achieve what you want : #CODE
The problem with ` join ` is that by default it performs a left index join , because your dataframes do not have a common index values that match then your comment column ends up being empty
Following on from the comments , if you want to retain all values in ` df1 ` and add just the comments that are not empty and have ID's that exist in ` df1 ` then you can perform a ` left ` merge : #CODE
This will drop any rows with empty comments , use the ID column to merge both ` df1 ` and ` df2 ` to but perform a ` left ` merge so retains all values on left hand side but will merge comments that match ` ID ` column , the default is ` inner ` which retains ` ID ` s that are in both left and right dfs .
Further information on ` merge ` and further examples .
You can drop them after or during the merge : ` df1.merge ( df2.dropna ( subset =[ ' Comment ']) , on= ' ID ')` If this is what you want then I will update my answer
@USER . merge by default will perform an ` inner ` merge , this means that it will give a result where the ID is in both left and right dataframes , if you want to retain all values on the left then perform a ' left ' merge , if you want a combination of both sides then perform an ' outer ' merge .
' Left ' merge is the way to go , thanks a lot !
Third question : I think that the code is quite pythonic , but I am not proud of that because of the last list comprehension which is running over the series of the dataframe : using the method apply would look better to my eyes ( but I'm not sure how to do it ) .
... which is exactly what happens here . map would be better ? ...
I'm not sure there is a better method than using apply in this case
perhaps ` reindex ` creates a new dataframe , ` ix ` returns a view
@USER you are , of course , absolutely right . what do ` loc ` and ` iloc ` do ?
The reason for the seeming redundancy is that , while using ` ix ` is syntacticly limiting ( you can only pass a single argument to ` __getitem__ `) , ` reindex ` is a method , which supports taking various optional parameters .
This is true for `` getting `` , but you cannot set with `` reindex ``
I am getting different results when using ` reindex ` with ` inplace=True ` vs using ` ix ` ( I updated the OP )
I would think that I'd have enough memory to work with the data in pandas and yet I run out of memory when I do something as simple as trying to drop a column in place .
in-place operations don't necessarily save you memory , as they are usually implemented as a copy , modify , and replace internally , FYI .
yes going to suggest that ; use a hash ( or really just a string to number map ); would be vastly more efficient in memory space
Unfortunately it isn't currently possible to apply Excel formatting when writing data with Pandas ` to_excel() ` .
You can apply column formatting when using XlsxWriter as the Excel writer engine .
Shift ragged rows of Pandas DataFrame to clean data with partial string search
I abandoned trying to shift everything to match up correctly .
the .iloc value was an attempt to merge the results together .
The following can be done in one pass ( using loc as iloc doesn't allow boolean masking ): #CODE
To get the end result , you could either concat these as a Series ( to avoid aligning the columns ) , or change the name of the columns to be more descriptive before concating : #CODE
If I understand you , you can combine ` transform ` ( which is the " propagate down " part , although I always think of it as broadcasting up ) with ` idxmax ` : #CODE
I can't believe it was as simple as using ` transform ` instead of ` lambda ` !
Want one that is not using ` transform ` ?
Either boolean mask : #CODE
This works extremely well unless len ( ip_list ) > 5000 .
Have I simply gone too far into " Big Data " where traditional methods like the above aren't going to cut it ?
I think your problem is that a matrix representation is not going to cut it :
I am looking for a way to search through the whole record and replace " .0 " with nothing ( "") .
Use a function and apply to whole column : #CODE
' cat1 ' ' cat2 ' are of the type `' bool '`
pandas apply filter for boolean type
I feel like talking the transpose twice is redundant , and that I am missing a simpler solution .
I think there may be a bug in the tz handling here , it's certainly possible that this should be converted by default ( I was surprised that it wasn't , I suspect it's because it's just a list ) .
This way you can use resample : #CODE
Tbh , most of the time you want datetime to be the index that way you can resample / groupby using TimeGrouper ( ' M ') .
@USER it may be better to ask a new question ( site helps that way ) , feel free to drop the url here if you do !
I noticed that you say " append " in your description , but I don't see it used in your code .
If you're expecting to append something but are using extend ( which basically concatenates iterables to the list whose extend method you are calling ) , that could be your issue .
Sorry , that should be append .
but your problem is not that really , its the mixed dtypes in a column . read in by chunks then either append to a list and concat , or append as you go to a `` table `` store . mixed types in a column are really bad
Then I pivot and plot like so : #CODE
I'd like to replace values in a ` Pandas ` ` DataFrame ` larger than an arbitrary number ( 100 in this case ) with ` NaN ` ( as values this large are indicative of a failed experiment ) .
Previously I've used this to replace unwanted values : #CODE
Is there a better way to replace values ?
As suggested in the error message , you should use loc to do this : #CODE
I was getting this warning while trying to reset the contents of an entire DataFrame but couldn't resolve it using ` loc ` or ` iloc ` : #CODE
I used the same to replace a single column , for df.loc I got this warning too .
You probably want to be use pandas Timestamp for your index instead of datetime to use ' freq ' .
Now you can do timeseries / frame operations , like resample or TimeGrouper .
This can be done fairly easily with ` map ` .
On IPython notebook when you display the dataframe it outputs it left-aligned which would appear correct , however when you output the first column , you can see that it doesn't align , if you remove the western strings , so keep just chinese characters , then it aligns correctly , this looks like a problem with either mixing character sets ( although I would expect it to treat everything as unicode ) or a character width issue with mixed character sets
One can loop over these elements and apply any sort of modifications .
However I get ` AttributeError : ' file ' object has no attribute ' replace '` on ` contents = contents.replace ( ' \n\n ' , ' \n ')`
I get an error with this code , " TypeError : object of type ' map ' has no len() "
Sorry , didn't notice you are using ` python 3.x ` , change it to ` list ( map ( ...... ))` should do it , in ` python 3 ` ` map() ` returns a ` map object ` rather than a ` list ` .
You can apply a multiindex to a dataframe with #CODE
You can access row in a dataframe with a multiindex with the ` ix ` method as follows : #CODE
Another options is to use loc here ( rather than ix ) , but much of a much-ness !
As far as I do not want to drop original columns
` lambda x : len ( x )` can be simplified to ` len ` ..
Once you tested ` flatten ` on one OrderedDict , it is straight-forward to apply it to a list of OrderedDict
I need to transform the timestamp column into floats ( say in seconds ) .
to get a dataframe df and then transform it into a numpy array #CODE
to transform the timestamps into seconds and unfortunately this turns out to be veryyyy slow .
How to resample large dataframe with different functions , using a key ?
I need help figuring out how to feed that list into the how= function of resample .
There might be a better way , but I first append a dummy column and calculate the ` freq ` based on the column , like : #CODE
Instead of adding a dummy column and summing it , you could simply choose one and use ` len ` ( in recent ` pandas ` , anyway ): ` df.groupby ([ " Year " , " Region "]) [ " Year "] .transform ( len )`
How to customize axes in 3D hist python / matplotlib
Let's = data= pandas.DataFrame ( { ' A ' : np.random.rand ( 100 ) *1000 , ' B ' : np.random.rand ( 100 ) *10 , ' freq ' : np.random.rand ( 100 ) *2220 } ) . should I change ax.set_xticks or w_yaxis.set_ticklabels , it's confusing a little bit
` { ' A ' : [ 1 , 2 ] , ' B ' : [ 2003 , 2008 ] , ' freq ' : [ 2 , 3 ] } `
How do I get rid of the year value and just replace the list in each cell in the dataframe with just the float datapoint ?
How would I apply the method you suggested on across every column ?
Inconsistent behavior of apply with operator.itemgetter v.s. applymap operator.itemgetter
` apply ` gives wrong result #CODE
apply is being passed an entire row which is a series of 2 elements which are lists ; the last list is returned and coerced to a series . embedded lists as elements are not a good idea in general .
apply custom function / method to the groups ( sort within group on col ' A ' , filter elements ) .
If you want to select one row per group , you could use ` groupby / agg `
to return index values and select the rows using ` loc ` .
If you wish to select many rows per group , you could use ` groupby / apply ` with a function that returns sub-DataFrames for
` apply ` will then try to merge these sub-DataFrames for you .
Another way is to use ` groupby / apply ` to return a Series of index values .
Again ` apply ` will try to join the Series into one Series .
1 pivot table , major axis being a serial number
I would like to divide each column of my pivot table by the value in the Serie using index to match the lines .
You say " using the same index " , but they're not the same : ` pt ` has a multiindex , and ` serie ` only an index : #CODE
And you haven't told the division that you want to align on the ` A ` part of the index .
If it makes it easier , I can change the line : ` df [ ' c '] = [ df_c1.T , df_c2.T ]` to no longer include the transpose : ` df [ ' c '] = [ df_c1 , df_c2 ]` , but the source data has to be JSON in the format shown .
My current solution ( more or less ) is to iterate each element in the original column `' c '` , and then do a join with its parent row while slicing the columns I want to keep .
I append this data frame to a list and then do a final ` pd.concat ` on the list of all the dataframes .
I apply pd.io.json.read_json() to the column but that leaves me with a column of DataFrames that I need to expand out .
Loop over them and see which ones are different , e.g. ` print ([ p for p in parts if len ( p ) ! = 4 ])` .
Overlaying actual data on a boxplot from a pandas dataframe
` Seaborn ` boxplots seem to essentially read the dataframes the same way as the ` pandas ` ` boxplot ` functionality ( so I hope the solution is the same for both -- but I can just use the ` dataframe.boxplot ` function as well ) .
My dataframe has 12 columns and the following code generates a single plot with one boxplot for each column ( just like the ` dataframe.boxplot() ` function would ) .
Can anyone suggest a simple way of overlaying all the values ( by columns ) while making a boxplot from dataframes ?
A general solution for the boxplot for the entire dataframe , which should work for both ` seaborn ` and ` pandas ` as their are all ` matplotlib ` based under the hood , I will use ` pandas ` plot as the example , assuming ` import matplotlib.pyplot as plt ` already in place .
To overlay stuff on ` boxplot ` , we need to first guess where each boxes are plotted at among ` xaxis ` .
By that I meant a sort of scatter plot overlayed on the boxplot - i.e. with dots in place of the numeric values .
I was able to find the xticks and xtick_labels which the boxplot set .
Another approach is to use ` stack ` : #CODE
which works because ` stack ` here returns the flattened version : #CODE
The standard way seems to use ` groupby ` and ` transform ` .
The problem is , ` transform ` works really slow .
The problem lies with ` transform ` .
I think the problem lies with ` transform ` , not the function it calls .
Are you trying to perform multiple aggregate functions on your groupby ?
@USER , no I am trying to generate a column that contains the statistics , which is exactly what ` transform ` in the documentation does : #URL except I am not happy with the speed .
I tried using date_ranges and the ix function .
If you only want to extract a 3 / 6 month chunk of data from your data frame why not truncate ?
But when I apply the conditions to obtain the final dataframe : #CODE
Does the len match the second condition if you do this : ` a [ ~ (( a.cond1 == ' 120.A ') & ( a.cond2 == 2012 ))]` ?
So if we use a slightly different method of ` MultiIndex ` slicing , the return is a ` DataFrame ` #CODE
I am trying to query data from a postgresql db and insert it into an sqlite db .
Because in version 0.13 ( or below ) there is a bug in the ` if_exists= ' replace '` implementation , which is fixed in 0.13.1 ( latest stable release at the moment )
I have multiple file in a folder , and want to read them , transpose them , and then rewrite them into a new folder .
I'm using Ubuntu , so this might not apply that accurately , but here's how I'll do it .
Printing out filename does show all of the files that i want to transpose though .
Also , is it alright if you edit your post and append your updated code ?
Using them will be far faster than using ` apply ` to call a custom Python function for each item in the Series .
If you want to strip all brackets or double quotes from both ends of each string , you could instead use #CODE
^ [ " means replace [ or " at the beginning , right ?
i created the following dataframe by using pandas melt and groupby with value and variable .
You can drop the `' None '` row like this : #CODE
No need to go via ` apply ` ; ` 100 * df2 / df2.sum() ` should work .
The following command will work in Windows , but when I run the exact same code on the exact same data in Unix , the column headers do not align with the columns after writing out to csv .
EDIT : Here is a look at my inputs which I would lie to merge by the first column " VAR " : #CODE
Do the columns align then ?
Thank you as well for the ` resample ` comment as well !
Fopr every unique key I store a list of corresponding integers ( I later convert this list toa pandas Series object append it to the orignal dataframe , and also drop ( not inplace but re-assigning ) the orignal string user column .
I have tried the dictionary approach , where I load the " keys " from user and map it back to integer counter - the problem though is that I have to be ABLE to read the whole data frame first .
On trying the islice ( recursive read ) , I needed to create " set " and memory ( again a dict ) - to map it back to the integer counters .
Then can append to this .
Pandas : Merge data with different timing
Together I would like to join them , such that #CODE
You just do an outer merge on the left and right hand side indices , are the column names different ?
Column names are the same , outer merge sounds promising .
Ah I understand your comment now I think - the outer merge works , but the data is in the wrong order ...
@USER If you do an outer merge and where you have duplicate dates in both dfs will result in duplicate rows as you can see in my answer , are you wanting to merge the duplicated rows as you could have many duplicated rows is my question
Just perform a merge the fact the periods are different and don't overlap suits you in fact : #CODE
You can rename , fill , drop the erroneous ' GDP_y ' column
@USER the default merge type is inner so the values have to match in both dfs , is that what you want ?
constantly appending is not efficient at all ; append the generated frame for each group to a list ; concat at the end
For looping over a groupby object , you can try ` apply ` .
You could write ` example_function ` to append to a data structure yourself , or if it has a return value , pandas will try to concatenate the return values into one dataframe .
I'm trying to group by a column , find the minimum date value in that group and insert it in a new column for all values in that group .
I think you want the ` transform ` method : #CODE
@USER , I didn't need to transpose anything
Pandas : Timing difference between Function and Apply to Series
Why is the apply ( lambda ) method ~ 3.5 x slower .
So in this case it looks like most of the performance difference is related apply converting each column to a ` Series ` and passing each series separately to rolling_mean .
The model is very simple , since I just want to familiarize myself the ` MovingOLS ` API , and I expect to get 2 OLS models if I understand the ` Moving ` and ` rolling ` parts correctly .
You are trying to estimate 3 parameters with a rolling window of 2 observations .
It should work if you add some rows and make your rolling window bigger .
I want to shift to the right an entire column , fill with NAN the first column and drop the last column : #CODE
shift shift the data by rows .
Use ` loc ` to find where column B values are not null : #CODE
Python set value multiindex Pandas
You should use loc . check out the indexing docs .
In order to set the value of only ONE element ( i.e. for a specific column in a specific row ) , first pick out the relevant series from the data frame ( df [ ' A ']) , and then use loc on that series -> df [ ' A '] .loc [ ' [ 21-23 )' , ' M ' , ' [ 10000-20000 )'] .
I would use the ` shift ` method to look backward in the dataframe : #CODE
But I'm still curious if there's a general solution , using something like apply or map .
` sweep ` is based on ` apply ` .
if you read the documentation for apply , the definition is amply clear IMHO .
Before you ask , yes apply is mentioned on the docs page for sweep ( look under see also ) .
Pandas has an apply method too , apply being what R's sweep uses under the hood .
You can use an apply with a function which is called against each row : #CODE
Note : that axis=0 would apply against each column , this is the default as data is stored column-wise and so column-wise operations are more efficient .
You can do the same in numpy ( ie ` data.values ` here ) , either multiplying directly , this will be faster as it doesn't worry about data-alignment , or using vectorize rather than apply .
Can you please comment on the use of ` lambda ` in ` apply ` ?
Are you trying to append them to an ` HDFStore ` iteratively ?
their are example about appending to a HDFStore panel , but you need to use append and you have to append complete panels
So I can't store a panel and then append dataframes to that panel whilst it is still in hdf5 then ?
for instance , the error you're seeing has nothing to do with plotting , so cut that stuff out .
Previsouly when eg adding a dataframe with a datetimeindex and a series with a datetimeindex , it would align on the index ( instead of the columns as other dataframes ) .
I've tried to resample this data by day using : #CODE
Additional information : I tried using ` pd.concat() ` as suggested by Joris ( I had to pass 0 as the axis argument as 1 resulted in ` V #URL reindex from a duplicate axis `) instead of ` .append() ` but the resulting ` DataFrame ` is the same as with ` .append() ` , a non-uniform non-monotonic time series .
The ` append ` method does ' append ' the rows to the other dataframe , and does not merge with it based on the index labels .
For that you can use ` concat `
` .append ` will append the rows ( and columns of ` df2 ` that are not in ` df1 ` will be added , which is the case here ): #CODE
Apart from that , the ` resample ` should normally work .
Thanks for you suggestion , I tried using ` pd.concat() ` ( I had to pass 0 as the axis argument as 1 resulted in ` V #URL reindex from a duplicate axis ` instead of ` .append() ` but the resulting ` DataFrame ` is the same as with ` .append() ` , a non-uniform non-monotonic time series .
I think the index is the problem but I'm not sure where to start looking , I thought that some time stamps might contain hour information while other not which is why I wanted to resample or groupby day .
And as the error says , ` concat ` cannot handle this .
So you should remove these duplicates indices ( and then the resample should be a good approach ) , or otherwise you could maybe use merge : ` df1.merge ( df2 , left_index=True , right_index=True )` .
But what error do you get if you first do ` .resample ( ' D ')` on both dataframes and then do ` concat ([ .. ] , axis=1 )` ?
Because this should normally work ( with the resample , you shouldn't have the problem of the duplicate values ) .
BTW , you have to use ` axis=1 ` , as ` axis=0 ` will concatenate the two dataframes along the first axis , so concatenate the row indices , which is indeed the same as ` append ` ( you want to concatenate the columns indices ( axis=1 ) , while matching the row indices )
To answer your question , doing ` .resample ` on both ` DataFrames ` ahead of the ` concat ` doesn't make a difference , I still get the ` ValueError `
And because of that , ` concat ` does not know how to match the indices between df1 and df2 ( as there are more matches possible ) .
Note that resample is not inplace .
Did you do the resample correctly ?
Obviously I could write plain Python that , given the period I re-sampled to in Pandas , could give me the Series I need , but I'd like to know if there is a trick within Pandas that helps me with this , or something I could do in Numpy , as I want to apply this to largish datasets ( hundreds of users , thousands of days , multiple login / logouts a day per user ) .
Assuming we can transform our Login / Logout data into two DataFrames indexed by time : #CODE
Then we can perform an outer join on the two tables , and fill the NA values the join created with 0s : #CODE
I edited to replace the word boundaries , maybe pandas doesn't like those .
Pandas error with basemap / proj for map plotting
The code is supposed to create a plot map of Haiti but I got an error as below : #CODE
This will give you ` len ( df.index.levels [ 0 ]) * len ( df.index.levels [ 1 ])` plots .
Then merge using correct answer below .
So my column 0 on either data set is the key i want to merge on , and i want to keep all data from both result sets .
But on the join I get the following errors : #CODE
What happens if you do ` underlying.drop_duplicates ( cols= ' dt1 ' , inplace=True )` and ` options.drop_duplicates ( cols= ' dt2 ' , inplace=True )` does the merge work ?
@USER drop_duplicates causes the same error as the merge ` AssertionError : Cannot create BlockManager._ref_locs because block [ IntBlock : ` .
I had a look at your data ` svxySynthetic.csv ` has unique values for ` dt1 ` but ` optionsArg ` has duplicated values for ` dt2 ` as you have one entry for ` call ` and another for ` put ` , and in fact out of 372032 rows , you only have 2411 unique ` dt2 ` values so how do you want to merge these ?
OK I see one of the problems , you have duplicate column names : change your line to this : ` options = pd.read_csv ( " txt2.txt " , names =[ ' dt2 ' , ' ticker ' , ' maturity ' , ' strike ' , ' cP ' , ' px ' , ' strike_1 ' , ' yield ' , ' rF ' , ' T ' , ' rlzd10 '])` and then decide how you want to merge the datasets
what if i join underlying to the option ?
If you swap the order and specify left join then it should work : ` merged =o ptions.merge ( underlying , left_on'dt2 ' , right_on'dt1 ' , how= ' left ')`
You should still be able to merge on the columns : #CODE
This will perform an inner merge so only the intersection of both datasets , i.e. where the values in column ` 0 ` exist in both , if you want all values , then specifcy ` outer ` : #CODE
But also explore the rolling window methods already provided by ` pandas ` , you may find something suit your real world application better : #URL
When I move my mask into loc , it works .
I saw something called diff on the dataframe / series but that does it slightly differently as in first element will become Nan .
Use shift .
You could use ` diff ` and pass ` -1 ` as the ` periods ` argument : #CODE
How to apply a function to a mixed type Pandas DataFrame in place ?
This is how I apply a function to Pandas dataframe , it works in place and modifies the original data frame .
But if I try the same on this data frame ( it has ints and floats instead of just ints ) , then it fails to apply inplace and always returns a dataframe .
So no way to apply a function in-place to a DataFrame in pandas ?
But of course it'll return a frame , apply returns a frame , this seems to work inplace for me in 0.13.1 ( even with floats ) .
Generally you'll want to vectorize rather than use apply , obviously here , as mentioned above , you'd use ` x [ ' b '] +=1 ` .
Also , I think using iterrows is preferable to using apply like this .
I realise this does answer the question ( well ) , but using apply with side-effects seems hacky / wrong / I really dislike it !
Python boxplot out of columns of different lengths
I convert the dataframe to numpy using df.values and then pass that to boxplot .
When I try to make a boxplot out of this pandas dataframe , the number of values picked from each column is restricted to the least number of values in a column ( in this case , column F ) .
Is there any way I can boxplot all values from each column ?
NOTE : I use df.dropna to drop the rows in each column with missing values .
I use df.dropna to drop the rows in each column with missing values .
How about don't drop the NaN ?
I think ` boxplot ` will handle NaN values itself .
If you want to iterate across your database and apply a function to each row , you might also want to consider the apply function #CODE
Use ` loc ` , ` iloc ` , ` [ ]` syntax , or ` ix ` to perform assignment and to select subsets of your data .
This error happens because ` _ix ` is not instantiated until you call ` ix ` ( and its value is ` None ` until that happens ) , but this implementation detail is completely irrelevant to you as a user of pandas .
If a column contains only strings , we can apply ` len ` on it like what you did should work fine : #CODE
Applying the function ` len ` will raise an error similar to what you have seen : #CODE
Continuing the above example , let us convert ` strange ` to strings and check if ` apply ` works : #CODE
When you call ` resample ` on your 1 column dataframe , the output is going to be a 1 column dataframe with a different index -- with each date as its own index entry .
If you do , a different solution is to merge the two dataframes on the date , after making sure that both have a column ( not the index ) with the date .
You can't resample at a lower frequency and then assign the resampled ` DataFrame ` or ` Series ` back into the one you resampled from , because the indices don't match : #CODE
If you want to get back ' df2 ' , you could put them into a dictionary to map the name back to the object .
Self Join in Pandas : Merge all rows with the equivalent multi-index
I'm trying to do a self join on the index Date , Gran , Country , Region producing rows in the form of
And have you looked at the ` merge ` and ` join ` functions ( #URL ) ?
The reason you get a NaN value , is because the index does not match ( in ` rows [ ' naics '] = naics.indnaics ` ` rows ` has index 0 , while ` naics.indnaics ` has index 89 ) , and assigning the value will try to align the indices .
I want to normalize the weekly sales given in A by dividing each row of A by corresponding CPI value given in B .
The DataFrames ' merge method should be faster even though it copies data .
The reason I am asking , is because I suspect ( ? ) it is faster to create a zero filled dataframe , and then replace each element as needed .
So it might be faster to create an empty dataframe with nxm dimensions and then replace elements as needed ( by copying a list to each column ) .
in general creating an empty frame , then filling it column by column is not very efficient ; use a dict / list instead , or create sub-frames and concat them
Then I tried to apply the same solution to ` pandas ` : #CODE
Actually , why not do it with a map like :
map ( lambda x : x.value_counts() , mdf.columns )
Pandas ' merge returns a column with _x appended to the name
The keys I want to merge with are in column A .
The suffixes are added for any clashes in column names that are not involved in the merge operation , see online docs .
So in your case if you think that they are same you could just do the merge on both columns : #CODE
What this will do though is return only the values where ` A ` and ` B ` exist in both dataframes as the default merge type is an ` inner ` merge .
So what you could do is compare this merged df size with your first one and see if they are the same and if so you could do a merge on both columns or just drop / rename the ` _x ` / ` _y ` suffix ` B ` columns .
I would spend time though determining if these values are indeed the same and exist in both dataframes , in which case you may wish to perform an ` outer ` merge : #CODE
Then what you could do is then drop duplicate rows ( and possibly any ` NaN ` rows ) and that should give you a clean merged dataframe .
THe output of that regression , i would like to insert as a column into this original dataframe .
How do you shift Pandas DataFrame with a multiindex ?
With the following DataFrame , how can I shift the " beyer " column based on the index without having Pandas assign the shifted value to a different index value ?
Use ` groupby-shift ` to apply the shift to each group individually : ( Thanks to Jeff for pointing out this simplification . ) #CODE
If you have a multiindex , you can group by more than one level by passing a sequence of ` ints ` or level names to ` groupby's ` ` level ` parameter .
ValueError : cannot reindex from a duplicate axis
So far I can modify the ` _metadata ` attribute to supposedly allow preservation of metadata , but get an ` AttributeError ` after the join .
I think something like this will work ( and if not , pls file a bug report as this , while supported is a bit bleading edge , iow it IS possible that the join methods don't call this all the time . That is a bit untested ) .
32-bit has a 4GB addressable limit , but python requires contiguous memory , so that's too big to stack reliably . in my experience 32-bit has issues with anything > 1GB ; 64-bit scales no problem however .
You need to convert each observation to a ` dataframe ` and ` concat ` them into the final ` dataframe ` as in this solution .
And here's what I would like to get to : Resample the datetime index to the day ( which I can do ) , and also count the unique users for each day .
You don't need to do a resample to get the desired output in your question .
And then if you want to you could resample to fill in the time series gaps after you count the unique users : #CODE
It seems from your description that you would rather have a mask , those locations where ` n is None ` for example , and only modify values at those locations .
This can be achieved with the ` loc ` indexer : #CODE
In this example , I made use of the built-in function ` isnull ` that can check all the elements of a ` pandas.Series ` to see if they are null ( ` NaN ` or ` None `) .
So by passing this as the first dimension of the index for ` loc ` , we can modify the values in only those rows .
I understand the first half of your answer ( before loc ) .
In general , in the Python scientific stack , you want to avoid for-loops in favor of operations that can be directly applied to arrays and handle their array-ness in the expected way .
You can use ` to_datetime ` or you could use ` datetime ` within ` apply ` .
Or use apply : #CODE
Can always drop the Day column afterward if you dont want it .
Yeah , I did the ` to_datetime ` answer a while ago and the came back and added the ` apply ` answer at about the same time as you .
not a speedfreak myself , but i tested yours and my apply method .
Youe is about 50% faster as mine has the overhead of creating a new dataframe before doing the apply .
If you want to compute " intensity " by ` intensity / mean ( intensity )` , you can use the ` transform ` method , like : #CODE
It turned out that ` .size() ` had way less rows than the original object ( also if I used ` reset_index() ` , and however I tried to stack the sizes back into the original object , there were a lot of rows left with ` NaN ` .
you would need to cast the ` drop_list ` to a set otherwise you get a ` TypeError : descriptor ' intersection ' requires a ' set ' object but received a ' list '` error .
Because questions describing your requirements and asking someone to write the code for you or explain how to write the code are considered off-topic for Stack Overflow , but none of the standard close reasons apply .
Then , all what is needed is just to ` sum ` the ` groupby ` object and then do a ` merge ` : #CODE
You can directly merge the two using pandas ` merge ` function .
The trick here is that you actually want to merge the country column in your ` datadf ` with the neighbor column in your ` borderdf ` .
Finally , merge back with the data to get the country's own GDP .
Faster way to get row data ( based on a condition ) from one dataframe and merge onto another b pandas python
Use ` factorize ` : #CODE
I want to replace ` None ` entries in a specific column in Pandas with an empty list .
and by the way , this gives me an error : ` ValueError : Must have equal len keys and value when setting with an iterable `
You can do an assignment where the length of the True values in the mask are equal to the length of the list / tuple / ndarray on the rhs ( e.g. the value you are setting ) .
Anything else is expressly disallowed because its ambiguous ( e.g. do you mean to align it or not ? )
Python Pandas Merge result does not contain values from right dataframe
I try to merge two dataframes : #CODE
I want to replace the ` {} ` for ` 1 ` .
There are no errors , just the ` {} ` does not get replace .
` replace ` will not work here as it is for strings .
enumerate is the wrong thing to do here , you should use iterrows or iteritems but the best thing to do is create the boolean mask and then just set the values
` replace ` will not work as it is a built-in that operates on strings .
Better is to do the mask on the entire dataframe , also you shouldn't perform chain indexing , thanks @USER for pointing out the error in my ways : #CODE
Pandas : how to use convert_objects to replace strings with NaN values
I am essentially trying to use convert_objects to replace string values with 1's in the following dataframe ( abbreviated here ): #CODE
Where there may also be a step prior to this , with the 1s as NaN , and fillna ( 1 ) is used to insert 1s where strings have been .
I wouldn't have known to even attempt to apply it this way if not for the previous post ( linked above ) .
I would like to draw a boxplot for the following pandas dataframe : #CODE
This results in an incomprehensible boxplot .
Is there any way I can stratify the MAT values , so that I get a different boxplot of N0_YLDF for the first quartile of MAT values and so on ....
Is there any way you can replace the x-axis labels by the actual MAT quantile value ?
I am trying to change the color of the boxes in the boxplot using pyplot.setp ( ax [ ' boxes '] , color= ' blue ') . however I get the error '' AxesSubplot ' object is unsubscriptable ' .
Ah , I found this reply of yours ( @ CT Zhu ) for the boxplot styling . that works : #URL
Pandas has the ` cut ` and ` qcut ` functions to make stratifying variables like this easy : #CODE
I suppose it's been a little while since I embarrassed myself on stack overflow ;-) Thanks for setting me straight
I want to drop all values for a similar value in col.1 after 1 is encountered in col.2 .
I think I want to do something like ` reindex ` but using a fill method of ` sum ` ( which doesn't exist ) .
Use loc and idxmax : #CODE
I think you should use ` ix ` ( or plain ` [ ]`) instead of ` iloc ` , since ` idxmax ` will return the actual index , so ` iloc ` will fail if the indexes aren't consecutive integers starting from 0 .
I've updated my answer , I thought in this case there would be no difference between ` iloc ` and ` ix ` , I wasn't aware of the index limitation
`` ix `` would give the wrong answer IF it had an non-natural integer index ( e.g. 2 , 4 , 6 , 8) .
@USER : Is there any official guideline these days on when you should use ` loc ` vs ` ix ` ?
well , use `` ix `` when you simultaneously need both location and position indexing on different axes , e.g. `` df.ix [ ' A ' , -1 ]`` , otherwise I always use `` .loc ``
Example output , where I want to group by follow , std_epoch , and Focal -- and drop duplicate values from column ' 0 ' , group-wise ( in this example , that's row 2 in the input ) .
I'm trying to drop NaN and duplicates , group-wise .
I can just drop duplicates first across the whole dataframe , then use groupby .
As noted in my comment , I don't need to use groupby to drop duplicates , I'd just put them in the same line in my previous code .
To drop duplicates : #CODE
In order to apply a method on a DataFrame that is grouped ; your need to use a loop as follows : #CODE
Now I need to shift the timeseries data ahead , say I want to shift the entire data by 60 secs , or a minute .
In your example data , if the Timestamp of 1 matches to the value of Timestamp 64 , where do you want Timestamp 4 to map to ?
Should it also map to Timestamp 64 ?
-- append the series to the dataframe
thanks so much - i didn't realize you could groupby with two columns - that was my problem . the one question i have is why is there a 1 in diff ( 1 ) ?
I get ` ValueError : cannot reindex from a duplicate axis ` , so that doesn't seem to work .
I have checked and the ` apply lambda ` part gives the expected results if I ` debug print ` it .
The problem seems to be assigning the apply lambda construct back to the DataFrame .
Note : it could be there are some NaNs causing this float upcasting , in which case you may have to reconsider your approach ( since you won't be able to convert to int ! ) , one option might be to do the string formatting and then apply ` to_datetime ` : #CODE
merge items into a list based on index
For a more realistic scenario , my index will be dates , and I want to aggregate multiple obs into a list based on the date .
Your code will not append you have to do ` df =d f.append ( df )` secondly are you sure you want to do this ?
aggregate multiple obs into a list based on the date .
You apply Principal Component Analysis ( PCA ) to any dataset , the resulting principal components are not correlated by definition .
Working on this , I did find that Filter 2 above doesn't generate the appropriate set of indices to drop .
Try replacing ` ix ` with ` loc ` in either example .
See [ Indexing and Selecting Data ] ( #URL ) for the difference between ` loc ` , ` iloc ` , and ` ix ` , as the last can be interpreted as either an index label , or positional .
I get different numbers that with ` ix ` , but still wrong ones =/
Merge columns together if the other values are blank
If multiple columns have data than don't merge / change for that row .
I don't fully understand how to get my apply function to actually apply to the row to change it .
Then , you can retrieve a subset of indices , by passing a list of tuples to the ix property .
MultiIndex was the concept I needed that I wasn't imagining ; thank you very much .
The solution I implemented was to take my exampledf and create a multiindex thus :
Is using a rolling mean the best way ?
Do you want to group by the year and month , or just somehow merge every two rows regardless of the Idx values ?
I want to just merge every two rows .
You can aggregate rows using a ` groupby / sum ` operation : #CODE
You would get that error if ` np.arange ( len ( df )) // 2 ` did not have the same length as ` df ` itself .
Here's a wild guess : Make sure you have ` np.arange ( len ( df )) // 2 ` and not ` np.arange ( len ( df ) // 2 )` for example .
One approach would be to use a custom dict to create a ' rank ' column , we then use to sort with and then drop the column after sorting : #CODE
In that case merge them and then depending on the complexity of your function either use a lambda or define your function and just apply it row-wise so ` merged = df.merge ( df1 , on= ' Date ')` then ` merged.apply ( myfunc , axis=1 )` or ` merged.apply ( lambda row : myfunc ( row ) , axis=1 )` I'd need to see your function first though before deciding the best approach , also it's getting late here in blighty so I may not answer
In fact what I would do is merge and then perform boolean masking on the merged df : ` merged [ merged [[ ' Value1 ' , ' Value2 ']] .max ( axis=1 ) > my_val ]` this will return the highest values for each row that are higher than your threshold value .
When performing the merge you may get duplicated columns where Value1 from both dfs don't match , by default they will have suffix ` _x ` and ` _y ` , you can rename or not care seeing as you just want the highest value
Let me see if I understand it correctly : I want to merge the two dataframes prior to using any function , correct ?
Yes that is correct , then apply a function row wise or if you're just looking for values larger than some threshold then do boolean masking using max() that will be very quick .
OK it looks like your date is actually your index , in which case you need to merge using the index , I think you can drop the on= ' date ' param but check the docs .
I'm trying to merge two dataframes ( call them df1 and df2 ) of different lengths which are both indexed by their dates .
Use ` join ` instead of ` merge ` ; it merges on indexes by default .
Looking at your error messages it looks like you have overlapping column names ; add something like the ` rsuffix= ' _y '` option to join .
I think it is most naturally to use ` join ` because it merges on indexes by default .
Left or right merge gives me a data frame that contains rows that are present in one of the data frames .
If it is just one merge key then you could do it with ` isin ` and ` ~ `
@USER ., I have more than one merge keys .
So , the left join here will falsely return that row , which matches in both DataFrames , as not being present in the right DataFrame .
Then when you join , you can check to see if that column is NaN for the inner table to find unique records in the outer table .
I suggest that you can transpose your dataframe first and then do some simple compares : #CODE
Is it possible to pivot table without specifying column and row names in Pandas ?
Is it possible to pivot dataframe table without specifying column and row names in Pandas ?
Do you just want to transpose it ?
I think interpolate needs regular spaced time series .
Looks like you need to resample before .
I've also read through the " Related " questions on this topic , but I'm still missing the simple rule Pandas is using , and how I'd apply it to for example modify the values ( or a subset of values ) in a dataframe that satisfy a particular query .
In a single dtype case ( which could be a 1-d for a series , a 2-d for a frame , etc ) . numpy * may * generate a view ; it depends on what you are slicing ; sometimes you can get a view and sometimes you can't . pandas doesn't rely on this fact at all as its not always obvious whether a view is generated . but this doesn't matter as loc doesn't rely on this when setting .
I first used the ` pandas ` ` shift ` method to add shifted lon / lat columns ( inspired by this SO question ) , so I could perform the calculations over a single row .
Then I used the pandas ` apply ` method ( as was suggested here ) to implement the ` pyproj.Geod.inv ` calculation , looping through slices of the ` pandas ` ` DataFrame ` for each individual in the population .
However , I can't seem to figure out how to convert a column into a datetime object so I can pivot out that columns and perform operations against it .
As I mentioned you can split it into a number of different queries if there are too many IDs , or if the IDs represent a significant proportion of the total rows in the table , then query the entire table and join the resulting DataFrames .
Maybe the shift function could be useful for you , check it out : #URL
Now use ` apply ` and ` cut ` to create a new dataframe that replaces the percentile with the decile bin it is in ( apply is iterating over each column ): #CODE
Use apply once again to get a frequency count : #CODE
I'm trying to replace the values in one column of a dataframe .
If I were to map ' female ' to 1 and anything else to ' 0 ' .
You can edit a subset of a dataframe by using loc : #CODE
I am attempting to join several dataframes together .
You can define an empty ` DataFrame ` and append all other dataframes to it .
so for your example just turn the values into a list and then concat : #CODE
I did this because the list is written by a whileloop using append : ` ... row =p andas.DataFrame ([ dict ( Symbols=stock , Date= stockhistory.index [ 0 ])])` ` ...
How can I replace the ` is-earlier-than ` with something that's actually meaningful to Python ?
You should use the apply function which applies a function on either each column ( default ) or each row efficiently : #CODE
and then call apply like this : #CODE
hist may work .
`` hist ( series , bins=30 , range =[ 0,150 ]) .figure ``
My current approach is to take each ` dict ` from the list one at a time and append it to the dataframe using #CODE
And then I merge the two ..
Now I want a ` isnull ` column indicating if the row has any ` nan ` : #CODE
Lookup from a pandas dataframe given a multiindex
Unfortunately you cannot use ` fillna ` to replace the NaN with an empty list due to this issue .
However , you can drop it with ` dropna ` , and in many cases you won't really need the empty item for the cases with no match anyway .
I am implementing pivot table modules just like MS excel pivot table .
So I am implementing pivot table manualy with groupby , melt modules of pandas .
Truth be told , they seem very quite comparable but there is far more documentation , Stack Overflow questions , etc pertaining to Pandas so I would give it a slight edge .
You requirement is a little unclear , if all your values can ever be small , mediu , or high and you want to drop rows that are any of these values then this will result in now rows so could you explain clearer what you require
You would be better off creating a boolean mask for your condition , update all those rows and then set the rest to the other value
linear fit by group in apply takes too long using pandas
but this fails with the error ` MergeError : No common columns to perform merge on ` .
Reference values in the previous row with map or apply
Normally , we can use either ` map ` or ` apply ` , but it seems that neither of them allows the access to values in the previous row .
If it is just based on the previous row , you can also calculate it and then shift it .
If you just want to do a calculation based on the previous row , you can calculate and then shift : #CODE
If you simply call ` pandas.DataFrame() ` the dictionaries will not be " expanded " , so you have to create another ` DataFrame ` from them , and append the columns you want to the original DataFrame .
Now to format , scale , and apply to the actual data set .
concat it with my df DataFrame
So the end keyword in ` date_range ` apparently sets the quarter preceeding ` df.index [ len ( df ) -1 ]` , is there a way to tell ` date-range ` to use the quarter following ?
` pd.date_range ( start =d f.index [ 0 ] , end =d f.index [ len ( df ) -1 ] + pd.datetools.QuarterEnd ( 1 ) , freq= ' Q ')` is what I came up with , would like to know if there is something simpler as according to #URL end and start keywords are strictly inclusive
The other differences are semantic ( e.g. , you can use ` or ` like ` | ` for bool numpy arrays , chained comparisons ) .
maybe add that you * can * quote the individual names ( only with eval ) , e.g. `` pd.eval ( ' df [ df [ " annual rate "] > 0 ]')`` works ( but a bit klunky )
I want to replace any entry that has ` np.Inf ` with the value ` np.NaN ` .
You can use ` df.replace ` to replace your ` np.inf ` values .
I am taking the second dataframe and doing some calculations with it to append to the first dataframe .
Could you use concat instead ?
AttributeError : ' DataFrame ' object has no attribute ' concat '
[ ` append `] ( #URL ) does * not * operate in place .
Say I have a dataframe , and I want to replace any entries higher than 1 with 1 .
You can't call the ` replace ` method on the result of ` DataFrame.update() ` because it returns ` None ` and ` None ` has only dunder methods .
you're right , but how does that apply here ?
/ edit says that you can actually directly group the data , if you don't want to reindex : #CODE
But when I calculate the median absolute deviation ( mad ) , I get an extra column ' a ' , which is all zeros : #CODE
maybe not important , but just for clarification , pandas mad() is not median absolute deviation , but mean absolute deviation . from the [ docs ] ( #URL ) .
The bug is that non-cythonized routines are calling ` apply ` rather than `` agg ` effectively .
I then use ` to_sql ` to append the data up to mySQL #CODE
I have tried ` stack ` and ` unstack ` but haven't been able to make it work .
I'm not sure how common " melt " is as the name for this kind of operation , but that's what it's called in R's ` reshape2 ` package , which probably inspired the name here .
As expected this apply works over the groupby object : #CODE
However , when specifying options for apply it throws up an error : #CODE
Pandas : Timing difference between Function and Apply to Series
There are different ` apply ` methods for ` DataFrame ` s and ` GroupBy ` objects .
And use ` transform ` so you can easily add the results to the original dataframe : #CODE
I did it on a large dataset ( 4 million rows ) and it was significantly faster if I avoided ` transform ` with something like the following ( it is much less elegant ): #CODE
The [ ' A '] doesn't matter for ` np.size ` ; it matters for ` transform ` .
I could be wrong , but I doubt it would help much ; I'm guessing that ` transform ` is the bottleneck .
#URL is the transform speed up - not implemented ATM ( but easy enough to inline )
Pandas DataFrame merge suming column
I'm trying to merge two ` DataFrames ` summing columns value .
Pandas 0.13.1 pivot_table ( pivot ) error - not in 0.12.0 - should ( and how ) I go back to 0.12.0 ?
It could be that I am misunderstanding pivot vs .
Say I have a dataframe and I do a boxplot : #CODE
You can use the ` ax ` keyword argument as below in the ` boxplot ` method #CODE
I also noticed that Pandas ` boxplot ` uses its own title .
Say I have a dataframe and I do a boxplot : #CODE
And I made sure to call ` boxplot ` with the axes returned by ` add_subplot ` and everything
and therefore , of course , the merge will not work .
My mistake you can pass that param on a groupby apply , could you post some sample data so I can see what your df looks like prior to the groupby
@USER So if you reset_index and then do groupby and apply does it work ?
@USER , OK I see the problem , you could using your original code just drop the duplicate index values : ` df.groupby ( level=0 ) .apply ( someFunc ) .reset_index ( level=0 , drop=True )`
python pandas unstack MultiIndex directly to sparse object
Is there a way to directly take a Series object with a three- or two-level MultiIndex and unstack it directly to a sparse DataFrame or sparse Panel ?
no , unfortunately unstack currently enumerates the cartesian space of all the indices ; need someone to enhance this ( whom has an interest in sparse ) !
I can't seem to figure out how to add a % of total column for each date_submitted group to the below pandas pivot table : #CODE
The most natural way is to do it as you create the pivot table .
If you want to add this as a column , you can indeed ` concat ` both serieses to one dataframe as suggested by @USER : #CODE
the ` level= ' state '` kwarg in ` div ` tells pandas to broadcast / join the dataframes base on the values in the ` state ` level of the index .
I am iterating through the rows of a pandas ` DataFrame ` , expanding each one out into ` N ` rows with additional info on each one ( for simplicity I've made it a random number here ): #CODE
you can do `` s.to_frame() `` , but you know this is going to be horribly slow . you are copying the entire frame EACH time you append , not to mention concat copies as well .
You can ` cut ` and then call ` describe ` : #CODE
Eg here #URL where ` cut ` is explained , ` describe ` is not mentioned .
I know I can just use ` apply ` twice , but is there a vectorized / cleaner way to do it ?
I think you want to use the ` apply ` method of the ** DataFrame** , using axis = 1 , e.g. df.apply ( lambda row : print row , axis=1 ) .
Also , do you want to interpolate missing dates or ignore them ?
You can use the ` apply ` method of the DataFrame , using ` axis = 1 ` to work on each row of the DataFrame to build a Series with the same Index .
Have a look at the resample method docstring .
pandas groupby add column from apply operation
I'd do something like the following using ` groupby ` and ` transform ` : #CODE
But do you mind explaining what's the difference between ` transform ` and ` apply ` ?
` apply ` we will work just fine too .
If you replace ` transform ` with ` apply ` , you should get the same output .
` apply ` is the more general method ; ` transform ` is appropriate when you want to return something like indexed .
17 func = stack [ -1 ]
Pandas stack chart with multiple dataframes
I would like to build a stack graph for Size with these intervals ( 0-5 , 6-10 , 11-15 , 16-25 ) .
I'm working on a project within I will do a lot of basic computations on many small 6x6 matrices ( multiply , inverse , transpose ... ) .
I would like to know how I can join data from a definition table with a start and end date column in to my event table with a date column where the date column is between the startdate and enddate columns .
I tried creating a function that would filter on table B and then using apply like below .
For comparison , here's the timeit of ` apply ` on the same frames : #CODE
To overcome this , you can use ` loc ` ( assuming ` result ` is the result of the resampling ): #CODE
Is it possible to compare all the rows that have the same item and keep only the item that has the lowest diff ?
This groups by ` item ` , ` as_index=False ` means that you want grouped output looking more like the original , ` [ " diff "]` selects the ` diff ` column , and ` min() ` says that we want the minimum value .
I'm using ` groupby ` on a pandas dataframe to drop all rows that don't have the minimum of a specific column .
Can I keep those columns using groupby , or am I going to have to find a different way to drop the rows ?
Method #1 : use ` idxmin() ` to get the indices of the elements of minimum ` diff ` , and then select those : #CODE
Method #2 : sort by ` diff ` , and then take the first element in each ` item ` group : #CODE
I know how to create a new column with ` apply ` or ` np.where ` based on the values of another column , but a way of selectively changing the values of an existing column is escaping me ; I suspect ` df.ix ` is involved ?
@USER For indexing with boolean vectors this is perfectly fine , if you want to add in other forms of indexing you would want ` loc ` .
Then ` concat ` together the resulting dataframes from each groupby : #CODE
Pandas drop unique row in order to use groupby and qcut
How do I drop unique ?
So you want to drop values that are unique ?
When I apply a numeric function to the group , such as max() or mean() , I get a DataFrame with type ` object ` returned #CODE
When I select only numeric columns first and then apply a numeric function to the group , such as max() or mean() , I get a DataFrame with a numeric type #CODE
I need to modify a column of my pandas dataframe , depending on the boolean value of another column . suppose I have one column of values , one column of true / false , and I want to sum 1 to those values with corresponding bool true .
` df [ ' val '] = df.apply ( lambda row : row [ ' val '] + [ 0 , 1 ] [ row [ ' bool ']] , axis=1 )`
Don't use ` bool ` as a column name , plus your values , if they are supposed to be booleans , are invalid , they should be ` True ` , ` False ` not ` true ` , ` false `
Use ` loc ` to create a boolean mask and update the values using this mask , example : #CODE
I am using the pandas DataFrame and have tried using different functions with no success ( append , concat , etc ... ) .
Weighted boxplot in Pandas
I want to draw a weighted boxplot , where the weights for each box are given by ColA_weights and ColB_weights respectively , I simply do #CODE
More generally , if you want to transform the groups of a ` GroupBy ` object with any arbitrary function , use the methods apply , transform , or filter .
i've had no success using transform , apply , or aggregate to accomplish my goal .
Change Particular Column values in a Pandas MultiIndex DataFrame
Your situation is more complicated that the one that general warning is addressing because you have a MultiIndex .
So I was playing with the ` concat ` method : #CODE
No I would like to replace the NaN's in the ` x1 ` column of ` data [ 1 ]` with the data that is in the x1data dataframe .
Not that ` interpolate ` isn't implemented on Panels iirc , which was the OP's original need .
@USER or maybe a nested ` apply ` 0_o
Based on your comments you can achieve what you want if you reshape ` data ` , interpolate using the ` DataFrame.interpolate() ` method and then return the array to its original value .
But the tricky part happens with ` sampleD ` , which had only ` count ` of ` 1 ` , so it didn't pass criteria and thus its value for ` psi5 ` is NaN , and either could be skipped since I'll probably do a pivot table from this and fill emtpy with na , or add a row with ` NaN ` s .
I'd probably use ` stack ` and ` unstack ` to do this in a vectorized way .
2 ) Unstack on all the indices except stop and fill the missing values created by unstack with zero : #CODE
How can I make it conditional to apply the ' cubic ' method to change values for those locations which can run ' cubic ' method ?
This will drop the rows of all ` NaN ` s .
Follow that up with a reindex : #CODE
Would it be quicker to do this in list comprehension , or with an apply function ?
You could first replace NaNs with empty strings , for the whole dataframe or the column ( s ) you desire .
However , by definition the ` apply ` ` lambda ` functions are bespoke for each definition .
I'm aware of the ` eval ` method but this doesn't sound like good practice ( as I'd one day like to open this up for others to use and ` eval ` is open to abuse ) .
While it's true that ` eval ` can be a security hole , it's possible to restrict what is available to it by modifying the globals : #CODE
I've done some further reading and there are other ways to exploit ` eval ` even with the whitelist but I think I'm prematurely optimising if I choose not to use it .
I've tried a lot different ways to mask the DataFrame or the underlying array ( df.values ) , but haven't figured out anything that works .
I would normally use ` apply ` , but I don't know how to use ` apply ` and still get the unique population value for each row .
More context : The reason I need to do this multiplication is because I want to aggregate the data by region , collapsing USA and CAN into North America , for example .
how to zscore normalize pandas column with nans ?
I have a pandas dataframe with a column of real values that I want to zscore normalize : #CODE
What's the correct way to apply ` zscore ` ( or an equivalent function not from scipy ) to a column of a pandas dataframe and have it ignore the ` nan ` values ?
Iterate and apply function over level ( s ) of MultiIndex dropping the iteration level
Can't you just apply a lambda that strips out the first index level ?
` reset_index ( level=0 , drop=True )` ; that should drop the first index and not insert it back into the data and preserve level=1 in the index .
So what we do here is we create a boolean mask , we ask for where ` Star_ID ` values is in both dataframes , however by using the ` ~ ` we ` NOT ` the condition which in effect negates it .
add / merge items to pandas data frame
Instead you want use the index labels to select / drop .
It is necessary to first sort your ` MultiIndex ` by the ` Frequency ` column to get the ordering you wish .
Conditional merge / get duplicate rows
What I want to do is a merge of the sort #CODE
So , the nicest thing would be to somehow put this into the merge .
I did not manage to , so my second approach will be to first to a ` how=right ` merge , grab all the rows and then go group-wise :
As it seems to be less performant , I prefer to find the duplicates using ` result.set_index ( ' id ') .index .get_duplicates() ` rather then ` result.groupby ( ' id ') .filter ( lambda x : len ( x ) 1 )` .
I am doing ` how= ' right '` - this will give me all the left rows ( yes , given the database , no left row will drop in the process ) , but in addition , all the right rows that match ( whenever there is more than one per left row , all of them ) .
Note that number of rows per ` a ` grouping in the actual data is variable , so I cannot just transpose or reshape ` pdf.values ` .
A rename is simply a delete followed by an append .
As soon as I apply the rename method to the frame , it doubles the output size .
( e.g. `` mode= ' w '``) , each time ?, tables by definition append .
So if you run your script twice it will append twice .
Doing a transpose ` df.T ` in a mixed-type frame , will necessarily convert to a type that can hold both types , meaning that a string and float will yield a ` object ` dtype .
So the best way to take advantage of pandas is to load the data in a way which allows whole columns to have NumPy dtypes other than ` object ` , and never transpose ( unless the entire DataFrame is of homogeneous dtype ) .
( See the docs for plot , or boxplot , for instance . )
Is there a way to do this without the for loop or using where ( ) or apply ( ) functions .
` apply ` should work well for you : #CODE
If you want to return 0 , you can do a ` fillna ( 0 )` after ` apply ` ing ` fund ` .
It would be faster to generate the ordering column using map as this uses cython so ` df [ ' Tm_Rank '] = df [ ' Tm '] .map ( sorterIndex )` , then order using this and then drop
If so , how can I apply it to only one of the index levels ?
And I would like to apply this function to every dataframe in a groupby object with something like the following : #CODE
How do I create functions like the above and have them properly apply to a groupby object ?
i've added some example as to what the problem is . the sample data generated isn't particularly good , but anything larger that would be appropriate to resample would be awkward to put there :-/ .
They are just random numbers ; none of the values repeat so each value in ' a ' would be a group and the resample want do anything .
df = pd.DataFrame ( { ' a ' :p d.Series ( randn ( len ( rng )) , index=rng ) , ' b ' :p d.Series ( randn ( len ( rng )) , index=rng ) } ); ;
i am used to being able to groupby and apply a transformation without difficulty .
i applied a transformation that makes the groupby for ' a ' effective , and the ` transform ` then works . now i just need to figure out what is different about my actual data .
use ` loc `
I want to use tools like transpose / pivot for this instead of copying the whole thing bit by bit to another DF and create it as I wish .
I think I would do it something like this ( using ` unstack `) : #CODE
Pandas unstack but only create multi index for certain columns
The goal here is to transpose this data , but such that some of the columns are single index and some are multi index .
unstack uses the specified level , and leaves the remaining index alone as unstack creates * columns *
Ok thank you , unstack could do with an option over which existing columns you wish to unstack .
you can give multiple levels to unstack , FYI
I would like to merge the two dataframes together to achieve a heirarchical index on the columns where the top level is the name of the dataframe it originated from , followed by the current column names .
You shouldn't use map on a frame , FYI ( which doesn't exist anyhow ) , you could use ` applymap ` , but MUCH more efficient to use a vectorized operation #CODE
Concat together and use the keys argument #CODE
I would like to transform my table above and resulting as below
It pop me no numeric for aggregate
The method you are looking for is unstack #CODE
You need to group columns with command grouby and also to play with command stack and unstack .
I've tried various merge and concat commands but haven't hit on the proper one yet .
But now I want to align and sum these vectors .
But now I want to join them so that I can sum them : #CODE
Is there a way to cut it down even further ?
This is equivalent to a ` concat ( ms ) .groupby ( level =[ 0 , 1 ]) .sum() ` .
Set no title for pandas boxplot ( groupby )
When drawing a pandas boxplot , grouped by another column , pandas automatically adds a title to the plot , saying ' Boxplot grouped by ....
as per Pandas : boxplot of one column based on another column
How to do a conditional join in python Pandas ?
( 1 ) essentially blow up the dataframe by merging on ` company ` and then filter on the 30 day windows after the merge .
This results in a merge for each group so it would be slower but it should use less memory
Now do a merge and then select based on if ` date ` falls within ` beg_date ` and ` end_date ` : #CODE
This merge essentially inserts your window end dates into the dataframe and then backfilling the end dates ( by group ) will give you a structure to easily create you summation windows : #CODE
Another alternative is to resample your first dataframe to daily data and then compute rolling_sums with a 30 day window ; and select the dates at the end that you are interested in .
I would like to drop the ` NaN ` s ( which are rows along the major axis ) and leave the data intact but it seems that calling ` .dropna ( axis=1 , how= ' any ')` will discard one row from the item that has 10 good rows and calling ` .dropna ( axis=1 , how= ' all ')` will leave one row of ` NaN ` s on the item that has 9 good rows .
First of all I group overwrite the scores by grouping and later stack the
Now , you can apply interpolation methods on the ` NaN ` values as described in the docs .
When using the ` drop_duplicates() ` method i reduce duplicates but also merge all ` NaNs ` into one entry .
How can i drop duplicates while preserving rows with an empty entry ( like ` np.nan , None or ''`) ?
You can add a column containing the ' Course number ' ( per user ) and then pivot it .
Dataframes in Pandas have a boxplot method , but is there any way to create dot-boxplots in Pandas , or otherwise with seaborn ?
By a dot-boxplot , I mean a boxplot that shows the actual data points ( or a relevant sample of them ) inside the plot , e.g. like the example below ( obtained in R ) .
I wonder if this is because the dataframe has points that exceed the boxplot whiskers ( but for some reason the outliers aren't displayed ) .
The ` pandas ` boxplot calls to ` matplotlib `' s boxplot .
And then use the ` boxplot ` method on the dataframe , I see something similar #CODE
Alternatively , you can use the seaborn ` boxplot ` function , which does the same thing but with some nice aesthetics : #CODE
My question is how do I append the series to the dataframe ?
Instead make a list of Series and concat those together with ` df = pd.concat ( series , axis=1 )`
2 ) Then use ` pd.DataFrame ` to create an instance directly from the dict without loop over each col and concat .
I have a Pandas DataFrame in which the index is ( notice the Freq : H ) - #CODE
Notice that the Freq is now None and also that there are less rows and a later start date-time .
Then , upon reload , the Freq of the DatetimeIndex is preserved .
Their is an option introduced in 0.13.1 ( might have been 0.13.0 ) , where you can set ` dropna=False ` on a ` put / append ` to avoid dropping an all-NaN row .
Note that if you are ` appending ` the frequency information will NOT be preserved if you append multiple times .
But your third and fourth ` for-loops ` perplex me : ` for i in range ( len ( pattern2-2 )): ` .
In other words , assuming that I want ` diff = A-B ` , symbolically I want : #CODE
The result for ` df.loc [ -1 , diff ]` should be ` NaN ` .
Just use ` shift ` : #CODE
The default is to shift by 1 , see the online docs
To apply to a groupby then you can do this : #CODE
What if want to apply the result within a group , respecting the boundaries of the group ?
No difference , still use shift see related : #URL
I think I need to do : ` diff = grouped [ ' A '] .shift ( 1 ) - grouped [ ' B '] .shift ( 0 )`
Just use ` df.B ` ungrouped ; you're not doing anything to it so it doesn't need to be grouped : ` diff = grouped [ ' A '] .shift ( 1 ) - df.B ` .
I figured out if I force it using ix function #CODE
It does not cut off any and gets me what I want .
Problem is I cannot specify the index position inside the ix function as I plan to iterate by row using apply function .
Seems a little cleaner ( at least to me ) to use ` isin ` since you have a nice list of the banned users ( you can then map the True / False to Yes / '' : #CODE
Like if I add ` Adminlist = [ ' Bill Gates ']` and ` df [ ' Banned '] = df.Users.isin ( Adminlist ) .map ( {True : ' Admin ' } )` I guess I would have to use a merge function then .
I was writing an answer with resample while Jeff made that comment .
could replace the longer ` dates= ...
pandas merge from mysql datatype mismatch issue
and then tried the merge , to no avail .
So .. this is for doing a rolling group by .
Its a bit unclear to me what you mean with a ' rolling groupby ' .
To me , rolling suggests that the timespan of the groups overlap , which is not what you show in your example .
So whats rolling here ?
pandas 0.13 system error : cannot set thread affinity mask
this is my first stack overflow question so please pardon any ignorance about the forum on my part .
I've been able to work around the problem for now by using ` apply ` , but am still concerned .
@USER ; You mean to the stack exchange answer ?
Using multiprocessing map with a pandas dataframe ?
I am using ( python's ) panda's map function to process a big CSV file ( ~50 gigabytes ) , like this : #CODE
Perhaps using multiprocessing's map function ?
Note that ( much like sharding in a Mongo database ) chunk-level parallelism doesn't work well if you need overlapping data ( like a rolling time series regression ) in the operations to be mapped .
I need a method to get the selected features , ( and preferably something to drop the unselected ones , for when I apply the models and selected features on new " test " data ) .
For each row , you convert the full string to lowercase , and replace the ' partial string to lower ' with the original partial string with two underscores added on both sides .
where ` item [ 4 ] == ' Iris-virginica '` filters what you want , and ` map ( float , item [: 3 ])` is for ` str ` to ` float ` , then ` np.mean ( ..., axis=0 )` is to get ` mean ` of the filtered data .
I've tried to reindex the dataframe back to a default index but that still gives me the results that ' DataFrame ' object has no ' wavelength ' attribute .
The " field " was supposed to show you can apply the " .values " method to various fields of the dataframe such as columns or a selected column .
I want to keep all the data , so resample won't do ...
This is not work I am not fussy about the precise for of the index ` ix ` , any of list of pairs , pair of lists , 2-d array , pandas.DataFrame is fine .
Well , you can avoid the apply and do it vectorized ( I think that makes it a bit nicer ): #CODE
To change the values directly , use loc : #CODE
The response below shows how to assign an axis to a plot function call from ` pandas.DataFrame.plot ` which makes it possible to apply the ` matplotlib.pyplot ` refinements .
How do I pivot the results to get the formats mentioned above ?
Use ` unstack ` .
Selecting out the Series in the Dataframe effectively allows me to assign to it and the original dataframe . this allows me to use the slicing syntac to apply logic influencing the results : #CODE
No but how will I do pandas operations , say I want to shift the series by say 10 seconds ?
Finally , merge the bin halfway points back into the original dataframe .
Then you can drop ` quartile boundaries ` if you want .
I want to append the Pandas dataframe to an existing table in a sqlite database called ' NewTable ' .
The dataframe I want to append : #CODE
I try to append the dataframe to NewTable .
But , I can append dataframes with new keys to NewTable without problem .
Is there a simple way ( e.g. , without looping ) to append Pandas dataframes to a sqlite table such that new keys in the dataframe are appended , but keys that already exist in the table are not ?
You can use SQL functionality ` insert or replace ` #CODE
it seems like all you want is a join
But in my situation , I use merge function before loop through all rows
It uses pandas loc function .
I merge 2 tables by school_id to get school_name in school_detail .
stack it first and then use value_counts : #CODE
If you want to translate this to a wide format #CODE
Python 2.7 Pandas : How to replace a for-loop ?
I then want to do summary stats on the observations and append those summary stats to a list .
I get all obvs , do summary stats on the obvs and append the results to the new dataframe .
So for each row in the dataframe , I want to go back 5 days , get the obvs , get the stats , and append the stats to a dataframe .
It is not a rolling windows operation .
Is it possible to use some merge or update function to do this ?
Use ` map ` : #CODE
@USER I added another example where we do no boolean masking and just set the sex column , this is nearly 5 times faster , so I think if you can either ensure there is consistent text strings or add additional keys to the map will optimise this method if you are worried about mixed case
Return multiple columns from apply pandas
For example , can I instead of returning one column at a time from apply and running it 3 times , can I return all three columns in one pass to insert back into the original dataframe ?
Groupby and aggregate operation on Pandas Dataframe
Firstly I would create the columns you wish to aggregate populated with zeros and ones , and then use groupby and do a simple sum of the values ...
Right now I am using this code to normalize my columns in the csv files so they can all have similar columns .
It will match and replace for example `' genUS '` , `' GENUS '` , `' Genus_666 '` #CODE
convert some rows in rows of a multiindex in pandas dataframe
Well , ` melt ` will pretty much get it in the form you want and then you can set the index as desired : #CODE
Now use melt to stack ( note , I reset the index and use that column as an ` id_var ` because it looks like you want the [ 0 , 1 , 2 , 3 ] index including in the stacking ): #CODE
If you don't need the [ 0 , 1 , 2 , 3 ] as part of the stack , the melt command is a bit cleaner : #CODE
The result of a resample places the time as the index of the dataframe .
Edited answer to make it conform to candlestick syntax .
I've tried a few methods - eg for each data frame replace the columns with a multi-index like ` .from_product ([ ' ABC ' , columns ])` and then concatenate along ` axis=1 ` , without success .
Add a symbol column to your dataframes and set the index to include the symbol column , concat and then unstack that level :
That works - the key concept is to add a column and then pivot / unstack etc .
You can do it with ` concat ` ( the ` keys ` argument will create the hierarchical columns index ): #CODE
Really ` concat ` wants lists so the following is equivalent : #CODE
You could try using DataFrame's ` apply ` .
Write a function that includes an exception handler and apply it to the DataFrame .
How to append data to a panel which is stored in HDFStore file
I have a Panel stored in a file , and I want to append more data to that panel
appending in memory works fine , but when trying to append data to the file I get this error : #CODE
So appending is allowed on any of the indices , hence you can append a new panel that has changed major and / or minor axes .
You can specify different axes to append if you would like , eg .
` axes =[ ' items ' , ' major_axis ']` or simply transpose the panel to get it in the form that you need .
This is a parameter that must be specified on the first append .
Also , I get an error on ` pd.MultiIndex.from_product ` : ` AttributeError : type object ' MultiIndex ' has no attribute ' from_product '` The method does not seem to exist .
You can loop through the ( alpha , epsilon ) indices using ` for i , j in np.ndindex (( len ( alphaRange ) , len ( epsilonRange )))` , and then do the calculation cell-by-cell , but this is likely to be slow way to perform the calculation and should only be done if finding the value at ` ( 0 , 0 )` is required * before * finding the value at ` ( 0 , 1 )` , etc .
you should avoid `` apply `` if you can vectorize , e.g `` df [ ' diff '] .where ( df [ ' diff '] .abs() > = 0.3 )`` will be much faster
We call that the first pivot .
We then check at every row if either the high or the low is higher / lower than first pivot with 0.3 .
Yes ... you are right , but the next minute / row 2014-05-09 09:35 : 00-04 : 00 we encounter a new higher high 187.71 and we forget about 187.67 and compute the diff from 187.71 backwards to the first pivot .
so i haven't tested this , but something like this will get you what you want . what happens if both the ` low pivot - diff ` and ` high pivot + diff ` in the same minute ?
Would not work ... because the trick is , as @USER -Kozela already identified , that you cannot mark a point as pivot until you find the next potential pivot ( ie if you are in an upward trend , you can't say it's done until you find a low lower by 0.3 ) .
It's a bit tricky since you cannot mark a point as pivot until you find the next potential pivot ( ie if you are in an upward trend , you can't say it's done until you find a low sufficiently low ) .
I made a few minor adjustments : 1.replaced " irow ( 0 )" with " iloc [ 0 ]" as it will be deprecated ; 2 . used iterrows() and made it a little faster ( 1.5 mil lines in 2m51s vs +5min ); 3 . replaced " row.low < pivot - diff " with " (( pivot-row.low ) / diff ) > 1 " and " row.high > pivot + diff " with " (( row.high-pivot ) / diff ) > 1 " because I was getting some swings below the threshold ...
Best way to set a multiindex on a pandas dataframe
I want to use MultiIndex to stack the data later , and I tried this way : #CODE
Supplying a list of columns will set a multiindex for you .
I've tried the join command , but it doesn't seem to preserve the dates .
What you want is called outer join and is controlled with ` how ` argument , see [ in docs ] ( #URL )
By default , pandas DataFrame method ' join ' combines two dataframes using ' inner ' merging .
Your join line should read : #CODE
You can use ` drop ` to retain your original data .
I've been going about it with what I think is a very roundabout way , i.e. making a new DataFrame for every ` Localization ` and working on from there , but there's a lot of lines and the problem of having to merge all the resulting DataFrames in the end .
The basic idea is to group data based on `' Localization '` and to apply a function on group .
your groupby can be collapsed down to : `` df.groupby ( ' Localization ') [ ' Size '] .transform ( lambda x : x / len ( x ))``
To answer your question , you * can * use `` df.values `` to get the underlying numpy array and process if you want , but you will then be responsible to translate back to a DataFrame ( if you want ) .
ok , that all looks vectorizable ( in general only a recurrent relation is NOT vectorizable directly , though sometimes they are possible , e.g. via shift / diff ) , but I understand your conundrum .
I want to merge old_close and new_close to form a new DataFrame that includes all the data in both the DataFrames but excludes all duplicate values on both indices .
I believe I can do this with combination of ` aggregate ` and ` transform ` in Pandas , but am not sure how .
The first example in the [ transform docs ] ( #URL ) is relevant very close to what you want .
I would like to group a ` DataFrame ` then apply ` myfunc ` along columns of each individual frame ( in each group ) and then paste together the results .
The docstring for aggregate specifies that it tries both of these , but does not specify the order in which they are tried .
then just iterate ( and maybe concat , you'll have to handle that yourself ) , e.g. `` concat ([ myfunc ( grp ) for g , grp in df.groupby ( ... ) ])``
ok , revised the answer ; you can do almost anything inside the apply FYI
There may be some problems with using dates as the ` x ` variable , so you might just want to make a dummy column with ` ( range ( len ( df ))`
` shift ` is the way to shift a column forward or backward , allowing you compare values in different rows .
` T ` is not defined in your example , and lines 4 + 5 also throw errors ( ` dt [ ' vc ']` , Indexing ) .
( its just a toy example , the real problem is getting a tuple out / in to lambda / map
As far as the unpacking goes , maybe you could let the function take an argument for whether to add or subtract , and then apply it twice .
And w / r / t speed , with len ( df ) = 10k and len ( ids_to_check ) = 20k , the try / except is about 2x slower .
Then I reindex to recover the original index and fill with the value for the missing entries #CODE
Well , I would probably convert it to a dataframe with the ` to_frame ` method and then transpose it ( example , using a slighty different series ): #CODE
You can assign an array to a column , as long as the length of the array / list is the same as the frame ( otherwise how would you align it ? ) #CODE
How are you supposed to align it to the frame ?
I don't see what data you want to merge .
The first merge works but does not contain any merged data ...
First your merge statements are not constructed correctly .
In your left merge , the left index is a single level , and you while you specify both ` right_index=True ` and ` right_on= ' event1 '` , the ` right_on ` attribute is taking precedence .
I should point out that the merge , if constructed correctly , ( ` pd.merge ( left , right , left_index=True , right_on= ' event1 ' , how= ' left ')`) does not produce an empty DataFrame ...
In your right merge , you specify using the right index with ` right_index=True ` and ` left_on ` takes precedence over ` left_index=True ` .
How can I merge the column ' Names ' so that I have all the names once and keep track of the corresponding data in the other columns ?
why are you trying to merge a dataframe with itself ?
in general , all you need to do is read in each sheet , set its index ( using ` set_index `) to be ' Names ' , and then merge from there with some sort of rsuffix based on the sheet name .
Pandas : Getting the mask associated with query
Given a query , is it possible to get the ` True ` / ` False ` mask that corresponds to the values returned by ` query ` ?
` inliers ` will hold the data that satisfied the query , but can I also get the mask of this query ?
Getting the mask can be useful , for example , to quickly negate the query , or to get a result of the same size of the original dataframe .
What I am looking for is a ` mask ` of the same dimensions as the original ` df ` , but with ` True ` or ` False ` for those values inside / outside the query .
Am I misreading his comment that " What I am looking for is a mask of the same dimensions as the original df " ?
I think so , this mask IS broadcastable
@USER what you are doing doesn't make sense , unless you want to use `` where `` ( which query / eval don't support ) .
`` where `` will give you a mask for each element , BUT , the way you setup the problem , `` values `` is a column and hence a row mask is enough ( and is broadcastable ) .
Can one save a pandas DataFrame to binary in " append mode " ?
Can one save a pandas DataFrame to binary in " append " mode , analogous to using mode= ' a ' in the to_csv() DataFrame method ?
I really dislike matplotlib and Bokeh requires too heavy of a stack .
In pandas I can set the date to be an index and use the shift method : #CODE
You could perform a groupby / apply ( shift ) operation : #CODE
I have a data frame in which I want to identify all pairs of rows whose time value ` t ` differs by a fixed amount , say ` diff ` .
For example , if ` diff = 22.2423 ` , then we would have a match between rows 4 and 7 .
The obvious way to find all such matches is to iterate over each row and apply a filter to the data frame : #CODE
Further , I want to look and check to see if any differences of a multiple of ` diff ` exist .
So , for instance , rows 4 and 9 differ by ` 2 * diff ` in my example .
If I can do this , then I can simply compare ` df.t ` , ` df.t - diff ` , ` df.t - 2 * diff ` , etc .
If you want to check many multiples , it might be best to take the modulo of ` df ` with respect to ` diff ` and compare the result to zero , within your tolerance .
In Pandas , is there an easy way to apply a function only on columns of a specific type ?
What is an easy way to apply a function only on columns of a certain type ?
And now you can use that list with an apply or whatever .
I'm trying to drop rows of a dataframe based on whether they are duplicates , and always keep the more recent of the rows .
This would be simple using ` df.drop_duplicates() ` , however I also need to apply a ` timedelta ` .
This table shows the rows that I need to drop in the ` Duplicate ` column .
However , one of the keyword arguments to pass is ` take_last=True ` or ` take_last=False ` , while I would like to drop all rows which are duplicates across a subset of columns .
As an example , I would like to drop rows which match on columns ` A ` and ` C ` so this should drop rows 0 and 1 .
Actually , drop rows 0 and 1 only requires ( any observations containing matched A and C is kept . ): #CODE
What I want is to drop all rows which are identical on the columns of interest ( A and C in the example data ) .
How can I " join " together all three CSV documents to create a single CSV with each row having all the attributes for each unique value of the person's string name ?
The ` join() ` function in pandas specifies that I need a multiindex , but I'm confused about what a hierarchical indexing scheme has to do with making a join based on a single index .
You don't need a multiindex .
It states in the join docs that of you don't have a multiindex when passing multiple columns to join on then it will handle that .
Merge df1 and df2 then merge the result with df3
One does not need a multiindex to perform join operations .
One just need to set correctly the index column on which to perform the join operations ( which command ` df.set_index ( ' Name ')` for example )
The ` join ` operation is by default performed on index .
That way , your code should work with whatever number of dataframes you want to merge .
There are a number of pivot operations I need to do to work with the data .
This can be achieved by a combination of ` stack / unstack / reset_index ` , but it appears to be really inefficient .
I haven't looked at the code , but I'm guessing a copy of the table is made on each ` stack ` / ` unstack ` , causing my 8GB system to run out of memory and start hitting the page file .
I don't think I can use ` pivot ` in this case either , because the required columns are in a multi-index
But it can be a variety of different transforms - the specific result is not important , but I'm rather interested in how to efficiently do pivot operations on multi-index dataframes .
Unstack essentially creates an enumeration of index x columns so it can create a huge memory space when you have a lot of columns and rows .
It gives a slightly smaller total space , in that you may have some zero entries in the original that are not here ( but you could always reindex and fill to fix that ) .
Groupby the ' level ' on the columns and apply f ; don't use apply directly , but just concat the results as rows ( this is the ' unstacking ' part ) .
However this creates dups ( on the price level ) , so need to aggregate them .
Essentially a similar result to yours could be achieved by reading the file with ` chunksize ` and modifying the algorithm to aggregate the results .
to aggregate the content of a list of csv files , i normally do : #CODE
return `` None `` as concat will ignore it
Most of the column names overlap ( i.e. are the same ) and the join just does fine .
Do I need to just stick with using a merge / join type solution instead of combine_first ?
To the extent the column names are the same , it will join ' inner ' or ' outer ' as specified .
This is partial string indexing , see here : #URL ; in this case its the same as if you specified ` df.loc [ Timestamp ( ' 2014-05-30 ')]` because its an exact match ( e.g. you have daily freq ) #CODE
In general its better to let pandas ' figure ' out the rhs alignment ( and like you are doing here , mask the lhs ) .
Pandas will always align the data , that's why this doesn't work naturally .
( 3 ) Or you could do it all in a function that was called by the ` groupby / apply ` : #CODE
SettingWithCopyWarning , even when using loc ( ? )
I found this description of how to resample a multi-index :
When using ` count ` , state isn't a nuisance column ( it can count strings ) so the ` resample ` is going to apply count to it ( although the output is not what I would expect ) .
You could do something like ( tell it only to apply ` count ` to ` value_a `) , #CODE
Or more generally , you can apply different kinds of ` how ` to different columns : #CODE
Prior to 0.14 it was difficult to groupby / resample with a time based grouper and another grouper .
pandas apply function that returns multiple values to rows in pandas dataframe
I would like to apply a transformation to each row that also returns a vector #CODE
This is beacause apply will take the result of myfunc without unpacking it .
To do this , use the " cut module " and add a new column called bins containing the generated intervals .
We can use the ` aggregate / agg ` method with a lambda function to do this : #CODE
I've got two DataFrames I want to concat .
I want to concat these columns while keeping the granularity of the float column .
Basically how can I concat these while keeping 124.028125 as the value for Row1 ?
I did df.ix [ 0 ] and saw there was still no precision , so I assumed that it had somehow cut off the precision .
pandas , apply string operation to column should be string type , but has missing values ( np.nan )
When I try to truncate each string in that column based on certain condition : #CODE
Merge a lot of DataFrames together , without loop and not using concat
I have > 1000 DataFrames , each have > 20K rows and several columns , need to be merge by a certain common column , the idea can be illustrated by this : #CODE
Does it even make sense to merge it , then ?
Furthermore , it is actually 2X slower than the loop version , while the loop version is already 2X slower than the ` concat ` version .
If it matters which dataframe the data appears to be in , just store that information as an extra column and concat without bothering with the index .
How to normalize data efficently while INSERTing into SQL table ( Postgres )
In order to normalize I would have to check for the distinct values of event_type in the raw log and insert them into the event_types table .
Yes , you should always normalize and to the so-called 3rd Normal Form ( 3NF ) .
You can then invoke this function as a simple ` SELECT ` statement , which will return the ` id ` of the newly insert log message : #CODE
c ) Normalization doesn't mean " replace text with ID numbers " .
Sorry , you want ` cut ` not ` GroupBy ` #URL
groupby the ' c ' column , and consider all the columns that you want passed to the ` apply ` EXCEPT for c ( this is what ` df.columns - [ ' c ']` does , as normally the grouping column IS passed to the apply .
updated for the unstack ; good thinking !
moving average or rolling mean pandas without any window size
How do i calculate a rolling mean or moving average where i consider all the items that I have seen so far .
Specifying a window will mean that i will get the first few as Nan and then it only does a rolling window .
Hos to use the merge function to merge the common values in two DataFrames ?
I have two DataFrames , I want to merge on the column " Id "
I wish to keep all values from both the df s but merge the duplicate values into 1 .
You could concatenate the DataFrames , groupby ` Id ` , and then aggregate by taking the first item in each group .
I have a two column dataframe df , each row are distinct , one element in one column can map to one or more than one elements in another column .
So in the final dataframe , one element in one column only map to a unique element in another column .
And then merging the intersection leaves you with rows that only meet both conditions : #CODE
Note , you could also do a similar thing using ` groupby / transform ` .
But transform tends to be slowish so I didn't do it as an alternative .
Yes , I did it with groupby / transform . thanks for your answer , I always wonder which one I should use .
Try using apply with a custom function over axis=1 : #CODE
Just specify the columns when you apply the function , like this : ` df [[ ' col1 ' , ' col2 ']] .apply ( ... )`
This should be pretty fast ; I think even the use of map here will be a Cythonized call .
This should still be pretty fast because it uses ` pandas `' vectorized string methods for each of the columns ( the apply is across the columns , not an iteration over the rows ) .
Jeff gave a great answer there , but it fails if the group size is less or equal to parameter I pass in head ( parameter ) when concatenating my rows as in Jeffs answer : In [ 31 ]: groups = concat .....
It doesn't truncate * anything* .
is there something analogous for this for read_excel to alter all unicode column names and strip random whitespace ?
Sorry in advance if this is a little long winded but if I cut it down too much the problem is lost .
And this could have been cut down much more with out losing the problem . strip out anything involving pandas , use synthetic data .
Please edit your question to strip out all of the pandas calls and pass around ` np.random.rand ( 50 )` as your data .
I want to replace python None with pandas NaN .
The alternative is to use [ Unidecode ] ( #URL ) to replace non-ASCII codepoints with reasonable ASCII equivalents and surrogates .
Ditto for U+2019 , which is a fancy single quote , `'` could replace it .
I have tried using a mask as follows :
So after playing a bit with this I came up with the following , but still can not concatinate / append .
It is important to note that whatever solution you find to append to your dataframe , you will not be able to avoid a full copy of your dataframe unless you have preallocated memory by defining an empty frame of sufficient size which you fill up sequentially .
I am calling a function from within a ' for each loop ' which attempts to insert values into a Pandas DataFrame based on a specified column start and end location .
e.g. with values of srowb = 1 and erowb = 18 it will generate a list ( tmp_brollb ) which has either len ( tmp_brollb ) = 17 or len ( tmp_brollb ) = 18
` isnull ` and ` notnull ` work with ` NaT ` so you can handle them much the same way you handle ` NaNs ` : #CODE
just use ` isnull ` to select : #CODE
To drop duplicates you can do one of the following : #CODE
How to create rolling percentage for groupby DataFrame
You can do that with a ` groupby / apply ` by grouping on ' product_desc ' and then using the built in ` pct_change() ` method : #CODE
Or you could use shift as you suggest with a small modification : #CODE
However , when I apply the sort they don't sort the same way .
Detailed Error stack is as shown below #CODE
I tried index_reset() to remove any multiindex .
You're essentially trying to index with a dataframe of boleens and ` .loc ` doesn't know what to do ( it tries to use it as a multiindex ): #CODE
However , I've experienced that shift does not yield very good performance .
well , shift is a vectorized operation , so it should outperform other methods ; if you have a specific performance issue pls post that .
I could modify both the DataFrame and the original Panel by selecting by label in the call to ` loc ` .
The documentation says that ` loc ` is strictly limited to labels , so I am not sure the code you have posted can work at all : I got a ` KeyError : ' the label [ 0 ] is not in the [ items ]'` when pasting the example into an IPython console ( ? ) .
@USER as the documentation states you should * only * assign values using the multi-axis indexers , `` ix / iloc / loc `` ; slicing and selecting * may * work ( and only in a single-dtyped case ) , but in general is not robust and should be avoided .
A quick look at the code shows that the ` freq ` parameter of the ` TimeGrouper `' s ` __init__ ` method gets converted into a ` DateOffset ` by the ` to_offset() ` function .
You generally just ` resample `` #CODE
their is another way , specify a tuple ( see above ); I guess you could put forth an enhancement to add a { name : value } to be accepted as well ( as the freq parameter )
You can do this , but since the right-hand-side ( the assignee ) is not labeled , it will just assign the first len ( assignee ) values #CODE
you should probably have to add a " mode= ' a '" argument in the " to_csv() " command , to say you want to write to the file in append mode ( otherwise the mode defaults to ' w ' , which indeed overwrites the previous content ) #CODE
Now I find that the class of ` a ` and the class of ` b ` are different ; ` b ` is a ` pandas.core.series.Series ` object and therefore you can not apply the method ` append2 ` to it .
And this would be more efficient that appending twice ( as an append copies ) .
I'm not quite sure how to apply the df.groupby syntax or the df.apply syntax to what I'm working with .
Here is a neat apply trick .
How does it know which axis to apply the value_counts to ?
I have N number of data frames ( e.g. df1 , df2 ) which consist of 1s and 0s and I am trying to find a way to use Pandas to multiply them all together based on a 3-dimensional index in order to find the intersection of them ( i.e. result ) .
They will align each time ; including the fill_value=1 will propogate the values when multiplied vs a NaN ( which is what I think you want ) .
You can reformat the values by passing a reformatting function into the ` apply ` method as follows : #CODE
@USER always worth checking different approaches usually you cannot beat using ` loc ` and numpy functions
I've added the version information and also the dtypes for the conflicting append .
So , you need to conform your dtypes in EVERY chunk to be the same .
you need to conform the dtypes before you write them to HDF , look at df.dtypes of each chunk before you are writing them . some data is change them ( e.g. you may have missing values in one but not in another ) .
How can I do an interpolating reindex in pandas using datetime indices ?
I have a series with a datetime index , and what I'd like is to interpolate this data using some other , arbitrary datetime index .
` reindex ` doesn't do what I want because it can't interpolate , and also my series might not have the same indices anyway .
` resample ` isn't what I want because I want to use an arbitrary , already defined index which isn't necessarily periodic .
I've also tried combining indices using ` Index.join ` in the hopes that I could then do ` reindex ` and then ` interpolate ` , but that didn't work as I expected .
After ` interpolate ` I could then use ` reindex ` to get back down to just the desired index .
I would like to merge two dataframes on time , but instead of merging exactly I would like to get the max ( time1 ) = time2
I think that pandas index.searchsorted is the best way to do this but I can't quite figure out how to get the merge to work
use ` reindex ` : #CODE
Pandas resample with all day range
I use code below to resample data : #CODE
But I want the whole day range from 00:00 : 00 to 23:00 : 00 , how can I change the resample date range ?
It is DateTime or I can not resample it .
Well , you ccan ` reindex ` after the resample using a hourly period index for the whole day .
I don't understand why you do ` crosstab ` and drop all the columns from ` Forma ` , maybe you want ` Series.value_counts ` . matplotlib can't do real 3D plot , try ` visvis ` .
Default is to open in ` mode= ' a '` ( append mode ) .
no , no automatic way to do that ( though you could write a function to take the max abs value of an int column and figure it out )
Next , for the duration , you can use ` diff ` for each group : #CODE
Python Pandas join dataframes on index
I am trying to join to dataframe on the same column " Date " , the code is as follow : #CODE
I'd like to set " Date " in both dataframe as index and I am wondering what is the best way to join dataframe with date as the index ?
This generated dataframe contains all weeks in the range so I need to merge those two dataframe into one .
Well it doesn't work because you set the ` index_col ` to date when you read the csv , you can either not set the index_col to ' Date ' OR set the ' Date ' col in df_train_fly as the index column also and pass ` left_index=True , right_index=True ` to the join
Actually you cannot pass left_index=True etc .. unless you do a ` merge ` so if you want to use join then drop the ` index_col= ' Date '` param in ` read_csv `
I think in your case if you did this it should work : ` merged = df_train_csv.join ( df_train_fly , how = ' right ' , lsuffix= ' _x ')` so drop the ` on =[ ' Date ']` param as this is for specifying a column to join on , if you leave this out the default is ` None ` which will use the index to join on
I used the other option that you mentioned : not setting the ' on ' param and let them join on index automatically , which is working !
I have posted the full data and I am still wondering how it works if we need to join on more than one field except index ?
Because when omitted , the index is used as join condition , but when other columns are specified , I think index is not used .
If you want to use the index and columns then you cannot use join for that , you have to use merge , it would be easier to not set ' date ' as an index and then merge on the multiple columns so like ` merged = df_train_csv.merge ( df_train_fly , on =[ ' Date ' , ' other_col '] , how = ' right ' , lsuffix= ' _x ')`
So the above join will not work as the error reported so in order to fix this : #CODE
the above will use the index of both df's to join on
You can also achieve the same result by performing a merge instead : #CODE
But it gives me very strange results ( all columns are NaN and date from df_train_csv is NaT ) and I guess the date in df_train_csv is in a different format from that of df_train_fly , because when I use inner join , an empty set will be returned .
Then apply : #CODE
In order to not Chain index , I would need ix - so would you do this while iterating row by row : df.ix [ df.index [ i ] , ' some_col '] =some_value
no , simply use ix it will figure out if u pass it integers ( and don't iterate - always better ways )
I want to add a multiindex header to categorize each columns .
` In [ 9 ]` , where I create the top level of the MultiIndex can be made in a bunch of different ways .
If you have more cols you can do something like ` np.arange ( len ( df.columns )) .repeat ( 2 )`
pandas groupby and join lists
I have a dataframe df , with two columns , I want to groupby one column and join the lists belongs to same group , example : #CODE
what does the function in df.groupby ( ... ) .apply ( lambda x : ... ) apply to ?
` object ` dtype is a catch-all dtype that basically means not int , float , bool , datetime , or timedelta .
However I am not able to merge these 2 columns properly .
groupby , apply , and set not behaving as expected ...
Now my attempt to concat letters produces a funny result : #CODE
[ @USER ] ( #URL ) , why does ` dat.groupby ([ ' names ']) [[ ' letters ']]` pass all the columns through to apply ( letters , numbers , names ) ?
Calculate rolling time difference in pandas efficiently
You don't need the final apply , see here : #URL you can simply `` astype ( ' timedelta64 [ D ]')`` or divide by `` np.timedelta64 ( 1 , ' D ')`` ( they are sligthly different in how they round .
I think I'd use ` diff ` here : #CODE
If you are just subtracting the first in each group , use a transform : #CODE
Based your code ( your ` groupby / apply `) , it looks like ( despite your example ... but maybe I misunderstand what you want and then what Andy did would be the best idea ) that you're working with a ' date ' column that is a ` datetime64 ` dtype and not an ` integer ` dtype in your actual data .
Given that you should get some speed-up from just modifying your apply ( as Jeff suggests in his comment ) by dividing through by the ` timedelta64 ` in a vectorized way after the apply ( or you could do it in the apply ): #CODE
But you can also avoid the ` groupby / apply ` given your data is in group , stage , date order .
Edit : As Andy points out you could also use ` transform ` : #CODE
So I think avoiding the apply could give some significant speed-ups
I think you may be able to do this more efficiently using a transform .
Yeah @USER , I thought about ` transform ` but at least for 0.13.1 I usually find transform no faster than a generic ` apply ` so I didn't include it .
@USER , I'm getting ` transform ` as much slower .
I thought Jeff had mentioned some performance issues with ` transform ` but maybe I am misremembering .
I'm also new to posting on stack over flow , so any tips on how to display a table in it would definitely be appreciated as well .
I will need to reshape my results back to the row x col dimensions of the original image so I can't just drop the all zero rows .
I would like to mask those rows for some calculations but I need to keep the rows .
I can't figure out how to mask all the entries for rows that are all zero .
` df.any ( axis=1 )= =True ` gives me ` True ` for the rows I'm interested in and ` False ` for rows I'd like to mask but when I try ` df.where ( df.any ( axis=1 )= =True )` I just get back the whole data frame complete with all the zeros .
It is a little unclear what you are saying , when you create the mask it is just a mask , the other rows are not removed from the original df .
Basically , I've been able to subset the data successfully but I haven't been able to mask it .
I'm really just having a hard time generating a mask of the correct dimensions .
I need to make a mask with the dimensions of the df using attributes of the rows rather than individual entries .
Alternatively you could create a list of the index points you want to mask , and then make calculations using numpy masked arrays , created by uising the np.ma.masked_where() method and specifying to mask the values in the index positions : #CODE
You should use ` loc ` and select the column of interest ' C ' in the square brackets at the end
You essentially want to aggregate the data grouped by column ' B ' .
To aggregate column C you will just use the built in ` sum ` function .
how do merge two dataframe in pandas
I want to merge two dataframe based first column .
How to merge df1 and df2 having first column as index
See #URL or google ` pandas merge `
From the documentation on how to merge , it looks straightforward : #CODE
I take a multi-indexed pandas series named ' dat ' and try to append it to an empty series named ' s ' .
Now , when I append it to the empty Series ' s ' it returns this : #CODE
You can get the dataframe you want to merge with : #CODE
To get the result you posted ` merge ` #CODE
IIUC -- unfortunately your code doesn't run for me with your data and you didn't give example output , so I can't be sure -- you're looking for ` merge ` .
or maybe an outer merge , to make it easier to spot missing / misaligned data : #CODE
When I create a pivot table on a dataframe I have , passing ` aggfunc= ' mean '` works as expected , ` aggfunc= ' count '` works as expected , however ` aggfunc =[ ' mean ' , ' count ']` results in : ` AttributeError : ' str ' object has no attribute ' __name__ `
How do I create a pivot table with multiple functions ?
The dataframe is 6k rows long- I'm looking for the counts ~600 codes ( rows in the pivot table ) , over the course of 15 months ( cols ) .
As of now , I'm sending a mean pivot table and a count pivot table to csv's , then combining in excel .
also , one last suggestion , try ` len ` ?
` aggfunc =[ np.mean , len ]`
Of course you don't show your data so this may or may not apply and I apologize if not , but for a large dataframe I'm dealing with I see a speedup of , well , 24 million times !
I've made things simpler in showing them , but " a " is actually a multiindex slicing on multiple axes in my application and tests , as is df2 .
I would like to compute on a new column the rolling cumulative sum within the group so that the output is : #CODE
I think all you need to do is use the ` level ` argument with ` groupby ` ( as described in the groupby with multiindex part of the tutorial ): #CODE
Is this technically a ` transform ` or ` apply ` operation ?
` apply ` is the most general category of operation on a group , so lots of things fall under its umbrella .
What distinguishes transform operations is that they produce something indexed like the input , and that happens here , so I guess you could think of it as a transform .
I am new to Python and am uncertain why I am seeing memory usage spike so dramatically when I use Numpy ` hstack ` to join together two ` pandas ` data frames .
As shown in this thread it is not possible to append an array in place and this would not be efficient since there is no guarantee to keep the extended array continguous in memory .
Call unstack twice : #CODE
Calling ` df.T ` would work for that case rather than calling ` unstack ` twice and I think you cannot use ` unstack ` in the case of a multi-index
For some reason , I was thinking the mask cannot be applied without a column reference .
Well , one approach is the following : ( 1 ) do a ` groupby / apply ` with ' id ' as grouping variable .
( 2 ) Within the apply , ` resample ` the group to a daily time series .
( 3 ) Then just using ` rolling_sum ` ( and shift so you don't include the current rows ' x ' value ) to compute the sum of your 70 day lookback periods .
Now we can do the ` groupby / apply ` : #CODE
It just becomes the next parameter in the ` apply ` .
No need for the ` lambda ` on the first one : ` apply ( ' { : 0 > 15} ' .format )` should work too .
as the apply function creates only one column with tuples in it .
You can put the two values in a Series , and then it will be returned as a dataframe from the apply ( where each series is a row in that dataframe ) .
I would drop to numpy to do this a little faster : #CODE
Does it cause an error or just fail to replace the character without spitting out an error ?
If all you're concerned about is the ` ` , you should decode in UTF-8 , and then just replace the one character .
The replace value has to be like ` u ' \xc9 '`
You can do a similar trick if the timestamps are part of a MultiIndex by extracting that " column " and passing it to DatetimeIndex as above , e.g. using ` df.index.get_level_values ` :
Anyhow , the following " if " statement doesn't appear to like the boolean mask , since it throws the same error even if I use a.bool() like it wants :
EDIT : Fixed my typo ( if indf row [ ' MASK ']) , but now getting ...
' MASK ' is boolean .
But why is it treating the bool as str ?
AttributeError : ' str ' object has no attribute ' bool '
But seems that you simply want to resample ; so do this .
Pandas DataFrame map string path in to integer path
I want to map all these strings in path to integers .
@USER Cunningham : There is no logic , need to map strings to unique integer value .
I want to map those tags to integers like " math " : 1 , " literature " : 2 .
You can feed a ` dict ` with the strings as keys ( ' math ' , etc ) and the integers as values into the ` map ` method .
You could also use ` factorize ` to accomplish much the same thing but you wouldn't control the mapping from string to integers ( although in this example it ends up being the same ): #CODE
Use ` to_datetime ` to convert to a string to a datetime , you can pass a formatting string but in this case it seems to handle it fine , then if you wanted a date then call ` apply ` and use a lambda to call ` .date() ` on each datetime entry : #CODE
Say I have a MultiIndex containing of two integers , and I want to select #CODE
Merge two DataFrames by column of sets
I would like to merge ` df2 ` to ` df1 ` by ` name_tokens ` to get ` ranking ` .
How should I customize my merge method to accomplish this ?
( I have millions of records in ` df1 ` , and it can be many to one merge to ` df2 ` , so a fast implementation is desirable ) .
Once you've separated out ` name ` and ` type ` getting the rank is a very simple join .
agggenfreq =d f2000 [[ ' freq ' , ' name ' , ' sex ']] .groupby ([ ' name ' , ' sex ']) .sum() [ ' freq ']
data = pd.DataFrame ( { ' agg ' : aggfreq , ' name ' : aggfreq.index } )
Maybe that same logic does not apply here , although I have seen people do that for selecting rows .
In this case , I might use ` endswith ` : #CODE
Edit : I initially was calling the intersection the union , but have since changed this .
Well , one way to do this is using ` isin ` ( but you can also do it with the ` merge ` command ... I show examples for both ) .
You can also do this with ` merge ` .
When you merge , the unique rows from the larger dataframe will have ` NaN ` for the column you created : #CODE
So select the rows where test == NaN and drop the test column : #CODE
Edit : @USER notes that the merge method performs much better for large dataframes .
I tried both and didn't get a solution after 10 minutes of sorting through dataframe with 80,000 rows and 5 columns , but it only takes a few seconds for the latter merge method .
Thanks @USER , I edited the answer to indicate the better performance of the ` merge ` method .
Perhaps replacing the ` map ` s with a single , looped one , would work .
@USER I tried to do that , but now I cannot map the functions to the result of split in the code above .
Also preallocate your arrays / dataframes with the fixed size you know you'll need , don't dynamically do concat / append whenever you can avoid it .
Then append / concatenate / update the contents of ' Desc ' in the first row with the contents of ' Desc ' in the second row if the rows are duplicates .
drop it ?
If you want one column ' desc ' which has all the values in it ( which is what I gather from your question , then you can just group the data on ' date ' and aggregate it by joining all the ' desc ' together into one string : #CODE
pandas dataframe concat with ' NA '
OR if you want concat , when you define ` df_a ` , use columns of df as columns #CODE
python pandas HDFStore : how to append dataframe containing complex numbers
Yet when I would map a lambda function to this Series of lists , I get a ` TypeError : ' float ' object not iterable ` .
To avoid this error , replace the NaNs in ` treatments [ ' DIAGNOS ']` : #CODE
Instead of ` im =d f_train.Im_as_np [ 0 ]` use ` ix ` to access data : #CODE
Python - Aggregate by month and calculate average
And my goal is to aggregate date by months and calculate average of months .
Probably the simplest approach is to use the ` resample ` command .
And if you want a month counter , you can add it after your ` resample ` : #CODE
Using ' apply ' in Pandas ( externally defined function )
I think I'm not really understanding how apply ( and its cousins , aggregate and agg ) works .
You could group by year , isolate the prop column , apply ` argmax ` , and use ` loc ` to select desired rows : #CODE
Note that the use of ` argmax ` and ` loc ` rely on ` df ` having a unique index .
Note that ` argmax ` is an ` O ( n )` operation , while sorting is ` O ( n log n )` .
` argmax ` returns the * label * of the row , not the integer index .
Note that in 0.14.0 ( releasing shortyly ) , you can bypass most of this with `` nlargest() / nsmallest() `` , rather than explicity sorting ( or using argmax ) , see here ( its down a little bit ): #URL
AttributeError : Cannot access callable attribute ' info ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method
No worries , the given solution still applies , you just have to call it inside of an apply of your groupby : ` df.groupby ( bla ) .apply ( lambda df_grouped : df_grouped.groupby ( level =[ 0 , 1 , 2 ]) .apply ( fancy_func ))`
The ` level ` argument lets you group by a level of a multiindex and works for both ` Series ` and ` Index ` objects .
Why do you drop the indices ?
In my real world problem , I need to use the unstack df as there are more complicated comparisons necessary
It's , however , hardly comprehensible for me , that I have to sort first in order to be able to get access to the rows via a MultiIndex .
What I often do is sort so that I can use ` transform ` + ` iloc ` to pick out the appropriate value , something like #CODE
I'm trying to create a boxplot using Plotly and I get an error when attempting to use a Pandas DataFrame that's been grouped .
The above code is for line graphs , but I'd like to iterate over my dimensions to produce a boxplot for each group in my DataFrame .
I'd like to plot a boxplot for each county ( representing the min / med / max , etc . of the scores in that county ) .
The only plotly boxplot example I find by googling doesn't make any mention of a Series .
Does a pandas dataframe , which is effectively a grid , have the ability to add an item into a specific row and column intersection ?
Yes , use ` loc ` , ` iloc ` , or ` ix ` .
how to apply Functions on numpy arrays using pandas groupby function
pandas indexing in multiindex dataframe
making a multiindex : #CODE
You can use the ` concat ` function : #CODE
I would generally not repeat and / or append , unless your problem really makes it necessary - it is highly inefficiently and typically comes from not understanding the proper way to attack a problem .
I wanna insert this line in it , so what Im gonna do is to repeat this line for N times , and append it to the original data frame , then resort it .
Simply open the output file in append mode : #CODE
Reshaping two-column data using pandas pivot
When you have this column , you can use ` pivot ` : #CODE
@USER I get this error when trying to apply . why might this be ?
values = np.random.normal ( 10 , 2 , len ( date ))
df [ ' set '] = ( df [ ' date '] .diff() < 0 ) .cumsum() #counts each jump back to the first date ( -ve diff = True = 1 )
And do you get this with the last step ( the pivot ) ?
in python pandas , how to merge two frames side-by-side ?
is there a way to conveniently merge two data frames side by side ?
or maybe there is some special parameter in append #CODE
possible duplicate of [ Pandas join / merge / concat two dataframes ] ( #URL )
You can use the ` concat ` function for this ( ` axis=1 ` is to concatenate as colomns ): #CODE
You can use ` isnull ` : #CODE
Now I want to read data from database and want to insert specific column values from the database to this dataframe .
If you constructed an index that is a hash of the full row , you could simply do an ( inner ) join of the two data frames .
Not that elegant , but I think this should work : concat ` all_combinations ` and ` valid ` , and then drop all duplicates : #CODE
Is there a way to do a weight-average rolling sum over a grouping ?
I want to apply a weighted sum to a DataFrame .
I want to apply a weighted average to the sum where the most recent is multiplied by 0.6 , 2nd by 0.2 , 3rd and 4th by 0.1 .
I think you can do it will a ` rolling_apply ` within a function called by a normal ` groupby / apply ` .
I need to drop the rows from all arrays if any of the arrays have a nan or inf on that row .
for example , if the first row of A has a nan or inf , I need to drop the first row of A , B , C
I don't think you can be anymore memory frugal than to generate a mask of the rows that you want to keep ( either boolean or a list of indices ) , and make a copy of each array , ` A = A [ I , :] ` .
Once you've constructed the mask , I don't see how a ' batch ' operation would save memory overhead .
I want to insert the name and student count from second into first but based on date column .
You can eg use the ` merge ` function ( see the docs on merging dataframes : #URL ) .
Above it uses the common columns to merge on ( in this case ` date ` and ` sitename `) , but you can also specify the columns with the ` on ` keyword ( see docs ) .
Well , I'm going to go out on a ledge and show you how to merge with suffixes : #CODE
If your times are not going to be equally spaced apart , you will need to reindex your dataframe with a TimeSeries index .
Sorry , ( now nuked ) comment was wrong , the correct kwarg is ` align ` .
But in that case I think using ` np.arange ( len ( df )) + 0.5 ` instead of ` df.index + 0.5 ` will work .
@USER yep np.arange ( len ( df )) + 0.5 works - terrific !
If I do this : plt.legend ( loc = ' lower left ') The new legend still consists of price and cost only ..
I did find something about loglog plots in the matplotlib documentation , but I don't know how to apply this to a Pandas dataframe .
I am using DataFrame for my data as I want to use group function to split the data and apply the above function .
Probably subindexing is fine in this case but you could have used ` loc ` also would work : #CODE
Merge rows of a dataframe in pandas based on a column
How to merge the rows based on date and sum up the count for the same date .
You're probably confusing the index ( which in this case is a multiindex ) with a column .
You could also use ` isinf ` without using it to replace values , e.g. ` x [ ~ np.isinf ( x ) .any ( axis=1 )]` .
I think you can simply using ` groupby ` and ` cut ` to group the data into time intervals .
I've also tried using the ix and groupby functions , but I can't seem to get either to work .
However this results in ` ValueError : Could not construct Timestamp from argument type ' bool ' `
Use indexing instead of apply , it's much faster : #CODE
Explanation in the docs : #URL another example on SO : #URL Probably in this case it won't be a problem ( but eg if you switch ` [ " new_column "]` and ` [ is_null ]` order , it could be a problem ) , but because it sometimes causes problems , and it is not always easy to tell when / when not it will cause problems , it is better to try to use the ` loc ` idiom to prevent this .
Since you've got to do the ` apply ` anyway , I think it is cleaner to move the ` json.loads ` and column creation all into the apply ( don't use the ` converters ` for ` read_csv `) : ` df [ ' field3 '] .apply ( lambda x : pd.Series ( json.loads ( x )))`
it's your testing that's flawed . when you resample , you're ending up with a bunch of ` np.nan ` values when there is no hourly data for the day .
I want to know how to replace sub-string with pandas , like ' SUBSTR ' in SAS .
But I couldn't apply to my specific problem .
This may not be general ( or efficient ) enough but one thing you can do as a work around is pass a ` MultiIndex ` to the ` groupby ` .
One solution is to turn the Series into a DataFrame , join to the grouper DataFrame , then groupby on the columns of the grouper then reselect out the columns of the grouped .
From this dataframe , I want to construct a dataframe that details all the money awarded in a single year , for making a boxplot : #CODE
Is there any reason you don't just use the pandas boxplot method ?
I'd do this in a two-step process : first add a column corresponding to the index in each year using ` cumcount ` , and then ` pivot ` so that the new column is the index , the years become the columns , and the money column becomes the values : #CODE
Thank you for introducing the dataframe pivot operation --- I guess I should start to RTFM , but SO turns out to be easier and faster in 95% of all cases .
You could sort the index in " human order " and then reindex : #CODE
I have an non-empty Excel-file where I want to insert a dataframe .
How can I only " append " it to that place , in a way like copy and paste , without deleting everything else ?
Would it be best to go through the csv file and replace all " without a comma before or after with ' ?
You can use ` loc ` : #CODE
Although unwieldy using ` loc ` will scale better with larger dataframes as the apply here is called for every row whilst using boolean indexing will be vectorised .
I don't think using loc works , since boolean masking always produces a copy of the DataFrame , rather than a view .
That is only true if the column already exists I think as I get no warning unless the column exists , I did some timings and using ` loc ` method is 1.66ms for a data frame size of 3000 rows versus the apply method which takes 60.2 ms
Notice how there's no z column using the loc method ?
I've added a way you can use ` loc ` now , the key is to do the first assignment to the new column which will produce ` NaN ` s and then you can do the boolean masking again and select the column in ` loc `
@USER you don't need to makes the rows on the rhs side , e.g. you can just do `` df [ ' y ']`` ( as it will align the lhs and rhs ) .
Also no need to create z before hand ( and the loc will create it ) .
foo.loc [ foo [ ' Country '] == ' Canada ' , ' z '] = foo.loc [ foo [ ' Country '] == ' Canada ' , ' y ']` with and without using ` loc ` on the left hand side
foo [ ' z '] = foo.loc [ foo [ ' Country '] == ' Canada ' , ' y ']` so the first assignment is overwritten which is why I used a boolean mask in my answer
@USER yes , as always you are right I will update my answer , thanks for the explanation , I thought you needed to mask on the right hand side too , didn't think it would be clever enough to align correctly
Efficiently joining two dataframes based on multiple levels of a multiindex
I frequently have a dataframe with a large multiindex , and a secondary DataFrame with a multiindex that is a subset of the larger one .
Here is an imaginary example , where I want to join df2 to df1 : #CODE
Needing to reset the index , then recreate the multiindex , makes this simple operation seem needlessly complicated .
A better method is to build up smaller frames , then do a larger merge .
There appears to be a quirk with the pandas merge function .
It considers ` NaN ` values to be equal , and will merge ` NaN ` s with other ` NaN ` s : #CODE
Is there a way to ignore missing values during a merge without first slicing them out ?
You could exclude values from ` bar ` ( and indeed ` foo ` if you wanted ) where ` id ` is null during the merge .
( I've assumed from your left join that you're interested in retaining all of ` foo ` , but only want to merge the parts of ` bar ` that match and are not null . ) #CODE
How do I apply a function to a pandas dataframe ?
I have tried to apply a function to a pandas dataframe like this #CODE
You are applying a function row-wise but then sub-indexing the entire dataframe , so change all references to fogo [ ' TMax '] etc .. to x [ ' TMax '] , however , what you are doing could be achieved by boolean indexing by generating a mask so ` fogo.loc [ fogo [ ' TMax '] > 24 , ' Causa '] = ' a '` etc ..
So the first problem in your code is that you are calling apply and setting param ` axis=1 ` this applies your function row-wise which is fine .
However , seeing as you are just setting three values for 3 different conditions then you could just use boolean indexing to create a mask and just set all rows that meet your conditions .
Right now I get multiple rows for say `' asset subs end dt ' = 10 / 30 / 2008 ` and `' reseller csn ' = 55008 ` if the dummy variable comes up 3 times .
when you do this ` groupby ` , everything with the same index is grouped together . assuming your data in ` EXPERTISE ` is ` notnull ` , you will get a new ` DataFrame ` returned with unique index values and the ` count ` per each index . try it out for yourself , play around with the results , and see how it can be combined with your existing ` DataFrame ` to get the final result you want .
cf.register_option ( ' large_repr ' , ' truncate ' , pc_large_repr_doc ,
validator=is_one_of_factory ([ ' truncate ' , ' info ']))
validator=is_one_of_factory ([ ' truncate ' , ' info ']))
( And I need to apply the same operation to many similar dataframes .
show df.info() , and the len of ID_list .
The Series must also have a ` name ` to be used with ` join ` , which gets pulled in as a new field called ` name ` .
You also need to specify an inner join to get something like ` isin ` because ` join ` defaults to a left join . query ` in ` syntax seems to have the same speed characteristics as ` isin ` for large datasets .
If you're working with small datasets , you get different behaviors and it actually becomes faster to use a list comprehension or apply against a dictionary than using ` isin ` .
But can how can I apply Cython to pandas ?
@USER , the link I provided gives a pretty good intro into using Cython with pandas : #URL I would probably try to use join or hash tables before you get into that though .
Summarizing the comments , for a dataframe of this size , using ` apply ` will not differ much in performance compared to using vectorized functions ( working on the full column ) , but when your real dataframe becomes larger , it will .
Indeed , I get 201us ( np ) vs 208us ( math ) , so almost the same for this dataframe , but for a larger one ( this one 100 times repeated ) , numpy is clearly faster than using apply .
Also for the concatenation , for this dataframe , using apply is not slower ( even a bit faster 500 vs 700 us ) , but for larger dataframes ( 7000 rows ) it is again clearly slower ( 200 vs 80 ms ) .
Regarding to the performance , I just notice if I use the vectorized functions , I may cause a memoryError ( I have 3G ram ) but apply does not have such a problem .
When using ` apply ` the id generation is performed per row resulting in minimal overhead in memory allocation .
yeah , this way works , but in this thread , #URL it is said the vectorized function is faster than using apply call , and from my experiments it seems true .
The vectorized functions tend to use more memory than apply call , but the confusion is that I still have lots of memory left when the memory error occurs
Also timed the apply solution and it takes about 5 minutes with 500k rows
So , maybe try to upgrade your pandas version , as the vectorized way is much faster as the apply ( 5s vs > 1min on my machine ) , * and * using less peak memory ( 200Mb vs 980Mb , with ` %memit `) on 0.14 .
So , maybe try to upgrade your pandas version , as the vectorized way is much faster as the apply ( 5s vs > 1min on my machine ) , and using less peak memory ( 200Mb vs 980Mb , with %memit ) on 0.14
Can I apply a function that uses ' shift ' on a grouped data frame , and return a simple data frame from pandas ?
I'm trying to get percentage change on each ticker date combination , for multiple lag ( shift ) dates .
The generated ` MultiIndex ` of ` table ` seems to be fine : #CODE
The link I posted shows the format of the CSV you need and how to get a MultiIndex in the columns directly from a call to ` pandas.read_csv `
( the number of rows within each group is irregular , so i can't simply cut the group )
You can use the ` TimeGrouper ` function in a ` groupy / apply ` .
Or an example with an explicit ` apply ` : #CODE
It works because the groupby here with as_index=False actually returns the period column you want as the part of the multiindex and I just grab that part of the multiindex and assign to a new column in the orginal dataframe .
You could do anything in the apply , I just want the index : #CODE
I did it in a silly way by looping over groups then add the column to groups , and last concat the groups ...
Yes , you can do it in an apply .
Just do a groupby in a function that the first groupby / apply calls
Depending on what your doing if I understand the question right can be done a lot more easily just using the resample method #CODE
Do I have to copy the ID's into new columns , replace the values in those columns with the dict values , and then work from there ?
If I understand you correctly then you can simply call ` map ` : #CODE
I'm not sure you can , this is probably the most efficient way as merging will copy the source dataframe I think and map on a series is highly optimised as it is written using cython .
@USER you could use a function and apply it to the dataframe but I'm not sure you would save much time , it would depend on how many unique ids there were , once you have created the dicts then using ` map ` is really fast
The map solution did word , and it's fast , so I'm happy .
Basically , just treat the query string like you would any other string where you'd want to insert a variable .
I can create a DataFrame with just h and then append g to it : #CODE
But now suppose that I have an empty DataFrame and I try to append h to it : #CODE
You could change the apply a little to only return a ` ranks ` Series .
That would allow you to just assign a new ` ranks ` column to the original dataframe as the result of the ` groupby / apply ` .
I tried to only return rank series ` dec ` , the groupby will append the whole series to each row-column ( cell )
replace the values using the round function , and format the string representation of the percentage numbers : #CODE
Being new pandas , I am unsure of how to apply answer applies : Overlaying multiple histograms using pandas .
I have tried the following , but the xlim edit on the subplot does not seem to translate the bin widths across the new x-axis values .
However , the bin setting seems to only apply to the range of data not the entire interval that we display using xlim .
Yeah , you can use both , sort of like the ` breaks ` argument in ` R ` ` hist ` .
python Pandas : transform dataframe adding grouped columns
Is it possible to transform it into something similar to : #CODE
add how you created the dataframe . but some hints : play around with the functions ` groupby ` , ` unstack ` , and maybe ` sum `
I just want to see if another way exist with pandas , like apply or shift .
I was going to use Pandas to do the diff .
But diff also performs on the timestamp column and I can't get it to ignore a column while
That puts the timestamps in the index ( where they will not be touched by ` diff `) but also leaves a copy of the timestamps as a proper column , to be process by ` diff ` .
Can Pandas be made not to return NaT and if not how could I check against them as I will have to replace them in the list .
I found part of my answer in the post Looking to merge two Excel files by ID into one Excel file using Python 2.7
However , I also want to merge or combine columns from the two excel files with the same name .
If the former then you can just drop one of the columns and rename the remaining column , if the latter then you can perform a merge without passing ` how= ' outer '` as this will default to inner and will merge on ids that are present in both
Then just to finish off drop all the extra columns : #CODE
This would work if I was working on simple data such as in my example , but as I said in the last paragraph there are ** 200+** columns with the same name ( i.e. test1 , test2 , ... test200 ) in df1 and df2 that I want to merge into one file .
I wouldn't know the names of these columns ( the true names of the " test " columns is unknown ) before hand to be able to conditionally select column values and drop extra columns .
Also , how do you join this back to original dataframe ?
We can resample this to days ; it'll be a much longer timeseries , of course , but memory is cheap and I'm lazy : #CODE
The default rolling sum has the wrong alignment , so I'll just subtract with a difference of one week : #CODE
How do I merge the birth rate back to the original table ?
And then I stack the dataframe : #CODE
In this case it would be the last three rows since C can either map to 13 or 14 .
You could use a transform , counting the length of unique objects in each group .
I tested the g.transform ( lambda x : len ( x.unique() )) , works good but is slow especially when there are a lot of groups .
Python Pandas : Transform multiindex dataframe
I need to transform it into a percentual value related to the diagonal : #CODE
however , this doesn't insert newdate into the newRow of the DataFrame as the index of the appended record .
I want to resample my time index to have the aggregate sum of volume per minute .
Just construct the range of datetimes you want and reindex to it .
You could also start with a constructed business day freq series ( and / or add custom business day if you want holidays , new in 0.14.0 , see here
exist ; and create a new table to write the title , section , and the rest of the columns in ` torange ` by expanding the range between s_low and s_high in ` usc ` .
Use ` loc ` for label indexing : #CODE
Once you understood what happened here , I'm sure you can apply this to find the maximum of ' t1 ' .
The only problem is that this only works for Series ( when it is a dataframe , it tries to map the different values to fill to the different columns , not the rows ) .
So with an apply to do it column by column , it works : #CODE
I have a data frame consisting of many days of time series data of aggregate counts of website clicks sampled periodically throughout the day .
I want to aggregate the separate days together to form one cumulative time series over all the days .
You're really asking about contigiuous groups ( rather than the standard groupby , which ignores whether items in the same group are neighbouring ) so I think you need to use diff : #CODE
also the ` astype ( bool )` isn't strictly necessary if you can guarantee that each group is non-empty and increasing by each time .
Pandas replace non-zero values
I know I can replace all nan values with ` df.fillna ( 0 )` and replace a single value with ` df.replace ( ' - ' , 1 )` , but how can I replace all non-zero values with a single value ?
look up the ` resample ` function ( #URL )
Just ` resample ` .
Note that the ` NaT ` are currently a bug ( in 0.14.0 ) , so you need to drop them first .
you would need to resample these columns separately then `` concat `` them as they could have different ranges , something like : `` concat ([ df.set_index ( col ) .resample ( .... ) for col in columns_that_i_want ] , axis=1 )``
Dimensions fit , but it seems to append all the columns to be divided to each other , creating the second number , which is of course larger by a factor than the column that it then tries to divide by .
How to inset a new row at given position in pandas DataFrame and shift indices below +1
How to insert a new row with A=9 and B=9 at position with index=2 and shift indices below +1 ?
There may be more efficient ways , but here is one approach - taking slices of the head / tail around the point you want to insert values , and stacking it all together with pd.concat .
Pandas time series resample
and I'd like to replace the datetime column with a different datetime column that has additional values in it , and obtain something like this #CODE
You can reindex if you have the desired index .
I have a list of the new datetimes , but I'm not sure I see how to reindex .
It is similar to the post : Merge 2 columns in pandas into one columns that have data in python , except that I also want to transform the data as I combine the columns so it's not really a true merge of the two columns .
Once combined , I want to drop the column " wbc_na " since " wbc " now contains all the information I need .
You can use ` loc ` to find where column ' wbc_na ' is ' checked ' and for those rows assign column ' wbc ' value : #CODE
So the loc method is basically returning the row # correct ?
` loc ` performs label based selection , the [ online docs ] ( #URL ) are worth looking at to understand the semantic differences between ` loc ` , ` ix ` and ` iloc ` , by the way you should have enough reputation to upvote now ;)
Finally join them .
` s.map ( len )` applies ` len() ` to each element and returns a series of all the lengths , then you can just use ` sum ` on that series .
I can't switch to eval in every point and in any case , this was an example -- at other times , they are parts of different frames , etc .
But is there a better way to do this as I need to duplicate holiday rows by 5 times , and I have to append 5 times if using above way .
Pandas replace function & " Returning a view versus a copy "
I'm trying to use the replace function on my data frame , and I know that the problem is that I'm modifying a copy and should use ` .loc ` or ` .ix ` to return a view instead of a copy , but I'm a total beginner and struggled a bit with the pandas documentation on this .
` Cannot access callable attribute ' reset_index ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method `
My fault was , that I did not realize , that I have to apply some function to the groupby dataframe , like ` .size() ` , to work with it ...
You could drop the ` NaN ` rows if they are surplus but I would think you would want to keep the rows as you have a different IP address .
Do want to overwrite the csv or append the new entries ?
If you want it back in the origal form , you have to ` stack ` it back ( the ` reorder_levels ` is needed because of ` stack ` inserts the ' stations ' after the existing row index , while we want it as the first level of the row index ): #CODE
( this could indeed trigger this warning , as you cannot use unstack with duplicate entries ) If so , is this needed ?
You can feed that label ( the tuple in this case ( multi-index )) to drop : ` data.drop (( ' Armamar3 ' , pd.Timestamp ( ' 2010-05-09 11:30 : 00 ' , tz=None )))
pandas groupby X , Y and select last week of X1 and X2 ( which have diff frequency )
Then you can select the rows you want in an apply call on the grouped object : #CODE
If you can't upgrade or don't solve the issue you have with 0.14 , you can try to use ` ix ` instead of ` iloc `
no , the issue was the handling of an empty mask in the row indexer
There are probably a few ways to do this , but one approach would be to merge the two dataframes together on the filename / m column , then populate the column ' n ' from the right dataframe if a match was found .
The n_x , n_y in the code refer to the left / right dataframes in the merge .
If you have one large dataframe and only a few update values I would use apply like this : #CODE
Both ` merge ` and the ` apply() ` approach contain unnecessary overhead : #CODE
OK , first problem is you have embedded spaces causing the function to incorrectly apply :
So you can call ` replace ` instead of calling ` apply ` : #CODE
you can also use ` factorize ` method also .
you can use `` series.replace ( dict )`` I believe to do the substitution , then `` convert_objects ( convert_numeric=True )`` to change to float ( forcibly ); you can also `` factorize `` to make categoricals ( e.g. map the strings to numbers )
@USER so is ` replace ` faster than calling ` map ` or ` apply ` and passing a dict now ?
Wasn't aware of ` factorize ` also , when was this introduced ?
replace should be much faster ; `` factorize `` has been their quite a while ( but not advertised :))
If not , any suggestions for not reading the EOF character into the dataframe or is the best way to manually test for it and drop the row after reading ?
My tentative solution is to apply this function after reading : #CODE
if this is consistent , you could modify your script to check for NAN on the last line and then remove it : ` df [ : len ( df ) -1 ]` In the API docs i only see the ability to change the lineterminator which won't fix your problem .
` df [ ' new '] = pd.Series ([ 0 for x in range ( len ( df.index ))] , index =d f.index )` .
also , a list comprehension is entirely unnecessary here . just do ` [ 0 ] * len ( df.index )`
Play around with the ` reindex ` and ` align ` methods to gain some intuition for alignment works with objects that have partially , totally , and not-aligned-all aligned indices .
Now it does print the dataframe with the multiIndex required .
Can you include the entire stack trace ?
Unfortunately I can't just strip out accents , as I have to interface with software that requires the proper name , and I have a ton of files to format ( not just the one ) .
I would like to transform it to : #CODE
But if your ` DataFrame ` always looks like the one shown here , which is like a upper diagonal matrix without ` 0 ` s inside , you can just take a short cut : #CODE
Also , I'd rather drop the columns that I no longer need than select the ones I do need , since my real data has 400+ columns .
Can you not just * replace * the column headers with row 0 ?
Now you can use an apply with zip : #CODE
The first quartile can be calculated by taking the median of values in the dataframe that fall below the overall median , so we can see what data.quantile ( 0.25 ) should have yielded .
I thought this may not be a " NaN " issue , rather it might be quantile failing to handle even-numbered data sets ( i.e. where the median must be calculated as the mean of the two central numbers ) .
I would like to use quntile to calculate the rolling q1 / q3 values in my dataframe , however , this will not work with NaN's present .
However , I am not willing to apply dropna ( how= ' any ') , since I do not want to lose valid data just because a NaN sits in the next column over .
The same as the median if ` q=50 ` , the same as the minimum if ` q=0 `
the order within group apply function
order is preserved within a group and to the subframe that is passed to apply or a reduction function . you should show what you are doing and why this matters .
@USER it matters for apply functions like x - x.shift ( 1 ) .
If you are using apply not only is the order not guaranteed , but as you've found it can trigger the function for the same group a couple of times ( to decide which " path " to take / what type of result to return ) .
I need to shift the results of one column forward one month to create a new column .
How do I reindex or otherwise clean them up ?
If the index is not meaningfull , you can just do ` df.reset_index ( drop=True )` and you will get a new integer range ( 0 .. len ( df )) index .
So I was able to fix it by adding ignore_index=True into the dataframe append statement .
The error ` ValueError : cannot reindex from a duplicate axis ` indicates in this case that you have duplicate entries in your index ( and for this reason , it cannot assign to a new column , as pandas cannot know where to place the values for the duplicate entries ) .
And then to get rid of the duplicate values ( if you don't need to keep the original index ) , you can eg do ` df.reset_index ( drop=True )` , or you can use ` ignore_index=True ` in ` append ` or ` concat ` .
you are chain indexing , see here : #URL ; use ix / loc
Elegant and efficient way to repeatedly merge dataframes into single column of a dataframe
I'd like to merge them all together into a single value column of an initial dataframe with keys from the same key space ( with additional columns to be preserved ) .
When assigning a Pandas object ( e.g. a DataFrame ) to a ' slice ' of a DataFrame ( e.g. returned by .loc as above ) , Pandas attempts to ' align ' the indices of the two .
When Pandas receives such an object on the RHS , it does the assignment without trying to align indices ( - which works since my example pre-arranged for the two to be in the same order via sorting )
Base on what you just described , should it be a classic case of ` merge ` ?
While with only 2 dataframes I could just use merge ( ..., how= ' left ') , doing that repeatedly doesn't combine the value column , it just adds additional columns value_x , value_y , ...
Is it feasible to ` concat ` all the ** common_i ** dataframes into one first ?
Indeed , since ` common ` doesn't contain keys not in ` df ` , it also works as ` = common.ix [: , ' value '] .values ` or even using ` loc ` instead of ` ix ` .
I'm essentially trying to do an on-disk left join , but without needing to re-combine the left table from its partitions .
and I would like to merge all of the columns containing similar names in to the same column .
In this case , the only option available in kwargs is ' append , ' which is a deprecated alias to ' if_exists .
When I write Pandas DataFrame to my SQLite database using to_sql method it changes the ` .schema ` of my table even if I use ` if_exists= ' append '` .
When using the pure XlsxWriter I can apply formats to cells what also works nice .
I'm hoping there's a nice optimized method from pandas like resample that can do this in a couple lines .
look up ` resample `
The get_val function below takes the entire frame , a ticker e.g. ' AAPL ' , and a date and uses index.searchsorted which behaves like map :: upper_bound in C++ - i.e. finds the index where the date would be inserted if you wanted to insert - i.e. find the enddate closest to , but after the date in question - this will have the value that we want and we return that with get_val .
Then we form an empty list which will be used to flatten the list of tuples of dates from the multiindex with the key of ' AAPL ' .
Then I map this series to get_val to grab the stock price that is desired .
4 ) reindex your new filtered dataframe
It is to do that from here , all we need to do is sort , resample and fill the series .
I used non-overlapping dates now , spanning 3 weeks but the intervals don't really need to be regular ; what is really important though is that the start dates ** must ** be business dates , otherwise reindex will cause that interval to be lost .
Based on this new dataframe , I can group it by ` ticker ` and ` row ` , and apply a daily ` resample ` on each of these groups and ` fillna ` ( with method ' pad ' to forward fill ) #CODE
The last command was to drop the now superfluous ` row ` index level .
Yes , an easy solution is to add ` how= ' first '` in the ` resample ` call ( the default is ` mean ` , which does not work with non-numeric values ) .
append data in a loop python base condition
then for sure you want to use `` table `` format , as you can append and even store it in chunks ; `` fixed `` is somewhat faster but you cannot append , nor query at all . read docs : #URL ( and cookbook is a link )
Passing the function ` pd.Series.mean ` directly to ` resample ` works - for some data , but trips up , for example if a sampling bucket has no values ( e.g. on minutes ` T ` above ) .
checked the type of dt :
` type dt class ' pandas.core.frame.DataFrame ' `
You showed the * type * of what you get when you select a column from a groupby object ( ` SeriesGroupBy `) , and the * type * of an unrelated Series after you apply ` pd.to_datetime ` to its elements , which by construction is ` Series ` .
python drop non-integer rows , convert to int
Is there a simple way to drop rows containing a non-integer cell value , then / and convert strings to integers , then sort ascending ?
I'd like to drop the numbers that don't fit into int64
Is there a way to just drop non-integers from the original input df1 rather than making the new series ?
` freq not specified and cannot be inferred from first element ` Do you know how do I specify frequency ?
I need to compare each word in strings of column B with the word in A and replace it if Levenshtein distance is 1 .
I can see that I could pre-process the file to strip out the commas - I'd like to avoid that if possible but would welcome suggestions if this is the only way .
I think the easiest way to plot this data with all the lines on the same graph is to pivot it such that each " template " value is a column : #CODE
I think the more pandonic ways are to either use ` resample ` ( when it provides the functionality you need ) or use a ` TimeGrouper ` : ` df.groupby ( pd.TimeGrouper ( freq= ' M '))`
python pandas drop value from column ' B ' if that value appears in column ' A '
This code appears to drop duplicates from ' A ' , but leaves ' B ' untouched : #CODE
Should I not join them into a single DataFrame ?
Is there are way to compare and drop duplicates if I don't ?
If you can cut it down to a minimum failing test case , we can probably figure out what it is .
How do I join table and field names in a pandas function ?
You can do it by using concat to make a new dataframe and plotting that , though I think you'll have to rename one of the columns .
to_sql is using insert sql method and this is so slow than copy from sql method .
How to apply OLS from statsmodels to groupby
So how can I go through my dataframe and apply sm.OLS() for each product_desc ?
@USER , ` clip ` , or ` .clip_xxx ` methods are actually slow in this case , only direct calculation is able to be slightly faster than the boolean indexing version .
Note : If you are concerned about speed then it would make sense to transpose the DataFrame , since appending columns is much cheaper than appending rows .
You can append a row using loc : #CODE
Merge multiple dataframes with non-unique indices
It seems to concatenate the individual series sequentially , while I want a merge .
This is very nearly a concat : #CODE
However , concat cannot deal with duplicate indices ( it's ambigious how they should merge , and in your case you don't want to merge them in the " ordinary " way - as combinations ) ...
Note : I can't seem to append a column to the index without being a DataFrame ..
Now we can go ahead and concat these : #CODE
If you want to drop the cumcount level : #CODE
Note2 : This is pandas 0.14 , in 0.13 you have to pass a numpy array to ` _cumcount_array ` e.g. ` np.arange ( len ( s0 ))`) , pre-0.13 you're out of luck - there's no cumcount .
I think the kind of merge I want is specific enough that I will have to do it in Cython if I want speed for big time series .
@USER -- ok , I guess I made it optional to pass an output array in 0.14 , so pass something like : ` np.arange ( len ( s0 ))` ...
Consider resample by ' M ' rather than grouping by attributes of the DatetimeIndex : #CODE
Note : you have to drop the NaN if you don't want the months in between .
If it feasible for you to replace ` { ` with `" { ` , and ` } ` with ` } "` , it can be read correctly by : ` pd.read_csv ( ' data / training.dat ' , quotechar= '"' , skipinitialspace=True )`
What is problematic for me is how to translate this statement into pandas in a way that is fast and idiomatic and etc .
However , for seven-day retention I would need to create 7 dataframes and merge them together .
( I should also point out that my research led me to #URL , which indicates a merge behavior that does not work on my install ( pandas 0.14.0 ) which fails with error message ` TypeError : Argument ' values ' has incorrect type ( expected numpy.ndarray , got Series )` . So there appears to be some sort of advanced merge / join behavior which I clearly don't know how to activate .
If I understand you correctly , I think you can do it with a ` groupby / apply ` .
I'm going to create rolling forward count within an ' id ' group of the number of times that id shows up in the next 7 days including the current day : #CODE
bypass read_csv and apply to_datetime after :
I think the problem is ` apply ` expects to return the same number of rows as the input .
You could also do it with a ` groupby / apply ` since it is more flexible .
Or could iterate over the rows concat
pandas vectorized string replace by index
What " merge " operation do you want performed ?
Group the dataframe by " Item " and apply a function that calculates the process time .
You can do it for each column seperate and then concat the results : #CODE
` from the concat function call .
If you are just updating existing data then the performance hit may not be an issue , it sounds like all you'd be doing would some stats on the updated values , note that groupby itself does nothing only when you apply a function does it do something .
If you know which group is to be updated then you can call ` get_group ( ' updated_item )` can call apply on just that group see : #URL
I'd like to append phstab on each iteration so so the output is just one long dataframe instead of four instances .
which will not aggregate the original rows while getting the sum column ??
IIUC , you want ` transform ` : it does the aggregation but returns an object indexed the same way as the original .
How do I to translate this json format into correct format that can be used pandas read_json()
How to reindex to merge two dataframes ?
I am trying to merge two dataframes that both have a ' product_desc ' column .
What do I need to do so these two dataframes will merge ?
However , I still get the above error message when I try to merge .
It might help to have a full stack trace of the error in your question .
I have no idea what goes wrong - but for a start you could try to subset your data and see if the merge works .
The merge function takes four arguments : dataframe 1 , dataframe 2 , left_on = " dataframe 1 column " , right_on = " dataframe 2 column which matching values in dataframe 1 "
Now I want to read them and merge into one big DataFrame .
When I read the 3 working files and merge them ; and read D11 independetly , the line #CODE
When considering calculations involving subsequent and preceding rows you should consider using ` shift ` , this is what is designed for
Ah OK , in that case you want to look at [ ` resample `] ( #URL ) , something like ` data.CH.resample ( ' 5min ' , how= ' diff ')` not sure if it should be ` diff ` or ` sub `
Calculate during merge using pandas
Python Pandas : Get row by median value
I'm trying to get the row of the median value for a column .
I'm using data.median() to get the median value for ' column ' .
How can get the row or index of the median value ?
You could use the ` apply ` method : #CODE
I did apply ( wordnet.synsets() )
For example , say that I know what slices I want to apply on each level name , e.g. as a dictionary : #CODE
When I try to pass the ` if_exists= ' replace '` parameter to ` to_sql ` I get a programming error telling me the table already exists : #CODE
From the docs it sounds like this option should drop the table and recreate it , which is not the observed behavior .
Strange , can you show ` pd.__version__ ` ( just to be sure it's not picking another version of pandas , as this was a bug ( with replace ) in 0.13 and older , but that should be fixed now ) .
How to merge two columns together in Pandas
dataframes have ` join ` and ` merge ` methods . either of those will work .
you could use an ` apply ` statement to select the values from the correct columns .
Also i hope stack overflow has been useful to you -- be sure to accept the answers to your previous questions ( including this one ) if it solved your problem .
Of corse we can get around this problem using some data wangling techniques like ` transpose ` and ` slicing ` .
However , as this is a series ( not a dataframe / csv / excel ) , these solutions do not seem to apply .
You can add them ( if you first multiply the hours by the number of nanoseconds ) , but you have to drop down to numpy to do the calculation* : #CODE
* Apparently Index overrides the ` + ` operator to make it append ...
Access the levels of a MultiIndex using ` get_level_values ` , see another question .
So I am trying to merge the following columns of data which are currently indexed as daily entries ( but only have points once per week ) .
I am not sure how to get merge / join function to do this .
#Create year variables , append to new dataframe with new index
Finally , do a pivot table to transform and get row-wise ` max / min ` : #CODE
When I take our the extra brackets I get the error : KeyError : ' Key length ( 7 ) was greater than MultiIndex lexsort depth ( 2 )'
I think because the pivot table is indexed by week , it cant be called by column .
Now we can use ` value_counts ` and then ` concat ` to get the current pieces : #CODE
I want to drop duplicate ' values ' , keeping the row with the lowest ' Date ' .The end result should be : #CODE
use `` apply `` ONLY as a last resort ( e.g. you can't do vectorized things ) . even if you have a very complicated function to do , you can often do vectorized calculations on most of it , saving the last for `` apply `` , which is essentially a loop .
Using apply took 172ms versus 39ms using Jeff's method , I can also confirm that it made negligle difference whether the apply was called inside or outside the function but it does modify the df so you didn't need to return the df as it was being modified inside the function
And then sometimes different solutions ( in this case using ` apply `) come up on google / stackoverflow and yet again I can NOT verify that there is no better solution as I dont have the insight into the library .
And then populate a third dataframe ( alternative transform the first one ) so that I get a version of the first one keeping only rows of co-occurrences that happened when Item1 was below the defined ' age ' .
You could ` merge ` DataFrames - it is similar to ` join ` in SQL #CODE
( I'd like to join using the index , which is a datetime value )
Edit2 : I written some code which should merge the data from both of my column , but I get a ` KeyError ` when I try to set the new values using my index generated from rows where my first df has values but my second df doesn't .
Look at this answer to merge [ in case you need to add suffixes in cases of similarly named columns ]: " #URL , Now read up on here : " #URL To figure out how to select certain indexes .
What you will want to do after the merge , find rows where a value is missing using variations of df [( df [ ' colA '] .isnull() == True ) & ( df [ ' colB '] .isnull() == False )] , and set the value if missing .
Pandas : apply a function to a multiindexed series
, where numbers.hash and local_time together is a MultiIndex .
Now I want to apply any function to each series indexed by numbers.hash only , e.g. summing the values in each time series that is made up of local_time and the value .
Or groupby and apply an arbitrary function #CODE
Note that if you have data in the list you can't show us , then replace the strings by " A " and floats by 1.0 , etc ., until you come up with an example you * can * post .
There are some results on how to do this for an index on stack overflow but not the columns and the ` parse ` function has a ` header ` that does not seem to take a list of rows .
My best idea so far is to use ` transpose ` , but the it fills ` Unnamed : # ` everywhere and doesn't seem to work .
@USER hmmmmmm good point , I was thinking the same about the loc ambiguity ( if they are labelled with 0 or 1 ... maybe best not to use at all ?
As long as ` msk ` is of dtype ` bool ` , ` df [ msk ]` , ` df.iloc [ msk ]` and ` df.loc [ msk ]` always return the same result .
@USER , in your example , if I change 0.8 to 0.2 I get ` len ( train )` equal to 59 and ` len ( test )` equal to 41 .
I would then like to append the position as a value for the 13mer which will be the key .
I have thousands of these rows to process , note that I want to transform the value of the ' smoking ' column so it's not a simple concatenation of all columns .
and then drop the other columns you don't need ( see also here : Delete column from pandas DataFrame ) , or just use the series
I just need to replace the yes / no in the smoking column with smoking / nonsmoking .
slicing a pandas multiindex using datetime datatype
I am trying to slice a pandas data frame utilizing a multiindex .
In fact you should be able to simplify it further : ` x.loc [ rows , ' price1 '] = x [ ' price '] / x [ ' amount ']` it should align corretly without the mask on the rhs
I'd try with and without the mask to see if it makes a performance difference , you'd be surprised with how premature optimisation does not match reality
Based on your sample dataset multiplied to create a 25000 row dataframe , using the mask and then assigning took ` 2.54 ms ` , without the mask and just assigning straight takes ` 950 us ` so I'd check the performance at your end using the real dataset
I know now that using ` loc ` is better to use anyhow but still ... as a new user without the insight into all functionalities , this might be very very confusing ( as it was for me ) that it works for some number and for some not .
moreover you should always use ` .loc ` if possible , if we compare the performance between using a mask and without a mask we can see that for a 25000 row dataframe based on your sample data it is faster without the mask : #CODE
Shouldn't the first timeit-code contain a mask on the rhs ?
I've written the following code to find gaps in columns , generate a list of indices of dates where these gaps appear and use this list to find and replace missing data .
So you should use ` loc ` : #CODE
note that it is not necessary to use the same index for the rhs as it will align correctly
pandas - using the ' melt ' function to reshape a table
I think melt can take me halfway there but I'm having trouble getting the final output .
Also you can put site , returnperiod and peril into the index and then unstack peril ....
Searching for both ends of range from pandas data frame in another data frame and expanding it to a new data frame
Removing duplicate columns from a pandas dataframe : behavior of transpose + drop_duplicates
The 2nd form does work ( ` df.T.drop_duplicates ( inplace=True )`) , but it is operating on a copy ( the transpose itself doesn't copy , but the ` drop_duplicates ` DOES ); so it is modifying a copy that you then don't have a reference .
But I can't work out how to merge them together into the format at the top .
You could use a transform : #CODE
Note : you have to use TimeGrouper to groupby months ( just like you would in the resample ) .
gives " AttributeError : rint " if I insert values and then apply np.round() .
Rather than drop the index , I think you can / should just use the ` .values ` .
I use astype ( ' S3 ') for three digits numerical index , astype ( str ) truncate them to single character .
use ` map ( int , df.DateVar.str.split ( ' / ') [ 0 ])` to convert each element to integer ?
I keep checking that I have dupes by using df.duplicated() .value_counts() and it does show as many rows as True but then when I apply ` df.sort ( df.columns.tolist() )` as you suggest , it still is not sorting all of the duplicated rows .
You need parentheses after your initial mean / median calls ( as below ) - otherwise you are assigning the function to DataFrame , not the value it returns #CODE
try ` for z in y : print len ( z )` to check if all y's have expected length ?
Trying to merge two timeseries gives me the error that the Series have no columns .
You need inner join , right ?
We can use ` shift ` and ` where ` to determine what to assign the values , importantly we have to use the bit comparators ` ` and ` | ` when comparing series .
` Shift ` will return a Series or DataFrame shifted by 1 row ( default ) or the passed value .
When I aggregate different columns with different functions I'm getting a hierarchical column-structure .
and a specific value using loc : #CODE
( To mix labels , loc , and position , iloc , you have to use ix ) #CODE
But I see the analogy , especially because one can easily transpose a DataFrame ( and thus still keep the MultiIndex ) .
You are not using the power of pandas here , just use `` .isin `` , and or / join , see this for benchmarks : #URL
Then I perform some calculation using this data from B and insert the result back to A .
Also in order to prevent the two plots from overlapping I have modified where they align with the ` position ` keyword argument , this defaults to ` 0.5 ` but that would mean the two bar plots overlapping .
move some bars to the right axes by change it's ` transform ` attribute
Then we shift the 4x4 up and down to get the other two values we want for each column .
Compute the rolling mean column-wise for a window of 3 #CODE
Select out the pairs of column / index that we want ( the unstack creates a multi-index with these pairs ) #CODE
In the script below , Why are tz and tz2 are different ?
In this case , tz displays as : #CODE
So in answer to your question , ` tz2 ` is correct as it localizes to a time zone that is correct for its date , while ` tz ` is ' correct ' for the current date .
Are you saying that the timezone of ` t.tz_localize ( pytz.UTC ) .tz_convert ( tz ) .tz ` depends on the value of ` t ` ?
Once PEP 431 is finished , ` datetime.tzinfo ` methods will take ` is_dst ` parameters where appropriate and ` pytz ` will be able to implement time zones that do the right thing without having the user jump though ` localize ` and ` normalize ` hoops .
The example is simplified- I'm trying to show what I'm doing with the full code block below- which has a multiindex .
When I use .join with the full code set , I get ` ValueError : cannot join with no level specified and no overlapping names `
The answer itself was automatically flagged by Stack Overflow and showed up in the review pool because it was a code only answer .
The full data set is a little over 1700 rows / 18columns after the transpose .
No , I have tried that and received this : AttributeError : Cannot access attribute ' values ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method
You need to apply some kind of aggregation to the GroupBy object to return a DataFrame .
Input : data = binned_data.sum() .reset_index() .values and then I got the Output : ValueError : cannot insert Weight , already exists .
You could also get the color to plot using the modulus / remainder operater ( % ): ` mycolor = colors [ icolumn % len ( colors ]` .
mycolor = colors [ icol % len ( colors ]
I have a list of dictionaries which I need to aggregate in Python : #CODE
and I'm looking to aggregate based on budgetImpressions .
Is budget impressions going to be the only element that is different for the dictionaries you want to aggregate ?
@USER the logic is to aggregate budgetImpressions
You can use the ` groupby ` functionality and aggregate by sums , then convert the output to a list of dicts if that is exactly what you want .
I want to apply a function f ( lat1 , lon1 , lat2 , lon2 ) which calculates the distance between two points ( defined using lat1 , lon1 , lat2 , lon2 ) .
For all 110k+ records in ` df1 ` do you want to apply your distance function for every record in ` df2 ` ?
I chose to use map and list comprehensions because they will be faster than a standard ` for each `
However I took this into account and used map , and nested comprehensions which are going to be faster than a for loop .
If I remove the " regex = True " from the replace arguments , I get #CODE
pandas apply function to multiple columns and multiple rows
I know how to apply a function to different columns , and how to apply functions to different rows of columns , but can't figure out how to combine both .
and then I insert temp list into df
I compared the time of three solutions for my df ( the size of the df is about 6k rows ) , the iteration is almost 9 times slower than apply , and about 1500 times slower then doing it without apply :
execution time of the solution with iteration , including insert of a new column back to df : 1 , 51s
execution time of the solution without iteration , with apply : 0.17s
execution time of accepted answer by EdChum using diff() , without iteration and without apply : 0.001s
Suggestion : do not use iteration or apply and always try to use vectorized calculation ;) it is not only faster , but also more readable .
Also if possible avoid using ` apply ` , as this operates row-wise , if you can find a vectorised method that can work on the entire series or dataframe then always prefer this .
seeing as you are just doing a subtraction from the previous row there is built in method for this ` diff ` this results in even faster code : #CODE
` abs ` should be ` np.abs ` in first case ?
The problem I'm facing is : I only have integers describing the calendar week ( KW in the plot ) , but I somehow have to merge back the date on it to get the ticks labeled by year as well .
So I thought about how to transform the data and found the method ` pivot ` which turns out to be a great function .
` reset_index ` is needed to transform the multiindex into columns .
Any ideas on how to merge back the real datetime for the calendar-weeks and use them as x-axis ticks ?
Then change the layout ( reshaping ) as mentioned in the question already by using ` pivot ` .
I think resample ( ' W ') is a better way to do this - by default it groups by weeks ending on Sunday ( ' W ' is the same as ' W-SUN ') but you can specify whatever you want .
Since ` argmax ` is the index of the maximum row , you will need to look them up on the original dataframe : #CODE
NOTE : I selected the ' size ' column because all the functions apply to that column .
If you wanted to do a different set of functions for different columns , you can use ` agg ` with a dictionary with a list of functions e.g. ` agg ( { ' size ' : [ np.sum , np.average ] } )` .
This results in ` MultiIndex ` columns , which means that when getting the IDs for the maximum size in each group you need to do : #CODE
I noticed that the code works for just the ` size ` column version but does not work when I do : ` grouped = df.groupby ([ ' code ' , ' colour ']) .agg ( { ' size ' :[ np.sum , np.average , np.size , np.argmax ] } ) .reset_index() ; grouped [ ' max_row_id '] = df.ix [ grouped [ ' argmax ']] .reset_index ( grouped.index ) .id ` .
using a dictionary as the argument for ` agg ` results in ` MultiIndex ` columns , so it needs to be ` grouped [ ' max_row_id '] = df.ix [ grouped [ ' size '] [ ' argmax ']] .reset_index ( grouped.index ) .id ` .
I can resample weekly , starting on Sundays and closing on Saturdays : #CODE
After trying the various options of ` resample ` , I might have an explanation .
The way ` resample ` chooses the first entry of the new resampled index seems to depend on the ` closed ` option :
when ` closed=left ` , ` resample ` looks for the latest possible start
when ` closed=right ` , ` resample ` looks for the earliest possible start
The next example illustrates the behaviour of ` closed=right ` , which is the one that I didn't understand in my initial post ( ` closed=right ` by default in ` resample `) .
I would extract the day from the minute data set and merge it with the day index of the other dataframe .
I've tried using the ` apply ` function across the column , but to no avail .
No , calling ` .median ` on the ` .groupby ` object returns the median for each group ( in my case there are 12 groups ) .
Did we decide this was a transform ?
I think you want to be using a transform here : #CODE
You were right , using the ` .transform ` : ` values_to_subtract = test [ ' A '] .median() - grouped [ ' A '] .transform ( ' median ')`
If you need these to be bool columns , then use ` astype ( bool )` on each column .
As an aside you can nearly this with a resample ( except for the last missing rows and Changed column ): #CODE
So I tried to use a map function that seems to work with other operations , but it also is defeated by use of a dictionary : #CODE
If I understand you correctly you can just call ` map ` : #CODE
Rather than fill as an empty column , you can simply populate this with an apply : #CODE
Pandas concat producing NaN
I think you want ` concat ( df , axis=1 )` .
I love stack overflow - a serious question : I want a line and bar on the same chart , and I cannot see what my code is doing wrong ( the bar chart doesn't get added ) .
The most difficult thing is how to align x-axis ticks .
Here we define the align function , which will align ` ax2.get_xlim() [ 0 ]` with ` x1 ` in ` ax1 ` and ` ax2.get_xlim() [ 1 ]` with ` x2 ` in ` ax1 ` : #CODE
Pandas ; calculate mean and append mean to original frame
Perform a ` groupby ` by ' Country ' and use ` transform ` to apply a function to that group which will return an index aligned to the original df #CODE
I'm aware of df.where function but apparently it's not possible to apply for columns , it works just for all DataFrame .
How to drop rows based on year on a based dataframe in a function using * args
or in more generic way using regex replace .
@USER there's a subtle thing here that = can create a copy if used along with loc / iloc / ix .
the columns ' names map similarly : #CODE
You can shift ` A ` column first :
Of course , when you hit this size , you probably need to shift to a proper database , but still , there are people out there who try to just ` read_csv ` this kind of stuff .
Should be pretty direct with pandas ` stack ` and some string parsing .
@USER I've never used stack before but actually it looks really useful .
You can use the ` value_counts ` function for this ( docs ) , applying this after a groupby ( which is similar to the ` resample ( ' D ')` you did , but resample is expecting an aggregated output so we have to use the more general groupby in this case ) .
To get this in the desired format , you can just unstack this ( move the second level row indices to the columns ): #CODE
Why not use the built in string method , rather than apply .
@USER using that function is actually slower than using ` apply ` ( see my answer ) .
@USER What about something like ` def f() : return len ( requests.get ( ' #URL ) .text )` ?
Idiomatic multiindex column assignment in pandas
Is there a less verbose , more idiomatic way to make a multiindex column assignment like this ( particularly one where I don't have to explicitly specify each sublevel on the left side ) ?
I like the ` concat ` solution .
When I try doing ` pd.concat ([ df , df [ ' Val '] / 2 ] , axis=1 , keys =[ ' Val ' , ' C ' , ' Half '])` I get ` AssertionError : Cannot concat indices that do not have the same number of levels ` .
Is this a fundamental limitation of concat ?
I think this option is preferable to the concat option because you don't have to risk incorrectly re-labeling the ' Val ' column .
Take the columns from the new dataframe and use them in the MultiIndex : ` half = df [ ' Val '] / 2 ; df [ pd.MultiIndex.from_product ([[ ' Half '] , list ( half.columns )])] = half ` .
( Searching for both ends of range from pandas data frame in another data frame and expanding it to a new data frame )
cumsum per group in column ordered by second column append to original dataframe
Will give a hierarchical index solution , but I could not find a way to take the resulting cumulative sum column and attach it to the original dataframe without multistep merge commands .
I have messed with lambda , apply , aggregrate commands can't quite get anything to work .
Nice use of the transform function this is a slightly better approach , if anyone else has another method feel free to post .
I want to use a combination of map & lambda functions to do this
I want to use a combination of map lambda functions to do this .
the map function does not append to NN .
Have you tried using ` concat ` and a generator expression instead : #CODE
` memory_map ` does not seem to use the numpy memory map as far as I can tell from the source code It seems to be an option for how to parse the incoming stream of data , not something that matters for how the dataframe you receive works .
pandas resample dealing with missing data
I would like to be able to use the resample method to compute annual statistics but for years with no missing data .
Here is what I obtain if I resample : #CODE
and then drop all rows where ` count !
= len `
if I want to aggregate just one column and be able to add the result back to the DataFrame .
This works because the ` transform ` operation preserves the index , so it will still align with the original dataframe correctly .
I found another approach that uses apply() instead of transform() , but you need to join the result table with the input DataFrame and I just haven't figured out yet how to do it .
You can join this back to df with ` df.merge ( group_features , left_on= ' Key ' , right_index=True )`
You could add a column full of ` True ` and then pivot : #CODE
It might be that there exists a fancier slicing , but I would keep it as simple as possible and produce a partial multiindex object that fits my needs : #CODE
Pandas wants to align the rhs side ( after all you are subtracing DIFFERENT indexes ) ,
Now related to this , which is my end goal : how would you substract column 1 to columns 2 , 3 for each multiindex ?
I updated , you can simply set it using the same mask ( ironically this is the ' reason ' why the rhs is not much simpler ) .
Merge with Overlapping columns
If I have two data frames that contain data during the same period ( although no necessarily for the exact same dates ) and I want to merge them together .
The merge is to be performed on the data and ID columns as follows : #CODE
Formulated another way , is there a method to use merge such that when performing and outer merge , in the resulting data frame where an inner merge would ' fail ' , to keep the value that is not null ?
This of course assumes that the name would have to be the same in df1 and df2 where an inner merge would succeed .
I know that I could easily do this after the merge : #CODE
I don't think there's anything built-in to merge to have the behavior you're describing .
Never seen partition or endswith .
Deducting the median from each column
I'd like to deduct the median from each column so that the median of each becomes 0 .
I'm guessing it is possible without iterating over the values , computing the median and then deducting .
` median ` of ` numpy ` computes median of overall data .
Does this really subtract the median of each column ?
Seems it would do the overall median .
So the next line gets the median along the columns #CODE
In numpy 1.9 ` median ` has a ` keepdims ` keyword argument that will spare you the ` None ` indexing ( which isn't needed in this particular case anyway ) .
If you're interested in the history of these zones , I suggest reading through the europe file in the TZ data .
I realize there are different settings one can apply that will change the look and feel of either individually ( matplotlib savefig() plots different from show() ) , however I haven't been able to find any easy to follow documentation that shows how to set the default fonts while using matplotlib backend : MacOSX .
I'm trying to get the expanding mean for the 3 max value : #CODE
I pulled your expanding mean into a separate function to make it a little clearer .
Edit 1 : Could it be fast to set the index of the chunk equal to the id column and then only keep rows that successfully merge with keep_ids ?
and use ` concat ` to take the year out .
Now you can assign all the entries in these rows to NaN using loc : #CODE
For example , the target result when index = 9 and n = 2 would transform these rows : #CODE
You lose a lot of the efficiency by leaving numpy / pandas , if you were to use a rolling function you don't need to create anything - it'll be entirely view-based .
I looked at the rolling window options .
So much so , that ` reindex ` ing will not work .
Is there any direct way how to add / insert series to dataframe ?
Generally , it's easiest to append dataframes , not series .
In your case , since you want the new row to be " on top " ( with starting id ) , and there is no function ` pd.prepend() ` , I first create the new dataframe and then append your old one .
But now you can easily insert the row as follows .
That's nice workarround solution , I was trying to insert series into dataframe .
Just assign row to a particular index , using ` loc ` : #CODE
If you don't want to set with enlargement , but insert inside the dataframe , have a look at #URL
something like ` for i in range ( len ( folder )): df_%i = pd.DataFrame() % i ` so I can then save my CSVs or output in these dataframes
You cannot write to the ` year ` attribute of a datetime , so the easiest thing to do is use ` replace ` : #CODE
A bit of a non-scalable solution would be to ` drop ` 2014 and then call ` max ` and ` min ` - #CODE
Pandas dataframe with multiindex column - merge levels
I have a dataframe , ` grouped ` , with multiindex columns as below : #CODE
How can I flatten / merge the column index levels as : " Level1|Level2 " , e.g. ` size|sum ` , ` scaled_size|sum ` . etc ?
If there are two levels , n i want to merge them into one , like ` size|sum `
Then , convert to a numpy array , transpose , and pass to the DataFrame constructor along with the columns .
I tried with groupby , but I still need to combine the units with the Metric and to transpose the value .
The pivot table will have a MultiIndex as the columns , this collapses it down to combined the name / units .
I think I would use a groupby and merge here : #CODE
Set cell value to NaN under certain conditions using lamda map in Pandas
Using lambda and map is very fast in Pandas but I can't figure it out .
pandas features a lot of standard statistics for so-called rolling horizons , such as a rolling mean , or a rolling standard deviation .
I could be wrong bu it seems that you can't append with ` to_csv `
To append to a csv use append mode ( see this or this question ) .
Any dataframes with identical multiindexed columns append and sort
The key here is that I don't know how the dataframes will be ordered in the list I basically need something that knows when to concat ( obj , axis=1 ) or concat ( obj , axis=0 ) and can do this to combine my list of dataframes .
You want to resample it and have OHLC , with give frequency ( let's say M or 2M ) buckets ending 18JUN .
So for M freq , last bucket would be 19MAY-18JUN , previous one 19APR-18MAY , and so on ...
BUT so far we can't apply the right DatetimeIndex to ts.resample()
seems that pandas dev team ( Date ranges in Pandas ) is aware of this issue , but in the meantime , how could you solve it to get ohlc with rolling frequency anchored on last day ?
It's simply a case of calling ` unstack ` on the ` groupby ` object then plotting it .
And its expensive to do a ` np.concatenate ` to aggregate , which is what is done under the hood .
But then you must manually align them and operations become tricky .
Use a combination of ` shift ` and ` all ` : #CODE
We get a dataframe with boolean values but we can't use this as a mask as is , we want to check that all rows are ` True ` so we use ` all ` : #CODE
We can now use this as our boolean mask to achieve what we wanted : #CODE
I would convert these to pandas Periods instead of Timestamps and then diff these #CODE
I think to use ` apply ` in this case will be difficult as it is conditional ( based on the surrounding cells ) .
Only things I did was drop an extra header row with ` df = df [ 1 :] .reset_index ( drop=True )` and
It could be that you're looking for some kind of merge : #CODE
I'm not 100% clear on your question , but I think you may want the pandas ` cut ` function , which bins data into groups .
To have different numbers of groups , just pass a different parameter to cut .
My problem is that in each of those plots , I want to draw two series , namely ` NMI_Louvain ` and ` NMI_Infomap ` as a function of ` muw ` that come from an aggregate measure of average over the variable trials .
Basically I want to create a new column " Ratio " that divides Price / Buy or Price / Sell , depending on which abs ( buy ) or abs ( sell ) is greater .
I am not really sure how to do this ... would I use an apply function ?
The condition ` ( abs ( df [ " Buy "]) abs ( df [ " Sell "]))` gives a 0 / 1 valued column depending on whether buy or sell is greater .
Here is the solution using apply - First define a function operating in rows of the DataFrame .
Finally , set the ` Ratio ` column appropriately using apply .
But is there a way to do it with an apply function or something as well ?
I've edited the reply to include solution using apply
If set the threshold to 1 and take the abs value like so : ` df.groupby ( level =[ ' categories ' , ' features ']) .filter ( lambda x : max ( abs ( x.xs ( ' subfeature1 ' , level= ' subfeatures ') - x.xs ( ' subfeature2 ' , level= ' subfeatures '))) > 1 )` .
i am looking to apply multiply masks on each column of a pandas dataset ( respectively to it's properties ) in python .
how can i apply the concat_mask on df , so that i select rows , in which all Boolean criteria are matched ( are True ) ?
If you wanted to index the original dataframe by the mask you could do #CODE
. Can You insert that into your answer ?
In the proper code i actually iterate throw all columns and apply various of diffenrent conditions to mask each column .
not yet #URL ( but you can do it in the apply ; this is for using an aggregate function ) .
Since we're using views here , this should be more efficient / faster than the apply ...
Next , I want to append , Concatenate the dataframe to the the one prior in a cumulative concat function ..
I prefer it to be in a column without a tz , as I'm comparing it to another value that is tz=None .
You have to append nanos ( since numpy datetime arrays * don't understand pandas offsets ): #CODE
@USER , in that case maybe the ` multiindex ` will be an suitable approach , let's see what the OP says .
And then if you really want to drop the zero values , you can keep only the nonzero ones using ` fast_df = fast_df [ fast_df [ ' value '] ! = 0 ]` .
Pandas rolling apply with variable window length
These timestamps are irregularly spaced and I would like to calculate something like the rolling mean ( say ) over the last N seconds , where N is a constant .
Unfortunately , resampling at regular intervals before calculating the rolling quantity is NOT an option - the rolling quantity has to be calculated on the entire dataset .
possible duplicate of [ Pandas : rolling mean by time interval ] ( #URL )
The normal way to deal with it would be to drop all the NaN's first : #CODE
The indices will still align when you do operations on both , I'm not sure what you mean ?
Weirdly , it works fine if I replace the ` v = ` with just ` print ` and remove the line with the append statement .
Just need to figure out how to append the new column to the dataframe and set it as the index , but I'm sure that's the easy bit .
You could try and use ` read_json ` and ` concat ` which may be more efficient .
If you need something more sophisticated , you could use ` exec ` or ` eval ` to construct functions on the fly ; there are obvious security hazards there , but you'd have the same issues using ` sympy ` ( it uses ` eval ` too . )
How do I reshape or pivot a DataFrame in Pandas
So column names are now the values that were in CF and the intersection of the row and columns in the new table are the values that were in the value column in the original table .
Is there a way to drop the " CF " text in the output .
now you need to append the result as a ; delimeted string to the csv file , with some kind of for loop #CODE
@USER I have updated the question with complete stack trace
A heat map is what I wanted #CODE
What I need to do is merge the location tree columns with the job listings data frame .
My thinking was to merge on the most abstract location first and then move to more specific location categories to merge the LocationNormalized values that are more specific .
I am using ` groupby ` + ` apply ` instead of ` resample ` because of the need to examine multiple columns .
I want to use ` math.atan2 ` on the ' Ux / Uy ' columns and am having trouble successfully ` apply ` ing any function .
My original question was : what kind of value should be returned from my ` apply ` ed function so that a groupby-apply operation results in a 1-column DataFrame or Series with a length equal to number of groups and group names ( e.g. Timestamps ) used as index values ?
this will be much more efficient to not use apply at all , rather compute the mean aggregates first , then use np.atan2 .
Just looking at your exception , looks like you're trying to apply function to each row but didn't specify axis=1 e.g. df.apply ( f , axis=1 ) #apply function to each row
I would like to drop the row where the userid is Nan .
If I wanted to drop another row I could do #CODE
Is there a way to drop the row without having to reindex ?
Since ` df.iloc ` was created mainly for selecting by integer index , you might want to use ` df.loc [ ... ]` for selecting by label and by boolean mask .
I already tried something like this . isnull is not defined for MultiIndex so the command would be more like df.loc [ ~ pd.isnull ( data.index.levels [ 0 ])] but this does not work since the returned array ignores the NaN value .
@USER the labels array of a MultiIndex is a location of the levels , so -1 is a missing value
Easier to reset and drop from the frame , then set the index .
Pandas uses the index " line up " operations in that the operation will apply only to the common indices .
So when you want Pandas to ignore the index , you need to drop the index .
lol , snap !
To write out you could write the header first and then append : #CODE
Below is some code that should work in general for a series with a MultiIndex , using a ` defaultdict `
It's not clear to me how the aggregate ( sum ) call will work , as none of the datetime values are actually the same .
then apply the filter like so :
It seems the most time consuming part is ` nonzero ` of ` numpy.ndarray ` .
Because these are irregular operations , I can't use merge / join .
In the meantime , do you know which ` pandas ` function rely heavily on ` nonzero ` method in ` numpy ` ?
practially all indexing functions use `` nonzero `` , which btw is one of the most heavility optimized functions .
well you can simply access the subfeatures field in the apply then .
Then I would like to translate the remaining variables in a numpy ndarray .
You mean like use [ ` drop_duplicates() `] ( #URL ) , you could calc std deviation using ` df.std ` and then drop those columns , it sounds like ` drop_duplicates ` would do what you want
Setting new MultiIndex into pandas DataFrame columns gives ValueError : all arrays must be same length
This tests whether you are mutating the input in the apply or not .
Just use `` apply `` or iterate over the groups .
Python Pandas merge samed name columns in a dataframe
What I would like to do is merge those same name columns into 1 column ( if there are multiple values keeping those values separate ) and my ideal output would be this #CODE
In an excel pivot table , I can do this by checking the ( blank ) box when selecting the columns to be included in a pivot table .
Merge after groupby
I have a dataframe ( df ) and trying to append data to a specific row
The goal is to compare the Fruit at Rank 1 to each rank and then append the value .
Right now i'm able to append to df but i end up appending the same value to each row .
I'm struggling with the loop and append .
Pandas : How to stack time series into a dataframe with time columns ?
What is the best way to stack it into a dataframe such as : #CODE
The ` pivot ` command reshapes the data , setting the times as columns .
I need to do ` groupby ` -based manipulations , especially ` transform ` s and ` aggregate ` s .
The situation somehow gets worse when passing from ` aggregate ` to ` transform ` .
I split the ` df ` in several pieces ( in a data-safe way ) , store them in ` hdf5 ` files , load them from 4 clients , manipulate the data in parallel , re-save in `' hdf5 '` and finally merge the result in the final ` df ` .
Using ( apply / aggregate ) is nice for a generalized function evaluation , but pandas cannot make too many assumptions about what is going on in the user function , and these are evaluated in python space .
Consider for instance : ` df.groupby ([ ' Day_id ' , ' Frame ']) .apply ( lambda x : x [ x.Avg_U > 0 ] .Speed .mean() )` or ` df.groupby ([ ' Day_id ' , ' Frame ']) .transform ( lambda x : len ( x ))` , I couldn't find yet a smarter vectorized form .
that's what `` transform `` does
you need to use the lastest from master for the transform speedup
`` .asfreq `` converts the freq on a regularly sampled series , you prob want to `` .resample `` , what are you trying to do ?
use resample ; asfreq doesn't make much sense on an index with out a frequency
retaining order of columns after pivot
I do a pivot : #CODE
, then replace missing values nan with zeros : #CODE
where columns were sorted lexicographically during the pivot .
In the pivot , add the " values " argument to avoid hierarchical columns ( see #URL ) , which prevent reindex to work properly : #CODE
then , as before , replace nan's with zeros #CODE
and finally use reindex to restore the original order of variable _Article across columns : #CODE
E.g. replace the 1 in the group by 99 only if the val == 0 is in that group .
I wish to replace the val 1 by 99 only if the val 0 appears in that group .
transpose() doesn't seem to be helping . neither does reindex .
: for x in range ( len ( data )): df= ... append each to a list , or whatever you wanna do with it
Like cols = map ( int , cols ) to make it work .
` df.to_sql ( ' Tweets ' , conn , flavor= ' sqlite ' , if_exists= ' append ' , index=False )`
How to improve the speed of groupby / transform ?
when u assign the rhs is automatically aligned to the lhs , that is , the matching labels are picked out . this is a feature ! in this soln u are just assigning values ( a numpy array ) so their is nothing to align ( which in this case is what u want )
Search and replace missing lines in the data structure by pandas
The location of these missing lines ( the missing of values at time points ) and replace them with the lines with nan .
Second , if you resample , then the time steps are regular , and so can be factored outside of the sum to get cumulative flow - you can just need to multiply the whole column by the time step .
Precisely , is the resample function with the " mean " parameter do a weighted average ( to adjust to the irregularity of the time step ) ?
The pivoted table will have a MultiIndex column , you can access groups relatively intuitively , for example : #CODE
Let's say I have a large dataframe ` large ` that has a MultiIndex on the rows .
In particular , ` small ` has fewer distinct values in the 0th level of its MultiIndex on the rows than ` large ` .
What I want is to get a list of the distinct values in the 0th level of the MultiIndex of ` small ` .
( For example , ` large ` might have hundreds of names in the ` Person ` level of its MultiIndex , and so I might not know what I'm going to get when I filter down to just the names that start with `' Jo '` .
Take a look at ` shift ` , which as the name implies shifts values up / down the index .
Using .loc with a MultiIndex in pandas ?
Does anyone know if it is possible to use the DataFrame.loc method to select from a MultiIndex ?
I've looked around and there doesn't seem to be any clear cut way .
but when I do this , the ` len ` column gets automatically cast as ` np.datetime64 ` , which I assume is happening because that's the dtype of the original ` date ` column : #CODE
I could go back and reconvert this ` len ` column to nanoseconds since GMT epoch , but that is pretty ugly and I feel like there must be a better way .
use `' size '` ; this is currently an API bug ( in that the ` len ` should just be translated directly to ` size `) , see here #CODE
How to resample a Pandas multi-index data frame via methods depending on the column name
I can resample ` g ` successfully using one method across all columns , e.g. by ` g.resample ( ' 6H ' , how= np.sum )` .
How can i resample ` g ` with different methods for each column , e.g. by summing the ' A ' columns and averaging the ' B ' columns ?
If you start with f , you can use a groupby with a TimeGrouper to do the resample " manually " : #CODE
To get from g to f you can reshape with stack : #CODE
Another approach , which might be " simpler " , in that you actually do the resample , is to make the dict from the columns : #CODE
pandas : groupby and unstack to create feature vector for classification
You're looking for ` pivot ` : #CODE
You might like to reindex the columns ( based on all the questions ) if you're not surjective .
Note : Originally this answer suggested ` pivot_table ` ( which uses an aggfunc on repeated values , by default mean , and that's not what you want here - as @USER points out ) , it offers some other additional features over pivot but is a little slower : #CODE
I have this feeling that in older versions of pandas , pivot was sensitive to NaN so you had to use pivot_table ...
@USER great point , perhaps ` pivot ` is the correct function to use here ( in my experience it's a little more sensitive though ) ...
@USER I have this feeling that pivot __used__ to not play well with missing data , but maybe I'm making that up .
Thanks , updated the answer ( definitely pivot is correct here )
Filling missing lines with " nan " with pandas reindex
As long as I know from the information about reindex function , missing holes should be filled with something ( default is " NaN ") .
Ideally , I would like to replace the last two lines with something similar to this : #CODE
if you combine this with another apply , you'll get info for the total columns #CODE
and using this as the column mask will give your result : #CODE
Python pandas join on with overwrite
I realize this question is similar to join or merge with overwrite in pandas , but the accepted answer does not work for me since I want to use the ` on= ' keys '` from ` df.join() ` .
Basically I want to replace the ' values ' in the ` df ` with the values in the Series , by ` keys ` so every ` keys ` block gets the same new value .
To my knowledge , pandas doesn't have the ability to join with " force overwrite " or " overwrite with warning " .
You can merge series of different sample frequency and ` pandas ` will align them perfectly .
It's designed to handle class labels in classification problems , not arbitrary data , and any attempt to force it into other uses will require code to transform the actual problem to the problem it solves ( and the solution back to the original space ) .
this is a good way to transform data once , but what if i want to reuse this transform on a validation set . you would have to fit_transform again and issues could arise such as my new data set not having all the categories for all variables .
numpy : aggregate 4D array by groups
I've come up with this , using itertools , to find mid-day timestamps and group them by date , and now I'm coming up short trying to apply imap to find the means .
You need to access the values which returns the inside object ; ` concatenating ` coerces back to an actual numpy array , then aggregate ( mean in this case ) #CODE
Since not sure what your end output should look like , just create a time-based grouper manually ( this is essentially a resample ) , but doesn't do anything with the final results ( its just a list of the aggregated values ) #CODE
You can get reasonable fancy here and say return a pandas object ( and potentially ` concat ` them ) .
I am trying to normalize a large ( about 900 MB ) json file into a pandas DataFrame using the json_normalize() function .
And when I use the ` .groupby ` function to group the data by another attribute ( when these Series are a part of a DataFrame ) , I get the ` DataError : No numeric types to aggregate ` error when I try to call ` .agg ( np.mean )` on the group .
I'm pretty sure there's nothing built in to ` pandas ` , but if you have the full stack installed you can use ` scipy ` : #CODE
Here is the solution I came up with but I really wanted to avoid using list and append and take advantage of a generator instead but not yet comfortable enough working with generators .
I am trying to get a rolling cumulative product to a series in pandas .
This is just a rolling regular product , isn't it ?
` rolling_apply ` the ` prod ` .
I have an enormous timeseries of functions stored in a pandas dataframe in an HDF5 store and I want to make plots of a certain transform of every function in the timeseries .
I'd like to consolidate all 3 data frames , so they all contain rows with indices that are present in all 3 of them .
Take a look at concat , which can be used for a variety of combination operations .
Here you want to have the ` join ` type set to inner ( because the want the intersection ) , and ` axis ` set to 1 ( combining columns ) .
Interestingly ` merge ` is faster for larger dataframes than concat , I compared concat with merge on a 40000 row dataframe and saw this ` 100 loops , best of 3 : 6.3 ms per loop `
` 100 loops , best of 3 : 4.87 ms per loop ` when comparing concat with merge
Use ` merge ` and pass param ` left_index=True ` and ` right_index=True ` , the default type of merge is inner , so only values that exist on both left and right will be merged .
So we merge all of them on index values that are present in all of them and then use the index to filter the original dataframes .
Merge seems like an interesting alternative to concat .
It seems ` merge ` is more versatile .
in either case , it is plotting your data against ` np.arange ( len ( data ))` so you can use ` set_xlim ` .
Shouldn't the source code be ` if len ( args ) == 2 : ` and ` elif len ( args ) == 3 : ` in lines 1835 and 1837 respectively ?
Hoping that this is an allowable SO question but I am hoping to get some advice on how to convert the below code which processes lines in a file to produce a dataframe into one that uses generators and yields because this implementation using list and append is far too slow .
Here is the solution I came up with but I was really hoping to avoid using very slow lists and append operation .
You can explore the possibility of using ` .str ` method , either you can extract the numbers using ` regex ` , or take a slice ` .str .slice ` , or like in this example , replace ` days ` with a empty string : #CODE
Or ` split ` instead of ` replace ` , e.g. ` df [ " Days_To_Maturity "] .str .split() .str [ 1 ] .astype ( int )` .
Anyone know how I can make it replace those parts correctly in pandas ?
One thing is you forgot to use ` str ` again for the second replace .
This does not appear to be adequately documented ; the docs mention that ` split ` and ` replace ` " take regular expressions , too " , but doesn't make it clear that they always interpret their argument as a regular expression .
I found a work around too , by doing ` .str .replace ( u " ( " , "") .str .replace ( u ")" , "")` and then adding a second replace to remove / change the text part .
` strip ` will still work if there is no ` % ` , it just won't do anything if there's nothing to strip .
Use resample function #CODE
ggplot expects the data to be in ' long ' format , so you need to do a little reshaping , with ` melt ` .
I really need to learn all the possibilities with map
followed by the the ' map ' function
There is a way to use ` df.apply ` -- there almost always is -- but I think ` apply ` should be avoided when possible because it is often a relatively slow alternative .
and when I try to apply pct_change : #CODE
I guess u want unique values in a certain level ( and by level names ) of a multiindex .
The valid API access for the * current * contents of a MultiIndex is indeed get_level_values .
I can transpose it , and add a new column with values easily : #CODE
However , if I add a date index to original dataframe , then transpose it , I can't add any new columns to the transpose .
Very ugly , but I was able to replace the functionality I needed by using ` df1.ix [ df2.index , ' UTCdatetime '] = df2.ix [ df2.index , ' UTCdatetime ']` after first combining the indices into df1.index .
Use a combination of ` idxmax() ` and ` loc ` to filter the dataframe : #CODE
` groupby ` will give you unique groups -- essentially what you have in your ` Group ` column , but a ` MultiIndex ` will get you closer to what it seems you want .
For each row in this dataframe I would like to count the occurrences of each of C1 , C2 , C3 and append this information as columns to this dataframe .
But is there an ` apply ` approach that can achieve this in a compact fashion ?
You could apply ` value_counts ` : #CODE
Merging ( inner join ) these two dataframes should work : #CODE
when i use a merge , I get something like this .
You can just apply ` dropna ` to ` a12 ` before ` merge ` : #CODE
PS I also had an issue with combine_first and tz ( pandas tzinfo lost by combine_first ) - doesn't appear to be related but just in case .
Usually If I needed to do something like this , I would keep a separate column of ' tz ' info .
Localize the timezone after the series is generated by passing the ` tz ` keyword argument to ` date_range() ` : #CODE
Pandas : apply tupleize_cols to dataframe without to_csv() ?
I was under the impression that this was the intended use of the loc function but it actually adds rows .
This is not a bug , see my comments here : #URL `` loc `` acts like a `` reindex `` when presented with a slice / index-like .
pandas apply np.histogram to reshape dataframe
So ` apply ` doesn't know how to map the return values .
Here is another way ( and conceptually how to think about apply ) #CODE
I knew I was missing something about the internals of ` apply `
pandas multiindex shift on filtered values
` diff ( 1 )` produces this error : #CODE
while ` shift ( 1 )` produces this error : #CODE
You can also do ` diff ` but I think this line is unnecessary ` g [ ' d0 '] = g [ g [ ' alert_v '] == 1 ] [ ' timeindex ']
return g [ ' td '] = g [ ' d1 '] - g [ ' d0 ']` as ` shift ` already calculates the difference so you should just return this
For multindex group , select rows , diff , and insert new column paradigm : this is how I got it to work with clean output .
shift throws key error , so just sticking with diff() #CODE
It is not clear to me how to correctly pass the fixed ` y ` to the function while having ` apply ` iterating through the ` x ` columns ( ` x1 ` , ` x2 ` , ... ) .
The function you pass to ` apply ` must take a ` pandas.DataFrame ` as a first argument , you can pass additional keyword or positional arguments to ` apply ` that get passed to the applied function .
Then , you can use ` groupby ` and ` apply ` like this #CODE
Here the comp_name and date form multiindex .
Use ` groupby ` and we can pass a dict of functions to apply to each column , for ` WL ` column we apply ` count ` from ` pandas.Series ` , the ` all ` applies a test on all values and returns ` True ` if all values in the series are ` True ` and ` False ` otherwise .
In order to assign these values back to the original dataframe you can use ` transform ` , unfortunately I couldn't figure out how to apply different functions to different columns as transform won't accept ` agg ` function or a user defined function .
Is there any way to pass along columns without any change in aggregate ?
sounds like you want ` transform ` this will return an object that is indexed same as the orig df , I don't know if it will accept ` agg ` functions , I cannot get that to work , maybe post as a different question specific to that requirement
@USER I found a way to do this by calling transform twice let me know if this answers your question and if so you can accept as an answer by clicking on the empty tick mark under the vote arrows
Pandas groupby apply how to speed up
and if ` objs ` is a list that contains above results you may join them with : #CODE
This should align on the levels ( though their is some ambiguity how to do it , e.g. on which level ) .
Python pandas boxplot from dataframe
I have the following dataframe , which has a multiindex .
I thought I could groupby run then create a boxplot to show the variance between the different runs , but that doesn't seem to work .
how do I create a boxplot from it ?
This clearly is a bug , ` aggregate ` will try to convert the result to the same ` dtype ` the original ` DataFrame ` has .
Here the ` sum ` will return ` 1 ` and ` bool ( 1 )` is ` True ` .
If both of the values in ` ok ` are ` False ` s , the result will be ` False ` ( ` bool ( 0 )`) .
Then , we ` melt ` the dataframe to merge the rows and columns together ( i.e. ' Jan ' column and ' 1997 ' row become a single ' Jan 1997 ' row with the correct percentage value ) .
Thanks . melt is what I was looking for .
rolling apply for a binary ( or n-ary ) function in pandas
I wanted to take a pandas DataFrame that has two columns and calculate rolling covariance between two of the columns .
To do this , I have the following function that I would like to use a rolling apply with - all this does is calculate covariance assuming zero mean if not centered and calculate the usual covariance when it is centered .
I would like to use a pandas rolling_apply function to calculate the uncentered rolling covariance using my custom function , real_cov_uncentered .
The trouble comes when I try to cast transform the table from a single column of space-delimited strings to a DataFrame of 8-b it integers .
It is possible that I need to replace ` c ` with ` c.getvalue() ` in the read_csv line , but when I do that , the interpreter tries to print the contents of ` c ` in the terminal !
There is a ` map ` function for pandas for doing this , so you would just do something like : #CODE
This will assign a new column ' col_id ' to your dataframe and map the string values to their int counterparts .
I thought it was a datatype issue ( i.e. float vs bool ) , but tweaking that didn't resolve the error .
` swap =p d.Series ( { ' swap ' : False} , index =d ates )` results in a series of ` nan ` s in ` float dtype ` , not the intended ` bool ` type ` False ` .
merge the describe method output in pandas
I'm using pandas to do data munging and I can't seem to figure out what seems like a basic merge .
I want to merge these together is order to get an output that looks like the following : #CODE
This is a rather simple case of ` merge ` , in which you can supply the suffixes with the additional ` suffixes =( '' , ' 2 ')` argument : #CODE
For this I am doing a groupby and call diff and then mean on the grouped data #CODE
Then join them up at the end .
You can just call ` apply ` and access the ` time ` function on the datetime object create the column initially like this without the need for post processing : #CODE
I have one weird issue for transfering these lines to a multi-column frame : ` df.loc [ mask , ' price ']` throws a ` KeyError : ' Key length ( 2 ) was greater than MultiIndex lexsort depth ( 0 )'` .
I just created a [ pastebin with the exact frame I am using ] ( #URL ) , but when I recreate the DataFrame from there , ` df.loc [ mask , ' price '] *= 2 ` works flawlessly .
it does not apply operator by elements , but returns a 2*n DataFrame of NaNs : #CODE
this uses indexing by column name , and doesn't use logical operators on columns , rather than that it traverses rows with apply function : #CODE
if i understand your problem correctly , you want ` merge ` these 2 dataframe on ` index ( date ) , Product , Hub ` and obtain ` Period ` from ` data_1 `
To join on index + some columns , you can do this : #CODE
Generate your pdf of matplotlib figures as before and then insert pages containing the dataframe table afterwards .
if precision is to decimal place , I'd multiply it by 10 and truncate maybe .
If you do have a multi-index then you can get generate a mask using the index level 0 ( the first ) and use this to select the values : #CODE
The mask itself contains all the level 0 values for each row : #CODE
How to resample data in a single dataframe within 3 distinct groups
I've got a dataframe and want to resample certain columns ( as hourly sums and means from 10-minutely data ) WITHIN the 3 different ' users ' that exist in the dataset .
A normal resample would use code like : #CODE
I want to have each resampled individually rather than having a ' lump ' resample from all ( e.g. 9.00 AM on 2014-01-01 would have data for all 3 users , but each should have it's own hourly sums and means ) .
You can ` resample ` a ` groupby ` object , groupby-ed by URLs , in this minimal example : #CODE
I figured out the ` openpyxl ` dependency problem and now it's working fine :) The output of ` print df.head ( 5 )` now prints a table but the table is cut two times with a \ is that just to make it easier to visualize longer rows on a small screen ?
Since ` sklearn_pandas ` doesn't currently support estimators accepting an ` y ` vector with labels , you will have to use it only to transform all features to a Numpy matrix and then use the ` KNeighborsClassifier ` in a separate step .
If you want to transform then estimate a model , use a plain ` sklearn ` ` Pipeline ` with the dataframe mapper as first step .
thanks for the reference although I haven't been able to apply it to my specific problem .
You may be able to apply this to linear models as well :
result [ mask ] = op ( x [ mask ] , y )
Now , if I delete this ' date_delta ' setting , then it compiles but the problem is that len ( result.params ) = # of unique dates , instead of the number of parameters for one linear regression performance .
should I replace all 0 values with np.nan ?
I could also ask : How do I merge a Python object ( ` meta : { ... } `) into a serialised JSON string ( ` df.to_json() `) ?
It worked fine with the test data ( 200 lines ) but gives me the following error when I apply it to the real data ( 20 million lines ): #CODE
So far I can get them to graph on top of each other but not stack .
How do I get them to stack ?
I ended up just not using pandas to append to the db .
use a lamda apply to pass groups to the function
Anyhow , to get the differences of the TimeSeries , I found using ` shift() ` to get the time differences threw a ` StopIteration ` error , using ` diff ( 1 )` threw no errors .
Havent explored the ` transform ` function too much ( dont think I could get it to work ); does returning modified groups in functions remove the need for transforms / broadcasting .
You can use ` timestamps ` as index for ` trades ` instead , and do a ` resample ` .
Could you update your question with your new comment as it clarifies things more , also explain how you want to merge the rows in the situation where you have fewer rows in A or B
This will work by calling ` apply ` and passing param ` axis=1 ` to apply it row-wise : #CODE
You simply ` groupby ` your ` time ` column and then apply the ` mean ` method to each element .
Try using ` transform ` , e.g. : #CODE
Surely , this can be done more compactly with Pandas ( pivot ? ) ?
Yep , it can be done in a very concise way using the build in ` cut ` function :
Thanks for the ` cut ` and heatmap suggestion !
Is there a concise solution with ` pivot ` ?
" However , when I use ` loc [ 0 ]` I get additional metadata : #CODE
Pandas , performance on multiple rolling statistics on different time intervals
I am new to pandas and would like to know how to best use time bounded sliding windows and rolling statistics calculation .
I process a continuous stream with different rolling data calculations ( weighted average , mean , sum , max , oldest ... ) within different time windows ( 1hr , 4hrs , 1day , 1week ... ) also grouped by different item ID .
An output stream is produced for each item with its own rolling statistics but also with statistics from similar items ( data are linked on closest variable time spans and previously identified similar items ) .
I currently created a custom code without using pandas where the huge speed improvement is due to : calculation of rolling statistics using differential calculation only ( ie . compute difference on new data and data discarded from sliding window ) , linking of the variable timespan of similar items as it happens in the stream .
does pandas calculate each rolling statistics on all the sliding window's values or does it do differential calculation on new / old values ?
Then , when creating " custom functions " for rolling statistics could I also do differential calculation to avoid the huge cost of re-processing all the values ?
what is the most efficient way to declare multiple rolling statistics on several time windows ?
output : for each item , I output its own rolling statistics and statistics from similar items but timespan are variable ( from 10mn to 40mn ) .
you'd have to show some samples and what you expect . rolling stats are not on-line ( e.g. incremental ) .
I think you provided the answer : if rolling statistics can't be incremental , I doubt I could achieve similar performance because each data is computed on all values then ( ie . in my case , for a month lag data it is nearly 3000 values each time ) .
#URL ( _setitem_with_indexer )` , I strongly suspect my nested loop assignment with ` loc ` to be the culprit .
Why not -- after labeling the interval's trial number ( and instead of creating all new column's with NaNs ) -- you assign the appropriate values via some sort of left join instead ?
( i.e. using pandas ** merge ** function -- is there some reason why that's not possible ? )
Pandas Merge on Name and Closest Date
I am trying to merge two dataframes on both name and the closest date ( WRT the left hand dataframe ) .
From the above question it doesn't seem like there is a way to do this with merge but I can't see another way to do the two argument join that doesn't use the pandas merge function .
Is there a way to do this with merge ?
I will post a copy of what I have tried but this was trying it with an exact merge on date which will not work .
You won't be able to merge using a partial match , you'd have to merge what you can and then perform a lookup for the other rows , I've done this before where there were inexact matches .
You have to write some function and then apply it row-wise to your merged dataframe
I was kinda implying that you'd need to try to write the function yourself , basically the elements are 1 . does a date exist , if not then do a searchsorted call on the data to find where if I was to insert the date , what the index value would be , then use this index value plus the previous value to see which is closest and return it .
I sure still would have liked to have a way to do it in merge .
Merge the two DataFrames on the new ` closest ` date column #CODE
also you can just drop the time portion by doing ` df [ ' date '] = df [ ' date '] .apply ( lambda x : x.date() )`
I am working with payroll data and need to aggregate tax data to get current to date , quarter to date , and year to date values .
you don't need to strip the time when you are resampling , FYI , that's the point of resampling by say ' D ' it ignores the times .
and I'm trying to apply a transformation in order to get a dataframe that looks like the following #CODE
I have tried using melt , groupby , and pivot_table but with no luck .
One way would be to assign to an empty dataframe the ' ID ' and ' Wt.1 ' columns to the empty dataframe as the target ' ID ' and ' Wt ' columns , this has a minor advantage in that you don't get a messy append at the end where you have ` NaN ` values and both ' Wt ' and ' Wt.1 ' columns .
what happens if you just do ` pd.read_csv ( ' data / training.csv ' , sep= ' \t ' , index_col=0 )` does this read the values correctly ?, also what happens if you drop the ` index_col ` param , it looks like it is unnecessary seeing as you are assigning a new one later , by default it will assume your csv has no index column so if that is your intention then remove the param
Now you want to wrap it in a new ` DataFrame ` with new timeseries index , ` pandas ` will try to match and align the original index with the new one , but there are no matches .
How to use a for loop to append the data frame generated from a function in python
My question is how can I apply this function to several classifiers and append their result as a long data frame like #CODE
I also tried map function in array like #CODE
It works but just give me a larger list and I don't know how to transform it to data frame .
But I am still a little bit confused while ` i ` is in ` hhh ` why it does not do the concat or append as your code does ?
I finally managed to do it by doing a reset of the index , apply , and set again :
You can simply use ` slice ` , and if the column is not ` str ` you need to map to ` str ` as well : #CODE
If the column is ` Timestamp ` you may also , map to ` pd.Timestamp.time ` : #CODE
I have already written the pandas program and was just going to insert it into the larger script so it was shown in the subplot .
What I'd like to do is normalize the values to the sum ( along the major_axis ) at the first timestep , i.e. , I'd like to do #CODE
yes , it works , but is not what I need ;) I want to normalize each * item * individually , so simply taking the ` max() ` won't do .
but still i would like to aggregate the group first and calculate the mean .
A simple method would be ` df.loc [: (( df [ ' A '] == 0 ) & ( df [ ' C '] == 0 )) .idxmax() ]` but this doesn't apply for every id
@USER you can just groupby id then apply idxmax ( filter before the groupby )
you are using in append mode .
Sorry I'm pretty new to python and stack overflow
Optimizing Pandas groupby / apply
If you need a transform of this ( e.g. the same values for each group . Note that with 0.14.1 ( releasing this week ) you will need to use this soln here , otherwise this will be pretty slow ) #CODE
I suspect you prob want to merge the final transform back into the original frame #CODE
I've been trying to apply your code to my dataset but keep running into MemoryErrors .
how to unstack ( or pivot ? ) in pandas
I thought I could unstack df2 to get something that resembles my final dataframe but I get all sorts of errors .
I have also tried to pivot this dataframe but can't quite get what I want .
There is still a bit of cleanup to do if you want to convert the index level " variable " into a column called " HOUR " and strip out the text " HOUR " from the values , but I think that is the basic format you want .
The ` stack ` method turns column names into index values , and
the ` unstack ` method turns index values into column names .
So by shifting the values into the index , we can use ` stack ` and ` unstack ` to perform the swap .
It has helped me better understand stack / unstack .
Now , I completely understand why pandas introduces NaNs to align the data .
I am not a pandas expert , but your pivot does not look bad .
If the measurement results had different names for different runs ( e.g. ` f1_1 ` , ` f1_2 `) , you could use ` concat ([ df1 , df2 , df3 ]) .sort ( ' t ')` , but still you'd have the ` NaN ` s there .
I would like to insert 3 rows after each grouping in column `' a '` .
What you want to do is not really an insert operation , as the data structure behind the ` DataFrame ` does not allow simple inserting .
( Or you can concatenate instead of append , if you find it easier . ) One thing to think of is what you do with your indices .
If you do not use them , you may ignore them (= create new as needed ) by using the ` ignore_index=True ` keyword argument on ` concat ` or ` append ` .
Just ` concat ` the inserts that you want to insert in ( and they will be appended in the rear , or ` df.append ( the_insert )` , which does the same thing ) and ` reset_index ` the resultant to get things in the right order : #CODE
I want to calculate rolling cumulative sums for each OrderId and put them into 2 new columns with respect to Side column , CumAmountBuy and CumAmountSell , where TimeSecO > TimeSecOT .
Effectively , I will try to join m1 and m2 together into a single dataframe using merged / join .
I thought about this but actually my m1 has 70,000 + records and my m2 has 4000+ records I am just afraid that if I merge them together the computer will run out of memory .
I have a pandas dataframe with mixed type columns , and I'd like to apply sklearn's min_max_scaler to some of the columns .
I know that I can do it just in pandas , but I may want to eventually apply a different sklearn method that isn't as easy to write myself .
how about ` len ( ts ) == 0 ` ?
` len ( ts )` worked for me ` ts.empty ` didn't
I use len function .
It's much faster than empty() . len ( df.index ) is even faster .
I have made a time-indexed ` Series ` of ` 1 ` s for valid data and ` NaN ` s where the data are invalid so I can use this as a mask on my ` DataFrame ` .
If your program takes 15 seconds when it should take less than one second , then the odds are 14:1 that a * single stack sample * will show you why it's taking that time .
Using apply is always the last operation to try .
Brilliant use of cut !
I have a dataframe with a multiindex .
How do you reorder and / or rename a specific column in a multiindex ?
How would you apply this to a single column ?
@USER The dataframe is generated from a pivot table that has the index reset .
Only multiindex on the columns- not the index .
And this hurts me in a line or two when I try to do a merge command .
( Perhaps a misunderstanding of the ix command ) .
Querying a non-exist column with ` ix ` gives all ` NaN ` .
How can I apply this process to all the columns I want at once and produce a dataframe of it all ?
Maybe you could ` merge() ` ( like ` join ` in SQL ) two dataframes if you had year , month , day in grouped data .
` merge() ` can do ` left join ` like SQL and should duplicate data from group in rows in dataframe .
However , the above does not work for a ` boxplot ` ( the tick labels for the x axis are rendered empty )
How can I use ` boxplot ` in a way that allows me to use ` matplotlib ` date locators and formatters ?
The ` skiprows ` option of the ` pandas.read_csv() ` doesn't apply here , because the number of rows to skip is very inconsistent from one file to another ( btw , I use ` pandas.read_csv() ` and not ` pandas.read_table() ` because the files are separated by a coma ) .
How do I convert a groupby dataframe I created in order to drop duplicates by a group back into a normal dataframe ?
Instead of creating a groupby to drop duplicates , have you considered : #CODE
At the end I can normalize the length of lists ( for example with some values like ` NaN `) and create from these lists DF .
You can use pandas ` shift ` and ` loc ` to filter out consecutive duplicates .
+1 , I think you have to use ` shift ( 1 )` in place of ` shitf ( -1 )` to get OP more expected result .
Merge the two dataframes #CODE
Pandas drop duplicates if reverse is present between two columns
I'm using Pandas and I want to drop rows which are present twice but simply reversed like the following ... from this ...
now concat the 2 dataframes : #CODE
We then aggregate the data by taking the mean .
I thought I could apply a list of strings , but it does not work either , because it is ` not in the ColumnDataSource ` .
python pandas : groupby apply function looks at prior rows
so I've updated my code below , maybe I'm not understanding how apply works , but I thought this would execute twice ( once for each group ) .
At the moment , I'm struggling to merge this information back to the original dataframe .
Thanks for the assistance , but the actual calculation I'm doing is different , and much more complicated ( I just posted this as an example - if you can calculate row x relative to row x-1 , I could transform that into what I actually need to do ) .
Panda's DataFrame double transpose changes numeric types to object
Looking in the source code , I can see how these results arise : indexing falls back to slicing if the MultiIndex is nonunique , and slicing always works even if the values aren't present ( just returning a zero-length slice ) .
I have two dataframes that I would like to concatenate column-wise ( axis=1 ) with an inner join .
The default ` concat ` behavior is to fill missing values with NaNs : #CODE
I want to keep the duplicate indices from df1 and fill them with duplicated values from df2 , but in pandas 0.13.1 an inner join on the columns produces an error .
In more recent versions of pandas concat does what I want : #CODE
Or maybe I shouldn't be using ` concat ` at all ?
You can perform a merge and set the params to use the index from the lhs and rhs : #CODE
Ok , merge works in both 0.13.1 and 0.14.0 , so thanks for that solution .
Concat gives me the ' cannot reindex from a duplicate index ' error in 0.13.1 , and in 0.14.0 I get : ' ValueError : Shape of passed values is ( 2 , 5 ) , indices imply ( 2 , 3 )' .
At least merge works
I have version 0.16.2 and with multi-index none of the solutions does not work for me ; merge gives a dataframe as output but with many columns of the left array as NaN - and they should not be .
its not specific to apply , but more general in groupby , nor is ever point mentioned in the doc-string .
Or something like ` df.columns = [ ' blah{} ' .format ( i ) for i in range ( 1 , len ( df.columns ) +1 )]` , or `" blah " + pd.Series ( range ( 1 , 6 )) .astype ( str )` , etc .
I then use the mask to assign new values into those locations .
I tried to apply this method to each subset of data with a nested-loop script : #CODE
Grouping data frames and applying a function is essentially done in one statement , using the ` apply ` -functionality of pandas : #CODE
pandas : apply a function to the many columns of a large DataFrame to return multiple rows
Taking the idea from From this answer : pandas : apply function to DataFrame that can return multiple rows
The crazy concat part at the end is important because it converts a comma separated list into it's own rows .
The ' key36 / 62 / 65 ' each occur twice in that weird concat construct .
If you have " oddly-shaped " json , then you can either ` json_normalize ` when reading , or parse the columns which contain multiple columns after reading in the DataFrame ( e.g. using a Series string method or apply ) .
Note : ` astype ( ' int64 ')` would also work , or just leaving it as bool ...
However , I think I might use a transform here ( to extract each groups quantile and then mask off this ) , with something like : #CODE
d.update ( dnew ) gives a valueerror , cannot reindex from duplicate axis ?
I wish to aggregate , say , the first 2 readings for each sensor
After you sort the df , all your loc / ix methods should work .
Sorting will make ` MultiIndex lexsort depth ` ( in the error message ) from 0 to 3 .
I would like to map each file to its unique dataframe .
An even better solution is to drop the range : #CODE
just use an ` apply ` with a function that creates a dictionary based on the ` str.count ` of the substrings
That said , I am trying to find out if there is a way to basically do what was done in the link I posted ( randomize column-wise ) and apply that to rows .
My question : how can I join the results to display as desired ?
I tried merge , join , and append , but they do not allow for empty data frames ( which happens ) .
Alternatively , you may use ` groupby ` together with ` cut ` : #CODE
I would like to interpolate the missing values .
Tried using : fillna ( method= ' ffill ' , inplace=True ) which seems to work , however I need to interpolate :
the page you linked suggested interpolate ( method= ' linear ') but this does not seem to work ?
Tried using : apply ( pd.Series.interpolate ( method= ' linear ')) however I get the following error :
Unrolling / expanding a dictionary of iterables into a dataframe
Then ` pd.concat ` is used to stack the columns together on top of each other .
I didn't think of the concat trick !
I wouldn't be surprised to be able to do the concat even without the list comprehension , but I could not find a way .
Another one using ` loc ` and loops : #CODE
Python pandas , How could I read excel file without column label and then insert column label ?
I have lists which I want to insert it as column labels .
Python Pandas concatenate / merge DataFrames using index or column identifiers
I would like to use the ' pandas.concat ' method to merge two DataFrames , but I don't fully understand all ' pandas.concat ' arguments .
Question : How can I achieve the same result ( shown above ) using ' concat ' ?
I'm asking for the concat method since I'm hoping that there is an argument ( s ) that will rely on the indices of the two dataframes to concat / merge them .
If you know how to achieve the same result using the concat method please be so kind and enlighten me .
and you need to keep order using values from columns ` AAseq Biorep Techrep Treatment ` then use ` merge ` #CODE
The ' merge ' function is EXACTLY what I was looking for / missing .
` merge ` works similar to ` join ` in SQL .
Do I want to join / merge , concat , or append these two Pandas DataFrames ?
How would I paste these two tables together using join / merge / concat / append / add in such a way that the population age 0-14 , and 15-64 columns are side-by-side ?
Instead , assign to an indexer like ` loc ` or ` ix ` : #CODE
I've noticed that for a ` DataFrame ` with a ` PeriodIndex ` , the month reverts to its native ` Int64 ` type upon a ` reset_index() ` , losing its ` freq ` attribute in the process .
My approach is to run only 16 simulations ( 8 wind directions and 2 wind speed ) and interpolate the flow distribution from these results .
I would like to be able to aggregate the data and obtain a linear interpolation of the velocity field ( v ) at each point ( X , Y , Z )
I first append the dataframe with the unknown velocities to the first dataframe with all the simulation data .
I then cut this new dataframe by binning the wind directions .
Use ` merge() ` - it works like ` JOIN ` in SQL - and you have first part done .
The basic idea is to merge the two tables so you have the times together in one table .
My approach would be to groupby the first level of indicies , and transform with ` range ` to get an array of positions within in each group .
Ive been hacking away with update , ix , filtering , where , etc ...
` ValueError : cannot reindex from a duplicate axis `
yeah , i tried that , it gives ValueError : cannot reindex from a duplicate axis do you know what this is about ?
My idea was to apply this function ( or similar ) column wise .
how to apply preprocessing methods on several columns at one time in sklearn
My question is I have so many columns in my pandas data frame and I am trying to apply the sklearn preprocessing using dataframe mapper from sklearn-pandas library such as #CODE
I'd like to insert a field into each record that counts how many records there are in the table where :
active [ i ] = len ( a [( a [ ' Start '] <= a.loc [ i , ' Start ']) & ( a [ ' End '] > a.loc [ i , ' Start '])])` .
Merge two DataFrames with some equal columns
I need merge both DataFrames in something like this ( ovewriting values on df1 that are empty on df2 , adding extra columns and rows that not exists on df1 ): #CODE
I tried using ` merge ` getting something like : #CODE
I don't know if I'm on the right path and should merge the suffixed columns or if there is a better way to do this .
Added ovewriting values on df1 that are empty on df2 , adding extra columns on df2 that should be present on df1 after " merge " and rows that should be appended on df1
The on key actually is actually only used to join the two dataframes when the left_index and right_index keys are set to False - the default value .
Otherwise it will just join the identically named columns that are found from the on value .
I dug a little deeper into the capabilities of merge and found the solution .
Pandas : How to unstack a series
Assuming that your two first columns form a MultiIndex , you can simply use unstack : #CODE
Is it possible to edit index using something like replace ?
alternatively , you may just ` map ` the index to ` int ` : #CODE
Pandas : Merge hierarchical data
I am looking for a way to merge data that has a complex hierarchy into a pandas ` DataFrame ` .
In reality I would probably replace it by a time-stamp of when this particular simulation was run .
Merge Observables
With the data-frames available , I join all the observables into one big ` DataFrame ` .
The ` merge ` function only works on pairs of data-frames , and it requires them to have at least one common column .
So , I have to be careful about the order of joining my data-frames , and if I were to add an orthogonal observable then I could not merge it with the other data because they would not share a common column .
I tried ` concat ` but it wouldn't merge common columns .
Merge All Data
Finally , I merge my data with the parameters .
Thanks to Jeff's answer I was able to push all my data into one data-frame with a generic merge .
And then I can merge all of them together by reduction .
This way you can easily concat more frames with different parms .
It seems messy , but based on your data might not be an easier way . you can prob get away with `` df_position.join ( df_weight )`` though ( its the same as your merge , but ' cleaner ')
I was able to merge them all together with generic merge operatinos by using your answer and adding the parameters as common columns to all data-frames .
Split a data frame using groupby and merge the subsets into columns
I would like to split the data frame into subsections by name and then assemble ( by means of an outer join ) the score columns into one big new data frame and change the column names of the scores to the respective group key .
So ` unstack ` seems the method of choice .
The function you look for is unstack .
In order for ` pandas ` to know , what to unstack for , we will first create a ` MultiIndex ` where we add the column as last index .
` unstack() ` will then unstack ( by default ) based on the last index layer , so we get exactly what you want : #CODE
It works but ` unstack ` provides a faster way .
Even though ` pivot_table ` is not as fast as ` unstack ` , for my real data frame with 10 M rows this method brought down the processing time from 28 min to a respectable 17 s , thank you :)
Also , if you insert ' df [ ' Yield '] = Px ( .05 )' , it creates the new column with the Px() for that Rate .
Note that the dataset is sorted by groups , which you need to do if you want to apply the following .
Unfortunately it does not work for me : I get ` ValueError : level > 0 only valid with MultiIndex ` when trying to apply your line ` c = test.groupby ( level=1 ) .count() ` .
We use ` loc ` here to perform label indexing , see the docs : #URL
its not the date that I'm changing conditionally , but rather whether or not I'm going to append another variable to my FileName series .
When I try to merge the two dataframes I get an error because I'm trying to merge two different types .
you can drop the `` skip_footer `` option and drop from the frame after ( though this might cause dtype issues itself ) , you could also pre-filter to get rid of the footer . as an aside , you could help address those missing issues !
that's trivial . replace ` df ` with ` df [ df.columns [: 2 ]]`
I don't know of any pure numpy solution for what you want but you can use list comprehension and zip to transpose .
Use concat : #CODE
If you want to append as rows then remove the ` axis=1 ` param .
Some of my columns contain NaN values which I do not want to include into the z-score calculations so I intend to use a solution offered to this question : how to zscore normalize pandas column with nans ?
pandas groupby filter , drop some group
I need to filter these groups and drop all groups which have no time greater than 5 .
This would work ( using from_tuples to make the pandas multiindex ) , but seems hacky to me :P
I would first use concat to create a larger DataFrame : #CODE
I think the easiest way to append a new index is to add a new column and then ` set_index ` : #CODE
You can use dictionary ` pd.DataFrame ( { ' date ' :d at.Date [ i ] , ' result ' : fin_dat [ 1 ] } )` , or you can transform table using ` .T ` - ` pd.DataFrame ( [ dat.Date [ i ] , fin_dat [ 1 ] ]) .T `
@USER I agree , but in another function I need to take a data frame and compute various statistics and append those to the output , but I can't do that without changing the original it seems .
N #URL simplicity's sake , i'm using a toy example , because copy / pasting dataframes is difficult in stack overflow ( please let me know if there's an easy way to do this ) .
Is there a way to merge the values from one dataframe onto another without getting the _X , _Y columns ?
I'd like the values on one column to replace all zero values of another column .
I want to replace zeros in df1 with the values in df2 .
I subset based on the names that exist in df2 , and then replace those values with the correct value .
This seems like it would be especially useful for stack overflow , but I often find myself needing to share small dataframes and would love it if this were possible .
This dataset is then fed into dc.js where I aggregate up as needed for different charts .
( The actual precip levels for June next to the average for June ... etc ) For my other metrics I was just able to add a column in pands of the hourly average and aggregate up as needed ...
Since your precip_avg starts as " the average precip level for all Jan 1st 2pm across all years " this average will not change when you apply other filters .
Assuming I understand you , and you're looking for the indices where either the element is True or the next element is True , you can take advantage of ` shift ` and use #CODE
Using ` shift ( distance )` you compare two rows : #CODE
If I only want the Year month and day how do I drop the trivial hour ?
The quickest way is to use DatetimeIndex's normalize ( you first need to make the column a DatetimeIndex ): #CODE
From 0.15 they'll be a dt attribute , so you can access this ( and other methods ) with : #CODE
I've tried turning the series into a dataframe with a dummy column in order to try merge or lookup but for various reasons I can't get that to work .
Previously answered questions don't seem to apply .
' I thought about reindex but your reset set combos really made it easy .
( There are lots of other ways to convert from ` bool ` to ` int ` ( ` +0 ` , ` *1 ` , etc . ) but this is more explicit . )
replace x = cond ( x > 100 y 50 , x , y )
Using StackOverflow and the documentation I have only been able to find how to apply a function dependent on a single variable to more than one column ( using the axis option ) .
This will be much faster than performing an apply operation as it is vectorised .
Using apply and iterating should always be the last choice , if possible find a method that operates on the whole dataframe
If we ` append ` to a nonexistent value , then it gets spontaneously created .
This means that all you have to do to group by year is leverage the apply function and re-work the syntax
So , I can't do some of the time series operations in Pandas , like aggregate by month and so on .
Length : 51225 , Freq : None , Timezone : None
To drop a range : #CODE
To drop a row : #CODE
It is not that drop doesn't work for dates .
2 . waht should I do to cut some range ?
yes it works fine but I need to drop ' 2014-07-16 14:24 ' thnx
to drop a range in a middle of the df .
The way it is written right now it suggests that you want to drop a range , and that's why I answered it that way .
pls , look at the index . pls , look at the qstn . the request should drop everything before 2014-07-16 14:24 , but it doesn't . you gave the workaround but the problem is still there : df.drop() doesn't work with datetime index .
Dynamically creating variables , while doing map / apply on a dataframe in pandas to get key names for the values in Series object returned
I also know that ` df2.apply ( funct1 , axis=1 )` contains part of mycustom " names " ( ie feature values ) , how would I then build these names using map / apply ?
I will have the values , but how would I create the " key " `' P_ ' +feature_name+ ' _ ' +feature_value+ ' _C '` , since feature value post apply is returned as a series object .
I have only unique values in the df2 ( for a feature_name ) , will pivot help ?
If pivot will help , please also explain the intuition on WHY I need to pivot in this case ( with no duplicate values ) ?
Have you tried ` reindex ` ?
Also I think there are some problems with the code . word = feature_name + " _ " + feature_value - Feature_value is a tuple object , so we cant concat them .
I am not accepting , since I am still looking at a map / apply way ( possibly ) to solve this .
itertuples() is what worked for me ( worked at lightspeed ) - though It is still not using the map / apply approach that I so much wanted to see .
To answer your second question most pandas functions handle NaN's appropriately , you can always just drop them : #CODE
The thing is that I didn't want to drop these NaT rows as I wanted to subtract this date column from another date column .
Are you looking for [ ` transform `] ( #URL ) ?
Can pandas transform do that ?
Then , pass that into the ` groupby ` object with apply .
convert groupby to dataframe , join the groups again
you can iterrows() to iterate through rows , and use date_range() and concat method .
Find intersection between rows
If I want to find the difference between two consecutive rows in a pandas DataFrame , I can simply call the ` diff ` function .
What I want to do now is compute the intersection of each set in rowise pairs . in other words , I'd like to use ` diff ` , but supply my own function instead .
I've tried using ` shift ` , but it throws an error #CODE
You should just replace the CountIf() with :
the final line uses the apply method , with the paramater key set to 1 , which applies the method -first parameter - row wise along the DataFrame and Returns a Series which is appended to the DataFrame .
I've tried using combinations of ` groupby ` and ` apply ` , but I'm new to PANDAS and keep throwing Exceptions .
` reindex ` is also not in-place
pandas transform to a boolean column matching its maximum
The result of your ` .... groupby ( ' id ') .date .transform ( lambda ii : ii == ii.max() )` is a Series , but the index of that series is range ( len ( df )) .
Second step ( drop row ):
Please post the entire stack trace ** in your question** .
Do you have an idea how to solve this problem and how to further drop the respective row ?
I have a DataFrame and would like to only keep a specific percentile ( e.g. 25-percentile ) and drop all other rows .
I then drop the NaN rows and only the first quantile is left .
Why do you need to drop the rows ?
It seems like a fairly common issue for time series , but despite messing around with groupby , diff , and the like -- and exploring SO -- I haven't been able to come up with much better than the below .
You can do this with merge and groupby .
And if you want to keep rows from sample that have zero for a total around you could do the merge like this : #CODE
Summing over a multiindex level in a pandas series
Using the Pandas package in python , I would to sum ( marginalize ) over one level in a series with a 3-level multiindex to produce a series with a 2 level multiindex .
If you know you always want to aggregate over the first two levels , then this is pretty easy : #CODE
If distance between compared rows is constant ( for example 15 rows ) then you could create new " shifted " column using ` shift ( -15 )` and then you have both values in one row .
At 930 , i want shift ( -1 ) , 931 : shift ( -2 ) etc .
I resample the array to daily data and join it to my original frame .
Also , after you join , to divide the first half columns with the second half of columns , you need to break the dataframe into half and rename the columns again .
use for loop to concat dataframe to a larger dataframe
I want to concat the data frames together to have a larger one but somehow my function will only return the last step of the result rather than the merged result #CODE
but you didn't concat rate with anything else .
oh , you need to do rate = pd.DataFrame ( { " classfier " : classifier_i , " model " : index_i , " recall " : recall_rate , " precision " :p recision_rate , " accuracy " : accuracy_rate } ) to make it a DataFrame before you concat it .
next you just apply a lambda function that finds the union between columns .
Both indices having timezone , one with freq and one without : #CODE
this is a bug . need some code to figure out whether to recreate the tz ( e.g. it has to be on all pieces and they have to be the same ) .
I think this works as long as both are > 2 in length ( it is because an index of len ( 2 ) cannot have an inferred_freq , only a set freq ) .
Have my own version of concat in my code , where I'm exactly doing that .
Pandas apply and lambda function efficiency
For large datasets , you can use indexing on the base numpy data , if you're prepared to transform your column names into a numerical index ( simple in this case ): #CODE
where ` i.index ` could be different from ` range ( len ( i ))` if you wanted .
I am doing some data preparation with Python using Pandas and I am working with a dataset that has about 80 variables with missing values and I want to capture any patterns of missingness to cut down on the amount of missing value indicators I have but I am having trouble finding any good strategies for doing this . here is an example of what I am getting at : #CODE
why i failed to join two dataframes when using python pandas ?
what i need is to join d1 and d2 using the first colum , the result should be , which kind of DataFrame operation is needed ?
That won't work it looks what you want to do is to add just the last column which you could achieve using ` concat ` : #CODE
or you should ` merge ` : #CODE
If we analyse what went wrong with the join , you have a clash of columns which it cannot resolve unless you specify a suffix : #CODE
We can now drop the superfluous column ` 0_y ` and rename the added column : #CODE
You can use ` join ` operation on the ` frequency ` object you created , or do it in one stage : #CODE
why the dataframe generated using merge operation is not of 3x3 dimension rather than 3x5 ?
i follow the instruction #URL , but is confused when the merge colums is not of the same index . for example , column 1 in d3 corresponding to column 1 in d4 .
why the following merge could create an exactly 3x3 DataFrame , while the formmer can create a 3x5 DataFrame ?
I don't see your problem here : Your expected result is just a subset of the columns panda's ` merge ` provides .
Merge syntax is ` x.merge ( y )` .
` 0 ` is the join column and is same as you would expect .
From the merge command in your first example we know that 0 , 0_x and 1_y will have the same values .
In your first merge you are merging lhs on column ' 0 ' and rhs on column ' 1 ' but you have no identical values so it has to create two columns with suffixes .
In the second example you merge on column ' 0 ' , whereby you do have identical values so it doesn't need to create an additional column , however you still have a clash of column names for ' 1 ' and values so it has to create the additional columns with suffixes .
I think your confusion stems from the expectation that because you've specified the columns to merge on that it will then use these columns like an index and match the other columns against these rows , it will not .
I have a large dataframe with 11 columns where I want to replace NaN values with zeros , if every value in another group of columns is NaN , and otherwise to convert the number that is not null to an integer .
( Obviously you can drop the copy if you're okay with an in-place change . ) #CODE
Try structure all dataframes in one -- stack vertically and use key as index a level of multiindex .
I wanted to divide the data sets into groups , then perform a Cartesian product on each group , and then aggregate the result .
The ` apply ` operation of pandas is quite expressive , I could first ` group ` , and then do the Cartesian product on each group using ` apply ` , and then aggregate the result using ` sum ` .
The problem with this approach , however , is that ` apply ` is not lazy , it will compute all the intermediate results before the aggregation , and the intermediate results ( Cartesian production on each group ) is very large .
I want to group the data by ` id ` , and then do a Cartesian product for each group , and then group by ` cluster_x ` and ` cluster_y ` and aggregate the ` count_x ` and ` count_y ` using ` sum ` .
Intermediate result after the ` apply ` ( can be large ) #CODE
( E.g. I don't see why you can't move a summation into the apply to avoid expanding more than one group at a time , but maybe that doesn't work in a real case for some reason . )
Generally though I don't know if ` pandas ` can handle two ` groupby ` or more complicated operations lazily ( without expanding all the intermediate results .
But I hope ` pandas ` can also support arbitrary lazy expressions ( currently it only supports one groupby and aggregate , if I am not mistaken ) .
The desired output will be another column of just the STATE abbreviation , so I understand that I am trying to find an equivalent of a ` df.split ( ' , ')` and to append the second [ 1 ] index from that split for a Series or dataframe .
IIUC , you could use ` merge ` : #CODE
is there a way to insert ` s ` into ` df ` without creating a reindexed copy of ` df ` first ?
Use ` concat ` : #CODE
How to drop the index column of DataFrame ?
Hey Paul , I think you are correct ; however , attempting to output a row of data directly to CSV results in each output overwriting the last . the row " b.ix [ j ]" can be written to CSV with ` b.ix [ j ] .to_csv ( path )` , but I do not know how to append data to a CSV file
to append the row to the existing file .
Why are vectorized operations like apply so much quicker ?
`` apply `` is NOT vectorized .
`` iterrows `` is even worse as it boxes everything ( that ' the perf diff with `` apply ``) .
This is dependent on what is going on inside the apply expression .
Much better to create new structures and ` concat ` .
The examples I've seen using ' map ' or ' apply ' generally show one datatable which seems intuitive enough .
I think you misunderstand merging -- it takes the intersection of the column you merge on .
I will then merge and apply on each as you have done , aggregating to a new table .
To me , looks like the easiest thing is to merge on ` letter ` and then ` groupby ` .
Resample in a rolling window using pandas
I cannot apply a rolling window because this would first be daily and secondly I need to specify the number of values ( a rolling window does not aggregate by time frame , some posts addressed this issue but they are not relevant to my problem as the rolling would still be for each new day ) .
I cannot apply resampling , because then the sample would be every 5 months , e ..
Finally , as the function is not linear I cannot reconstruct it by first doing a monthly sample and then applying a 5 period rolling window on it .
So I would need a sort of resampling functionality applied to a rolling window defined by time interval ( not number of values ) .
One approach could be to combine several ( 5 in this example ) resampled ( 5 months ) time series , each with one month of offset and then align all these series into one ... but I do not know how to implement this .
User @USER is correct , in addition I only care of a monthly result , so I need to apply the same for June 2012 , July 2012 , etc .
Just program a loop or equivalent to cycle the start and end month values and apply your function to the values in the resulting dataframes .
Then , add 5 new columns for the 5 rolling periods that each date will cross .
Then - use the ` melt ` function to convert the data from wide to long , so each rolling period will have one entry .
You should be able to groupby the newly created column , and each date will represent the right 5 month rolling period .
Say I have a Series of data based on a symmetric ( distance ) matrix , what is the most efficient way to drop duplicates from the following series ?
Still , the [ higher voted answers ] ( #URL ) there apply to your situation .
` df [ ' E '] = abs ( df [ ' D '] * 0.5 )` , in the first line of the 4th code block .
I think the problem is that the map function from seaborn is passing the whole ` df [ ' E ']` list to matplotlib's errorbar function , not just the part that applies to that subplot .
I like to turn on the flag for `' append ' -- df.to_sql ( con=con , name= ' tablename ' , if_exists= ' append ')` Since the program does several small writes to the tables during the day , I don't want the entire table overwritten with replace .
Then you can drop the temp table .
Call ` apply ` on the dataframe ( note the double square brackets ` df [[ ' A ']]` rather than ` df [ ' A ']`) and call the string method ` isdigit() ` , we then set param ` axis=1 ` to apply the lambda function row-wise .
What happens here is that the index is used to create a boolean mask .
This will be considerably faster that using ` apply ` on a larger frame as this is all implemented in cython .
Just use ` diff ` e.g. ` df [ ' timestamp '] .diff ( 5 ) > 123ms ` or similar
Just make a list of the columns from the dataframes you want and use concat : ` df_list = [ test_1 [ ' Position '] , test_1 [ ' Force '] , test_2 [ ' Position '] , test_2 [ ' Position ']] pd.concat ( df_list , axis=1 , ignore_index=True ) .to_csv ( " filepath ")` if the order is as regular as you say you could easily write a simple for loop to build this list
Then i tried to append the DataFrame like this : #CODE
I have tried using the dataframe quantile method but there are a couple of missing data points which cause it to drop the whole column .
What is the best way to show how many True and False Values I have of both classifications and then drop some rows of data from the classification with the highest number ?
I would ideally want to drop 4 of the above dataset which has trustworthy value of 0 .
To drop last 4 rows with 0 #CODE
The grouping worked however when I drop data from tail I don't have 4 data points dropped .
When i drop 1 , 2 , 3 or any number from tail i end up with 29 false and 187 true .
something like ` A [ ' id '] = range ( len ( A ))` .
Do a left merge of A and B , which by default merges on common columns #CODE
Filter C to just those observations not found in B and drop the marker .
Now , I'd much rather not simply drop those columns from the df ( or a copy of it ) .
I suppose I could make a copy of the df , and drop those non-numeric columns before doing columns.values , but that strikes me as clunky .
and if they conform to the standard whereby the non-numeric columns of data are all ` object ` types ( as they are in my dataframes ) , then you can do the following to get a list of the numeric columns : #CODE
Pandas Series concat raises error
When I ran it on version ' 0.12.0 ' , it actually did the join correctly .
Now , how do I insert that graph into an Excel file using XlsxWriter ?
If on the other hand you want the matplotlib style charts generated by Pandas then export them as images and insert them into a worksheet using the XlsxWriter ` insert_image() ` method .
I tried using iloc or ix with the dataframe but that didnt work .
This returns a series , with true / false values for each stock , but I can't figure out anyway to usefully merge this back with the original dataframe and count the number of Trues for each date .
Do you want ` transform ` so that it returns an object with its index aligned to the original dataframe ?
like ` b = stock_data_df.groupby ( ' stock id ') .transform ( apply ( lambda x : x [ ' price '] > = pd.rolling_max ( x [ ' price '] , 20 )))`
Transform returns a series aligned with the original dataframe , this sounds like what you want .
Not sure if you so the edit , but I tried ` b = stock_data_df.groupby ( ' stock id ') .transform ( apply ( lambda x : x [ ' price '] > = pd.rolling_max ( x [ ' price '] , 20 )))` and then I get ` TypeError : ( ) takes exactly 1 argument ( 0 given )`
So I have am using Pandas to create a data frame with some columns which have types bool , int64 and date time .
In a pandas hist ( ) plot with sub-histograms , how to insert titles for the x and y axes , and overall title ?
Also , how to insert an overall title for the plot ?
` hist ` should return ` axes ` objects which can be used to set things like titles and labels .
edit - you can also use stack for same result
The following is basically what the pivot method does in case someone needs to do something similiar but more complicated like even more hierarchical columns .
You could probably use unstack to solve this as well
When I run the application , it reads the CSV and converts it to a Pandas dataframe , which I then use SQLAlchemy and pyodbc to append the records to my table in SQL .
df.to_sql ( ' Calls ' , engine , if_exists= ' append ')`
This is just a plotting question ( how to apply the axvline to each of these subplots ) .
I used unstack , which is essentially pivot ...
I was wondering if there is an equivalent way to add a row to a Series or DataFrame with a MultiIndex as there is with a single index , i.e. using .ix or .loc ?
Easier to reindex and / or concat / append a new dataframe though .
It constructs a boolean indexing array then uses nonzero , so pretty fast .
You can still use the ` plt ` methods , but they will apply to whatever the current plot is .
can you verify that an array you create on the fly works with ` hist ` ?
How to Join two separate text data files and align data over varying time intervals and avg
I need to match up ( align ) the data from one instrument to the data at the upper and lower position times ( duration either 7 or 8 minutes ) .
I want to be able to merge these two separate files and output into a new list .
But first I must align the data among those time frames .
How can you align dates with 6 days between them ?
Downsize series containing boolean data with resample
Currently , my resample uses ` last ` thus would overlook any system faults that occur on any row but the last .
What I would like it to do : if ' fault ' occurs on any row during the 15-min series , then the resulting timestamp after the resample would read ' fault ' else would read ' ok ' .
I know the solution reside in ` how= ''` of resample , but because I am new to numpy and pandas I can't figure out what to use .
Use ` resample ` to convert to 5 minute frequency .
and give it as the ` how ` argument of ` resample ` .
Use the function to resample ` ts ` .
What is the best way to merge these two dataframe such that it appends additional columns to position with the region it falls in if it falls in any region .
I'm trying to find an alternative solution where I load position and region into dataframes with a multi-index with chromosome as the outer index and then do the merge independently for each chromosome .
an a , or dog , replace , starting with that any character , with ' XX-XX ' , regardless of case .
So how would I find ALL instances of that condition and replace ALL of them with XX-XX ?
I'm assuming is what you're actually asking for is how you can override the format for all integers : replace ( " monkey patch ") the ` IntArrayFormatter ` to print integer values with thousands separated by comma as follows : #CODE
I have a set of tab delimited files that I have to go through read them , use them as pandas dataframe , do a whole bunch of operations on them and then merge them back to one excel file , the code is too long so I am going to go through the problematic part of it
If you've saved the JSONP response in the file ` jsonv3.json ` , you could strip off the function call wrapper and process the content with something like this : #CODE
I transposed the dataframe and now wish to remove the dateindex and replace it with strings ( labels ) , then export it to .csv to be used in a format that can be read by javascript ( doing all data manipulation in python ) .
If I use df_year.columns = [ ' value '] after transpose , it throws away the ' label ' and puts ' value ' on the second column ..
Again , you need to do these steps after you transpose the data - in your edited example it's before .
The trick here is using the apply method on the Series df [ ' date_time '] .
Update : this post suggests that MERGE is the way to go for the purposes of joining two data frames
An outer merge should recognize these as unequal and not join these rows together
the problem was at the file level : the entries in the ` filename ` column of the ` dfscores ` file being read had a ` trailing whitespace ` which caused the JOIN to fail .
unable to add rows to multiindex dataframe if values are both numeric
i can update an extant numerical ` MultiIndex ` ( e.g. ` df.loc [( 1 , 1 ) , :] `) , but i can't add a new row with a non-existent numerical ` MultiIndex ` . however , i can add a new row whose index contains at least one string !
MUCH better to concat frames .
Sorry you want to replace negative values with ` nan ` ?
The other option is to just drop the columns that are the problem .
@USER I would rather do ` df = df [ df [ ' A '] ! = df [ ' A '] .shift ( -1 )]` - first ` [ ' A ']` then ` shift ( -1 )` to shift only one column not all ` df ` .
I'm trying to replace the ` NaN ` values on column ` Age ` for people that were alone with the mean age of those who were alone , and the same way with those who weren't alone .
Advanced averaging with multiindex dataframe in pandas
Ironically it's possible if you have a multiindex .
Seems to me there should be some parameter somewhere that says to the data frame , " Hey , apply this function you already know about , but instead of doing to the original column itself , as you do it , make it a new column at the end of the data frame "
Please explain exactly what you want to achieve , in general you want to avoid any form of iteration and using apply if the calculation can be vectorised .
If you want to append another dataframe which has the same index then use append or concat , no need to merge unless the order is different and you want to join on some id column
You can just use concat and set the axis to 1 ...
it automatically gives NaN padding ( outer join ) if you set parameter join to inner , it would give no padding .
Would you mind expanding on your answer ?
either the pivot is a multilevel dataframe of the are a single NaN in that answer . check your intermediate value of pivoted . also suspect that the " pivoted.to_sparse() " returns a copy . try : " pivoted =p ivoted.to_sparse() " so capture the to sparse in the original variable
I would like to replace df1 score values with df2 scores whenever both id and event match in the two dataframes .
I would like to transform a Python Pandas DataFrame ` df ` like : #CODE
That is a pivot operation , you can either use ` .pivot_table ` : #CODE
what is the difference between ` pivot ` and ` pivot_table ` ?
Even if it's longer I like your groupby solution because it helped me to understand groupby , aggregate and unstack methods .
You are getting this error because the function you are passing to apply doesn't return anything .
Then the apply will run through without error .
` unstack ` can be used to pivot a level of a MultiIndex into columns .
Why can't I mask this pandas dataframe column ?
I am trying to mask pandas arrays just like my advisor showed me but keep getting an error no matter what I do .
A combined ( joined by date ) view of the data can be achieved using ` merge ` which can join dataframes on indicies and / or columns .
I had just figured out the merge ( from your earlier suggestion ) and was then going to try and figure out what to do next .
I want to merge them into a single datetime column .
But I cannot merge the results together without resorting to string functions , which are again mainly row-by-row .
Pandas - GroupBy and then Merge on original table
I'm trying to write a function to aggregate and perform various stats calcuations on a dataframe in Pandas and then merge it to the original dataframe however , I'm running to issues .
And then join on the original table : #CODE
This works fine until I want to join on the original table :
For some reason , the grouped dataframe can't join back to the original table .
By default , ` groupby ` output has the grouping columns as indicies , not columns , which is why the merge is failing .
Then , your merge should work as expected .
Alternatively , you may transform ` + 02:00 ` to UTC offset format and parse it with `' %z '` directive : #CODE
How can we drop that speed by one or two orders of magnitude ?
Making the values floats ( which isn't feasible for this use case ) would drop the speed by about half .
This is approximately the whole use case , using the suggestion of BrenBarn to use ` iloc ` instead of ` ix ` .
I want to transform this dataframe ( e.g. first 9 rows ) to this : #CODE
Everytime pd.isnull ( row [ ' Control ']) ( This should be the only if condition ) is true then merge this row with the previous row ( whose ' control ' value is not null ) .
For ' Action ' , simply merge them without any punctuations added ( e.g. FDA / OGROP / ORA / CE-FO / CHI-DO / CHI-CB / )
I am trying drop the current row ( if its [ ' Control_# '] is null ) so the next row ( whose [ ' Control_# '] is null ) can be added to the previous row ( whose [ ' Control_# '] is NOT null ) iteratively ..
Have you looked at the shift method ?
I think you need to group the rows together and then join up the column values .
This method is applied along the Control series to sort indexes into groups , which is then used to split up the dataframe to allow you to merge rows #CODE
That is really the only tricky part after that you can just merge the rows by applying a function to each group that gives you your desired output
edit above regex to clean join string joins #CODE
If each is notnull then each = each + joiner ( I only see you joined one each with a joiner ); but how did you manage to get ` If next_each is null : then returns each ( joiner not added ); else if next_each is not null : then returns each = each + joiner + next_each ` work ??
And how did you make sure if next_each is null ( e.g. there's nothing below ` M EGAN `) then ` M EGAN ` does not join by joiner ` , ` ?
Nested dictionary to multiindex dataframe where dictionary keys are column labels
What I need is a multiindex where each level corresponds to the keys in the nested dict and the rows corresponding to each element in the list as shown above .
I'm guessing I'm supposed to use one of the ` map ` ` apply ` or ` applymap ` functions , but I'm not sure which , or how .
There might be more elegant ways to do this , but once you understand how to use things like ` loc ` to get at a particular subset of your dataset , you can do it like this : #CODE
I wasn't familiar with the loc function .
I'll post regarding two parts : ( i ) Working with dataframes sliced using time-specific functions and ( ii ) Applying statistical functions using rolling window operations .
Now , if you want to operate rolling statistics onto a time series indexed pandas object , have a look at this part of the manual .
maybe show an example of `` resample ( ' 2W ' , how= ' mean ')`` which i think is relevant here
You can also use ` sets ` to check for ` subsets ` also and ` map ` the elements to ` ints ` : #CODE
We then use ` loc ` to perform label indexing to find where the column ' cat ' has the value that matches ' Home Applicances ' or just to do plain boolean indexing ( second example ) .
I wasn't clear enough , the whole file is dictionary of words , which words are keys and values are list of tuples ( Date and freq ) . actually I'm not sure what I want is a DataFrame with each word as raw and each tuple as columns . but the problem is that the number of tuples are not equal for all the words .
` df.loc [ i ]` will append new rows .
Obviously I could use reset_index but given that I want to group over site and time [ to aggregate over folds 1 to 9 , say ] this seems wrong .
More generally grouping and then regrouping based on multiindex values .
Apply regex replace to python pandas data frame
For some reason , the function runs properly on strings , but when running it on the data frame with apply it returns empty strings , and not the first three octets .
You can use the ` replace ` method of the ` str ` attribute ( to access string manipulation functions , see docs ): #CODE
You can always sort your dataframe by a then drop duplicates , but might not be the fastest option
The bug here is that ` Grouby.filter ` should apply to the entire subframe , not rows within the subframe .
To select with a mask .
Here the mask is a boolean array whether the values exist .
Might be easier to simply reindex .
It's interesting that the first ` s.loc ` you wrote returns a single-index series , while the second ` s.loc ` call and the ` reindex ` call return a two-index series .
I'd groupby the ' bug_id ' column , then take the first entry and append this back to your dataframe : #CODE
It keeps giving me error of ` DataError : No numeric types to aggregate ` , I think the ` aggfunc ` is the issue because the default value is ` np.mean ` , is there other ` aggfunc ` that list the cell rather than computing some value for the cell ?
does ` df.groupby ( cut ( df.a , bins ))` work ?
I apply groupby on that series .
Selecting one repeated multiindex in a dataframe on pandas
You can also stack these to do
This can then be used as a mask in the final output : #CODE
python pandas apply function group by group
My question is : can I avoid to iterate group by group to apply my_stat_function , and is there exist something faster , maybe applying the function apply ?
You can apply functions to groups : ` df.groupby ( ' user_id ') .apply ( my_stat_function )` or similar , have you tried this ?
Thank you for your help , my problem is that I don't know how to define my_stat_function in order to apply it like this , because I need the full data of each group , It is not a row by row execution .
You could groupby user and apply the function , you'd have to rewrite your function though
I can use ` drop ` to get rid of several rows using the first ` n ` MultiIndex levels like this : #CODE
I can also ` drop ` from a single level : #CODE
So how can I drop ` [(8 , 3 ) , ( 9 , 4 )]` ( i.e. the 3rd and 4th rows ) from my dataframe ?
generally speaking you don't want to iterate through a dataframe . look into the ` apply ` method : #URL Also , the point of stackoverflow is to be a resource for future readers who might have a similar question .
You can then apply some boolean logic to find the count which you're interested in : #CODE
So , in this case , Is NOT isnull .
Do you want to do a shift : ` wealth = wealth.shift() * ( 1 + returns )` ?
shift returns your df shifted by 1 so you should be able to do ` wealth = wealth.shift() * ( 1 + returns )`
The ` shift ` is necessary since you do not get the return on the first day .
Pandas join on 2 columns
I'm having some trouble getting these two dfs to join the way I would like .
I believe the way you have set up the groupby results and the second dataframe , this merge call will work : #CODE
The short answer to this is you need to use ` aggregate ` method while reducing ` groupyby ` object Pandas GroupBy Aggregate .
I am trying to do a pandas apply function on a 33 MB dataframe ( in CSV form ) and it is going incredibly slow .
I was doing an apply on a much bigger dataframe ( 16 GB ) and it finished in about 6 hours .
PS- also if someone knows how to add a progress bar on an apply function that would be a great added bonus :) Thanks again !
Then , rather than using apply , you could merge your existing data against the stock df , something like this : #CODE
If you want to fill missing values , rather than having custom logic in apply , it's much faster use ` fillna ` #CODE
Will ` concat ` with ` inner ` option handle the case for you , if your dataframes are already matched ?
You need to combine the functions that apply to the same column , like this : #CODE
? I want to create a new bool column " med " that has True / False based on ?
But since we have lots of rows and relatively few columns , we could loop over the columns , and in the typical case probably cut down substantially on the number of rows to be looked at .
how to resample state change data for line chart ?
I need to replace every other value to ' 1 ' except ' 0 ' .
Just use something like ` df [ df ! = 0 ]` to get at the nonzero parts of your dataframe : #CODE
Are you looking for [ rolling functions ] ( #URL ) ?
Multiindex in pandas pivot table
I am working on a pivot table that looks like this : #CODE
( eg a sample of the original frame and the pivot call )
There might be something more clever built in but one way is to work with MultiIndex as a list of tuples , and map out the new column names as you described .
Then apply to the columns and assign : #CODE
`' _ '` above was used a marker for the columns no longer desired , so the last step would be to drop those .
You just need another transpose in the end , since you want to put them on rows .
They both transpose lists , so you get a list with the first value of each list , then a list with the second value of each list , and so on .
` izip ` would truncate all lists to the length of the shortest , izip_longest adds ` fillvalue ` s to match the length of the longest .
Pandas : convert a multiindex column headers into normal column header ?
My data frame looks like this , with columns header being MultiIndex , ( True , False are boolean type ) .
If names aren't important , you could do this , which will reset the row index , and replace the MultiIndex columns with a tuple with each level .
I would just map each category to a y-value using a dictionary .
I get a strange error on the second line : ` ValueError : cannot insert title , already exists `
performance ( standard optimized operations compared to enlarging a dataframe iteratively with ` concat ` , wasting memory
Trying this this solution on another dataset , I get en error saying that level > 0 requires MultiIndex .
@USER I don't remember what I did a year ago , but yes , using ` level > 0 ` requires MultiIndex .
But read the whole page instead ; the Pandas ` groupby ` feature is more flexible than R ` dcast ` ( it's designed to also accomplish everything SQL aggregation , Excel pivots , etc . can do ) , but that means your ideas may not always map one-to-one between the two .
Pandas documentation probably doesn't need to discuss the differences between ` dcast ` and ` aggregate ` in R , but how would you explain this otherwise ?
I thought I could pass any function to ` apply ` on a group .
I was looking at this : #URL and the docs : #URL You definitely want to use transform in my opinion
@USER I agree about ` transform ` , although I thought you can mimic the behavior of ` transform ` with apply ( which is more generic )
Pandas : Merge array is too big , large , how to merge in parts ?
When trying to merge two dataframes using pandas I receive this message : " ValueError : array is too big .
How can I break this problem up so that I can execute this merge method on smaller parts and build up the output table , without hitting my RAM limitations ?
Then iterate through these groups doing the following on each : execute a new merge , filter and then append the smaller data into your final output table .
I'd love to hear any other solutions to this ' too big to merge ' problem , thanks .
I have an unbalanced panel that I'm trying to aggregate up to a regular , weekly time series .
Unfortunately , if I try to resample , I get an error #CODE
Are you ask for a process to interpolate , or a process to aggregate , or both ?
Firstly , prepare a function to map the day to week #CODE
Assume now your initialized new dataframe is ` result ` , you can now do a join #CODE
The ` Nan ` is what you need to interpolate .
Turns out the key is to resample a groupby object like so : #CODE
I should also note that sometimes I might need to aggregate the data by day / week / month .
Both levels of the MultiIndex are identical date ranges , spaced daily .
I want to resample the DataFrame on a weekly basis , for both levels of the MultiIndex , but I'm having trouble .
If I resample df directly , I get the following TypeError : #CODE
Fair enough , I unstack and resample on the first level , which gives half my answer : #CODE
Now , if I attempt to resample the second axis ( also index by date , in theory ) , I get the same error : #CODE
About your " should work " comment , are you referring to my first or second resample attempt ?
If I drop that , I get " The grouper name cohort_date is not found " .
And replace the rest of the Nan by 0 .
else replace all Nan by 0 for this column .
if you can story the 1st Nan , can replace them after doing df.fillna ( value=0 , axis=1 ) . trying to construct algorith to store and replace the 1st Na in each column should not be difficult and avoids expensive looping
The other libs , OTOH do not do this and consequently do not drop data .
I want to be able to get the mean and median accross the dataframes .
If you want to compute stats across each set of rows with the same index in the two datasets , you can use ` .groupby() ` to group the data by row index , then apply the mean , median etc .
This method will work even when your dataframes have unequal numbers of rows - if a particular row index is missing in one of the two dataframes , the mean / median will be computed on the single existing row .
You can simply assign a label to each frame , call it ` group ` and then ` concat ` and ` groupby ` to do what you want : #CODE
much faster than my general function , you can also use apply on this for more general functions
Here is a solution first unstack both dataframes so they are series with multiindexes ( cluster , colnames ) ... then you can use Series addition and division , which automattically do the operation on the indexes , finally unstack them ... here it is in code ...
That'll work fine for the mean , but it's harder to generalize to other summary stats such as the median , which the OP was also asking for .
Didn't keep that in mind when I made the answer but I do think that this solution would absolutely be able to do that you can combine each of the series I made into a dataframe and run the median solution on the frame rowwise yeilding you the seiries which you can unstack
I have a problem loading a dataframe from csv when I have a multiindex with more than one date in it .
When running df.index , I see that the multiindex object contains the two dates , but now also holds time information , where the time is 00:00 : 00
I'd like to point out that ` cut ` is an ordered factor in the ` diamonds ` dataset as is appropriate .
When I try to store this dataframe to hdfstore , I get an error saying " can't set attribute ' freq ' in node " , #CODE
I was able to use format= ' table ' and not get the error " can't set attribute ' freq ' in node "
Is this something I would use ` groupby ` for and then apply a function to it ?
And then you call aggregate methods on those columns , which gives you a single value for each group : #CODE
Pandas dataframe merge and element-wide multiplication
You can just get a list of the columns you want to multiply the scores by and then an apply function ...
I have a function which takes a couple of column values as input and outputs an value and I wanna apply that function to each row in a dataframe .
I suspect it has something to do with the eval / numexpr method of querying in the HDF and Query methods .
Resolve Pandas data frame merge conflicts with a function ?
Let's say I have two dataframes , which I would like to merge , but there is a conflict because rows and columns overlap .
So now when I merge them I get repeated rows because of this conflict : #CODE
Instead of using concat use merge : #CODE
The ` how= ' outer '` does a union of keys from both frames ( SQL : full outer join )
For example , replace each link with a key , storing the keys in a dict : #CODE
This roundabout method of converting my ` array ` to a nested ` list ` and then converting it back to an array via ` apply ` is bothersome .
You'll have a column dtype of object , which is slow to begin with , and you can't really do any fast aggregation ops , so you'll have to fall back to relatively slow apply ops .
You can perform a ` groupby ` on ' Product ID ' , then apply ` idxmax ` on ' Sales ' column .
@USER actually it works without ` drop_duplicates ` , I originally was using ` transform ` but using ` agg ` also works and doesn't require dropping duplicates .
@USER when I was using ` transform ` it was trying to align the results to the original df and so repeated the index values as we had multiple ' Store ' rows for each ' Product ID ' this was why I had ` drop_duplicates ` but I realised this was unnecessary as an aggregate function with ` idxmax ` would work fine
So , I thought I would take the test B scores for the score_range , create a new dataframe , insert the data and plot the histograms of the columns with the following : #CODE
Is there a multiple column map function for dataframes ?
And a function , which I want to apply to each row , storing the result into a new column .
You can use ` apply ` with the ` axis=1 ` argument to apply by row .
The more general apply will be slower .
When you have more data a general apply will not scale very well especially the row by row version since each row is converted to a series of uniform type which if you have mixed types will be very annoying to use and inefficient .
What I'm saying is that spending a bit of time trying to avoid apply will probably yield reusable and more performant code .
Trust me , the string methods in pandas are orders of magnitude faster than calling ` apply ` .
What you are looking for is ` apply ( func , axis=1 )` This will apply a function row wise through your dataframe .
I've tried various combinations of pivot , un / stack and set_index to achive the target structure but failed .
Define two columns with one map in Pandas DataFrame
I would like to apply this function to one column in my dataframe and assign the result to two columns .
That's too bad , since performance is much better when you can apply functions to whole Series rather than to individual values one-at-a-time .
2 ) group it up and apply a function to index your values .
3 ) reset indexes : use unstack to pivot your value indexes into columns and reset index to reset your date as a column ...
How to apply a function ( numpy ufunc ) with two array arguments to one pandas Series ?
As for ` apply ` , you're looking at the wrong documentation .
The workaround is to not use ` apply ` for this ; the broadcasting rules make ` apply ` redundant for your situation .
I still don't know how apply should work if I need it again , but this solves the problem at hand .
I would give you credit , but this would mislead others finding the question for an answer on ` apply .
@USER szl : Well , this * is * how you apply a NumPy ufunc in the way you want .
Maybe you could be even clearer about no use of apply could broadcast ( if that's the right word ) the second argument if the ufunc expects an array .
Perhaps @USER can merge these comments into his answer ?
Sorry I meant ` df = df.reset_index() ` this will reset index ( i.e. drop current one ) and create a new one starting from ` 0 `
Also just to make sure that you know range ( 1 , N ) won't go through every element in the array as it will start with the second one in the format I gave you - I was copying what you had in your question . should be ` for i in range ( len ( arr )): ` or ` for each in arr : ` or to get index val pairs ` for i , each in enumerate ( arr ): `
I would like to merge based on one key column and take the max or non-missing data for each equivalent row .
You need to save the result of your join : #CODE
I would like to pivot ( ? ) it into matrix .
you could use the ` unstack ` method : #CODE
I was using ` delta = dt - UNIX_EPOCH
How to merge two dataframe in pandas to replace nan
I have 2 dataframes , A and B , I want to replace only NaN of A with B values .
Notice that if instead you want to replace A with only non-NaN values in B ( that is , replacing values in A with existing values in B ) , A.update ( b ) is perfect .
Make a mask of A where A == numpy.NaN
Assign B to A using the mask as a boolean index for both .
It has a ` len ` of 0 even if the replaced object wouldn't have a ` len ` .
It is straightforward to normalize the data so they both start with a nominal temperature value of 20 C at time = 0 seconds , but what I really want is to synchronize the data so that the temperature ramps begin at the same time .
My question is , say I want the following attributes ( measures ) calculated ( all per user id ): average price , total price , max / min price , median price , average duration , total duration , max / min duration , median duration , and number of times logged in ( so number of instances of id ) and all on a per month and per year basis .
For context , I may want to visualize the group of users who paid on average more than 8$ and were in the app a total of more than 3 hours ( to this point a simple new table can be created with the info ) but if I want it in terms of what shows they watched and whether they were on wifi ( other attributes in the original data set ) and I want to see it broken down monthly , it seems like having my new table of calculations won't cut it .
Would it then be best to create a yearly table and a table for each month for a total of 13 tables each of which contain the user id's over that time period with all the original information and then append a column for each calculation ( if the calc is an avg then I enter the same value for each instance of an id ) ?
Pandas : conditional rolling count
2 ) apply it to your Series after converting to dataframe #CODE
first merge your indexes into one index using the following #CODE
Then just group by your new indexes and merge the values together ...
My current code moves the column by index via " df.columns.tolist() " but Id like to shift it by Name .
We can use ` ix ` to reorder by passing a list : #CODE
The loop is hidden within the ` map ` commands that act on ` lines ` .
X is the input array that is passed to the function when calling the resample method on a dataframe object like so ` df.resample ( ' M ' , how=my_func )` for a monthly resampling interval .
The pandas resample function expects a specific string to be passed to the how parameter .
I haven't tried it yet but I think the resample method can return python objects too .
I'm trying to think of how to modify this but I keep coming back to having to pass the function to resample ( besides essentially rewriting the resample function )
I have poor understanding of pivot tables overall , but you have student B twice and identical assessor D for student B and values 100 and 2 .
I am guessing you need to aggregate it in some why , otherwise the output does not look like pivot table .
Which returns me a list of unique values , I then tried to merge it to the original database ( with df.merge ) but had no luck .
Then , merge , left on the original column , and right on the index .
Pandas merge with logic
I would like to merge two dataframes , but can't exactly figure out how to do so without iterating .
Basically , I want to merge a row from df2 to df1 if df1.date > = df2.start_date and df1.date = df2.end_date .
I thought about creating a date series vector to merge with df2 , so that I can combine on date , but it seems very manual and does not leverage the power / speed of pandas .
The naive iterative approach is ` O ( n*m )` , where ` n = len ( df1 )` and ` m = len ( df2 )` , since for each date in ` df1 ` you would have to check its inclusion in up to ` m ` intervals .
I was getting ~ 1.5 seconds for len ( df1 )= 720 and len ( df2 )= 25 , your implementation is clocking in at .002 seconds .
to individually query each column and find the information I'm looking for but this is tedious even if I figure out how to use the abs function to combine the two queries into one.How can I apply this filtration to the whole dataframe ?
Not sure if this is close enough ` df.apply ( lambda x : x.index ) [ abs ( df ) > 1 ]`
You don't have to apply the filtration to columns , you can also do #CODE
I use this function with pandas to apply it to each month of a historical record : #CODE
reindex Series should fill with NaN
Unless i completely misunderstand what ` reindex ` is for , i believe that the snippet below should fill in the series with a ` NaN ` for the time index that was removed : #CODE
` reindex ` is not in-place .
see docs here : #URL this is a multi-multi merge and soln is at the bottom . pretty much `` pd.merge ( df1.reset_index() , df2.reset_index() , on=levels_to_join ) .set_index ( levels_to_set )``
( i.e. as a join )
and when I apply the command pd.to_datetime() to these columns I get fields resulting that look like : #CODE
I can create it by writing a short function as so and using ` apply() ` to apply it iteratively : #CODE
You can apply a lambda , e.g .
how to apply different functions to each group of pandas groupby ?
I want to group the dataframe by the column ' type ' and apply different function to each group , say , ` min ` for group with type A , ` max ` for group with type B and ` mean ` for group with type C .
The intent behind ` groupby ` is such that you can apply the same operations to subgroups of your data , as grouped by the ` groupby ` operation .
As soon as the apply method began to execute , that value started to soar .
Each group passed to your ` do_nothing ` is a DataFrame , so you are creating 3 million DataFrames ( which ` apply ` then has to aggregate into a single result ) .
Too bad the ` diff ` function does not have a ` fill_value ` otherwise this could be a one-liner .
Replace characters in large gzip file
I am trying to replace some characters in my hive output so that Pandas can read it properly as a DataFrame .
I thought about the chunks idea but was concerned that the edges may cut my characters in half so that they were not able to be recognized by replace() .
Your can read a chunk of 10Mb , split where it has and end-of-line character , replace in every element of the split except the last one , and fuse the last one with the first element of the next chunk split .
using pandas I tried to make a horizontal bar plot , but the y-axes label is cut off .
Python Pandas : drop a column from a multi-level column index ?
How can I drop column " c " by name ? to look like this : #CODE
KeyError : ' Key length ( 1 ) was greater than MultiIndex lexsort depth ( 0 )'
My first thought was iterate through the rows and apply numpy.hstack for joining , store it and numpy.vstack the stored rows , but it doesn't work as intended .
I have a multi-indexed pandas dataframe and want to create a bar-chart where I group by one index and stack over the other .
You could convert the dataframe to be a series with ` stack ` :
This works because ` stack ` will convert your data from 5x3 to 15x1 , thus treating it as if it were all in the same column and ensuring that the correct degrees of freedom is used for the standard deviation .
Refer to the documentation on ` stack ` here and here
I don't think you can replace the datetime ` NaT ` as you've found , what is the problem with having ` NaN ` / ` NaT `' s ?
To replace missing values with a string #CODE
I had the same issue : This does it all in place using pandas apply function .
I did have to replace some information in my Dataframe / images to generics ( e.g. app1 , app2 ) .
You might try creating a pivot table from the grouped dataframe .
Then , create a pivot table : #CODE
The resample function is really good for this .
Pandas - merge two DataFrames with Identical Column Names
I want to merge them into one DataFrame while keeping the same Column Names .
How can I merge the values from each DataFrame into one ?
Then , I can resample another column using the index .
Basically the way to do this is determine the number of cols , set the minimum number of non-nan values and drop the rows that don't meet this criteria : #CODE
` axis=1 ` tells it to drop rows rather than columns
I had to use len ( df.columns ) instead of len ( df ) .
When I create a pivot table , I get the following warning :
What I did was just create a pivot table : #CODE
In addition , when I try to use the values in the pivot table to fill in NA values , I get the following warning :
As Ed said , pivot_table wants to be more explicit about the row pivot in the future since it sets the row as the index for the pivot result .
Something you could do to speed it up would be to boolean mask on those rows where you have duplicate values : ` df [ df.d.isin (( df.d.value_counts() > 1 ) .index [ df.d.value_counts() > 1 ])]` this returns a dataframe containing just the duplicate rows
@USER - good point , this will join any repeated data so your answer handles that case correctly while this doesn't
An example of how to produce such a column could be ( assuming that the DT key could be used as a sentinel value ) ` df [ ' helper '] = ( np.where ( df.d == ' DT ' , df.index , np.nan ))`
In the case where you are using a DataFrame , you can use DataFrame.where to use another frame's values to replace the values when null .
count in pivot table of pandas dataframe
How do I calcuate the count in the aggregate column of pivot table ?
The aggregate function np.count does not seem to work .
Using ` len ` or `' count '` as argument for ` aggfunc ` does work : #CODE
Yes len or ' count ' as argument for aggfunc does work .
But len with ' quote ' and count without quotes do not work .
may be len is built-in .
` len ` is a built in function , ` count ` is not an existing function .
One approach - filter your dataframe to just the negative samples before you do the groupby , then combine back with your larger frame using ` merge ` #CODE
The problem of course being that -1 is not an index of ` a ` , so I can't use ` loc ` .
But I'll be working with tables where multiple rows have the same index , and I don't want to reindex my table every time .
A possible solution for now would be to create the database yourself , and then append the dataframe to it .
Use ` loc ` and a boolean mask to filter the dataframe and select the ' id ' column , then call ` unique() ` to remove duplicates : #CODE
I got it : for x in range ( len ( slices ))
I need to do a " custom " resample that fits futures hours data where :
After doing the resample , the output for the above df should be : #CODE
Playing with reindex with no success .
My goal is to create a python script which will take two files entered at the command line and drop columns from the first file if column headers are present within the second file , and write the output to a new file .
These relate to columns that I wish to drop from the large file .
Are there more memory efficient ways of achieving the column drop ?
To get your code to work all you need to do is drop the empty string from your dropcols list .
Of if you want to handle the case where your dropcols list works even if you specify a column not in the larger dataframe , you could do something like this - taking the intersection of your dropcols and the columns in the dataframe .
+1 for the intersection method !
The key is to apply ` usecols ` in ` pd.read_csv ` .
I had resorted to splitting my files into managable chunks in order to process them , else used awk to drop columns .
Python Pandas - Group by an aggregate ( count of conditional values )
Looking at your code , it seems you could use pandas built in moving average / sliding windows functionality , combined with a group by and apply .
Next , I want to join this dataframe with the the larger dataset , based on the entry being between the two ` Equal_Span ` columns .
Length : 26761 , Freq : None , Timezone : None `
Using a MultiIndex value in a boolean selection ( while setting )
But that one uses a simple index and I can't figure out how to generalize it to a MultiIndex : #CODE
If you want to append to the table , below is a little bit more hacky workaround .
Conditionally replace missing data pandas
Would there be an easy way to replace the two 0s only in Atime with the times ( 2.0 , 2.4 ) listed in Btime ?
Use ` loc ` and boolean indexing to select the values and assign back : #CODE
How do you merge Pandas DataFrame rows into a string in one column ?
You could use the agg function after grouping the dataframe .
2 , We can put all the items in the list into a big ` DataFrame ` by ` concat ` them row-wise , since we will do step #1 for each item , we can use ` map ` to do it .
4 , We probably want to rotate the ` DataFrame ` 90 degrees ( transpose ) and ` reset_index ` if we want the index to be the default ` int ` sequence .
" pd.concat ( map ( pd.DataFrame.from_dict , Data ) , axis=1 [ ' fields '] .T " I had went the route of iterating through the results 1 by 1 , and creating a tuple for each timestamp / path , appending that to a list , and then reading that list of tuples with from_record .
shift columns of a dataframe without looping ?
i need to shift each column down by one * ( its position in the array ) .
so a kind of diagonal shift : #CODE
this makes me think that there must be an easier , perhaps vectorized way ... perhaps using some kind of " apply " , however i'm not sure how to do that when each column needs to be shifted down as a function of its position in the array .
Especially with hierarchical indexing it is possible that the complex indices are rebuilt after each shift .
One possible approach ( at least to isolate the problem ) is to just extract the data into a ` np.array ` ( take the ` .values `) , shift it , and recreate the DataFrame .
pandas dataframe select columns in multiindex
What I would do is to create 3 new dataframes ( if it is always 1 , 2 and 3 days you want to shift ) .
Then just merge : #CODE
This merge means a left join ( you'll keep all values from df2 , regardless if there's a match or not in the df_1 table ) , you're merging on the index ( using index as key )
I want to replace all non-NaN values with a ' 1 ' , if there is a ' 1 ' anywhere in that row .
Pandas cut with user-defined bins
I am working in Python and using the cut functionality in Pandas .
In other words , I would like to predefine ` bins ` as ` [ 0 , 100000 ]` and then append user defined integers to this set of numbers .
If you want to append rows to D within your function , you could declare it as global : #CODE
Create initial tables / drop existing indexes
@USER dunno works fine without any warnings on my environment , You could try [ winpython ] ( #URL ) you'd need tp update pandas though from [ here ] ( #URL ) it's very simple just open the winpython control panel and drop the exe into it to upgrade
Just apply the function directly - I guess this will take more CPU as it's calculating all the maxes , then just getting the ones you want , but doesn't create a new variable .
I suspect the ease of use and the richness of the Pandas API will greatly outweigh any potential benefit you could obtain by rolling your own interfaces around numpy .
First , when you resample , add a ` .groupby ( level=0 )` so the original timestamp is preserved .
Then apply a groupby on the first level of the MultiIndex to apply the operation you want .
How can I aggregate ( sum ) over an index which I intend to map to new values ?
That isn't fully vectorized , though , because of the ` apply ` .
I am trying to merge 2 dataframes .
I would like to group / aggregate this in such a way that any non-blank instances of qty change or price change are counted at the index level .
df_users reindex on userids
or multiindex ?
Next , ` resample ` the irregular data up to daily values so it is normalized .
Don't append DataFrames like that at all ( nor start with an empty one ) , each append is a copy .
I was trying to figure out how to use concat instead , didn't think to do it that way ( obviously ) .
I agree ( I just added this recipe to the cook book ) , but would someone like to do a PR for the doc-string and concat section ?
I am trying to understand how to apply function within the ' groupby ' or each groups of the groups in a dataframe .
So far I've tried hist , but it errors out : #CODE #CODE
I'm having some trouble figuring out what I'm doing wrong here , trying to append columns to an existing pd.DataFrame object .
Specifically , my original dataframe has n-many columns , and I want to use apply to append an additional 2n-many columns to it .
The problem seems to be that doing this via apply() doesn't work , in that if I try to append more than n-many columns , it falls over .
In general you want apply to return either :
why are you showing using apply anyhow ?
@USER I don't get why the concat example doesn't work ( it ought to return a Series ) ?
@USER , I guess it is because ` x [ 0 ]` is going to be the cell value ( therefore one of ` numpy.int ` , ` numpy.float ` , ` object ` , etc ) , and ` concat ` fails ?
Here are solutions using apply .
This uses apply , but may be faster , depending on the shape of the data .
How to normalize text in a list that is itself in a column of Pandas dataframe ?
This is a followup to this very useful Q / A : Faster way to transform group with mean value in Pandas
I just updated from 14.0 to 14.1 to see how much improvement there was in groupby / transform operations .
( moved extra info above at stack exchange's nagging )
apply function on dataframe involving two rows
I want to apply a function to calculate distance based on the longitude and latitude .
I know there are ways to apply function along axis 1 and 0 , but they seem only apply to single row or column .
If you just want the difference then just call ` diff ` to do the calculation
One way I had considered , but turned out to give an error ( ` ValueError : cannot reindex from a duplicate axis `) was to have duplicate column names .
For something like this where you actually want to list out each column of bands and siblings I would try to use a multiindex
I would like to group the series by hours or days and apply a function group-wise which calculate the ratio #CODE
See ` shift ` to understand the params
I think there are many use cases in which neither align nor merge or join help .
For me aligning means map series A into B and have a way to deal with missing values ( typically sample and hold method ) , align and join cause a not wanted effects like several timestamps repeated as a result of joining .
I still do not have a perfect solution , but it seems np.searchsorted can help ( it is much faster than using several calls to join / align to do what I need ) .
How can I map A into B so that B so that the result has all timestamps of A and B but no repetitions ( except those which are already in A and B ) ?
There's also the [ ` join `] ( #URL ) method but I didn't test if this is faster .
@USER : it seems join is the fastest , %timeit df1.join ( df2 , how= ' outer ') # 100 loops , best of 3 : 10.1 ms per loop
If you need to synchronize then , use ` align ` , docs are here .
Otherwise merge is a good option .
Align and join is not what I need , I need to map timeseries A into B , no relational DB logic is required , I do not want to create new timestamps ( repeated ) .
Python Pandas resample , odd behaviour
I have 2 datasets ( cex2.txt and cex3 ) wich I would like to resample in pandas .
Thus , the resample will try to fill in between the junk date , to current- and you'll see errors .
I am organizing my data folders into multiindex dataframes with a structure similar to this : #CODE
I can achieve this if I know the exact number of data points in that range ( i.e. range * sampling freq ) , but the point of setting the Time to one of the indexes was to get around having to do that .
Well I still need to create a multiindex in that the Sweeps are the second index .
` ix ` is not encouraged .
` loc ` and ` iloc ` ( for pure positional indexing ) are prefered .
There is no inplace parameter to the apply function .
No , apply does not work inplace* .
In general apply is slow ( since you are basically iterating through each row in python ) , and the " game " is to rewrite that function in terms of pandas / numpy native functions and indexing .
* apply is not usually going to make sense inplace ( and IMO this behaviour would rarely be desired ) .
Pandas rolling mean with Nan values
It would probably be better to interpolate January data than ignore it with the min_periods=11 .
All you have to do is replace ` data ` with ` data.interpolate() ` .
And it is really easy to interpolate or backfill .
I would like to resample it to a regular time series with 15 min times steps where the values are linearly interpolated .
However using the resample method ( df.resample ( ' 15Min ')) from Pandas I get : #CODE
I have tried the resample method with different ' how ' and ' fill_method ' parameters but never got exactly the results I wanted .
Basic idea is find the closest two timestamps to each resample point and interpolate .
` np.searchsorted ` is used to find dates closest to the resample point .
I am using a Compound screen made up of 4 by 4 monitors with small ( but nonzero ) bevel -- the gap between the screens .
I guess it might make sense to use the median for integer columns instead .
python lists : merge and replace
My goal is to replace elements in ` A ` with those corresponding elements in ` B ` that have the postfix .
The way I can think of is to split and transform ` A ` in to a two-column dataframe and merge with ` B ` , then put the ` * ` column back to column in ` B ` .
So you really want to check , for every element ` a ` of ` A ` , whether there is some element ` a* ` of ` B ` , and if there is some ` a* ` , to replace ` a ` in ` A ` with ` a* ` .
That is , the ` A ` , ` B ` in my example are actually one element of the two lists ` A_L ` and ` B_L ` that I want to merge .
I want to do a element-wise merge on ` A_L ` and ` B_L ` .
I'm not sure what you mean , but you just need to replace ` B ` and ` A ` with the respective list .
Therefore , when I understand you right , just replace ` B ` with ` B_L [ 0 ]` and ` A ` with ` A_L [ 0 ]`
So to do the element-wise merge , I need to loop through the index for ` A_L ` and ` B_L ` ?
I just tried list comprehension it also works : ` [[( x + ' * ') if (( x + ' * ') in B_L [ i ]) else x for x in A_L [ i ]] for i in range ( 0 , len ( A_L ))]` .
FYI , it is in general MUCH faster ( in 0.14.1 / and esp in 0.15.0 / master ) , to do that transform like : `` y / g.groupby ( .... ) [ ' cumsum '] .transform ( ' last ')`` as it take an optimized path , but YMMV
Similar to Jeff's answer but can't we just use transform ( ' sum ') and skip the cumsum step ?
Then you can aggregate easily : #CODE
Just apply it to every column : #CODE
Pandas data frame : resample with linear interpolation
However , dt = df1.datetime - df2.datetime is then not defined .
it's defined at ` dt = df1.datetime - df2.datetime ` , or do you mean something else ?
Did you melt your data.frame before plotting ?
Yes sorry I forgot to add the melt :
you just open like normal `` store = pd.HDFStore ( filename , mode= ' r ')`` ( mode is append by default , but if you aren't modifying doesn't matter ) .
There's nothing built-in , so you'll need to calculate it with apply .
( just realized I'm late with an answer but I'll leave this an an alternate answer if you want finer control over the output format ) Just replace the TextToWrite line with :
Shift and subtract to get the delta time between events
Also once you have a grouped df is there a way to transform it back to a general dataframe .
But when you group by you end up with a group-by object which you can iterate over . for a specific group is there a way to apply a shift to it .
But this is not guaranteed to give you the ` ith ` value since ` ix ` tries to index by label before trying to index by position .
So if the DataFrame has an integer index which is not in sorted order starting at 0 , then using ` ix [ i ]` will return the row labeled ` i ` rather than the ` ith ` row .
Your problem is strange , I can't explain but the following worked for me : ` import datetime as dt mydata [ ' new '] = mydata [ ' event_date '] - dt.datetime ( 2006 , 1 , 1 )` could you confirm
Consider the following example in which we setup a sample dataset , create a MultiIndex , unstack the dataframe , and then execute a linear interpolation where we fill row-by-row : #CODE
It finds a mask for the trailing NaNs by using backfill .
Pandas dataframe float index and transpose error
I agree it will be faster but I could not find any method in the docs to transpose the data before reading it in .
Transpose the dataframe and drop the index .
Use the pandas method ` isnull ` to test : #CODE
How do I continue and insert rest of the data ?
how to merge a groupby output ( series ) back to the dataframe
how do i put the z-scores back in the portfolios as a new column ( join creates the columns with the array , not like a continuous values )
i should indeed use transform . thank you for pointing me in the right direction !
Use transform ( instead of apply ) , as chrisb suggests , and assign the result to ` portfolios [ columname ]` : #CODE
Is there any logic to apply , or methods , any suggestions are highly appreciable , since I am in this for long time .
But you could loop over the ` csv's ` and collect all individual Dataframes in a list or something , perhaps store some ` id ` as well , and then use ` pd.concat ` to merge all Dataframes into a single one .
Pandas making cumulative Z scores map onto dataframe
I would like to map a Z score onto the Dataframe for a particular stat for each team .
This function does what I want and returns a dictionary but I don't know how to map the dictionary on to the dataframe , or if it is the best way of going about it .
You can use ` .map ` to map a dictionary to values contained within a pandas dataframe .
Pandas multiindex dataframe set first row in a column to 0
and then apply the above technique .
just and with a mask using `` isin `` ( or just sub-select afterwards )
Triple transpose might not be as elegant as your answer .
Another method without using transpose is to create a boolean mask on whether the first row has values larger than 0.5 and then drop the NaN's with a threshold and then finally make a list of the df columns to filter the original df .
It is being run on each row of a Pandas DataFrame via the apply function : #CODE
Turns out if any column in the original dataframe is an object , the series created via apply is of object type .
Then define my group operation and apply it : #CODE
Python Pandas multi-index values not moving over in pivot table
However , it will merge all values from all columns and then calculate the statistics per group , so you will loose the distinction between columns .
It isn't really any better than the answer you linked , but I think the way to achieve this in seaborn is using the ` FacetGrid ` feature , as the groupby parameter is only defined for Series passed to the boxplot function .
ok , I didn't understand you wanted the boxplot side by side .
As the other answers note , the ` boxplot ` function is limited to plotting a single " layer " of boxplots , and the ` groupby ` parameter only has an effect when the input is a Series and you have a second variable you want to use to bin the observations into each box ..
But , you'll first have to " melt " the sample dataframe into what is called long-form or " tidy " format where each column is a variable and each row is an observation : #CODE
Then , using the approach from this answer , pivot into columns , and drop the sentinel .
This won't be ultra performant because of the apply , but probably reasonable on data of your size .
I've tried to lead / lag the variables on Data2 to create 180 new variables and merge it to Data1 , but that was very inefficient and lead to memory issues ( used over 100GB of RAM ) since Data2 is somewhat large .
Merge those and things are already pre-shifted for you .
I can do this with several steps ( e.g. merge , than fillna ( 0 ) , finally sum ) but was curious to know if there is a fast way to do this as it is a case which I am encountering frequency .
@USER : I am still struggling in finding a similar approach for join or merge , e.g. I join ( left ) on the index df1 and df2 , the result has values of df2 with no corresponding timestamps ( these values are NaN currently ) which have been extrapolated using a fill_method , e.g. taking the last known value of df2 .
Note you can also use ` df.head ( 10 )` and ` df.tail ( len ( df ) - 10 )` to get the front and back according to your needs .
So if these are pandas dataframes , just do a standard merge with the last column of each dataset as the key .
Merging on floats here is not ideal , but may be OK , just check that you get a one to one merge at the end .
IIUC , what you're looking for is that the operative convention is that of numpy bool arrays , not Python bools : #CODE
how to select multiple values from a single level of a dataframe multiindex
See here for multiindex using slicers , introduced in 0.14.0 #CODE
You can either preprocess the data as levi suggests , or you can transpose the data frame after creating it .
Not sure why ` factorplot ` would be unable to do this -- couldn't the empty intersection of ` x1 , x2 ` just not be plotted ?
How do I apply filters and functions to previous rows in pandas ?
The proper use of apply is a bit unclear to me .
Faster alternative to grouby / shift
note that the groupby and shift is implemented in python ATM - needs cythonization to be fast - open issue is here : #URL
@USER So shift outside of groupby is cythonized but inside of groupby is not ?
a pull request to cythonized the groupby would be appreciated . the actual shift code is just creating and using an indexer so it is quite fast itself ( and would not benefit from cython ) . groupby needs to avoid processing in python space so essentially would create thrn indexer ( eg what you are using in the loc , but for the general non sorted case ) . nothing to hold out .
efficient expanding linregress
I need the expanding linear regression from the start of the df .
So I only want to map each data point to its associed cluster center ( the one which is the closest of the datapoint ) .
Further , you should not repeatedly using ` get_group ` , instead use the cythonized functions , ` apply ` , or iteration , see docs here
Say I have a large dataframe and that I want to apply one operation to every element in a column .
I was looking for an option like example 2 , except it won't apply the formatting to future dataframes .
You can apply any operator or across a column .
You don't need the apply , just do ` - frame.abs() `
I would like to map ` tp ` do ` df ` such that the the second element in each tuple lands in a new column that corresponds with the row matching the first element in each tuple .
So for those that will point out I have not actually asked a question , how would I map ` tp ` do ` df ` such that the the second element in each tuple lands in a new column that corresponds with the row matching the first element in each tuple ?
I suspect the problem you were having with ` map ` is that it takes a function or a dict .
There are many places where a list of 2-tuples can be used in place of a dict , but this isn't one of them ; ` map ` will try to treat that list as a function .
You need to turn your list of tuples into a dict which is ridiculously easy to do in python , then call ` map ` on it : #CODE
The docs for ` map ` show that that it takes as a function arg a dict , series or function .
The online docs show how to apply an operation element wise , as does the excellent book
Stack / Unstack Multi-index pivot table in Python Pandas
How do I stack / unstack the " Peat-Forming " to have " PBL_AWI " and " Description " underneath ?
Was thinking of spilting the tables ( b = p_table.loc [ ' Non-Peatlands ']) Add an Total row and then append both tables back together .
For the totals you . calculate and append to dataframes peat and non_peatlands .
You might have to play around with the MultiIndex to get it to merge .
eg ` peat.index = pd.MultiIndex ( ' set the correct index probably should use from tuples ')` tuple of the Total peatlands looks like this ` ( " Total Peatlands " , "")` to get the cells to merge properly .
python pandas to drop lines and substitute values in specific columns of a csv file
automatically drop lines that do not have values at specific columns , for example columns 1 and 2
To drop the rows with NA , you can do : #CODE
To replace a certain value , you can do : #CODE
also , the example above would not work , because I want to be able to not drop if NaN is present in some other column
You can always first replace ' somestring ' with NaN , or just manually select the rows with ` df [ df [ ' col '] ! = ' somestring ']`
And then ` unstack ` to get the result : #CODE
How to resample one of the series with the other's indices , using interpolation ?
First you can align both serieses ( so they both have the same indices . It is also possible to only reindex one of both to the index of the other with ` reindex `) : #CODE
Then you can interpolate the missing values ( eg with linear interpolation based on the time index ): #CODE
Thanks , I was thinking linespace would take care that , [ figure ] ( #URL ) , if I insert zero will lose the shape and also if I add random numbers also lose the shape .
I don't know what that figure really shows but you could just append a new df : ` df2 = df2.append ( df2 , ignore_index=True )` or a new df : ` df2.append ( pd.DataFrame ( np.random.rand ( 50 , 2 ) , columns = list ( ' ab ')) , ignore_index=True )`
Is it an option to use ` pandas.tslib.repr_timedelta64 ` but then cut off the minute / second part ?
You mean , split the string on ' : ' and then append ' h ' ?
So I rather stuck to a list that I insert into the existing ` DataFrame ` and thus get back the index .
Given a pandas DataFrame or numpy array of floats , if the value is equal or smaller than 0.5 I need to calculate the reciprocal value and multiply with -1 and replace the old value with the newly calculated one .
The typical trick is to write a general mathematical operation to apply to the whole column , but then use indicators to select rows for which we actually apply it : #CODE
You can simply get the list of weeks where there is a change , then compute their differences , and finally join those differences back onto your original DataFrame .
With the above code , the NaN is put in automatically by merge .
the code below gets you the index you want but I've had to do some work to strip the extra `'` and ` ( )` parts from your file .
Thanks , I solved my problem with the strip function
This statement produces a ` ValueError : Cannot shift with no offset ` .
If you have a DatetimeIndex , the ` shift ` shifts your data with a period of time .
If your index has no frequency , you have to provide that to the shift method with the ` freq ` keyword ( eg ` freq= ' s '` to shift the data one second )
If you just want the difference between two consecutive values in the index , you can use the ` diff ` method ( of a Series , a bit easier than shift and substract ): #CODE
Merge misaligned pandas dataframes
I would like to merge A and B , but my current solution results in a misalignment .
The current solution to merge the dataframes is as follows : #CODE
Because you shouldn't have this shift in individual rows , unless you screw up big time with your commas ( or other separator )
Your function to merge the different frames is perfectly fine !
Is there any easy way ( without any merge , join ) ?
For your first question , ` agg ` takes a dict of { column name -> function } #CODE
The problem is I want to dynamically create a dataframe to which I can append one row at a time .
The ' data ' is being populated properly but I am not able to append .
pandas apply with inputs from multiple rows
I need to do an apply on a dataframe using inputs from multiple rows .
However , if I need ' a ' from the current row , and ' b ' from the previous row , is there a way to do that with apply ?
When doing backtesting , currently I pull the full date range I need for a security into memory , and then resample it into a frequency that makes sense for the test at hand .
Point taken about apply not being magically faster .
Considering your point about apply and speed , I think what I'll do is try to vectorize as much as I can , and see what I'm left with , and then come back with more questions as I have them .
This should directly answer your question and let you use apply , although I'm not sure it's ultimately any better than a two-line solution .
That would be a way to get all your variables into the same row but I think it will likely be more complicated than the concat method or just creating intermediate variables via shift() .
I timed your first method vs adding a shifted column and then deleting it after I finished the apply , and interestingly they were virtually identical ( and not very fast ) for any df size I tried , so I guess the time is all spent in the apply itself .
@USER It's not just the apply , it's also somewhat expensive to concatenate / merge or create a new variable .
In my example data , about half the time cost is due to concat and half due to apply .
Just replace the apply ( mean ) with mean() and you'll see it's still slower .
I can strip out the rightmost ' .csv ' part like this : #CODE
I would like to apply a function to a pandas DataFrame that splits some of the rows into two .
How can I add a column ( or replace the index 0-9 ) , by a timestamp with the now time ?
but it appends to the wrong level within the MultiIndex .
pandas - DataFrame expansion with outer join
I want to generate a pandas DataFrame representing a map ` witter tag subtoken - poster ` where tag subtoken means anything in the set ` {hashtagA } U { i | i in split ( ' _ ' , hashtagA ) } ` from a table matching ` poster - tweet `
Then I would simply apply the extract function with a regex : #CODE
Use ` notnull ` and specifically ` idxmax ` to get the index values of the non ` NaN ` values #CODE
I have a Dataframe with customer ` id ` , ` date ` and ` price ` and I want to aggregate all prices except for the purchase of each ` id ` on the first date .
Is there any way to apply ` read_csv() ` to this without any major hassle ?
` diff ` is just the difference , if you want the difference quotient instead you can do ` df.POP.diff() / pd.Series ( df.index , index =d f.index ) .diff() ` .
` diff ` on an index means set-difference , so you have to convert it to a series first .
Pandas map by index value without dummy column
I would like to map the values in the Probability column to ' s ' for ' success ' for the first 10 values , and ' f ' for ' fail ' for the rest .
To do this , I create a dummy column called Index , apply a transformation , and then drop the dummy column .
Using slightly different numbers ( for no particular reason ) , you can stack to for a Series and then use boolean indexing : #CODE
Is there any way to drop those records ?
python re-sample at a uniform semiannual period ( equivaent of ' BQ ' in pandas resample )
is there an ' BQ ' equivalent semiannual resample in python ?
i've a set of records , some of them follow jun-dec , some jan-jul , some feb-auh etc . how do i resample all of them to jun-dec ( concurrent for jun-dec , and following jun / dec for other records ?
Should give the right answer ( and can drop the first entry ) .
Pandas aggregate -- how to retain all columns
Use agg to return max value for each group #CODE
I found the unstack ( level ) method to work perfectly , which has the added benefit of not needing a priori knowledge about how many Codes there are .
Then you can get the keys of the groupby ( user_id ) , and aggregate as needed with a minimum of queries .
Rather , stata just does a normal shift / lag but sets lags to missing as appropriate .
I know that there are duplicates in the MultiIndex ( because I don't care about a distinction that the underlying databse does care about )
I am trying to take many CSV's and stack the column data into one file using the headers to
align the data .
You'll need to create a list of all the csvs using os.listdir or glob , loop through the list and read each csv into a dataframe and then use the concat function to merge them all into one long dataframe . pandas.concat() is header aware and will automatically align the headers for you .
Instead of creating a list , you could create a blank dataframe - pd.DataFrame() , then concat the frame within the loop .
Even if you could help me just merge these data frames .
I came from R so I'm used to the easy cbind or merge .
Getting Average of Pandas with GroupBy- Getting DataError : No numeric types to aggregate -
and gets the error : ` DataError : No numeric types to aggregate ` #CODE
Also , perhaps would be better to put all the data-frames in a list and then ` concat ` them at the end ; something like : #CODE
I'm just trying to extract info contained in a multiindex into a series or array .
There are some other ' get ' methods of multiindex but I can't get any of them to do what I want ( and am not sure if any of them does what I need anyway )
If you want to use ` apply ` it can be done , but you will loose the column names : #CODE
I have more than one column to aggregate actually 4 .
See edit , if you want to use ` apply ` , it can be done but you will loose the column names .
Basically the ` apply ` function should return a ` Series ` of length of 3 , compare the two new edits .
Seems mad that ` df.groupby ( ' A ') .agg ( list )` doesn't work , I remember having some ~~argument~~ discussion that agg should work with lists ...
I personally think it make little sense to do such things ( ` agg ` into lists ) .
However , when I run the executable , I get a ValueError : cannot reindex from a duplicate axis from the following line : #CODE
This situation arose in my data because I had a redundant index item that didn't vary at the lower levels of a ` MultiIndex ` so I didn't think it would mater ( as it was nice to retain in the data set as a different representation of another index item : It was ` FullCountryName ` and ` ISO3C ` code ) , however I found that because it wasn't defined in one country case it therefore dropped that country from the resulting dataframe ( without any warnings ) .
It's like going from a series to a dataframe is an implicit transform operation ?
Probably the most readable way to do it , although I generally try to avoid apply / lambda if possible for speed reasons if there is a good alternative ( though speed is not a concern here ) .
I know one can mask out certain rows in a data frame using e.g. #CODE
Calling df [ mask ] yields my new " masked " DataFrame .
I want to stack the output vertically in a csv .
` to_csv ` accepts a mode argument to append : #CODE
I'd apply ` value_counts ` columnwise rather than doing ` groupby ` : #CODE
How to flush away the default suptitle of boxplot with subplots made by pandas package for python
I have a DataFrame where I would like to keep the rows when a particular variable has a NaN value and drop the nonmissing values .
In the above DataFrame , I would like to drop all observations where opinion is not missing ( so , I would like to drop the rows where ticker is aapl , goog , and wmt ) .
Note : Here we're reading 2 1 3 to mean [ " B " , " A " , " C "] , however you could replace the index of the result as desired ( i.e. replace it with one of these ): #CODE
The transpose method is not working as expected .
Use melt function for that .
you can use sort() after you melt to get the exact order
Maybe use stack ?
You can use ` map ` from a dict of your second df ( in my case it is called df1 . yours is DF2 ) , and then multiply the result of this by the amount : #CODE
So breaking this down , ` map ` will match the value against the value in the dict key , in this case we are matching ` Currency ` against the ` NAME ` key , the value in the dict is the ` OPEN ` values , the result of this would be : #CODE
Merge multiple data frames with different dimensions using Pandas
What I want to do is to merge them based on ` head ` column .
Pandas DataFrame column to cell in pivot table
And I'm looking to get a pivot table out of this with A in the columns and B in the rows , then C and D as sets , as below : #CODE
` groupby ` and then ` unstack ` : #CODE
Pandas join memoryError
I'm trying to join 2 dataframes , essentially trying to replicate a vlookup in python .
I suspect this depends on how many rows share a sku number ( as this can explode - i.e. if 2 rows join with two rows then you get 4 rows etc . )
There is numpy's histogram function , and matplotlib's histogram plotting function ' hist ' .
I see now , sorry , I misunderstood before ... and aside from rolling your own , using a combination of ` np.histogram() ` ( to get the column hights ) and ` plt.bar() ` ( to plot the interleaved bar-charts ) , I don't know of any quick tricks to do what you want . plt.bar() is not too difficult to make interleaved bar-charts though ..
This presentation doesn't rescale , it horizontally translates the individual histograms so that they don't overlap and then labels the X-axis with the column names ( at median values ) rather than represent scale .
I basically want to apply the solution that you described to ** each item ** in the series ( i.e. get the offset with respect to ** each timestamp** ' s hour in ` ms `)
If you just want this code to work , just replace the return with this , which uses the numpy divide function , which will broadcast on shape and not try to match indicies : #CODE
I think ` np.divide ( diff , pd.DataFrame ( time_intvall ))` should always work .
See the docs on apply .
Yeah , or you could even still use apply on the main df , but then change item_grouper to iterate .
So I have a dataset consisting of several million rows of trading data that I am trying to apply a filter to as in the following code .
As far as I can tell , there is no way to avoid iterating row by row here and still take the trimmed rolling means and standard deviation .
The lack of hashability of dictionaries also makes it impossible to calculates the trimmed rolling means and standard deviations so I can't convert the dataframe to a dict .
( For now , let's assume that ` DataFrame ` has a regular ` Index ` on both the rows and the columns , not a ` MultiIndex ` , but I'm curious how answers would change if we dropped that assumption . ) Let's call that ` tabledf ` .
However , beware that the ` matplotlib ` LaTeX system does not necessarily like line breaks , so you may need to replace ` \n ` by spaces in the string given by ` to_latex ` .
Desired pivot / groupby / crosstab Output : #CODE
Unless you actually need the query string for some reason ( maybe to print it out ? ) , building a string to eval it as an expression , instead of building an expression , is usually the wrong answer .
/ e : I just realized that I missed an important condition in my question : I only want to drop the observations , if one or several SPECIFIC columns have duplicate values .
Then you could truncate them and evaluate as an integer .
I'm trying to apply a filter to some time series data like below and make a new series for outliers .
I first cut it in chunks and store them in a ` HDFStore ` named ` input.h5 ` then I use ` groupby ` on ` user_id ` following Jeff's way .
` group_user= store.select ( ' clean_input ' , where =[ ' user_id == %s ' %user ])` is too heavy in time complexity since I have really a lot of groups , and I am sure there is a lot of redundant sorting in the routine of ` store.select ` if I apply it 10 millions times .
But you have not introduced the variable sub_group_chunk , and If i try chunk.groupby ( sub_group_hash ) I get an error of the form : long object have no attribute _____get.item____ because It tries to apply the function to the index .
I figured out this also works : ` pd.read_sql ( " SELECT CONCAT ( y , ' : ' , w , ' : 1 ') AS dt , d FROM t " , db , parse_dates = { " dt " : " %Y : %U : %w " } )` .
It still does CONCAT in SQL , so I'm not sure if it's better or not than what you suggest .
Really love being able to iterate through the groups of a groupby operation and I am getting the result I want from the groupby but I am unable to merge the final result into one dataframe .
And I just want to merge them all on the col1 key .
I'll try to join the github world soon when I have time .
Now let's make something that has the right shape as what we want ( we shouldn't assume that we'll necessarily see every month , after all ; our test example only has 5 months with nonzero values ): #CODE
Accessing columns after applying the stack function on a pandas dataframe
How can I access the columns after the stack function is applied on a dataframe ?
I trying to create a function that replace df [ ' S '] using df [ ' S '] [ index-1 ] value .
@USER I'm iterating because I'm using it to insert values in a DB coming from an XLS .
That said , you can use ` concat ` to stack dataframes .
I'll accept the one that uses ` ix ` since it conforms more narrowly to the question as asked .
The indexers ( ` ix ` , ` iloc ` , ` loc `) can do a lot more , but because of the syntax ambiguity between tuple keys and MultiIndex selection , it's likely less error prone .
In my experience , this approach seems slower than using an approach like ` apply ` or ` map ` , but as always , it's up to you to decide how to make the performance / ease of coding tradeoff .
you can use ` nameframe = eval ( " output_ " +str ( num ))` in the loop , but I agree with @USER you should be storing these as a list upon creation , or if you want to keep names use a dictionary .
E.g. , if each dataframe is a different group , you could set a MultiIndex on the combined frame , like #CODE
Your second line can be simplified to ` df_length = sum ( len ( x ) for x in dfs )`
You could apply ` f ` first , and pass the return value to ` groupby ` : #CODE
Pandas SettingWithCopyWarning When Using loc
You've assigned ` df1 ` to be a reference to a slice of your ` df ` but now performing the ` loc ` assignment has only modified ` df1 ` , it smells like a bug to me but I am not 100% sure if this is intended or not
vlookup in Pandas using join
I want to perform a merge , or join opertation using Pandas ( or whichever Python operator is best ) to produce the below data frame .
Perform a ` left ` merge , this will use ` sku ` column as the column to join on : #CODE
Another method is to use ` map ` , if you set ` sku ` as the index on your second df , so in effect it becomes a Series then the code simplifies to this : #CODE
Could someone comment on what " join " method is for ?
A method called ' join ' doing similar but not exactly database join is confusing .
@USER you can think of it as a DB style join , it joins on index by default or on a list of passed in columns , however it is not as flexible as merge which allows the columns on the left and right hand side to have different names , also if there is a clash you have to specify the left and right suffix there is a brief example here : #URL
@USER If " join " is DB style join , then df_Example1.join ( df_Example2 , on = ' sku ' , lsuffix= ' _ProdHier ') should work .
@USER the problem is the index mismatch , although you are joining on the specified column it will try to align on index so none of the join types will work in this case so a merge is a better option here , join should be thought of as simple join's where the indices are aligned so it's not quite a full blown DB style join in that sense
I'll stick to merge :)
pandas partial join on multiindex
I can't naively do ` dfa.d / dfb.m ` or ` join ` because it says that ` merging with more than one level overlap on a multi-index is not implemented ` .
if you look for my error message on [ the source code ] ( #URL ) it seems to make sense ( " join on 1 but no more than 1 ") ; I wonder why though .
get the non-matching index level ( s ) out the way by ` unstack ` ing them into columns
` stack ` the columns back to where they were .
You could replace ` dfb.squeeze() ` with ` dfb [ ' m ']` .
Python pandas to mimic excel pivot table
You can pass a dict to the ` agg ` method .
The keys are column names , and each value is the corresponding function you wish to use to aggregate the column : #CODE
Note that pandas has function ` pivot_table ` for spreadsheet like pivot tables as documented at #URL Though I ( as a non Excel person ) find @USER ' s solution easier to comprehend .
In Python with Pandas how to shift through an Index of unknown frequency using a dictonary
I have been trying to shift or tshift through a DataFrame index by one ' shift ' from a subset of slices in a dictionary ' lix ' .
I would like to take lix [ ' A '] slice and shift it by one forward , but per so.index .
Then I would take lix [ ' B '] and shift it by one forward etc .
For example , I can get the end of the slice and use pd.datetools.BDay ( 1 ) to offset the end by one Business day , but that's not what is in so.index , so how to add one shift such that the result lands on a valid date from so.index ?
re shift : pd.Series.shift ( so [ ' IBM '] [ lix [ ' A ']]) will return a DataFrame with the data shifted by one forward , but the idea is to get the shifted Index .
Have you tried [ ` shift `] ( #URL )
Sorry , just noticed that with your last column , you'd have to replace ` list ` with a custom function that converts the string representation of the list into an actual python list , the ` list() ` call won't do it so you'll just get `" No_Data " for all rows .
What has this to do with the resample ?
pandas apply function to corresponding cells of multiple frames
I'm trying to apply a function to corresponding cells of two identically sized dataFrames to create a new frame .
( it will depend on the timing of the applied function if the looping will be determinant for speed ) And can the function you want to apply be vectorized ?
If the rows already line up ( ? ) , wouldn't it make sense to merge them first ?
Even if the rows don't exactly line up , you can merge the two datasets based on a certain column that should match to compare them ( eg ` pd.merge `)
It may be that the v0.14.1 for different distros was cut off slightly different git versions depending on how you installed pandas .
That commit was from 2012 , 2014 was only a couple of months ago , so even if two 0.14.1 versions were cut from different commits ( i.e. one not from the tag ) they'll both definitely still have that .
" _ I don't know if this can apply to your use case , but have you investigated [ PyTables ] ( #URL ) ?
In any case parsing text files is slow , so if you ever can translate it into something more palatable ( HDF5 , NumPy array ) , loading will be only limited by the I / O performance .
You could replace ` df [ df [ ' three '] == i ]` with a ` groupby ` on column three .
And perhaps replace ` [ ' two '] [ j + 1 ] - [ ' two '] [ j ]` with ` df [ ' two '] .shift ( -1 ) - df [ ' two ']` .
The loop in total now runs six times ( outer twice and inner 3x ) , and you append once outside the loop .
If all you want to do is to get the difference between the rows of column two you use the shift method .
You're looking for a groupby with an apply .
Hi zerovector , I am getting an error when I try to apply that idea : def func ( x ):
return pd.Series ( { ' Obs ' : len ( x ) , ' Sum ' : sum ( x ) , ' Zeros ' : len ( x [ x.price == 0 ]) } ) -- the error says : TypeError : unsupported operand type ( s ) for + : ' int ' and ' str '
this is a simple question that is escaping me -- for the last part : len ( x [ x.price == 0 ]) } ) , the column is actually called price .
-- however , when i type in len ( x [ x.price . == 0 ]) } ) , python does not like the syntax -- do you know of a simple way to get tell python not to read the period ?
An apply function was what I was looking for !
I have a DataFrame with multiindex in the form of ( p , t ) tuples , where p and t are floating point numbers .
` KeyError : ' MultiIndex Slicing requires the index to be fully lexsorted tuple len ( 2 ) , lexsort depth ( 0 )'`
Quick question though : How would you do to drop the 1.5 level ( desired output without the ' Pump ' level ) ?
I am wondering if there is a way to do a pandas dataframe apply function in parallel .
I will give it a try with the apply function and report back .
Instead of populating res , which waits for all of the workers to finish all available tasks , is there a way to get the output from worker 1 and append it into a dataframe , than say the output from worker 3 ( order does not matter ) and append that and so on ?
It doesn't seem to work with imap or map ..
You can use ` stack ` here , which will produce a Series with the row and column information in the index , and then call ` nlargest ` on that : #CODE
Is there an elegant way to append all of them ?
Did you look at [ ` concat `] ( #URL ) ?
Following parallel processing example works in IPython by using concat method : #CODE
You can add subtotals to the pivot tables directly but it requires a bit of munging .
You then stack the resultant pivot table .
Summing multiple columns with multiindex columns
I have a dataframe that is created from a pivot table , and looks similar to this : #CODE
I'm looking to iterative over the upper level of the multiindex column to create a sum column for each company : #CODE
If you want them to add to the original dataframe , as in your example above , you can add a level in the columns and concat : #CODE
( Note that there is no .fit function for OLS in Pandas ) Could somebody shed some light on how I might get future predictions from my OLS model in either pandas or statsmodel-I realize I must not be using .predict properly and I've read the multiple other problems people have had but they do not seem to apply to my case .
The word " coordinates " reminds me of ` pivot ` , since
pivot is the tool to use .
In fact ` df1 ` looks like the result of a ` pivot ` operation .
So we melt ` df2 ` .
ANd append another file with the same headers to it .
Because I want to append another file to it , I am trying to ensure that the columns are correct .
Examine the stack trace : #CODE
I have used ` merge ` in this fashion before , so i was very surprised when the ` filtered ` dataframe returned as empty .
Improve code to replace values above the median by the median value itself in a pandas DataFrame
I'd like to replace all the values above the median value of a column by the median value of the column itself .
You can use numpy where and apply to do it for all columns in a DataFrame : #CODE
I have also experimented with writing my own smoothing function , which carries forwards values when there is a > 20% drop .
Given the clear trend in my data , I opted for a variation on " Median Absolute Deviation " : to compare each point in the measurement series with the rolling median , generate differences , and select a cutoff point appropriately .
I start by chunking the CSV file , and apply a ` groupby ` ( on the two last figures of the ` user_id `) on the chunks files so I can store a total of 100 files containing groups of users , and storing them in a HDF5 store .
From what app is what I meant as you are going to copy into the clip board the data from MS SQL , a text editor , python console etc .. from what app did you do this , also can you post data and steps to reproduce
I'm obviously new so don't think I'm formatting everything properly within the stack overflow site .
I've tried to replace all of the " inf " with NaN and then use the Pandas function to remove them but that didn't seem to work ( may have done it wrong ) .
i see that pandas.factorize() is the way to go , but it only works on one column . how do i factorize the dataframe in one go with one command .
Then concat that to the numeric data .
Dividing multiindex columns by sum to create percentages
I have a dataframe that is created from a pivot table , and looks similar to this : #CODE
I'm looking to iterative over the upper level of the multiindex column to divide each company by it's sum to create a percentage : #CODE
This is a variation of a question asked yesterday : Summing multiple columns with multiindex columns
Er .. have you looked at [ ` concat `] ( #URL ) ?
then something like map , zip , join combination should get what you want ( ? ): #CODE
efficiently join two labels of a dataframe
I want to join ( as in sum up ) two labels , while I relace the new label .
We construct a dataframe to append to your existing one .
We set the new index to ' c and d ' , then sum those rows where the labels are in ' c ' and ' d ' , then finally drop those .
However , I'm getting a ` MultiIndex ` ` Series ` as output when I expect a singly-indexed ` DataFrame ` .
The difference lies in the fact that ` grouped.median() ` calls an optimized ( cythonized ) ` median ` aggregation function , while ` grouped.quantile() ` calls a generic wrapper to apply the function on the groups .
So ` grouped.quantile() ` does a general apply and not an aggregation .
So that I keep only rows for which the time index is in the intersection of the time indices of the old series .
In pandas , there is an ` align ` operation , and would work like this : #CODE
Can you advise for a tutorial for the other join possibilities ( outer .. ) ?
In R , the function merge seems to accept any number of series .
I prefer chaining the ` join ` method : ` a.join ( b , [ options ]) .join ( c , [ different options ]) .join ( ... )`
You could also use ` concat ` , like ` df = pd.concat ([ a , b , c ] , join= ' inner ' , axis=1 )` , and then split the DataFrame back into Series
In R with zoo it would be : ` merge ( a , b , all = FALSE , retclass = NULL )` .
Note that ( 1 ) ` all=FALSE ` returns the intersection only and ( 2 ) ` retclass=NULL ` returns the merged series by writing the outputs back to the arguments .
Look at the help file for ` merge.zoo ` , not the one for ` merge ` .
@USER I get ` Type Error : ' bool ' object is not callable ` when I do that
In terms of how you make a new column based on other columns - if you absolutely need to iterate , then you can assign using ` loc ` , as you did in one example .
But you should always look for a vectorized solution , then look at ` apply ` , and only then think about iterating .
I'm not sure I would post this as an answer as it's a little too f*cked up even for me : ` df.loc [ df.Location_Id.isin ( df.groupby ( ' Location_Id ') [ ' Item_Id '] .unique() .apply ( lambda x : len ( x ) > 1 ) .replace ( False , NaN ) .dropna() .index ) , ' average_item_price '] = df [ ' averageprice '] / df [ ' averageprice '] .shift() `
To Ed : df.loc [ df.Location_Id.isin ( df.groupby ( ' Location_Id ') [ ' Item_Id '] .unique() .apply ( ?? lambda x : len ( x ) > 1 ) .replace ( False , NaN ) .dropna() .index ) , ' average_item_price '] = df [ ' averageprice '] / df [ ' averageprice '] .shift()
First create a date mask that gets you the correct dates : #CODE
This give yous a boolean mask for groupby of each store for each date where there are exactly two unique ` Item_Id ` present .
From this you can now apply the function that concatenates your prices : #CODE
I think this could be join on the two subFrames ( but perhaps there is a cleaner pivoty way ): #CODE
The ` update ` method unlike a normal ` dict.update ` adds to the values , it does not replace the values
If you return a Series of the ( split ) location , you can merge ( ` join ` to merge on index ) the resulting DF directly with your value column .
If I'm not mistaken , it only works if ` df ` has index that is ` range ( len ( df ))` , right ?
` join ` is shorthand for merging on index with both frames , so the indices need only be consistent ( which it will be here as the apply and col selection don't affect it ) .
b3 = b2.reset_index ( drop = True )
Anyway , you can either map / broadcast the Pandas ` Timestamp.strftime ` function over it , or use that ` to_pydatetime ` function that you found and then map the Python ` datetime.strftime ` function over the resulting array .
Given a ` YYYYMMDD ` string , to convert that to an integer , you just call ` int ` on it or , if it's in a Pandas / Numpy array , you map or broadcast ` int ` over it , or , even more simply , just cast its dtype .
First , align ` df2 ` with ` df1 ` , which creates a frame indexed with the union of ` df1 ` / ` df2 ` , filled with df2's values .
You could use lambda function and ` apply ` .
In this case , I made the new index level by taking the first character of the original columns , but of course you can apply another function here if wanted .
You can create a dict and call ` map ` : #CODE
As user @USER has pointed out using ` map ` or ` apply ` should be a last resort if a vectorised solution can be applied .
What you wrote is incorrect , you are calling apply on the df but the column as a label does not exist , see below : #CODE
in general I wouldn't show a map / apply based soln if the vectorized one works ( it's confusing and much slower )
Multi column join in pandas from groupby
I have a large dataframe and a small dataframe that I would like to join together .
I want to join the maximum value per week per group to the large frame .
This is what I want the final frame to look like after the join : #CODE
You can avoid that by using ` how= ' left '` or ` how= ' right '` as a join parameter .
By default , join uses ` how= ' inner '` , which leaves you with only the intersection of the merged values .
You want to use ` transform ` which will return a like-indexed version of the aggregation .
pandas rolling appy on a dataframe
If you groupby with as_index=False you can merge with tf : #CODE
You can pass density parameter to hist , like this #CODE
How can I make it recognize NUMPY hist when I have both pandas and Numpy loaded ?
The ' density ' option works in numpy's histogram function but not on pandas's hist function .
My data frame , I looked through other stack questions and noticed other people got this , too .
I believe the functionality you're looking for is in the hist method of a Series object which wraps the hist() function in matplotlib
Computing a generic rolling function over an array of size ` n ` with a window of size ` m ` requires roughly ` O ( n*m )` time .
In my experience , one of the more enjoyable parts of using the Python scientific stack is coming up with efficient ways of doing computations , using the fast primitives provided in creative ways .
In the linked question , the solution presented was to transform a groupby clause and then attach it to the dataframe , however this is creating ordering errors in the series .
You can ` transform ` groups indexed on both ` group ` and the week simultaneously : #CODE
If you want instead a rolling maximum , you can use ( appropriately enough ) ` rolling_max ` .
You can either resample yourself or get ` rolling_max ` to do it , something like #CODE
Are you trying ( 1 ) to shift the definition of a week by a few days , or ( 2 ) do you want a rolling window , so that each day has a different week we're taking the maximum over ?
A moving window , each day with a different week , using that day as a median .
simple question here -- how do I replace all of the whitespaces in a column with a zero ?
I've managed to do this with apply , like this : #CODE
Depending on the size of frame and number of matches it may be more efficient to use join operations :
Then , join them on `' PersonID '` : #CODE
and , align with the original series : #CODE
The first join is actually a Cartesian product , right ?
If i use ` aggregate ( np.mean )` the average of all datapoints of one column is calculated .
This feels sloppy to me , but my loop / apply attempts have been much slower .
With that size , the apply attempts were taking hours .
The ` -1 ` tells NumPy to replace ` -1 ` with whatever positive integer is needed for the reshape to make sense .
I tried to use some pandas built-in commands , but couldn't find the right one since they aggregate by day without taking into consideration that each day contains sub columns in them .
` ts = pd.Series ( df , index = pd.bdate_range ( start = ' 2013 / 01 / 01 00:00 : 00 ' , end = ' 2013 / 01 / 31 23:59 : 59 ' , freq = ' S '))`
I would go for apply : #CODE
I need to shift a column of uneven timestamps by a column of uneven timedeltas .
I can use merge but there must be a cleaner way , without having to drop column Z .
You can grab the columns you want in the merge syntax #CODE
The way I did it was to create a second data frame representing the groups , merge the two data frames , group by the row group names , and sum , #CODE
or replace ` df.groupby ( ' x ' , axis = 0 ) .sum() ` with ` df.set_index ( ' x ')` : #CODE
You can use strptime and a format to get the date string to insert into the yahoo web.get : #CODE
I know that with dummy variables I can get the value of the columns and transform as the name of the column , but is there a way to merge them ( combination ) easily , as R does ?
or the ` pivot_table ` ( with ` len ` as the aggregating function , but here you have to ` fillna ` the NaNs with zeros manually ): #CODE
If you want to ' melt ' this into one level like in R you can do : #CODE
Is there a way to merge the convert_me and age_col ?
You can use a combination of ` groupby ` , ` transform ` and then ` drop_duplicates ` #CODE
Pandas multiple index with multiple aggregate functions
Is there an easy way to obtain an output like this without manually create all the pivot parts ?
so the selection can be expressed as a left join : #CODE
Thanks , this is a great sample . but in this way i must manually create the pivot parts , are there no way to use pivot or pivot_table directly ?
If I remove the min_periods=0 from df [ ' freq_average '] = pd.rolling_mean ( df [ ' freq '] , 5 , min_periods=0 , center=True ) I get a result similar to what you are referring to .
` from the matplotlib docs ` Return value is a datetime instance in timezone tz ( default to rcparams TZ value ) .
` read_csv ` difference in implementation and performance , the next thing I tried was to append the records into a CSV and then read them all into a DataFrame : #CODE
You could use pytables / HDF5 to do the concat on disk ... see #URL but that may not be enough .
I want to resample to monthly data taking account only months with less than 10 day NaN values .
I've tried using the resample function , by this way :
Note : count by using ` isnull ` and ` sum ` .
Then do the resample ( this is what ` groupby ( g ) .mean() ` does ) #CODE
Your SQL is the equivalent of an inner join , so how about this ?
Now I want to replace one of dataframe ` NaN ` s , I used ` ffill() ` , it worked but I need to limit filling ` NaN ` , I don't need what I marked red .
Pandas group by to aggregate string field
You can then use apply to concatenate : #CODE
If the fierst column is intger type you suggest to cast to string and the to use apply ?
Question How do I add the score of the permutated words and stack it
I'm not sure how to calculate the rolling sum ( with decay ) without iterating through the dataframe , since I need to know yesterday's score to calculate today's score .
` pd.expanding_apply ` applies the rollingsum function backwards to each row , calling it ` len ( dataset )` times .
` np.linspace ` generates a dataset of size ` len ( dataset )` and calculates how many times each row is multiplied by ` exp ( - 0.05 )` for the current row .
Percentiles combined with Pandas groupby / aggregate
I am using a dict in combination with a Pandas aggregate function as below : #CODE
You can groupby the Label column , apply the list constructor .
I originally wrote this in a SQL query which accomplished the task quite simply with a join : #CODE
First merge ` df ` and ` df2 ` on the ` bin ` column , and then select the rows where ` cut_min = perc cut_max ` : #CODE
Follow-up question : What if I wanted to do this like a left outer join ?
I ended up using the first alternative method ( altering df2 to cover all possible values of bin and perc so that I could use an inner merge without losing rows ) .
if you want to convert it to strings , you can apply ` strfitme ` ( ` df [ ' timestamp '] .apply ( lambda x : x.strftime ( ' %Y-%m-%d '))`) .
Pandas : Read Timestamp from CSV in GMT then resample
To convert a Timestamp / datetime64 column use tz_convert ( if the are tz naive , i.e. don't have a timezone yet , you'll need to tz_localize first ): #CODE
How to replace efficiently values on a pandas DataFrame ?
I just compared ` update ` against ` map ` and was a little surprised that ` map ` outpferformed ` update ` , for 90k dataframe ` update ` takes 19.4ms vs 8.61ms for ` map ` .
The reason I say this is that Jeff has always commented to me that ` map ` and ` apply ` are last resort methods so I thought ` update ` would perform better
@USER : I think ` map ` with a function argument can be slow .
` map ` with a series argument should be pretty fast .
I think I was told as a comment that map is a cython optimised for loop , not sure if that changes if the datatype is a series .
Looping through df dictionary in order to merge df's in Pandas
I would like to merge them ' inner ' by their indexes but with iteration using a for loop .
( Caveat : Using ` pd.concat ` is fragile here -- I'm assuming the DataFrames do not have NaN values , but may have different indexes . The ` dropna ` is then used to produce an inner join . )
The concat option is a nice alternative .
is it possible to transform the hierarchical df to a normal df .... like get rid of the extra level with df1 ..
Or to drop a level , ` df.columns = df.columns.droplevel ( 0 )` .
ah yes , is " keys " in concat !
concat has you covered : #CODE
MultiIndex from product of unique values of two indexes
How's about using union * to join the two MultiIndex together : #CODE
* Initial answer was to use append , but I think union is more readable .
I want to groupby three columns in the dataframe , and then apply a function to each group .
how about a merge and then groupby ?
I think I would probably reindex after the fact : #CODE
Note : In 0.15.0 you'll have access to these as Series dt accessor : #CODE
Do you already have tz info ?
BTW , what do you mean by having ` tz info ` ?
Either way , the goal is just to get some iterable of rows and insert each one by looping over the rows and calling ` execute ` .
also how does this work with PYODBC to insert the values ?
For the ' write to sql server ' part , you can use the convenient ` to_sql ` method of pandas ( so no need to iterate over the rows and do the insert manually ) .
The text contains unwanted unicode characters which I normally strip out using #CODE
Can you not just call ` dropna ` or you want to replace the ` NaN ` with some value ?
sounds like you should store the data with pytables , and use pandas to do the join and file I / O .
Pandas : fastest way to check if words in Series A endswith one word of Series B
You can pass ` endswith ` a tuple here ( so you might as well use that instead of a Series ): #CODE
What I needed to do was to add a " transform " to my dataframe , in this case a Moving Average .
You can then map a dictionary onto those values : #CODE
Well , I want to grab all of the data frames that end in ' cd ' , for example , and append ( union all ) them into a final data frame .
Just create your own and append to some list or dictionary of lists ?
When accessing one value of the column above , this gives back a ` Timedelta ` , and on this you don't need to use the ` dt ` accessor , but you can access directly the components : #CODE
The reason that ` df [ ' column_with_times '] .apply ( lambda x : x.days )` does not work is that apply is given the ` timedelta64 ` values ( and not the ` Timedelta ` pandas type ) , and these don't have such attributes .
Yes , exactly BUT my len ( data1 ) and len ( data2 ) is not the same .
Still , I'm struggling to find a way to do the kendalltau calculation on a dataframe with multiple columns on a rolling basis .
A possible workaround is to pass ` np.arange ( len ( A ))` as the first argument to ` rolling_apply ` , so that the ` tau ` function receives the index of the rows you wish to use .
The number of combinations grows like ` n**2 ` , so ` tau ` gets called on the order of ` n**2 * m ` times where ` m = len ( A )` .
Thanks for the answer , and sorry about not including the cvs file , I cut the code that I used to make mine out of the post and forgot about it .
Is 64 still magic if you drop one of the columns ?
For example if a certian calendar has February 30th in it then this will not work hence why I want to try to index on the Year , Month , Day multiindex
So that's how map works .
If you want to be more picky then you could use some mask / criteria to manually set to NaN ( but I don't see how you would expect astype to do that : s )
the above gave me error msg like comparsion by diff types .
Again I may be mistaken here , but using my example of querying date / time range ; I'll probably incur a performance penalty as I'll need to read each table and have to merge them if I want a consolidated output right ?
See an example of selecting using a where mask
Recombing data is relatively cheap ( via ` concat `) , so don't be afraid to sub-query ( though doing this too much can drag perf as well ) .
Can one use comparisons to merge two pandas data-frames ?
Since the values you're joining on are no longer unique , you may not have the merge working as you expect .
Wondering if a non-SQL solution would be easier ( ie : parse + merge in python ) .
I don't understand what operation you want to perform on the rows , but the ` shift ` method would suit you .
I like the map with dictionary approach .
I would assume to_excel() would only try to apply the parameter to float-formatted columns ( or even specific cells ) rather than to every piece of data , so I'm not sure what I'm missing .
I have a pandas dataFrame object , and I want to check if its index field is a multiIndex , or if the index is " length one " .
I think you are looking for a rolling apply ( rolling mean in this case ) ?
Python pandas : replace select values in groupby object
Also , ` append ` will naturally append * rows * not columns , fyi .
I transform a lot of columns into fewer columns , and calculate a cluster id using the new columns .
` concat ` will only put things together that have the same index ( i.e. if one of your Series has row indices of [ 0 , 1 , 2 ] and one of them has indices of [ 2 , 3 , 4 ] , it will give you a DataFrame with indices [ 0 , 1 , 2 , 3 , 4 ] , with NaNs in the places where there is no overlap )
I'm attempting to use pandas to join content from three separate flat files into a single .csv .
it'd be better to concat them but set the ' StoreID ' prior to the concatenation , at the moment you are overwriting each merge with the last set operation so all your rows will have the same ' storeID ' .
I'd just create a temp df or the intial merge , set the ' storeID ' column and then either merge or concat to your item_merged ' df .
My approach would be to merge outside the loop , make a temp df that is a copy , set the storeId for this df and concat to your merged df and then write out to a csv : #CODE
The intent is to loop & concat all itemID to StoreLocation pairs for every 1 storeID
Certain rows have duplicate values only in the second column , i want to remove these rows from the dataframe but neither drop nor drop_duplicates is working .
drop and duplicates create new datafraames .
By default drop and in fact most pandas operations return a copy , for some and in fact these functions then can be passed the param ` in_place=true ` to perform the operation on the original df and not return a copy
As mentioned in the comments , ` drop ` and ` drop_duplicates ` creates a new DataFrame , unless provided with an inplace argument .
Question : is there a proper way to aggregate the dataframe , so I will get a new time series with the standard deviations for each restaurant ?
Does anyone know of any workarounds to obtain mean / median on timedeltas ?
Cleaner pandas apply with function that cannot use pandas.Series and non-unique index
you can tell ` groupby ` to drop the keys using the keyword parameter ` group_keys=False ` : #CODE
It is possible to replace all occurrences of a string ( here a newline ) by manually writing all column names : #CODE
You can use ` replace ` and pass the strings to find / replace as dictionary keys / items : #CODE
The use case : I want to apply a function to each row via a parallel map in IPython .
Well technically " df.groupby ( np.arange ( len ( df )) // ( len ( df ) / 10 ))" to get a fixed number of groups ( 1 per core ) instead of fixed size .
I understand that I can merge them together and get a score for each word : #CODE
So , " play " is in df2 but is removed after the merge but I would like to keep it in the result after the merge .
So , I would want the merge to give this instead : #CODE
For ( i ) you just need to specify ` right ` join , and fill null values : #CODE
Use [ shift ] ( #URL )
More generally , I understand that the approach with ` FacetGrid ` is to make the grid and then ` map ` a plotting function to it .
In particular , how can I write my own plotting function ( to pass to ` map ` for ` FacetGrid `) that accepts multiple columns worth of data from my dataset ?
It can be easier then to just call , e.g. , ` g.set_axis_labels ( " Date " , " Stat ")` after you use ` map ` , which will rename your axes properly .
I need to cast mydf to DataFrame , concat , then cast back #CODE
I don't think concat will ever return a MyDF though , so best to hope for will be ` MyDF ( pd.concat ( mdfs ))` .
But out of curiosity about quandl I took a glance at their page and wonder if you just need to replace GBPUSD with QUANDL / GBPUSD .
I feel like there must be some sort of rolling window , or simple way select the last three trials .
If you want to duplicate that data in every row of the original DataFrame for the corresponding subject , use ` map ` to grab the pattern for each subject I D: #CODE
Now what I want to do is replace the 0 values with NaN values when all row values are zero .
But , if all you're trying to do is replace rows of all zeros for specific set of columns with ` NA ` , you could do this instead #CODE
To answer your original question , ` np.where ` follows the same broadcasting rules as other array operations so you would replace ` ???
Perhaps it used to - but not after you used ` drop ` .
So i cant really use the sum function , nor can i use drop duplicates as i need the duplicates to be consecutive .
Or do i need to reindex ?
And how would i reindex but keep the current order ( skipping over dropped rows )
I still think ` iloc ` would fix your issue , and if you want to reindex simply set ` df.index ` to something ( range the size of your df , for example ) .
Pandas - Create a new column with apply for float indexed dataframe
As a general rule avoid using ` apply ` if there is a vectorised operation , for basic operations such as add / subtract / div and multiply there are built in operator support for these that are many orders of magnitude faster .
Pandas : how do I merge minute data rows in a timeseries
You can use the ` stack ` method for this : #CODE
To merge both levels into one index , you can do ( for each index entry , I just concatenate both levels with a space in between ): #CODE
One can " shift " the data by a certain amount : #CODE
One way would be to use ` shift ` to move the relevant column down ` n ` rows and then concatenate the entries ( they are strings so we can use ` + `) : #CODE
thanks - i like shift . and yes , i agree it is better not to have lists .
pandas MultiIndex degeneration
I want to make a multiindex with lexsort-depth 7 for a dataframe .
The Pandas multiindex constructor excludes those with the same value .
Returning multiple columns with pandas apply and user-defined functions
In reality , my function will be much more complicated - so I only want to run it once within the apply and assign ` output [ 0 ]` , ` output [ 1 ]` , etc to individual columns .
( n.b. , I edited this answer in light of the comment below ) so the apply step could take a single function with shared calculations and return the required series for the merge step .
However , I used your form with json_normalize() : ` df = concat (( json_normalize ( d ) for d in data ) , axis=0 )`
nbconvert multiindex dataframes to latex
This example shows only the first of the problems , this output will show the multiindex stacked one on top of the other , but I can't find a way of formatting it before output .
@USER I think the multirow ( MultiIndex column ) issue might already be fixed in master !
From the docs , the dict has to map from labels to group names , so this will work if you put `' A '` into the index : #CODE
` apply ( Series )` gives me a DataFrame with two columns .
To join them into one while keeping the original index , I use ` unstack ` .
Then I join it back into the df .
Python bcolz how to merge two ctables
Were you wanting the full range of behaviours of merge , or something more specific ?
I would probably pull off numpy chunks from the ctable as recarrays , merge them in memory , and then append them onto a target table .
Python summarize row into column ( Pandas pivot table )
Here's the nicest way to do it , using ` unstack ` .
there is precisely one entry for that loc ,
This is the end of the stack trace : #CODE
` iloc ` is for integers only , whereas ` ix ` will work for both integers and labels , and is available in older versions of Pandas .
Anyway , you can do this using apply : #CODE
The function in the apply works by first seeing if the ID is in the dictionary ( and tuples won't be ) and if it is , go for that , and if it isn't find the first one that is ...
I need to slice several intervals out of one dataframe indexed with Freq : 120T .
The start date of each of the desired intervals is given by a second dataframe indexed with Freq : None .
Is like df1 has the granular data ( higher freq ) and df2 gives you the date .
You're looking for pivot ... at least once you've extracted the rows you're interested in .
Those that are on the same date , use ` normalize ` and ` isin ` : #CODE
Once it's in that form pivot away ( if there is likely to be missing data you may have to use ` pivot_table ` which is a bit more flexible ) !
You can map the columns of the pivoted result : #CODE
I * think * you mean you want to pick the next ( ? ) time after date and time dt ( for several dates with the same start time ) and then pick the subsequent N records ( inclusive ) ?
We can use this in a concat for each day in the index : #CODE
Note : I couldn't get this to work cleanly with asof itself ... which may be more efficient .
Now I just need to take the df and pivot it .
If I unstack once the result is : #CODE
Python pandas - particular merge / replacement
This takes a couple steps , left ` merge ` on the columns that match , this will create ' x ' and ' y ' where there are clashes : #CODE
So , I want to replace nan's in Y column that correspond to observations in X with " * NY " part , to numbers in Y that correspond to observations in X that have the same numeric part but without " * NY "
This was a bit more annoying to code , basically we can apply a custom function that performs the lookup for you : #CODE
OK but generally using ` apply ` should be a last resort , there are vectorised functions in pandas and numpy that will perform math operations on the whole df , please check the numbers I can't guarantee anything
Compared to the apply function which took 4.3s so nearly 250 times quicker , something to note in the future
You could reset the multiindex to columns by : #CODE
There is a ` pandas.pivot_table ` function and if you define ` datadate ` and ` id ` as indices , you can do ` unstack ` the dataframe .
I think you can do that with melt , but it seems messy !
It gave me error " no numeric types to aggregate " .
Inconsistent behavior of ` dataframe.groupby ( allcolumns ) .agg ( len )`
Now , here's the result of grouping on all the columns of ` df0 ` and aggregating with ` len ` as aggregator function : #CODE
Based on this result , I expected that the analogous operation for ` df1 ` , namely ` df1.groupby ([ ' X ']) .agg ( len )` , would give this : #CODE
What's the simplest way to get the output I expected ( as shown above ) from ` df1.groupby ([ ' X ']) .agg ( len )` ?
The simplest way to get what you're looking for in #2 may be a pivot table .
Pandas ' eats ' the aggregator column , so you are left with nothing to aggregate .
Pandas merge 2 databases based on 2 keys
I am trying to merge 2 pandas df's :
However it does not merge these as one row .
Even after modifying df1 and df2 to have matching keys , the merge still works as expected for me .
For example , if ` df1 [ ' Year ']` are integers , but ` df2 [ ' Year ']` are strings , Pandas won't join them as equal .
For string dtypes you may wish to strip whitespace as well : ` df [ ' ISO3 '] = df [ ' ISO3 '] .str .strip() ` .
Update : I needed to transpose the numpy array so that ` rate_df ` was correctly oriented .
@USER ah , you need to transpose the array ( my rate_df was different from yours ) .
You probably were working with my original data and that's why the transpose was needed .
However , if we call ` df.drop ( name , axis=1 )` , we actually drop a column , not a row : #CODE
Use ` axis=0 ` to apply a method down each column , or to the row labels ( the index ) .
Use ` axis=1 ` to apply a method across each row , or to the column labels .
) Second , I must say I'm confused where exactly I should insert my data frame variables into this code ?
Issue with reindexing a multiindex
I am struggling to reindex a multiindex .
But , there is still a problem : the reindexing works , but does not seem to preserve the order of the provided index in the case of a MultiIndex : #CODE
This is possibly a bug with ` reindex ` on a certain level ( I opened an issue to discuss this : #URL )
A solution for now to reindex your series , is to create a MultiIndex and reindex with that ( so not on a specified level , but with the full index , that does preserve the order ) .
Why do you try to replace the NaNs with None ?
First , I don't think you need to replace the ` NaN ` values with ` None ` , as ` NaN ` is the default indicator for missing values and will be ignored by ` mean ` by default in pandas ( ` mean ` has a ` skipna ` parameter that is True by default ) .
You're pretty close , basically you just want to ( 1 ) use diff instead of shift and ( 2 ) combine with groupby to automatically get NaNs .
Change some levels in a MultiIndex
To filter a dataframe you can use a mask : #CODE
My data frame and pivot table look like this : #CODE
Also is there a way to insert grouped records with the count info into a MySQL database ?
You'd have to substitute the ` NaN ` values with something that can be represented as an int like ` 0 ` , or convert to string and replace the ` nan ` string with an empty value and then export
However , you might want to consider expanding the sample data to better fit your situation .
I'm expanding the sample data here to hopefully make sure this is handling the situations you are dealing with : #CODE
You can call the ` str ` method and apply a slice , this will be much quicker than the other method as this is vectorised ( thanks @USER ): #CODE
Just out of interest how would I go about apply this to the index column ?
@USER not easily , the problem is that although ` pd.Series ( df.index ) .str [: 1 ]` itself works , how do you merge or add this back to the df ?
What I want to do is replace the values of a continuous variable that has been discretized by ` cut ` with the mean of another variable over those cut ranges .
Imagine the situation where I calculate mean bad rates for a continuous variable on a training data set , build a model and then have to apply that same transformation logic to new data .
In ` pandas ` for ` python ` there is a group-by-apply paradigm that uses ` transform ` to broadcast the aggregated values back to the same dimension as the input .
I should clarify ( and will do so in an edit ) that I need to be able to apply whatever transformation I devise to new data .
I am specifically looking for a ` data ` argument that is a Python list , not a dict , so this clause does not apply .
That's too specific for a generic map data structure , but it's fine to have it as a special extra case , and was implemented by subclassing ` dict ` and adding the extra behavior .
Then , by assumption , since the order matters , it can faithfully map backwards to an ordered list of lists used to construct it .
I tried to read some examples on forums and tutorials but i didn't achieve to apply it to my work .
but i can't achieve to apply them to my example
The critical values and p-values of Tukey-HSD would not apply in that case .
What would be possible in this case is to estimate the full model with OLS , define all desired pairwise contrasts , use the ` t_test ` to get the raw p-values for the comparisons , and then apply one of the multiple p-value corrections that are available .
Panda merge return empty dataframe
both dataframes have a value 1 to 16 on the avgSpeedBinID field , however , when i try to merge the data frames together #CODE
i just cast the str one into an int and the merge worked .
One way you could avoid alignment on column names would be to drop down to the underlying array via ` .values ` : #CODE
Pandas dataframe left merge without reindexing
Wondering if there's a more intuitive way to merge dataframes #CODE
Left merge does not work because it reindexes #CODE
It says let's join two tables on column A and also the index of the right table with nothing on the left table .
You could pass this as a freq , but this may / will be inaccurate for times which don't evenly divide : #CODE
In 0.15.0 there'll be a dt accessor : #CODE
Panda's boxplot but not showing the box
Note , ` showbox ` and ` whiskerprops ` are the ` kwds ` of boxplot , which are in turn passed to ` matplotlib.boxplot ` .
This can be done by lagging the columns in pandas with ` shift ` , or trying to do a complicated self-join that is likely very bug prone and creates the problem of perpetuating the particular date convention to every place downstream that uses data from that code .
another option is to resample and take the first entry : #CODE
I first thought I could do this with a ` join ` or ` concat ` operation , but I don't think that would work .
Also notice how the df2.index is in Freq : None , vs the df1.index is in F #URL
For example , you'd always expect a dataframe to have the DatetimeIndex as the index not the columns - doing the transpose forces people to rethink their mental model .
The problem with autocompleting chained methods is that you'd have to eval each method , determine the output , find its type , and then autocomplete the appropriate items .
Making no assumption about the indices and if they are aligned or not , you can align the rows first by using ` .reindex_axis ` and then use numpy broadcasting rules over ` .values ` , and finally re-construct the frame : #CODE
Applying a specific function to replace value of column based on criteria from another column in dataframe
The reason it overwrites is because the indexing on the left hand side is defaulting to the entire dataframe , if you apply the mask to the left hand also using ` loc ` then it only affects those rows where the condition is met : #CODE
The use of ` loc ` in the above is because say I used the same boolean mask semantics this may or may not work and will raised an error in the latest pandas versions : #CODE
I need to insert this values in MySQL using PHP .
Then take the left-hand side mask and sets them equal .
So align is the special sauce ...
You are looking for shared x- and y-axes which can be enabled by passing ` sharex=True ` and ` sharey=True ` as keyword arguments to ` hist ` .
as a side note , it appears there's currently ( Sept 2014 ) a bug in Pandas plotting where hist breaks with by if the number of plots does not fully fill a row .
hdfstore error on append with pandas
It's a rather large project , so I'm not sure what code I can offer , but this happens on the first append .
Not sure what to ptdump since I'm doing an append , but above lay the others !
Transpose it , sort it , transpose it back ?
Pandas concat ValueError : Buffer dtype mismatch , expected ' Python object ' but got ' long long '
when i try to concat the train dataframe with the label series based on pandas example it
full stack trace #CODE
How do you expect them to concatenate if the indexes are totally different ( one a MultiIndex and the other a default integer index ) ?
I thought if you named the columns and didn't specify join then it would just create a data like [ trainDataColumns , trainLabelColumn ] i tried ignore_index=True but that didn't change anything
Nope , ` concat ` merges on the index .
If you just want to merge them , you can reset the index of both ( ` reset_index `) and concatenate then .
Why don't you use append instead ?
Right now I construct the list of tuples , before passing it with ix .
What is ` len ( df1 )` , ` len ( df2 )` and what is the dtype of the ` time_0 ` column ?
If ` df1 ` and ` df2 ` share an index then you can create a boolean mask and then index ` df2 ` by it .
python : join group size to member rows in dataframe
Or should I learn how to merge the groupby object to the original dataframe with a join operation .
How to apply Pandas Groupby with multiple conditions for split and apply multiple calculations ?
I know how to do this manually , but I'm hoping for some help doing it with groupby or perhaps pivot table .
My only question is , what about the task makes it not doable with groupby or pivot ?
The pairing of the columns is not something that can be naturally expressed using groupby or pivot .
You then map plotting functions onto that grid , passing any required arguments to the mapped plotting functions .
I don't think you can do what you want in the way you suggest , because your series is not " the same size " as one level of the MultiIndex .
The normal display only * shows * you three values for that level , but each level of the MultiIndex is actually the same length as the whole DataFrame .
If you transform your index ' a ' back to a column , you can do it as follows : #CODE
I thought it would have been also possible to do ` df [ filt [ df.index.get_level_values ( ' a ')]]` without resetting the index , but this results in ` ValueError : cannot reindex from a duplicate axis ` , anyway to bypass that ?
If the boolean series is not aligned with the dataframe you want to index it with , you can first explicitely align it with ` align ` : #CODE
Note : the align didn't work with a Series , therefore the ` to_frame ` in the align call , and therefore the ` [ 0 ]` above to get back the series .
The solution in this case is to append the data to a Python list and create the DataFrame in one fell swoop at the end :
934 if text or len ( elem ):
I could possibly live without floats and use strings , but curiously the things in my Dataframe appear to BE strings , since when I try to apply the round() function on any value extracted from there , it will protest that the input is not a float ...
You should replace it with this : #CODE
Then I'm using pandas sql to make a left outer join between two pandas dataframe .
Turns out my problem was with the text in the dataframes that I was trying to merge .
Pandas : reindex and interpolate non-contiguous data
I want the dataset to insert and fill the missing rows with NaNs
Also the result of your concat is also strange , it's showing you having double the number of rows for ` 0 ` compared to the rest , I think you need to do some more debugging to see where you are going wrong , look at your code one operation at a time and print the shape and head at each stage until you are confident there are no errors
To explain : what I _thought_ I was doing at that point was splitting DET_SCN into two Series objects which I then named SITE_REF and ARM_REF and then I was concatinating those Series objects into the dataframe ( and then deleting DET_SCN after the reindex ) - I think the 0 results are because I copied cleanframe.head() and cleanframe.info() into the wrong place before I'd named the two Series objects :(
Hmm , now it's telling me it can't reindex on a duplicate axis ?
AttributeError : ' generator ' object has no attribute ' reindex '
I'm trying to reindex each dataframe in self.symbol_data to reflect the union of each dataframe's index , creating a generator object in the process .
transpose multiple columns Pandas dataframe
As someone who fancies himself as pretty handy with pandas , the ` pivot_table ` and ` melt ` functions are confusing to me .
I prefer to stick with a well-defined and unique index and use the ` stack ` and ` unstack ` methods of the dataframe itself .
If you really need the ` p ` values in a column , just add it there manually and reindex your columns : #CODE
@USER ` stack ` -> ` melt ` ; ` unstack ` -> ` pivot ` , but they don't aggregate the data in any way and only operate on the row and column labels ( indices )
I could join the limits and this df and filter with a boolean condition .
The Freq of df1 doesn't need to always be ' 240T '
df2 freq attribute is always going to be ' None '
With some help here I tried this approach , but only worked well when both df's freq was ' None ' .
Notice that this code uses a ` for-loop ` with ` len ( df2.index )` iterations #CODE
From there it is not hard to create a boolean mask of all the desired indices for rows in ` df1 ` you wish to select : #CODE
So this alternative method trades a ` for-loop ` with ` len ( df2.index )` iterations
The problem is the assumption of freq = ' D ' in df2 .
As I pointed out in the considerations , the dates will not be continuous , so df2 freq would be = ' None ' , unless there's a way to change that .
I don't think the code depends on the ` freq ` .
Then , use the ` pivot ` function to reshape the data .
I would like to apply dummy-coding contrasting on it so that I get : #CODE
You can use ` isin ` to filter for valid rows , and then use ` replace ` to replace the values : #CODE
If the resulting values are all integers , you change the above ` replace ` line to enforce the correct ` dtype ` : #CODE
You can tell ` aggregate ` to perform multiple actions on multiple columns .
@USER well , add more #URL pairs to the dictionary you're giving aggregate .
You can just use ` concat ` and pass param ` axis=1 ` , to append the arrays as columns : #CODE
The only thing I can think of is to use Python to export to csv files , and then write an Excel macro to merge all the CSVs into a single spreadsheet .
In addition - you don't need an excel macro to merge csvs into a spreadsheet , writing a script with xlwt is fairly straight forward .
Maybe I could use it to merge the CSVs into an Excel , but at this point I'm inclined to do that via a VBA macro invoked from Python , also because formatting the sheets will presumably be easier that way .
It's weird , but some of the drop down tool bar format options work and some don't .
If you want to aggregate any of the columns , you may consider indexing by the date , and then e.g. ` cumsum() ` , or ` pd.rolling_apply() ` to get a ' running total '
Not looking to aggregate the data .
If you set the ' CreditCardName ' as the index of the second df then you can just call ` map ` : #CODE
I also could write a different function to apply to the grouped object that returns both the max and the time where that max occurs ( at least in theory - haven't tried to do this , but I assume it's pretty straightforward ) .
You could consider using the transform function with groupby .
Then you could write a very simple transform function that will return for each group of data ( grouped by the sweep index ) , the row at which the minimum value of ' Primary ' is located .
Then to use this function simply call it inside the ` transform ` method : #CODE
I tried to do this with transform but it was just returning the entire dataframe .
I'm not sure why that should be , it does however work with ` apply ` : #CODE
I'm not totally sure why this function does not work with ` transform ` - perhaps someone will enlighten us .
your first solution ( the trans_function + transform ) works perfectly .
Will have to read about groupby transform some more to understand what's going on there . thanks !
In my opinion the ` transform ` and ` apply ` functions are very opaque and the docs are not a great help .
yeah just quickly reading through the docs on transform I'm still a bit confused .
That will give you a boolean mask .
You can then use that mask to filter your dataframe .
This is a simple merge and combine_first .
to keep original index , i have add right_index=True as merge parameters .
How to properly pivot or reshape a timeseries dataframe in Pandas ?
My approach was to create a multi-index ( Date - Time ) and then do a pivot table or some sort of reshape to achieve the desired df output .
And even if that worked well , I can't find how to do what I need with a multiindex df .
The parameter length is fit to the data I have so the hour part for the index matches for all columns when you reshape / pivot the df .
( The repeated hour makes ` unstack ` and ` pivot ` calls raise ` ValueError : Index contains duplicate entries , cannot reshape ` .
Now read in the DataFrame , and pivot on the correct columns : #CODE
is incorrect , as you have 18:00 : 00 twice for the same date , and in your initial data , they apply to different dates .
Then set your index using a multindex and unstack ...
it is probably better to do datetime conversion doing ` df [ ' t '] = pd.to_datetime ( df [ ' t '])` rather than call ` apply `
You need to extract prefixes and then do a left join .
Unfortunately , I can't reindex because the objects are not uniquely valued in my original data frame , i.e. there is more than one phrase in the 2nd dataframe that starts with ' One ' .
@USER if you have master branch of pandas left join on index with multiple matches is fixed through [ this pull request ] ( #URL ) .
if you do not have master branch specify ` how= ' outer '` in the last line , and then drop rows which are null in first column , or use ` pd.merge ` .
There are a number of ` MemoryError ` issues here on Stack , but the common theme I have seen is to make sure one uses a 64-bit machine and has plenty of RAM ( before getting into chunking and whatnot ) .
Also , you could transform all your arrays into float32 to save space .
How does pandas merge really sort its result when using its default , sort=False ?
I'm a bit confused how the default sort works when I run a merge / join with 2 pandas dataframes .
How to implement a function with non overlapping and rolling features simultaneously in Pandas / Numpy ?
To do this , I think I need to use merge , join , concatenate or boolean indexing or grouping in Pandas .
Can you elaborate a bit more on how you merge the coordinates ?
I merge them if they are continuing
Did you get stuck using merge and join ?
we need to write a apply function first and then use group by .
Pandas join on field with different names ?
According to this documentation I can only make a join between field having the same name .
Do you know if it's possible to join two DataFrames on a field having different names ?
I think what you want is possible using ` merge ` .
The pandas function ` append ` can be slow for larger dataframes .
Instead , I would recommend storing the ` newframes ` in a python list and than using the concat function which appends all the frames only once .
Your particular problem might be more cleanly ( and quickly ? ) solved with a cross-product solution ( #URL ) , but I'd have to think more about how something like this would apply .
Python Pandas If value in column B = equals [ X , Y , Z ] replace column A with " T "
If column B equals [ X , Y or Z ] replace column A with value " T "
Remember to use ` ix ` or ` loc ` to avoid setting values on a copied slice .
Use ` isin ` and ` loc ` to set the value : #CODE
@USER using loc on this toy example took 915 us , using where took 615us , so in this case where is 30% faster
@USER for a 50,000 row df loc is slightly faster 9.27ms vs 11.2 ms
Do I always have to use append for something like this ?
And the rhs is very problematic as you normally want to align , so its not obvious that you need to broadcast .
1 ) Readability , which can partially be achieved with eval , but I don't believe eval can be used over multiple lines ?
Say I want to merge two dataframes , df1 ( consistent of columns ' a ' , ' b ' , ' c ' , ' z ') and df2 consisting of columns ( ' a ' , ' b ' , ' d ' , ' y ') , together .
However , is there a less verbose way I could pass in this information through the merge function without having to put the column titles in a list format ?
You could also just merge the whole thing and drop columns ' y ' and ' z ' .
By default ` pd.merge ` will merge based on all columns shared in common .
In your case it is not working , it is preferable to use the newer forms of indexing such as ` loc ` , ` iloc ` and ` ix ` .
I think it is better to use loc : ` x.loc [ x [ ' Low '] == 0.0 , ' Low '] = np.mean ( avg_low )` I was told that you should use ` loc ` unless you cannot by Jeff
I have used ` pandas.groupby ` to group a pandas DataFrame on two columns and calculate average and median times .
At this point , I only need to drop the date portion and add millisecond precision to achieve my goal .
Pandas - replace all NaN values in DataFrame with empty python dict objects
I'd like to replace the NaN with an empty dict , to get this result : #CODE
Just use loc as described below .
This works using ` loc ` : #CODE
Using ` replace ` can solve that .
However , we can use ` replace ` !
I think split is a little more clear than regex but you can ` apply ` any function you choose to a series .
How to do resample of intraday timeseries data with dateOffset in Pandas / Numpy ?
This is what happens with a regular resample : #CODE
+1 for that resample trick , very nice !
When selecting the query indexes on the rows , but for each row you get every column . selecting a subset of columns does a reindex at the end .
chunk thru the table , see here and concat at the end - this will use constant memory
So to at least cut down on the number of calculations , you could do something like this - using itertools to only calculate the distance for pairs , and then using pandas to fill in the rest .
For example , if at day 4 07:30 I have missing data , I want to replace a ` NaN ` entry with the average of the measurements at 07:30 at day 2 , 3 , 5 and 6 .
Note that it is also possible that , for example , day 5 , 07:30 is also ` NaN ` -- in this case , this is should be excluded from the average that is to replace the missing measurement at day 4 ( should be possible with ` np.nanmean ` ? )
I am aware of that function ( and Pandas also has a built-in interpolate method ); sadly , this is not really what I am looking for / what I described is an established method when it comes to the data I have .
Perhaps resample() and then apply the above method ?
Good idea using resample , that should indeed work .
Missing data , insert rows in Pandas and fill with NAN
Here we set the index to column ` A ` but don't drop it and then reindex the df using the ` arange ` function .
Then reindex with a new index , here the missing data is filled in with nans .
You can pass additional args to ` np.std ` in the ` agg ` function : #CODE
I had tried " df.groupby ( ' A ') .agg ( np.std ( ddof=0 ))" , but I did not try adding the ddof in the agg parenthesis .
While in numba you need to ' manually ' align them .
Then we can apply ` df1 = df1 [ df1 [ 3 ] .isin ( df2 [ 0 ])]` .
But I also want to add a new column with the expanding mean for each side , like this : #CODE
What I want is something like excel's mean.if function , which computes the expanding mean for all values of the current side , applied to all previous rows .
what kind of window are you thinking of for the rolling mean ?
And why would the rolling mean be zero at the edges ?
I think you're actually looking for an expanding mean , not a rolling mean .
An expanding mean considers every previous value .
Now we want to groupby ` side ` and take the expanding mean : #CODE
Your result needs to be ` shift ` ed by 2 since you want the mean to include all previous ones , and not the current one ( make sure this is what you actually want , this seems a bit funny ) .
You can replace the ` shift ( 2 )` with ` len ( res.index.levels [ 1 ])` to make it a bit more general in case you have more than 2 sides .
Didn't know about expanding mean .
That is , the shift does not work since the differents side do not turn up in any particular order .
Google tells me that when I set my index , I should set ' drop ' to False .
Does it speed up lookups or does it add some semantic information useful for things like stack / unstack / pivot / groupby ?
I guess ' drop ' solves this , but the fact that this isn't default means I'm misunderstanding something here .
` df.loc [( ' MSFT ' , ' BATS ')]` , assuming that's the order of your MultiIndex .
Indices are used to select and align rows of a data frame .
You need the xs method to access the inner levels of a multiindex .
pandas dataframe time series drop duplicates
1 ) when indexing columns by a boolean mask where the mask only has one true value .
The idea is that doing an operation should not sometimes return a DataFrame and sometimes a Series based on features specific to the operands ( i.e. , how many columns they happen to have , how many values are True in your boolean mask ) .
I guess the lesson is to not generally assume numpy will translate a dataframe or series the way you think it will .
Get integer row index of MultiIndex Series
I have a pandas Series with a MultiIndex , and I want to get the integer row numbers that belong to one level of the MultiIndex .
If the index that you want to index by is the outermost one you can use ` loc ` #CODE
If it is just to compare the locatoins in each series you should place all your series in a list , and then concat them .
You can get all the values in one go by using ` loc `
Pandas to form clusters based on diff column
I want to drop all values after index ` 5 ` because it has no values , but not index ` 2 ` , ` 3 ` .
@USER : I changed the answer so as not to use ` argmax ` and ` df.loc ` .
The resulting series should have the same index as A but the values should be based on a rolling sum over a time window of the values in B , but in relation to the indices in A .
2 ways , define a func and call apply #CODE
another way which I think is better and will be much faster is we could add the ' class ' column to your existing df and use ` loc ` and then just take a view of the 2 columns of interest : #CODE
this evaluates whether the price is greater than percentil_75 , if so then class 4 otherwise it evaluates another conditiona and so on , may be worth timing this compared to loc but it is a lot less readable
@USER you can upvote too ;) , the thing to take from this is to avoid loops and using apply unless it is not possible , what you want to do is to find if you vectorise your operation , that is perform your operation on the entire dataframe or series rather than a row at a time .
Didn't test it though , ` loc ` is pretty fast .
How to iterate over pandas multiindex dataframe using index
drop the sep arg in your ` to_csv ` unless you think you need it
apply is just a loop and should be avoided where possible , your solution is fine , there are many ways of doing what you want .
You create a lookup function and call ` apply ` on your dataframe row-wise , this isn't very efficient for large dfs though #CODE
I don't know how optimized it is , but may be faster than the apply solution .
On the toy dataset apply takes 470us , lookup takes 531us
Hmm for some reason timeit gets a memory error when I try this on even a modest sized df of say 4000 rows , for 400 rows I get 8.17ms using apply and 3.05ms using lookup , so I expect lookup to scale better
A simple inner merge would work : #CODE
pandas rolling window regression ols precision
I see that pandas rolling window multiple regression does not give very precise results .
Also , make sure that regular OLS gives the same answers before moving on to rolling .
regular ( non rolling ) OLS in pandas gives the exact same results as Excel .
Rolling give approximately the same result ( generally close , but not always ) .
Actually I know rolling windows OLS works exactly sometimes .
I got rolling window OLS to match exactly with Excel for a simple regression .
You can create a function that determines if the value column ends in `' _regen '` and then apply it your values : #CODE
Yes but how to you actually pass the arguments within the apply function .
Within the apply function arguments are passed with the " args= " parameter , something like this a.loc [ a [ ' value '] .apply ( func=has_substring args= " regen " , ' key '] += ' _regen ' How ever I can't get this to work .
What you have will work , args just needs to be an array within the apply function : ` a.loc [ a [ ' value '] .apply ( has_substring , args =[ ' regen ']) , ' key '] += ' _regen '`
There is a built in method ` rolling_sum ` , I also call ` shift ` at the end to align it the way you want : #CODE
Should I reindex the table ?
Minor : ` notnull ` is also a method of DataFrames .
However , renaming the columns as you suggest at the end will replace the column names , meaning I lose the first row of data .
Is there a more pythonic way to insert a row into a data frame ?
Use the ` loc ` attribute to assign data .
Still hoping there's a built-in way to insert a row in a desired location .
I don't think I'm being clear on what i mean by ' insert ' row .
I don't mean append a row .
I mean insert a row at a certain location .
So , if i wanted to insert row ' e ' between rows ' b ' and ' c ' , I would first df.loc [ ' e ' , ' E '] then df.reindex ( ' a b e c d ' .split() )
The parameter ` drop=True ` forces it to drop the old index .
One way would be to use ` str ` methods to get the numbers , make a new dataframe from that , and then join ( or concatentate ) the results .
You can use ` apply ` to check whether each row satisfy the condition , and use the resulting boolean Series to do the slicing : #CODE
` apply ` does the trick .
It looks like the calculation performs a full outer join on the two columns using the indices , which is somewhat unexpected .
It looks like the calculation performs a full outer join on the two columns using the indices , but when I tried to reproduce this behaviour using toy problems I never got similar result .
I would like to reformat this to a datatime object so that I can ` resample ` the data into half hourly bins .
Just for interest , here is a timedelta resample in action : #CODE
Pandas - resample a DataFrame by half-hourly frequency
I'd like to resample this into half hourly sets .
But if this is an issue , you can store it in a separate table and do SQL join .
( Did you mean you only store the outcome once , at the end of the timeseries ? ) Anyway if this is an issue , you can store the outcome in a separate table and do SQL join .
Or , you could backfill the outcome column everywhere except the last row with NAs after padding it .
how about concat ?
This is what I'd like to achieve , but with ` series = pd.concat ([ series1 , series2 , series3 ])` you run into the same problem I had initially : the ` ValueError : cannot reindex from a duplicate axis ` ( it's just not exposed in this example ) .
you should be able to create a dataframe from the concatenated series and merge this back to your original dataframe
I have tried something like pivot table or unstack in pandas , but it is not working in this case .
I drop the duplicates in index and now it works !
The desired element dataframe ( i can just drop the n1 , n2 ... columns once i have the data ) #CODE
I'm not looking to drop entries where the " n " field is blank .
Rather than looping and merging , I think a better approach would be to reshape df1 to ' long ' format , using ` melt ` .
Then , from there , you can join against your other frame and summarize however you want .
I would like to compute the rolling max and min on these values grouping by symbol .
For rolling max #CODE
However , when I do the samething using rolling min #CODE
How can I append the CSVs without column names .
My only solution to this has been to write an Excel macro to strip out characters not usually liked by databases when importing - and then import the file using python .
But resample is an issue .
If somebody wish to know , it could be done with the function ` pivot ` #CODE
How to merge / join / concat dataframes on index in pandas
I would like to merge / join / concat df2 and df3 on certain index positions of df1 : #CODE
However , concat ([ df1 , df2 , df3 ] , axis=0 , keys .... ) puts the dfs consecutively together ...
You won't be able to achieve what you want using ` concat ` like this or merge for that matter without reindexing .
By default for ` concat ` the original index values will be used and will just stack the dfs after each other .
If you tried to merge using the indices then they will clash and create additonal columns e.g. ' X_x , ' X_y ' etc ..
The easiest way is to reindex the dfs and then call update to overwrite the NaNs at the desired rows : #CODE
1st problem : I couldnt figure out how to use ' apply ' or something instead of the for loop .
But ' join ' appears to add some rows somewhere inbetween ( maybe due to duplicate indices ?? ) .
Pandas sorting by group aggregate
I use ` np.vectorize ` to apply this function on a ` DataFrame ` - ` dataFrame ` - that has about 22 million rows .
plz show the related code where you apply ` getData ` over the data-frame
it looks like you need to do a simple merge .
Warning : In the current implementation apply calls func twice on the
You can also use this when you have multiple columns you want to aggregate .
Pandas : fillna() method with multiindex - NaNs are filled with wrong columns
I can reproduce it with a much simpler example , not using DateTimeIndex or MultiIndex .
Its actually quite complicated because you need to align the blocks ( dtypes ) of the 2 frames ; certainly not tested .
I wasn't sure if I was using this method wrong but an obvious workaround would be to iterate through each column and apply fillna() with axis=0 .
Ask you to insert the name of the Excel file to be read
` test.exe ` does not run even when I insert ` from pandas import read_excel ` in ` test.py ` but I do not ask for any ` input ` file
I have python v2.7.5 ( via activestate ) , pandas 0.14.1 ( via scipy stack 14.8.27 )
How to transpose ( stack ) arbitrary columns in Dataframe ?
Now I want to transpose ( stack ) columns ' 2010 ' , ' 2011 ' and ' 2012 ' into rows to be able to get : #CODE
By using ` df.stack() ` pandas " stacks " all columns into rows , while I want to stack just those pointed .
So my qestion is how to transpose arbitrary columns to rows in pandas Dataframe ?
Alternatively , you could probably ` melt ` into a flat format , pick out the largest two values in each Date group , and then turn it again .
Pandas : join a row of data on specific index number
I want to join df2 on df so it looks like this : #CODE
It already does do a join with a specific index , it just so happens to be that your index in ` df2 ` is 0 and so when it joins it places the `' a ' , ' b ' , ' c '` in index 0 .
You have to make sure that the row that you want to join has the same index as where you want it to eventually be , yes .
Use ` np.vstack ` to stack a list / array / whatever of rows : #CODE
we can use boolean masking to select the values of interest , then ` concat ` them passing ` axis=1 ` , you can then just rename by directly assigning to the ` columns ` attribute : #CODE
Of you can ` join ` and set the desired suffixes ( thanks to @USER ): #CODE
Another way is to ` merge ` ( which is what ` join ` uses underneath ): #CODE
Or ` join ` instead of ` concat ` : ` df [ df > =0 ] .join ( df [ df < 0 ] , lsuffix= " A " , rsuffix= " B ")` .
If I replace all the slashes with ' over ' then I get the following : #CODE
How can I add summary rows to a pandas DataFrame calculated on multiple columns by agg functions like mean , median , etc
After doing some graphing with the data in this long format , I pivot it to a wide summary table format with columns for each ID .
However , I can't find a concise way to calculate and add some summary rows to the wide format data with mean , median , and maybe some custom aggregation function applied to each of the ID-based columns .
I tried things like calling ` mean ` or ` median ` on the summary table , but I end up with a Series rather than a row I can concatenate to the summary table .
Join two DataFrames on one key column / ERROR : ' columns overlap but no suffix specified '
I think you want to do a merge rather than a join : #CODE
It's ignoring the on kwarg and just doing the join .
For some reason I thought ' merge ' requires two dataframes to have the same length .
You could use nested ` concat ` operations , the inner one will concatenate your last row 3 times and we then concatenate this with your orig df : #CODE
the name is a bit irrelevant , you can just append the resultant df to a list or dict , the name is just a reference to an object , why does this matter .
let data1 [ data1.delta1 > 0 ] .column , data1 [ data1.delta1 == 0 ] .column to be appended to a list or dict outside the loop and then find intersection later ?
Now concat them : #CODE
What's the easiest way to append to the same data frame the result of dividing Row1 by Row2 ?
in addition , if you want to append multiple rows , use the ` pd.DataFrame.append ` function .
DAMMIT , forgot the ` loc ` . well , it's fixed now . the main point was to show how to access rows by number .
( And then have to do some sort of join between files ? )
I'm just learning Pandas and I hadn't stumbled across ` transform ` yet .
Append Two Dataframes Together ( Pandas , Python3 )
I am trying to append / join ( ? ) two different dataframes together that don't share any overlapping data .
I am trying to append these together using #CODE
EDIT : in regards to Edchum's answers , I have tried merge and join but each create somewhat strange tables .
OK , what you have to do is reindex or reset the index so they align
Use ` concat ` and pass param ` axis=1 ` : #CODE
` join ` also works : #CODE
As does ` merge ` : #CODE
In the case where the indices do not align where for example your first df has index ` [ 0 , 1 , 2 , 3 ]` and your second df has index ` [ 0 , 2 ]` this will mean that the above operations will naturally align against the first df's index resulting in a ` NaN ` row for index row ` 1 ` .
To fix this you can reindex the second df either by calling ` reset_index() ` or assign directly like so : ` df2.index =[ 0 , 1 ]` .
Arithmetic operations align on both row and column labels .
To count nonzero values , just do ` ( column ! =0 ) .sum() ` , where ` column ` is the data you want to do it for .
@USER : You could get ` inf ` if there were no nonzero tags ( so that the count of nonzero tags was zero ) .
This is tricky , I can't think of way to do this without some iteration , you could get the indices of these markers using something like ` mask = df.loc [( df [ ' A '] .shift() == 1 ) | ( df [ ' A '] == -1 )]` then collapse this again using ` mask.loc [( mask [ ' A '] == -1 ) | ( mask [ ' A '] .shift ( -1 ) ! = -1 )]` which should then display the start and end indices and then iterate over or pull the indices into a list of tuple pairs where the pair has beg , end and set these to 1 .
Anyway I want to ask , does somebody know about probably some specific method in Pandas , which could replace ALL strings in some column with the corresponding (= different ) integer ( or float ) values ..?
However , it is still not working as checking the len ( forward [ ' OPR_DATE ']) doesn't show me a reduced column at all .
len ( forward [ ' OPR_DATE ']) = 116560
len ( cash [ ' opr_date ']) = 7781
len ( forward ) = 98012
len ( cash ) = 7781
You can do an inner join operation .
I tried to apply this solution to my larger data frame and at first glance it worked wonders , but now taking a closer look there are several instances of " looped phrases " that slipped through the cracks .
Following up with the groupby and transform : #CODE
Once the values are ordered , then you could apply ` groupby-transform-max ` as usual : #CODE
It's not the most performant , but there's also ` df [ " Value "] .groupby ( map ( frozenset , df [[ " A " , " B "]] .values ) , sort=False ) .transform ( max )` .
Cleanest way to perform pandas join involving an index and also columns
Suppose I want to merge ( concat ? ) two pandas tables by joining on both an unnamed index and a column ( here " identifier ") .
a bad cut and paste !
I first tried using apply but it's not possible to return multiple Series as far as I know . iterrows seems to be the trick .
You can then stack this DataFrame to obtain : #CODE
where repositoryid and date are the levels 0 and 1 of a multiindex , respectively , and userid and watchers are the data columns .
Now I want to drop all rows where date > creationdate+timewindow , where timewindow is some constant .
I thought boolean masking would be the best solution , but I was not able to make it work with the multiindex .
So , my question is , 1a ) how do I drop rows quickly from a multi-indexed data frame conditional on the level 1 index , or equivalently 1b ) how do I insert single-indexed data frames into multi-indexed data frames , and 2 ) is this even the best=fastest way to solve my problem , or should I try a completely different approach ?
( I also tried to not use indices , but that was too slow . I played around with join , merge , groupby , etc , unfortunately I did not manage to get them to solve my issue . I spent the last 5 days studying the excellent book " Python for Data Analysis " and trying to find solutions online to this problem , again without success . I hope that maybe an advanced pandas user has an elegant solution to this seemingly simple issue ? Thanks a lot in advance ! )
Alternatively , instead of using ` mask ` and ` df.loc ` , you could use ` query ` : #CODE
no need for apply ; you can directly subtract a Timestamp from a column to yield a timedelta64 dtype
Is loc or xs faster in pandas ?
I have a DataFrame of ( x , y ) coordinates that I would like to transform into array's to perform pairwise distance calculations on .
Desired output - A new DataFrame of grouped / aggregated coordinates in an array so that I can apply a fuction to each array : #CODE
Distance calculation I wish to apply ...
groupby and aggregate into a list of lists #CODE
note that to apply your distance function you have to do : #CODE
Then you could join the two dataframes : #CODE
You have to understand the XML structure and know how you want to map its data onto a 2D table .
This code it work to transform to df this type of Excel XML file : #CODE
I want to compute the mean of each row and subtract it from each value in that specific row to normalize the data , and make a new dataframe of that dataset .
I know the issue is with ` df.index.time ` since if I replace this by ` range ( 0 , len ( df.Volume ))` , I get the plot .
I also tried to replace ` df.index.time ` with a new variable ` time ` where #CODE
To count the number of rows in a DataFrame , you can just use the built-in function ` len ` .
it's probably better to replace the ` ~ pd.isnull ` wit ` pd.notnull ` for readability
One method could be to apply a lambda to the column and use the boolean index returned this to index against : #CODE
FYI - no need to use apply here ( tz_localize / convert are methods on index and series )
I opened this question separately : [ Unable to apply methods on timestamps in Pandas using Series built-ins ] ( #URL )
Unable to apply methods on timestamps using Series built-ins
but I don't know how to append ` age ` to ` new_df ` , hope anyone could give me some advice .
And I would like to transform it to the following DataFrame : #CODE
If a sequence is given , a MultiIndex
I pivot this dataframe .
We can use this operation to mask our dataframe , just in the same way as we did with the color of our minions .
You can of course do any kind of operation you want in a mask #CODE
As pointed out in the comment below by @USER , there may be a bug in pandas when using ` pivot ` to do this with a DataFrame that has duplicate entries in the index .
This is also problematic because in this case , using ` set_index ` followed by ` unstack ` will be very tedious , since one of the would-be index levels is already the index , and the other is not .
We want to " append " ` name ` into the index , without needing to first pop the unnamed , existing index out of there , which can be done but leads to annoying , unreadable syntax .
Original answer : Use the facilities provided in the ` pandas.DataFrame ` ` pivot ` function , to pivot on the column that you want to serve as the categories .
If you are having trouble with errors related to the index , try adding the index as part of the pivot : #CODE
In some of the documentation and associated other SO questions , it looks like some of the pandas devs try to deflect this by saying that ` pivot ` is a " reshape " operation , as opposed to something that alters the index .
Python : weighted median algorithm with pandas
I want to calculate the weighted median of the column ` impwealth ` using the frequency weights in ` indweight ` .
What is the best way to go about finding weighted median ?
So it makes sense that the mean and median of ` indweight ` exceed its sum .
I had never used it before , but it has a weighted median function that seems to give at least a reasonable answer ( you'll probably want to double check that it's using the approach you expect ) .
the weighted median function is very similar to the accepted answer if you look at the code but doesn't interpolate at the end .
In short : I used a pivot table to transpose my matrix and then I was able to plot the single cols automaticly , at example 2 here .
With some help from stackoverflow and the pandas documentation ( thanks ! ) I figured out how to pivot the data : #CODE
The ` pivot ` is making a DataFrame whose columns have a MultiIndex .
Since the top level , ` value ` , is the same for all columns , you could simply drop it : #CODE
Or , better yet , specify the ` values ` column when calling ` pivot ` : #CODE
I wonder if there isn't a way to use to_json directly instead of pivot ?
The pivot changes column values into column labels .
None of the ` orient ` options ( split , records , index , columns , values ) does that , so no I think you need to call pivot .
What I would like to do is merge these two DF together and align them as so : #CODE
How do I align the data to fit with my desired output ?
Merge on both ` Words ` and ` Type ` : #CODE
My code will apply the norm.ppf for every element of lts .
You're not using Pandas for any purpose other than Series being a container , so it will be faster to cut Pandas out of this calculation .
Then this will be an ` apply ` since you'll consider things rowwise .
So you can drop those rows that match your criteria with #CODE
What I am trying to do is collecting whole rows from these locations and append them to a new dataframe , the index value being ` i ` in the script given above .
How can I get the entire row ( since I know the index through ` argmax() `) and append it to a new dataframe ?
Inserting NaNs when there is no data is accomplished by making the ` index ` column the index and then calling ` reindex ` .
You can vectorize ' Words1 ' into a series and then apply a regex : #CODE
It fails while evaluating your mask #CODE
This creates a mask by comparing each element in the list to [ 0 , 0 ] instead of comparing the list df [ ' list '] to [ 0 , 0 ]
use ` loc ` : ` df.loc [ df [ ' hello '] .str .contains ( pattern )= =False , ' col '] = newVal `
` loc ` uses label based indexing see the docs : #URL
You can replace your wacky ` elif ` construction with a simple ` else ` in this case .
We can use ` shift ` to compare if the difference between rows is larger than 1 and then construct a list of tuple pairs of the required indices : #CODE
I use numpy's searchsorted to find the index value ( integer based ) where we can insert our value and then subtract 1 from this to get the previous row's index label value #CODE
Now you can build a mask containing only the dates you are interested in : #CODE
You can resample to a particular day of the week , using a built-in pandas offset #CODE
Then just join all your entries in the fruit column with a comma , eliminate extra spaces , and split again to have a list with all your fruits .
I believe you want to pivot this using ` pd.pivot_table ` .
See the examples on pivot tables to understand better how this works .
The second part of my code is function I am trying to apply #CODE
So problem solved , second problem springs up -- the changes being made don't " stack " within the for loop .
Python Pandas replace returns strange results
if you are just wanting to replace the existing values or add them y performing a lookup then you can do ` df [ ' new '] = df.map ( from_to )` , however you need to post more code , input data , existing code and desired output
I use replace instead of map because there are more than on columns I need to convert .
As you can see , the rolling mean now gives a slightly different number .
In the past , the rolling mean computation suffered from a similar issue , but it seems that internal improvements in its algorithm over the past few versions have brought it to a more precise result .
First of all , let's establish that the new rolling mean result is more precise .
Using the greater ` np.float128 ` precision results in a value closer to that of the new rolling mean version .
This clearly proves that the new rolling mean version is more precise than the previous one .
This improves the precision of the sum divide approach , but doesn't affect that of the rolling mean approach : #CODE
Inconsistent behavior of ix selection with duplicate indices
Further , I need to build my code around this issue , so my question is , how do I detect what kind of object is returned by an ix selection ?
Currently I am checking for the returned object's len .
( Note that it's usually a better idea to use ` loc ` ( or ` iloc `) rather than ` ix ` because it's less magical , although that isn't what's causing your issue here . )
I want to replace all the scores with either ' very low ' , ' low ' , ' medium ' , or ' high ' based on whether they fall between quartile ranges .
Finally you are performing chain indexing which may or may not work and will raise a warning , to set your column value use ` loc ` like this : #CODE
it's being caused by the first replace line and the inclusion of the values ' Very Low ' in the final df ... just not sure how to get around this
Take a look at this stack overflow example .
How do I subtract ( and replace with result ) A B and C in row one with the average in row 1 ?
That's all well and good but I was wondering , is there a pandas function to stack them vertically into TWO columns and change the headers ?
After loading the dataframe , I insert a Year column in position 0
The code below defines a ` colors ` dictionary to map your diamond colors to the plotting colors .
To select a color I've created a ` colors ` dictionary which can map the diamond color ( for instance ` D `) to a real color ( for instance ` red `) .
@USER Ok I see :) I've edited it again and added a very simple example which uses a dictionary to map the colors , similarly to the ` groupby ` example .
I want to merge the following pandas Datadrame : first_df and second_df #CODE
To merge it I'm using : #CODE
the default merge type is ' inner ' this means where in your case the index values are present in both dfs , if you wanted the union of all index values then you could pass param ` how= ' outer '` , or ` left ` or ` right ` see the docs : #URL
I had to change ` pivot ` to ` pivot_table ` with the entire dataset but you certainly pointed out the right direction .
We then simply resample it using ` pivoted.resample ( ' W ')` .
Assuming you have a series ` s ` , with a MultiIndex andthe two levels you have shown , you can groupby the first level and apply the `' first '` / `' last '` aggregations to get the values you want .
You could apply your function row-wise which would look something like ` df [ ' ls '] = df.apply ( lambda row : checker ( x.Review ) , axis=1 )` but ideally you want to vectorise your function so that it can be done on the whole column , at the moment your function looks incomplete so it's hard to suggest what improvements can be made
I aggregate my Pandas dataframe : ` data ` .
The easiest way would be to specific a single column of the groupby ( doesn't matter which one ) , and use ` transform ` instead of ` apply ` , like this .
The reason this didn't work while your first did is that your function returns a single value , rather than an array of values , so ` transform ` broadcasts back to the original frame's shape , while ` apply ` is more flexible and generally passes back whatever shape your function returns .
For example , if you want columns where all values are above 100 , your mask would be : #CODE
If you just want the column headings , you can apply the mask to itself .
Originally , I used append api to create a single table ' impression ' , however that was taking 80sec per dataframe and given that I have almost 200 of files to be processed , the ' append ' appeared to be too slow .
Also , why is append so much slower than put ?
Would I have to apply some sort of function on each data object ?
Then with that , you can aggregate by office and use a transform ( which returns like indexed data ) to calculate a percent of office .
Use a boolean mask to get the rows where the value is equal to the random variable .
Then use that mask to index the dataframe or series .
You can sort by the date and then drop the duplicates , keeping the last ones like this : #CODE
But it may be preferable to set the date as the index , if we can drop the old indexes , and then just sort by the index : #CODE
I tried to aggregate by gene but I got an error , possibly due to the NaN .
If possible I would like to keep the output as a pandas data frame since I will have to merge this to another df in the future #CODE
got it to work by changing ` if ser.empty : ` into ` if len ( ser )= =0 : ` Thanks !
You don't necessarily need to drop the columns , just select the columns of interest : #CODE
pandas dataframe replace full word
The above replace results in : #CODE
I could loop through the list and do a word replace , but I have a very large dataset and looping may not be efficient .
I use it very often and its performance is still ok even at DataFrames that are in len > = 10^6
Reshape / merge columns in Pandas dataframe after using pivot_table
The following pivot function is working but I need to reformat results .
Perform a ' left ' ` merge ` in your case on column ' B ' : #CODE
Another method would be to set ' B ' on your second df as the index and then call ` map ` : #CODE
We can perform a groupby on ' A ' and then apply a function ( lambda in this case ) where we join the desired delimiter ` ; ` with a list comprehension of the B values .
In Python you can join things by using ` some_delimiter.join ( things_you_want_to_join )` , e.g. `' , ' .join ( " abc ") == ' a , b , c '` .
We can apply that to the ` B ` column after grouping on ` A ` : #CODE
And finally we ` join ` : #CODE
I cannot find a way to lookup a multiindex in Pandas 0.14 .
So , is there some kind of step I'm missing here when setting a multiindex ?
Also , has this always been to way to lookup a multiindex ?
I don't want to cast to int and then mod / truncate ( because , in the case of zip_code and theoretically tax_id too , ' 07443 ' will get converted to 7443 which isn't good ) .
I just want to clip the ' .0 ' and have to_excel() treat the whole column as strings ( unicodes , more specifically ) .
I wouldn't worry about it , it seems ok but I think you have to show what you've tried at the very least plus any code and approaches , it would help also to show what you've tried that works on simple data i.e. not in a pandas dataframe and are asking how to apply that to pandas would probably help .
Perhaps you could try using fuzzywuzzy or similar to get a ratio of how similar the phrases are then join of that .
since , per the docs , " [ a ] dditional keyword arguments [ to ` apply `] will be passed as keywords to the function " .
hmmm this gets me sort of the way here but not quite -- how can I apply that if I need to actually CALL the dataframe in the arguement ?
I am trying to resample a pandas data frame with a timestamp index to an hourly occurrence .
However the built in functions of time series resampling do not include mode as one of the default methods to resample ( as it does ' mean ' and ' count ') .
How can I join this data and append corresponding low frequency data columns to the higher frequency data if it falls on that day ?
One way would be to create a custom apply function and check each datum's YMD and look up the corresponding low frequency data , but that seems pretty inefficient .
Perhaps you are looking for resample ?
Note : you can do this super cleanly with a merge ( assuming no overlapping columns ): #CODE
is to do this with a reindex :
Now reindex on the start of the day ( with normalize , in 0.15 you'll be able to use ` .dt .normalize() `) : #CODE
It seems that normalize doesn't take any parameters , for example , if my low frequency data was on a monthly basis , or bi-weekly .
That is basically matrix multiplication of ` X '` and ` X ` where ` X '` is transpose of ` X ` : #CODE
It does what you describe , go row-wise ( so apply over ` axis=1 `) along ` df ` and use the entries as index for selecting in ` P ` .
Python Pandas : replace given character if found in column label
I would simply like to replace " " with " and " where found , either at the column labels or index labels .
What you tried won't work as ` Index ` has no such attribute , however what we can do is convert the column into a Series and then use ` str ` and ` replace ` to do what you want , you should be able to do the analagous operation on the index too : #CODE
I just wonder if it is a good practice to physically rename and replace / select raw data and create new files from it .
You should switch to using pandas built in group by and then merge it with your original frame .
I think the normal operation would be to do a ` transform ` on a groupby in order to return an object with it's indices aligned with the original df : #URL
interpolate values between sample years with Pandas
If you're using a newer version of Pandas , your DataFrame object should have an interpolate method that can be used to fill in the gaps .
Parallelize apply after pandas groupby
I have used rosetta.parallel.pandas_easy to parallelize apply after group by , for example : #CODE
I break my dataframe into chunks , put each chunk into the element of a list , and then use ipython's parallel bits to do a parallel apply on the list of dataframes .
Then I put the list back together using pandas ` concat ` function .
It works for me because the function I want to apply to each chunk of the dataframe takes about a minute .
write a silly function to apply to our data #CODE
then it only takes a few ms to merge them back into one dataframe #CODE
Can you explain to me why apply does not automatically parallelize operations ?
It seems like the whole benefit of having the apply function is to avoid looping , but if it is not doing that with these groups , what gives ?
Keep in mind that apply is usually syntactic sugar and underneath it's doing the implied loop .
By doing small modification to the function it can be made to return the hierarchical index that the regular apply returns :
By the way : this can not replace any groupby.apply() , but it will cover the typical cases : e.g. it should cover cases 2 and 3 in the documentation , while you should obtain the behaviour of case 1 by giving the argument ` axis=1 ` to the final ` pandas.concat() ` call .
Do the rolling and window functions work ?
` print ( Boston1.DESCR )` gives the slightly mysterious comment , " Median Value ( attribute 14 ) is usually the target " .
Yes with the resample , there is no more line between two points .
And the first append is like :
Pandas : Quickly joining on a MultiIndex or reindexing
I thought the issue might be in using ` join ` , so I tried another route .
It is indeed faster to use ` reindex ` instead of ` join ` , but now ` MultiIndex ` is even slower , comparatively !
and then use ` df.loc ` to reindex the DataFrame with ` class_list ` : #CODE
pandas : how to apply scipy.stats test on a groupby object ?
I did this using the loc index in pandas : #CODE
How can I truncate a table using pandas ?
However , before entering the above code I want to truncate ( or even drop ) the table .
This code does drop the table but then generates exception from sqlalchemy as it tries to re-create an empty table .
I can catch and ignore it , and the next call to_sql() with if_exists= ' append ' will create the table correctly , but this is pretty ugly .
This almost works : it truncates the table but then insert a single record with all fields as NULL ...
Thanks @USER , the 2nd case I tested by ` df1 = df.drop ( df.index )` actually create an empty DataFrame with the correct columns but as I wrote it truncate but insert a single record of NULL in all fields .
The question is whether it's possible to truncate it using pandas api
i was able to accomplish this doing something similar , but instead of transform , i just used groupby ( .. ) .sum() .
Is there any benefits to using transform ?
When i try to insert this into the same data frame , the values are all NAN .
@USER transform spreads the result across the group , useful if you are using it later .
The inserting with NaNs is exactly that - that's why transform is needed ( it matches the original index ) .
I feel the following is an elegant solution to this problem from :( Pandas DataFrame aggregate function using multiple columns ) #CODE
How to use groupby to apply multiple functions to multiple columns in Pandas ?
Since you are aggregating each grouped column into one value , you can use ` agg ` instead of ` apply ` .
The ` agg ` method can take a list of functions as input .
While I am still getting acquainted with pandas I am trying to understand how to fill in gaps in time series when using a DataFrame with MultiIndex containing PeriodIndex .
So the question is if this this actually an expected behavior of the ` reset_index ` applied to ` PeriodIndex ` as part of ` MultiIndex ` or is there something in the code that needs to be fixed ?
Look at [ ` shift `] ( #URL )
@USER I don't understand how shift will help here .
shift allows you to compare whole arrays against a shifted row , what you are doing could be done very quickly using numpy or pandas , for example you could take a slice of your data ( imported into numpy or pandas ) and then filter them using your criteria as all you are doing is looking for matches between 3 consecutive rows
If you do ' for idx , i in enumerate ( big [: 500000 ]): for jdx , j in enumerate ( **big [ idx : ** ]): ' you will cut the number of iterations .
@USER I've already cut the initial table and dropped all unnecessary rows and columns .
One option is to aggregate the grouped ` B ` column using ` pd.unique ` and the builtin Python ` len ` function : #CODE
There is a built in ` unique ` attribute so it could be re-written like this : ` df.groupby ( ' A ') [ ' B '] .unique() .apply ( lambda x : len ( x ))`
ok , I can combine the function ` value_counts ` and ` groupby ` through ` apply ` function .
There is probably some battle / tradeoff between the vectorised ` value_counts ` and calling ` apply ` , there maybe a better way but I've not figured out a better way yet
1 ) does multiindex really have the potential to speed this process up like I think ?,
Then , you can use the ` pivot_table ` function to transform it into your desired output : #CODE
Furthermore , I have a problem by selecting the x-range of the zoomed-in window since I do not know how to properly transform the dates to the internal integer representation of the matplotlib .
Panda's DataFrame dup each row , apply changes to the duplicate and combine back into a dataframe
I need to create a duplicate for each row in a dataframe , apply some basic operations to the duplicate row and then combine these dupped rows along with the originals back into a dataframe .
I'm trying to use apply for it and the print shows that it's working correctly but when I return these 2 rows from the function and the dataframe is assembled I get an error message " cannot copy sequence with size 7 to array axis with dimension 2 " .
Any insight on how I can achieve it within apply ( and not by iterating over every row in a loop ) ?
The ` apply ` function of pandas operates along an axis .
I am not sure if would append a column if one didn't exist but you can give this a try : #CODE
Pandas concat gives error ValueError : Plan shapes are not aligned
My understanding of concat is that it will join where columns are the same , but for those that it can't find it will fill with NA .
Heres the concat statement #CODE
I recently had the same error , it turns out that I had a duplicate column name in the dataframe ` df_e ` when joining using an append statement ` df =d f_t.append ( df_e )` .
I don't know whether this answer would have solved the OP's problem ( since they didn't post enough information ) , but for me , this was caused when I tried to ` concat ` dataframe ` df1 ` with columns ` [ ' A ' , ' B ' , ' B ' ' C ']` ( see the duplicate column headings ? ) with dataframe ` df2 ` with columns ` [ ' A ' , ' B ']` .
Change ` df1 ` to ` [ ' A ' , ' B ' , ' C ']` ( i.e. drop one of the duplicate columns ) and everything works fine .
merging an empty pandas Series with inner join does not give null result
When I use the pandas concat function , it mostly gives me results I would expect : #CODE
But when I merge two series , one of which is empty , I do not get a length-zero result : #CODE
Also , I wonder if merge might be what you are looking for .
But again , it isn't real clear to me what the point would be of intentionally using concat on an empty series or dataset in the first place .
I'm creating a dataframe from the intersection of two Series .
I could write a handler outside that just tests if either one is empty before trying to join them .
As far as merge , it seems like that only handles dataframes , not series .
Plus if I have two dataframes , concat works as planned .
pandas rolling computation with window based on values instead of counts
But I don't understand how to apply it to my case .
( using apply , map or rolling ? )
What you needed to do from the answer you linked to was to turn the index into a series so you can then call apply on it .
Let's say that I have the following pivot table ( this is the one created in the documentation ): #CODE
I have 40+ text files that I want to combine together ( in a ' wide ' csv , as opposed to a ' tall ' csv . That is , I don't want to append the files ) and produce a new csv .
Using Pandas ( merge ) I am able to achieve what I want , but I presume there is a simpler way .
You can apply ` merge ` to a list of DataFrames using reduce : #CODE
To query the results of your ` groupby ` operation , you can use ` loc ` .
This code should resample your data to 2.5s intervals #CODE
The reason the first element in your return array is 2.67 is that you're using the ` last ` method for ` how ` to resample your data .
now , ` concat ` with the original frame , ` sort_index ` and find cumulative sum : #CODE
` stats = users.groupby ( ' lang ') .agg ( len )`
slightly more readable to use ` notnull ` so ` data_df [ data_df.c.notnull() ]`
I was going down the third path and getting tangled , but I had no idea there was an existing metaphor for this in the whole pivot area .
So my idea was , I will create an excel-template with these tricky parts and then from python just insert dynamic data to this template .
Can one perform a left join in pandas that selects only the first match on the right ?
Can one perform a left join in pandas that selects only the first match on the right ?
This would straightforward to add as an option to `` merge `` , but is not implemented at the moment .
To reshape this to the expected output , we can now use ` unstack ` to move the ' year ' index level from the rows to the columns ( and the use ` fillna ` to get 0's ): #CODE
thanks a lot joris , i feel stupid , unstack was not in my scope of known function :-(
` resample ( ' 2.5S ' , how= ' sum ' , label= ' right ')`
The problem with ` resample ` is that it only considers the time values where I have data ( i.e. the time window defined by the index of the dataframe ) , but I want to get samples on a different set of timestamps .
try using `` freq= ' 2500L '`` ; there is a bug with fractions in the frequency ( it will make it 5s freq which is wrong ) .
You can simply resample and it will work like you want .
You reindex if you really want to ( either before or after ) to your full-range , but those will be NaN
If I re-index beforehand , will ` resample ` start counting ` freq ` from the first timestamp in the dataframe ?
yes it will ' snap ' it to the first freq ( you can offset this using `` loffset `` if desired and / or `` label `` , which counts from the biggest stamp )
the final output would be a stacked bar chart , with each segment of the stack ( on the ordinate ) representing the count() of values in each bin , and the abscissa showing time values ( e.g. days ) .
I tried to transpose the matrix , but that led to nowhere .
After the entire data is outputted to the Excel file , what is the easiest way to apply conditional formatting to it programmatically using Python ?
The end result is a gradient of colors based on the values , a " heat map " if you will .
After the data is written , I need a way to apply the conditional formatting using python .
Here is an example of how to apply a conditional format to the XlsxWriter Excel file created by Pandas : #CODE
pandas intraday 8Min resample bug ?
How to merge Series to DataFrame as columns , broadcasting
You could construct a dataframe from the series and then merge with the dataframe .
I suggest a small edit : ` df.merge ( pd.DataFrame ( data = [ s.values ] * len ( s ) , columns = s.index , index =d f.index ) , left_index=True , right_index=True )` .
minor correction : ` * len ( df )` not ` * len ( s )`
Next , ` join ` concatenates this new frame with ` df ` : #CODE
BTW in your answer the ` 2 ` ought to be ` len ( df )` to be generally applicable .
I have data , in which I want to find number of NaN , so that if it is less than some threshold , I will drop this columns .
For small Series got a 3x speed up in comparison with the ` isnull ` solution .
It will depend on the size of the frame I think , with a larger frame ( 3000 rows ) , using ` isnull ` is already two times faster as this .
Converting panda's dataframe group iteration into groupby with apply
I can do it in a loop like below ( it works ) but can't rework it into a groupby with apply .
Interesting problem which I would solve by writing a function which you then pass to apply .
Note that when calling passing a function to apply , the fist argument passed is the DataFrame itself and this is done so automatically .
That is why you don't need to specify the ' df ' argument when passing the function to apply .
I know I can apply ` dropna() ` on this output , but isn't the above already supposed to filter out the values I ask for ?
concat pandas DataFrame along timeseries indexes
transpose multiple columns Pandas dataframe
Apparently , there are indeed repeating client-order pairs on later dates , contrary to Note 2 above , which is what's messing my data ( so much for checking only 20,000 rows in a million , pfft ) and the pivot .
DSM's answer ( and my initial hunch ) is correct that pivot is the solution .
The only way I can reproduce that error is if your index isn't really what it looks like it is ( e.g. it's really a multiindex , and so df.index [ 0 ] is a tuple . )
There is a related question on SO , but it uses placeholders and search / replace functionality to postprocess the HTML , which I would like to avoid :
default param for shift is 1 so not necessary to set ` shift ( 1 )` although it is clearer code by specifiying it
` df [ ' B_shifted '] = df.groupby ([ ' A ']) [ ' B '] .transform ( ' shift ')` .
The former notation is more flexible , of course ( e.g. if you want to shift by 2 ) .
I use this to get the vote totals and I apply a function to the group to get the number of precincts reporting and the total number .
So to handle all of the #ABS duplicate , I just replace them , like so : #CODE
But this doesn't solve the problem of duplicate PrecinctIDs - of which there are many - impossible to do a find and replace on every exception .
I am grouping the duplicate precincts before I count them , so I need to replace the duplicate precinct_name before I count them .
I think you can use the select method to apply a filter to the index : ` df_raw.select ( lambda r : r.lower in my_list_of_rows )`
Pandas : DataFrame too long after merge
Say I have to DataFrames , one longer than the other , that I want to join on a specific column , as in the following example : #CODE
Then I join them with : #CODE
However , I have two DataFrames that I'm trying to merge , with 28,011 and 15,676 rows , respectively .
The outer join will also include observations in df2 with no much in df1 .
Sounds like you want a left join .
` len ( pd.merge ( df1 , df2 , left_on= ' col1 ' , right_on= ' col1 ' , how= ' left '))`
` len ( pd.merge ( df1 , df2 , left_on= ' col1 ' , right_on= ' col1 ' , how= ' inner '))`
My bad , the column I was merging on did actually contain some duplicate values , so for exact identification I needed to merge on two columns , in the end
The map is there because I needed to broadcast the df row into the new dataframe at several rows ( not just once ) .
Took me more time to create the dataframes then to merge them haha
Immediately after loading the data frame I drop duplicate rows using the command #CODE
Then concat the chunks .
` tshift ` requires a freq argument for this usage ( because the freq is potentially and usually not regular once you group ) , so ` df.groupby ( ' A ') .tshift ( -1 )` returns an empty frame ( it is raising for each group , slowing it as well ) .
Aside from this , this issue here is waiting for a cythonized implementation of shift ( and tshift ) too .
My actual code had the freq indicated ; I just erroneously dropped it in the example .
you realize transform with a constant is the same as subtracting 5 from the column ( equivalent to my 2nd example ) - obviously any group operation is a function of the number of groups - calculation functions are mostly cythonized
But I wonder if there is a ( more pythonic ? ) way to do it with melt ?
as an FYI . this is a chained assignment and will not in general work ( and will show the `` SettiWithCopyWarning / Error `` , better to do : `` df.ix [ 0 : len ( X ) , ' seris '] = ' X '``
I forgot concat indeed , this answer is very explicit !
And a lot of fixes went thru for tz types of things .
In order to select / reindex these have to be the same tz , otherwise it doesn't make any sense .
Pandas better way to aggregate over multiple series
So while the Groupby aggregate does indeed organize my data logically its how I want ( I just don't quite get GroupbyObjects aswell )
I have tried to do a for loop on distinct cat names and then insert the item .
Thank you , i had a do an insert of column to the new data frame and was able to achieve what i needed thanks
Thank you , i had a do an insert of column to the new data frame and was able to achieve what i needed thanks df2 = pd.DataFrame ( series.apply ( lambda x : pd.Series ( x.split ( ' , '))))
I see so if I do not append the data columns when I store it using ` to_hdf ` it would not work if I use ` store [ ' table2 '] = df_tl ` either .
and I aggregate on groups , the output will usually be an empty DataFrame : #CODE
Why is the behavior different for this one aggregate function ?
So in fact ` mean ` , ` median ` , ` sem ` , ` std ` , ` var ` and ` ohlc ` will all raise an exception .
Compare what happens when you call apply with ` mean ` : #CODE
Merge pandas dataframe , with column operation
In that case you may have to merge / concat all the dfs together and then sum all the columns where there are clashes ignoring the ` NaN ` values
You should post as an answer if it works for you , none of merge , join , concat will do what you want because essentially you are not merging data you are performing an operation so it's distinctively different so I can't see how this could be done in a single pass
Pandas report top-n in group and pivot
2 ) Now next problem is that I want to to pivot the top 5 ( ie so I have one row for each element of d1 )
so to pivot I have to create the ranking along d2 ( ie 1 to 5 - this is my columns field ) .
So I have initialized an empty pandas DataFrame and I would like to iteratively append lists ( or Series ) as rows in this DataFrame .
How to check if the signs of a Series conform to a given string of signs ?
Add calculated column to a pandas pivot table
I have created a pandas data frame and then converted it into pivot table .
My pivot table looks like this : #CODE
I was wondering how to add calculated columns so that I get my pivot table with Autopass% ( ` Autopass ( cb ) / TotalCB*100 `) just like we are able to create them in Excel using calculated field option .
I want my pivot table output to be something like below : #CODE
How do I define the function which calculates the percentage columns and how to apply that function to my two columns namely ` Qd ( cb )` and ` Autopass ( cb )` to give me additional calculated columns
Clearly some code has been written @USER : You don't get to the stage of making a pivot table in pandas without some code .
@USER Yes I imported the data using read_csv into a dataframe and the used the pivot_table function to create the pivot table .
` df1 ` be some transform of ` df0 ` such that ` len ( df1 )` equals ` len ( df0 )` , and ` df1.columns ` equals ` df0.columns ` ;
How to apply a custom formula over a group element in a grouped.apply() .unstack() method ?
It looks like you are looking for apply in this instance .
The map ( str , row ) step is needed so Orange know that the data contains values of discrete features ( and not the indices of values in the values list ) .
Basically , after doing the groupby and then the mean aggregate function , I want to be able to just plot the resulting data .
Pandas agg function - how to return results without the multi index ?
When I use groupby and agg , I get results with a multi-index : #CODE
How can I return the results , without the Multi-index , using parameters of the groupby and / or agg function .
Finding the intersection between two series in Pandas using index
I have two series of different lengths , and I am attempting to find the intersection of the two series based on the index , where the index is a string .
The end result is , hopefully , a series that has the elements of the intersection based on the common string indexes .
Pandas indexes have an intersection method which you can use .
Tried common = s1.index.intersection ( s2.index ); printing len ( common ) gives 0 .
I would like to merge these two to a single timestamp such as : 2013-06-25 07:15 : 00 .
And then I'll want to merge into a single DatetimeIndex .
And then interpolate value_to_find to get the ` B ` value .
Maybe using query , map or something similar .
If you are using plain numbers you can easily call interpolate and you will have numerous options at hand for that ....
and then apply a method to the groups or whatever you want to do .
But beware that ` shift ( -3 , freq= ' m ')` shifts date to ends of months ; for example , 8 Apr to 31 Jan and so on .
Pandas creates duplicate index entries on merge
When merging data sets , I would expect pandas to not duplicate the values in the columns in the join .
Also , the multindex created from the join is stated as being unique , but a groupby suggests that it is not .
I think , not ` asfreq ` but ` resample ` fits your needs : #CODE
Do I have to make a new dataframe for each year group and merge them ?
I got all the years into a list of frames but when I do the join no data gets added of course because while the day and month may line up the year does not .
There is no merge to be done .
All your data is date-specific , just " stack " the data behind each other , and then make sure you group by month , day to get the yearly correlation ( for the same day )
If you want to apply additional filtering , with this index , you can even select a specific year using ` df [ ' 2013 ']` .
I am trying to drop multiple columns ( column 2 and 70 in my data set , indexed as 1 and 69 respectively ) by index number in a pandas data frame with the following code : #CODE
I'm trying to join two dataframes with dates that don't perfectly match up .
For a given group / date in the left dataframe , I want to join the corresponding record from the right dataframe with the a date just before that of the left dataframe .
then a regular join or merge between `' join_date '` on the left and `' date '` on the right will work .
this gives me an error " cannot join with no level specified and no overlapping names "
You could just as well say that you are " leaving Pandas " and going back to " pure Python " , and then apply some type conversion on the values that will be ` dict ` values .
To do this , you would group the data and use the ` apply ` method to apply a function that does the above .
And then pass that function to ` apply ` using the groupby object as follows : #CODE
In Pandas , how to apply 2 custom formulas in a groupby.agg() method ?
The apply functionality is probably what your are looking for : #CODE
Well , the docs on aggregate are in fact a bit lacking .
Typically when comparing rows you want to use the shift method #CODE
How to concatenate within ' apply ' on a grouped object
The procedure to apply to a and b is : set all rows equal to the row where c is a minimum .
I am using an apply function on a grouped object .
I use ' reindex ' to change a and b .
Is there a better approach to the one I have taken ( i.e. using reindex concatenate ) ?
You provided strings to the ` concat ` function instead of dataframe objects : In the ` wvmexp ` function do : #CODE
You can use ` loc ` as a boolean mask to assign just to the rows that meet the criteria , even for such a small df it is faster , for a larger df it will be significantly faster : #CODE
Interestingly for a 50,000 row dataframe , the ` loc ` method outperforms the nested ` np.where ` method : I get 4.24 ms versus 12.1 ms .
I wanted to know if there is a way to make the dataframe insert an index column automatically .
@USER yes - it could be generalised by using ` & ` and changing the shift value , e.g. ` df [( df.p ! = df.p.shift ( 1 )) & ( df.p ! = df.p.shift ( 2 ))]`
I try to align these into a single dataframe .
How can I align > 2 time series and combine them in a single dataframe ?
You could rename the columns and the second align would work .
One alternative way would be to chain the ` join ` operator , which merges frames on the index , as below .
Correct , ` join ` handles the index logic for you .
There aren't too many practical cases I can think of , but if for some reason you need to only align the index , you'd save a little time by using ` align ` .
I need to resample some data with numpys weighted-average-function - and it just doesn't work ...
Either apply a regex pattern or apply a function that returns the characters you want , even if length did work , it'll just return the length of each row which is no different to doing nothing .
I indeed cannot find a way to compute " wide " rolling application in pandas
docs , so I'd use numpy to get a " windowing " view on the array and apply a ufunc
thus ` strides =( arr.shape [ 1 ] * isize , arr.shape [ 1 ] * isize , isize )` means skip 5
This is a great little intro to the strides though so I thank you , immerr .
How can I keep the intersection of a panel between dates using Pandas ?
I'm trying to calculate returns for a portfolio that changes every month and the only coherent way to do it is take the intersection of securities for consecutive dates and take the difference of the sum of those prices .
Group this new dataframe on ID , and use the ` shift() ` method to get the differences in the stock prices using the ` apply ` method
This doesn't work b / c ' date ' and ' ID ' aren't aligned . date is a block vector of the entire date range repeating every len ( date.unique() ) rows and ID is a block vector of the entire range of IDs repeating every len ( ID.unique() ) rows .
An alternative method which is likely to be even faster is to use ` loc ` and your condition to only strip the last character where it contains a trailing s , all other rows are left untouched : #CODE
No , of that you'd have to apply a regex or a lambda to test each word for that character and strip it
` axis=0 ` drops rows , ` axis=1 ` will drop columns , sorry the meaning of axis changes depending the on the operation
NaN , so I assume the change would be to use isnull ( frame.A ) ?
I've been able to create a boxplot using the boxplot() function built into pandas ' DataFrame by doing : #CODE
The the ` boxplot ` method of ` pandas ` ` DataFrame ` , on the other hand , plots boxes at x = 1 , 2 ...
Python , how to merge 2 pandas DataFrame
How can I merge the two of them to have this similar output ?
If the lengths are the same you can just concat ` pd.concat ([ df1 , df2 ] , axis=1 , ignore_index=True )`
You can just ` concat ` in this case : #CODE
` join ` would also work : #CODE
Then merge the sub-tables back together in a way that replaces NaN values when there is data in one of the tables .
Interesting question , you can get the index of the values of each columns in the sorted values of each columns ( here in the ` mask ` ` DataFrame `) , and then keep the values that have the index within you defined boundary .
You can apply a post-processing step that first converts the string to a datetime and then applies a lambda to keep just the date portion : #CODE
Create a new column , then just apply simple ` datetime ` functions using ` lambda ` and ` apply ` .
Try exporting to CSV right before doing the ` apply ` .
Is there a command to drop the rows where the date is not a date ?
There is no need to transpose your DataFrame ; just use the ` axis=1 ` argument to ` cumsum ` .
perhaps using concat or merge ?
Yes , merge would be the way .
It sounds like you need to merge the two DataFrames on the column ` n ` : #CODE
combine = concat ( df , ignore_index=True ) #I know something like this needs to be created but I am not sure how #CODE
you don't need to concat if you are just wanting to write out the modified portion .
Essentially you just write out the first chunk as normal and then on each subsequent chunk add the params to ` to_csv ` : ` mode= ' a ' , header=None ` this will append to the existing csv and not write the headers out again as you already did this the first time .
Currently , I'm doing this through a ` for i in xrange ( len ( df2 ))` loop , checking if ` df2.A.iloc [ i ]` is present in ` df1.A ` , and if it is , I store ` df1.A , df1.B , df1.C , labels [ i ]` into a dictionary with the first element as the key and the rest of the elements as a list .
df2 is basically only being used so I know which relevant element of ``` labels ``` I want to join with a row of ``` df1 ``` .
The line of code to replace ` inf ` , etc ., now throws the following error : #CODE
You can pass a function to apply , but it shouldn't return a dict .
Basically , I will be applying a series of functions to create new columns and transform existing ones after the group by , for instance :
I've designed this method so I can apply any function I wish to the table at any point .
But the problem is : I may need to apply an aggregate function which needs access to more than one column ( table manipulation ) .
And I will need to apply more than one of these in batch to different data !
` df.loc [ len ( df )]= [ 2.0 , ' James ' , NaN , 96 , 87 ]`
But how are you going to ' align ' the data ?
Not sure how to apply this to my dataframe ...
How to resample a dictionary of dataframes ( Yahoo Data ) in Pandas ?
Doesn't the fact that the resample works for a single dataframe invalidate your point ?
But I can run the resample with the same conversion as a dict .
You can replace your last line with ` z [ key ] = df.resample ( ' BQ ' , how=conversion )` to save the results .
Any idea how I can normalize rows of this data frame where each value is between 0 and 1 ?
there is an apply function , e.g. frame.apply ( f , axis=1 ) where f is a function that does something with a row ...
You can use the package sklearn and its associated preprocessing utilities to normalize the data .
Your problem is actually a simple transform acting on the columns : #CODE
Pandas passing wrong dtypes when calling apply
It seems to cast all values to ` bool ` , unless I " touch " the ` DataFrame ` by adding a new column .
This happens regardless of whether I use row or column-based ` apply ` ( i.e. ` axis=0 ` or ` axis=1 `) .
You can read them into multiple dataframes and concat them together afterwards .
Why not just set them to floats and then do a diff ?
Not an expert in Pandas , but I would like to know if there is a pythonic way to transform a series in a Pandas DF into columns headings with data consisting of arrays of " 1s " and " 0s " .
Use ` unstack ` might be faster : #CODE
Why is very time consuming to transform a pandas-series with pandas-datetime values into a list ?
Do I need to apply this as a lambda function ?
You can use ` join ` , telling pandas exactly how you want to do it : #CODE
Python Pandas pivot_table missing column after pivot
After pivot what I want is following data frame : #CODE
So far I have done following to to build the above matrix but I am unable to get the ` expert_level ` column as shown after the pivot .
Then join your pivot and lookup #CODE
If you also want to include ' total_min_length ' , you can do a second pivot : #CODE
and join all three instead of two : #CODE
@USER -Hypothesis that error occurs when you join two frames with some column names that are the same , and haven't specified the ' lsuffix ' and ' rsuffix ' parameters .
Pandas : apply different functions to different columns
My workaround was inserting a column of ones into the dataframe , doing groupby on that column then passing a dict to the aggregate method .
As you can see , loc is not a method but a property , which then in turn has its own get method .
` loc ` is a property that creates returns a name called ` _loc ` if its not ` None ` else it creates a ` pandas.core.indexing._LocIndexer ` on demand .
You could round the ` Price ` column , store them in a ( temporary ) ` approx ` column , then perform a ` groupby / agg ` operation : #CODE
The same thing happens when I try to resample : #CODE
I've tried the HDFStore ( append ) method and find it is massively slower even in my small test sample ( pytables < 10secs , HDFStore=60secs ) .
I might have to re-visit that as when profiling I find it spends 99% of the time in the HDFStore append .
Using Pytables I split each record ( 2MB ) into multiple tables ( 80-120 bytes each row ) and append to each group / table and it takes around 1 sec per record .
In both of those cases , the order that individual rows are sent to the agg function does not matter .
This is NOT true for resample however as it requires a monotonic index ( it WILL work with a non-monotonic index , but will sort it first ) .
The best way is probably to not try to do it all in one step ; first use ` nth() ` then use ` agg() ` , then merge them .
Create a boolean mask based on your selection criteria .
You need to select the value first and then pass the mask .
` df [ " Col "] [ mask ] = ' value '` updates the original dataframe .
` df [ mask ] [ " Col "] = ' value '` will assign to a copy which is not what you want .
If you want to shift the date for 2 months you have to use : #CODE
At this point , I should be able to use something like an " apply " function on ` df ` using ` months ` , but am a bit lost ...
I think a way should exist to avoid the use of apply but I didn't find it .
To do the latter , I think groupby ( or join ) may be best .
I think ' apply ' is a good way to go .
Or is there some way to apply a condition in string splits , without iterating over each row in the data frame ?
Depending on where you access the viruses from , this may or may not be a concern , because some databases offer the option to replace spaces with " _ " .
Element-wise median of a lot of matrices , python pandas
I want to essentially make a list of every i , j component in a dataframe across keys and take the median of all of those .
You can think of this as stacking the matrices on top of each other and taking the median value for each i , j element .
I would like to avoid making the list of n ( n+1 ) / 2 unique i , jth pairs and then taking the medians , then putting them back in their proper place in the final median matrix ( dataframe ) .
In general , the median requires all of the data in memory , unless you only want an estimate .
Ah yes , you cannot have a median when some values are NaN ; unless of course you want a median of all the non-NaN values .
@USER that only works for median of a series , right ?
I need median of a list of matrices .
I am using Pandas and I need to drop duplicates within the set for each person .
Or is there another way - such as apply ?
This will pivot your df ( which is in the long format ala ggplot style ) into a frame from which pandas default plot behavior will produce your desired result of one line per network type with index as the x-axis and count as the value .
Pandas to calculate rolling aggregate rate
I'm trying to calculate a rolling aggregate rate for a time series .
It just returns a new Series with the lowercase transform applied .
Is it possible to calculate the median of a list without explicitly removing the NaN's , but rather , ignoring them ?
I want ` median ([ 1 , 2 , 3 , NaN , NaN , NaN , NaN , NaN , NaN ])` to be 2 , not NaN .
You mention list but tagged this as pandas , for a series by default calling ` median ` on a series will ignore ` NaN ` values : #URL
I would clean the list of all NaN's , and then get the median of the cleaned list .
where ` x ` is the list you want to get the median of
Then to get the median just use the cleaned list : ` median ( x )``
There are a couple of ways you could mask out the values in the upper diagonal using ` df.mask ` .
Python pandas insert list into a cell
I want to insert the list into cell 1B , so I want this result : #CODE
because it tries to insert the list ( that has two elements ) into a row / column but not into a cell .
I want insert the ' abc ' list into ` df2.loc [ 1 , ' B ']` and / or ` df3.loc [ 1 , ' B ']` .
Having an integer column and a srting column I get the same error : ValueError : Must have equal len keys and value when setting with an iterable
Dataframe after reindex does not show all items in multiindex
Main goal - to reindex DataFrame with new multiindex that contains new values #CODE
query() on multiindex columns in pandas DataFrame ?
There's probably a way to avoid the int ( bool ) shenanigan .
it sounds like you want to aggregate the data .
Then ` df.groupby ( ' list_d ') .agg ( { ' count ' : lambda x : sum ( x ) } )` should aggregate the data as you want it to , and put ' list_d ' in the index ....
Proper way to merge DataFrame columns in Pandas ?
The reason I do this is because in different datasets I have different numbers of columns with related info that I want to merge into one column / row so that I can do frequency / mean calculations on them .
This seems fine to me as you are concatenating all your dfs in one go , if you did a join or merge you end up repeatedly joining / merging and each time you allocate space for the additional rows / columns .
` etc .. however this approach would require that the indices align which may not be true , in any case I think concat is appropriate
I was also just able to find use of the " loc " command - again , not quite what I want to do but :
Is there a memory efficient way to replace a list of values in a pandas dataframe ?
I am trying to replace all of the unique strings in a large pandas dataframe ( 1.5 million rows , and about 15 columns ) with an integer index .
Or you could create a series using the unique strings as the index and the integer values as the values and again call map pass this series
I will try to use the applymap method tonight , but I am sceptical that it would be any better than replace .
I think your best bet is to just create a Series with the unique string values as the index , then just assign an increasing int value for the series and then use this as the param to the ` map ` call : #URL this uses merge but you could just call map if this was a Series
` apply ` doesn't seem to work on a dataframe .
To do this , I need help breaking down the end points of months so that they span the correct months then I will resample the data using resample ( ' M ') in pandas .
After this , my plan is to resample the data using df.resample ( ' M ') by resampling on the ' start_date ' and ' offline_total " columns so that I have an accurate picture of how much of this substance is offline given a month .
I want to resample this into all months from 2010-01 to the end of the dataset , throwing out any data that does not fit this range and filling in 0s for months where there is no original daily data .
Very slow single pandas apply / groupby call
I want to apply a function to each row .
I've tried using groupby and row apply : #CODE
If you want to apply a function to * every * row , groupby doesn't make sense .
This might not always be the case , so the row apply would only be a temporary fix ( but it's not faster , anyway , so I guess not )
Any thoughts on a quick way to transform this data ?
pivot is the key piece I was missing .
As a work-around I'm trying to do the same thing filter does with groupby and apply but it doesn't work as expected .
The problem with your example code is that the ` apply ` doesn't know what to do with the ` None ` when putting the dataframe back together .
Your ` apply ` function needs to output the same type of object every time .
if len ( x ) > 2 : return x
not sure if I understand your question , but you can search for year startswith 1970 and replace with 2013
But , if you figure that out , you can just " enlarge " the original data frame with all NaN values ( at index values from ` DatesEOY `) , and then apply the function about to ` YTDSales ` instead of bringing ` output ` into it at all .
Using only boolean comparisons , ` shift ` , and the fact we can sum boolean columns to get the number of Trues because ` int ( True ) == 1 ` , we can do : #CODE
Pandas Multiindex df - slicing multiple sub-ranges of an index
I know I can do this in separate lines with ix , i.e. : #CODE
More information on slicing is on the Pandas website ( #URL ) or in this similar Stackoverflow Q / A : Python Pandas slice multiindex by second level index ( or any other level )
I came up with this function , using ` apply ` : #CODE
The pandas site doesn't seem to have any documentation on loc .
Thanks again @USER .I was aware of loc but couldn't find any documentation on it to actually learn exactly what it does .
then I apply the multi-indexing and unstacking so I can plot the yearly data on top of each other like this : #CODE
Tableau - blend , join , or modify raw ?
Question : Do I use a blend between the files ( there are about 8 different data sources right now ) , a left join , or do I just use python / pandas to literally add the information to the raw data source ?
While it is easy to use , it's highly inefficient , because it will virtually perform the join every single time a calculation is made .
Performing a left join on Tableau when connecting to data ( I'm assuming you're using csv files ) is a very good solution , as is making a single table with the information in Pandas .
I believe it's simpler to have Tableau doing the join .
Then I will left join that with my larger raw files which have the ID+Date unique ID .
Blending can cause some complex behavior behind the scenes and you have to pay careful attention to the blending fields ( analogous to join keys ) .
Time-series boxplot in pandas
How can I create a boxplot for a pandas time-series where I have a box for each day ?
To be clear : 1 . using fromtimestamp() without specifying timezone + replace ( tzinfo ) as you do is wrong if the local utc offset is not the same as the offset in the input date it does not depend on Python version or anything else 2 .
A quick patch would be to define ` df_new [ ' values '] = df_new [ ' values '] .str .split ( ' , ') .apply ( lambda x : map ( float , x ))` .
You need to truncate the first two and the last characters before converting .
Combine two Pandas dataframes , resample on one time column , interpolate
I'm trying to merge df1 and df2 , interpolating y2 on df1.t .
I figured out one possible solution : interpolate the second series first , then append to the first data frame : #CODE
" ValueError : cannot reindex from a duplicate axis "
Groupby or pivot in pandas ?
AttributeError : Cannot access callable attribute ' to_csv ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method
Perhaps I can mix this with the transpose function , but then I lose track of datatypes , right ?
Or you could create a custom function via the map tool :
you can upgrade to 0.15 now with conda ; the hrs is trying to align the data ( in the add ) so u can specify .values ( for each term ) to simply ignore the alignment
I'm trying to replace one record of pandas DataFrame ( df ) with the latest version of that label ( found in a separate DataFrame ( latest_version ) .
Just do it step-by-step , the only non-trivial part is the ` unstack ` -call .
I can think of two ways , setting or assigning to a new column just the ` date() ` attribute , or calling ` replace ` on the datetime object and passing param ` hour=0 , minute=0 ` : #CODE
The fastest way I have found to strip everything but the date is to use the underlying Numpy structure of pandas Timestamps .
I have a .csv file that I would like to resample at 1 minute granularity .
#URL Try using the intialization part of the documentation examples with your resample method .
@USER -Hypothesis my original answer was incorrect , have corrected it now , I now apply row-wise dividing each item by the row sum , the values seem correct to me now
you can replace the cols with ` result.loc [: , ' 0 ' : ' 13 ']`
I would like to insert a column ' Interval ' which shows these intervals like this .
If I apply the rule in the df example , the output should be : #CODE
@USER you can do ` A [ mask [: : -1 ] .cumsum() [: : -1 ] > 0 ]` if you do not want to see the warning , but that warning is totally fine in here .
@USER without adding the ` [: : -1 ]` at the end , index of ` mask [: : -1 ] .cumsum() > 0 ` does not align with the dataframe . the warning basically says that pandas will do ` reindex ` to align the indices . see [ here ] ( #URL ) for some examples of alignment .
If you don't know how many columns you have but you do know the column naming convention , construct a list of columns to aggregate dynamically .
However , I only want to apply sum() to ' measure1 ' ...
In Pandas / Numpy , How to implement a rolling function inside each chunk using 2 different columns ?
I need to perfom a rolling calculation inside each chunk , that involves 2 columns .
Specifically , I need to create a column ' E ' that is equal to column ' B ' minus the rolling min of column ' C ' , in function : #CODE
I need to apply the formula above for each chunk .
You just need to add the param ` axis=1 ` : apply ( lambda row : func ( row ) , axis=1 )` then whatever your func does access the columns of interest : ` def func ( x ): return x [ ' A '] + x [ ' C ']` as an example
I don't know if this gives you what you want though ... your rolling min doesn't give you the numbers you specify in your desired output ( maybe you just want min() in that function ? )
Not on a rolling basis .
pandas : how to merge 2 dataframes on ' X ' from df1 and f [ ' Y '] from df2
i want to merge 2 dataframes df1 and df2 ,
using right_on : result of function f apply to col ' Y ' from df2 .
Why not just add the column , do the merge and then drop the column ?
You can pass ` left_on ` and ` right_on ` arguments to merge : #CODE
Now we can merge the dfs : #CODE
Obviously replace ` df ` , ` df1 ` with ` frameA ` and ` frameB ` in your case
here's a little trick to normalize the times : df [ ' Date '] + pd.offsets.DateOffset ( minute=0 , second=0 ) .
Then , they will merge on the exact same index , and since they have similar columns ( ' Buy ' , ' Sell ') , we need to specify suffixes for the merger :
merge = frameA.join ( frameB , lsuffix = ' A ' , rsuffix = ' B ')
This will cause a shift , and then we improperly assign values to the wrong index is the one guiding the joining .
Then , you merge the dataframes : #CODE
Then with the merger , they should merge on this index , and there should be only one index .
Another way is you can always just drop the column after everything is done :
` pandas ` still has to transform it to a tabular structure .
then you will prob need to have to drop down to pure numpy / write your own code
I feel like this should be a simple task , but currently I'm thinking of reading it line by line and using some find replace to sanitise the data before importing .
Pandas aggregate all columns by group
How do I drop these rows ?
I just want a way to drop these quickly .
The date column should always have a value ( notnull )
( I think this date thing is the key issue because I have formulas which subtract StartDate from EndDate .. if one of those contains a word then it messes up the entire process . Maybe I can create some error exception or drop error rows ? )
Can I drop specific rows based on the default index ( like 362739 , 781146 ) .
basically what you're asking can be done either by using 3 ` loc ` statements or a bunch of nested ` np.where ` statements but it's better you show us real input data and desired output data
the elif / else is done with the loc / update , since these cases are distinct .
Or is the problem this is a " rolling " style question , in which case you may have to [ cythonize ] ( #URL ) to get performance .
You can use ` groupby ` and ` agg ` to do this in one step ( groupby tutorial ) , without iterating one by one : #CODE
How can I append the values I receive in this array to my original data frame ?
unexpected behavior pandas groupby transform
Now , when I use ` transform ( np.mean )` on the ` DataFrame ` , I am getting : #CODE
Can somebody explain am I doing something wrong , or has there been a change in behavior of ` pandas transform `
Doesn't this example violate the contract of the transform function ?
The docs state : ` Thus , the passed transform function should return a result that is the same size as the group chunk .
one thing you have to take care with is if a string in searchfor has special regex characters ( you can [ map with re.escape ] ( #URL )) .
And to drop the index names : #CODE
The A -> B value and B -> A values seem to have been interchanged , can pivot fix this ?
never mind , I can always transpose again :) Thanks !
Can anyone suggest a way to apply a function sequentially , so not only using the last calculated value of the column being computed but also using the present and past values of other columns in a pandas dataframe .
Anyway it sounds like " Event " needs to be a vectorized logical-or of ` diff ( df$Start ) & diff ( df$End )` .
The crux of what I am trying to do is to be able to apply a function such that it has access to current values of all columns and past calculated values of the current column .
The tools you want are ` diff ` and ` cumsum ` .
` diff() ` will give you both positive and negative diffs ; since you're only interested in the earliest positive change , then compare the output of ` diff ( ... ) == +1 `
Drop the row where you have an empty value , for some reason it actually has a length of 1 when I tested this with your data , it may work for you using len 0 .
How to insert empty excel dates into oracle with Python+Pandas ?
I've got a Python application that is using pandas to grok some excel spreadsheets and insert values into an oracle database .
How do you insert your date value in the DB ?
If you want to manipulate the ` NULL ` values , use ` NVL ` function and use the desired value you want to replace the NULL value with .
Anyway , one I was able to come with is to add a ` BEFORE INSERT TRIGGER ` to fix incoming timestamps .
After that , from Python , using ` pandas.DataFrame.toSql ( ..., if_exists= ' append ')` : #CODE
Beware that , if you ever need to rewrite an other DataFrame to the same table , you will first need to delete its content -- but not drop it , otherwise you would loose the trigger at the same time .
So if you replace that in your code , it should work ( so ` pdsql = pd.io.sql.SQLDatabase ( engine , meta=meta )`) .
How do I append an existing column to another column , aligning with the indices ?
Currently , when I attempt to append them together , I get NaNs and the same indices are duplicated .
I created an empty dataframe so that I can put all three dataframes into by append .
DataFrames have a ` join ` method which does exactly this .
I have to resample my dataset from a 10 minutes interval to a 15 minutes interval to make it in sync with another dataset .
Using two forms of resample , with ` close= ' left ' , label= ' left '` and ` close= ' right ' , label= ' right '` .
Resample everything to 5 minute data and then apply a rolling average .
Something like that is apllied here : Pandas : rolling mean by time interval
Resample and average with a varying number of input : Use numpy.average with weights for resampling a pandas array
Resample everything to 5 minute data and then apply linear interpolation .
Pandas data frame : resample with linear interpolation
if you reindex ( not resample ) your ` 10T ` dataframe to ` 5T ` , you'll have NAs at e.g. , XX : 05 , XX : 15 , ...
So then build a new 5-minute index and reindex the original dataframe #CODE
and then linearly interpolate #CODE
If I eval those strings , I just get ` Ignore ` when I print : #CODE
We create a filtered list where we append every element in d if it's in the fields , then compare the result to d .
I want to replace the 7th to 5th from last character with a 0 if it's a /: #CODE
Is your replace operation basically trying to make sure that the days in each date are zero-padded , so ` 4 / 1 / 2012 -> 4 / 01 / 2012 ` ?
The simplest way to fix this is to replace #CODE
reorder column in pandas pivot table
does it work if you ` reindex ` : ` df.reindex ( index = lst )` ?
groupby the user and apply a lambda : #CODE
Use the vectorised ` str ` method ` contains ` to create a boolean mask and use the negation operator ` ~ ` , pass this to ` loc ` and prepend your string to the current value : #CODE
Any ideas how can I easily parse / strip this into an efficient dataframes ?
The string entry ' nan nan ' cannot be converted using ` to_datetime ` , so replace these with an empty string so that they can now be converted to ` NaT ` : #CODE
It runs , and you can cut and paste it into your favorite editor .
Python pandas apply function if a column value is not NULL
I want to apply a simple function for rows that does not contain NULL values in a specific column .
And my apply code is the following : #CODE
To fix it , you could simply wrap the isnull statement with ` np.all ` : #CODE
I've also tried playing around with the replace function , but to no avail ( the values to be replaced are all zero , so an implementation of .replace ( 0 , np.nan ) could also work ) .
You'll have to make a choice about what to do with those values , either set them to some int value like -1 or drop them
One common pitfall with ` dtypes ` which I should mention is the ` pd.merge ` operation , which will silently refuse to join when the keys used has different ` dtypes ` , for example ` int ` vs ` object ` even if the ` object ` only contains ` int ` s .
be careful with eval .
I want to stack this into this form : #CODE
` pivot ` and ` fillna ` are what you want : #CODE
I was trying the ` pivot ` thing but somehow couldnt find the suitable example .
@USER , glad it helps :) you may read more about ` pivot ` [ here ] ( #URL ) .
Use ` nan_to_num ` to replace a nan with a number :
Just apply this : #CODE
Then you can merge the arrays however you want .
see above solution for .fillna example . another way to do this is to interpolate . another command in the later version of pandas .
If values in B are larger than values in A - replace those values with values of A .
Since pandas allows basically anything to be set on the right-hand-side of an expression in loc , there are probably 10+ cases that need to be disambiguated .
Personally , I would use eval which is very efficient for large array , something like this : #CODE
I guess eval _is_ very efficient , but it wastes time by checking the whole df everytime , while I check only a handfuls of intervals at a time .
I try to change the timezone of a multiindex DataFramebut I get an frozen list error .
The structure is a little funky for a basic pivot function for how I'd like to reshape it .
I've tried using shift but couldn't get the desired result : #CODE
Basically how would I apply ` df [ ' col1 '] .str .contains ( ' ^ ')` to an entire dataframe at once and filter down to any rows that have records containing the match ?
When ` mask ` is a boolean NumPy array , ` df.loc [ mask ]`
selected rows where the ` mask ` is True .
If ` mask ` is a DataFrame , however , then
` df.loc [ mask ]` selects rows from ` df ` whose * index * value matches the index value in
` mask ` which corresponds to a True value .
dt = num2date ( x , self.tz )
return _from_ordinalf ( x , tz )
dt = datetime.datetime.fromordinal ( ix )
Pandas join issue : columns overlap but no suffix specified
When I try to join these 2 dataframes : #CODE
The error is a bit cryptic , the problem here is that you have no values that are common to perform the join on , if you did this it would be fine : ` LS_sgo.merge ( MSU_pi , on= ' mukey ' , how= ' left ')`
You'd have to post your raw data , also did you try merge ?
Merge works !
Why not join though ?
You'd have to post your data in order for me to look at this and explain why join didn't work
Your error on the snippet of data you posted is a little cryptic , in that because there are no common values , the join operation fails because the values don't overlap it requires you to supply a suffix for the left and right hand side : #CODE
` merge ` works because it doesn't have this restriction : #CODE
Pandas replace issue
I can use pandas replace to replace values in a dataframe using a dictionary : #CODE
What do I do if I want to replace a set of values in the dataframe with a single number .
E.g I want to map all values from 1 to 20 to 1 ; all values from 21 to 40 to 2 and all values from 41 to 100 to 3 .
How do I specify this in a dictionary and use it in pandas replace ?
You can do that using ` apply ` to traverse and apply function on every element , and lambda to write a function to replace the key with the value of in your dictionary .
I will create lambda function to map the values .
You can replace the column by replacing it : #CODE
values that you wanna transform .
eg : I wanna transform every value ' 1 ' to ' Productive ' , ' 2 ' to ' Moderate ' .. etc .
You can use this to go through the dates that you have classified as " year-month " and apply cretiria on it to get related data .
You can use either resample or TimeGrouper ( which resample uses under the hood ) .
To get the desired result we have to reindex ...
I couldn't get TimeGrouper to work , but resample ( " M ") did the trick .
What you want is a ` reindex ` .
First create the index as you want ( in this case just a range ) , and then reindex with it : #CODE
My experience with their distribution is that the SciPy stack is almost always guaranteed to work , regardless of OS .
Python folium GeoJSON map not displaying
I'm trying to use a combination of geopandas , pandas and folium to create a polygon map that I can embed incorporate into a web page .
Although the mapf.create_map() function successfully creates a map , the polygons don't render .
My interpretation of this warning is that by using columns name ( `' column3 '`) and ` loc ` I do not really access ( refer to ) the desired cell of the data frame .
Add a separate issue on Github to have the str.slice method take series objects and apply element-wise .
getting the index of a row in a pandas apply function
I can apply it like so : #CODE
Aside : is there a reason you need to use ` apply ` ?
( Sometimes apply * is * the simplest way to do something , and performance considerations are often exaggerated , but for your particular example it's as easy * not * to use it . )
The names of the groups will replace the default numbers as column headings in the returned DataFrame .
I was thinking about using ` cut ` but the bins attribute seems to only take integers .
keep_default_na : bool , default True
I believe ` loc ` may also work instead of ` ix ` here .
Dealing with DST time with set_levels function [ multiindex pandas ]
The second method gives good result but it is very much longer for big multiindex dataframe ( 16000 for dates and 200 for sce )
If in ipython you can turn on ` pdb ` and start debugging : ` %pdb ` execute the command and then ` %debug ` you will then be able to walk the stack and display the values
Usually you could drop to a debugger when an exception is raised to inspect the problematic value of ` row ` .
@USER , you could arrange for code to [ drop to the debugger when an exception is raised ] ( #URL ) .
` apply ` is too slow .
@USER : I have a script that I ( would like to ) run fairly frequently that spent about 300 seconds in this computation when using ` apply ` ( more like 100 with the awful hack above ) .
I was not able to find a way without at least using an ` apply ` for setup but assuming that is okay : #CODE
Since you are only creating the delta with the apply I would hope you experience a speedup .
Here is a way to do it ( by adding NumPy datetime64s with timedelta64s ) without calling ` apply ` : #CODE
I'm trying to convert pandas dataframe to json in python . but I couldn't get the result format that want in json i'm new to pandas is there a way to pivot and simply use All I can think of is create loop over and over to get key " freq " and state .
Of course , ndarrays don't have an ` apply ` method , which is exactly the error you're seeing .
You'll either need to split the dataframe into sub-frames or apply a function that operations on ndarrays .
Just append each line to a list : #CODE
Since Limit varies on each row , you should use , for example , apply like following : #CODE
you can use mask to solve this problem , I am not an expert either but this solutions does what you want to do .
If the element in column A is n , I need to insert the n th element from the List in a new column , say ' D ' .
By my reading the issue is constructing ` D ` from the elements of ` A ` and ` List ` ( " If the element in column A is n , I need to insert the n th element from the List in a new column , say ' D ' . ")
pandas transform dataframe pivot table
I can transform the following dataframe : #CODE
by using pandas pivot table : #CODE
Is there a quick way to create the pivot table with percentage of row totals instead of raw sum of counts ?
And you want to replace numbers in the row to percentages ?
Error of pytz if frequency of dataframe is below 1 hour for DST change [ multiindex pandas ]
The below function works well if freq is hourly but doesn't work with below freq .
Since you want to apply the operation generically , to any given ` foo ` function , you have no choice but to call that function ` na ` -times- ` nb ` times .
` resample ` is the indeed the function you need , but therefore you need to set the date as the index : ` df.set_index ( ' Date ')` .
Since the ` repr ( df )` you posted prints the date in a different format and the column headers do not align with the data , there is a good chance that the data has not yet been parsed properly .
Are you sure you cannot apply indexing plus , for example , a lambda expression to apply these filters ?
You could also create a mask in a loop and apply it all at once : #CODE
Now both ` dt ` and ` dt2 ` will be dictionaries like this : #CODE
But , this was the answer I was looking for : #URL Using ` shift ` , and storing the shifted values in another column .
The function I was looking for was ` shift ` .
How do I insert an incremental index in a dataframe , or reassign / realign existing index items .
Either override ` loc ` to return a ` CustomFrame ` , or convert its return value in ` get ` .
aggregate by ` idxmax ` .
Your mask idea was an interesting idea too ..
I re-read your question and saw that the mask was in effect doing the same as your code ( and so might not have been what you were after ) .
Pandas pivot table maximum value
I have the following pivot table ( df ): #CODE
If ` x ` is not one of the column labels , I want to interpolate between the two nearest ` x ` values .
Another option is to use the ` interpolate ` method , but therefore , you should add a column with NaNs with the label you want .
With the ` interpolate ` method : #CODE
Is there a way to use the interpolate option without adding a column ?
If you need a min or a max based on those variables , you could create another column and use rolling_min or rolling type formulas .
I want to append a column s , such that in every row , it contains the sum of ` v ` s within a 1-second lookback time interval , such as #CODE
set ` ts ` as index and then ` groupby ` second , and transform with ` cumsum() ` as a new column s , then apply ` reset_index ` , like this : #CODE
oh you mean the freq from the first record on or I got it misunderstood ?
More efficient pandas python command to drop Nan rows ?
I want to drop rows where BookDate is NaN .
@USER , that seems weird that to drop rows I have to create a copy and end up using 2x the RAM .
well , if you think about it you have to create a new array that is the same size as the existing one , then copy over the non-dropped rows . you don't a-priori know that you have rows to drop .
df [ ' rank '] = np.arange ( len ( df )) + 1
It might be simplest to normalize the entire DataFrame in one go ( and avoid looping over rows / columns altogether ): #CODE
reindex your output per the final version of this answer : #URL
Agreed with Paul about ' loc ' usage .
The solution i suggest requires you to reindex your data with your Time data .
You can use a list of datetimes with the desired frequency , use ` searchsorted ` to find the nearest datetimes in your index , and then use it for slicing ( as suggested in question python pandas dataframe slicing by date conditions and Python pandas , how to truncate DatetimeIndex and fill missing data only in certain interval ) .
` pd.concat ` to stack the sub-DataFrames : #CODE
Another option , inspired by HYRY's solution , would be to hide the common columns in the index , and then apply HYRY's ` stack ` ing trick : #CODE
Here is a method use ` MultiIndex ` , ` stack() ` and ` merge() ` : #CODE
Pandas : map values of categorical variable to a predefined list of dummy columns
If we naively use ` pandas.get_dummies() ` to map values to indicator variables , we will end up with only 12 of them instead of 24 .
Is there a way to map values of the categorical variable to a predefined list of dummy variables ?
I want to apply one single function call on " df " .
Instead of having a if else clause in every different places in my code , I insert just one line : " process_df ( ... )" now , thanks to your help .
I have an pandas Series , and I want to find the index / position ( or a boolean mask ) of the last times some value appears before some other specific value .
You could use boolean indexing and ` shift ` .
I ran into trouble with the Pandas pivot function .
I am trying to pivot sales data by month and year .
However when I pivot across years and months the months are sorted alphabetically .
So , the result index is `` object `` dtype , as its converted automatically by the pivot , and it is not reordered according to the category .
You're right after ` pivot_table ` it will reindex the ' Month ' and thus sort alphabetically .
Luckily you can always convert your ` dataset [ ' Month ']` to ` pandas.datetime ` and convert it back to string after ` pivot_table `' s reindex .
pandas reindex with data frame
I have a DataFrame with a multiindex with three levels , for instance : #CODE
and a Series with a multiindex with the first two levels of the DataFrame index : #CODE
` resample ` ?
In Pandas version 0.14 and older , you can use ` apply ` to extract the dates from the ` datetime ` values : #CODE
In 0.15 you can use the new ` dt ` accessor , like ` series1.dt.month `
pandas drop duplicates of one column with criteria
If you want to drop any duplicates , this should work .
Then drop duplicates w.r.t. column ' A ' : #CODE
pool.join() does nothing in this script pool.map already returns a list in the order of the arguments and pool.close() already closed the pool you are trying to join .
Ideally I would want something equivalent to a map method for a dataframe : #CODE
I would rather a key-value map is saved in the series but this works for my specific case .
I think you should wrap the dict in a Series , and then this will already expand in the groupby call ( but then using ` apply ` instead of ` agg ` as it is not an aggregated ( scalar ) result anymore ): #CODE
I didn't know about this ` unstack ` thing , it seems to be a nice solution .
How to transpose this dataframe ?
Unfortunately , this structure is not really to my liking- I would like to transpose the whole thing so that the I have the Series Codes as Columns , a year column and a country column - while the value is in the rows .
I figured that I should use the pivot function , but the result is not what I expected .
bool , DataFrame or Series .
it's easy from here figuring out the actual times . you can also resample to make it clearer , but this should do it .
edit : adding how to resample by seconds : #CODE
Using apply in pandas data frame gives ValueError
I have a vector that I want to apply a pearson correlation to all rows of a pandas data frame .
python pandas : apply a function with arguments to a series .
I ensure that args is a tuple which is what the ` apply ` function is expecting and I get the result I was expecting
Are you sure you need pivot ?
From what you were trying to do , you were trying to pass ' group ' as ` index ` so the pivot fails .
However I'm not entirely sure what expected output you want , if you just want shorter presentation , you can always use ` transpose ` : #CODE
Python pandas to_sql ' append '
My program runs one month of data at a time and I want to append the new data onto the existing database .
Any ideas on why I cannot append to a database ?
add an additional level ( 3rd ) to the multiindex in the same dataframe representing the scenario
I'm trying to merge many ( a few thousand one column tsv files ) data frames into a single csv file using pandas .
I've tried to merge the data frames one at a time using pandas : #CODE
Look at the docs for merge , when called from a frame , the first parameter is the ' other ' frame , and the second is which variables you want to merge on ( not actually sure what happens when you pass a DataFrame ) .
I would think the fastest way is to set the column you want to merge on to the index , create a list of the dataframes and then ` pd.concat ` them .
For example , I would like to select only the data for each lat and lon combination where there are N or more entries ( years of yearly data ) such that ` len ( fal_mean [ ' yr '] = N )` #CODE
` len ( fal_mean.loc [( 35 , -75 ) , ' 101 ']) N `
e.g. ` fal_mean.loc [: , len ( fal_mean.index.get_level_values ( ' yr ') 32 )]` does not work .
This is easy to do in a for-loop , but it takes quite a long time for large features sets ( like going back to 1960 on " ge " and making each row contain the preceding 256 session values ) .
>>> timeit.timeit ( ' data.featurize ( data.get ( " ge " , dt.date ( 1960 , 1 , 1 ) ,
It looks like ` shift ` is your friend here and something like this will do : #CODE
To do that , I set up the loop counter to go from 0 ( first list element ) to range ( 0 , len ( col_headers )) which is the last loop element .
the problem is you are generating a range from 0 to the len of the column list and using the index , which is a number , to index back into the df but those columns have names rather than an index so it'll fail .
However , what confuses me is why my method works with the " -1 " ( i.e. picking up column headers A , B , C , D ) but does not work when I drop the " -1 " ( i.e. picking up column headers A , B , C , D , Date ) .
Easiest way is to filter the len of the groups ( according to the minimum of the resample period ) #CODE
You can then drop them : #CODE
@USER replace ` input() ` with ` raw_input() `
This doesn't happen when using e.g. concat .
How to calculate new vectorized column from bool and float columns in pandas dataframe ?
" but i would think , yeah , just transform them into zeros and ones and go back to multiplication . the ` np.where ` solution is good too .
Use a mask #CODE
` df.apply ` is to each value in array , so ` df.apply ( list , axis=1 )` is equivalent to : apply ` list() ` on each value in the array , ie .
Actually no . apply works with entire row ( if axis parameter = 1 ) .
The behavior you're attributing to apply , element-wise operations , is what df.applymap does .
I think my presentation skills need improving , I only refer to ` sum ` as @USER mentioned in comment about why ` sum ` works on ` apply ` but ` list ` doesn't .
Just assign another column as a ` cumsum ` of ` indicator ` , then apply ` groupby ` , this should do the trick : #CODE
pandas multiprocessing apply
I'm trying to use multiprocessing with pandas dataframe , that is split the dataframe to 8 parts . apply some function to each part using apply ( with each part processed in different process ) .
there is a space in the ` res = df.apply ( process apply , axis=1 )` , is that right ?
currently apply only saturates one core of the CPU .
So here is the useful stack trace error .
You didn't get " a stack trace error " .
You got an error message including a very useful stack trace that tells you how to find the problem .
If you're using Python 2.x , try changing the line to calculate size to ` size = math.ceil ( float ( len ( df )) / n )`
The SettingWithoutCopy warning is well-known in pandas , but as far as I understand , I am already using a single-level indexing ( ` ix `) and not nesting indexers ( e.g. ` [ ] [ ]` .
In Pandas 0.12 , if you used the resample method on a DataFrame with a custom resampling function , it would make one call per dataframe row to the custom function , giving access to the values in all columns .
In Pandas 0.15 , the resample method calls my custom function once per dataframe entry , and the only available value is that entry ( not the entire row ) .
Note that multiple SKUs can reside in a single ContainerCode , so we need to add to the quantity , rather than just replace it , and there may be multiple entries in Containers for a particular ContainerCode .
I also tried using a regular merge : #CODE
We then perform the join between the two , and each containercode existent in containers would recieve the summed quantity from inventory
You can do the drop in place , but to the best of my knowledge you can't join in place .
, however it doesn't work when you apply #CODE
I want to group the below dataframe based on ' id ' , then have the aggregate sums of ' flow ' for all values of ' id ' except 0 ; those should stay independent .
One way would be to use ` transform ` to assign the new flow values back and then drop duplicates : #CODE
You should replace the first line by the following : ' df2 = df.replace ( np.NaN , -1 )' , ' g = df2.groupby ( ... )' .
I have a huge CSV file with a bunch of metrics across item IDs that I'm trying to compare to one another , and I want to find the quartiles of every item within each metric and replace each actual number with its quartile ranking within the column .
R merge two different time series with same date
I did mention that " How can I merge TS1 and TS2 either using zoo or Python-Pandas ?
In pandas , you just join on the index : #CODE
I'm importing this into pandas because I need to be able to join this data with other DataFrames and I need to be able to query for stuff like : " get all legs of variant 1 for route_id 5 " .
Alternatively , you could use ` apply ` to split each variant on commas : #CODE
Thus , to avoid possibly complicated regex or a relatively slow call to ` apply ` , I think your best bet is to build the DataFrame with one integer variant per row .
This is pretty much what I was hoping it wouldn't be :-/ I can avoid the string manipulation by doing that when I'm parsing the data and instead of inserting a string " 1 , 2 , 3 , 5 " into the variant column I'd split it during parsing and insert a list or a tuple instead .
I did some more work and this works more or less , I have yet to figure out how to properly align the labels , I still do to like why I have to have a numeric x axis , when it is really categorical ( again ggplot2 deals with this on its own ) - but ar least this can be done in code #CODE
Everything works fine if I replace ` df [ ' p_hat '] .plot() ` with ` plt.errorbar ( df.index.values , df [ ' p_hat '] .values , yerr =d f [[ ' low_delta ' , ' high_delta ']] .T .values )` , but I would like to make this work in pandas or at least know why it doesn't .
I have considered ` groupby ` but I can not get the way to aggregate taking into account the condition of preferred values .
The issue is with trying to insert null's .
The issue is with trying to insert null's .
However the more Pandas-esqe approach is to normalize your data ( no matter how many different trials you have ) to something like this : #CODE
I suspect it has to do with the double mask I am running ( also indicated below )
I think it is this double mask that is causing the memory to explode .
do you really intend to be doing a join between frame5fin and frame5 at the end of the loop ?
Maybe that should be an append ?
does the join do what you expect when you test it with a subset of data ?
When you join these Pandas tries to match every value in every column .
As ` frame5in ` gets bigger , this join slows down considerably .
If you want to keep missing values you might consider using the append method then after you're done looping you can take the result and reindex it to add in dates with no data .
However that code is a join , not appending to the dataframe ` frame5fin ` .
When you join both of these you get a dataframe with an index ranging the union of the two indices ( in this case June 13,199 4 to Aug 1 , 2014 ) and it will put nans in the second column for anything before May 13 , 1998 .
Just use ` head ` and ` tail ` and ` concat ` .
the header does not automatically align .
Or use a dummy character and replace it at the end : #CODE
Applying a custom groupby aggregate function to output a binary outcome in pandas python
Is it because the multiindex works only if it's sorted ?
You should be able to use ` loc ` and ` np.repeat ` , as done [ here ] ( #URL ) -- could you confirm that I'm reading you goal correctly ?
Mask array entries when column index is greater than a certain cutoff that is unique to each row
I want to efficiently mask a large array with several hundred thousand rows and ~500 columns wherever column index is greater than ` cutoff [ i ]` , 0 = ` i ` number of rows .
Now I basically want to create a ` mask ` with ` mask [: , cutoff :] =True ` , but of course this gives me error .
Transform slow pandas iterrows into apply
This is slow , but I am not sure how to translate it into something using apply .
You can use ` apply ` function , but you need to do extra work here ( just to simplify the work ) .
1 ) Using slicing ( with ` .loc ` or ` .ix ` if using MultiIndex or slicing on both axes ): #CODE
You can then change the boolean values to ' pass ' or ' fail ' using ` replace ` : #CODE
Use ` map ` and ` lambda ` can achieve what you need : #CODE
Note that using ` map ` will become much slower than the ` numpy.in1d ` method , when data dimension gets large : #CODE
Pandas expanding window with min_periods
I want to compute expanding window statistics , but with a minimum number of periods of 3 , rather than 1 .
While I've read about " shift " and other functions , I'm not entirely sure how to tie it all together .
Insert columns with boolean value to indicate the sequences of data that passes the filter : #CODE
You could reset the matplotib backend to one that does not require a display , such as AGG , PNG , SVG , PDF , PS , Cairo , or GDK : #CODE
@USER -L I don't want to replace it with an integer value .
I want to replace it with an empty string .
Well you can use ` loc ` and change just the values ` 1 ` with mapping ` C ` with the newly mapped dictionary key of ` C ` : #CODE
And apparently using ` loc ` is more efficient in this case .
Also you can use ` map ` and ` lambda ` , like this to achieve the same result , this is more efficient than your attempt : #CODE
What is the trick behind the map function in Python ?
What trick the map function uses to be so fast ?
The right-hand side of ` df [ ' out_col '] = map ( my_func , row.col1 , row.col2 , row.col3 )` is one value ( dependent on ` row `) .
It is not ` map ` that is fast , but ` iterrows ` that is very slow .
You can use ` itertuples ` and will get something almost as fast as ` map ` , but as @USER says , you should try to see if you can apply this function on the whole columns .
Dear JD , I dont want to create the custom column in the dataframe gain.I want to apply the where condition for the custome column
Python Pandas , aggregate multiple columns from one
what does .agg ( len ) do ?
yep , ` .agg ( len )` uses ` len ` as the ` aggfunc ` and return the length of each group , which essentially is the count .
Series ( for example , columns of a DataFrame ) have a convenient ` map ` method .
Then go through the ` rno_cd ` column , and apply a function that transform the data .
You can use ` apply ` and function ` tranform ` where you can verify whether x is a key so you get the values using your dictionary ` D [ x ]` if it's not the case , you just return `" unknown "` #CODE
How to join two panda series without duplicating keys
How can I merge them to get the following result : #CODE
( the first " wrong " hist plot needs around one minute ) Any idea , what I can do ?
Just remove all values ( replace by NaN ) which are not the minimum .
I can also transpose the dataset if the operation is easier per column .
Just remove all values ( replace by NaN ) which are not the minimum .
Is there a way that I can apply a function to a pandas dataframe that returns a list for each row that it is applied to , and then take that list and put each item into new columns of that existing dataframe ?
Right now the return of the apply function is a list of lists each of the inner lists is a 9 item list like that shown above .
I am fine putting the response into a new dataframe such as the below , but I haven't figured out how to get the apply function to write to each new row for each return or get the list of lists in to the right form .
You could use the ` diff() ` Series method ( with ` fillna ` to replace the first value in the series ): #CODE
I would like to group these densities by the outer level of the MultiIndex ( expenditure ) , and draw the different densities for different levels of expenditure into the same plot .
My initial approach was to merge the different kdes by generating one ` ax ` object and passing that along , but the accepted answer inspired me to rather generate one df with the group identifiers as columns : #CODE
Then group by ` expenditure ` , iterate through each expenditure , pivot the data , and plot the kde : #CODE
Then you could just append these to your existing DataFrame to fill the missing , like this : #CODE
Alternatively , if you a DataFrame with the pairs you ' should ' have , ( a 1-5 , b 1-4 ) , you could merge that against the data to fill the missing .
Apparently you get two figures because ` m ` is the context from which you call every map drawing command , and there after you draw in current active figure using ` plt ` based plotting commands .
replacing my plotting functions withe snippet you sent plots the map without contours nor scatter plot .
Is it possible to extrapolate to at least fill the map ?
You could then interpolate further away from the original dataset ( that is , extrapolate ) , but check results , because the farther away , the more error you get .
You're almost there , but Basemap can be temperamental , and you have to manage the z-order of plots / map details .
Also , you have to transform your lon / lat coordinates to map projection coordinates before you plot them using basemap .
I was able to get contours on a map in r but I understood that one could not extrapolate ( to fill the domain ) in r.Though am still not extrapolating I think I can be happy living with python .
@USER YMMV means ' your mileage may vary ' that you might feel differently about the colour map and line width choices I made , and that you should feel free to experiment with these .
I like this structure of data before ` apply ` ing , before I am usually able to just ` df [ ' someNewColumn '] = df.apply ( ... )` .
@USER , ` level=0 ` will give the same result as above as you have only 1 index by the time you do the ` apply ` . the ** ...
** is exactly from the output as you're trying to apply the ` ...
It's not obvious to me which version of pandas you're using , but your apply does not work for me at all .
I read his code too quickly and didn't notice the apply was on the grouped data .
If the OP wants to normalize the ` coef ` column based on the mean of the groups , the solution will be a little more nuanced .
So first the merge .
Now let's drop them , and get rid of the marker column .
I already know about the np.isfinite and pd.notnull commands from this question but I do not know how to apply them to combinations of columns .
You can use ` apply ` and lambda function where you choose non-Nan value .
I can use this to make a new dataframe for each combination-of-interest and then join them all together .
It seems to struggle with rolling back this version , as I get the error : #CODE
Also , are you sure you want a groupby and apply together ( i.e. not ` agg ` instead ) ?
So if apply a simple function , ` mean ` , to the grouped data we get the following : #CODE
I think what you really want to do is merge ` df2 ` back into ` df `
To merge df and df2 we need to remove the index from df2 , rename the new column , then merge : #CODE
This is such a common idiom that Pandas includes the ` transform ` method which wraps all this up into a much simpler syntax : #CODE
Pandas : use of aggregate with a MultiIndex
I have a question about the correct use of ` agg ` in ` pandas ` .
When I use ` groupby ` and ` agg ` to apply this function to every stock-date time combination ( ` full_book_ask.groupby ( level =[ 0 , 1 ]) .agg ( ctt_ask )`) I get an error : ` KeyError : ' avail_shares '` .
I have also tried the same with the ` apply ` functionality but this raises the error message ` Exception : cannot handle a non-unique multi-index !
The issue is that the first argument that is automatically passed to agg is a series ( for each column in the data frame which is to be aggregated ) and not the dataframe itself .
You can try using ` transform ` as the object passed to transform is the entire dataframe within the group .
@USER Pride , you are correct , I understand now why I can't use ` agg ` .
However , both ` apply ` and ` transform ` give me an error as well : ` Exception : cannot handle a non-unique multi-index !
I guess there is a problem with the use of ` ix ` but I don't know why or what I should do to solve it .
For this I would have to transform this data into something much bigger : #CODE
Then I could transfer this into a series and be able to resample it : #CODE
finally resample and plot : #CODE
I hoped their would be some line of code which I could insert at the top .
I have two CSV files that I would like to merge .
1 ) One way to perform such a merge / join operation without loading the files into memory
I realize I can do all my operations separately and merge them , and that is fine if I am using python , but when it comes down to choosing a tool , any line of code you do not have to type and check and validate adds up in time
I think you're looking for the agg function , which is applied to groupby objects .
As an R user I was amused to see that the ` agg ` -function is not very much like the R aggregate function .
OP asked for a way to apply multiple aggregate functions at the same time .
I read the agg documentation and I have a further question . in deployer I can specify both a name for a new column and what column of the original it applies to .
In agg it appears you can do either but not both ( or an example is not provided to do that ) - is there a way to do that ?
There ought to be a metric you can apply that takes a baseline picture of memory usage prior to creating the object under inspection , then another picture afterwards .
but way faster than stack #CODE
One naive idea is to copy the array , and iteratively replace the top values with ` NaN ` grabbing index as you go .
what you want to use is ` stack ` #CODE
and then aggregate over all rows in the reduce phase .
step 3 ) truncate time part in seconds
If you are dealing with a `` DatetimeIndex `` , you can use the ` normalize ` method .
Instead of using ` datetime.datetime ` , use ` datetime.date ` and it will automatically truncate the hour / minute / second for you .
I think you are looking for the ` replace ` method ( see docs ): #CODE
If you want to reset the full time part , you specify all parts in ` replace ` : #CODE
You try do a merge on both dataframes , which should return the ( set ) intersection of both .
I wanna get strings from Series that was drop duplicates .
You can use eval ( "" [ 1.5 , 2.5 , 3.5 ]"") , but I hear it's bad practice .
You can map your lists to strings by using `" , " .join ( your_list )` given that you only use floats .
Next , use the apply function in pandas to apply the function - e.g. #CODE
I have verified no additional whitespaces are in the string with the strip function .
AttributeError with an outer join in pandas 0.15.1
Experimented a bit with the stack and unstack methods but never got what I was looking for that way .
You can use ` all() ` ` any() ` ` ix [ ]` operators .
I think so , it seems to align to the records format shown in the [ pandas documentation ] ( #URL ) , i.e. [ { column -> value} , ... , { column -> value } ]
Python Pandas drop columns based on max value of column
Pandas DataFrame insert column based on values in lists
We consider it a bug as it is inconsistent with the operation of creating a single index , and only occurs with multiindex .
Inspired by Pandas : drop a level from a multi-level column index ?
When i ran your code it looked good except it also seemed to Drop US Coke Week 4 which I need to maintain as it is still in sequence to the preceding week .
instead of a bool
How can I merge all of them in the same figure ?
However , when I try to merge df1 and df2 , RAM consumption goes directly up to almost 100% and a a system freeze results .
The merge command : #CODE
I suspect your join is going many to one or something .
I tried to set up an example with 6 columns and I was able to do the join with 50,000,000 records on my MBP in about 2 minutes .
then I do the merge : #CODE
Trying to merge then resultied in a memory error .
` TypeError : cannot astype a datetimelike from [ datetime64 [ ns ]] to [ bool ]`
rolling polynomial regression in pandas
so far I'm able to calculate rolling coefficients of a simple regression ` ( Y= coef1 * A + coef2 * B )` like this : #CODE
in statsmodels I can do a polynomial regression , but there's no rolling window option : #CODE
how could I ' mix ' the 2 and have the rolling coefficients of this polynomial regression ?
Specifically I have a date , I want to take the date and run off to a service and return the Day of the Week , and insert it into that row in a new column :
but I don't know how to best iterate over the dates in the DataFrame and insert the function return .
Well , with a transpose on the original frame and the result this actually works
Finally we must replace the obtained Series with ` value ` if ` col == " E "` and ` value == False ` .
You can't apply a condition on the index of a Series , thats why you need the ` reset_index ` first .
But when I try to use the asof method with a float , I get a TypeError : #CODE
Although this is not stated clearly in the docs , I think ` asof ` only works with a DatetimeIndex , and not with other index types .
@USER : I think you are right about the ` asof ` being limited to DateTimeIndex .
Pandas ( bug in 0.14 ? ) Int64Index : " intersection " with repeated indexes
It seems pretty dramatic , and I am relying on this ` intersection ` all over the place .
But unless I apply the method row by row ( way too slow ) , it fails to do this .
Since index positioning in Python is 0-based , there won't actually be an element in ` index ` at the location corresponding to ` len ( DF )` .
You need that to be ` last_row = len ( DF ) - 1 ` : #CODE
Pandas - intersection of two data frames based on column entries
You can merge them so : #CODE
To drop NA rows : #CODE
I am not interested in simply merging them , but taking the intersection .
Changed to how= ' inner ' , that will compute the intersection based on ' S ' an ' T '
Also , you can use dropna to drop rows with any NaN's .
Perhaps it's possible to insert some " post-processing " to fill in the gaps .
To get the filled in months , you can reindex ( this feels a little hacky , there may be a neater way ): #CODE
and then you can reindex by the period range : #CODE
and reindex .
@USER that's what the reindex is needed for !
I want to truncate this to show only HH : MM : SS ( e.g. 00:00 : 00 to 1:14 : 11 ) .
Join / Merge two pandas dataframes and filling
I want merge / join the two frames by time .
The ` how= ' left '` will have the merge keep all records of df1 , and the ` fillna ` will populate missing values .
Isn't this the default behavior of a left join ?
creating a pivot or summary of text data
And I am trying to pivot it to look like this ( header row doesnt matter much to me ): #CODE
I can only find examples but not the documentation for the agg function itself .
( Slightly more pythonic , if you wanna keep the order of files : drop the ` count ` variable outside and do ` for count , file in enumerate ( filenames )` )
Not sure exactly what you want as output ( e.g. if you want to transform or not ) .
This is a transform type of operation .
IOW . seems you wanted to calculate diff betwee 4 and 2 ( and 3 is in that group ) .
For example I am just trying to create a mask .
Now the mask works .
calculating rolling mean pandas with time-weighted values
New to pandas , and I'm trying to get a rolling mean with a fixed window size .
My idea was to then apply the rolling mean on this time period .
Sounds like you want to resample everything in a 90s window .
How can I fix this - interpolate the values ?
Pandas provides a method for DataFrame and Timeseries named ` resample ` .
so we can do ( to resample with 2 hours sampling period ) : #CODE
though you dont ' really need to do this , e.g. resample will call this to convert string inputs .
How to do a left inner join in python / pandas ?
Also , this isn't really a join operation ; it's a selection .
What I'm trying to do is what this website , #URL calls " left excluding join " .
Joins are used to align rows and columns from different tables .
A true left join would look like this : #CODE
Doing a right join : #CODE
@USER I know -- my point is that since you don't actually want a join operation , you should edit the title of the question to better reflect what you actually want .
Also , you can use ` .dropna() ` to remove those rows or use a ` right ` join .
Is there a way to replace the time-stamps in the original data-set with the new group's time-stamp in an efficient way ?
I want to be able to append to a ` .txt ` file each time I run a function .
Is there a way to append the output in a format that can be more easily imported into pandas
Second , ` ix ` has been more or less deprecated .
Use ` loc ` and ` iloc ` instead .
How do I apply a lambda function on pandas slices , and return the same format as the input data frame ?
I want to apply a function to row slices of dataframe in pandas for each row and returning a dataframe with for each row the value and number of slices that was calculated .
What I want is to apply lambda function f from column 0 to 5 and from column 5 to 10 .
I was thinking that I could do the same for the second slice ` b = pandas.DataFrame ( f ( df.T.iloc [ 5 :: , :]) ` and then concatenate both frames , and than transpose again .
However , ` concat ` takes lists or dictionaries not DataFrames ..
I want to apply the function to the slice on columns 0 , 1 , 2 , 3 , 4 and then also on 5 , 6 , 7 , 8 , 9 .
@USER , you're right , I got mixed up with ` loc ` , answer edited , and Yes , you can just reassign that at one go
it worked and now it doesn't apply any kind of calculation , even when I take exactly your code ..
how do I transform a DataFrame in pandas with a function applied to many slices in each row ?
I want to apply a function f to many slices within each row of a pandas DataFrame .
I thought I could maybe use a transform function for this , but in the pandas documentation I can only see that applied to a dataframe that has been grouped and not on slices of the data ....
If you're applying the same function to all of the groups , why not just apply it to the whole dataframe ?
does the function aggregate the values in some why ?
But yes , one of the things I want to do I guess is transform matrices into same-size output matrices , not so much aggregating- but mostly I need to apply functions to slices of the rows of my input matrix ..
I'll be trying to apply your answer to my needs .
Read my data from file and trying to apply indexes to the ( 43 , 49 ) df .
Another way is to use the dt accessor ( new in Pandas 0.15 ): #CODE
Other way ( probably was ` resample ` does under the hood ) #CODE
However , looping their indexes which are basically lists and write to file , will be much more efficient and quicker than ` reset_index ` and ` concat ` in OP use case .
I merge the dataframes into a single dataframe , including some processing .
Artifact when plotting multiindex pandas dataframe
I have my data organized into a multiindex dataframe .
Well , you could insert a ` np.nan ` value between the two data sets , but that's not the standard way to handle this problem .
pls read the extensive docs : #URL sounds like you want append which is not possible with a fixed store - not enough code to tell what you are actually doing
return ( seq [ #URL + size ] for pos in range ( 0 , len ( seq ) , size ))
setting levels apriori when using factorize in Pandas to cover missing cases
I understand how to use factorize to encode levels of a factor , such as " L " and " W " ( for wins and loses ) into numeric values , such as " 0 " and " 1 " : #CODE
I need some way to preemptively declare the fact that there are 3 different levels when I create the dataframes , and map the correct numeric value to the correct level .
I tried merge with different options but without success .
and viola , merge is working like a charm : #CODE
Once you have the boolean ` mask ` , you can select the associated rows using ` df.loc ` : #CODE
Note there are many other vectorized str methods besides startswith .
If you don't want to change your index , another option would be to index by ordinals instead of by booleans : ` df.iloc [ np.where ( mask ) [ 0 ]]` .
I am trying to translate the following SQL query to run on a large pandas HDFStore : #CODE
Sorry but how would I apply this for a dataframe ?
I'd like to get the date and time in a datetime format to merge it with other data .
as indicated below The mask should receive a number such as 227
This code generates the error ' Int64Index ' object has no attribute ' apply '
Index types don't have an ` apply ` method , but ` Series ` does .
To apply a function to your index , you can convert it to a series first , using its ` to_series ` method : #CODE
You should probably just join the dataframes .
` n = 330000 ` is pretty absurd for stack overflow .
Just replace n=330000 by n=11 .
Having said that , using ` for i in range ( len ( total_val_count.index ))` -- or , what amounts to the same thing , ` for i in range ( len ( total_val_count ))` -- is not recommended .
I am trying to concat two dataframes :
You could do ` ABFluid.index = AB1.index ` before the concat , to make the second DataFrame have the same index as the first .
Adding up all columns with multiindex on axis 0
I get the error ` TypeError : the label [ 0.5 ] is not a proper indexer for this index type ( MultiIndex )` while I won't get this error when I create a dataframe myself with multiindex .
The aim here is easy retrieval of point coordinates for any combination of two places ( ` a , b -- ( 1 , 2 ) , ( 3 , 4 )` etc . ) , but I have no idea how to calculate this , or whether I could use a MultiIndex to do it .
Now I need to replace the timestamp for each row of my table with the actual time to yield : #CODE
Inner Join the separate dataframes produced in a .
You want then to join the data within in s_id ?
To solve your second problem you need to group the data by s_id and transform the data according to your requirements .
and then pass it to ` apply ` on data grouped by s_id : #CODE
So ` apply ` passes each chunk of grouped data to the function and the the pieces are glues back together once this has been done for each group of data .
One possible method without using ` regex ` is to write your own function and just ` apply ` it to the column / Series of your choosing .
You could apply that regular expression to the elements in the data .
Pandas : Feeding index values to apply
I'm having problems when trying to use apply on the result of a groupby operation .
And I now would like to use apply , but I need to feed it id1 , which is part of the index , so I get an error when I try to do the following : #CODE
[ BTW , I have also tried to merge df1 and df2 in one table ( so that each row in df2 has a field with the corresponding col1 , col2 and col3 from df1 ) , but when I do the groupby and sum() it aggregates col1 , col2 and col3 values ( which I don't want )]
This should be a pretty simple question , but I'm looking to programmatically insert the name of a ` pandas DataFrame ` into that ` DataFrame `' s column names .
run the replace : #CODE
You have to pick out which column you want to apply ` nunique() ` on .
To answer your question about why your recursive lambda prints the ` A ` column as well , it's because when you do a ` groupby ` / ` apply ` operation , you're now iterating through three ` DataFrame ` objects .
Applying an operation to that will apply it to each ` Series ` .
I am using groupby and apply , so I am not explicitly pulling the groups , which is why i need to do this .
so you can see in the resulting printed output that each iteration of the ` apply ` gets all columns of the input dataframe .
I'm not sure how to grab a tuple of keys from an ` apply ` but I can from a loop : #CODE
so are you asking how to write a function which , when you apply it to grouped data , can see the keys ?
I get it now . not sure how to do this from apply , but I added an example of how to do it from a loop
Parallelizing apply function in pandas python . worked on groupby
IIUC , you can build a number-to-colour dictionary and then use the ` replace ` method .
An all Pandas solution would be to make the color map a pandas dataframe and then join it to the primes dataframe : #CODE
You can avoid ` map ` and ` lambda ` altogether and use ` isin() ` , which ends up working more like the typical Python idiom of ` if item in [ ' a ' , ' b ' , ' c '] : ` #CODE
Still trying to figure out how to not have data paths connect across ( other than interpolate nulls )
If you can't translate this concept in a Pandas frame context , you should ask for someone else's help because I'm not a Pandas man ...
where n = m * len ( list with dictionaries ) ( where length of each list in ' data ' = m )
The stretching line has caused a multiindex with an irrelevant column - this removes it
Finally merge on the original index and get desired DataFrame
Unstack doesn't return a dataframe
` unstack ` works on the index , so you have to first set it as index : ` dt.set_index ( ' var ') .unstack() `
@USER @USER means that you can replace the ` datetime.timedelta ( minutes=1 )` by ` pd.Timedelta ( ' 1 minute ')`
Just use ` reindex ` , for example : #CODE
Alternatively , both Series and DataFrame objects have a ` reindex ` method .
For instance , you can insert new values into the index ( and even choose what value it should have ): #CODE
Yet another option for both Series and DataFrame objects is the ever-useful ` loc ` method of accessing index labels : #CODE
I only interest for specific function , which transform all values to the log-values for the whole column .
The same " apply " pattern works for SFrames as well .
you problem is that the webserver user ( ` www-data ` I presume ) has no .matplotlib config file in it's home ( which is ` / var / www ` , the web root ) . easy fix is to place a .matplotlibrc into the application dir and set backend that doesnt require a display ( like ` Agg `)
Changing the mpl config to use a different display driver like ` Agg ` does not stop matplotlib from trying to create a config directory .
I would do that probably through a join .
I'd join the existing dataframe to one with an index that had all the values I wanted on the x axis and do the join with the ` how= ' outer '` option .
I'm guessing that I can't apply a sort method to the returned groupby object .
There may be a pandas way to get around this , but you could also write a quick script to append some ` , ""` for each column that a line is lacking .
Drop row in pandas dataframe if any value in the row equals zero
How do I drop a row if any of the values in the row equal zero ?
This ` str ` attribute also gives you access variety of very useful vectorised string methods , many of which are instantly recognisable from Python's own assortment of built-in string methods ( ` split ` , ` replace ` , etc . ) .
You ALWAYS want to append to a list , then concat in one go . see the end of the first section : #URL
Then append that to a main dataframe .
hey JD , see jeff's comment on my post . apparently pandas ` concat ` function is smart enough that it's faster to append to the list ( #URL )
For each stadium , I want to generate a series that gives a rolling 7 day sum of visitors .
I think the data needs to be resampled before applying the rolling sum .
Now we can apply the rolling sum .
If you want to modify things it is much better to simply pass the frame and a mask around .
then produce a chain of these and concat all at once .
2 ) If I have to pass a mask to your second ` b ` ( in case the slicing index is defined outside of ` b `) do I understand correctly that second ` b ` needs to be modified according to the first ` b ` ( i.e. mask needs to be passed along with ` df `) ?
python - insert intermediate values into Pandas DataFrame
I need to insert intermediate rows for every 1 to -1 and -1 to 1 transition .
Find 1 -> -1 and -1 -> 1 transitions , count them , change index values , reindex with full range to introduce missing rows #CODE
You can define a function which returns your different states " Full " , " Partial " , " Empty " , etc and then use ` df.apply ` to apply the function to each row .
Then using apply : #CODE
unstack " mullti-indexed " column| in pandas
the idea is to first stack the first level of the column to the first level of index , and then swap two indices ( pandas.DataFrame.swaplevel ) #CODE
Pandas Filter function returned a Series , but expected a scalar bool
TypeError : filter function returned a Series , but expected a scalar bool
I am creating the dataframe by concatenating two other frames immediately before trying to apply the filter .
i'm not sure about the issue u mentioned regarding the interactive console . technically speaking in this particular case ( there might be other situations such as the intricate " import " functionality in which diff env may behave differently ) , the console ( such as ipython ) should behave the same as other environment ( orig python env , or some IDE embedded one )
so when u try to using filter to apply the lambda function upon x , x is actually one of those dataframes : #CODE
For non-pertinent technical reasons I am only interested in every Nth value of the output of these pandas rolling mean and rolling std calls , so I am looking for away to only compute every Nth value .
What I'd like to do is : for each day , apply a function that takes the sum of all logvol between 14:40 : 00 and 15:00 : 00 .
I have a feeling it has to do with the resample function but I'm not sure exactly how to use it .
How would I write a function to map this ?
In this scenario , you'd replace the last line with ` raise TypeError ` , but ... meh .
For real fun , try this with ` np.float64 ` or ` np.bool ` and everything works fine , because they're actually subclasses of ` float ` and ` bool ` .
It seems shift doesn't use the order of the dates , instead it uses the index order to shift .
If you want to shift it in a sorted manner , first sort it before the groupby .
If you want to shift it in a sorted manner , first sort it before the groupby .
No , you will need to write code to transform that into some format that pandas can read directly ( like CSV ) .
You could use ` diff ` followed by ` cumsum ` to give each group of Trues and Falses its own number : #CODE
Calling ` diff `` on those results in an empty df , which results in no groups .
I want to replace ' ABC ' and ' AB ' in column BrandName by A .
Probably the easiest way is to use the ` replace ` method on the column : #CODE
Plot multiple boxplot in one graph in pandas or matplotlib ?
Then pass that axes to the second call to ` boxplot ` using ` ax=ax ` .
Create a DataFrame ` df ` from the above array and pivot on the third column using ` pivot_table ` .
pandas rolling sum of last five minutes
I've tried reading through the Merge , join , and concatenate - documentation , but can't get my head around how to do this in Pandas .
You can combine ` pandas.DataFrame.any ` and list indexing to create a mask for use in indexing .
I want to replace the NaN in the the top right with the same values as in the bottom left : #CODE
I know I can generate the upper right version with " m.T " but I don't know how to replace NaN with non-NaN values to get the complete matrix .
` m [ np.triu_indices_from ( m , k=1 )]` returns the values above the diagonal of ` m ` and assigns them to the values values above the diagonal of the transpose of ` m ` .
Pandas merge giving error " Buffer has wrong number of dimensions ( expected 1 , got 2 )"
I am trying to do a pandas merge and get the above error from the title when I try to run it .
I am using 3 columns to match on whereas just before I do similar merge on only 2 columns and it works fine .
Is it possible to merge on three columns like this ?
Is there anything wrong from the merge call here ?
pandas index.asof with multiindex
I've put it in a dataframe with a multiindex : #CODE
I'm trying to find the latest date before an arbitrary cut off ( say before 2014-10-31 or 2014-09-30 ) in the series for each id . index.asof or Series.asof seems to be what I want but I can't figure out how to use it with multiple indices .
There is no reason the data has to be in this multiindex structure , just seems to make sense given I have a timeseries for each id .
I'm trying to find the latest date before some arbitrary cut off .
By the way it seems ` .asof ` when applied to the ` groupby ` dataframe evaluates the whole index and not the index of the group , so your version with ` asof ` does not work as expected : #CODE
Python pandas tz_localize throws NonExistentTimeError , then unable to drop erroneous times
Ok I've used the following code which has worked to drop the offending times :
can you post the row for " 2006-04-02 02:00 : 00 " and for some of the rows you are trying to drop ?
Your drop command doesn't look like it should work based on the description in the docs .
To get rid of the offending times , I would create a mask on the dataframe , ie : #CODE
Probably there's a way to make drop work too .
This works to drop rows .
Is there a way more concise than adding a mask for each time window ( eg . mask1 , mask2 , mask3 ) ?
mask = (( df.foo < 1 ) & ( df.foo > -2 )) is the general idea .
Should change ix to index
Calculate rolling correlation with pandas
I would like to group those stocks by PERMNO and calculate the rolling correlation between the stock return ( RET ) for each PERMNO with the market return ( vwretd ) .
If you want to join them all together after processing ( this might be confusing to others , by the way ) , just use ` concat ` after processing groups .
I work in archiving these files ( and use Python ) , so feel free to drop me a line if you have future questions .
convert to radians and shift everything up 1 row so that we can look ahead #CODE
` apply ` that function to each row , save in the original dataframe #CODE
pandas multiindex selection with ranges
pivoting pandas dataframe into prefixed cols , not a MultiIndex
I can pivot this table like so : #CODE
This is what I want except that I need to collapse the ` MultiIndex ` so that the ` company ` is used as a prefix for ` share ` and ` price ` like so : #CODE
What is a better way of accomplishing a pivot resulting in a columns with prefixes as opposed to a ` MultiIndex ` ?
The above version only handles a 2D ` MultiIndex ` but it could be generalized if needed .
Unexpected KeyError Pandas while trying to aggregate multiple functions
There is no ' r1 ' column in your dataframe , so you can not aggregate something that doesnt exists
Your program fails because there is no ' r1 ' column in your dataframe , so it can not aggregate something that doesnt exist .
You might start by looking for conditional ` apply ` - there are plenty examples on how this can be done .
Do I need to use merge , concat , or join ?
Also I think by using map , the operation is no longer vectorized and thus slower than could be ?
Pandas ' inbuilt ` map ` is explicitly for vectorised calculation across a series .
Again I prefer to use the explicit ` map ` method when transforming one series into another series .
Thanks for letting me know about ` map ` being vectorized .
I apply str.split to a df to create that list
Most of the time when you have non-scalar quantities in a Series or DataFrame you've already taken a step in the wrong direction , because you can't really apply vector ops .
so two choices here . the first one is to correct the str format ( modify " 24 " to " 00 ") , then apply the { pd.to_datetime } func : #CODE
I know about ` scipy.interpolate ` mentioned in this article ( which is where I got the images from ) , but how can I apply it for Pandas time series ?
Interpolate the data using ` .interpolate ( method= ' cubic ')` .
1 If you're getting an error from ` .interpolate ( method= ' cubic ')` telling you that Scipy isn't installed even if you do have it installed , open up ` / usr / lib64 / python2.6 / site-packages / scipy / interpolate / polyint.py ` or wherever your file might be and change the second line from ` from scipy import factorial ` to ` from scipy.misc import factorial ` .
To achieve this we can perform a left merge first : #CODE
Now what if my number_y can have null values going into the merge ?
Is there another way of detecting if the join was successful or not ?
The key to this was to detect when the rhs is not present in lhs , in that case you'd still have a column clash so I guess you could filter where all columns matched in values to your filtered merged df , e.g. after ` merged_null ` step , rename `' number_x '` back to `' number '` drop column ` number_y ` and you could do a reverse merge : ` merged_null.merge ( data1 , on =[ ' id1 ' , ' id2 ' , ' number ' , how= ' left ') this will ensure now that only the valid rows remain
pk becomes a string of previous key , and current key with a ' to join them .
I am using ` pandas.DataFrame.resample ` to resample random events to 1 hour intervals and am seeing very stochastic results that don't seem to go away if I increase the interval to 2 or 4 hours .
Typically for nested data types like this , I merge the inner data with the outer .
Pandas : Generate a histogram / pivot against timeseries data
ii ) a count of results split by period and a second column value ( i.e. a pivot ? )
And how about my part ii ) Perhaps there's a method using groupby and / or pivot ?
And for ( ii ) you can use ` pd.pivot_table ` , once you've created a ` month ` column which holds the year-month for you to pivot with .
Perhaps check what kind of index you have ( i.e. make sure it's a ` Float64Index ` or if it's something that would translate to categorical ) ?
I get " TypeError : cannot compare a dtyped [ object ] array with a scalar of type [ bool ]"
I have a pandas dataframe with a multiindex .
I think you are a little confused here , the reason you need to reset the index is because if you didn't there would be nothing to match the mask index values against as pairs df is using your currency.pair string as the index value . the length is not the issue in this case , it's the fact that the index values cannot align with your mask index
` pairs `' index has a ` dtype =o bject ` , while ` mask ` has a ` type=int64 `
Replacing the index of mask with the index from pairs we get the expected behaviour .
Your ` apply ` approach would work too if you used ` x ` instead of ` df.radon ` : #CODE
( Non-rhetorical , BTW -- I can't remember what methods are nan-aware and what ones aren't . I don't * think * ` len ` cares , but I wouldn't bet twenty bucks on it . )
Starting with this dataframe I want to generate 100 random numbers using the hmean column for loc and the hstd column for scale
Just do the merge twice , and make one of them based on the sample_id , one based on the sampled_id , and put one column into the other .
You could use ` groupby / transform ` to assign one unique `' Sample_ID '` to each ` ( ' Sample_Artist ' , ' Sample_Song ')` group .
How to drop columns from dataframe having all values equal to False ( Boolean ) ?
Is there any way to drop all 5 of them in one command ?
Pandas replace values
I want to replace all rows in the dataframe which do not contain ' pre ' to become ' nonpre ' , so dataframe looks like : #CODE
I can do this using a dictionary and pandas replace , however I want to just select the elements which are not ' pre ' and replace them with ' nonpre ' .
Use ` unstack ( ' cat ')` , followed by ` fillna ( 0 )` to replace the NaNs with zeros : #CODE
` stack ` moves column level values to index level values .
` unstack ` moves index levels values to column level values .
Use concat and pass ` axis=1 ` and ` ignore_index=True ` : #CODE
Is there reason you don't just store the dfs in a list and then concat them all ?
Aside from speeding up the concat process from 2 hours to 10 minutes , it also solved the problem with the loaded csv - it runs at normal speed now ( for whatever reason ) :)
A much more dirty solution would be to fetch the string representation of the list object and just replace the u .
This is a bit tricky due to the presence of the ` NaN ` , one method would be to sort the columns that have no NaNs and then sort the columns with NaN and concat them together , does that sound reasonable ?
My next attempt will be to reset the indexes on both ` df ` and ` sel ` and then use a join .
Your intuition to use ` join ` is good .
On my Mac this method results in a big stack of plotting windows all stacked on top of each other
map the values on pandas dataframe using the values on the corresponding index in a list
I want to map this DataFrame #CODE
Is there some kind of " merge " or " join " like : #CODE
Then you could use Merge #URL #CODE
You can do a join the following way :
This does an inner join by default , so only the rows from ` data ` with days which appear in the ` days ` frame will be in the result .
And I get an empty dataframe .. this is probably a stupid mistake but I'm not sure I understand why the merge on ' days ' would return nothing .
Ok I was able to merge but now I see all my minute data is missing from the index ( as it was there in the original " data " dataframe ) .. any ideas on how to merge but retain this index ?
Unfortunately , that solution is painfully slow and I'd much prefer something that utilizes nice features in Pandas such as groupby or apply .
` mod [ j ]` for all ` i ` in ` range ( len ( m ))` and ` j ` in ` range ( len ( mod ))` .
As we care only about ' mod ' values that have adjacent ' m's , aggregate over axis=0 : #CODE
If you want to have an object in a DataFrame that displays itself a certain way , you will have to write your own class that uses ` __repr__ ` to display itself the way you want , and then insert instances of that class in the DataFrame .
The DataFrame just stores objects ; the way the objects are displayed is determined by those objects , not by the DataFrame ( except that the DataFrame will cut off the display if it is too long ) .
take a join of two csv files over a common column in python
What I want is to take a join of these two files over ` objectID ` .
What am I doing wrong here and how can I do the join correctly
I think you are looking for a left join , try #CODE
` resample ` is grouping times according to the frequency , and then aggregating the associated values according to the ` how ` method , which is by default taking the mean .
I do not know how can I filter out the two columns from merged output of pandas merge above and form the list of lists form those two columns like above format ?
-> 1286 numticks = len ( self.get_major_locator() ( )) 1287 if len ( self.majorTicks ) numticks : 1288 # update the
in num2date ( x , tz )
343 tz = _get_rc_timezone()
--> 345 return _from_ordinalf ( x , tz )
in _from_ordinalf ( x , tz )
223 tz = _get_rc_timezone()
224 ix = int ( x )
--> 225 dt = datetime.datetime.fromordinal ( ix )
226 remainder = float ( x ) - ix
Length : 744 , Freq : None , Timezone : None
So basically the ` dt ` attribute allows you to access the components of your datetime to perform the comparisons you desire for filtering
I have found workaround which is extremely slow due to the " in python " apply : #CODE
One approach is to sort ` df ` first , and then use ` groupby ` and aggregate with ` head ` and ` mean ` : #CODE
I tried to create a mask : #CODE
I'm pretty sure the fail is in the mask logic I implement .....
you put me on track #URL if I create a column dfmatches [ ' date '] = dfmatches.index.day then a mask = dfmatches [ ' date '] .shift() < dfmatches [ ' date '] I got what I want minus the fact it displayed the 1st day and not the last .
Dunno why shift on index doesn't work the same way
I want the data separated by year and then the same function ( pivot ) is applied to each data set .
So something like ` vals = df.Year.unique() ` then you can just iterate over this and append to a list the pivot tables or add to a dict or something similar
The way I used to solve this was to output to a .csv , read it back in ( which makes it time zone naive but keeps the time zone it was in ) , then strip the + ' s .
Then you can easily concat these ( I suppose with the mark / undo ) , if you really really need .
If you copy the above and use ` pd.read_clipboard() ` I believe you should get a hierarchical index ( or MultiIndex )
In Python , how can I transform a dictionary into a df column , where the keys match the df.index values ?
Consequently you have a scrolling box in a scrolling box , which is not ideal as you have to shift focus between the doubled-up scroll bars to navigate all the way through the data .
This allows one data frame to have a by-variable ( join column ) specified as a factor ( categorical ) and the other to have its by-variable be a string .
Further if an object array was merged in it is allowed , but the resulting character of the returned merge would now be object ( IIRC ) .
How can i apply a function to the dataframes index ?
I think your title is a bit misleading , what you are saying really is you want to apply a function to a specific row using the index value
For label based indexing use ` loc ` : #CODE
How can I use this function to transform one column into multiple columns ?
Another way would be to apply a lambda and concatenate the result : #CODE
@USER my edited answer ( 2nd ) answer is based on that question , the point being it is not something that works automatically , you have to coerce the return type and then merge / concat afterwards
python pandas : case insensitive drop column
I have a df and I want to drop a column by label but in a case insensitive way .
Is there any magic I can apply to the code below ?
One way I could conceive a solution would be to groupby all duplicated columns and then apply a concatenation operation on unique values : #CODE
This obviously changed the number format from datetime into tz datetime .
Calling the apply method via interactive shell finished execution in ~2sec , but strangely , letting the code run as compiled / interpreted on the same dataframe takes more like 15-20 seconds .
I know how to drop rows , #CODE
Moreover , if I try to drop ( just as an example , I need to keep the these rows ) multiple rows #CODE
Use ` startswith ` instead of ` contains ` : #CODE
It can be used as a selection mask , as shown above .
What if I had to replace this string with another string ? for instance sth like : dataset [ dataset [ ' Postcode '] .str .startswith ( " WC1 ") .replace ( " center ")] .
And I don't want to use ` xs = pd.Series ( data=range ( len ( ts )) , index =p d.to_datetime ( ts ))` instead of ` xs = pd.Series ( data=range ( len ( ts )) , index =p d.to_datetime ( ts ))` since I want to use some operation as #CODE
But I don't want to use ` Series ( data=range ( len ( ts )) , index=ts )` .
Actually your new approach is based on matplotlib rather than pandas . your ` xs.plot ( use_index=False )` can be replace by ` ax.plot ( xs.values )` .
thanks , but when I'm doing class_price_df.columns I have MultiIndex ( levels =[[ u'class ' , u'email '] , [ u'Automotive & Gadgets ' , u'Cameras ' , u'Computers & Laptops ' ....... ) how can I get Index ([ u'email ' , u'Automotive & Gadgets ' , u'Cameras ' , u'Computers & Laptops ' ......... ) ?
thanks @USER but I still have MultiIndex ( levels =[[ u'class ' , u'email '] , [ u'Automotive & Gadgets ' , u'Cameras ' , u'Computers & Laptops '
python pandas - sumf if rolling arguments
The problem is that I my conditions are rolling , i.e. first the code should sum the quantities of all deals done on 19 / 11 with Delivery Beg 02:00 .
I'm not sure you're need rolling sum .
If you want then get rolling sum , you can use ` cumsum() ` function on it .
Btw , if looping takes to long . you can use pivot to use ' Delivery Beg ' as a column index and then take the sum for each column .
The objective then , is to map the line-grouped coordinates from the file in the big dataframe .
May be not the most elegant way , but you can merge grouped result with the initial DataFrame : #CODE
pandas pivot table with same rows and columns
I would like to create a pivot table from this dataframe with both the rows and the columns of the pivot table equal to ` df [ ' event ']` .
I would like to get something like ( the agg function is arbitrary . I am concerned with the grouping ): #CODE
Since that doesn't match the index you've said you want , the data doesn't align , and so you get a DataFrame with the index you said you wanted and the NaNs highlight that the data is missing .
And if you use a partially-matching index , you'll get values where the indices align and NaN where they don't : #CODE
Indeed , that is what you've done : ` date ` starts off as ` [ ]` , and then you append one value to it , so the index you're passing to the dataframe is just a singleton list .
Moreover , you do not need to call ` df [ ' Diff '] .apply ( wdw )` , which calls ` wdw ` for each value in the Series ; you can compare whole Series with a ` pd.Timedelta ` : #CODE
Your ` x / np.timedelta64 ( 1 , ' D ') <= 10 ` is cleaner than my ` ( df [ ' Diff '] .values < np.timedelta64 ( 10 , ' D ') .astype ( ' < m 8[ ns ]')` ; I've changed my answer to use your comparison -- it generalizes to Series nicely .
This HDF5 was created directly with pytables , so maybe I will just drop back to pytables and read the arrays there and use pandas to write to SQL .
I have tried ` numpy.histrogram ` ( which returned error : ` TypeError : ufunc add cannot use operands with types dtype ( ' m 8[ ns ]') and dtype ( ' float64 ')`) and ` hist ( series )` ( which returned error : ` KeyError : 0 `) .
Alternatively , instead of calling ` df.to_hdf ` , you could append to a HDFStore : #CODE
Once the array is created , you can use its ` append ` method in the expected way .
I want to replace the values in a column by a calulation ( which is a multiplcation of comlun values ) .
Pandas rolling window function offsets data
The ' center ' and ' freq ' arguments do not appear to change output either way .
So , I am wondering if I need to find a mean value of my data for every window , shift the data to zero by that mean , then run the rolling_window function , and then shift both the original data and the smoothed data back to the original magnitude ..... any other ideas ?
I would like to normalize each measurement in the second dataframe ( df2 ) with its corresponding stock solution from which i took the sample .
SO : Binary operation broadcasting across multiindex
since the index of ` pd.Series ( np.repeat ( 1 , len ( df )))` is ` Int64Index ([ 0 , 1 , 2 , 3 , 4 ] , dtype= ' int64 ')` .
Using a Series is helpful when you need to align the new column with the existing DataFrame using the index .
You can use the function ` repeat ` on Series to duplicate the ` id ` values in groups of three rows , and then use ` pivot ` to reshape the DataFrame : #CODE
How can I insert a dataframe in Excel using xlwings without pywintypes.com_error ?
The problem I encounter is that when I insert the data in the sheet there is an empty line at 5002 , again at 7503 , 10004 ....
Currently I open the csv and insert the lines 15000 at a time , but it takes 90 seconds .
Is there a way to insert an existing sheet containing the data in the workbook I use ?
I've tried to index my Series by the values that aren't part of dt ( which is the variable I assigned to month end range ) but I wasn't able for some reason as I kept AttributeError : ' NotImplementedType ' object has no attribute ' dtype ' when I tried to run : #CODE
While if I replace #CODE
Pandas apply with argument that varies by row
I am attempting to apply a function to each row , where the function takes a ' size ' argument .
but is there a better way using apply functions ?
Why don't you make ` size ` another column in your data , so it is passed as part of the ` apply ` ?
So you can call ` comb ( diff , size )` where ` diff ` is an array of shape ( 225 , 50 ) and ` size ` is an array of shape ( 50 , ) .
Can Pandas interpolate ?
Pandas interpolate data with units
If you want to interpolate a " dependent " variable over an " independent " variable , make the " independent " variable ; i.e. the Index of a Series , and use the ` method= ' index '` ( or ` method= ' values '` , they're the same )
You could then use this timeseries to subset / reindex your DataFrame .
I am unable to specify the minimum size for the index in a to_hdf append operation .
Is it possible to aggregate ( in one step ) the val and group by the index as well as the aggregating_indicator ?
create new column from conditional statement without mask pandas
You can create function and ` apply ` it to your dataset : #CODE
Pandas interpolate NaNs based on different column
Now I would like to interpolate to fill the NaN based on the wind_speed .
The idea is to use ` set_index ` to put the values you want to interpolate at in the index .
Something like ` data.set_index ( ' wind_speed ') [ ' Velocity '] .interpolate ( method= ' index ')` . interpolate doesn't have a ` levels ` argument .
What does ` ValueError : cannot reindex from a duplicate axis ` mean ?
I am getting a ` ValueError : cannot reindex from a duplicate axis ` when I am trying to set an index to a certain value .
However when I try to create ` sum ` index for sum of all columns I am getting ` ValueError : cannot reindex from a duplicate axis ` error .
I don't really understand what ` ValueError : cannot reindex from a duplicate axis ` means , what does this error message mean ?
This error usually rises when you join / assign to a column when the index has duplicate values .
What is the most efficient way to join multiple tables with the same index / col ?
I want to persist this dataframe on disk , so I am thinking maybe there is a better way using command line tools like sed / awk / cut to get a csv going which I can then just load .
I would just read in the data from the CSVs , join , dump to a master CSV .
It's hard to say without you posting some concrete input and expected output but it sounds like the UNIX utility ` join ` is what you're looking for .
I'm doing neither here , I'm suggesting the answer is ` join ` .
To be fair the answer is probably join though !
" What is the most efficient way to join multiple tables with the same index / col ?
Unfortunately merge only takes two tables at a time , so you'd need to nest them or use reduce .
Here's a merge that uses the index : #CODE
Here's a merge that uses the column `' a '` : #CODE
If you need to merge an indeterminate number of dataframes , you can use the ` reduce ` function , found in the functools module , with ` partial ` to set the column to join on ( as well as other parameters you desire ): #CODE
This can be used to apply functions to particular columns as a file is read in .
You could use this to apply a function to the third column .
I routinely deal with tables and join them based on columns , and this joining / merging process seems to re-index things anyway , so it's a bit cumbersome to apply index criteria considering I don't think I need to .
Many functions , such as ` set_index ` , ` stack ` , ` unstack ` , ` pivot ` , ` pivot_table ` , ` melt ` ,
Sometimes we want the DataFrame in a different shape for presentation purposes , or for ` join ` , ` merge ` or ` groupby ` operations .
( As you note joining can also be done based on column values , but joining based on the index is faster . ) Behind the scenes , ` join ` , ` merge ` and ` groupby ` take advantage of fast index lookups when possible .
Time series have ` resample ` , ` asfreq ` and ` interpolate ` methods whose underlying implementations take advantage of fast index lookups too .
So not just lookups , but also merge operations would be faster if index columns are used , right ?
Yes , join calls merge ( in most situations ) .
where the " B " value can be variable len > = 1 .
I am using pandas data frame's map function to apply the function ` toTheExp ` to my column of data already .
YS-L so this would replace the existing month_date column rather than creating another one , with the dates in date format as a date-time obect ?
Generally , anything else other than ` float ` , ` int ` , ` bool ` , ` datetime64 [ ns ]` and ` timedelta [ ns ]` will be regarded as ` object ` .
A little unclear what you want , ` applymap ` is for a dataframe it doesn't make much sense to call ` applymap ` on a series when ` apply ` is specifically for this .
pandas merge with MultiIndex , when only one level of index is to be used as key
I want to recover the values in the column ' _Cat ' from df2 and merge them into df1 for the appropriate values of ' _ItemId ' .
This is almost ( I think ? ) a standard many-to-one merge , except that the appropriate key for the left df is one of MultiIndex levels .
Or is there a better approach to this merge ?
or use ` reset_index() ` , ` merge() ` and then set new multiindex
Python Pandas : How fill date ranges in a multiindex
I have tried different ways such as stack / unstack and reindex but different errors occur .
and create ` MultiIndex ` and ` reindex() ` : #CODE
The issue I foresee is that on the real dataset there will be 5 levels in the multiindex .
Now that Pandas has ` loc ` and ` iloc ` I don't recommend ever using ` ix ` , since its behavior is not immediately apparent from the syntax .
ok thanks unutbu - i'll use iloc and loc from now on for rows and columns .
So given that loc and groupby return new dataframes how does one access the January dataframe is it still just df.loc [ January ] as before in your comment ?
Right now , I am trying to replace a stored procedure with a Python service , and the temp tables with Pandas dataframes .
is it possible to filter the columns as well in the same operation where the are sliced like : " df [ df [ ' category '] ! = ' purple '] [[ ' amount ' , ' freq ' == [ 3 , 5 ]]]"
you can do it like this ` df [( df [ ' category '] == ' blue ') & ( df [ ' amount '] == 5 ) & ( df [ ' freq '] == 2 )]`
Python : how to translate complex SQL aggregate statements into pandas ?
a different aggregate function on each field ( e.g. I need the sum of
Say I have a table with data at the city level and I want to aggregate it by country and region .
Useful , but I'm afraid it's still a bit convoluted when having to calculate , say , 20 aggregate fields .
I find it odd that I can manually replace the ( i ) value with " green " , or " blue " when I copy the line of code and run it separately but it doesn't work in a loop .
Python : merge lists or data frames and overwrite missing values
If you want sequential index , you can apply ` reset_index ( drop=True )` to the result .
Trying to work through Roman Pekar's solution step-by-step to understand it better , I came up with my own solution , which uses ` melt ` to avoid some of the confusing stacking and index resetting .
Output ( obviously we can drop the original samples column now ): #CODE
Get Row and Column Names ( argmax ) for max entry in pandas dataframe
Assuming that all your pandas table is numerical , something you can do is transform to its numpy interpretation and extract maximum locations from there .
However , numpy's ` argmax ` works on the flattened data , so you will need to work around : #CODE
Transform table to numpy data and calculate argmax : #CODE
I think you want ` diff `
Just call ` diff ` : #CODE
How would I drop all the ` NA ` , ` Nans ` and ` 0 ` in the columns so I would get this : #CODE
Another method is to create a series from the columns and use the vectorised str method ` startswith ` : #CODE
To select only the required rows ( containing a ` 1 `) and the columns , you can use ` loc ` , selecting the columns using ` filter ` ( or any other method ) and the rows using ` any ` : #CODE
If you want to drop the date column , you can use #CODE
Pandas groupby strip timezone in index
pandas merge on index not working
When I try and ` merge ` them using
bw.join ( pw ) gives an error ` AttributeError : ' Series ' object has no attribute ' join '` which is why I was going down the path of merging ...
I had forgotten that join was a df only thing
Change the series into DataFrame then it is possible to merge #CODE
Or if the merge is to be performed in a parallel manner ( bw and pw have the same index , same number of items ) .
That is why you can merge after that .
So i have 5 columns in a dataframe that i like merge as a list in an existing column .
the code below perfectly fine , but i need to create a function that will do that for me rather than creating lambda functions for every block of columns that i like to merge .
( I still have other similar columns that i like to apply the same logic on them . ) #CODE
Name : start , dtype : bool
So you should use ` ix ` which allows you to be explicit in indexing by integer value ( or label ) and then specifying the column label you want to set the data : #CODE
The indices are non unique , but will always map to the same value , for example ' a ' always corresponds to ' 1 ' in my sample , b always maps to ' 2 ' etc .
So long as your indices map directly to the values then you can simply call ` drop_duplicates ` : #CODE
EDIT a roundabout method would be to reset the index so that the index values are a new column , drop duplicates and then set the index back again : #CODE
python string replace conditional
I want to know how to replace values in a column using a condition ( a DataFrame in Pandas ) .
I want to merge several strings in a dataframe based on a groupedby in Pandas .
I don't get how I can use groupby and apply some sort of concatenation of the strings in the column " text " .
You can groupby the `' name '` and `' month '` columns , then call ` transform ` which will return data aligned to the original df and apply a lambda where we ` join ` the text entries : #CODE
EDIT actually I can just call ` apply ` and then ` reset_index ` : #CODE
Then slice data2 and append row to data2sub based on selection : #CODE
By implementing a simple ` getnearpos ` function from a previous Stack Overflow answer : #CODE
Is it possible to use percentile or quantile as the aggfunc in a pandas pivot table ?
Simply do : ` selected_names = np.random.choice ( df.name.unique() , 2 , replace = False )` followed by ` df [ df.name.isin ( selected_names )]`
merge two txt files together by one common column python
how to read in two tab delimited files .txt and map them together by one common column .
How to shift entire groups in pandas groupby
I would now like to shift the whole thing down by n groups , so that their current order is preserved .
The desired output for a shift of n=1 would be : #CODE
a shift of n=2 should be : #CODE
I have been messing around with groupby / transform / apply but haven't gotten anything to work so far .
If I groupby and then shift , it shifts each group giving the output of : #CODE
I can think of an alternative way to do it with ` replace ` .
To shift by 1 group : #CODE
To shift by more than one group , you just need to shift column ` b ` of ` x ` .
If you want to shift by ` n ` groups , you need to shift ` x.b ` an additional ` n-1 ` times .
Just insert the line #CODE
It essentially uses columns ` a ` and ` b ` for the appropriate find / replace lists .
Your way to replace it with a higher date seems OK to me .
EDIT I've even checked by calling ` duplicated ` on the dataframe after the drop , and all rows show ` False ` .
If you cannot show your data , please do at least copy the code you use when trying to drop the duplicates .
With the resulting index , you can reindex .
I'm trying to merge two dataframes which contain the same key column .
Does anyone know how to get pandas to drop the duplicate columns in the example below ?
would adding more columns to merge on still give you the desired result ?
The ` _x ` and ` _y ` columns originate from the left and right frames in the merge .
You can pass a list of columns to [ ` drop `] ( #URL ) but rename will require passing a dict to [ ` rename `] ( #URL )
that indicates that the values do not agree or are missing from lhs or rhs , you therefore need to rename the ` _x ` columns and drop all the ` _y ` columns , you'll need to use ` drop ` and ` rename ` as suggested I can post a dynamic method to do this
In that case you need to drop the additional ' _y ' columns and rename the ' _x ' columns : #CODE
If you added the common columns to your merge then it shouldn't produce the duplicated columns unless the matches on those columns do not match : #CODE
They have matching keys as well as matching values in exactly the columns which are duplicated , and then two additional columns which are only in the right dataframe and not in the left ( hence the merge ) .
Is there anyway to randomly apply changes of stings by row to a Pandas data frame .
Pandas use groupby to apply a different function for each value of the groupby variable
I'd like to use groupby , but instead of applying the same functions to each group , I want to specify which function to apply to which group value .
I'm providing a very simple example here to illustrate the point , but in reality there are many values of my groupby variable , and my functions are all user-defined and fairly complex -- so solutions that involve selecting each group separately or apply the same functions to all groups will not be practical .
( Answers of that sort were provided to this very similar question : how to apply different functions to each group of pandas groupby ? but they don't address my question ) #CODE
Or , alternatively , if I store the names of the functions as strings , how do I then pass them as functions to apply ?
yes I am parsing school years and need to transform 2013 for example to " 2013 - 2014 " but it is turning out " 2013.0 - 2014.0 "
This can be done by a simple for loop , but I would like to use map , apply or some other pandas functionality .
What I need to do is collapse this frame into an Nx2 pandas DataFrame which has the index and column values that have ` True ` at the intersection as the record values .
( Make sure to drop one of the columns so you don't have linear dependence if you have an intercept ) .
You can concat the result of that to the original df .
Length : 895 , Freq : None , Timezone : None #CODE
Create a boolean mask , then call ` any() ` : #CODE
I have a dataset that contains individual observations that I need to aggregate at coarse time intervals , as a function of several indicator variables at each time interval .
I assumed the solution here was to do a ` groupby ` operation , followed by a resample : #CODE
Applying ` unstack ` to this table :
See here : #URL this gives you the ability to arbitrarily group / resample ' at once ' , also here : #URL ( the groupby enhancements section )
The only crazy idea that I would check : can it be that you copied the command from my post and single quotes were replace by something else ( back quotes , for example ) ?
This causes problems when trying to append the chunks into HDFStore dataset .
Is there logic built into pandas that I can use that help me manage the promotions efficiently , or would I have to built my own promotion tree , test each chunk for the best datatype , and somehow merge the detected chunks dtype to the prior dtype and promote as needed ?
How to combine pivot with cumulative sum in Pandas
Which i can easily pivot with the dates as columns using the following function : #CODE
I want a single title for a group of subplots - but I don't know how to apply the size property when it's in the brackets plot ( title=MyTitle ) .
My goal is to append the non-date values of df_ls ( below ) column-wise to their nearest respective date in df_1 .
I have googled this extensively without any luck and have only found ways to append blocks of dataframes to other dataframes .
I haven't found a way to search through a dataframe and append a row in another dataframe at the nearest respective date .
I can tell you from experience that this will need to be performed in some kind of for loop / apply method .
Also in a ` MultiIndex ` scenario query by ` index ` uses absolute row numbers as opposed to the logical index .
I want to apply this function on my data , stored in a data frame .
Is there a way to apply this function on groups of subject+trialcode ?
I think you want to ` merge ` on ` ID ` column : #CODE
If I have more ` DataFrame `' s similar to ` df1 ` and ` df2 ` , can I merge them directly , or only iteratively ?
Don't understand what you mean , if you mean if you have more than 2 dfs you can just iteratively merge them all
So there is no ` merge ([ df1 , df2 ,..., dfn ])` but only ` df1.merge ( df2.merge ( ... ( dfn )))` .
Yes , I did look at ` concat ` which does take a list of dfs but the result is not quite what you want as it will introduce a hierarchical index or level in the columns which is not what you want .
Now I need to re-group this by ' trialcode ' and drop thhe ' subject ' column .
pandas python : concet / merge / join 2 dfs on index
I have two dfs which I would like to concat on index to make a multiindex df .
where condition with inner join in pandas
Can you please help me on how to use the where condition with the inner join ?
If you look at the ` merge ` docstring , there is is option for suffixes on columns that have the same name .
I think your problem at datetime type , you have to normalize before you plot
You can replace month by year , day , etc ..
I think resample might be what you are looking for .
See this post for more details on the documentation of resample
pandas resample documentation
But if the DataFrame has a MultiIndex , there can be a big difference : #CODE
Convert freq string to DateOffset in pandas
In pandas documentation one can read " Under the hood , these frequency strings are being translated into an instance of pandas DateOffset " when speaking of freq string such as " W " or " W-SUN " .
I have been looking at ` resample ` ( see also this and this thread ) , since it looks like the right tool for the job .
Can ` resample ` be used for this ?
If you first define the set of pre-defined days ( ` days ` in my example below ) , you can reindex with that and specify the filling method ( ' ffill ' will propagate last valid observation forward , so this means take most recent for a time series ): #CODE
If you don't want to give a specific set , but just all dates from the start to end of the original Series , you can use ` resample ` do reach the same : #CODE
Groupby and apply ` .mean ` multiplying by 30 : #CODE
Tough , I don't know what you mean by " ( resample and fill the timestamp and the mean speed value )" .
In general , appending multiple single rows is not a good idea : better to eg preallocate an empty dataframe to fill , or put the new rows / columns in a list and concat them all at once .
See the note at the end of the concat / append docs ( just before the first subsection " Set logic on the other axes ") .
I added a link for concat / append , but for the internals , I don't think there is any good documentation .
` anaconda ` is free and comes with a basic numpy-scipy stack and tons of scientific packages .
My problem is that I'm having trouble getting pandas to create a date column which I can then apply a timedelta to .
and I want to insert a new column into df1 that looks up the corresponding index values in df2 and inserts the value of column ' eins ' that has the same index value as df1 into the new column of df1.The result is supposed to look like this : #CODE
DataFrame balancing by using reindex
I would like to transform the DataFrame to have exactly the same _ItemId elements for each _Channel modality ( which I refer to as " balancing " , like in panel data ) .
I thought I would use the reindex() function but I am uncertain on how to create the index I need to insert the missing elements .
How to properly insert NULL timestamps to postgres using pandas
I'm using ` pandas ` and ` sqlalchemy ` to insert data to postgresql , using pandas ` to_sql ` method .
If i change the date column to ` str ` and map the NaT values to None ( they get transformed to the string ' NaT ' when casting ) , then I can call to_sql and everything works #CODE
I have a dataframe with some data ( names , amount_x , amount_y and " diff " column which subtracts amount X and Y .
I would like to color the cells , where " diff " is a positive number ( and make them green ) and where it is negative , make it red ) ?
The default concat is to use ' outer ' .
If you pass a specific index , I'm pretty sure it does a left / right type of join .
Meaning if the row is not present in the index you pass , it will insert a blank row .
The actual use ` join_axes ` is to replace the indexes of the dataframes that you want to concatenate with a different one ( or actually one per dimension ) .
does that ( with ` join =o uter `) .
The location of the legend is specified using the ` loc ` argument to ` legend ` .
Then you can use the ` map ` method of Series : #CODE
Conditional join Pandas.Dataframe
I try to partially join two dataframes : #CODE
However , I want to apply this procedure several times with different filter criteria to df1 .
When you apply a function on the groupby , in your example ` df.groupby ( ... ) .agg ( ... )` ( but this can also be ` transform ` , ` apply ` , ` mean ` , ... ) , you combine the result of applying the function to the different groups together in one dataframe ( the apply and combine step of the ' split-apply-combine ' paradigm of groupby ) .
Pandas : resample with an external Series
I want to ` resample ` a ` DataFrame ` containing intraday data on market volume and market prices using an external ` Series ` with a ` datetime ` in it .
Then the ` index ` column is extracted as a series to map ` Datetime ` column in ` df ` .
Thanks , clever solution combining ` reset_index ` , ` set_index ` and ` map ` .
Select rows and subrows in pandas multiindex dataframe by condition
Now you can apply the condition like this : #CODE
You can append to an open file , as this question shows
What I'd like to do is merge them into a single dataframe with two columns : one ` key ` , the union of both ; and the other ` source ` , a list of which of the two original dataframes contained said key .
and then join them and calculating a new column : #CODE
If adding a column isn't an option you can use ` np.in1d ` after the merge .
You need to ` apply ` your logic to each row , like this : #CODE
Keep the highest results after a rolling function but without intersecting windows
I'm looking for a way to perform a rolling function in Pandas , and keep a specific value , for example the highest or lowest , and then removing all other windows intersecting that range .
The absolute warmest is easy , perform a rolling function and sort the result .
But given the nature of how temperature changes , the second , third etc will likely be the same period with just a 1-day shift in window .
the product name is a merge of 2 cells of two rows " no of sales " and " sales value " for each of 1000 or so areas for a given month .
Now join the two rows together with ` .
One advantage of using a MultiIndex is that you can easily select all columns which have a certain level value .
If you want to reindex on both ' date ' and ' colour ' , one possibility is to set both as the index ( a multi-index ): #CODE
You can now reindex this dataframe , after you constructed to desired index : #CODE
Another way would be to use ` resample ` for each group of colors : #CODE
I have a relatively large DataFrame object ( about a million rows , hundreds of columns ) , and I'd like to clip outliers in each column by group .
By " clip outliers for each column by group " I mean - compute the 5% and 95% quantiles for each column in a group and clip values outside this quantile range .
This works , except that it's very slow , presumably due to the nested ` apply ` calls : one on each group , and then one for each column in each group .
I tried getting rid of the second ` apply ` by computing quantiles for all columns at once , but got stuck trying to threshold each column by a different value .
However , I presume a more substantial speed up would be achieved if there's a way to do the operation in bulk on the DataFrame without having to operate column by column , similar to how one could standardize or normalize in bulk , e.g. , #URL
Therefore , I would like to apply the function to the entire column .
Also , the ` apply ` method of a GroupBy object doesnt have the ` axis ` keyword , die ` apply ` method of a DataFrame does .
@USER : I think I read in the linked stackoverflow question that ` axis ` has been added to the ` apply ` method of the ` GroupBy ` object .
Note : in newer versions of pandas , use ` index ` / ` columns ` instead of ` rows ` / ` cols ` when creating the pivot table .
I get a 3x3 array after running ` df.groupby ( ' client_id ') .apply ( lambda x : x [ ' items '] .astype ( int ) .diff() ) .values ` with the diff values .
How do I drop any row where one or more column values are outside a conditional statement like greater than or less than :
Yes , you can use ` df.drop_duplicates() ` ( docs ) You can also pass it a ` subset ` list of column names so as to drop duplicates from those columns .
All you would have to do is merge these DataFrames : #CODE
merge data frames with duplicated indices
Is there any way to merge two data frames while one of them has duplicated indices such as following :
Use join : #CODE
You could set ` files ` as the index in ` df1 ` and then apply a function which uses ` loc ` to look up the ` pkid ` value corresponding to the index : #CODE
so , this solution works best in that case since i can just add to the on parameter of merge . thanks !
A faster method than @USER ' s is to use ` map ` here because you have a unique index then this will be much faster than calling apply which is essentially a for loop : #CODE
If you pass a series or dict as a form of lookup to map as a param , if the index is unique ( which in this case it is and keys have to be unique for a dict ) then the lookup will be blisteringly fast .
yep , this is MUCH better than using apply as its fully vectorized
Ah - I must remember to use ` map ` more often !
does there happen to be a way to set and map according to multiple indices ?
You mean for multiindex ?
Yes that will happen , map performs a lookup , so long as the column value exists in either the index of a series or key in a map as the param then it will set the value for all instances of that value
For some reason it appears that ` agg ` is passing the Series twice to the function .
The calling-twice behavior could be related to the issue described [ here ] ( #URL ) for ` apply ` : it calls the function twice on the first group in order to check whether the function mutates the existing data .
I know apply and transform pass different packets of data such that it is quite hard to ascertain from print statements what is going on , but agh is fairly straightforward .
I would like to insert a Duration column ( integer ) , defined as #CODE
There are some problems with how ` .transform ` handles data-types ; to avoid those problems I first add an integer column and then transform over that column even though I do not need its values .
I tried groupby.apply but there were unexpected results due to the apply calling the function twice on the first group .
In the current implementation apply calls func twice on the first
The pandas DataFrame.add method has the following note : " Mismatched indices will be unioned together " but I'm looking for the most efficient way to drop mismatched indices .
What's the most efficient way to do this ( i.e. drop mismatched indices instead of union them ) ?
I should have been more specific in my question but really what I want to do is drop rows > ' 2014-03-31 ' since I only have data for those periods in one DataFrame .
When rows change , mark their ` activeState ` field as ` False ` and insert a new row with ` activeState=True ` .
How do I transform this frame to a numpy matrix of such : #CODE
You could use ` groupby ` to group by User , ` value_counts ` to compute a histogram , and ` unstack ` to reshape the result : #CODE
pandas resample .csv tick data to OHLC
I want to resample into Daily OHLC using pandas so i can import it into my charting software in the correct format .
Can you help me convert the data in the fomat i have into OHLC with pandas resample .
Yeah you need to replace " column name " with the column you actually want to plot .
I have tried variations of concat etc with no success .
My recommendation would be to concat the list of dataframes using pd.concat .
This will allow you to use the standard group-by / apply .
In this example , multi_df is a MultiIndex which behaves like a standard data frame , only the indexing and group by is a little different : #CODE
I'm not sure of the most efficient way in pandas to map my timeline of events into something and plot it .
A Series of ` datetime64 ` s has a ` dt ` attribute which allows you to access the hour and minutes as integers .
So that seems to work for the values that exist , but seems to assign a value of -61 for all empty spaces / null values and doesn't interpolate between the actual values .
Upon further inspection , whenever pd.to_datetime() is called , it's replacing all NaNs with NaTs , which seems to cause interpolate to fail .
However , I still can't get interpolate to work .
Finding it hard to strip just the date out since I am working with two columns rather than two values .
By " Finding it hard " I mean that strptime on the x [ ' datex '] doesn't work because those are series and not values and I can't apply it to the x in " lambda x " or use %Y%M%d instead of %s .
Finding it hard to strip just the date out since I am working with two columns rather than two values
Well , since you're already using ` apply ` , you're dealing with two values ( not columns ) , so you can call the ` date ` method on each : #CODE
computing rolling averages by integer days in pandas
Do I need to convert the days to something that pandas recognizes as a date value in order to use some of the rolling statistical functions ?
First reindex ` df2 ` to fill in the missing days with zeros : #CODE
Now if you ` unstack ` this reindexed DataFrame and take the transpose , you have a DataFrame where each column is an entry of ` i-j ` and contains the counts per day : #CODE
You can calculate the rolling mean of this DataFrame : #CODE
To finish , you can ` stack ` this frame of rolling means to get back to the shape of the original reindexed ` df2 ` : #CODE
it seems that the reindex ( new_index , fill_value=0 ) is setting everything in the count column to 0 .
python / numpy / pandas fastest way apply algorithm for expanding calculations
You can group by ` Location_Id ` then pivot on ` date ` and ` Item_Id ` and get the correlations : #CODE
` loc ` uses index label indexing , -1 and ` len ( df ) +1 ` does not exist hence the error , for integer based indexing use ` iloc ` , also if you repeatedly appending rows this will be horribly inefficient
For future reference , I could resolve that using ' concat ' using a new df2 .
Pandas get_dummies to output dtype integer / bool instead of float
You can replace that inner for loop with : #CODE
and I get ' Exception in Tkinter callback ' with a long stack crawl ending in #CODE
Since you didn't specify a tz argument , I'm guessing its trying to parse it out of your datetime .
While this can be done using ` len ( pandas.unique ( my_index )) len ( my_index )` , I was wondering whether the ` MultiIndex.has_duplicates ` property can be used for this purpose .
In other words I'd like a function ` f ` so that I can apply ` f ` to a series ( or multiple series ) as well as applying ` f ` to a float ( or multiple floats ) , and ideally apply ` f ` to a combination of floats and series .
I'm writing several pivot tables using pandas .
In a two-dimensional pivot table , the below code works as it should .
Use a groupby to get at each combination of ` col_1 ` and ` col_3 ` , then unstack to get the ` col_3 ` values as columns : #CODE
pandas rolling on a shifted dataframe
I understand that for the rm columns the 1st 4 items aren't filled because there is no data available , but if I shift the column calculation should be made , shouldn't it ?
Due to your first shift you create your NaN's at the end .
However I am struggling to figure out how to apply one function ` convertToMeters ` to the first column and ` convertToNewtons ` for the second column .
it will apply the respective function to each column and not just the desired column .
Probably you can write this function so that you can just do ` df [ ' col_meters '] = convertToMeters ( df [ ' col '])` without using the apply .
What I need is to drop rows from each group , where the number in column ` B ` is less than maximum value from all rows from group's column ` B ` .
So I want to drop row with index ` 0 ` and keep rows with indexes ` 1 ` and ` 2 `
So I want to drop row with index ` 4 ` and keep row with index ` 3 `
Well I mean just to delete rows from groups and keep these groups as they are - I need to apply several filters and after each apply is needed new groupby .
You just need to use ` apply ` on the ` groupby ` object .
You can pass lots of functions to the transform method .
You can even pass some strings as common function names like `' median '` .
So , for example , if you want all rows in above the median B-value in each A-group you call #CODE
works , although I also had to drop the penalty term .
I want to transform a normal indexed time series dataframe into a dataframe that uses the hour atribute for the index , but displays the days data in the columns .
And I want to transform it into this :
You can use a pivot table #CODE
For just one of these columns , I need to replace the NaN immediately before a non-NaN value with a value from another column .
The reason I want to utilize Pandas is to take advantage of some of it's other features ( most importantly , a rolling average ) .
pandas : iterating over DataFrame index with loc
Similarly if I create a column to store the int day value and then perform the apply then it works also : #CODE
@USER that I don't know , ` ix ` first tries label based and then integer for based , for your info ` iloc ` also shows this error so it's puzzling why ` ix ` works
Does this replace the ` s-s [ 0 ]` part ?
If I create one dataframe row for each XML row that contains information and then set a MultiIndex on ` [ MessageID , StreetName , LinkID ]` , I get an Index with lots of ` NaN ` in it ( which is generally discouraged ) because ` MessageID ` does not know its children ` streets ` and ` links ` yet .
When doing a GroupBy on ` [ MessageID , StreetName , LinkID ]` , I do not know how to get back a ( probably MultiIndex ) dataframe from the pandas GroupBy object since there is nothing to aggregate here ( no mean / std / sum / whatsoever , the values should stay the same ) .
We can now se the columns of interest ( messageId , streetName , linkId ) as MultiIndex on that dataframe : #CODE
I have fixed the code above to make sure ` transform ` is used on series and not on dataframe ( thus avoiding the error ) .
I guess you could easily turn this into a function to apply on a dataframe inplace .
Please see edit in the answer to see the fix for this error ( since you added more columns ` transform ` was being applied on a GroupbyDataframe and not on the Series ) - all it takes is to pass name of the column after ` groupby ` to apply ` transform ` on .
It seems I can apply some functions without problems to a DataFrame , but other give a Value Error .
The first apply works fine , the second one generates a :
Rather , I'm interested in why the apply above doesn't work .
To add process-based parallelism , we bring in the ` Pool ` class from the ` multiprocessing ` stdlib module , and pass the ` Pool ` instance's ` map ` method as a keyword argument to ` compute ` : #CODE
` pd.to_datetime ( dt [ ' Date '])` and ` pd.to_datetime ( str ( dt [ ' Date ']))`
Actually it's quicker to convert the type to string and then convert the entire series to a datetime rather than calling apply on every value : #CODE
Translated , this means : take the randomly-generated value from dfrand that corresponds to the NaN-location within " data " and insert it in " data " where " data " is NaN .
How can i apply do_calcuations without loops like this .
groupby these event numbers and apply ` do_calculations ` to each group .
How do you stack two Pandas Dataframe columns on top of each other ?
Afterwards , you can just join them using ` pandas.concat ` .
You could construct a ` mask ` -- a boolean array which is True where the Series index equals the particular value : #CODE
I have been looking at scikit-learn and have been trying to work out how to output an array or a dictionary of the cut points for each level of a decision tree .
I can see how to generate an image of the tree but I am looking to use these cut points for further feature engineering .
I have found a way that works but I'm not sure why it works like this , if I convert both series into dataframes then join them it works as expected : #CODE
I want to then transform v1 in data frame d1 such that the only the levels common with d2 remain and the rest are relabeled as " other " , i.e. , d1 [ " v1 "] should be transformed to : " a " , " b " , " c " , " other " , " other "
Apply vs transform on a group object
The example on the documentation seems to suggest that calling ` transform ` on a group allows one to do row-wise operation processing : #CODE
In other words , I thought that transform is essentially a specific type of apply ( the one that does not aggregate ) .
The function passed to ` transform ` must return a number , a row , or the same shape as the argument .
If you look at the example in the documentation that I copied above ( i.e. with ` zscore `) , ` transform ` receives a lambda function that assumes each ` x ` is an item within the ` group ` , and also returns a value ** per item ** in the group .
` transform ` will look at the dataframe columns one by one and return back a series ( or group of series ) ' made ' of scalars which are repeated ` len ( input_column )` times .
X and Y are actually pairs of coordinates and the function I would like to apply is the vincenty distance from the geopy package .
And another problem : Even if I just want to apply the vincenty formula to the series in my dataframe , I receive an error message : #CODE
But the only way I can apply the formula to several entries is using lists .
Substitute operator.mul with the pertinent function you need to apply .
Beware that the way in which you add new axes to your data implies a transposition in the result matrix and what you want to do depends eventually in how you like to read the result , as in algebra ( ` x ` is the row and ` y ` is the column ) or just like a map ( ` x ` is longitude and ` y ` is latitude ) #CODE
Try read the whole file and put it in the list , then you can loop the list and strip data .
We need to add the power demand for every appliance to get the total aggregate power demand over time .
Use the ` shift ` method : #CODE
pandas data frame indexing using loc
You can use ` apply ` to pass the values individually : #CODE
I'm trying to translate a simple linearised least squares problem to statsmodels , in order to learn how to use it for iterative least squares :
The ( contrived ) data comprise measurements of the time it takes for a ball to drop a given distance .
You may want to use ' value_counts ' to count the number the instances of a particular time event and then resample the dataframe to fill na , like so , #CODE
How to replace non-ASCII by ASCII in pandas data frame
I would like to replace some non ASCII characters in the pandas Data Frame .
I would like to create some replace rules : eg .
One way would be to create a dict and iterate over the k , v and use ` replace ` : #CODE
But it doesn't apply for the third and second row if there're commas outside the quotes ( single or double )
I could do a left merge , but I would end up with a huge file .
Is there any way to add specific rows from df2 to df1 using merge ?
Unclear why you think a left merge would produce a huge file , by performing a left merge on the product id you are stating that you are only interested in matches in the product_id column only
Just perform a left ` merge ` on ' product_id ' column : #CODE
However , I notice that , as the range of each element is from 0.00 to 1.00 and I only care the first 2 digits , I could transform each element into a unsigned 8-b it integer by multiplying with 100 and then cast to np.uint8 , which could reduce this dataframe to an acceptable size : ( 1 / 8 * 37GB ) .
So you have to make a decision whether to drop the ` NaN ` values , replace them with something like 0 or other value that can be represented by an ` int `
When introducing NAs into an existing Series or DataFrame via reindex or some other means , boolean and integer types will be promoted to a different dtype in order to store the NAs .
Maybe you can use sqlite3.exe to aggregate data out-of-the-core
... than a pivot table like this : #CODE
For example : ` df [ ' sum '] , df [ ' prod '] = df.sum ( axis=1 ) , df.prod ( axis=1 )` gives you what you desire
One way to do this is to pass the columns of the DataFrame to the function by unpacking the transpose of the array : #CODE
You could write ` df [ ' sum '] , df [ ' prod '] = sumprod ( df [ 0 ] , df [ 1 ] , df [ 2 ])` to get the same result .
Merge and fill two dataframes in Pandas with different time intervals
I have two dataframes I'd like to merge in Pandas .
If I do an outer join , I can merge them , but only the rows every 15 minutes will have data from both dataframes .
You want to resample the two dataseries so that they have the same interval and fillna with method ' ffill ' #CODE
Assuming the len of col1 and col2 would be equal
I can do with apply , but I was wondering if there is any way to only indexing instead .
You probably want to replace this with , #CODE
I also discovered where() that has an optional argument , so you can select based on a bit mask between two sources .
This works of course , it is similar to the apply solution , it just make the loop over the columns explicit .
I may be doing something wrong , but on my machine I got the following timing information : 1.58 ms for apply on subset of columns , 62.3 ms for fillna , and 2.65 ms for the explicit loop .
You could replace the ` 0 ` values with ` NaN ` , then forward fill them , also it's useful to post data and code , see [ ` fillna `] ( #URL ) , so I think something like ` df [ ' Total_Data '] .replace ( 0 , NaN )` and then ` df [ ' Total_Data '] .fillna ( method= ' ffill ' , inplace=True )` I think should work
I don't know why you need the sturct like ` [ shape_id :[ shape_pt_sequence , [ shape_pt_lat , shape_pt_lon ]]]` , It's not very useful for data selection , you can use MultiIndex : #CODE
Second , we're going to use the dataframe method ` apply ` .
What ` apply ` does is it takes a function and runs every row ( axis=1 ) or column ( axis=0 ) through it , and builds a new pandas object with all of the returned values .
For that we'll use the ` shift ` method and join the result to the original dataframe .
` rsuffix ` and ` lsuffix ` are what append " 1 " and " 2 " to the column names during the join operation .
So now we can ` apply ` the Haversine function : #CODE
Then , instead of a list comprehension , data = np.empty ( len ( df.index )); data.fill ( df.latitude [ column ]); lat1 = pd.Series ( index =d f.index , data =d ata ) seems to be a bit faster .
I'd like to convert a Pandas DataFrame that is derived from a pivot table into a row representation as shown below .
to get rid of the multi-indexes , but this results in this because I pivot now on two columns ( ` [ " goods " , " category "]`) : #CODE
hmmmm , melt ( my answer ) loses month from the index :(
However , I don't quite understand why using ` stack ` previously worked and I now should use ` unstack ` .
It seems to me that ` melt ` ( aka unpivot ) is very close to what you want to do : #CODE
If we drop it first the melt works OOTB : #CODE
@USER I wouldn't say it's a bug , but could potentially be a feature / enhancement ( to not drop index ) .
I'll try this out , as I now have to select either ` stack ` or ` unstack ` depending on the number of grouping elements .
` melt ` will be more streamlined .
This will drop * any * row which has a NaN .
` dropna ( how= ' all ')` will drop a row only if * all * the values in the row are NaN .
You could form a mask : #CODE
so , you just need to replace the first value with cumulative value at ` .iloc [ 0 ]` .
If someone can suggest a better column truncate function that would be welcomed too .
If you just want to truncate the col then ` df [ col ] = df [ col ] .str [ 0 : maxsize ]` I think should work and it will be vectorised so should be fast
I fixed code example to align with this .
i mean i can go the c++ way and iterate through elements of the dataframe and store and append it to a list .
Now you can use ` stack ` : #CODE
If I want to get the element-wise floor or ceiling , is there a built in method or do I have to write the function and use apply ?
` value_counts ` seems a strange choice in nunique , ` Series.unique() ` doesn't sort , so you could just call len on that ( in O ( n )) ?
You can drop them using ` dropna ` : #CODE
Can I ignore that while apply the dropna or use that column as axis ?
If you read it in as a csv you specify this col as the index : ` df = pd.read_csv ( ' file.csv ' , index_col=0 )` or just set the index after ` df.set_index ( keys= ' col1 ')` then you can drop the rows as per my example
Is it feasible to apply your techniques when no_row=1600000 and no_colors=230000 ?
Towards that end I have resorted to ` shift ` ing the data back by an appropriate number of days as demonstrated in the following code #CODE
It won't do this unless its necessary ( as its a tiny bit computational ) , e.g. when you resample .
You might want to reindex to make it one .
Downsampling to a lower freq ( where you don't have overlapping values ) , forward filling , then reindexing to the desired frequency and end-points ) .
Please show the full stack trace leading to the ` TypeError ` as otherwise it's impossible to tell which code is erroneously trying to set that ` dtype ` .
The Items data structure is a hash map indexed by the item ID .
The Categories is a hash map that is indexed by category ID .
so what is an easy way to go about converting the time from a unix timestamp to a useable time format for the resample code ?
You can read in the timecode from the csv as an integer and then apply pd.to_datetime #CODE
I got the time converted into this format 2014-12-19 23:19 : 07 453.3 0.050 but i cant seem to get the resample code to work .
Note I had to drop the ` NaN ` row as this will introduce a mixed dtype which cannot be used for ordering e.g. float > str will not work
Efficient algorithm for expanding grouped tabular data
[( A1 [ i ] , A2 [ i ] , A3 [ i ]) for i in range ( len ( B )) for _ in range ( B [ i ])]
For len ( A )= 1e6 , I find that the itertools solution is about three times slower than the list comprehension .
Some people like using ` stack ` or ` unstack ` , but I prefer good ol ' ` pd.melt ` to " flatten " or " unpivot " a frame : #CODE
Sorry about the ugly looking code , I ' m using the mobile Stack Exchange app .
From querying Google and SO , it seems like getting MySQL to do this transform is a bit of a pain .
Then ( since False == 0 and True == 1 ) we can apply a cumulative sum to get a number for the groups : #CODE
and replace #CODE
How would I take the dataframe and cut the data into two dataframes ?
" How would I take the dataframe and cut the data into two dataframes ?
There are lots of ways to apply this to your problem , but if I understand your approach correctly -- a straightforward application that follows your structure would be something like this : #CODE
Made a ' BINS ' column and used cut to group them : #CODE
How to reindex a multi-index pandas dataframe ?
Or , You can make DataFrame from Series and concat then #CODE
I have a pandas dataframe which I would like to split into groups , calculate the mean and standard deviation , and then replace all outliers with the mean of the group .
I have also tried defining a transform function separately : #CODE
Note : If you want to eliminate the 100 in your last group you can replace ` 3*std ` by just ` 1*std ` .
I would like to ` bfill ` and ` ffill ` a multi-index ` DataFrame ` containing ` NaN ` s ( in this case the ` ImpVol ` field ) using the ` interpolate ` method .
For example , I have to interpolate across the ` 2014-12-26 ` ` call ` options separately than the ` 2014-12-26 ` ` put ` options .
I was previously selecting a slice of the values to interpolate with something like this : #CODE
If I use the ` interpolate ` method to fill without selecting a slice ( i.e. across the entire frame ) , ` interpolate ` will interpolate across all of the sub indexes which is what I do not want .
I'd try to unstack the data frame at the OptionType level of index .
If multi index df is the most desirable one for further computations , you can restore the original format using stack method .
you'll need to unstack twice to have the date in the column labels too ; it won't make sense to interpolate across dates if the strikes are in ascending order .
I can replace any occurrence of ' a ' with ' AAA ' as follows : #CODE
If you replace ` unique_df ` with ` unique_df [ ' Letters ']` , it works .
Assuming I want to replace more than one value , is there a way of doing it in one swoop instead of stepwise as below : ` unique_df.loc [ unique_df [ ' Letters '] == ' a ' , ' Letters '] = " AAA "
So I want to change freq of data for of every 15 sec that price at given time ( like 15:30 : 15 ) is last price for every ticker .
Something like ` resample ( ' 15s ' , fill_method= ' pad ')` should work .
If you want to start at 15:30 : 00 , you can drop the first row from the DataFrame .
First , I'm wondering if I can achieve the same result more efficiently using ` merge ` , ` zip ` , ` join ` or the other concatenation functions .
The fastest method would be to set ' Letters ' as the index for df_a and then call ` map ` : #CODE
Calling map here is nearly 18 times faster , this is a vectorised function and will scale much better .
In terms of efficiency in this given example , how does the nested loops and map stack ?
Well your solution is O2 as you loop over 2 loops , this I suspect will be between On and nlogn , ` map ` is a vectorised function so will operate on the whole array rather than a row value at a time
I applied map in my actual dataset and for some reason my target column is not being filled in .
Before applying ` map ` , I ensured ' Visit ' columns in both data frames contain [ unique values ] ( #URL ) using ` groupby ` .
I think I need to use the apply method to trim the column data .
Otherwise you'd have to do something like ` df.applymap ( lambda x : x.rstrip ( " . "))` , or drop down to numpy ` char ` methods .
I tried apply and could not seem to get that to work either .
Note that I'm preserving the dates of the " closest prior price , " instead of simply using ` pandas.asfreq ( freq = ' q ' , method = ' ffill ')` , as the preservations of dates that exist within the original ` Series.Index ` is crucial .
This seems like a silly problem that many people have had and must be addressed by all of the ` pandas ` time manipulation functionality , but I can't figure out how to do it with ` resample ` or ` asfreq .
You can then join these together .
You could use ` stack ` to flatten the frame to a series , use boolean indexing to select the terms you want , and finally turn the resulting index into a list : #CODE
I had an idea : do you think I should just manually create the date_range myself and then append the values to the range ?
I would like to normalize my data by dividing every row by the first value of that very row .
This is an example of a ` pivot ` operation : #CODE
I guess stack is the answer but again I don't know how !
I want to be able to map all these timestamps to the corresponding #URL just specific to the week , irregardless of which week they may be .
I also tried using the apply function , but very likely I am doing it wrong .
I can see that I could easily replace the res.params .
I do not select the complete table so I cannot just replace the table .
the column order / type remain unchanged so it just needs to replace / update the rows and I have a primary key indexed , auto-increment ' id ' column if this makes any difference .
I know that you can use to_sql() to append or replace , but that doesn't help you very much .
I only found [ this post ] ( #URL ) but that works with " insert ignore on duplicate " .
I know this because when I check ` len ( Sframe )` after selecting ` 2 ` to eliminate duplicates ( and their unique , original values ) , I get the same number as before dealing with the duplicates .
Which field are you using for ` identifier ` when you merge ` Sframe ` and ` SframeDup ` ?
That's what I thought about my original code but for some reason when I check len ( Sframe ) at the end in the main code , it still has the duplicate values even if the conditional statement applies option 2 to remove duplicates .
When I check len ( SframeWOdup ) at the end of the code , it shows that the duplicates are removed .
The final goal is to be able to use ` pd.resample() ` to resample from the series above .
You can ` merge ` the two DataFrames : #CODE
I would look at the result from ` merge ` and remove the columns I didn't want .
I'll just apply a sort again and get it in the correct order .
Unclear if it should work at all but you could apply the function : ` s.apply ( pd.DataFrame.mean )`
index : bool , optional whether to print index ( row ) labels , default
" ValueError : cannot reindex from a duplicate axis "
That is : I would like to merge the columns with the same Timestamp ( I have 17 columns ) , resample at 1 min granularity and for those column with no values I would like to have NaN .
Assumed that you have your ` Timestamp ` as index to begin with , you need to do the resample first , and ` reset_index ` before doing a ` groupby ` , here's the working sample : #CODE
As said in comment , your ' Timestamp ' isn't datetime and probably as string so you cannot resample by DatetimeIndex , just reset_index and convert it something like this : #CODE
Now just run the previous code again but replace ' Timestamp ' with ' ts ' and you should be OK .
I tried to do that before and when I resample with df.resample ( ' 1Min ' , how= ' max ') , I get the following : TypeError : Only valid with DatetimeIndex or PeriodIndex and I don't know how to go about this .
I am trying to write Python Pandas code to merge the data in two DataFrames , with the new DataFrame's data replacing the old DataFrame's data if the index and columns are identical .
Now merge them : #CODE
Then merge using the same code as before : #CODE
If you are after performance , you can set index on ' x ' and drop the list comprehension and make it generator , it further reduces the lookup time .
Basically I make the index into a column , then melt the data frame .
gives the whole records of customer with ` email 1 ` but I want the aggregate table .
Pandas concat : ValueError : Shape of passed values is blah , indices imply blah2
I'm trying to merge a ( Pandas 14.1 ) dataframe and a series .
Have you tried ` append ` instead of ` concat ` ?
Try checking out ` len ( df.index )` and ` len ( h1.index )` .
But then I tried join - thanks !
Aus_lacy's post gave me the idea of trying related methods , of which join does work : #CODE
Some insight into why concat works on the example but not this data would be nice though !
I had a similar problem ( ` join ` worked , but ` concat ` failed ) .
How to use python pandas to get intersection of sets from a csv file
The only other option I can think of would be to drop python all together and reimplement FlowCytometryTools in Objective-C to avoid py2app , but that would be pretty labor intensive .
As updated by OP , the blank filed is actually NaN , you can make use of isnull method from pandas , like this : #CODE
I will not downvote but your answer is somehow wrong as ** notnull ** will only filter ** NaN ** whereas OP intentionally uses "" , which isn't ** NaN ** and therefore will not be filtered .
I have to calibrate a distance measuring instrument which gives capacitance as output , I am able to use ` numpy polyfit ` to find a relation and apply it get distance .
I need to apply a correction like if the value is between the two numbers it is rounded and shows the limits of detection
use your raw data of known distances ( your calibration set ) and apply the fit . you can then see how much variation you have from the actual values ( range ) and standard deviation ( 1 sigma )
pandas : Rolling correlation with fixed patch for pattern-matching
I am looking for a way to compute the correlation of a rolling window and a fixed window ( ' patch ') with pandas .
Using NumPy will make the computation much faster because we will be able to remove the ` for-loop ` and handle all the computations done in the for-loop as one computation done on a NumPy array of rolling windows .
Then translate it into corresponding code done on NumPy arrays , making adjustments as necessary for the fact that we want to do the computations on a whole array full of rolling windows instead of just one window at a time , while keeping ` patch ` constant .
To create the array full of rolling windows in a memory-friendly manner , I used a striding trick I learned here .
simple pivot table of pandas dataframe
You need to specify ` len ` as the aggregating function : #CODE
Using Pandas to shift only a slice of a DatetimeIndex series and put it back in to the file shifted ahead
Simply put , I am wanting to pull out a slice of the series and shift it ahead an hour ( minutely data ) , and then put the shifted slice back in and ffill any missing values with 0 .
Here is the data I need to shift an hour : #CODE
My approach has been to slice out the time chunk that I need to shift data by an hour , shift the data by an hour , and then replace the original data with the shifted slice .
Notice how the sift has worked , and it has added another couple columns , but I wanted to replace the original columns ( df , on the left ) with shifted ( those 2 on the right ) .
If you identify where the duplicates exist you could use ` resample ( ' col_with_dups ' , how= ' mean ')` .
I am wondering if I am wanting to use another method than the one I used above ; perhaps merge or join ?
Effectively I want to replace the df indexes with shifted ones , so is there a " replace " method ?
Well you could ` merge ` the two data frames and then specify if you want to keep the indices from either the right hand side data frame or left hand side .
I am having a hard time incorporating the desired changes using merge , but have gotten the data merged in , just not correctly ( i.e. : replacing the original slice ) .
Try ` df.update ( shifted )` rather than your merge .
ValueError : cannot reindex from a duplicate axis Does the data where shifted will go need to be deleted for that method to work ?
After many trials and errors I have found a way to remove a slice from a Dataframe indexed by datetime , shift it an hour , and then combine it back into the original dataframe .
- if length of the gap is lower than 5 NaN , then interpolate
Now we want to interpolate when gap is 10 and put something where gap > 9 #CODE
It's also useful to know that the ` reindex ` method can be used to subselect rows .
The ` custom_df ` as ` Freq : C ` , while ` custom_df2 ` has ` Freq : None ` .
The ` freq ` is used by certain methods , such as ` snap ` and ` to_period ` .
I've added some information on how to exclude specific days using [ ` CustomBusinessDay `] ( #URL ) and ( alternatively ) by calling [ ` reindex `] ( #URL ) .
You can use ` melt ` to convert a DataFrame from wide format to long format : #CODE
You could use ` stack ` to pivot the DataFrame .
To create a DataFrame like the one you specify you could just reset the MultiIndex after stacking and rename the columns : #CODE
I had already tried to use the stack() function but did not know that reseting the MultiIndex would have solved the problem for me .
I am trying to append data to a log where the order of columns isn't in alphabetical order but makes logical sense , ex .
Seems you have to reorder the columns after the append operation : #CODE
The same alpha sorting of the columns happens with concat also so it looks like you have to reorder after appending .
An alternative is to use ` join ` : #CODE
` join ` does what you want I think except it aligns on index which may or may not be what you want though
@USER I see your problem , join seems to preserve the column name and data order
Pandas : Take the median accross multiple dataframe
Across all the dataframes , I want to take the median of each i , j element .
Now to calculate the median just do : #CODE
How to replace all Negative Numbers in Pandas DataFrame for Zero
@USER I mean the logic behind the fact that I can apply the comparison to the whole DataFrame only if the rhs is a Timedelta , although it works just fine with separate columns and ints .
How to efficiently apply a function to each DataFrame of a Pandas Panel
I am trying to apply a function to every DataFrame in a Pandas Panel .
Is there something which would apply a function to each DataFrame ?
apply is correct , but the usage is :
The code I'd like to replace ( with something that will take an arbitray number of k's ) is : #CODE
Searching the web has been frustrating because I'm trying to do a join ( in the pythonic sense ) and most search results relate to joining columns in the database sense ( including as adapted in pandas ) .
Also good to see a solution using ` apply ` ; it could be handy for implementing more exotic concatenation functions .
try to use str.cat over apply whenever you can . feels a gazillion times faster .
You can use ` pandas.apply ( args )` to apply a function to each row in the ` transdf ` data frame if you know that the rules set in the ` segmentdf ` are static and don't change .
I have a TimeSeries data object to which I append data worst case every 200ms .
Well ` append ` is a wrapper for ` concat ` so if you can use an implementation using ` concat ` it should be marginally faster .
An ` append ` operation returns a new Series .
change schema for sql file ( drop quotes in names )
Unfortunately , I do not have access to their code , so I can't fix it ( probably simple strip method would do the trick ) .
But if you need to insert the data into sqlite with a schema without quotes , you can do the following .
OK - there should be an short error message right at the bottom of the stack trace , e.g. beginning with ` ValueError ` or ` TypeError ` or something similar .
As documented , you can get the number of groups with ` len ( dfgroup )` .
I am attempting to resample tick time series data in Pandas .
I use resample #CODE
I don't understand why it starts with " 2014-07-01 10:00 : 15 " while I resample slice starting with " 2014-07-01 10:00 : 20 " .
[ resample ] ( #URL ) has many arguments , that control resampling .
However I found out that first index doesn't depend on the length of the slice , so a little weird solution is to resample small slice , check first index , get needed base , and then resample whole data .
Now I try all ( is x less than y ) , which I translate to " are all the values of x less than y " , and I get an answer that doesn't make sense .
Next I try any ( is x less than y ) , which I translate to " is any value of x less than y " , and I get another answer that doesn't make sense .
I think I need some help with the " melt " function .
I know that I can use the melt function but I have no clue how this function works ...
How can I use the melt function to fill the new df ?
You are probably looking for ` pivot_table ` , which is like the inverse of ` melt ` .
However , I get the following error : " No numeric types to aggregate " .
I tried the replace function in that way : replace " .
So the trick was to put the replace together ?!
Pandas pivot gives ValueError
I am trying to pivot it by count : #CODE
The rest is straight forward : just ` groupby ` the data into groups , apply the ` convolve ` and then ` concat ` the resultants together .
I've tried using stack / unstack and pivoting the data frame ( as some of the other solutions here have suggested ) , but I haven't had any luck .
OK , I see your problem you have data spanning across multiple columns , you need to concat the various columns into a single df , I'd first fix the column names and it will be easy
You can use ` apply ` to force all your objects to be immutable .
Few names are duplicated , and I want to append a simple indicator to the duplicates .
However , I think the ` apply ` function does not allow for ` inplace ` modification , right ?
Also , you should be able to replace everything after the ` = ` with ` df.name.duplicated ( take_last=True ) .apply ...
If you want to append the indicator to only those after the first , you can do it with a little special casing : #CODE
If you want to use this to replace the original names just do ` d.Name = ...
The problem is that I want to merge and update DataFrames that come from different sources .
Many operations in Pandas align on the index .
You can apply a ` lambda ` to only the relevant column , instead of the whole row : #CODE
What is the approximate byte size of the data you're trying to insert ?
The TL ;D R answer : While BQ is not strictly enforcing the max rows per request of 500 rows / insert at this time , there are some other limits elsewhere in the API stack related to the overall request size that are preventing the call from succeeding .
I'm particularly concerned about this 500 rows / insert at a time .
One thought that I had was to convert to it a string , left strip the space , and then take [ 0 ] , but i can't figure out the code .
You could change your apply to df.new_var = df.ID.dropna() .apply ( checker ) or test for NaN in your function
Failing that , what is the best way to insert a column index into the first row of data ?
Then you can change names before or after your concat
later we need to merge them .
Just transpose it after you've created the DataFrame : #CODE
What I want to do is to merge them and create single big data frame that looks like this : #CODE
You could shorten to ` df [ ' CountryCount '] = df [ ' PersonId '] .map ( df.groupby ( ' PersonId ') [ ' Visited '] .nunique() )` and avoid the merge
I think using ` nunique ` is better than calling ` apply ( len )` though ;) +1
@USER IMHO , I think you should edit to avoid the merge part and put the nunique() , which looks great .
@USER check out timings , ` nunique() ` is worse than ` apply ( len )` :)
When you want to " broadcast " across a group , you typically use ` transform ` : #CODE
I think transform is a correct way to use here , and this also a fastest on if we use ` unique() .size ` instead of ` nunique() `
After the for loop and before you define frame , replace the left bracket ( ' [ ') with whatever you want .
Now add another column for the week and year ( one way is to use ` apply ` and generate a string of the week / year numbers ): #CODE
Finally , group by `' Week / Year '` and `' Category '` and aggregate with ` size() ` to get the counts .
I noticed that it is not a smart thing to drop missing NAN values .
I usually do interpolate ( compute mean ) using pandas and fill it up the data which is kind of works and improves the classification accuracy but may not be the best thing to do .
You can either drop the rows that have any missing values but this may reduce performance , or set the missing values to some value that doesn't affect the prediction but this may still skew your model if you have many missing values , it really depends .
You can use the mean / median but you will have to measure the performance of all approaches and see what is best , it depends on whether there is any value in those features and what model you selet
Is there a way to drop or ignore the index column ?
Then you can unstack the MultiIndex into columns : #CODE
append item to list in pandas by index
I still would like to know why the append doesn't work .
I'm trying to replace months represented as a character ( e.g. ' NOV ') for their numerical counterparts ( ' -11- ') .
However , to avoid redundancy , I'd like to use a dictionary and .replace to replace the character variable for all months .
However , if you need / want date strings with 3-letter months like `' NOV '` converted to ` -11- ` , then you can convert the Timestamps with ` strftime ` and ` apply ` : #CODE
Then you can join the columns back into one using ` apply ` : #CODE
what column are you calling your ` apply ` on ?
I would like to apply the function to the new column and get the results by referencing the other two columns .
also avoid using ` apply ` where possible as this is just going to loop over the values , ` np.where ` is a vectorised method and will scale much better .
No need to drop down to ` numpy ` -- frames and series have ` where ` methods too ..
@USER no worries , the key thing to take away from this is to look for a vectorised method that will operate on the whole df or series rather than calling apply which loops over the values
Simply add ` axis=1 ` to the ` apply ` arg list .
I'm trying to append a pandas DataFrame ( single column ) to an existing CSV , much like this post , but it's not working !
If you want to append ` scores ` as a new column on the original csv data frame then you would need to read the csv into a data frame , append the ` scores ` column and then write it back to the csv .
@USER I don't think it's possible to append the column to the original data frame within the csv without opening the file and parsing the data since python has no way of knowing that there is a data frame in the csv to append to .
@USER , then it will be difficult , provided that you still need to open+read each line of your csv to find the linefeed and append your new column .
Not knowing much about your data but assuming it is a time series analysis , I would try to create a correlation matrix among all the features you have , and maybe merge features with very high correlation .
I used the code provided below and was able to drop the time to about 3.5 seconds .
How is a DataFrameGroupBy structured to then apply functions to it ( like a for loop ) ?
I think you want to use ` shift ` : ` goods_data [ ' time_difference '] = goods_data.groupby ( ' id ') .time_retrieved .apply ( lambda x : x - x.shift() )`
The mental image I have for what I'm trying to do is strip the zeroes in each row up to the leftmost non-zero value , and then " left align " that row .
Each row would start with a nonzero number and then proceed as before .
From reading other topics it seems like I may want to transpose and then use a mapping function ?
The columns would indicate the 1st nonzero month of data for each row , the 2nd would be the calendar month following it , etc .
If you " left align " the rows won't that mess up your columns ?
Here , I rename the score column so that when I join it to the to the original dataframe , I know which column is which .
A follow-up question : Will the fact that groupby was used pose problems when I try to append these records to an existing table ?
I've seen that pandas includes several rolling functions in the pandas.stats.moment module , and I guess what they do is somehow similar to the subsequencing problem .
Then we need to specify two values ( in a tuple of two integers as the argument to the parameter ` strides `) .
The ` strides ` argument does not accept the " steps " we used , but instead the bytes in memory .
To know them , we can use the ` strides ` method of numpy arrays .
It returns a tuple with the strides ( steps in bytes ) , with one element for each dimension .
Is it possible change it to shift forward by ` N ` observations , as opposed to ` 1 ` ( as implemented in your answer ) ?
Why does simply making the data tuple ( or also a list : list ( map ( list , pos )) works just as well ) solve the pandas error ?
How to replace all value in all columns in a Pandas dataframe with condition
What I want to do is to replace all values that is less than 1 with 0 .
This we then pass to ` df ` as a mask and can then assign the new values as ` df [ df 1 ]` see the docs for further examples
` len ( df.columns.levels )` and ` len ( df.index.levels )` both work
@USER ` len ( df.index.levels )` does not work if you don't have a multi-index
As @USER mentioned above ` len ( df.columns.levels )` will not work in the example above as ` columns ` is not ` MultiIndex ` , giving : #CODE
You probably need something like len ( set ( df.a )) which works on both index or normal column .
Python , use " order by " inside a " group concat " with pandas DataFrame
There's no group concat function in python / pandas , so we'll have to use some groupby .
However , no combination of groupby or pivot is giving me anything remotely like what I want , even though I know this should be fairly simple .
from there you can transform it into your tuples like so : #CODE
DSM's answer , which selects rows using a boolean mask , works well even if the DataFrame has a non-unique index .
` transform ` is used when you want to " broadcast " the result of a groupby reduction operation back up to all the elements of each group .
Selecting with a full boolean mask is more robust than selecting by index values .
and the stack trace : #CODE
This df is creating by ` concat ` on a bunch of * .xlsx files .
Assuming I understand what you're after ( setting aside weird duplicated-index cases ) , one way is to use ` loc ` to index into your frame : #CODE
Create an DataFrame with only index =[ ' C '] and concat : #CODE
I have tried to merge the two dataframes and obtained a new dataframe containing df1 and the columns ` 0.0 ` - ` 23.0 ` matched to the correct date .
Maybe I'm a slow catcher , but I did not manage to transform this into a working solution .
pandas dataframe columns with non-numeric data get deleted on resample
Before the ` resample ` ...
After the ` resample ` ...
When I ` resample ` the data , the ` object ` and ` timedelta64 [ ns ]` columns get deleted .
Like @USER said , it was a case sensitivity problem in table name : I re-wrote the table : ` i.to_sql ( ' stat_table ' , engine , if_exists= ' replace ')` and then it works : ` a =p d.read_sql_query ( ' select * from stat_table ' , engine )`
Why not just do ` train_df = df.iloc [ 0 : floor ( 2 * len ( df ) / 3 )]` and ` test_df = df.iloc [ floor ( 2 * len ( df ) / 3 ): ]`
The index of DataFrame lost its name after append a row
This looks like a bug and happens whether using ` ix ` or ` loc ` if you append e.g. ` df.append ( pd.Series ([ 35,172 ]) , ignore_index=True )` then the index name is preserved , this also happens on ` 0.15.2 `
How can I pivot this Pandas DataFrame ?
If you use ` pivot_table ` instead of ` pivot ` , it works : #CODE
The problem is that ` pivot ` cannot handle a list of columns for the index / columns argument .
I asked a related question here , although I wanted to go a bit beyond expanding the lists into columns so the answers are more complicated than what you need .
Pandas DataFrame , replace value of a column by the value of an other column
for each line , if the value of ' sku ' is the same as the value boost1 or boost2 or boost3 then I want to replace the matching value by the value inside boost4
Each Boolean mask has to be applied to both side of the assignment , so that the dimension and index will match : #CODE
` .loc ` should also be perfectly fine , does this not give the same output as ` ix ` ?
With some looping , query and extract information out of database A , do a bit of combinations to them , and insert them into database B .
The other point here is that effectively you have created a tuple by the way you declared ` Mix ` , you'd be better off making this a list initially so you can append to it like so : ` Mix =[ ' business_unit ' , ' isoname ' , ' planning_channel ' , ' is_tracked ' , ' planning_partner ' , ' week ']
The other point here is that the way you defined Mix will result in a tuple , if instead it was a list then we could append the additional column of interest and all would be fine : #CODE
I can work with manipulating the list or at least having two or more lists referencing them when needed or I could append to the list like you have and then remove on other occasions .
Pandas rolling sum , variating length
But is it possible to apply a cumulative sum every ` X ` days ( or data points ) say , yielding only the cumulative sum of the last ` Y ` days ( data points ) .
If the index is a ` DatetimeIndex ` , you can ` resample ` to a daily frequency , take a ` rolling_sum ` , and then select only the original dates : #CODE
Even though it doesn't start " one week in " in this case ( resulting in sum of 3 in the very first case ) , it does always get the correct rolling sum , starting on the previous date with initial value of zero .
How to replace NaNs by preceding values in pandas DataFrame ?
What I need to do is replace every ` NaN ` with the first non- ` NaN ` value in the same column above it .
I want to perform outer join of two dataframes with the same row index using Pandas 0.14.1 .
` join ` works : #CODE
` concat ` fails : #CODE
Discovered that downstream from the successful ` join ` .
This may cause ` concat ` to fail , though the error thrown isn't obvious .
Back to your idea of raw SQL , that would make a difference , if e.g. the Python code that creates the ` StrategyRow ` instances is slow ( e.g. ` StrategyRow.objects.create() `) , but still I believe the key is to batch insert them instead of running N queries
I want to append the files so that any columns with the same word ( like Bword in this example ) are matched while new words are added in new columns : #CODE
Is there a different approach which could align matching columns while appending ?
IIUC , you can simply use ` pd.concat ` , which will automatically align on columns : #CODE
When I use your approach I find the columns concat by index number rather than by column name ( eg by 0 , 1 , 2 , 3 instead of Aword , Bword etc ) .
I suppose you could always define a function ` f ` that accepts the parameters of column ` A ` , column ` C ` , and column ` L ` and then ` apply ` it to your data frame .
I'm trying to fill a column ( signal ) in a dataframe conditional on another column ( diff ) in the dataframe being compared to 2 variables .
If you want all deliveries per date with the maximum value , use ` transform ` to generate a boolean mask : #CODE
Obviously you would replace the ` 0 ` with the name of the column that your dates are under within your data frame , but I'm sure you get the idea .
Are you trying to merge based on the index ?
Yes , I'm trying to merge by index .
In this case index =d ate.I have tried append , but that seems to concatenate .
As a first step you need to clean your TXT file , at the moment it's a SQL script not a csv file , you could strip out everything up to and including the left bracket and then remove the trailing bracket and any new lines .
Anyway what the following does is to open you text file and remove the ` insert into ` , open and close braces , and quoting characters ( not necessary but I prefer this style ) and any extraneous spaces .
Some other remarks : 1 ) you don't need to loop over the values to use ` to_datetime ` , just provide the column at once : ` rev [ ' rev_date '] = pd.to_datetime ( rev [ ' rev_date '])` , and 2 ) you can use the ` .dt ` accessor instead of the ` map ` function : ` rev [ ' rev_date '] .dt .year `
I have built a pytables table and filled it using append like so : #CODE
not a dataframe with columns based on the Hybrid columns and rows for each append .
Table.read will pull in the entire table , and Table.read_where allows you to apply a conditional statement to filter the data that is returned .
The trick is to create an empty data frame and append in it .
I have multiple Series with a MultiIndex and I'd like to combine them into a single DataFrame which joins them on the common index names ( and broadcasts values ) .
You can of course extend this with several joins , the join solution detects common indices automatically .
Going around , I have found also this solution based on apply method
You could use ` groupby-transform ` to generate a boolean selection mask : #CODE
or perhaps do I need a merge ?
Or , specify ` right_suffix ` and / or ` left_suffix ` in the join .
I think numpy broadcasting ( which I thought should apply here ) goes from last dimension to the first .
` Series.map ` is quite versatile -- not only does it accept sequences , it can also take a callable ( like the Python built-in , ` map `) , or a Series or a dict .
So I downloaded a number of .csv files containing FX data ( tick-level ) off dukascopy.com and I am keen to convert them into Pandas Series objects and then in turn resample them to OHLC .
If I understand correctly , you are asking for a way to algorithmically aggregate the Start Num values into different heats .
This can be done easily using the ` diff ` -compare- ` cumsum ` pattern : #CODE
Transform dataframe to get co-author relationships
Is there a way to transform the DataFrame using pandas to get this results ?
We can add a dummy tick column , pivot , and then use the " it's simply a dot product " observation from this question : #CODE
Note that doing the above will require you to drop the extraneous ' head0 ' column which can be done by calling ` drop ` like so : ` df.drop ( ' head0 ' , axis=1 )`
pandas replace not working for specific values
I've written the following code to replace values in a column : #CODE
Concerning your question about ` data.iloc [ data [ 9 ] > 10000 , 9 ] = ' low '` , the first part creates a Boolean mask which is not compatible with the ` .iloc ` method .
This seems to be similar to an old stack thread Pandas compiled from source : default pickle behavior changed
pandas concat DataFrame on different Index
I have an arbitrary list of ` pandas.DataFrame `' s ( let's use 2 to keep the example clear ) , and I want to ` concat ` them on an ` Index ` that :
is neither the ` inner ` nor the ` outer ` join of the existing ` DataFrames `
I would like to join these two ` DataFrame `' s on an intersecting ` Index ` , such ` my_index ` , constructed here : #CODE
Use existing ` pandas ` functionality instead of building my own hack , i.e. ` reduce ( map ( ) )` etc .
Return views of the intersection of the ` DataFrame `' s instead of creating copies of the ` DataFrame `' s
Before I RTD I thought ` concat ( [ df_1 , df_2 ] , join_axes = my_axis )` was the specific functionality I was looking for , however , you most certainly would know !
return pandas.concat ( map ( lambda x : x.reindex ( index ) , df_list ) , axis = 1 )`
Feel free to post that as an answer ( and accept it if you find that's the best solution . ) The reason why I chose to avoid ` concat ` here is because it [ can raise an error ] ( #URL ) if the index contains duplicates while ` join ` does not .
As far as I know there is no direct shortcut for this , but you can do it using ` map ` : #CODE
Python 2.7 & Pandas : How to replace values at 12:00 with values from 11:55 ?
How do I explicitly say ' replace the values at 19:40 : 00 with the values at 19:35 : 00 ?
I've used panda's broadcasting capabilities to replace values in 1 column ( actually several columns ) at once .
It's possible to give pandas ' [ replace ] ( #URL ) a dict of from ( keys ) and to ( values ) pairs .
Append the rows to your other dataframe using ` df.append ( other_df )` .
Is there any smart way to do this or to apply gensim from pandas data ?
Depending on how you want to have the final dataframe , you can use ` pivot ` for this : #CODE
I want to drop rows from a pandas dataframe when the value of the date column is in a list of dates .
` iconv -f " UTF-16le " -t " UTF-8 " file1.txt file2.txt ` leads to a very weird behaviour : a row in between lines is cut .
perform some simple maths on the frames then drop NaN and unneeded columns #CODE
then I merge the frames together #CODE
Script to normalize subfields in CSV using pandas dataframes
Until I learn how to use map / apply , looping through a pandas dataframe is good enough .
Is there any way I can add / append new items on the same index ?
Are you trying to append the new item into a new column or as a new row with the same index ?
While there are numpy append and insert functions , in practice they construct new arrays from the old and new data .
There is also a list insert , but it is slower .
Or a faster and shorthanded version using map : #CODE
There are several of these " expanding " functions ( mean , variance , max , etc . ) which often come in handy .
` df [ ' ids '] .str ` allows us to apply vectorized string methods ( e.g. , ` lower ` , ` contains `) to the Series
` df [ df [ ' ids '] .str .contains ( ' ball ')]` applies the Boolean ' mask ' to the dataframe and returns a view containing appropriate records .
I want to append a pandas dataframe , ` df ` , to ` table ` in ` mydb ` .
Won't the manual ` executemany ` with an ` insert into ` query also fail ?
Yes , you may be right : with ` sqlite3 ` library , ` insert or replace ` in the query solves this problem with ` conn.executemany() ` .
If I include ` or replace ` in the query here , I get an error : ` ProgrammingError : ( ProgrammingError ) syntax error at or near " or " .
As far as I know , postgresql has no such feature to do an insert with update on duplicate values , see eg #URL
What about performing the ` insert ` for each record in the array and catching the error when the key is present ?
It would be faster to replace the comma separator on all the columns of interest first and just call ` convert_objects ` like so : ` df.convert_objects ( convert_numeric=True )`
How to include strings with Pandas resample
I need to resample the data using Pandas in order to create clean half-hour interval data , taking the mean of values where any exist .
I don't think it's possible to do what you want with the resample function , as there's not much customisation you can do .
Here comes my problem , I would like to apply a PCA on the table which requires the whole DataFrame to be loaded but I don't have enough memory to do that .
The PCA function takes a numpy array or a pandas DataFrame as input , is there another way to apply a PCA that would directly use an object stored on disk ?
which we can then pivot to get the state changes : #CODE
I stack the ` start ` and ` end ` columns on top of each other and then filter with a ` groupby ` and an ` aggregate ` function on the ` length ` .
Then in order to achieve the desired looking output I ` pivot ` the table .
And finally the data frame after the pivot : #CODE
I imagine that @USER ' s solution is more efficient than mine in regards to computational time since the ` append ` method is rather costly because it requires construction of an entirely new object upon each call .
You need to pass param ` ignore_index=True ` to ` concat ` #CODE
Pandas : create a new column in a dataframe that is a function of a rolling window
I have a data frame and can compute a new column of rolling 10 period means
can do the same for rolling medians .
I'd now like to compute other rolling functions of N periods , but can't for the
particular , I want to compute a rolling 10 point Hodges Lehman Mean , which is
How can i turn this into a rolling function that can be applied to a Pandas
If you look in the [ documentation ] ( #URL ) , you can see the ` rolling_apply ` function , which allows you to apply any function in a rolling way .
Your function must take the data inside the " rolling window " as an argument .
In short , it gives us most of the efficiency of the mean , with much of the robustness and none of the shortcomings ( e.g. locking in to a static central value ) of the median .
Pandas replace values in dataframe timeseries
I want to replace one datapoint , lets day 2.389 in column Close with NaN : #CODE
Replace did not change 2.389 to NaN .
Unfortunately in this case it did not work , but if replace would actually find the value , this would be the way to go .
` replace ` might not work with floats because the floating point representation you see in the ` repr ` of the DataFrame might not be the same as the underlying float .
You can then use the boolean mask to select and change the desired values : #CODE
Pandas apply to dateframe produces ' < built-in method values of ...
Perhaps even weirder is that , when I call the functions I'm getting from the apply ( e.g. ` data.output [ 0 ] ( )` , ` data.loc [ 0 ] [ ' output '] ( )`) I get the equivalent of the following list :
I don't know quite what you mean by " a branch is taken " -- does that mean : ` apply ` with a returned ` dict ` is not possible at all ?
Instead of applying whatever function you're calling to a DataFrame , map it to lists of the columns you need
You can pass a function to a ` groupby ` object using ` apply ` : #CODE
Assuming I'm understanding you correctly -- it's weird to have column names being the same as the values of all the elements in those columns -- I think you can simply throw ` groupby ` at it after ` melt ` ing : #CODE
How to resample dataframe with counts into new column and aggregate column into list
And want to resample it by days , create a new column with counts and aggregate the labels into a list .
EDIT : If the order of the elements is important then replace ` list ( set ( x ))` with ` list ( OrderedDict.fromkeys ( x ))` .
Pandas : Proper way to set values based on condition for subset of multiindex dataframe
I wan't to take a subset of a multiindex pandas dataframe , test for values less than zero and set them to zero .
Using the slicers to create a mask looks really useful ( may have to use this in more of my code ) .
Actually , the method on the index already existed for a long time , and because this is so handy , it was added to columns / series through the ` dt ` accessor only recently .
@USER In any case ( however you are totally right there are some docs missing ) , for more compley code I would always advice to use ` .iloc [ ] / .loc [ ]` to be very explicit about label-based vs position-based indexing ( as ` [ ]` can be both depending on the situation , just like ` ix ` which is documented a bit more )
@USER yes , I agree that the territory of [ ] extends into or overlaps with .iloc , loc and ix -- a doc regarding the entire territory would , I think , be very helpful to users .
There are additional examples in the documentation Pandas merge join and concatenate documentation
Just call ` map ` and pass the dict , this will perform a lookup of your series values against the values in your dict , this is vectorised and will be much faster than doing this in a loop : #CODE
The ` get_level_values() ` method can accept a label ( such as ' instance ') or an integer and retrieves the relevant part of the MultiIndex .
You might still need to normalize the data with ` sklearn.preprocessing.StandardScaler ` for instance .
You need to extract a numerical representation for all the relevant features ( for instance by extracting dummy variables for categorical features ) and drop the columns that are not suitable features ( e.g. sample identifiers ) .
Then you can apply a function to each subset .
You can set ` min_periods ` to 1 and it will just give you a rolling mean for all values , otherwise it waits until it has seen at least ` min_periods ` values before it starts calculating the mean .
I want to mean normalize my data frame , when I implement the first version of code I am getting the normalized values , but when I implement version 2 I am getting an error called ` stop iteration ` .
I'm not sure how you can do things within the function , it's just not really how groupby and apply work .
I tried to merge , concat and append but I always have the same issue .
It may also be better to perform a merge in your case : ` df1.merge ( df2 , left_index=True , right_index=True , how= ' outer ')`
Perhaps a simple ` concat ` would work for you .
The code below will apply the lambda function to each row of ` df ` .
Using just the ` pandas.DataFrame.plot ( xerr =) ` , the error bars work so long as the ` pandas.Timedelta ` is in the same units as the ` DatetimeIndex ` Freq .
` RainD.loc [ ' 1971 ']` results in ` AttributeError : ' DataFrame ' object has no attribute ' loc '` .
Apply a lambda with a shift function in python pandas were some null elements are to be replaced
I was assuming a lambda function would automatically apply the function in a sort of iterrows fashion .
index = pd.date_range ( start = ' 2014-01-01 ' , end = ' 2014-01-04 ' , freq = ' 5S '))
I want to reshape a Pandas dataframe to have a new multi-index based on a combination of some of the original columns , and at the same time unstack some of the rows .
` unstack ` can move index level values into column level values .
( I'm not very familiar with Pandas , but this describes a very generic idea - you should be able to apply it . If necessary , adapt the Pandas-specific functions . )
But if we're really trying to squeeze out every drop of performance , we can use the ` searchsorted ` method after dropping down to ` numpy ` : #CODE
The ` apply ` method calls the lambda function once for each row of the Series ,
using Pandas , only use the ` apply ` method if there is no other option .
So ` zip ` , like ` apply ` , should be avoided here if possible .
I wonder if there is a way to query and merge different tables when row numbers between different tables are not the same .
With separate tables I will have to use pandas to do the merge ** after ** the H5 store has been queried .
You need to transform the categorical variables into numbers to plot them .
now I can plot it again ... and drop the added columns afterwards .
I tried backfill and forward fill and replace to get the ' 1 ' to repeat for every row in the ' evidence_id ' column .
You need to assign the result of [ ` fillna `] ( #URL ) so ` df = df.fillna ( ' Null ')` most pandas operations return a copy , some take a param ` inplace=True ` also the [ ` replace `] ( #URL ) needs to be assigned back also
You don't need to call ` replace ` if all you want to do is propagate the last valid value , just do ` df = df.fillna ( metho= ' ffill ')`
possible speed up of pandas apply
I think the main bottleneck is the apply method .
You are right that using ` apply ` here is also a potential bottleneck , since it is calling a Python function once for each row of the dataframe .
Since ` df [ ' Time '] =newstime ` is a boolean mask , this new dataframe ` df [ df [ ' Time '] =newstime ]` makes a copy of data from ` df ` .
Since this uses basic slicing instead of a boolean mask , ` df_later ` is a view of ` df ` .
I finally understand the difference between mask and slicing !
Will you break it and delete or break and replace with a similar feature ?.
I want to replace the #VALUE ! and #DIV / 0 !
If the ` ` is the only problem , just create a map , e.g. ` {int : " Integer " , float : " Float " , ...
Are you using ` eval ` or ` ast.literal_eval ` to parse the values of some line to the best-fitting type , or is there another line , or some other list , with the type information ?
Oh just noticed that you mentioned you're using ` set_context ` , you can also pass that information to the ` rc ` parameter in that function and it will apply to all figures .
is it preferable to append a dict to the list and create the ` df ` only at the end due to superior performance , or just better readability ?
` concat ` ( and therefore ` append `) makes a full copy of the data , and ... constantly reusing this function can create a signifcant performance hit .
How can I cut out the relevant section of both the predictor and the response so that they both have data ?
@USER about nans : you can use pandas ` dropna ` method as in elyase's example , or use the ` missing= ' drop ' argument for the statsmodels OLS model .
You can add append to a list from ` for loop ` and then create a ` df ` from it .
How can I merge the " code " column into a string so the result would look like ...
aggregate the groups using `'' .join `
then aggregate the groups using ` join_digits ` , which multiplies the values by
I want to join these two dataframes but not on rows on which the x and y columns are equal across the two dataframes , but on rows where A's x columns is a substring of B's x column and same for y .
Then I group ` POS ` values and mean normalize the OPW columns and then store the normalized values as a seperate column ` [ ' resid ']` .
The main idea behind ` groupby ` and similar functions is " Split - Apply - Combine " whereby , in general , you :
Apply some aggregate function to each of the individual groups ,
Finally in pandas you need to apply some aggregate function to your groups ( the apply stage ) , we're going to use ` count() ` to count the amount of results .
The DataFrame function between_time seems to be the proper way to do that , however , it only works on the index column of the dataframe ; but I need to have the data in the original format ( e.g. pivot tables will expect the datetime column to be with the proper name , not as the index ) .
Pandas merge not giving expected output with datetime
and then I merge using : #CODE
Also how will this merge using DateTimesy .
You can use the ` xlsxwriter ` engine from Pandas to apply a conditional format to data in an Excel worksheet .
Merge Many json strings with python pandas inputs
Which gets me the correct result , the problem is the ` eval ` usage is very costly .
For example , if I remove the ` eval ` and just deal with the smattering of ` \\ ` that result from ` panel_no_eval_to_json ` function here : #CODE
So my final goal would be to loop through the ` Panel ` ( or my object , that has different keys / attributes , many of which are ` Panel `' s and ` DataFrame ` s ) , and merge the ` json ` streams created from invoking ` to_json() ` into an aggregated ` json ` stream ( which would actually be the flattening data representation of my data object ) just as is performed by using the ` panel_to_json ` function above ( the one with ` eval `) .
From my goals and objectives , you can see that I'm trying to " merge the json streams created from invoking ` to_json() ` into an aggregated json stream "
I was going to put that that simply using ` replace ( " \\ " , "")` didn't accomplish what I was looking for , but I thought that was clear from my goals and objectives .
Have you tried using json.loads ( tmp ) instead of eval ?
In the end , the fastest way was to write a simple string ` concat ` -er .
I decided to translate some of Python code from Peter Harrington's Machine Learning in Action into Julia , starting with kNN algorithm .
I'm not an expert with ` Distances.jl ` , but I think you can replace it with #CODE
More performance could be extracted by doing the transpose once in ` mass_kNN ` , but that required touching a few too many places and this post is long enough .
Use ` ix ` to perform the index label selection : #CODE
I have tried potential solutions using cumsum , concat series.shift ( 1 ) but had a block .
A workaround would be to use ` transform ` to generate a boolean mask for each group which selects maximum rows : #CODE
why do i need the ' astype ( bool ) ?
Try changing ` return mask ` to ` return mask.values ` in the ` onemax ` function .
The ` astype ( bool )` is needed because the ` transform ` method converts the boolean values returned by ` onemax ` to ints .
To use the values as a boolean mask , we need to re-convert them to bools .
Pandas merge two dataframes with different columns
Trying to merge two dataframes in pandas that have mostly the same column names , but the right dataframe has some columns that the left doesn't have , and vice versa .
I've tried joining with an outer join : #CODE
I've also specified a single column to join on ( on = " id " , e.g . ) , but that duplicates all columns except " id " like attr_1_x , attr_1_y , which is not ideal .
I think in this case ` concat ` is what you want : #CODE
Is there a restriction on the number of columns for a concat ?
I'm giving @USER the answer as this method is certainly the easiest way to achieve the append .
How can I iterate over Pandas pivot table ?
I have a pivot table I want to iterate over , to store in a database .
I've played with reshape , melt , various for loops , syntax stabs in the dark , chains of stacks , unstacks , reset_indexes , etc ..
Alternatively , if you want to manually insert data into database , you can use Pandas ' to_csv() , for example :
It's a lot to download , but if successful , will allow you to always have the latest version of the entire NumPy --> Pandas stack .
That being said , I often find it easier to clean and aggregate data in a database instead of via Pandas / etc .
How to melt two data frames in Pandas
In terms of outcome , I want to replace all ` nan ` with ` None ` to then pass it on into JSON
If I read correctly , @USER needs to replace all instances of ` NaN ` in the dataframe with ` None ` .
stack from Github in a virtualenv using Python2.7 :
That's a very convenient way of installing the scipy stack , especially for mac and windows users .
To find values at particular locations in a DataFrame , you can use ` loc ` : #CODE
So here , ` loc ` picks out all of the rows where column B is equal to its minimum value ( ` df.B == df.B.min() `) and selects the corresponding values in column A .
Sorry are you asking to merge the 2 dfs on ` Asset ` column ?
How to append a new column to my Pandas DataFrame based on a row-based calculation ?
Your output shows user ids that are not in you orig data but the following does what you want , you will have to replace / fill the ` NaN ` values with 0 : #CODE
Here we generate the desired column by calling ` transform ` on the groupby by object and pass a string which maps to the ` diff ` method which subtracts the previous row value .
Transform applies a function and returns a series with an index aligned to the df .
I've been through lots of questions in stack overflow but still can't figure this out .
I understand it's returning a Bool etc but basically I want to apply multiple conditionals to a DataFrame ( If And Else , Else if ... ) But continue to get Ambiguous Error asking to you use any() , all()
will evaluate to a ` Series ` of type ` bool ` , not a scalar .
` allFiles ` is an empty list , and ` Sframe = pd.concat ( list , ignore_index=False )` then tries to concat an empty list of data frames which gives you the error .
I don't know how to use apply , or whatever else , to figure this out by group and return a dataframe of only those groups .
I was looking at apply but I needed to use filter #CODE
To join the Dataframes : #CODE
Anyway I can collapse them elegantly , without using the reading from dict and then concat to the list as applicable .
Create a group by on the column you want to reduce over and then apply a function that returns the results of the group by an a list per group .
I'm using the following dictionary and the pandas resample function to convert the dataframe to monthly data : #CODE
Instead of ` M ` you can pass ` MS ` as the resample rule : #CODE
Pass ` custom_start_months ` to the fist parameter of ` resample `
I also found this list of example resample options here : #URL
how to use map function to split character , get min values , and store in a newly created pandas column
Firstly you are missing a trailing parentheses on you line : ` loansData [ ' FICO.Score '] = list ( map (( lambda x : min ( x ) , fico ))` secondly even with this added you get a TypeError : ` TypeError : map() must have at least two arguments .
Actually you have 1 too many leading parentheses , it works if you do this : ` loansData [ ' FICO.Score1 '] = list ( map ( lambda x : min ( x ) , fico ))`
Also , when I tried to count using the groupby function ( rx is another grouping variable that will be in the multiindex ): #CODE
The 10-size group is arbitrary and something I want to be able to shift as occasion permits .
You can just call ` diff ` , the default period is ` 1 ` : #CODE
You can also use ` shift ` which is a more general purpose operation that returns a series shifted by a period ( again the default is 1 ) , in this case though on this size dataset it is slower : #CODE
So you can see here that ` diff ` is ~4000x faster than looping over each row , as it is vectorised it will scale much better than a non-vectorised approach .
Wondering why ` diff ` is not mentioned in this page ?
` diff ` works on all series that are numerical dtypes as such it is a general function , you can always suggest a doc enhancement at [ github ] ( #URL )
This can be done with ` pandas ` using ` map ` to map the data from one column using the data of another .
I just played around with some fake data of very large size , and the map routine is lightning fast and quite robust .
In ` python2 ` the iteration could use ` map ` : #CODE
Down the other portion of my code it stays datettime ( which is the correct chain of events , yes ? ) and so now they both reference the same date but test as not equal so I can't apply DataFrame.update to push data from one to the other .
Is there an easy way to reach in and change the second level types of the multiindex to change those back into datettimes ?
I can't find away to apply this to the whole level of the index at once .
In the construction of a ` MultiIndex ` , the values get changed into ` Categoricals ` .
Note , the easiest way I know to change the MultiIndex of a DataFrame , is to reassign a new MultiIndex .
I couldn't use ` .dt ` it give me an error : ` AttributeError : ' Series ' object has no attribute ' dt '`
At first , I thought this would be quite simple - I used pandas asfreq to transform my daily data to weekly and monthly to calculate daily , weekly , and monthly pct_change to begin the analysis .
The ` asfreq ` method is used to reindex your data .
What you want to do is calculating rolling statistics .
call ` concat ` and pass param ` axis=1 ` to concatenate column-wise : #CODE
For example , as you have no clashing columns you can ` merge ` and use the indices as they have the same number of rows : #CODE
And for the same reasons as above a simple ` join ` works too : #CODE
Not sure why , on my real data , having 1000 rows in each df , ` concat ` gives me 2000 rows but ` join ` works fine .
And the line ` Hist.agg ( len ) .dropna() ` looks like : #CODE
The trouble is , the function I would normally use to fill the gaps here would be DataFrame.fillna() using either the backfill or ffill methods .
If I use backfill instead , I get this : #CODE
Note that I wanted the final csv file to have plot and then all the D100 data in date order , hence the pivot at the end .
I think it's cleaner to use ` transform ` , but unfortunately my tests show it's slower .
@USER ` transform ` instead of ` aggregate ` ?
Select only one index of multiindex frame
Essentially I want to drop all the other indexes of the multi-index other than level ` first ` .
( I wonder if it makes sense to add a native insert option using this approach to help people in this situation , or if we should simply post a recipe . )
Working with an existing excel file , you can drop an anchor in the data block ( Sheet3 ) you want to import to pandas by naming it in excel and do : #CODE
I can not append them into a single data frame since they have different column names and column orders .
how to merge two pandas DataFrames and aggregate one specific column
How to get the aggregate result ?
Also , if you want to use ` pandas aggregate ` functionality you can also do : #CODE
The only disclaimer is that using ` agg ( ... )` is the slowest of the 3 solutions .
It should be quite straightforward to get time objects into a dataframe ( load them as string , then use an apply to transform into a time object )
But I don't know how to transform the z_col into a weight 1D array from the hist2d function .
You could also try the [ ` eval `] ( #URL ) method so something like ` df.eval ( ' B =val ')` which may be faster
In your case you need to replace these column headers and do what you want to obtain from the data frame .
inProp will look like a tuple of my input values for activation , num - a tuple of expected two-neuron output ( either ( 0 , 1 ) or ( 1 , 0 )) and num.argmax() will translate it into just 0 or 1 - real output .
I would like to mask the DataFrame finding which strings contains at least one keyword .
What I would like to do is to mask the whole DataFrame with the all Keywords array , so I can have something like this : #CODE
You need to access the vectorised [ ` str `] ( #URL ) method so : ` df [ ' Tweet '] .str .findall ( r ' | ' .join ( phrases ) , flags= re.IGNORECASE ) .astype ( bool )` should work
@USER , as a side note , although current method is very quick , it could benefit even more if you apply ** compiled ** regexp .
What you can do is use match instead of findall to speed things up ( and cut down on memory usage ) .
I am not 100% agreed on my ** map ** + ** lambda ** combinations , only because I need to control memory usage not to max out .
A simple ` apply ` can solve this .
Another way to achieve this is to use pd.Series.isin() with map and apply , with your sample it will be like : #CODE
Updated : as @USER pointed out , it could be even more efficient to combine both map and regexp , in fact I don't quite like map + lambda neither , here we go : #CODE
@USER , I think speed-wise , using regexp will be faster as my method requires 3 x ** apply ** to transform the string content , efficiency wise , using ** map ** will yield each ** lambda ** whereas ** findall ** may eventually max out memory
If you really are that concerned about memory usage ( which shouldn't be the case here as you are already holding a DF in memory that is bigger or equal in size to the resulting df ) and want to use apply , then at least use regexs in your lambda function .
I still want to know why I can't append this data though ...
As you can see , the Boolean mask has to be applied on both side of the assignment , and you need to broadcast the value ` 1 ` on the ` y ` column .
But how do I know that I can append age to the groupby object like that ?
Sorry , I confusingly used the word " append " .
Yea I gotta force myself to learn the map function .
The one-liner also employs some other tricks : Pandas ' ` map ` and ` diff ` methods and a ` lambda ` function .
` map ` is used to apply the ` lambda ` function to all rows .
There is a built in method for this ` diff ` : #CODE
as pointed out calling ` diff ` here will lose the first row so I'm using a ugly hack where I concatenate the first row with the result of the ` diff ` so I don't lose the first row
Using ` diff ` like this drops the first row .
> final_dat= new_dat2.xs ( temp4 , level= ' Class ' , axis=1 , drop_level=False ) This was used to filter for multiindex ( rows and columns )
And I have tried also join that doesn't work , morevoer I cannot use join in an iterative fashion since doesen't work well with " \ " .
I need this because I want to insert and run a VBA macro after creating the file .
Error : AttributeError : ' Series ' object has no attribute ' dt '
This is a bit odd , your second df has a different interval for the it's index so your call to ` loc ` will fail because that label doesn't exist , can you explain better what you require , are you trying to assign to ' C ' the value of z for the row in df2 where it's index is up to and including the corresponding index value in df1 ?
TIL : Pandas Index instances have map method attributes .
I think I understand what you want , I would append each df to a list and use ` concat ` to make a single df , for each df we can just add a new column for each symbol : #CODE
pandas merge dataframe fill in missing values
One of the first answers I received suggested using merge outter which seems to do some weird things .
I think you want to perform an ` outer ` ` merge ` : #CODE
Please let me know the command , I am trying with apply but it ll only given the boolean expression .
So basically we can groupby the ' id ' column , then call ` transform ` on the ' year ' column and create a boolean index where the year matches the max year value for each ' id ' : #CODE
What you are doing is called [ chained indexing ] ( #URL ) so you are right you are operating on a copy in this case , you should use the new indexing methods ` ix ` , ` iloc ` and ` loc ` to avoid this
Merge / Join 2 DataFrames by complex criteria
My current strategy is to sort both lists by common fields and then run nested ` for ` loops , perform conditional ` if ` tests , aggregate predefined dict with items which were found and those that did not match .
I think it would be ideal if I could merge / join two ` DataFrames ` , but I can't figure it out how to merge on the complex criterion .
The efficient way to join on nearby locations is to use a tree data structure , as described nicely in the scikit-learn documentation .
If you want to merge based on nearness for multiple columns , it's as simple as setting ` join_cols = [ ' d ' , ' e ' , ' f ']` .
Call ` transform ` on the ' measurement ' column and pass the method ` diff ` , transform returns a series with an index aligned to the original df : #CODE
If you are intending to apply some sorting on the result of ` transform ` then sort the df first : #CODE
You can use transpose and count to easily achieve what you want , like this : #CODE
Finally transpose back and assign data frame with replaced columns : #CODE
Assuming I understand what you're trying to do , you could replace ` -9999 ` by ` NaN ` : #CODE
when reading your data , normalize , and replace -9999 with n / a .
on your ` diff ` data frame which produces an output of : #CODE
You can do it in one step with ` X [[ ' b ' , ' c ']] = Y [[ ' b ' , ' c ']]` ( assuming you spell out the columns explicitly ) , but I don't think ` merge ` and ` join ` will give you a way to do the two different operations with a single call .
( The DataFrame ` columns ` attribute gives a pandas Index , and these objects support set-like operations such as difference , union , and intersection . )
@USER : I'm not sure , but my hunch is that anything involving a join will be slower ( or at least no faster ) .
It adds one month and tries to replace the day with the original day .
I would like to add a new column , ` d ` , where I apply a rolling function , on a fixed window ( 6 here ) , where I somehow , for each row ( or date ) , fix the value ` c ` .
One loop in this rolling function should be ( pseudo ): #CODE
I am trying to apply a filter on a series of values stored in a pandas series object .
how can I get from the mask both " strange " and " thing " ?
This should resample your series to daily values and calcualte the OHLC columns automatically .
` to_datetime ` is another option , though for more esoteric formats , ` strptime ` and ` apply ` work wonders .
It looks like what you may want to do is ` map ` the values I can't answer now as I have to go home first but I think something like ` df [ ' D '] = df [ df.A == 0 ] [ ' B '] .map ( df1.set_index ( ' ID ') [ ' value '])` may work , this is some ad hoc code that may not work , I can't test until I get home
Hi , I looked at your data and figured out the error , on your toy example it worked fine because the ` NaN ` values appeared at the end , unfortunately this is not what your real data looked like so you need to mask the left hand side also so the following worked : ` df1.loc [ df1.A == 0 , ' D '] = df1 [ df1.A == 0 ] .merge ( df2 , left_on= ' B ' , right_on= ' ID ' , how= ' left ') [ ' value ']` see my updated answer
Slightly tricky this one , there are 2 steps here , first is to select only the rows in df where ' A ' is 0 , then merge to this the other df where ' B ' and ' ID ' match but perform a ' left ' merge , then select the ' value ' column from this and assign to the df : #CODE
This works as it will align with the left hand side idnex so any missing values will automatically be assigned ` NaN `
Another method and one that seems to work for your real data is to use ` map ` to perform the lookup for you , ` map ` accepts a dict or series as a param and will lookup the corresponding value , in this case you need to set the index to ' ID ' column , this reduces your df to one with just the ' Value ' column : #CODE
So the above performs boolean indexing as before and then calls ` map ` on the ' B ' column and looksup the corresponding ' Value ' in the other df after we set the index on ' ID ' .
What does work is if you apply the same mask to the left hand side like so : #CODE
So this masks the rows on the left hand side correctly and assigns the result of the merge
Do you want to aggregate it , as it does not fit in memory ?
See example here : #URL Not sure this will solve it , but that will do the query in chunks , and you can aggregate or merge them in pandas
Pandas dataframe join groupby speed up
I do some grouping , counting , and finally join the results back to the original dataframe .
But a similar join , takes quite some time ~couple hours : #CODE
I am hoping that there is an smarter way to get the same aggregate value .
Can you post sample data and expected output , at the moment I can only guess that the ` join ` operation looks unnecessary but ` groupby ` is an expensive operation generally
OK , I can see what you are trying to achieve and on this sample size it's over 2x faster and I think is likely to scale much better also , basically instead of joining / merging the result of your groupby back to your original df , just call ` transform ` : #CODE
We can see that my combined code is marginally faster than yours so there's not much saved by doing this , normally you can apply multiple aggregation functions so that you can return multiple columns , but the problem here is that you are grouping by different columns so we have to perform 2 expensive groupby operations .
Thank you Ed , I tried the transform idea an in fact takes longer ... while playing I found a way to make it go faster ...
Transform suggestion by @USER : #CODE
Use groupby , then select one column , then count , convert back to dataframe , and finally join .
float 32 from pandas rolling functions rather than float 64
One problem , however , is that I am using pandas rolling mean and rolling std as part of the processing which seems to automatically cast the results of the operation on the float32 array back into float64 defeating the purpose of the original recasting .
However , I don't see a way to achieve what you want only using parameters passed to hist .
you replace the single ax.hist() call with the 8 lines or so that I showed above ( and further customize the logic / colors as you want ) .
I would like to apply the search on this DataFrame " df " : #CODE
then I've build a mask to filter the DataFrame : #CODE
To get the new ' keyword ' column right even when searching through a longer string , I suggest making the ` mask = [ m.group ( 1 ) if m else None for m in map ( r.search , df [ ' name '])]` and then ` df [ ' Keyword '] = mask ` should return the correct keyword found in each string .
In short , first create a mask and then use this mask for indexing to change values in place : #CODE
Now let's set all NaNs to 1 , via bool indexing , making use of numpy's ` isnan() ` function : #CODE
Sorry , I implicitly assumed that I want to drop the NaNs anyway , so I would just take the difference between the shape before and after .
it does appear to be significantly slower call ` sum() .sum() ` see my update but your ` map ` scales better
How do I drop rows containing all values as zero after column 3 ?
You can subscript the columns , replace ` 0 ` with ` NaN ` , drop any rows that don't have at least 1 non ` NaN ` value and use ` loc ` on the index : #CODE
I forgot the abs !
You can pre-create an axis object using matplotlibs ` pyplot ` package and then append the plots to this axis object : #CODE
Alternatively , you could concat the two series into a m x 2 DataFrame and plot the dataframe instead : #CODE
I want to be able to use all of the built in functionality of pandas , like being able to merge DataFrames together , slicing , indexing , plotting etc .
I also want to keep track of the units of the data so that when I merge , plot , etc .
Then my merge method looks something like this #CODE
how to drop all rows after 49th minute or you want to drop just rows 47:49 ?
I want to drop rows 47:49 .
you can use ` concat ` to concatenate a list of DataFrames #CODE
unfortunately if you want a unique random value for each item there have to be len ( items ) calls .
Numpy , pandas , Matlab , R rolling sum inconsistency , with varying length of history
The problem is that fast rolling window implementations accumulate rounding errors , which do depend on the length of history outside the rolling window .
So when I do a 20 day moving sum for 1995.07.01 , the result will have more accumulated rounding error in the case of running the rolling sum operation on a timeseries since 1990 .
It would be important to keep the performance advantage of the fast rolling operations , i.e. avoiding simply summing the last N values for all dates ( which would be O ( N ) times slower than the faster rolling window implementations ) .
Disadvantage of the approach is that I have to do this for rolling_cov and other rolling functions too , which are slightly more involved .
If the quality flags are the same , then interpolate the value between the two valid values on either side .
I have tried to solve this by finding the NA rows and looping through them , to fix the first criteron , then using interpolate to solve the second .
Need to aggregate count ( rowid , colid ) on dataframe in pandas
You can pass a list to ` isin ` which will return a boolean index which you can use to filter your df , there is no need to loop over and append the rows of interest .
Would it be more efficient to create a second pandas dataframe from the " customer_list " list and then perform a SQL type merge ?
I think you've neglected to pass the ` axis=1 ` param to apply , so it's operating column-wise hence the error .
what you should do is call ` transform ` if you want to assign the result of some groupby operation back to your original df : #CODE
We can test that my ` transform ` method is the same as your last method : #CODE
How to append a list as a row in pandas.DataFrame() ?
I am iteratively reading a log file and parsing / extracting data and would like to append that to a dataframe .
Please post data that reproduces your error , basically you need to return either a Series or DataFrame to append to your existing df , also this is horribly inefficient , what is the format of your data ?
That said , in your case you basically want to broadcast ` average_read_intervals ` over some currently non-existent columns of ` R ` , which you could engineer by changing the shape of ` R ` through duplicating its column ` len ( average_read_intervals )` times .
You can subscript the cols like so : ` for i in len ( df ): if i + 1 !
= len ( df ): # sm.OLS ( returns [ returns.coloumns [ i ]] , returns [ returns.columns [ i+1 ]]) , fit() ` os similar
You can index dataframe columns by the position using ` ix ` .
This would be the value at the intersection of row 0 and column 1 : #CODE
A workaround is to transpose the ` DataFrame ` and iterate over the rows .
does it act differently if you transpose the data ?
Simple pandasql join is failing
Hmm - even the simpler join #CODE
where ` name_map ` is a map created from the ` types ` table above .
The ' transposition ' you are looking for is really [ ` unstack `] ( #URL ) .
I'm looking into using the ` sklearn.linear_model.LinearRegression ` module but am unsure on the syntax to use the ` LinearRegression ` in the ` apply ` function that BrenBarn suggested .
To concatenate , first rename and then join : #CODE
first melt your DataFrame to convert it from wide format to long format , and
I thought about doing an ` apply ` of some sorts to the ` Category ` column after ` groupby ` but I am having trouble figuring out the right function .
you don't need a ` groupby ` here , you just need ` sort ` and ` shift ` .
What is the best way to solve this issue and insert row wise as I needed in this case .
I can solve it by writing the ( virtual ) ' append ' method in my subclass , but in that case I would need to do it for many methods and if I cannot use the inherited methods no sense in subclassing at all ( I can just define the DataFrame as a member variable of my class ) .
In addition , I can't pivot on this either .
If you need help with the pivot then you should include the code that you tried so that others can help to produce a solution .
I'm trying to essentially do the same as a pivot table in Excel would do but using pandas .
I would like to pivot it to get the table arranged as : #CODE
OK , I just realized that ` pivot ` and ` pivot_table ` are two different methods .
Unstack Pandas DataFrame with MultiIndex
What I'm trying to do is to reshape the DataFrame with MultiIndex via ` unstack() ` method .
As the error message says , you apparantly have duplicated values in the index , and ` unstack ` cannot handle these ( multiple values to put at the same place ) .
You can chain a ` .drop_duplicates() ` call before the ` unstack ( ' indicator ')` call if don't need to keep the duplicate indices .
As requested by OP , if you want to implement an ` apply ( lambda ... )` to all the columns then you can either explicitly set each column with a line that looks like the one above replacing `' col1 '` with each of the column names you wish to alter or you can just loop over the columns like this :
but how can I apply this to my entire data frame ??
Python Pandas inner join
I'm trying to inner join DataFrame A to DataFrame B and am running into an error .
Here's my join statement : #CODE
Do I need to correct my join statement ?
Or is there another , better way to get the intersection ( or inner join ) of these two DataFrames ?
use ` merge ` if you are not joining on the index : #CODE
For longer frames it'll be much faster than ` apply ` , but for smaller ones you won't be able to amortize the startup cost and soit might be a little slower .
Drop range of columns by labels
What's the best way to drop all columns between column `' D '` and `' R '` using the labels of the columns ?
Edited to cut out instead of extract .
So you open a file , use ` csv.writer ` to insert a line and then into the same opened files you export the DataFrame .
and then mask the DataFrame : #CODE
So the boolean mask works fine and detect only the tweets containing at least one keyword .
You can use ` loc ` to handle the indexing : #CODE
I was using ` map ` to convert all of them to ` ints ` but I think there might be an inbuilt function in pandas to do this more efficiently .
dt = datetime.datetime.strptime ( dateStr.split ( " ") [ 0 ] , " %Y-%m-%d %H : %M : %S ")
How to merge pandas string in python ( join sql alike ) ?
I'm trying to merge ( or left join sql alike ) for 2 table ( df1 , df2 ) using string value " name " to merge .. similar to sql : #CODE
in python I can use integer to merge / but when I use " name " which is string I got an error .
This work fine if I use integer on , but when I try to use merge table with name ( string ) i got this error below , How can I fix this issue ?
You could read the csv in chunks and only append if the subset meets your condition
Or , to avoid having to use ` df.loc [ mask ]` to remove unwanted rows from the last chunk , perhaps a cleaner solution would be to define a custom generator : #CODE
Better strategy to drop ALL duplicates from pandas.DataFrame ?
Does anyone know a better strategy how to drop ` ALL ` duplicates from ` pandas.DataFrame ` ??
So you only want to drop where both a and b are duplicates , is it true that where there are duplicates in ' a ' there is also a duplicate in ' b ' ?
You can just use the subscript notation with ` loc ` which is label based indexing : #CODE
That ( by which I mean ` iloc `) actually won't work without a tweak for the OP's case , as the OP's index begins with 1 .. could just use ` loc ` , though , no ?
I want to find values inside a interval , and not slice it with loc .
@USER please explain ` loc ` selects labels that match those values as my output shpws , it is not simply selecting the 10th row up to 10000th row .
@USER OK , for example , if I do ' loc [ 10:100 ]' I receive a error point to the inexistence of the index=10 , in case it does not exist in my DataFrame .
I am using Pandas and I have a dataframe which is a merge from multiple different SQL tables and looks something like this : #CODE
Then I want to drop the columns Col_2 to Col_4 .
Use ` any ` and pass param ` axis=1 ` which tests row-wise this will produce a boolean array which when converted to int will convert all ` True ` values to ` 1 ` and ` False ` values to ` 0 ` , this will be much faster than calling ` apply ` which is going to iterate row-wise and will be very slow : #CODE
As you are running pandas version ` 0.12.0 ` then you need to call the top level ` notnull ` version as that method is not available at df level : #CODE
AttributeError : ' DataFrame ' object has no attribute ' notnull '
Thanks for the suggestion , i get an attribute error for isnull as well
Using the ` %timeit ` module running in IPython Notebook the ` for ` loop compared to the ` apply .
According to the ` %timeit ` module running in IPython Notebook , the for loop implementation is over 3 times as fast as a lambda apply .
I thought the for loop is less efficient as l thought the dataframe apply is optimized for matrix type operation like MATLAB .
@USER If by stitch you mean append the column back to the original data frame then you can do something like ` original_df [ ' new_column '] = df_dt ` assuming your ` df_dt ` data frame is a nx1 column vector .
I know that this very possible to do with an apply command but I would like to keep this as vectorized as possible so that is not what I am looking for .
So far I haven't found any solutions anywhere else on stack overflow .
I knew there had to be a way to do this without resorting to an apply function .
I want to create a new DataFrame such that each row is created from the original df but rows with loc counts greater than 2 are excluded .
That is , the new df is created by looping through the old df , counting the number of loc rows that have come before , and including / excluding the row based on this count .
The output excludes the 4th row in the original df because its loc count is greater than 2 ( i.e. 3 ) .
Also , be careful with your column names , since ` loc ` clashes with the ` .loc ` method .
If you want to mutate a series ( column ) in pandas , the pattern is to ` apply ` a function to it ( that updates on element in the series at a time ) , and to then assign that series back into into the dataframe #CODE
While this will work , ` apply ` is the method of last resort : it tends to be pretty slow .
In this case , we can reach for the vectorized ` dt ` accessor , and get the times via ` df [ " date "] .dt .time ` ( after we've ensured the column is datetimelike , anyway . )
How do I found out tz for both dates ?
Length : 50 , Freq : None , Timezone : UTC
but can't figure out how to apply this to my problem ?
What you are asking for can be achieved by using [ ` masking and where() `] ( #URL ) and [ ` shift `] ( #URL )
PS the original problem is hiding another issue that shows up when taking out the shift portion of the function .
The second replaces all 13s with ` nan ` , does a forward-fill ( so that the 13s are replaced by the last non-13 number or nan ) , and then does the usual shift check .
You could use ` pd.Int64Index ( np.arange ( len ( df ))) .difference ( index )` to form a new ordinal index .
Pandas join : Does not recognize joining column
I'm trying to join two data frames : #CODE
That's the data frames , I want to join them over the common column ` TUCASEID ` , of which there are intersections : #CODE
Well , that's weird , the only column that appears in both data frames is the one to join over , but well , let's concur [ 1 ]: #CODE
Despite there being a huge intersection .
Your index values don't match why not just merge them ` df_sum.merge ( emp , on= ' TUCASEID ' , how= ' outer ')` or you just interested in adding the ' status ' column for each ' TUCASEID ' row ? in which case do ` df_sum [ ' status '] = df [ ' sum [ ' TUCASEID '] .map ( emp.set_index ( ' TUCASEID ')`
Dunno but ` join ` joins on index usually , it is strange the behaviour which I can recreate but the other methods I suggested should work
` df.join ` generally calls ` pd.merge ` ( except in a special case when it calls ` concat `) .
Therefore , anything ` join ` can do , ` merge ` can do
Under the hood , ` df_sum.join ` calls merge this way : #CODE
Furthermore , the merge is empty when called this way : #CODE
because the ` left_on= ' X '` needs to be ` on= ' X '` for the merge to succeed as desired : #CODE
do you get an image file when you replace ` plt.show() ` with ` plt.savefig ( ' test.png ')` ?
The only non-comment line is ' backend : agg ' .
I'm thinking that the most natural way to extend this would be to make filldate a list ( or a series ? ) and then have a new mask like #CODE
[ Pandas : rolling mean by time interval ] ( #URL )
@USER Bradley , I'm including days forward in the running mean ( 2 days forward and 2 days back , rather than 4 days back to present . So I'm looking at a central moving average ) though it's simply a shift of days .
Because the dataframe can have missing days or days with more than 1 observation the data needs to be resample a frequency of days , with blank days filled to 0 : #CODE
Adding a solution that gives the average using the number of observations in a 5 day centered window rather than a five day rolling average .
Perhaps I need to do something with the freq or how parameters ?
the ` how ` parameter handles the aggregation of the resample .
If you use mean then the rolling mean will be calculated from the mean of the days ratings when upsampled to the frequency of days .
I'm not looking to resample anything .
Many statistical functions contain a parameters to append the computation weights , for example np.average :
And are there commands for other aggregation functions such as ` median ` ?
In practice I have a number of other columns that stop be taking the stack - unstuck round trip and would like to avoid the processing overhead too .
You indeed do not need to unstack to get what you want , using the ` product ` method to do the multiplication you want .
Nan values when I merge these data frames
I am want to merge two data frames in pandas but I am getting Nan values .
` map ` also works with dictionaries directly , like ` data_org [ ' sex '] .map ( { ' I ' : 0 , ' F ' : 1 , ' M ' : 2} )` .
@USER I am getting an Nan value if I apply this map function directly to my data_org data frame .
It's unclear what you are trying to do here , you performed a conversion of your sex character values to their corresponding int value and then you try to merge this back , why not just convert them in the orig df : #CODE
You can call the vectorised ` str ` methods to split the string on decimal point , join the result of split but discard the last element , this produces for example a list ` [ 2,985 2 ]` which you then join with a decimal point : #CODE
How should I align the text labels against the x tickers in the graph here ?
I need to align the " black " mass as in the picture .
So I need seven boxplot particular adjacent for year 2010 and below the respective seven boxplots for 2011 und 2012 .
For column ' 2nd ' and ' CTR ' we can call the vectorised ` str ` methods to replace the thousands separator and remove the ' % ' sign and then ` astype ` to convert : #CODE
@USER Sounds like the apply isn't working .
A possible solution is based on pandas merge method to create databases on the basis of relations .
2 : reset the indices and merge on the base of the proper labels #CODE
It's clear that one has to merge ` b_reset ` and ` df_reset ` in a relation one-to-many , linking ` level_0 ` of ` b_reset ` and ` index ` of ` df_reset ` : #CODE
I am trying to create a new data frame by indexing just values in the " diff " column .
I can easily drop them later , but this is obviously a hackish solution .
apply to the entire dataframe a user-defined function involving another dataframe in pandas
Also , I am always confused by apply and applymap , what is the difference and when should use one over the other ?
maybe using apply ?
@USER apply won't be as efficient / fast as vectorizing or using where , since basically it has to loop through within python just like you do in your example code ( rather than using much faster numpy / C ) .
@USER : Merge takes two data frame as input .
You could set your Probe and Gene columns as the index and then use ` concat ` as shown there .
Note : this means it may not be as efficient as using concat if you have lots of DataFrames in tmp .
You have to use ` loc ` : #CODE
Without loc , ` df [[ ' QRR ' , ' QTM ' , ' QXM ']]` , pandas is trying to select those columns ( which don't exist , hence the " not in index " message ): #CODE
ok you just need to use loc :) updated the answer
The ` kind= ' hist '` option was added to ` Series.plot() ` in version ` 0.15.0 ` .
merge multiple dataframes into one using pandas
how can I merge all these in to the following data frame : #CODE
should I use merge or join ?
` join ` uses ` merge ` underneath , as a rule of thumb if you are going to join on indices then use ` join ` if not then use ` merge `
For your sample data you can achieve what you want by performing ` concat ` twice , this assumes that the last 2 dfs align with the master df .
The inner ` concat ` concatenates the 2 supplemnentary dfs into a single df row-wise , the outer ` concat ` concatenates column-wise : #CODE
An approach to your real data problem would be to add a ' fruit ' column to each supplemental df , concatenate all these and then merge back using ' fruit ' and ' date ' columns as the keys : #CODE
@USER you could groupby the fruit and date and then set the price , your problem though is that if you have lots of dfs with no identifier in them ( i.e. a fruit column ) then you can't merge them back without setting all the rows that match the dates with the same price , What you could do is add the fruit column for each supplementary df , concatenate all these supplementary dfs and then you can merge them back using the fruit column and date column , that would be how I would approach this
When I stack the years , the new index level has no name : #CODE
Is there a nice way to tell ` stack ` I'd like the new level to be called `' year '` ?
But you can always open an issue with an enhancement proposal to add this functionality to stack ( #URL )
transform column with categorical data into one column for each category
At first I tried using pivot ( with timestamp as an index ) , but that didn't work because of those duplicates .
I don't want to drop them , since the other data is different and should not be lost .
Since index contains no duplicates , I thought maybe I can pivot over it and after that merge the result into the original DataFrame , but I was wondering if there is an easier more intuitive solution .
As your ` get_dummies ` returns a df this will be aligned already with your existing df so just ` concat ` column-wise : #CODE
You can drop the ' cat ' column by doing ` df.drop ( ' cat ' , axis=1 )`
and I want to aggregate the daily data to monthly data , for every subgroup .
The best way to use it is generally to do ` for ix , row in data.iterrows() : ` , not using ` next() ` .
@USER Hayden this is partial string indexing ; does not work with a scalar only slices ( and loc is strict )
What's the most efficient way to do something like this ( which doesn't work since the argument to ` replace ` is a slice ) ?
` get_loc_level ` is the similar of loc , i.e. label based rather than by position :
Where might I apply it ?
UPDATE D: But when I access the entry using loc I got something like \u0102\u02d 8\ xe2\x82\u0179\xc2\u015 , \u0102\u02d 8\ xe2\x82\u0179\xe2\x84\u02d8
I'm trying to merge those DFs in the list so that it produces this : #CODE
You also need to specify it as an outer merge ( to get the NaN rows ): #CODE
Note : I use the transform method here because I want to end up with the same index as the original dataframe .
How to insert dataframe to a data frame in Pandas
What I want to do then is to insert ` pg ` into ` rep1 ` , resulting in : #CODE
Call ` concat ` and pass param ` axis = 1 ` to concatenate column-wise : #CODE
there is no insert point as such with concat , merge or join
how can I specify the insert location ?
And this seems to truncate the column names : #CODE
I want to merge all wheat varieties ( i.e. spring and winter ) into a single , level 0 called Wheat . the out come should be : #CODE
Some timings indicate that both my solution as that of @USER is much faster than the apply ( pd.Series ) approach ( and the difference between both is negligible ): #CODE
please add the full stack trace .
i recently bumped an unexpected issue with pandas rolling funcs .
looks like pandas.DataFrame.quantile member func is consistent with the numpy.percentile func . however the pandas.rolling_quantile func returns diff results . reduce the row number to 5 , the problem will be gone ( all three methods return the same results ) . any thoughts ?
Python pandas apply on more columns
How can I generate more columns in a dataframe using apply with more columns ?
But what if I want to use more than two columns at apply ?
Using the Series constructor within the apply usually does the trick : #CODE
4 ] finally , we merge the two dataframes : #CODE
I had tried similar methods but was too narrow minded on solely using the Datetimeindex where groupby , map , and apply were running into errors .
What I would like to do is calculate the growth over rolling month periods year over year , so for example , I would want to find the value of ` ( 2014 / 01 - 2013 / 01 ) / ( 2014 / 01 )` which is ` ( 15 - 10 ) / ( 15 ) = 1 / 3 ` and save this for the first rolling period .
There will be a total of 12 rolling periods for each ` ID ` .
you can use shift to offset the rows in the dataframe .
shift a copy of the dataframe 12 periods to get the value 12 months ago , minus from the current record , and divide by current record : #CODE
update the sign on the period in the shift function
I think it's easier to use plain ol ' argmax : #CODE
There still is a problem if ' display ' is not only 0 / 1 : ` argmax ` gives the index of the first maximum value , not the first non-zero value .
As you wrote , there is another issue : if ` seen ` is always zero for a given user , ` argmax ` will return 0 , just like it would for a user with a non-zero ` seen ` on the first row .
Pandas Merge on Specific Attributes of DateTimeIndex
Is there a way to perform this merge as I have outlined it ?
Then just do a merge on the whole index .
Pandas resample bug ?
What I want is a new column of data ( or really to replace the modifiedAmount column with one ) that contains "" in cases where the original modifiedAmount was EITHER :
I see this option when I want to apply a scalar result , but I couldn't figure out how to put df into lambda like this : #CODE
you can use ` apply ` column-wise on the whole dataframe .
If you want to form a single string from that you could join together multiple ` p ` texts as lines , eg : #CODE
How to pivot a dataframe in Pandas ?
I would like to transpose the table so that the values in the indicator name column are the new columns , #CODE
What does it do when you call ` pivot ` on it , that you're calling " not much success ?
they are transposing the sorted zipped list , ` zip ( imp , names )` zip together , then sort ` sorted ( zip ( imp , names ))` finally transpose ` zip ( *sorted ( zip ( imp , names )))`
@USER - No , I know the meaning of transpose . but how does zip of zip perform a transpose , thats where I am confused with . in numpy you can transpose an array using .T command . but zip function is used to create pairs right ?? but how does it perform a transpose here ??
@USER you might be able to use slicing to index a range of columns in a single operation and an ` apply ` .
I felt like it is what I am looking for , however I was not able to apply it on my problem .
If you really need to replace column ` H_SC ` with ` H_AVSC ` in your ` finalTable ` , rather than add on the averages :
What I mean by ' intersection ' :
If you mean set intersection then that would imply that you want all the values that occur in both ` SMA_45 ` and ` SMA_15 ` regardless of them occurring on the same day .
Or perhaps you only want an intersection of their values on the same day .
Pandas merge on aggregated columns
... and then group and aggregate by a couple columns ...
Is it possible to merge ` df ` with the newly aggregated table ` gb ` such that I create a new column in df , containing the corresponding values from ` gb ` ?
Can I transform the multi-index of the ` gb ` DataFrame back into columns ( so that it has the ` b ` and ` c ` column ) ?
Can I merge ` df ` with ` gb ` on the column names ?
Is it possible to place the result of the ` agg ` into a new column so that the merge doesn't rename columns ?
The advantage of using ` reset_index ` , I suppose , is that the number of rows in the aggregate table corresponds to the number of aggregation keys ( i.e. [ ' b ' , ' c ']) , which I may want to keep for further analysis beyond the merge .
@USER - you would have to rename the the column names that clashed in order for the merge to not add a suffix .
Whenever you want to add some aggregated column from groupby operation back to the df you should be using ` transform ` , this produces a Series with its index aligned with your orig df : #CODE
There is no need to reset the index or perform an additional merge .
The easiest way ( I see ) to convert the sqlite table into the format Matplotlib's ` pcolor ` requires to draw a heat map is to use the Pandas DataFrame's ` pivot ` method .
Since this ` pivot ` will reorder the column and rows , there is no need to fuss over the order generated by the SQL query .
I think what you are trying to do is use your mask to mask into the original df , print or get the sum , and additionally get the length .
To get the length you need to use the mask with the original df , call ` dropna() ` to drop the ` NaN ` rows , and then call ` len ` on the resultant df : #CODE
I would convert the string to a datetime and then use the ` dt ` accessor to access the components of the time and generate your minutes column : #CODE
Is there a way to store the result of ` value_counts() ` and merge it with / add it to the next results ?
Rpy2 and Pandas : join output from predict to pandas dataframe
I would like to pass back the values calculated using the ` caret ` ` predict ` method and join them to the original ` pandas ` dataframe .
But how can ` join ` this to the ` withheld ` dataframe or compare to the original values ?
loc fails on a DataFrame using the DataFrame's own index ?
My objective is to have ' first ' and other aggfunc like numpy.mean() so that I don't have to do it twice and merge the tables .
You can pass a dictionary to ` aggfunc ` with what functions you want to apply for each column like this :
You could however replace the call to ` cumsum ` with a call to ` argmax ` : #CODE
Note , however , that ` argmax ` returns an index level value , not an ordinal index
Therefore , using argmax will not work if ` x.index ` contains duplicate
So while using ` argmax ` is a bit faster in some situations , this alternative is not quite as robust .
The most obvious way to obtain that would be to stack all the daily results into one dataframe , then group it by day and run the stats on the result .
You can use shift to offset the date and use it to calculate the difference between rows .
So I tried doing ` timeDelta = df.index - df.index.shift ( 1 )` and that generated ` ValueError : Cannot shift with no offset ` .
Why can't I apply shift from within a pandas function ?
But while directly applying shift works , in a function it doesn't work : #CODE
Why not call shift directly ?
Try passing the frame to the function , rather than using ` apply ` ( I am not sure why ` apply ` doesn't work , even column-wise ): #CODE
will pursue further , but is this performant on par with apply ?
My assumption is that this is more performant than apply as ( if ? ) the shift and sum are vectorized .
This throws an error when I try to use the dt specification to create the DF #CODE
** From the [ documentation ] ( #URL ): " The main types stored in pandas objects are float , int , bool , datetime64 [ ns ] , timedelta [ ns ] , and object .
elementwise , i.e. like doing map ( func , series ) for each series in the
The reason I'm asking this is that I can't get shift to work in a formula , but even if the question in the link is answered , I find this question interesting enough by itself .
or ` argmax ` : #CODE
Actually I will be appending 500+ files with various size to append
If you want to drop the rows which have NaNs use dropna ( here , this is the first ten rows ): #CODE
I can see it is possible to multiply col1*Value , then col2*Value and so on ... and make up a new dataframe to replace df1 .
reindex the second frame with the index from the first .
reindex has a paramater called ` method ` that can be used to fill in gaps in the new index .
Where is it that I have to apply ` toarray ` or ` todense ` ?
I want to convert those data into a list where the ` i ` th element indicates the position of the nonzero element for the ` i ` th row .
If you know that you will only have a single ` 1 ` in your row , then you can transpose the original data frame so the indices of your columns from the original data frame become the row indices of the transposed data frame .
Your original data frame is not the best example for this solution because it is symmetrical and its transpose is the same as the original data frame .
It either disappears or is duplicated with each append .
It will add the header , but also replicate it every time you append to the dataframe .
This is one method , I'm trying to figure out a vectorised method , basically you define a function that takes your row and then call apply , passing the function name and param ` axis=1 ` to apply row-wise .
A faster method would be to define a mask and using the bitwise operators : #CODE
there is another method calling ` eval ` : #CODE
The mask method is over 2x faster than the query and eval method for this sample dataset .
The ` apply ` method is actually the fastest method but it will not scale as well as the other methods as this essentially loops over each row .
Unless you're * very * fast at typing , I think the mask method is still slower .
@USER indeed this would get tiresome to type out , I was a little surprised that ` eval ` was a little slower than ` query `
See this post : #URL If not evenly , I believe ` griddata ` will interpolate but I have not tried that so I am not certain .
That is [ utf-16 little endian BOM ] ( #URL ) you should be able to set the encoding to ' utf-16 ' when loading and writing , otherwise just strip it out
Actually My original Data is like this : `" DOEClientID " , " DOEClient " , " ChgClientID " , " ChgClient " , " ChgSystemID "` After using replace and putting data into the text file from original csv ...
I want to replace the rows in A with rows in B where a specific column is equal to each other .
Just call ` update ` : this will overwrite the lhs df with the contents of the rhs df where there is a match in your case replace ` df ` and ` df1 ` with ` A ` and ` B ` respectively : #CODE
python pandas join to one row
I am wanting to join two dateframes together with a left join but want every matched items be be joined to just one row in the resulting dateframe .
Causes the rows in df1 to be duplicated for the merge to work creating what I would call a uni file and not a flat file which I would like .
We can then merge this back to the original df : #CODE
But am struggling to apply it when there are multiple columns in the df1 .
So based off the answer below I did a pivot using the following code : #CODE
Not sure if the pivot eliminates duplicates or what , but it did not come out correctly .
To get counts use the ` aggfunc = len `
It is the length of the array generated by the pivot at the intersection of the values in the row and column ( for ethnicity in your example )
As for why you get your result when you ` zip ` the ` transpose ` , it's because the iterable object that is returned are the column names rather than the column contents
you can use ` group by ` with ` agg ` .
I've written a function called muzz that leverages the fuzzywuzzy module to ' merge ' two pandas dataframes .
EDIT : Added choices= right [ match_col_name ] line and used choices in the apply per Brenbarn suggestion .
I also , per Brenbarn suggestion , ran some tests with the extractOne() without the apply and it it appears to be the bottleneck .
One possibility is to pull the ` right [ match_col_name ]` outside of the ` apply ` , so that you don't recalculate it every time .
You should try profiling your code and testing it with different inputs to see if the bottleneck is really in the apply or in the fuzzy matching itself .
Yes , but that is inside the function you apply , which is called once for each element in the series you apply it on .
If you don't like the post merge names , you can always change them using rename() .
I know I can drop columns from a dataframe or add them later .
First take out elements satisfying predicate ' A ' and then use those groups to apply ' C ' grouping .
In simple words ==> When we bucket on two columns , I don't want the second column ( or inner column ) cut points two be exactly same for all groups of outer column ( first column ) .
that should be one of the behaviours of groupby . apply qcut to subset of values in column C .
Coz already it's been cut into 2 parts via ' A ' cut
is it possible to do grouping via ' A ' , and then for each of those groups apply grouping to column ' C ' via map / lambda's ?
Do I misunderstand fillna , and must specify it that it actually should really replace NaN , instead of just pretending to ?
Post raw data and current code , it sounds like you want to filter the 2 dfs separately and merge them on gene only if the condition is satisfied in both dfs
I'd set the index to you ' Code ' column , then ` reindex ` by passing in a new array based on your current index , ` arange ` accepts a start and stop param ( you need to add 1 to the end ) and then ` reset_index ` this assumes that your ' Code ' values are already sorted : #CODE
You should be able to ` concat ` , in which case before the call to ` reset_index ` , set the index to ' Code ' for your other data and then call ` pd.concat ([ df , other_df ]) .reset_index() ` this should stack them on top each other , hope this is clear
I think you should still be to reindex still even before your first value , I just tried it and it still works fine
You could also do it with a merge .
I get these using groupby / agg : #CODE
My question is , how can I perform a merge on these two rows taking into account that sometimes there will be a comma separated value ( like an array ) in the Performance DF that needs to have find the two corresponding revenue rows from the Revenue DF together - and the date .
That way you can merge and you're basically there : #CODE
How to do a rolling aggregation of data week wise in python ?
Now I want to do a ` rolling aggregation ` and store the aggregation .
By rolling aggregation I mean that say for week 1 ( 2013-06-21 to 2013-06-27 ) I want to add the profit of previous date ( s ) and store it with current date .
Can anyone help me in doing this rolling aggregation ?
A good way to demonstrate this effort is to include the code you've written so far , example input ( if there is any ) , the expected output , and the output you actually get ( console output , stack traces , compiler errors - whatever is applicable ) .
You appear to think that Stack Overflow is here to write code for you , but you have underspecified this and haven't shown any efforts to solve this yourself so we can help you fix that code .
I want to do rolling sum per week .
so for the week of 2013-06-21 to 2013-06-27 there would be one rolling sum but then from 2013-06-28 it should start from fresh and not take the previous value of 2013-06-27 into account
open all csv in a folder one by one and append them to a dictionary or pandas dataframe
You should use a list and ` concat ` : #CODE
That meeds my goal , but turns out : ` assert ( df.column.diff() [ 1 :] <= 0 ) .all() ` ( first value of diff is a NaN ) is slower than just sorting with a mergesort .
:/ But maybe on a bigger dataset diff would payoff .
But I can't find how to apply this to multiple columns
An alternative is to apply : #CODE
Note : it may be a more efficient to do this while reading in as your data ( i.e. during construction / concat ) ...
Pandas eval with multi-index dataframes
I would like to use ` eval ` to substract ` ( ' bar ' , ' one ')` from ` ( ' flux ' , six ')` .
Does the eval syntax support this type of index ?
You can do this without using ` eval ` by using the equivalent standard Python notation : #CODE
Consider the following solution to computing a within-group diff in Pandas : #CODE
It's interesting that you sort entries within apply ( e.g. as opposed to sorting them * before * running groupby and apply ) .
Also , looking at [ this answer ] ( #URL ) from Jeff , I see that he applies ` transform ( Series.diff )` instead of just ` diff ` as in your code .
@USER -Reina in situations like this ( when the function doesn't " reduce ") then transform and apply are the same .
Add the diff column : #CODE
How to pass keyword arguments in reduce over Pandas merge function
What I want to do is to perform outer merge so that it produce the following result : #CODE
One way is to use ` functools.partial ` to partially apply the merge function .
You can see that there are actually two locations where the outer merge must supply a missing value , not just one .
This first creates the boolean vector ( as a one-dimensional array rather than a DataFrame ) , and you can use this as a mask : #CODE
How to resample / downsample an irregular timestamp list ?
Pandas : Performance For Rolling Rank On Large Dataframes
I would like to rank 3 columns over a rolling window of N days .
However both are still considerably slower when compared to rolling mean ?
I have seen that a similar question has been asked previously [ Pandas performance on rolling stats ] [ 2 ] .
However this appears to simply apply the function a fresh to each window and over a large dataset in can take a long time to iteratively apply that function .
I would like the rank of the item within the rolling widow with ( if possible ) the min value for ties .
pandas multiindex assignment from another dataframe
Pandas : Proper way to set values based on condition for subset of multiindex dataframe
Pandas tries to align the index of ` df_ ` with the index of a sub-DataFrame of
So the alignment fails because ` df_ ` has a single index , not the MultiIndex that
If the order of the values is different then it might be easiest to make the index of ` df_ ` a MultiIndex and let Pandas deal with the alignment for you .
You can use the ` pivot ` method specifying which columns should be used as the index , as the column names , and as the values : #CODE
The loc function says the index is incompatible with datetime .
One way is to use ` loc ` and wrap your conditions in parentheses and use the bitwise oerator ` ` , the bitwise operator is required as you are comparing an array of values and not a single value , the parentheses are required due to operator precedence .
We can then use this to perform label selection using ` loc ` and set the ' C ' column like so : #CODE
You can set ` sdf ` to the mask and use this with ` loc ` to set your ' C ' column : #CODE
@USER I've updated my answer basically you just set ` sdf ` to the boolean mask and use this with ' loc '
I don't want a copy , I want a view , but I decided to take a copy as you suggested and then do ` df = df.combine_first ( sdf )` to merge them back together .
I'm trying to replace data in one ` DataFrame ` columns with data from another ` DataFrame ` based on some rule .
You can combine the 0th and 1st using nth and then concat these : #CODE
Apply will only return more rows than it gets with a groupby , so we're going to use groupby artificially ( i.e. groupby a column of unique values , so each group is one line ) .
The way I did it was split the list into seperate columns , and then ` melt ` ed it to put each timestamp in a separate row .
Finally , use ` melt ` to get it into the shape you want : #CODE
If you want to stay in pure pandas you can throw in a tricky ` groupby ` and ` apply ` which ends up boiling down to a one liner if you don't count the column rename .
Then inside that operation I want to split only this list for this date which is what ` apply ` will do for us .
I'm passing ` apply ` a pandas ` Series ` which consists of a single list but I can access that list via a ` .values [ 0 ]` which pushes the sole row of the ` Series ` to an array with a single entry .
Then we simply drop the unwanted index .
Speed wise this tends to be pretty good and since it relies on ` apply ` any parallelization tricks that work with ` apply ` work here .
This might seem elementary to experienced users but I was not super clear on this and it was surprisingly unintuitive to search for on stack overflow / google .
You can use ` ix ` to set multiple columns at the same time without problem : #CODE
You can drop the .keys() , in python 2 this creates a list unnecessarily ( which has to then be converted into a set ) .
Pandas DataFrame : append give index by name to end of df
So you want to prepend those rows and then shift them to end of the df ?
Use the vectorised ` str.contains ` method to mask the df for the rows that contain the string and prepend the str .
As it's currently your index , it's probably simpler to reset the index to restore it as a column , then filter the rows and prepend the text , use ` concat ` on the filtered rows to reorder them and then set the index back again : #CODE
( 3 ) save the header columns for concat later #CODE
( 5 ) output : concat [ header data ] . write output #CODE
I've checked out map , apply , mapapply , and combine , but can't seem to find a simple way of doing the following :
I want to apply this and create a new column in the dataframe with the result .
In pandas it would be done by performing a ` groupby ` and then calling ` min() ` on the 1st column , here my df has column names ` 0 ` and ` 1 ` , I then call ` reset_index ` to restore the grouped index back as a column , as the ordering is now a bit messed up I use ` ix ` and ' fancy indexing ' to get the order you desire : #CODE
dtype : integer , but loc returns float
Do you have to use ` loc ` ?
Upon selecting a row index with ` loc ` , integers are cast to floats : #CODE
To get around this and return an integer , you could use ` loc ` to select from just the ` age ` column and not the whole DataFrame : #CODE
So I drop the last field ( ` refGage `) and it works , so I think it's masked values which only appear in that field .
I used df.ix() to replace the filled-in tokens for what was masked out .
` isinstance ` , ` len ` and ` getattr ` are just the built-in functions .
len and getattr have pretty straightforward names , but not clear what " isinstance " is doing , so having trouble figuring out why / where it's being called by pandas .
You are doing way too much in the apply of the groupby .
I am not exactly sure what you are trying to achieve , but using try except blocks , loc , and mutation of the passed in data INSIDE OF A GROUPBY is quite inefficient .
so you mean , take my dataframe's columns EF , get a copy dataframe from .astype ( uint64 ) , and then maybe replace EF by the copy , and then to_csv() that
Easy way to apply transformation from ` pandas.get_dummies ` to new data ?
you can create the dummies from the single new observation , and then reindex this frames columns using the columns from the original indicator matrix : #CODE
What is the easiest way to insert rows ( days ) in the gaps ?
and then resample ... the reason being that importing the .csv with header-col-name Date , is not actually creating date-time-index , but Frozen-list whatever that means .
You'll could resample by day e.g. using mean if there are multiple entries per day : #CODE
You can then ` ffill ` to replace NaNs with the previous days result .
Now you've updated with ohlc , I'm not sure if you can how= ' ohlc ' on an already ohlc DataFrame ( there is an issue about that though ) , you can resample each column individually ( i.e. high with how= ' max ' , low with how= ' min ' , rate with how= ' mean ') .
How would I aggregate ( sum / average ) the values column by the year , and dico columns , for example .
If I try the groupby function it seems to output some odd new DataFrame structure with bool in it , and all possible years / dico combinations - my objective is rather to have that simpler new sliced and smaller dataframe I showed above .
I.e. , I want to filter out all bool columns that are False .
` loc ` lets you subset a dateframe by index labels : #CODE
Group by lets you aggregate over columns : #CODE
I'm looking for a function something like this fabricated one ( ` pd.get_datetime_limits ( rule , dt )` : #CODE
Then apply the rule to the datatime index , and to a datetime object using ' to_period ` for filtering : #CODE
I think the ` read_csv ` argument ` na_values ` might * replace * missing values with what you choose , and not guarantee that that string is going to be * interpreted * as NaN .
It seems like it is not an encoding problem , since when I replace ` L cke ` with ` NaN ` in the original file by hand , it still causes the same error .
If 2 above is a problem , fixing it is difficult because there is no option to ` read_csv() ` to strip trailing space .
I've found a solution in R language here , but it's difficult for me to translate it to Python / Pandas code .
I think using the ` transform ` method on the groupby object would give you a bit of a cleaner syntax .
You need to mask the rhs also so change ` df.loc [ df.A 0.5 , ' LAST_TIME_A_ABOVE_X '] = df.index ` to ` df.loc [ df.A 0.5 , ' LAST_TIME_A_ABOVE_X '] = df.loc [ df.A 0.5 ] .index ` : #CODE
Merge the values of two columns in a Pandas Dataframe applying a function to deduplicate and concatenate
What I want as an end result is a merge operation with some logic to clean up separators ( # ) and deduplicate values : #CODE
I think you can concat all the dataframes , the result dataframe will has MultiIndex .
I have used concat on dictionaries before .
The intent here is to read in a CSV and replace each IP address in the IP address column with a value returned by randIP() .
I have a column of just IP addresses that I would like to replace in a group-wise fashion and return the modified ( full ) data frame at the end .
Instead , use the grouping operation to create the new IPs , then use ` map ` to map the old IPs to the new ones , and then ( if you want ) assign the new ones back into the DataFrame : #CODE
Using ` groupby ` with ` apply ` will give you a Series mapping old to new IPs : #CODE
Using ` map ` on the old IP column and passing this new Series will map old IPs to new IPs : #CODE
then unstack and fill : #CODE
I tried also to mask the DataFrame with ` masked = df [ ' Key '] .str .contains ( ' our ')` , then ` df1 = df [ mask ]` , but I can't figured out how to make a new column with the new groupby counts .
Drop rows if value in a specific column is not an integer in pandas dataframe
If I have a dataframe and want to drop any rows where the value in one column is not an integer how would I do this ?
The alternative is to drop rows if value is not within a range 0-2 but since I am not sure how to do either of them I was hoping someonelse might .
So to finish I wanted to join the grouped dataframes by the same key I used to group them .
Now I'm getting a " *** KeyError : ' key '" because the grouped dataframes doesn't have a " key " column that would allow me to perform the merge operation .
Is there a way to keep the key column in the groupby so I can perform the merge or a easier way to achieve the same result shown in the desired output ?
Use pandas asfreq / resample to get the end of the period
@USER you'd have to resample the index to hours or minutes and then pass 12 as the second param to ` np.split `
@USER so basically you'd have to create a new index and set the ` freq ` to ` h ` or ` m ` to get an index that is hourly or as minute intervals e.g. ` pd.date_range ( start = startdate , end = enddate , freq= ' h ')` , however why do you need to do this ?
typically you resample data to a different frequency rather than a datetimeindex
In that case you just resample the data and split like I've shown and plot the splits , the other thing you can use is the index method index_slicer but I've never used it so can't advise
If for some reason it won't work , you can apply a strptime function to the str series .
This is because in the end , I want to do the aggregate sum of other variables minute by minute .
pandas - How to aggregate two columns and keeping all other columns
Something like df1 = df [[ ' movie id ' , rating ']] .groupby ( ' movie id ') .agg ( np.mean ) then merge it back in pd.merge ( df , df1 , on= ' movie id ' , how= ' left ')
If all the values are in the columns you are aggregating over are the same for each group then you can avoid the join by putting them into the group .
Then pass a dictionary of functions to ` agg ` .
Note ` len ` is used to count
You could first create a mask ` ma ` and set all the row values before the minimum to ` False ` .
Next , use this mask find the values in each row after the minimum to hit 4 times the minimum ( indicated by ` True `) : #CODE
How would I apply it on a specific slice of the data frame , e.g. I want to run the script for each row , but only using columns 6 through 12 ?
Say you want to calculate the number observations 0 in the column C over the last 3 observations on a rolling basis .
To match OP's desired output , you could then ` stack ` , ` reset_index ` on the names and then remove rows where the two names are the same .
for any value p in x.e , and any value u in y.e , abs ( p - u ) 100
From the reply , it looks like row 90 failed to insert with reason " timeout " .
If you are using an insert id you can simply retry the failed rows , or retry the full request if desired ( though each retried row will count against your table quota ) .
You can use ` loc ` to fetch row index labels instead .
Pandas DataFrame - how to merge groupby calculations back into DataFrame ?
After a groupby aggregate calculation , it produces a DataFrame / data series with only the groupby keys and calculated value .
You just have to break the sql statement down to ` selection ` and ` join ` and ` grouping ` steps and do them manually one after another .
tableA.B column is correlated to tableA.A , but cannot be used in join because tableB doesnt have that value
So you can use it in an inner join to get the rest of the attributes .
Keep that and drop ` 1 ` .
` .isnull ` gives a DataFrame of Bools , flip the Trues / Falses with the ` ~ ` , sum that to get the count of non-null per column , select out the columns where the sum is less than 2 ( column 1 ) and drop that .
( This was tested in Postgres , not in Redshift ; but the Redshift documentation suggests that both ` WITH ` and ` DISTINCT ` are supported . ) I would have liked to do this with a window , to obtain rolling sums ; but it's a little onerous without ` generate_series() ` .
But , when I apply it to my " test " table of 18k x 40k , it leads to a ` MemoryError : ` ( I have 60% of my 32GB of RAM occupied after reading the corresponding csv file ) .
The reason that the dtype is a float is probably because you have missing data ` NaN ` which cannot be represented as an int , you need to decide whether to replace these values or drop the rows , also what are you trying to achieve , it looks like you are trying to build a string representation of your data
Can you help me understand how to reference the column name in the ` dt =p arseDeviceType ( user_agent_string )` call ?
Like : ` dt =p arseDeviceType ( ' user_agent_string ')` .
Or you can slice the columns and pass this to ` drop ` : #CODE
When I replace take off the timezones with ` dates = pd.date_range ( ' 2014-11-01T00 : 00:00 ' , periods=100 , freq= ' D ')` , ` events.plot() ` generates a beautiful graph with no issue .
Simply replace ` fun ` with ` computeErrorForLineGivenPoints ` : #CODE
Basically you can call ` agg ` on the groupby object and pass a dict with column names as the keys and the function to perform as the values : ` df.groupby ( ' A ') .agg ( { ' C ' : np.sum , ' D ' : max} )` see : #URL
I don't understand your question if you did this : ` df.groupby ( ' A ') [ ' B ']` then you explicitly select just column ' B ' and then you can still apply your functions to this only
I am writing a script that reads a csv file and uses the pandas library to create a pivot table .
How do i replace ONLY the NaNs with the contents from second csv file without changing any original values .
interesting pandas loc behavior
I've attached a screenshot of a pd.DataFrame I am using and I've observed some interesting behavior within loc that to me is counter-intuitive / doesn't make much sense to me , as after reading the pandas API I would have thought are equivalent ( at just for example quicker than loc ) .
pandas : merge / combine / update two dataframes
I need merge or combine these two dataframes and use column ' id ' as a key .
If I apply an operation to the Value column I would then like to recalculate the groupby operation : #CODE
I think the calculated groupby MultiIndex should be re-usable to re-calculate the new agg functions ( np.sum is an example ) , but I can't work out how to apply it .
Why do you do the ` agg ` chained with the GroupBy ?
True and I often do , but in this instance I want to be able to use the groupby data / construct to apply the operation on a fresh data frame .
Pandas speedup apply on max()
Not really , the way transform is implemented is basically broadcasting .
My idea was to use a mask with a random number between ` 0 ` and the length of my dataset but I'm not sure how to setup a mask without shuffling the rows around i.e. a method similar to sampling a dataset .
From this I'd like to be able to extract the stats I want and in the order I want e.g. simply display mean , std and median .
` reindex ` can select rows in order : #CODE
You will have to create a separate dataframe for each file and then join or merge all the dataframes .
Subsetting DataFrame using ix in Python
I got some fantasy football data and I'm trying to sort it out so I can later apply on it , the full force of scikit-learn .
Here's solution for you test data , I think you can easily apply it to your real data #CODE
What I did is a groupby to identify where the sub-tables should be cut : #CODE
I have a dataframe full of dates and I would like to select all dates where the month == 12 and the day == 25 and add replace the zero in the ` xmas ` column with a 1 .
On top of this you should use ` loc ` to perform the indexing : #CODE
Just use ` isin ` and apply your condition directly to the columns : #CODE
Ah , in that case you want ` iloc ` ( and not ` loc `) - I've edited .
I want to ignore the hours minutes and seconds and join these two ` data frames ` .
How can I join these two data frames ?
Here's a post which uses ` normalize ` to achieve this .
Awesome didn't know about ` normalize ` . the solution is simple , ` df.index = df.index.normalize() `
How can I normalize the data in a range of columns in my pandas dataframe
I want to normalize the data in each column by performing : #CODE
This would work fine if my data table only contained the columns I wanted to normalize .
I only want to normalize the Age , Income , and Height columns but my above method does not work becuase of the string data in the name state and gender columns .
This will apply it to only the columns you desire and assign the result back to those columns .
You'll need to transpose ( T ) then transpose back to get the dataframe to concat the way you want .
Otherwise you can call ` apply ` like so : #CODE
In the case where the above won't work as it can't generate a Series to align with you df you can apply row-wise passing ` axis=1 ` to the df : #CODE
I tired append function and join but it dint work ..
pandas apply over a single column
how can I apply this function over a single column of pandas ?
but I don't see any ability to apply it to a column of the dataframe .
You can apply to a series also but if you want to use the axis param then you can force a df with a single column using double sqaure brackets : ` df [[ col_name ]] .apply ( func , axis=0 )` 3 . to check if an element is nan you can use the top-level ` isnull ` method so ` pd.isnull ( x )` will return True or False
It's unclear your 3rd question , are you asking how to replace nan values ?
You can use ` fillna ` or ` replace ` to do that . for question 4 . you can do ` df [ ' col '] = df [ ' col '] .astype ( str )` to convert the column to str
You have to decide what to with the nan values , you can either drop them or convert them pre or post conversion , it's not that difficult , also generally on SO it should be 1 question per post
map directly to c-types [ inferred_type- mixed , key- block0_values ] [ items- [ ' operation ' , ' snl_datasource_period ' , ' ticker ' , ' cusip ' , ' end_fisca_perio_date ' , ' fiscal_period ' , ' finan_repor_curre_code ' , ' earni_relea_date ' , ' finan_perio_begin_on ']]`
Python / Pandas : How to quickly pivot datetimeindex on date and time ?
How could I pivot the timeseries so that the index is composed of individual dates and the columns are times ?
I think the best way is to transform the timeseries into a dataframe , then add a date column ( ts.index.date ) then add a time column ( ts.index.time ) and doing a pandas.pivot .
I'm trying to take a dataframe and transform it into a partcular json format .
Here's the json format I'd like to transform into : #CODE
I am ultimately trying to merge two dataframes together , but I am running into an issue when I try to specify the column on which they should be merged .
I was able to aggregate a total dollar amount by state .
I've been looking around at solutions here : Mass string replace in python ?
If you can write this as a function that takes in a 1d array ( list , numpy array etc ... ) , you can use df.apply to apply it to any column , using ` df.apply() ` .
Because you need to replace letters one at a time , this doesn't sound like a good problem to solve with pandas , since pandas is about doing everything at once ( vectorized operations ) .
Instead of printing the words , you can append them to a list and re-create a DataFrame if that's what you want to end up with .
We also append the world directly to our just created output-list ( ` newwords `) .
So for each character we want to replace ( all keys of ` md `) , we do the following stuff .
Additional question from OP : Is there an easy way to dictate how many letters in the string I want to replace [( meaning e.g. multiple at a time )] ?
So there is no intersection of the replaced parts .
Would love an explanation of how though :) Also , is there an easy way to dictate how many letters in the string I want to replace ( e.g. instead of one at a time , two at a time , etc )
It must record the first or last result and insert it into each row .
Pandas already knows that it must apply the equation to every row and return each value to its proper index .
pandas equivalent of R's cbind ( concatenate / stack vectors vertically )
This was the code snippet to read it back ( read it one-column at a time and doing a ` concat ` operation ): #CODE
For instance , I can compute the value for data record 3 by taking ` len ( set ([ 4 , 4 , 6 , 12 ]))` which gives 3 .
I have df1 , which is a rolling dataset I use , updated daily .
So the procedure used to transform the index is : #CODE
join series back to dataframe : #CODE
I have ` n ` small dataframes which I combine into one multiindex dataframe .
Usually some combination of ` concat ` or ` merge ` or ` join ` , but you're going to have to let us know what the result should look like .
you can build the ` multiindex ` after the concatenation .
define a function to replace letters where value 5 with other : #CODE
apply the function to the dataframe : #CODE
I am trying to transpose the following data set #CODE
I would like to transpose the data set so that the years are a column and each seriesname is its own column .
The obvious solution would be convert the column into ` datetime ` which should map to the excel timestamp ( ` 16.02.2015 00:00 : 00 ` -> ` 42051 `) .
I would like to create a function that does this for one imo , and then I can apply it to all of them but I am unfortunately stuck .
I would like to replace the strange characters with ' 0.00 ' but I get an error - #CODE
the code I use to replace the characters : #CODE
I'm not savvy enough with Python to see how I need to map my existing array of texts with this class .
Then transpose the frame and sum along ` axis=1 ` to get the total per word : #CODE
If all you are interested in is the frequency distribution of the words consider using ` Freq Dist ` from ` NLTK ` : #CODE
open a ` .csv ` file on the local computer , append the line of 8 elements , close the file .
So , each price message arrives at your network interface , has to get through the TCP stack ( presumably ) before it hits your CPU .
It seems I should be able to have a rolling window of most recent in-memory data which , whenever it becomes old , is saved by the data management package , not my own code .
For the rolling window of in memory data , use an array or collection of x quotes , as many as you need , and add to the top , like .
You can concat the columns , then convert to a list using numpy's ` tolist() ` : #CODE
I want to end up with a new series , which has the summed up aggregate of all indices : #CODE
Add calculated column to a pandas pivot table doesnt help me
It makes sense to me to multiindex by symbol - so AAPL , AAPL_chg and AAPL_ma would all fall under AAPL .
Find where the adjacent difference is nonzero : #CODE
like Bob said , rolling_mean of the diff , and I'd spend some time with the window size for rolling_mean while deciding what I meant by " rapidly " .
Looks like ` rolling_* ` don't apply to irregular time series yet , though there are some workarounds : Pandas : rolling mean by time interval
Pandas resample time serie in equal parts
I am trying to resample a pandas time serie in N equal parts .
How can I resample in N equal parts , N being > 10
The resample method should use both fill_method= ' pad ' and closed= ' right ' #CODE
This code works for me ( I didn't know you could pass a dict to resample so thanks ) .
I kindof disagree with using df as the variable name here , I also think I'd just use len : ` df.groupby ( " Name ") .filter ( lambda x : len ( x ) > 2 )`
should I declare a new list and add header to it and append it ??
I would like to pivot it to show the following format : #CODE
A simple method would be just to use two ` loc ` calls and filter on gender : #CODE
A better way would be to use ` transform ` on the groupby object which will align the returned data : #CODE
You could do so by doing this ` df [ ' female_avg_income '] = df [ ' female_avg_income '] .fillna ( mehot= ' backfill ')` , also if my answer helped you then you can accept it , there will be an empty tick mark at the top left of my answer
I want to loop through and apply a function to the dataframes within ` groups ` that have more than one row in them .
Since I am only interested in applying the function to values where ` len ( value ) 1 ` , is it possible to save time by embedding this condition to filter and loop through only the key-value pairs that satisfy this condition .
Correct , you can use whatever is faster as long it returns a scalar to ` transform ` .
I am attempting to use the pandas DataFrame.plot ( kind= ' bar ') function to add error bars when I have a multiindex in the MxN DataFrame .
As it stands , I have only been able to get it working by bludgeoning the columns of high / low bounds into an Mx2xN numpy array using the rather brutal tools of newaxis and append .
Then you can apply the following logic .
If you truly want to drop sections of the dataframe , you can do the following : #CODE
Just two quick questions : What if I want to append to an existing excel file instead of writing to an entirely new file ?
Not sure how to append to an existing excel file : one approach would be to read the existing file into a dataframe , concat the new dataframe using ` pd.concat ` , and write to a new excel file .
Aggregate by Project Type and get the min , max , mean and std of " Age " for each of project type .
You can specify the agg functions in a list like this #CODE
How to remove rows from multiindex dataframe with string indices
I have a dataframe with multiindex , from which I want to delete rows according to some index based pattern .
Trying to solve another problem I figured out that one can use the index of a selected part of a dataframe in ` drop ` : #CODE
You have to use loc , iloc or ix when doing more complicated slicing : #CODE
I would have to use ` iloc ` ... what is the difference to ` loc ` ?
However , if I index with ` df.iloc [ mask , :] ` then I get a completely mixed up " frame " index ( swapping between 0 and 1 ) + the data seems to be strange as well .
Check out the docs I link to for the difference between loc and iloc .
I've tried making two series and concat them but it didn't work .
This uses ` loc ` to perform label indexing and then 2 conditions which use ` ` as we are comparing arrays and with parentheses becuase of operator precedence .
Yes that is correct , you need to index using ` loc ` , ` iloc ` or ` ix ` to ensure that the assignment is happening on a view , see the docs : #URL
if I add try to append it using the line data [ ' Stars '] .append ( star ) - I get the following error --
What should be done to append it and the rows that dnt have a star should have NA in it .
I was trying to declare a list of default value NaN then add contents to others and append .
The best way to avoid this type of issue is to firstly have one loop and then to apply a try except value to each item you're scaping .
I'm trying to take an existing data frame and append a new column .
I suspect that using iterrows and ix is inefficient , but not sure how to correct this .
Is there a way that I can apply this to the entire dataframe at once , rather than looping through rows ?
Now , once you insert the above dataframe into python the ordering of the columns gets out of wack .
1 ) Copy the dataframe and directly insert it into ipython WITHOUT using read_clipboad() .
4 ) Select what was pasted in #3 and copy to clip board using Control C
python pandas dataframe join two dataframes
I am trying to join to data frames .
What is your merge giving you ?
How to apply rolling functions in a group by object in pandas
This is two problems : looking at the fruits separately , and doing a rolling mean over irregular data .
Numpy datatypes are different from builtin datatimes like plain ` float ` , so you'd have to give more information about how you want to map between them .
Accessing uninform < dt > < / dt > < dd > < / dd > tags
I want to access all the elements but as I populate the table .. since the dl and dt are not uniform in the order they are place in I get actor name in the running time and at times actor name in format .
Pasted in your question , is that the full stack trace , too ?
@USER Yes , it is the full stack .
Pandas unstack problems : ValueError : Index contains duplicate entries , cannot reshape
I am trying to unstack a multi-index with pandas and I am keep getting : #CODE
Then I try to unstack the location : #CODE
There was [ an unstack bug related to those ] ( #URL ) .
The question is , do you want to aggregate these or keep them as multiple rows ?
Another option ( if you don't want to aggregate ) is to append a dummy level , unstack it , then drop the dummy level ...
Apply rolling mean function on data frames with duplicated indices in pandas
Didn't realize the usage of ' freq ' until now , thank you !
If you have a ` list ` , why not write your processing logic as a separate function that takes a single group as an argument , and then use ` map ` to spray it across the list of groups ?
` DataFrame ` has its own ` hist ` method : #CODE
I don't believe ' hist ' was a supported type in 0.14.1 .
You can test for null values using ` isnull() ` or ` not null() ` , drop them from a data frame using ` dropna() ` etc .
You can check this question and its answer to see how to apply a function row by row : #URL Otherwise , in order to fully answer the question i.e. be able to have all desired fields , we need to know exactly what's in the data ( not just one line ) .
If you replace that list of dicts with columns with native NumPy / Pandas dtypes , then you will definitely save space .
I had a thought that I will try soon : To help the memory issue , I'll create a small dataframe that unwinds the nested column ( per your code ) , then merge that on ID with the main dataframe .
I am working on data analytic's and wanted to use pandas for the same , i have dataset of size ( appox .. 50million rows ) , i was successful in using pandas from command prompt , wrote scripts / programs to load data into dataframes , used pandas functions such as where , joins , merge etc ..
Although apply doesn't offer an inplace , you could do something like the following ( which I would argue was more explicit anyway ): #CODE
Expanding on the above for inplace replacement of data : #CODE
How do I manage to resample this dataset to an hourly resolution so that both time formats are resampled and none of them is ` NaN ` ?
However , doing the resample with ` df.resample ( ' H ')` in the latter case yields a ` TypeError ` .
drop rows that have duplicated indices
I would like to drop the outdated duplicated rows based on values from some of the columns .
For example , in the following DataFrame , how can I drop the first and third rows with ` index = 122 ` ?
Based on your new information , the easiest maybe to replace the outdated data with ` NaN ` values and drop these : #CODE
I want to drop the row that have outdated data .
In the example , I want to drop the row with ` col1 = - `
But it will drop all rows with ` - ` values .
Instead I only need to drop those that have duplicated ` index ` .
You could use ` groupby / transform ` to create a boolean mask which is ` True ` where the group count is greater than 1 and any of the values in the row equals `' - '` .
But this will drop any ` index ` that have ` col1 ` equals to ` - ` .
Instead , I only want to drop rows with duplicated indices .
Another problem with ` transform ` is it tends to be really slow with large number of observations .
Just one comment : ` transform ` takes forever for large data set .
One work around is to create another reduced dataset of index count and then join it to ` df ` to get the ` count ` column .
Are you using ` df.join ( df.groupby ([ ' index ']) [ ' index '] .count() , rsuffix= ' _count ' , on= ' index ')` to avoid ` transform ` ?
That ` df.loc [ mask ]` is slightly faster than ` df [ mask ]` when mask is a boolean array is gravy on top .
Regarding your question on ` transform ` , yes that is what I am using .
@USER , by the way , ` .count() ` takes about 2.5 secs on the same sample . still much faster than ` transform ` .
python pandas concat- reindex column labels
When I combine with concat #CODE
You need to pass ` ignore_index=True ` , this will reindex the rhs if there is a clash : #CODE
Rename the columns after the ` concat ` like so : #CODE
How to transform a time series pandas dataframe using the index attributes ?
How can I apply a function that transforms it into a dataframe like this : #CODE
you can pivot on the ` date ` and ` time ` components of the index :
pivot : #CODE
Looking at the code for ` geom_boxplot ` it doesn't seem possible to adjust what the axis map to : geom_boxplot.py
Great thanks @USER -e , will use horizontal boxplot for now and have a go at extending the geom_boxplot when I got time .
you can use ` map ` and ` strftime ` like this : #CODE
If you were construct a list of data frames and each of those data frames represented one of csv files you could leverage pandas ' concat .
I could use a list comprehension to apply the selected result on the ` get_loc ` function , but perhaps there's some Pandas-built-in function .
I guess something like ` np.arange ( len ( df )) [ pos.values ]` is not what you are looking for ?
Is there a way to have that sparse matrix , but still join it to the labels so I can feed it into an sklearn ML algorithm easily ?
I cant figure out how to append these dataframes together to then save the dataframe ( now containing the data from all the files ) as a new Excel file .
` pd.concat ` will merge a list of dataframe into a single big df .
If anyone in the future wants to try this just replace the `]` with a non-superscript one :)
Use ` shift ` to get a series of the dates you want to subtract , filling in your start date at the beginning : #CODE
Then ` concat ` the new Series to your original dataframe .
I'm not sure of the terminology for this - I did think I could do something with ` cumsum ` and ` diff ` but I think I'm leading myself on a wild goose chase there ...
Your idea with ` cumsum ` and ` diff ` works .
First , we compute the cumulative sum , operate on that , and then go back ( ` diff ` is kinda sorta the inverse function of ` cumsum `) .
Following your initial idea of ` cumsum ` and ` diff ` , you could write : #CODE
@USER you just need to append ` .fillna ( 0 )`
Apply permutation matrix to pandas DataFrame
Mathematically , there is a permutation matrix ` P ` that reshuffles one matrix labels to another , so I can apply this transformation by constructing the matrix .
You could use ` reindex_like ` to reorder the rows / columns of one DataFrame to conform to another DataFrame .
Calculate the weights you'll need to apply to achieve your target age / gender distribution : #CODE
Pivot the sample data to get counts of each color , by age and gender : #CODE
Reindex your weights to align with the pivot table index : #CODE
Get the weighted distribution , then normalize it : #CODE
How to drop rows from pandas data frame that contains a particular string in a particular column ?
I have a very large data frame in python and I want to drop all rows that have a particular string inside a particular column .
For example , I want to drop all rows which have the string " XYZ " as a substring in the column C of the data frame .
which would ' cut out ' the weekend and treat Friday's half-day and Sunday's half-day like a single business day .
` ValueError : cannot reindex from a duplicate axis `
What I'd probably do is create a target MultiIndex and then use that to index in .
I have a DataFrame that I'm looking to use a ` groupby ` on but I'm looking for a little bit of an unusual function to aggregate with .
My first thought was to append the submitting user's email to the group list and then sort the list and then column alphabetically .
We can build a dictionary mapping users to groups , merge groups as we go , and remove invalid emails .
There are examples online for using a .csvreader with python which loops through the rows and adds them as they are being read but I can't find any example of how to take data stored in a dataframe and apply it to a table defined as part of an extract .
I've read all the documentation I'm just not sure how to apply it .
There is no clear instruction as to moving data from a dataframe to a virtual table in python and I know this is a problem for other people using different type of target table ( #URL ) . when I try to insert the dataframe to the table as shown in the tutorials , an error returns : " dataframe not callable "
Based on your traceback , the problem is that you are trying to insert a numpy.int64 instead of a " regular " integer .
When I attempt to insert ( overwrite ) a row in the table it seems rather slow , depending on the size of the table I get something like 0.044seconds .
So , why not structure your dataset ( append rows , update records , select columns ) using an indexed , relational , SQL-engine database beforehand , then import into Python data frames for analytical / graphical purposes ?
I then want to apply poisson sampling noise to all data in the abundance frame .
I have created a pivot table with a three-level multi-index ( Group , Product , and State ) .
A portion of the pivot table looks like this ...
The method in the post above worked very well for reordering and displaying observations in my DataFrame ; however , when I created a pivot table from the DataFrame , the ordering is changed .
I believe I need a way to specifically re-order the pivot table's multi-index level 2 ( state names ) by providing a list , though I have tried and failed to accomplish this .
I am still unable to prevent the pivot table mutli-index level 2 , which represent the state names , from resorting alphabetically .
The DataFrame that the pivot table is based on is reorganized properly by state .
Once I create the pivot table , the ordering is changed .
It seems that I need to specifically reorder the pivot table multi-index , rather than the DataFrame data it is based on .
What happens when you reset the indexes of pivot ?
I also tried creating a pivot table using groupby() and unstack() .
Finally append it to a master list .
Once I'm done 49 of those , create a ( 49 , 37 ) array , and save it or append it to another list that will hold all the time steps .
I will append True if the row has " Verified page " in it .
you can use melt : #CODE
You have to call ` apply ` and pass the data to ` strftime ` : #CODE
Pandas merge dataframes with different datato achieve specific output
I'm experimenting with pandas , and facing with merge problem , f.e
print df_to merge #CODE
df2 = df.merge ( df_to merge , how= ' outer ' , left_on= " 0 " , right_on=0 ) .
Might need to rename 2 of your columns post merge
You need to do a merge with " outer " mode : #CODE
If you make both dataframes have string column names , e.g. ` df_to_merge.columns = map ( str , df_to_merge.columns )` , then I think ` jeanrjc `' s solution will work .
dear @USER how can I merge this two dataframes and achieve x_val to be in x_val's group ? and not to be in exp's group ( I describe it on your answer )
Iterating a DateTimeIndex in Pandas back and forth with a pivot
Thus allowing me to go back and forth after identifying a pivot row in DF .
Normally we would do ` df.ix [ 2 , ' col2 ' : ' col3 ']` but because your index is ambiguous you get the 2nd rather than the 3rd row as ` 2 ` appears as a value in the index at position 1 so the label selection succeeds as ` ix ` tries label selection first and then position selection .
The simplest thing would be to reset the index and then you can call ` ix ` , we have to drop the index again as this is inserted as a column : #CODE
You could reassign the index that would be less typing e.g. ` df.index = np.arange ( len ( df ))`
I used k-fold in cross validation that used arr ( len ( arr )) .
Do you have the same problem if you only try to append the first rows of the dataframe ( ` merged.head() .to_sql ( ... )`) ?
Pandas multiindex from series of dataframes
Iterate through the files and read them into pandas , parse the date and add it to the dataframe , then use ` set_index ` to create your multiindex .
See [ the docs ] ( #URL ) on " reshaping and pivot tables " .
I then open up ipython notebook and run my pandas modification on each file ( I manually edit the name of the location to match the most recent file ) and append it to my running / consolidated file .
( example , open latest raw data from file 1 --> modify it as needed --> append it to a consolidate file --> open latest raw data from file 2 --> ... )
pandas has already saved me a ton of time ( I used to manually open up files and paste them below in Excel , then I learned macros and some addins to merge files together ) , but maybe there is more I can learn to automate this further .
There is also two reports which require a login and then there is also a drop down ( we have to select a range of dates and the report type from drop downs and then manually press the Download button .. I have not been able to automate this at all )
Use ` replace ` : #CODE
you can apply a lambda function to the column ` a ` in your dataframe that returns the lowercase of the string contained , if your correction is just making the string lowercase .
the ` apply function ` method can be extended for other more specific replacements .
If you want to replace certain words - #CODE
However , when I try to introduce a MultiIndex and use the .ix and .xs methods , things seem to go awry .
However , as a workaround , you could extract the single-level index from the multiindex , and use its ` get_loc ` method to find the integer indices corresponding to the desired dates in 2012 :
Rather than passing the dict returned by ` parse_qs ` directly to ` DataFrame.from_dict ` just do a little preprocessing step to search for keys that have list values and replace them with , say , the first value in the list .
You could also transform lists into tuples , e.g. ` tuple ([ ' 336 ' , ' 45 '])` .
That is , ` removeDuplicates ([ col1 , col3 ])` would translate to #CODE
I would probably just use map : #CODE
You can use regex , or whatever , provided the function you pass to map takes each item and returns a tuple .
I want to translate the data use by ' date ' , one day in a row .
There is a ` groupby / cumcount / unstack ` trick which converts long-format DataFrames to wide-format DataFrames : #CODE
I tried to expand my data , and use the code you provide to translate the format I want .
Find the length , ` N = len ( df )` .
Multiindex pandas groupby + aggregate , keep full index
One way to select full rows after a groupby is to use ` groupby / transform ` to build a boolean mask and then use the mask to select the full rows from ` s ` : #CODE
Another way , which is faster in some cases -- such as when there are a lot of groups -- is to merge the max values ` m ` into a DataFrame along with the values in ` s ` , and then select rows based on equality between ` m ` and ` s ` : #CODE
How to join two pandas DataFrame when the join condition column has different name
want to join on ( join , Join ) , which represent the same thing with slightly different tokens
how to solve this join cleanly ?
Just to elaborate on Cel's comment try ` df_left.merge ( df_right , left_on= ' join ' , right_on= ' Join ' , how= ' outer ')` see the docs : #URL
Try renaming the columns to the same name - i.e. the second dataframe to ' Join ' to ' join ' .
df2.columns = [ ' c ' , ' d ' , ' join ']
You can then perform your join on ' join ' .
Currently feel like I'm trying to cut something with a spoon .
Should I set article_id as the table index for example , or multiindex article_id and user_id ?
How can I append new clicks while not having duplicates ?
I tried with a multiindex and appending , but duplicates ( naturally ? ) do not get overwritten .
If I index article_id / user_id , How can I insert new article_id / user_ids that have not appeared before ?
For example , when appending a click for a user , query the whole user , if the click is not in the returned set append it and delete the old results .
` cols ` is depracated , and replace by ` columns ` , and allows you to choose the columns to write .
@USER : dogs-pigs-cats is not a natural ordering , so the sort function won't handle that ( unless you map those values to the ordering you desire ) .
Also you can probably generate the new rows by calling ` apply ` which would be much easier to read than what you're doingnow
Should I just apply a function directly to row.loc ( ' I ') ?
For the email bit we can just use the same regex and call ` findall ` , for the other bit we just pass the func as a param to ` apply : #CODE
@USER OK , I've updated my code , we can use your email regex directly as a param to ` findall ` , for the name bit we can just pass that as the param to ` apply `
So you need to provide a complete path or append the path to where the csv resides to your sys path .
How to prevent from plotting outlier in boxplot in pandas
I have a DataFrame ( called ` result_df `) and want to plot one column with boxplot .
I am trying to apply this function to each row in player_points_position and create a new column ` zscore ` .
After the count drops below a certain threshold , I'd like to drop off the remainder , for example after a 10 case threshold was reached .
Use ` datetime.strftime ` and call ` map ` on the index : #CODE
Then you could convert ` newwords ` into a DataFrame , and use ` pd.merge ` to merge ` newwords ` with ` kw ` by joining on the original word : #CODE
Now when I do shift + Enter it is not printing the result of a command which it was doing earlier .
Cryptic warning pops up when doing pandas assignment with loc and iloc
So as you can see , the statement in my code serves to insert new rows into my DataFrame ` df ` and fill the very last column ( within that newly inserted row ) under ` cycles ` with a ` NaN ` value .
I thought that using ` loc ` and ` iloc ` follows the recommendation already ?
Rather my situation is such that I have missing ` name ` that I need inserted to complete the ` build_number ` and when I insert them in I want to make their corresponding ` cycles ` value at that same row be ` NaN ` .
For example , with reference to my df above in my question , I want to insert additional rows of ` 390 jpeg NaN ` and ` 390 blowfish NaN ` to complete the listing of all ` names ` for the given ` build_number 390 ` .
So you just want to append new rows , can you post desired output to your question
And apart from that , I'm not trying to iterate over each index ; I want to iterate over a list of missing benchmark names and insert each of them into the dataframe .
Yes , that't it , added first condition if index < len ( df1 ) -1 : and error is gone .
I've added the code to the append .
It's code I've taken from elsewhere ; you're right , I don't need to create an empty df and the append the new df to it . dayfirst did work , I was applying it to the wrong place . thanks for your help !
I can group by id and apply a function per group to define wich data is the right one .
I can unstack the dataframe and put ' group ' as columns and apply a function .
One method would be to replace the erroneous data with ` NaN ` , then drop those rows , sort the df by id and group , groupby id and take the first value : #CODE
Pandas apply exponential decay + dataset to function
I'd like to make a function in pandas that calculates the resulting , continuous dataset for my activation function , but I don't know which functions to apply .
So my final goal is to drop values in one column of a ` pandas ` ` DataFrame ` according to some condition on another column of the same ` DataFrame ` , plus several next values e.g. : #CODE
So this will drop the records where the condition is satisfied , but how do I drop the next 3 records after the condition was satisfied too ?
We can use the boolean condition index to slice the df using ` loc ` and set the following values : #CODE
What I want to do is create a pivot table with the registration weeks as keys .
So with this you can only go with option 2 , rolling retention .
how to iterate rows and append column name
I need to iterate over rows and append column name with ' true ' value in a string with output like : #CODE
pandas - apply time and space functions to groupby
To compute differences between adjacent values in a Series , use the ` diff ` method .
The problem is that ` apply() ` takes a very long time and I'm trying to cut it out where I can .
Point is that ` postgres ` doesn't appear have an agg function to compute frequencies for items in a group .
possible duplicate of [ pandas groupby and join lists ] ( #URL )
Any ideas as to why ` concat ` isn't recognizing the loaded , previously pickled dataframes , when they seem to be perfectly good ?
This code works when I drop the check to see if Independent is null .
Also you are performing what is known as chain indexing which may operate on a copy rather than a view , see the #URL basically you should use ` ix ` or ` loc ` instead
But due to a lot of observation cleaning , the row names in my pandas data frames don't usually correspond to ` range ( len ( df ))` .
pandas merge not using sql
I intend to not use sqldf just use merge but cant figure it out ..
I get some example for filtering bull after the merge but not sure whats next ..
Carry on with the ` merge ` based solution .
Head and tail of the datasets are the same* , also mean , min , max , median of weight and time .
Loc vs . iloc vs .
ix vs . at vs . iat ?
` loc ` is label based indexing so basically looking up a value in a row , ` iloc ` is integer row based indexing , ` ix ` is a general method that first performs label based , if that fails then it falls to integer based .
Detail explanation between ` loc ` , ` ix ` and ` iloc ` here : #URL
loc : only work on index
ix : You can get data from dataframe without to be this in the index
It's a very fast loc
I am using Folium to create a map which has a set of polygon that need to be colored based on other data .
Your method is overly complicated you can achieve what you want by calling ` transform ` and passing the function ` count ` and assign this back to the orig df as a new column .
` transform ` returns a series aligned to the original df , see the docs : #CODE
However I'm getting a Value error ' cannot reindex from a duplicate axis ' #CODE
oops I wasn't putting the brackets in concat ... thanks :)
There are various methods of joining , merging and concatenating multiple dfs , in your case ` concat ` is what you want :
` df.to_csv ( loc , index=False , header=False , sep= ' \t ' , mode= ' a ' , encoding= ' utf8 ')` .
Purpose of loc flag : location of file flag = indicates whether I am writing the 1st row , or 2nd row onwrads I dont need to write headers again if the 1st row has been written already .
Try with counting len of both the original & this list ?
` flag ` and ` loc ` are undefined in the snippet .
Ignoring the flag and loc which are irrelevant your df goes to csv fine , but reloading it back in is problematic as it's parsing the data in a weird way so I get 1682 as a len as it's parsing each character rather than treating it as a list of strings
@USER I updated the code , with loc & flag .
[ You can append rows by appending dataframes together ] ( #URL )
This may be an excellent opportunity to highlight the " Split , Apply , Combine " premise and with a simple case use ?
Perhaps Read csv into pandas , index as a datetime object , then groupby day , aggregate sum / divide by count ( aka average ) ?
@USER : that is what I meant to say [ here ] ( #URL ): combine all values for the same day of the year ( not counting leap days ) in different years e.g. , ` sum ( data [ ' 12-01 ']) / len ( data [ ' 12-01 '])` will give you average ( assuming each day has all hours otherwise you have to decide whether to find 2005-12-01 average first and then to combine it with 2011-12-01 average or just use the same weight for all hours values .
I am now having trouble working with that format and I can't even find out how I can transform this data back into the " stacked " or " record " format .
plot the data as a boxplot
If you have a DataFrame where all columns are booleans ( like the slice you mention at the end of your question , you could apply ` all ` to it row-wise : #CODE
Dropping columns or rows does NOT change the underlying MultiIndex , for performance and philosophical reasons , and this is officially not considered a bug ( read more here ) .
Use ` set ` ` intersection ` method to get same keys from the dictionaries .
I have a ` pandas.DataFrame ` called ` df ` which has an automatically generated index , with a column ` dt ` : #CODE
( The only route within Pandas I could see was to do something like ` pd.DatetimeIndex ( df [ ' dt ']) .to_period ( ' h ')` which seems heavy in comparison and changes the dtype of the column . )
@USER : ` merge ` is for join-style operations .
If you just want to concatenate DataFrames , that's what ` concat ` is for .
Edit : So I guess you can strip the date . save it to a variable in the format you desire and put it back into where you need to in your CSV cell
I'd create a df from your list and then perform a left style merge with your database df this will add the values where they exist and put nan where they don't : #CODE
Usually , you can apply the function in one of the following ways : #CODE
A Series does not have an ` iterrows() ` method and ` apply ` applies the function to each column ( not rows ) .
Is there a cleaner built in method to iterate / apply functions to DataFrames of variable length ?
I realize there are methods to ensure you form length 1 DataFrames , but what I am asking is for a clean way to apply / iterate on the various pandas data structures when it could be like-formatted dataframes or series .
Instead of doing either of those things , I think it is better to make sure you create the right type of object before calling ` apply ` .
You have to ensure the columns are integers ( rather than strings ) and reindex : #CODE
and I want to replace any value that has a pattern of a number and then one letter or a number and then two letter with NaN .
` replace =d f.replace ([ r ' [ 0-9 ] [ A-Z ]'] , [ ' NA '])
I was hoping by using the pattern of [ 0-9 ] [ A-Z ] would take care of a number and just one letter and then [ 0-9 ] [ A-Z ] [ A-Z ] would replace any cells with 2 letters but the file stays the exact same even though no errors are returned .
@USER I think @USER is interpreting your data as a string output . pandas does have a [ replace ] ( #URL ) method .
I've updated my answer to also include how it would work with ` replace ` as well .
I see how I got off track trying to use replace now , both methods are very helpful though , and again , many thanks .
If you wish to go the route of using ` replace ` , you need to modify your call .
I have used Transpose instead of Rot and it works if done at an individual level and not on the whole matrix .
I would suggest , to separate cut off and conversion to list : #CODE
Python Pandas : Using ' apply ' to apply 1 function to multiple columns
Essentially , I'd like to know if I could apply ` function ` to ` df ` to get the following output : #CODE
you need to apply the function on each row , for this you need to specify axis=1 #CODE
Use apply / map for pandass dataframe column
But id does not work with ` apply ` method : #CODE
What am I doing wrong and why apply method takes this int index as parameter ?
After reading your data and puting in a dataframe , you can groupby values based on one of the columns ` groupby ([ ' month '])` , and then apply a function on these values , Pandas includes a number of common ones such as mean() , max() , median() , etc .
Or pass any other function using aggregate #CODE
How can I change my original df to append those 2 additional columns ?
How to replace values in pandas with column names
At the first step I used ` df.T ` to transpose the dataframe , and tried something like ` df.value_counts() ` , however I'd
Perform a ` merge ` and pass the list of columns to param ` on ` , the default type of merge is `' inner '` which only matches where values exist in both dfs : #CODE
If your ' id ' column is your index , you'd have to reset the index on both df's so that they become a column in the df's , this is because the inner join will produce an incorrect result if you specify the ` on ` list of columns and also specify ` left_index=True ` and ` right_index=True ` : #CODE
You've not explained how you arrive at the desired df , you can reindex your df to add the missing values but that will work for only one of your columns , what do you do about column 2 ?
Or , if you know the ` a ` column will be sorted for each ` n ` , you could keep track of the last end-value handled by onebyone() and insert some extra rows to catch up to the next start value you're going to pass to onebyone() .
I only know I could use ` concat() ` to combine columns and use ` apply ( lambda xxx ... )` to set up a suitable function .
yes , I could rewrite it to use the length of the column index using the index , something like ` for i in range ( len ( df.columns ))` : col_1 =d f.columns [ i ] col_2 =d f.columns [ i+1 ] and use these col identifiers to index into the df as usual
I am trying to unstack a column in python but it isn't quite doing what I am expecting .
I want to unstack by month so that all the days from a months are in one row .
pandas merge and fill a dataframe with summary data
I'd like to do some kind of merge or join so that I get #CODE
What you want is a left join .
A faster method that doesn't involve renaming / dropping columns is to set the index of frameB to ` title ` and call ` map ` on frameA passing in the other df and passing a series .
If we compare the performance of merging against map , we can see that map is much faster nearly 5X faster : #CODE
How to plot pandas notnull values over time ?
append multiple files into one , skip the first row and sort
PROBLEM IS , come March there is an hour from the preceding month in each file from the Daylight Savings shift , like so : #CODE
Some pseudo code , you don't need to skip the rows here as the concatenation will align the columns for you , also it will be much faster to just make a list of dfs and then concatenate them all together rather than appending one df at a time : #CODE
I would like to create a pivot table as below : #CODE
That operation -- the moving of index labels to column labels -- is the job done by the ` unstack ` method .
So it is natural to unstack ` aggd ` : #CODE
That can be done with ` loc ` : #CODE
Error creating pivot tables in pandas
I am trying to create a pivot table summarizing a csv file , and then email that pivot to myself .
I was apply to modify the format a bit to help me out on another file as well !
I'm trying to pivot data in a way so that the index and columns of the resulting table aren't automatically sorted .
I think I may need to do something with aggregate , but I've been unsuccessful using it so far .
I'd use ` melt ` to turn the frame and then ` size ` : #CODE
I had never used melt before , but it worked perfectly !
My take using ` melt ` and ` pivot_table ` .
I would like to leave the id column but merge in one new file the content , something like this ( i.e. generate a new file ): #CODE
Or merge them ( pd.merge() )
1154 loc : int if unique index , possibly slice or mask if not
Just trying to replace a manually entered variable which works with an equivalent passed in from a csv file .
Pivot Pandas DataFrame into hierarchical columns / change column hierarchy
I want to pivot a dataframe like : #CODE
I was asked not to use a loop to do this , and to use apply , but I cannot figure out the syntax ( see attempts below ) .
But other than that , the for loop you implemented could you also say column [ 0 ] or would I have to transpose my information ?
if you want to store the results in your data frame , I would define a function and then apply it to the data frame like this : #CODE
Use ix should be able to locate the elements in the data frame , like this : #CODE
Another option for problems like this is to use numexpr , see enhancing performance with eval section of the pandas docs .
Based on the explanation of concat ( objs ) here , #URL , which says :
unclear what you expect here concat expects a list-like object , you passed a single df this would work : ` odf = pd.concat ([ df ] , ignore_index=True , verify_integrity=False )`
If memory allows , I would suggest building a list with all the adjacent values for comparison to start with ( in my sample using zip ) , and append the result to a new list , re-assign the whole result list back to the DataFrame after completion .
I may cut the whole list into parts to fit the memory for large datasets .
I then want to replace my data frame with the percentile each observation falls in .
In my new data frame I would then want to replace 24.883 with 0.5 .
I'm trying to merge a dataframe ( ` df1 `) with another dataframe ( ` df2 `) for which ` df2 ` can potentially be empty .
The merge condition is ` df1.index =d f2.z ` ( ` df1 ` is never empty ) , but I'm getting the following error .
After a merge , I would expect all columns to be part of ` dfm ` .
How do you merge two Pandas dataframes with different column index levels ?
but a merge or concatenate on the frames will give me a df with a one-dimensional column index like that : #CODE
Now you are able to do the concat without mangling the column names : #CODE
Thoughts / Solutions with ' map ' or ' transform ' are also very welcome .
of course , one alternative is writing a for loop employing df.loc [ i , col ] and df.loc [ i-1 , col ] , but I generally find apply or transform with functions computationally faster
Have you looked at using the ` shift ` method to shift rows up or down ?
The solution above is very general in the sense that it is letting you use ` map ` while jailbreaking from the row being mapped but it may also violate assumption about what ` map ` is doing and , for example . deprive you from the opportunity to parallelize easily by assuming independent computation on the rows .
Also , there are some standard processing functions built in like ` diff ` , ` shift ` , ` cumsum ` , ` cumprod ` that do operate on adjacent rows , although the scope is limited .
You could use ` groupy / agg ` : #CODE
How can I join these two so that they are in the same dataframe ?
then convert the Series to a DataFrame , and merge it with ` df ` : #CODE
You could use a mask and apply it to the dataframe #CODE
You almost had it , you just need to drop the columns after assigning the values : #CODE
The above is then groupby'd on customer and then we can apply a filter where the number of unique ( nunique ) customers is equal to 2
Pandas get the index of an element in a map function
I could use the index of the element in the Series to get the node in the sessions DataFrame but I did not found any way to retrieve the index of the element passed to the map .
Merge start and finish timestamps and create another column that has value 1 for start timestamps and -1 for finish timestamps .
Would it be better to pivot the dataframe and then plot from there ?
I have a grouped DataFrame which i want to aggregate with a dictionary of functions which should map to certain columns .
Also it doesnt allow for multiple functions on the same column as the ` agg ` function does .
Next , these 3 columns should be combined into one column - the mean of the order numbers - but I do know how to do that part ( with apply and axis=1 ) .
I am looking for a way to merge categories of one object of ` pandas.Series ` to categories of another .
If you now remove category ' b ' , this will end up with a ' merge ' in practice : #CODE
There isn't a built in method to do this , so what I'd do is parse and split the lines and append to a list based on your entire row length : #CODE
I don't think you can control this , your best bet would to filter these values or just replace them manually
If your date strings are of a common format e.g. year-month-day , you could try ` pd.to_datetime ([ ' 1M '] , format= ' %Y%m%d , coerce=True )` - dates not able to be parsed will be replace with ` NaT ` .
A work around , to load a utf file correctly into excel , is to get the program insert a macro into your excel sheet after you have loaded it which imports the data .
But if I use boxplot style , it doesn't work : #CODE
Is there any way ( maybe through matplotlib ) I can get pandas to plot 2 axes for boxplot ?
use a list ` allBlue = [ ]` and append to the list ` allBlue.append ( row [ ' imageName '])` then iterate over the list ` for iter in allBlue ` .
Rolling Window for different Groups
When I was trying to aggregate over month per status , I was doing #CODE
Now I would like to compute a rolling window for each of the groups in status .
I tried to first compute the quarterly window , and then create a rolling mean over 12 months : #CODE
So instead of using ` test ` , unstack ` test ` first so you get two columns -- one for ` emp ` and one for ` unemp ` : #CODE
Then apply the rolling_mean to ` result ` , so you get two columns of rolling means : #CODE
( The rolling mean is placed in the center of the window when ` center=True ` . )
To drop rows - take first observed , you can also take last #CODE
Do the want to append the columns in the same position regardless of their label or do you want the table to widen as new labels are added ?
If you concat vertically matching columns will overlap and new columns will widen the table .
I was under the impression that using the join= " outer " argument on the concat function would just append everything straight up and down vertically without regard to column names .
I was originally under the impression that concat with the join= " outer " argument applied would just append straight up and down without regard to column names .
` join ( str ( u [ k ] .get ( header , 0 ))` looks like here is an issue here .
Instead of using groupby and apply , #CODE
It works when I just pass a single string to that function but not when I apply that function to a column ----> df [ ' Duration '] = df [ ' Avg . Session Duration '] .apply ( convertTime ) .
I can do these by creating 100 list of 1000 dimensions each with a univariate distribution ( mean 0 and 1 ) and can then concat it together in numpy .
Python Pandas Panel4D resample
To resample by dates the index needs to be a DatetimeIndex , TimedeltaIndex or PeriodIndex , not a MultiIndex .
and now we can resample the dates : #CODE
Pity that Panel4D resample feature is not sorted yet .
Read large csv file with many duplicate values , drop duplicates while reading
That particualr column of my file contains perhaps 20 values at most ( sample names ) , so it would probably be faster if I could drop the duplicates on the fly instead of storing them and then deleting the duplicates afterwards .
If I am right , you first need to concat three columns in a string ` value = str ( col1 ) +str ( col2 ) +str ( col3 )` and then use the method to convert it in binary .
to check if the freq was 10hz or 100hz just change the units to ` np.timedelta64 ` so for 10hz : ` np.timedelta64 ( 100 , ' ms ')` and for 100hz : ` np.timedelta64 ( 10 , ' ms ')`
convert the diff series which is a timedelta64 to a datetime64 , use the allclose method , convert it back to timedelta64 then check it is how i'm seeing it .
Assign the result of ` df.groupby ( ' User_ID ') [ ' Datetime '] .apply ( lambda g : len ( g ) 1 )` to a variable so you can perform boolean indexing and then use the index from this to call ` isin ` and filter your orig df : #CODE
Using ipython for interactive manipulation , the autocomplete feature helps expanding columns names quickly .
It belongs in the call to ` Pool ( ... )` , not the call to ` map ` .
I want to group that data set by dist and divide the count column of each group by the sum of the counts for that group ( normalize it to one ) .
One alternative way to get the normalised ' Count ' column could be to use ` groupby ` and ` transform ` to get the sums for each group and then divide the returned Series by the ' Count ' column .
This avoids the need for a bespoke Python function and the use of ` apply ` .
However I would like to join this information with a series containing the number of unique values .
Is there a Pandas-esque way to get unique as a Series so I can join the result with valueCountSeries in a new DataFrame ?
An example DataFrame would really help here , IMO it's unclear how you're hoping to merge these ( what the result should be ) .
I would now like to resample and " pivot " the dateframe created from the logfile .
Resample first , then groupby / pivot ?
To be more specific I the cells should contain the count for colA / colB combinations for each specific resample timeintervall .
Ii would just say to let them in and then to drop rows that have ` -- ` in the first column .
The purpose of the code is to join 2 CSV files and works as exptected .
do you just want to drop all rows that == 0 ?
I want to delete / drop row whenever it is found on the other df that i have .
I don't know if this is pseudo code or not but you can't delete a row like this , you can ` drop ` it : #CODE
If I replace the last line by ` df = df [ df [ ' A '] .isin ([ ' UBS '])]` I do get the expected error message , ` cannot reindex from a duplicate axis ` .
Pandas tries to align these two Indexes , and as part of that process the values in the indexes are compared .
Now if you insert #CODE
which you could use to build a boolean selection mask : #CODE
I am really sorry for being that naive , I promise to learn as much as I can if someone could guide me towards the right direction ( regarding the theory and technologies to apply ) .
Pandas : interpolate missing rows and plot multiple series in dataframe
So , effectively we want to unstack and transpose so that 0 / 1 are the index , which we do by : #CODE
Do I really need to ` apply ` and iterate through each row , or is there a more efficient alternative ?
You could use ` map ` here : #CODE
` map ` can take a dictionary , Series or function and return a new Series with the mapped values .
It is also very efficiently implemented ( much more so than ` apply ` , for example ) .
I would like to switch the data frame to a structure where the different dates ( currently in the columns ) become the lower level of the MultiIndex , and the ` state ` becomes the upper level of the MultiIndex .
This can be accomplished with ` pivot / transpose ` : #CODE
Many pandas functions ( like ` stack ` and ` unstack `) accept level parameters which either accept names or integers .
I am trying to map the data objects from a Python-Pandas dataframe to a Tableau TDE file compatible datatype .
I think ` transform ` is the right approach , but you need to grab the column directly : #CODE
but with the ` transform ` broadcasting the results back across the groups .
One more question : what is the function used to ` transform ` not a standard function like ` mean ` , but a customized function say ` myfun ` .
also , when the ` transform ` function refers to the ` mean ` function , does it use ` pandas ' mean ` function ?
You replace the series using list comprehension , setting the day to 1 but using the original month and year .
With pandas , how do I calculate a rolling number of events in the last second given timestamp data ?
For this I would like a rolling data set of something like : #CODE
Finally , create a new data set by calculating the length of each sub group ( which represents the number of requests in that second ) and aggregating the results into a single DataFrame ( something like ` new_df = groups.aggregate ( len )`)
You first need to transform the timestamp into a string which you then groupby , showing the count and average service times : #CODE
One simple method would be to assign the default value first and then perform 2 ` loc ` calls : #CODE
Python - pandas ==> WINDOW aggregate
I'm looking for the pandas equivalent of the SQL window aggregate function OVER() #CODE
I don't think you need to groupby the week day also , just call ` transform ` and pass func ` nunique ` which will return the number of distinct days :
One method would be to store the result of an inner merge form both dfs , then we can simply select the rows when one column's values are not in this common : #CODE
Another method as you've found is to use ` isin ` which will produce ` NaN ` rows which you can drop : #CODE
If match should only be on row contents , one way to get the mask for filtering the rows present is to convert the rows to a ( Multi ) Index : #CODE
If index should be taken into account , set_index has keyword argument append to append columns to existing index .
If columns do not line up , list ( df.columns ) can be replaced with column specifications to align the data .
mask = np.random.rand ( num_samples ) < .7
That just a boolean mask that is later used to set some items to np.nan .
I think that a merge might be even faster .
What is a better way to apply the function to each row ?
Just do an inner merge and discards all the ` NA `
` index = pd.data_range ( start = df.date [ 0 ] , end = ' 2015 / 03 / 06 17:07 : 05 ' , freq = ' S ')` in the pd.Series ( ) function .
Resampling approach doesn't seem to work because I can't resample when indexed by Id and Date ...
All I want is to insert data from the dataframe to a sqlite db and add a column which can act as a primary key
Pandas tries to insert the index of the dataframe as a column ' index ' , but as there is already an column named ' index ' , it uses a next option , namely ' level_0 ' .
But as you append to the table , this column is not present .
specify to not insert the index of the dataframe : ` dataFrame.to_sql ( ...., index=False )`
or you can let ` to_sql ` create the database table itself ( no append ) , then table definition in python is not needed .
Make the dates datetimes then use the datetime replace method .
Pandas scalar value getting and setting : ix or iat ?
I often hear ` ix ` being generally recommended .
However , after some tests , we found that ` ix ` would be comparable or faster for reading cells , while ` iat ` much faster for assigning values to cells .
I do not provide the results , but the trend is exactly the same - ` ix being much faster for getting , and iat for setting individual cells ` .
It seems to make sense that time spent on ``` iat ``` is about the same each way , but then why is ``` ix ``` so much slower when it's on the left hand side than right ?
This information suggests that calls to get values using ` ix ` are limited by ` get_value ` in the best case and calls to set values using ` ix ` would take a core committer to explain .
Try setting the drop inplace parameter to True .
The append gives a diagonal matrix even when ignore index is false : #CODE
Is there a better way of doing this ? and is there a way to fix the append ?
I have looked into ways of creating a function to do this , but confused as to how to map and / or apply it in my case , especially the part returning the result as a new column .
Just trying to find the most elegant way to apply a really simple transformation to values in different columns with each column having it's own condition .
I was thinking the where function in pandas would come in handy here but wasn't sure how to apply it .
I am trying to insert a column , say ' new_col ' , that calculates the percent change in VAL that is grouped by ID .
What you need to do is use ` apply ` to apply a function that calculates what you want .
What you want can be calculated more simply using the ` diff ` method : #CODE
However , it is good to be aware of how to do it with ` apply ` because you'll need to do things that way if you want to do a more complex operation on the groups ( i.e. , an operation for which there is no predefined one-shot method ) .
I also ran into problems when trying to resample the data at 512Hz ( double the frequency ) .
If I change the sample period from ' 3906U ' to ' 2S ' and resample to
Refer to [ 10.1 . Analyzing the frequency components of a signal with a Fast Fourier Transform ] [ 1 ] .
That error is an unrelated error which I have no context for as for proposals I don't think I have time to be honest , if it's a real job you may be better posting something on careers stack exchange
But , when I try to transform the dictionary into a data frame it somehow overrides my previous sorting and retrieves an alphabetically ordered list of state names .
To transform the dictionary into a data frame I am using : #CODE
` pandas.core.groupby.DataError : No numeric types to aggregate
doing multiple operations like this will append to the rhs store .
I want to sum the Fill Qty per order then compare it to Order Qty , Ideally I will return only a dataframe that gives me Order ID whenever aggregate Fill Qty is greater than Order Qty .
Why not use for in for to insert every field ?
Change the print line with insert into PostgreSQL .
( After a long stack trace , the next end ) #CODE
I'd like to align the index so that it's in line with the column headers .
Essentially , I'd like to keep the column headers where they are , but shift everything up by one cell .
What worked for me was to reset the index so that ' Date ' becomes an ordinary column , then call the ` dt ` property ` date ` to assign back just the date portion and when writing to excel pass param ` index=False ` : #CODE
Can you please help me understand diff between annotate and text ?
Python Pandas - Main DataFrame , want to drop all columns in smaller DataFrame
[ Not sure if this is important , but ' public ' was created using the ' join ' function ] .
The expensive way to do this is to merge the two DataFrame objects , calculate correlation , and then throw out all the stock to stock and industry to industry correlations .
Again we could perform 2 inner merges this would merge on the values that exist in all dfs : #CODE
How about treating each column as a set and then taking the intersection : #CODE
Pandas NameError : name ' merge ' is not defined
Trying to merge two data frames :
dt [: 3 ] gives me :
I need to merge two tables by yearID and teamID .
I couldn't find explanation online for this error for ' merge ' .
Ideally I need to merge two tables and sort them by teamID by yearID .
My goal is to merge these two tables to create a new one , that will show wins and salary for each team for each year .
The error you're receiving is because you're not calling the pandas module along with the merge method .
When I insert pandas Series into dataframe , all values become NaN
This works fine for me too , but my own data doesn't , unless I append the " .values " .
Generally it's less compact ( at least before zipping ) and almost always slower , but you shouldn't lose any info unless you intentionally round or truncate before writing out the values .
If ** arr ** contains extra elements not in ** frame.index ** then they will be added with NaN values which you will then need to drop from the ** ans ** table .
Dataframe Replace Regex
I am currently trying to replace a set of str values with a int value in python for my Dataframe .
I have read through the wiki and know how to replace with word boundary etc .
But I am wanting to replace any occurrence that begins with a with the value 1 , b with the value 2 , etc .
of which i want to transform to #CODE
Hence why I am trying to transform the abcdefg responses with 12345678 .
You don't need a regex here , just create a lookup table and apply to your DataFrame's column based on that column's first character , eg : #CODE
To apply this to all columns , then loop over the columns : #CODE
@USER then just apply it to all relevant columns ?
Your output from your starting df is wrong , the last row should be [ 2 , 4 ] , aside from that we can call ` loc ` on the index generated by a boolean filtered df plus drop any ` NaN ` values : #CODE
Pandas efficiently normalize column titles in a dataframe
So it should be more efficient as to function call overhead , but I'd personally view it as more readable by having a ` dict ` of keys , which are the " to " values , with the values being the " from "' s to replace .
You can groupby the user_id column and then call ` apply ` and pass a lambda which filters the results where the start time is equal to the max value , we want to generate a boolean index from this .
We can then call ` reset_index ` but due to the way the groupby was filtered we will get an error with duplicate columns so we have to drop this duplicate column : #CODE
The inner boolean condition produces a boolean mask on the multi-index , this is then needed to pass to the lambda to produce the above : #CODE
thank you , the ` apply ` version actually worked but not the direct ` unique() ` version .
If you knew the structure of the JSON , you could create a DataFrame using MultiIndex , but the JSON structure would need to be fixed .
You're expecting the last rolling sum to be the results of ` 0+0 ` , but it's not , it actually something like this : #CODE
I think the best thing to do in this case is to use numpy's ` around ` function and replace the second step above with : #CODE
I've got a DataFrame who's index is just datetime.time and there's no method in DataFrame.Index and datetime.time to shift the time . datetime.time has replace but that'll only work on individual items of the Series ?
Extending on @USER Haffner suggestion , there is also join #URL
Assuming that ' cust ' column is unique in your other df , you can call ` map ` on the sales df after setting the index to be the ' cust ' column , this will map for each ' cust ' in budget df to it's sales value , additionally you will get ` NaN ` where there are missing values so you call ` fillna ( 0 )` to fill those values : #CODE
The solution below uses a lambda function to apply a regex to remove non-digit characters .
Solution is great . but when I apply this to my original data frame I am getting an error " invalid literal for int() with base 10 : ' 16a '"
Didn't like try to transform ' a ' into an integer before .
Recently I received a ` ValueError ` because the data I was trying to append to one of the columns is longer than the declared column size ( 2000 , with ` min_itemsize `) .
Do I append new rows the wrong way that causes the file size to multiple with every append operation ?
I tried adding ` min_itemsize ` for all data columns in the append line per suggestion in this thread : pandas pytables append : performance and increase in file size : #CODE
In Pandas , what is a good way to select sets of arbitrary rows in a multiindex ?
I'm in a situation where I'm trying to do a lookup on a multiindex dataframe , with indices from another dataframe : #CODE
The main trick is just to normalize the data such that User1 is always the lower number ID .
For more concise code that avoids creating new variables , you could replace the middle 3 steps with : #CODE
However , it ( 1 ) returns an empty dataframe and ( 2 ) does not replace the NaNs which result from the merged cells .
What is an apposite function of ` pivot ` in Pandas ?
pandas - show results of apply next to original dataframe
I have a pandas DataFrame , then I apply a function to a bunch of columns and I get a new result column .
You can call the vectorised ` str ` method on the ' match_up ' column to split the string , map these to int and create a list so we can filter the second df on to create df2 and df3 : #CODE
so I can use merge to merge these 3 data frames right . but what would be the ` on ` value while merging ?
To remove rows with empty strings , make a boolean mask , e.g. ` df [ ' people '] !
This will take any column and convert it into a pivot of categorical indicators .
pandas merge by coordinates
I am trying to merge two pandas tables where I find all rows in df2 which have coordinates close to each row in df1 .
Is the problem simply that you think ` append ` acts in-place ?
One way to calculate this is to use ` apply ` on the ` groupby ` object : #CODE
In this situation you can't really use the recommended ` any ` , ` all ` etc . as you are wanting to mask your df and only set the values where the condition is met , there is an explanation on the pandas site about this : #URL and a related question here : ValueError : The truth value of an array with more than one element is ambiguous .
You are going to need to extract the strings and convert the to ints in order to ` merge ` correctly ...
When I apply this on my real data frame ( problem set ) .
` df.stack() ` creates a multiindex .
Generate ` n=100 ` draws to get ` z ` such that ` len ( z )= 100 `
I am expecting to get a DataFrame or array of dimension ( len ( df.x.unique() , n=100 )
so if I merge these three data frames on ` team_df3 ` and ` team_df2 ` values I am not getting the desired out put .
It would probably make sense to add the year columns to df2 and df3 so that the year could be used as an additional column to perform the merge on to avoid this ambiguity
@USER but again I have to drop the year column from the final data frame right ?
You can drop after if necessary but I think you need it to avoid this ambiguity , also having a column containing multiple identifiers is a very bad idea
@USER ok then I will go with you . but how do I add it in front of ` df2 ` and ` df3 ` and merge it .
Now we can merge using year and the team columns to avoid the ambiguity : #CODE
You can then drop the columns you are no longer interested : #CODE
Hi EdChum , I tried to implement your solution . but I get ` ValueError : Length of values does not match length of index ` . when I assign ` df1 [ ' year '] = list ( map ( int , ( df [ ' match_up '] .str .split ( ' _ ') .str [ 0 ])))` .
If the lengths don't match then you could merge it but I'm guessing that df1 in my code is in fact df2 in yours , this may mean you are missing year information in df2 , what you could do is add the year information by merging it something like ` df1.merge ( df , on= ' team_df2 ' , how= ' left ')`
Ok I am working on it right now . will let you know after I apply it to my code !!
Python Pandas : How to replace a characters in a column of a dataframe ?
and I want to replace the ' , ' comma with ' - ' dash .
Use the vectorised ` str ` method ` replace ` : #CODE
How would I change this to ' rolling_corr() ' so that the rolling correlation is calculated every 10 days ?
I would like to join new state-date data , which is at a different frequency , and then aggregate / group over time .
join ` foo ` from the other database for every date-time combination , and create 2-year based values .
` join ` does ` how=inner ` on default , so this seems to be the issue .
Suggested Solution : Append as Column
One suggested solution was to append them as a column instead :
Using ` sort level ` to align the data frames : #CODE
A few questions -- what is the method in the join statement ?
I need to join before aggregation because they have different frequencies : ` dfNew ` starts in ` 2003 ` , ` test ` starts in ` 2004 ` , but I want to match these via the mean values of ` test [ ' foo ']` .
@USER my ` yourtimeseries ` does not contain ` state ` - but I see the approach and am also thinking of a way to make ` append ` work
If you do your append and then classify your buckets of time in another column , you'll be able to easily group and calculate on those buckets .
I tried to add them as a column in your spirit , but apparently when the index shapes are not the same , ` append ` is quite inefficient , and my Python instance crashes - see update .
I read another post on bucketing using the cut method .
Could you show the exact code you used to ` append ` - to create your ` final_df ` ?
My use of ` append ` crashed Python on the whole dataset .
How do you apply a function to one of several columns in a DataFrame ?
I would like to apply a function to data in one of the columns and return the same DataFrame but with new values in the column to which the function was applied to .
Sorry it's unclear why you need to apply on several columns when the following does what you want : ` df [ ' Numbers '] = df [ ' Numbers '] .apply ( lambda x : int ( ' 1 ' +str ( x )))` can you explain what you are trying to do ?
I only want to apply on one column .
In your example I would get a DataFrame or Series with one column only , which I would further have to merge with the the initial DataFrame .
Nope I assign back to the original column so there is no need to merge back , also I'm applying on just that column
I saw that it's possible to convert the column into the datetime format by DF = pd.to_datetime ( DF , ' %Y-%m-%d %H : %M : %S ') but when I try to then apply datetime.datetime.year ( DF ) it doesn't work .
No need to apply a function for each row there is a new datetime attribute you can call to access the year attribute : #CODE
You can access the months and other attributes using ` dt ` like the above .
I tried to apply DF.year and it is not working ...
No , you do ` df [ ' col_name '] .dt .year ` replace ` col_name ` with whatever your column name is , also what is your pandas version ?
I get AttributeError : ' Series ' object has no attribute ' dt ' when I use the original DF with additional column and the above error if I define a new DF1 = pd.to_datetime ( DF ) ..
Maybe I should use explicit name rather than the dt abbreviation ?
` rolling_mean ` allows ` center=True ` , which will will put ` NaN ` / drop remainders on the left and right .
If you take approach ( 1 ) above , you just need to drop some observations .
It's very easy to drop the later observations and retype the same command .
It's a little trickier to drop the earlier observations because then your first observation doesn't begin on Jan 1 of an even year and you lose the automatic labelling and such .
Here's an approach that will drop the first year and keep the last 4 , but you lose the nice labelling ( you can compare with annual data above to verify that this is correct ): #CODE
To do this , just replace ` sum() ` with ` mean() ` and multiply by 24 .
Finally , just use rolling sum :
It then uses the very handy ` itertools.compress ` to choose just the column names that ` pandas ` awesome selection function ( ` ~ valid.ix [ rownumber ]`) to pull out " items that aren't valid on this row " and join them .
This solution nicely leverages the tools to cut out complexity .
The problem I am running into is that I am unable to stack these varying data types .
@USER its close but I cannot even get them to stack and I don't want to plot the entire DF just the value and location and group them by hit type
You can add an index to the ` bedrooms ` column and stack the dataframe .
I would also of course transform the " condition " into a boolean array and fill in the NaNs .
My problem is that when I try to use the pivot method I get an error .
You can just change the index and unstack it ...
I tried set_index() and I get the following error message ( same one as before ): Series lengths must match to compare . this occurs when i set the conditions for comparison , similar to your bool = (( df1.Channel == df2.Channel ) & ( df1.Quad == df2.Quad )) line
pandas shift time series with missing values
However just applying pandas shift function directly to the column ' value ' would give ' previous_value ' = 10 for ' time ' = 2003 , and ' previous_value ' = 12 for ' time ' = 2007 .
( I'm not sure if it's as easy as setting the ' freq ' attribute ) .
Assuming I have a Pandas dataframe similar to the below , how would I get the rolling correlation ( for 2 days in this example ) between 2 specific columns and group by the ' ID ' column ?
I posted another question but I think I am getting close .... using the apply method with a lambda function seems to be headed in the right direction .
I also tried creating a new column with a ( PID , Trial ) tuple , but using merge on that gave similar results as above .
You want an inner join : #CODE
You can use a ` DataFrame's ` ` shift ` method .
I've figured it out thanks , had to drop the "" at the end
How to merge 2 complicated data frames in python pandas ?
You could add " NA " as an identifier for NaN values and drop them afterwards if you don't have too many NaN values
Not that I can think of but parsing row by row will be slow , it'd be better to just read it all in and drop afterwards unless you expect a lot of NaN values
This should read in all ` ND ` values as NaN's and then you can drop them in-place immediately after .
I am having a problem trying to implement the ' rolling ' functions in Pandas ( i.e. rolling_std() and rolling_corr() ) when using the group by functions .
I have tried using the below formulas but I keep getting ' ValueError : cannot reindex from a duplicate axis ' .
How do I insert 0 value for each ` type ` column for each gap's first and last day ?
I need to insert a 0 value on the 4th and the 9th of January .
Sometimes I need to map the UUID for an item to a Numpy row .
I was tempted to use Pandas and replace that dictionary for a Pandas index .
I realise I have to transpose the output once successfully parsed , but I'm having problems in reading the json data contained in the dataframe column .
Is there a way to directly load each list into a column to eliminate the transpose and insert the column labels when creating the DataFrame ?
The actual series is a limit order book snap shot , which has names ' bid.1.price ' , ' bid.2.price ' ..., ' ask.1.price ' , ' ask.2.price ' ..., and I would like to be able to alter the values of such entries based on a fast selection scheme .
Create a mask on the index using list comprehension .
Merge multiple columns into one while repeating two columns
Pandas : adding multiindex Series / Dataframes containing lists
How do I add / merge two multiindex Series / DataFrames which contain lists as elements ( a port-sequence or timestamp-sequence in my case ) .
` series1.combine_first ( series2 )` works , however it does not merge the lists - it simply takes one .
Further , the encoding must be specified not when opening the store , but on the ` append / put ` calls #CODE
They do what they do based on the result of bool ( the_object ) , and that's it .
I would like to simultaneously replace the values of multiple columns with corresponding values in other columns , based on the values in the first group of columns ( specifically , where the one of the first columns is blank ) .
I'd like to replace the '' values in b1 and b2 with the corresponding values in a1 and a2 , where b1 is blank : #CODE
In other words , construct a ` missing ` Boolean mask of cells where the data are missing , and a copy of the original data with all columns shifted two places to the right .
I'd like to apply the model to column ` c ` , but a naive attempt to do so doesn't work : #CODE
If you replace your ` fit ` definition with this line : #CODE
While this example is trivial in my case i have a ton of dropped mappings that might map to one index as an example .
I would like to select certain rows from a DataFrame and apply a result from lambda from it , and I am not able to assign it correctly , either all the other columns become NaN or the DataFrame not changed at all ( I believe this is related to DataFrame returning a copy , read that caveat )
Wait , hang on , now I'm confused ; if you're writing it out to csv , which will translate everything to strings , why do you need to translate to strings in the data ?
Thank you @USER , I asked it now to now what alternatives there were but didn't remember the map lamba solution .
For example , is there a way to get it to work with median instead of sum ?
That's a clever way of constructing the join table with the condition .
Is there an equivalent of ` rolling_apply ` in pandas that applies function to the cumulative values of a series rather than the rolling values ?
I realize ` cumsum ` , ` cumprod ` , ` cummax ` , and ` cummin ` exist , but I'd like to apply a custom function .
Ah , sorry , the bracket was in the wrong place ( the stack should be within the get_dummies ) .
@USER Yes , I wrote that first , but I found it with stack a bit cleaner ( shorter ) , but it gives exactly the same output .
Apart from that , you did the ` get_dummies ` within the apply ( so for each row instead of once on all ) , which made it slower as the approach above .
What I wanted is to interpolate and update the original dataframe , so I tried ` inplace ` since the documentation states the follow :
inplace : bool , default False
@USER I made a mistake , it's interpolate , not filter .
Initially , I'm only interested in the cases where the string DOES fit the form of " P-A1-1019-03-C15 " , so it would be nice to be able to drop rows which don't match that specific format .
[: -3 ]` which will strip the last 3 characters off ( I think , I may be off by one ) or do this : ` df [ ' col '] = df [ ' col '] .str [: 15 ]` if you want the first 16 characters
Then I could do the filter with ` df [ df [ ' col '] .str .contains ( regex )]` first , then , once all the strings are uniformly formatted , strip the last three ...
For the second column , ` SpeedMedian ` , I want it equal to 1 if the ` Duration ` in row i is less than the median duration for a given ` ID ` , else 0 .
I tried creating an empty ` list ` and using ` append ` : #CODE
I tried ` fdata [ fdata [ ' GEN_TICKER '] == i fdata [ ' date ' j ]]` and I get another type of error : ` TypeError : cannot compare a dtyped [ float64 ] array with a scalar of type [ bool ]` - but individually ` fdata [ fdata [ ' GEN_TICKER '] == i ]` and ` fdata [ fdata [ ' date ' j ]]` both work .
Your main problem was that running hist doesn't return an axis even though it uses one .
AttributeError : ' module ' object has no attribute ' hist '
As an aside you could just call ` plot ` on the df : ` df.plot ( kind= ' hist ')` see the docs : #URL
in order to use ` hist ` function
Shift index by desired number of periods with an optional time freq
Note that we didn't actually need to sort by ` ID ` -- the groupby would have worked equally well without it -- but this way it's easier to see that the shift has done the right thing .
Is there a clever way to use merge in this scenario ?
It would be easier to have gov be a real column and have a dataframe that had all the values in and then merge on gov to fill in the values you want
I have also tried using ` matplotlib.subplots() ` to create the subplots individually but this lead to the shared x-axis no longer only by at the intersection of all the ` Series ` being plotted .
Python : pivot a pandas DataFrame when the desired index Series has duplicates
It seems that the ` pivot ` method is appropriate but of course when I tried ` my_data.pivot ( index= ' user_id ' , columns= ' event_id ' , values= ' attended ')` I got the error that the index has duplicates .
It's easy enough to use ` any ` or clip the values instead if you want to guarantee the frame consists only of 0s and 1s .
thanks for the help but one follow-up question if you don't mind : when I look at the resulting pivot tables columns or index I get just the event_id or user_id numbers ...
so what are event_id and user_id in the new pivot table ?
the resulting thing from calling pivot table is a DataFrame so how would I access these identifiers for example ?
Then I execute a join statement .
The moment it hits the join , memory utilization hits 99% instantly and the mouse and all input locks up .
And it does the same thing when I'm just join ng a couple tables together .
` DataFrame.join ( other , on=None , how= ' left ' , lsuffix= '' , rsuffix= '' , sort=False )` can only join two tables at once .
Obviously , with correct parameters , pandas should be able to join large tables .
I set the index to dtype str , but failed to include engine= ' c ' which resulted in the join to max out ram and eventually fail with a cryptic memory error .
I seemed ` unstack ` could help me but I couldn't understand how exactly .
After dataframe append I have the same amount of the rows
However , this would be terrible slow , since each time ` append ` is called a new DataFrame must be created and all the data from ` data2 ` and ` x ` would need to be copied into the new DataFrame .
That's on the order of ` n**2 ` copies being made where ` n ` is the number of calls to ` append ` .
I'm trying to create a new column ' bought ' using apply function .
` apply ` is essentially just syntactic sugar for a ` for ` loop over the rows of a column .
` apply ` is used to join up the strings for each value : #CODE
where ` col_to_join ` is the column from ` buys ` which contains the values you want to join together into a string .
The use of ` apply ` to join the strings is unavoidable here , but only one pass through the grouped values is needed .
To match each string in ` boughtSessions ` to the approach value in ` clicks [ ' session ']` you can use ` map ` .
Unlike ` apply ` , ` map ` is fully vectorised and should be very fast : #CODE
New to Pandas , need to create a df from 2 others ( not a merge )
One cannot answer your question without merging the lists , and as you don't provide a foreign key linking them , the merge cannot be done .
1 ) strip those -9999 out before loading them with bumpy .
The first part of my question : is this the preferred way to merge the nltk output and my original data ( rating , object_id ) of each item ?
If I treat them separately : what would be the preferred way to merge them with the main dataset containing the ratings ?
Using apply ( or some other vectorisation ) to perform calculation involving two ( or more ) data frames ?
Naively I could split this into two apply statements , each effectively replacing each iterrows call .
Are you just trying to merge on ' time ' ?
Pandas concat from excel tables with missing data
I have tried using concat on subsets of the data ( and test data ) and it seems to work , but not for the whole data set .
How to pivot categorical variable in pandas ?
How to pivot this data ?
I'm using pivot_table / pivot , but it always asking for numerical .
How to pivot categorical variables ?
Obviously , I can go over all rows of the DF and just merge the values .
python apply function to list and return data frame
I am trying to apply this function to a list and I would like to merge all the results to one data frame .
I want to apply it to list ` [ 1 , 2 , 3 , 4 , 5 ]` , and get the result as a data frame which looks like : #CODE
( the ` to_datetime ` part , or the ` apply ` part )
If you use ` dropna ` before using ` to_datetime ` and ` apply ( ... strftime() )` , this will work .
My current solution is to use stack / unstack in the following way : #CODE
then you could use ` reorder_levels ` to avoid ( most of ) those stack / unstack calls : #CODE
Using append is something that will work for both cases I think : ` b.append ( pd.Series ( { ' new_col ' : 123} ))` .
` import datetime as dt ` ` df [ ' DateTime '] = df [ ' TimeStamp '] .apply ( lambda x : dt.datetime.fromtimestamp ( x ))`
Fine , once you change your index to datetime values just do a resample : df = df.resample ( rule= ' 1M ' , how= ' mean ')
I want to apply that function in Time column of data frame .
One option is to use 3 ` str ` ` replace ` calls : #CODE
I think your problem maybe that you're not assigning the result of your ` apply ` back : #CODE
The file has a multiindex : headers on two lines .
Produces a non multiindex dataframe
Any idea how I can multiindex properly ?
Try map ( tuple , l ) , before calling index = pd.MultiIndex.from_tuples ( l )
Instead of using ` reindex ` , set the columns to your new index directly : #CODE
` reindex ` doesn't just replace the index values ( though that sounds like what it should do , and the documentation isn't especially clear ) .
That's what's happening to you : when ` reindex ` hits ` [ ' Foo ' , ' B ']` , which doesn't exist in your original dataframe , it fills the column in the new dataframe with ` NaN ` .
I've tried using the pd.merge() thinking that if I use all columns as the key to join using outer join , I would end up with dataframe that would help me get what I want but it doesn't turn out that way .
Thanks to EdChum I was able to use a mask from a boolean diff as below but first had to make sure indexes were comparable .
append rows to a Pandas groupby object
I am trying to figure out the best way to insert the means back into a multi-indexed pandas dataframe .
This is not exactly what I want -- I want to insert the group means as rows back into the group .
use the ` transform ` method of the groupby object
@USER I do not want to ` transform ` the data , just compute some aggregate statistics and insert back into the dataframe .
The main thing you need to do here is append your means to the main dataset .
The main trick you need before doing that is just to conform the indexes ( with the ` reset_index() ` and ` set_index() ` so that after you append them they will be more or less lined up and ready to sort based on the same keys .
Broadcasting errors seem to happen quite often using Numpy ( matrix dimensions mismatch ) , however I do not understand why it does effect my multiindex dataframes / series .
and then scale the count array to apply kde() to it ?
Python Pandas ix timestamp
I d like to use ix for slicing the dataframe but I need to enter just hours and minute in the index , not the exact value .
In this case you need to actually insert the string ` i ` into the query expression : #CODE
On each loop iteration you want to insert the * current value * of the variable ` i ` into the query string , hence ` df.query ( ' type == " %s "' % i ) for i in ( ' Type1 ' , ' Type2 ' , ..., )`
I can accomplish this using ` shift ` : #CODE
It is arguably better to map the input data dictionaries to have all lower-case keys before they are pulled into the ` DataFrame ` .
Based on your comments , it looks like you want to join your list of lists.I ' m assuming they are in list structure because ` array() ` is not a method in python .
Lastly interpolate the NaN values linearly to get the final result #CODE
Note that ` np.array ( map ( int , s ))` is sufficient - it isn't required to build a ` list ` first ...
One pandas method would be to call apply on the df column to perform the conversion : #CODE
Difference between stack / unstack , shape and pivot in Pandas
Try this .i believe you can use the append #CODE
One method , so long as datetime is already a datetime column is to apply ` datetime.strftime ` to get the string for the weekday : #CODE
It will be quicker to define a map of the weekday to String equivalent and call map on the weekday : #CODE
What is ` dt ` ?
I get this error = ` AttributeError : ' Series ' object has no attribute ' dt '`
the ` dt ` is a datetime attribute accessor that was added in ` 0.15.0 ` , also your series has to be a datetime dtype , if needed you can convert : ` df [ ' datetime '] = pd.to_datetime ( df [ ' datetime '])`
I added another example of how it should work . try : ` import datetime as dt
I think the idea here is to use ` pd.merge ` to join all required columns from the original data back into the grouped data frame .
Wonder why a series has a ` str ` and why calling ` len ` on this ` str ` returns False for empty lists .
Conform the index to another frequency .
Then its straightforward to resample to another frequency .
Then you resample and somehow the days are there ?
First , drop the ` NaN ` values using :
Or , actually , as the ` Freq : MS ` above shows , just ` freq= ' MS '` will do : #CODE
Then I am trying to use basic formulas with rolling lookback windows to further build out the data in the panel .
Python : look back n days rolling standard deviation
I have a question about dealing with the rolling standard deviation :
My intuition is to split the data frame into daily data set and then calculate the rolling standard deviation , but I don't know how to deal with these indexand i guess my methods may takes a lot of time to calculate .
To resample this time series so as to get one value per day , you could use the ` resample ` method and aggregate the values by taking the mean : #CODE
Above , we computed the rolling standard deviation and then resampled to a time
If we were to resample the original data to daily frequency first and then
compute the rolling standard deviation then in general the result would be
The alternative to expanding the time series is to write code to compute the
Hi unutbu , I found the data doesn't include the whole 2-min data set . who should i set the dynamic window size or i should change another way to calcite this rolling standard deviation ( I have updated my questions ) do you have any suggestion for the new problem ?
I want to do a diff along the ` date ` column , but re-start the calculation for each ` country ` .
The diff should give this : #CODE
Sounds like a standard application of groupby and diff .
In your example , we want to group by the country column , then do a diff along the users column ( you say along the date column , but that doesn't seem to match your expected output ): #CODE
One idea is to replace ' didn't attend ' with -1 and ' not invited ' with 0 but I'm worried this will alter the significance of events .
As EdChum mentioned earlier , one idea is to replace with the mean on the axis .
Other options include replacing with the median for example .
In this case , this will replace the NaN's with the mean across each axis ( for example let's impute by event so axis=1 ) .
I would plot a few histograms for the data , by event , to sanity check whether this preprocessing significantly changes your distribution - as we may be introducing too much of a bias by swapping so many values to the mean / mode / median along each axis .
Then replace your NaN numbers across each event column using random numbers generated from Bernoulli distribution which we fit with the ' p ' you estimated , roughly something like :
` p = 0.25 # estimated probability of each trial ( i.e. replace with what you get for attended / total )`
` # s now contains random a bunch of 1's and 0's you can replace your NaN values on each column with `
The only tricky part is passing the dictionary to ` agg ` so we can perform two aggregation operations at once : #CODE
It's usually easier to work with these flat formats as much as you can and then pivot only at the end .
It does raise some more questions at the end with how to pivot / filter / group the df_new dataset ( I'd like to group all items that have ' G ' anywhere in them ** as well as ** something else , separately group just the ' G's , etc . ) but hopefully I can do this using another group / agg as you suggest .
python pandas translate DF but got another Error
I want to use pandas to translate my CSV file by DF .
So this time I combine with four files and delete som columns , after this , I use my code to translate .
Assuming your ' Date ' index is a datetime , you can just create a date range and append that .
Your way looks like a simpler way to merge -- essentially doing an outer join implicitly -- but it didn't quite work when I tried it .
how to convert pair lists ( to matrix ) to heat map ; python
How to convert pair lists ( to matrix ) to heat map ?
I want to generate heat map for this data .
Use fillna to replace the mode only for the NaNs : #CODE
Note : reading the docs , this will raise if no item occurs at least once , so you may wish to use a more robust function in the transform : #CODE
If you wanted just the row cound then perform ` len ( count )` or ` count.shape [ 0 ]`
You're looking for the ` drop() ` function , and there are 2 ways to drop rows , by name and by index .
Or you can drop rows using numerical indices of the index like this : #CODE
No need to drop just select the rows you're interested in : #CODE
I am writing a method to normalize the values ( to make them between 0 and 1 ) .
pandas chunksize - concat inside and outside the loops
But I didn't understand the behaviour of the concat method and the option to read all the file and reduce memory .
Then I have to apply a function to the dataframe to create a new column based on some values : #CODE
And I'm also wondering how may I concat the chunks using the for loop .
Don't you need to concat the df with itself ?
So , should be better to concatenate the dataframe inside the loop instead of build the whole dataframe outside and then apply the function ?
Thus far my code is able to insert a stored procedure and function into a database and execute both without issue .
However when I try to run the code a second time , in the same database , the drop commands don't seem to work .
First , proc_drop.sql , which is to used to store the SQL drop commands .
I realize there are two kinds of drop statements in the file .
Have tested a number of different iterations and none of the drop statements seem to work when executed by Python / SQL Alchemy but all work on their own when executed in SQL Management Studio
My drop SQL originally fed into the " deploy_procedures " function as a file and to be execute in the body of the function .
I've since isolated the drop SQL reading / executing into another function for testing purposes only .
And as far as I can tell , this is because the drop SQL at the top of my explanation is never run .
To overcome this I've tried batching the merges as well ( select a subset of the data , merge these to a DataFrame , which is then merged into my main DataFrame ) , but this approach is still too slow .
Slow meaning it only processed ~4000 columns in a day , with each append causing subsequent appends to take longer as well .
One part which struck me as odd is why my column count of the main DataFrame affects the append speed .
This results in appends at a constant rate of ~ 0.2 seconds which is acceptable ( versus Pandas taking ~150seconds for my full dataset per append ) .
I'd first replace the string values with their boolean equivalents calling ` replace ` , you can then use label indexing to select that row , generate a boolean series where the value equals ` True ` and use this to select the columns : #CODE
If you're worried about calling replace on the whole df then you can call ` replace ` just on that row : #CODE
Didn't think of replace .
In my case it seems safe to replace on the whole dataframe , is there a way to only replace the strings of one specific row , i.e. the row called ``' active '`` ?
@USER you can call ` replace ` just on that row by using ` loc ` to perform row label selection
I'm trying to aggregate summing the payments , but without summing the ages ( averaging would work ) .
You can pass a dictionary to ` agg ` with column names as keys and the functions you want as values .
You could either replace them with an empty string or drop them
Yes , but take care if you use ` setattr ` , since if you call something like ` data.groupby ( ... ) .agg ( ... )` , the returned DataFrame may drop the metadata .
actually , loc and iloc both take a boolean array , in this case the ` mask ` . from now on you can reuse this mask and should be more efficient .
well that is working with aggregate too , but I am trying to get y [ ' seed ']
I'm trying to create this widget in Flask , to build a pivot table interface :
What i failed to mention above is that prior to calling the to_hdf method , i was doing a pandas.concat to merge two files .
How to create predefined color range in Matplotlib heat map from a Pandas Dataframe
What I want to do is to create a heat map with the following condition :
And my data looks like this ( both before melt and after melt ) #CODE
I strip the date and then later did the following : ` df [ ' day '] = pd.to_datetime ( df [ ' day '])` but didn't get the actual dates to show up
How to drop values which are below the group means ( originally 20% of group means ) ?
There must be a smarter , Pandas way like using something like apply / join / whatever .
" drop values which are below the group means " Which group ?
Here's one way of doing it , using ` groupby ` and ` apply ` : #CODE
In this case , we tell it to get all members of ` g ` ( the subset of your series , grouped by levels one and two of your MultiIndex ) where the value is less than the group mean ( ` g.mean() `) .
IIUC , you can use ` transform ` for this : #CODE
` transform ` takes the groupby reduction result , here ` mean ` , and expands it up to match the original index , which means we can use it to create a boolean mask : #CODE
Also prefer the top level pandas ` isnull ` and ` notnull ` rather than numpy versions ` sum ( dftest.employment_total.isnull() )`
I want to be able to append a suffix to a variable name in pandas , and to have that suffix be able to change .
In the line of code below , I want the mask variable name to be appended with var " lastyear " .
I think this is a little better , since the former computes the median of all the columns in ` df ` and then drops all the columns except ` continuousvar ` .
The latter computes the median only for ` continuousvar ` , so it should be a little faster .
Make a df of all possible combinations then left merge your data to that combination df .
I've already done a merge and have columns 1-4 and need to append column 5 , the new manager ID .
please include a sample of the two data frames you are looking to merge .
We then call ` map ` on the df OldManagerId values and this will perform a lookup for the corresponding NewId and set this value : #CODE
s , f = list ( next ( zipped )) , map ( float , next ( zipped ))
Try removing the map float and see if you see any values that would casue an error
I have tested it with 3000 lines of similar data and get no error , try removing the map call and see if you see any strange output
When I removed the map I got this : ` ValueError : too many values to unpack
Trouble passing in lambda to apply for pandas DataFrame
I'm trying to apply a function to all rows of a pandas DataFrame ( actually just one column in that DataFrame )
Can you show ` df.info()` and which columns you are trying to perform the calculations on , you will not be able to pass 2 columns row-wise if you are calling apply on a series
How to merge multiple CSV files using Python Pandas
I'm wondering how to merge multiple CSV files using Pandas , but using two specific criteria :
As in , I don't want data to be merged as it would via a SQL Join .
I want the CSV file values to be merged as new columns , and not as it does in the append function , where it puts the values underneath the first grouping
I've been using pandas to transform raw file data and import into a database .
For a work project I am trying to read several .csv files , convert them to data frames , concatenate some of the fields into one for a column header , and then append all of the dataframes into one big DataFrame .
I am unable to apply the last operation across all of the DataFrames .
It seems I can only get it to apply to the last DataFrame in my list .
Once I get past this point I will have to append all of the DataFrames to form one large DataFrame .
Unfortunately I am not able to convert the provided ` bool ` values to ` 0 ` and ` 1 ` and sum them .
You could perform a list comprehension , looping over the columns and then use a boolean condition on that column , drop the values that don't meet the condition and call count : #CODE
I need to create an index on a specific frequency , however I need to apply that frequency only for certain months .
Pandas : how to merge 2 dataframes on key1.str.endswith ( key2 )
i want to find the best way to merge 2 dataframes on key1.str.endswith ( key2 ) , an example is sometimes better than words : #CODE
The last bit is to call map on this on the other df after setting the index on the color column , this will perform a lookup on the color value in df and return the corresponding code .
The str method is over 2X faster than using the lambda , this may not be so surprising as the ` str ` methods are vectorised as is calling ` map ` .
So @USER ' s answer is marginally faster than @USER beaveau's but using map here is faster still .
In fact if we combine @USER ' s regex ` str ` method with map we get faster than my original method : #CODE
So using ` map ` here is nearly 2X faster than merging
The idea was just to use merge rather than having a very long line ;)
Then ` pd.merge ( df1 , df2 )` will merge on the common column ( s ) which in this case is the ` color ` column : #CODE
Ideally I would have used ` pandas.period_range() ` , but since it doesn't accept multiple in the ` freq ` parameter , I turned to ` pandas.date_range() ` .
But in the case where the ` freq ` is longer than the time between start and end , it returns a range with 0 elements .
The only time I would see the result being empty is if the ` freq ` has a multiplier of 0 ( ie " 0D " , " 0H " , " 0W ") .
I can append them one by one using Pandas data frame but it is very time consuming .
and what is the merge supposed to accomplish ?
Using trunk pandas , it took me 17 min to write a 10G csv file from a frame of that shape ; it took 8 minutes to write the transpose ( more rows , fewer columns ); and only 30s to write to HD5 .
To which I want to join a df that contains VALUE with is only date specific .
Isn't there a way to apply ` stack ` only to some columns ?
Just don't unstack in the first place : #CODE
Disclaimer : I had posted a related question previously , where the trick suggested ( don't unstack at all for the join ) was useful for that part , but in the end I actually want to unstack for various reasons ( including plotting ) .
My attempts to get this were partially described in the other question , but one way in particular I was trying was ` pivot ` :
Attempt : Pivot var2 , then merge again #CODE
Here's my attempt to merge afterwards : #CODE
Or alternatively just do a merge after your pivot to bring back the missing column ?
I can't really think of a way to make pivot do what you want .
I suspect you might have better luck using stack / unstack than pivot though .
@USER I managed the workaround , but it took me some time : I needed to ( i ) recast the column as a data frame , ( ii ) use join , not merge / append .
Having seen your original question , I think you'd be better off doing the join after the unstacking .
Take ` df ` from your original question , unstack it along the ` status ` level , then select everything from the top-level column `" var "` , and then do the join after that's done .
I have a large pickled Sparse DataFrame that I generated , but since it was too big to hold in memory , I had to incrementally append as it was generated , as follows : #CODE
don't concat in a loop !
The idiom is to append to a list , then do a single concat all at the end .
Thanks , but you can't append to that file , right ?
you cannot append to a single mode , but you could simply write nodes as needed and just append them when u read it out
Essentially I want to find the intersection of a line and a data series in pandas .
to interpolate , there are many other choices ( e.g. quadratic or cubic
` pivot ` doesn't get me there , and since it is not a sublayer , ` unstack() ` probably doesn't do the trick either .
Now you can use unstack .
Now just join and sort : #CODE
Python Pandas replace NaN in one column with value from corresponding row of second column
I need to replace all NaNs in the ` Temp_Rating ` column with the value from the ` Farheit ` column .
The problem is if I then try to join them , I am not able to do this while preserving the correct order .
How can I only find ` Temp_Rating ` rows with the ` NaN ` s and replace them with the value in the same row of the ` Farheit ` column ?
First replace any ` NaN ` values with the corresponding value of ` df.Farheit ` .
how to apply ceiling to pandas DateTime
Resample Length Mismatch Error
I get an error when I try to resample the following time series .
But when I try doing the same thing from my Compute Engine server , the request always fails for some of the stocks I'm interested in , although it typically works for options on larger companies , such as ' aapl ' or ' ge ' .
Trying it on an Ubuntu box at home , I get the same behavior as I am on Google Compute Engine , whereas it works on my Mac and works for ' aapl ' or ' ge ' on Ubuntu .
First , you sort it using a MultiIndex , then you unstack it so that each entry in the Name column becomes its own column .
What am I missing on the join statement ?
Can you confirm that ` concat ` works on the dataframes that are producing an error when joined ( ` df_all ` and ` df_addresses `) ?
Yes , concat works , join doesn't .
I can concat one table at a time , but can't join them .
. I've found that on= doesn't make a difference in his case , but that th the join does occasionally pick up the wrong field , so I try to be explicit .
This works : # Load Authors and Addresses - only 1 index ' ISI_LOC ' to join
I was looking to have the function take the locationargument and apply it to all dataframes in one call .
Is there some way to get the ` datetime ` related groups from the first grouping , and apply them to the second grouping instead ?
EDIT : Also , the ` concat ` call is taking much longer in my real code than my toy example ; in particular the ` get_result ` function takes a lot longer despite the production df having fewer rows and I can't figure out why .
Basically for pandas / numpy speed you want to avoid ` for ` and any ` concat / merge / join / append ` , if possible .
Two short follow-up questions : 1 ) if I'm starting with a dataframe ` df ` already supplied to me , is it reasonable to construct a numpy array for the columns I want to add , and then concat that to ` df.values ` at the numpy level ?
You can also use the ` dt ` accessor : ` mydf.DateTime.dt.date `
Looking at the docs , the only thing coming close appears to be the parameter ` colormap ` - I would need to ensure that the ` x ` th color in the color map is always black , where ` x ` is the order of ` foobar ` in the data frame .
At least I don't get this to work , especially it seems like using frames with MultiIndex columns complicates things .
First task , use a boolean mask to find all values that meet your condition and assign the new value , this is a generic scenario I don't quite understand what you are trying to do exactly : #CODE
this won't work because you are calling apply on the df , naturally this will iterate over the columns and you are trying to check the probability column it's unclear to me what you are trying to do here , are you checking just the probability column or all columns ?
General dtypes map to specific dtypes , but may be different from one installation of NumPy to the next .
On the left all " Industry Category " items and on the right the desired " Parent Category " I like to apply to the dataset .
@USER would you expect ` replace ` to be faster than ` map ` ?
If the second csv was just a lookup and you set the index to be the ' Industry Category ` I would expect ` map ` to be faster
@USER I have no real grasp on the relative time differences between ` map ` and ` replace ` unfortunately .
I have to create a csv and apply the the2nd step of this [ link ] ( #URL ) .
Once I have the dict , I have to map the df with .map ( category_list.get )
@USER I think you can either create a dict or just create a df but in the latter the index needs to be the ' Industry Category ' , you can use ` map ` only if the keys are unique which is true for dict but for a df this needs to be true for your csv data , I expect ` map ` to be the fastest method based on personal experience
I would create a dictionary and use ` apply ` and ` lambda ` .
When i apply the code to generate a single new column , all works great .
But when there are few columns to be created , the change does not apply .
it has to do with not changing all data values , but i don't understand why the change does not apply - after all , before the second iteration begins , the DF seems to be updated and then ` tbl [ tmp_col_name ] = ' No Week '` for the second iteration " deletes " the changes made in the first iteration , but only partially - it leaves the new column created but filled with ' No Week ' values ...
You are performing [ chained indexing ] ( #URL ) which may or may not work , in your case it doesn't work you need to use the new ` loc ` , ` iloc ` or ` ix ` accessors to set the data .
Using loc , iloc or ix accessors to set the data works .
My guess for why this is so slow is at least partly because ' append ' creates a new object every time it's called , and it gets called a LOT .
If I understand what you want the following should work : ` df.sort ([ ' low_price ']) .set_index ([ ' low_price ']) .reindex ( index = np.arange ( df1 [ ' low_price '] .min() , df1 [ ' low_price '] .max() ) , method= ' pad ')` so this sorts the df by low_price , then sets this to the index , this allows us to reindex using a new index of minn and max value and then filling the missing values
However I get an error : cannot reindex a non-unique index with a method or limit .
After setting to be the index we can then call reindex and compute a new index and fill the missing values using ` method= ' pad '` : #CODE
Use the ` axis ` parameter in ` diff ` : #CODE
Assign the results to a new dataframe ( temp2 ) , and then insert a new record that sums any remaining items in the list .
So long as the indices match then this will align correctly .
I've been using the Categorical function to replace categorical values with numerical ones .
On second thought , instead of chunksize , just read in the first row and count the number of columns , then read and append everything with the correct number of columns .
Note that the test based on ``` len ( test.columns )``` is just one way you might test things .
Use ` count ` to produce a boolean index and use this as a mask for the columns : #CODE
For example , ` Series ` objects have an ` interpolate ` method which isn't available in PySpark ` Column ` objects .
Please put error messages in code blocks instead of posting an image of the stack trace .
I'm struggling to find a way to iterate over Df , and for each row , apply a definition that iterates to search for the nearest match in Df1 ( with the aim to add data from Df1 to Df ) .
If you search SO on " nearest match merge join " you will find a few workarounds though .
If you know SQL , it might be easiest to merge that way , since the ``` on ``` in ``` inner join ``` will allow for more flexibility than the ``` on ``` in pandas ``` merge ``` .
You also select the kind of plot you want , in this case a hist , using ` kind= ' bar '` .
So I'm trying to offset or shift a part of my dataframe by 2 hours , but I can't seem to get it to work with my following code : #CODE
I don't think so it's easy enough to add comments and append that this seems unnecessary , also you'd have a support issue with some people maybe wanting different comment markers , multi-line support etc ..
I also have a map { date -> code} .
I'd like to get a Series indexed by date only , such that for each date the code is looked up in the provided map and then the pair ( code , date ) is looked up in the original Series .
then you can form a second MultiIndex out of the ` codemap ` dict : #CODE
You can use ` apply ` to call a lambda function that splits the string and then joins on the unique values : #CODE
After creating a DataFrame from a pivot table and applying .unstack() , I came up with a table like this #CODE
Also , it would be good to know how to rename the index " Date " as I have to merge two tables where in one case the time is named " Date " and in the other " Close Date " .
enables a pure label-based slicing paradigm that makes [ ] , ix , loc for
Basically we can drop the ` NaN ` rows first and then call ` apply ` and use ` datetime.strftime ` to apply a new format : #CODE
One way would be to use a regular expression ( ` re ` module ) to get search for a known pattern and replace it with the desired output .
I ultimately want to store in a separate variable the rolling difference from one row to the next using ` shift() ` , but I found subtracting dates to produce odd results .
Key error on pandas merge ( left join )
I am trying to join df_logins to df_purchase on age , gender and region with the following pandas code : #CODE
There is therefore no intersection between the tables .
I am trying to subtract a column from another set of columns in a Pandas MultiIndex DataFrame , but when I try to do it I get only ' nan ' values .
I'm working in pandas doing pivot tables and when doing the groupby ( to count distinct observations )
` aggfunc={ " person " : {lambda x : len ( x.unique() ) }} ` gives me the following error :
Rather than removing duplicates during the pivot table process , use the ` df.drop_duplicates() ` function to selectively drop duplicates .
( ii ) ` bool `
The main types stored in pandas objects are float , int , bool ,
did you want to append only the value of fourth column ?
pandas syntax in the middle choosing the right rows ; ` join ` all the words-colunn values together , ` split ` them apart into separate words .
, and I am trying to replace it with standard Missing values - NaN
-- I got it working by using replace with regex as below .
You can replace this just for that column using ` replace ` : #CODE
I'm trying to map the following function over a pandas dataframe ( basically a list ) in python 2.7 : #CODE
Is there some way to map an if statement like this in python ?
You are looking for the reindex function .
How do we aggregate the time series by hour or minutely granularity ?
Check out [ Better way to aggregate time stamped data ] ( #URL )
How do I use pandas to add a calculated column in a pivot table ?
I did the following to add a calculated field ( column ) in the pivot table
This is because you have a Hierarchical Index ( i.e. MultiIndex ) as columns in your ' pivot table ' dataframe .
How can I transform the given data set as mean centred and scaled to unit variance using pandas or numpy or any appropriate python module , data also contain some missing values as " Nan " that should also be removed prior to modelling task pleas help .
With some googling and playing around the data probably i found that this method may normalizing data on Row basis and I want to normalize data with column basis .
Can you post what the final df values should be and give examples of the calculations you are trying to apply , thanks
You could drop the ` NaN ` values and compare the output no ?
Next , in order to normalize your data on a column basis , your method is actually correct : #CODE
By default , sklearn will normalize your variables by feature ( column ) ; to normalize by sample ( row ) you can add ' axis = 1 ' to the arguments of the scale function .
i have a small doubt on normalization say i have one class variable " activity " and it has two values " active " and " inactive " what is the best way to normalize 1 ) should i normalize both " active " and " inactive " data separately then combined both for machine learning or 2 ) i can do normalization on combined data .
It really depends on both your data and the specific machine learning / statistical algorithm you would like to apply .
With your example , I would imagine " active " and " inactive " data are ( hopefully ) not going to be significantly correlated and so I would just normalize each separately with sklearn's scale function
The goal is to identify the low volume prices over a rolling range of rows .
Im identifying many low volume rows over a rolling ' window ' .
Lets say I set the rolling window to 50 .
Also , on another front , you might try to use an ordered map , since you say the prices are all unique .
Then you could just look at one end of the map ( depending on how it's sorted ) to pull off the minimum key / value pairs .
I'm assuming , of course , that the implementation of that map is well done , but if it's in your standard library , it probably is .
Step 1 : implement a rolling min with 101 periods ( 50 up and 50 down from current point ) .
Now it takes a few seconds depending on the size of the rolling window !
a sequence of values to map onto the current colormap .
How could I transpose table and change index in this way ?
Python pandas : split and append ?
One method is to apply a function to your df to split the ' cc ' column and create a new dict that contains each split country and their associated count , you can then construct a new df from this , groupby the country and perform the sum on the count : #CODE
I tried to use pivot like this : #CODE
The ` item ` values you would need to make pivot work look like this : #CODE
Once you have the right values in the ` item ` column , the ` pivot ` call follows naturally : #CODE
assumes that the ` site_num ` s cycle through ` 0 , 1 , 2 ` and that ` len ( x [ ' site_num '])` is a multiple of 3 .
The ` groupby / cumcount / pivot ` method will work even if ` len ( x [ ' site_num '])` is not a multiple of 3 by adding NaNs as necessary .
I have a pandas dataframe that I'm trying to drop rows based on a criteria across select columns .
This doesn't drop the rows , so I tried : #CODE
What you want to do is generate the boolean mask , then drop the ` NaN ` rows and pass ` thresh=1 ` so that there must be at least a single non- ` NaN ` value in that row , we can then use ` loc ` and use the index of this to get the desired df : #CODE
I didn't know about the Boolean mask thing- I'll have to read about that further .
In the answer below , you're still assigning with a copy -- ` loc ` in on the wrong side in other words .
This is the out come of the above code : Now how can i verify wither my data has been normalize by columns and has mean = 0 and variance = 1 or not .
My confusion seems to be about which function to utilize ( apply , mapapply , etc . ) , if I need to reference an index value in the original df to apply a completely unrelated value to the initial df , and the most optimized , pythonic way to accomplish this .
Just append .date() at the end : pd.Timestamp ( dt.datetime.now() ) .to_period ( ' M ') .to_timestamp ( ' M ') .date()
You can also use @USER ' s method for finding the date of the last day , and assign it directly to the column instead of ` apply ` ing it .
The second arg is the list of values to replace with .
Load a part of each sorted subset , join them into one DataFrame and sort it You'll have to be careful here on how much t oload from each source .
How to merge rows in DataFrame according to unique elements and get averages ?
Get subset of pandas DataFrame using dynamic expression including the shift method
And transform them into something that can be used to slice / filter / query the dataframe .
I tried using the query method like this ` values = df.query ( condition )` , but I think the ` shift ` is causing it to fail because I get this error : ` NotImplementedError : ' Call ' nodes are not implemented `
Break this up into column + shift ( here the columns are all one-letter so I did the simplest thing possible , which isn't very robust , but switching to something more clever is trivial ): #CODE
But I can not save the file back to the original file and insert behind " location "
And I want to insert the ' lat ' and ' lng ' columns after ' location ' , like this : #CODE
Performance : Python pandas DataFrame.to_csv append becomes gradually slower
Then I transform them ( exactly the same tranformations to each ) and append them to a csv file using the ` DataFrame.to_csv() ` method .
Does ` pandas.DataFrames.to_csv() ` append mode load the whole file each time it writes to it ?
2 ) drop the columns you do not need after the normalization .
It must have something to do with the ` pandas.DataFrame.to_csv() ` append method . but I tried reading the code but this python is to advanced for my experience with it .
Did you try to save into different files rather than append ?
Use apply rather than iterating ( for zfill ) .
Each time the file is opened before it can append to , it needs to seek to the end before writing , it could be that this is the expensive ( I don't see why this should be that bad , but keeping it open removes the need to do this ) .
I'm doing some more tests for confirmation so I can apply the bounty with a fair judgement .
You could define a function and call ` apply ` passing the function name , this will create a df with min and max as the index names : #CODE
If you insist on a list of lists we can transpose the df and convert the values to a list : #CODE
Note also that you can use ` describe ` and ` loc ` to get what you want : #CODE
Python pandas pivot not uniqe index
I thought the best option would be to do a pivot table something like #CODE
I believe you want pivot_table instead of pivot
Should I write a script to insert commas where all the column breaks are and then read it in as CSV ?
Also I did minMax function to get minmax values of each column and insert them in a list #CODE
Python pandas : merge loses categorical columns
Now you can merge as usual since there is no trouble merging integer valued columns .
To be honest the categorical conversions would be done at the end anyhow , so this is not really going to save you anything ( by doing it inside merge ) .
Basically this uses the index values from criteria and the boolean values to mask them , this will return an array of column names , we can use this to select the columns of interest from the orig df .
I've got a loop to merge the files and it works well for the three firsts .
I tried the solution for a similar issue in Pandas Merge ( pd.merge ) How to set the index and join to no avail .
Well when you merge on keys if any of the other row values don ; t match then this creates a clash and this appends new columns with suffixes ` _x ` and ` _y ` , you need to decide what to do in this case
I would like to merge the ipr files using the first and second columns ( renamed " IPR " and " description " by the script ) .
Select range of rows on secondary index in pandas MultiIndex
Unless I'm missing something , all you need to do is build a mask and shift it : #CODE
I'd like to be able to aggregate the output from multiple files , i.e. , to be able to group by these two columns in all the files at once and print one common output with total number of occurrences of ' yes ' or ' no ' or whatever that attribute could be .
That way , it doesn't merge any possible duplicates in col5 .
Sorry but if you have another question please post another question , don't keep expanding your requirements
How can I replace the 27 with just 01 without altering anything else ?
Use the datetime attribute to filter on date and then ` apply ` a function to replace just the day component : #CODE
I've found apply and map great for speeding up calculations on particular rows in a DataFrame .
The question I've got is : Is it possible to return a value with apply or map functions which refer to a previous row ?
Apply and map are great at vectorising row by row calculations - is it possible to refer to the previous row so I can make that calculation ?
look at [ ` shift `] ( #URL )
String wildcard in pandas on replace function
I basically want to replace any email address in a data frame with the new domain .
For a specific column , replace the substring ' @* ' where * is any set of characters with ' @USER .com ' .
You can use the vectorised ` str ` method to split on `' @ '` character and then join the left side with the new domain name : #CODE
another method is to call the vectorised ` replace ` which accepts a regex as a pattern on the strings : #CODE
Pandas ' ` replace ` will let you use regular expressions , you just have to set it to true .
I'd like to be able to aggregate the output from multiple files , i.e. , to be able to group by these two columns in all the files at once and print one common output with total number of occurrences of ' yes ' or ' no ' or whatever that attribute could be .
I.E. , why cant feature ` c_0348 ` map to ` [ 0 , 1 , 2 ]` vs ` c_0348f =[ 1 , 0 ]` , ` c_0348k =[ 1 , 0 ]` , ` c_034 8p =[ 1 , 0 ]` ?
How to get the intersection of two dataframes ?
How do I get this intersection ?
You can just perform a ` merge ` , this will use all columns and the default type of merge is ` inner ` so values must be present in both dfs : #CODE
Does [ this equivalent of transform in r ddply in python pandas question ] ( #URL ) help ?
My second suggestion to specify this in the groupby call with ` as_index=False ` , seems not to work as desired in this case with ` apply ` ( but it does work when using ` aggregate `)
To apply an arbitrary function , ` func ` , to each row : #CODE
Can I replace with better code even though it is working as is ?
2 ) Try ` frame3 [ " name_len "] = frame3 [ " name "] .map ( lambda x : len ( re.findall ( ' [ a-zA-Z ]' , x )))`
I then normalize the name as such by replacing the space with " # " and add trailing " ?
So you need to replace that duff value and then you can just call map to perform the lookup on the ` NaN ` values : #CODE
So you see here that apply as it is iterating row-wise scales poorly compared to the other two methods which are vectorised but ` map ` is still the fastest .
You'll probably find that calling map like this will be the fastest method
Right now I just have it iterate over the company names , and a RE pulls the symbols , puts it into a list , and then I apply it to the new column , but I'm wondering if there is a cleaner / easier way .
I'm new to the whole map reduce lambda stuff .
pandas - transform data view
I would like to transform this such that data is transformed like this : #CODE
I could .. after the concat , reset_index , then drop the ' id ' column then set the index to the pandas generated index ( which should be values 1 .. n ) . then i'm thinking i do a dataframe.apply to find parent by ' date ' and ' level=0 ' and set parent accordingly .
but it will be faster to use the inbuilt ` to_datetime ` rather than call ` apply ` which essentially just loops over your series .
Using apply will be substantially slower than your first method by the way
Not at the moment , cumsum is a vectorised method apply will not beat this .
As @USER notes , cumsum is going to beat apply .
Are you looking for something like ` df.groupby ( ' col1 ') .apply ( lambda x : pd.Series.count ( x ) / len ( df ))` ?
But isn't there a way to divide by sum from groupby operation as opposed to len of DF ?
that is great but the only issue I believe is that if there were nulls I would have to write something a little verbose like this : ` df.dropna ( subset =[ ' A ']) .groupby ( ' A ') .size() / len ( df.dropna ( subset =[ ' A ']))`
Python Pandas replace values by their opposite sign
And I would like to replace all values that are negative to their corresponding positive values .
As of now I have just begun writing the replace statement #CODE
Just call ` abs ` : #CODE
Another method would be to create a boolean mask , drop the ` NaN ` rows , call ` loc ` on the index and assign the negative values : #CODE
You could load the csv in chunks and then compare ` value_counts() == len ( df )`
If can upgrade to pandas 0.16 , I would also advice to explicitly use ` loc ` in this case ( so it certainly does not fall back to the positional integer location ) .
But note the bug is also in ` loc ` below pandas 0.16
Why are you just taking the first element of either dataset and comparing each ( presumably ) character for the length of it , eg : in ` for i in range ( 0 , len ( train [ 0 ]))` - surely ` len ( train [ 0 ])` is going to the number of columns ?
Use ` shift ` to compute the inter-row criterion that you want to distinguish .
Once I have the data in the combined_list variable , I want to be able to pass it to a for loop and iterate over each line and insert it into my SQLite db if the date does not exist or update the existing record if the date is found .
I have my separate lists and I have the database insert working .
What I have above does pivot the Status column only .
I can use pivot because I think you want Trial and Time and the index .
You might also replace ``` sep ``` with ``` delim_whitespace=True ``` as Bob did in his answer .
Now , suppose I have multiple data frames outside the function , say ` df1 ` , ` df2 ` , ` df3 ` on which I want to apply ` myfunc ` .
I then insert a ` lookup_key ` column in the front with some random integers .
I am trying to drop rows where Tenant is missing , however isnull option does not recognize the missing values .
How can I drop records where Tenant is missing ?
Now I replace any empty strings in the ` Tenants ` column with ` np.nan ` objects , like so : #CODE
Now I can drop the null values : #CODE
There's no error if I replace both functions above with ` .count() ` .
Python Pandas merge two data frames based on multiple values field
I think a nicer , more general way , to do this is using stack / unstack : #CODE
Now you can either merge , or if you're lucky just switch out the index : #CODE
Note : If you have duplicates ( A , B , Cs ) you're going to need to merge ( which is a little fiddlier , but could be cleaned up ) .
Now let's say I want to reindex this , like such : #CODE
You can use ` DataFrame.interpolate ` with the `" time "` method after a ` resample ` .
Or drop last ` .any() ` to see all columns which have text and which don't : #CODE
I have put a similar question in the cross validated stack exchange with the hope that someone will find a solution from the perspective of machine learning way of using such structure in a suggested algorithm .
In order to solve this big problem , I find read_fwf in pandas module and apply it but failed .
We won't take the time to reindex the rows .
My purpose is , by taking out the data frame ` ( .ix )` from original total data frame , to construct other new data frame via concat or append .
In this case , ` df = df.ix [ 2:6 ]` does replace existing data .
then we convert Col C to a dict , transpose it and then join it to the original df #CODE
Then I want apply fill_between() on area between A and B series : #CODE
Note that ` stack ` will give you a multi-index which may not be desirable in this case .
` stack ` in combination with ` reset_index ` will do the trick .
Regarding your first question , try this instead frame3.loc [: , " space_len "] = frame3 [ " name "] .map ( lambda x : len ( re.findall ( ' [ # ]' , x )))
Eventually I'd like to chain simple transform functions like ` select_blue_cars ` : #CODE
Edit : Having thought about this a bit more I think that I'm looking for a single transform which selects a copy from the dataframe without explicitly calling ` .copy() ` .
You can also use ` ix / loc / iloc ` ( though you need to create an empty column for ' price ' first ):
You can also get around this by using ` ix / loc / iloc ` .
@USER is probably using version 15 or 16 , and it's quite likely ``` loc ``` has improved in performance since version 12 .
Added to my answer below for readability , but loc in 15.2 is 25x faster than chained .
How to append selected columns to pandas dataframe from df with different columns
I want to be able to append df1 df2 , df3 into one df_All , but since each of the dataframe has different column .
something like append only this two columns ?
You can also use set comprehension to join all common columns from an arbitrary list of DataFrames : #CODE
I'm not sure it would be quicker to merge my sheets together ( but some sheets contain additional headers ) , perform this task individually and then merge them after or even structure the data how I want it when scraping the data .
I would put them in a list when reading from excel then just concat that list into a single dataframe ( in one line ) .
And once you have those yo can get the column means etc . and go on to normalize by the mean : #CODE
I want to normalize the data to the first row so I can plot a line graph for each Dx .
It seems confusing to me to mix the categorical average with the aggregate average .
you are appending each row which causes a copy each time of n^2 operations ; append to s list and concat at the end
Then I would like to plot the results as a distribution with ` df.plot ( kind= ' hist ')` .
Sorry no it doesn't , is your output df actually what you're after because that is whay I get after the ` concat ` without any sorting
OK , a way to sort by a custom order is to create a dict that defines how ' name ' column should be order , call ` map ` to add a new column that defines this new order , then call ` sort ` and pass in the new column and the others , plus the param ` ascending ` where you selectively decide whether each column is sorted ascending or not , and then finally drop that column : #CODE
Python find out records in dataframe by column values greater than or equal to their median in each subgroup
I want to subset ` df ` by the ` value2 ` column , the value of which is greater or equal to the median of each sub-group specified by the index of ` group2 ` .
For each group , it finds the median , then replaces and value below the median with ` NaN ` .
Simple join of two Data Frames in Pandas
I would like to join them into a resulting data frame where the row indices are equal , such as an inner join in SQL .
I can't seem to use merge because the integer index has no name #CODE
or ` join ` : #CODE
Or ` merge ` and set ` left_index=True ` and ` right_index=True ` : #CODE
It is a result of calling ` groupby() ` and ` agg .
([ np.mean , np.std , np.median , len ])` .
KeyError : ' ( cons_all_rank , median )'
How do I stack the following 2 dataframes : #CODE
Using the pandas merge operations does not work since it just lines the dataframes horizontally ( and not vertically , which is what I want )
I was hoping to drop the columns where all values in the column == 0
Then I transpose , slice and transpose back
Just use a boolean condition to make a mask and then call ` any ` and pass param ` axis=0 ` to drop the columns that are all ` NaN ` , the ` any ` call will create a boolean series which you use to select the columns where all the values are not ` NaN ` : #CODE
if ` dt ` is the ` datetime ` module then use ` utcnow() ` instead ` now() ` here .
To rearrange ( though you may not need to ) use ` sort_index ` ( or we could reindex if we saved the initial DataFrame's index ): * #CODE
This is error "' Series ' object has no attribute ' dt '" .
@USER use ` pd.to_datetime ` rather than apply .
If you use the apply it may not create a Datetime column .
I think you may have to update to 0.15.X for the dt accessor .
If I understand what you're after you can add the column by performing a groupby and then calling ` transform ` and passing the function ` cumsum ` : #CODE
Transform returns a series aligned to the original df so you can add it as a column , see the docs : #URL
You have a couple of options , the easiest IMO is to simply unstack the first level and then ffill .
I think this make it much clearer about what's going on than a groupby / resample solution ( I suspect it will also be faster , depending on the data ): #CODE
If you're missing some dates you have to reindex ( assuming the start and end are present , otherwise you can do this manually e.g. with ` pd.date_range `) : #CODE
and stack it back ( Note : you can dropna=False if you want to include the starting NaN ): #CODE
Whether this is more efficient than a groupby / resample apply solution will depend on the data .
For very sparse data ( with lots of starting up NaN , assuming you want to drop these ) I suspect it won't be as fast .
Sounds Like you want a merge .
` fillna ` has a ` value ` argument which can be used to map missing values by common index , but this expects the argument type to be ` Series ` or ` dict ` , not ` DataFrame ` .
Python Pandas ' apply ' returns series ; can't convert to dataframe
I use apply to run the function and it returns a Pandas series object .
The goal is to geocode 166 unique countries , then join it back to the 188K addresses in df_addr .
But I haven't found the magic to convert series into dataframes and this is the first time I've tried to use apply .
Your latter method is also incorrect because you drop the duplicate countries and then expect this to assign every country geolocation back but the shape is different .
It is probably quicker for large df's to create the geolocation non-duplicated df's and then merge this back to your larger df like so : #CODE
this will create a df with non duplicated countries with geo location addresses and then we perform a left merge back to the master df .
Then I applied a mask to filter , for instance , the Users with more than one trace : #CODE
I'm looking for a function instead of the ` lambda g : len ( g ) 1 ` able to filter Users under different conditions , as I said before .
Can I apply some dt.month == june conditions ?
align three time series in python
I would like to align the signals only after time = 800 , based on having the minimum points aligned .
I have tried the following to align two of the signals in pandas : #CODE
To find and align the minima : #CODE
Use a boolean mask , then use ` df.loc [ mask ]`
Using a boolean mask :
Make a boolean mask .
Unlike the boolean mask solution , the ` start_date ` and ` end_date ` must be dates in the DatetimeIndex .
If you actually have strings that look like tuples , you can parse them first and then apply the same pattern as above : #CODE
If it is actually strings , you can first convert it to lists like so , then apply the above operation : #CODE
Aside : not only is it easier to insert text ( as opposed to an image ) into a question , but if you copy and paste the text then other people can select those parts and use ` pd.read_clipboard() ` to reproduce your frames .
I ended up doing it another way ( filling down the 3pm quotes during an ordered merge on a matching ' date ' column with the original quotes frame , then dividing the columns across ) and got the same answer .
Just perform a ` merge ` on ' a ' column and perform a left type merge : #CODE
is because the default type of merge is ' inner ' so values have to exist in both lhs and rhs , see the docs : #URL
Using Unstack in Python
I am trying to unstack a column in python but it isn't quite doing what I am expecting .
I want to unstack by year so that all the days from a year per stationn are in one row .
You need to make ` year ` an index before you call unstack : #CODE
Missing values replace by med / mean in conti var , by mode in categorical var in pandas dataframe -after grouping the data by a column )
I have a pandas dataframe , where all missing values are np.nan , now I am trying to replace these missing values .
The last column of my data is " class " , I need to group the data based on the class , then get mean / median / mode ( based on data whether data is categorical continuos , normal not ) of that group of a column and replace missing values of the group of the coulmn by respective mean / median / mode .
get median / mode / mean of groups of the cols
replace the missing of those groups
but currently I landed up , finding replacement values ( mean / median / mode ) group wise and storing in dict , then seperating the nan tuples and non-nan tuples .. replacing missing values in nan tuples .. and trying to join them back to dataframe ( which i donno yet how to do ) #CODE
See note regarding ` mode ` at the bottom as it is slightly trickier than ` mean ` and ` median ` .
Note that ` mode() ` may not be unique , unlike ` mean ` and ` median ` and pandas returns it as a ` Series ` for that reason .
Strange results with groupby , transform , and NaNs
A workaround for this could be to use a transform ( rather than an aggregating ) groupby method : #CODE
I'd be surprised if there was not a cleaner solution using ` aggregate ` or something .
This seems to be concatenating Player names because of ` aggregate ( { ' Salary ' : sum , ' Player ' : sum} )` .
To create multiple columns when using ` apply ` , I think it's best to return a ` Series ` rather than a list .
drop : boolean , default False .
Do not try to insert index into dataframe columns .
resample
in resample
I'm doing a rather simple insert into a local MongoDB sourced from of a Python pandas DataFrame .
All is well so far until I attempt the insert , where I'm getting a ' Cannot encode object ' .
Looking at the dict directly showed that everything looked good but then ( while writing this question ) it dawned me to check each type in the dict and found that a long ID number had converted to a numpy.int64 instead of a simple int ( which when I created the dict manually as an int would insert fine ) .
This directly from the to_dict output that faulted on insert .
The only difference ( as far as I can tell ) is the Long int , which interestingly enough , when I did the Mongo insert it shows that field as being ' Number Long ' within the document .
Rolling argmax in pandas
I have a pandas TimeSeries and would like to apply the argmax function to a rolling window .
However , due to casting to float from rolling_apply , if I apply ` numpy.argmax() ` , I only obtain the index of the slice of the ndarray .
Is there a way to apply a rolling argmax to a Series / DataFrame ?
Here is a work-around , essentially doing the apply ' manually ' , should be pretty efficient actually .
While not idea for my use case the issue can be fixed by using MultiIndex #CODE
You can also use the ` apply ` method for a one-liner , which is simpler and clearer but also slower even than your approach : #CODE
How do insert the below data which I was previously plotting sequentially on top of each other in to the created figures above ?
Sure thing , once you learn how to use ix , loc , and iloc in pandas you can do just about anything
Problems with creating large MultiIndex ( 10 million rows ) in Python Pandas used to reindex large DataFrame
My situation is I have a DataFrame with a MultiIndex including a ` TimeStamp ` and number ( wavelength from 280-4000 nm ) where the wavelength number spacing changes from every 1 nm to 5 nm .
I need 1 nm spacing and plan to and plan to linearly interpolate after reindexing my DataFrame .
I tried to create a ` MultiIndex ` using ` pd.MultiIndex.from_product() ` and providing two lists of about 4000 items in length each which resulted in Python using up all my computer's RAM .
and the ` MultiIndex ` is used to reindex my df #CODE
I think when you specify ` dtype= ' string '` , you are essentially specifying the default ` S64 ` type , which will truncate you string to 64 chars .
But what I actually would like to do is plot the minimum and maximum of my process at every date , and color the area between the two extremas with a color map that would reflect the concentration of the paths in the plot ( so for instance red around the mean and gradually cooler as we go to the extremes ) .
The proper way to do this however would be to mask the array in the contour plot but I don't quite have the time to figure that out right now ( have a look at numpy mask array options and take a stab at it ) .
You'd want to mask the array to only show up between your max and min values .
convert to int to drop decimal place and convert int to string with 3 digits .
This is a bug to me , your index is of dtype datetimeindex , when perform loc and trying to divide it produces NaN , if you did this then it would work ` df.iloc [ 3 ] / df.iloc [ 5 ]` and produce the correct result .
The reason why ` df.loc [ df.index.year == 2007 ] / df.loc [ df.index.year == 2009 ]` or ` df.loc [ df.index.year == 2007 ] .divide ( df.loc [ df.index.year == 2009 ])` are not working is that ` pandas ` tries to align the data by their index .
I've +1 ed everyone's contribution to this , I think you are correct and it makes sense , it's one of those subtle things that doesn't tend to bite you when using unique int indexes and selecting rows using ` loc `
Want to apply formula and label result to multi-index dataframe .
I get into trouble when I concat two dataframes .
Gut tells me I need to group-by and then apply formula , I can handle that ( I think ) , but how do I bring label ( ' pattern ') along ?
Aside : this is more personal preference than anything else , but I like to use ` np.sign ` ( 1 for positive , 0 for 0 , and -1 for negative ) and then ` replace ` rather than three lines for your ` pattern ` translation .
Spliting dataframe in 10 equal parts and merge 9 parts after picking one at a time in loop
I need to split dataframe into 10 parts then use one part as the testset and remaining 9 ( merged to use as training set ) , I have come up to the following code where I am able to split the dataset , and m trying to merge the remaining sets after picking one of those 10 .
test_index = np.random.choice ( df.index , int ( len ( df.index ) / 10 ) , replace=False )
The AAPL values are diff as well .
I would try to replace your ` return ...
` statement with the following ` rows = results.all() \n local_session.close() \n cnt = len ( rows ) \n return rows , cnt ` .
You're doing a join between two classes ( ` T ` , ` User `) - do eager load instead of default lazy load .
If you have 800,000 rows and doing a lazy join , that may be a problem .
Find and replace multiple values in python
I want to find and replace multiple values in an 1D array / list with new ones .
I would like to replace #CODE
What is the fastest way to do this ( for very large lists , i.e. with 50000 values to find and replace ) ?
To replace values in a list using two other lists as #URL pairs there are several approaches .
In pandas I would create a dict from the 2 lists and then call ` map ` which will perform a lookup and replace the values : #CODE
This approach isn't vectorized , ` map ` uses iteration .
For long lists , it is a bit faster doing the ` map ` , but the time necessary to construct the ` Series ` means the iteration-based approach ends up being faster overall .
So , I think you have to generate the ' total ' dataframe separately and ` concat ` it with the original dataframe .
Is there a way to unstack the table so that only the statistics for one of my columns is being displayed horizontally from ` .describe() `
I could unstack the data frame for the second level and then sum by row again , but that seems too complicated .
I'd construct a dict of the values you want to replace and then call ` map ` : #CODE
As in : ` map ( lambda v : d [ v ] , df )`
What would you do if you have , say half a million of such distinct values over few millions of data-points ( len [ df ] = ' very big ') ?
Is there a way to create a dictionary fast without actual applying d [ ' fff '] = index for all possible values of keys before applying map ?
You could construct just a series of the distinct values and use that : so df [ ' data '] .unique() returns a series of all the unique values the index will be an auto generated int64 index , so you could create a dict from that ` d = dict ( zip ( df [ ' data '] .unique() .values , df [ data '] .unique() .index ))` you could substitute the last param for ` np.arange ( len ( df [ ' data '] .unique() ))`
However , since you want to visualize density , it should not be a big issue to simply just drop the missing values , assuming the missing / unobserved cells should follow the same distribution as the observed / non-missing cells .
Plot rolling mean together with data
I would like to plot the data of this DataFrame , together with the rolling mean of the data .
I would like the data itself should be a dotted line and the rolling mean to be a full line .
One with the data and one with the rolling mean .
I would like both the data and the rolling mean of the worst case column to be red .
Also , the data and the rolling mean of the avg case column to be blue .
AttributeError : ' Series ' object has no attribute ' dt '
The concat is working well but when I write the data back to csv the data loses some coherency : #CODE
If you set the index to be the dates for both df's then multiplication will align where the indices match : #CODE
If you have duplicate date values then what you could do is left merge the other df's value column and then multiply the 2 columns so the following should work : #CODE
I get " ValueError : cannot reindex from a duplicate axis " because some indices are duplicates .
Another method would be to left merge the value column from df1 to df on the dates and then multiply the 2 columns
In Pandas , merge two dataframs with complex multi-indexing
I would like to merge two dataframes on columns Name and Depth .
Then merge to get this : #CODE
You can join ( on the index ): #CODE
And I think you can attach the rest of df1 using a ` join ` I can't remember .
I'm not sure if you're asking about concat also , but it's pretty straightforward : #CODE
I would like to merge them in one file .
I would like to merge the diferent contents in one id .
So what does this error mean and how can I multiprocess this map function ?
I usually have a ` mp=True ` argument in my function that runs the function in multiprocessing mode by default but can be set to ` False ` to run it with a regular , non-multiprocessing ` map ` ( using an ` if ` test ) so I can debug these sorts of errors .
So , once the debugging is done , replace : #CODE
pandas - apply datetime functions
The problem I have , is that , in the upper subplot , the boxplot on the right hand side should be shifted of 1 to right to align with the ` ( Yes , False )` label .
Just set the index and unstack .
use ` merge ` : #CODE
i have large number of values , how can insert all of them in the place of somevalue and how can i save the each iteration result
I'm new to pandas but it seems you want the ` cut ` function .
I am trying to drop duplicate entries from a Pandas DataFrame in Python .
can you first find the indices of the rows that need to be removed and then use ` drop ` to remove them ?
Python pandas resample montly works weekly doesnot
not even 1 of 7 is getting values because resample ( ' w ') is using sunday as last day . data is stock data which has Friday as last day .
If you want to set all the values in that week to the mean / max use a transform : #CODE
if that is not coercing to float in the transform ( on master ) pls post an issue
How to create a pivot table on extremely large dataframes in Pandas
I need to create a pivot table of 2000 columns by around 30-50 million rows from a dataset of around 60 million rows .
How can I do a pivot on data this large with a limited ammount of RAM ?
Why not use a RDMS to aggregate your dataset ?
And as your pivot indicates , with what I assume are two byte-size fields ( ids ) and one integer ( qty ) field a temp db table should not be too extensive to store and query .
Originally comes from gbq , which sadly doesn't have a pivot function
Why not run an equivalent [ query ] ( #URL ) to your pivot : ` SELECT [ catid ] , Sum ( qty ) FROM [ datasource ] GROUP BY [ catid ]` ?
@USER For a groupby sum , that map / reduces , in the sense you can groupby and sum on chunks then add up the results .
Optimizing pandas filter inside apply function
You can use a merge .
Why didn't you normalize your labels ( i.e. chinese|Chinese -> chinese ) after you've read the dataset before extracting the features ?
To answer your other questions , I didn't normalize the labels because I am doing initial testing , and I think my current way to code it before making the list is fine .
* ps . one way to create pasteable code for a multiindex DataFrame is to use ` reset_index() ` , show that then ` set_index() ` .
Then we merge the ' filtered ' version with our original .
I now need to apply this function to several million rows and it's impossibly slow so I'm trying to figure out the best way to speed it up .
Is it possible to pass two rows of a dataframe as arguments to the function and then use Cython to speed it up or would it be necessary to create new columns with "` diff `" values in them so that the function only reads from and writes to one row of the dataframe at a time , in order to benefit from using Cython ?
You can use it a a mask to update the columns you need from you original dataframe .
Adding groupby is just going to have the effect of putting a NaN in the first row whenever you do a ` diff ` or ` shift ` .
If your second column is already summed by term you can use ` agg ( numpy.sum )` instead : #CODE
Also I merge the second dataframe with the empty dataframe and I expect that merged dataframe has also a multilevel column , but the multilevel column is converted to a single level column with a tuple of my levels .
Not sure why do you need to merge an empty dataframe with anything , sounds like an overkill ...
But if you must ... just replace last line with this : #CODE
Thanks for your response , this will work for a single merge , but I want to do the merge every time I call the method .
Now I want to replace the age attribute in dataset with the categorical age attribute .
But i am unable to replace the OLD age attribute values in the dataset .
but it is not letting me replace or assign different values to the column in the dataset .
Can anyone suggest me a way that I am able to either append a new AGE column with categorical values or replace the old column with these new values .
If it weren't a MultiIndex , I could select those with year ` 2007 ` through ` df.loc [ ' 2007 ']` .
How is it to map to a date in ` 2007 ` ?
then you could replace all the values in the series with those from 2007 with #CODE
The more robust ( but slower ) way is to use a join : #CODE
I have an excel file ( .xls format ) with 5 sheets , I want to replace the contents of sheet 5 with contents of my pandas data frame .
So , I decided to do this task in VBA and drop python completely .
pandas drop_duplicates behavior in a multiindex
Thanks Ed , ' dt ' works perfectly .
Geopy error : GeocoderServiceError : HTTP Error 500 : Internal Server Error using pandas apply function with str concat
Working function ( see code Python Pandas apply returns series ; cant convert to dataframe ) has stopped working .
Short of ditching apply and iterating through the table , or passing 5 parameters and then iterating through the table .
What you're doing is a little perverse to be honest , you're calling ` apply ` on a series and then trying to construct a str from lots of columns , this is the wrong way to go about this , you can call apply on the df and pass ` axis=1 ` so that the row is passed and either access each column in a lambda func and pass them to ` locate ` or in ` locate ` extract each column value , or just create a series from the concatenation of all the columns and call apply on this : #CODE
But you can iterate efficiently with the apply method .
Is is possible to drop ` df2 ` rows if those rows are also present in ` df1 ` ?
One possible solution to your problem would be to use merge .
Checking if any row ( all columns ) from another dataframe ( df2 ) are present in df1 is equivalent to determining the intersection of the the two dataframes .
Then we can look at the common columns using ` common_cols = list ( set ( df1.columns ) set ( df2.columns ))` between the two dataframes then merge : #CODE
EDIT : New question ( comments ) , having identified the rows from df2 that were also present in the first dataframe ( df1 ) , is it possible to take the result of the pd.merge() and to then drop the rows from df2 that are also present in df1
EDIT 2 : How to drop the rows from df2 that are also present in df1 as shown in @USER answer .
This occurs because the value of 1 in column A is found in both the intersection DataFrame ( i.e. ( 1 , 0 , 2 , 3 )) and df2 and thus removes both ( 1 , 0 , 2 , 3 ) and ( 1 , 1 , 1 , 1 ) .
If all the dataframe's columns are to be checked , could you replace your ` on =[ ' A ' , ' B ' , ' C ' , ' D ']` with ` on =d f1.columns ` ?
Andrew , one last question ( I also added it to the original post ) - having identified the rows from ` df2 ` that were also present in the first dataframe ( ` df1 `) , is it possible to take the result of the ` pd.merge() ` and to then drop the rows from ` df2 ` that are also present in ` df1 ` ?
@USER : I believe I found a way to drop the rows of one dataframe that are already present in another ( i.e. to answer my EDIT ) without using loops - let me know if you disagree and / or if my OP + EDIT did not clearly state this :
With this in mind , based heavily on Andrew's approach , here is how to drop the rows from ` df2 ` that are also present in ` df1 ` : #CODE
Then I am able to process it using apply , for example : #CODE
pandas multiindex dataframe , ND interpolation for missing values
Is it possible in pandas to interpolate for missing values in multiindex dataframe .
It looks like you want to interpolate based on the MultiIndex .
I don't believe there is any way to do that with pandas interpolate , but you can do it based on a simple index ( method= ' linear ' ignores the index btw and is also the default so no need to specify it either ): #CODE
Anyway , this is the kind of 1d interpolation you can do pretty easily with pandas interpolate .
You may want to remove pandas interpolate from the question too as it doesn't look like that is going to be of any use here .
Columns get lost on pandas frame join
I would now like to combine both data frames and aggregate the data per year per team .
How do I properly do a group-by and join these two frames ?
The solution to this problem is to apply ` reset_index() ` to " end " the group-by operation .
What is this " freq " argument ?
It looks for me that ' freq ' in index is not exists .
I solve this with map and lambda function ....
y = map ( lambda x : x.year , kk.index )
I.e. , how can I have my decorated DataFrames replace the stock DataFrames ?
Just apply a ` filter ` : #CODE
ValueError : cannot reindex from a duplicate axis
But I don't know how to reindex the week column in the MultiIndex with a date_range .
At a high level , I perform something tantamount to a cross join by ...
This I've also tried but honestly I was not able to treat properly your multiindex .
Anyway , if you want to apply a value to a multiindex , it would be something like ` df.loc [ ' 0hr '] .loc [ ' 0.01um '] [ ' t '] = xxx ` , assuming that ' t ' is a column .
How to replace NaN with sum of the row in Pandas DatatFrame
I am trying to replace the NaN in certain columns with the sum of the row in a Pandas DataFrame .
@USER they are not known but I meant more any other way than normal replace .
No not really depending on how your data is composed it may be quicker to just do 10 calls to replace which will be fast rather than creating some function that is called for each value
@USER I do not agree because if I replace 1 with one for instance what about 51 ?
apply sort to a pandas groupby operation
How do I apply sort to a pandas groupby operation ?
The command below returns an error saying that ' bool ' object is not callable #CODE
Normally the sort is performed on the groupby keys and as you've found out you can't call ` sort ` on a groupby object , what you could do is call ` apply ` and pass the ` DataFrame.sort ` function and pass the column as the kwarg param : #CODE
Well you introduce another level of indexing by adding this sort criteria , you'd have to drop the superfluous indices after calling ` reset_index ` I'm afraid
Regarding your first issue : Somehow you have managed to insert a [ ZERO WIDTH NO-BREAK SPACE ] ( #URL ) character after the `" http : "` .
You also need to use ` apply ` rather than ` agg ` here : #CODE
Just perform an outer ` merge ` : #CODE
The reason you get that result when using ` concat ` is that you are concatenating column-wise and it's aligning on the common index values
Is it possible to do this within a for-loop so i effectively can merge 10 csv-files together in the same way ?
You'd have to merge each of the files to each other , concat will not produce the csv you're looking for
This works if I am fine with a resolution of seconds , but I don't understand what is the best way to aggregate the data to obtain a resolution of minutes or hours .
Also to add this code works perfectly if I replace GradientBoostingClassifier with RandomForestClassifier .
sum of values larger than median of each row in pandas dataframes
Is there an efficient way to find the sum of values whose absolute value is larger than the median of the row in a pandas data frame ?
How to generate the sum of numbers in each row which are larger than the median of the corresponding row ?
This uses ` .gt ` which is greater than and uses as the value the ` median ` ( row-wise by passing axis=1 ) .
Just out of curiosity , is it possible to use ' apply ' to realize the same function ?
Yes you could but I'd advise against it because apply is slow whilst this will be vectorised , apply should be a last resort always
Since you want to sum values in each row which is greater then median , and if you want to preserve Day values , below approach works fine #CODE
thank you John , I was very curious about how to use apply to realize it .
I want to replace the ' T ' in the preciptotal column with the value .01 .
This ` lag ` defines how many indices I need to shift in either direction of the first tuple to get the dataframe .
Operations between Series ( + , - , / , * , ** ) align values based on their
... yeah , that's unexpected ; the ` raw ` df is right , but ` map ( newSeries , raw )` fails .
Okay , map over a df is apparently creaky , but DataFrames have a bunch of useful iter-types and using one of * those * works .
( As did ` map ( fn , roo.index )` . )
Python How to find average of columns using dataframes apply method
Using the dataframe's apply method , create a new Series called ` avg_medal_count ` that indicates the average number of gold , silver , and bronze medals earned amongst countries who earned at least one medal of any kind at the 2014 Sochi Olympics .
Anyway , your groupby / apply is probably OK , you just have something wrong with the function itself .
And , Now , let's see what was the error that was happening , when you apply np.argsort() after first group on series object .
I copied and ran ` df = pd.read_clipboard() ` in iPython ; for me ` df.info` shows MultiIndex instead of DateTimeIndex ...
Reindex the original frame to be all dates in the inclusive range .
Note that you have to drop the index ( with the .values ) here to broadcast it properly .
How to insert a row in a Pandas multiindex dataframe ?
I have a Pandas dataframe with a multiindex ( Reg , Type , Part , IsExpired ) - #CODE
I'd like to insert a row for ( EMEA , Disk , A , True ) - #CODE
You could ` unstack ` and then ` fillna ` : #CODE
Otherwise stack it back : #CODE
* Note : when doing stack / unstack there is often an alternative ` pivot ` / ` pivot_table ` method ...
So for the row in question , I end up with - [ EMEA Disk A False 22 ] where I can't use unstack / stack method .
Is there a way to somehow insert a row for True here ?
then reset the index to drop the second level : #CODE
what you need is pivot table . this blog explains it very clearly : #URL
Then you can reorder the levels and transpose the dataframe .
Python pandas multiindex getting info from series
I created a multiindex pandas series from a timeseries and now I want to read the data in it .
In this multiindex , the first level is the date , and the second level is the hour of the day .
Note : the groupby from above is actually equivalent to the simpler ` resample ` : #CODE
You solved my problem , but I'd still like to know how to do it with the multiindex
I only have csv-file , and i tried to apply your solution but could not make it work .
I know how to append an existing series / dataframe column .
You could perform an ` inner ` merge :
for a many to one relationship then you can perform a ` left ` merge : #CODE
Once you have the described table relationship you should be able to get the necessary data by doing an ` INNER JOIN ` : #CODE
pandas dataframe drop rows by multiindex
I'd like to drop rows from a pandas dataframe using the MultiIndex value .
I use ` unstack ` and put the results in a dataframe : #CODE
The method ` drop ` is working perfectly when the index is normal ( see drop ) , but , how do I build the ` label ` when I got a ` MultiIndex ` ?
and I want to transform it in a way that it results in
Use a pivot table : #CODE
I need to shift all the individual columns in the following dataframe to extreme end .
Is there any switch in ` shift ` function to do that ?
I am trying to accomplish this by first converting the dataframe into a list of lists , then insert nans , and in the end , convert back into the dataframe .
You could iterate over each col , get the index position of the last valid value and then shift by the length of the df minus this index position , as indexes are 0 based you need to offset by 1 : #CODE
You then use the boolean mask to mask your columns : #CODE
I have played around with DateOffset and looked into resample but haven't been able to take it further than that .
To calculate the rolling sum for t-3 through t-1 , use a window length of 3 and shift the data by 1 ( the default if no parameter is specified ) .
You can also apply type conversion to other columns : #CODE
I though to first use Sed in a subprocess to remove the " [ " and replace the "]" with a comma , and the use pd.read_csv to get the separate values in a dataframe and then transpose to get it in a column .
replace the ` [ ]` with spaces
replace commas with newline characters
Simple replace with ` awk ` should do : #CODE
How can I apply a search , such that the result would be the index for each value , if it exists , in an efficient way ( since I know the column `' A '` has uniqu values ) to get the following results : #CODE
I've tried to use Deciamal module for this , but when I tried to resample it like this #CODE
So I cast it to float64 right before resampling , resample and cast to Decimal again , right ?
That may possibly happen but usually the imprecision occurs at the lower digits but if you cast to Decimal at the end it should clip this
Draw different sized circles on a map
I would like to plot these locations on a map using circles .
I managed to draw blue dots on the map , but I don't know how to draw the circles with the corresponding size and color .
pandas boxplot for clustered boxes : how to set multilevel x axis labels
Other than the " hold state " part , the rest is basically a bitwise shift .
I have tried a some join / merge ideas but can't seem to get it to work .
Just ` concat ` them and pass param ` axis=1 ` : #CODE
Or ` merge ` on ' Symbol ' column : #CODE
` df2.groupby ( df2.index.hour ) .mean() .reset_index() ` should squeeze the df to an hourly one , also you could resample
To add a aggregated column from a groupby use ` transform ` this will return a Series aligned with the original df : #CODE
If you want to resample by hour you could just groupby the hour and then call ` reset_index ` : #CODE
I'm running the following function on my json response from some APIs and trying to normalize it into a .csv file using the pandas json_normalize() function .
Pandas : Merge datasets of different frequency
I have both monthly and bi-annual data , which I want to join .
A straight join would match these only for the firsts of Januarys - what is the proper way to join these ?
I would like to drop duplicate rows but also keep track of which rows of the original ` DataFrame ` were identical before I dropped duplicates .
A better approach than approximation would be to profile the function to get a sense of exactly why it takes too long , followed by using ctypes / Cython / numba to translate the function as-is into a C function that runs without as much overhead .
You shouldn't even need to do an ` apply ` for that , can directly just use the columns in the dataframe .
Simply using ` numpy ` will solve the problem most of the time , but for those cases when you don't really need all of ` numpy ` and you don't want to add the coupling to require use of ` numpy ` data types throughout some code , it's very handy to know how to drop down to the built-in ` ctypes ` library and do it yourself .
This is precisely the problem that ` numpy ` was built to solve , and so homebrewing your own ` ctypes ` code whenever it both ( a ) makes sense to incorporate ` numpy ` data types in your application and ( b ) there exists an easy way to map your code into a ` numpy ` equivalent , is not very efficient .
Python 3.4 - How to transform a Pandas Data Panel ( not frame ) to a mySQL database ?
This still did not solve the problem for transforming the full datapanel but did manage to allow me to transform a dataframe slice to a .db file .
So really you just need to get all the values and patterns from ``` valuewhen() ``` , and then ( in this example ) map ``` 3 -> 0 ``` , ``` 4 -> 3 ``` , ``` 7 -> 4 ``` , etc . and just do that ``` n ``` times .
Assuming spreadsheet1 is loaded to pandas df and spreadsheet 2 is loaded to df1 then you can assign the values from the result of a ` merge ` : #CODE
Just to explain a little when we merge we perform a left merge and this will produce a clash of columns as we have column names that clash : #CODE
We are interested in just columns ` A_y ` and ` B_y ` , we perform a left merge because if we did the default merge type which is ' inner ' then this will not align with the original df , i.e. the values will be shifted to the top of the df .
For example I want to replace the column ` LCV ` with the columns ` LCV-a ` and ` LCV-b ` .
You can do this by ` apply ( pd.Series )` on that column : #CODE
Can you include the entire stack trace ?
That's quite a long lambda function in your apply , I recommend writing as a function .
This looks like a typo ( it's weekday not weekdays ) , either the latter or using loc should work : #CODE
Are you using merge for a SQL-style inner join ?
Or could you possible concat instead ?
I'm just trying to understand the type of join you are doing ; if it's inner join , then sticking with merge is good , but if you can do concat , the code would be a lot simpler .
I'd reset the index so that it becomes a column , this allows you to call ` apply ` on it , then for each datetime apply a lambda which calls ` replace ` and null the minute and seconds attributes , then drop the duplicates and set the index back : #CODE
Use resample : #CODE
The reason for using ` .dropna() ` is that an hourly resample will create rows for each hour between the first and last which will be filled with NaNs if there is no data within the hour .
First , an empty list , then I append each of these lists individually .
What if you append None instead .
In the small example you give , the transpose of the DataFrame has columns of a single type so maybe you should be working with that .
Note , however , that simply taking the transpose won't change the data types .
You could create a mask for each of the common conditions such as ` product == 0 ` , ` region.isin ([ ' UK ' , ' US '])` etc ..
Per EdChum's recommendation , I created a mask for the conditions .
It will be faster than ` apply ` since it uses vectorized operations .
If you are willing to change the output a bit , you can do something even more useful with a ` MultiIndex ` : #CODE
How to replace values in pandas DataFrame respecting index alignment
I want to replace some missing values in a dataframe with some other values , keeping the index alignment .
I would like to replace the missing values of ' b ' rows matching them with the corresponding indices of ' c ' rows .
When the dataframe has more columns and we want to replace nans only on a selection of the columns , this method does not work because subsetting the dataframe creates a copy , not a view .
@USER , right off my head I will say filter the selection of the columns , then apply the same on the values copy
You can use fillna to remove these and replace them with 0 .
In the class I have a ` create_data ` method that queries a databank and creates a ` Pandas DataFrame ` that I later join to another ` DataFrame ` .
I would like to replace `' a '` with ` a2 ` where ` a2 ` has 5 unique values .
simply multiply the number by 100 to scale it in the range ( 1 , 100 ) , and then apply the same algo , however you can play with numbers and find out your own way of doing the same .
Identifying the run lengths of a MultiIndex
If so , insert this between steps 1 and 2 : #CODE
The difficulty then is how the indices align .
If you want to sort the index so that the data is reordered along with the data then use ` reindex ` : #CODE
Note that you have to assign the result of ` reindex ` to either a new df or to itself , it does not accept the ` inplace ` param .
You can use ` reindex ` and call ` natsorted ` on the index ` df.reindex ( index=natsorted ( df.index ))`
@USER sorry ` reindex ` is one of the few functions that does not accept param ` inplace ` , so yes you have to assign it to itself
I believe your reindex trick is a good work around to no key value available for ` df.sort() `
I leave it to the user to decide if this is better than the ` reindex ` method or not , since it requires you to sort the column data independently before sorting within the ` DataFrame ` ( although I imagine that second sort is rather efficient ) .
So this will groupby the fullname and zip as you've stated , we then call ` transform ` on the Amount column and calculate the total amount by passing in the string ' sum ' , this will return a series with the index aligned to the original df , you can then drop the duplicates afterwards .
Note that I first replace periods with commas before parsing .
It basically pivots by date and grade , rolling forward the cumulative sum .
Now when I do this on this data frame it will strip the column headers .
Doesn't Pandas provide a way of extracting the data from a frame , and then replace it with a new copy ?
You can also avoid the ` drop ( ' level_1 ' , axis=1 )` by using ` reset_index ( ' Name ')` .
First create the mask of dates / times you don't want , and then invert it .
You can see each facet's data but not alter it in place , and the way heatmap expects its arguments isn't enough like how plot , etc ., expect theirs to map over easily , and in short roll-your-own looked like the way to go .
Turns out you can do it pretty conciesely with just seaborn if you use ` map_dataframe ` instead of ` map ` .
How can I join 2 rows in a dataframe into 1 row in a new one ?
You explicitly set the index for your right df , if it's the same number of rows and you want them to align why not just use the same indices as left : ` right = DataFrame ( bf.values [ 1 :: 2 ] , index= left.index )` ?
then the concat would produce what you want no ?
Your attempt to ` concat ` aligned the 2 df's by index thereby producing a disjointed df with 4 rows rather than 2 rows : #CODE
The above creates a new df using the values from your df but you also took the index values also , seeing as the left and right df's have the same number of rows and you're want to concatenate them column-wise so that the indices align then you can just use the same index from the left df : #CODE
I have a time-series data in " stacked " format and would like to compute a rolling function based on two columns .
I can apply ` stack ` at the end to get back to tall format .
I am just getting stuck on " setting with chained indexing " and setting with iloc / loc / ix .
I can't figure out how to represent this using iloc , loc and ix .
Carrying on from the same idea , you could set a MultiIndex for ` df2 ` and then stack .
It seems you can set a flag with pandas to_sql ( if_exists= ' append ') .
The best approach I have found so far is to create an ` ON INSERT ` trigger in the database table , that updates all fields if inserting a duplicate primary key .
to_sql ( if_exists= ' replace ')` with this new dataframe object .
You could use ` loc ` and a boolean condition to mask your df ( here representing a.csv ) and set the label to 1 if that condition is met : #CODE
Pandas indexing by both boolean ` loc ` and subsequent ` iloc `
I want to index a Pandas dataframe using a boolean mask , then set a value in a subset of the filtered dataframe based on an integer index , and have this value reflected in the dataframe .
Chaining an ` iloc ` onto the ` loc ` call above works to index : #CODE
try ` df.loc [ mask [ 0:2 ] , ' c '] = 1 `
oh right , i forgot that ` mask ` was a series ... perhaps ` df.loc [ mask.iloc [ 0:2 ] , ' c ']` ?
This does work but is a little ugly , basically we use the index generated from the mask and make an additional call to ` loc ` : #CODE
EDIT : Have to be a little bit careful with this one as it may give unwanted results with a non-unique index , since there could be multiple rows indexed by either of the label in ` ix ` above .
I must pivot multi values ??.... float and int ... but it doesn't work .
In short , you are specifying values with ``` value ``` , so you could just just convert that to float before doing the pivot .
" Check length of DataFrame , If length of DataFrame > X , Remove first row , Append new row " ?
Unclear why you need to keep appending but if you want to impose a limit why not just check ` len ( df )` and not append once the len reaches your limit ?
@USER Something like a buffer dataframe of ` len ( df )= =X ` whenever new row of data comes in , the older or first row is removed , like FIFO , to preserve the buffer size .
Instead of appending , why not replace the old row with the new row ?
You can just append a new tuple to the end , and if it gets too full it will dump the corresponding on from the beginning .
One way would be to pre-allocate the rows , and replace the values cyclically .
In most of the scenarios that I deal with where truncated append performance matters , neither of the above are true , but your scenario may be different .
So , I believe I need to upsample or reindex while spreading the monthly values over every day in the month , but I can't get it to work properly .
Have a look at [ ` resample `] ( #URL )
I guess I wasn't explicit enough -- I have been trying to use resample , but the updated DF starts at January 13th rather than January 1st .
Next , pivot the DataFrame , using the month as the index and the ticker as a column level : #CODE
Now we can reindex and forward-fill the DataFrame : #CODE
I hadn't even thought of using pivot table , and the DateOffset trick is sweet .
Then create a boolean mask #CODE
Edit : I have defined intersection as the following : #CODE
This will produce a boolean mask of the rows where they match one of the values in ` intersection ` and we invert the mask using ` ~ `
I'm getting a correct solution as ` df.ix [ ~ df.isin ( intersection ) .apply ( lambda x : x.any() , axis=1 )]`
No , I'm filtering the entire df based on whether a series doesn't have that value , it'll return a boolean series that can be used to mask the entire df
@USER Also calling ` apply ` should be the last resort when working with arrays , it is not vectorised and therefore will not scale well
You could reset the index and drop the rows and set the index again , but the index should still be preserved , my example shows that the index values are preserved , you'll have to post data , code , desired output and errors giving feedback using comments is not that informative
How do I join 4 python lists to create a pandas dataframe ?
I want to join them all together to create a table .
A pandas dataframe will be the easiest way to work with the data so I'm wondering how I join all 4 lists ?
Do you want it as 4 columns , or append them to have one long structure ?
The other option was to create 4 dataframes from the individual lists and then join them but this didn't seem very efficient - the dictionary method will work better - thanks .
I guess the best option is to make an inner join and then the ( df2.c3 df1.c1 ) ( df2.c4 df1.c2 ) filtering but the problem is that the inner join would create a huge table , since classid are not indexes and not unique row identifiers .
so what's wrong with your current boolean mask ?
You can supply series for comparison but it depends on what you're doing , you should just filter the df first and then you could iterate over the unique classid values and then drop the rows where the other 2 criteria are not satisfied
Iterating should be the last resort , I'd merge the other dfs columns c1 and c2 to df : #CODE
The filtering on groups is not what I need , but the merging is what i intended through " left inner join " .
This is how databases do , that is why you can have " join " and " where " in the same SQL statement .
I was looking for an answer to a similar question but to produce a mean etc on nonzero items .
You'd have to resample your data to daily , at the moment it doesn't look like your lat and lon change at all over the course of the day , at the moment your question is very broad as you are a long way from the final code so really this question should address the distance measuring by itself and you should then check existing questions about resampling
I have dataframe with two columns dt ( date-time stamp ) and value .
So long as dt is a datetime dtype already you can filter using date strings , if not then you can convert doing this : #CODE
I was trying to create a function for the ' apply ' method , and noticed that if you run apply on a series the series is passed as a np.array and if you pass the same series within a dataframe of 1 column , the series is passed as a series to the ( u ) func .
With ` df ` as above , make a pivot table : #CODE
What's wrong with the answer using ` pivot ` ?
I know the ` merge ` would allow me to add the column , but I would have to either overwrite the existing DF or create a new one , like this : #CODE
Is there not a way to join on one field ( maybe an indexed column , maybe not an indexed column ) and update / add the field ?
Perform a left ` merge ` : #CODE
In ` pandas ` , specifically with this merge , I would need to overwrite DF2 .
No you can conditionally overwrite but it depends on the relationship between the 2 dfs , so you could either mask or select the rows of interest from the lhs and rhs and assign the values
The ` loc ` call will mask the lhs so that the result of the transform aligns correctly
Well it should be obvious why that would fail , you're trying to assign the transform result which is aligned to the df ` df2 [ df2 [ ' rr_quality '] > = 150 ]` to the original unfiltered df .
How did you create the pickle file ( was it created in pandas ? ) , can you post the entire stack trace ?
@USER also to get the total number of groups use ` len ( df [ 1 ] .unique() )` .
mode isn't a groupby method , though it is a Series ( and DataFrame ) method , so you have to pass it to apply : #CODE
For each set_up in set_ups I want to apply ' value ' to ' set_up ' and groupby ( level=0 ) or df.A and df.B .
First , we can perform a groupby / apply operation to obtain the Protein / Peptide pairs with the two largest Peptide counts for each Protein : #CODE
Now we can merge the original DataFrame , ` df ` with ` counts ` , by joining on the columns of ` df ` and the index of ` counts ` .
Using an inner join guarantees that only those Protein / Peptide pairs which are present in both ` df ` and ` counts ` show up in ` result ` : #CODE
you can then perform a groupby on ' Date ' and call ` transform ` on the ' temp ' column to perform the count : #CODE
To filter a df using multiple values from another df we can use ` isin ` , this will return a boolean mask for the rows where the values exist in the passed in list / Series .
In order to filter out these values we use the negation operator ` ~ ` to invert the mask : #CODE
As a start , this will give you every row where the difference is greater than 5000 . data [ abs ( data [ " 2 "] - data [ " 2 "] .shift() ) > 5000 ] .
Using only rows you showed ( and so changing the diff to 100 instead of 5000 ): #CODE
Here is the code , ` count_consecutive_zeros() ` use numpy functions and ` pandas.value_counts() ` to get the results , and use ` groupby() .apply ( count_consecutive_zeros )` to call ` count_consecutive_zeros() ` for every group . call ` reset_index() ` to change ` MultiIndex ` to columns : #CODE
However , I am unable to merge that result back into the original DataFrame .
set the index of ` df ` to ` idn ` , and then use ` df.merge ` . after the merge , reset the index and rename columns #CODE
Use the ` transform ` method on a groupby object : #CODE
so if I have following groups [ g1 , g2 , g3 , g4 , g5 ] , i want to iteratively call them in pairs like [ g1 , g2 ] , [ g2 , g3 ] , [ g3 , g4 ] .... and take the intersection of the 2 groups of series everytime .
so if I have following groups [ g1 , g2 , g3 , g4 , g5 ] , i want to iteratively call them in pairs like [ g1 , g2 ] , [ g2 , g3 ] , [ g3 , g4 ] .... and take the intersection of the 2 groups of series everytime .
pands groupby by diffent key and merge
and also the every user_id , item's amount of distinct type_id , and merge then by the user_id
It creates a heat map as shown below .
I am wondering if there is a way to do something clever ( or basic ) with pandas to make use of vectorization - maybe using shift or an offset function ?
I would like to join / merge them , sort of an outer join so that rows are not dropped .
None of the pandas tools I know worked : ` merge ` , ` join ` , ` concat ` .
merge's outer join gives a dot product which is not what I am looking for , while ` concat ` can't handle non unique indexes .
You want to use the ` on= ' outer '` argument for ` join ` ( ` test1.csv ` and ` test2.csv ` are the files you gave ): #CODE
I've managed to sort it out using pandas ' ` concat ` method .
Now we can use ` concat ` : #CODE
I'd look at seeing if you can export it in it's raw form , otherwise this must be a common problem and someone somewhere has probably coded a method to strip the emojis out of the text
where list_1 and list_2 was generated because len ( group_set ) from my current code is equal to 2 .
In your sample df , so long as all the present values are the same then the following would work , we use a boolean condition to mask out the ` 0 ` values and then call ` mean ` and pass param ` axis=1 ` to calculate the mean row-wise : #CODE
and then drop the NaN .
I renamed your columns to ' author ' and ' citations ' here , we can groupby the authors and then apply a lambda , here the lambda is comparing the number of citations against the value , this will generate a 1 or 0 if true , we can then sum this : #CODE
Pandas : column of type str converted to tslib.Timestamp after using apply function
Is this a implementation problem of pandas or the apply function that I was using ?
Python pandas : equivalent to SQL's aggregate functions ?
However , I need different aggregate functions on different columns : on some columns none at all , on others sum and avg , on other just the maximum , etc .
No you can pass a dict consisting of columns and functions to ` agg ` to perform this
You can apply multiple functions to multiple fields : #CODE
I realise I could do each field seperately but in the actual problem I am trying to solve I perform a join between the table being updated and an external table within the " updater " ( i.e. square ) and want to be able to grab all the required information at once .
I want to be able to align information from the table containing the date ranges against the point-in-time table .
Please note a merge won't work due to problems with the data ( this is actually being written as part of a dataquality test ) .
So what are you trying to achieve here ? for squaring the values this is trivial to perform and doesn't require the use of ` apply ` in this case
I am trying to create arbitrarily many fields and assign them values simultaneously using the apply method .
Once I can do this I can replace the trivial function with any value .
But it will involve having the apply return only a single field at once .
What I was curious about was being able to use apply to return arbritrarily many fields .
Unlike Numpy , Pandas doesn't seem to like memory strides
Pandas seems to be missing a R-style matrix-level rolling window function ( ` rollapply ( ..., by.column = FALSE )`) , providing only the vector based version .
That is , it gives me 3 strides of 3 rows each , in a 3d matrix , allowing me to perform computations on a submatrix moving down by one row at a time .
Or maybe it would be better to write the data windowing while taking the array ` strides ` and ` itemsize ` into account .
To fix things , you could copy the data in C order and use the same strides as in your question : #CODE
Alternatively , if you want to avoid copying large amounts of data , adjust the strides to acknowledge the column-order layout of the data : #CODE
But as @USER ' s comment points out , if you want to do more than print the sum of the rows , or if you want to transform the data in any way , you will probably find pandas or numpy to be more efficient both in processing time and programming time .
Pandas : Weighted median of grouped observations
I would like to compute the median income group .
which visually is what I mean - the median here would be ` 7 ` .
Sorry you want the index of the median value of INCAGG ?
Well the median value of your incomes is the one at row 5 is what pandas tells me
You just want a weighted median .
The second code line above is dividing into rows above and below the median .
The median observation will be in the first ` False ` group .
This will work fine when the median is unique and often when it isn't .
The problem can only occur if the median is non-unique AND it falls on the edge of a group .
Finally , I converted it to a Series and calculated the median : #CODE
I have this pivot table : #CODE
How do I make a dictionary with ' store_nbr ' as the key and ' item_nbr ' as the values from the pivot table shown .
What did you pivot from ?
As a first step you could load the first csv and then drop the duplicate authors and write this out as the clean csv , this sounds like a messy many to many relationship , it sounds like the publication is the unique thing here so you probably want to repeat the publications for each author
@USER I originally had it insert directly into an sql database .
I've since noticed that removing the resample ( " W ") solves the issue .
How about posting ` head ` of the various dataframes ( incl . the result of resample ( ' W ') , etc . ) .
python pandas .filter() method using boolean mask
I've accomplished this by creating a boolean mask from which I plan to filter out all days that satisfy the criteria : #CODE
And receive ` TypeError : filter function returned a list , but expected a scalar bool ` .
This is super messy and there must be a basic way to say , here is my dataframe ` z ` , then ` groupby ( ' z.index.date ')` , then ` .filter() ` based on the boolean mask ` r ` .
The original approach you suggested is correct , although you have to use a ` transform ` on the groups ( by ` date ` AND ` source `) instead of an ` apply ` .
` transform ` return the group information with the same structure of the original dataframe .
You could compare the length of ` len ( df ) == len ( df [ ' ID '] .unique() )`
I would like to avoid using an append because my dataframe is has over four million lines , so I think an append might take a little too long .
It splits up the DataFrame by store_nbr , calls is_good on each each group ( apply ) to determine the rows you want to keep , puts everything back together in the right order , and then takes a subset of rows from the original frame .
I'm not sure what your level of expertise is based on the question , but I'd start by looking at the docs on pandas ` merge ` .
If you want to apply it on a full column , you can also do : #CODE
I need to do a boxplot by month .
I can't run your code as ' cut ' is not defined , but what version pandas and numpy are you using ?
@USER Yes , I also noted that . transform is throwing the error .
For example , whenever you see " groupby + unstack " you should think " pivot_table " : #CODE
How can I aggregate by the value in the identifier column , like this : #CODE
How to insert one column in an existing DataFrame into a new column in a table in existing database
I created a new column in my table ( which originally has two columns ID and Name ) , labeled " Total " and I want to insert column 1 values into it for each corresponding ID .
Then rewrite the whole table again using df.to_sql ( ..., if_exists= ' replace ') .
I will try to recreate the table using a query and replace it with my DataFrame using df.to_sql ( ..., if_exists=replace )
Aggregate by repeated datetime index with different identifiers in a column on a pandas dataframe
Now I want to do a boxplot by month , so I would imagine that I can group by it : #CODE
How can I create this boxplot without the dummy dataframe ?
this then allows you to groupby by month and generate a boxplot : #CODE
for any more complicated mask , you'd set up the mask separately ; see the pandas docs
I'm not quite sure what you want end-goal wise , but if cphlewis's suggestion to go with seaborn isn't what you were looking for , you might try converting your DataFrame to a multiindex , instead , and plotting it out that way .
No columns are text : only int , float , bool and dates .
The only thing I can think of is to export just the structure , i.e. column names and data types but no rows , to SQL , then export the file to CSV and use something like the import / export wizard to append the CSV file to the SQL table .
Or just export the data to a csv and then use bulk insert ( which is very , very fast ) .
The ` DataFrame.to_sql ` method generates insert statements to your ODBC connector which then is treated by the ODBC connector as regular inserts .
The proper way of bulk importing data into a database is to generate a csv file and then use a load command , which in the MS flavour of SQL databases is called ` BULK INSERT `
grouping into time intervals can be done using resample : #URL
Then we resample #CODE
If you want to change the index then either use ` reindex ` or assign directly to the index : #CODE
So if you assigned the data to the values rather than the df itself then the df does not try to align to the passed in index
After stepping through the code here , the issue is that it's using the passed index to reindex the df , we can reproduce this behaviour by doing the following : #CODE
Update this is expected behaviour , if you pass the index then it will use this index to reindex against the passed in df , from @USER
This is the defined behavior , to reindex the provided input to the
EdChum is absolutely right with the suggestion to use reindex , but I think what's happening here is that when you use a DataFrame as the argument for the data parameter , it uses the whole existing DataFrame when creating the new DataFrame .
Average of aggregated value by month and boxplot average differs on pandas data frame
Now I want to generate a boxplot , so I use this : #CODE
Boxplots provide the median , not the mean .
Box plots use the median , not the mean .
Most box plots use the median .
The Pandas [ documentation ] ( #URL ) does mention that the median is used , although it doesn't describe that it is the central line .
You can ` groupby ` the ' user_id ' select the ' video_views ' column and call ` transform ` and pass the function ' sum ' and test if this is greater than ` 0 ` , this will return a boolean series , which you may want or if you prefer you can cast this to an int which is what I've shown : #CODE
I'm making use of pandas , and I've iterated through each of the files to build an array that includes all of the unique values I'm trying to replace .
I can't individually just use pandas.factorize on each file , because I need ( for example ) ' 3001958145 ' to map to the same value on file1.csv as well as file244.csv .
I've created an array of what I'd like to replace these values with just by creating another array of incremented integers .
The function I wrote to do this works ( it relies on a pandas.factorized input rather than the arrangement I described above ) , but it relies on the replace function and iterating through the series so it's quite slow .
This keeps the categories in the same order , so when we factorize we get the same numbering scheme .
To prove it , we select the codes ( the factorized map ) from each .
I wasn't familiar with the symmetric difference method ( it's a SQL JOIN ! ) , but I see how that will be useful in the future .
The source data only records changes in amounts , and I want to insert the missing dates with the amounts pre skip .
I then unstack the DataFrame so that I just have the dates in the index .
Finally , I fill forward the values to remove the NaNs , and stack the currencies : #CODE
However , you can easily switch rows and columns with the transpose ` .T ` , and then it may be more tractable , and in fact the control mean is a one liner .
You can do that using ` unstack ` : #CODE
Python pandas : How do I drop specific levels in a hierarchical index if any column values are NaN ?
I want to drop entire levels ( in this case , countries ) in my hierarchical index if ANY of the data values for that country are NaN .
You were on the right track with ` df.dropna() ` , just that you need to ` unstack ` your data before you apply it .
pivot the ` DataFrame ` to make " Year " the inner most column labels #CODE
I want to cut this one by a specific timerange .
you could read all the csv's into a list of df's , then ` concat ` them all set the index to be the time and then filter using ` df.loc [( df.index.time > ' 09:00 : 00 ') & ( df.index.time < ' 09:15 : 00 ')]` , whilst loading them you could sort on index and chuck away the ones that don't have that range anyway
My way would be to only select Order 1 as a dataframe , then only select Order 2 , then merge , then subtract .
I do that with ` reindex ` and ` rename ` , chaining them with the original aggregation in a fluent style , like so : #CODE
Or is there a way to explicitly specify the column names and ordering right in the ` agg ( ... )` step ?
I am asking because I am new to the idioms of this API and want to get them right , and because it looks like ` reindex ` and ` rename ` both create ` DataFrame ` copies , which could be a bigger issue with large data sets ( I am aware of the ` inplace ` parameter for ` rename ` , but that wouldn't work in my fluent setup ) .
The implementation of DataFrames is very efficient for this , and doing the agg internally is creating copies just like this .
` .columns ` is more efficient as it just replaces one list with another , rename has to lookup each entry and replace it ( useful sometimes ) .
Right now when I try to do this all in one ` agg ( ... )` step , the ` val_2 ` p-value column definition overwrites the original mean column definition , because they're both in the same ` dict ` .
My question : is there some way to do this all in one ` agg ( ... )` step , without having to create a second result ` DataFrame ` and performing the ` merge ` ?
( See my other closely-related ` agg ` question here . )
You can drop a level #CODE
It uses a list comp to concat your column names which I got from here #CODE
The last expression would apply to your case if ` res_tmp.fittedvalues ` are the predicted or fitted values of your winsorized model , and ` y_orig ` is your original unchanged response variable .
Maybe I can use that in combination with some boolean mask .
Now if you get two rows with 1 match they will have len ( cols ) -1 miss matches , instead of only differing in non-NaN values .
Then apply it pairwise to every column using #CODE
I am looking for a way to sort a multiindex data frame based on one of the indices .
To make it clear you only want to assign a copy of the data ( versus a view of the original slice ) you can append .copy() to your request , e.g .
Perhaps you meant df.loc [ 0:28 ] , noting loc is an inclusive slice .
We can then iterate over this list of tuples and slice the orig df and append to list : #CODE
Find the ` NewEntry ` rows , add ` [ -1 ]` and ` [ len ( df.index )]` for boundary conditions #CODE
But , if your looking to daily / monthly aggrgartions the function , you are better off using ` resample ` or ` TimeGrouper ` !
I have used the ` resample ` but it also need the function such as ` sum ` , ` mean ` or etc
How to replace comma with dash using python pandas ?
I am looking for a way to be able to replace commas in the key with ' - ' and then be able to separate different key , value pairs in the count_dic.something like this : #CODE
Since ` count_dic ` is actually a ` dict ` , then you can apply ` len ` to get the number of keys , eg : #CODE
I am new to python and pandas and am attempting to make aggregate plots of orientation data across response time for my research .
But then ` len ( data )` would equal 1 , so ` len ( data ) 0 ` is not the right condition to check to see if ` data ` is an empty list of lists .
But since you want a list , just append .tolist() to the result .
Python DataFrame - apply different calculations due to a column's value
You could do this using 2 ` loc ` calls : #CODE
I want to replace the NaN values with a ' yes ' or ' no ' depending on which count is greater based on the ' first ' column and if they are equal make it a ' yes ' .
However , in your current example ` b ` doesn't conform to your logic as ` b ` has one ` yes ` and one ` no ` .
Then we map a lambda function that takes the first row of our boolean indexed testCounts #CODE
Boolean index like before and map the dict ( d ) to " first " #CODE
How do I merge two tables and divide numerical instances from a previous value count in each cell in Python ?
I figured the logic to this would be to merge original table with pivoted table and have each cell with a numerical dividing each particular sum of instances by the value.count of the PlayTennis responses , but I just don't know the syntax for it .
To do this , you can use different aggfuncs when you create your initial pivot table , as follows : #CODE
And , then insert it into the query string - #CODE
pandas : read_csv how to force bool data to dtype bool instead of object
When reading the csv the bool column gets typecast as object which prevents saving the data in hdfstore because of serialization error .
To convert the ` NaN ` value use ` fillna ` , it accepts a dict to map desired fill values with columns and produce a homogenous dtype : #CODE
If your data has empty dicss , then you can just test the ` len ` : #CODE
merge multi-indexed column names on pandas dataframe
Is there a simple way to merge the column names , so that I end up with a simple column index with their proper names ?
then put the pieces back together using concat : #CODE
Here is the code so far ( apologies , tried to cut excess off but leave enough to follow + txt ): #CODE
I notice one shrieker : you're using map as a variable name ?
` data [ ' projected_lon '] , data [ ' projected_lat '] = map ( * ( data.Lon.values , data.Lat.values ))` is just wrong , I think .
Replace negative values in dataframe by NaN , replace NaN with fillna method
I would like to replace negative entries in the column of a dataframe .
I am setting the values to NaN and then apply the fillna method .
replace series data with blanks
Aside from display reasons why do you want to replace with blank , is it just so that when you export it back to xls you get the display you want ?
drop duplicates will complete remove those rows .
Use ` loc ` and ` shift ` to detect when the rows change value , we can then use the boolean mask to set these rows to blank : #CODE
join multiple csv files in one folder using pandas [ MemoryError :]
I have 5 csv files in one folder , In here I want to join all columns from each csv file into one dataframe .
When I only join 2 csv files , it works well , but when I want to join 5 csv files , I faced an error .
Your indices are duplicated so basically when performing a right merge it's exponentially finding matches hence why you run out of memory , What is the first column supposed to be ?
I've posted an update as I think that seeing as your dfs are equivalent row-wise you should be able to just append to a list and then ` concat ` them all
Also looking at what you're doing I think you don't need to merge , just ` concat ` a list of dfs : #CODE
it's tricky with lists , as it's an iterable it's expecting the length of the elements to match so the following would work : ` f [ ' DFA '] = [[ ' Binary ' , ' Html ']] * len ( f )` this creates a list where it's repeated by the length of the df
Re your question about bootstrap_plot doing the Right Thing : well , this is an area where pandas is still improving in general , but there's often going to be a bit of manual labor in this area and it's not generally that hard to do something with fillna or notnull .
Even if I only have 1 worker child process , doing a simple ` df.loc [ ix ]` can start taking many seconds each time , and the worker does seem to be duplicating the data from the managing process .
So I tried to create a new column with the dates to try to transform it : #CODE
You could build a boolean mask using ` isnull ` : #CODE
` ~mask ` is the boolean inverse of ` mask ` , so ` df.loc [ ~mask ]` selects rows where ` a ` is not null and ` c ` is not 0 .
And then I'd have to do a truncate table .
... will let you map the columns to their respective SQLAlchemy types .
I can run a SQL statement to drop or disable the constraints , but I'd like the option to create the table with no constraints at all in the first place
My goal is to resample the data and average it using the coefficiets , ( missing data should be left out ) .
The problem is that the code runs forever , and I'm pretty sure that there's a better way to resample with coefficients .
Could you please help me understand how the correlations translate to the expected output above ?
After loading your csv using ` read_csv ` like so : ` df = pd.read_csv ( file_path )` you can then use ` pivot ` and pass the columns Employee and Date as the columns and index respectively : #CODE
Please show the format of ` user_input ` , also you should be able to replace ` frame = pd.DataFrame()
the fact remains your code as it stands does nothing with it as you overwrite it with the result of ` concat ` , but after the concat line you should change ` frame [ ' Date '] = pd.Series() ` to ` frame [ ' Date '] = pd.to_datetime ([ user_input ] , format= ' %m . %d ')`
For now I'm finding it less fiddly to disable or drop the constraints with a SQL statement .
But you can do this with eg ` {col : Boolean ( create_constraint=False ) for col in df.select_dtypes ([ ' bool ']) .columns } ` .
I think the main problem here , is because I'm trying to apply the ` pool.map ` to ` rpy2 ` function and not a Python predefined function .
You could put both conditions in as a mask and use ` ` : #CODE
Sorry are these strings or integers you're converting , for the % one you could strip the % sign off and then cast it , also can you post raw data to show us what it looks like , thanks
Modify your ` replace ` to include ` regex=True ` : #CODE
If this is just for numerical work you're doing in an interpreter , ` eval ` is alright , but for any code you're releasing , it's very insecure : it will evaluate whatever code is in the string passed into it , thus allowing arbitrary code execution .
Python datetime transform
If you're starting with things in string format you can convert to datetime and then use pandas ` dt ` to extract hours and such .
Now you can extract hours / minutes / etc . very easily with ` dt ` methods .
dt accessor
How to change the arguments of map function to get repeatation of dates in two groups :
I am struggling with the easiest way to do a case insensitive merge in pandas .
Is there a way to do it right on the merge ?
Lowercase the values in the two columns that will be used to merge , and then merge on the lowercased columns #CODE
When you want to add a calculated column from a ` groupby ` you should use ` transform ` : #CODE
` transform ` returns a series aligned with the original df
You could use ` merge ` and extract the overlapping keys #CODE
ok.will check on pivot table
It looks like it's confused the id with the index , is the df loaded correctly otherwise or do you want to just ` reindex ` or ` reset_index() ` the df ?
A possible solution is to call ` df.reset_index ( drop=True )` to reindex your new ` DataFrame ` with ordered numbers according to your needs .
So it there a way to " merge " them instead of place side by side ?
JonhE , your method did not work because the mask : #CODE
maybe the .map() was trying to apply both the mask and the operation : #CODE
Nonetheless , I found a solution by integrating a mask : #CODE
to a second mask : #CODE
I then used this second mask in the operation : #CODE
I'd either construct a df for each dict and then ` concat ` them all , or flatten all the dicts into a single dict and construct a df from that flattened dict
I edited my code as follows , yet I still have the same issue : ` tickersasstrings = map ( str , tickers )
Unstack a MultiIndex pandas DataFrame counter-clockwise instead of clockwise
I have a multiindex pandas DataFrame that looks like this : #CODE
In order to get a stacked bar chart it seems like I need to unstack the Severity index column .
pandas - unstack with most frequent values in MultiIndex DataFrame
I would like to group the ` User ` field , keeping only the most frequent ` DLang ` code , then unstack and count the numbers of ` User ` in each ` GridCode ` .
Also , I think the example data you provided is cut off by a few rows ( there is only one ` ca ` entry , not 3 ) .
Merge just the most frequent / max occurrence df with the original input .
This will drop rows where users used a ` DLang ` other than the most frequent #CODE
So the boolean condition looks for freq values greater than 5 and also where the word is not in the other df using ` isin ` and invert the boolean mask ` ~ ` .
In this case , I might ` set_index ( " Gene ")` and then use ` loc ` to pick out the rows : #CODE
Ah , that is very nice use of loc and quite a bit simpler than mine .
You can use the apply function : #CODE
Well if that's the case you can construct the ` eval ` string , I will put an example on my answer ;)
It returns an aggregate with different shape from the original dataframe .
median of pandas dataframe
I would like to look at the rows that are to within a factor of 10 from the median of the count column .
I tried ` df [ ' count '] .median() ` and got the median .
I can use any measure as the distance from median ( absolute deviation from median , quantiles etc . ) .
the resulting DataFrame using the dict of Series and ` reindex ` to generate
I have a ground truth dataset ' gt ' ( with 100 entries ) which looks like this : #CODE
In other words , you want replace ` gt ` ' s ` org_o ` names from ` df `' s ` match ` and then take the counts or ..
@USER .carstensen yes and the fact that they belong to the same group in the gt data
Filter out rows with ` gt [ ' org_o ']` values in ` df [ ' org_o ']` using ` df [ ' org_o '] .isin ( gt [ ' org_o '])`
A minor addition would be in case , if ` gt [ ' org_o ']` has repetitive values , you can take unique array instead .
We are using numpy's where function to check if the ` org_o ` in ` df ` is present in ` gt ` .
As far as efficiency is concerned , this " lookup " is fairly efficient , because when using ` isin ` , pandas will convert the values to compare ( in this case ` gt [ ' org_o ']`) into a set , so the lookup time will be O ( n * log m )
Step 2 - The criteria for the join is - #CODE
tip : join all of the records , then do the filtering later .
You can perform a ` merge ` , it will automatically align on the common columns and the default type is ` inner ` : #CODE
And , the result using ` merge ` #CODE
However , when I try and join the effective coverage series , and print out the dataframe of just one of the groups , it just returns : #CODE
And does not successfully join my newly calculated effective coverage .
And , then ` apply ` ` split_cumsum ` over ` df.groupby ( ' Group ')` #CODE
Thanks John , I didn't quite appreciate what circumstances would make sense to create a function and apply it to a DataFrame , but this solution is definitely a cleaner approach .
Also , you could use ` diff ` in ` groups ` #CODE
For each row , extract string items and join them .
That is not so surprising but ` apply ` does not scale well , I just did timings on a 600 row df and the timings were 6.24ms vs 33.3ms comparing my method against yours , I expect the performance difference to increase significantly on much larger datasets
@USER Absolutely , ` apply ` doesn't perform well on larger datasets .
Now what I want to do is for date ( t ) , append date ( t+1 ) to the dataframe .
alternatively if you want to replace the date column you can use ` replace ` #CODE
Then call ` map ` on the orig df and pass the ` date_df ` and access the ` date_lookup ` column , ` map ` will use the index to perform a lookup which will return the corresponding next value : #CODE
Conveniently , if you try to add ` bool ` s in Python , they get forced to ` int ` s , their superclass .
you need to unstack so ` type ` are columns , and then use the ` subplots ` parameter : #CODE
Is there some sort of ` apply ` equivalent ( like in ` pandas `) that would make this more efficient ?
Pandas Apply ( axis=1 ): produce more than one row
I have a function I want to apply by row like so : #CODE
Unfortunately , my current method isn't the correct usage of the apply method : #CODE
This question is similar to pandas : apply function to DataFrame that can return multiple rows that Wes McKinney has answered .
In particular , look at ` transform ` .
I am trying to insert a pandas dataframe in to oracle table with the following code : #CODE
Isn't there any other means to insert other than this ?
You can use ` duplicated ` which will return a boolean series to mask the df and then call ` unique ` to return an array of the duplicated IDs : #CODE
I have a dataframe ' gt ' like this : #CODE
and I would like to add column ' count ' to gt dataframe to counts number member of the groups , expected results like this : #CODE
Call ` transform ` this will return a Series aligned with the original df : #CODE
It looks like the param ` index_col=0 ` is taking precedence over the ` dtype ` param , if you drop the ` index_col ` param then you can call ` set_index ` after : #CODE
An alternative is to drop the ` index_col ` param and just call ` set_index ` on the df returned from ` read_csv ` so it becomes a one-liner : #CODE
Call ` map ` and pass the dict , this will perform a lookup and return the associated value for that key : #CODE
Let , ` df ` the dataframe with two columns , apply conditional absolute minimum over rows using ` axis=1 `
OK , after reading and understanding your question and not being able to find a vectorised approach , we can define a custom function and call ` apply ` and pass each row .
So this will check if either column is null if so return the min value , it then compares the abs value of either column and then returns the column that has smallest abs value but the original value including sign : #CODE
But * pandas * basically does few queries : check if table exists , create it if needed , and the insert statement itself .
You could then apply ` np.std ` to that array : #CODE
Pandas DataFrame merge between two values instead of matching one
I have a Dataframe with a date column and I want to merge it with another but not on a match for the column but if the date column is BETWEEN two columns on the second dataframe .
I believe I can achieve this by using apply on the first to filter the second based on these criterion and then combining the results but apply has in practice been a horribly slow way to go about things .
Is there a way to merge with the match being a BETWEEN instead of an Exact match .
instead of merge on date=BeginDate or date=EndDate I would want to match on date BETWEEN ( BeginDate , EndDate )
I ended up realizing I was over thinking this I added a column called merge to both tables which was just all 1's
then I can merge on that column and do regular boolean filters on the resulting merged table .
Set column name for apply result over groupby
What I'd like to do is assign a name to the result of ` apply ` ( or ` lambda `) .
Pivot table problems : error ' No numeric types to aggregate '
I can't figure out why my pivot table isn't working , here is the pivot table and a sample of the data I'm working with : #CODE
Also , You can ` groupby ` your data on `' Outlook ' , ' PlayTennis '` get the count and use ` unstack ( ' PlayTennis ')` #CODE
The time column values might be different ( one DF could be longer than the other , the data wasn't sampled at all of the same times , etc ) , but if I merge the tables on the time ( and file ) column , then any discrepancies are filled in with NaN ( and I will fill them in with DF.fillna ( 0 )) and the DF can be resorted by the time .
Currently I can't even tell if you want to resample when you query , when you concatenate variable results , or when you write to csv .
If you had an example or suggestion of how to resample at any of those stages it would be helpful to at least see what that would look like .
So are you wanting it to align to the ` df.index ` ?
have you tried ` join ` or ` merge ` ?
If i understand you can use ` concat ` : #CODE
I cannot use a straight append because I need the ' column merging ' ( for lack of a better word ) functionality of the ` join= ' outer '` argument in ` pd.concat() ` .
I am concatenating on column name with an outer join which means that any columns in ` df2 ` that are not in ` df1 ` will not be discarded but shunted off to the side .
Looks like you are trying to row-wise concat , even though you text indicates that you what column-wise .
Concat , note using ` axis=1 ` as this is column-wise concat .
If you want to specifically have groupB always on top , you can then use the reindex method and provide a custom index with rows of groupB on top .
I have some code , reduced to the example included below , which takes some raw data , creates a pivot table from it , then merges it with another dataframe , and finally stores the results in an HDFStore object .
You can then easily slice and reindex the data as you please .
We make a pivot table out of it , forward fill the missing values , and re-order the column levels for convenience .
The purpose if to assign colors to a large number of items at run time based on a given color map .
As far as lookup + interpolation , I don't think you will find that combo , but it seems your problem can just as well be approached in two steps -- 1 . lookup , 2 . interpolate .
If you can pre-process your lookup table ahead of time , you can replace the binary search + interpolation with a simple lookup .
which I want to translate it to list of dictionaries per row #CODE
Use ` df.to_dict ( ' records ')` -- gives the output without having to transpose externally .
Your answer give me a better understanding how to use pandas groupby and apply .
What you want to do is ` groupby ` on the index levels and apply a function that calls ` mannwhitneyu ` , passing the two columns ` course1 ` and ` course2 ` .
You're performing [ chained indexing ] ( #URL ) which may or may not work , the recommended method for indexing with a view to updating / adding values is to use the new ` iloc ` , ` ix ` or ` loc ` , please see the [ docs ] ( #URL )
@USER I understand that ` loc ` is used for label based indexing .
How can I use ` loc ` to index the correct value with this MultiIndex in my DataFrame ?
` loc ` supports slicing the beg / end point is included in the range : #CODE
I would like to aggregate all the lists with the same id .
But what function would help me aggregate lists together .
I wrongly assumed sum() would not apply to lists .
I can obviously sort the ` dataframe ` but I am unsure on how to translate the labelling .
Perform a ` groupby ` on ' clients ' column and then call ` transform ` on the ' Price ' column passing in the ` rank ` method : #CODE
` transform ` will return a Series aligned with the original df
apply if statement within sort.head()
Ah . the ` reindex ` +1 for the one-liner
Pandas pivot and create extra columns for duplicates
These data do not map well to my situation .
If so how can I apply a function row by row and assign the output to a new column ?
You don't need to use apply when calling the function .
I reworked the formulae to apply to series : #CODE
You then need to use apply on the series to apply the function to each element in the list .
Okay , I just restarted the kernel in my ipython notebook and now with the above i get this error - ` AttributeError : ' numpy.float64 ' object has no attribute ' apply '` for the a= code that you gave above
It looks like you need to use ' cat ' like ' str ' or ' dt ' .
You could ` aggregate ` on ` ID ` with ` pd.Series.nunique ` and get ` count ` from ` trail ` #CODE
if you want to apply a function to every element , see ``` applymap() ```
Similarly to loc , at provides label based scalar lookups , while , iat
1 ) Merge columns 1 2 : #CODE
You can then drop col1 and col2 : #CODE
I have enough data to truncate some values from each vector without compromising the accuracy of the results .
When I apply ` plt.xcorr ( df.Val1 , df.Val2 )`
Keep doing this for all rows , such that we take log diff of next with previous and keep the next observation for other features in dataset .
Could you replace all the whitespace with blanks : ` df [ ' Col2 '] = df [ ' Col2 '] .str .replace ( ' ' , '')` and do the same for Col4 and then my previous comment should work
To avoid creating any new temporary variables or dataframes , you could just replace ' Col2 ' with new values and then drop ' Col4 ' .
Afterwards , just drop ' Col4 ' #CODE
You need to assign the result of ` append ` as it returns the result of the append : #CODE
I think the second link gets pretty close using unstacking multi-indexes , and I may be able to perform this if I am willing to aggregate , but I am trying to avoid that .
Something that would probably get you close to the right answer and also be pretty fast would be to ``` resample ``` and then ``` shift ``` .
Now we can shift this DataFrame by 365 dates : #CODE
Stack the SampleLocation for both df2 and df2_lagged : #CODE
Now merge over the lagged data to df2 .
The purpose being to then transpose , and index by the timestamp .
To get a " new column " you could use transform : #CODE
My pivot table looks like this : #CODE
@USER Yes , you just need to use ix rather than loc .
I tried , ` ix [ 0 ]` and am getting an error : ` KeyError : 0L `
Would importing this in to pandas as a pivot table and iterating them over row by row efficient ?
I would suggest you look into Map / Reduce .
I hold workshops on how to build map / reduce implementations in bash , but the chance that you are in Berlin is quite slim I guess .
` map ( sum , zip ( *lisolis ))` is good enough .
Look inside your function : you are only calling numpy functions , that is , you leave a small ( if any ) margin for Cython to translate to C code .
Cython is nos just ' type-definign ' variables but you have to make sure that , when calling cython's compiler , it will be able to translate as much of your code to C as possible .
Then you can use ` replace ` #CODE
Call ` df = df.ffill() ` and then call ` factorize ` , ` factorize ` returns a tuple of array values and an index which is composed of your Series values , we only want the array values here : #CODE
output from ` factorize ` : #CODE
Note : Before you apply ` factorize() ` you need to ` fill ` your ` NaNs `
How to replace a string value with None - python , pandas dataframe
In the above example , when I try to replace ' NaT ' the dataframe actually fills the value with preceeding value and not None .
In the actual dataframe I'm working with this usually throws up a type error telling me that I can't do replace None with method pad .
I don't think map ( str ) was working in the code I gave for the question but it works below where I assign it to only one field instead of the whole dataframe .
Lifting out the mask : #CODE
Breaking down the mask : #CODE
The single_line mask will not be the same length as the mask_x_y_equal .
Currently , to factorize() while enforcing the ordering I want , I first convert ` rate ` to Categorical , then sort the ` df ` and finally factorize so ` Bad ` gets the smallest integer value and ` Good ` gets the highest .
I don't know if this is better but you could remove the sort and do this instead : ` df [ ' factor_rate '] = df [ ' rate '] .map ( dict ( zip ([ " bad " , " neutral " , " good "] , np.arange ( 3 ))))` define your category list and use this to construct a dict and pass this to ` map `
The thing that the above solves is any worries about order and behaviour wrt to factorize as you are now in charge of the categories and their order
If you're concerned or reliant on categorical then another approach is to define your categories in a list and an order , use this to create a dict to map the order to the categories and pass this dict to ` map ` : #CODE
You can use ` groupby ` and ` transform ` to create new data series that can be used to filter out your data .
Before I accept your answer , how about the case when len ( selected_items )= =1 , i.e. I have only a single item to match to ?
I was thinking another option would be to create boolean masks for each of my cases and then summarize each mask .
" , I understand that I could simply do a count to get this information , but I want to apply the following restriction , " If there are n days between activity dates , only count the days before that gap " .
Your snippet seems to sum the days where the timedelta is less than 5 , I'm looking to drop the days after the first occurance of a 5 day delta , not just discount the day that has that delta .
This groupby / merge only builds a dataframe where there are visits , where as I want to be able to aggregate other fields regardless of if there is a visit or not .
add a helper column to aggregate ( needed as all other columns are used in the indices of the pivot table ) #CODE
then create a pivot table so the ` VisitDate ` column is in an index by itself , and the other dimensions are in the other .
Then resample the index to day .
Then unstack the frame , reset the index and fill ` NAN ` with 0 to get the frame you need .
Even with @USER ' s example you could still drop the non-matching indices right ?
Since the data is sorted and looks like ` End_time-Start_time == 250 ` , you could try rolling window sum ?
I guess you could reindex to normalize the windowsize , unclear how expensive that would be ( depends on your data ) .
The proper way to handle a big data problem is to use map / reduce .
The problem you are having can easily be solved with map / reduce .
How do I change order / grouping / level of pandas MultiIndex columns ?
I'm trying to reorder / swaplevel / pivot / something columns in a pandas dataframe .
I started playing around with pivot but that function doesn't really do what I need it to ...
How to reindex csv data efficiently ?
I then want to reindex the data so that I can access it through time type query : #CODE
I am certain I must be doing something wrong in my approach , as the goal is to read many files and merge them into a pandas DataFrame / Timeseries
It seems more logical to apply the filter to the time column as it is being read in then to operate on it later .
For example , ` lower ` ( for converting to lowercase letters ) , ` count ` for counting occurrences of a particular substring , and ` replace ` for swapping one substring with another .
Could you explain in words how you want to join the dataframes c and d ?
d is a dataframe of devices , with some components ( CPU , Freq and Voltage ) .
I have taken some large datasets from csv files , stacked them , and am trying to replace missing and negative values in my final column , with 0's .
Pandas resample by first day in my data
the groupby solution was what I was looking for , but it is still sort of strange to me that resample , doesnt really sample from the existing dates ( like what we did in groupby ) .
However , I've found one case where it's not immediately obviously how to apply his solution : using a custom grouping function .
But unlike most Pandas Cython tutorials or examples I am not apply functions so to speak , more manipulating data using slices , sums and division ( etc ) .
@USER in apply / agg the function needs to take the subDataFrame / each group . tbh I'm a little confused at what you're trying to do : s
I want to resample my data based on a categorical variable .
However if I understand you correctly , you want to resample to a certain target distribution ( e.g. 50 / 50 ) of one of the categorical features .
You could all ` df.filter ( regex= ' HW ')` to return column names like ' HW ' and then apply sum row-wise via ` sum ( axis-1 )` #CODE
It sounds like you want to resample ( with aggfunc=len ) ... but it's not clear as you don't include the expected result .
Use resample and sum to get the number of events per time period - examples below
I need to do a ` groupby ` , then apply a custom aggregation function using ` apply ` , as follows : #CODE
FIRST QUESTION : My idea is to use Cython to define the custom aggregate function .
I don't mean to replace the sort .
I'd like to insert / append one more row to this 2D array , how to do this ?
[ ` append `] ( #URL ) is designed for this
If you want the values themselves , you can ` groupby ` ' Column1 ' and then call ` apply ` and pass the ` list ` method to apply to each group .
You could ` groupby ` on ` Column1 ` and then take ` Column3 ` to ` apply ( list )` and call ` to_dict ` ?
I'm trying to compute the variance of a time series for many sampling frequencies ( the so called signature plot ) , I used the resample method looping on a set of frequencies but python stops before completing the task ( no errors , just freezed ) .
As @USER said , you can replace this piece of code #CODE
Suggest you are using ` xlwt ` module , first adjust your collection , then do ` merge ` to get your xls file correct .
@USER this is incredible .. clever use of apply !
Same code should also apply to a binary confusion matrix like : #CODE
I guess you have to replace ` series [ ' column_of_ints ']` with ` series ` only in the ` function ( series )` , you're passing a series to the function not a dataframe .
Don't use ` apply ` you can achieve the same result much faster using 3 ` .loc ` calls : #CODE
df [ ' column_of_ints '] is a Series not a DataFrame , there is no ` axis=1 ` for ` apply ` method for a Series , you can force this to a DataFrame using double square brackets : #CODE
yes , it will , you can either use 3 ` loc ` calls to handle each boolean condition to filter the df , or a ` where ` cal that has a nested ` where ` to handle the 3 conditions
1 ) Output will have column names repeated there , rather it should be ` pd.DataFrame ( map ( str.split , a ) [ 1 :] , columns =[ ' date ' , ' name '])` 2 ) Column names should rather taken from list than manually passing it .
How to insert a " Holidays " column into 2Darray with Pandas / Numpy
I would like to insert one more column ( HOLIDAY ) which has a bool value : 1 if the date is holiday , 0 if the date is not holiday .
I tried using pandas merge , but not sure of how to go about it without explicitly iterating using a for loop .
Just ` concat ` them as a list and pass param ` ignore_index=true ` , then assign the index values to the 3rd column , convert to str dtype and then append the txt ' .txt : #CODE
I need to aggregate this information some how , as I'm doing all of the above over a set of id's .
what you really need here reshaping with pivot #URL .
Using pivot_table rather than pivot .
I've managed to create the pivot table , however , I'm still struggling to show the labels on x-axis correctly .
plt.xticks ( np.arange ( len ( reporting_periods )) , reporting_periods , rotation=90 )`
Then , merge the average snowfall to df1 and then compute the difference : #CODE
Using map to pull directly from your series of averages #CODE
I have a distance matrix in hand where I would like to get its max , min , mean , median , etc . values ; expected describe() to do it for me , but looks like I was wrong .
@USER No it gives me the same warning even on using apply instead of map
( total no . of characters were calculated with ` sum ([ len ( i ) for i in open ( ' .. / test.txt ') .readlines() ])` )
Matplotlib multiple columns hist subplotting
One way to do this apply ` clip_upper() ` on 90 percentile value ` np.percentile ( x , 90 )` for each column #CODE
I had imagined @USER elegant solution would faster than ` apply ` .
Below benchmarks for ` len ( df ) ~ 130K ` #CODE
And for ` len ( df ) ~ 1M ` #CODE
I had thought the same thing but , having just checked timings on my machine , it appears ` apply ` can be surprising sometimes :-)
Not sure why , but you would notice that in the benchmarks posted , this method seems slower than apply method .
I am looking to calculate some aggregated statistics on a ` DataFrame ` where I can aggregate using any set of categorical columns , including the empty set - that is , even just no aggregation at all .
I emphasize the second because , at least among the portions of the API with which I am familiar , the choice to aggregate or not aggregate seems to lead to different sets of methods .
So , since apparently one cannot control the sorting algorithm using the pandas sort method , just use a lower cased version of that column for the sorting and drop it later on : #CODE
You can groupby the ' animal ' column and then call ` transform ` on the ' animal ' column can just call ` fillna ` and pass param ` method= ' ffill '` which will fill any ` NaN ` values : #CODE
Note that these days you can use ` expand=True ` instead of ` apply ( pd.Series )` : #CODE
You can't pass a Series as a param to a function unless it understands what a pandas Series or the array type is so you can instead call ` apply ` and pass the function as the param which will call that function for every value in the Series as shown above .
You could call ` apply ` and use ` datetime.strptime ` : #CODE
A function can be applied to a ` groupby ` with the ` apply ` function .
My question is : what should I do in a loop to merge ( concatenate ? ) the chunks back in a .csv file after some processing of chunk ( marking the row , dropping or modyfiing the column ) ?
Note that ` mode= ' a '` opens the file in append mode , so that the output of each
Should I use apply ? or map ? or perhaps apply_map ?
How can I create a pandas pivot table showing the percentage of one of three options ?
I want to create a pivot table for each season , country and league showing the H as percentage of the total matches .
Pandas-Pivot , stack , melt
You can use groupby and apply : #CODE
I want to transform this data frame to the following : #CODE
I'd stacked to append each file using pandas .
The main issue here is that append returns None : #CODE
and None has no append method , so this will raise an AttributeError : #CODE
I think what you want to do here , to make one frame from several , is concat : #CODE
For insertion , updating it is recommended to use the new ` loc ` , ` iloc ` and ` ix ` for indexing
Generally you should be using ` ix ` #CODE
Or ` loc ` #CODE
You could ` apply ` on dataframe and get ` argmax() ` of each row via ` axis=1 ` #CODE
Here's a benchmark to compare how slow ` apply ` method is to ` idxmax() ` for ` len ( df ) ~ 20K ` #CODE
In my naive implementation , I try to use hash map for each combination of col1 and col2 values but it makes the indexing problem really big .
why a hash map and not a vector of vectors ?
It's surely tab separated because i can do ` cut -f ` on unix and surely 401 columns from ` awk ' { print NF ; quit } ' bigfile.csv `
Pandas DataFrame apply function doubling size of DataFrame
For each non-binary column , I want to identify the values larger than its 99th percentile and create a boolean mask that I will later use to remove the rows with outliers .
I am trying to create this boolean mask using the ` apply ` method , where ` df ` is a DataFrame with numeric data of size a * b , as follows .
The second b columns seem to be the intended mask .
Why is the ` apply ` method doubling the size of the DataFrame ?
Unfortunately , the Pandas apply documentation does not offer helpful clues .
specify the end of the year , use quotations , and also use ix to select index values : #CODE
How to insert a large record into Orient DB using pyOrient ?
I am trying to insert roughly 5 to 10mb into the node's data property , however , the database seems to become non responsive when doing so .
Is there a way to insert properties with large properties ?
Then append the data .
How can I pivot a df with duplicate index values to transform my dataframe ?
gives this error ` DataError : No numeric types to aggregate ` but im not even sure that is the right path to go on .
There was an answer in [ this ] ( #URL ) post where someone used pivot_table() instead of pivot and it resolved the same error , might be worth trying ?
@USER I saw that post and tried earlier , but got this error - ` DataError : No numeric types to aggregate ` and decided not continue down that path , because I understand that error even less .
You can use groupby to aggregate by the common factors , take the max of time to get the most recent dates , and then unstack the msg to view confirmed_settings and sale side by side : #CODE
If there are , you can't use ` pivot ` , because you can't tell it how to treat the different values encountered during aggregation ( count ? max ? mean ? sum ? ) .
This will help you figure out which user-msg records have duplicate entries ( anything with over 1 ) , and after cleaning them out , you can use ` pivot ` on ` df ` to successfully pivot ` _time ` .
Ability to apply different stats to different columns ( for now just count , sum , mean , weighted mean )
Essentially insert a row into the existing dataframe df1 .
when combining pandas dataframe ( concat or append ) can I set the default value ?
Pandas merge two dataframes with different columns
If I concat two dataframes ( A B ) that have some of the same columns , but also have columns that are not present in both , in the resulting dataframe the entries for the columns that are not common to both A B have a value of NaN .
I would rather not simply replace NaN after the concat operation as there may be NaN values in the original dataframes that I want to preserve .
Please show what your 2 df's look like you may want to ` merge ` or ` combine_first `
A possible solution is ' conform ' the indices before concatenating both dataframes , and in that step it is possible to define a fill_value : #CODE
The best result I have achieved is with the hist method of dataframe : #CODE
A workaround might be to log10 transform the data before plotting , but the approaches I have tried , #CODE
` data.apply ( math.log10 )` did not work because ` apply ` tries to pass an entire column ( a Series ) of values to ` math.log10 ` .
And can I apply tight_layout to data.hist somehow ?
Avoiding double counting in pandas merge
After I finish the aggregated files I want to again merge the two files at the granular row level .
Is there a way to join / merge this with File 1 which only has one record per PersonID+Date+CustomerID without creating duplicate Visits on the first file ?
However , some clarity is needed : Why would you drop duplicates in File 1 to leave only 1 visit per CustomerID per date ?
The only way I could clean it was to drop the duplicates .
Yeah perhaps I'll drop the CustomerID for now ..
If , by any chance , you want to use the ` CustomerID ` to count the visits , you can use ` pivot_table ` to aggregate it really quickly .
Remaining step would be to merge it with the second dataframe to get our chats per date per person .
When you do the aggfunction Len , why do you do it that way ?
Coming from an Excel-reliant environment , I crunch numbers quickly using pivot tables , which ` pandas ` executes in much better ways .
` aggfunc ` is similar to Excel's ` View values as ` option , and feeding it ` len ` is similar to using ` Count ` in Excel's pivot table .
Rest variables I will drop from the dataframe
If you take all the columns , get rid of the ones in your < 0.5 set , and then drop the resulting ones , isn't that the same as just selecting the ones in the < 0.5 set ?
When two DataFrames each have a column with the same name ( like ' PCBD1 ') , and you merge them on the indices and not that column ( left_index=True , right_index=True instead of on= ' PCBD1 ') , then pandas will rename them to avoid having two columns with the same name ( PCBD1_x comes from My_Features and PCBD1_y comes from Drug ) .
concat two dataframe using python
We need to concat both the dataframe like #CODE
If the index is not common then this will stack the rows , you'd need to pass ` ignore_index=True `
Due to prior filtering , the biggest possible diff value should be between 10 and 20s .
My workaround would be to copy the index , cast it to int64 , cast t0 to int64 , substract t0 from all rows and then convert the diff column back to timedeltas , but that seems extremely inefficient and ugly .
how to transform pandas dataframe for insertion via executemany() statement ?
You could merge , concat them all and write to DB in one go perhaps
( with ` all_data ` as the dataframe ) map dataframe values to string and store each row as a tuple in a list of tuples #CODE
How can append the three columns onto each other then get the distinct values elegantly ?
I'm not sure I can give you a great explanation for that warning beyond what's in the documentation , but it appears what you did works fine and that warning doesn't always apply even when it appears .
Is there an easy and straightforward way to get a 100% area stack plot like this one
and then use ` pandas.melt() ` to pivot the data : #CODE
What I would like to do is to transpose one of the columns , so that instead of having multiple rows with same medicine and different diseases I have one row for each medicine with several columns for diseases .
i.e. I don't want to assign ' medicines ' as index column because I will merge it on some other key .
I'm thinking you want a pivot table .
And , finally we can pivot : #CODE
Now , you can ` concat ` -- ` md ` with ` dval ` #CODE
Just ` groupby ` by ` level=0 ` or ' Greek ' if you prefer and then you can call ` diff ` on values : #CODE
And I want to pivot table .
And not after the ` map ` .
Given the current DataFrame , however , you could transform it into the desired
Now we can replace the NaNs with 0 : #CODE
Like if I need to find which variables it gave as significant or if I am running a Regression model then the coefficient stand for which variables as I might have used Onehotencoder to transform categorical variables and then it would change the var order from original .
get_support ([ indices ]) Return a mask , or list , of the features / indices
There is currently no method there , but you can use the feature_importances and threshold them as described in the docstring of transform .
I want to drop the rows which have " _cntrl " in the name column .
I've looked into df.drop but could not figure out how to drop based on string based matches .
Now some of the great things about pandas is that it will try to align using existing column names and row labels , this can get in the way of trying to perform a fancier broadcasting like this :
a pass through transform function ... but this just gives you your DataFrame back ...
Call ` apply ` on ' B ' and pass a lambda which just accesses the single key in the dict : #CODE
I think you may have missing values ` NaN ` which is why you get that error , you can mask the df to ignore the rows with missing values : #CODE
You then have to decide what to do about the missing values , you can replace them by calling ` fillna ` : #CODE
Actually you can use the fact that the values are already boolean values to mask the series : #CODE
Similarly for numpy arrays you can use the boolean values to mask the array and get the index values using ` np.where ` : #CODE
@USER I did try using ` boolean indexing ` , but then since I need to find runlengths , I figured I would need to find those shifting indices and then perform ` diff ` , so ` np.where ` might be the preferred way here .
As a side note , instead of doing ` for i in range ( len ( jdata.keys() ): ` and then using ` jdata.keys() [ i ]` over and over , just do ` for key in jdata : ` .
Also ` for i in range ( len ( jdata.keys() )): ` creates unneeded list of integers .
Can you post raw input data , as for your first question I think this should work , if ` col_list ` is a list of your variables then ` df [ col_list ] .apply ( sm.tsa.stattools.adfuller , maxlag=None , autolag= ' BIC ' , regression= ' c ')` you can assign these to new columns and then transpose if you want the columns as index values .
OK , I'll knock something up , basically my approach would be to just walk the series / df that is returned from my previous code snippet and then pull the individual components out and append them row by row
That gives you a ` bool ` column .
You can use ` astype ` to convert to ` int ` ( because ` bool ` is an integral type , where ` True ` means ` 1 ` and ` False ` means ` 0 ` , which is exactly what you want ): #CODE
To replace the old string column with this new ` int ` column , just assign it : #CODE
One way but not more efficient for long dictionaries is using ` itertools.combinations ` to find the combinations between your dictionaries then loop over the combinations and then the sets and get the intersection between the set items : #CODE
You need to c reate a ` view ` object of your items that tread as ` set ` objects then you can calculate the intersection of items with ` ` operand .
You can make 2 calls to ` dropna ` , ` dropna ` accepts a ` thresh ` param which won't drop the entire axis if there are ` n ` non-Na values so the following drops rows then columns : #CODE
So you can do the statistics you want on all columns , then get the values for the column you want , substituting an empty ` Series ` for missing columns , then drop ` NaN ` columns ( I use ` Bad Value ` to demonstrate what happens to missing columns ): #CODE
Wow this works for all cases unless I use ` apply ` and a lambda function
Apply a weighted average function to a dataframe without grouping it , as if it was a single group
I want to apply a function that computes something similar to a weighted average absolute deviation of all the elements of my data frame .
If I don't use groupby , pandas would apply this function to every row of the dataframe , which is not my goal .
Okay , so in your post when you say " If I don't use groupby , pandas would apply this function to every row of the dataframe " , that's not necessarily true .
This is gotten from a simple pivot , if you are ok to not using the for-loop : #CODE
Can you first build a DataFrame and then transform / reshape it ?
After that , I use the ` unstack ` method : #CODE
I'd split the dfs into 2 list the ones that can be combined column-wise and the ones that can be combined row-wise , concat those separately and then concat the 2 concatenated dfs
The ` concat ` way requires you to group things in a specific order , but is more than twice as fast : #CODE
I'll go with the combine_first because due to some idiosyncrasies there is no easy and predictable way to join the data frames first in one direction , then the other .
Use the older ` openpyxl ` engine to apply formats one cell at a time .
But it means writing loops to apply formats cell-by-cell , remembering offsets , etc .
This will hopefully improve in forthcoming releases when we add support for named styles you'll still have to apply these individually as resolving all the possible styles for an individual cell ( built-in , row , column , individual ) is an expensive operation which will be much less complex than it currently is .
You do use pivot to get the ` identifiers ` in columns and then plot #CODE
I have also tried to strip the characters with the following : #CODE
A single-liner - you could extract numbers from via regex and ` apply ` on the ` duration ` column like split into multilines for readability #CODE
And , then apply on #CODE
How do I export multiple pivot tables from python using pandas to a single csv document ?
Say I have a function pivots() which aggregates pivot tables #CODE
I know how to export a single pivot table #CODE
You can use ` to_csv ( path , mode= ' a ')` to append files .
What's REALLY confusing me is , when I try to step through the function ( not using apply ) with just one row , I get the DataFrames that I expect i.e. , not the Series and then Timestamp .
User ` join ` and then ` reset_index ` : #CODE
Counter in Pandas groupby apply
Is there a way to have a counter variable in the function called through a pandas groupby apply ?
Note : this is an implementation detail , the number of times the function in an apply is called may depend on the return type / whether the apply takes the slow or fast path ...
Isn't the first call to apply the initialisation of the groups though , I thought I saw this as an explanation in a previous answer somewhere ...
I am using Python to insert some columns from a pandas dataframe to MySQL .
I'm getting the following error message when trying to insert the data in the table : #CODE
Great I should have thought of using apply .
These don't map nicely to pandas structures .
Mainly as were not implemented , or they don't map well to higher level structures .
I'm a Stata user and in Stata , I'd be using replace command conditional on regexm .
We then apply another function to this that converts the str numbers to ints , puts these in a list and returns the smallest value : #CODE
this is an approach that I hadn't thought about and one that I'm likely to employ down the road . for age , I wanted the series [ 62 , 55 , 67 ] at the end , and the problem I'm having now is that I can't target just row2 when I apply split ( ' ') .
return min ( list ( map ( int , x )))` to ` def highest ( x ):
return max ( list ( map ( int , x )))`
I want to apply df [ ' age '] =d f [ ' e0 '] [( df [ ' e0 '] .str .match ( pattern7 )= =1 )] .apply ( lambda x : str ( x ) .split ( ' ') [ 1 ]) to only rows for which df [ ' e0 '] .str .match ( pattern7 )= =1 ) so as to not overwrite what was already in the age column ...
If you make these a list you can apply loc ( which gets you the desired result ): #CODE
4 ) If you really want to prefilter the signal with a rolling mean , choose a window size different from 1 .
Mark Ransom gave a smarter answer , if you have to do the FFT you can just cut off the noise after the transformation .
But , even if I used ` rolling mean ( ..., 100 )` which gives me pretty smoothened signal [ 1 ] , the result remain 34.8 Hz !
So , I calculate mean frequency as : ` freqs = fft.fftfreq ( len ( Hn ) , 1 / frate ) ;
ind = np.arange ( 1 , len ( Hn ) / 2+1 ) ;
cut = 0.7 * psd.max() ;
ind1 = np.where ( psd > cut ) ;
I need to merge the two dataframes on the dates , but since they aren't exact , i probably need to convert them first .
Save it in a different column and ` merge ` on those columns .
@USER This is going to be much slower than using either dt.date or dt.normalize ( normalize being the fastest ) .
You can normalize a date column / DatetimeIndex index :
Note : At the moment normalize isn't exported to the dt accessor so we need to wrap with DatetimeIndex .
split string column by " _ " , drop the preceding text , recombine str by " _ " in pandas
2 ) drop the ' NHL ' string
You could use ` apply ` like this : #CODE
Surprisingly , applying ` str ` seem to be taking longer than ` apply ` : #CODE
However , if you need to split and join , you could use a ` string method ` like- #CODE
Alternatively , you could also use ` apply ` #CODE
I would probably use HDF5Store to save the intermediary results ... or you could just append it back to another sql table .
The thing that I'm wanting to do is take the actual score value and apply a color map to it .
@USER : The code you posted on pastie works for me when I replace the ` custom_distance ` function by something which doesn't have extra dependencies .
Summarizing Dataframes with ambiguous columns with apply function
The code works for almost all cases except for ` apply ` functions that count specific cases inside a column : #CODE
Is there a way to incorporate the apply function into the dictionary ` sumdict ` ?
You could join array of elements from ` df [ " city "]` like #CODE
If I drop ` np.nan ` into your example : #CODE
There's a trick here to use pd.Series ( 1 , index= ... ) and let pandas align : #CODE
You can reset the index and then simply join : #CODE
If you want to include the NaN row ( which isn't in your answer ) , then outer join : #CODE
As an alternative , you can create ` gene ` in pure python ( rather than using apply ): #CODE
@USER Note : in your edit , this creates a different Series ( the index is now 0123 whereas in the original it's 0113 ) , so join will give a different result . ??
In that case , you can populate your ` dataframe ` with ` loc ` - #CODE
You could set index on predefined order using ` reindex ` like #CODE
Is there a way to conditionally drop duplicates ( using drop_duplicates specifically ) in a pandas dataframe w / about 10 columns and 400,000 rows ?
That is , I want to keep all rows that have 2 columns meet a condition : if the combination of date ( column ) and store ( column ) # are unique , keep row , other wise , drop .
The most efficient way to calculate the intersection of Product_ID's would be using numpy's ` in1d ` .
That gives you a mask .
Then , you simply slice your DF2 using the mask to get the new dataframe you want .
I guess mask should be ` mask = np.in1d ( DF2.Product_ID , DF1.Product_ID )`
If x in y does not equal 0 then replace with 1
How to column stack arrays ignoring nan in Python ?
pandas - apply UTM function to dataframe columns
I would like to apply this function to a pandas dataframe .
You could use ` apply ` method over the columns like
You can do this by ` apply ` ing a ` rolling_sum ` after we ` groupby ` the Type .
AttributeError : Cannot access attribute ' index ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method
On the topic of using ` DataFrame.resample() ` or ` Series.resample() ` , if I want to resample for Business days , I would use :
How can I explicitly choose to resample based on a particular country's holiday ?
1 ) Some files contain call drivers or other categorical information which repeats constantly ( for example , Issue Driver 1 with like 20 possibilities and Issue Driver 2 with 100 possibilities ) - these files are about 100+ million records per year so they become pretty large if I consolidate them .
Is it better to create a dictionary and map each driver out to an integer ?
Merging will put NaNs in common columns that you merge on , if those values are not present in both indexes .
What you can do instead is reindex the dataframes to a common index .
In your case , since the maximum index is 5 in the target dataframe , lets use this list to reindex both input dataframes : #CODE
Now , we will reindex according both inputs to this index : #CODE
And , now we merge it to get something close to the target dataframe : #CODE
instead of fillna , I used the method parameter of the reindex method
reindex ( ind , method= ' ffill ')
For example , using ` zip ` or ` merge ` ?
Here's on approach to do it using one ` apply `
And , ` apply ` and store the result to ` df [[ ' hour ' , ' weekday ' , ' weeknum ']]` #CODE
Depending on the task that is performed by ` lambdafun ` , you may get some speedup by storing the result of ` apply ` in a new ` DataFrame ` and then joining with the original : #CODE
Even if you do not see a speed improvement , I would recommend using the ` join ` .
One way is to groupby and apply function to take list , and then convert to dict .
Pandas dataframe apply function
3 ) How to use apply function on the above dataframe ` temp ` ?
It throws an error `' Series ' object has no attribute ' dt '`
` df [ ' Ship Date ']` is a series for which dt attribute is not present .
Then ` apply ` lambda function .
What I want to do is to replace the ` ACTION_DATE ` value to ` 03 / 31 / 2006 ` , if the ` GROUP_ID `' s value is ` B ` .
Let's say we want to resample this to monthly data .
The fact that you need to do the ` reset_index() .set_index ( ' dt ')` and then ` groupby ( ' symbol ')` instead of ` groupby ( level= ' symbol ')` seems to defeat the purpose of a multi-index !
It's not the most elegant , but life is short , so I'd apply ` list ` to get the values and then ` pd.Series ` to expand them into columns : #CODE
Is there a way I can apply this to all values in a column ?
-Take the first element as the ticker and convert the rest into numbers using translate to undo the string formatting
-Make a DataFrame per row and then concat all at the end , then transpose
Then I want to save that factorization and apply it to other ` DataFrame ` ( look input doesn't have c values in column A ):
As wroted in the question - I know about ` get_dummies ` - but this doesn't resolve my problem to apply the same mapping to the other series object .
You don't need to use ` map ` on columns - ` get_dummies ` have an optional parameter ` prefix `
For instance , if you intend to replace missing values by the most frequent value or the median .
This knowledge ( median , most frequent value ) must be obtained without having seen the testing set .
Would it be useful to transform the data in a ` sqllite ` format ?
In particular , if you actually do need to do array-wide work across all 350000 rows , and can't translate that work into SQL queries , you're not going to get much benefit out of sqlite .
( Although this doesn't apply to the question , it may help someone searching later : If you're using Python 2.x , make sure to explicitly use pickle format 2 ; IIRC , NumPy is very bad at the default pickle format 0 . In Python 3.0 + , this isn't relevant , because the default format is at least 3 . )
My goal is to replace certain values in each column from current list and then to save it : #CODE
I just want to replace certain values to NaN ...
It's possible this could be improved by keeping the file open , rather than opening the file each time in append mode : #CODE
pandas left join and update existing column
I am new to pandas and can't seem to get this to work with merge function : #CODE
With a left join on column a , I would like to update common columns BY THE JOINED KEYS .
How should I do this with Pandas merge function ?
Here's a way to do it with ` join ` : #CODE
Note : ` update ` only does a left join ( not merges ) , so as well as set_index you also need to include the additional columns not present in ` left_a ` .
I am trying to normalize the missing values in matrix .
Last line should replace the values in dataset1 by mean values from ` ds2_mean [ 1 ]` .
And after that can I replace NaN with the average value of it's neighbours in dataset1 ?
It should replace all values of x in ds1 by mapped value .
By default fillna will use the index so how do you want the mapping from ` ds2 ` to map to the missing values in ` ds1 ` ?
Are you wanting to map using the values in ` ds2 [ 0 ]` as the index lookup ?
yes , I want to use the index from ds1 find value in ds2 [ 0 ] and replace it with ds2 [ 1 ]" sorry for inconvenience
pandas - set new index level for multiindex series
I have a pandas series with MultiIndex and would like to replace one of the levels with the values in a seperate list .
The ` inplace=True ` does not work here , because apparently one cannot simply replace a Series with a DataFrame , which results by calling ` reset_index ` .
Add the new columns by calling ` shift ` and pass ` -1 ` as the interval : #CODE
Use ` shift ( -1 )` on the dataframe #CODE
As you see I used the ` rolling_window ` function which I believe applies a rolling window analysis , and the data / function applied is the " pred " which , as you can see , is a OLS prediction from my previous HAC-OLS .
Hi I'm trying to interpolate a Dataframe where I have a datetimeIndex index .
First tried to set the index and then interpolate .
Then use ` interpolate ( method= ' linear ')` on the series to get values .
The df.reindex call returns a column of NaNs and subsequently the interpolate fails .
Just as an add on to @USER ' s answer , you could also use ` resample ` which is slightly more convenient than ` reindex ` here : #CODE
However if you're actually interested in finding the difference between time periods separated by 3 minutes , not the 2m56s in your data , you're going to need to resample your timeseries using the resample method as documented here :
You might also be be able to use the map directly on the index without resetting , but given that it's a multi-index I generally find it much easier to work ( syntax-wise ) with a specific column rather than a specified level of a multi-index .
I'm trying to learn all the various ways I can use the aggregate functions in Pandas but when looking at the docs I can't really tell much .
in all cases above , the function can also be replaced by a string for the most common ones ( eg `' mean '` , `' median '` , `' std '` `' first '` , ... )
Now I want to replace the blank NaN field with a string " No Data " .
You could pass the values you want to replace in a dictionary to the ` replace ` function : #CODE
Apply function with pandas dataframe - POS tagger computation time
I'm very confused on the apply function for pandas .
I'm just not sure the way of setting up my apply statement or my function .
I may possibly rewrite it to output multiple numbers for different parts of speech , but I can't wrap my head around ` apply ` .
For instance , I can run ` noun_count [ row ]` and get the correct value for any index but I can't figure out how to make it work with apply how I have it set up .
Basically I don't know how to pass the row value to the function within the apply statement .
Rather than expanding this question ( somewhat discouraged here ) , I think you might get more help reposting the additional stuff as a new ( and briefer ) question along the lines of ' how can I make this faster ?
or , using the ` loc ` label selector , #CODE
You can do this with unstack :
How do I apply a function designed for one number to an entire dataframe in pandas ?
I'd like to backfill each missing row with the previous index value respective to it's id .
This involves breaking the DataFrame into a list of subframes and use ` concat ` to stick back together .
Now , just append all of these , but drop the ones we don't need ( if we already have an existing row with ' index ' = ' date2 ' , as for id=2 here ): #CODE
I am only interested in the slope of the fit so at the end , I want a new dataframe with the entries above replaced by the different rolling slopes .
don't you want : ` model = pd.ols ( y =d f [ ' A '] , x =d f.index , window_type= ' rolling ' , window=3 )`
I havent tried it out , but I don't think you need to specify the ` window_type= ' rolling '` , if you specify the window to something , window will automatically be set to rolling .
It won't help your datetime problem , but hopefully it helps with the rolling linear fit part .
Why does Pandas DataFrame Resample change column Order
For time series , there is the massive benefit of handling time series data using a datetime index , which allows you to resample smoothly to different intervals , fill in values and plot your series incredibly easily .
Any function in numpy / pandas / python to search and replace
[[ np.mean ( neighbors ( i , j , a )) for j in range ( len ( a [ 0 ]))] for i in range ( len ( a ))]`
Note that ` CIF / align / aft_port_end / extend_pressure ` is not meant as a path to a group / node / leaf .
`" CIF / align / aft_port_end / extend_pressure "` is not a path to a group node / leaf .
Just bite the bullet and replace ' / ' with ' | ' before writing your variables to HDF5 .
I am trying to fill in the percentile numbers to have all values from 10-90 for each section ( 10 , 11 , 12 , etc . ) and interpolate the values using the pandas interpolate function .
Mostly the work is done by ` reindex ` but I had to loop with a groupby rather than apply directly due to the index not being unique .
You can use ` data.index.get_level_values() ` to filter multiIndex #CODE
Try to replace
My question is , how can I restrict the factors / classes in ` Flag ` column to say , only include ` Category ` and ` Type ` without having to manually drop columns ?
I have to format the dates on each and then turn them into text , create an object column which adds to the filesize , and ( due to the raw data issues themselves ) drop duplicates so I don't accidentally inflate numbers .
Pandas will correctly merge rows based on the triplet of values from the `' Date ' , ' CustomerID ' , ' AgentID '` columns .
You can do this without using loops by using ` loc ` , like shown below : #CODE
pandas ` append ` does not happen in-place .
Instead , save each row that you want to add into a list of lists , make a dataframe of it and append it to the target dataframe in one-go .
What should I use if I want to update ` some_series ` in place from ` other_series ` , but also have the NA values apply ?
You have to ensure that the Series you want to update includes that of the other ( e.g. with reindex ) .
To update just the index of s use the intersection : #CODE
@USER updated ( use intersection rather than union )
the following would work , this uses ` loc ` and the other series index to mask the elements we want to overwrite : #CODE
The ` loc ` is acutally unneccessary in this case
What do you mean by " The loc is acutally unneccessary in this case " ?
I would like to know how to make a new row based on the column names row in a python dataframe , and append it to the same dataframe .
I'm assuming this is what you want as you've not responded , we can ` append ` a new row by creating a dict from zipping the df columns and a list comprehension of the middle character ( assuming that column name lengths are 3 ): #CODE
ix --- lets you read the entire row -- you just say which ever row you want .
Using append works , but for my purposes is WAY TOO SLOW #CODE
Create a list of lists and making a dataframe of that will be faster than append .
Essentially , what I am trying to do is join Table_A to Table_B using a key to do a lookup in Table_B to pull column records for names present in Table_A .
I join on real_name and I compress / collapse the result by groupby a.real_name since I don't care if there are multiple records in Table_B for the same real_name .
In the mySQL query result I can then separate the NULL records to be sent for manual data-entry and automatically insert the remaining records into Table_B .
You can drop duplicates based on columns ` raw_name_left ` and also remove the ` raw_name_right ` column using ` drop ` #CODE
The instrument measures at the sub-second level , and I am using pandas to resample the data into 5 minute intervals , instead of messing around with modulo division for time values .
After that you can join : #CODE
The reason I'm not sure if this is the best way is because ` apply ` tends to be slow .
If you're doing this for many columns , I would suggest using a MultiIndex rather than a dot seperated string : #CODE
Find a null value and drop from a dataframe in Pandas
What I want to do is that I want to find null in the " data " column and drop the row from the dataframe .
You can keep only the notnull values using a boolean mask : #CODE
This is the more sensible / readable mask as the ` ~ ` operator is not so readable +1
What I would like to do is replace the column content with just the phone number part of the string above .
To get to your desired table , you basically just have to unstack ` gpm ` to move Bracket and Win into the columns : #CODE
Finally , just join what you need : #CODE
Combine several bool pandas.Series with uncommon indices with an logical and
If I understand what you want , I'd concat the 2 series column-wise and then call a function row-wise that drops the ` NaN ` values and returns the logical ` and ` of the 2 columns or the lone column value : #CODE
Firstly construct a df from the np array and then ` merge ` this .
We merge the left side on ' id ' and the right side on column ' 0 ' and perform an outer merge , we have to drop the ' 0 ' column as it's superfluous to what we want : #CODE
why cant you just pivot the first data frame like this #CODE
Now if I drop the column ' Libell _Article ' from df ( which is a string column ) , I don't get the error message anymore .
I came across a weird situation using boolean numpy arrays as a mask for selecting parts of a pandas dataframe .
It seems that problems could arise with a mask being a numpy array rather than a series or dataframe since you lack index and column names / labels .
If you got a result like this using an array mask on an array or a dataframe mask on a dataframe that would be much more of a concern .
For fast solution replace this #CODE
Sorry I don't understand how you arrive at the diff column as an output , are you just performing a cumsum on the differences and then indicating the direction of change ?
best explained by example : for 1 , the next value is 2 then 3 so 3-1 = 2 hence +1 value in diff column , for 4 on the other the positive change is only 5-4 =1 so the the next change is negative since 2-4 = 2 so -1 is the value
so for 4 the value change to 5 but since 5-4 =1 is not > = 2 it does not lead to a positive value . iterating further we arrive at 4 ( no change ) 3 ( change of -1 ) and 2 ( change of -2 <= -2 ) therefore diff = -1 for a negative change .
The values in the fourth and sixth rows are both 4 , so why isn't the " diff " 0 for the fourth row ?
Call ` resample ` and pass the rule as ' 10Min ' : #CODE
Interpolate pandas DataFrame to different TimeSeries
I've looked into the ' resample ' and ' interpolate ' methods , but I don't think they do this .
I want to read the ` uk.txt ` file from a uk nga geonames download using python blaze and then odo to insert it into a Postgresql db .
I get the error ` ValueError : cannot safely convert passed user dtype of i8 for object dtyped data in column 0 ` that I think I understand as meaning " a datatype cant be converted to insert into the db "
A can create a ` Series ` object and resample it to group it by months : #CODE
I have tried using ` resample ( " M " , how= ' mean ')` on ` multiMmean ` and list comprehensions but I cannot get it to work .
You could imagine a Helper column of df [ ' Helper '] = range ( 0 , len ( df )) and use the difference in the helper column for this difference .
I think I'd just drop to cython .
Pandas notnull is not working on column in data frame
Usually I would use notnull but on this column it is not working .
You could use ` transform ` and place the result in ` df [ ' Avg ']` #CODE
Use pandas ' builtin str methods for efficient string operations , and add them on directly to avoid a slow join : #CODE
maybe show a timeit for apply vs using the str ops :)
Call ` map ` and pass the dict : #CODE
I have a Pandas multiindex dataframe and I need to assign values to one of the columns from a series .
Series ( and dictionaries ) can be used just like functions with map and apply : #CODE
To me this should just work so there must be a way of doing this without resorting to ` map `
I figure map is as good of a way to do this as any .
Could also do via merge , but I suspect that's a little slower ( but maybe clearer to read ) .
I can easily do this iteratively with loops , but I've read that you're supposed to slice / merge / join data frames holistically , so I'm trying to see if I can find a better way of doing this .
A join will give me all the stuff that matches , but that's not exactly what I'm looking for , since I need a resulting dataframe for each key ( i.e. for every row ) in A .
You then want to apply some function to each group of rows in ` b ` where the ` b [ " key "]` is one of the values in ` keys ` .
Under the covers , these are really similar uses of ` apply ` .
The unique grouped-by values -- those of ` b [ " key "]` -- always constitute the index , but if the applied function returns a scalar , ` summary ` is a ` Series ` ; if the applied function returns a ` Series ` , then ` summary ` constituted of the return ` Series ` as rows ; if the applied function returns a ` DataFrame ` , then the result is a multiindex ` DataFrame ` .
` loop_iter = len ( A ) / max ( A [ ' SEQ_NUM '])
How do you check a condition of several pandas DataFrame.Series element-wise and apply the result to a new column ?
I have managed to do this , but it's slower than I would like ( takes 2 mins for a single 60mb file ; mostly in the apply part as seen below ) and I'm thinking that there must be a better way of doing it
Or should I calculated / spread the values when I do the merge ?
pandas cut with infinite upper / lower bounds
So ` cut ( 250 , bins =[ 10,50,100,200 ])` will produce a ` NaN ` , as will ` cut ( 5 , bins =[ 10,50,100,200 ])` .
I realize I could do ` cut ( weight , bins =[ float ( " inf ") , 10 , 50 , 100 , 200 , float ( " inf ")])` or the equivalent , but the report style I am following doesn't allow things like ` ( 200 , inf ]` .
We can construct a boolean mask that test if the values in the df are greater than ( ` gt ` ) 0 and less than ( ` lt ` ) 1 and then call ` np.all ` and pass ` axis=0 ` to generate a boolean mask to filter the columns and then multiply all values in that column by 100 : #CODE
I'd convert the dates to datetime objects using ` to_datetime ` , this then allows you to use the ` dt ` accessor to access the ` year ` attribute and we can then call ` isin ` and pass a list of years of interest to filter the df : #CODE
AttributeError : ' Series ' object has no attribute ' dt '
How should I transform from datetime to string
There is no ` str ` accessor for datetimes and you can't do ` dates.astype ( str )` either , you can call ` apply ` and use ` datetime.strftime ` : #CODE
Drop them first is one option so ` dates.dropna() .apply ( lambda x : x.strftime ( ' %Y-%m-%d '))`
As of version 17.0 , you can format with the ` dt ` accessor : #CODE
However , the dates mysteriously transform themselves to an ugly and unreadable format when plotting the same data as a bar plot .
Here are two ways you could build the selection mask without looping over the index values :
and then use ` scipy.ndimage.binary_dilation ` to expand the mask to a 7-day window : #CODE
Pandas group by aggregate using division
I'm wondering how to aggregate data within a grouped pandas dataframe by a function where I take into account the value stored in some column of the dataframe .
I cannot paste my current output because stack overflow doesn't like the format but what is currently happening is the multiplication is not occurring and the numbers are iteratively being spliced i.e. first element = 240.8856716 , second = 240.8856716240.8856716 and this continues .
To apply the same condition to to dozens of columns I could use ` isin ` , but it seems not to work if I need to substitute `' first '` with a regex , as in ` regex = ' ( ? = . *first ) ( ? = . *second )'` .
This will be different than working the columns but would make it easier for your to apply if-else conditions to ( I hope ): #CODE
The whole point of ` applymap ` is that you can apply a function on every cell of the data frame .
All right , after digging into ` pandas.core.strings ` here's the answer using ` applymap ` : ` df.applymap ( lambda v : bool ( re.search ( regex , v ))) .any ( axis=1 )`
Pandas DataFrame Name after pivot
I could rename column b before doing the pivot or unstack the pivot , rename the column and stack again .
But how do I apply this function on each element of a pandas data frame ?
Pass the ` hash ` function to ` apply ` on the ` str ` column : #CODE
Merge pandas dataframe where columns don't match
I want to merge the data frames in a way so that data from ` Dataframe1 ` corresponding to 30 Apr 2014 appears against all dates in May 2014 in ` Dataframe2 ` .
Then I'd set the index of df1 to this ' month ' column and call ` map ` on df2 against the month of the ' date ' column , this will perform a lookup and assign the ' val ' value : #CODE
Then you could make a new DataFrame , consisting of the rows where ` mask ` is True : #CODE
I then try using apply to run it on a dataframe to create a new column .
For a start there is a built in ` str.split() ` which is vectorised so you could eliminate that from your code so ` df [ ' word_split '] = df [ ' string '] .str .split() ` and then call apply on this column and change your line in your func to this ` listoflists = st.tag ( x )`
Repair Result to Aaron_schedule 03540.xml Errors were detected in file ' Macintosh H D: Users : AaronsMac :D ocuments : Aaron_schedule.xlsx ' Removed Records : Merge cells from / xl / worksheets / sheet1.xml `
They're still vectorized / aggregate operations .
You could iterate through them and apply the ` to_datetime ` function OR
This almost worked , but it looks like the time shift was 11 hours forward rather than backward :
you can simply do a vectorized : `` s-pd.Timedelta ( ' 11 hours ')`` , no need for apply
I know that I can mask data I'm not interested in , for example to analyize for state 1 : #CODE
I hope the comments give enough explanation - the key point is you can use a custom rolling window function and then cumsum to group the rows into " clumps " of the same state .
lookback / shift in pandas dataframes calling functions
can I somehow use pandas shift function in user defined functions ?
It's generally expensive to do it this way , as you're losing the vector speed advantage when you ` apply ` a user defined function .
You can hold the Series as a variable but if you want to assign to those row's then you can use ` loc ` which is what Bob answered
Pandas how to merge two dataframe ( seems easy , ok ;)
I am trying to make a left join over two dataframe in python / pandas .
What is the join column .
There is no bug on this code : according to the documentation ( actually O'reilly ' python for data analysis ' , p179 / 180 ) a many-to-many left join make a cross-product over the lines .
I have long list of date in dataframe that need to be converted into datetime , the date is in the form " %d%m%Y " , and I apply ` datetime.strptime ( x , ' %d%m%Y ')` , which works fine until meet the date " 3122012 " , which should be datetime.datetime ( 2012 , 12 , 3 , 0 , 0 ) , but instead it throw the error message : #CODE
( FYI if i insert a print print ( vals ) in the middle of that loop , it prints #CODE
Now given the nature of how ' log_lower ' and ' log_upper ' is defined , there's overlap at the end of each row's ' log_upper ' and the subsequent row's ' log_lower ' , with those values being the same - does the append function in the loop or numpy.concatenate later take that into account when making the array and cancel one of the overlaps ?
Create a mask : #CODE
Filter the data frame using this mask : #CODE
How to create a new derived column in a pandas dataframe new column using only notnull vaues
Often I want to map a function to a column containing nulls , and I find my self having to write some sort of logic checking for nulls .
Is there a preferred way to apply a function to only the nonnull rows of a column ?
Use the isnull or notnull methods to filter out non-null elements of a column #CODE
Thank you for this , I had initially tried something like this but didn't realise that you only have to apply the filter on the RHS of the assignment and not the LHS .
Use ` loc ` and ` notnull ` to mask the df and the vectorised ` str ` methods to ` split ` the strings : #CODE
Set index on ' TOTACTVAL ' and unstack to level -1 and reset index #CODE
The ` reset_index ` is needed to transform the resulting Series ( with ` Name ` index ) back to a DataFrame with two columns ( ` Name ` , ` TOTACTVAL `) #CODE
Pandas : How to use apply to create new dataframe
and appy that function using apply to the created DataFrame using #CODE
Did I get anything wrong regarding the use of the apply function ?
Because you're passing your ` data ` df as a reference and assigning directly to it each time by calling ` apply ` in your func then it overwrites with the last operation : #CODE
I was encountering the same issue when trying to write a pivot table to Excel .
What is the right way to aggregate ?
2 ) Create a map ( is there a way to do this from reading another file ? Like I would create a .csv and load it as another dataframe and then match it ? Or do I literally have to type it out initially ? )
3 ) Basically do a join ( VLOOKUP ) and then del the old column with the long object names #CODE
You want to replace the categorical text with integers hoping it takes up less space ?
I also connect these .csvs to Tableau so I was thinking I could do a join in Tableau too .
Note the drop in memory usage down to ` 976.6 KB `
Now use ` map ` to replace ` int ` coded data with categories descriptions and convert them to ` Categorical ` : #CODE
Yes , I want to group by marker and date , and it seems that [ ' marker ' , df.index.date ] doesn't cut it , and without having to do a group by inside a group by just because of the difference in index sizes .
You can subset based on any criterion you can apply to each item in the column .
You can then drop those values and recast the remaining values as integers like follows : #CODE
I'm pretty sure using ` map ` is slower than using ` df.year_week.str.len() < 6 ` .
You can use ` df . column .map ` to apply a function to each element in a column : #CODE
but this does not always work ( code from diff columns ) . for example , none of the following will remove \r or ' , ' #CODE
python pandas use map with regular expressions
and I want to map it to a pandas df : #CODE
Is there a way to use regex to match map on " Commerciante " column ?
Why don't you use ` apply ` and on a modified dictionary lookup : #CODE
And , apply it like this - #CODE
First clean your data , then map it .
And use it with apply #CODE
As a final small point : ` if ( i len ( df.a ) -1 ): ` is unnecessary - it's guaranteed by the surrounding ` for ` loop .
This is basically the right logic , but the resampling doesn't have a fixed start , so the date ranges for the 3-day periods don't align between categories ( and I get NaN / 0 values ) .
You can maybe first reindex , so you are sure every category starts at the same date ?
I want to resample flatten dataframe to multi-indexed columns .
I try somethings with groupby or stack but I don't find a good way ...
With ` unstack ` ( to use this you first have to set the multi-index ): #CODE
Therefore , in general , always better to use ` loc ` to be sure
If I understand your question correctly you should be able to just call ` merge ` on ` dfBig ` and pass ` dfSmall ` which will look for matches in the aligned columns and only return those rows .
A few minutes after posting this I thought " this sounds like an SQL merge " and google pandas merge .
Will lead to a stack trace ending with the error : #CODE
You're explicitly specifying the dtype so it's expecting all values for that column to conform to that type , if you know the values to treat as ` NaN ` you can pass these to ` na_values ` however it looks like it's better to just let ` read_csv ` guess but this will mean that the dtype will be ` float64 ` as ` NaN ` cannot be represented in ` int64 `
You can do each of these operations ( append and retrieve ) in a single line , even when you have multiple ` DataFrame ` s .
If I understand your question correctly , your problem here is that your date strings are of an inconsitent format so when you construct a datetimeindex or use ` pd.to_datetime ` then any missing time portion ( or date portion for that matter ) a default value will be supplied , You could replace the ` 00:00 : 00 ` with ` NaT ` afterwards
I cannot replace these with ' NaT's after , as I also have real time values of 00:00 : 00 .
I would create a mask , something like this : #CODE
You'd be better off using ` df.iloc ` as this is integer based rather than label based as your index values may not be 0 based int64 , so something like ` for i in range ( len ( df )): #doSomething with df.iloc [ i ]`
Should I completely drop any interaction with duplicates ?
In other cases , grouping is preferable , but since each interaction in your example appears to be truly independent , grouping seems ill-advised , the only criteria you could naturally group on would be rating , and seems like a bad decision to aggregate that separately from any analytics you're performing .
I think that is fair enough and worth testing ( total samples if I just drop duplicates on a daily basis versus a ' acceptable time frame ' window .
pandas : how to merge multiple indexes together ?
I have two levels of index , and I want to merge them into one level of index .
I looked at methods such as reset_index and reindex , but they don't seem to be what I need .
will give you tuples in a single level if that's what you mean by ' merge ' .
But I don't get a 2D heat map , when I do : #CODE
Not sure whether this is all that much more efficient , but you could ` pivot ` and then add the frame to its transpose , something like : #CODE
Here is the documentation on ` add ` and ` pivot ` .
The third line ` df.add ( df.T , fill_value=0 ) .fillna ( 0 )` just adds the transpose to convert the triangular matrix to a symmetric matrix .
datetime dtype values have a ` dt ` accessor where you can access the seconds attribute .
I tried using the Unix Unexpand program to replace the space between the second and third column but got two tabs by this method .
The tab character is ` \t ` so you can put that where ever you need , or replace it with a space `' '` .
and functions like ` pd.merge() ` where there is an option to merge " on " something did not do what I wanted .
If I understand what you want then you can call ` map ` and pass the series : #CODE
With regards to optimising your code you can replace this line : #CODE
Use lambda and apply .
` business [ ' number of categories '] = business [ ' Categories '] .apply ( lambda x : len ( x.split ( ' , ')))`
num_categories = len ( split_up )
it'd be better to use the vectorised str split method : ` business [ ' Category '] .str .split ( ' , ') .apply ( len )`
---> 31 business [ ' # Categories '] = business [ ' Category '] .apply ( lambda x : len ( x.split ( ' , ')))
Assuming that Category is actually a list , you can use ` apply ` ( per @USER ' s suggestion ): #CODE
Even accounting for the ` len ` function call : #CODE
` business.Category.apply ( len )` would work also
You should use the vectorised str methods : ` df.Category.str.strip() .str .split ( ' , ') .apply ( len )`
I am trying to aggregate some statistics from a groupby object on chunks of data .
Python pandas pivot multiindex
I am importing the data frame and I am trying to do a pivot like this : #CODE
` pivot ` looks like this : #CODE
My reason for doing all of this is that I want to generate a heat map with matplotlib and hence need the data to be in matrix form .
Pandas shift moving discarded elements to the beginning
I can't find a simple way to shift a pandas DataFrame , but instead of discarding the values at the end , moving them to the beginning .
In my case the ' bid ' column has different values for each stock , so I will not know what value to replace it with .
` .unstack ` pivots a level of a MultiIndex .
" Continuous values can be discretized using the cut ( bins based on values ) and qcut ( bins based on sample quantiles ) functions "
When would you use qcut versus cut ?
Conversely , for ` cut ` you will see something more uneven : #CODE
That's because ` cut ` will choose the bins to be evenly spaced according to the values themselves and not the frequency of those values .
pandas - map nested dictionary values to dataframe column
and I want to map on a new column the following nested dictionary : #CODE
I'd flatten the dict to create a new dict and then you can call ` map ` as before : #CODE
Use ` reindex ` : #CODE
Now we can unstack the last index level : #CODE
For the second df , we can call ` pivot ` to rotate the df to create the columns from the ' bid ' values , we need to reset the index and then we can merge the 2 df's together to get your desired result , you can replace the NaN values with blank strings if needed : #CODE
What you want to do is pivot the table .
` .unstack ` pivots a level of a MultiIndex .
It seems both unstack and pivot have an issue with ' duplicate entries ' as I've discovered here #URL After that I came across the ` pivot_table ` function which seems to work .
Yeah , which of pivot , pivot table and unstack you want to use will depend on how your data is structured and what you want to achieve .
Also , I assume it would be better to read the roster dump and create separate tables to normalize the data as best as possible .
Just pass the boolean comparison as a mask : #CODE
There is also the ` gt ` which means greater than : #CODE
Your comment doesn't make much sense the mask ` df [ ' input '] > df [ ' output ']` is no different to my last edit , we call operator ` > ` on the series ` df [ ' input ']` passing ` df [ ' output ']` as the param , this will return a boolean series which we can use to filter the df
You can groupy the ' ITEM ' and ' CATEGORY ' columns and then call ` apply ` on the df groupby object and pass the function ` mode ` .
What is the best way we can use the append prefix 0 logic in case user input is < 10 .
Without having access to your data , for pandas at least there is the top level ` isnull ` and also this is available for the ` df ` or ` series ` , you can also call ` dropna ` : #CODE
If I simply replace ` np.isnan ( uniqueID )` by ` pd.notnull ( uniqueID )` in the code of the question it works .
` dropna ` is for dropping entire rows or columns if nan's exist or if a threshold is not meant , ` pd.notnull ` is a top-level function that just filters the entire df of any nan values when used as a mask
should work , depending on whether you'd like to drop the ` id ` level .
pandas groupby() receiving error message " level > 0 only valid with MultiIndex "
I'm trying to perform a simple groupby operation but it's receiving an error message " level > 0 only valid with MultiIndex " and being a python newbie , I don't understand what that means or know where the error is .
How to reference groupby index when using apply , transform , agg - Python Pandas ?
Now I want to groupby date in df1 , and take a sum of value A in each group and then normalize it by the value of B in df2 in the corresponding date .
The question is that neither aggregate , apply , nor transform can reference to the index .
Liam , I've imported the datetime module as dt , but I'm receiving this new error .
AttributeError : ' Series ' object has no attribute ' dt '
@USER the ` dt ` accessor was added in ` 0.15.0 ` can you upgrade ?
The below code works , but preferably I'd like a more elegant solution ( such as passing an argument to hist ) #CODE
IIUC , all you need is ` pivot ` .
and we know the InputID and TargetID are unique , we can simply ` pivot ` : #CODE
FWIW I did the same thing as above with a data set of your size ( with 3330 ids ) which has substantially more rows and the pivot operation still finished inside of 9 seconds .
Thumbs up to DSM's pivot solution
How can I join columns in Pandas ?
Now I d like to join the columns YEAR+MONTH+DAY+HOUR+MIN to make a new one , for example #CODE
After some tinkering around , I wrote a function that can be used with the ` apply ` method on a ` groupby ` .
You don't need to call ` tolist ` , just replace ` straight ` with ` trans ` -- no need to cast as a list .
I've been trying to transform the date contents of a column into a new column containing the month and year which can the be grouped by a seaborn box plot function .
It seems like shift is close : I could get the rainfall 60 days ago , 59 days ago , etc .
i think rolling windows is what you need .
If df is your Pandas DataFrame where each row contains values for temp , wind , rainfall , etc ., then you can get a rolling 60 day total as follows : #CODE
How to use join to fill missing values of a column - Python Pandas ?
I want to do kind of a left outer join to fill the missing values in df1 , and generate
How to export DataFrame to_json in append mode - Python Pandas ?
I want to save " df " with to_json to append it to file output.json : #CODE
* There is append mode= ' a ' for to_csv , but not for to_json really .
The existing file output.json can be huge ( say Tetabytes ) , is it possible to append the new dataframe result without loading the file ?
No , you can't append to a json file without re-writing the whole file using ` pandas ` or the ` json ` module .
Also , I would like to shift all timestamp by a fix amount of time , such as 1ms ` timedelta ( 0,0,100 0 )` , how can I implement it using method 2 ?
If I want to shift all timestamp by a centain time such 1ms ( timedelta ( 0,0,100 0 )) , how can I do it ?
You can use ` apply ` to extract the numerical values , and do the counting there : #CODE
Pandas create pivot from grouped columns
I have an xls sheet where I want to combine columns ( for each ethnicity ) into an index column ( named ethnicity ) that I can use to partition ( construct a pivot table for ) other numeric variables .
pandas : concatenate dataframes , forward-fill and multiindex on column data
I am able to achieve this using a combination of ` concat ` , ` sort ` and ` ffill ` .
I believe a better way to represent the data would be use a multiindex with the 2nd dimension's value coming from the ` id ` column .
The trick is to use ` append ` which adds a column to the existing index .
This is what I want to do , for all the rows with same value of column a , I want to drop duplicates , but value of column b should be summed across those rows , and for rest of the columns , I want to keep the first value .
You can use an the ` pandas.DataFrame.groupby ` method to collect rows corresponding to unique values in any given column , and then use the groupby object's ` aggregate ` method to sum these up .
I'd assign to column ' b ' the result of grouping on ' a ' and summing , you can then drop the duplicates : #CODE
If not already you need to convert to datetime , then you can call ` apply ` and use ` datetime.strftime ` to do the formatting : #CODE
Use ` loc ` and a boolean mask to set the values you desire : #CODE
You can use ` apply ` : #CODE
Merge dataframes in a dictionary
And I want to merge them all into one like this : #CODE
But is there a way to automatically zip through the dictionary and merge ?
You can just pass the dict direct and access the ` values ` attribute to ` concat ` : #CODE
This assumes that you are just looking to concatenate all the dfs , if you are going to merge then you need to explain what the merge criteria is
When I try to plot a histogram , I apply : #CODE
also what does ` df.plot ( kind= ' hist ' , bins=50 )` show
pandas dataframe aggregate calculation
You can use the ` groupby ` method with the ` size ` aggregate to do this
Why not leave the data alone in a global and pass the index on the stack ?
Since ` index ` is an array of type ` bool ` , you are doing advanced indexing .
It has the array ` shape ` , ` strides ` ( default here ) , and pointer to the data buffer .
A ` view ` can point to the same data buffer ( possibly further along ) , and have its own ` shape ` and ` strides ` .
You have choice of when to apply the ` index ` , now or further down the calling stack .
pandas - drop rows under Datetime criteria
I would like to drop Users under certain Datetime criteria , for instance when they are present more than , let's say , 3 or more times in the same minute of the same hour of the same day .
I let you transform the later in dataframe ...
Now we can fill in the missing station number by calling map on the df with the ` NaN ` rows removed , and the station name set as the index , this will lookup the station name and return the station number : #CODE
Would I need to strip out white space and put to lower case the Start station . hope that makes a little better sence .
Yes you'd have to clean and extract the data before calling ` map `
You'd have to merge in that case something like ` df.merge ( df.dropna() , on= ' Start station ' , how= ' left ')` I think should work
This caused problems with merge and grouping .
To overcome this , I was forced to drop some instances and merge with a separate file containing proper station names ( merged on station name after cleaning , white space strip , and putting to lower case . ) Thanks to all for the help .
Conditional transform on pandas
I also tried with apply and I think it works , but I need to reset the index , which is something I'd rather avoid ( I have a large dataset and I need to do this repeatedly ) #CODE
Just out of curiosity : any reason you prefer ` pd.Series.std ` instead of ` x.std ` inside the ` transform ` ?
If you don't have the seaborn package installed , you can just drop color= sns.desaturate ( " indianred " , 1 ) as this was purely for aesthetics .
So I am trying to create a new column in the dataframe with the sum of ' Data3 ' for the all dates and apply that to each date row .
You want to use ` transform ` this will return a Series with the index aligned to the df so you can then add it as a new column : #CODE
Pandas groupby : apply vs agggregate with missing categories
` aggregate ` retains all " known " categories , but ` apply ` only keeps the categories that are present in the data .
I understand why ` apply ` might do this ( it chooses not to pass a group full of ` NaN ` s to the reduction function ) .
The above snippet is just a toy example : I actually need to use ` apply ` to get the result I want .
Question : What's the best way to use ` apply ` but create an output shape matching the one returned by ` aggregate ` ?
And I need to merge them into a single row with ind_con_dem and ind_con_gop ( so I can create comparison metrics ) .
Nor can I merge the resulting series of a df.groupby.apply ( lambda x : len ( x ))
You want to use ` transform ` : #CODE
` transform ` returns a Series with an index aligned to your original df so you can then add it as a column
I'd recommend a dict if you want to quickly access each df , but a list is less typing to concat into a single df
This issue with using concat is that it lacks the arguments that to_csv affords me : #CODE
It'd be more performant to read them into a list and then call ` concat ` : #CODE
Read the file as you have done and then strip extra whitespace for each column which is a string : #CODE
Appending the function above with ` .values ` will give you the result you desire , but it is then up to you to replace the index and columns .
Between the stocks ( every combination ) , computed the 20 day rolling correlation into a dataframe .
1 ) Calculate the 200 day simple moving average for each of the 20 day rolling correlations .
Getting the rolling mean of the rolling correlation is relatively simple for a single stock against all others .
is it possible to do a rolling mean of all the rolling correlations as a matrix ?
Essentially , I would like to view a matrix of the most recent 200day averages of 20 day rolling correlations .
How to remove just the index name and not the content in Pandas multiindex data frame
Try ` df.groupby ( ' asn ') .filter ( lambda x : len ( x ) 1 )` which will return you a ` DataFrame ` .
I also tried using MultiIndex but I didn't manage to set the double ( date , bin ) index afterwards ...
I always translate the task to SQL in my head , and in SQL there's only ` top n ` keyword .
I would like to plot this Series , together with the rolling mean .
I got the exact same error as you ( always when trying to plot a rolling average ) .
i = choice ( range ( len ( df )) , df.value )
pandas transform a csv into a h5 file avoiding memory error
I think it should be possible to append or something similar
for to_hdf check the parameters here : DataFrame.to_hdf you need to ensure mode ' a ' and consider append .
Here's a fairly general solution you can apply to multiple columns .
Then , reset the index of the original ` DataFrame ` , and merge : #CODE
Use this ` func ` and apply over the the ` dff.groupby ( ' Group ')` #CODE
and the rolling mean can be found by : #CODE
I'm sure this is not a case for a pivot table , or maybe a transposed method of displaying the data .
( I haven't tested it , but perhaps you could try to apply it and report the error message , if any )
The key to this answer is that the ` apply ` method accepts arbitrary positional keyword arguments and passes them to the function .
You can use ` apply ` with option ` axis=1 ` .
In the above example you start with ` x ` and replace each value with the corresponding value from ` y ` , as long as the new value is not ` NaN ` .
Is there a faster way to append many XLS files into a single CSV file ?
IOW , add ` datas = [ ]` ; ` datas.append ( data )` and then ` frame = pd.concat ( datas )` or something in the appropriate places , and remove the append .
There are several links in google and even in stack overflow that are great starters for profiling : #URL #URL #URL
In my case I want the percentile distribution including the median so multiplying the variable of interest with the weights won't work .
Well can you try ` df_conf [ col_name ] = df_conf [ col_name ] .str .encode ( ' utf-8 ')` replace ` col_name ` with whatever column you want to encode , saying that can you encode your index ?
` shift ` returns the row at a specific offset , we use this to subtract this from the current row .
Can you post sample df's and desired output , you can use another series and call ` reindex_like ` or add a temp column with the order you want order by that column and drop the temp column
I realized I could change the column that was the index to reindex , but this seems clumsy and prone to error even if it does work .
create empty list , add item using append , create series #CODE
df.fillna ( 0 ) command won't replace NaN values with 0
I'm trying to replace the NaN values generated in the code below to 0 .
I tried to save the df as an sql table using : ` df_pubs.to_sql ( ' conferences_pubs ' , db , flavor= ' sqlite ' , if_exists= ' replace ' , index=True )` when ` index=False ` , it works but I need the Conference Name ( ie the index ) to be added as a column , so when I put index=True it gives me an error : ` ProgrammingError : You must not use 8-b it bytestrings unless you use a text_factory that can interpret 8-b it bytestrings ( like text_factory = str )` .
The docs show that you are conforming your data to the new index which will introduce ` NaN ` values where none existed hence why you lost a row , this is why there is a ` fillna ` param for ` reindex ` .
Reshape ( stack ) pandas dataframe based on predefined number of rows
I can certainly do it using a loop , but I'm guessing ` ( un ) stack ` and / or ` pivot ` might be able to do the trick , but I couldn't figure it out how ...
Symmetry / filling up blanks - if the data is not integer divisible by the number of rows after unstack - is not important for now .
My goal is to transform this data such that the rows are the " time windowed " versions of my original data .
I've tried various combinations of pivot , shift , and groupby , but still can't coerce this output format .
If I understand you correctly and your month column is monthly already then you can ` groupby ` ' category ' column and call ` transform ` on ' var1 ' and pass function ` shift ` and assign this back to your df : #CODE
Didn't know of " transform " .
It's because the indices don't align , if the 2 df's are the same length you could do ` df_if [ ' Conference '] = df_conf [ ' Conference '] .values ` to assign the np array values
How to apply Cython to Pandas DataFrame
I have the following pandas DataFrame dt : #CODE
The sku is now the column so you need to use ` loc ` to perform label selection : #CODE
You can get the row with ` ix ` : #CODE
How to aggregate in the way to get the average of ` b ` for group ` a ` , while excluding the current row ( the target result is in ` c `) ?
So replace afterwards .
I'm thinking perhaps the better way is to just aggregate including the row , and then subtract , weighted by counts ( your first comment helped with the idea ) ?
I've run the code above with ` df = pd.concat ([ df ] *1000000 )` so that ` len ( df )` is 9 million , and ` %timeit df.groupby ([ ' a ']) [ ' b '] .transform ( ave_others )` completes in 1.18 seconds .
After reading the whole Dataframe , I tried to apply function on one Serie : #CODE
Use ` loc ` : #CODE
Ideally , I want to reinject / merge results in other dataframe .
After doing that , I want to apply the Porter Stemming algorithm which is readily available in nltk.stem .
why do pandas comparison operators not align on index ?
The fix is easy - just reindex .
All I am doing at the moment is loading the .csv as a dataframe and then writing it to the db using ` df.to_sql ( table_name , engine , index=False , if_exists= ' append ' , chunksize=1000 )`
You could ` apply ` and construct the datetime using your desired date values and then copying the time portion to the constructor : #CODE
How to resample time series with pandas and pad dates within a range
I then want to ` resample() ` the group series each to weekly , but want to align the first date across all groups .
Use the ` apply ` method .
Assuming the data has been pre-sorted on df [ ' index '] , try using ` loc ` instead : #CODE
An alternative solution is to create a pivot table , forward fill values , and then map them back into the original DataFrame .
Normalizing columns of multiindex dataframe in Pandas ( maybe a bug ? )
However , when I attempt to normalize on level 1 of the columns as follows : #CODE
You can use a combination of ` loc ` and ` np.diagonal ` to achieve this : #CODE
` loc ` here will perform row label lookup : #CODE
Error when using pandas dataframe map function in ipython notebook
Any idea why I get this error on the 2nd use of the map function but not the first ?
Something doesn't add up you have 891 rows for your gender ` value_counts ` but 889 for your embarked , this means you must have ` NaN ` values , the [ docs ] ( #URL ) state that ` NaN ` value are not counted you can confirm this if you try ` df [ ' Embarked '] .value_counts ( dropna=False )` , this means you need to handle the NaN by filling them first before calling map
You probably have to drop the NAs from your dataframe .
So you either drop them first ( using ` dropna `) or fill them with some desired value using ` fillna ` .
I was not aware that ` value_counts() ` dropped NaN values by default . added this command ` df2 =d f2.dropna ( subset =[ ' Embarked '])` and that removed the NaN , however I get a different error now when trying to run the map command : `
You'd have to merge in that case to find the common rows and then do some kind of diff between the merged and df2 but it depends on the order and how they differ as it becomes complicated depending on how they differ
possible duplicate of [ Pandas : conditional rolling count ] ( #URL )
I am trying to search from one matrix and replace that value on 2nd matrix .
Also one thing , it concerts ` 0 ` to ` nan ` , any way to keep 0 only , I tried wtih ` arr.replace ( np.nan , 0 , inplace=True )` but it says ` AttributeError : ' numpy.ndarray ' object has no attribute ' replace '`
With Pandas it's often a good idea to try and use ` apply ` together with an anonymous function to perform your calculation on every row .
Unfortunately , the approach from your link doesn't apply here .
Finally I just transpose the dataframe to get ids as rows and categories as columns .
Merge Only When Value is Empty / Null in Pandas
The merge works fine and as expected I get two columns col_x and col_y in the merged df .
Here's what I mean , how can I merge df.A : #CODE
You can then ` drop ` the extraneous columns : #CODE
when you open csv file from excel , it will convert your data to any type it should be in this case your data converted to date type then excel apply default date format to that data .
Like : cat filename | sed ' s / ISODate ( " // g ' | sed ' s / ") // g ' > newfile This will strip the ISODate ( "") but keep what is inside it .
Interestingly , at times , I noticed ` len ( x.unique() )` is much faster than above methods .
Using ` apply `
You could use ` pandas `' s ` apply ` for this .
How can I now apply this test on a slice of the dataframe ?
The quickest method IMO is to just filter the whole df and then drop the resulting ` NaN ` values ( using ` dropna ` setting a threshold on the column axis of at least 1 non- ` NaN ` value : #CODE
I am not sure whether it is really the answer to my question , though ( How to apply the test to the dataframe ) .
=0 ` specifically masks the entire df so will produce a True / False mask when used to mask the df this will put ` NaN ` values where the mask is ` False ` we can then drop these values column-wise ( using ` axis=1 ` but we only drop these if all values in the column are ` NaN ` as your desired
Your exception stack indicates you used this function instead : df = pd.Series.from_csv ( data , sep= ' , ' , index_col=0 )
Pass the dict as an arg to ` map ` : #CODE
I am currently able to do this through some very long-winded method with iterrow rows however for a dataframe of ~16000 , it took 45minutes and I would like to cut it down because I have larger dataframes to run this one .
if so , you can merge ( #URL ) them into a new dataframe first , and then compute the max / min , and extract the desired columns .
How do I translate these `' -- '` to ` 0.00 ` and also set this column as a float datatype when reading the CSV ?
You could pass ` na_values =[ ' -- ']` this will convert these values to ` naN ` and the dtype will then float , or replace these values after loading to 0.00 and then the dtype of the column would be float , as for your columns error you'll need to post the raw input in order for us to reproduce your error
The other is to load as before and ` replace ` these values to ` 0.00 ` : #CODE
You can supply a custom aggregate function to a crosstab using the ` aggfunc ` parameter : #CODE
Is there any way I can maybe resample ( wrong word ? ) the data so that I can have axes on a minute by minute basis while still maintaining the data points in the right place ?
it is a common issue in which you need to decide how to handle them , you can either drop them or substitute them with mean or some other inidcator value
I just picked a strategy to replace missing data with the mean , using the ` Imputer ` class .
I'm trying to compare two dataframes and check which elements are different and insert them into another dataframe .
So the idea is to add to df1 the data from df2 that is not yet in df1 ; line 2 in df2 does not exist in df1 so I want to insert it .
How should i write code to find not existing line so that I could insert it ?
You can use outer join to achieve this result #CODE
I believe you want to perform an outer ` merge ` : #CODE
Load the dataframe , with multiindex : #CODE
Write a function to replace the index : #CODE
and through reindex I should then be able to find the missing years as those years which have NaN values .
Could I define a lambda function and apply this to the index values , for example ?
you should be able to do ` import datetime as dt df.index.apply ( lambda x : dt.datetime.strftime ( x , ' %Y%m '))` I think should work
My aim is to draw a 2x2 figure panel describing the relationship between Av_density and pred2 separately for each LOC (= location ) with years marked with different colours .
This produces the figure panel divided by LOC accurately , with Years in different colours , but the scatter of the data points does not look right .
How do I merge columns by two in a table in pandas ?
I want to merge columns in it in this way : #CODE
As pointed out by @USER the column reordering is unnecessary as pandas will naturally align against the columns anyway so : #CODE
If you divide by a Series ( by selecting that one row of the second dataframe ) , pandas will align this series on the columns of the first dataframe , giving the desired result : #CODE
One way to do this is simply by using ` merge ` function to choose which keys you want to join on .
You can pass a dict of functions to a ` groupby ` to perform the stats using ` agg ` : #CODE
I recommend using ` conda ` ( or the Anaconda python distribution ) when using the scientific python stack .
I have tried to replace this by using the POSIX c library functions ` strptime ` and ` mktime ` directly but have not been able to get the right answer for the time conversion .
( Not this is a very general ISO parser , handles tz , odd formats and such ) .
pandas : join two table and populate value when even there is not a match
I want to join the following two table df1 and df2 to get a table like df3 where all IDs , A , B , C , D need to be present for each Time .
But left join only gets me this ...
Your merge doesn't make sense you have ID in df1 which has A and B but in df2 it has C and D also so you're going to get NaN values as you've found .
I don't think the issue is the merge , but rather you need to define what values are missing .
How to avoid return twice the first groupby object after to apply it in other function ?
You need to avoid for-loops and use the ' apply ' methods .
I'd like to append to an existing table , using pandas ` df.to_sql() ` function .
I set ` if_exists= ' append '` , but my table has primary keys .
I'd like to do the equivalent of ` insert ignore ` when trying to ` append ` to the existing table , so I would avoid a duplicate entry error .
but when I try to write a function in pandas to apply this to every cell of a column , it either fails because of an attribute error or I get a warning that a value is trying to be set on a copy of a slice from a DataFrame #CODE
how can I apply this code to each element of a Series ?
The problem seems like you are trying to access and alter ` row [ ' text ']` and return the row itself when doing the apply function , when you do ` apply ` on a ` DataFrame ` , it's applying to each Series , so if changed to this should help : #CODE
Alternatively you might use ` lambda ` as below , and directly apply to only ` text ` column : #CODE
normalize column in pandas dataframe based on value in another column
I want to normalize the values in one column of a pandas dataframe based on the value in another column .
Then how do I use this to normalize the value in each row ?
And then replace the column with a new one with these values : #CODE
You could use ` transform ` , which performs an operation on each group and then expands the result back up to match the original index .
The second array in C ( array ([ 0 , 7 , 2 , 5 , 3 , 6 , 1 , 3 , 3 , 0 , 8 , 5 , 4 , 6 ]) gives you the positions of the values you want to replace in ds1 .
So you have to replace the values in ds1.values.ravel() with the index of the first array of C with the values in ds2 with the index of the second array of C
pandas dataframe join with where restriction
I want to join these two DataFrames based on the ` name ` column , however as you can see the names are not unique and are repeated each year .
Hence a restriction for the join is that the ` year ` of ranks should match the first four characters of ` date ` of tourneys .
I am aware of the ` join ` method in pandas , however , I would also need to restrict the join with some sort of ` WHERE ranks.year == tourneys.date [: 4 ]` .
Create a new ` date4 ` for ` df2 ` and then merge ` df1 ` and ` df2 ` #CODE
Now , merge ` df1 ` and ` df2 ` on ` [ ' year ' , ' name ']` and ` [ ' date4 ' , ' name ']` combinations .
However , if only this code , not append to ` df ` , I can get a correct series .
You can apply on ` df.groupby ( ' Sym ') [ ' close ']` using ` pd.rolling_max ( x , 2 )` instead #CODE
I think what happens here is that because you pass the data from ` p2 ` and using one of the columns as the index , the index values no longer align so you end up with 0 values .
So far I have been using numpy loadtxt but I am having some issues with some text files generated by a very old program ( which I cannot replace ): #CODE
Another advantage is that you can call many other function on ` grouped ` once the grouping is done , such as mean , median etc .
python pandas : drop a df column if condition
I would like to drop a given column from a pandas dataframe IF all the values in the column is " 0% " .
We do it with an apply on axis=1 :: #CODE
Or with pandas loc querying API ( used to be said slower ): : #CODE
Well this works ` df.assign ( ts4= np.where ( df.a * df.b - df.c > 1 , ' XS ' , ' L '))` , the problem with expression ` ts4=lambda x : ' XS ' if x.a * x.b - x.c > 1 else ' L '` is that it wonly works in the apply because you're using ` axis=1 ` so you're comparing single scalar values , your expression isn't so it's not valid
What I am hoping to achieve , is to apply the logic of ` conditions ` to ` indicators ` in order to produce a new dataframe called ` signals ` .
Just define a function that fits your needs then apply it .
In the example given , this would translate to #CODE
Just include the column selector in ` loc ` , using ' : ' to indicate all columns : #CODE
Now , I want to just take the average of Column [ 1 , 3 , 5 , 7 , 9 , 11 , 13 , 15 ] from new.csv and append it to another file or this file .
Apologize for the lengthy description , but I can't seem to append the columns in python .
This will join the rows and write them to a new csv : #CODE
I would like to replace the single ' bidVol ' column with 78 columns , one for each total generated over the five minute intervals of a trading day .
Then replace your ` contracts ...
Very frequently , regressions will drop some observations because they are missing one or more regressor fields .
Of course , I could drop the correct rows myself before the regression , like this : #CODE
When we want to compare models , then it is better to drop missing values for all variables , so we have a common dataset for the estimation of different models , which means that the user has to do it .
As you can see , it eliminate the index but not in the column header , leading to a data shift .
Pandas DataFrame drop row
I need to drop the 2 row ` ( 2013-07-02 29624840 )` ?
Use ` drop ` method #CODE
Use one more back slash before " \n " ( \\n ) to consider " \n " as a new line . also for ` lt ; ` and ` gt ; ` check if the pd.DataFrame has any other variable where we can mention the data is of html .
This would apply the desired operation to all the rows and is considerably faster
This gives ` AttributeError : ' list ' object has no attribute ' apply '`
join two dataframe together
having two dataframe , I want to join them together ?
just remove the last entry and then join ?
Use ` apply ` and pass your func to it : #CODE
The correct way as EdChum pointed out is to use ` apply ` on the ' location ' column .
I want to get line plotting ( x axis : DT ) as the second chart in #URL
I then apply the following filter to ` ORD_ticks ` to get ` ORD_prices ` : #CODE
may be ` resample ` could be of use ?
I have a list of Timestamps and want to get the rows immediately prior to those timestamps , so am using ` asof `
You can ` shift ` column-wise by passing ` axis=1 ` , ` loc ` with row-label selection and a column selection will ensure only those rows and columns are modified : #CODE
So it should also work to replace ` next ( axs )` in your example with ` ax = axs.next() ` and change the argument of ` plot ` to ` ax=ax ` .
I can easily plot they separately , but because of the index , it will shift the plots according with the index : #CODE
I've seem some answers here , but you have to concat , create a multiindex and plot .
possible duplicate of [ Parallelize apply after pandas groupby ] ( #URL )
If you use a ` Pool ` , from any of the parallel packages , the best you are going to get a list ( using ` map `) or an iterator ( using ` imap `) .
How do I drop rows from a Pandas dataframe based on data in multiple columns ?
I know how to delete rows based on simple criteria like in this stack overflow question , however , I need to delete rows using more complex criteria .
I need to drop all rows that don't have at least one code with a leading digit of less than 5 .
So basically this takes the first character of each column using the vectorised ` str ` method , we cast this to an int , we then call ` lt ` which is less than row-wise to produce a boolean df , we then call ` any ` on the df row-wise to produce a boolean mask on the index which use to mask the df .
I haven't used any to make a mask in that manner before .
No it won't see my edit , when you call ` dropna() ` on a series ( which is what we're doing here when calling ` apply ` on a df ) it drops an entry in the series not an entire row
To fix I added a reindex to the line of code : df [( df.apply ( lambda x : x.dropna() .str [ 0 ] .astype ( int ))) .lt ( 5 , axis = 0 ) .any ( axis = 1 ) .reindex ( index = df.index , fill_value = False )]
So here I call ` transform ` on the ` groupby ` object to return a series aligned with original df to add the ' sum ' column , I can then filter the df as usual .
I created a pandas pivot table using ` pd.pivot_table ` .
Is there a way to transpose this so when I export it to excel all the values are in one row ?
You can transpose a table with df.T
Nested dictionary to multiindex dataframe where dictionary keys are column labels
For the expanding product , there's ` cumprod() ` .
For the rolling version , I think you'll have to use ` rolling_apply ` to apply ` prod() ` to each window .
But I really like the window and single division approach you've taken for doing the rolling version .
if i just set timezone to utc with tz_localize ( tz =p ytz.utc ) , timings are : 11.4 sec , whereas the dates didn't change at all from standard datetimes .
Datetimes with a naive tz ( e.g. NO timezone ) ` Series ` are efficiently represented with a dtype of ` datetime64 [ ns ]` .
Hope someone with better knowledge than me comes here to say otherwise , but I only know the insert ( row ) method
I imagine it is EmpID | ManagerID | StartDate | EndDate as a separate table and then join it back somehow ?
Create climatology from pandas dataframe ( append the mean by day-of-year )
I would like to calculate the climatology for this dataframe i.e. find the mean of all values corresponding to Jan 1st ( for the years 1950 to 1953 ) , and then append the mean value to the dataframe for the time period from Jan 1st 1954 to Dec 31st 1960 .
thanks smci , I am computing the mean by day-of-year , the question is how to append it to the existing dataframe for a new set of year ( 1954 to 1960 )
You could use ` resample ` function over a year period ` AS ` #CODE
And , then append to ` cum_data ` #CODE
thanks , however I want to append this to the original dataframe , and also update the time period from 1954 to 1960
Replace line 12 with ` tmp = tmp.reindex ( pd.date_range ( ' 1954-01-01 ' , ' 1960-12-31 ' , freq= ' D ')) .ffill() `
On a leap year , I insert the entry defined by ` fill_leap ` at the 60th day ( 31 days in January + 29 days in February on a leap year ) #CODE
I've tried the ` merge ` suggestions .
Do I have to change header labels to match that of the original table and drop a few columns ?
This is a perfect example of where you'd want to use ` pandas.merge ` ( basically the Pandas equivalent on SQL JOIN , but where columns equality is the only join condition allowed ): #CODE
I think you want a ` left ` style ` merge ` : #CODE
I see that ` merge ` has a ` suffixes =( ' _x ' , ' _y ')` which is applied to overlapping columns .
Do I just need to drop columns I don't want ( suffixes with ` _x `) and change the headings on the ones I do want or is there a better way ?
You can either ` drop ` , ` rename ` or just not select them in the merge .
The reason they end up in the merge is because the values clash on lhs and rhs so it generates a column for both sides to preserve them
All credit goes to @USER and @USER for steering me in the right direction with ` merge ` .
And , then merge on ` dd ` and fill ` NaN's ` with ` 0 ` #CODE
After that you can drop Day , Month , and Year from your dataframe .
Great : adopted to the " real " dataframe where I have to apply this loop , it worked out perfectly !
I normally use ` apply ` for this kind of thing ; it's basically the DataFrame version of map ( the axis parameter lets you decide whether to apply your function to rows or columns ): #CODE
To do that , you can use ` apply ` with ` axis=1 ` .
( This doesn't really have anything to do with ` apply ` ; indexing with a list is the normal way to access multiple columns from a DataFrame . )
However , it's important to note that in many cases you don't need to use ` apply ` , because you can just use vectorized operations on the columns themselves .
As above , there are two basic ways to do this : a general but non-vectorized way using ` apply ` , and a faster vectorized way .
You can define a function that returns a Series for each value , and then ` apply ` it to the column : #CODE
The only output I get is the below , which doesn't go up the stack to my code : #CODE
I'd say you use your IDE's debugger , set a breakpoint at the given line and look at the stack trace .
Placing a breakpoint at line 1182 of pandas\core\ generic.py ( where the warning was generated from , which I found by full-text searching the pandas / core directory ) , and then moving up the stack got me to the line number I need .
pandas dataframe replace blanks with NaN
I have a dataframe with empty cells and would like to replace these empty cells with NaN .
Has anyone a suggestion for a panda code to replace empty cells .
I think the easiest thing here is to do the replace twice : #CODE
Tried doing that and I get an error saying ` Join on level between two MultiIndex objects is ambiguous `
You have to get rid of the ` Inside__Outside ` column from your index , since you are not using it to join the two tables .
You can stack them one on top of the other , group on ` Week_No ` and sum on ` item_Number count ` : #CODE
Just append them together and group them by the first level - #CODE
Make a second dataframe with the median values by date .
I would like to divide df1 by df_med so that I generate a dataframe of data normalized by the median value for that day of year .
The reason the division didn't work is because the grouped df is a different shape , so one way would be to call ` transform ` so that it returns a series with an index aligned to the original df and then you perform the division : #CODE
I hadn't used transform before and that is the ticket .
` transform ` is specifcally designed for these kind of situations , generally it is used for adding new columns from the groupby operation
Maybe , I should not use " map " and " format " ?
Most interestingly , the " map format " DOES in fact change the length of the strings ( and the spacing ) , but the " justify= ' right ' " did not work on them .
data = ascii.read ( table ) ( and apply the formatters )
I guess we could define ` def idmap ( l ): u = np.unique ( l ) im = { u [ x ]: x for x in range ( u.shape [ 0 ] ) } return map ( lambda x : im [ x ] , l )` to save three lines .
Also , you can apply the mappings directly in the argument of ``` coo_matrix ``` to save space , but that is a bit messy .
Now I try to resample by week and plot the mean : #CODE
Can you post what structure the additional data looks like , essentially you can construct a df from the date data and concat them or add them to this df
use the insert method if you care about column order #URL
I want to reindex the DataFrame to the end of the month , like so : #CODE
this generates the same " AttributeError : ' Series ' object has no attribute ' dt ' "
If you're running a recent-ish version of pandas then you can use the datetime attribute ` dt ` to access the datetime components : #CODE
Then I will append it .
` append ` adds each ` frame ` to the list as you loop through the files .
I would like to get whatever values are common from df1 and df2 and the dt value of df2 must be greater than df1's dt value
One way would be to merge on ' value ' column so this will produce only matching rows , you can then filter the merged df using the ' dt_x ' , ' dt_y ' columns : #CODE
The series produced by ` ( df [ " a "] | df [ " b "])` is of type ` bool ` .
Thus , I have to do the ` apply ( lambda ... )` to get the desired ` int ` column .
Ah , so ` __or__() ` is to ` | ` just as ` __eq__() ` is to ` == ` , and ` Series ` implements ` __or__() ` as returning a ` bool ` , correct ?
So the decision for ` Series.__or__() ` to return a ` bool ` instead of an ` int ` was motivated by the fact that there had to be some kind of boolean comparison of ` Series ` , but with no ` object ` method for ` or ` , the ` __or__() ` method and ` | ` operator were the only way to go ?
I would suggest to create a new excel sheet and merge it with the old one by ` ID ` on your choice them using #URL
You can use ` pivot ` #CODE
Cool I didn't know about pivot either ...
For example , ` for eachP in perf_array : df_res.loc [ len ( df_res ) +1 ]= [ strategy , ticker , strat , stratROI ]` You don't even use ` eachP ` in the loop , and aside from that , I can sort of guess what you are trying to do there , but can guarantee that the code is not doing what you want it to .
Q1 on line 5 I am using following vars : strategy ( opbtained from the iteration itself , so starting with strategy0 ending on strategy3 , ticker ( same thing from where it is in the iteration from 560 diff company tickers , Return-On-Investment ( ROI ) is the interesting part that where the actual performance of the prior 2 vars ( strategy and ticker combined ) ROI is a percentage .
I have been trying with a function and apply . here is my data set and code #CODE
I think that a mask would work , but I would like to be able to adjust by day of year ( an integer 1 through 365 or 366 ) .
Perhaps I need to write a little function to generate the mask parameters for an arbitrary day of year .
This does work , and it is easy enough to create a mask with something like
The easiest method that you can use is to get the year's data with partial string indexing , then ` resample ` annually #CODE
With this method , you can cut out the creation of the timedelta , the second date_range , and the complex and erroneous slicing .
The resample method is standard , using ' A ' as a signifier for " annual " frequency and how= ' first ' to just grab the first matching item .
From what I read , it seems that HDF-stores should be able to save the data in the required format AND allow me to append rows , so I can convert the strings to GPS-entries bit by bit , thereby not getting problems with my memory limitations .
The problem with this setup is that the DataFrames won't align on columns D & E .
` update ` will align on the indices of both DataFrames : #CODE
that solves one part of the problem definitely alexander , which is replace values in df1 with values in df2 for the same indexes .
See pandas DataFrame concat / update ( upsert ) ? for the discussion .
An alternative would be to apply strip to the columns to ensure they don't have leading spaces : #CODE
That worked , but the odd thing is that the space does not show up on Jupyter ( visually ) but did when I cut and pasted it to this question .
Anyway , since your datetime variable differs only in the hour , you could just extract the hour with a ` dt ` accessor : #CODE
Then apply ` maxminbid ` function on ` Auction_id ` grouped objects #CODE
Then you can use ` loc ` to select the appropriate values for each ` Auction_id ` and use max on their values .
My main goal is to match the index value from ` ds2 ` into ` ds1 ` and replace it with corresponding value , so the output would look like #CODE
Now , we'll just use ` ds2 ` to directly map all the elements in ` ds1 ` : #CODE
You really need to inspect ` dt ` ( the result of ` read_csv() `) for different pieces of test data .
Almost never you need indices in Python , so ` for i in range ( 1 , 4 8) ` as well as the ` for n in range ( len ( transactions ))` make me believe that you first should write your logic with more idiomatic Python .
Using the traditional SQL " insert into " involves hitting the database millions of times which doesn't seem right .
Now , I wish to search for and extract all matching keywords from each ' post_message ' , in each row of the ' large ' csv file , and append all the matched keywords in a new column ' keywords ' .
I would cut down the number of rows to search by running grep for " Pancrea " first : #CODE
Filling blanks after merge in MultiIndex timeseries
I have multiple MultiIndex timeseries that are merged together and the index is ( " location " , " timestamp ") .
I have tried different groupby / resample expressions and failed .
Instead of ` ffill() ` , you could also , interpolate the values , using ` pd.Series.interpolate ` #CODE
Be sure to ` append ` to the lists instead .
So this first constructs a view of your df where the status is ' Infected ' , we then set the index to the address , this creates a lookup table where we can then lookup the address using ` map ` in the ` infected ` index and return the status .
I use ` loc ` here to only select the addresses that are in the infected index , to leave the other rows untouched .
I'd like to use a ` DataFrame ` with a ` DatetimeIndex ` to align data from different sets ( the columns ) on their timestamps .
Is there a quick way of determining if any ` DataFrame ` columns contain a ` nan ` ( essentially a set intersection ) to determine which are the timestamps or rows that have valid data ( non- ` nan `) in all columns .
There's a somewhat more native approach combining ` notnull ` and ` all ` : #CODE
Sometimes it's simpler to use ` df.dropna() ` rather than materializing a mask , but having an explicit mask can be handy too .
The same is true for the other indexers - ` loc ` , ` ix ` , ` at ` and ` iat ` .
And I apply the following group-by code #CODE
I would like to replace row values in pandas .
Here , I would like to replace the rows with the values ` [ 101 , 3 ]` with ` [ 200 , 10 ]` and the result should therefore be : #CODE
In a more general case I would like to replace multiple rows .
Therefore the old and new row values are represented by nx2 sized matrices ( n is number of row values to replace ) .
This code instead , allows you to iterate over each row , check its content and replace it with what you want .
I found this solution better , since when dealing with large amount of data to replace , the time is shorter than by EdChum's solution .
I suspect the way you are doing it ( as improved by @USER ) is about as good a way as any since you are starting with a mask ( the ` b ` dataframe ) .
Now if you aren't actually starting with that mask , then there might be a better way .
One way is to stack the result : #CODE
The default setting will drop the null values .
Just drop it if not .
Oh , that's interesting you can do it with stack .
Also , I think the problem is essentially turning multiple columns into one , which I think naturally falls into the realm of pivot and stack etc .
On the other hand , it would seem that stack is around 2-3 times slower
This is on a Windows 7 Enterprise Service Pack 1 machine and it seems to apply to every CSV file I create .
How to apply group by on data frame with neglecting NaN values in Pandas ?
What you really want to do is to pivot the table so that the values in ` station ` columns become column headers .
The trouble is that the implicit ` name ` attribute of the on-the-fly calculated ` Series ` inside of your calls to ` join ` will also be `' Temp '` since it's a derived calculation from that column .
You can provide an ` rsuffix ` argument to ` join ` which will append a given string to the name , for the column coming from the right operand of the join ( in this case , the one inside the function call ) .
However , note that you are always omitting any ` on ` arguments ( join criteria ) in your use of ` join ` -- meaning you default to " joining " by the index .
This is preferable because it much more clearly expresses your intention , which is not to join but to create a column .
Further , since the default join method is `' left '` , if you happened to have duplicate indices , you could potentially end up joining multiple times for each duplicate in the left-hand index , and because that index is the same as the right-hand index ( and would therefore also have duplicates ) it could mean you would silently and erroneously introduce more duplicates with each join .
You may also choose to use ` map ` instead of ` apply ` since upon accessing a column , you'll be working with a Series object .
works great thanks , but why would I use applymap and apply the function on all the df cells , there is different types of data and all other stuff ?
I expanded a bit in the middle to highlight why simply creating a new column of data makes more sense here than using ` join ` .
On the other part , note that I'm not suggesting to use ` applymap ` which is a DataFrame method , rather to use plain ` map ` which is a Series method .
` Series.map ` is meant specifically for element-wise operations , whereas ` apply ` has some extra checking for functions that can vectorially operate on the whole ` .values ` data object in one go .
I'm using odo from the blaze project to merge multiple pandas hdfstore tables following the suggestion in this question : Concatenate two big pandas.HDFStore HDF5 files
I thought that a mask would do it , so I tried .
I am looking for a way to stack ` results [ 1 ]` , ` results [ 2 ]` etc . on top of each other to yield a DataFrame like this : #CODE
You can use any function within the ` apply ` .
I'd profile that code but it's doing a lot of melt and merge operations , can you post your starting and desired df result , you can probably achieve this using a combination of multi-indexing / pivoting / stacking
This is an example of using a where mask #CODE
A regular select does a reindex and will use a lot of memory .
Another approach would be to use ` value_counts ` to create an aggregate series , we can then use this to filter your df : #CODE
Another solution if you like map and list #CODE
One clunky method would be to temporarily add the ` abs ` value of column b , then ` sort ` using that column and then ` drop ` it : #CODE
An alternative would be to take a view of the ` abs ` values of ' b ' , call ` sort ` on it and then call ` reindex ` passing the index of the series : #CODE
Thanks to the master @USER for this unknown method ( to me anyways ) , you can call ` order ` on result of ` abs ` which results in cleaner code : #CODE
Map vs applymap when passing a dictionary
I thought I understood map vs applymap pretty well , but am having a problem ( see here for additional background , if interested ) .
That works as expected -- both operate on an elementwise basis , map on a series , applymap on a dataframe ( explained very well here btw ) .
If I use a dictionary rather than a lambda , map still works fine : #CODE
So , my question is why don't map and applymap work in an analogous manner here ?
df.applymap() dont apply .map() on each Series of the DataFrame , put map .apply() on each Series .
I guess that applymap and map are not equivalent , which I don't dispute , but I don't have any better understanding as to the why or how .
To quote from the link above ( to a very popular SO answer ): " applymap works element-wise on a DataFrame , and map works element-wise on a Series .
Doing map ( dct , df [ 0 ]) will give you the same error as df.applymap ( dct ) and df [ 0 ] .apply ( dct ) will also give the same error .
The example showing how map produces a series and apply produces a dataframe also explains some results I'd gotten in the past and not understood .
You could just assign / merge this result back to your df , so using the above code you could do ` df1.merge ( total.reset_index() , on= ' full_code ')` I think should work
Pandas : how to merge two dataframes on offset dates ?
I'd like to merge two dataframes , df1 df2 , based on whether rows of df2 fall within a 3-6 month date range after rows of df1 .
I am trying to apply this related topic [ Merge pandas DataFrames based on irregular time intervals ] by adding start_time and end_time columns to df1 denoting 3 months ( start_time ) to 6 months ( end_time ) after DATADATE , then using np.searchsorted() , but this case is a bit trickier because I'd like to merge on a company-by-company basis .
What I would like to do , is to use Pandas to interpolate the missing values the best I can , and then re-index the time series with 5-minutes intervals .
Self join DF on c , d
How can I group by a specific column but then append the values of one column of that group to a new column .
The above groups on ' type ' and then we call ` transform ` on ' string ' column and call a lambda function that ` join `' s the string values .
Also the ` transform ` here returns a series with an index aligned with the original df .
If you want a solution which can include specific phrases ( which you know before hand ) in your count , you could replace the spaces in the the phrases with another character ( say " _ ") .
Pandas merge dataframes based on closest match
Right now , I am doing a simple merge but that does not do what I want
To join the two DataFrames on ` N0_DWOC ` you could use : #CODE
I didn't include it in the above example , but if it were there , the ` join ` would merge ` df_b [ ' N0_DWOC ']` as a new column called ` N0_DWOC_b ` .
You can use the groupby function using the ' Id ' column and then use the resample function with how= ' sum ' .
and I have a map which renames certain columns as such #CODE
( occasionally ` name_map ` might map a column to itself , e.g. ' one ' -> ' one ') .
@USER , I actually want to map ' one ' to ' one ' , as the map is generated by another function , and this is a possible result of that function .
I suppose I can drop items that are mapped to themselves ... and then run your code below
If you can do that ( and you don't have two columns with the same name ) , you can do drop and rename easily .
I think the easiest way would be to drop the columns which are not present in the ` name_map ` values list ( since you want to remove the first ` two ` column ) #CODE
Pandas : assigning values to a dataframe column based on pivot table values using an if statement
I'm trying to figure out how to use an if statement inside of .ix [ ] ( or otherwise ) I have a pivot table I'm using to get a lookup value into my main dataframe .
Here's a chunk of the pivot table ( entitled ' data ') : #CODE
However when trying to insert the dynamic portion to include an if statement , things don't work out so great : #CODE
I'm actually trying to assign values to a new column from values calculated in a pivot table .
if row 1 under column ' foo ' == ' thing1 ' AND column ' bar ' == ' thing2 ' then I want to get the calculated value held in the pivot table for where those two meet .
I used .ix [ ] to drill down to the value , then .apply() to apply a function across each row ( axis=1 ) finding the line's values just as you would a dataframe .
Now all that needs to be done is pivot the table and fill the null values with ` False ` .
The dataframe has only unqiue date-value pairs , hence we can drop .
Based upon the answers , ` df.apply ( lambda x : len ( x.unique() ))` is the fastest .
You could do a transpose of the df and then using ` apply ` call ` nunique ` row-wise : #CODE
As pointed out by @USER the transpose is unnecessary : #CODE
Similarly I think ` df.apply ( pd.Series.nunique )` would also work ( and avoid the need to transpose if that's an issue ) .
@USER yes that is better actually , initially I thought that the transpose was necessary to get the columns as the index values
However when I try append ( with ignore index ) I get #CODE
You can create a list of the cols , and call ` squeeze ` to anonymise the data so it doesn't try to align on columns , and then call ` concat ` on this list , passing ` ignore_index=True ` creates a new index , otherwise you'll get the month names as index values repeated : #CODE
Another way to do this is to ` unstack ` the ` DataFrame ` .
On each group you apply the ` mean() ` function , and you get the group whose index is ` 1 ` , which correspond to the ` Feature Active == 1 ` group .
I need to transform it as ` new array ` : #CODE
I don't know the SQL method but in pandas you want to ` pivot ` : #CODE
If you also add ` bookmaker ` to the index and then ` unstack ` it : #CODE
Say I want to combine df1 , df2 and df3 and merge on index and column ' id ' : #CODE
but I struggle with the merge function .
Then if we stack the DataFrame we get this Series : #CODE
Now if we turn around and unstack the Series , the values are aligned based on the remaining index -- the date and the ` id ` : #CODE
One way to address the issue would be to group by the ` date ` and ` id ` and aggregate the values .
Another option would be to pick one value and drop the others .
So a span of 60 obviously wouldn't apply here , as Pandas would just interpret that as every 60 datapoints rather than every 60 seconds .
@USER recommended the resample method in Pandas , and that is what I was looking for .
You could add an ordinal variable for which group-of-60-seconds each row belongs to , then group by that ordinal value and apply the averaging function across the entire group , on a group-by-group basis .
I think you want [ ` resample `] ( #URL )
@USER yes , resample is it , thank you !
Trying to figure out how to do that with resample .
Pandas : Get grouping-index in apply function
What are you actually trying to do with " mytest " , it looks like what you're looking for is actually a df.groupby and then an agg which can view the index , or a df.resample and then an apply which can also reference the index
Calculations within pandas aggregate
You want to [ ` resample `] ( #URL ) there is a [ section ] ( #URL ) on this
Thank you so much @USER : resample is what I need !
Try using ` resample ` : #CODE
yeah , i've been messing around with aggregate also but I can't seem to figure it out .
You could ` melt ` ( ' unpivot ') the DataFrame to convert it from wide form to long form , remove the null values , then aggregate via ` groupby ` .
I can manually cut and copy paste these two portions in separate files and read in , but I want to automate it using regex .
Pandas dataframe insert rows
I want to insert rows in DF and modify its related values :
The code can only append rows but how to modify its values in a faster way ?
Possible solution is to reindex the resulting dataframe with a constructed index of all series names .
pandas DataFrame set value on boolean mask
Above , I want to replace all of the ` True ` entries in the mask with the value ` 30 ` .
I wanted to set the ` True ` values in the mask and leave the ` False ` values the same .
No worries , I updated my answer , you just invert the mask to achieve what you want
You can't use the boolean mask on mixed dtypes for this unfortunately , you can use pandas ` where ` to set the values : #CODE
Note : that the above will fail if you do ` inplace=True ` in the ` where ` method , so ` df.where ( mask , other=30 , inplace=True )` will raise :
OK , after a little misunderstanding you can still use ` where ` y just inverting the mask : #CODE
It's the opposite of what I thought was asked for ( although I could have it backwards ) and it could be easily reversed by taking the complement of the mask .
@USER when you use ` where ` , the mask will produce the original values where the mask is ` True ` , the ` other ` value will used for where the mask is ` False ` , so the default for ` other ` is ` NaN ` , so it looks confusing but this is the expected and desired result
@USER yes this does seem ambiguous , I read his question as a way to replace ` NaN ` values but it reads more like your answer but the OP accepted my answer so it's unclear to me whether he wanted this result but asked for yours
How can I strip off the hours , minutes and seconds so I am only left with 2014-05-12 ?
I'm getting an error that says my series does not have a ` dt ` attribute
@USER it will only have the ` .dt ` attribute * after * you create it with ` pd.to_datetime ` , only datetime columns have the dt attribute
The Pandas merge is like a SQL INNER JOIN , where by default it will join on any common columns ; in this case the column ' bidder ' : #CODE
` groupby ` is the same as GROUP BY in SQL ; basically it will loop through distinct values of the column that you pass it , and then you can perform whatever aggregate function you want on each group .
Merge DF_B_2 with DF_A using bidder column and how=inner .
Building on @USER ` case_insensitive_order ` , a single liner using ` reindex ` #CODE
- yields either one row by 3 columns ( the way above ) or if I transpose it is 3 rows and 1 column
When you need to append some new user just make a new list of all user details and append it
You could do ` pd_users.T ` to transpose , if you wanted to , and would then see ( via ` info() ` or ` dtypes `) that everything is then stored as a general purpose object because the column contains both strings and numbers .
It's pretty common ( and generally pretty fast ) to change indexes , then change them back , transpose , subset , etc . when working with pandas so it's usually not necessary to think too much about the structure at the beginning .
But you could do ` pd_users == pd_users2.T ` ( put a transpose on either one ) to further verify .
It will raise an exception because the the two dataframes no longer conform .
You can drop ` lower ` column if you want .
print ( len ( upregulated ) , end= ' \n ')
I want to apply the mapping to df1 .
I could also convert df2 to a dictionary and use the replace method , but it does not convince me either .
There is a ` map ` function for this , which accepts a dict or series , in the latter it uses the index to perform the lookup : #CODE
On your test data ` map ` is nearly 2x faster I would expect this to be true for large data also as it's cython optimised and doesn't need to do as much checking as ` merge ` .
for performing vlookup style assigning you should use ` map ` , it's designed for this , the only caveat is that the index of the series must be unique which is not an issue when you pass a dict .
If you know the name of the column you want to drop : #CODE
and if you have several columns you want to drop : #CODE
You need to pass ` axis=1 ` for ` drop ` to work : #CODE
How can I both interpolate and regularize the frequency of the observations ?
Then resample the data to have a 5 minute frequency .
Note that , by default , if two measurements fall within the same 5 minute period , ` resample ` averages the values together .
Finally , you could linearly interpolate the time series according to the time : #CODE
might need to resample at a shorter frequency so cubic or spline interpolation
There's a way I can instead first add the regular 5 minutes timestamps to the series with nan as values , and then interpolate them with an order 3 spline ?
I mean , there would be any difference if instead linearly interpolate the time series with ** resample() ** , we build first a time series with the plain entries in the data , then we add entries of the type
and so on , and finally interpolate them with an order 3 spline ?
You don't have to interpolate linearly .
.unique() only works for a single column , so I suppose I could concat the columns , or put them in a list / tuple and compare that way , but this seems like something pandas should do in a more native way .
and ` df3 = pd.DataFrame ( range ( len ( ts2 ) , index=ts2 )` .So how do I create a stepwise-dataframe df4 , where for each date T in ts3 the value between [ T , T-1 ) for df4 is df3 [ T ] .
Can the replace method be used to replace anything created by to_html() ?
@USER ` df.to_html() ` returns a ` unicode ` object , so you can use ` find ` , ` replace ` , ` strip ` , ` upper ` , ` lower ` , just like you would a string and replace anything you want .
My purpose is to apply two functions at the same time .
Basically , I want to cut my dataset for extreme values by looking for the 5% quantile at the lowest part of the dataset and the top % at the other end .
works but it's not precise because the 2nd function builds on top of the previous cut .
So how can I cut both at once ?
I have a pandas DataFrame with time as index ( 1 min Freq ) and several columns worth of data .
If so , I want to interpolate only if the gap is not longer than 5 Minutes .
I understand those reasons , of course I nowhere specified that it should not interpolate longer gaps than 5 minutes .
I understand that ` interpolate ` only extrapolates forward in time , but I want it to also extrapolate backward in time .
If it has only one value , it cannot interpolate values .
But it should not interpolate at all , if the gap is longer than the limit of 5 .
So here is a mask that ought to solve the problem .
Just ` interpolate ` and then apply the mask to reset appropriate values to NaN .
Hey JohnE , very nice idea to interpolate / bfill for all values and simply just use the results at previously defined positions .
I will check for the runtime and see if i can create the mask more efficiently with some hard thinking if needed .
FYI : I took your idea , adding cases , as the case of some NaNs , which should be interpolated , is quite rare : `` if df.isnull() .all() `` - the mask is all False , `` elif df.isnull() .any() `` do the groubpy-stuff you provided and for `` else `` set the mask to all True .
Also it is somewhat faster when not running the loop but putting the mask creation in a method and feeding `` data.apply() `` with it .
FWIW , I suspect the groupby / transform is now taking up most of the time , but don't see an obvious way around that .
Pandas : transform a DataFrame by splitting each value into multiple values
Can someone suggest how I transform my original table to get what I want ?
Sorry tabs became spaces when I pasted into Stack Overflow .
To place ` ( Blank )` on an equal footing with the other tiles , let us replace it with a single character , such as an underscore ( ` _ `) .
Each tuple consists of the three values : ` ( row [ ' points '] , row [ ' freq '] , letter )` .
DF1 ( my left join ): #CODE
Edit - is it possible to do this with a asof command ?
It looks like your data does not have headers and the first left columns of each file should align ( i.e. , if file A has 10 columns and file B has 15 , then the first 10 of each file align with each other ) .
it's not too much of a stretch to insert NaN's into the data using reindexing so that i get this : #CODE
my first idea was to convert the data to a pivot table : #CODE
and then do the forward-fill but i cannot figure how to restrict to year ( or how to unpack the pivot table back to it's original form ) .
though if i edit the pivot table create statement to change ` year ` to the columns from the index , i _seem_ to get what i'm looking for but i'm having a great bit of difficulty in unpacking the new table .
You can use a pivot table or stack / unstack : #CODE
Finally , stack and sort your data , and then reset your index if desired : #CODE
So above where I just print the json you can append to a list .
So , First of all you need to sort them , then you can apply ` itertools.groupby() ` function which returns : a group value and an iterator .
How to perform aggregate options on one groupby column , giving two column outputs
I am performing a bunch of aggregate stats on a groupby data frame .
For more details , please refer to Naming returned columns in Pandas aggregate function :
However when I replace the 2d list with a 2d numpy array : #CODE
As @USER suggested and I always agree with Jeff , for this kind of selection ` ix ` is the typical selection method but it's behaviour differs from ` iloc ` in that it does include the end row selection unlike ` iloc ` : #CODE
this is a use case for ix
The first and last data lines make this a little tricky so this next line backfills the first line and repeats the last line so that the following mask works correctly .
Now my idea was , to " resample " the data using the index which contains the value for the length .
Of those Dataframes I have a few so that it woulf be really nice to allign them all to the same " framerate " and then cut them e.g. so that I can compare different datasets .
Standard conversions you could do on a datetime would be ` astype ( int )` or use the ` dt ` accessors .
`` hist `` -> `` histogram `` ( `` hist `` is pyplot or something ) .
There is a pandas equivalent to this ` cut ` there is a section describing this here .
` cut ` returns the open closed intervals for each value : #CODE
What I want to do is merge the ` count_watched_yeterday ` column from ` df_two ` to ` df_one ` based on the index of each .
Firstly I think you need to pass dfs not a series , ` df_two [ ' count_watched_yeterday ']` is a Series not a df , by using double square brackets your force a df of a single column , secondly you're supposed to pass a list of columns for the ` on ` parameter , not the indices , so this won't work , if you want to merge on index you have to set the params ` left_index=True ` and ` right_index=True `
This works because it will align on the index values , where you have no matching values a ` NaN ` will be assigned as the value
This avoids apply .
This is briefly mentioned in the docs on multi-indexing , although obviously that doesn't quite apply in your case , I'm not sure where to go for a good overview of how slicing works for sorted / unsorted indices .
How to apply multiple formats to one column with XlsxWriter
In the below code I apply number formatting to each of the columns in my excel sheet .
However , I can't seem to figure out to apply multiple formattings to a specific column , either the centering or the numbering end up being over written .
Is it even possible to apply two types of formatting to one column ?
Is it even possible to apply two types of formatting to one column ?
Simple pandas pivot table issue
What I would like to do is create a pivot table that counts the instances of each ios_id for each video_id and feed_position .
You almost had it , just don't include ' video_id ' in columns : columns is just for what's going along the top of the pivot table , and index is for what's going down the left .
For ` sub_df ` , I want to drop all rows with missing NaNs and re-check the true types of the columns .
I really dont understand what could be wrong .. without ` ix [ 1 ]` it displays the whole column / row perfectly . but its just not able to store the names in the variable ..
` ix [ 1 ]` would only return what is at entry number 1 .. meaning to say , based on my csv , it will only return entry if value is ` Alice Suh ` if it is anything else .
How to expand the data based on the single column in python ( transpose ) ?
I then call ` apply ` on that list to turn it into a Series , this will auto generate the names of the columns 0 ..
You could also use ` unstack ` after resetting the index of each group : #CODE
Within ' loc ' values are permuted .
Each value in a ' loc ' is from a different population ( ' pop1 ' , ' pop2 ' , ' pop3 ') .
So far I have have been able to form a massive list of tuples that combines every within ' loc ' rearrangement with every between ' loc ' rearrangement .
I need to get every combination possible of ' loc ' .
My problem is that I need to make calculations using any given permutation of ' loc ' for a particular ' pop ' .
You can use ` shift ` and ` cumsum ` to create ' id's for each contiguous block : #CODE
You want to perform an aggregate using ` agg ` : #CODE
I think you would still need to enclose the mask conditions in parentheses ` ( )`
I'm new to Pandas ( 0.16.1 ) , and want custom sort in multiindex so i use Categoricals .
Part of my multiindex : #CODE
I create pivot table with MultiIndex levels [ Defect , Own ] .
How can i custom-sort two levels in multiindex ?
I convert " Own " level to Categorical with the following code : create new column , replace index level with it .
Sorting a multiindex can be done using the Dataframe.sort_index function .
` map ( str.lower , index )` , would work for you .
For versions older than ` 0.16.1 ` ( thanks to @USER for pointing that out ) you can call ` map ` and pass the ` str.lower ` function : #CODE
@USER I'm using version ` 0.16.1 ` so it should work if you upgrade , otherwise try the ` map ` method in my updated answer
` from pandas import Panel , DataFrame , Series , read_csv , concat , to_datetime `
You can use ` clip ` : #CODE
You want to [ ` resample `] ( #URL ) so something like ` df.resample ( ' 30min ')` should work
@USER It's not resample .
Dataframe has a built in method for this called resample just do ...
Assuming you want the average value for each 30 min . period averaged over the month , try using resample ( in case your data is not already evenly spaced at 30 minute intervals ) , and then use groupby .
If you make ` samples ` a DataFrame with columns ` user ` and ` item ` , then you can obtain the desired values with an inner join .
I would read all of the files in a directory , append them into a single file , drop duplicates , apply some changes ( add some columns , perform some calculations on timestamps ) , and then save the consolidated new file in another directory .
join two pandas dataframe using a specific column
I am new with pandas and I am trying to join two dataframes based on the equality of one specific column .
pandas : setting last N rows of multi-index to Nan for speeding up groupby with shift
I am trying to speed up my groupby.apply + shift and
thanks to this previous question and answer : How to speed up Pandas multilevel dataframe shift by group ?
And now I can do my shift globally rather than per group .
Test setup ( for backwards shift ) if you want to try it : #CODE
ie replace each item X in a list with X-N , X-N +1 , ...
I ended up doing it using a groupby apply as follows ( and coded to work forwards or backwards ): #CODE
Note that the requirement to do a sort eats into the effectiveness of this , but its still faster than doing the shift per group .
I think what you're after is a sequence of pivot tables ?
You can use ` DataFrame.apply ` with ` axis=0 ` to apply a function to every column of a dataframe .
Then you can pick out the columns using a list comprehension , and finally use ` DataFrame.drop ` do drop the columns : #CODE
Nice one marius , never thought of using ` loc ` to index columns .
I would suggest using ` all ` on the boolean condition on the entire df rather than use ` apply ` : #CODE
You can then use this to mask the columns to return which column you wish to drop from ` df2 ` as shown above .
Although maxymoo's answer is correct generally one should avoid using ` apply ` if there is a method that is vectorised and can operate on the entire df which this does
But when I tried to do something like this on my ` DataFrameGroupBy ` object , it threw an error : ` Cannot access callable attribute ' astype ' of ' SeriesGroupBy ' objects , try using the ' apply ' method `
` ffill ` and ` bfill ` replace ` NaN ` values not 0 values , this is by design , you'd have to replace your ` 0 ` values with ` NaN ` values in order to achieve your desired result
Pandas data frame : replace nan with the mean of that row
To replace ` np.nan ` with the mean of the row in which the ` np.nan ` occurred , I used the fillna method as follows : #CODE
The following works , you can calculate the row-wise mean and pass this as the values to replace the ` NaN ` values , you have to transpose the mean so that the alignment is correctly performed : #CODE
@USER I think he needs to unstack the `` industry `` column in order to apply `` rolling_corr `` on the result .
@USER if you apply df.corr() to the current structure , all you get is a correlation of all the columns , eg : correlation between mean and max .
@USER - you can use ` pivot ` to get the industries as columns and rows as indexes .
You could do this by performing an appropriate ` unstack ` followed by a regular ` rolling_corr ` .
` unstack ` the appropriate index level using the above link as an example .
But don't worry , using unstack is working correctly .
@USER The choice of `` unstack `` vs .
`` pivot `` depends on the index structure , in this case .
Using unstack ( level=-1 ) followed by the correlation works .
Don't worry it's probably just me not using pivot correctly .
Edit : Oddly it did not replace the ` inf ` value at [ ` 1431183600x96x30 ` ` 76921 ` , ` yellowCard `]
The ugly way would be to regexp replace the string , like this : #CODE
Firstly I'd ` merge ` df1 and df2 on the ' Name ' column to add the weight column to df1 .
Then I'd ` groupby ` df1 on name and apply a ` transform ` to calculate the total weight change for each person .
` transform ` returns a Series aligned to the orig df so you can add an aggregated column back to the df .
Then I'd merge this column to df2 and then it's a simple case of adding this total weight change to the existing weight column : #CODE
So the above groups on ' Name ' , applies a ` shift ` to the column and then ` cumsum ` so we add the incremental differences , we have to call ` fillna ` as this will produce ` NaN ` values where we have only a single weight change per Name .
I have tried using append and join , but neither option has worked so far because the one dataframe has an ' industry ' and the other doesn't .
I have managed to merge them correctly using : #CODE
Check out the [ `` merge ``] ( #URL ) command - it's incredibly versatile , and let's you choose which columns to merge on the left and right , index or not ; also what to do with merged columns .
Thanks @USER I have managed to merge them correctly but have run into a problem .
@USER , I updated my example to explain better how to use the cut / qcut functions .
Pandas - How can I set rules for selecting which duplicates to drop
What I want to do is drop the values that have the same index ( date time ) , but I want to make a rule like :
I have tried using groupby and apply in several different ways but I cant get it to work .
Note that by using ` transform ` here , we get a boolean mask which allows us to select all rows -- including those with duplicate values of ` kwh ` -- which have the minimum distance from ` X ` .
You could use ` del df [ ' dist ']` to drop the dist column when you no longer need it .
Though I was wondering if you could do it immediately using lambda , apply and groupby .
I am sorry I am trying to insert code into comments I cant do it
You can do this with merge .
More info if you search " Brief primer on merge methods ( relational algebra )" in the pandas docs .
That said , you can translate the same expression in R using an anonymous ` function ` and ` apply ` : #CODE
I have to say this is weird way to apply a function to a pandas df , anyway this is an example which shows what it does : #CODE
The lambda expression is calling ` apply ` and passing ` axis=1 ` which means row-wise and test each named column for whether the expression is True or False , the ` ( 0 , 1 )` casts this to an ` int ` , otherwise you'd get a boolean dtype returned .
with codecs.open ( filename , encoding = encoding_type , errors = ' replace ') as csvfile :
In the Matplotlib example you can try to replace #CODE
I want to transform this into a melted dataframe suitable that has a single column for mean values and a single column for standard deviation values , where the ` id_var ` is ` name ` .
is there a more concise way or a different format to keep dataframes in to make them melt friendly for purposes of breaking data into mean / stdev ?
I then apply this #CODE
To make a weighted random choice of the rows , use the answer in this question ; specifically , make a weighted selection of ` range ( len ( df ))` with the weights given by ` df [ Conversion Rate ]` .
To join the resulting dataframe with the second one , use ` merge `
and use ` groupby / apply ` : #CODE
I'm not sure offhand , probably with a merge but there's a couple of complications : ( 1 ) not unique identifiers so you'd have to come up with a way to fake that ( sort first , then save index values ) , ( 2 ) you need a rule for which ones to drop .
For example , you need to drop one occurrence of " 2 " , but which one ?
I'm pretty sure it can be solved with a merge / join but just not sure exactly how to do it off the top of my head .
For example , suppose I want the aggregate size per exchange for a large table of trades .
i want to make an index called " time " to index these dataframes , and then aggregate values from columns " a " and " b " across the time index . how can you do this in pandas ?
does this use multiindex under the hood ?
` pd.concat ( ..., keys= ... )` returns a DataFrame with a multiindex .
the documentation to concat is impenetrable and its hard to find examples of this relatively simple task in the docs
Or pass ` axis=0 ` to ` loc ` : #CODE
How do I drop an entire row from from the DataFrame when encountering .nan in this feature ?
As to why your method failed , you were calling ` apply ` on a Series ( ` df [ ' ID ']` is a Series and not a df ) and there is no ` axis ` param so the following works : #CODE
Ah I see the problem so did you try ` na_values =[ ' NaN ']` or did you use replace ?
Alternatively you can ` replace ` them using : #CODE
Working on the database of matches I would like to retrive the rating of both players and apply two functions ( i already have them defined ) predicted_result ( rating1 , rating2 ) , and updated_rating ( rating1 , rating2 ) .
The input sequence to map is a list of dicts .
Have you cared to read ` eval `' s [ documentation ] ( #URL ) at all ?
I mean , I am able to preserve the input if I pass eval copy ( dat ) instead of just dat , but I am not sure if that is the most efficient way to do what I want ( dat is much bigger in practice so I'd like to avoid copying it ) .
@USER I edited the code to reflect the indents and also pasted in the new code that should replace that mess .
For those wanting to get to the finish line with a ` DataFrame ` , I constructed a ` dict ` to include the group ( ` _ ` in @USER ' s ` for ` loop , ` grp ` in @USER ' s approach in the question comments ) , used ` DataFrame.from_dict ` and ` melt ` to get it into the format I wanted , then ` map ` to pull teams out of the tuple .
It looks like it's binding the function object as the column value rather than unpacking it to a dict , what I'm doing above is to return the ` value_counts ` as a list and then call ` apply ` again to unpack the single element list .
This forces the dict to be unpacked into a single element list in the initial ` apply ` call : #CODE
I'm trying to do apply simple functions to mostly numeric data in pandas . the data is a set of matrices indexed by time .
I wanted to use hierarchical / multilevel indices to represent this and then use a split-apply-combine like operation to group the data , apply an operation , and summarize the result as a dataframe .
Pandas multiindex and pytables ... separate indexes or one concatenated index ?
What is the structure of a pandas multiindex on HDF5 when the data frame is saved to HDF5 through pytables ?
But I don't seem to be familiar enough with the details under the covers to know from your answer if the two-field multiindex is stored as two ( or three ? ) separate indexes or one concatenated index .
I'd say it looks like they are separate indexes ( one for each field in the multiindex ) and not one concatenated index .
But that is too clunky to keep having to change on-the-fly , and anyway , it would only replace the ` .head() ` functionality , but not the ` .tail() ` functionality .
Not sure how robust this is , but you can sort ` df2 ` so it's index is decreasing , and use ` asof ` to find the most recent index label matching each key in ` df `' s index : #CODE
I do realize I could generate a mask that would select everything by gathering together all the values along the appropriate axis , but this is nontrivial and adds a lot of code .
Of course , you can't do it quite this easily if you don't trust the input - if there could be script in your data which eval might pick up , etc .
How to produce & join combinations in a pandas dataframe ?
First join on [ ' a ' , ' b ']
For missing cases join on [ ' a '] and pick first value .
So far I've been able to produce combinations and do first join using the following code : #CODE
I figured there might be a way to "` map `" the read data if I first fed the ` pandas DF ` with all column headers , then I map each file against the columns in the main file .
Once i used ` read_csv() ` , then ` to_csv() ` will look at the main merged file and "` map `" the available fields to the correct columns in the merged file .
Try using the pandas concat [ 1 ] function , which defaults to an outer join ( all columns will be present , and missing values will be NaN ) .
Here is a complete example that demonstrates how to load the files and merge them using ` concat ` : #CODE
1 ) ` groupby ` on ' id ' and call ` apply ` on the ' vehicle ' column and pass method ` nunique ` , you have to subtract 1 as you are looking for changes rather than just an overall unique count : #CODE
2 ) ` apply ` a lambda that tests whether the current vehicle does not equal the previous vehicle using ` shift ` , this is more semantically correct as this detects changes rather than just the overall unique count , calling ` sum ` on booleans will convert ` True ` and ` False ` to ` 1 ` and ` 0 ` respectively : #CODE
I knew a lambda had to be used somehow , just didn't think of using the shift function .
How to concat two DataFrames by comparing overlapping columns in pandas ?
I've tried looking into the ` merge `
, ` join ` and ` concat ` methods , but I didn't find any solutions to my question -- the lack of an absolute time in order to compare the two objects seems to be the problem .
and then using ` concat ` .
This is fraught with danger , you have no method to uniquely determine which values overlap as your time values clash in both df's , you could try to determine if a range of values match and then concat the remaining values ** if ** you were sure that this is correct but you have a small range in df1 that overlaps with df2 and really there isn't a sane method I can think that would not overwrite or lose values in the merge
Alternatively load the entire thing into a single df and then slice the df to get the 3 separate dfs and then ` concat `
So you could use map for obtain your result .
what are " l = [ 1 , 2 , 3 ]" and " ml = map ( set_negative , l ) # ml = [ -1 , -2 , -3 ]" for ?
Are just for ilustrate the use of map function : ` l ` is a list containing the integer values 1 , 2 and 3 and ` ml ` is a list we have obtained as the result of apply the ` set_negative ` lambda to ` l ` .
Merge two files in Python PANDAS ?
I already know how merge 2 files using Python - I am looking forward to achieve this job in ` PANDAS ` particularly .
Why do you want to merge the files in pandas
It seems like pandas either expects apply to return a scalar for each column , or a vector of the same length as the column .
Another version that works the same as EdChum's answer , but splits within ` apply ` , instead of within ` np.percentile ` : #CODE
Well here is a way of doing it , first load the list into a df , then add the ' occurrence ' column using ` value_counts ` and then ` merge ` this to your orig df : #CODE
Then you join the grouped dataframe with the original one using the pandas.merge function .
That returns a boolean mask rather than the values where that mask is true
@USER ah I get it I thought the OP wanted the values rather than the mask to use , at least that is my understanding now
Merge pandas dataframes where one value is between two others
I need to merge two pandas dataframes on an identifier and a condition where a date in one dataframe is between two dates in the other dataframe .
I need to merge this with this dataframe B :
In SQL this would be trivial , but the only way I can se how to do this in pandas is to first merge unconditionally on the identifier , and then filter on the date condition : #CODE
It seems that it would be much better if one could filter within the merge so as to avoid having a potentially very large dataframe after the merge but before the filter has completed .
To be honest that would be the way I'd do it , the merge params allow only exact matches on index or columns not using a special criteria like you want to the best of my knowledge
To some extent one should not expect SQL like functionality when it comes to dfs especially as you can filter the dfs before / after the merge , you could add a request as I'm not a dev but they are very responsive though
I think the problem is always about points in time and timespans and supporting operations with these kinds of objects on join would allow for lots of new solutions .
After that you can fill your DataFrame with these objects , and join on the columns they live in .
Important note : I do not subclass datetime because pandas will consider the dtype of the column of datetime objects to be a datetime dtype , and since the timespan is not , pandas silently refuses to merge on them .
` PointInTime ( 2015 , 3 , 3 )` should also have been included in this join on ` Timespan ( datetime ( 2015 , 2 , 1 ) , datetime ( 2015 , 4 , 1 ))`
I tried to make all PointInTime equal to each other , this changed the behaviour of the join to include the 2015-3-3 , but the 2015-2-2 was only included for the Timespan 2015-2-1 -> 2015-2-5 , so this strengthens my above hypothesis .
I will use this column as the second level of a multiindex .
You can pass param ` axis=1 ` to ` apply ` so that it process each row rather than the entire column : #CODE
In the current implementation apply calls func twice on the first column / row to decide whether it can take a fast or slow code path .
In the current implementation apply calls func twice on the first group to decide whether it can take a fast or slow code path .
Perform a ` groupby ` on ' id_col ' and then a ` transform ` passing function ' min ' , this will return a Series aligned to your original df so you can add as a new column : #CODE
So the ` groupby ` and ` transform ` is faster on this dataset , I expect it to be significantly faster on your real dataset as it will scale better .
What I would like to do is append the contents in ` column1 ` in ` input.csv ` to ` output.csv ` for each row .
In my opinion you should read the entire csv as a df , then ` apply ` your crawl method on column2 and create the new column and then write the df to your output : #CODE
using apply .
I am trying to map transaction to specific location by updating transactions dataframe .
apply pandas qcut function to subgroups
I tried to create and insert a converter function for the Date field to format the dates correctly , but for some reason it applied after Pandas had already read the date incorrectly , and thus formatted incorrectly .
Except , for this data set , there are two days of data where the instrument apparently started taking data at a freq of a sample per minute , instead of a sample every 10 minutes .
if you remove it , you will be able to use the hist function .
However , I find that in many cases for discrete data I want ` value_counts ` rather than ` hist ` , because ` hist ` bins the values , whereas ` value_counts ` actually counts each individual distinct value .
even ` firstperiod.megaball.value_counts() .sort_index() .plot ( kind= ' hist ')` produces that same incorrect chart ...
@USER : Because ` hist ` * bins * the values , it does not necessarily display a separate bar for each individual value .
@USER : You could use ` hist ` .
If the ' Search term ' ( index ) CONTAINS `' american brewing '` or `' americanbrewing '` , apply the label `' Brand '` , else apply `' Non-brand '` to a column with the header ` Label ` .
Where ` True ` , apply ` Brand ` , Else , apply ` Non-brand ` to the ` Label ` column .
You could convert the ` index ` to ` Series ` and apply transformations .
pandas matplotlib .plot ( kind= ' hist ') vs .plot ( kind= ' bar ') issue
That's because the " hist " plot is not just plotting data , but actually first estimating the empirical distribution of the raw data and then plotting the result .
That is , " hist " is going to bin the data , count the instances per bin and plot that , so there is no need of doing the ` value_counts() ` ourselves .
Now I want to transform it to a new dataframe like #CODE
For a large dataframe , it's must faster to use ` map ` rather than a for loop : #CODE
No matter which of the given alternatives I apply - it just doesn't work .
Pandas set value in dataframe , multithreading - loc vs .
I can't figure out a way to do this in a single loop , the problem here is that you want some kind of rolling apply that can then look at the previous row , the problem here is that the previous row update will not be observable until the ` apply ` finishes so for instance the following works because we in run the apply 3 times .
how to drop dataframe in pandas ?
But I want to drop the whole dataframe created in pandas .
like in R : rm ( dataframe ) or in SQL : drop table
Hi Many thanks for all the answers , I have tried to apply the df.ix [: , : ' 4 '] .apply ( lambda x : x.isin ( df [ ' 1 '])) .all ( axis=1 ) .sum() to the last 4 columns of my example ( columns 2 , 3 , 4 and 5 ) by using df.ix [: , : ' 5 '] .apply ( lambda x : x.isin ( df [ ' 2 '])) .all ( axis=1 ) .sum() and have checked by hand ( with a dataset that provides me an answer greater than 0 , but keep getting a different result .
Pandas duplicate columns with a shift
There are some ways to append columns to a pandas data frame .
I have tried pivot / unstack / stack which im sure is the correct way to accomplish this , however I have been unable to get the desired shape .
I am fairly certain pivot / stack / unstack needs to be used but I just can't get it working to create the correct dataframe shape as per the original question ( with each exchange+symbol having a list of ' close ' values underneath )
After loading your df you can select a specific row using label indexing ` loc ` : #CODE
like ` df [ ' new_col '] = np.arange ( len ( df ))` ?
You then need to reindex this new series to fill in any missing days : #CODE
If you want to combine multiple , I suggest you use the map function : #CODE
to_latex() produces incorrect result with MultiIndex DataFrame
The ` df.to_latex() ` method prints the data shifted down one row with respect to a ` MultiIndex ` ( but does the right thing with a single-level index . ) #CODE
I want to replace all the NaN with some random values like .
In a Python Pandas ` DataFrame ` , I'm trying to apply a specific label to a row if a ' Search terms ' column contains any possible strings from a joined , pipe-delimited list .
In the case that data do exist , then , is there a cleaner method than : ` df = pd.DataFrame ( data =( data if len ( data ) else None ) , columns =[ ' a ' , ' b ' , ' c '])` to set it ?
My solution was to bring in just the columns needed to find the duplicates I want to drop and make a bitmask based on that information .
Then , by knowing the chunksize and which chunk I'm on I reindex the chunk I'm on so that it matches the correct position it represents on the bitmask .
I find , however , that its performance does not appear to stack up against pickle .
pandas map and join multiple columns
I need to pass 5 columns from my pandas df and join the resulting 5 values into the same row .
I need to pass each df row into that function and the values in the resulting dict and bool to be inserted in the same row ` ( cleaned_BP , cleaned_Pain , cleaned_Pulse , cleaned_RR , cleaned_Temp , BOOL isCleaned )` and run this in all my df .
You could do everything with your map and lambda : #CODE
Another solution would be convert the messages into their CountVectorizer sparse matrix and join this matrix with the feature values from the posts dataframe ( this skips having to construct a dict and produces a sparse matrix similar to what you would get with DictVectorizer ): #CODE
Rather than doing a very long one-liner , I think 4 separate assignments which are masked using ` loc ` would be more readable : #CODE
It would be possible to rewrite the above into a multi-nested ` np.where ` statement but I find that once you get above 3 conditions it becomes difficult to read and there isn't much to gain performance wise against separate ` loc ` statements
optimize pandas query on multiple columns / multiindex
I want to consolidate it into : #CODE
It doesn't look like ` str.contains ` supports multiple patterns , so you may just have to apply over the rows : #CODE
You can call ` unique ` on column ' A ' and then join with ` | ` to create a pattern for matching using ` contains ` : #CODE
For your final version , remove all the ` plt.draw() ` , ` raw_input() ` , ` plt.ion() ` and append a ` plt.show() ` at the end .
I aggregated on pivot tables some time series information , so each pivot table have columns labeled 2015 , 2014 , etc .
I want to compare each pivot table , so I'm plotting them on the same axis : #CODE
Creating tz aware pandas timestamp objects from an integer
If you time is in UTC , but you want it in another tz .
Additionally , seaborn is built with pandas in mind , so I'm surprised that its heat map function doesn't suit your needs .
Lastly , if the main purpose of this question is to learn how to generate an image of a heat map , I recommend removing the bits about weasyprint since that seems orthogonal to the heat map question .
If / when i substitute in kind= " hist " , the chart values get all out of whack .
I've tried this as well , to no avail ( gives me a blank hist ): #CODE
Both ` Series.plot ` with ` kind= ' hist '` and ` pyplot.hist ` generate a histogram from raw data , but it looks like you're passing in pre-binned data .
My goal is to be able to do a map on the a column , but with a knowledge of the related index .
If you're looking to perform some kind of timestamp calculation using your index you can call ` apply ` and access the index using the ` name ` attribute : #CODE
Is there a way to apply to a pandas dataframe while threading state ?
My goal is to be ably to do an apply , but one which maintains some state .
Now , I know that I can just have a variable ( and be aware that apply is called twice , I believe ) , but I'm wondering if there is a more idiomatic way to do this ?
" maintains some state " <--- what does this mean and how does ` apply ` not currently achieve this ?
` shift ( 1 )` compares the previous row , next row would be ` shift ( -1 )`
MY advice would be to save yourself a ton of pain and install an environment that comes with the numpy / scipy / pandas stack bundled already such as [ anaconda ] ( #URL ) or my personal choice [ winpython ] ( #URL )
How to apply different aggregation functions to same column by using pandas Groupby
We get something multiindex by level ' A ' and ' B ' and one column with the mean of each group
@USER I'd say that question is different , the OP here is asking how to apply multiple different functions at once , not to generate multiple columns from a single function
basically call ` agg ` and passing a list of functions will generate multiple columns with those functions applied .
To save a bit of typing it's possible to pass just strings of the function names to ` agg ` , for example ` agg ([ ' mean ' , ' std ' , ' count '])` ( at least in newer versions of Pandas ) .
I have been going through the help docs , and trying various experiments ( groupby() , multiindex , value_cuounts() ) .
I'd perform a ` groupby ` on the ' DATE ' and ' GROUP ' columns , then call ` transform ` on ' STATUS ' column and call ` value_counts ` / ` count ` , ` transform ` will return a series aligned to your orig df , so it allows you to add it back as a new column : #CODE
I can do drop [ ' X ' , ' Y '] followed by drop_duplicates() to get it to the desired format .
The problem is that I have to apply this to a bigger data set ` len ( df.index ) = ca .
Also , Stack Overflow already prepends the tag to every page title so Google will handle it better .
But I just get the error ` ValueError : cannot reindex from a duplicate axis
What I am looking to transform the dataframe into : #CODE
Alternatively , you ` groupby ` on `' Col X ' , ' Col Y '` and ` unstack ` over ` Col Y ` , then fill ` NaNs ` with zeros .
Flexible relative value calculation in python / ValueError : cannot reindex from a duplicate axis
All I get is the error : ` ValueError : cannot reindex from a duplicate axis
I'd load the csv and parse the ' DATECHANGE ' column to a datetime , then ` set_index ` to this column , call ` resample ` and pass param `' fill_method=ffill '` to perform a daily resample and then ` reset_index ` .
You have to temporarily set the index to the ' DATECHANGE ' column as resample only works with datetime like indices .
You can ` shift ` the name column and then take a slice using ` iloc ` : #CODE
@USER Cunningham : How would I apply this ?
How do I append the original columns to this data so I can see the original data and not just the line number please ?
To extract the city , I've used a nested list comprehension together with ` join ` .
You then just need to join them with a space .
For naming a group insert ` ?
So I want to join the second frame to the first based on ` Key1 ` and ` Key ` but the first DataFrame is larger and I still want to retain the columns that don't match ( with ESD as Sand ) in the new DataFrame .
Eventually I will merge anything with an ESD of Sand too .
Would perhaps a join instead of a merge be what I need ?
I don't really care if the duplicate column heading carry over to the join , I will just remove them ( there are two columns named Species ) .
it is better to merge on a list of columns : #CODE
since you can merge on ` [ ' ESD ' , ' Species ']` directly , it is better to avoid the
You can use ` df.drop_duplicates() ` to drop duplicate rows .
Then merge these geometries with the original dataframe on matching ids : #CODE
i needed to drop the time and just do a date comparison , so to extract the date i use .date() #CODE
Re-shape pandas dataframe stack / unstack
Tinkering with stack / unstack / melt and shifting columns into indicies etc , but have not been able to achieve my goal .
I want to stack all of ` [ ' HOUR1 ' , ' HOUR2 ' , HOUR3 ']` into a single ` column = HOUR ` .
Similarly , I want to stack all of ` [ ' PRICE1 ' , ' PRICE2 ' , ' PRICE3 ']` in a single ` column = PRICE ` , such that the value in this field is aligned with the corresponding value in the ` HOUR ` column .
Drop the hours and prices levels from these stacked DataFrames .
This even occurs if you force it to a bool dtype from the getgo : #CODE
I want to add subtotals to my dataframe : groupby some index level then append new dataframe to the main one .
UP D: that's because of manually added multiindex level " ????
@USER The goal is to append subtotals ( got with groupby-sum operation ) to the main dataframe .
So i recreate multiindex to be the same in both dataframes .
pandas does not append to df as it should of each line of iteration
First use a ` groupby ` to get the aggregate function that you want , in this case a flag against each date and value : #CODE
Then merge back to the original dataframe to have your aggregates displayed against each row : #CODE
The true / false column does not have a column name , so how would I rename it and then merge it back into the original dataframe ?
What is the syntax for pivot tables in Pandas ?
First create an empty dataframe with the timestamp index that you want and then do a left merge with your original dataset : #CODE
I'm trying to append new values to a pandas series inside a while loop .
I tried using a pivot table #CODE
Not sure if a pivot table will account for this .
However I have issues with first making it a dataframe : df = pd.DataFrame ( byqualityissue ) df.pivot ( index= ' CompanyName ' , columns= ' QualityIssue ' , values= ' 0 ') next with KeyError : ' 0 ' , and finally I'm not sure if a pivot table will work since each company only which calls it DID get , but doesn't necessarily label a zero for the types it did not get .
[ unstack ] ( #URL ) should help you in this situation .
Index it by both Company and Quality Issue , then unstack it on Quality Issue .
Finally , replace the ` Nan ` values that occur because no matching data were found #CODE
I used unstack to reorganize my data , replaced NaN values with a 0 , added up all the rows and appended a new column with those values , then sorted .
` mask = np.r_ [ df.columns.get_loc ( ' A ')]`
` mask [ np.where ( mask == True ) [ 0 ] [ -1 :]] = False `
` df.iloc [: , mask ] = 0 `
As far as I can tell , aggregate only applies a function on a given grouped column , and I don't know how to get it to apply a function that involves multiple columns .
How can I pivot the results , from within BQ , to produce output as follows : #CODE
As some additional background , I actually have 2000+ categories that I need to pivot , and the quantity of data is such that I can't do it directly through a Pandas DataFrame in Python ( uses all the memory , then slows to a crawl ) .
After trying out the accepted answer below , I found that trying to use it to create a 2k+ column pivot table was causing " Resources exceeded " errors .
See the manual for other pivot table example .
Tried doing the pivot on my laptop with 32GB of ram and it just chokes .
When the file is cut down to ~ 1.04 million rows , my program works properly .
unclear what you're trying with this line ` sales = pd.concat ( df , ignore_index=True )` you're trying to concat a single df with nothing else , there would be no difference in just using ` df ` by itself or assigning ` sales = df ` , also you can't concat a single df , you need to pass an iterable such as a list ` sales = pd.concat ([ df ] , ignore_index=True )` which is pointless as this is a list of a single df
So what you are saying , is that I have to have multiple lists in order for concat to be used .
This operation will only apply to groups that are larger that 2 in the original dataframe .
I would try a slightly differnt tack : Pivot the table so that you have a column for each value in ` col2 ` containing the dates and the values of ` col1 ` as the index .
Given a DataFrame with a hierarchical index containing three levels ( experiment , trial , slot ) and a second DataFrame with a hierarchical index containing two levels ( experiment , trial ) , how do I drop all the rows in the first DataFrame whose ( experiment , trial ) are not contained in the second dataframe ?
One way to do it would by using ` merge ` #CODE
You just need to reindex ` merged ` to whatever you like ( I could not tell exactly from the question ) .
This makes sense ( I used a similar approach using the join method , but was thinking that somehow I could get the reindex method to do what I wanted ) .
If you don't mention the column ( e.g. `' value '`) , then the keys in the dict passed to ` agg ` are taken to be the column names .
Pivot the sample values for each group . aggregate by max ( pValue )
Get the corresponding Status , desc corresponding to the sample with the higher pvalue and replace its value with a pValueString .
For pandas pivot table , you pass the rows you want as ` index ` and the columns you want as ` colums ` : #CODE
How do I calculate the standard deviation with a pivot table in Pandas ?
I want to use pivot tables in Pandas to have it split up the data by sport , and for the corresponding value for each sport have the mean " number " value for all people who play that sport .
I can do this pretty easily with pivot tables , but if I wanted to do the same thing for calculating the standard deviation , I cannot figure out how .
Are pivot tables not advisable for doing this task ?
I tried an alternate solution which involved ` apply ` ing a ` lambda ` function to each element of the ` Series ` but that took longer .
Merge two daily series into one hour series
I have two daily series and I have to merge them into one hour series with the 1st series for the first 12 hours and the 2nd series for the remaining hours .
How do I create a pivot table in Pandas where one column is the mean of some values , and the other column is the sum of others ?
Basically , how would I create a pivot table that consolidates data , where one of the columns of data it represents is calculated , say , by ` likelihood percentage ` ( 0.0 - 1.0 ) by taking the mean , and another is calculated by ` number ordered ` which sums all of them ?
In STATA in which I mostly program , I would either use replace and if ( tedious ) or loop if I have many categories .
Btw , you could do the same thing in stata with ` egen ` and ` cut ` .
Glad that ` cut ` is useful here .
@USER that could work but if df [ ' a '] was nonzero to begin with then this would give the wrong answer .
Now get the distance from points to lines and only save the minimum distance for each point ( see below for a version with apply ) #CODE
( taken from a github issue ) Using ` apply ` is nicer and more consistent with how you'd do it in ` pandas ` : #CODE
Why is concat reformatting my headings ?
However , in using ` concat ` ( I think this is where the issue is , anyway ) The output ` CSV ` file has been changed to ( 0L , ' HeadingTitle ') .
Then when you call ` unstack ` , #CODE
2 ) How do I apply a function to a set of columns to remove SettingWithCopyWarning when reformatting DATA columns .
I sort and then shift a unique I'd .
If that ID matches the ID , then I shift up date .
Also another column to determine what the reason of the visit was , also just another shift .
If " dt " is your DatetimeIndex object : #CODE
rolling sum of the ` condition ` column within each group
I know I can do it with a custom apply , but I'm wondering if anyone has any fun ideas ?
Using an apply ( even with a fast lambda ) will be orders of magnitude slower on any real dataset .
As the apply is essentially a python loop .
The problem is , that I am getting this error form the last line of code , where I try to apply the function with ` df.apply ( flex_relative , axis =1 )`
The only thing I found so far was the link below , but calling a R function won't work for me because I need to apply that to quite big datasets and I may also implement an optimization in this function , so it definitely needs to be built in python .
if its always perfectly sequential why couldnt you just do ` range ( len ( df [ col ]))` ?
Why not just simply replace the column , with ` frame [ col_name ] = range ( frame [ col_name ] .shape [ 0 ])` ?
Getting No numeric types to aggregate error when using pivot_table
I know this should be really easy but I keep getting errors when trying to pivot .
This tells me ' No numeric types to aggregate
You don't need the line ` pdres =p dres.set_index ([ ' pricedate ' , ' hour ' , ' node '])` , as ` pivot ` works on the columns .
In the end we append the value of the days played to a list which we can attach to our original dataframe .
The following should be more optimised , basically I'd ` groupby ` on the team , apply a boolean test of whether the difference in the datetime is equal to a timedelta of 1 day .
Then for where this is True then apply a ` cumsum ` on this and add 1 .
then concat and taking the mean : #CODE
Finally concat them and calculate mean #CODE
Then I think you can get what you want by combining ` groupby ` and ` pivot ` : #CODE
Then we ` pivot ` : #CODE
So the problem is with your ` tweet [ ' text ']` ( inside the ` map ` function ) in : #CODE
Also , as a side note , consider list comprehensions as an alternative to ` map ` and ` lambda ` : #CODE
Sorry I didn't know that , please forgive me this time as i am new to stack .
I have the data ( from MySQL ) loaded into pandas dataframes , and now need to apply some matching function where the rows are matched if the time of measurement is within some limit ( time 8s apart ) .
This seems like a ` join ` condition that is much much better expressed in the database prior to loading into Pandas .
If you have it in MySQL , just do a join based on the condition you want directly , like ` select * from table1 a join table2 b on ABS ( a.dt_stop - b.dt_stop ) <= 8 ` -- or use fancy datetime functions as needed .
I realize I haven't done the apply stage , which is probably how I will generate new rows and new columns , but I don't know the next step or if there's a cookbook example for something like this .
Merge two time series with offset ?
and would like to merge them in order to get : #CODE
Finally , concat : #CODE
One method would be to convert the time strings to datetime but only take the time portion and then call ` apply ` and call ` datetime.combine ` to produce your datetime for both columns : #CODE
I get the following message : ' AttributeError : ' Series ' object has no attribute ' dt ''
I want to take Equipment Quality from the original file and replace the space with an underscore .
You probably want to replace with ` import matplotlib.pyplot as plt ` and ` %matplotlib inline `
One way around this is to strip the cells , but there must be something better .
How is this done typically on stack overflow ?
To get the columns containing the coefficients for each combination of date , expiry and symbol you can then merge ` df ` and ` coeffs ` on these columns : #CODE
I forgot to reset the index on coeffs before the merge .
It complained because it was trying to merge on ' date ' , ' expiry ' , ' symbol ' but these formed the index of ``` coeffs ``` rather than columns .
I am generating a pivot table report using the pandas Python module .
Return multiple objects from an apply function in Pandas
I'm practicing with using ` apply ` with Pandas dataframes .
So , I'd like to use the 2nd dataframe , ` DFa ` , and get the dates from each row ( using apply ) , and then find and sum up any dates in the original dataframe , that came earlier : #CODE
Obviously I'm new to ` apply ` and I'm eager to get away from loops .
I just don't understand how to return values from apply .
I don't think apply is best option for this .
My main problem is trying to return from the apply .
@USER ' Brien : The performance of DF.apply ( func , axis=1 ) is comparable to calling func in a loop . apply is useful when you want to align the output into a single DataFrame .
Also , note that ` apply ` returns a ` DataFrame ` .
There's a bit of a mixup the way you're using ` apply ` .
Once you make the changes , as ` foo ` returns a scalar , then ` apply ` will return a series : #CODE
10 number combinations and see how they stack up against the posted odds .
For python , there is something called the pydata stack .
That being said , I see no questions about any other package in the pydata stack that parses CSV , so it's fairly safe to assume , this is how people do it in python .
Use a boolean mask to filter your df and then call ` str ` and slice the string : #CODE
You can mask it on your criterials and then use pandas wonderful string handling methods #CODE
I've read the docs about slicers a million times , but have never got my head round it , so I'm still trying to figure out how to use ` loc ` to slice a ` DataFrame ` with a ` MultiIndex ` .
First , let's strip out the name ( ' id2 ' in this example ): #CODE
Here is another version using ` pandas ` built-in function ` stack ` .
It depends , it should be a separate question , you could use ` concat ` potentially
I'm trying to concat the dataframes created by each function but I get NameError .
Well your dfs are local to the functions but you then return them which means you need to append each to a list and then you can concat .
I think that I can obtain the result with the ` pivot ` function but I am not sure of how to do it
In the example below , I load a example data-set from UCI , remove lines with missing data ( thanks to the help from a previous question ) , and now I would like to try to normalize the data .
Here is a cut of the code that I have #CODE
Python Pandas merging two data frames and map one row from one data frame to all rows from the other
For example A , B merge with C gives me all the mapping for A , B , C .
Then merge A and B together on= ' key ' like this : #CODE
For example , is there a way to apply ` dateParser ` directly to the index ( perhaps inplace ) so I don't have to ` reset_index ` first ?
Python Pandas , transform dataframe
I also tried pandas pivot function , groupby , etc ... but I guess my understanding of pandas and python in general is not sufficient for this transformation .
One way to do this is with ` melt ` .
I end up with a list of numpy arrays that I can then apply a function to : #CODE
I've looked at using groupby , but that doesn't work because grouping on the markers column only returns the rows where the markers are , and multi-indexes and pivot tables require unique labels .
We can shift and take the cumulative sum to get : #CODE
Therefore , I join the index of ` count_df ` ( ` left_index=True `) with the ` CompanyName ` column of ` df ` ( ` right_on= " CompanyName "`) .
This seems to do what you want , basically add a count column by performing a ` groupby ` and ` transform ` with ` value_counts ` and then you can sort on that column : #CODE
You can drop the extraneous column using ` df.drop ` : #CODE
I'll study the several uses of loc .
If there are spaces in your column names , you should probably be using converters to strip those out , like you would if importing from a CSV or Excel file .
ValueError : cannot reindex from a duplicate axis using isin with pandas
ValueError : cannot reindex from a duplicate axis
1864 # trying to reindex on an axis with duplicates 1865
if not self.is_unique and len ( indexer ):
-> 1866 raise ValueError ( " cannot reindex from a duplicate axis ") 1867 1868 def reindex ( self , target , method=None ,
ValueError : cannot reindex from a duplicate axis
I'm pretty sure your problem is related to your mask #CODE
The error ` ValueError : cannot reindex from a duplicate axis ` is one of these very very cryptic pandas errors which simply does not tell you what the error is .
This SO thread provides a nice transpose technique to remove the duplicate .
i'm not actually sure I need the .join ( school ); will remove that when i'm back in office with secure connection . the school join is already implicit within the classes .
Afterwards I want to merge the two split series ( actually , after the processing they are now two-column dataframes ) in the order they originally were .
Above I want to merge the annotations column into one .
The index is preserved so you can simply append them and sort them #CODE
the ` notnull ` will filter out the missing values
I think you have ` NaN ` values you could drop these prior to the filter so after the ` frame = fill_rate() ` line do ` frame = frame [ frame [ ' Media '] .notnull() ]` could you try that
I know how do this by loading up the flat file and then manipulating the Series objects directly , perhaps by using ` append ` or just creating a new DataFrame using a manually-created MultiIndex .
You might also look into using MultiIndex , depending on whether you need to do multidimensional transforms .
Is there a difference between a hierarchical index and a MultiIndex ?
I feel like I'm abusing numpy.where() to get around not knowing how to map values from multiple columns of a dataframe to a lambda function in a non-iterative way .
How can I non-iteratively map from two dataframe columns to one
Now that I'm on board as far as loc is concerned , I agree this seems slightly more readable than maxymoo's answer .
For your question about how to map multiple columns , you do it with #CODE
I think this is a one-liner using ` transform ` : #CODE
` transform ` takes the groupby result and broadcasts the result back up to the original index .
How to align the pandas series in ipython notebook ?
So how can I adjust these align in ipython notebook ?
Thanks @USER that is close to what I want , however if I try to merge the means and the data frame I don't get what I'd expect .
I've added code to merge the means into the old data , but see my comment above - it is highly unlikely that you really want derived data together with the old data .
If I replace ` .ix ` by ` .iloc ` instead , it seems to work .
You can just perform a left style ` merge ` : #CODE
DatetimeIndex : 2284 entries , 1958-03-29 00:00 : 00 to 2001-12-29 00:00 : 00 Freq : W-SAT Data columns ( total 1 columns ): co2 2284 non-null float64 dtypes : float64 ( 1 ) memory usage : 35.7 KB
** TypeError : Cannot interpolate with all NaNs .
How use dataframe as map to change values in another dataframe
I have one large dataframe that acts as a map between integers and names : #CODE
Then I have another dataframe where I want to convert the ` Gene ` column to the ints given in the map ( the names in ` to_convert ` can be overwritten ): #CODE
Like I said , what I'd like to do is replace the names in ` to_convert ` with the integer values from ` gene_int_map ` .
I'm sure this is super-simple , but it seems no permutations of options for merge will do it .
I'd also like to replace the values in a one-column dataframe with the integers in ` gene_int_map ` : #CODE
Call ` set_index ` on the ' Gene ' column in ` gene_int_map ` and pass this as the param to ` map ` and call this on your ' Gene ' column on your other df : #CODE
And then replace values ( using ` map ` as suggested by @USER ): #CODE
` AttributeError : ' DataFrame ' object has no attribute ' map '` Your previous answer seemed to work though .
@USER : I changed it because ` map ` is faster than ` replace ` , while still allows the use of a dictionary , which in my opinion is very clean and straight-forward .
apply custom function on pandas dataframe on a rolling window
You want to apply a risk calculation function ( let's say VaR ) named compute_var() on last 90 closing prices , on a rolling basis
By setting the index to ' Disease ' the dfs will align on the index values
They concat to the right , always !
Since your Series has a DatetimeIndex , you could use the ` asof ` method : #CODE
How can I merge them in a single CSV interpolating the File1 and File2 with the time series of FileN ?
When you say interpolate , what do you want to interpolate and how ?
You can convert the int values to a boolean and then use this as a mask : #CODE
If you want to mask out the ' Disease ' column then just use the other df columns to select the cols of interest : #CODE
3 ) Finally ( so far as I've noticed ) , pandas used to truncate floats to a few decimals but now I'm getting the full representation :
It's unclear to me what is your index and what isn't but if the indices match then you can use a boolean mask and then call ` max ` and pass ` axis=1 ` : #CODE
Wait , are both functions meant to apply to individual strings instead of a whole row , or just ` perform_function1 ` ?
They're meant to apply to individual strings .
If you want to apply function to certain columns in a dataframe #CODE
Pandas is quite slow acting row-by-row : you're much better off using the append , concat , merge , or join functionalities on the whole dataframe .
How to apply a function to the elements of a pandas dataframe
I want to apply a lambda function to the elements of a dataframe , in the same way as np.sqrt returns a dataframe with the sqrt of each element .
However pd.DataFrame.apply apply the function to an row or an column .
Is there a similar comand that apply a lambda function on each element ?
I did a set intersection operation to take care of that and got an output array ` result ` containing all the values that are both in ` missing_vals [ ' field ']` and ` data.columns ` .
Now I want to index into ` data ` using each element of ` result ` , check to see if that column contains the value corresponding to the element in ` missing_vals [ ' key ']` and replace it with ` NaN ` .
` for i in range ( len ( result )):
The ` replace ` method replace all occurances with the replacement value and if there are none it does nothing .
I tried Pivot table but it does not solve the problem perhaps it requires to have same number of element in each group .
This is really quite similar to what you are doing except that the loop is replaced by ` apply ` .
The ` apply ` method aligns the various indices for you ( and fills missing values with ` NaN `) .
Answers I founded online , such as ` grouped = grouped.filter ( lambda x : len ( x ) 2 )` return a DataFrame where the data are not grouped .
Create a pivot table : employer names vs caller names .
EDIT2 : A " pivot [: 5 ] .transpose() " will bring you very close to the final table form you suggested , where the top five callers are the table " headers "
I am able to group the data and select the weights for each sex but am unable to simply create a new variable " wt_diff " and have the " wt_diff " appear on each row regardless of sex so that each city / date / sex group would in fact have , on the same row , the weight diff between the sexes .
But I am not sure how to access ` the_row ` from inside the transform .
To avoid the following error , I would like to replace any integer in my DataFrame with Unix Time :
How can I easily replace any non-datetimes with the epoch represented datetime ?
To call ` rebalance ` on a DataFrame other than the one returned by ` setup_df ` , replace ` c = df.colmap ` with #CODE
I'm trying to apply one function ` f1 ` to rows ` [ ' Utah , ' Texas ']` and ` f2 ` to other rows .
I think it would be easier for me either to create separate DFs for each function or to transpose ` frame ` and then ` frame [[ ' Utah ' , ' Texas ']] .apply ( f1 )`
You can do this with the ` map ` method without writing a function or using ` apply ` at all : #CODE
then the syntax would be ` df.apply ( func , axis = 1 )` to apply the function func to each row .
I'm not sure if dis-aggregation is the right step here , but it should in the end allow me to cross-reference with another ' aggregate ' dataset .
A very straightforward way would be to ` concat ` pairs horizontally , concat the results vertically , and write it all out using ` to_csv ` : #CODE
Since isnull will handle this , I wrote this function to apply to data [ col_name ]: #CODE
It might also be of interest to stack the data frame so that you have the dates and times together in the same index .
Here is the full stack trace : #CODE
pandas find max value in groupby and apply function
How may I set my maximum H value ( 4 for Dublin and 5 for Madrid ) as a constant / city in order to apply the function all over the DataFrame ?
Is it possible to specify the order of levels in Pandas factorize method ?
I am using pandas to factorize an array consisting of two types of strings .
AFAICT you can't do that directly with factorize , but it's quite easy to build a ` dict ` ( which you can then use for pandas's ` map ` ) .
Again , you can just use this for pandas's ` map ` .
How to stack data frames on top of each other in Pandas
Finally , I used concat to concatenate the 8 dataframes together ( Stacked on top of each other ) .
And further , have a look at ` concat ` , eg ` pd.concat ([ list of frames ])` stacks the list of frames into one dataframe .
Thus , when pandas does the ` concat ` , it doesn't just append the dataframes to the bottom , it expands the dataframe to have new colums with the right names and then appends the rows .
Does it just rename the columns 1 to 12 or apply some sort of indexing ?
How to apply functions with multiple arguments on Pandas selected columns data frame
What I want to do is to apply a function : #CODE
The ` DataFrame.apply ` method takes a parameter ` axis ` which when set to 1 sends the whole row into the apply function .
This makes it a lot slower than a normal apply function since it is no longer a proper monoid lambda function .
The error message is telling you that you cannot cast a pandas Series to a ` float ` , whilst you could call ` apply ` to call your method row-wise .
You should look at rewriting your method so that it can work on the entire ` Series ` , this will be vectorised and be much faster than calling ` apply ` which is essentially a ` for ` loop .
I tried to concat the Series #CODE
You can achieve this by a performing the concat you started with and extending it to the other axis .
Creating two DataFrames from these series allows you to merge them back together and create the index you want : #CODE
The suggestions of farhawa helps a lot to unify the MultiIndex into a single Index ( which makes sense in my case as the index is a datetime and the indices weren't really multidimensional . Most time the MultiIndex is probably the better solution ) .
and get a MultiIndex #CODE
I want to merge the different data frame in a single one like this : #CODE
` concat() ` does not do resample I think .
I suspect you just want to do an outer merge and then interpolate ?
Can you replace the ??? with actual numbers ( or at least approximations ) and perhaps have some values not equal to 18.8 in the second dataframe .
You can ` concat ` the two ` DataFrames ` , ` interpolate ` , then ` reindex ` on the ` DataFrame ` you want .
Next , you need to interpolate to fill in the missing values .
I interpolate using `' time '` ` mode ` so it properly handles the time indexes : #CODE
Keeping only the time points in that ` index ` is also easy , you just slice using that ` index ` ( you use the ` loc ` method for slicing in this way ) .
Now I want to replace TBA in the first dataframe with corresponding TBA in the second dataframe where the dates match .
If you call ` set_index ` on ` pdata ` to ` date_2 ` then you can pass this as the param to ` map ` and call this on ` tdata [ ' date_1 ']` column and then ` fillna ` : #CODE
I guess I will have to ` apply ` or map a ` split ( " , ")` to the ` Term ` column , but what do I do after that ?
You can use ` str.split ` to do the splitting ( instead of apply and split approach , but similar ): #CODE
Now , to obtain your desired output we have to stack these columns , but first merge it with the genes column to have this information in the stacked frame : #CODE
From here , we can reset the index , rename our column with terms , and drop the integer column ( from the automatically generated column names ) we don't need anymore : #CODE
use ` stack ` followed by ` reset_index ` to convert ` date ` and ` id ` column labels into columns : #CODE
The left join option is in order to keep rows that don't have matches on crawldf .
You can read the documentation for merge here :
so to remove these rows do ` df [ mask ]` .
To filter out some rows , we need the ' filter ' function instead of ' apply ' .
When using panda's ` resample ` function on a DataFrame in order to convert tick data to OHLCV , a resampling error is encountered .
It seems that , by using if_exists= ' replace ' , Pandas drops the table and recreates it , and when it recreates it , it doesn't rebuild the indices .
When I use if_exists= " append " the problem doesn't appear , it is only with if_exists= " replace " that the problem occures .
Best solution is only to append , and create the table with the correct primary keys another way .
Does Pandas concat interpolate missing values in multivariate time series ?
resample time series with python
I have several csv files in every folder and I need to merge all the csv file of each folder in to one csv file per folder .
I want to join both dataframes via the ` id ` column .
Why is ` join ` empty ?
aah , there seems to be an issue with the ID columns . for df1 they are of type object , and for df2 they are ints . hence , join does not work ...
Possible workaround to first create the table , and then use the ' append ' option in ` to_sql ` .
In the end however I found it simpler to just make the table before and append to it .
I have a large mysql table ( name= Table-B ) which I want to do a left join to with my dataframe .
I don't want to load a massive table into pandas just to then left join to df which will return a small fraction of the rows in Table-B .
and then perform the join in the database , and read the result into a new DataFrame using ` read_sql ` : #CODE
The join must be performed either by MySQL or Pandas .
Since we want to avoid loading ` Table-B ` into a DataFrame , the only other option is to perform the join on the database side .
Even if Pandas were to provide a function to make it * appear * as though you could join a database table with a DataFrame , under the hood Pandas would have to move the DataFrame into a temporary database table of move a table into a DataFrame .
You need to unstack your results : #CODE
I did not know anything about unstack .
This question is mostly about needing some CONVENIENT way to assess the outcomes of different variations of commands ( eg , to assess how different attempts to aggregate the data even work !!! )
conditional replace based off prior value in same column of pandas dataframe python
I'm working with a pandas dataframe and looking to fill / replace data in one of the columns based on data from that SAME column .
in excel what I'm trying to do would be " =IF ( AND ( A2=0 , B1=-1 ) , -1 , A2 ) so that I could then drag down column ' B ' and that would apply .
Pandas interpolate : slinear vs index
In Pandas interpolate what is the difference between using ` method ` of ` slinear ` vs ` index ` ?
Which will change your ` df ` , without raising a warning and there is no need to merge anything back .
I know I can use s.value_counts() / s.count() on a series to get the % status distribution per column , but how do I aggregate over multiple columns ( i.e. , combine T_1 / T_3 , T_2 / T_4 since I know they belong to a particular test type )
ix will keep line 6 to 52 base on your index .
First , I pull all my Opportunity data , then I pull all my Account data , and then I merge .
However I would like to optimize this process by only pulling data for account [ ' ID '] that exists in the oppty [ ' AccountID '] column , as opposed to pulling the entirety of the Account data before a merge .
This should extract the data using a left inner join , which omits unmatched rows in both tables .
It also does the join in SQL , reducing the amount of database traffic ( and therefore also network bandwidth ) by having the database server do the work , thereby reducing the computational load on your desktop client system .
shift index of different dataframe
Transforming pandas data frame using stack function
You could use ` melt ` on ` State ` Column like #CODE
One way would be to create a dict mapping the ` unique ` values in your ID column to your new sequential values and then call ` map ` passing in this dict : #CODE
This seems very wasteful why not just construct a df with the correct number of additional columns and then ` concat ` ?
I think the problem is that on the third iteration the code barfs because although you allow duplicates , you now have 2 columns with empty strings as column names but this then fails the internal checks as it's expecting a unique column to append but it now finds 2 , if your column names were unique then this would work but again , why not just construct a df with the correct dimensions and concat once
Rather than inserting a column at a time , I'd create a df of the dimensions you desired and then call ` concat ` : #CODE
you can create a dataframe based on your list , and merge with your dataframe #CODE
Is it better to define a function ( to split and then to turn the values into minutes ) and apply it to each column instead ?
I am hoping someone will provide a function I can use to apply / map everything to several columns at once efficiently .
You could just measure this but generally ` apply ` should be the last resort as it doesn't scale as well as its a for loop and if called on a series it executes per row .
With respect to turning it into a function so you can apply to 15 columns you put your code for you last method into a function and then call ` apply ` on a df , this will call it for each column but it will try to execute the function on the whole Series
I would imagine the dt accessors are the best / fastest way : #URL
And is it better to have a separate line of code for each column I am doing this to , or is it better to define a function and apply that to each column ?
just wrap it up in a small function , and either apply it across the columns , or since you prob don't have too many of these , just do it per column .
Completely remove one index label from a multiindex , in a dataframe
I tried with ` drop ` #CODE
Note : executing the cubic spline interpolation via the apply function takes quite a mount of time ( about 2 minutes in my PC ) .
If you file is larger you could also create the dataframe as you go and reset the OrderedDict each time to avoid storing all the data in the dict also , you just have to append the last group outside the main code , we can also use itertools.islice to get all the slices and use itertools.izip to zip if using python2 : #CODE
This answer is going to be a lot like @USER ' s deleted answer ( using ` unstack ` instead of ` pivot_table ` , but they're equivalent here ) with one extra step at the end : #CODE
and then we set the index and unstack : #CODE
Merge multiple pandas columns into new column
In R , I use the " cut " command and then plot the x , y of the cut .
For the follow-up questions , we can do something more powerful using boxplot .
I'm trying to apply a weighted filter on data rather the use raw data before calculating stats , mu , std and covar .
Using pct instead of absolute return would remove the non-stationary if you use EWMA properly ON A ROLLING WINDOW .
reindex a pandas.panel with a MultiIndex
I was trying to reindex a Panel with a MultiIndex .
But whenever I try to reindex , like in this example I got this strange behaviour .
g [ nongrp_cols ] .apply ( lambda d : d.apply ( lambda s : s.value_counts() ) / len ( d.index ))`
pd.DataFrame.groupby.apply really gives us a lot of flexibility ( unlike agg / filter / transform , it allows you to reshape each subgroup to any shape , in your case , from 538 x 122 to N_categories x 122 ) .
But it indeed comes with a cost : apply your flexible function one-by-one and lacks of vectorization .
How do I apply the date subtraction operation , then combine ?
You can use apply like this : #CODE
FWIW , using ` transform ` can often be simpler ( and usually faster ) than ` apply ` .
` transform ` takes the results of a groupby operation and broadcasts it up to the original index : #CODE
vlookup equivalent to join 2 tables using pandas
using vlookup in Pandas using join ): #CODE
@USER - I referenced the post you suggest as duplicative , and tried to apply the .map() function it recommends .
You can use the merge function from the pandas library as follows :
You can read about the merge function here : #URL
getting all corresponding max values in pandas pivot table
Pivot the sample values for each group . aggregate by max ( pValue )
Get the corresponding Status , desc corresponding to the sample with the higher pvalue and replace its value with a pValueString .
Create your pivot table separately : #CODE
Finally merge the two together #CODE
Then get the max pValue per Group / Sample ( in pivot table form ) .
Finally , set the index of the previous DataFrame to the index of the new one and join .
Drop a level from the column index , then drop the name : #CODE
If this is possible , is there a way for me to specify a cut off index after which this behavior would take place ?
For example if the cut off index would be the key 1 , the result would be : #CODE
This depends ; you will have to take out a sheet of paper and calculate the error your overall statistics will get if you don't interpolate and just zero-fill these NaN .
Just find each NaN , and linearly interpolate to the adjacent four values ( which is , sum up the values at ( y +- 1 , x +- 1 ) ) -- this will seriously limit your error enough ( calculate yourself ! ) , and you don't have interpolate with whatever complex method is used in your case ( you didn't define ` method `) .
In case of NaN's , some of the averaged values will be NaN ( every averaged pixel where a NaN was in the neighborliness ) , but you don't care -- unless there are two adjacent NaNs , the NaN cells that you want to replace in the original matrix are all real-valued .
Then you just replace all the NaNs by the value in the averaged matrix .
OItherwise I'm stuck at the initial problem : What's the fastest way to interpolate over missing values along the z-axis .
I am going for 1D interpolation ( for every NaN interpolate from z+-1 ) .
Then find all the indices of of * NaN * and replace them with the according value from the pre-calculated " mean-array " ?
Ok this seems like it should be easy to do with merge or concatenate operations but I can't crack it .
Some methods I've tried are to select the rows that are in one but not the other ( an XOR ) and then append them , but I can't figure out how to do the selection .
The other idea I have is to append them and them delete duplicate rows , but I don't know how to do the latter .
You want an ` outer ` ` merge ` : #CODE
The above works as it naturally finds common columns between both dfs and specifying the merge type results in a df with a union of the combined columns as desired .
Python pandas HDFstore append dataframe with missing column
The problem is how to append dataframe with missing column .
Say column ' c ' is missing , I try to append only [ a , b ] , I cannot do it directly because the dataframe ` cannot match existing table structure `
I try to reindex the dataframe including column ' c ' , and it still doesn't work , because the new column ' c ' is all NaN by default , and its dtype is float64 , still doesn't match .
Now I am stucked , how can I append this partial column dataframe into HDFstore ?
The reason for that error is you use ' or ' to ' join ' two boolean vectors instead of boolean scalar .
The only thing I can think of is to either generate the dirct for each row where you can drop the ` NaN ` values , or to parse the json dict and strip the entries out , I don't think dfs will allow a form where the dimensions are different for each row .
creating a list is necessary here otherwise it will try to align the result with your original df shape and this will reintroduce the ` NaN ` values which is what you want to avoid : #CODE
drop the NaN value when converting DataFrame to dict , and then
Or would you recommend sticking with a multiindex ?
Concat Columns produces NAN even though axis is the same for all datasets
I am trying to concat columns from multiple dataframes .
Alternatively , you can ` unstack ` the ` symbol ` index on the inputs before the ` concat ` to get the ` symbol ` as the column names .
Then , after the ` concat ` , you can overwrite the column names with ` [ ' AUD ' , ' CAD ']` .
Some groups don't contain all periods , and for these groups I want to append x rows with the missing periods in it .
You can set the index to ' ID ' and ' PERIOD ' and then construct a new index by generating the product of both columns and pass this as the new index to ` reindex ` , this has an optional ` fill_value ` param which you can set to the str ` NONE ` : #CODE
You can unstack the results on ` PERIOD ` and then stack them back with the ` dropna ` option set to False .
To compare a series with itself shifted by a single row you call ` shift ` so you can replace your code with ` MyFrame [ ' A '] !
It's unclear what you're attempting but you can compare the entire Series or df by calling ` shift ` : #CODE
Then unstack the multiindex to get a DataFrame and set null values to 0 to get the final results : #CODE
I did something similar but want to get it as a data frame so that I can join with other existing data frames .
The intermediate ' grouped ' is a multiindex series .
The unstack changes it into a dataframe by making it single index .
How to append data to pandas multi-index dataframe
How can I append data to a Pandas Multi-Index DataFrame ?
DataFrame to merge #CODE
So , a ` merge ` + ` groupby ` give #CODE
What if I ran a for loop would their be a way to append and create the dataframe as I go ?
I think what I could do is append eachTicker to the array list then from there I could build a dataframe from the arrays :-D ( epiphany moment )
The error message is pretty clear : " The normalize function assumes floating point values as input , got object " , so you should check what you're reading into your dataset .
I was not able to run your code ( probably missing inputs ) but you can probably transform your ` prices ` list in a list of dict and then build a ` DataFrame ` from there : #CODE
The following snippet should work after your ` apply ( crawl )` .
Either you didn't copy it correctly into Stack Overflow or you've changed it somewhere else .
KeyError when using melt to restructure Dataframe
I used ` melt ` to do this but get the Error ` KeyError : ' years '` and don't know how to handle this : #CODE
pandas dataframe drop columns by number of nan
I'd like to drop those columns with certain number of nan .
For example , in the following code , I'd like to drop any column with 2 or more nan .
So the above will drop any column that does not meet the criteria of the length of the df ( number of rows ) - 2 as the number of non-Na values .
A typo in your code ` len ( df )` should be ` len ( dff )`
When I try to insert a portion of this string into a dataframe I get this error : #CODE
On the other hand if I just insert a string literal like " baloney " I do not get this error .
From what I understand I am following the steps to apply PCA as they should be .
Then you shouldn't pass use transpose in demo_df.T .
I am trying to drop some rows from my Pandas Dataframe ` df ` .
I decided to try and split this Column and then drop the resulting individual columns , but I am getting the error ` KeyError : ' curv_typ , maturity , bonds , geo\time '` in the last line of the code ` df_new = pd.DataFrame ( df [ ' curv_typ , maturity , bonds , geo\time '] .str .split ( ' , ') .tolist() , df [ 1 :]) .stack() ` #CODE
Try to edit the CSV file and replace geo\time with geo_time or something perhaps ?
I'm not sure how to fix that , perhaps use SED to replace all the commas with a \t ( tab ) char and then try to read it in ?
on a * nix machine , can use the ` tr ` command to replace those commas
But is there a way to replace the commas with tabs in a script as I need to automate the whole process ?
You should always treat the boolean series as a mask to use against the original series / df
How to merge two DataFrames of unequal size based on row value
But I want to conceptually understand how to join two unequal sized DataFrames with different " indexes " based on a column name .
I tried merge and concat and join but dont get the result I want .
A default ` merge ` works fine here , assuming your index actually is your index : #CODE
Here the ` merge ` looks for common columns and performs an ` inner ` merge on those columns .
You can be explicit and specify that you want to merge on the ' Name ' column : #CODE
I'd ` concat ` using a DataFrame ctor : #CODE
You need to assign the result of the concat so ` df =p d.concat ([ df , pd.DataFrame ( columns=list ( ' BCD '))])`
Can I append the columns to the last column ?
It seems like concat is doing automatic reordering because my original columns are moved around as well .
` reindex ` will return a new DataFrame , with columns appearing in the order they are listed : #CODE
The ` reindex ` method as a ` fill_value ` parameter as well : #CODE
I have several pandas data series , and want to train this data to map to an output , df [ " output "] .
OK , first let's prepare your data set , by selecting the relevant columns and removing leading and trailing spaces using ` strip ` : #CODE
If you have too many levels for this to work , or you want to consider the individual words in ` catB ` as well as the bigrams , you could apply your ` CountVectorizer ` separately to each column , and then use and use ` hstack ` to concatenate the resulting output matrices : #CODE
So for instance I cut from a frame : #CODE
It will be faster I believe to use the vectorised ` str ` method to split the string and create the new pixel columns as desired and ` concat ` the new columns to the new df : #CODE
Pandas append list to list of column names
I'm looking for a way to append a list of column names to existing column names in a DataFrame in ` pandas ` and then reorder them by ` col_start ` + ` col_add ` .
To avoid the ` KeyError ` on the capitalised column names , you need to capitalise after calling ` concat ` , the columns have a vectorised ` str ` ` title ` method : #CODE
Using ` apply ` check if value is list ` isinstance ( x , list )` and take the value , and then ` apply ( pd.Series , 1 )` to split as columns #CODE
I would like to take a given row from a DataFrame and prepend or append to the same DataFrame .
Rather than concat I would just assign directly to the df after ` shift ` ing , then use ` iloc ` to reference the position you want to assign the row , you have to call ` squeeze ` so that you assign just the values and lose the original index value otherwise it'll raise a ` ValueError ` : #CODE
To insert at the end : #CODE
This also happens for multiindex columns .
Is there a proper way to update the index / columns and drop the empty entries ?
To be clear , I want the filtered dataframe to have the same structure ( multiindex index and columns ) .
I do not want to drop the level .
It returns only the selected values in the index , and as such it does not conform to the original dataframe anymore .
Notices that the ` labels ` in the ` MultiIndex ` are different .
If you changed the levels , then you would need to update the labels of the MultiIndex every time you slice .
This also happens for multiindex columns .
proper way to update the index / columns and drop the empty entries ?
Consideration ( 2 ) is that as mentioned above , if you trim the multiindex you can't merge any data back into your original , and also its a bunch of nonobvious steps that aren't really encouraged .
The underlying fundamental is that index does NOT return updated contents for a multiindex if any rows or columns have been deleted and this is not considered a bug because that's not the approved use of MultiIndexes ( read more : #URL ) .
The valid API access for the current contents of a MultiIndex is get_level_values .
Two adjacent cells might both be merged , but in different merge groups .
I've taken some steps towards this , which included using ` melt ` in the code below , but for some reason that got rid of my ` Date ` Column and resulted in the dataframe above .
Can you not add the currency identifier following the ` melt ` ?
unstack the new sites to be columns
I'm trying to merge two data frames , testr and testc , but I keep getting a Key Error on " Channel ID " and not sure what the problem is .
Here is my code to merge with .info() on each dataframe : #CODE
I currently have a dataframe as follows and all I want to do is just replace the strings in ` Maturity ` with just the number within them .
For example , I want to replace ` FZCY0D ` with ` 0 ` and so on .
` a [ ' Names '] .str .contains ( ' Mel ')` gives you a series of bool values #CODE
` a [ ' Names '] .str .contains ( ' Mel ')` will return an indicator vector of boolean values of size ` len ( BabyDataSet )`
How to replace commas with tabs in TSV File
In my dataframe below I am trying to replace the commas in the column ` curv_typ , maturity , bonds , geo\time ` with tabs and in the strings below it too , so that I can then create new columns from this .
I searched around and found similar questions / answers , but none that I was able to apply .
Did you only add this ` + range_name_list [ ix ] + " | "` or did you also make other changes ?
Yep , so I changed ` df [ ' Range '] = ' | '` so that the default is a pipe rather than an empty string -- this way the string in the range column ends up being ` |condition1|condition2|etc| ` based off the change that you noted , which also included taking out the break statement ( ` + range_name_list [ ix ] + " | "`) .
To avoid this , you only perform the indexing when the length > 0 ( ` if len ( ts.value_counts ( sort=True )` will evaluate to True if it is not None and the length is greater than zero ) .
I want to use interpolation to replace some missing values in dataframe .
Is there a chance that you can replace those licensed data with some randomly simulated data and upload an artificial sample file to the Stack Overflow website ? to work out a sample code and test it , I need the real structure of your datafile .
I am trying to replace the strings in the ` Years ` column of the Dataframe below with just the numbers in the string .
I could get all Wednesdays of a week , using df.resample ( ' W-WED ') , but I cannot merge them back correctly to the original data set so that I can compute cumulative product of returns for a week starting on Wednesday by PERMNO and DATE .
How can I merge back and fill in correctly the dates ?
I was wrong in the beginning thinking that I should create a new data set containing all dates that fall on Wednesday , and then merge it with the original , bigger data set , filling missing values with the appropriate Wednesday dates to create a groupby variable .
I tried using replace , lstring-rstring but I'm not able to replace the extra characters from thr readingtime column #CODE
Tried loc as well but not getting errors
I'm trying following code - I'm trying following code , is there any more efficient way ? for i in range ( len ( da2 )): da2.iloc [ i , 3 ] = da2.iloc [ i , 3 ] .__str__() [ #URL Can you suggest better way ?
You can apply a lambda function to the column of the data frame , extracting the date from the dictionary via ` x [ ' $date ']` , and then just take the date / time portion ( ignoring the time offset ) .
apply function throws an error = DataFrame ' object has no attribute ' datetime ' Checked Pandas version , its Also dt.datetime should be df.datetime ( just a typo ) right ?
No , I've imported datetime as dt ( see edit above )
This adds the extra column ` name_count ` , which you can drop with ` summary_df.drop ([ ' name_count ] , axis=1 , inplace=True )` .
What the ` apply ` function does , is that for each row value of ` df [ ' A ']` , it calls the ` applyFunc ` function with the parameter as the value of that row , and the returned value is put into the same row for ` df [ ' B ']` , what really happens behind the scene is a bit different though , the value is not directly put into ` df [ ' B ']` but rather a new ` Series ` is created and at the end , the new Series is assigned to ` df [ ' B ']` .
You could use ` str.extract ` to search for regex pattern ` BULL|BEAR ` , and then use ` Series.map ` to replace those strings with ` Long ` or ` Short ` : #CODE
Pandas : Conditionally append substrings " y " + " z " to column B if column A contains " x "
If it does not contain ` Xn ` or ` nX ` , then insert `" 1 "` .
You can use ` df.asfreq ( ' 1d ')` to resample your data based on the date column and fill in the missing values automatically .
I would like to identify `" AAPL "` in column ` Name ` , pass that through a dictionary `" AAPL " : " Apple "` and then insert that into a string in new column ` Description ` .
Note : I perhaps need to set up another column to inform whether the leverage is positive or negative and use that to decide whether to append `" - "` in front of the multiplier as ` -10X leverage .
You can define an explicit function to apply to the entire ` Name ` Series .
If you want to add more words to replace just add them to maps and the same with signs .
@USER Shouldn't I apply ` split ` as : ` df [ ' Description '] = df [ ' Name '] .map ( split )` to run it ?
Not sure where ac_result comes from but if you have adat like that then just split and strip to get names price then pass the list , try this code and see how it works .
Pandas map to DataFrame from dictionary
and call that for all instances when I need to map company names .
How can I make it ` map ` for matches in ` companies ` instead of ` lambda x ` ?
Then map your replacements in a dictionary : #CODE
Then replace : #CODE
This will replace all instances as defined by the dictionary , but it will not exclude the rest of the string as ` .map ( lambda x : " American Express " if " AXP " in x else ""` .
Can I make replace discard the rest ?
Applymap is for the whole df , is you ; re doing just a single column ( series ) , then just use apply .
But actually I just replaced that line using ` loc ` instead of ` np.where ` , probably a more standard way to do it .
Beyond that there's a few different ways to do that line including ` loc ` and ` np.where `
Pandas : Difference between pivot and pivot_table .
According to the pivot documentation , I should be able to reshape this on the score_type_name using the pivot function .
Additionally , I want the struct_id to be for every row , and not aggregate in a joined row like it does for the table .
So can anyone tell me how I can get a nice Dataframe like I want using pivot ?
Additionally , from the documentation , I can't tell why pivot_table works and pivot doesn't .
If I look at the first example of pivot , it looks like exactly what I need .
Not sure , but I think the ` pivot ` v .
I usually use stack / unstack instead of pivot , is this closer to what you want ?
I'm not sure why your pivot isn't working ( kinda seems to me like it should , but I could be wrong ) , but it does seem to work ( or at least not give an error ) if I leave off ' struct_id ' .
The stacking helps , but I still need it like my desired output , so I can actually join stuff to it .
I will mark this soon if no one can tell me why pivot doesn't work .
I'm curious too if anyone can explain the issue with pivot !
I wish to have the values of ' Original_Level ' cap to the level where another column ' NS ' hits a trigger ( in this case abs ( NS ) > = 4 ) .
The below df shows a cap of 13.122 and 50.887 when abs ( NS ) > = 4 #CODE
I am looking for a generic solution , that works away from the lowest abs ( NS ) level in both directions to hit both the -4 and the +4 trigger .
An additional note , it will always be true that the abs ( NS ) continues to grow in size from the min ( abs ( NS )) level .
You want to use ` clip ` for this , firstly find the indices of your upper and lower clip values using ` idxmax ` and ` idxmin ` and then pass these values as the params : #CODE
Sorry you're trying to apply to a groupby object ?
Notice that if you unstack the ` id ` index level of ` df ` then you get : #CODE
You could do a pivot such that the male and female pay for the same job are on the same row .
I'm plotting several histograms using Pandas ` hist ` method .
Interested to apply multivariate hexagonal binning to this and different color hexagoan for each unique column " ball , mouse ... etc " . scikit offers hexagoanal binning but cant figure out how to render different colors for each hexagon based on the unique data point .
interested to apply hexagonal binning to this #URL
Columns name dropped on append in pandas
I need to join dataframes like the ones shown above , but columns name ` hello ` ( it is not a column as it may seem ! ) is dropped on append operation .
UP D: ` concat ` / ` append ` drops axis 0 / 1 names when they differ .
So , i think , forcing ` .names ` after ` append ` is the best solution now .
i think names should be preserved on append if they are the same , otherwise dropped silently
It is similar to ` df1.update ( df4 )` but using a ' outer ' join manner for unseen records in ` df1 ` .
So the above uses ` rolling_sum ` and ` shift ` to generate the previous 2 years sum and we then divide the citations value by that value .
How can I either drop completely empty columns or delete all columns bar the first 11 ?
To drop all the columns after the 11th one .
Maybe draw something in paint if it is about plotting or an example dataframe if you are trying to transform your dataframe into something easily plottable .
Pandas Merge , can't get my dataframes to match up on the key correctly ?
I have two dataframes that I want to merge on a column named Channel ID , this column exists in both dataframes , of course .
Since this is an inner join , rows with NaN for ID can be dropped ( so you don't need the float type to handle NaNs anyway ) .
It appears to contain variables from both datasets , so isn't that a successful merge result ?
I would suggest reposting but being super clear in the following ways : post data before and after merge , show EXACT code you are running , and then show results after running code and EXACTLY what is wrong .
I transformed it with the help of @USER -tavory using pandas melt function : #CODE
drop rows with equal mult indexes
Is there a short-cut to drop rows with equal multi indexes ?
merge the dataframe on ID #CODE
Use this as a mask to get to the desired ` dfC ` in your question .
The ` merge ` did the trick , but I thought it was more usefull to just do a ` dfMerged.dropna() ` after the merge and that will be the set with the difference .
yes , essentially , the answer was really about the ` merge ` method , which allows you to sql-like joins .
I want to completely reindex the data dataframe so each row now has a unique index value .
which gave the error " ValueError : cannot reindex from a duplicate axis .
So in the end I did find a way to reindex like I wanted .
I'm not sure I fully understand why that happens ( I guess pandas wants to map unique values to unique values ? ) .
Pandas multilevel concat / group / chunking
It is the ` agg ` line that is generating the error .
All you need in a ` Pipeline ` is any object with a ` fit ` and ` transform ` method that returns an array and takes the right positional arguments ( array ` x ` , one-d optional array ` y `)
loc and iloc helps .
Are you sure you don't mean ` range ( 1 , len ( DF )): ` ?
Python Pandas DataFrame resample daily data to week by Mon-Sun weekly definition ?
Let's say I resample the ` Dataframe ` to try and sum the daily data into weekly rows : #CODE
Interestingly , I was unable to name the columns ` animal ` and join with ` suffix = ( "" , "")` -- that throws an error ` ValueError : columns overlap but no suffix specified : Index ([ u'animal '] , dtype= ' object ')` .
Is there a way to drop columns in a Dataframe with column names having a particular letter as I wasn't able to find any information on this ?
I want to drop all column headers having the letter ` F ` in them .
I have a dataframe of data that I am trying to append to another dataframe .
No need to iteratively append one line at a time .
Have you looked at the [ examples in ` append `] ( #URL ) ?
It performs a sort of merge with the existing values ( according to the indices ) , and those are floats , so it must convert these too .
You should replace it with something else , depending on your problem ( ` fillna ` ? ) .
thank you but I still can't merge the dataframes correctly :(
You could drop rows from ` frame ` where ` Channel ID ` is NaN : #CODE
As Ami Tavory explained , you can't drop the NaNs solely from ` frame [ ' Channel ID ']`
I wish to have the values of ' Vol ' cap to the level where another column ' NormStrike ' hits a trigger ( in this case abs ( NormStrike ) > = 2 ) .
I am looking for a generic solution , that works away from the lowest abs ( NormStrike ) level in both directions to hit both the -2 and the +2 trigger .
An additional note , it will always be true that the abs ( NormStrike ) continues to grow in size from the min ( abs ( NormStrike )) level as it is a function of abs ( distance from spot to strike )
There are 2 issues , first , it did not work after groupby and second , if a particular group of data does not have a NS value that exceeds the " clip " value then it generates an error .
I looks like that you are trying to backfill current record with an observation in a future date ?
I assume if the clip has been triggered , then NaN will be put .
You can replace it by your customized choice .
. .the remaining issue is that I do not know how to replace NaN with the closest Vol level .
Similar to the situation where you have to interpolate yield curve .
Using Jianxun's routine i am forces to have the trigger be abs ( 2 ) on both sides .
Is it because you're printing ` loc ` and not ` iloc ` ?
No , iloc is used when you are integers indexthis case ' i ' while loc is used when you want to use label index here ' distance '
and then ` unstack ` to move the ` type ` index level into the column index .
I get this error for every additional table ( class ) included in my join .
These guys seem to be talking about a related issue , but they use a different pandas method to bring the dataframe in and want to keep duplicates , not drop them .
Anyone have thoughts on how to implement a similar styled function , but drop the duplicates as the query comes back ?
In most cases this is innocuous as the columns are simple the join keys .
[ CDATA [ hello ]]` is interpreted as ` lt ; hello gt ; ` .
However -- you can't tell from the parsed object tree whether your document contained a ` CDATA ` section with literal ` ` and ` ` or a raw text section with ` lt ; ` and ` gt ; ` .
If you want to read from a file , replace ` doc ` with ` open ( filename , ' r ')` .
Converting multiple columns to categories in Pandas . apply ?
Why does ` apply ` ( ` axis=0 `) return a Series even though it is supposed to act on the columns one by one ?
I would like to use pd.write_csv to write " filename " ( with headers ) if " filename " doesn't exist , otherwise to append to " filename " if it exists .
The write or append succeeds , but it seems like the header is written every time an append takes place .
How can I only add the header if the file doesn't exist , and append without header if the file does exist ?
And if you want to customize this even more , probably best to make the sql table manually , and then use ` to_sql ( ...., if_exists= ' append ')` to append the data to that created table .
perhaps ` np.arange ( len ( df ))` may be more optimal ?
It looks like all you're doing is just filling / adding / merging the quarters data , why not just do a merge ` df.merge ( df_h , on= ' Country ')` ?
I don't understand what you're saying , but basically you should be able to merge the data , can you post raw input data for both dfs and the desired output
I have tried different things , like Graphs and then taking the degree of the nodes , and pivot tables to express the number , but I want an efficient way that will allow me to continue with the processing of the domain after the if occur > 2 statement .
I m trying to do a linear regression on the results of a dataframe groupby by date and aggregate the results on another dataframe .
I'd suggest that you use ` groupby ` / ` apply ` returning a ` Series ` ( see Returning Multiple Values From Apply ) .
The ` concat ` seems to be on the wrong axis
Note the ` axis=1 ` - it means to concat horizontally , which I believe is what you mean here .
It was a typo , the concat is on the axis=1 .
I have the following dataframe ` df ` where I am trying to drop all rows having ` curv_typ ` as ` PYC_RT ` or ` YCIF_RT ` .
Use ` isin ` and negate ` ~ ` the boolean condition for the mask : #CODE
can we use A.shift ( 1 ) in a counter manner that dynamically stores updated results while looping ? as in parallel shift adder ?
Given that neither of those conditions apply here , I don't see a reason to change what you have .
You are changin the columns , but you need to reindex ( otherwise you change the data ) .
I'm attempting to transform a pandas DataFrame object into a new object that contains a classification of the points based upon some simple thresholds :
You could also do it with apply , but it will be slower than the ` np.where ` approach ( but approximately the same speed as what you are currently doing ) , though much simpler .
That's probably a good example of why you should always avoid ` apply ` if possible , when you care about speed .
You could also do this , which is faster than ` apply ` but slower than ` np.where ` : #CODE
I think you can do with with a melt , a sort , a date parse , and some column shuffling : #CODE
Whats the proper way to append the second DataFrame ?
Merge a csv and txt file , then alphabetize and eliminate duplicates using python and pandas
To sort and drop duplicates : #CODE
Set last row in ` Date ` column to ` Total ` ( I used this instead : ` df.ix [ len ( df [ ' Date ']) -1 , 0 ] = ' Total '`) .
Creating pivot table in Python then exporting to excel
I'd like to know if it's possible to create a pivot table in Python and then export that pivot table with all its sorting properties into excel .
Note I don't want just to values to be exported , but the entire pivot table with all its properties .
So when I go to the exported excel pivot table , I can still see its report filter , column label , row labels , and field list .
It should be as if I manually created the pivot table .
Since you added a Pandas tag to the question do you mean is is " possible to create a pivot table in ** Pandas ** and then export that pivot table " ?
by creating a MultiIndex from ` idx.index ` and ` idx.values ` , and then selecting the values using ` df.stack() .loc [ idx ]` : #CODE
The multiindex probably wouldn't work * for me * as I already have a big multiindex in the real application , but another neat idea and example of manipulation there .
The stack and group by sum are just the same .
To make sure that grouping and summing works on a ` DataFrame ` with a ` MultiIndex ` , you can try this to get the same correct output : #CODE
Actually I'd like to insert JPEG images without decoding ( to save place ) into a data frame ( pandas ) when I got stuck at the phase of loading the images properly to able to read back them with PIL .
` img = np.asarray ( map ( ord , img ))`
Thanks for the hint , however mahotas open images with decoding , as I wrote above I'd like to read it without decoding and insert it into an array ( actually into pandas DataFrame ) and save like that , and then read it back as array and only then decode it to an image .
Hopefully , I can read the historical data and have it refer to a dynamic rolling date of 30 days .
Then I would read the new data which is a separate file which is always Month to Date values ( all of June for example , this file is replaced daily with the latest data ) , and concat them to the old dataframe .
My only solution is to read the entire file ( df_old along with the new data I added ) and then drop duplicates and then overwrite it again .
@USER I think that is a deprecated alternative way to drop duplicates .
I would imagine this should be solved with lambda map , but can't figure out how .
I think you might want to use a merge , something like : #CODE
Also , wouldn't it be simpler to simply append the ` ID ` to the ` Lower File Name ` in order to generate a unique name ?
I had to make one modification as it was appending " None " to the non-duplicate file names : ` df.apply ( lambda x : ' { 0}{1} ' .format ( x.ID if x [ ' Lower File Name '] in duplicates else '' , x [ ' File Name ']) , axis=1 )` And yes , it would have been much easier to just append the ID in all cases but unfortunately had to match another file in that format .
The easiest way to " get around " this odd Pandas functionality is to generate a mask using ` df.duplicated ( col_name ) | df.duplicated ( col_name , take_last=True )` .
In pandas you can do a boolean mask , selecting a row only if it is differs from either the preceding or succeeding value : #CODE
Pandas DataFrame casting to timedelta fails with loc
I'm trying to cast a column on a multiindex from timedelta64 [ ns ] to timedelta64 [ s ] , and I also have a multiindex for rows .
Another strange thing , if I filter for one site , and drop the Region so that I end up with a single-level index , it works ...
Your problem is that your data contains ` NaN ` values so you need to drop them using ` dropna ` first : #CODE
You can even replace NaN in different columns with different values .
I would like to replace NaN with different values based on the gender column .
In this case you will have to explicitly iterate over the df and see if you find ` NaN ` then replace it based on the other column :
What s the best way to create and then merge to an existing Dataframe in a loop ?
My loop needs work as I am overwriting the Dataframe each time I open a new txt file as the merge function isn t working as I d hoped .
But I am looking to merge each iteration generated Dataframe with the previous iteration dataframe ... but on a per txt file basis
In what sense do you want to " merge " the dataframes ?
Hi ASGM , ideally i'd like to merge each dataframe based on txt file name , so yeah two dataframes if two txt files .
Sorry , I'm still confused - what do you mean by " merge " the dataframes if you still want two separate dataframes at the end of the process ?
Then merge them at the end .
you want to declare a list of dfs outside your loop and then concat at the end so ` df_list =[ ]` and then after ` df =p d.read_csv ( url , sep= ' , ' , skiprows=2 )` do ` df_list.append ( df )` and then outside the loop ` frames = pd.concat ( df_list )`
You should declare a list outside your loop and append to this then outside the loop you want to concatenate all the dfs into a single df : #CODE
I must say that for me it is confusing to use append and concat a the same time .
I always thought that ` append ` adds new Data Frame as a new columns , and ` concat ` rows or columns if set ` axis=1 ` ....
I have the following two tables and I am looking to insert a dataframe ` df ` into ` Table4 ` only if the ` parametrized_fl ` column in ` Table2 ` is equal to 1 .
How to standardize / normalize a date with pandas / numpy ?
My question is , how can I standardize / normalize data [ ' dates '] to make all the elements lie between -1 and 1 ( linear or gaussian ) ??
UNIX timestamps that we can normalize using a ` MinMaxScaler ` from ` sklearn ` #CODE
You simply need to ` stack ` and then call ` to_dict ` : #CODE
Used the following to unstack the data to prep for plotting #CODE
I was only able to test this now , and it does solve my problem ( I just need to transpose the data frame at the end ) .
Pandas csv / multiindex subsetting
I am trying to read a .csv file that has two rows of header information as a multiindex , so that I can later access a column given 2 identifiers .
` ( null ) Codon Freq minmax Codon Freq minmax ...
It still isn't being read in as a multiindex object , but I can at least read it in with index_col = 0 and subset it with .ix like I intended .
So the above uses the vectorised ` str ` methods ` strip ` , ` extract ` and ` isdigit ` to achieve what you want .
After you changed your requirements ( which you should not do for future reference ) you can mask the df for the bull and bear cases : #CODE
You could either nest another ` where ` or mask the df for the negative case but really you should declare up front what you want or post another question as incrementing your requirements is not good practice
` pv.reset_index() .groupby ( " ??? ") .filter ( lambda g : len ( g )= =1 ) .index .values ` returns numpy array
For example , I can use the function grouped [ ' C '] .agg ([ np.mean , len ]) to generate the statistics on column ' C ' but what if I want to generate these statistics on all columns A - F ?
If not , is there an easy way to iterate over all columns and merge in new aggregation statistics results for each column ?
You don't have to call ` grouped [ ' B ']` ` grouped [ ' C ']` one by one , simply pass your entire groupby object and pandas will apply the aggregate functions to all columns .
maybe simpler to replace first line with ` df3 = df1 [[ ' Lat1 ' , ' Lon1 ']]` so you don't need to drop ' tp1 ' at end ?
As to why you get the ' cannot reindex from duplicate axis ' if you observe what it's returning : #CODE
Then it should be clear why it moans , the index values are repeating and pandas is trying to align the index against your original df , to get around this you can get the raw values as a np array by calling ` .values ` attribute : #CODE
I'm not sure this will work reliably , it'd be better to just use a variable that defined the slice ranges / indices / mask and use this to reference back to the original df
You'd have to expand the ranges to generate a list / array of all index values , you can then pass to ` loc `
After resetting the index , convert the dtype using ` df [ ' date '] = pd.to_datetime ( df [ ' date '])` you should then be able to merge
The reason you cannot join is because you have different dtypes on the indicies .
Now you can merge the dataframes on their index : #CODE
python pandas merge with any one value
i could not find any option to do this fuzzy match , i see merge works with an exact match ??
This is a little messed up , firstly split the ' codes ' column on the separator and ` apply ` ` map ` to each element and pass the other df with the index set to ' code ' , this will perform a lookup , we reduce this to just the codes that match .
We then generate an intermediate df from this , we reapply the splitting to the original df to add the ' pcode ' column , this then allows us to ` merge ` : #CODE
You needed to add an additional `' | '` to join your terms : #CODE
` resample ` is your friend .
The ` TimeGrouper ` method and the ` resample ` method produce identical results .
I would like to map a row-wise function across its columns #CODE
Ideally you want to avoid using ` apply ` if there is a vectorised method , so just ` df [ ' x '] + df [ ' y ']` would work
You can apply a function row-wise by setting ` axis=1 ` #CODE
I think that's the problem , I get same error message for ` np.array ([ 1 . ]) & np.array ([ 0 . ])` but it's fine if I replace those with integers or booleans .
` groupby / apply ` to a columns of ` df ` unless the columns or levels you group by
I'm creating a tkinter gui that will take user input for a variable that then gets passed to SQL and the queried data ( in this case a single column data frame and boxplot ) .
Select your columns from the DataFrame and then apply your function ( possibly a ` lambda ` expression depending on usage ) .
I am sure I am being dense , but if the DataFrame has several columns , then df [ mask ] will return the ( full ) DataFrame filtered by the mask ?!
@USER Notice the ` abs ` function on the days .
The easiest way I can see to do this is to join them all into a single DataFrame , sort the columns by time , then shift and subtract to get the delta : #CODE
Basically , openpyxl version 2 deprecated the style-conversion functions that map dictionary style definitions to the new openpyxl api , but Pandas still uses the old style , and for some reason the deprecation pass-through function errors out .
If you have pandas version > = ` 0.17.0 ` you could try to apply ` pandas.to_numeric ` for each column ( or may be you know suspicious columns ): #CODE
Let's pass ` raw=True ` for ` apply ` : #CODE
use ` groupby ` followed by ` transform ` : #CODE
I am using the below code which gives me the summary of count in the pivot table , #CODE
but what i want is the % of row calculation as in excel pivot when you right click the pivot and select " show value as -> % of Row Total " .
i am trying to manipulate the pivot data which will give me the row total , not the data from the dataframe and what i wanted is " % of row total " .
you can actually just pass ` aggfunc=len ` , since ` len ` is already a function :)
Hi maxymoo in the link you have given they are manipulating one of the column from the dataframe , but my question is different i am trying to manipulate the pivot data which will give me the row total and what i wanted is " % of row total " .
Then you can basically use the solution @USER linked to , but you need to use ` iloc ` or similar b / c the table columns are a little complicated now ( being a multi-indexed result of the pivot table ) .
can't you not simply just check if the ` ix ` returns a Series or String ?
I am trying to plot a dynamically size able bubble ( scatter map ) .
You can definitely put multiple aggfuncs in a pivot table , I'm just not sure offhand about translating all of that to a heatmap .
If you only want the ' CREATE TABLE ' sql code ( and not the insert of the data ) , you can use the ` get_schema ` function of the pandas.io.sql module : #CODE
How can i replace ` [ " x2 " , " Total "]` with ` [ " x2 " , " x2 "]` leaving ` x1 ` as is ?
` .rename ` can replace all `" Total "` values , not just the one I need .
I am trying to perform a some arithmetic operations in Python Pandas and merge the result in one of the file .
If the ` col ` with time from ` path_1 file ` is present then look for ` col [ ' Nos ']` and then replace the ` NAN ` with the subtracted values respect to that ` col [ ' Nos ']` .
Merge the dataframes together ( equivalent to a SQL left join or an Excel VLOOKUP
It will replace all the the spaces with ` , ` .
` apply ` might work well for you here : #CODE
and avoid the need to use the cumbersome ` loc [: , ' val1 ' : ' val3 ']` syntax
Python pandas : Compare rows of dataframe based on some columns and drop row with lowest value
I made a nice small h5 table and it was perfect until I tried to append more data to it ( literally just one day of data since I am receiving daily raw .csv files ) .
If I have append = True does that mean it will create the file if it does not exist , but append the data if it does exist ?
This file is mainly used to quickly read data into a dataframe to join to other .csv files which Tableau is connected to
I have a list of servers , and each server has a number of patches that apply to that server .
I realise there have been a lot of similar questions on here , but I am new to pandas and can't seem to apply the right syntax to my problem using the documentation - can anyone help ?
I'm trying to figure out how to apply a lambda function to multiple dataframes simultaneously , without first merging the data frames together .
My hope is that there is a way to apply lambda to just the underlying dataframes so that I can avoid the cost of stitching them together first , and then dropping that intermediary dataframe from memory before I move on to the next step in the process .
The real goal , is to conserve system resources by avoiding the concat in the first place .
Another possibility is to use ` query ` , but this really depends on your use case and how you intend to aggregate your data .
Can someone please explain to me how to change some subset of the values in a Pandas MultiIndex data frame ?
Merge the two dataframes into one : #CODE
then you can perform a ` groupby / transform ` to count how many items are in each group : #CODE
However , it is slower since the ` groupby / apply ` is doing an addition and division once for each group , whereas #CODE
then you can perform a ` groupby / transform ` to count how many items are in each group : #CODE
However , it is slower since the ` groupby / apply ` is doing an addition and division once for each group , whereas #CODE
So far I can think of ` apply ` followed by ` itertools.chain ` , but I am wondering if there is a one-step solution .
It is far more typical to normalize your data in Python before you send it to Pandas .
The introduction of ` NaN ` is because the intermediate object creates a ` MultiIndex ` , but for a lot of things you can just drop that : #CODE
Group by uid and apply ` value_counts ` to the msg column : #CODE
Perform a ` groupby ` aggregation on the first df , call ` sum ` and then ` merge ` this with the other df : #CODE
The basic key to making stuff using pandas faster is to replace loops done iteratively in python with array operations .
How do I find out which members are in the cluster if I transform the
and then pivot #CODE
This we can transform into a condensed distance matrix via ` squareform ` and feed into the linkage algorithm : #CODE
more over after I write this to csv file how do I transpose all at once ?
I can plot the Heat map with Seaborn very well and with suggestion can get annotation .
Second doubt is now that in ` pivot ` I am giving ` value= ' 00:00 : 00 '` but I need it to dynamically read the last column from the file .
If list of functions passed , the resulting pivot table will have hierarchical columns whose top level are the function names ( inferred from the function objects themselves )
write in chunks as you go and append to the csv , use ` mode = ' a ' , header=False ` after the first write .
The ` if os.path.isfile() ` is not going to cause much overhead and is easy to apply the logic
Actually the best way would be to create a new file each time the program starts , and then append for each pass in the function .
Another option would be to write to a NamedTemporaryFile then use shutil.move to continually replace a file on your file system every n rows or whatever period you want
The argument " a " will append the new df to an existing file and the file gets closed after the with block is finished , so you should keep all of your intermediary results .
This is a possible solution that will append the data to a new file as it reads the csv in chunks .
I am not sure what I am doing wrong here , I am simply trying to call a function with a if-then-else filter in it and apply to a dataframe .
You want to apply it to each row , this will do what you want using apply and a lambda : #CODE
Sometimes using apply or a for-loop to compute row-by-row is unavoidable , but in this case a quicker alternative would be express the calculation as one done on whole columns .
.I just checked , in my application , this is 5.3 times faster than my " def " solution and 2.8 times faster than the df , apply approach .
When I read an excel file in IPython ( or Jupyter to be exact ) , the dataframe seems to be read ok , but I can't display it or work on textual columns on it ( for instance , to merge with another excel when the key is the textual field ) , because I get a " #CODE
In the first group ( Chicago on 5 / 25 / 2015 ) the downside trigger occurs at index = 3 and therefore all values with a larger ( abs ( dist )) on the downside will have the same b level as the value just before the trigger ( index =4 ) .
Slicing dataframe , but len is wrong
I am slicing a dataframe along its indices and selecting a response variable , but the result of len is not correct :
I thought len should return 1000
Isn't len ( df.loc [: 1000 , ' a '] taking the first 1000 rows of df ?
` .loc ` selects rows by index label value ( or by boolean mask ) .
to select the first 1000 rows ( assuming ` len ( df ) = 1000 `) .
I am trying to merge the ` Date-time ` column of all the files into a big data file with respect to ` Ids ` #CODE
Fill in empty place with ` NAN ` and if next file has that value then replace the new value with ` NAN `
You can just replace those buffer ` io.StringIO() ` with your ` file path ` and it should work .
Can someone give me advice on how to translate this basic idea to something clean ?
It is because , if we merge too list with some unique values in each , there will be identical values in each list which will turn into one value instead of two after the set() function .
Now , I would like to apply a fit to each frequency group ( 400 , 800 and 1200 ) and do this efficiently within a loop .
I hope , that there is someone , who could explain , if it is possible to apply the curv_fit routine within a loop and if so - how .
In the above example , how can I select from 100 to 150 and 200:300 without using concat ?
The above operation , has a bottleneck when using pd.concat and can eventually been speed up using np.vstack ... but still I'd like to select the two ranges all at once without copy the underling data as concat would do .
concat #CODE
slice and drop #CODE
You can use ` pd.cut() ` to cut your time-series index into chunks , and then use ` groupby ` to perform your customized calculations .
We therefore group by the date index and aggregate , asking if the user visited on any of the entries for that date : #CODE
You should be able to do this using ` apply ` which is for applying a function to every row or every column of a data frame .
One is to use " for " loop which loops and apply function on all the elements and will create a another series of extended URL but I am not able to , dont know why , I tried to write it like #CODE
If the short urls are a column in the pandas dataFrame , you can use the ` apply ` function ( though I am not sure if they would resume on error , most probably not ) .
Hi Anand - Thanks for help . but it is working same as apply function . as soon as it faces a error it pass completely . error is like HTTPError : HTTP Error 404 : Not Found ....
Please edit your post with your code and full stack trace , telling me the error without the full context is a waste of my time
" taking each entry in the column and retrieving time-series data from files with the same names as the entries " This sounds like you want to do a " join " operation .
In pandas this is done with the ` merge ` function .
I am only on the stack overflow for a few weeks .
I just want to create a simple pivot table on this to calculate the average score for each manager for each day .
It should be a simple pivot off of a string field using mean as the automatic aggregate function on a float field ?
So pivot and pivot_table do work in creating appropriate output .
I am using ` automap() ` to map the table ` yc_node_hist ` to a Class .
In the try , except statement you should replace
There are 2 methods here , first is the assumption that non- ` NaN ` values should be set to ' new ' using ` notnull ` : #CODE
I had to study about cut because I was not familiar with it .
` data_q = data.groupby ( pandas.TimeGrouper ( freq = ' 3M '))`
Case insensitive dictionary for Pandas map lambda function
Below , I use ` lambda x : ` in a function to ` map ` value to a ` pandas ` column if they show up in the dictionary ` benchmarks ` .
A regex match is made for the word in ` symbol ` but doesn't translate with the dictionary .
CANCEL_FLAG should also cut the chain because a value of 1 designates that the contract was cancelled .
The copy , merge , fillna , and rename code is repeated for 5 merged dataframes then : #CODE
I have a maybe a hopefully easy question that I wasn't able to find an answer on stack .
` iloc ` and ` loc ` are probably the most direct analog in terms of syntax and purpose , but are much slower .
I think in a nutshell that the tradeoff here is that ` loc ` and ` iloc ` will be slower but work 100% of the time whereas ` values ` will be fast but not always work ( to be honest , I didn't even realize it would work in the way you got it to work ) .
The speed is in between the ` values ` way and ` loc / iloc ` ways .
Anyway , you're trying to apply a numpy concept to pandas and it doesn't work like that .
Use ` shift ` and ` np.log ` : #CODE
You can replace ` StringIO ( raw_data_string_buffer )` in ` pd.read_csv ` function with your own file path .
You can pass the dict values to ` concat ` , example : #CODE
Could you cut this down to a simpler example ?
My guess for what is going on here is that the filter simple store a mask , but doesn't actually force a copy until it has to .
Try to replace #CODE
I know that there is a nifty pivot function for dataframes from this documentation #URL for pandas , so I've been trying to use df.pivot ( index= ' recvd_dttm ' , columns= ' CompanyName ' , values= ' NumberCalls ')
You need to aggregate the data before creating the pivot table .
( there is no columns named ` recvd_dttm ` and that's why it throws the error ) I think you can replace the last line of the code with ` result.pivot ( index= ' Month ' , columns= ' CompanyName ' , values= ' NumberCalls ')` and it should work .
So maybe I need to just build out a dict of all the columns and map it all after the data frame is pulled ...
and then drop col and rename col1 .
You could express this as a groupby / agg operation : #CODE
I didn't realize that if ` groupby.apply ` returns a ` DataFrame ` , it is added as another level in the ` MultiIndex ` .
Only use the mask on the RHS : #CODE
I could change the functions to convert invalid data to ` NaN ` and then drop ` NaN ` s from the dataframe , but then I would silently be dropping erroneous data as well as those empty lines .
` would only drop lines that had all nans .
I need to merge 2 csv files by one column called ' name ' .
I've been able to merge these files but only losing all the rows containing special characters ( e.g. my merged file will only contain Aahl , Peter ) .
Your problem had nothing to do with encoding , you just used the wrong type of merge .
You wanted to perform a ` right ` merge : #CODE
Take a view of the desired left and right side dfs , then ` rename ` the columns and then ` concat ` them and ` sort ` on ' id ' column : #CODE
loc = self.get_loc ( k ) File " / usr / local / lib / python2.7 / dist-packages / pandas / core / index.py " , line
If the value of ` df.y [ i ]` is always greater than any ` df.x [ n ]` value , then append as ` False ` to ` df.new [ i ]` .
The following works but this really is a loop as it uses ` apply ` : #CODE
That's correct ` apply ` will not scale well , one thing you could do is assuming that all values are not unique you could create a dict of the unique values as keys and the row position values as the values and merge this , this should perform slightly better , your issue here is that you are trying to compare a scalar against the entire df for every row value , there isn't a built in method for doing this so you have to call ` apply ` here
In pandas you can use ` loc ` to mask the df and pass the 2 boolean conditions wrapped in parentheses due to operator precedence as you have to use the ` ` operator when comparing arrays : #CODE
pandas : merge on column of collections.Counter ( or even just dict ) objects ?
I need to perform a merge of two pandas DataFrames using columns with ` collections.Counter ` objects ( #URL ) .
The merge raises a weird error .
I need to merge based on values of two pairs of columns .
Merge with pandas creating new columns ?
I have a dataframe that has multiple references to another ( as if it was foreign keys in SQL ) and so , I want to merge them , in a way that I can have all the info from the second dataframe in the first .
And what I pretend to merge is in a single table like : #CODE
The merge option ( as I am using it at the moment ) is not giving me any result .
Anyway , try to get pk / fk1 / fk2 to be all the same type and it should be a straightforward merge .
Here it is a little bit easier to read and only filling na's on the columns added through the join : #CODE
For example , how are you apply a Month ` Series ` to the table ?
I have a DataFrame with multiindex on the columns .
` master_count.csv ` : expected Output ( Append / merge ) #CODE
` master_count.csv ` : expected output after another execution ( Merge / append ) #CODE
And I was wrong with my print or append syntax .
The idea is that you should make a single column to hold the time information , and then for each preprocess , insert the new data into new rows , and give those rows a value in the time column indicating what time period they come from .
I think you're asking how to apply a function to all columns of a Data Frame : To do this call the ` apply ` method of your dataframe : #CODE
Can I specify a file I know exists within the file , or perhaps concat all of the files into one read ?
Or replace : #CODE
You can first convert the ` df ` to a multi-level index type and then ` unstack ` the level ` Class ` will give you what you want .
Typically you can process the chunk and add it to a list , and then after this for loop concat all processed chunks in this list together .
Have been trying to just replace the NaN values in my DataFrame with the last valued item however this does not seem to do the job .
This method will not do a backfill .
You should also consider doing a backfill if you want all your NaNs to go away .
I am putting my data into a bokeh layout of a heat map , but am getting a KeyError : ' 1 ' .
The pivot table I am using is below : #CODE
This returned ` TypeError : cannot compare a dtyped [ object ] array with a scalar of type [ bool ]
groups ` mask ` by ` df [ ' ID ']` , and assigns ` True ` to all the rows of that group if any value in the original ` mask ` is True , and ` False ` otherwise .
` df.loc [ mask ]` selects the rows where ` mask ` is True : #CODE
` df.loc [ mask , ' Class ']` further selects the column ` Class ` : #CODE
` df.loc [ mask ] [ ' Class '] = value ` may fail to modify ` df ` since ` df.loc [ mask ]` returns a copy .
( The same holds true of ` df [ mask ] [ ' Class '] = value `) .
So instead of using ` [ ... ]` twice , use , ` df.loc [ mask , ' Class '] = ' Wet '` : #CODE
Those might be values of ` mask ` .
The line ` df.loc [ mask , ' Class '] = ' Wet '` will alter values in ` df ` .
The rows are selected by the boolean mask , and the column is specified as `' Class '` .
After all the updates I think what the OP is after is a mask like ` mask.groupby ( df.ID ) .transform ( " any ")` after you drop the ID condition from your mask = line .
I see , as you noticed I was incorrectly printing mask instead of df .
It's column-wise vectorized operation and it is much faster than row-wise apply .
Best way with Pandas to find rows with a select duplicate column values and aggregate them
It's ugly though : nested apply functions .
Is there a better way to test if ` n ` number columns have the same value , and if so apply a function to them or add them to a dictionary ?
For my purposes , I'm looking to aggregate rows that have the same day value , but different poll name value .
I am trying to make a heat map like this one from bokeh :
Ah , yes , that was a bug in 0.15 preventing you to use such an instantiated class ( which is the case when providing a keyword ) , see #URL You need 0.16 for that fix ( or you can apply it yourself as it is only a 2 line fix , see the linked PR )
` hist ` takes a 1-dimensional array .
( see the ` hist ` documentation for more ) .
I am trying to insert data from a dataframe ` df ` into a numpy array ` matrix_of_coupons_and_facevalues ` .
An easy workaround would be to apply ` .values ` to the series first and then apply ` std ` to these values ; in this case ` numpy's ` ` std ` is used : #CODE
Just access the ` dt ` week attribute : #CODE
If you'd have a column of strings , it wouldn't try to apply the sum to them too .
Perhaps a workaround is to use transpose ` x.T.fillna ( x.mean ( axis=1 )) .T ` #CODE
` pd.Series ` have a clip method ( defined in pandas / core / generic.py ) .
` np.clip ` ( defined in numpy / core / fromnumeric.py ) defers to the first argument's ` clip ` method if it has one : #CODE
I did ` print ( len ( df.index ))` at the beginning of this section and got ` 12 ` printed , so I am slightly confused why ` IndexError ` raises .
Must the duplicate rows appear contiguously , or do you want to drop duplicates wherever they appear ?
It seems to me that you are iterating over rows , matching a condition and then drop rows based on the condition that is met .
To find the matching rows and drop them , I could do , #CODE
I have a bunch of tick data that I can successfully resample into time data using : #CODE
Also - when I choose to resample with smaller timeframes there are occasionally bars where the values are N / A because there was no price changes for that period .
Was the error with my code because datetime.date works on a single row , and with functions working on rows one at a time I hvae to do apply ( fun() ) rather than fun ( apply() ) ?
Is there a way to reindex the rollTestLogReturnsData dataframe with intergers so it would look like : #CODE
Does it raise an exception ( if so you should post the stack frame ) or output a result you don't expect ( in which case , you should post the output ) ?
If you try ` type ( pd.isnull ( df [ ' C2y '] [ 0 ]))` , you find that it actually returns ` type ' numpy.bool_ ' ` as opposed to ` type ' bool ' ` .
I think I do understand your code ( would be easier if you provided at least a few lines of your csv ) , and maybe you could use ` shift ` like in this [ SO answer ] ( #URL ) .
Perhaps the easiest one is ( if you want to add the row to the end ) is to use ` loc ` : #CODE
` loc ` expects an index .
` len ( df )` will return the number of rows in the dataframe so the new row will be added to the end of the dataframe .
An exception for this is that if you want all the columns to have the same values you are allowed to have that value as a single element in the list , for example ` df.loc [ len ( df )] = [ ' aa ']` .
Accidentally I found that this solution ( using loc instead of append ) is ** WAY FASTER ** so anyone should consider using it .
You should append Series or DataFrame .
I am trying the merge the datetime series with a repository data while grouping by name and summing the values .
Here is another alternative that might appear simpler ( though learning to apply functions to groups is a great idea ! ) #CODE
For information , Jianxun's answer is faster that this alternative : 1.7ms versus 1.87 ms per loop for mine ( for 1000 loops ) ( 10% diff )
rolling majority on non-numeric data
I'd like to replace every value in column ' a ' by the majority of values around ' a ' .
Here is one way to do it by defining your own rolling apply function .
The last ` reindex ` part tries to align the index with original ` df ` and populates unseen index with ` NaN ` .
` argmax ` , for instance , can lead to a similar pitfall when DataFrames have duplicates in the index .
not sure of another method other then a loop but to keep track of the values just append them to a new list .
Here is one way of approaching it , using the apply method to subtract the first item from all the other obs .
Python merge 2 lists / SQL JOIN
If I have 2 lists or data frame ( pandas ) in python how do I merge / match / join them ?
You want an ' outer ' ` merge ` : #CODE
So the mask returns this : #CODE
You can just add a check in your func if you don't want to mask your df : #CODE
In python 3 you need to use ` list ( map ( ... ))` , I guess .
So just change ` map ( lambda ... )` to ` list ( map ( lambda ... ))`
How to resample a df with datetime index to exactly n equally sized periods ?
I've got a large dataframe with a datetime index and need to resample data to exactly 10 equally sized periods .
So far , I've tried finding the first and last dates to determine the total number of days in the data , divide that by 10 to determine the size of each period , then resample using that number of days .
Another potential shorter way to do this is to specify your sampling freq as the time delta .
I believe the reason is that the ` resample ` implements a ` left-inclusive / right-exclusive ( or left-exclusive / right-inclusive )` sub-sampling scheme so that the very last obs at ' 2015-02-03 00:00 : 00 ' is considered as a separate group .
I want to create a pivot table from a dataframe , but exclude rows which equal a certain value ( in this case , in the column that I'm aggfunc-ing on , but I'd like to have a more general answer ) .
1 ) just create a copy of the original df w / o those values , and pivot on that .
2 ) do that into the pivot table directly , like so : #CODE
[ EDIT ] To clarify , I meant how to set the name of the column where the aggregated values will be in the pivot table , during creation of the pivot , not afterwards , as you would regularly rename columns in any df .
The real question is how to pivot only rows that have ( or don't have ) certain values .
FWIW , I would do it as a method rather than function -- ` df [ mask ] .pivot_table() ` instead of ` pd.pivot_table ( df [ mask ])` -- but I don't think that matters in any real way .
IIUC , you need to merge ` C ` with ` A ` : #CODE
( this will add a column to it ) and then merge the result with ` B ` : #CODE
I will work on trying to find a way to insert new rows for each item in the list .
for some reason I am getting the following error : AttributeError : ' numpy.float64 ' object has no attribute ' endswith ' . the trace back points to 860 if name.endswith ( " * ") : any suggestions ?
You could use ` groupby / transform ` to prepare a boolean mask which is True for the rows you want and False for the rows you don't want .
Once you have such a boolean mask , you can select the sub-DataFrame using ` df.loc [ mask ]` : #CODE
How to merge Python pandas data frames on multiple keys
If I want to do an inner join only if the intersection rows has equal relationship from multiple columns in both A and B , i.e , these columns combined are the matching criteria .
Update : use concat function , with dataframes includes in list as parameter , axis=1 .
So I tried using pandas merge function after converting ` matrix1 ` and ` matrix2 ` into pandas DataFrames .
There should be solutions for that ; one of them by using masked arrays , for example ( mask out extra columns and rows in that matrix , so you can keep infs and nans in the original matrix ) .
Unless you generate a mask with a boolean id e.g
` mask = np.ones ( len ( merged_matrix ))
mask [ 0 : len ( matrix1 )] = 0 `
Use ` groupby / apply ` to sort each group individually , and pick off just the top three rows : #CODE
To do this , we will apply a sort to the resulting data-frame and then only return the top 3 results .
We can do this by defining a sort function that returns only the top 3 results and use the apply function in pandas .
This indicates to panda that " I want to apply this sort function to each of our groups and return the top 3 results for each group " .
I am using a script to interpolate stop times from the format HH : MM : SS into minute int values .
I don't really understand the ' functional requirement ' , but anyway if you have a more complex operation you can use pandas ' apply
you want to access the index inside the ` apply ` function , and I don't think you can
and then use the apply function , #CODE
for example , for i in range ( len ( df )):
for n in ( range ( len ( df )):
In the simplest case , the mask might look like this : #CODE
( likely the simplest way is to remove things with less than 2 occurances ) and then just resample til the spit has both on each side ) , but I wonder if there is a clean method that already exists .
Shouldn't it be ` iloc ` instead of ` loc ` so it works right if the index is not a simple range ?
You need ` apply ( your_func , axis=1 )` to work on a row-by-row basis .
Another way would be to call ` unique ` on the transpose of your df : #CODE
I don't want to drop the column or change it's type just to be able to view the data frame in the variable explorer .
I have a dataframe which has a value of either 0 or 1 in a " column 2 " , and either a 0 or 1 in " column 1 " , I would somehow like to find and append as a column the index value for the last row where Column1 = 1 but only for rows where column 2 = 1 .
Then we set ` C3 ` to be the ` index ` but only where ` C1 == 1 ` ( the ` mask ` does this )
Then we have to mask away all the values where ` C2 == 0 ` , same way we set the ` index ` earlier , with a ` mask ` .
Although why the mask index only where it's true ?
` df.index [ mask ]` =
This looks like a bug in the ` DataFrame ` ctor in that it's not respecting the key order when the orient is ' columns ' a work around is to use ` from_dict ` and transpose the result when you specify the orient as ' index ' : #CODE
This could be done in 2 steps , generate a new column that creates the expanded str values , then ` groupby ` on ' A ' and ` apply ` ` list ` to this new column : #CODE
The intermediate step is necessary as I can't figure out how to get the ` apply ` and ` lambda ` to not try to expand the list and remain 1-dimensional
First reshape the ` df ` using ` pivot_table ` and then ` apply ` ` np.repeat() .tolist() ` .
Save to csv a pivot table get error " Process finished with exit code 139 "
@USER I see that in pivot mode has 272 row .
The ` aggfunc ` argument is supposed to get a function that is used to aggregate for cases where the given ` index ` and ` columns ` combination in ` pivot_table ` gives multiple values .
Because you do not aggregate them , you end up with a dataframe containing arrays as elements .
Using ` pandas ` and the ` resample ` function could make your life easier .
Using resample function
Note for ` np.savetxt ` you'd have to pass a filehandle that has been created with append mode .
when I use ` np.savetxt ` , I think the text file doesnt open in ` append ` mode .
You can ` apply ` ` round ` : #CODE
If you want to apply to a specific number of places : #CODE
To mask the first row off you can just do : #CODE
you just do : ` returnsDf.iloc [ 0 ] = dayPricesDf.iloc [ 0 ] * np.exp ( rollReturnRandomDf.iloc [ 0 ])` there is no need for ` shift `
You can ` groupby ` on ' Symbol ' and then call ` apply ` passing a lambda and use ` shift ` : #CODE
To add this as a new column , you can groupby as above and call ` transform ` on the 2 columns of interest and index the ' Close ' column .
The key is using the ` pivot ` method , which restructures your data by date and time .
Can you just ` df.groupby ( ' months_to_maturity ') .apply ( len )` before you drop the duplicates ?
I am just trying to understand what ` len ` means in your statement above .
` len ( df )` returns the number of rows in a DataFrame .
By using the masks you only set the value where the boolean condition is met , for the second mask you need to use the ` ` operator to ` and ` the conditions and enclose the conditions in parentheses due to operator precedence
Actually I think it would be better to define your bin values and call ` cut ` , example : #CODE
You can now call ` map ` and add your new column : #CODE
( as a side node the pivot example above does not work if Cat_col is not unique )
Append all the dataframe to a list and use ` concat() ` to get the final result outside the for-loop .
The basic purpose was to select a loan by index , pull its total payments received and number of payments to construct a cashflow array , join every month's cashflow from the selected loans and sum the cashflows each month to have one giant cashflow array , and then calculate an IRR off that .
@USER Thanks for the tip , it cut my runtime down by quite a bit !
Python Pandas Conditional Statement 2 Dataframes Diff Lengths
Could I use ` hist ` to get the same result , without counting by pivot ?
Hmm i'm not sure how you'd achieve this with ` hist ` , I only have used hist to plot a single series .
I also tried using concat , but I can't figure out how to graph the concatenated DataFrame on the same graph to compare the two keys .
You're one the right track , but you want ` merge ` rather than ` concat ` .
I have a pandas dataframe , from a txt file , and would like to insert it in an xml doc I'm making .
But I just can't seem to insert the converted dataframe xml into the xml doc made .
I've tried the insert add suggested , and a few variations , but I can't seem to shake the error : ExpatError : junk after document element : line 2 , column 2 Read into it , but not luck .. still not adding into doc created ... anything I could try ?
I will look into the string to xml ( also been looking to a merge option ) .
To calculate the new ' Skew ' column , you can do a ` groupby ` and define your customized ` apply ` function .
Thanks Jianxun , creating a " shift " variable did the trick !!!
Does scikit learn's fit_transform also transform my original dataframe ?
I am using scikit learning's StandardScaler() and notice that after I apply a transform ( xtrain ) or fit_transform ( xtrain ) , it also changes my xtrain dataframe .
I am starting to think it is because my xtrain is a view of my original dataframe , so the transform affects it ..
How to apply line attribute at each group ?
My code is supposed to merge the two tables based on a common column named SearchID . the problem is that the code runs for sometime and then terminates without reporting any error in my eclipse console . the number or rows in each tables is : trainSearchStream| 392,356,948 and
My goal is to join several tables according to a common column then export them to a cvs file .
I tried pandas merge function . but my code would terminate .
` 1 / 2 ID ` is the column head that need to apply UPPERCASE .
How can I apply upper case to the first three letters in the column of the DataFrame ` df ` ?
A key error is generated when you try to access a key in a hash map / dictionary that does not exist . for example #CODE
Hey Joe , that was just an excel column to get an evenly divided x-axis ( Jianxun solved this with ` x = np.linspace ( 0 , 1 , len ( group ))` .
See this Warning Message : ` Warning In the current implementation apply calls func twice on the first group to decide whether it can take a fast or slow code path .
In this case it is a drop in stock prices that triggers an event that has a dummy variable for that is 1 or 0 if the event is triggered or not .
Just find the date difference in days between each date and the previous date and drop if the difference is less than 30 days .
python - replace last n columns with sum of all files
I have 8 csv files with 26 columns and 600 rows in each . now I want to take the last 4 column of each csv files ( Column 22 to column 25 ) , read the files and sum them up to replace all the 4 columns in each file . for example ( I am showing some random data here ):
Now , I want to sum each element of " h , i , j , k " of from these 2 files , then replace the files last 4 columns with this new sum .
We need to call the attribute ` .values ` here to return a np array because otherwise it will try to align on the index which in this case do not align .
In the case of your 8 dfs you can loop over them and aggregate whilst looping : #CODE
Also join the first column accordingly .
The error from 225 to 272 appears recursively until at the very end of the stack trace , when it changes slightly to : #CODE
You should be able to figure out from the stack trace ( that massive load of text which appears when the error happens ) where exactly the error is occurring .
The KeyError ' 1 ' message suggests that your pivot table doesn't have a column with the label ' 1 ' .
Do ` print ( pivot_table.columns )` to see what your column names actually are ( if you have a multiindex columns ) .
lambda custom aggregate function
I am trying to utilize a numpy agg function to find the percent of the total count each of the counts are for each temp .
You might want to do a ` transform ` instead of ` agg ` #CODE
How can I make it aggregate the larger scalar ?
@USER I've added some code to illustrate how to use ` transform ` .
I want the aggregate total by fruit .
One more question - instead of locations , if I want to use the index and a subset of column names to merge values in two dataframes ?
In pandas ' median ' has a special meaning #URL .
However , I was not aware of that and named a data column as ' median ' .
So ` df.ix [ 1 ]` already had a member named ' median ' ?
However , according to your example once I have renamed a column ' median ' that should just overwrite the builtin median method / function and I should still get the output I wish right ?
In your example , you have just overwritten the median property ( if it existed ) to 12 .
How would i append Y with d & e
You can use a tuple notation to indicate a column of the multi-indexed columns ( and you need ` append=True ` to not replace the existing index ): #CODE
How would i append Y with d & e .
The DataFrame.set_index method takes an append keyword argument , so you can simply do like this : #CODE
The ` name ` of ` pd.Series ` become the ` column name ` when you concat them together .
instead of creating a ` DataFrame ` and then transpose it through ` df = df.T ` .
` apply ( Series )` applys ` Series ` function to the column , which creates ` Series ` from the inner ` dict ` .
` join ` append created ` Series ` to the end of column , and ` drop ` deletes the original column .
Python apply a func to two lists of lists , store the result in a Dataframe
My idea to achieve the goal would be constructing a ` pandas.Series ` of length ` len ( AP )` for each ` OP ` , and then append the ` Series ` to the final ` Dataframe ` .
How to replace None only with empty string using pandas ?
I would like to replace all ` None ` ( real ` None ` in python , not str ) inside with `''` ( empty string ) .
OK I can reproduce your issue , it looks like ` None ` is being promoted to a ` NaN ` , I will post a way to only replace ` None `
It looks like ` None ` is being promoted to ` NaN ` and so you cannot use ` replace ` like usual , the following works : #CODE
So we generate a mask of the ` None ` values using ` applymap ` , we then use this mask to iterate over each column of interest and using the boolean mask set the values .
I have tried multiple approaches from stackoverflow and the pandas documentation : apply , apply ( lambda : ... ) , pivots , and joins .
However , I've had a lot of problems trying to reform the dataframe using pivot .
No luck with apply or apply ( lambda ) !
The only problem is that when I go to " unmelt " the dataframe , the pivot completely messes everything up .
@USER OK , if you prefer the melt / pivot way , you might want to post what you tried and see if anyone can fix or improve it .
My issue is that sometimes I get a new file with old data that I want to append to the existing sqlite table .
Is there a pandas to_sql function to insert or replace when I append the new data ?
If you attempt to insert duplicate data you'll get a ` sqlite3.IntegrityError ` exception .
Is it easier to just drop duplicates within SQLite ?
The above will not insert duplicate data into the SQLite table .
I want to do a rolling average on a set of astronomical measurements
I want to do something like a rolling average to create data points that are evenly spaced out and puts the point in the middle of the window .
I need to have a 60 day window ( so not the first 60 points but an increase of 60 in the first column ) , and after each average I need to shift down 20 days and retake the average till I get to the end of my data .
So far I have been able to import the data and run a standard rolling average , however it uses the first sixty points , and I can't figure out how to get it to read the first column .
I am trying to read all ` csv ` files in a directory and merge a specific column in all files to a new ` DataFrame ` .
Try concat :
You can use ` groupby ` on column ' ts ' , and then ` apply ` using ` .any() ` to determine whether any of ` val ` is ` True ` in the cluster / group .
I would use ` transform ` instead , e.g. ` df.groupby ( " ts ") [ " val "] .transform ( any )` .
` transform ` would be the best choice .
transform has the same problem of truncating my dataframe , I lose all additional columns .
@USER This can be easily solved by just transform that particular column .
@USER : to be clear , it was never that transform was truncating your dataframe , it's just that it was only returning the new version of the ` val ` column .
Rolling median for a large dataset - python
I have a huge file with 200K lines , I need to find out the rolling median by counting distinct words in each line .
I have used numpy to calculate median as below #CODE
I feel that this is not efficient as numpy creates a new array everytime i insert an element .
Is there a way to insert an element into a numpy array inplace ?
The rolling median is not trivial as it seems .
I don't understand , you need to compute the median each time you read a line in the file ?
yes , since its rolling median .
I need to read line by line , count unique words per line and find the rolling median .
In particular , you'll want to implement the kind of fast ' running median ' that @USER suggests .
Don't set Instance as the index and use the column for your groupby and drop the Instance column ( last column in this case since it was just added ) .
From a couple of other posts , a simple way to concatenate columns in a dataframe is to use the map command , as in the example below .
The map function returns a series , so why can't just a regular series be used instead of map ?
I checked to see if map returns a different type of object under the hood , but it doesn't seem to : #CODE
I'm confused about what map is doing that makes this work and how whatever map does carries over into the subsequent string arithmetic .
` map ` maps the input values against a corresponding value in the passed in type .
is there already a working solution to insert NaN / None values from a pandas dataframe into sybase-ASE tables ?
However , when I try to insert this into sybase-ASE using bulk_copy / bcp , the None gets inserted as ' nan.0 ' for some reason .
And can you provide a small code sample that you use to insert the data ?
Here is a code sample of how I convert the dataframe to a list and then do the bulk copy to insert this data into sybase ase : inList = df.values.tolist() blk = self.sybase_conn.blkcursor() blk.copy ( table , direction= ' in ') blk.rowxfermany ( inList )
However , I am running into the following error when trying to execute the to_sql : dataframe.to_sql ( ' sybase_table_name ' , engine , if_exists= ' append ' , index=False ) sqlalchemy.exc.DatabaseError : ( DatabaseError ) Msg 102 , Level 15 , State 181 , Line 1
Out [ 9 ]: ' 1.0.6 ' dataframe.to_sql ( ' sybase_table_name ' , engine , if_exists= ' append ' , index=False )
To strip the leading or trailing whitespace off the strings , you could use #CODE
You could also use datetime and replace catching feb 29th returning the year - 1 and feb 28th if it was : #CODE
One idea I have is to do a ` value_counts ` on every column , ` transform ` and add one column for each value_counts ; then filter based on whether they are above or below a threshold .
` m = 0.03 * len ( df )` is the threshold ( it's nice to take the constant out of the complicated expression )
I am trying to make the last two rows of my dataframe ` df ` the first two of my dataframe with the previous first row becoming the 3rd row after the shift .
You can just call ` reindex ` and pass the new desired order : #CODE
You could take slices and concat them , depends how complicated this gets
I may not be understanding your comment , but the axis=0 is not about column order but rather doing a row shift vs column shift .
( And doing some quick tests on 1,000 x3 data suggests it is about 3x to 4x faster than ` append ` . )
I want to pivot the data using : #CODE
The idea is to generate a heat map that shows the count of " Text " by " Location " and " Date " .
The ` No numeric types to aggregate ` suggests that ` Text ` must be numeric for ` df.pivot_table ` to work ....
If ` Date ` and ` Location ` make up a MultiIndex , ` df.unstack ( level= ' Location ')` would be much simpler .
None of the Merge , join , and concatenate would work for me since I need to keep the two files separate .
I tried using the Working with missing data and ` .replace() ` but I'm not sure how to properly extract the values needed from ` main_df ` to replace ` NaN ` values in ` data_df ` .
We can just first divide the full dataset using ` groupby ` , and then apply the above procedure within each group .
I can pivot the table with : #CODE
Still can't figure out how to drop ' q ' from the dataframe
This is a general question about how to apply a function efficiently in pandas .
I often encounter situations where I need to apply a function to a ` pd.Series ` and it would be faster to apply the function only to unique values .
So to speed it up , I'll extract the unique values of ` date ` , apply the function to those , and then merge it back in to the original data : #CODE
And , would it make sense and be feasible to add a feature to pandas that would take this unique / apply / merge approach automatically ?
( It wouldn't work for certain functions , such as those that rely on rolling data , so presumably the user would have to explicitly request this behavior . )
FWIW , here's some sample data for which the merge way is about twice as fast for me : ` mf = pd.DataFrame ( { ' date ' : np.random.choice ( pd.date_range ( ' 1 / 1 / 2011 ' , periods=365 , freq= ' D ') , 900 ) } )`
@USER Doesn't the ` apply ` need to be ` transform ` if you are going to be back to your full size data and not a collapsed version ?
don't you still need to merge here ?
( Perhaps my timing was off though , you can check yourself , of course ) But to be clear , I do think the ` groupby ` with ` transform ` is probably the most clear and readable approach .
I have about 7000 homogeneous DataFrames ( same columns but different sizes ) and want to concat them into one big DataFrame for further analysis .
It looks like you just want to append all tables together ( vertically ) .
If that's the case , you might want to consider ` append ` in a ` pd.HDFStore ` .
Here is an example to use ` pd.HDFStore ` to append many tables together .
I'm interested in using some Pandas functions for this that behave the same ` as to_period ` as there's some postprocessing done ( grouping , pivot table , index ) which I don't want to break .
But when you normalize your features in ` LinearRegression ` , the ` .coef_ ` gives very close approximation to the variance explained by that particular feature , that is ` R2 ` .
error code is ` assert ( len ( items ) == len ( values ))`
There are strict requirements on the shape and form of the data being passed , you can pass just the data and transpose it to get the initial data as a single row and then overwrite the column names : #CODE
use ` shift ` to get the previous row
How to merge two programs with scheduled execution
I am trying to merge two programs or write a third program that will call these two programs as function .
I am not able to merge them nor able to put them into some format that will allow me to call them in a new ` main ` program .
Is their a way to merge both in one program or write them as functions and call those functions in a new program ?
How to apply a function on a Series
I would like a apply a function to rename the values .
Use ` map ` to perform the lookup : #CODE
Your error occurred because you called apply on a single column df : #CODE
So this is different to a Series where ` apply ` iterates over each value which can be hashed but here it's passing the entire ` Series ` which cannot be hashed , it would work if you did this : #CODE
Thx , but I'd rather keep the values if not in ` translate ` .
Use ` isin ` to generate the boolean mask and negate it using ` ~ ` : #CODE
Pandas append rows to dataframe with two indexes
and then append rows to that with the index in the right spot
the append does not change my T in my 0.16.1 version
I've searched through a few threads and either I'm getting an error or I'm not getting the expected result when trying either the pandas replace method or the regex re.sub method ( python 3.x ) .
I've tried a few different ways of escaping the quotes in the replace function , ( ie using """ and ' to define the find part ) .
Your ` .replace() ` method is trying to find an exact string match to replace rather than a subsitution based on existing parameters : Pandas.DataFrame.replace() .
I also want to append all the results into one pandas dataframe .
The input data is above . the reading values are discrete so the prob values are an aggregate percentage of the reading values .
Replace N1 , N2 , N3 for the sizes you want for apple , peach , orange .
Can you show me how to properly append this ?
but how do I insert my df into my table in the database ?
but it only shows how to insert values .
I am just trying to insert these two rows to the left of the most recent entries , and then save over the previous file .
replace the ` NaN ` s with empty strings by calling ` fillna ` , #CODE
how do i insert without inputting all column names with pyodbc execute many ?
I am trying to insert my data into my ms access sql connection .
How do I write an insert statement where I do not have to input all the column names and " ?
wondering if the best method is to merge or perform a customizable function on a
In pandas , you can both append column names and reorder the data frame easily .
To append frames and re-order them you could use the following .
If you want to reindex anything just use a list .
It would be more performant to read df from each csv into a list and then concat all the dfs in the list rather than concatenating each one
@USER reading the article will also explain more complex functions but concat should cover everything
However , it appears that one can only merge two data frames at a time .
Is there a sub function that you would recommend that allows me to check for the same headers and then append the corresponding value of that header if the check is true ?
@USER Append them in a list like so pandas.concat ([ df1 , df2 , df3 ]) .
It also goes over append v . concat .
I want to merge the ` df1.ID ` into ` df2 ` based on df2.Date between ` df1.StartDate ` and ` df2.EndDate ` .
You'll have to define a func to perform the lookup , ` merge ` will not perform the match for you as it requires exact matches on value rather than between ranges
e.g. ` df2 [ ' ID_matched '] = np.piecewise ( np.zeros ( len ( df2 )) , [( df2 [ ' date '] > = start_date ) & ( df2 [ ' date '] <= end_date ) for start_date , end_date in zip ( df1.StartDate.values , df1.EndDate.values )] , df1.ID.values )`
I first tried to use .groupby ( ' id ') .max but couldnt find a way to use it to drop rows .
I tried with ` groupby ( ' id ') .max() ` and it works , and it also drop the rows .
We can generated the edge list using ` groupby / apply ` and
matrix is symmetric , we can add its transpose to make it symmetric .
So after you append an item to it , you're erasing it
You don't need to cast your dict as a DataFrame in order to append , you can pass the dictionary directly to ` append ` like this : #CODE
If you want to load them all in one batch , you could clean the responses with a list comprehension to keep only entries with the right keys , and then pass this in to ` append ` ( you won't be able to skip individual exceptions though . ) #CODE
Hey thanks , it's great to know I can append directly - although how would I do this without brining all the fields back from msg ?
` len ( df2.index ): 12
print len ( df2.query )
, why does pandas replace the values at index 5 and 6 with 3s , but not leave the values at 0 and 1 as is ?
This ` interpolate ` behaviour in pandas looks strange .
Since all your dates are distinct , it's equivalent to replace ` date ` with ` np.arange ( len ( date ))` .
You may use loc too : #CODE
I'm not sure I completely understand why I can pass two arguments to loc , and I'll need to think through using ' describe ' .
Python Pandas - ( not working ) Extracting from csv into other csvs using dataframes and drop
I am also trying to keep the columns and indexes the same , so when I extract the CSV I can then drop the data from the main dataframe .
And then simply use pandas ' very handy ` melt ` function to unpivot the dataframe : #CODE
When running drop duplicates on python pandas there seems to be a bug which causes the DataFrame to be sorted in the wrong order .
Specifically , I was trying to provide two columns to perform the drop duplicates on .
Whether to drop duplicates in place or to return a copy
pandas : slice on MultiIndex by range of first index , and choose secondary index label
I have some sales data grouped by ` ( ' dt ' , ' product_id ')` like this : #CODE
how can I view the income of product ` 10016 ` in between ` 2015-01-15 ` and ` 2015-01-16 ` , and it's best that I can just slice this huge dataframe using the given MultiIndex , cause the data is relatively large .
Assuming your first and second dfs are ` df ` and ` df1 ` respectively , you can merge on the matching columns and then mask the ' a ' and ' b ' conditions : #CODE
You can then drop the additional columns : #CODE
Applying a query function to a RDD using map and lambda
One possiblity is to use ` resample ` using ` rule= ' BQ ' , how= ' max '` .
I think the following should work for you , this groups on the quarter and calls ` transform ` on the ' Value ' column and returns the maximum value as a Series with it's index aligned to the original df : #CODE
Probably you pass the argument `' w '` ( write wife ) instead of `' a '` ( append to file ) .
Looks like you're looking to append in most programming languages " write " also truncates the file to zero length before writing anything .
will append new lines to the existing file .
More than 6.8 Gb of RAM are used while performing the ` map ` operation .
It would be nice to use pandas ( I need to merge that dataframe with an other one , and so on ) .
@USER I tried your previous solution but I get : ValueError : incompatible categories in categorical concat when doing adsInfoDF = adsInfoDF.append ( chunk )
I am trying to choose any day in January and July of every year spanning the period between two datetime objects ( ` row [ ' orig_iss_dt ']` and ` row [ ' maturity_dt ']`) and insert them it into ` my_dict ` .
Basically , I want to apply the same transformation to a huge data set I'm working on now , but I'm getting an error message : #CODE
Why do I get an object type when apply group by with size instead of an integer type ?
` groupby / transform ` returns a DataFrame with a dtype for each column which is compatible with both the original column's dtype and the result of the transformation .
I'm not sure why ` transform ` is coded to work like this ; there might be some usecase which demands this to ensure correctness in some sense .
To work around this , use ` groupby / agg ` : #CODE
and then merge the result back into ` df2 ` : #CODE
It's essentially an integer , so all you really need to do is normalize it with respect to the starting date .
I can't figure out how to transform this data in a vectorized way , any help ??
I even tried stack and didn't find unstack when I was looking !
Better yet would be to normalize them to ASCII characters under 127 .
I met a problem in formatting pivot table that created by Pandas .
HDF + pandas : how can I use a where mask with multindex ?
It would be perfect to use a where mask , but I can't make it work with a multiindex ( since I have to have a where with two conditions ) .
can't use a where mask with a multiindex : #CODE
So I can't use the where mask .
I have to use a where mask , so I can't just select two columns as you do in the last line .
This documentation provides details of selecting data with a ` where ` mask .
then the concat with the x values from the original frame and the new " y " frame #CODE
pandas two dataframes , some sort of merge
I need some sort of join or merge which will give me dataframe like this : #CODE
( also , since there are two values in author3 are 7 , the approach by @USER Cunningham using ` .max() ` will only return one instance instead of both ) You can define a customized ` apply ` function to accomplish your task .
python dask DataFrame , support for ( trivially parallelizable ) row apply ?
Edit : Thanks @USER for the map function .
It seems to be slower than plain pandas apply .
The idea is also very simple : use ` np.array_split ` to split big dataframe into 8 pieces and process them simultaneously using ` multiprocessing ` ; Once it's done , use ` pd.concat ` to concat them back to the original length .
You can apply your function to all of the partitions of your dataframe with the ` map_partitions ` function .
Note that func will be given only part of the dataset at a time , not the entire dataset like with ` pandas apply ` ( which presumably you wouldn't want if you want to do parallelism . )
` map `
You can map a function row-wise across a series with #CODE
But avoid ` apply `
However , you should really avoid ` apply ` with custom Python functions , both in Pandas and in Dask .
I tried the map method and it seems to be slower than pandas apply .
@USER Slightly off topic regarding pandas ; i try to use map over apply because I've heard it's faster , but I'm not sure why it's faster .
Using ` len ( chunk )` will only give me how many each one has .
Speeding up Pandas apply function
For a relatively big Pandas DataFrame ( a few 100k rows ) , I'd like to create a series that is a result of an apply function .
But I think the main problem is the number of calls to the apply function , so ` cython ` , ` numba ` , ` numexpr ` , etc .
The trick is not to use ` apply ` , but to do smart selections .
I get the following error with this stack trace : #CODE
Well , one way to isolate the issue is to cut down the number of things happening in the line .
With Pandas in Python , how do I sort by two columns which are created by the agg function ?
You should use ` .agg ` instead ` .apply ` if you just want to pass two aggregate functions ` mean ` and ` count ` to your data .
What's the most elegant way to merge several Pandas DataFrames ( DF ) from different dictionaries ?
SO I am looking to merge all combinations of DataFrame between the two dictionaries .
So ultimately using Pandas I am looking to merge the following combinations below using dictionaries Dict1 and Dict2 to help .
So I would merge List1+Filetype1 and perform some stats gathering ( then post stats to Excel ) , then List2+Filetype1 and perform some more stats gathering ( then post stats to Excel ) .
Else Ami's reply says it all , the iteration process itself is irrelevant in comparison to the merge process .
They are stored as ` dtype =o bject ` , to make numerical computation on them we need to transform this data into numerical type .
We have to apply the ` map ( func )` to the series in the dataframe : #CODE
DataFrame.applymap ( func ): Apply a function to a DataFrame that is intended to operate elementwise , i.e. like doing map ( func , series ) for each series in the DataFrame
so to transform all columns in dataframe : #CODE
The ` astropy.io.ascii ` reader understands this format natively so this is a snap to read : #CODE
I want to transform the following column : #CODE
In your case you would apply it like this : #CODE
Grouping data on unique column keys and joining ( pivot table ) in Pandas Python
I want to reshape the data so that I'll have a pandas dataframe where each column will be a ticker with its respective closes and if there is no data for the date row key it will have NaN ( ' left ' join ) .
I had to use ` reset_index ` because ` pivot ` requires a column as index ( at least until this bug is fixed ) .
I thought it had to be pivot , but couldn't get to work ..
You just need to first groupby ` Id ` and then move all the processing into an apply function ` my_func ` .
Ideally you don't want to iterate row-wise if possible rewrite your filtering and use the ` dt ` accessor to get the ` time ` attribute for your entire series :
pandas.date_range accurate freq parameter
If you try to use ` pandas.date_range() ` , you need to specify the frequency ( parameter ` freq `) as ` str ` or as ` pandas.DateOffset ` .
The ` groupby / apply ` above returns a ` pd.Series ` .
Curious question , why does line 4 diff from intended result ?
You could drop down to ` PyTables ` which supports queries that won ` t read the entire dataset into memory but only chunk by chunk ( however you won't have the convenient panda functions ) .
apply function to a DataFrame GroupBy and return fewer columns
I want to group my ` DataFrame ` then apply a function of several columns which returns a single result .
But if I do an ` aggregate ` , I get the result returned twice ( once per remaining column after the ` GroupBy ` . ) What am I doing wrong ?
I think this is because ` agg ` tend to keep the same ` ndim ` as original group rather than squeeze the group from ` ndim=2 ` to ` ndim=1 ` .
Or , put in another way , the rule ` agg ` is to aggregate a subgroup dataframe ( nx2 ) across ` axis=0 ` so the returned shape must be 1x2 .
This can be fixed using ` apply ` rather than ` agg ` , as ` apply ` has no constraint on the returned shape .
How to merge two pandas DataFrames in Python ?
I am trying to join two pandas data frames with an inner join .
I believe joining on dates is valid , however I cannot make this simple join work ?
Your date columns are not datetime dtype , ` df1 ` looks like a ` str ` whilst the other is a ` tuple ` so you need to convert these first and then the merge will work : #CODE
And from there our merge works .
there seems something wrong because my file has 7816361 lines counting the header while my list has an extra element ( len of list 7816361 )
Assuming you just want to join the files based on their line number you could do the following : #CODE
Otherwise you'll need to perform a join or merge on your two dataframes .
But it doesn't work when I apply it for all columns .
You can use ` cumsum ` to compute the cumulative sum ( add 0.001 before that ) , then ` shift ` that column by 1 , finally set the first row to be 0 .
I am transforming a single dataframe using multiple cpu cores and want to insert the result into MySQL .
Conditional concatenation of python pandas dataframe ( sql join on self )
We have an existing dataframe and wish to extract a series of records and concat ( sql join on self ) given a condition in one command OR in another DataFrame .
Yes , We can retrieve the data from both and then concat them :
I mean technically we could do a for loop but I'm sure there is a neater way to do this with some cool DataFrame functionality ( concat merge join -> i just don't see the conditional join ) .
Normally I would join a SQL table on itself given a condition but for some reason I don't see it in this situation
Yes that may work but I find it a bit strange that we don't have a conditional concat , or append or something similar to the merge functionality but then for appending rows : like this ( will return a nasty error ofc ): pd.concat ( out1 , out2 , left_on= ' A ' , right_on= ' A ' , how= ' left ')
@USER look at that boolean selector inside ` [ ]` , the first part ` mask ` refers to the first condition , the second part ` .isin ` tells us whether for a particular row ` A ` is in the previous mask and ` df1.B == 2 ` are simultaneously achieved .
But with ' pandanic ' i meant something as nice and elegant to use as the merge function but instead of merging on rows we merge columns .
Following that , I'd say we concat but I don't have the merge functionality in the concat function :(
As such the isin() doesn't seem to apply ?
Can you reindex by date ?
I am making a heat map that has Company Name on the x axis , months on the y-axis , and shaded regions as the number of calls .
I am taking a slice of data from a database for the past year in order to create the heat map .
Then I use the pivot table to create an OrderedDict for Plotting #CODE
what's the original frequency before you make the pivot table ?
How to drop the separator when concatenating two csv files with pandas ?
Recall , how can I fix the above approach and merge both ` text ` columns and tag column in order to get something like this : #CODE
Any how , I read the to_csv documentation but I did not found any " drop separator parameter " .
Also you'd better make sure that all the ID's line up in your files , otherwise you might want to consider using ` merge ` rather than ` concat `
How to drop rows from a pandas dataframe where any column contains a symbol I don't want
Semantically speaking unless your df just has a single Id then really you need to perform a ` groupby ` or construct a lookup and merge the values back to your df
You can group on ' Id ' column , call ` first ` to return the first value for that group , this returns a Series with ' Id ' as the index , you can then call ` map ` on the orig df ' Id ' column to perform a lookup and assign the corresponding value for each ' Id ' : #CODE
Merge values split in different columns overwriting the 0 values
Then i want to merge all of them so result should be something like
Will there only ever be one nonzero column in each row ?
You can use ` .max() ` and apply it row-wise by specifying ` axis=1 ` #CODE
Then i want merge the columns repeated
I'm not sure exactly what you're expecting , but you could replace your lists with numpy arrays ( I don't think it'll improve your specific code ): #CODE
I have some sales data indexed on ` ( ' dt ' , ' product_id ')` like this : #CODE
just updated my post , note that I've made mistakes in my question , it's not a pandas groupby object but a pandas DataFrame indexed on ` [ ' dt ' , ' product_id ']`
Then perform a left merge on your original df and select the ' H_y ' column , the name comes from the fact that the columns clash and ` ffill ` this : #CODE
Result of ` merge ` to show what it produces : #CODE
OK , IIUC you can group as before but then filter the group where ' Code2 ' equals ' Code ' and then use this to merge against : #CODE
Call ` add ` on itself and ` shift ` , you then need to call ` dropna ` : #CODE
pandas - select / mask the first n elements by value
I would like to keep / mask the first three elements in the rows by value keeping the order of the columns , so that the resulting dataframe appears as : #CODE
You can then concat this back to get the ' I ' column back : #CODE
Actually setting index_col= ' I ' when reading allows to avoid the concat !
Just drop the year column and sum the month columns : #CODE
I am trying to join ( merge ) together two dataframe objects on multiple conditions .
If I wanted to merge together two dataframes based on multiple ( equals ) conditions , I would do something like the following : #CODE
Note : your example dataframes do not meet your join criteria .
I did actually think of using this method - the problem is that the merge operation on the full dataframes creates an enormous output .
So an inner join on this chromosome alone results in 1855145691 entries !
Then I use the pivot table to create an OrderedDict for Plotting #CODE
Possibly making len ( recvd_dttm ) to iterate over ?
And replace `' Month '` with `' Day '` below .
Perhaps you could try something like ` print ( len ( list ( df.iterrows() )))` to see if it is indeed 55 .
@USER ` print ( len ( list ( df.iterrows() )))` results in 55 .
However I want to apply the function to each row in the df and make a new column .
After I get the coordinates I'd like to map them .
You can call ` apply ` and pass the function you want to execute on every row like the following : #CODE
Or do it in a one liner by calling ` apply ` twice : #CODE
So the above gets all the unique values using ` unique ` , constructs a dict from them and then calls ` map ` to perform the lookup and add the coords , this will be more efficient than trying to geocode row-wise
Based on your data only since the rows seem exact duplicate , and only if you want , drop the extra ones and execute geocoding to one of them .
If you want to keep all your rows , ` group_by ` the city / state combination , apply geocoding to it the first one by calling ` head ( 1 )` , then duplicate to the remainder rows .
That's an interesting point , it's probably better to just get the unique values , geocode them and merge these back , I will update my answer
I will have to groupby and then apply .
First , you could transpose each data frame so that the rows are the years and the columns are the countries , then take each respective column from the 3 data frames and join them together .
I am using pandas.cut in the following manner to map single age years to age groups and then aggregating afterwards .
Then I pass the data-frame into another function to aggregate : #CODE
I guess it depends on the number of levels for small number of levels this will be faster than calling apply but code wise it doesn't scale to increasing levels well
You can map using the ` bisect ` module : #CODE
pandas merge dataframe and pivot creating new columns
two_c ` , we need add ` Time ` column to ` Sample ` index to build a multi-level index and then ` unstack ` to get the required form .
Then , a ` groupby ` on ` Animal ` index is required to aggregate and reduce the number of ` NaN ` s .
How to customize the facecolor and hatch pattern of boxplot in seaborn ?
I want to customize the facecolor and hatched pattern of the boxes in a grouped boxplot .
I have examined the document of seaborn and found that seaborn only returns a ` matplotlib.axes._subplots.AxesSubplot ` object but not a ` dict ` object which I can use to set the properties of the boxes in the boxplot .
The closest solution that I have found is to use ` Pandas.Dataframe `' s ` boxplot ` method where it takes an input argument called ` return_type ` .
If ` return_type=true ` , the return value of the boxplot method is a ` dict ` object that can be leveraged to fulfill my goal .
` ValueError : Cannot shift with no freq `
I think the error message ( interpreted as ` time index cannot shift with an integer with no time frequency attached `) complains about the last line of your code ` df_percent.index-10 ` .
@USER by the way , I don't think you need to do this in order to product that bar chart . pandas is smart enough to align two bars side-by-side .
Why not just ` reindex ` your dataframe to have obs every month and fill ` NaN ` by 0 ?
I am now able to read the data successfully , unfortunately I can no longer append data to an HDF5 store that I need to work with .
Specifically , Python crashes whenever I attempt to append to this dataset .
The append takes place through pandas as :
If I've read your question correctly , your indexes align exactly and you just need to combine columns into a single DataFrame .
However , the code runs painfully slow using ix to index the corresponding location .
If data is proprietary , replace them with some random numbers .
You can use apply for that : #CODE
Yes , that is what apply does .
Publishing a pandas pivot table via html
Has anyone any experience of publishing a pandas pivot table to a html page ?
Its simple to publish a list as a table but i dont know the syntax for a pandas pivot .
A pivot table is nothing else than a DataFrame , so it has the ` to_html() ` method described in the docs here
Since you only want to change the first element of each group , you can do a customized groupby apply function to do this .
How to merge pandas calculated series into pandas dataframe
I think ` transform ` should work : #CODE
I need to transform this to #CODE
I tried pivot but it returns an error
Hmm My dataframe had 12 rows but when i tried the unstack operation the resulting dataframe has only 6 rows not exactly what i want.My resulting dataframe should also have 12 rows
So doing a pivot would reshape these 3 rows to only one row because those data have been moved to columns .
I get DataError : No numeric types to aggregate
Hi I went ahead and changed the datatype of AvgWaitInMs to int and the pivot worked
I also tried to replace the " = " with " ' = "
Edit : My exploration suggests most ways of creating a MultiIndex automatically lexsorts the unique labels when building the index .
I've been playing with this a bit more - it seems like most methods to generate the MultiIndex sort the unique values within each level on creation , eg MultiIndex.from_tuples (( ' b ' , 2 ) , ( ' a ' , 1 )) ends up with levels =[[ ' a ' , ' b '] , [ 1 , 2 ]] , labels =[[ 1 , 0 ] , [ 1 , 0 ]] .
I'm trying to aggregate the mean visits per page made by visitors to a website grouped by their visitor id's and pages they visited .
what should the output be , if you add that to your question it will make it lot easier to know exactly what you want , if you wanted unique visits you could simply drop dupes based on id and page ` df.drop_duplicates (( " visitor_id " , " page ")) .groupby (( " visitor_id " , " page "))`
Pandas : Nested merge
I have three dataframes that I need to merge together .
As these dataframes are pretty large , I'm looking for the most efficient way to merge these dataframes over year and sector , while respecting the sector-sector2 links .
You should make an ` reset_index ` on each dataframe and then merge them .
But wouldnt that merge along the ( new integer ) index ?
All right , let say you want to merge ` df2 ` with ` my bridge ` .
My solution ( as changing tz in my DataFrame did not change this behaviour ) was to set ` xticklabel=False ` in ` heatmap() ` and use ` plt.xticks() ` directly .
Now , I am trying to resample this data so that only the business monthend values come back , and if I do this : #CODE
It drops out as you can't aggregate strings .
The pattern I use for this problem is to ` pivot ` the table so you only have the date as an index .
This will allow the ` resample ` function to work .
You can then ` stack ` the ` dataframe ` and reset the index to put the field ` id ` back as a column .
After this , I will apply the resample method , and then use the reset_index() method to get back a DataFrame that looks more or less what I have before : #CODE
This worked , but ` len ( a ) 30000000 ` and it is too slow .
So I would want to replace the id in the first row with 22 because the item is the same as row 1 or 2 .
need to unstack the series " ser " as well ?
I'd probably drop the Beta level like this : df.index = df.index.droplevel ( level= ' Beta ')
I want to convert ( pivot ) my table to look like #CODE
How would I be able to pivot this table ?
You probably want the [ transpose ] ( #URL ) function .
I have a function which I apply it on the rows of a dataframe .
I am trying to drop specific rows from my 3-column dataframe based on values in two of the columns .
Remove column from Pandas multiindex
I would like to remove the level ` ItemCode ` from the multiindex , yielding : #CODE
Of course replace accordingly .
If I understand correctly then you can groupby on ' Col2 ' and then call transform on ' Col3 ' and call ` ffill ` : #CODE
So , since the output of the ` str() ` operation cuts off my data ( and might even truncate column values , If I had used longer strings ) , I don't get 1000 , I get 60 .
I'm not entirely sure this is methodically correct insofar as language processing , but using ` join ` will give you the count you need .
This is more of ` join ` issue .
A simple workaround is to convert ` x ` in the list comprehension into a ` string ` before using ` join ` , like so : #CODE
Or you can be more explicit and create a list first , map the ` str ` function to it , and then use ` join ` .
When I try to join the list , I get this error , " TypeError : sequence item 368 : expected string , float found " .
I have a script that uses FFMPEG and CMD to cut video files based off of an excel document row by row .
You can ignore changes numbered 1 and 3 , and replace 5 with ` df.to_excel ( datafile )` .
I am trying to merge several different times series from a number of dataloggers into a single pandas dataframe .
I've created a new Datetime Index representing the range of min and max datetimes at a 30 min frequency beginning on the hour , but I'm unsure how best to merge the data to this index .
Upsampling each of the loggers and performing a merge on the Datetime Index would be simple enough , but I am wondering if there's a more intelligent way to do this by merging datetimes to a PeriodIndex ?
you can create to data frames from two datasets and merge them .
This is equivalent to drop row if both col1 and col2 values are duplicated : #CODE
loc is very slow , will be faster with .ix in my experience .
Merge repeated columns
I know that i can merge the columns by : #CODE
Another possibility could be to create dataframes with only the repeated columns , apply either sum or max and then merge everything .
exec ( compile ( scripttext , filename , ' exec ') , glob , loc )
Sorry what's wrong with ` df [ ' group '] .value_counts() [ ' a ']` or ` len ( df [ df [ ' group '] == ' a '])` ?
Basically , you need to first ` shift ` last three columns and then combine with the first 4 columns , and finally calculate the ` min ` .
How do I replace the ints with the float values from another column ( by same row ) , but leave all the nulls ?
You can use ` concat ` for this , and the keys of the dictionary will automatically be used for a new index level : #CODE
You can also give this new index level a name using the ` names ` argument of ` concat ` ( eg ` names =[ ' key ']`) .
Passing ' reputation ' for ` x ` will use the values of ` df.reputation ` ( all of them , not just the unique ones ) as the ` x ` values , and seaborn has no way to align these with the counts .
I'm trying to map my directory in a pandas dataframe but the auto index is always 0 .
When you append ` df.AccName ` you are in fact appending the entire column .
What I am trying to do is basically initialize a column and as I iterate over the rows to process some of them , then add a filled list in this new column to replace the initialized value .
You could also knock off ` .index ` in your " Method 1 " when trying to find ` len ` of ` df ` .
The ` len ( df.index )` also actually is similarly faster than just ` len ( df )` .
I would like to create df [ ' y '] which will have the first ' cord ' value and shift to get index value .
Now drop the cord1 column #CODE
pandas multiindex ( date , time ) time display issue
BTW , I have used ` loc ` method to select date like this : #CODE
I would like to have a rolling mean of column ` Mass32 ` , so I do this : #CODE
Convert pandas freq string to timedelta
But , you can also use ` freq ` instead of ` freqstr ` , which returns a frequency object directly : #CODE
So we filter the series using ` notnull ` to get the first valid index .
As @USER has pointed out it is better to use the built-in method ` first_valid_index ` as this does not return a temp series which I'm using to mask in the above answer : #CODE
I want to create a dataframe in pandas and then pivot it but it needs column headers to work properly .
I had previously been importing a completed csv into a pandas DF which included the header so this all worked smoothly but now i need to get this data direct from the DB so I was thinking , that's easy , I just join the two lists and hey presto I have what I am looking for , but when i try to append I actually wind up with this : #CODE
You could insert it into data location 0 as a list #CODE
I took that to mean ( 1 ) find the 3 day moving average , and then ( 2 ) find the 3 day rolling maximum of the 3 day moving averages .
Complicated datetime merge pandas
Is there a way to merge ` table2 ` onto ` table ` such that the elements of ` table2 ` are joined on the closest but smallest than or equal element of ` table ` , and then fill the table backwards ?
OK , If I understand correctly to test for membership you can use ` intersection ` : #CODE
So you just want to test membership rather than to mask correct ?
Sounds arbitrary that it would support intersection but not complements .
Edit : Prior version used apply / lambda which is not really necessary .
And here's mask that selects only values between the 25th and 75th percentiles .
Why does order of comparison matter for this apply / lambda inequality ?
Ge the total for all the columns of interest and then add the percentage column : #CODE
Assuming your logic for function ` week ( rawdate )` is correct , You can use the ` series.apply ` function to apply a function to all values in a series ( a pandas column ) , and this returns a new series , which you can assign to your new pandas column .
anyway try the ` apply ` function i specified above .
Try using transpose .
@USER I guess that's because the column ` country_name ` is string , and after transposing , it has no numeric value . try drop that column and then transpose .
I want to know if there is a generic python panda code I can apply
With that as a template for the desired output , we can just loop over the relevant columns of the original dataframe and use ` groupby / apply ` with ` join / unique ` to replace the values in the existing columns : #CODE
I tried to apply this on my data set .
Now I wanna append this group mean as a new column into the original dataframe , in which each mean matches the corresponding Y002 code ( 1 or 2 or 3 ) .
You can append to it using its ` append ` method and get the last two elements as ` l [ -1 ]` and ` l [ -2 ]` .
As a reminder , it's always a good practice to normalize your data first before running regression .
RE : " it's always a good practice to normalize your data first before running regression " #URL
I am using ` q ` to transform my csv file : log.csv ( file linked ) .
I don't see a quick exact fit to your problem , which may require ignoring / eliminating some columns before using ` pivot ` .
You can use ` set_index ` together with ` stack ` .
Many libraries / packages have these sorts of functions which you can usually find by searching for " moving " or " rolling " .
Documentaion for pandas moving / rolling functions
the diff column may also be a ` datetime ` object or a ` timedelta ` object , but the key point for me is , that I can easily get the Year and Month out of it .
Then you can use ` DataFrame.apply() ` function to apply it to each row .
Had to use map instead of apply because of pandas ' timedelda64 , which doesn't allow a simple addition to a datetime object .
Pandas : drop column and release memory
Then I transform the dataset into a proper time-series object : #CODE
To release memory I want to drop the redundant Date and Time columns .
The ` del ` part releases memory but not the ` drop ` part .
This is the code I use to retrieve data from the server and align it to a regular time grid ( e.g. one price every 15 seconds ) .
do the ` drop `
From the main process , ` join ` the child process , and read the reduced DataFrame from the disk .
I want to merge them together based on matching id's in the first column ( SP1 in df1 and SP2 in df2 ) but I want to retain the id's that are only in one of the dataframes still .
Looking at the source code , ` dropna ` appears to only only apply to the values that get assign to ` hue ` .
For now , simply drop the NAs yourself via the appropriate pandas methods .
i have to merge them in to the same cell before applying this method .
There might also be a way to do this with a loop or ` apply ` , can't quite think how though .
The keys of the dict become columns in the result , and the values can be callables or names of common stats ( such as `' min '` , `' max '` , `' mean '` , `' median '` , `' prod '` , `' std '` , `' var '` , `' sum '` , `' size '` , `' first '` , `' last '`) .
Also , looks like you may have row-oriented data and pandas is going to expect column-oriented , you might want to transpose .
The really cool thing about anaconda is that it ships already compiled versions of various packages of the stack of scientific software .
Also , because I invoke python function in the ` apply ` , cython or jit does not help in this case ?
join DataFrames on a partially matching index
I'm trying to find a more elegant way to join two ` DataFrame ` s where the index levels of one DF are a partial subset of the index levels of the other DF .
But now I try to join the latter two together : #CODE
Because the index of the second table doesn't exactly match the index of the first table , the join fails .
This seems totally contrary to the intuitive behavior of an SQL-like inner join .
This is the least-ugly workaround I could come up with , but it's still quite bad in that it requires expanding the size of the second array for no good reason .
Drop all pretense of using an index , and join without index
If using ` join ` but not ` merge ` ( FML !! ) there is another bizarre side effect : the joined- ` on ` column is reduplicated in the output : #CODE
to drop ` b ` .
prefix : string , list of strings , or dict of strings , default None - String to append DataFrame column names Pass a list with length equal to the number of columns when calling get_dummies on a DataFrame .
This will append some prefix to all of the resulting columns , and you can then erase one of the columns with this prefix ( just make it unique ) .
This answer is a little sparse on the details , but pivot or pivot_table would be another good way to approach the problem .
Instead of using ` unstack ` to swap index levels , you could use ` swaplevel ` : #CODE
That's done with ` unstack ( level= ' year ')` .
@USER suggested ` merge ` : #CODE
The merge method in pandas is fairly optimized , so I tried my luck with it and it gave me a significant speed increase .
Pandas join 2 dataframes
But I try to join them together : #CODE
Is there an elegant way to merge that list with the reset of the data so I can export to JSON more easily ?
and I would like to cut off the ` NaN ` s at the beginning and at the end ONLY ( i.e. only the values incl . ` NaN ` from 1950 to 1954 should remain ) .
Pandas DataFrame : replace all values in a column , based on condition
I want to select all values from the ' First Season ' column and replace those that are over 1990 by 1 .
How can I replace just the values from that column ?
Complicated merge based on start and end date pandas
My question is now ; is there a way to append ` Fix ` and ` Performance ` as columns to ` df ` such that the elements of the respective columns are on the rows where ` Start ` and ` End ` are valid , as determined by ` Date ` ?
Here is one approach where you ` apply ` a function row by row to generate the two wanted columns : #CODE
You can first do a SQL-style ` outer merge ` and then remove those inconsistent records with ` Date ` falling out of ` Start-to-End ` Interval .
Never occured to me to use the outer merge .
Replace it by ` .iloc [ 0 ]` .
In dplyr in R allows me to group by multiple columns and then aggregate with different functions on different columns .
The size or count of non-nulls of any column in any group will be the same as the length of the group , so it'll just return len ( group ) * 2 , right ?
i was wondering how to make this exact code work in python . its more about how to split apply and combine in a similar fashion . not so much that the end result has any meaning .
As you can see , the ' freq ' is None .
I am wondering how can I detect the frequency of this series and set the ' freq ' as its frequency .
If there are gaps , is freq set by the smallest difference two timestamps ?
Maybe try taking difference of the timeindex and use the mode ( or smallest difference ) as the freq .
Under ` File origin ` select ` 65001 : Unicode ( UTF-8 )` from the long drop down list .
Currently I do that and then replace the extra symbol by a space which works but is a bit silly imo .
I'll ge back to you .
In theory , this type of representation helps to apply machine learning algorithms to data set as its normalized .
How could I apply decision tree algorithms such as CART and Naive Bayes for this type of data sets ?
Python Pandas Pivot Table Groupby Date Columns Using 7-Day Frequency
Using Python 3.4 and Pandas , my pivot table looks like this : #CODE
What I want to do is group by the ` Day ` COLUMNS using a 7-day ` frequency ` to get a ` summed ` pivot table that looks like this : #CODE
One way is to use ` .dt ` of ` pd.Series ` to get ` weekofyear ` and do pivot based on that column .
It looks like the take-away though is to assign two labels like a ' Before ' and ' After ' period before sending to the pivot table .
I was hoping for a solution that could group the column ` dates ` data while in the ` pivot table ` without having to pre-process the ` dataframe ` ( assign ' Before ' / ' After ' labels ) .
Is there a way to groupby dates in a pivot table if those dates are columns using Pandas ?
why I have to transpose the data in ` f = np.reshape ( kernel ( positions ) .T , X.shape )`
Maybe consider a dimension-reduction via PCA to transform 3-D to 2-D without losing too much information .
why I have to transpose the data in ` f = np.reshape ( kernel ( positions ) .T , X.shape )`
3 ) If I want to get the number of zero crossing , should it be only len ( Zerocross ) ?.
There are two reasons whiskers length vary from one boxplot to any other boxplot
For the one boxplot where the are outliers beyond the whiskers on both the top and the bottom , the whiskers do look about the same size .
How does one shift everything up , including entries to replace the column name .
@USER Uhh , I actually encounter the same problem as you did , and I convert it mannually using ` df = df.fillna ( ' NA ')` to use ` NA ` replace all ` NaNs ` .
If I convert it to a dataframe and call ` df.dtypes ` , it says ` dtype : object ` and I can convert the values of Column A to bool , int , or string , but the dtype is always an object .
The list comprehension looks fine to me but I suggest you try the code I put in my answer below as it should be all you need , with some minor adjustments for however many characters you want to keep / drop .
Using Merge on a column and Index in Pandas
I'm not sure how to reference the index and if merge is even the right way to do this .
If you want to use an index in your merge you have to specify ` left_index=True ` or ` right_index=True ` , and then use ` left_on ` or ` right_on ` .
You must have the same column in each dataframe to merge on .
In this case , just make a ' Project ' column for ` type_df ` , then merge on that : #CODE
Applying aggregate function on columns of Pandas pivot table
I generated the following pivot table via taking maximum of values in ` Z ` column : #CODE
Selecting the first n groups is a bit vague , perhaps you mean ** how can you join the first n groups into a single dataframe** .. something along those lines ?
If a sequence is given , a MultiIndex is used .
I would like to drop the lines in a dataframe ` where ( EndTime CurrentDate )` .
Using pandas to append hdf5 table leads to a pytable exception AttributeError
Merge on key ( unicode column name ) error
There were two pandas dataframe like above which called ' left ' , ' right ' each . and I tried merge like below code .
It seems pandas merge left ( right ) _on=key feature couldn't recognize unicode column name .
It's just because of I tried merge right after groupby . like this .
By default , groupby output has the grouping columns as indicies , not columns , which is why the merge is failing .
Then , your merge should work as expected .
' was an index not column because I did groupby on ' left ' without as_index=False just before merge .
You can use the vectorised ` contains ` and the regex pattern ` \d ` to see if the string contains any digits to create the boolean mask and use ` ~ ` to negate it : #CODE
Here the ` contains ` generates the following boolean mask : #CODE
Inspect a large dataframe for errors arising during merge / combine in python
You want the resample method of a timeseries data-frame .
why not just do a many-to-one merge to create a column of constants ` 1.4 1.1 1.5 ` for each state ` CT MA CA ` and do the calculation column-wise .
I'd construct a dict of your states and desired multiplication factor and just iterate over the dict to get the state and cost factor tuple , use ` loc ` and the boolean mask to selectively multiply only those rows in your df : #CODE
Would it be better for you to simply replace all the ` NaN ` for something like " NorthAm " .
They are in a different column though , so is there a way to replace NA to NorthAm in only the Region column ?
The error message is just telling you that ` result ` has 4 columns and you're trying to replace with 3 columns .
Here's what you can do to replace the ` NaN ` in one column only : #CODE
But I cannot apply this to my code ...
I want to replace all values greater than 1 with 0 .
and you can replace the 0s and 1s by whatever you want .
This you should do a groupby on option and apply a sum and retrieve the count ...
The expanded dataframe is basically going to be used in a pivot table like fashion to see how respondents graded various questions .
Yes , this line is just because having 5 digits was making my formatting of table on stack overflow more difficult :) .
For you you just need the function and to apply it to your dataframe
How to replace strings in a DataFrame column using priority based regex ?
Some of the job titles have multiple actual titles like " CEO and Director of Marketing " or " Vice President and Software Engineer " so I would like to replace with the highest ranked bucket .
The ` replace ` method under ` str ` is easier to use .
Suggestion : install genumeric and use the helper function ssconvert to translate the file to csv .
how do i get the diff of the Date Index ?
Actually , I also realised that i needed to format the dates from strings before applying the above ( i.e. replace the first line with df [ " Diff "] =p d.to_datetime ( df.index , dayfirst=True ))
However , no matter how I banged my head , I could not apply ` .str .contains() ` to the object returned by ` df.columns ` - which is an ` Index ` - nor the one returned by ` df.columns.values ` - which is an ` ndarray ` .
( one could apply any of the ` str ` functions , of course )
Then , I found the ` map ` function and got it to work with the following code : #CODE
Of course in the first solution I could have performed the same kind of regex checking , because I can apply it to the ` str ` data type returned by the iteration .
I am very new to Python and never really programmed anything so I am not too familiar with speed / timing / efficiency , but I tend to think that the second method - using a map - could potentially be faster , besides looking more elegant to my untrained eye .
EDIT : I just found the ` Index ` method ` Index.to_series() ` , which returns - ehm - a ` Series ` to which I could apply ` .str .contains ( ' whatever ')` .
Your solution using ` map ` is very good .
I want to join this to an pandas dataframe .
Sorry for the late response but if I have the np.random.choice within a loop to produce a bunch of outputs how can I append them all to one dataframe ?
if you get a chance please look at how I can append this from a loop
Simply iterate through simulation and append values into dataframe : #CODE
DataFrames , like arrays , occupy contiguous memory and it is very expensive to append to them .
It's always better to append to a list ( which is designed for that ) and convert to a dataframe at the end .
Only until recently did pandas allow the ` df.loc [ i ]` as a row append .
And this [ SO post ] ( #URL ) shows the popularity of the row append .
` test.max ( columns=1 )` finds the max in each column ( like R's ` apply ( test , 2 , max )`)
After ' groupby ' , I need to merge the new dataset with the old one .
You can pass a list of functions to perform on a ` groupby ` object using ` agg ` .
You still need to join it back to your original DataFrame : #CODE
An alternative method is to create a ` groupby ` object and then iteratively call ` transform ` : #CODE
I wasn't aware of shift .
I have 2 questions ( stack ) here .
Use ` transform ` to produce a Series with it's index aligned to your original df , you can then assign as a new column , additionally you can't cast ` datetime64 [ ns ]` using ` astype ` to ` timedelta [ D ]` so you have an additional step to call ` to_timedelta ` : #CODE
Hi Anand , yes indeed I want and but if I replace & with and it returns :
Cyclic shift of a pandas series
I am using the shift method for a data series in pandas
Is it possible do a cyclic shift , i.e. the first value become the last value , in one step ?
You can use ` np.roll ` to cycle the index values and pass this as the values to ` reindex ` : #CODE
You want to ` pivot ` : #CODE
Until we get native support for a Cartesian join ( whistles and looks away .. ) , merging on a dummy column is an easy way to get the same effect .
replace NaN with empty list in pandas dataframe
I'm trying to replace some NaN values in my data with an empty list [ ] .
However the list is represented as a str and doesn't allow me to properly apply the len() function .
is there anyway to replace a NaN value with an actual empty list in pandas ?
This works using ` isnull ` and ` loc ` to mask the series : #CODE
You have to do this using ` apply ` in order for the list object to not be interpreted as an array to assign back to the df which will try to align the shape back to the original series
But then I try to apply it to DataFrame obtained from a .csv file : #CODE
My index is the result of some selection , and it is composed of not correlative #URL [ 3 , 24 , 34 ] ix [ 0 ] throws an error .
An alternative approach would be to add the ' Count ' column using ` transform ` and then call ` drop_duplicates ` : #CODE
how to change the following example to apply for a column ?
loc function in pandas
Can anybody explain why is loc used in python pandas with examples like shown below ?
You can always drop to the lower-level ` matplotlib.hist ` : #CODE
How to transform / interpolate / rebucket columns in Pandas DataFrames based on column names ?
How do I transform from df_input to df_output below , using interpolation / rebucketing ?
Pandas join two dataframes based on multiple dates
I have two ` pandas ` ` DataFrames ` , where I am trying to preform an advanced join .
In the following example I want to join ` df ` and ` df2 ` based on ` my_key ` where the date range ` from_dt ` and ` to_dt ` have an overlap .
Ideally I would like something like ` apply_chunk() ` which is the same as apply but only works on a piece of the dataframe .
whats about using the apply method ?
Anytime you find yourself using ` apply ` or ` iloc ` in a loop it's likely that Pandas is operating much slower than is optimal .
` unstack ` to move it into a new column index level .
Now the Series has a single level DatetimeIndex , so you can ` reindex ` like this : #CODE
And now undo the ` unstack ` operation , thus moving the column index level back
Since we want to align on the bounds , I dropped down to the underlying bin indices : #CODE
Note that if we have an element which doesn't fit in a bin , say 100 , then the categorical will give NaN and the code -1 , and so it'll ( correctly ) be skipped when we insert into ` df_new ` .
You need to modify the apply ` func ` as below to count consecutive non-zero values .
@USER No , just replace the ` def func ` with the one shown in ` Edit ` part of my post .
However this loses the ' clsb ' column so what you can do is ` merge ` this back to your grouped result after calling ` reset_index ` on the grouped object , you can reorder the resulting df columns by using fancy indexing : #CODE
This works all fine as long as I don't apply the ` groupby() ` -method beforehand .
pandas dataframe with Date index -> insert into MySQL
If not , does it simply insert ` null ` to the db or does it throw an error ?
You can just filter the df with your boolean condition and then call ` len ` : #CODE
You ` apply ` this and pass arg ` axis=1 ` to apply row-wise : #CODE
With " " instead of '' for df [ ' diff '] = df.loc [: , " Time_1 " :] .apply ( func , axis=1 )
Also , this is pretty inefficient as it uses sort and apply , neither of which are necessary .
It's also fully vectorized as it avoid use of the ` apply ` method .
a system analysis by variing parameters ( in this case ` rpm ` only ) and append every last line of a results dataframe ` results_df ` to a summarizing dataframe ` df ` containing giving the baviour of my system in depencence of the varied ` rpm ` .
In order to get an appropriate index for plotting and data analysis I converted the varied values ( here ` rpm `) from the list into a pandas series ` ser ` and concat this series with the summarizing dataframe ` df ` containing the results I am interested in .
Unfortunately offsets don't support operations using array like objects so you have to ` apply ` the offset for each element : #CODE
and the resulting heatmap should map the ` Code ` column of the dataframe to the grid representation ( so that the cells from 11 to 25 should be colored in white , as no values is there ) .
:-) Anyway , the key steps are first set background color to white via ` sns.set ( style= " white ")` , and then plot heapmap with ` mask ` parameter to remove those unwanted values .
You can ` groupby ` on ' name ' and ' id ' and just ` apply ` ` len ` function : #CODE
It's because the index values don't match so it will try to align using the indices .
However , the above assumes that the order matches across all your csv's , if you have some identifier columns then it's better to join / merge using this so that the row values are aligned .
An alternative you might try is to replace exit() with os._exit ( os.EX_OK ) .
pandas iloc vs ix vs loc explanation ?
` loc ` works on labels in the index .
` ix ` usually tries to behave like ` loc ` but falls back to behaving like ` iloc ` if the label is not in the index .
It's important to note some subtleties that can make ` ix ` slightly tricky to use :
if the index is of integer type , ` ix ` will only use label-based indexing and not fall back to position-based indexing .
if the index does not contain only integers , then given an integer , ` ix ` will immediately use position-based indexing rather than label-based indexing .
If however ` ix ` is given another type ( e.g. a string ) , it can use label-based indexing .
As per the subtleties noted above , ` s.ix [: 6 ]` now raises a KeyError because it tries to work like ` loc ` but can't find a ` 6 ` in the index .
If , however , our index was of mixed type , given an integer ` ix ` would behave like ` iloc ` immediately instead of raising a KeyError : #CODE
Keep in mind that ` ix ` can still accept non-integers and behave like ` loc ` : #CODE
General advice : if you're only indexing using labels , or only indexing using integer positions , stick with ` loc ` or ` iloc ` to avoid unexpected results .
If however you have a DataFrame and you want to mix label and positional index types , ` ix ` lets you do this : #CODE
Using ` ix ` , we can slice the rows by label and the columns by position ( note that for the columns , ` ix ` default to position-based slicing since the label ` 4 ` is not a column name ): #CODE
One related question I've always had is what relation , if any , loc , iloc and ix have with SettingWithCopy warnings ?
@USER : ` loc ` , ` iloc ` and ` ix ` might still trigger the warning if they are chained together .
Now , it's probably worth pointing out that the default row and column indices for a DataFrame are integers from 0 and in this case ` iloc ` and ` loc ` would work in the same way .
I think it's also worth mentioning that you can pass boolean vectors to the ` loc ` method as well .
How to append a 1 to the end of each single digit element within an array , list , or dataframe in Python Pandas ?
The items are currently integers , but they can be converted to ` str ` or ` bool ` , whatever necessary to do the task .
if len ( variable ) < 2 then variable+ " 1 "
probably easier to multiply by 10 and add one than to cast to string , concat a " 1 " , and re-cast to int :)
This can be changed by setting ` freq = pd.tseries.offsets.DateOffset ( months=6 )` .
So this works because it will drop all rows that contain a single ` NaN ` so you just use the first index value from this to slice the df : #CODE
I'm trying to write a custom function that essentially extends replace functionality ( with some domain specific concerns ) Essentially I have a series that looks like : #CODE
For the first one you can achieve this using ` map ` so something like ` s.map ( s1 )` assuming ` s ` and ` s1 ` are your respective series , for the second output you can do this using ` merge ` so something like ` pd.DataFrame ( s ) .merge ( df , left_on =[ col_name ] , right_index=True , how= ' left ')`
I don't know how your data really looks like so you may need to modify my code slightly but the following works using ` map ` : #CODE
for the second one you can perform a left ` merge ` but you have to construct a dataframe from your series : #CODE
For the above you can set the index after the merge
Actually for your second problem it's easier to use ` reindex ` and pass the series values : #CODE
Merge 2 data frames in pandas
and I need to merge them according to column Time - to get the coordinates of satellites from only the same time ( I need all GPS coordinates and all Glonass coordinates from particular time ) , the result from above example should look like this : #CODE
Because when you merge those two datasets the first row of Glonass should end up on all the rows of GPS where ` time= 2013-06-01 00:00 : 00 ` .
You have 4 entries for ' 2013-06-01 00:00 : 00 ' in your GPS df and 1 in your glosnass df , if you merge these why would you not expect to see your glosnass values replicated for each of these time entries ?
The only way I can think of to do this is to loop over ` mylist ` and create an new dataframe for each element of it and merge / concat or whatever them afterwards .
Calling the DataFrame's ` any ` method will perform better than using ` apply ` to call Python's builtin ` any ` function once per row .
You can ` apply ` the function ` any ` along the rows of ` df ` by using ` axis=1 ` .
In this case I'll only apply ` any ` to a subset of the columns : #CODE
I have tried playing around with ` input_df.names.str.contains() ` and ` input_df.names.isin() ` , but I can't figure out how to find a name in ` input_df1 ` that matches a name in ` input_df2 ` , compare them for the shortest name , and then replace the longer one with the shorter ( which is what my mind thinks should be done ) .
I have a list of filenames called ' filenames ' that I want to merge into a single dataframe .
- merge the whole list of dataframes into one dataframe
Another way might be to sort the list , compare and shift successive elements , and then sort back .
About Python 3 : ` map ` will return an iterator that is exhausted after the first loop .
I was forced to go for the looping-thing since I couldn't figure out how to align the rows ( i.e. games ) from different data frames based on date index .
Besides , if I happened to know how to align the rows , I dunno how to sort the columns ( i.e. rank the teams ) as a whole .
But I guess I just resample the data so that there are no ' missing ' dates for teams ?
So just replace #CODE
pandas / seaborn - using a double mask as heatmap background
Is it possible to set more than one mask to the background of a heatmap ?
where ` mask = np.isnan ( data_mat )` .
Actually the 0.6.0 version of Seaborn automatically masked the `' nan '` values of a dataframe without the need of the mask .
Now I would like to set as mask / background of the heatmap another array like the following : #CODE
plotting the left-bottom triangle in another color ( let's say grey ) , so the resulting heatmap should be mask in white for nan values and in grey for the true values of the ` data_map_2 ` array .
The new mask should represent constant values on the background representing a particular geographical shape of a city .
how to use different aggregate functions for separate columns in pandas ?
I'm trying to use two separate aggregate functions and I know I can do this : #CODE
String replace within index / MultiIndex
I also have a dictionary with values to replace my codes with : #CODE
Clearly the downside here is that the ` replace ` will replace throughout the ` DataFrame ` , not just in the index columns .
On the upside , doing it this way , you get access to all the functionality of ` replace ` like regular expressions .
Then finally change ` codes ` for ` codes_dict ` in the call to ` replace ` .
My current methodology is : create an identical " filler " column in both dataframes , merge on this column ( creating an len ( dataframe1 ) *len ( dataframe2 ) length dataframe ) and then filter on the columns I want : #CODE
I have a feeling you actually want to concat the dataframes and then reduce them down to solid second observations .
I want to be able to apply these using a dictionary like so : #CODE
I just found the ` truncate ` method which will definitely work for this ...
I found out that to do what I've mentioned I needed to replace ` slice ( None )` with simply ` None `
How can I drop columns such that ` number_of_na_values 2000 ` ?
In my tests this seems to be slightly faster than the drop columns method suggested by Jianxun Li in the cases I tested : #CODE
` unique ` of course doesn't contain duplicated values , and that's why its ` len ` is less than ` count ` .
But I don't see anything about the different ways to aggregate the data ( like mean , count , anonymous function ) which you can do in pandas .
Sorry , is your main question how to do it at a certain interval ? or your main question is how to find aggregate statistics inside pandas ?
You can replace ` mean ` for any function you'd like .
You can replace " hour " with " second " , " minute " , " hour " , " day " , " week " , " month " , " year " .
There's not a built in way to do it that I know about but you could do something like ` dt [ , lapply ( .SD , mean ) , by=minutes ( floor ( as.numeric ( difftime ( ts , ymd ( ' 1970-01-01 ') , units= " mins ")) / 5 ) *5 ) +ymd ( ' 1970-01-01 ')]` where you find the number of minutes since some arbitrary date divide by whatever range you want , take the floor of that , multiply that by the same thing you divided by and add it to the same arbitrary date .
To get distribution / histogram , you can use ` value_counts() ` on ` pd.Series ` object and then normalize by ` .sum() ` to calculate percentage .
Or use the ` csv ` lib to read and join the columns after zipping with transposing with ` itertools.izip ` : #CODE
If you actually want to keep the other columns just drop after creating the new column : #CODE
@USER , are you trying to join the first two and keep all the rest or just create a df from the first two ?
If the lines really were in csv format , you'd just change the arguments for split and join above from ' ' to ' , ' .
pandas : create a left join and add rows
I would like to join two Datafame together #CODE
Indeed your result df doesn't make sense are you sure don't just want an outer merge like my answer ?
Perform an outer ` merge ` : #CODE
I want to transform it into a single column data with index being year-month .
I try to stack my original data but it becomes a time series , which has the year mix with my values .
` set_index ` to ` Year ` first , and then ` stack ` .
This will drop you into the pdb debugger .
Looks like a nice usecase for a multi column apply .
` pandas.get_dummies ` turns a single column of data into columns for each distinct value as 1s / 0s , which I'm converting into a bool , and compare with or ( ` | `) to get whether the service was on either port .
It seems like I have to code the rolling window function myself .
Raise or translate to caller
What I do here is that I aggregate accounts of drug purchase records ( cost in TKOST ) for each person ( LopNr ) and month , while also separating the drugs into categories using their ATC codes .
I wanted aggregate panels , with rows like #CODE
Yeah , it would break it but you can replace ` ATC.str ` with ` ATC.astype ( str ) .str ` .
I am digesting several csv files ( each with one or more year of data ) to categorize medical treatments into broad categories , while also keeping only a subset of the original information , and even aggregate up to a monthly number ( by AR=year and month ) of treatments per person ( LopNr ) .
You initialize ` all_treatments ` to a DataFrame , and then append to it .
` AttributeError : ' NoneType ' object has no attribute ' append '`
It becomes None with the first attempt at append .
Using ` map ` and lambda functions is not efficient and doesn't make use of the speed-ups that make pandas so useful .
Pandas join / merge 2 dataframes using date as index
The output should look like this ( I want merge only data with dates whitch appear in both dataframes ): #CODE
By adding the ` count ` column , you can now merge on both ` Date ` and ` count ` which gets you close to the result you want : #CODE
Therefore , you could remove those rows with a boolean mask like this : #CODE
Another way to restrict the merge to only those dates which are common to both
` df1 ` and ` df2 ` would be find the intersection of ` df1 [ ' Date ']` and
` df2 [ ' Date ']` first , and then apply ` pd.merge ` to sub-DataFrames of ` df1 ` and ` df2 ` which contain only those dates : #CODE
Note : there may be more document numbers in the dataframe than there are .txt files ; for anything that isn't in folder0 or folder1 , I'd like to write NaN values and then just drop those from the final dataframe .
In the workflow I replace the 0 values with NaN values to avoid this issue
Creating a mask along the diagonal is probably the best .
How to do a timeseries join in pandas when dates don't match ?
Call ` apply ` on the ' scores ' column on the ` groupby ` object and use the vectorise ` str ` method ` contains ` , use this to filter the ` group ` and call ` count ` : #CODE
To assign as a column use ` transform ` so that the aggregation returns a series with it's index aligned to the original df : #CODE
Pandas Dataframe resample
So was hoping that something like resample ( ' 15S ' , COLUMN = TS ) would do the trick .
Can you describe how you envision the structure will translate to a dataframe ?
Does not seem to conform to any of the supported formats as it seems to be only a single " record " .
That means I need to consolidate the value2 and value39 boolean value into one row .
I want the new dataframe to have one row per id , which means I need to consolidate the boolean values into one row and get rid of the duplicates .
Still not getting the hang of pandas , I am attempting to join two data frames in Pandas using merge .
I have gone through the documentation for the merge function and have tried the following code in various iterations , so far I have only been able to have a blank data frame with correct header row , or have the two data frames merged on the 0 -- ( N-1 ) indexing that is assigned by default :
After searching on SE and the Doc s I have tried resetting the index , ignoring the index columns , copying the Date_Time column as a separate index and trying to merge on the new column , I have tried using on=None , left_on and right_on as permutations of Date_Time to no avail .
What I am looking to do is have the two data frames merge where the two ' Date_Time ' columns intersect .
and then do your merge .
You can use ` join ` , but you first need to set the index : #CODE
Just replace it with : #CODE
I tried with pivot table #CODE
any alternative to pivot table to do this ?
You can use ` df = df.T ` to transpose the dataframe - if I'm understanding the question correctly .
Outer join will still apply , but the original answer is correct : #CODE
Pandas color map
I'm looking to translate a dataframe of equipment date ranges and characteristics into their annual total install time by characteristic groupings .
I'm looking to translate a dataframe like this : #CODE
Using this dataframe I'm looking to translate to produce the following samples where the quantities are the install time of units within the given year : #CODE
Efficiently create sparse pivot tables in pandas ?
I have been using the pivot function within pandas , but the result ends up being fairly large .
I know I can pivot it and then turn it into some kind of sparse representation , but isn't as elegant as I would like .
Alternatively , is there some kind of sparse pivot capability outside of pandas ?
edit : here is an example of a non-sparse pivot #CODE
Pivot tables are just ways to view your original data , which is already sparse ( other than converting ` person ` and ` thing ` to integers )
` person_u ` and ` thing_u ` are lists representing the unique entries for your rows and columns of pivot you want to create .
` pd.DataFrame.sort() ` takes a colum object as argument , which does not apply here , so how can I achieve this ?
To sort by a single aggregate , you can do something like : #CODE
Using np.average in groupby or any aggregate function with parameters
You could use groupby / apply with a custom ( lambda ) function : #CODE
If I would like to use the resample function of pandas ( essentially groupby slice ) , could I do that as well ?
After importing the data , I'd like to drop rows where one field contains one of several substrings in a list .
Use ` isin ` and pass your list of terms to search for you can then negate the boolean mask using ` ~ ` and this will filter out those rows : #CODE
Another method is to join the terms so it becomes a regex and use the vectorised ` str.contains ` : #CODE
Alternatively you can read the csv in chunks , filter out the rows you don't want and append the chunks to your output csv
I'd really like the code to drop anything where a term in to_drop is contained within the text of title .
So , for example , if to_drop somehow happened to contain " Bag " it would drop row 1 .
I need to find a way to import my data to a ` .dataframe() ` in such a way that I can get a value from a field from ` ( row [ i ] , col [ j ])` and work to replace it in ` ( row [ k ] , col [ l ])` .
One way to append the row of new data is via a dictionary .
However , it would be much more efficient to replace ` myfunc ` and ` rolling_df_apply ` with a call to ` pd.rolling_sum ` : #CODE
Using MSSQL ( version 2012 ) , I am using SQLAlchemy and pandas ( on Python 2.7 ) to insert rows into a SQL Server table .
` finaloutput.to_sql ( " MyDB.dbo.Loader_foo " , engine , if_exists= " append " , chunksize= " 10000 ")`
Then , join the two and product two scores : #CODE
Apologies , but when I run >>> ` scores.hist() ` , I get ` TypeError : unorderable types : str() < float() ` The porton of axes.py that gives this error should only do so when ` bin ` is specified as a hist parameter .
By the way , I'm not sure it is just a typo but there is no ` bin ` option for ` hist ` ; what it has a ` bins ` option .
I want to join the two dataframes into one output .
How would I join these two dataframes ?
AttributeError : ' str ' object has no attribute ' transpose ' I have also tried to move it inside and move all the lines beginning with data but nothing seems to work
However , the result for one url gives me dataframe with 48 columns and I should append each new data row to this dataframe .
I need to transform all group columns in a DataFrame except the one column with the output variable .
You can define a list of the cols of interest and pass this to the groupby which will operate on each of these cols via a lambda and ` apply ` : #CODE
Once you make the initial definitions then you can replace `' is_good '` with any other column or subset of columns that you're not interested in so you would only have to do it once .
The best would be if I could mark also mode and median of the data all within 1 plot .
You can use matplotlib / pylab with ` scipy.stats.norm.pdf ` and pass the mean and standard deviation as ` loc ` and ` scale ` : #CODE
Either multiply it by a constant ( but then it won't be a normalized distribution ) , or normalize your histogram ( I think there's a ` normed=True ` argument in matplotlib ) .
True ... though I think you could normalize the data , twin the x-axis for the plotted distribution and then set both y-axis limits to be the same .
How to append rows in a pandas dataframe in a for loop ?
I tried append , concat . .. without success . .. thank you !
I have two dataframe and try to join them but failed , can you please help me .
I want to join them to be : #CODE
The reason your code doesn't work is because its trying to align on index but the index values don't match so it's better to just add the column as anonymous np array values like JoeCondron has answered
So just insert a column to the second data frame , since this is the one you want to keep the index of .
Merging ( Joining ) would need a key to join on .
have merged 2 dataframes with left join . works as I expected until I attempt to use the generated value in a simple string concatenation .
Turn nested dataset into dataframes as well , and then ` merge ` them into the parent dataset .
You should 1st transform your json in a similar way as in [ here ] ( #URL ) .
melt and crosstab on a heavy file
Basically , I have melted the DataFrame before using the crosstab function in pandas to aggregate the rows , trying to reproduce what I was trying to do on R .
Contrary to R , I can melt the entire file ( which makes a loooot of rows ) , but it crashes when I do the crosstab function .
Write to_csv with Python Pandas : Choose which column index to insert new data
But in your case you'd have to write ` n ` number of times , but here you append the values to a list and then plot them easily by calling just your ` x list ` and ` y list `
Selecting a data value from pandas dataframe based on row and column to append to list
I want to append this value 50 to Numbers which is from column 24 row 50 .
Here is the documentation on " setting with enlargement " and this should work for loc or ix ( but not iloc ) .
We can do this for each row of ` dates ` by using ` apply ` : #CODE
By making ` lambda ` , above , return a Series , ` apply ` will return a DataFrame ,
I have tried both the ` aggr ` and ` apply ` methods , but haven't been able to do it .
The html code for each individual dataframe looks good , however when I append the html to a list I end up with a bunch of ` \n ` characters showing on my webpage .
To display 3 separate tables , join the list of HTML strings into a single string : #CODE
It's a little unclear what you are trying to do but you probably want some of the moving / rolling functions : #URL
We can now pivot the data by date and location .
Finally , use ` resample ` to get the mean value of your data over ten year periods : #CODE
Do you want to merge the two dataframes then ?
I first tried to use the ` hist ` method for DataFrames , but encountered an error , and a blank 4x4 series of histograms .
Realizing ` DataFrame.hist() ` " plots the histograms of the columns on multiple subplots " , moved away from this and tried ` outer_df.plot ( kind= ' hist ' , stacked=True )` .
` ValueError : Invalid chart type given hist ` happens when I tried the above .
Creating large Pandas DataFrames : preallocation vs append
As I understand it , this is due to Numpy grabbing all the memory it needs at once instead of having to reallocate memory with every ` append ` operation .
As you , see I find that preallocating is roughly 10x slower than using ` append ` !
Preallocating a dataframe with ` np.empty ` values of the appropriate dtype helps a great deal , but the ` append ` method is still the fastest .
1 ) Why is the ` append ` method faster ?
It is much faster , but still not faster than append .
I would like to go with the ` agg ( lambda x : x.idxmax() )` approach as I did a test of the speed of using ` sort() ` vs ` agg ( lambda x : x.idxmax() ` on a 10 million synthetic data set .
Just apply a lambda function on the groups like so : #CODE
3for j in range ( len ( b2.index )):
1597 if len ( self ) > 0 and self.inferred_type in [ ' integer ' , ' boolean '] :
This can easily be done by performing an inner join on all of them using the ` pandas.merge() ` function .
First of all , is there ' right ' argument in join function ?
It is supposed to be the merge function , not the join function .
I think if you want to do this manually , you can do something like store the indices first and then drop all at once #CODE
I would really appreciate some help or suggestions in how to transform this activity log into the format that I've highlighted .
and I need to first map each record as follows ( pseudo code ) ` if df.ix [ row , col ] == 1 : df.ix [ row , col ] = col ` .
Now , you can map column 2 : #CODE
Currently my ' ave_data ' DataFrame outputs the following data ( please ignore all dashes , they are to align data only ):
You can apply a function that returns the category : #CODE
How to append a dictionary to a pandas dataframe ?
What I need to do is to create dictionaries out of the json files and then append each dictionary to the pandas dataframe as a new row and , in case the json file doesn't have an attribute matching a column in the dataframe this has to be filled blank .
Any idea on how to join them differently ?
How to merge or combine high frequency and low frequency data in python
You probably want [ ` resample `] ( #URL ) or [ ` align `] ( #URL )
Python - Pandas , Rolling Mean , and difficult Groupby common ID with a few select parameters
Hello I am having trouble creating the desired output from a rolling mean and groupby function in python .
- First I want to calculate the rolling mean using groupby for the ' id ' field .
Second , I would like the average of the previous 3 IDs to populate the current ID , without the values of the current ID included in the current rolling mean iteration ( maybe with shift ? ) .
Second part is easy , just another groupby , but this time with shift .
Then use ` combine_first ` to merge where values are kept from the left-most dataframe if available .
I read data from ` json ` , and some of them are duplicates so I want to drop them , please note that there are 2 column ( ` douban_info ` and ` omdb_info `) are still in ` json / dict ` format
So how can I drop these duplicates successfully ??
I can't change the format of my input , and thus have to replace the commas in my DataFrame in order for my code to work , and I want python to do this without the need of doing it manually .
could you include the stack trace
@USER I've added the report , not sure if this is the ` stack trace `
Yes that was the stack trace .
( You can also replace ` None ` with ` np.nan ` if you prefer )
You want to avoid iterating , if possible , and instead find a function to apply , such as this : #CODE
A possible reason maybe the ` nan ` data ? but I've already drop dulicate and na
Or if you're going to use apply on a single series , you need to include an ` axis ` parameter .
However , I get the error ` AttributeError : Cannot access callable attribute ' ix ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method ` and don't know how to do this with the ` apply ` method .
If you just want to drop the rows in ` df ` , just use ` drop ` like this : #CODE
pandas dt accessor error , column made with to_datetime not datelike ?
Does anyone know what function to use for this , I don't think I use loc ?
Then I have code to pull the required data and format it into a pivot table for graphing .
I'm pretty sure the solution is to take the df.index and put it into a list , then cut out repeats and add it onto the y_range .
The other is DataError : No numeric types to aggregate , If I use it after taking my slice of data without setting the index .
Aha ok , sometimes the resample thing can get tricky if your dataframe has data that doesn't aggregate well - you could try constructing a new dataframe using only your index from the original one - maybe something like ` q = pd.DataFrame ( pd.Series ([ n for n in range ( 0 , len ( yourframe.index ))] , index= yourframe.index )) .resample ( ' M ') .index ` which should create a DataFrame q , that you can then iterate through the strftime function ` [ dt.datetime.strftime ( n , ' %b-%Y ') for n in q ]` - is that any better ?
If that's too slow , you can also skip the type checking and just apply the string conversion to columns matching the numeric type .
Python pandas : can I speed up this apply statement ?
I do all of this with apply statements .
I have a feeling I should learn LINQ and how to apply it to SQL - it might make these things much faster
Your question 3 is easy if you again convert to periods , then adding 1 adds to of the same freq , month in this case .
Is there a way ( more efficient than using a for loop ) to replace all the nulls in a Pandas ' DataFrame with the max value in its respective row .
You can use ` apply ` , iterate over all rows and replace 0 by the maximal number of the row by using the ` replace ` function which gives you the expected output : #CODE
I know that I could mask out the squares that are not statistically significant , but I still want to retain that information ( and not set the values to zero ) .
You could plot twice , applying a mask to the cells you do not want to emphasize the second time : #CODE
Dataframe replace could not replace the data
I want to replace the outlier data by min / max in the sample , but the dataframe.replace doesn't work .
so my objective is to replace the maxoutlier by the maxresult .
btw , ` replace ` is for replacing strings , not for what you are trying to do .
With result.loc [ result.value > threshold , ' value ' ] = maxresult.value , the value of result is replace by NaN rather than the value I want .
Here is one workaround by defining customized ` apply / map ` function to unpack the list .
Maybe use ` transpose ` operator to do the trick ?
Select by column and drop column
I always then explicitly drop the column immediately after .
But it sort of feels like there should be a way of doing this that doesn't involve having to explicitly drop ` is_good ` .
The idea is to build a multi-level index and use ` .xs ` to select will automatically drop one level .
Programatically in Python I have 2 x 2D arrays with different lenghts and I want to resample the longest array to the same intervals as the shorter array , interpolating linearly when necessary to create the corresponding data points .
You can't apply my example to your problem ?
I think going with ` reset_index() ` is the way , but there is an option to drop the index , not push it back into the dataframe .
I guess diagnoses is a generator and since you drop something in line 2 of your code this changes the generator .
You can use str.split on the series and apply a function to the result : #CODE
Is apply as efficient as this " vectorized " function definition ?
` File " / home / seidav / anaconda / lib / python2.7 / site-packages / pandas / core / series.py " , line 2060 , in apply
Here is one way to use ` groupby ` with ` reindex ` .
Whereas ` df.fillna ( 0 )` fills all NA / NaN values with 0 , is there a function to replace all non -NA / NaN values with another value , such as 1 ?
boolean index like ` df [ len ( df ) 0 ] = 1 ` throws ` ValueError : cannot insert True , already exists `
But this gives me the error ` DataError : No numeric types to aggregate `
@USER I get an AttributeError : AttributeError : ' DatetimeIndex ' object has no attribute ' apply '
OK the following should work , convert your datetimeindex to a series so you can call ` apply ` and use ` strftime ` to return an array of strings : #CODE
I am having an issue where Pandas is not understanding my merge conditions .
There will always be many more duplicates of Make / Model Hour in df2 so I only want to left merge matches to df , no matter how many duplicate instances within df .
If the ' Date ' merge condition worked , this is the output I am trying to achieve : #CODE
I have tried even splicing the ' Date ' column into 3 columns ( ' Month ' , ' Day ' , ' Year ') and changing the dtype to int64 , bool , obj and no success there either .
Running the below code before the merge should put the dates into a common format so that the merge works properly .
I'd prefer if you could write an answer with your supported code to plot pdf output vs KDE of original data and save the output of KDE to apply first derivative on it
I have a pivot table that shows customer calls by month .
I would like to reorder the pivot table so that it matches the date_list like so : #CODE
Suppose ` df ` is your pivot table .
Let's say your pivot table is named ` pt ` .
If the list changes , your index value will change , and hence your table columns will shift .
I couldn't find an answer to this in the existing ` SettingWithCopy ` warning questions , because the common ` .loc ` solution doesn't seem to apply .
I'm loading a table into pandas then trying to create some mask columns based on values in the other columns .
Note : When I replace ` hist ` in my code below with ` describe ` , it perfectly gives me 100 describe series .
How to drop rows which has elements equal to a specific value
I want to drop the rows which contains elements equal to minus one .
The following code could drop the rows if columns 1 equal to -1 , but each columns could have -1 .
Data type of pandas column changes to object when it's passed to a function via apply ?
I need to use the ` dtype ` of a pandas column in a function , but for some reason when I call the function using ` apply ` , the ` dtype ` is changed to ` object ` .
If you pass ` reduce=False ` to your ` apply ` call , the result is correct : #CODE
Since we don't have a corresponding data that could map to the actually time , so you might need to check that if the time ` 00:22 .9 ` is elapsed time or whatever ( ` 00:22 .9 ` might represent ` 07:22 .9 ` ? ) .
Here I transform all the data into a dict list as below : ( the data is built by myself ) #CODE
the ` %d-%b-%y ` is the mask corresponding to ` 01-Nov-13 ` ( day-month-year ) , check here for other masks .
How to filter shift + / - 1 day in Pandas ?
If the dataset is larger , I would use the merge operation : #CODE
As you're trying to learn categories from one DataFrame to apply to a different DataFrame , using scikit-learn might provide a more elegant solution : #CODE
Perhaps you could put the whole stack trace , the DataFrame itself ?
I want to cut the hours / minutes on the following data to keep only the ' YYYY-MM-DD 00:00 : 00 ' .
Note that you can also call normalize on your index : ` index.normalize() `
Code-only answers [ are discouraged ] ( #URL ) on Stack Overflow , because a code dump with no context doesn't explain how or why the solution will work , making it impossible for the original poster ( or any future readers ) to understand the logic behind it .
See #URL and map the proper column with such encoding function
Representing a SQL join with a merge in python ( pandas ) when one of the join conditions has an increment
I'm trying to transform the following SQL Server code into pandas #CODE
I can easily do a merge ( join ) on one condition : #CODE
but I don't seem to find a way to merge / join on the second condition as one of the values has to be incremented by one i.e. alfa.value = beta.value + 1
Append two multiindexed pandas dataframes
Can you please help to append two multiindexed pandas dataframes ?
Trying to append df_future to df_current .
Use ` concat ` to concatenate the two DataFrames , and ` sortlevel ` to reorder the first index level : #CODE
Appending in pandas is called concat .
The ` concat ` function works no matter if you have multiindex or not #CODE
You have to use ` isnull ` for this : #CODE
Or use ` apply ` : #CODE
This will drop you in the pdb debugger .
You then want to transpose this data so that the dates are now the row and the countries are the columns ( use ` .T `) .
I wish I could replace the 2 by string.index ( " | ") , but how do I call the string ?
If you want to take a copy of a row then you can either use ` loc ` for label indexing or ` iloc ` for integer based indexing : #CODE
If you want to remove that row then you can use ` drop ` : #CODE
Similarly you can pass a list to ` drop ` : #CODE
If you wanted to drop a slice then you can slice your index and pass this to ` drop ` : #CODE
Given multiple entries that have the same ` p ` , ` g ` , ` a ` , and ` s ` , I need to drop all but the one with the highest ` v ` .
I need to drop the first two rows and keep the last one .
You can specify the label you want to remove with ` drop ` .
By the way , you can apply your logic of ` df [ df [ ' col_1 '] ! = 754 ]` also on the index .
You can pass a tuple of your index labels to ` drop ` #CODE
OK if it's intra log difference then you can do this succinctly using ` diff ` : #CODE
pandas database merge on multiple columns not merging correctly
I want to merge two dataframes on multiple columns ( 11 to be exact ) .
Why are you doing an outer join ?
As far as I got you need an inner join .
I know that I can simply do one datetime minus the other to find the timedelta - but I don't know how to apply this to make a new column .
Basically , I need to resample the raw data ( every 24 seconds ) into 1 minute bins , and count how many ( valid ) hspeed values are within the bins .
You can call ` map ` on ` df2 [ ' c4 ']` after setting the index on ` df2 [ ' c3 ']` , this will perform a lookup : #CODE
Is it actually possible to call " map " on more than one column ?
To answer your question , no it won't work that way , for your updated example use merge
Split dataframe into chunks and add them to a multiindex
I want to group every 7000 rows into a higher dimension multiindex , making 11 groups of higher dimension index .
Note : The ` // ` operator is for floor division to truncate any remainder .
If I understood your problem , the " SQL " way to do it is to merge with another dataset containing the values you want , I don't know if it's faster something like this : ref= pd.Dataframe ( { ' id ' : [ x ] , ' cycle ' : [ y ] } ); pd.merge ( df , ref , on = [ ' id ' , ' cycle ']) .
Pandas merge dataframes on same column
I am writing a scraper and I want loop through a list of links and merge all results as columns to a dataframe on same key ( like a left join ) .
I run this code in the Ipython Notebook , the resulting csv that comes from the dataframe does not make sense , however if after running the script I merge df and df2 on the mutual column " questions " , I get the join that I need , but in the script there's something wrong .
I also tried to save every review to .csv and then merge everything with Pandas , but I get a weird , rare undocumented error :
Here is one way using ` np.triu ` to mask out upper triangular matrix and reshaping correlation matrix by ` stack ` .
I then need to append this data back to the original hdf5 file ( with a column indicating whether the same Customer_ID showed up multiple times ) .
The only solution I have is to read the whole file into memory , drop the duplicates , and then write the file all over again ( replacing it completely ) .
Like an ' insert and replace ' command that I can use in pandas ?
I need to see if the latest data ( from yesterday ) existed previously ( within a 30 day rolling window ) .
Below you can see that I append the latest data to the original file , I then read that file into memory , drop duplicates , then write it again ( overwriting the existing file ) #CODE
Python pandas : replace values multiple columns matching multiple columns from another dataframe
I searched a lot for an answer , the closest question was Compare 2 columns of 2 different pandas dataframes , if the same insert 1 into the other in Python , but the answer to this person's particular problem was a simple merge , which doesn't answer the question in a general way .
I have used merge as a workaround : #CODE
If you merge chr , ochr and pos , ostop , then there is no need to update .. maybe you mean to update chr -> CHR and post -> STOP ?
this is how it is in your example , after the merge , you want to update df1.chr -> df2.CHR and df1.pos -> df2.STOP , maybe correct that if its a typo
This can be done in one line of an SQL Update join query .
Start by renaiming the columns you want to merge in df2 #CODE
Now merge on these columns #CODE
There is an ` apply ` for that : #CODE
Choropleth map from Geopandas GeoDataFame
I'm trying to make a choropleth map from polygons in a Geopandas GeoDataFrame .
I pivot on ` student_id ` and ` exam_time ` to get the number of exams in a session or in a day .
I am building a timetabling heuristic that changes the times of the examinations one at a time so I need to update this pivot table a lot of times .
Is there a way to update the resulting pivot table without recalculating the whole thing in pandas or should I keep track of the changes on the pivot table myself ( i.e. by adding and subtracting 1 to the changed slots ) ?
Edit : Here's how I construct the pivot table .
That said , I'd expect the most efficient way to do this would be to store a pivot table for each exam ( then add them all together ) .
Then if you change one exam , you just subtract the old one and add the new one to the combined pivot table .
@USER I added the code for the pivot table .
Here is sample code , the idea is update the total pivot table by subtract the pivot table of old rows and add the pivot table of new rows .
Just use a Boolean mask -- everything is already implemented in pandas .
` df.assign ` only accepts kwargs ( it can't be directly passed an actual dictionary ) , and even the ( rather kludgy anyway ) method of expanding a non-str-keyed dict as kwargs is explicitly guarded against : #CODE
Join will return a copy instead of affecting the existing dataframe ( joins the two dataframes on the matching indexes ): #CODE
And , of course , this column actually comes from another dataframe and the label isn't changing , so I can just join it directly .
Probably could have done that with ` concat ` too .
If I drop it #CODE
( You do not need to drop the index column beforehand )
I then need to pivot the data and show each item as a column with a multi-level index for reference / publish date .. there can be many duplicate reference dates for a single publish date .
but I get the following error on creating the multiindex dataframe : #CODE
You need to reset your index , and then reindex your DataFrame to :
Based on the revised post , I believe a pivot table would do the trick .
You can ( probably should , if it takes over an hour ) use numpy for this , and then select between the two use cases with a mask , e.g. ` mask = lcd_temp.loc [ ..., ' Total_Defaults '] == 1 ` .
Assuming you are using Pandas / NumPy , the standard way to replace an ` if-then ` construction such as the one you are using is to use ` np.where ( mask , A , B )` .
The ` mask ` is an array of boolean values .
The result is an array of the same shape as ` mask ` with values from ` A ` and / or ` B ` .
Didn't know about mask - nor about the efficiency difference with Numpy ( I'm a Newbie ) .
I would assume there must be some special function ( such as apply , roll_apply ) to iterate through these values , but I couldn't figure that out .
My goal is to merge two DataFrames by their common column ( gene names ) so I can take a product of each gene score across each gene row .
That last part should work fine , but I have not been able to perform the first merge on gene names due to a ` MemoryError ` .
I have written many a comment on dataframe merge Q / As to this effect -even in R .
Seems to work for my merge , but I [ wouldn't be able to query ] ( #URL ) my new database , correct ?
I tried a few alternatives such as only building a map if the names in the first column in sequential rows are different , but this just adds a statement into it and makes the code much slower .
how do I apply normalize function to pandas string series ?
I would like to apply the following function to a dataframe series :
How do I apply the ` normalize ` function to all members of the series ?
` map ` is used to apply a function elementwise ( and of course you wouldn't have to chain it all together like this ) #CODE
However , under the hood , the function is resampling my timeseries with an interval of 1 ms ( which is the ' freq ' that I supplied ) .
You can use ` reindex ` to align the ` ewma ` result with your original series .
It uses the apply function : #CODE
Note that if you loaded the other keys into the dict , you could do the apply without the swapper function : #CODE
I don't really see anything wrong with your merge approach - I guess it could be a little shorter as ` df.merge ( keys , left_index=True , right_on =[ ' A ' , ' B ' , ' C '] , how= ' right ') .R ` ?
Also , consider creating a new function to use in the apply for now until you can work the problem out .
Simply add axis = 1 to your apply function and it will work : #CODE
` in_group [ " t_wins "] = in_group.apply ( lambda x : len ( temp [ temp [ ' t_date '] < x [ ' date ']]) , axis=1 )`
You add the column to the subgroup inside the apply function , and then when you return that subgroup it replaces the existing subgroup .
The way that I have found to do is by making a copy of the column and operating on it ( I have to do this because a ` DateTimeIndex ` has no ` apply ` method ) .
Then I append each resulting dataset into a new dataframe which looks like the above .
My usual approach for these problems is to pivot and think in terms of events changing an accumulator .
You could then drop the zeros if you don't want them .
If I am using the rolling average function of pandas , how do I code the logic to inspect predecessor or successor row for a particular point ?
The easiest approach is to reshape to data to a long format using ` .stack ` , which can be be passed straight into rolling mean .
The issue is that ` x = pd.np.array ( map ( float , str ( x ) .split ( ' , ')))` in ` split_stat() ` seems to be creating a map object , not a numpy array .
@USER : In Python3 , ` map ` returns a map object , not a list .
So use ` x = pd.np.array ( list ( map ( float , str ( x ) .split ( ' , '))))` .
Building a conditional time series pivot table in Pandas
And finally use ` reindex ` to make sure every date has a row : #CODE
To also strip out the time part of the datetimes , you can use : #CODE
Two problems when I try to do this : ( 1 ) My source data frame contains datetime64s and I'm struggling to strip the time information from them ; and ( 2 ) when I run it the series name is lost so the columns in the final data frame has names 0 , 1 , 2 , ...
Regarding ( 1 ): Do you want to strip off the times * before * the ` value_counts ` is called , or merely off of ` result.index ` ?
( 1 ) ` df = pd.DataFrame ( df.values.astype ( ' < M 8[ D ]') , columns =d f.columns )` can be used to strip the times off of the datetime64s .
This can be fixed by using ` df.values.astype ( ' < M 8[ D ]') .astype ( ' < M 8[ ns ]')` to strip the times from the dates * and * convert the data back to ` datetime64 [ ns ]` ( instead of ` datetime64 [ D ]`) dtype .
Given ` datetime.datetime ` values in the columns , you could strip off the times using
stack trace on small python plotting program snippet
This piece of code works on ubuntu15.04 desktop but on my 14.04 laptop I get the following stack trace which I am having trouble interpreting .
Edit : note - I have tried running the code with both ` python ` and ` python3 ` and the same stack trace occurs .
Also , ` df.plot ( x= ' age ' , y= ' count ' , kind= ' hist ')` shows
``` dataframe.plot ( x= ' age ' , y= ' count ' , kind= ' hist ')``` ( if it works , add a vote to my answer in that link :)
Try setting the index to the age and then calling the ` hist ` method on the ` count ` column .
It could be done by calling ` apply ` on the df like so : #CODE
This uses double subscripts ` [[ ]]` to call ` apply ` on a df with a single column , this allows you to pass param ` axis=1 ` so you can call you function row-wise , you then have access to the index attribute , which is ` name ` and the column name attribute , which is ` index ` , this allows you to slice your df to calculate a rolling ` std ` .
what's the difference between ` ix ` and ` iloc `
[ ix ] ( #URL ) is just a way for indexing using label values , ` iloc ` uses integer based indexing , as your index is str based I can't use ` iloc ` to slice this way , it would also work if you use ` loc `
I've fixed the error in #URL and opened a pull request to merge in the fix , but feel free to checkout my fork / branch .
If you want to consolidate all the DataFrames should create list containing each of the individual DataFrames and concat them .
Then you can use ` os.path.join() ` to join it with the other directory and get the resultant path .
os.path.basename will give you the file name just join it to the new path and save it however you want : #CODE
However , when I apply this method to whole length of data , I couldn't wait until the operation was done .
You should simply drop a level from your nested dict to make life easier .
I did use pd.DataFrame() on each of your dict's , however , what I did was apply a dict comprehension using each of them to strip out the nested zeros .
Now you can map the column names to whatever #CODE
Is there a way to sum / aggregate these orders ?
Python pandas : banal apply statements incredibly slow
apply sum calculated using pandas group by to all elements of group
Please suggest how can I apply sum on all rows of one account .
You could use ` groupby / transform ` to calculate a value for each row of each group : #CODE
You can use ` itertools.product ` to generate all combinations , then construct a df from this , ` merge ` it with your original df along with ` fillna ` to fill missing count values with ` 0 ` : #CODE
create a MultiIndex by MultiIndex.from_product() and then ` set_index() ` , ` reindex() ` , ` reset_index() ` .
( I've not had to use ` reindex ` or ` reset_index ` before , but I'll read up on them shortly ) .
Is there an easy way to basically join them ?
If yes , do you want to append that table to the one saved as hdf file ?
looks like you want a special case of a pandas pivot table . try that
When I do this with the actual data , the ` Value ` lists get cut off after the first 4 elements .
How to map a function in pandas which compares each record in a column to previous and next records
You can use the ` map ` method on your json-text column to apply a ` lambda ` function which will parse the json using ` json.loads ` then return the field that you want .
I know I can get the std values by ` std_vals = df.std() ` , which gives the following result , and use these values to drop the columns one by one .
However , I don't know how to use the Pandas indexing to drop the columns with low std values directly .
To drop columns , You need those column names .
You can use the ` loc ` method of a dataframe to select certain columns based on a Boolean indexer .
Create the indexer like this ( uses Numpy Array broadcasting to apply the condition to each column ): #CODE
Then call ` loc ` with ` : ` in the first position to indicate that you want to return all rows : #CODE
` ids_with_latlong [ " loc "] .str .extract ( " [ -+ ] ?
1 ) remove extraneous characters with ` replace ` ( or maybe this is where the regex is best )
Being new to Pandas , I'm not sure what to do with that , even though the documentation seems clear , I can't figure out how to use ` loc ` here .
As a side note , you might want to change the name of the column , since it potentially conflicts with the ` loc ` method .
you'll be OK with ` df [ ' loc ']` but not ` df.loc ` for referring to the ' loc ' column .
This can be done by first construct multi-level index on column names and then reshape the dataframe by ` stack ` .
Then , if I use your code : level_group = np.where ( df.columns.str.contains ( ' 0 ') , 0 , 1 ) on my groupby ( replacing the df ) , I get an error : Cannot access attribute ' columns ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method
The first trick is to gather the columns into a single column using ` stack ` : #CODE
and finally what if I wanted to completely unstack everything so that split and sex become columns as well ?
When we read data from data_frame , we can firstly trunctate it by desired qty and then we need to remove previous indexing and then we can concat that data : #CODE
Any idea on how to append it to a df ?
In pandas 10 this worked , as documented in various pandas upgrades it does not work . without having to refactor the code totally is the a way of changing line 2 so that it can still use the loc method while still using the condition as defined on line 1 .
If I want to add a column ` wgt ` to ` df ` I need to merge this result back to ` df ` on ` name ` and ` index ` .
Use ` transform ` , this will return a series with an index aligned to your original df : #CODE
Ah , so you have to give ` transform ` a Series .
` transform ` can't handle this , right ?
Sorry you're asking how to use 2 columns in a groupby / transform ?
What I did is to apply this function to get the count of repeating elements in B #CODE
2nd , to count the number of repeats of particular AB combination , we may want to use ` gourpby ` , and merge the resultant with the original ` DataFrame ` : #CODE
You can pass the ` dtype ` as a param this will map the column to the passed dtype : #CODE
You can also be more explicit by using the ` ix ` indexer : #CODE
You can use a simple merge to accomplish this #CODE
How do I replace the content of an entire Pandas ' cell with a string ?
I just want to replace the content of that cell ( whatever it is , rather than a regex ) , with some other string ( literal )
Your example uses "` iloc `" rather than "` loc `" , which means you need to access both columns and index by index , not name .
not related to the question : replace your last loop with ` for job in jobs : job.get() `
Apply async usage :
One way would be to introduce a column ranking the subgroups and then pivot .
First sort by the score and do a group by transform : #CODE
Then pivot : #CODE
` stack ` just converts it to a series so you can use ` str ` and then ` unstack ` converts it back to a dataframe .
But generally speaking , ` apply / lambda ` will be slower than using built in pandas methods .
From fastest to slowest : split ( 3.28ms ) , applymap ( 3.91ms ) , stack ( 4.25ms ) , extract ( 9.71ms ) .
I am trying to replace all instances of " 3 " in the variable GEND to missing ( NaN ) .
You can use loc to replace the 3's : #CODE
That replaces the 3rd ` loc ` with ` NaN ` , print out what ` df.GEND.loc [ 3 ]` is and you should see what it is doing .
@USER loc performs label indexing , so it returns just the row where the index is ` 3 `
Left merge .
I don't believe you can ignore them as they are being read , but once they have been read you can easily drop them using ` drop_duplicates ` .
Depending on the cleanliness of your underlying data , you may first need to pre-process to strip spaces , etc .
I need to strip out the date , which I'm doing with :
It would be interested to see if the approach of using ` map ` with ` datetools.parse ` is as scalable as the standard approach given here , here and here .
It should not ` apply ` from `" Column B "` this should still be done from `" Column A "` .
For future readers it is probably not apparent that ` apply ` on two columns is interchangeable with ` apply ` on one column and " reading " another column at the same time .
possible duplicate of [ How to apply a function to two columns of Pandas dataframe ] ( #URL )
The ` axis=1 ` argument passed to the apply method puts the whole row into the apply method as a single tuple argument .
The reason is that ` str.replace ` will be able to handle NaNs , while the custom replace with lambda will error on that : #CODE
This is a really messy way to do this , first use ` first_valid_index ` to get the valid columns , convert the returned series to a dataframe so we can call ` apply ` row-wise and use this to index back to original df : #CODE
The idea here is to use ` stack ` to move the columns into a row index level : #CODE
All we need to do is reindex the result ( using the original index ) so as to
` argmin ` gives the index of the first ` False ` value in each row of the result of ` np.isnan ` in a vectorized way , which is the hard part .
I did poke at the groupby function earlier , but missed the agg ( ... ) part of it .
How to plot a multiindex dataframe having suplots for the first level index ?
I have a pandas multiindex dataframe with quarters 1-4 and hours 0-23 as the index .
The test inputs you show , converted to dataframes , have no problem with concat .
Because the script you show will properly join those " test inputs " , meaning the simplest explanation is that your actual inputs are not the same as those ones .
pandas DataFrame update / combine when indices don't align
Now I would like to merge or combine these two data frames such that the values are collected in a single column , with the ` NaN ` in ` df1 ` replaced by the corresponding observation-year values from ` df2 ` .
i.e. essentially doing a regular merge and then replacing the ` NaN ` s in one column with the values for the other column by hand .
I feel like some sort of ` df.combine ` , ` df.combine_first ` , ` df.update ` would do what I'm after , but they all seem to align on indices .
Yes that's correct , I think your best bet is to merge like you have done already , to some extent as you are interested in just the values the index of the other df could be overwritten or reset
And I can't take a cross section for just the cells I want to set to 99 , because I need to slice by index AND by row , so I need a blend of iloc and loc .
One possibility would be to reset the index to drop the Name level and then put it back again afterward , but that seems yucky .
This gives you a dataframe with ` len ( listOfAttributeColumnNames )` ( apparently 6 ) columns .
Thanks , I missed the join of the columns after refactoring my code .
I want to cut my in-memory overhead with some string folding ( pass 3 outlined here : #URL ) .
No I don't think so , for instance ` df [ ' A '] == float ( ' nan ')` still won't work , bottom line is you have to use ` isnull ` or ` notnull ` to test for ` NaN ` correctly
You could replace ` nan ` with a unique non-NaN value that will not occur in your list , say `' NA '` or `''` .
You can do the following , this tests each row for membership of ` 1 , 2 ` using ` isin ` and if so this generates a boolean series , you can use this to index into the columns by calling ` apply ` again , we convert this to a list because the dimensions won't align if you don't do this : #CODE
output from inner ` apply ` : #CODE
I have also tried using ` sep=r ' \s* '` , but seeing as how the ` * `' s move around in the file , it makes it to where some rows are shifted , and therefore the columns do not align .
Those asterisks everywhere are really problematic because , as you mention , sometimes they replace the ` | ` delimiter and other times they're in the middle of a field seemingly at random .
If I define the negative difference to ` diff = then - now ` : #CODE
Although it is clear from your answer , I'll leave for clarity , to find the absolute value of a subtraction of timestamp : abs (( ts1-ts2 ) .total_seconds )
I have two dataframes , I wanted to merge them into one single dataframe based on the matching row .
This is what I tried with merge , #CODE
You can also use ` join ` which is basically the same as ` merge ` but automatically uses the index .
Also , your example DataFrames don't have ` Set_1 ` as the index , and if you run your example ` merge ` statement , you raise an exception , so something's odd ....
The pandas merge function takes keyword arguments : ` left_index= ` and ` right_index= ` .
When set to ` True ` , the merge function will use the index / indicies of the dataframe ( s ) for merging .
it would help me produce output that was a lot neater and a little more ' human-like ' if I could use pandas and xlsxwriter in a way that would stack two dataframes , one on top of the other , on the same sheet of the Excel spreadsheet I am outputting .
Is there a neat way I can just take my dataframe and my summary dataframe and stack them on the same sheet ?
I am trying with Pivot tables , but not able to get the desired result .
This is over 100x faster for the example above ( where ` len ( drg )` is close to 4000 ): #CODE
Python merge excel documents with dynamic columns
However , since they are not 100% identical , I cannot simply merge them together and upload it into a database without messing up the data .
Any recommendations to a db that would allow me to dump a few thousand excel documents and then create join queries to the VIN column ?
I am trying to figure out how to iterate through each value in this pandas DataFrame to see if it's absolute value is higher than some defined threshold ex : .01 > abs ( value ) .
` reindex ` would work - be aware that it will create missing values for index values that are df , not in df2 .
Alternatively in this longer solution I explicitly conform the indices .
That won't work properly , since the ` dict ` constructor will replace the value of each ` state ` as it goes , rather than summing .
I think the ` agg ` is unnecessary here
How to replace string values in pandas dataframe to integers ?
I want to replace them with integer values in order to calculate similarities .
and I want to replace region == ' Geo RS / SC ' ==> 1 , region == ' Geo PR / SPI ' ==> 2 etc .
You can use the ` .apply() ` function and a dictionary to map all known string values to their corresponding integer values : #CODE
performance degradation when switching from pandas column concatenation to using apply on dataframe
Do you want to keep the ` NaN ` or replace them with blank string values ?
Generally using ` apply ` should be avoided it's essentially a for loop and not vectorised , it would be better to write a func that performed vectorised operations rather than using ` apply ` at all
Create pandas dataFrame from two other using append
Just append them or do some more sophisticated combination ?
You are calling ` append ` and dropping the result on the bit floor .
If you want to append to the frame in the sense of list.append , you need to use something like #CODE
However , I can't quite understand how to use a forloop or use the apply function through all of the columns in the dataset .
I'm attempting to use the apply function : #CODE
` axis=2 ` is not a valid param for ` apply `
What I gather from the comments sections and your original post , you want to replace each character in each column according to it's frequency of occurrence .
I need to find out the number of unique destinations users go to , median and mean of unique destinations .
So , I am looking for aggregate stats over all groupby results .
Also , I am not looking for stats on each groupby , but aggregate over all groupy results .
My ultimate goal is to delete the time from this column and join it back to the spreadsheet .
Pandas append perfomance concat / append using " larger " DataFrames
What I originally did was running the script without the pivot call , but then I run into memory issues on my local machine ( only 8GB ) .
Since there is lots of redundancy in each file , pivot seemd at first a nice way out ( roughly 2 / 3 less data ) but now perfomance kicks in .
If I run the following script the concat function will run " forever " ( I always interrupted manually so far after some time ( 2h > )) .
Concat / append seem to have limitations in terms of size ( I have roughly 10000-20000 columns ) , or do I miss something here ?
EDIT I : To clarify , each csv file has about 10-20mio rows and three columns , after pivot is applied this reduces to about 2000 rows but leads to 10000 columns .
I still wonder why append is not performing .
If you are appending or concating in a loop ( and the loop is inevitable ) , you should at least append the ` data_tmp ` to a list , and then after the loop concat them all in one time ` pd.concat ( list_of_all_tmp )`
The problem arises actually already when I append only two bigger pivoted tables .
If I collect this two dataframes in a list ( as suggested ) , concat them and then only pivot it is rather fast ( cannot do this for the full list of files , due to memory constraints ) , but if I first pivot each table and then append / concat it runs undefinedly .
( some random numbers that mimics the dataframe ) Because if I try to concat two dataframes of 2000 rows x 10000 columns , that runs in less than a second .
I tried to simulate the data , have a look please , it should take some to concat this pivoted data-frames , ( it highly depends on how many IDs are choosen to be in each file )
Maybe it makes a difference if I would assign integer column names and map them latter back , I would suspect that string comparison is slower than an int based index
As @USER mentioned , you should append all of the pivot tables to a list and then concatenate them all in one go .
Using your reproducible example code , I can indeed confirm that the ` concat ` of only two dataframes takes a very long time .
However , if you first align them ( make the column names equal ) , then concatting is very fast : #CODE
I figured two things : First , when you replace the id_list with an integer index ( simply add ` id_list = range ( 0 , num_ids )` instead of the simulated ids it has a massiv perfomance impact .
3 ) When using string based columns , make sure to use the align method before concat is called as suggested by joris
How do I merge DataFrames using a function ?
I am trying to merge DataFrames of stock prices with varying start dates .
My question is , how do I merge the DataFrame with my placeholder DataFrame ?
You can first create a list of dataframes and then concat them all at once .
It gave me an error saying unstack is not an attribute ... any idea why ?
@USER That error looks quite strange , because the function returns a ` pd.Series ` object and ` unstack ` is a native pandas method , not attribute .
I have two DataFrames with the same indexing and want to append the second to the first .
But I want it to append a new column where the indexes match : #CODE
Use ` concat ` and pass param ` axis=1 ` to concatenate the list of dfs column-wise : #CODE
You can also use ` join ` but you have to rename the column first : #CODE
Or ` merge ` specifying that you wish to match on indices : #CODE
In general , the ` concat ` approach is better .
If you have different indexes , you can choose to do an ` inner join ` or ` outer join ` .
I know I can make a partial function with " or " and my vector and apply it to the df , but this is probably unidiomatic and needlessly time-consuming .
where we added the ` strip ` method .
I am performing some data analysis with pandas in an ipython notebook and my end goal is to visualize some pivot table data in the notebook .
The pivot table functionality I am implementing is not basic pivot operation but with some advanced drop down selection features , and I have implemented these from this link : #URL
Now , I am looking to cut the step of saving json to local and then reading this json into an HTML file .
My thought was to merge the dataframe back onto itself on ` user ` , and generate counts based on unique pairs of ` fruit_x ` and ` fruit_y ` .
Unfortunately the merge yields two types of unwanted results .
Am I using the drop function wrong , or do I need to keep track of the index of the row ?
You can first use ` .tolist ` to convert column to a list and then use ` .join ` method to concat all separate words together .
You could use ` str.cat ` to join the strings in a column with a separator of your choice : #CODE
You can first stack the DataFrame to create a series and then index slice it as required and take the min .
The result of ` df.stack ` is a ` Series ` with a ` MultiIndex ` where the inner level is composed of the original columns .
Here's another method which avoids stacking and is a lot faster on DataFrames of the size you're actually working with ( as a one-off ; slicing the stacked ` DataFrame ` is a lot faster once it's stacked so if you're doing many of these operations you should stack and convert the index ) .
A hacky way , but should be fast , is to concat the shifted DataFrames : #CODE
I have a ` pandas.DataFrame ` with with multiple columns and I would like to apply a ` curve_fit ` function to each of them .
I can apply the function and get an array in return : #CODE
Can this be done with something similar to ` apply ` ?
I think the issue is that the apply of your fitting function returns an array of dim 3x3 ( the 3 fitparameters as returned by conner ) .
From there I need to return this in a list , but there will be years that correspond to the date with missing data , so I need to replace the missing data .
Here , you try to append ` np.nan ` to your " list " countsList - but countsList is not a list anymore , but a DataFrame !
I'm sorry , this is not the ` abs ` , you can only + or - 360 degree , ` 50 ` and ` -50 ` is qutie different .
OK , you can use a boolean mask after generating the column to conditionally add 360 to the negative values
Well all you're doing is creating a df for each column which seems unncessary , semantically they should perform the same , ` apply ` on a df calls the function on each column in turn ( as ` axis=0 ` is the default param value )
Use ` loc ` , place the boolean condition within the square brackets ` [ ]` and the column of interest after the comma so you are not performing chained indexing : #CODE
To match right row with certain accuracy , we can have variable diff = 0.3s .
To match right row with certain accuracy , we can have variable diff equal to 0.3s .
Or use the ` dt ` attribute to access the year : #CODE
But when I run the same script from my terminal , I get an error message : ` AttributeError : ' NoneType ' object has no attribute ' unstack '` #CODE
Can probably be improved using ` map ` or ` apply ` .
So to append two new columns to your ` DataFrame ` you can do : #CODE
I want to find the minimum for EACH file ( not just a single file ) and aggregate the minimums and dates from all of the various file into a single new dataframe .
You can either concat them to a single dataframe or just find the minimum in each one then the minimum of the minimums .
The former is what I'd like to do ( aggregate / concatonate them to a single new dataframe ) .
Could you show me how to create a new data frame and aggregate all of the relevant data to that dataframe ?
I've been trying to figure this out for awhile now and haven't been able to apply any of the solutions I've found online for splitting columns in pandas yet .
I have to apply the column split to 90+ consistently formatted columns .
I'm trying to then add the lat / long points to a leaflet.js map using d3 using Mike Bostock's tutorial - #URL
I can see how once I can access fields of data I might be able to lever d3 to create a map and a separate bar chart .
I then used this block #URL to get a working map .
Maybe a ` groupby ` in conjunction with ` apply ` ?
I'm not familiar with ` apply ` yet .
you can use apply on the groups , which allows you to transform a group .
group apply
@USER You can also define your own apply function to achieve the goal by using the ` cumsum ` trick .
Dataframe reindex on a column
I want to have this have reindex #CODE
Next , I want to separate out group_level and stack those 2 new factors : #CODE
I removed the pre-edit answer as the strip method wasn't good at all but then I thought of using replace which is in the second approach above .
If you do go that way , don't use strip , use replace .
I've edited the question to show hoe to apply it in your case .
Basically , I'm reading from json log files into a pandas dataframe , but the append function is what is causing the issue .
Searching " inplace panda append " , I found this : #URL
If you need to build a ` DataFrame ` up from pieces , it is generally much more efficient to construct a list of the component frames and combine them all in one step using ` concat ` .
Passing a dataframe as an an argument in apply with pandas
Ok , the problem here is the combination of ` func ` and ` apply ` .
The ` apply ` method of a dataframe applies the given function to each COLUMN in the data frame and returns the result .
So the function you pass to ` apply ` should expect a pandas Series or an array as input , not a dataframe .
will apply the ` sum ` function to each column and give a series containing the
Secondly , the ` args ` parameter in ` apply ` is only used when the function you are passing takes additional arguments ( besides the series , which should be the first argument ) .
The you might want to apply this to each column of a dataframe with divisor = 2 .
You can first create a dataframe that is the dot product of your price df and volume df , and then groupby on ` date ` ( which can be obtained by ` .date ` on a ` pd.DatetimeIndex `) and aggregate by ` sum ` .
For your case , just apply the same ` groupby.agg ( sum )` logic to hourly price dataframe , and then calculate dot product with daily volume data .
You should use ` resample ` to calculate the mean price for the day , and then apply it to the volume : #CODE
This multiples price times volume for each matching column , sums up the total for all columns in that row ( i.e. hour ) , and then uses resample to sum up the results for each day .
This happens even if I strip the zeros out of the original file and call ` to_sparse() ` ( so that the fill value is NaN ) .
What I want to do is replace all 0's in the sex column with ' Female ' , and all 1's with ' Male ' , but the values within the dataframe don't seem to change when I use the code above
Also , you can combine the above into a single ` replace ` function call by using ` list ` for both ` to_replace ` argument as well as ` value ` argument , Example - #CODE
Maybe there is work around with just using a pivot table with the aggregate function as count
No promises that this is the slickest way , but I think you can get where you want to go with two groupbys , and a ` cut ` to get the levels : #CODE
I executed groupyby separately it is fast takes 5 seconds only.I think it is transform taking long time here .
the ` transform ` function passes the entire group the arguments for the handler function , it means that pandas needs to store this data somewhere , This method ( inspired by map-reduce patterns ) does not store the entire group data , only the aggregates ( in ` res `)
If you want to transform the output , then doit like this #CODE
As i understand in this case using transform I am running loop for 50m elements .
I think it's the transform which is causing the problem .
During the transform , for me it quadrupled ( so 3x extra ) and took 2 min .
If the OP is getting deep into swap , I could believe things would drop to a crawl .
( Plus it seems a little weird that it takes me 8x longer to do the transform than the sum , which should really be just the sum + a repeat . )
@USER this is on master , fast transform has existing IIRC since 0.16.0 ( maybe before ) .
if_exists : append : If table exists , insert data .
Here `' columns '` is equivalent to ` df.columns ` and the result of ` eval ` is a Serials whose index is ` df.columns ` , therefore when the columns are named , the result of ` eval ` cannot be used as index of the original DataFrame .
In the first example , it is just happened that returned Series can be used as index , but it might not apply to the general case .
I have a dataframe and would like to replace each cell value based the formula
The problem with the ` apply ` / ` map ` / ` applymap ` functions is that they don't
Here is a faster code using ` apply ` , but it will provide wrong result in case there are 2 or more months in the same column with the same value , because ` np.where ` returns an ` np.array ` of the indexes that it found the value of ` x ` in , but there's no way to store it and use the next index the next time we encounter the same ` x ` value : #CODE
Instead , you could unstack ` df ` , and use its index , ` df.stack() .index ` , and ` pd.to_datetime ` to parse the index and column labels into dates : #CODE
and unstack to obtain the desired result : #CODE
Since ` value.Count.values ` and ` np.nan.values ` return arrays with one item , you can instead append the item to ` countsList ` directly : #CODE
Care to join me there ?
Sure , let me join now .
I need to reindex the 2nd level of a pandas dataframe , so that the 2nd level becomes a list ` np.arange ( N )` for each 1st level index .
I think you can first set columns ` first ` and ` second ` as multi-level index , and then ` reindex ` .
If you want to have a field in your original database to indicated whether or not the data is consistent , you can use ` transform ` .
In this context , it was a bad idea to use the ` apply ` function .
Below is the code I apply to get the fitted volatility from the regression equation y = ax^2 + x + c and the results are great .
I need to have the regression variables and FitVol apply to all of the original data , not the data that was filtered to have the abs ( Delta ) be between 0.01 and 0.5
I want to merge these two together so that all instruments ( i.e. , A , B , C , D , ... ) can be shown in the same file with all of the measurement time periods .
With no overlap I think ` append ` or ` concatenate ` would be the more common and straightforward way to do it , but I don't see any problem with using ` combine ` or ` combine_first ` .
Assuming ` first ` is ` df1 ` and ` second ` is ` df2 ` , using ` concat ` appears to solve your problem .
Before I was using the csv module to append columns to a csv and re write it , but to be more efficient I want to load it into a pandas data frame .
I am having trouble obtain the desired results through transpose ....
Or , alternatively , you could set ` id ` as the index before calling ` stack ` : #CODE
` stack ` moves the column level values into the index .
the ` id ` column into the index first , and then to call ` stack ` .
You haven't to apply ` for ` loop or ` iterrows() ` at all in pandas : #CODE
Is there a way in pandas to merge two data frames with varying lengths by using a conditional statement ?
You ** cannot ** join on a conditional statement with any pandas functions .
To align the index and fill the gaps #CODE
Or I should ` drop ` instead ?
Group by then apply function then flatten back to dataframe in Pandas Python
However , to avoid using loop , you can do something like this : get the 1st observation using ` head ` and the delta's using ` diff() ` , then ` concat ` them together .
This will make sure the ` diff ` operation orders by quarter
pick the columns you want to diff , and apply ` pd.DataFrame.diff `
Now apply the logic described above : #CODE
and , if you want , join the differenced columns back to the original ` df ` : #CODE
Clearly there's a little bit of work still to do to tidy up ( i.e. drop the ` NaN ` values if you want ) but that's left as an exercise !
I'm trying to make a pivot table using pd.pivot_table .
Check this documentation of pandas pivot table function .
Merge multiple DataFrame columns into one
I'm trying to transform a DataFrame with a dynamic number of ` a_P ` columns looking like this #CODE
A possible solution using Pandas ` concat ` ( #URL ): #CODE
And of course , soring by ` P ` is easily done by appending a ` .sort ( ' P ')` to the end of the melt statement .
This is exactly what ` melt ` is designed for .
You can call ` map ` on a temporary column and pass your other df ` growth ` with setting the index to column ' year ' , this will perform the lookup : #CODE
I tried agg ( bitwise_or.reduce ) and got a cryptic error .
I need to be able to keep track of when the latest maximum and minimum value occurred within a specific rolling window .
For example if I were to use a rolling window period of 5 , then I would need an output like the following : #CODE
All this shows is , what is the date of the latest maximum and minimum value within the rolling window .
And I want an expanding apply function that identifies whether we reach a new maximum value for a given id .
I can't seem to pass two columns to the expanding apply function .
Its been a long time since I worked with ` apply ` like a couple releases ago minimum , so my recollection may be bad , or things may have changed .
The temptation when passing your own function to ` apply ` is to do this : #CODE
Python Pandas : Merge or Filter DataFrame by Another .
One situation I sometimes encounter is , I have two dataframes ( ` df1 ` , ` df2 `) and I want to create a new dataframe ( ` df3 `) based on the intersection of multiple columns between ` df1 ` and ` df2 ` .
I know I can do this with merge ...
but then I have to figure out how to ` drop ` columns that end with ` _del ` .
Is there a better way to return ` df1 ` without resorting to ` merge ` ?
Is there a way to ` merge ` but NOT return ` df2 `' s columns to the ` merge ` and returning only ` df1 `' s columns ?
You can first set those join-key columns as index and use ` df1.reindex ( df2.index )` and a further ` .dropna() ` to produce the intersection .
In example data above , this works great but when I apply this concept to my real datasets , I get ` Exception : cannot handle a non-unique multi-index !
This concept seems like a great alternative approach to ` merge ` .
You should be able to replace that whole function using #CODE
I want to apply a matrix multiplication in these columns , that is to replace columns 4 , 5 and 6 with the vector resulting of a the multiplication of the previous vector with a matrix .
You can use ` values ` property to get the ndarray and use ` dot() ` to transform all the vectors at once : #CODE
The only reason to use the loop is to cut down on boiler-plate code .
I can drop the " df " in ' dfA '
split the columns up into a multiindex with fruit / year / month as the levels , stack it up , group as needed .
How to correctly use pandas agg function when running groupby on a column of type timestamp / datetime / datetime64 ?
I am checking for rows that have missing values , but there's no function for that , so I have to do ( len - count() ) , so I pass a list of functions to agg() .
x.groupby ( lambda x : True ) .agg ([ ' count ' , len ]) .
If I leave the column type as a datetime , the len function's output is coerced to a datetime .
One ** could ** use ` df.apply ( lambda x : x.isnull() .sum() / float ( len ( x )))` for that case , but it's also useful to see stats at various levels of aggregation , so using groupby makes for a more uniform treatment .
If you have several summary stats you want to gather ( e.g. ` len `) , you need to pass in a list of functions to ` groupby() .agg() ` .
How to set values of a masked Pandas Series from a different mask
I would like to extract some elements from that series using a mask and put them back to same or different locations in the same series using a different mask .
When setting values on a Pandas series , the index is used to align values .
I imagine it'd be better to do this either using fillna or loc , but I can't figure out how to do this with either .
@USER categoricals are more restrictive than strings in some ways , e.g. you can't just append more rows if those rows contain previously unseen strings .
if so i would gest that the apply method is a good hint here but like @USER -sc i would suggest you to restructure your data model or have a look at pytables etc
Create mask based on multiple conditions that compare datetime64 timestamps
I would like to create a mask column that is True if :
If you now assume that something / somebody who has no ` Time_in1 / Time_out1 ` did not have a break , you can replace the empty strings ( or ` NaNs `) with a datetime outside your query , for example : #CODE
basically what I want is to append the values of each column ( but B ) to the data frame and add one more column that has as values the name of the column associated with each value .
A more general approach is to use ` stack ` , but this requires a bit more manual adaptations : #CODE
Yes , this is indeed the easier way than the manual ` stack `
How do I resample a Pandas dataframe containing datetimes without losing tzinfo
I have a dataframe that I want to resample .
Coerce back to strings , join and parse .
drop a series of true false in pandas removes first two lines
Also only giving it a list of ` False ` will just drop the first line .
The pandas drop function takes a list of indicies to drop , not a mask .
How to properly use the drop method
The proper way of using masks to drop data is either to mask , then access the index and hand this to the drop function : #CODE
where you could replace ` [ " D " , " E "]` by ` list_of_columns ` so you wouldn't have to repeat the column names .
Plotting multiindex DataFrame bar plot where color is determined by category
I have a multiindex DataFrame that looks like the data below .
I am trying to create a ` DataFrame ` with a rolling cumulative percentage change .
With a rolling cumulative percentage change .
You can create a column with the mutation type ( A -> T , G -> C ) with a regular expression substitution then apply pandas groupby to count .
We can do this very easily with a mask : #CODE
You can transpose the df and apply a lambda that drops the NaN rows , slices from 4th value onwards and returns the first valid index : #CODE
Pandas read_SQL is not working , I am trying to access a SQL pivot function ( Example 1 ) .
I am trying to plot a map using basemap of the US and plot Los Angles , Chicago and New York on my map .
I am trying to transform this DataFrame :
` stack ` moves column level values into the index .
So it is natural to think about calling ` stack ` : #CODE
The desired DataFrame only has ` D ` s in the index , so let's drop the outer level values : #CODE
and then calling ` unstack ` to move the index level to a column level : #CODE
` stack ` and
` unstack ` .
Pandas : groupby and transform with datetime
Thanks for your answer , but I need the transform because I need to preserve the original dataframe for further processing .
I want to munge the data , calculate some stats and create a pivot table .
You'll also find that ` len ( u ' \u043f\u0440\u043 8\ u0432\u0435\u0442 ')` is 6 .
I am trying to join the dates and forward fill the values .
sorry are you wanting an outer merge : ` a.merge ( b , how= ' outer ') .ffill() ` ?
You can't update both dfs in place in a one liner , you can either update 1 or create a new df , not update both , additionally why don't you just assign back the result of the merge , I don't see anything wrong with doing that
This will join and align on indices taking the union of both dfs , this will introduce ` NaN ` values which you can fill using ` ffill `
From the result of the above you can just assign back the cols of interest , also it looks like what you really want is to reindex using the union of the indices : #CODE
but with a larger array ( more nonzero elements ) I get a display error .
I have a dataframe that I want to insert into a ` mysql ` database .
What I'd like is not to forward fill the weekly data point for every day of the week ( since it is usually a sum of all the values of during the week already ) , but forward fill or replace the data with it's mean .
Here is one way using ` .cumcount ` to separate series into different groups and then ` transform ` .
@USER then may be you can use the idea and apply to panda DataFrame
Pandas has lots of rolling aggregation , resample , and grouping operations .
Note that you can pass ` np.arange ( len ( df )) // 20 ` to groupby in case the index isn't already the standard .
In case you need to convert existing columns in a dataframe here the solution using a helper function ` conv ` and the ` apply ` method .
To only test your code , replace the `' UTCTIME '` string with a actual date string or use a variable with the string .
If you don't want to keep the index , drop it afterwards : #CODE
So using your method , I would need at least 3 different actions for each time I want to plot ( save a new variable , compute new index column , then drop it ) .. that makes things quite slow :/
I am trying to extract grouped row data from a pandas groupby object so that the primary group data ( ' course ' in the example below ) act as a row index , the secondary grouped row values act as column headers ( ' student ') and the aggregate values as the corresponding row data ( ' score ') .
So , for example , I would like to transform : #CODE
I have spent some time looking through the groupby documentation and I have trawled stack overflow and the like but I'm still not sure how to approach the problem .
Looks like my evening is going to be spent with the pivot documentation .
` print ( l.get_results() )` returns the Series's as I expected them to be , however , ` print ( series )` returns an empty series and I don't know how else to append the Series's to ` series ` .
Merge CSV into HDF5 in pandas leads to crash
I want to merge them into a single ` HDF5 ` file .
I have also seen it pop up a window showing a stack error .
then resample daily : #CODE
Is there a way to resample just one column , and preserve the value in another column at the same index ?
My first guess is to just resample the ` heat_index ` column ...
Here's one way - the ` .groupby ( TimeGrouper() )` is essentially what ` resample ` is doing , then the aggregation function filters each group to the max observation .
What are the feature ranges ( min and max ) before and after polynomial transform ?
This may be some limitation of ` concat ` and probably not a bug but something where you have to decide how the data should be merged / aligned
I found it was referring to the first index my solution was : ( thought not sure how efficient it is but the concat works afterwards ) #CODE
The above uses ` dt.week ` and ` shift ` s by 2 rows and then forward fills the ` NaN ` values .
does the ` -2 ` shift come from Saturday instead of Monday as first day of the week ?
But it won't work in my code because i have duplicate dates in rows , e.g. 3 rows ' 2015-08-09 ' , 10 rows ' 2015-08-10 ' , etc ... then the ' shift ' won't work in this case .
I am sure this can be done using pandas merge and then outputting the rows that didn't merge , but I just can't seem to get it right .
I am able to do both tasks separately to find mean and mode for particular columns , but I can't work out how to join the two outputs together , but then I was also wondering if there is a way to calculate mean or mode of selected columns in one hit ?
In my current method , although I am able to do both tasks separetely I am struggling to join the two outputs together .
Especially the ` list ` in fields of a ` DataFrame ` , they make loops or ` apply ` almost unavoidable .
Especially the list in fields of a DataFrame , they make loops or apply almost unavoidable .
Python pandas : retrieve the field associated to the min of another ( cross apply equivalent )
In SQL I was used to doing this with a cross apply .
PS other than calculating the min first , then doing a join on primary key and date
I can do this in two steps : 1 ) group by primary key and calculate min ( date ) 2 ) do an inner join between the starting table and the table calculated in the previous step , on primary key and date , to retrieve the amount
Python & Pandas : Unable to drop columns
I try to drop the data , but it reports some column does not exist .
However , This it cannot ` drop ` ` NCDC ` either .
You can then drop your columns : #CODE
@USER yes what you suggested gives the OP what they want , I've updated my answer , I should've realised that doing the ` resample ` after grouping should work
If you set the index to ' date ' then you can ` resample ` quarterly : #CODE
Thank you , I am getting ` DataError : No numeric types to aggregate ` when I do this on my DataFrame though .
Yeah this gets tricky here you'd have to merge / add back the additional info if you want to retain it
You could strip off the index , join the frames , then add back the index #CODE
type switch from int to float64 after merge creating error
I'm trying to merge two dataframes in Pandas .
However , after the merge , the type is switched to " float64 " for some reason .
Note that this is not my join column
I'm trying to append to a ` Series ` Object within a loop as seen in the code below , but find that the appending is not taking place at all .
` bond.to_cash_flows() .amounts ` results in a Series like the one shown below and I am just trying to append to this Series .
So , how could I replace this line ?
I'm getting an error saying ` raise KeyError ( ' %s not in index ' % objarr [ mask ])
` cashflow_series ` is the Series , and I am just trying to add a certain number of months to this date by using ` replace ( month = cashflow_series.index [ k ] .month + dict_no_months_between_payments [ count ])` on the index and use this to replace the previous index date .
Would you know how I could do what I am attempting by just changing a ** single index ** of a Series , as ` reindex ` would change all the indices ?
` cashflow_series ` is the Series , and I am just trying to add a certain number of months to this date by using ` replace ( month = cashflow_series.index [ k ] .month + dict_no_months_between_payments [ count ])` on the index and use this to replace the previous index date .
I am currently using ` reindex ` but that would change all the indices and hence I'm stuck as to how to change just one index .
You need to create a copy of the old index values , modify it , and then replace the one in your series with this copy .
@USER , the ` snap() ` method could still work with some basic checks : if the snapped date is before the given date , move the given date forward by the given interval and snap again , then go back a day .
" Does it only snap forwards ?
Updated answer does " snap forward " .
) bulk insert using the mapper and pandas data .
Furthermore , ` to_sql ` does not use the ORM , which is considered to be slower than CORE sqlalchemy even when using bulk insert ( #URL )
But pandas ` to_sql ` does not use ORM at all , as I said before , and is in fact already doing a bulk insert .
The Class Current needs to align with the dataframe imported in the CSV and the table in the db1 .
How to change color bar to align with main plot in Matplotlib ?
You then use ` .loc ` again , but this time as a mask to filter out the bad records .
If you want to apply an arbitrary Python function , you will have to loop it .
And if I sacrifice the arbitrariness and just want to grow by a constant ( rate ); i.e. a rolling increment sort of thing ?
Can you write out the formula you want to apply ?
If I don't include the second line ` hist [ ' bins '] = hist.index ` , I still get an ` AttributeError : ' Categorical ' object has no attribute ' flags '` and to the best that I can tell , the traceback is identical .
Can someone explain what the ` flags ` are and why they only seem to work when I set the ` index ` to ` bins ` and then replace the ` bins ` by the version stored in the ` index ` ?
Then drop duplicates : #CODE
Consider using a pandas.merge and then drop any resulting joins .
I am trying to append to a Dataframe dynamically , but get the error ` ValueError : Incompatible Indexer with Dataframe ` in the line ` df.loc [ count ] = pandas.DataFrame ( amounts ) .T ` .
AttributeError : ' bool ' object has no attribute ' all '` I'm using ` In [ 12 8] : np.__version__
I can replace it with a tuple , and still get the scalar .
I am having problems with a sort on a index of a multiindex pandas dataframe .
The same can be seen when plotting the index of the first sorted pipeinfoDF , where the sort_index keeps the order of the levels in MultiIndex , but only changes the order of the labels .
My question therefore is : is there a way to loop over the rows of a multiindex data frame which takes into account the labels order ?
I can not imaging that you can not use the groupby method on sorted multiindex dataframes without messing up the sort order .
Still , you need to keep a list in order to prevent double counts of the label , because unique apply only to a unique ( label , side ) combination .
Is it possible to have unique() apply on the label alone ?
sorry , n = len ( conditions )
Unfortunately , when you zoom or pan the plot , the labels change and do not align with the data points .
Why is there a mistake in my replace function ?
Why the replace function can't transform 0.01 to 3 in first print statement ?
On a column you could also do ` .map ( lambda x : min ( x , 0 ) )` to apply the standard python ` min ` to each cell , but ` np.minimum ` is probably going to be the fastest way .
OR you can apply over two columns : #CODE
Now I have two columns in my dataframe - old ' col3 ' and new ' c ' and need to drop old columns .
This way , you can apply above operation on multiple and automatically selected columns .
Pandas : resample timeseries with groupby
I would like resample the data to aggregate it hourly by count while grouping by location to produce a data frame that looks like this : #CODE
use ` unstack ` to move the ` Location ` index level to a column level : #CODE
trouble accessing multi index column after join
However when I join the above table with another one , I can't figure out how to access the table anymore .
I have a pivot table in a data frame and I'd like calculate group percentages .
Divide the dataframe by sum of the number of games per country and team , obtained using ` groupby ` and reshaped using ` transform ` .
To make this more clear , you can see what ` transform ` is doing .
You can also use ` transform ` in the method proposed by @USER and skip his last step : #CODE
The goal is to loop files , fetch content from those files and append them to the dataframe ` df ` .
But what I don't get is , how does ` append ` know how to interpret the entry , i.e. how does it know it should merge the entry with the already existing column names ?
Merge two pandas dataframes on not exactly matching timestamps
I need to merge these to dataframes into one on ' time ' column so that for records 3 , 4 , 5 from dataframe 1 indexes 1 , 2 , 3 from dataframe 2 were on the right .
Is there a good way to merge it the way to merge these on time with time not exactly matching , but given some matching threshold maybe ?
Assuming your time is a string , one thing you could simply do is just strip out the last two or three digits of the time and then perform the join .
I need a way to uniquely join those records , meaning find a single closest matching time ( smallest absolute difference ) for each record .
After accumulating the matches , just drop duplicates to keep the more precise matches .
Then I can simply merge groups1 with groups2 on common index .
And if you want to append the new columns to the old data frame , simply use the same ` DataFrame ` for both input and output
You are calling ` replace ` on the Series object .
This is not the string ` replace ` method but a pandas method that replaces entire values .
and I am getting the following stack trace : #CODE
What I want to do is append DataFrame2 values to Data Frame 1 ONLY if the ' MedDescription ' matches .
I had an iterative solution where I access the dataframe 1 object 1 row at a time and then check for a match with dataframe 2 , once found I append the column numbers from there to the original dataFrame .
This sounds like an opportunity to use Pandas ' built-in functions for joining datasets - you should be able to join on ` MedDescription ` with a the desired columns from DataFrame2 .
The ` join ` function in Pandas is very efficient , and should far outperform your method of looping through .
This is the way I used to join the 2 DataFrames , it seems to work , although it deleted one of the Indexes that contained the devices .
It sounds like you want to merge the target columns ( ' MedDescription ' , ' Min ' , ' Max ' , ' Days Unused ') to df1 based on a matching ' MedDescription ' .
Export pandas DataFrame to LaTeX and apply formatters by row
I know about pandas ' transpose , but I want to format my rows and the method to_latex supports only formatters by column .
Only thing I thought about was to manually apply the formats converting all my columns to strings before transposing and exporting
The ` interploate ` method in ` pandas ` use the valid data to interpolate the ` nan ` values .
To all the answerers : as the OP mentions pandas , it may be desirable to enclose in apply / lambda .
` abs ( x * 10 ) % 10 ` works for both positive and negative numbers .
For this , you can use ` ix ` : #CODE
Again , the index is a DateTimeIndex with dtype = ' datetime64 [ ns ]' , length = 1412 , freq = None , tz = None
Expanding on what @USER is suggesting , you could also slice by reversing the order in one of two ways : ` df.loc [ ' 2015-08-12 ' : ' 2015-08-10 ']` or ` df.loc [ ' 2015-08-10 ' : ' 2015-08-12 ' : -1 ]` But without an explicit sort ( a good suggestion of course ) , it's not necessarily implied that the results of the slice will be sorted either .
For computing median , it should be done according to count of occurrence of every instance ( in column ) or just according to instance ?
I need to find rolling mean per day .
Then I iterate thru the dates and calculate the rolling mean : #CODE
The error disappears , but I do not get the rolling mean .
` pd.rolling_mean ` simply takes the rolling mean of the most recent 200 daily observations .
By using this method , I understand that it will take the average per day and create a rolling average for the daily averages .
I need a rolling average per day .
As in calculating the rolling average for the 50 000 datapoints I have for a day , and start all over again the next day , just using the datapoints for that day .
What you are doing here is taking each of the lists you created above using the map function and then placing it inside another list .
There is one difference though : ` .drop ` throws an error if you try to drop column or columns which don't exist in the DataFrame , while ` .difference ` doesn't .
For completeness , you can also easily use ` drop ` for this : #CODE
one more question , will I manually ` e = re.sub ( ' / ' , ' - ' , c )` and apply ` to_datetime ( e )` that can improve the performance ?
Supplying an ISO formatted datestring is a bit faster to parse , but that will not outweigh the time to replace the ` / ` with ` - `
You could use reindex to expand the DataFrame , and TimedeltaIndex to add the hours : #CODE
The only thing I cannot understand now : in the second one you used ** for i in range ( 1 , 4 ): ** - as far as i've got it , I need to replace it with custom numbers , by , lets say , calculating the max N in df , and then checking : if row.N <= maxN , then proceed .
I thought maybe the problem was that in some entries there was one or more spaces before or after " LARKFIELD " in the incident description , so I did a search / replace to try to strip out any spaces , but I still get only 115 values when searching by " LARKFIELD , " even though I know there are many more incidents in that area .
What does ` len ( larkcounts )` show ?
I thought whitespaces might be the problem and had tried to strip them out , but being relatively new to Python and pandas I think I may have not done that correctly .
I'm sure there's an easy way to do what you're trying to do but pandas may not be much help for the data gathering itself and this is not really a good stack overflow question ( I'm not sure where to suggest you look for the answer though )
I think what your asking ( correct me if I'm wrong ) is best accomplished by putting the two columns in a single dataframe , using ` shift ` to offset one of your columns , then doing an ordinary subtraction .
This will merge ` df_a ` and ` df_b ` on all columns shared in common .
Yes , that's actually better than my concat :D thanks .
shift by partition using groupby
I have a dataframe with one column I would like to shift , but over partition rather than the whole dataframe .
You can store your intermediate DataFrames in a list and use ` pd.concat ` to join them together : #CODE
Python join - how to join a data in loop ?
How can I join a data below , #CODE
If you want to keep your method of looping through each , then you can simply remove the last ` / ` by doing ` rstrip() ` on it to strip from the right side .
You're performing [ chain indexing ] ( #URL ) which is why you get the warning , you should use the new indexing methods ` loc ` , ` iloc ` or ` ix ` to ensure you're working on a view rather than a copy
you already accessed the column before calling the ` .apply() ` method on it , notice that you call the apply function as - ` df [ 0 ] .apply ` , which means apply it in 0th column of ` df ` dataframe .
A last column ( Floods ) indicates when this is the case ` abs ( df [ ' Flood_Index ']) 1.5 )` .
how to transpose dataframe ?
You can think of this as a ` pivot ` .
Ewma in pandas but over rolling weekly data .
I'm trying to calculate the ewma in pandas on a " rolling weekly " way .
After doing this if i want to get a " rolling weekly ewma " that includes every day of the week , I need to combine each vector that was generated .
Now this combined vector ( of each day ) is the " rolling weekly ewma " I'm talking about .
and lastly join them back to the original frame with : #CODE
Now I take my dataframe above and replace first digit 8 with digit 9 #CODE
Pandas extrapolating data with resample
And I am trying to resample it to a frequency of , say , 5 minutes .
if I apply a groupy say with column col2 and col3 this way #CODE
remove overlay text from pandas boxplot
I am trying to remove the overlay text on my boxplot I created using pandas .
I just want to remove the " boxplot grouped by 0 ...
I basically want to insert the indices of those values of ` s ` which are not 0 into a list ` l ` , but don't know how to do this .
I would like to take the mean of each array per row and replace it with column ' X ' ; #CODE
Could you recommend any NumPy data structure to replace the dictionary ?
I chose the ` dict ` -> ` list ` combo for fast key-search and fast append that doesn't require reallocation of the entire array .
One option is to use groupby and apply to end with a pandas Series : #CODE
The problem is ( see below ) , the data I'm interested in is after the large drop in values , I'd like to see a bit more clearly the difference between the green and red lines .
I think you should cut out the data that you want to see and make a graph of that .
Just cut away all values that are too high and make a new graph .
Within each group , I drop the NaNs , but then reindex to the full h1 thought h4 set , thus re-creating your NaNs to the right .
It's fine if there's a join in an intermediate step - I'm mostly worried about long-term memory waste .
I would do a left merge and then argsort : #CODE
Note : that if you used a different index ( for fruits ) , it will be ignored / replaced with ` range ( 0 , len ( fruit ))` .
Now reorder using iloc ( by position ) rather than loc ( by label ) .
Note : It's important to left merge as an ordinary merge will change the order ( !! ) .
why don't you merge the columns and then drop the unneeded one #CODE
Can I map the ' make-model-variant ' column to a set of unique integers ?
` a.sort() ` modifies ` a ` and does not return anything so replace by : ` a.sort() ; print a `
I would like to calculate the rolling mean of a time series but using a list of different window sizes .
where the above method would return a DataFrame object of rolling means of varying window lengths of the series argument .
If you really want to vectorize this you'd have to do it explicitly by re-writing rolling_mean , or rolling your own with numpy , numba , cython , or similar .
Say for example you want to calculate the rolling mean over ` windows = [ 20 , 40 , 60 , 80 , 120 , 250 ]` #CODE
This will create a dataframe with each column being the different rolling mean .
I can generate week number and aggregate sales based on those , like so ...
You could replace with datetimes which have 00:00 : 00 in the time by doing #CODE
` resampled = df.resample ( ' D ' , [ sum , len ]); `
` period_range = pd.date_range ( start = ' 2014-01-05 ' , end = ' 2014-12-31 ' , freq= ' 7D ') ; resampled = df.resample ( ' D ' , [ sum , len ]); resampled.reindex ( period_range )`
` df.resample ( ' D ' , [ sum , len ])` gives daily sums .
If you want weekly sums to Sunday you have to resample with weekly frequency .
So ` resampled = df.resample ( ' W ' , [ sum , len ]); resampled.reindex ( period_range )` seems to work , right ?
" Expanding Mode " Analogue to Pandas " expanding_mean "
First , note that you could write your expanding mean like this .
This doesn't apply to Python lists , since the length and item size can vary arbitrarily .
I would probably do something like ` df [ ' num_choices '] = np.array ([ len ( row ) for row in df.options ])`
I want to insert values from one data frame to another data frame , when a particular condition is met .
I want to insert it to the top of the dataframe .
Once I do this , one value from pl is insert to the top of the sub data frame . but I am having a problem here .
Then I use ` loc ` to get the index values of non-negative values in the series .
If you use the solution 3 ) , you have to replace the data stored in database every month .
That's the result of the grouping operation that I want to apply .
This is like a resample operation .
Might be simpler just to resample .
assert len ( df.groupby ( pd.Grouper ( freq= ' ms '))) == 1
This argument is new in 1.9 ... but there is a workaround , try ` np.linspace ( 0 , len ( pep_list ) , n+1 , endpoint=True ) .astype ( int )`
Another option where you can control the format is using the ` strftime ` method in an apply ( this would actually be equivalent to writing a loop , but shorter ): #CODE
merge few pivot tables in pandas
How I can merge two pandas pivot tables ?
After each operation , drop all relevant columns , then finally count all remaining columns .
` df.precedingWord.isin ( neuter )` is just a Series of True or False ( results of the previous test ` isin `) , and pandas will just access True indexes with ` loc `
However , when I apply this to my full dataset , with multiple dates in utctime , the x-axis remains a time - I want it to show the dates in this case .
I have tried converting .to_julian_time and to_py_datetime after following some other stack posts .
Groupby on level 0 ( parameter1 ) and apply ` idxmax() ` and get the values : #CODE
It's a useful and common practice to append predicted values and residuals from running a regression onto a dataframe as distinct columns .
Dont drop non present indexes on group_by - Pandas
You can reindex with the complete list of categories after the groupby : #CODE
Can you pass a 2d array to reindex ?
You have to pass either a MI or an array of tuples ( I don't think a plain array will cut it ) see ` MultiIndex.from_arrays ` or ` MultiIndex.from_product ` ( you * may * have to splat the array ... ) .
In your case you have to reshape the pandas column to a matrix having ` len ( data )` rows and 1 column .
Go through the matrix line by line , than apply in each element ` ord() - 65 ` if it is an alphabet else use it as it is .
drop the ` NaN ` value when converting DataFrame to dict , and then
The JSON C source code explicitly tests for ` int ` , ` long ` , ` float ` and ` bool ` keys , converting all those to strings .
Juse use ` abs ` : #CODE
With pandas : if , in a row , a word in a column does not occur in string in other column , drop row
How can I drop the rows in which ` one ` doesn't occur in ` two ` ?
For instance , on the third row ` banana ` doesn't match ` bananas ` : drop row .
In the fourth : ` hello ` doesn't match ` hello-kitty ` : drop .
Another method would be to calculate the list of indexes to drop and store them in a list and then at the end use ` DataFrame.drop() ` .
I am not sure if there is a better way to do this , but you can iterate over each row and then split the string in column ` two ` and then check if the string in column ` one ` exists in it or not , and then append the rows that match to a new dataframe .
Hi @USER , Q1 . result is a list , before you append any element to it you have to declare a list instance of it .
pandas - pivot table to square matrix
I would like to pivot it , and then return a square table ( as a matrix ) .
So far I've read the dataframe and build a pivot table with : #CODE
` reindex ` can add rows and columns , and fill missing values with 0 : #CODE
You can use a combination of ` stack ` and ` unstack ` : #CODE
Use ` groupby / agg ` to aggregate the groups .
For each group , apply ` set ` to find the unique strings , and `'' .join ` to concatenate the strings : #CODE
groupby / agg .
[ ' count ' , ' mean ' , ' median ' , ' min ' , ' max ' , ' mad ' , ' std ' , ' var ' , ' sem ' , ' skew ' , ' quantile ']) .sort ([ 1 ])` by tring to sort on the ` column [ 1 ]`
I need to replace all the spaces in a dataframe column with a period .
I append two blocs of dates using something like ` dates.append ( date_range ( df.DateStart.iloc [ i ] , df.DateEnd.iloc [ i ]) )` , looping on both clusters and date intervals , but the appending results in a non-flat list .
It is more complicated , because using ` multiindex ` : #CODE
It's working alright using df.loc and mask but I want to make it automatically get current year data and last year data without entering it manually .
If the mask above misses ` 2015-01-01 ` it is because that date is not in your Series .
Since pandas ' ` resample ` function has a ` how ` keyword that is supposed to be any numpy array function , I thought that I could maybe try to use resample to do that with ` polyfit ` , but apparently there is no way ( right ? ) .
reindex on a multi index changes structure and drops index names
I am attempting to to a reindex of a multi index Pandas.Series using a list of tuples .
After I do the reindex as follows #CODE
I ended up building an new Index / MultiIndex , naming it correctly and then doing the reindex on this .
Pandas map 0 / 1 data frame entries to column names
convert the dtype of the df to a ` bool ` , then call ` apply ` and use the boolean mask to mask the columns , you need to pass param ` axis=1 ` to ` apply ` the column mask row-wise : #CODE
Your code ` my_df.apply ( lambda x : colnames [ x ])` won't work because firstly when calling ` apply ` on a df without specifying the ` axis ` will call the lambda on each column in turn , secondly the ` 1 / 0 ` will interpret this as an index value rather than a boolean flag .
I was doing some reading on google and the sqlalchmey documentation but could not find any kind of built in functionlity that could take a standard sequel formated table and transform it into a cross tab query like Microsoft Access .
And I want to pivot the data as follows :
Pivot : Call ` pivot = df.crosstab ( ... )` to create a pivot in memory .
You could ` fillna ` to replace the missing values - passing in a ` DataFrame ` with the last value of each group .
So even the sum of a short list of mask is converted to a 64 bit integer ( which can then easily be converted to a boolean mask array ) , and you won't run easily into the integer limit .
And very interesting about the mask just being a list of integers -- I didn't know that .
Append with date_range() in Python
The issue occurs because you have three columns with only ` NaT ` values , which is causing those columns to be treated as objects when you do apply your condition on it .
You should put some kind of condition in your ` apply ` part , to default to some timedelta in case of ` NaT ` .
Or a simpler way to change the ` apply ` part to directly get what you want would be - #CODE
( The timings are at the bottom . ) The key is simply to avoid using ` groupby ` at all in this case by replacing with ` shift ` and such -- but because of that you also need to make sure your data is sorted by the groupby column .
about 40 times faster on the multiindex data from the question .
Now transpose the df and perform a list comprehension , this will construct your lists and concatenate them using ` pd.concat ` : #CODE
It feels like the data may be in a particularly unhelpful shape , but I don't know how to make it better - I tried a pivot table with the categories as columns , but since utctimes are unique to categories , that didn't work .
To find the total number of minutes in each group you could aggregate using a lambda function such as #CODE
a bit more idiomatic instead of the agg to do : `` ( grouped.last() - grouped.first() ) / pd.Timedelta ( ' 1m ')`` , might be faster as well , though YMMV .
No way to group AND perform this type of calc ( mainly because the grouping zonks the freq ) , but just adding a column solves the problem .
Add in an extra column that we want to diff as well #CODE
add the binary flag with your grouper , or group at a lower freq . not really sure what you are trying to do .
You can do this by combining ` resample ` and ` len ` .
To use ` resample ` , first set the index to `' Time and Date ` : #CODE
I must say you understand correctly , but when I try to resample I get and TypeError : Only valid with DatetimeIndex , TimedeltaIndex or PeriodIndex .
This works , but since I need hight granularity on us , resample on us hangs.is There any solutuion for that issue ?
And note that this is easier than converting both to dataframes in that you only need the indexes ( axis=0 ) to align when multiplying series , but you need both indexes and columns ( axis=1 ) to align when multiplying dataframes .
Now I want to join ` df1 ` and ` df2 ` , but I don't know how to match the indices .
How do I tell pandas to align indices to ` df1 ` , meaning that the first entry of ` df2 ` is actually index 40 ?
You can take a slice of your df that is the same length as ` df1 ` , then you can overwrite the index values and then ` join ` : #CODE
How to normalize by another row in a pandas DataFrame ?
I want to normalize all rows by the matching color experiment 0 .
When I say normalize , I don't mean the vector ranges ( 0 , 1 ) , but that each is divided by an outside value .
One way would be to use ` transform ` ( here using ` idxmin ` , although there are many alternatives ) to get the indices of the rows we want to use as the denominator : #CODE
Since we're handling the alignment ourselves , we need to call ` .values ` to drop down to the underlying array -- otherwise pandas will try to outsmart us and align things correctly based on the indices .
and finally join them , renaming the new columns : #CODE
Python Pandas drop columns not found in both dataframes
I've come up with a solution but it's problematic because it will drop ` any ` column that has ` NaN ` .
I have a feeling there's an easier way to return a dataframe containing only the columns that are found in both two different dataframes ( columns intersection ) .
You can use the ` intersection ` of the 2 df columns : #CODE
Here I am asking how to intersect and in ` pd.concat ` , there it is , join by ` inner ` .
Have you tried ` resample ` ?
@USER I am applying customized functions with APPLY function .
It seems to me that resample or TimeGrouper fills in the gap automatically , even there is a time gap of one year .
I have switched to resample and it almost works .
Only resample takes the first column of df , does it apply to multiple columns of df at the same time ?
Then I can append it with .max .mean .min Excellent Point !
want to aggregate time series with customized function by 10 min , so #CODE
I think you want to ` agg ` ( aggregate ) , not ` apply ` , as for each of your group , you want 1 returning value : #CODE
what do you mean by normalize ?
The other way is much easier and involves using ` resample ` to convert to daily observations and backfill daily consumption .
From there , all you have to do is aggregate .
( Note that the first and last months are based on partial data , you may want to either drop them or pro-rate the daily consumption . ) #CODE
Basically , after calculating the daily consumption , do a partial resample by adding the first and last day of each month .
From there you can probably aggregate in a similar way although you need to essentially do a weighted sum rather than simple sum .
I will implement it and see how it goes , but can you also explain what ' 1d ' means in the resample method ?
@USER ' 1d ' just means 1 day for the frequency of the resample .
This would not replace whatever is inside the ` time_unit ` string and take ` x.week ` , instead it would try to take the ` time_unit ` attribute of x , Which is causing the error you are seeing .
Apply the daily frequency to weekly frequency ( eg . Monday to Sunday )
Apply daily frequency to monthly frequency ( eg . how many times I see " 2012-01 -** " in my column )
Last apply to get annotation #CODE
I tried with pandas but pandas asks for a function such as aggregate sum .
For example , if I would like to find the rolling mean , I would enter this : #CODE
But that would give me the rolling mean across dates .
The first rolling mean datapoint August 12th would contain ` 4999 ` datapoints from August 11th .
However , I would like to start all over each day , so as the first 4999 datapoints on each day do not contain a rolling mean of ` 5000 ` , as there might be a large difference between the last data one date and the first data the next day .
For the following passes , it will ` append ` with no header .
Using the ` pandas.date_range ( startdate , periods=n , freq=f )` function you can create a range of pandas ` Timestamp ` objects where the ` freq ` optional paramter denotes the frequency ( second , minute , hour , day ... ) in the range .
` pd.date_range ( start =p d.datetime ( 2000 , 9 , 10 ) , periods=4 , freq =p d.DateOffset ( years=1 ))`
Merge two tables based on intersection in Python
I have to merge them in such a way that the new table looks like this
Approximate Matching for Numeric Values for Pandas Merge
I'm trying to merge two data frames based on multiple columns
I want to merge these two dataframes using the first three columns .
I cannot just sort and merge by index because the two dataframes are not of the same length ( there is missing data in one ) .
Is there a way to merge these dataframes where the first two columns must be an exact match and the third column needs to meet some minimum threshold ( e.g. , diff =2 ) ?
Ultimately , to follow this method , I had to combine the columns I wanted to merge on to create a unique string variable for each observation .
You would first need to transform the nested dictionary into a list of dictionaries or a dictionary of lists , and then only you can convert it to a DataFrame .
Pandas pivot table
It will still take awhile to apply across 500,000 columns .
This should be faster than a temporary sort +1 , the problem here is that the ` apply ` is trying to return a df with the same shape as the original df which is not what can be achieved unless you take the raw values and return some other data structure like in your answer
I thought that running this ` while ` loop would only cause memory consumption to drop because I am excluding some data from the ` DataFrame ` .
That eats up enough memory for two copies of ` A ` , and that's without taking account of extra memory for the bookkeeping that the ` loc ` implementation does .
Apparently ` loc ` causes pandas to allocate enough memory for an additional copy of the data .
Once ` loc ` is done and the unallocated memory is freed which you can force by calling ` gc.collect() ` the memory load drops down to double the size of the ` DataFrame ` .
On the next call to ` loc ` , everything is doubled and you're back up to a quadruple load .
On the second iteration , after ` loc ` was called for the first time , memory usage doubles to ` 2x ` .
Subsequently , you'll see it go up to ` 4x ` during each call to ` loc ` , then go down to ` 2x ` after garbage collection .
I can confirm by using this same gadget with ` top ` that if you use ` drop ` with ` inplace=True ` the memory is not doubling from copies on each pass .
The place where the original usage of ` loc ` ultimately bottoms out is in the ` _NDFrameIndexer ` method ` _getitem_iterable ` .
` _LocIndexer ` is a child class of ` _NDFrameIndexer ` and an instance of ` _LocIndexer ` gets created and populates the ` loc ` property of a ` DataFrame ` .
From the code : ` key ` will be your Boolean index ( i.e. the result of the expression ` ( A.coordinate = one_center - exclusion ) | ( A.coordinate = one_center + exclusion )`) and ` self.obj ` will be the parent ` DataFrame ` instance from which ` loc ` was invoked , so ` obj ` here is just ` A ` .
On any reasonable modern machine , using the ` drop ` approach should be problem-free for the size of data you describe , so it is not the case that ` A `' s size is to blame .
@USER I suspect something else is going on then , because when I memory profile this way ( with ` drop `) using the snippet that Michael Laszlo posted , I do not see memory growth .
Pandas : Make function map partial Dict match
How can I make it accept partial matches , and replace the matching word , while keeping the rest of the string intact ?
I would like ` { " GOOG " : ` in the dictionary to be sufficient to transform the word to ` : " Google " } ` .
Otherwise , a solution might be to first ` replace ` the matching word in the string keeping the rest of the string intact and then match the regex substring with the dictionary .
I understand how ` len ( dff ) == 8 `
Pandas will either set all rows to the same value if you pass a scalar , will align your array like object if they are the same length , or align along indices / columns if it's a Series / DataFrame this is expected behaviour , for instance you'd get an error if you tried ` dff [ ' counts '] = np.arange ( 7 )`
could you give a quick example of " align along indices / columns if its a Series / DataFrame " ?
How to apply a condition to a large number of columns in a pandas dataframe
= 0 ` produces a boolean mask : #CODE
as you've mentioned in your question you may need to drop rows that have a value in a certain range you can do this by the following
the ` in ` is a problem here as it won't work with array like structures as you expect , using ` isin ` allows you to produce a mask for matches in the passed in list of values
Now produce a mask and use it to mask your df : #CODE
The mask can be shown to be a boolean array : #CODE
Do i need to call transform or some other function after the loop is done ?
Will just calling fit give me this or do I need to call transform in some way ?
@USER , After you completed pca fitting with partial_fit on all chunks of data you can call transform , if you want to transform your data ( reduce dimensionality ) , maybe by chunks again ( in separate for loop , after fitting ) .
You have to call transform after fitting because model must learn on all examples from available dataset , otherwise it will not transform data correctly .
That's why you cannot use fit_transform with IncrementalPCA , only partial_fit , fit , and transform .
I didn't know that it would cause the search to be conducted twice , but that is of course a good reason for expanding the code from one-line .
Each files has unique IDs which will not change for all files and so I need to merge these files based on that id .
the elements of ` files_merge ` are strings - file names . string types do not have a ` merge() ` method . you will need to write some code that reads from the file , creates e.g. a dict mapping the first column to the second , then merge the dicts
I would like to replace ' - ' to ' 0.00 ' using dataframe.replace() but it struggles because of the negative values , ' -1 ' , ' - 45.00 ' .
How can I ignore the negative values and replace only ' - ' to ' 0.00 ' ?
Or you can just use ` replace ` which will only match on exact matches : #CODE
I believe that I need a clever way to replace the ` \\ ` with ` \ ` but for well documented reasons , ` .replace ( ' \\ ' , ' \ ')` doesn't work .
To access the date / day / time component use the ` dt ` accessor : #CODE
This dt accessor is part of pandas right ?
I dont have to import datetime module for using dt ?
It you're using a recent-ish version of pandas ( I think [ 0.15.0 ] ( #URL ) or newer ) , in the above code where you see ` dt.datetime ` this implies ` #import datetime as dt ` but if your series is already a datetime dtype then you can perform ` df [ ' col_name '] .dt .date `
use the transpose of the dateframe : DataFrame.T #CODE
You can set error parameter True which will drop the blank or malformed lines . nrows is not suitable as per my view because you have to manually add the row count for each file .
GOAL : Efficiently join Pandas DataFrames by comparing columns with a string matching algorithm
What I would like to do is join these DataFrames on ` name1 = name2 ` and ` address1 = address2 ` but not based on the exact string .
What I would like to do now is apply a function that takes in two strings and produces a score of the similarity between them .
Based on my reading , the only way to join the strings if we are not looking for exact matches is to iterate over the DataFrame .
want to groupby tag and aggregate time index , so the expected result is #CODE
You can use ` groupby ` and ` apply ` scheme .
Generally your idea of trying to apply ` astype ` to each column is fine .
Note : While using ` pandas.DataFrame ` avoid using iteration using loop as this much slower than performing the same operation using ` apply ` .
Your ` / ( len ( df ) *100 )` looks like it has the 100 in the wrong place ( you're dividing by an extra factor of 100 , not multiplying a fraction by 100 to get a percentage . )
You're correct DSM - I had the closing bracket in the wrong place , I have edited the code above , it should've been ( len ( df )) *100
And I want to replace ` Band 1 ` with ` LG68 ` and ` Band 10 ` with ` LG69 `
Maybe try using ` map ` function with a ` dict ` to describe the mapping relation .
Specify an experiment tag so that all the data frames that I insert for that specific experiment can be collected
You can ` groupby ` on ' id ' and then ` apply ` ` list ` to ` value ` column and then call ` reset_index ` : #CODE
I resolved this error by creating list of tuples ( i.e. apply ( tuples )) instead of list .
Pandas dataframe transpose with original row and column values
Does anyone know how to transpose pandas dataframe with original row and column values ?
Maybe using pivot ?
Need to keep row index too but yes melt seems to be a way .
Do you want to replace the original files or create new ?
Not exactly sure what you want the final output to look like , but if you don't mind having NAs you can try append your list of dictionaries as a dataframe to the original dataframe .
You can then convert this list to a dataframe of its own , and append it to the dataframe you already have .
I am not trying to create a dataframe , rather map the values in the list of dictionaries as new columns to specific rows where they are read from .
Can you show what you tried with ` merge ` , for instance it should work if you did ` merged = pd.merge ( df_sensor1 , df_sensor_2 , on= ' timestamp ')` and then repeat for ` df_seonsor3 ` , or if you set the index to timestamp on all the dfs then you could just do ` pd.concat ([ df_sensor_1 , df_seonsor2 , df_sensor3 ])`
I used ` merge ` exactly like you wrote , but that apparently does an inner join , so only data points that have timestamps in all tables are written to the merged table .
I've tried an outer join as well , which does include all the data but also doesn't get the ordering right .
I did just try ` concat ` though .
Use a ` join ` to merge them with the `' outer '` method
You can create another dataframe called ` df1 ` with flipped columns , and then append it to ` df ` .
Use ` loc ` for creating a new frame : #CODE
Last , use ` pd.concat() ` to merge the two frames : #CODE
I use the names of the columns and append the column names of ` col3 ` and ` col4 ` on ( is there a better way to do this ? ) and create the dataframe , ` df2 ` : #CODE
I used ` python pandas ` to replace the missing values in few columns by mean and the rest by median .
This is in the exceptional case where there are an equal number of NaNs and actual data : pandas appears to mask / ignore the NaNs , numpy includes them .
For your reference the ` apply ` method automatically passes the data frame as the first argument .
Also , as you are always going to be reducing each group of data to a single observation you could also use the ` agg ` method ( aggregate ) .
` apply ` is more flexible in terms of the length of the sequences that can be returned whereas ` agg ` must reduce the data to a single value .
I found also this question related to a similar issues , but I can't figured out how to apply that method in my case .
You can first create a mask to indicate whether a particular row is ` maximum ` in its group .
And then , select using this boolean mask #CODE
pandas boxplot : swap box placement for comparison
To get desired output , use ` groupby ` explicitly out of ` boxplot ` , so that we iterate over all subgroups , and plot a ` boxplot ` for each .
If you need to specify more then one character you can combine ` itertools.takewhile ` which will drop lines starting with ` xxx ` : #CODE
@USER , you can still do it when reading from the csv , what is the ` xxx ` in ` startswith ( ' xxx ')`
You can apply the same logic , just keep the rows that have the first elements the start with " AN using a generator expression
The FuzzyWuzzy library has a convenience function that can be used to replace the inner loop by a single function call that extracts the best matches : #CODE
I think a combination of ` to_frame ` , ` unstack ` , and ` swaplevel ` can get you there .
Presumably , there is a ` pandas ` string methods that I can drop into the above .
that returns ` TypeError : ' bool ' object is not iterable ` .
To clarify , in your for loop , insert a statement such as ` continue if not os.path.isfile ( os.path.join ( pth , f ))` prior to calling ` df.read_csv ` .
Convert values into fewer categories and aggregate
I want to group the ` sector ` column such that there are a smaller number of values , then aggregate ` val ` over the levels of ` from_country ` and ` to_country ` .
I think that link is applying overwriting your sector values so ` df [ ' sector '] = df [ ' sector '] .map ( agg )` and then set this to your index or groupby it
Built on @USER ' s idea , you can use ` df [ ' sector '] .map ( agg )` together with the other two columns ` from_country ` and ` to_country ` as external columns to do the ` groupby ` .
What I'd like is to take some column names and merge them into one column , storing their data as a list per row , like so : #CODE
You could apply a lambda on each column group : #CODE
I've tried to use ` apply ` like this , but can't figure out the correct syntax : #CODE
Pass param ` axis=1 ` to ` apply ` to iterate row-wise : #CODE
I'd like to drop leading rows with 6 NaNs or more .
Specifically , in this case , I'd only like to drop row with Indices ' 1991-12-31 ' and ' 1992-01-31 ' .
The ' 1992-02-29 ' has 5 NaNs - what's different about it that lets you determine that you don't want to drop it ?
@USER , drop * leading * rows .
If any are returned , drop the first one ( i.e. the leading row ) .
I need to drop ALL leading rows with > 5 NaNs , not just first one .
If so , the edit above should work where I simply changed idx [ 0 ] to idx in the ` drop ` statement .
There are some special characters from some spanish data source names that end up here when attempting to create a pivot table .
I would like to be able to handle this in the to_sql call as opposed to having to strip the characters from the headers .
My code feels not very elegant as I am collecting all groups in a dictionary to transform them again into a DataFrame .
Now , if , for example , multiwordexpr is equal to 1 , I want to duplicate the row and insert it in the database .
You can preallocate your df , the required length will be ` len ( df ) + df.multiwordexpr.sum() ` then you can use .ix [ ] to set the correct rows .
You could iterate your original df and append to a new one while splitting the multiwordexpr rows .
I am still experimenting , but seems that the best way is to append to a vector and the create a new df from a list of lists ( built by appending df.col.values ) .
Why not preallocate your df ( as you can calculate the needed size ) and write to it directly that should give you O ( 1 ) on the insert ?
But anyway I would need to shift the index , right ?
So , I have a df of len x and I know that I need , let's say , x+10 .
If I need to insert a line at position 2 , I need to shift the index from 2 on .
Yes , you just have to increment your index for each insert .
What does float ' object has no attribute ' replace ' when I try locale.atof in Pandas ?
I do ` locale.setlocale ( locale.LC_NUMERIC , '')` and then ` df.idh.apply ( locale.atof )` , but it gives me the above mentioned error : ` AttributeError : ' float ' object has no attribute ' replace '` .
How do I tell ` apply ` to skip those ?
You can do this by using the ` aggregate ` function , with the ` mean ` and ` count ` functions as arguments .
What I'd like is to take some column names and merge them into one column , storing their data as a list per row , like so : #CODE
Then you can add in your missing months with ` resample ( ' MS ')` ( MS stands for " month start " I guess ) , and use ` fillna ( 0 )` to convert null values to zero as in your requirement .
What is the most efficient way to replace NaNs in multiple columns based on a default value column ?
Therefore , I have to transpose df , fill the na , and transpose back #CODE
Firstly the conversion to decimal is really ` float ` dtype due to the resampling as this will introduce ` NaN ` values for missing values , you can fix this using ` astype ` , you can then restore your ' timeline ' column which get lost as it can't figure out how to resample a ` str ` so we can apply ` strftime ` to the index : #CODE
I tried to get along with Panel and Multiindex , but it seemed that I would have duplicate data or too complex indexing .
You can ` groupby ` on ' col1 ' and then ` apply ` a lambda that joins the values : #CODE
A stacked hist has to share x-values , and you've set your two data frames up so that they don't share any x-values , so there's nothing to stack .
If you want to stack them by order , you could reset the indexes , merge them into one data frame , rename the columns , and use " plot ( kind= ' hist ' , stacked=True )
df.plot ( kind = hist )
I have tried a wide variety of merge and concat commands , but do not get the desired result .
The strange thing is : what seems the most logical command , according to my understanding of the relevant documentation and the examples there ( i.e. use ` concat ` on the two df with ` axis=1 ` and ` join= " inner "`) , works on toy data but does not work on my real data .
Here is my toy data with the code I used to generate it and to do the merge : #CODE
And although the two dataframes seem no different from the test data , I get an empty dataframe as the result of the merge .
My merge command : #CODE
I have tried many variants , like using ` merge ` instead or creating extra columns replicating the indexes and then merging on those with ` left_on ` and ` right_on ` , but then I either get the same result or I just get NaN in the " topicwords " column .
You can try casting ` someScores.index = someScores.index.astype ( np.int64 )` and then join
I get good values for " intersection " and the merged dataframe also looks good .
Inner join only returns rows whose index is present in both dataframes .
We are getting closer , there is indeed something wrong here regarding the intersection .
After that , intersection was fine and ` concat ` worked .
I do not want to create a new dataframe , so ` inplace=True ` should still apply .
How can I merge two pandas DataFrames based on a function instead of just where values are equal ?
I'd like to merge the columns on those strings , but on the Levenshtein distance as opposed to just where the strings are equal .
Is it possible to merge DataFrames based on functions like this ?
pandas groupby with custom agg function too slow or uses too much memory
dataframe append new series column with data
I have a Panda DataFrame structure and I want to add another column to it , but I can't do it with append , add or insert .
` d_data = dict ( zip ( ls_keys , ldf_data ))` is what I want to replicate because it doesn't fetch the data that I want , but I need to figure out a way to append a new column to my dict .
Maybe it works , but I made a workaround to actually first design the DataFrame , and then programatically populate the data in all columns and in this way I escaped the need to append the new column .
You can ` apply ` a lambda to your dates and call ` datetime.strftime ` : #CODE
Yes I updated my code sample to add line ` import datetime as dt `
I am trying to to align all pages around that point .
If we want to recover the output as a pandas data-frame , we need to ( or at least this is the way I do it ): 1 ) pandas.DataFrame ( data =o utcome of DictVectorizer transformation , index=index of original pandas data frame , columns= DictVectorizer() .get_features_names ) and 2 ) join along the index the resulting data-frame with the original one containing the numerical columns .
( This is just a simplified example - in real life , my dataframes are larger and I will need to shift by more than one unit )
How to set a value in a pandas DataFrame by mixed iloc and loc
But I wonder if there is a way to use the magic of iloc and loc in one go , and skip the manual conversion .
I don't think you can do this as the indexing methods are specifically for either integer ( iloc ) or label based ( loc ) indexing , if you starting mixing both styles then you have to convert the integer <-> label to do what you want
I don't know what the question is , but here's a great answer regarding ` iloc / loc / ix ` , and is probably relevant to your question #URL Also , a small sample dataset would help here and make the question more concrete .
I used the function for def scale and want to apply for it , like this fashion : #CODE
For context : This is done to count cases for specific rows ( criterion defined by a column ) when I aggregate the table to some groups .
I have purchase records of drugs , which I want to aggregate up to monthly counts and sums of cost and dose by drug category .
Apply a func to generate a new colum based on value of other colums in Pandas
When I ' apply ' that function to both ' Time on Page ' ' Pageviews ' columns , wouldn't it take the value from both columns as the argument and return one value , which is ' AvgTimeOnPage ' , as the output ?
@USER Ah , I see , apply takes ONE argument ( which is a row / Series ) and you access the column as ` x [ ' Time on Page ']` inside the apply .
You should use ` factorplot ` , or if you really want to use ` FacteGrid ` directly , you have to pass the ` hue ` variable in ` map ` .
Using apply on a column
I'd like to somehow apply a function to each column , converting it to a list and placing it in a new DataFrame .
However , apply only operates on individual entries .
I simply needed to apply a split function to each string in the dataframe !
The only other method I can think of is to write a for loop that'll go through a dictionary of the values and replace them .
So you need to transpose the df ( if necessary ) in order for broadcasting to work , then the series needs to flattened to a 1-D array which in this case can be done by calling ` .values ` attribute
You'd have to define a func yourself and ` apply ` it , also you need to add all this information to your question and to pose representative code to show your desired output including a series with an index in a different order to your df
And then apply it to some function that returns a series like so : #CODE
You could create a new ` MultiIndex ` from your data , and ` reindex ` , like this .
To account for the missing indices , you'll probably need to reindex the DataFrame returned by ` df.groupby ([ ' col4 ' , ' col2 ']) .sum() ` : #CODE
Essentially this performs a reverse lookup , we iterate over the ingredients series using ` apply ` and then test for membership of this ingredient in the whole df using ` contains ` this will handle plurals in this case .
Pandas reindex dates in Groupby
I would like to ` pd.groupby ` the ' id ' column , and apply the reindex to each group in the dataframe .
Which returns error : ` AttributeError : Cannot access callable attribute ' reindex ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method `
This has the added advantage of changing the ' D ' in the resample part to ' B ' if you only want business days for example ( of course this depends on what you want ) .
Are you sure that works ` Pilik ` because I initially tried ` resample ` but it didn't add in the missing days ?
I copied the example using ` pd.read_clipboard() ` and my solution with ` resample ` also yields the desired result .
I can also reindex the data frame by the chromosome labels chr1 , chr2 , ...
Merge two tables based on multiple keys in Python pandas
I would kindly ask if in Python Pandas ( or other modules ) has any functions to support merge ( or join ) two tables based on multiple keys .
I don't know if it is really achievable in Python ( or even R ) , because based on my reference to Pandas documentation and past examples in stack overflow nearly all examples only relate to join or merge on one key .
To merge by multiple keys , you just need to pass the keys in a list to ` pd.merge ` : #CODE
In fact , the default for ` pd.merge ` is to use the intersection of the two DataFrames ' column labels , so ` pd.merge ( a , b )` would work equally well in this case .
( Or if you still have questions about ` merge ` , let me know and I'll do my best to answer them . )
How do I append multiple CSV files using Pandas data structures in Python
I have about 10 CSV files that I'd like to append into one file .
My thought was to assign the file names to numbered data_files , and then append them in a while loop , but I'm having trouble updating the file to the next numbered date_file in my loop .
Whatever you call after groupby , actually does something to those partions - in this case ` filter ` , ( exluding some partitions ) , but it could also be an ` aggregation ` ( summarizing entire partition ) , or a ` transform ` ( modifying all the values )
Now I want to apply functions based on metadata criteria : #CODE
I reset multiindex in ` g14 ` , ` g7 ` and ` g3 ` appending ` reset_index() ` .
To join the data back to the original dataframe as a new column I will need to add call time and a column called employee_id .
Edited my original comment - I would like to be able to merge the results of my grouped total counts to another dataframe which has 1 record for each customer ID .
You can use apply , like this .
Although generally , storing lists in a ` DataFrame ` is a bit awkward - you might find some different representation ( columns for each element in the list , MultiIndex , etc ) that is easier to work with .
You just want to append instead of over-writing .
You shouldn't append the header for every element of brands .
This causes an append to the dictionary for every line in ' source ' .
How can I merge A , B and C correctly with ` numpy.hstack ` , ` scipy.sparse.hstack ` or ` FeatureUnion ` ?
Using ` FeatureUnion ` , you simply need to append your new transformer to the transformer list : #CODE
How can I merge the matrices taking care about the dimensions ?.
I need to apply this calculation to every cell .
But then I am struggling to find a way to apply this function to each cell in the dataframe .
Or alternatively , if you need to convert from your original dataframe into a numpy array , then you can replace the last line with : #CODE
if you want to drop the original column you can do this #CODE
I need to ` identify ` them , and then do ` interpolate ` from nearby data to replace them .
One way to detect corrupted data or outliers is to first calculate the rolling median ( it's robust to outlier ) of a series , and then compute the distance between the actual observation and rolling median .
If you have trends in your serie , you may rather apply it on time moving window instead of the whole serie .
Here is an example about how interpolate mean value for NaN only in 1x3 footprint : #CODE
You can also merge the two functions toghether but for quality analysis , I don't and always keep raw data , valid data , and interpolated data .
I was using " pivot " but that doesn't work as the indices are not unique .
So I created another group but pivot seems only allow to have one index as option .
dtype : bool ` is correct only index row 4 is ` True ` because all rows are equal , so what's wrong this ?
A quick workaround would be to replace any ` nan ` in ` df ` and ` df2 ` with something that you know is not in your dataframes eg ` foo ` : #CODE
Pandas - understanding output of pivot table
if I replace my dataframe with #CODE
The output of the same pivot is going to be #CODE
For your second question , the reason that ` groupby ` generally fails if that there are no numeric columns to aggregate , although it seems to be a bug that it completely crashes like that .
Hello , When I save the plot the legends become stacked on one another though I chose different loc for them .
I didn't add that code , so I'm assuming the code has been removed by a merge upstream , while I was working on my local branch .
To me REMOTE is the upstream version , LOCAL the local file I'm working on and BASE a partial merge git was able to carry over , but requires some manual intervention to complete .
Pandas dataframe aggregate by time stamp into x minutes bins
Now I want to aggregate my pandas dataframe into 1 minute bars .
` resample ` is a standard way
once you have this , you can aggregate by this column however suits your analysis .
As @USER mentioned , ` resample ` is the tool you need here .
You can pass ` how= ' ohlc '` to ` resample ` to get the desired output .
That ` transpose ` use-case seems reasonable , may be worth making an issue .
Pandas exponentially weighted functions : are these moving window or expanding window ?
Pandas has moving window functions and expanding window functions .
I would expect 2 sets : ( a ) one set of exponentially weighted functions for moving window and ( b ) another set of exponentially weighted functions for expanding window .
These are expanding window functions .
Now we want to replace missing values with ` group ` -wise median values .
why do you need to do nested if-else statements ? for example , in r this is generally bad practice , and you can simply do ` with ( test , ave ( value , group , FUN = function ( x ) { x [ is.na ( x )] <- median ( x , na.rm = TRUE ); x} ))` which will work for n groups
In this case , you can use a ` groupby ` to fill by the group median : #CODE
Pandas can't join DataFrames after set_index with exception ` IndexError : index 2 is out of bounds for axis 0 with size 2 `
When I run the code below the join operation throws ` IndexError : index 2 is out of bounds for axis 0 with size 2 ` .
So for your example , you could repeat your sample data ` len ( df ) / 24 ` times and set it into your original dataframe using ` iloc ` , ( i've had to select ` iloc [: -1 , :] ` since your index actually contains one extra time point on the last day ): #CODE
Python pandas merge or concat dataframes
I want to concat or merge ( not sure which ) the list of dataframes into one data frame ( called result ) so they look like : #CODE
My current concat result dataframe looks like : #CODE
This should concat column-wise , assuming each dataframe has the same column names .
possible duplicate of [ Pandas join / merge / concat two dataframes ] ( #URL )
@USER you mean to apply the function as follows ?
You can create a dictionnary , and then map your dataframe with it #CODE
You can also append the new values at the end of another list ( fast ) instead of modifying a value in the middle ( not that fast ) .
Stack Overflow discourages product recommendations .
Python Data Frame resample on micro second
I am working with resampling data frame and it works on hours , days mins , but doesn't resample less then sec .
you can see varable sf represent resampling freq .
How can I resample data frame on micro seconds granularity ?
If I understand your problem correctly you can't resample microseconds to another frequency that is less than a second , right ?
( I think your problem is that you are trying to resample data that are in a microsecond format to microseconds itself , which doesn't make any sense . You want to either upsample or downsample ( as in my example ) . )
Now apply a ffill lambda as follows : #CODE
More elegant than the apply function is to use ` result [ xstring ] = tst.bla.str.contains ( xstring )`
SAS - transpose multiple variables in rows to columns
If we don't want it , we can transform them to ` la0 ....
What I would like to do is slice each group down to 3 hours max and append something to the 6 and 9 length groups to denote that it is the same page like the following : #CODE
So , I truncated my data set in the question to make it easier to read and thinking that whatever solution came would also apply ..
You can use ` pd.rolling_min ` and set ` window=k+1 ` to calculate the rolling minimum with a lookback window of ` k+1 ` .
I would need it like this because i need to pivot the data .
You can use ` np.arange ( len ( df ) // 34 )` to do the trick .
I would need it like this because i need to pivot the data .
By the way , it's not a good idea to name your variable ` map ` , since this shadows the built in ` map() ` function
Is there a more efficient way to do calculations and insert them into a dataframe with predefined column names ?
The variable map is not actually something I use , I just changed the variable names from my actual code !
How do you suggest I replace my current column header objects in a nice way ( that's also supported ) ?
` color = [ ' b '] * ( len ( df ) - 1 ) + [ ' r ']`
` ax.scatter ( df.ix [ len ( df ) -1 , ' carry '] , df.ix [ len ( df ) -1 , ' vol '] , color = ' r ')`
I'd like to quickly transpose them into a series of 30000 arrays with 100 elements .
That is , an element-wise transpose of a Series of arrays into a new Series of arrays .
Use ` max ` and check for equality using ` eq ` and cast the boolean df to int using ` astype ` , this will convert ` True ` and ` False ` to ` 1 ` and ` 0 ` : #CODE
` for i in range ( len ( df )): ...
Pandas really slow join
I am trying to merge 2 dataframes , where I want to use the most recent date's row .
Using the ` groupby ` / ` agg ` example , it takes 8 minutes !
use ix instead #CODE
If you want to change the display format then you need to parse as a datetime and then ` apply ` ` datetime.strftime : #CODE
Pandas drop index type columns
I want to drop multiple columns from ` pandas.DataFrame ` and can't do it .
I've tried many many things using both ` pivot ` function or using both ` set_index ` with ` unstack `
use ` .pivot_table ` followed by ` concat ` : #CODE
Dataframe groupby or stack / pivot
I need to transform this dataframe into the something like the following : #CODE
I think something like stack or pivot is the answer but I can't get it working .
I have tried append and concat but it just gives me all the columns and all the rows stacked on top of each other .
Just do a default ` merge ` this will perform an inner join on common columns : #CODE
` apply ` a lambda to convert to timedelta and then subtract : #CODE
That now leaves me with $23 to spend on extra presents that birthday ; the same rules as above apply on any additional presents .
Using the median on the 3 numbers is a nice way to work out what granny will contribute .
Python pandas : how to join a Series to a DataFrame ?
Is there any way to join a Series to a DataFrame directly ?
The join would be on a field of the dataframe and on the index of the series .
I want to add the column in the Series to the dataframe , but how do I specify whether it should be a left outer join or an inner join , and how do I specify which column of the dataframe the index of the series should match ?
For example inner / outer join .
Does this join on the indices ?
In my case , the indices are different , and I want to specify the fields to join on , but concat doesn't seem to have on , left_on , right_on parameters like pd.merge does
Solves only this part >> I want to add the column in the Series to the dataframe , but how do I specify whether it should be a left outer join or an inner join
This converts the index to a column , which gets added to the result of the merge function .
print ( ' Stock : ' , col , ' max diff : ' , sl.max() - sl.min() )`
apply ` df.str.contains() ` to ` s2 ` using the contents of ` s1 ` as the matching pattern
The best I could come up with is to use ` apply ` instead of manual iterations : #CODE
sorry to break this to you but ` apply ` is essentially a ` for ` loop , the code just looks cleaner
But if you are just going to get the values of each group , you don't even need to ` groubpy ` , ` MultiIndex ` will do : #CODE
I'm a python novice trying to preprocess timeseries data so that I can compute some changes as an object moves over a series of nodes and edges so that I can count stops , aggregate them into routes , and understand behavior over the route .
I've looked into melt and unstack and I don't think those are correct ...
Is there a simpler , dare I say ' elegant ' , way to map the dataframe rows based on the multi index value directly into a reusable data structure ( right now the dictionary stop_dict ) .
I'm looking for a way to generate a radar chart from a pandas pivot table that looks like this .
Is this simply a table join on item with some filtering , sorting and more filtering ?
Next step was to replace the ` df3.name ` column with a forward-filled groupby series : #CODE
How does one use " takeable=True " and indexers in dataframe.set_value , especially with multiindex
Depending on if your indexers are positions ( ` iloc `) or labels ( ` loc `) .
Well , I tried that first and wasn't able to set complex areas in a multiindex with expansion , is it your impression that should work ?
From what I've read the key will be finding how to pass the correct mask I'm trying to work with into the iterator at the heart of pandas / numpy .
I need to drop any rows that have those " invalid " values for time .
With pandas use the ` diff ` method to find negative rows ( i.e. rows where the next time value is less ) , which are then filtered out : #CODE
If the dtype of the column is datetime ` diff ` should still work , IMO this is the correct answer
group by rolling sum in pandas
What I essentially am going to ultimately created is two columns , one with a rolling sum number of ' Right ' and another column of a rolling sum of number of ' Left ' .
My understanding in pandas is to use a pd.rolling_sum() function but i'm not quite sure how to groupby and apply it while setting a condition .
When I did a pd.rolling_sum I used a shift , but again I couldn't find where to groupby with this .
So to go through that column , it is grouped by gameID and PlayerA and is a rolling sum / countif with the condition being ' Right ' .
But since we're working with a series of bools anyway , the ` == True ` part doesn't add anything , and we can drop it : #CODE
See comment @USER In short , can I " universalize " this to make it apply to every car ?
Or you can transform the two dimensional array to a one dimensional array : #CODE
You could ` groupby ` the ' ID ' column and perform an aggregation : ` f.groupby ( ' ID ' , as_index=False ) [ ' ID '] .agg ([ np.count_nonzero , np.unique , np.max ])` , not sure how to obtain the ` freq ` calculation though
Don't know without your data , but I think that using groupby with agg is the way to go
We can use the boolean mask to filter the columns : #CODE
You can apply the recipe from here #CODE
I need to perform a rolling linear regression for X periods at a time .
But what I have been unable to figure out how to do is a rolling linear regression , for example with a 20 row rolling window .
You can update these in a rolling window fashion , adding new points and deducting old ones .
That loop works fine , but I trying to see how I could use map and apply to make this look cleaner .
From reading , apply is a loop function underneath , so I'm not sure whether I will get any speed from doing this .
This works fine with basic addition , but I was trying to find a way to apply a more general function .
You should use ` loc ` to ensure you're working on a view , on your example the following will work and not raise a warning : #CODE
I do have a bit of a shift on my axes though .
From my original dataset , the scatter point should be at the intersection of the column and the index , with the point darkened according to the degree in the data .
I cannot understand the error I see when using apply or transform :
1 2 ) ` transform ` expects something " like-indexed " , while ` apply ` is flexible .
` InnerFoo ` actually returns another function , so it needs to be called before being passed into ` apply ` .
I think you're problem is the freq= ' M ' is rolling today back to 0 8/ 31 .
The maximum consecutive losing trades per year , are you trying to do a rolling sum of losers until you hit a zero ?
Then with that , you can aggregate by winners , losers ( which returns like indexed data ) to calculate a percent of winners , losers .
I've tried using ` map ` with ` dummies ` , but haven't yet been able to figure it out .
I'm currently learning how to use Pandas , and I'm in a situation where I'm attempting to replace missing data ( Horsepower feature ) using a best-fit line generated from linear regression with the Displacement column .
and then use ` update ` to replace the NaN values : #CODE
Ed Chum's solution shows how to replace the value in arbitrary rows by using a boolean mask and ` auto_data.loc ` .
OK , after looking at what you want it'll be quicker to take the first rows and then ` shift ` the rest of the df after performing a ` groupby ` and then ` concat ` : #CODE
Yes , you're right you need to pass ` -1 ` to ` shift ` so it shifts the other direction
Try ` X_NAN = X [( X > 1000 ) .all() ]` I think your issue is that some cols will have some ` NaN ` and when you call ` dropna ` it will drop any column even if it has a single ` NaN ` value
But you have given me an idea with apply .
As you can see in comments , you have to join ` rootDir ` , ` dirName ` and ` fname ` .
Join one or more path components intelligently .
you can create empty df and append one column of data from csv's - #URL but you have to change for adding column ` code ` : ` df = pd.DataFrame() ` and ` df = df.append ( g [ ' code '] .tolist() , ignore_index=True )`
can be used to apply the format string using the column data .
ValueError : In safezip , len ( args [ 0 ])= 1 but len ( args [ 1 ])= 21
How do I calculate a rolling mean with custom weights in pandas ?
The second line calculates a rolling average with a window of 5 and equal weights on each of the five observations .
For example I could be interested in the percentiles ` [ .1 ,. 9 ]` I'm finding the location of these percentiles in the DataFrame with the associated values by checking where in the first DataFrame I should insert the percentiles .
` df [[ ' pC ' , ' Truth ']] .plot ( kind= ' hist ' , stacked=True )`
If that's what you want , I guess you could use something like ` melt ` to also add to the dataframe those complementary probabilities ` ( 1-pC ) for Truth == 1 ` for each bin .
Use a ` for loop ` to index each line and use another ` for loop ` nesting it to read the data in each column . use ` lstrip ( '"')` and ` rstrip ( '"')` function to strip off the quotes . then read .
Use ` MultiIndex.from_product() ` to make multiIndex from cartesian products of multiple iterables .
Two levels will be loc and S .
I have 4 different ` df.hist ( columns= , by =) ` that I would like to insert inside of a GridSpec ( 2 , 2 ) .
I would use the pivot method as : #CODE
and then apply the method over it .
The pd.pivot() give me this error : pandas.core.groupby.DataError : No numeric types to aggregate
The function you're looking for is ` pivot ` .
it is giving me an error : pandas.core.groupby.DataError : No numeric types to aggregate
I search output [ 3 ] for the string ' station ' and then append the results where this is true to an empty list , results .
Then filtering as above and attempting to append these results to the new dataframe #CODE
You can create a boolean mask by calling ` apply ` on ' type ' column to create your new df : #CODE
python pandas : applying different aggregate functions to different columns
Apply a function to translate a column in pandas dataframe with condition on other columns
I want to translate certain reviews in the text column to english and I am using a function dfApply #CODE
Why not just convert the whole column to English , then use a mask of non-english rows to replace only the ones you need to ?
That is a bit easier than using apply with your conditionals happening in each step .
After performing the conversion you can use the datetime accessor ` dt ` to access just the ` hour ` or ` time ` component : #CODE
The above function ( an indentation error occurs when copying into Stackoverflow - please simply ignore ) pulls a custom object ( event ) out of a Queue object - if the event is a tick event , it is meant to append the ask variable and the corresponding timestamp ( as an index value ) to a pandas Series object ( see last nested if statement in function ) .
When I try to accsses the data using loc [ ] or lookup() the memory usage increases dramatically to ~16GB , which causes the thread to be very slow due to memory swapping .
Is it possible to drop ` NaN ` values and leave only 4 values per one time step ?
and that the function inside ` apply ` should make use of the ` isin ` method .
Another solution might be to merge the two datasets but I think this is not trivial since there is no unique identifier in the ` DataFrame ` .
probably but calling ` apply ` will also be very slow as this is just a ` for ` loop
Then apply your method : #CODE
The rhs will align to the index on the lhs , as we use ` loc ` here it ensures that the correct rows will be assigned
I'm struggling to correctly merge a few datasets in pandas .
I'm still trying to get my head around the various join / merge / concat operations and how they work , but I'd love a pointer or two .
create a dataframe with index ' timestamp ' and columns ' var_name ' , ' val ' for each of A , B , C , and D . from there , concat , sort , and unstack .
Assuming that your index is a Timestamp , try to ` resample ` it at your desired frequency ( e.g. hourly , daily , weekly , etc ) .
As suggested by another stackoverflow user , I am using sklearn preprocessing package to normalize columns of a pandas dataframe .
It is supposed to create a dataframe from a file , normalize the dataframe and save the results to a new file .
I'm trying to find a way to merge all the workbooks and sheets into a single sheet .
I think I can combine single-sheet Excel files ( with glob and append . ) , but when I try it with multi-sheet files the data gets messed up .
I thought about applying a map , but it works on the row as a whole if I understood it correctly .
AFAIK , you would have to separate the two parts and append as lists since the columns of interest are different and converting to a dictionary would include the ` NaN ` s otherwise .
apply conditional if loop over groups
And I find I need to append ` get_values() ` to make this membership test behave as I'd like .
So I want to replace all the `" .
@USER - I got this error : ' float ' object has no attribute ' replace ' .
I think it apply the function to the index instead of the value .
where ` df 3 ` returns a Boolean mask over the entire DataFrame according to the condition , and ` sum ( axis=1 )` returns the number of ` True ` in that mask , for each row .
Finally the ` =3 ` operation returns another mask that can be used to filter the original DataFrame .
You can directly create the DataFrame from the dictionary , but when creating that , the ` keys ` would become the column and ` 0 ` / ` 1 ` , etc would become the indices , if you want it the other way round - ` keys ` as indices and ` 0 ` / ` 1 ` , etc as columns - you can then take its transpose .
One way you can accomplish this is to skip creating a pandas series altogether and just go right to a DataFrame , then transpose the dataframe : #CODE
Read the dictionary into the DataFrame , then transpose it with ` .T ` , or with ` transpose() ` : #CODE
I am trying to concat the csvs in each subdirectory and then export it .
Maybe try to gather the csv records first and concat them after the loops are done .
I've read through and tried all ` merge ` combinations .
and then align this with my df #CODE
Thanks . reindex does exactly what I want .
OK , I'd ` reindex ` using your time_series , then ` groupby ` on your index and then apply ` isnull ` and call ` sum ` : #CODE
You can use concat ([ list of dict ]) or merge ([ list of dict ]) to get the desired result .
Pandas ` concat ` and ` merge ` functions are not built for working on dicts , they are built for DataFrame and Series etc .
Pandas boxplot covers / overlays matplotlib plot
Following what reported here I tried to create a boxplot with an overlayed scatter plot .
You can perform an inner ` merge ` : #CODE
You can then drop the ' postcode ' column or rename it first : #CODE
I still dont understand why they would call this " merge " , should it not be called " converge " ?
You're merging on columns / index , you can specify the criteria for merging on lhs and rhs , if there are column names that match then it will match these and the default type of merge is inner so only values that are present on both sides
You can just replace ` 1 ` with arbitrary value and then call ` idxmax ` and ` reset_index ` as before : #CODE
If you want to add the values then you can call ` apply ` and use the ` new_df ` values to perform a lookup from ` cos ` df : #CODE
I want to plot a heat map of these data with COLS on the horizontal axis and FUNCS on the vertical axis with cells that are scaled according to FLUFF .
As cel mentioned in the comments , if your data is actually sparse , you might want to do a ` .reindex ` to insert all the rows and columns , filling the NaNs appropriately .
This can be done by using ` dt ` .
Do I have to transpose the array ( it seems there are many functions for rows in pandas and numpy but relatively few for columns ( I could be wrong , of course ) to get average calculations for a column done ?
Now I tried to print using for in in xrange ( len ( Beer_Name_L )): beername=Beer_Name_L ( i ) print df [ df.beer_name == beername ] .Flavor_Score .mean() but got an error message " TypeError : ' list ' object is not callable
and i want to merge them into 1 dataframe .
But the thing is that the ( index ) columns needs some altering before i can use them to merge .
( preferably during reading the csv data ) so i can merge the files .
I'd suggest you just read the csv as is , then split / parse the source ( mix values ) into individual fields which can then be used to merge with other dataframes .
And then transform it to results dataframe like below .
Of course , I can generate an array and append it ( right ? ) but it would seem to be a cumbersome way .
You could do something like yourDataFrame [ ' Flavor_Score_stddev '] = yourDataFrame [ ' Flavor_Score '] .someFunction() to map values of ' Flavor_Score ' to new values using someFunction() and put the new values in a new column named ' Flavor_Score_stddev ' .
I reproduced the issue but could not fix it and it messed up my clipboard so was unable to cut and paste in any app without closing IPython .
Also I could see that Legend B is cut and not showing all the characters .
Next , I want to map the ` Max CPC ` associated to the ` CPC Rank ` to the ` Type Rank ` which is determined based on ` Criterion Type ` and my own custom rank :
This meant I wanted the highest ` Max CPC ` to apply to the first , the second highest ` Max CPC ` to the second , and so on .
When defining ` New CPC ` , ` map ( mapping )` seems to cause a ` pandas.core.index.InvalidIndexError : Reindexing only valid with uniquely valued Index objects ` error .
If so how should you map your values in this case ?
normalize pandas data into one to many relation
You can use ` str.split ` , followed by a ` apply ( pd.Series ) .stack() ` ( the ` apply ( pd.Series )` makes different columns of the elements , ` stack ` is for turning this to rows ): #CODE
Note , with recent pandas , you can replace the ` .apply ( pd.Series )` with ` expand=True ` in str.split : ` df [ ' sports '] .str .split ( ' , ' , expand=True ) .stack() `
Is it possible to merge the columns like index and current_tm for respective country ?
You can use merge : ` pd.merge ( res , df , on= ' country ')` ( assuming ` res ` is the result of above , and ` df ` has still the country column )
join method worked as well # res.join ( df )
You could first do a groupby on DF2 , counting one of the columns , and then merge the resulting DataFrame with DF1 , something like this : #CODE
I have tried to apply your answers but I do not get good results as you can see below : I just have the same value 1970 - 01-01 instead of having a column with the same value stored in datetime column .
You can also concat them into a single multi-index dataframe : #CODE
Which then allows me to resample the count further , e.g. : #CODE
Trying to merge 3 different dataframes that each have a different number of rows and used these commands #CODE
Note the ` how= ' outer '` , which does a full outer join .
df [ ' unq '] = numpy.arange ( 1 , len ( df ))
I'm not looking to concatenate strings , just shift everything over .
I saw a method using " R " and melt , however I would like to stick with python / pandas if possible .
Then you can use the rolling counts because it will be sorted by date .
That will be much faster than using the apply method and checking len for each row .
You then unstack the id , which effectively creates a pivot table with dates and times as the rows and ids as the columns .
Then , just group on the the timestamp and id , and unstack the id .
Pandas dataframe apply function to entire column
` .apply() ` is the method to apply a function to a ` Series ` on a row-by-row basis .
apply a function that return a list
I know how to split a string , but I could not find a way to apply it to a series , or a Data Frame column .
For all but the last names you can apply `" " .join ( .. )` to all but the last element ( ` [: -1 ]`) of each row : #CODE
Conditional join result count in a large dataframe
What I want to do is replace all the values that are on Saturdays with the values from the previous Friday .
Yes if you have regular intervals then you just need to ` shift ` by the interval amount
You can use ` loc ` and a mask to select the rows you wish to modify and assign the values ` shift ` ed by your regular interval : #CODE
I'm only splitting you data into halves rather than deciles here due to the small example dataset , but everything should work the same if you just increase the number of bins in the initial cut : #CODE
However , expanding the window to full-screen resolves my problem , repositioning everything in a way that allows the graph to be visible .
I am wondering if join and merge could be valid use cases for this specific context .
You definitely want to do an outer join on both frames ?
Don't merge on any columns with null values , you will get an inexact match which will grow your merged df exponentially , I'd either drop them or assign some dummy value
From what I understand I think the requirement is to have the union of all of the tables - not solely the intersection from the shared " keys " ( in this case the headers ) .
This is useful as I was not seeking to find the intersection of the data sets ( which , in theory should not be apparent in the data structure that I am wrangling ) .
You need to ` unstack ` your dataframe : #CODE
I have a large dataframe where I store various metadata in a multiindex ( see also here ) .
Yes , you can probably use much less RAM than the 3.8gb text file - No you cannot use the same strategies as you would apply to a file on your disk .
Passing additional arguments to python pandas DataFrame apply
I have a DataFrame df and I tried to iterate each row to map values of two columns to new values , but I have problems passing in the dictionary containing the map to df.apply #CODE
The ` args ` argument given to apply function is passed ` func ` argument ( lambda function given ) .
and want to plot a boxplot where the x-axis is ' Av_Temp ' divided into equi-sized bins ( say 2 in this case ) , and the Y-axis shows the corresponding range of values for Tot_Precip .
Since you're using pandas already , why not use the boxplot method on dataframes ?
If you want a standard boxplot , the above should be fine .
My understanding of the separation of boxplot into boxplot_stats and bxp is to allow you to modify or replace the stats generated and fed to the plotting routine .
If you need to draw a boxplot with non-standard stats , you can use boxplot_stats on 2D numpy arrays , so you only need to call it once .
Insert a row in a multiindex data
I have a multi-index dataframe ( multindex on the rows ) and I want to insert a row at a certain row number .
This question has been aksed before ( How to insert a row in a Pandas multiindex dataframe ? ) but I have a bit different problem I want to solve
Since there is no insert_row function in pandas I thought to insert a row by first copying my data frame up to a certain row to a new frame using a slice , then append the row I want to insert and then append the rest of the original data frame .
In other words , if I slice the rows up to label1 I indeed get a dataframe which looks like this slice , but the multiindex levels still contain the higher levels to .
If you take that into account , the first anwer with the unstack / stack does not work .
After the unstack , I can not impose a different order A / B or B / A anymore .
Now , the unstack method does not allow to insert a row with for instance B first and then A , because after unstack / stack , the order is always A / B .
So first I unstack df , create new dfs and then concat them .
Last I stack df back .
I don't think you can still use teh unstack method in that case
If you just want to insert a new set of data at a certain position try the following .
With drop you recieve a new object , there will be no KeyError issue anymore .
df.append() flattens MultiIndex ...
Having browsed other threads relating to appending to a pandas dataframe , row by row I have read up on concat and append and setting with enlargement but still have this funny problem where is I create an empty DF with a ` MultiIndex ` and then append to it , the ` MultIndex ` appears to be flattened to a normal ` Index ` .
To go back to a ` MultiIndex ` , at the end of all my row appends I must reset the index .
Inspecting the columns I get back a ` MultiIndex ` as I'd expect and I can index using the " outer " column name : #CODE
So , now I append another row .
I've been googling but can't find a solution and don't really see why append isn't happy .
Pandas dataframe Cartesian join
Result ( expected result size = len ( first ) * len ( second )): #CODE
Create a surrogate key to do a cartesian join between them ...
I've searching for something like this kind operation ` df [ len ( df )= =19 ]` , is it possible ?
Did you try ` df [ df.apply ( lambda x : len ( x [ 1 ]))= =19 ]` ?
You could take advantage of the vectorized string operations available under ` .str ` , instead of using ` apply ` : #CODE
I'm trying to join two series with ` pd.concat ([ a , b ] , axis=1 )` , but the result is a dataframe filled with ` NaN ` s , here's what I mean :
so here's two series that won't concat well together : #CODE
I've serialized the head of above series , uploaded to evernote , contains code to load and concat them
@USER they were calculated from a groupby object , grouped by ` [ ' dt ' , ' product_id ']` , does it have anything to do with this situation ?
Your ` by_status.groupby ([ ' dt ' , ' product_id '])` operation should result in a MultiIndex , but the results of ` a4.head() ` and ` a5.head() ` pasted above indicate that it changed to tuple pairs somewhere along the line .
I do not understand why ` concat ` is not working , but I managed to achieve your objective using ` merge ` .
Then merge the DataFrames on ` dt ` and ` product_id ` : #CODE
the groupby operation should and did result in a multiindex , when I convert a pandas series to dict using ` Series.to_dict() ` , the underlying multiindex is just converted to tuple , not that the index itself is a array of tuple , the code in my post is the exact code I used to generate these series objects , and as you can see it doesn't contain any middle steps , I'm going to post the raw serialized data
even if it's the wrongly formatted multiindex , the concat operation shouldn't result in a all-NaN frame , right ?
Why don't you open a new ticket " Possible Bug : Pandas concat with MultiIndex " .
I am reading a bunch of daily files and using glob to concatenate them all together into separate dataframes.I eventually join them together and basically create a single large file which I use to connect to a dashboard .
Is it possible to to use ` mask ` function in this plot ?
To do something like this ` plot ( x , y ) mask=x > 11 plot ( x [ mask ] , y [ mask ] , ' y ')`
Two ways to join lists in dataframe : as rows and columns
Now I join them so that each list becomes a column of a data frame : #CODE
If , however , I join lists as rows of a data frame , then Python saves ` 66 ` : #CODE
Or you can use ` map ` with ` None ` .
( but map changed in Python 3.x , so that only works in Python 2.x ): #CODE
` concat ` requires an NDFrame object , so I first create a DataFrame from each of the Series .
` pd.read_table ( filename , usecols =[ 0 , 8 , 9 , 11 ] , parse_dates =[ 1 , 2 ] , dtype={ ' LopNr ' : np.uint32 , ' INDATUMA ' : np.uint32 , ' UTDATUMA ' : np.uint32 , ' DIAGNOS ' : np.object } )` , assuming the dtype would apply to the data before it enters the converter , hiccups on a string in some of the rows : ` ValueError : invalid literal for long() with base 10 : ' string '`
Given that all the dataframes have the same columns , you can simply ` concat ` them : #CODE
I'd like to know how I can replace the NANS of my DataFrame by the mean of the others at the same hour and day of the week .
pandas resample MAX-VALUE with corresponding ANGLE-VALUE
I have to resample wind_velocity and wind_angle from the 2 sec period to 2min and receive the maximum wind_velocity (= MAX ) with the corresponding wind_angle ( not MAX ) .
And one last addition , and this one is likely not trivial : is there a way to have the program detect the same file origin and replace it with the new one ?
Duplicates will keep first instance and drop all instances thereafter .
Hence I concatenate ( i.e. , stack ) new on top of old .
How to combine single and multiindex Pandas DataFrames
Along with a multiindex dataframe : #CODE
When I try to use ` concat ` it flattens the multiindex as follows : #CODE
If instead I append a single column to the multiindex dataframe ` df2 ` as follows : #CODE
How can I instead generate this dataframe from ` df1 ` and ` df2 ` with ` concat ` , ` merge ` , or ` join ` ?
This can be done with ` groupby ` and using ` apply ` to run a simple function on each group : #CODE
For each group , if any of the values are not the same as the first value , aggregate that group into its first value ; otherwise , aggregate it to ` nan ` .
and if you want to apply your custom function you can use apply where it takes your custom function as a parameter , and it passes each group to your custom function #CODE
By I am incorrect hist .
` len ( hour_list )` seems to work fine .
I am having a diff . issue though .
Just to compare against using ` apply ` : #CODE
If you had not called ` apply ` on the ` groupby ` object then you could access the ` groups ` : #CODE
I'd construct a ` dict ` of how you want to ` map ` the values and call ` map ` on the column , example : #CODE
` object ` will be the displayed dtype here irrespective of whether you have missing values or not , this means python object for ints , floats , datetimes and bool will be displayed as dtype , object for all others this is correct behaviour
Including the group name in the apply function pandas python
Is there away to specify the groupby call to use the group name in the apply lambda function .
is there away to get the group name in the apply function , such as : #CODE
How can I get the group name as an argument for the apply lambda function ?
I tried to use datetime.replace() function to replace the day .
Here how to shift the index to the end of each month .
You can choose different frequency by changing the ` freq ` parameter ( see Offset Aliases ) .
and I would like to replace all the NaN in columns stating with ' dummy ' with previous values ( and only these columns while the rest of the dataframe remain unchanged )
@USER Hey , it doesnt work . the clip function doesnt return me the results of - 0.9999 or 0.9999 .
I can't say as I don't have your data , you may need to investigate that line as ` clip ` does the correct thing
This is difficult to do in place as pandas will expand the structure after the ` apply `
the dtype probably falls through to ` object ` as it's not categorised as int , float , datetime or bool so if it's behaving correctly then this is just a display artifact
You can use the vectorised ` str ` methods to replace the unwanted characters and then cast the type to int : #CODE
The calculation will be less of a formula and more like small shifts : replace values from 100 columns where year = x and month = y with values from those identical columns but a different year and month .
If you want to apply values from other parts of the df you could put those in a dict and then pass that into your apply function .
How to calculate a rolling count of a categorical variable in pandas
I'm attempting to do a rolling count on a dataframe .
For example , for Col-A , I aggregate by day , hour and minute from another dataset to a value ' k ' .
I want to insert this value ' k ' into a dataframe at the ' right ' row-index .
can always append more rows later )
do you want to keep the df2 or can we append to that df ?
I would first do an outer merge on the dataframes .
Next , make a new dataframe that merges these two ( use an outer merge ) .
Using this map of NYC I'd like to change Manhattan to be bright blue .
This can be efficiently done by converting a single tz at a time ( but since we have many , groupby already separates these out ) .
If I'd like to unpack and stack the values in the ' nearest_neighbors " column so that each value would be a row within each ' opponent ' index , how would I best go about this ?
Then you can use ` pd.merge ` to join this back to your original dataframe as @USER suggested in the comment to your original question .
Pandas dataframes - join on similar timestamps
I want to join them in such a way that every row in ` small_df ` is joined to a row in ` large_df ` that comes immediately after it , so that the desired result looks something like #CODE
I know I need some kind of merge , but not exactly sure .
I think the correct solution involves some sort of filling ( bfill of ffill ) combined with merge , but I am not sure how to do it .
Do the documented rules apply to sub-expressions as well then ?
e.g either have the Python program insert #CODE
Expand and merge Pandas dataframes
I have two dataframes I would like to merge .
You want to do an outer merge and set ` left_index=True , right_index=True ` : #CODE
outer ` join ` would work also : #CODE
The problem with ` apply ` is that you need to return mulitple rows and it expects only one .
I think what I needed to know is that it's not possible to reassign different dimensions in an apply function .
In reality , each row can map to a different number of results .
The ` groupby ` version of ` apply ` supports ` DataFrame ` as return value in the way which you intended : #CODE
Just call ` apply ` and call ` tuple ` : #CODE
You could create three copies of your initial dataframe , then drop two columns in each , i.e. df_1 would have cols =[ SAMPLE_ID , ROW COL , LAB_L ] , df_2 would have cols =[ SAMPLE_ID , ROW COL , LAB_A ] , etc .
Finally you can merge these three data frames together .
Simply use the ` melt ` function : #CODE
Panda's merge several csv's with one common column
I have 13 csv files to merge .
You should do an outer merge as follows , making use of the built-in reduce method : #CODE
First , let's assign the aggregate df to a new object .
How do I apply a regex substitution in a string column of a data frame ?
You should assign another DataFrame to hold the index and value of such , and apply to the original DataFrame base on the groupby field ( as index ) .
Then apply and perform the lookup : #CODE
I added a simpler version using numpy array , then just add it to the df before you append .
Just append to the original data frame .
If the timestamp column is your index , you can use ` resample ` , for example , to count the number of observations over six hour intervals : #CODE
One way is to resample a pandas DatetimeIndex #CODE
I am wondering if it is possible to use this function to , if in the case of an empty dataframe , return just that empty dataframe's headers and append it to the concatenated dataframe .
For each of the new data frames I then apply this logic : #CODE
A hack is to set the dtype to object before doing the apply : #CODE
Now I want to find the percentage change between the median values of the 2 quarters so I do this : #CODE
You basically want to do a pivot table .
This solution worked if i apply group by column only on Sex field .
This function which will drop any existing tables with the same name ( `' data '` in this case ) and recreate a new table named `' data '` with contents corresponding to the ` DataFrame ` ` data ` .
Using string format method will replace the value in braces with the variable value .
How can I interpolate based on index values when using a pandas MultiIndex ?
For a given country , sex , and year , my age-pattern has missing data , and I want to interpolate it based on the value of the age .
Surely there must be some way to interpolate based on the index values .
I found this hacky work-around that gets rid of the MultiIndex and uses a combination of groupby and transform : #CODE
This answer isn't great , because it will only interpolate a single column at a time , while panda's DataFrame interpolate multiple columns all at once , something I'd like to do .
f = open ( path , mode , errors= ' replace ')
How to apply cubic spline interpolation over long Pandas Series ?
I need to replace missing data within pandas Series using cubic spline interpolation .
pandas - boxplot median color settings issues
I have this issue coloring the median of the boxplot generated by the following code : #CODE
Looking at the code for ` DataFrame.boxplot() ` there is some special code to handle the colors of the different elements that supersedes the ` kws ` passed to matplotlib's ` boxplot ` .
Thanks , but for my purposes in plotting a multindexed dataframe I need the boxplot function ( with ' by ' column ) instead of the kind= ' box ' version .
Actually the following workaround works well , returning a ` dict ` from the boxplot command : #CODE
Partly because there's as of yet no convenient cartesian join ( whistles and looks away ) , I tend to drop down to numpy level and use broadcasting when I need to do things like this .
If I understood well your question , I would normalize your data multiplying each value by the current maximum and then divided by the sum of all elements .
Please note I changed it from ` pd.append() ` to ` df.append() ` , there is no append function directly in pandas module , there is only ` pandas.DataFrame.append() ` ( or ` series.append() ` , but I guess you maybe wanting ` df.append ` for your case ) .
How to join 2 pandas time series
This is not really a ' join ' problem but a ' re-indexing ' problem . pandas supports this and can do this in one line of code .
How do I replace the NaN in the index with a string , say " No label " ?
Finally , I use ` .loc ` to locate these elements values in the column and then replace them with ` None ` .
Does this mean that I can not replace the column with a calculated value ?
" To use your code again , one way to do this would be to index exactly the part of the DataFrame you want to replace on both sides .
1 ) Does this mean that I can not replace the column with a calculated volume ?
You can replace a column with a calculated volume .
Try referencing the column you are trying to replace with new values as :
Thus it has to be a loc type referencing column's name or something like this .
you also need to align the bars to the
This approach is very slow as it involves iteration and invoking the apply method for each group .
@USER . thought about this slightly more and realised to use cummax() rather than rolling max .
Yes , I do not show that I merge the result of calculate_duration with the result of the aggregation in the code above .
I append the some error message here :
Explanation : First prepare a binary mask which is True where ` df [ ' RF '] 0 ` : #CODE
Next , dilate the mask to join together islands of ` True ` s ( rainy days ) separated by 5 or fewer ` False ` s ( non-rainy days ): #CODE
So does icecream demand map to particular ranges in temperature in your data ?
I have a concern though : Documentation says there is no ` read_only ` arg , but a ` use_iterators ( bool )` arg , which is for lazy iteration .
` resample ` creates empty rows that I wouldn't like to have .
I know I can drop them but I'm looking for a solution that will take date from multiple columns .
Someone else on SO had a similar question , but their solution was to use resample .
use [ ` isnull `] ( #URL )
If you're comparing scalars , one way is to use ` assertTrue ` with ` isnull ` .
Before doing an assert_frame_equal check , you could use the .fillna() method on the dataframes to replace the null values with something else that won't otherwise appear in your values .
Using ` fillna() ` to replace ` NaN ` with some other scalar to be compared for equality is redundant and so isn't used in Pandas ' unit tests .
calculating expanding mean for all columns in df pandas
@USER transform made my first couple columns disappear but it looks like the rest of the data might be correct
@USER so the values ARE correct . the two cats are year and name . why does transform work and is there a way to get it to work without having to add the columns back in after the fact ?
@USER that's a good point , despite it works there is something tricky in ` transform ` ...
Re transform vs apply : generally speaking you use transform to keep the same number of rows when the function would otherwise reduce the number of rows .
Assuming I have an integer index that does not start from 0 or is not formed of successive numbers , how can I make this work without having to convert the index to string or reindex starting from 0 ?
drop the temporary key column
You need to apply your function to a data frame , not a series #CODE
I do not EVER want to combine merge rows .
But somehow append or concat is not working
Basically I append all the dataframes created from your csv files into a list .
I then use the pd.concat function to merge all the rows between each element of the list of dataframes dfs .
You could append row ` i ` ( in your case , ` i == 3 `) ` j ` times ( ` j == 8 `) with #CODE
thanks . will try both and see which works best for my purpose - especially since I'm having to reindex with @USER ' s suggestion in order to plot against original data
While you can append to the DataFrame , it's a relatively inefficient operation , as each step takes a copy .
` reindex ` provides an easy way to align the data to a new index , then you can forward fill the the values with ` fillna ` method .
In Pandas for Python I have 2 datasets , I want to merge them .
Getting an error : NotImplementedError : operator ' / ' not implemented for bool dtypes
I assumed bool would evaluate to 0 or 1 .
I would apply your operations to a copy of the ` DataFrame ` and stack back together - something like this : #CODE
You could use ` apply ` to generate the values for each range , then ` melt ` to reshape the data into long form .
convert my list has unicode string which I ' m not be able replace in python
have strip multiple operation but not able to do it .
But It's stack overflow rules .
How do I remove / strip the row number from a variable in Python
So , how do I either strip or remove \nXXXXX from datex and timex ?
However , when I join them the result is empty , as one of them is the number ` 533 ` and the other is a string `" 533 "` .
It's encapsulated in the following function that assumes the rows are indexed with some range ( m ) for positive integer m and the DataFrame is simply indexed ( no MultiIndex ) as in the example provided in the question .
@USER : thanks for the information . combining row selection with concat is good and I will keep it in my toolchest
I use a conditional list comprehension to join by lists .
Alternatively , you can just reindex the list using ` idx ` : #CODE
I want to resample a pandas dataframe from hourly to annual / daily frequency with the ` how=mean ` method .
Would be nice if this was integrated in the resample function itself ....
I would like to avoid the ` merge ` because leads me to uncorrect behaviours .
If you actually have a multiindex pass a tuple with all the multiindex levels instead of the single label .
You can see records for April and May are missing , and I'd like to insert sales as zero for those missing records : #CODE
` reindex ` to add rows for the missing months ,
Edit : I just noticed that specifying ` right=False ` makes the lowest interval shift to 0 rather than - 0.4 .
Plotting multiindex pandas DataFrame with matplotlib
In that case I believe you can use transpose on the original edit ( edited )
A transpose will not help here.Please look at the updated question.Pardon the naive skill .
You could have a len ( df1 [ ' s ']) x n matrix to represent the sets in df1 [ ' s '] and then an n-length vector to represent df2 .
And btw , please replace ` index=None ` with ` index_col=None ` in the code above ( in the comments ) to correctly read the CSV file .
My goal is to transform this from a dataframe to an array of two dimensions where X is the name of the rows , Y is the name of the columns and the counts make up the records in the table .
To get the same result as a pivot table , you can also perform a ` groupby ` operation and then unstack one of the columns : #CODE
Using new_df I would like resample the master_df searching for and binning genes that are similar .
It may take me some time to evaluate and apply .
Next you can simply map : #CODE
Cannot merge CSV file into one dataframe
I am using ipython notebook and struggling to merge 208 CSV files into one dataframe .
But I need a fast append operation .
Put that in your scripts for both your figures and that should align them both nicely .
I have to generate those figures first and align them using Latex .
You could do it this way , call ` np.min ` on the df as a np array , use this to create a boolean mask and drop the columns that don't have at least a single non ` NaN ` value : #CODE
Wouldn't it make sense to merge the values when creating the columns rather than merge them all into a single df and have to sort it afterwards ?
I created a list with the value I want to use to replace the current index by doing : #CODE
then I tried to replace the old index doing : #CODE
I have tried with resample and groupby but These just seem to work with the index .
But you could use ` set_index ( ' Timediff ')` if your only problem is the need to use resample on a non-index column .
pandas groupby aggregate ignoring blank or none values
How can we apply a groupby and an aggregate on multiple columns ignoring the blank / None / NaN values ?
Basically , i want to aggregate columns over date and take count of the remaining columns ignoring the None / blank / NaN values .
I am trying to apply a groupby and count agregation function on these values as :
To do this , create a new environment like above ( make sure to pick a different name to avoid clashes here ) but replace point 4 by ` ( pandas_env ) user @USER : ~$ pip install pandas ` .
a ) in column C , replace the ` np.NaN with 999 ` ,
Those operations return a copy of the data ( most of the pandas ops behave the same ) , they don't operate in place unless you explicitly say so ( the default is ` inplace=False `) , see ` fillna ` and ` replace ` : #CODE
Aggregate groups in Python Pandas and spit out percentage from a certain count
I am trying to figure out how to aggregate groups in Pandas data frame by creating a percentage and summation on the new columns .
I would like to aggregate by groups in A , and C should be a percent of ( frequency of ' 1 ' divided by frequency of non-missing value ) , and D should be a summation of non-missing values .
Furthermore , how do I aggregate by A and B columns with the aforementioned C and D column summary statistics ?
You could use ` groupby / agg ` to perform the summing and counting : #CODE
The example uses brew colors but you can replace those with any Html like RGB color .
But when I try to do the same on the resulting pivot table : #CODE
A describe() of the pivot table gives this : #CODE
The error looks like ' ProductCategory ' is not a column , it looks like you set it as the index , can you show what your pivot table looks like
Try to reset the index of your DataFrame named ` pivot.reset_index ( inplace=True )` and then use ` loc ` to make selections as usual .
Alternatively , you can just use ` loc ` as follows on the original ` pivot ` : #CODE
Thanks , modified the join to be for Type 1
I tried this ( it gives the same error ): validation [ ' intercept '] = pd.Series ([ 0 for x in range ( len ( validation.index ))] , index= validation.index )
validnew [ ' intercept '] = pd.Series ([ 0 for x in range ( len ( validation.index ))] , index= validation.index )
My understanding is that ` [ group_indexes ]` filters the ` app ` dataframe as a boolean mask .
The first argument gives the locations to put the ticks ( ` df.index ` here is equivalent to ` np.arange ( len ( df ))` , but if your index isn't evenly spaced use the arange ) .
It looks like you want to replace a column with the max value in that column , grouped by the values in another column .
You should be able to use ` groupby() ` and ` transform ( max )` to get what you want : #CODE
Might be worthwhile to add that in a dataframe with lots of columns , one should just transform the particular column one is interested in , e.g. ` df.groupby ( ' var1 ') .var2 .transform ( max )` ( when I leave this out , the groupby / transform operation actually takes longer than the for loop on my dataframe ) .
Because your are updating the same set of values in that dataframe at the same time , you can try to use a temp variable to hold the result and apply back to the column , see if this helps ridding of the warning message .
Pandas resample function not working on DateTimeIndex
Then when I try to resample the index into weeks like so
My type check earlier says I have the proper index , but the resample function doesn't thinks so .
axis=1 means it's trying to resample the columns ( which isn't a DatetimeIndex ) .
Note : this is the default for resample : #CODE
I have files going back a number of days , so I call a certain number of them and concat them into my DataFrame , typically using the last 20 days .
Here is the feedback I get from plugging that in : AttributeError : Cannot access callable attribute ' unstack ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method
You want to pivot your data so that dates are down one axis and time the other .
I don't sure about iloc in this case , but if you want more strictness - you can always use selection by label with loc attribute .
Loc selects rows by index ( not integer-location like iloc ) which in default case is just index of row in numpy matrix .
Why not just strip them out ?
I see you can do two conditions on one line , but I don't see how to apply it .
The second line is perfect , but how do you implement the first stage , where location id can be specified for the group by to apply to the subset ?
How to transform list of dictionaries with jagged arrays to a DataFrame
Approach is to create a DataFrame for each row with color counts , transpose and concatenate .
To perform selection using the time information alone , you can make use of the ` dt ` accessor available to datetime-like columns : #CODE
You can make use of the ` dt ` accessor for datetime-like Series : #CODE
Using Python , I am struggling to merge 208 CSV files into one dataframe .
Then loop again and append each one using a ` try / except ` block to catch the errors .
I use pd.pandas to get the result with sum of values , but I cannot figure out how to get the ' % Diff ' , for example , how can I get the percent value comparing data of Jan . with Feb .
I've tried various permutations of stacking a multi-indexed DataFrame , grouping , pivoting -- I can't seem to figure out how to take the value from the " Car " column and transpose it to a new column with the value " True " , merging people together by , say , their name .
Use the ` map ` method of Series to look up in the dictionary : #CODE
Do I need to create a separate series and then use ` concat ` to add it to the dataframe , or is there a way to do it all with one line ?
You want to filter the df first , if you pass a list of the tickers so ` [ ' LT ' , ' HDFC ' , ' ACC ']` instead then you can do ` d [( d [ ' Ticker '] .isin ( ticker )) & ( d [ ' Date '] > start_date ) & ( d [ ' Date '] < end_date )]` and then call pivot
then pivoting the edited data frame , check this pivot if you want to get more information about pivoting #CODE
You need to replace ` 24 ` with ` 0 ` in order for this to parse : #CODE
Because Pandas needs to map which index values have an equivalent true value , so it can select the proper outcome .
About reseting the index , I thought you needed it multiindex as they were .
Discretization of continuous attributes using np.histogram - how to apply on a new data point ?
After I " learned " my bins from train data , using ` np.histogram ( A [ ' my_var '])` how do I apply it on my test set ? as in which bin is the my_var attribute of each data point ?
How can I reindex df so that the latest value for a given index in each column is used ?
Pandas : How to apply a function to different columns
I want to apply the function to just columns ` B ` and ` D ` .
But I cannot fathom how to select distinct columns to apply the function to .
How to apply a function to two columns of Pandas dataframe
Pandas : How to use apply function to multiple columns
Pandas : apply different functions to different columns
Python Pandas : Using apply to apply 1 function to multiple columns
You can replace ` x.str.lower() ` with ` function ( x )` .
None of them seem to apply to this problem , and all of them say that in order to generate unique values from every column , you should either use a groupby function , or select individual columns .
I have tried .unique() , .value_counts() , and .count , but I can't figure out how to apply any of those to work across multiple columns , rather than a groupby function or anything that was suggested in the above links .
I have simplified my answer to suppress an unnecessary call to ` transpose ` .
Just call ` apply ` and pass ` pd.Series.value_counts ` : #CODE
For some reason I struggled to get the merge working properly , so I've had to do it explicitly via :
No idea why , but it was returning an empty merge before and now it returns a merge of the expected proportions .
Use ` insert ` and make use of broadcasting if you want a column of all 0s : #CODE
I had understood ' converters ' specified a function to apply to the column . evidently I was wrong - thank you for pointing this out , it's very useful !
The read_excel() function has a converters argument , where you can apply functions to input in certain columns .
It won't be super-performant , but you should be able to ` apply ( pd.Series )` : #CODE
In python , opening a file with ' a ' means append as write only .
Using ' a+ ' will append with read and write access .
I have also founnd a page describing google_exchange codes indicating the one I'm looking for ( ' EPA ') but I don't how to transpose it in DataReader .
Reshape / pivot pandas dataframe
I am trying to figure out a way to pivot the data so the variables are : ` id , year , a , b `
I am thinking about creating a hierarchical dataframe , but am not sure how to map the ` year ` in the original variable names to a created hierarchical column
now all i have to do is move the years to a column , but i think that should be easy to do w pivot / reshape
You can use ` factorize ` to encode the different values in the column as integers : #CODE
I think you may have to apply the operation to each column individually since ` factorize ` only takes a 1D array as input .
Accordingly , I followed the guidance provided elsewhere on stack overflow , but pertaining to ` re.sub ` , and attempted to apply it to ` pandas.DataFrame.replace ` ( using replace with ` regex=True , inplace=True ` and with ` to_replace ` set as either a nested dictionary , if specifying a specific column , or otherwise as two lists , per its documentation ) .
a function is an object just like anything else in python , so when you replace one object with a function , it makes sense that you achieve your result .
I am using apply right now , but this really should work with ``` pandas.DataFrame.replace ``` .
regardless , you should use apply or #URL
You don't need to pass ` df1 ` to the ` hist ` method .
I can code for a specific time interval , but how do I translate " at least Y continuous seconds " into python ?
then you can merge both frames on index , and you can do whatever you want #CODE
if you don't want to merge both frames , you can apply the same logic on ` dfV `
Thanks @USER . how can we apply the function to only the rows missing values in C and D ?
check this link indexing-view-versus-copy if you want to know why I've use ` loc `
If you want to use the ` cuts ` string then you can use ` query ` with ` loc ` you need to use ` isin ` with the index and invert the boolean mask using ` ~ ` in order to set the ` False ` rows : #CODE
pandas concat arrays on groupby
After I applied some filtering on ` agg_df ` I want to concat the IDs #CODE
TypeError : dt must be datestring , datetime or datetime64
If I were using arrays I would normally stack them along the third dimension and then average on the third dimension .
I have played around with groupby and resample , but I have been unable to make those work .
I would add a new column to the ` tips ` dataframe mapping the boolean mask : #CODE
You could compute the difference between consecutive Timestamps , and form a mask which is True when the difference is not 1 minute : #CODE
Then take the cumsum of the mask to identify which rows belong to which group : #CODE
If the product is not there it creates a new set of rows to insert the values of the product .
Pasting 20 lines of code out-of-context and a stack trace isn't likely to get a response .
Once you have that , you can apply it to every row using the ` apply ` method on dataframes : #CODE
Then date columns ` min ` ( values `' No '` and last previous `' Yes '`) and column ` date1 ` ( other values `' Yes '`) can be join by columns ` count ` .
how about doing what you've shown above but iterating over each column . then just store the resulting indexes as sets ; finally you can just find the intersection of the sets for your final desired rows .
You can use ` apply ` to make the code more concise .
The ` apply ` function applies the ` contains ` function on each column ( since by default ` axis=0 `) .
The ` any ` function returns a Boolean mask , with element True indicating that at least one of the columns met the search criteria .
Could you point me to something that would explain a bit more the syntax of the last command : ` df [ mask ]` I'm not familiar with that use of brackets .
I am trying to transform a large dataset , and I am stuck .
Create a boolean mask using ` shift ` that tests whether the previous row contains 1 or not : #CODE
the boolean mask looks like this : #CODE
It shifts the the series by a single row , there is no rolling as it will introduce a ` NaN ` value at the first row
How does the code know where to shift , it doesn't look intuitive when we have " ...
I call ` shift ` on the column so ` df [ ' Roll '] .shift() ` will move the rows down by one , you can see this when you try it yourself
that i could create from iterating over the list of ` LOC ` and doing a ` groupby ` and ` get_group `
and as a second part , i also can't seem to figure out ( and unable to phrase the question well to look for answers here in stack overflow ) how to produce the change in measurements .
for instance ` locations = df [ ' LOC '] .unique() ` will return an array of unique locations you can then use this to filter the df like ` df [ df [ ' LOC '] == locations [ 1 ]]` ...
Use ` diff ` so ` df [ ' MEAS '] .diff() ` will give you what you want
Or would it be better to just load everything in memory and use a multiindex dataframe ?
When using HDF5 , would it better to load the multiindex dataframe all together .
You can make use of the DataFrame's ` apply ` method .
Here ` isin ` generates a boolean mask , we use this to filter the df : #CODE
I want to transpose the dataframe and then set a 2-column multi-index : #CODE
However , df.columns is a periodindex object after I transpose .
I can probably send this to a dict and then replace the timestamps with labels , but there has to be a better way that I haven't found in the docs yet .
Cannot resample my Dataframe properly
I want to resample this dataframe following , #CODE
Resample bins the data into the frequency passed , so in this case it's going to group all the times from 1 day together .
I would like to go through all the columns in a dataframe and rename ( or map ) columns if they contain certain strings .
do you know if i wanted to replace the whole string with ` agri ` rather than just ` agriculture ` ?
The pattern specified in `` replace `` is regex , so you could do something like : `` replace ( ' . *agriculture* ' , ' agri ')`` ; however , that would cause the updated DataFrame to have two columns with the same name .
error : nothing to repeat ` it seems like it does not like the wildcards in the replace function
when i throw it in i then receive ` ValueError : cannot reindex from a duplicate axis `
You are looking for ` DataFrame.pivot_table() ` , pivotting based on the columns - `' Published ' , ' Station '` , taking values from column - ` TypeFuel ` for the new columns in the pivot table and using values from ` Price ` as its values .
@USER This not a duplicate , the OP does not even use apply here .
@USER Try using ` for i in range ( 1 , len ( df_z )): ` , as range starts with 0 and the i=0 case is already done before the for loop .
But look at ` len ( df_z )` , the result will have the same number of rows .
Instead of ` range ( len ( df_z )` , try using : #CODE
Pandas : How to structure row-wise apply which requires previous output as input
pandas - sum objects in DataFrame column and join with DataFrame
And next I need to sum every id's ' buy ' and join the new column ( I named it buy_count ') with my DataFrame .
But I can't insert ' buy_count ' to the DataFrame : #CODE
You can call ` map ` against ' id ' column of ` df1 ` and pass ` buys ` to perform a lookup : #CODE
python replace string function throws asterix wildcard error
I am currently using hierarchical columns and would like to only replace ` agri ` for that specific level ( level = 2 ) .
Based on your new and actual requirements , you can use ` str.contains ` to find matches and then use this to build a dict to map the old against new names and then call ` rename ` : #CODE
Recursively read files from sub-folders into a list and merge each sub-folder ' s files into one csv per sub-folder
I am trying to work out how to use ` pandas ` to recursively navigate a folders sub-folders , take each file in the sub-folder and merge it into one CSV file per sub-folder .
Alternatively , you can also apply a function based on the column ` col1 ` : #CODE
The ` apply ` approach should be preferred in this case as it is much faster : #CODE
2 ways , use a couple ` loc ` calls to mask the rows where the conditions are met : #CODE
I was looking at this example #URL but couldn't apply it to my own
I plan on creating a pivot table to have each unique value as a column to look across multiple students ' selection per each item .
But I want the order of the pivot to be by the Question Position , or the third value in the concatenation .
I've tried sorting my data frame by the original column I want it to be sorted by ( Question Position ) and then creating the pivot table , but that doesn't render the above result I'm looking for .
Or is it possible to sort a pivot table by the third value in each column ?
I'm fairly new to both Python and Pandas , and trying to figure out the fastest way to execute a mammoth left outer join between a left dataset with roughly 11 million rows and a right dataset with ~160K rows and four columns .
It should be a many-to-one situation but I'd like the join to not kick out an error if there's a duplicate row on the right side .
This works with small files but produces a MemoryError on my system with file sizes two orders of magnitude smaller than the size of the files I actually need to merge .
If a system with 8 GB RAM isn't enough to merge two files each 1 / 100th the size of the files I need , how much RAM would be enough ?
AFAIK , all join / merge operations can be done by sort-merges .
No idea why 8 GB of system RAM isn't enough to merge those , even with Windows 7 .
If that method can be used to conduct a left join between two datasets I'd need some more explanation .
You probably have repeated values in the merge ' keys ' which results in a Cartesian product of such rows in the output
Since it's a left join and not a full join , the number of rows in the output should be the same as the left dataset .
By splitting the left dataset into chunks , turning the right dataset into one dictionary per non-key column , and by adding columns to the left dataset ( filling them using the dictionaries and the key match ) , the script managed to do the whole left join in about four minutes with no memory issues .
I'm looking for a way to transform 1 long row into a certain number of fields , so that each field has the same number of columns .. seems like this should be simple , and i've used pandas to create dataframes from lists of dicts hundreds of times w / o encountering this issue until now ..
Pandas : rolling std deviation / mean by trading days
I am trying to extract the rolling std deviation and mean on trading data by using ` rolling_* ` functions of ` pandas ` .
Next , well group and transform the data so that we take the mean price for each ticker on any given day and that dates are unique in the index : #CODE
And to get a rolling standard deviation , just use ` pd.rolling_std() ` .
You can ` import json ` and apply ` json.loads ` to convert the string data in your ` geojson ` column to ` dict ` .
As a simple approach , I would transform your sample table into a boolean presence matrix , which would then allow you to perform the logic you need : #CODE
And since this case has type of ** String** , the only way to evaluate it is to use ` eval ( case )` .
Finally you aggregate by choosing the first or the last of the group .
Pivot table from a pandas dataframe without an apply function
I want to create a pivot table with the following output : #CODE
I thought using the pivot function to the dataframe ( df_pivot = df.pivot ( index= ' ID ' , columns= ..., values= ' count ') but I am missing the columns index list .
I thought applying a lambda function to the df to generate an additional column with the missing column names but I have 800M IDs and the apply function to a grouped dataframe is painfully slow .
Hi , yes , I thought using the following function : def f ( x ): x [ ' Index '] = range ( len ( x )) return x and then do the following : df.groupby ([ ' ID ']) .apply ( f )
Then apply the pivot method setting the new ` subindex ` as columns and fill ` NaN ` values with 0 : #CODE
I cannot think of a way to translate this where statement into pandas syntax .
The only way I can think of is to add an arbitrary field to people_usa ( e.g. ` people_usa [ ' dummy '] =1 `) , do a left join , then take only the records where ' dummy ' is nan , then delete the dummy field - which seems a bit convoluted .
I'd like the option to specify the field ( s ) to apply this to
use ` isin ` and negate the boolean mask : #CODE
so 3 and 4 are removed from the result , the boolean mask looks like this : #CODE
using ` ~ ` inverts the mask
Is there any easy way to do this if you have multiple columns to check / join ?
You could do a ` merge ` and then eliminate the rows that exist in the merged df otherwise you'd have to build a boolean condition for all the columns you want to compare but presumably when checking the multiple columns you're stating that it's unique for those columns , correct ?
Yes merge is what I have been doing but it feels like a hassle .
@USER : You could use ` mask = people_all [ primary_key ] .isin ( people_usa [ primary_key ]) .all ( axis=1 )` .
I think you have to do it this way , pandas will try to use existing indices and column labels for alignment without renaming you can't infer how the values should align without renaming
You could try the melt function #CODE
Sorry , I want to simply identify the column that contains the text ' Measure ' in it , which I then apply the filter measure_filter too using .isnin .
Sorry for not being as clear cut , I've attempted to update my question to be more concise .
I'm wanting to apply the logic to just identify the word ' measure ' within the following code ` ( df [ ' hereisalltherandomtextmeasure '] .isin ( measure_filter ))`
but instead of having my weekofyears columns , I got stuck with a MultiIndex I could not get rid of .
If list of functions passed , the resulting pivot table will have hierarchical columns whose top level are the function names ( inferred from the function objects themselves )
( perhaps using lambda and apply )
You could apply a join to the list elements to make a comma separated string and then call the vectorised ` str.split ` with ` expand=True ` to create the new columns : #CODE
A cleaner method would be to apply the ` pd.Series ` ctor which will turn each list into a Series : #CODE
Maybe something like ` df [[ ' id ' , ' email ' , ' address ']] = df.col3.apply ( pd.Series )` then drop ` col3 ` ?
Pandas Merge 2 data frames by 2 columns each
How can i merge 2 data frames by 2 columns :
Can you post an example data and df , your text description is not clear enough but generally you want to merge and pass the list of cols to merge the ; hs and rhs on : ` pd.merge ( df1 , df2 , left_on =[ ' Key_Merge1 ' , ' Key_Merge21 '] , right_on =[ ' Key_Merge1 ' , ' Key_merge22 '])`
OK , you have to rename ' PRODUCT_GROUP ' in DF2 in order for the ` merge ` to work : #CODE
the merge will naturally find the 2 columns that match and perform an inner merge as desired
You could use ` apply ` to remove the nulls and take the integer location like this .
Can pandas groupby transform a DataFrame into a Series ?
The error makes sense in that I think ` .transform ` wants to map a DataFrame to a DataFrame .
I modified my answer to pass a DataFrame to DataFrame function to ` transform ` .
If you change ' transform ' to ' apply ' , you'll get : #CODE
That's not quite what you want , but if you drop level=0 , you can proceed as desired : #CODE
I'm also not sure if it will create multiple DFs or if I can append the groupbyobjs .
What I want to do is join two dataframes on columns and keep the index of one of them ( but the index is unrelated to whether I join them or not ) .
For example , if ` df1 ` is the dataframe that has certain timestamps as its index that I would like to keep , then to join with ` df2 ` on the ' key ' column , my expected code would be #CODE
What is the proper way to join two tables , keeping one of the indices ?
I am doing an inner join on ' key ' .
Joining the tables really depends on the data , there is not a * single * proper way to join all the tables with * one * universal command .
As for keeping just one index , that should be done automatically in this case now that outer join is keeping all keys .
Then just call the function , specifying the name of the columns to shift #CODE
For this you could combine ` np.random.permutation ` ( which returns a shuffled version of an array ) with a ` groupby ` and a ` transform ` ( which returns a like-indexed version of the group ) .
Attempt using SQL : I don t have access to the database as I m writing this , but I had to join two tables to get the Team field , and I think I unsuccessfully tried something along the lines of ( this may not be completely right but should be close to what I tried ): #CODE
Then what you'll need is to index each column using then apply ` df.resample() ` #CODE
You can pass param ` parse_dates =[ 0 ]` for ` read_csv ` so try ` df= pd.read_csv ( " myfile.csv " , names =[ ' DateTime ' , ' Freq '] , parse_dates =[ 0 ])` this will parse the first column as a datetime , it should be significantly faster
return pd.read_csv ( " f20141.csv " , names =[ ' DateTime ' , ' Freq '] , parse_dates =[ ' DateTime '] , skiprows=1 )
Watching my system as it reads the file , the memory usage crawls up until its completely maxed out ( all 6gb used ) , then after the ` del ` command , the memory usage seems to drop down , but not all the way down ; now approximately 1gb of memory is being used .
Edit : after quiting ipython the memory usage seems to drop back down to the original level
How to transform date range stored as two columns ( start , end ) to create new row index and create accumulated rate for values ?
I was wondering how to transform a date range stored as two columns ( start , end ) to create new row index ?
If that's the case , instead of iterating over the rows you can consider apply , map , or applymap methods based on your need .
@USER I have plenty of if statements inside that funcion , else I would have used apply
How to do " ( df1 & not df2 )" dataframe merge in pandas ?
I want to merge do a " ( df1 not df2 )" kind of merge on keys ( x , y ) , meaning I want my code to return a dataframe containing rows with ( x , y ) only in df1 not in df2 .
_merge is Categorical-type and takes on a value of left_only for observations whose merge key only appears in ' left ' DataFrame , right_only for observations whose merge key only appears in ' right ' DataFrame , and both if the observation s merge key is found in both .
You could add a dummy variable to your ` useractivity_ids ` then use the pandas ` merge ` to compare and filter .
In the next version of pandas ( 0.17 ) , ` merge ` has an ` indicator ` keyword that lets you do this without the dummy variable .
you can do left join between the two data frames #CODE
You could drop the rows that don't contain ' years ' and then remove the word and cast the dtype so something like ` df.loc [ df [ ' Duration '] .str .contains ([ ' years ']) , ' Duration '] .str .rstrip ( ' years ') .astype ( float )` should work
No idea , normally it will try to sniff the form of the csv and guess the number of columns , if it doesn't conform to the sniffed format it should fail so I'm surprised it worked and in fact your previous code worked as it should have borked once it saw the second line didn't match the first line format
Is it possible to transform it into a more generic solution ?
And this will strip out ` year ` or ` years ` from the ` emp_length ` column , although you are still left with text categories .
I want to interpolate by row and am using this : #CODE
I'd drop the ` inplace ` argument ( it's rarely of benefit anyway ) and then the operation should work .
thanks @USER , this is perfect ! but , I now realize that I should be doing interpolate and not ffill , I am getting an error there .
Python interpolate not working on rows
Related to Error in gapfilling by row in Pandas , I would like to interpolate instead of using fillna .
However , this does not seem to replace the NaN's .
Assuming you have a unique-indexed dataframe ( and if you don't , you can simply do ` .reset_index() ` , apply this , and then ` set_index ` after the fact ) , you could use ` DataFrame.sample ` .
When you do indexing , you insert a boolean vector into the dataframe and it returns rows where the vector has the ` True ` value .
I need to transform the DataFrame so that there is a single row for each name the page number column combines all the pages where the name appears .
You can do this with groupby and apply : #CODE
Is there some way to map the color of a dot to how many times it occurs in the data set ?
You could do this by adding a new F column and then calling ` pivot ` : #CODE
Pandas Left Merge / Join not Resulting in what is expected for left join
So I may simply not be very informed on what a left join is , because I am getting tripped up ...
Here is my definition of a left join :
When doing the left join , why do you expect only the first pairing but not the second ?
This requires calculating the full left join first .
You then want a pivot table where you have unique dates as your index and your symbols as separate columns , with daily returns as the values .
You need to apply ` transform ` to the ` groupby ` , which preserves the shape of your original DataFrame .
I have a series of variables of string type and I have to transform them in order to use sklearn estimators .
So then you can scale the other numerical features D , E and transform everything into a numpy array to use in sklearn .
Since , I have only a maximum of one duplicate ( ie . two copies of the same row ) , I drop duplicates by taking the last and the first in df1 and df2 respectively and then attempt to merge the two .
Your question is very broad here and typically it should be 1 problem per question , you can combine any number of columns to create a datetime column and then [ ` resample `] ( #URL )
See here for [ how to combine date and time ] ( #URL ) then look at resample as suggested
when I print dHaGreen0 , it returns a list of bool , and " dtype : bool " .
dHaGreen0 is an array of bool values .
If so , replace " dHaGreen0 == True " with " any ( dHaGreen0 ) == True " .
If so , replace " dHaGreen0 == True " with " all ( dHaGreen0 ) == True " .
I would like to use the pandas ewmstd function as a rolling window ( using only last N elements ) instead of as a expanding window ( using all elements ) .
How to create this multiindex dataframe
I'm trying to recreate this multiindex dataframe to help me run some tests however I'm struggling .
Merge columns and create new column with pandas
pandas resample with function that returns an array
Because that fft function changes the shape of the input you can't just apply it directly .
Then use a map function followed by a fillna ( method = ' ffill ')
Taking the ` argmax ` is only 1.5 % of total execution time .
If we apply the above , we remove rows 0 and 2 .
I'm trying to generate a cumulative column between any two given datetime ranges using a pandas pivot table , however it's unclear how to actually achieve this .
I'm applying this to a DF where dt index isn't necessarily unique anymore , ie . as well as { ' count ' : 0 } there will be { ' Location ' : ' Japan ' } , if I wanted to count the accumulated value between 2 dates ranges ( as per above ) by location , ( Location -> Date -> Accumulated Count ) Do you know how to achieve that ?
Got a key error on ' loc ' , seems like the ix filter removes this column ( so I added it back in before the grouping ) temp =d f [ ' loc '] then df [ ' loc '] =temp after the filter .
I guess that the first step to do is to join the two tables , but I am not able to express the predicate "` time ` between ` start ` and ` stop `" .
Yes , it similar , but actually I am not sure I need to merge the two tables .
No join is necessary .
The above code is what I have written to merge the .dat files into one Dataframe .
That did indeed drop the duplicates :) Thanks a lot .
You can drop the duplicate entries by calling ` drop_duplicates ` and pass param ` subset= ' movie_id '` : #CODE
The stack trace references different code version .
The errors happened because of my poor cut and paste between the real code an the " reproducible " code .
Pandas apply to multiple rows with missing dates
I have tried all kinds apply functions , but I can't seem to go up and down 7 days to collect the data I need .
I looked at using shift for instance , but because not all dates are filled in for all the groups , this does not work .
I also tried to use the map function , this also look too long .
Once you create Categorical Data , you can insert only values in category .
I have a Pandas dataFrame ( called ' pivot ') which is 13843 rows by 40 columns .
There were instances where I was trying to pivot duplicate data rows .
That is said , now we have multiindex [ ' category ' , ' year ' , ' month ']
The multiindex is [ ' category ' , ' month ']
` concat ` your dfs after reading them in , then ` groupby ` on ' lat ' and ' lon ' and then call ` size ` to return the count , ` reset_index ` to restore your grouped columns and finally rename the generated column ` 0 ` to ' time ' : #CODE
Pandas groupby + transform taking hours for 600 Million records
Unless I'm misunderstanding something , you're really doing an aggregation - ` transform ` is for when you need the data in the shape as the original frame .
I would like to replace some of the values in the foll . dataframe :
I would like to replace the values in the columns with a numeric value derived by multiplying a scalar ( say 0.5 ) by all values in this dataframe :
I saw replace , but not sure how to use it here
You can use the boolean mask and then call ` fillna ` : #CODE
Here ` df1 [ df1 ! =0 ]` will produce a df of the same shape with ` NaN ` values where the condition is not met , you can then call ` fillna ` on this and pass the other df which will replace the ` NaN ` values where the index and columns align .
The result of the boolean mask : #CODE
Ref about ` replace() ` : Replace all occurrences of a string in a pandas dataframe ( Python )
I could write a wrapper function that takes each row as input and then extracts the year , but I think it s useful to know how to apply pandas syntax for future reference .
` or len ( block ) == 5 ` ?
so if len ( block )= =6 and block.isdigit() or len ( block )= =5 and block.isdigit() ' ?
More concisely ` if len ( block ) in ( 5 , 6 ) and block.isdigit() `
If the " error " you want to detect is there not being a five or six digit number in the campaign string , you should put a ` break ` after the ` append ` call and use an ` else ` after the loop ends .
I would like to replace the values in a pandas dataframe from another series based on the column names .
How do I replace the values in the first dataframe based on the values in the 2nd series ?
Additionally pandas tries to align along index values and column names , if they don't match then you'll get ` NaN ` values so you can get around this by calling ` .values ` to get a np array which just becomes anonymous data that has no index or column labels , so long as the data shape is broadcast-able then it will do what you want : #CODE
That means there is no alignment between either the row values or the column names , if the column names definitely match then try ` df.loc [( df [ ' Country Code '] == replace_cnt ) & ( df [ ' Item '] == crop ) , s.index ] = pd.DataFrame ( columns= s.index , data= s.values.reshape ( 1 , len ( s.index ))) .values ` can you edit your question with what the row / columns are for the lhs and rhs , thanks
The reason that works is because when you assign values , pandas will try to align the lhs and rhs index and columns so if for instance the index values don't match then ` NaN ` will be assigned , the same thing happens if the column names don't match .
I've been playing with Pandas and think I have the fundamentals working , but I can't seem to figure out how to do one last transformation ( from my placeholder value in the pivot to an actual English word ) .
In the code below , the piece I need help with is the comment that says " I need to figure out something I can put here that will replace any non-null value found in the cells of column pivottally [ c ] with the string ' registered ' .
cut time spells into calendar months in pandas
Basically , it would suffice for me if I could cut spells at turn-of-month datetimes , getting from the data in the first example to the data in the second : #CODE
This is the cut DateTime you can use to compute the portion of the stay attributable to each month . cut - start is the first month ; end - cut is the second .
How does this cut the time spell at end-of-month ?
Classic case of pivot .
First , let's introduce a count column , then create a pivot table .
Let's ignore your regex , since that is not the issue ; just apply it to the column beforehand .
What is the Pythonic way to apply a function to multi-index multi-columns dataFrame ?
Given a multi-index multi-column dataframe below , I want to apply LinearRegression to each block of this dataframe , for example , " index ( X , 1 ) , column A " .
Some columns of the original data can contain missing value , and we cannot apply regression directly on them .
Would it be legitimate to use the interpolate method to fill the null values ?
yes . transform do the job !
The transform function seems work in a all column same index way , but not column by column .
And I want to drop the ` stockid ` column and get the rest data .
If you want to drop this column then use ` new_df = df.drop ( ' stock_id ' , axis=1 )` .
` df [ df [ ' A '] 1.0 ]` : this works - But I want to apply the filter condition to all columns .
what filter condition do you want to apply , what is an example ` df ` and what are you expecting as output ?
Based on that count , I am planning to drop the column .
I had been happily using django-tables2 but I decided I wanted to transpose the table so the header row was the first column and then the rows were columns .
My approach so far is to use django-pandas to convert the data from queryset to dataframe and then to use pandas to transpose the data .
Then I can get a queryset , convert it to data frame and transpose it .
In other words , I want to first group the data by area , then by month of the year and then plot the result as boxplot .
Also , how can I get rid of the " Boxplot grouped by [ 1 1 1 ... 12 12 12 ]" and " 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ,.....
Create a function which transforms the " refund " orders into negative values and , then , apply it on the Series : #CODE
How to perform string replace operation using pandas in python ?
I tried using the pandas built in string replace function put its returning the original column without actually performing the string replace operation .
Why not just use an ` apply ` on the column , and do something like ` lambda lst : ' ' .join ( lst )`
what does ' ' mean in the apply function ?
I think actually it's better to test for each value and apply ` any ` : #CODE
my apologies I didn't notice you have " 2200 " columns , I jumped too quick to a conclusion . but hey same rules apply you can do : ` df [[ col for col in df.columns if col.endswith ( " _x ")]]` .
In pandas , how do I replace label IDs with label strings ?
I'd like to replace the ` labelID ` column with the label names , so that I have something like this : #CODE
Are you just wanting to add the label string or are you looking to replace values ?
I want to replace the IDs with a useful string , because I'll be plotting these for a presentation and the label is much more useful than an integer .
Then you'd have to ` map ` like the other answer but it was a little unclear to me hence my comment , your sample was a bit too basic really
You can use ` map ` to replace values in a series ( or , if you wanted to replace across the entire dataframe you could use ` applymap `) .
This dictionary is then passed to the ` map ` function , which is applied on the Series you want to relabel #CODE
I originally tried a concat but if became too difficult to separate the data afterwards and I struggled with using a ` .loc ` function .
I then replace the default tab delimiter with " , " .
Can I make pandas cut / qcut function to return with bin endpoint or bin midpoint instead of a string of bin label ?
I want to apply very specific formatting to each column in the dataframe using a dict like the following : #CODE
I know I can use applymap for multiple columns or apply on a single column : #CODE
How can I iterate through each column in a dataframe and apply formatting using a dictionary where the ` dict ` ` key ` is the ` column ` and the ` value ` is the ` string ` formatting ?
The easiest way would be to iterate through the ` format_mapping ` dictionary and then apply on the column ( denoted by the key ) the formatting denoted by the ` value ` .
I'd like to replace id2 with the value from the array .
In that case I made a function to apply , in place of the lambda function , so that it could except the error , but this returned all nulls .
pandas plot replace xticks
I would like the ` x ` axis be a counter of the boxplot .
The ` boxplot ` method doesn't return the axes object like the ` plot ` method of DataFrames and Series .
The ` boxplot ` method returns a ` dict ` or ` OrderedDict ` of ` dicts ` of line objects by the look of it .
If it is , I think it would be more or less easy to solve with ` apply ` , there's many questions around but probably [ this one ] ( #URL ) will help you .
What you definitely don't want to do is insert one row at a time .
If , for any given row , you will append at most one extra row , you could do the following steps :
2 ) append an uninitialized dataframe to the end of your original dataframe , with the same length
This will have the effective efficiency of approx 4 copies , much better than a copy for each insert .
How to do join of multiindex dataframe with a single index dataframe ?
The single index of df1 matches with a sublevel of multiindex of df2 .
How to do join of multiindex dataframe with another multiindex dataframe ?
How to do join of multiindex dataframe with a single index dataframe ?
@USER proposed a solution to work with a multiindex df2 and single index df1 #CODE
Anyway , suppose ` mult ` shares the first and second level with ` sngl ` you can just drop the second level from the index of ` mult ` and index in : #CODE
Since version 0.13 , it is possible to append to a dataframe by referring to indices in .loc or .ix which are not yet in the dataframe .
To replace each NaN with a number instead of dropping rows , do this : #CODE
Use ` loc ` for label based indexing : #CODE
Python Pandas Merge Causing Memory Overflow
I'm new to Pandas and am trying to merge a few subsets of data .
Renaming a Column ID to Allow for Merge #CODE
Successful Merge #CODE
Try a second merge .
check the shape of each data frame before and after the merge
One question is how big do you expect the final merge to be ?
The ` agg ` function will do this for you .
This will display only the group by columns , and the specified aggregate columns .
In this example I included two agg functions applied to ' Y1962 ' .
To get exactly what you hoped to see , included the other columns in the group by , and apply sums to the Y variables in the frame : #CODE
Can your solution use ' yrs ' inside agg ?
I'd use map : #CODE
The issue is that you have some columns that are int , hence when trying to apply the regex on those int values it fails with the error - #CODE
You can convert your columns to ` str ` and then apply the ` DataFrame.filter ` - #CODE
You will need to convert to ` str ` before you can apply regex on the column name , a way ( not sure if the most efficient ) to not convert the column names to ` str ` permanently and still get the required data is - #CODE
If you want shift data , you get ` NaN ` value in last row .
I shift rows , fill ` NaN ` to ` 0 ` .
In the dataframe above , I try to drop the column named ' Item ' , with the foll . command : #CODE
The slice gets me a unique Item , so I cannot drop this column before .
what are you trying to drop ?
Do you want a completely new dataframe from the slice and then drop the column from that dataframe ?
Right , I am trying to create a dataframe and then drop that column .
Most probably you received the ` vals_bel_lux ` through slicing , in which case the issue is occuring because you are trying to do ` inplace ` drop ( by passing ` inplace=True ` argument to ` drop ` method ) .
You can drop the columns min , 25% , max , etc .
yes - however , I should have said that I want to apply the new series the requests.post method , in order to practice passing functions to series items .. for instance , I could have 100 columns that I would not want to write out .
The same principles would apply if you are using excel reader .
Convert pandas multiindex into simple flat index of column names
Before I do something awful like converting it to a string and strip it out , I thought I'd ask here .
transforming data frame in ipython a little like transpose
You can use ` pivot ` : #CODE
Pandas - join by time proximity
I want to join them in such a way , that for every row ` R ` in ` left_df ` , I find the row in ` right_df ` that is closest in time to ` R ` out of all rows in ` right_df ` , and put them together .
Potentially a better data structure ( rather than a Series of lists ) is to stack : #CODE
Note : I thought you used to be able to return a list in an apply ( to create a DataFrame which has list elements ) this no longer appears to be the case .
Why ' hist ' instead of ' bar ' ?
it does require some paradigm shift in thinking ...
How to get the index and column name when apply transform to a Pandas dataframe ?
Given two multiindex dataframes ( df1 and df2 ) , I want to group df1 and do a transformation .
How to retrieve the index and column name in Pandas transform ?
I think a join across multiindex levels might be better ?
Anyhow proceeding with a transform ; to scari's question i will assume same size .
Conditionally replace data in a pandas multi-indexed dataframe
I want to evaluate ' COL1 ' , subframe ' foo ' for values greater than 0 and replace ( in-place not copy ) ' COL4 ' values for the corresponding rows with a new value , COL1 / 1 .
I've been able to conditionally replace with a regular dataframe but , something isn't clicking when I try more advanced multi-indexing and setting .
And when evaluating COL2 , do you wish to replace COL4 values again ?
You can replace bits as needed .
You can replace the parameters to repeat the operation in another slice / columns of the dataframe , as needed .
I think you had problems with resetting a column of a dataframe with multiindex because the multiindex needs to be sorted for operations to return views ( and not copies ) of the dataframe .
It does ' apply ' over the rows , but I can't quite get how you could do this without that .
See Table C for an example of results ( I need to be able to join to either table ) .
Just get the len of each list in the columns , then select where the len is 0 .
2 ) There is a much better way to apply a function than map to validate urls ?
Try to replace the ` / ` with something like ` _ ` .
I might do this with apply rather than eval ( especially if I didn't trust the source ): #CODE
how to do left join using pandas
How can I let these 2 join into 1 dataframe , the result shall be like this : #CODE
What you're asking for isn't a left merge as you skipped the row with ` R3 ` , you just want to perform an inner ` merge ` : #CODE
a left merge would result in this : #CODE
` isin ` will produce a boolean mask and you can invert this using ` ~ ` this will remove the rows that contain the id's
The mask looks like this : #CODE
Then just drop the columns you don't want at the end .
Then replace the code you shared with #CODE
how to transpose multiple level pandas dataframe based only on outer index
One way to do this would be to reset the index and then pivot the table indexing on the ` level_1 ` of the index , and using ` level_0 ` as the columns and ` 0 ` as the values .
Here's the stack trace : #CODE
Since you are using ` pandas ` module , you access elements with ` loc ` or ` iloc ` or ` ix ` .
How about adding an index name to data dataframe and then use a join ?
You want a resample at minute ( ' T ') frequency , but you need to specify how the resampling is done .
You then resample as follows : #CODE
You can , of course , drop NaNs if desired .
So in the case above , each row will transform into two duplicate rows .
If the ' quantity ' column was 3 , then that row would transform into three duplicate rows .
I also suggest you to try defaultdict instead of your ` if k in dict ` create ` [ ]` otherwise ` append ` .
If it is because IPs / URLs are repeated , then you will want to transform IPs and URLs from normal columns to indices : parse by chunks as above , and on each chunk do #CODE
You can use the ` diff ` method for this , together with ` fillna ` to fill the first NaN with a 0 : #CODE
append pandas.DataFrame.GroupBy results into another dataframe
I used groupby to aggregate my dataframe , and tried to save the results in another dataframe ( df_ngram ) .
How can I aggregate the results from groupby ?
You need to append the intermediate DataFrames to a list and then concatenate the results .
pandas concat ignore_index doesn't work
I am trying to column bind dataframes and having issue with pandas concat , ignore_index = True doesn't seem to work : #CODE
I want to bind the columns ( append ) .
I tried append , that doesn't seem to work either .
I think ` ignore_index ` only ignores the labels on the axis you're joining on , so it still does an outer join on the index labels .
` ignore_index=True ` ignores , meaning doesn t align on the joining axis .
it simply pastes them together in the order that they are passed , then reassigns a range for the actual index ( e.g. ` range ( len ( index ))`)
so the difference between joining on non-overlapping indexes ( assume ` axis=1 ` in the example ) , is that with ` ignore_index=False ` ( the default ) , you get the concat of the indexes , and with ` ignore_index=True ` you get a range .
This needs a * lot * more info , at least the build log / stack trace .
I tried ` merge ` and ` concat ` to join the dataframes together , but the resulting plot is not what I'd like because those functions insert ` numpy.NaN ` in places where the date is not the same , which makes discontinuities in my plots .
I can use ` pd.fillna() ` but this is not what I'd like , since I'd rather it just connect the points together rather than drop down to 0 .
pandas pivot on multiple columns gives " The truth value of a DataFrame is ambiguous "
This pivot , with ` SubjectID ` as the index works : #CODE
Function pivot doesn't support multiple columns and indexes , I think it is not implemented yet .
Best possible way to merge two pandas dataframe using a key and divide it
or merge on ' unique ' and then do the calc as normal : #CODE
To drop them permanently do ` Temp_plot = Temp_plot.drop ( unwanted_dates )` or ` Temp_plot.drop ( unwanted_dates , inplace=True )`
Can't I drop them as suggested by @USER ?
So the best idea would be to have a list that contain all those dates and then drop them after resampling .
Using ` td [ ' Price '] .values ` means that the values from the column are in a NumPy array : this has no index and pandas does not try to reindex the values .
So I decided to use notnull but took error again .
I would like to apply something like ' text to columns ' function in excel .
I would like to merge 2 dataframes based on shared string column , but sometimes that column values differ slightly .
In above frames I would like to join based on ' street ' column , but treat for example , ' A Street ' and ' A Str ' values as same .
sorry for being vague , I was thinking if edit distance ( number of changes ( insert , delete , update ) to characters to make 1 string look exactly like other ) is say less than 5 treat them equally .
Pandas - create dataframe manually and insert values
Also you should be able to call ` isnull ` on the series itself , there is no need to use the top-level ` pd.isnull ` so ` ascomb [ ascomb [ ' Discipline '] .isnull() ]` should still work
Hello Thank you for script , but I just edited a little about my sample file in above question , because this script is throwing an error as , Use of uninitialized value in join or string at script.pl line 34
Then ` df1 ` is append to ` df ` and last is set index from list to output ` df ` .
Filter pandas dataframe by comparing column to multiindex group
Use ` transform ` to have the mean for each line existing witin each group and compare it to ` df [ ' C ']` .
Apply this mask with ` loc ` : #CODE
1 ) I'd like to strip the YYYY-MM-DD data and just keep the time data , how can I do this ?
you can strip the date by doing : #CODE
How to speed up append to an existing dataframe
I am trying to append or add rows to the existing dataframe which has around 7 million rows .
You can use np.where , if you need append rows by conditions : #CODE
You could use ` diff / cumsum ` to assign a group number to the boolean values .
Groupby and sum rows to aggregate multiple values of PRODUCT column
If you really must remove the ` microsecond ` part of the datetime , you can use the ` Timestamp.replace ` method along with ` Series.apply ` method to apply it across the series , to replace the ` microsecond ` part with ` 0 ` .
rbind.fill() which is a way to append data frames with unequal number of columns .
You are looking for the function ` concat ` : #CODE
I would like to drop all groups that are not present in the list of ids .
and got this error : ` AttributeError : Cannot access callable attribute ' drop ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method `
My ultimate goal is to merge the contents of a folder full of .xlsx files into one big file .
Another is that you can then concat these into one DataFrame efficiently in one pass : #CODE
Note : This is significantly faster than append , which has to allocate a temporary DataFrame at each append-call .
Hey EdChum thanks for the reply not sure why it would truncate my data I thought it would just pump out what's in the dataframe as default .
Did I mess up somehow and tell it to truncate ?
The default is to truncate , if you have several million rows you don't want it to crash your machine / pipe out for ages .
So if we only group along a subset of the index levels , we can use the ` shift ` method of the dataframe to get a rolling difference within each group : #CODE
but when I apply it to a for loop , it shows integer type values .
Anyway , have you tried selection with ix ?
Length : 7396 , Freq : None , Timezone : None
Since ' date ' is the index you can use ` loc ` : #CODE
I find ix indexing using datetime strings works when the DataFrame is indexed with the date column Series .
I just remember ix is for rows because Wes McKinney emphasizes it .
exec ( compile ( scripttext , filename , ' exec ') , glob , loc )
exec ( compile ( scripttext , filename , ' exec ') , glob , loc )
You can map the index with a lambda function , and set the result back to the index .
I want to join these 2 tables using ` pd.join ` but the problem is that my tables do not have a ` header ` .
You've not stated what the desired df should look like , even if you did join on first and third column respectively , what do you expect to do with the remaining columns ?
I want to join the 2 tables and export them as a txt file .
I'd set the index to the ordinal columns that you want to merge on , then merge , rename the index name as you need to reset the index afterwards : #CODE
Having the data formatted like this will allow me to easily work with pivot tables and carry out analysis .
From there I find the pandas package extremely useful for the types of manipulations you require , Setting multi-indices allow for easy reshaping ( through stack ) .
You can further call MultiIndex and stack to arrange Price and Quantity to what you want .
You can call ` apply ` with a lambda that calls the vectorise ` str ` methods to slice your strings : #CODE
Looking up values from one csv-file in another csv-file , using a third csv-file as map
I want to merge ` e_id ` , ` t_id ` , ` gene_name ` and ` value ` into one file , as represented by desired_result.csv .
Merge them all to one file .
After you load all the csvs using ` read_csv ` you can just iteratively ` merge ` them so long as the column names are consistent : #CODE
The above works as by default it will try to merge on matching column names and perform an inner merge so the column values must match on lhs and rhs .
I'll look into the documentation on merge , perhaps I'm doing something wrong .
I'd look at the output of each merge , also you don't necessarily need to merge the entire df to debug this , ie .
` e2t.iloc [: 5 ] .merge ( e_data.iloc [: 5 ])` will merge just the first 5 lines
For some reason , e2t and t_data won't merge in my example , which is identical to the data in the question .
If I'm changing " Gene1 " etc to integers ( or , conversely , the IDs to strings ) , the merge goes fine .
Python Pandas Group by the same value and replace with the mean
pandas - resample - upsampling before downsampling
My objective is to resample this data frame with a fixed time window ( e.g. : 1 second ) using last for regularization when upsampling and the mean for downsampling .
Is this possible at all using pandas resample function ?
You can't mix upsample / downsample in a single ` resample ` operation .
Thanks for your answer , it was not clear to me that you had to make multiple calls to resample .
Sorry are you looking to ` concat ` : ` merged = pd.concat ([ df , df1 ] , axis=1 )` ?
You can call ` transform ` and pass the ` cumsum ` function to add that column to your df : #CODE
You can't shift the index , so you first need to reset it .
Then use a ` loc ` operation together with testing both up and down one shift .
However , I would like only to replace the values with ` NaN ` if these conditions were met by a maximum of 2 consecutive entries ( so I can interpolate later ) .
So now you have groups of contiguous numbers that meet your condition , the next step performs a ` groupby ` to count how many values are in each group ( ` transform ( ' size ')` .
Finally a new bool condition is used to assign missing values to those groups with 2 or less values meeting the condition .
If the values are already aligned , then you could drop down to NumPy and use #CODE
This is quicker since there is no need to align rows based on indices first .
Whilst this answer is correct we should actively discourage using ` apply ` where a vectorised solution exists , of course if the version of pandas is so old that the ` .str ` methods don't exist then this would be a valid answer
For instance you can pass a ` chunksize ` operate on the chunk and write to csv and on each iteration append to the csv
Next step would be to basically opposite of a pivot table the dataframe ` df ` to look like below .
I wrote the function that will compare those 2 strings and return True or False , the problem is I fail to see how to apply / applymap to the consecutive rows : #CODE
For instance , if each ref matches to at most one id or if you can somehow sort both DataFrames in a meaningful way and merge them recursively .
First ` NaN ` are converted to ` 0 ` , then apply function above and it return NaN instead of problematic values .
Post full error messages that you are receiving from your stack trace .
Note that ` copy() ` is required if you wish to later apply changes to that new dataframe ` dfa ` .
Conditionally concat a dataframe in python using pandas
I want to conditionally merge them to a new dataframe
You could ` outer ` merge on ` A ` and ` C ` columns #CODE
then replace ` B ` values where ` C ` values are not null .
You can use ` ix ` to access by index : #CODE
I want to map all strings to integers , so I want to iterate over rows and columns and replace all strings with integer values
For example there is a ' marital status ' column , its field can have ' single , married , divorced ' status , so if a field contains single , I would map it to 0
You should use masking to replace desired value .
The problem with masking is that there are too many unique values and columns and I'd have to hard code all of them , this is why I want to use a loop to replace values .
( It is necessary to convert the list ` item.split() [: 2 ]` back to a string using ` join ` , because lists cannot be used as set elements , which is required here to make them unique . )
` inplace=True ` will prevent you having to do ` df =d f.set_index ( ' timestamp ')` and by default it'll drop the column .
My objective is to merge row by row every list of words or sentences .
Just change the join to : #CODE
Pandas : merge two dataframes ignoring NaN
If you rename the columns of your second dataframe you can then use the concat and groupby like this : #CODE
Going by the idea in the answer for this question - merge 2 dataframes in Pandas : join on some columns , sum up others
While panda's ` groupby ` is able to aggregate data with functions like ` sum ` and ` mean ` , is there a way to aggregate into a list of objects , where the keys of these objects corresponds to the column names these values were aggregated from ?
how can we get Pandas to transform it into this hypothetical output : #CODE
Function apply cannot access to index link , so it is count from length of columns ` colD ` minus 1 .
For starters if you just did this ` df3.loc [ df [ ' field '] == ' a ' , ' field '] = ' 111 '` and so on you wouldn't need to iterate at all just call ` loc ` like above 4 times
Merge two pandas dataframes based on depth range
I would like to merge two dataframe on columns Id and top_depth and bottom_depth .
Then merge to get this : #CODE
In your question you asked for a left join .
These rows are more like a right join .
You can see that the ` apply ` approach is not working : #CODE
your apply code worked for me in ` 0.16.2 `
This means you can reindex the seconds sheet , forward fill , and you're done : #CODE
This allows you to reindex by the dates from your DataFrame : #CODE
Issue : When I iterate over the grouped object , month , row is a tuple , so I converted the tuple into a list and added it to a dataframe using thye append command .
If you can use len ( column [ 0 ]) it's trivial for you to iterate over it yourself and perform .join
TypeError : can only join an iterable
You can use ` Series.str.join() ` and give the delimiter to join by as argument .
If you have 2 commas , each row would shift by 2 columns .
If a sequence is given , a MultiIndex is used .
Pandas Left Merge with xlsx with CSV producing null value columns in output
The column of interest for the merge in this dataframe is ` REFERENCE_ID ` with #CODE
The columns to merge on are ` ID_y ` and ` REFERENCE_ID ` ( in the data dataframe ) .
To merge Lookup and data on the to key columns ` ID_y ` and ` REFERENCE_ID ` , respectively .
Left merge without reindexing
Conditional Merge
My hypothesis is either that there is a data type conflict or because there are repeating numbers in the data dataframe , that the merge is not taking place .
I have also tried to rename the ` ID_y ` columnname to ` REFERENCE_ID ` and perform a merge as follows : #CODE
I have also attempted to set the index as ` ID_y ` and ` REFERENCE_ID ` and then perform the merge on the index : #CODE
Both your join fields are different data types .
Consider converting the join fields to same data types using astype .
How to drop duplicates in Pandas DataFrame by checking for a condition ?
I want to drop duplicate rows by checking for duplicate entries in the column ' ID ' , and retain the row which has a value of 10 in column a .
Just filter the values in ` a ` before drop duplicates with : #CODE
I want to drop duplicate rows by checking for duplicate entries in the column ' ID ' , and retain the row which has a value of 10 in column a .
I then create a list of duplicates and a DataFrame mask for these duplicated ` ID ` rows .
Finally , I use ` .loc ` to select rows which are not in my mask ( i.e. those that are not duplicated ) or rows that are duplicated but have the value 10 in column ` a ` .
Since my real df used strings as column names , I was having an issue with the columns being reordered when using ` append ` .
How can I set the level values of a Series , either by using a dictionary to replace the values , or just with a list of values as long as the series ?
I guess this is probably much more efficient than my dumb method of just replace every value , value for value ...
How to read multiple files and merge them into a single pandas data frame ?
I want to read multiple files located in the same directory and then merge them into a single pandas data frame .
I tried this approach , however I don't know how to apply ` concat ` inside the for loop .
You can use list comprehension to create the list of DataFrames to concat and then call ` pd.concat() ` on that list .
You could do something like this , using ` str.split ` and then ` pivot ` : #CODE
thank you really elegant . had never used pivot
Pandas - how to replace specific values in a Series ?
I am looking for the most efficient way to replace that by numerical values ` [ 1 , 2 , 3 ]` .
Use ` factorize ` to encode a new column : #CODE
I think max will drop the others hence the ValueError .
First , apply ` isinstance ` to determine which elements are floats , then slice your series to get the elements back .
Then just apply ` str ` and you're good .
I can do the proportion using group / apply : #CODE
Here we've used ` transform ` , which means " perform a groupby operation and then broadcast the result back up to the original index " : #CODE
Firstly you should normalize your column names , at the moment they contain spaces ( this explains the KeyError you saw earlier ): #CODE
You will just need to replace the dummy data with your data .
I think this may also be doing an apply , which is also slow .
The easiest way to do this is to use as_index=False , then reindex : #CODE
Pandas reindex dates in Groupby
replace substring in pandas data frame column
What i am trying to do is to replace each value " ABC D; MEAN " with " ABC D; X-BAR " .
Sub string " ABCD " may vary but pattern " ; MEAN " is constant i want to replace .
Looked into different options using " replace " method but don't know how to replace sub string only and not whole string .
use ` str.contains ` to create a boolean index to mask the series and then ` str.replace ` to replace your substring : #CODE
The boolean mask looks like the following : #CODE
For a 40,000 0 row df using ` str.replace ` is faster than using ` apply ` : #CODE
I'm going to create separate columns for the N , S , E , W and C values to merge on to another dataset .
I am trying to apply something like this ....
You can use the boolean condition to generate a mask and pass a list of cols of interest using ` loc ` : #CODE
I can aggregate it no problem ( if there's a better way to do this , by the way , I'd like to know ! ) #CODE
It's not json , you should replace all ' with " , and remove redundant " , "
I know it should work somehow with ` apply ` or ` transform ` , but I have no precise idea unfortunately .
From the Rank ( column ) you can cut and get_dummies : #CODE
Now you can join / whatever these with your original DataFrames .
pandas transform with multiindex erring out
Then I intend to look at each column of each group and impute the nans by the group and column specific statistic e.g median #CODE
How to join two pandas data frames
Now I want to merge these two data frames based on the field ` frid ` .
I know about ` join ` command , but it works differently ( ` df1.join ( df2 )`) .
use ` merge ` and pass the column you want to merge on , by default this performs an ' inner ' merge : #CODE
Here's one way , with ` apply ` and ` first_valid_index ` : #CODE
To get these efficiently you could drop to numpy : #CODE
actually , ideally i would need to check before deciding which row to drop .
how can i drop multiple rows from my data frame ?
The documentation contains more information on expanding window moment functions .
not really you can't sort by something that doesn't exist fundamentally unless you created a dummy series / df and used that index to reindex your df
Operating on multiindex dataframe
Then merge it back to the original table : #CODE
You can replace the last line with :
df2 [ " diff "] = df2 [ " followers "] - df2 [ " followers_a "]
You can use ` map ` to do this .
As well as a dictionary , ` map ` can also accept another Series as an argument , or a function .
So somehow I need to create an object with which I can compare that xml document and then reference the variable names in the ' freq ' list .
You can create a dictionary of items you'd like to replace : #CODE
Then , apply ` get ` to the column , returning the original value if it is not in the dictionary : #CODE
Use ` map ` for this , it's cyython-ised : ` df [ col ] .map ( d )`
It's the opposite , I would like to drop items that are in the dataframe less than n times .
You can use ` value_counts ` to get the item count and then construct a boolean mask from this and reference the index and test membership using ` isin ` : #CODE
Seems massively over kill - what about ` df.groupby ( ' a ') .filter ( lambda L : len ( L ) > 2 )` ?
( Since I'm new to Stack Overflow I can't post an image of my graph unfortunately . ) #CODE
I have the jaccard similarity function defined as ` jaccard() ` I only want to know how to apply it to ` df ` so that I can have this type of representation matrix by the end .
So you need apply ` jaccard ` function to ` df ` ?
use data from two or more columns when using .map to apply a function
When I write df [ " diff "] = df [ " date1 "] - df [ " date2 "] I get a value like 75 days which I can't sort .
How can I write it such that I map to function calc_diff ( date1 , date2 ) where #CODE
I df [ " diff "] = df [ " date1 "] , df [ " date2 "] .map ( calc_diff )
Instead of needing a full python installation along with pandas and all relevant libraries installed in each machine it would be nice to be able to do something like A.gen_sql ( ... ) and generate an sql ( text ) output of the insert / update statements that would update each server .
Try my solution - only set column ` DATE ` in ` df ` to ` index ` and then ` reindex ` by this .
For your task , allocate array of subplots ( ` plt.subplots ( nrows= ?, ncols= ? )`) , iterate over ` df ` columns and call ` matplotlib `' s ` hist ` for each pair ` subplot ` + ` column ` .
In order to do that , you need first to " melt " your data , in ` R ` parlour , i.e. make it " long " .
I would make a dummy column here rather than use agg : #CODE
I tried converting ` date ` to a ` datetime ` object but still received the same error message , even though ` type ( dt )` shows that its ` type ' datetime.datetime ' ` #CODE
All Pandas methods that rely on time-based indexing ( such as interpolation , and ` asof `) expects the index to be a ` DatetimeIndex ` .
Then , you can group by ` Date ` and ` Symbol ` and aggregate by summing orders .
Finally , use ` unstack ` to convert the ` Series ` to tabular format .
That looks like a good way to convert my indexed_orders to align with the respective columns in my trades data frame .
If you want those empty rows back , you can reindex the dataframe with your desired date range .
How do I reindex a data frame like Priozhenko's df for my desired date range ?
@USER When I try to reindex to my desired date range (= dates from my above code ) , I get columns filled with NaN under my Symbols .
Basically , you need to convert the dates in the Date column to Pandas datetime datatype before you reindex .
Pandas merge returning empty dataframe
The merge works after this .
So to get to ` y ` I take ` x ` and apply some function .
... and then cast the result of " apply " to a list .
How can Pandas read_csv() be forced not to stack rows with identical datatypes into an array ?
merge two dataframes pandas
So if ` data ` has ` key1 ` in it 5 times , and ` extra ` has ` key1 ` in it 2 times , then you will have 10 entries for ` key1 ` when you merge the two dataframes on the key column .
And I am using it to merge the 2 dfs .
You have to state what * fixed * means to you , have you tried passing ` how= ' left '` to ` merge `
The left join shouldn't make a difference .
I think what you actually want to do may be to join on the index , in which case you will have the same number of output rows as input .
I have to caution you that I'm not sure this is what you want , ` combine_first ` uses the other df to fill in the missing column values , it doesn't merge on the key as such .
The key difference here is that ` values ` attribute returns a np array , this can be useful when you want to assign the values and ignore the index which the former will generate and will mean that the lhs will try to align on the index , in your case it makes no difference as you're adding the result to ` df [ ' start_date ']` which will have an index anyway
In your case there is no difference , generally calling ` values ` attribute returns a np array of your data , typical cases where this is necessary is when you want the array as plain values without the index / column names as pandas will attempt to align on index and column values .
Is there a way to append to an array without this happening ?
Generally np and pandas perform well when the array is not growing , by repeatedly appending to it you will periodically force it to allocate a new memory block and copy the values which may explain why it borks when you append just a single element . does this post help you : #URL
Reverting from multiindex to single index dataframe in pandas
Why not drop non integer / float values ?
If ` NaN ` cannot be in column ` columnname3 ` , you can drop rows by this column : #CODE
In this data , you can drop all rows with ` NaN ` values : #CODE
How do I take the highest probability and append it to the data frame just like I did with the class ?
I have been playing around with apply , but not sure how to use it
Nice and simple , no need for apply .
The ` apply ` function returns a new DataFrame instead of modifying the input in-place .
You could write your logic as a function and then apply the function to your dataframe using applymap() .
I am trying to apply a function which returns the latest or maximum date for a stock ( on which I have collected prices for multiple days ) .
Where , ` values ` contain numpy arrays , ` apply ` , ` lambda x : pd.Series ( x )` on ` df [ ' values ']` #CODE
And , you could use join to extend the columns .
You need something like ` df [ df [ ' b '] ! = df [ ' b '] .shift() ]` - I'm sure I've seen very similar questions on Stack Overflow , so I'll have a look and see if there's a dupe .
Eventually , I think having the distance matrix as a pandas DataFrame may be convenient , since I may apply some ranking and ordering operations per row ( e.g. find the top N closest objects to object ` first `) .
I want to aggregate them over their differing ` prev ` attribute and count the ` n ` attributes .
i don't think this will be too complicated . look into ` index ` , ` multindex ` and ` aggregate ` .
Then merge all 3 tables so that you will have all 3 columns in the main table : #CODE
Use ` apply ` on column to do ` df [ ' B '] .apply ( lambda x : sum ( map ( int , x.split ( ' , '))))` #CODE
line 258 , in transform
Querying Pandas DataFrame with column name that contains a space or using the drop method with a column name that contains a space
I am looking to use ` pandas ` to drop rows based on the column name ( contains a space ) and the cell value .
I have tried various ways to achieve this ( drop and query methods ) but it seems I'm failing due to the space in the name .
If I understood correctly your issue , maybe you can just apply a filter like : #CODE
That will work , if I dont get an answer using the drop method will happily accept this as a work around .
@USER Why do you need a method using ` drop ` ?
` DataFrame.drop() ` takes the ` index ` of the rows to drop , not the condition .
Can you explain why the answer by Fabio is better than the drop method ?
As I understand it the drop method was developed for this exact purpose ?
The only advantage I see for ` drop ` method is that it has an inplace argument which can be used to change the dataframe inplace .
If you must keep the original values in " results " you can use another list to " shift " the index ( the value 1 in results [ 1 ] is for your first example of [ 5 , 6 ]): #CODE
was trying to do a " for i in range ( len ( results ))" before the " for item in results [ i ]" that you did but not working for me ...
I want legend to be outside of plot but visible and not partly cut off .
is there a way to center align it ?
You can use the apply method :
Select the columns in the list using ` loc ` and then use ` .prod() ` across the rows ( by specifying ` axis=1 `) .
The problem is , when I transpose the DataFrame , the header of the transposed DataFrame becomes the Index numerical values and not the values in the " id " column .
Original data that I wanted to transpose ( but keep the 0 , 1 , 2 ,... Index intact and change " id " to " id2 " in final transposed DataFrame ) .
DataFrame after I transpose , notice the headers are the Index values and NOT the " id " values ( which is what I was expecting and needed )
If I understand your example , what seems to happen to you is that you ` transpose ` takes your actual index ( the 0 ...
Pandas agg multiple summaries on same column
There are two other options that come to mind to get a similar output : you could select the column using brackets , and then pass a list of the reduction operations you want to apply : #CODE
I have a dataframe in pandas with the columns ` Year ` ( int ) , ` Loc ` ( ordered pair of ints ) , and ` Rain ` ( boolean ) .
Use ` agg ` function on groups #CODE
True , I was giving hint , that you can use numpy funcs to agg .
I guess it's difficult to know until you try which ufuncs already map from string to their np equivalents
I believe you would need to transform the dictionary a bit to be able to convert it to the DataFrame as you have given .
Pandas pivot or groupby for dynamically generated columns
@USER , I read docs , but didn't find an appropriate command , I try merge ([ df1 , df2 ] , on= ' id ') , but it was error :
@USER , it's my mistake , I put df1 and df2 in brackets , but the correct variant is : merge ( df1 , df2 , on= ' id ') .
What I tried until now : I tried a ` groupby() ` and then ` apply() ` or ` transform() ` however I dont know how to merge the information of two columns ( playerA and playerB ) .
how to concat sets when using groupby in pandas dataframe ?
How can I skip ( don't apply ) the filters that are None ?
Filtering in pandas - how to apply a custom method ( lambda ) ?
How can I apply ` df [ ' column2 '] .apply ( lambda x : ' str2 ' in x.split ( ' , '))` to #CODE
To apply this , simply use this to filter the DataFrame .
For example : ` ..... apply ( lambda x : [ ' str2 ' , ' str4 '] in x.split ( ' , '))]` ?
You can use ` any() ` or set intersection .
Another method - ` len ( { ' str2 ' , ' str4 ' } & set ( x.split ( , ')) > 0 ` .
sorry , ` len ( { ' str2 ' , ' str4 ' } & set ( x.split ( , ')) > 0 ` doesn't work for me because `' str2 ' , ' str4 '` is in a variable ` filter1 = [ ' str2 ' , ' str4 ']` .
And ` len ( {filter1 } & ...
How to combine ( merge ) an array of identical DataFrames into a single one ?
How do I merge or combine an array of ` DataFrames ` in pandas ?
did you try ' result = concat ( dfs )' at the end ?
Pandas DataFrame already has an ` append ` method to merge two DataFrames
NameError : name ' concat ' is not defined
My Questions and / or things I've read about on SO but haven't / am unclear on how to apply :
Why are you calling it via ` apply ` ?
@USER When I don't call nextday via apply , the calculations don't get applied for days 2-last day .
I tried it as well using for index , row in d.iterrows() , but that's about the same speed as the apply method .
Is there another / different way I can apply the calculations without using apply ?
Your ` nextday ` function doesn't seem to use ` row ` at all , which it doesn't make sense to use via ` apply ` ( which is for row-by-row function application ) .
e.g. when you have ` d [ ' ET_WL '] -d [ ' infilP ']` that subtracts on all the rows in ` d ` , so there isn't any reason to call it via an apply .
Not sure depending on your OS , but on Windows you should be able to just download then replace the user name in ' folder ' with your username , then run .
For each " hit " , where a row in ColumnA in df2 has the same value as a row in Column1 in df1 , I want to append a column to df1 with the vaule ColumnB of df2 has for the row where the " hit " was found , so that my result looks like this : #CODE
I recommend you to use DataFrame API which allows to operate with DF in terms of join , merge , groupby , etc .
4 ) in the end I execute left join for df1 and dfs obtaining needed result .
You could ` apply ` ` tuple ` on ` axis=1 ` #CODE
IIUC so long as the dtype are datetime64 already then you can just use ` diff ` which will create a timedelta and then call the attribute ` dt.seconds ` : #CODE
Note though that if you have multiple columns then it will mask all those as ` NaN `
@USER you could either set to ` NaN ` as you suggested but you then still need to ` concat ` anyway
The csv data looks fine in the csv file but when it's writing to html the output is not as expected ( data missing from tables , as though it's been cut short ) and it contains various ` \r\n ` entries .
The suggestions I have read is to replace the \r\n using the ` replace ` method directly on the dataframe .
To fill the missing data lines with pandas reindex function
I am trying to fill the missing lines in my time series data using pandas reindex function .
Pandas : rename a index label in multiindex dataframe if NaN
How can I replace the inner index label of the df below ?
nope , doesn't replace the NaN .
Looks like I have to replace the entire index which is very annoying .
You can then construct and index of the previous round and opposing player for each row and map it to the corresponding action : #CODE
Apply the following function over the dataframe to generate a new column : #CODE
Are you sure you are replacing df with the result of calling ` apply ` ?
Apply doesn't change the dataframe inplace , rather returns a copy of it , so you need to store it or else the results are vacuous ...
I have tried setting a new df to the results of apply via ` df2 =d f.apply ( sep_yearmonths , axis=1 )` then ` df2 =d f2.groupby ( ' month ') .sum() `
( If it is possible to merge and reopen the question that would be great ... )
You can add a secondary axis by specifying ' secondary_y=True ' when you apply your plot function directly on your dataframe .
If you want to append each y in y_test to results , you'll need to expand your list comprehension out further to something like this : #CODE
Can you post the full stack trace ?
Pandas ' apply method
You should consider a func that is passed to apply that simply makes some calculations and returns either a scalar or array like structure to avoid ambiguous behaviour , using apply to modify a df in place is not going to work in practice as especially if you iterating row-wise yet wanting to mutate the df row-wise
Generally speaking the answer is that ` apply ` is NOT in place but you made this overly complicated .
Generally you would use ` iteritems ` OR ` apply ` , not both .
In this case , you have no need to use ` iteritems ` in addition to ` apply ` .
Perhaps I should use two ` iteritems() ` instead of ` apply ` .
You're almost certainly better off with ` apply ` than any of the ` iter ` -options , and there are often better options than apply .
@USER : Just to follow up , maybe I can iterate over columns and use ` map ` on each element .
negate the boolean condition using ` ~ ` to invert the mask : #CODE
However it's not coming up right as len ( df ) is ` 1350 ` rather than ` 24 ` since the dataframe as a whole is from 1 day worth of data .
Python pandas replace string
This is a typical use case of ` loc ` .
If the type of elements of ` C ` column is ` list ` , then I believe one method to do this would be to use ` set ` intersection between your list and the elements of ` C ` column using ` Series.apply ` method .
How to apply tz_convert with different timezones to different rows in pandas dataframe
You can have a vectorized approach ( the minus operator is here to negate the boolean mask ): #CODE
@USER - The code above requires true division in ` dt = max_time / N ` so that's why it doesn't work on Python 2.7 .
Pandas merge not working properly .
I am trying to merge two large DataFrames .
Indeed they do line up before the merge over at least part of the key : #CODE
It's my impression that a left merge should not change the number of entries ?
You might need to add another ` translate ` to parse out any characters you don't wont .
Dataframe has method isnull : #CODE
the order of the ` var1 ` in the ` activity_seq ` column follows the ` dt ` column ( i.e. in chronological order ) .
You can ` groupby ` on ' userid ' and then on ' var1 ' col call ` apply ` and pass ` list ` to create a sequence , you can rename / reset if required .
Call ` diff ` and pass ` -1 ` to shift upwards a single period : #CODE
Also you should almost never need to iterate row-wise so avoid using ` for ` loops , ` apply ` , ` iterrows ` etc ...
The lines below apply to data where ` INDATUMA ` and ` UTDATUMA ` are of the format 20071231 , e.g. Date parsing seems to work for ` indate ` and ` outdate ` , those values make sense .
This works because we can use ` diff ` to compare the jumps in the index ( after converting to a Series ) , and then if we take the cumulative some of the bools where the difference is greater than 1 , we get a growing index for each group : #CODE
You now need to convert the prices to a DataFrame in order to merge it to your original dataframe : #CODE
I want to drop duplicate rows with respect to column ' a ' with the general rule as take_last = true unless some condition say , c = ' Blue ' , in which case I want to make the argument take_last = false .
I want to change the dataframe to numpy.ndarray with datatype float32 , so I want to drop those column which dtype is object or other type which is not number .
I'm having a heck of a time trying to figure out how to pivot or groupby the values without having to do this individually for each column .
Not sure if I need to convert the boolean to int , or if I am missing a transform or a map step .
IIUC you can just call ` apply ` and pass ` value_counts ` : #CODE
As @USER has pointed out if you have columns with all ` True ` / ` False ` then it will insert ` NaN ` for the non-existing values in which case you can call ` fillna ( 0 )` like so : #CODE
@USER no diff here , I tend to use ` .ix ` when slicing cols personally
Normally after I stack a ` DataFrame ` , I convert it back into a ` DataFrame ` .
I stack and convert it back to a ` DataFrame ` like so : #CODE
But looking at the docs it doesn't look like ` stack ` takes any such arguments .
how can i convert such dictionary to one with levels ; i want to create a dictionary for each id and insert the data inside that dictionary where i can reach data for each unique id individually
maybe make your ID your dict key then map it to a list of tuples ?
if you do want to have the attribute names , then map each key to a list of dictionaries with Code , q , and color keys .
Python pandas map dict keys to values
I have a csv for input , whose row values I'd like to join into a new field .
I tried to map values to keys with a dict comprehension , but the assignment of a key like ' FIRST_NAME ' could end up mapping to values from an arbitrary field like test_df [ ' CITY '] .
This might be one possibility to easily map it to a correct row .
You can use ` melt ` to reshape from wide to long , like this : #CODE
I would drop ` variable ` column
actually ill need that for a join later on
How to merge two columns as per third column ( ID ) in pandas ?
How to merge these two dataframes in pandas so that the resultant dataframe is like : #CODE
` concat ` with ` axis=1 ` concatenates by the index , leaving none where ther is no overlap .
and I want to melt ` SP1_Lower ` and ` SP2_Lower ` based on ` POLY_KEY_I ` , but I want to retain the associated ` SP1_Percent ` and ` SP2_Percent ` .
I can melt it with this : #CODE
pandas DataFrame concat / update ( " upsert ") ?
I am looking for an elegant way to append all the rows from one DataFrame to another DataFrame ( both DataFrames having the same index and column structure ) , but in cases where the same index value appears in both DataFrames , use the row from the second data frame .
This is analogous to what I think is called " upsert " in some SQL systems --- a combination of update and insert , in the sense that each row from df2 is either ( a ) used to update an existing row in df1 if the row key already exists in df1 , or ( b ) inserted into df1 at the end if the row key does not already exist .
@USER well YMMV . but this actually is quite efficient , involves one set op ( on the index ) and 1 take ( the indexing ) , and a single copy ( the concat ) .
I'd like to join ( merge ) them in pandas as follows :
Try to merge on ID = ID
Merge on Last , First , DOB = Last , First , DOB
merge on ID #CODE
merge remaining entries on dob , first and last #CODE
concat both types of merges :
I'm assuming if I want to continue to join on other columns if there were some that did not match in Last , First , DOB , I would continue the pattern above ( i.e. df5 = pd.merge ( df1.drop ( df4.index ) , df3.drop ( df4.index ) , on = [ next column ( s ) to use for joining ]) ?
The merge method accept the following parameters :
inner : keep the intersection ( that is the default behavior )
So if you want the intersection : #CODE
Regarding your point " a " I would strip the first letter out of the " Last " column and then concatenate it with the two others .
then I would use the merge method : #CODE
How to filter through pandas pivot table .
I have a pivot table created from pandas ( DataFrame object ) .
To clarify this is how the pivot tables looks like .
The pivot table is called pt .
The very last ` : ` inside the ` loc ` function signifies to select all columns in the dataframe ( ` v1 ` and ` v2 `) .
I'm trying to left join multiple pandas dataframes on a single ` Id ` column , but when I attempt the merge I get warning :
Either way I can't figure out how to " unstack " my dataframe column headers .
Python Pandas - Using list comprehension to concat data frames
It is worth noting however , that concat ( and therefore append ) makes a
If you have a loop that can't be put into a list comprehension ( like a while loop ) , you can initialize an empty list at the top , then append to it during the while loop .
Also , if your data comes naturally as a list of dicts or something like that , you may not need to create all the temporary dataframes - just append all of your data into one giant list of dicts , and then convert that to a dataframe in one call at the very end .
You can use a combination of ` apply ` and this [ answer ] ( #URL ) to achieve this but why is this an issue ?
I want to apply following rules :
Now I want to create a pivot table on this data using #CODE
I am able to successfully create the pivot table with a little change .
Any suggestion on how could i create pivot table having [ ' msisdn ' , ' prob '] in rows , [ ' desc '] in columns and an indicator variable for presence of desc in values filed would be much appreciated .
What I want to do now is to compare Column1 with ColumnA and append the rows of df2 to the rows of df1 that have the same value in Column1 as df2 has in Column A , so that the result looks like this : #CODE
I was thinking of using pandas .groupby() function and set the columns 1 and A as keys , compare them and then merge the grouped objects where the keys are identical , but I could not find an efficient way to compare the keys of grouped objects of 2 dataframes .
You can specify which columns to ` merge ` for the lhs and rhs dfs : #CODE
Use ` shift ` to perform the comparison and filter the rows out : #CODE
You can then use loc to slice your DataFrame : #CODE
You could call ` apply ` and pass a lambda and call ` squeeze ` to flatten the Series into a 1-D array : #CODE
I'm not sure if this is quicker though , here we're applying the mask column-wise by calling ` apply ` on the df which is why transposing is unnecessary
The groupby is fast but the fill function apply on the dataframe group by date is really too slow .
In that case replace & with |
Then I'd apply the ` between_time ` pandas function to filter the dataframe by start and end date given by the ` bounds ` dataframe : #CODE
pandas stack with row numbers
Using column selection and stack is very close : #CODE
It seems like the ' label= ' part of the plot ( ... ) call ought to take a list or series or something and apply the values according , but I can't figure out how .
Can you try just ` df.to_sql ( name= ' Table ' , con=engine , if_exists= ' append ' , index=False )` ?
@USER Yes , you are right , I had done that anyway but for some reason thought it would insert them all as TEXT .
Bug in pandas resample / TimeGrouper
Now let's create 1-minute bars using pandas ` resample ` method : #CODE
I don't believe all pandas sorts are necessarily stable , so I'd guess that resample is doing some non-stable sorts ?
That said , this may be better written as a merge .
Thought about doing a merge but this seemed liked fewer steps .
Should have been an outer merge to return all rows , my bad
Edit : Something I tried that doesn't work , prepending a category using concat : #CODE
Depending on your taste , this may be a nicer way to do it still using concat : #CODE
I would just use list comprehension with a dict or pandas to map the states into a region factor , and then use patsy with region as a regular factor variable .
I checked the methods available for test ( test . ) but I did not see anything where we can select columns and also aggregate them there ( I guess we can use ` agg ` for ` sum `) .
If it was only 3 time mores , then I would transform my data into array and use ` np.reshape() ` and then re-assign a time index but this won't work in this particular case .
You want to resample , with interpolation for non-integer time points .
The method that I am showing here reads each row and calculates the distance and append it in an empty list which is the new column " Distance " and eventually creates output.csv .
Python pandas conditional replace string based on column values
I merge a temporary dataframe ( ` DF2.set_index ([ ' ColX ' , ' ColY ']) [[ ' ColZ ']]`) into DF , which adds all the values from ColZ where its index ( ` ColX ` and ` ColY `) match the values from ` COL1 ` and ` year ` in ` DF ` .
One more thing ( hopefully ): What if I wanted to add the condition : if less than all conditions ( 2 ) are met ( found ) , replace the current value with ' n / a ' ?
In a csv file which I read using pandas , there's a column of type bool but in the string format which is ' F ' or ' T ' .
I probably should use ` apply ` , but how exactly ?
One method to do that would be to use ` Series.map ` , to map `' F '` to ` False ` and `' T '` to ` True ` .
You could use floor division ` // ` to drop the last two digits and preserve the integer type : #CODE
Performance is drastically improved by increasing the ` arraysize ` attribute of the Cursor - allowing me to get decent performance out of ` fetchall() ` . pandas ` read_sql() ` takes a ` Connection ` object as input and the cursor is created within the function , therefore it's not obvious to me how I can apply that same setting and still take advantage of the ` read_sql() ` function .
time series : resample from hour to quarter .
Does it work if you pass param to ` resample ` ` closed= ' right '` ?
It's similar to a merge , then drop_duplicates but with the twist that it should after that also delete all items in the first table .
Still , you can define any number of columns you want in the ` on ` argument to ` merge ` ( provided the columns are in both data sets , of course ) .
How to merge column data of the same value and sum its specific data
How can I merge column data of the same value and sum its specific data ( in this case based of the DATE column )
Surely , when I ' append ' this to a new list and convert to a dataframe , there still would only be three columns in the dataframe ?
You need to concat the list ` df = pd.concat ( result , ignore_index=True )`
Sorry try ` for i in range ( 1 , 31 ): result.append ( pd.DataFrame (( find_peaks ( df1 [ ' R '] , df1 [ ' I {} ' .format ( i )])))` and then the concat should work
I suggest making a list of dfs and then ` concat ` them : #CODE
Merge multiple rows with same value into one row in pandas
So basically I want to merge all rows with the same values in all columns into one row .
where start and end are ` df.index [ 0 ]` and ` df.index [ -1 ]` respectively , the ` freq ` param accepts a frequency value which here is 90 days which suits your requirements .
Given that your initial dates have weekends and holidays and that you want your intervals to be 90 actual days , you can use ` asof ` to get the most recent value as of the 90th day interval : #CODE
Python Pandas : Aggregate my indexes into a range of ages
I want to aggregate some of the ages together though .
Aggregate using ` sum ` and drop the empty rows ( ` dropna `) to get what you want .
Although I'm able to create a new DataFrame column for Implied Volatility using the Pandas DataFrame apply method , I'm unable to create a second column for Vega .
I suppose I could create two dataframes first then join them , but I was wondering if there is a better way .
pandas : MultiIndex Slicing - Mixing slices and lists
However , the map ( str ) removes trailing zeros from the values in the series .
Obviously as you can see I'm actually working with a data frame ( but as map is a data series func I thought this would simplify the question ) , if there's a way of achieving this with applymap instead I'd be happy to know .
Awesome thanks , that's worked like a charm ( with map ( to_4d ... ) on both series ) .
Is there any way to use map with a format string something like this :
The two tricks I'm using here are 1 ) using %i in the format string to signify that it's an integer ( %f means a float , but it renders w / o trailing zeros ) and 2 ) the apply function on df .
Using argmin instead of ` df3.apply ( coalesce , ... )` is significantly quicker if the DataFrame has a lot of rows : #CODE
You could use ` fillna ( 0 )` to replace the NaNs with zeros : #CODE
I am trying to avoid having convert the df into a list then append the cleaned results into new list , then convert it back to a df .
square brackets / list comprehension around the join give you another boost fwiw :)
Note : you can also do this with map ( which is a little cleaner ): #CODE
But this seems unnecessary when you can reindex after creating as per @USER post .
or if s1 and s2 are columns in a pandas DataFrame df , then we can use similar logic and the apply function : #CODE
manipulating multiindex columns in Pandas
I have a simple multiindex dataframe in pandas .
To avoid the warning / error use loc and do these in one assignment : #CODE
But it looks like the proposed solution with " merge " does not really fit my needs , as I have many columns ?
I finally was able to make it work using : New.loc [ index , " Time '] = Hist.loc [ Hist [ ' url '] == row [ ' url '] , ' Time '] .values [ 0 ]
What I do is I read the table in chunks , manipulate each chunk , append the chunk to a new table .
pandas rolling computation to find set of small numbers
I want to determine the start of the rolling window ( of a given length ) that contains the largest set that contains the smallest valued numbers within the given window size .
I've tried to read the ` pandas ` document to see if there is a rolling computation that can help , but no luck !
and in order to obtain start of the window you may either shift the end of window or alternatively shift the series : #CODE
How could I replace this disgusting code ?
because their is quite a bit of parsing involved this is an AST transform ; much more than ' just pass it thru '
and of course you could replace ` col+row ` with a call to an arbitrary function ` f ` : #CODE
Unstack or Pivot Only Some Columns
I only want to unstack " COL4 " and " Year " by way of COL1 while keeping COL2 and COL3 in tact .
You can set multiindex by first 3 columns and use unstack with ` level=0 ` .
This produces the end result that I want , however , there is a lot of this data , in many iterations , and since the lists can be of N length ( meaning , date / device type will always have ` len ` 2 , but date will always have ` len ` 1 , whereas country , device and date will always have ` len ` 3 ) , I'm looking for a more efficient / programmatic way of doing this .
I thought using JSON normalize would help but I get the error ` TypeError : string indices must be integers , not str ` whenever I try this : #CODE
Geo Pandas Data Frame / Matrix - filter / drop NaN / False values
Then I stack the dataframe , give the index levels the desired names , and select only the rows where we have ' True ' values : #CODE
I want to replace NaNs is a dataframe with the row average .
An alternative is to fillna the transpose and then transpose , which may be more efficient ...
I'm trying to get the dict key to align to columns , and the value to be a row value , then append each record to the dataframe .
But if you planning to use such construction to create DF with many rows you should join all { #URL in 1 dict for each record , and append them to list : #CODE
python merge panda dataframes keep dates
I'd like to merge two dataframes together but add in a column based on some logic .
What I want to do is merge both of these dataframes into a single data frame that looks like this : #CODE
I can't find a way to merge them and keep the dates .
The only way I can think of is to merge them with renamed date columns , then loop over the entire dataframe sorting the dates appropriately but this seems inefficient .
Then you can use ` groupby / agg ` to obtain the desired result : #CODE
replace df with the name of your DataFrame .
resample ( ' M ') change the frequency of the series to monthly .
As the question asks I have a dictionary of pandas dataframes that I want to save so that I don't have to resample the data next time I start the ipython notebook .
When I apply #CODE
i=2 ( drop first two in each sequence ) #CODE
Merge two data frames and select specific columns
I want to merge two data frames by a column Number .
By the way the code works without " append " within for loop .
How to merge two tables in pandas , KeyError : ' timestamp , city '
I cant join 2 data frames : #CODE
Pandas : keeping dates in order when using groupby or pivot table
and I wish to create a new DataFrame from this which acts as a pivot table and merges the date fields and sums weighted returns for that particular date to get something like this : #CODE
So I tried to use a pivot table function but I get very confused reading the documentation regarding it's use .
Don't think you need the pivot at all here .
( Your pivot by the way specified the name ` Weighted Return ` with spaces while your df had no spaces , explaining the key error )
The error with your pivot table is because you entered the column name as " Weighted Return " ( note the space ) instead of " WeightedReturn " .
I have tried your interesting solution but I got ITypeError for df [ ' for_group_by '] = df.PAS_DATE + df [ ' shift '] : ufunc add cannot use operands with types dtype ( ' O ') and dtype ( ' < m 8[ ns ]')
Simply shift the PAS_DATEs by this timedelta .
The iteration is now being done in the pandas package , in order to map the data into an indexed dataframe .
I keep getting the following error when I apply it to real data ( in which case , data frames are of different sizes ):
If ' data ' is a pd.DataFrame and you iterate over range ( 0 , len ( data )) and then add data to your list ' all_info ' , you simply add the whole DataFrame ' data ' i times to the list .
I want to drop values that appear only once in a column .
Do you want to drop the entire row or replace the value with NaN ?
@USER I want to drop the entire row , but replace the value with NaN also help me .
You can easily get this by using ` groupby ` and ` transform ` : #CODE
Is there a way to ` apply ` a function to one column of a dataframe while leaving the other columns fixed ?
If ` apply ` is not done ` inplace ` you still have to make an assignment , so what is the difference ?
@USER my point is that if I want to compose functions ` f1 ` , ` f2 ` and ` f3 ` , the syntax would be ` d.assign ( A=f3 ( d.assign ( A=f2 ( d.assign ( A=f1 ))))` ; agree that this is better than in-place , but I would argue that this is less readable than a " forward pipe " style syntax using something like ` apply `
I want to apply a 5 to any ids that have a 1 anywhere in the number column and a zero to those that don't .
Use ` transform ` to add a column to your df as a result of a ` groupby ` on ' ID ' : #CODE
You can use ` DataFrame.drop_duplicates ` before grouping , to drop duplicates based on ` Department ` and ` ID ` .
I generate a multiindex dataframe like this example #CODE
IIUC then you can use ` loc ` and pass a tuple consisting of a ` slice ` and column label to access the col of interest at that level : #CODE
Have gone over all of the pandas docs on merge , join , and concatenate , read through all of the similar questions on Stackoverflow and Scriptscoop , and have watched hours of pandas tutorials on YouTube .
This post would be too long if I went through all the various combinations of join , merge , and concat functions I've tried , and why they didn't work .
Looks like you want an ' outer ' merge ?
You could ` groupby ` on ' Area ' and ` apply ` ` list ` : #CODE
If you want to split the values out you can call ` apply ` and pass ` pd.Series ` ctor : #CODE
Now a slight variation so that the data can be plotted in multiple bar plot : how can I transform it so that the areas are columns and each value is in a row ?
ax.set_xticks ( range ( len ( poverty.index )))
` mrna.iloc [ ' ERBB2 ']` but that only takes a integer , and doesn't map to string
Pass a boolean condition to generate a boolean mask , this mask is used against the index and will return just the rows where the condition is met : #CODE
Thanks , I realized soon after that the transpose method didn't also transpose the indices .
and merge : #CODE
Merge two python pandas data frames of different length but keep all rows in output data frame
I know that the merge is doable through ` df1.merge ( df2 , left_on= ' Column1 ' , right_on= ' ColumnA ')` , but this command drops all rows that are not the same in Column1 and ColumnA in both files .
What you are looking for is a left join .
The default option is an inner join .
I think he is actually looking for ` left ` join :)
I have a multiIndex dataframe that I am merging with another multiIndex dataframe and it merged successfully but now it only has one Index ?
which gives me an outer join .
I tried reindex , which was obviously not for this purpose .
I tried using append , but did not get any output .
Especially as you can use [ ` loc / .ix `] ( #URL ) to set with enlargement
I tested this by saving the original dataFrame and then just loading it , running the truncate function , and then trying drop_duplicates on the result and I get the same error .
` stack ` pivots a DataFrame into a Series with a hierarchical index which then allows us to use the ` value_counts ` function .
Is it possible to append Series to rows of DataFrame without making a list first ?
I was trying to make each row a ` Series ` and append it to the ` DataFrame ` .
Is it possible to append Series to rows of DataFrame without making a list first ?
I think what you're looking for is a ` groupby ` followed by an ` apply ` which does the correct logic for each user .
I know word_tokenize can for it for a string , but how to apply it onto the entire dataframe ?
You can use apply method of DataFrame API : #CODE
For finding the length of each text try to use apply and lambda function again : #CODE
nonzero ( self )
716 bool = nonzero
Merge multiple column values into one column in python pandas
You can call ` apply ` pass ` axis=1 ` to ` apply ` row-wise , then convert the dtype to ` str ` and ` join ` : #CODE
actually it seems to append all values from column2 to X five times , not four as I wrote before ..
Index with float can ` reindex ` with problem , inconsistency is probably because of floating point accuracies .
Pandas best way to subset a dataframe inplace , using a mask
The mask is ` df [ my_column ] 50 `
I would typically just use ` df = df [ mask ]` , but want to avoid making a copy every time , particularly because it gets error prone when used in functions ( as it only gets altered in the function scope ) .
` df.drop ( df.loc [ mask ] .index , inplace = True )`
Essentially I want to give it a condition to drop , and drop inplace .
The df.loc [ mask ] .index will give the me indexes to drop , correct ?
Sorry what's wrong with ` df = df [ mask ]` ?
Well ` mask ` itself is a boolean index
` df.drop ( df.loc [ mask ] .index , inplace = True )` seems to work , but I expect there might be a better solution ( as mine will probably fail on multi-level indexes etc )
It would still be a copy and replace , but I'm curious as to why you avoided iloc
@USER Column2 was created like this : ` ResData [ ' AR_Genes '] = ResData [ ResData.columns [ 1:10 ]] .apply ( lambda x : ' , ' .join ( x.dropna() .astype ( str ) .astype ( str )) , axis=1 )` and then merged with the already existing Column1 using a left merge .
A method by which you can do this would be to apply a function on the grouped DataFrame .
But I could resolve it through putting the output of the condition in a new df and apply the code on that , that worked .
I have two Dataframes with different column names I want to join on .
I have not found an example of a join with two different names or using a column index .
In lakeDF I have TBL_ID or Index 0 I want to join on .
I've double checked and found that they both have the same unique keys to join on .
yes , so you need to decide how to merge here , you can pass ` how= ' left '` as a param
You can merge on different columns for lhs and rhs , additionally you may need to specify the type of merge in your case as you had duplicate values on the lhs then you can elect to perform a ' left ' ` merge ` : #CODE
Setting pandas boxplot y-limit dynamically
How can I append calculated values in a for loop to pandas.DataFrame Sequentially ?.
I'm lost with one exercise , where I need to aggregate different features of dataframe for every session , given between some datetime points , so that the final data would have the following format : #CODE
When I try to drop a single row in a pandas dataframe with datetimeindex , it shifts the index
When i try and drop a single row by its index value , the number of rows become N-1 correctly , but the times in the index shift .
I just noticed the shift is by the UTC / Timezone offset ... but the timezone shift on the index appears to have not been dropped .
or just drop the tz and localize where its at ( which is what I think you want ) #CODE
The loc function takes an int not an Int64Index .
If you only want to remove certain rows within matching groups , you can write a function and then use ` apply ` : #CODE
( If you really like your current way of doing things , you should know that ` loc ` will accept an index , not just an integer , so you could also do ` hdf.loc [ group.index , ' remove_row '] = 1 `) .
Anyone know how to apply a method to change it ?
what does apply ( floor ) do here ?
I have list of indices corresponding to rows that I want to drop from my ` DataFrame ` .
How can I drop rows whose ` .ix() ` match ?
Our mask if for the bad rows , and all other rows would be anything that is not the mask ( ` ~mask `)
What I don't understand is why ` y_imputed ` returns a numpy array that differs in length from the original len of the column in the original data frame ?
The dataframe is a python object , you can map functions to specific columns .
You can also join dataframes together if you want to create one huge dataframe ... again , the main issue here , is that you did not give us enough information to go on
Also , what I'm looking for is a " query " or " method " to apply to the whole df and return the desired subset .
Next , we merge the country count ( grouped on ` key_1 ` and ` key_2 `) to the dataframe .
How can you drop the index name once you reassign it ?
I am trying to drop anything with NaN .
Using ` loc ` with a simple boolean filter should work : #CODE
Finally , use ` loc ` ( rows , columns ) to locate rows that satisfy a boolean condition .
I define a function to strip the time from the datetime fieldn ( dCount ) and then create a new column ' date ' #CODE
I'd add a new col using ` date_range ` passing the ` min ` and ` max ` date values , then call ` apply ` on a df with a single column passing param ` axis=1 ` to ` apply ` row-wise , you can then count the number of rows that meet your condition using ` sum ` ( as this will convert ` True ` to ` 1 ` and ` False ` to ` 0 `) and add this as a new column : #CODE
So I'd construct a new df with a date range , you can just call apply on this and ` sum ` the number of rows that meet your condition .
This gives us the count for dates less than or equal to the given one , so we need to shift by one day : #CODE
Reindex to get the missing dates in the range : #CODE
I'm assuming you want the actual index location ( zero-based ) , you can call ` apply ` on your ' date_time ' column and call ` np.searchsorted ` to find the index location of where in ` bounds ` df it falls in : #CODE
@USER has pointed out that ` apply ` is unnecessary here and of course he's right , this will be much faster : #CODE
No need to use apply here : ``` In [ 40 ]: bounds [ ' date_start '] .searchsorted ( df [ ' date_time '])
yeah some logic are same i just want to learn how to make these simple logic work , so that I can in future easily apply different things .
You can use ` isnull ` to determine which entries are ` NaN ` : #CODE
Reading csv's into dataframe , change parts of frame , and append all to a single frame
I want to read all the files into dataframes and drop the data I dont need , and then combine the rest into a single dataframe .
DataFrame.drop will accept a list of labels , no need to drop individually .
The error comes from trying to use the method ` notnull ` on ` NaN ` ( which is represented as a floating-point number ) .
Another way would be to use ` notnull ` on the whole Series , and then iterate over those values ( which are now boolean ): #CODE
You should use apply method of DataFrame API : #CODE
You can find more information about apply method here .
you can just do ` df [ ' usids '] = df [ ' uids '] .apply ( set )` , there is no ` inplace ` param for ` apply ` anyway you have to assign the result
@USER apply function take so long ( 19 second ) as my list includes 30,000 uids .
` apply ` is just a ` for ` loop so this will be slow unfortunately , there isn't a ` toset ` method
want to transpose it using pandas , following is the code I tried : #CODE
Once loaded , you can also simpy use ` source.T ` to transpose the dataframe .
How to apply different aggregation functions to different columns and give the results different names ?
The column naming part is something that is missing unfortunately , it's probably already an improvement issue on github , you'd have to rename the columns after the aggregation or do the column naming / agg one col at a time I think
Do I need to use a pivot table ...?
Use ` loc ` : #CODE
The boolean condition generates a boolean mask that is used to mask the index against so will only return those rows .
I often use pd.Series.resample() , and am wondering if there is a way to resample interpolate monthly gridded data in the form ( time , lat , lon ) to say ' MS ' ( monthly start ) .
For example , you could resample a xray DataArray object ` da ` to a month start frequency like this : #CODE
How about specifying a chunk size , something like ` iterator=True , chunksize=1000 ` and use concat ?
might as well use a list comp , passing a generator to join will still get turned into a list
it looks like you want to pivot ?
` df.pivot ( index= ' ix ' , ' columns= ' key ' , values= ' value ')` ?
So to be clear , you want to do basically an inner join on the ` DATE ` key in df2 such that it is within the Start / End Date range ?
Then check the output of ` map ( df1.index.asof , df2.index )` .
Notice the only one that does not match ` len ( msgs_df )` is ` / all_msgs_idx2 ` which corresponds to the ` append_as_multiple ( dropna=True )` .
I can easily replace the nan in each group by a call to df.fillna .
But the challenge is that in need to do the replace operation in the original dataframe as I need to do more transformation post replace operation .
use the statistic per group and replace the nans in the original dataframe or produce a new copy .
Do you only want to append one result row , or do you want to append arbitrarily many ?
Is there a reason you don't simply construct the new rows in a separate dataframe and append them afterwards ?
In general , trying to append to an object while iterating over it is a terrible idea , and prone to errors .
Using raw row-indices is a bad code smell that you're doing it wrong ; either insert an explicit id column , or start handling the data by filter expressions , and without referencing row-indices .
You have still shown zero reason whatsoever why you need to append to the dataframe1 .
In fact you don't need to append anything , you just directly generate your filtered version .
If you really insist on using iterate+append , instead of filter , even knowing all the caveats , then create an empty summary dataframe , then append to that as you iterate .
Only after you're finished iterating , append it ( and only if you really need to ) , back to the parent dataframe .
For the loop I use ` iterrows ` , and instead of ` append ` ing to an empty dataframe , I use the index from the iterator to place at the same index position in the empty frame .
The multiple keys could map to the same processing function and you could easily add additional cases .
When the `` row_count `` is 2 you the code selects `` func1 `` from the map to process the data .
For other row counts you insert that number as a key and define a new function for the value ( i.e. `` func2 ``) .
Fixing the order of the DataFrame columns returned by ` apply ` :
python pandas insert column
I am writing code to insert a new column in a csv file : #CODE
I did change it before I apply the code , and the error come out
Potentially you could unstack these into columns : #CODE
I have create a table in hive by using HQL and need insert a dataframe to it .
If you can set the type of the codes to strings when you read in the data , then you can replace the lambda function with ' len ' .
You might have problems if one of the country code lengths is 3 as NaNs will be interpreted as the correct length , so you will need to replace the NaNs .
create multiindex from dataframe pandas
what is interesting that when I try to write the output of function in csv file , it doesn't keep multiindex structure , have you the ideas why it happens ?
Thinking about it , you can remove the transpose and just use axis=1 in the apply .
You could ` apply ` ` value_counts ` : #CODE
` apply ` tends to be slow , and row-wise operations slow as well , but to be honest if your frame isn't very big you might not even notice the difference .
quickly drop dataframe columns with only one distinct value
Is there a faster way to drop columns that only contain one distinct value than the code below ?
You can use ` Series.unique() ` method to find out all the unique elements in a column , and for columns whose ` .unique() ` returns only ` 1 ` element , you can drop that .
You can create a mask of your df by calling ` apply ` and call ` value_counts ` , this will produce ` NaN ` for all rows except one , you can then call ` dropna ` column-wise and pass param ` thresh=2 ` so that there must be 2 or more non- ` NaN ` values : #CODE
The problem is i need to map the correct values back to the correct index which is difficult when the ordinary of dictionary changes .
You can simply convert your ` dictA ` to a DataFrame and then take transpose , to make columns into index and index into columns .
yes , ` isnull ` will create a boolean series , ` all ` returns ` True ` if all are ` True `
I wrote following codes to transpose the datafile containing digit as well as alphabets : #CODE
How to append new columns to a pandas groupby object from a list of values
We can achieve this by just using ` % ` modulus operator to generate either a ` 1 ` or ` 0 ` , cast this using ` astype ` to create a boolean mask .
Use the boolean mask to mask the index of ` value_counts ` .
Now you have a list of fruit , iterate over each fruit by filtering the df and get the last index label using ` iloc [ -1 ]` and ` .name ` attribute and append this to a list .
Now ` drop ` these labels in the list : #CODE
This works by using ` transform ` ( and ` cumcount ` , which kind of behaves like a type of transform because it broadcasts up to the original index ) to give us a frame-length series we can work with : #CODE
You could use the apply function : #CODE
Try it now , i didn't use ` loc ` for the the ` append ` earlier .
So far I have followed a very inelegant and na ve strategy and used ` shift ` to see which events occurred in the next periods : #CODE
I think your shift command thing is okay , but that's just me .
My goal is to append to the data list by iterating over the movie labels ( rather than the brute force approach shown above ) and , secondly , create a dataframe that includes all users and that places null values in the elements that do not have movie ratings .
pandas replace ( erase ) different characters from strings
However , I would like to use a list so I can quickly replace ` high ` , ` school ` , ` / ` , etc .
I've also thought about using ` concat ` .
I can get the numerator , i.e. the change in the quantity over each month , using ` resample ` .
Since `' time '` is a coordinate in the DataArray you provided , for the moment it is not possible 1 preform resample directly upon it .
Any ideas how to translate this pandas dataframe solver linear problem using PuLP or any other solver approach ?
Then you can you set your date column as an index and resample at the 30 minutes frequency to get the mean over each interval .
You can avoid loops by using ` apply ` #CODE
will give me the dataframe indexed with integer from 0 to len ( myDF ) #CODE
As you can see for ` pandas.read_csv() ` the index_col defaults to ` None ` so you get ` 0 ` to ` len ( DataFrame )` as index , whereas in ` pandas.DataFrame.from_csv() ` , ` index_col ` defaults to ` 0 ` , so it takes first column as default .
Can you try ` df.resample ( ' 5min ')` this will resample your df to 5min blocks , by default it should preserve the values
You can reset the index using ` reset_index ` to get back a default index of 1 , 2 , ..., n ( and use ` drop=True ` to indicate you want to drop the existing index instead of adding it as a column to your dataframe ): #CODE
C :\ Users\user\Anaconda\lib\ site-packages \pandas\core\ groupby.pyc in apply ( self , func , * args , ** kwargs )
Now , you can join this back with the original dataframe and replace the 0 with nan .
I something goes wrong in the merge .
@USER , as requested , answer updated , you only require to enumerate the lists in data array , give it an index , and add it to each dataframe to append to df .
Probably not the most efficient way ( using append ) , but will get your job done .
You may use apply with regex : #CODE
* ( dot star ) means zero or more of any characters ( which is anything really ) and \b [ string ] \b means the whole thing must contains the full [ string ] , take it as startswith - endswith
I just need to shift by 12 months in increments of 1 , for seeing the time of maximum correlation within one year .
This is what I get when I correlate with pandas and shift one dataset : #CODE
Btw check into [ pandas apply / ufunc object ] ( #URL ) .
You can actually put a numpy function into the pandas apply object .
I just need to shift for seeing the max correlation within one year .
I tried iloc and loc too , but result is the same .
I have a wide dataframe that I want to stack and pivot and can't quite figure out how to do it .
What I would like to do is pivot the table so that the column values for Org are the Column names and the column values for each name are the matching values from D1 , D2 and D3 and finally have Topic as the index .
EDIT : As Randy C pointed out , if I use pivot I can get the following ; #CODE
I am having some issue with getting my data onto a map with Basemap and having those points change in color .
I have read many different things online about how to do this , but I still get a map with no points .
My problem is that my shape file generates a map just fine , but I don't get any points on top of it .
I want to plot the dataframe columns ` d3data [ ' GrossCounts ']` on top of the map and have the color scale with the ( integer ) value of ` d3data [ ' GrossCounts ']` .
I have checked and confirmed that they are within the bounding box set up in the map .
It is just a map of the roads and what not , but no points on it .
If I look at the link I see a bigger map .
if I just set color= ' k ' , it still does not plot the points on the map itself .
I don't know why I never saw zorder in any of the online examples that I had found , but adding this makes sure that the map itself is sent to the back so the points are brought to the front .
pandas : pivot table inside multilevel dataframe
I'm trying to pivot table , in order to transform some rows values in columns , so from this dataframe ` df_behave ` #CODE
@USER is right , for MultiIndex , you'd better reset_index and set for the fields he mentioned and perform an unstack .
I think the problem in this ligne ``` df_behave [ ' value '] = df_behave [ ' value '] .astype ( int )``` , cause just before this assignment I have the error ``` DataError : No numeric types to aggregate ``` , and after assignment ``` ValueError : invalid literal for long() with base 10 : ' True '``` , it's quite strange
@USER is right , for MultiIndex , you'd better reset_index and set for the fields he mentioned and perform an unstack .
You can use ` DataFrame.apply ` with ` axis=1 ` ( to apply the func to each row ) and in that function do your logic .
Also I believe ` how= ' any '` is the default so you might as well drop it .
For instance , I'd like to use a ` MultiIndex ` with first the day , and then the time ( so that I can group and aggregate ) .
Or I'd like to join the two so that I have a single column that's the full date .
If you just want to join the two , you could join them as strings : #CODE
Python pandas regular expression replace part of the matching pattern
Is there a way to keep part of the regular expression pattern ( in my case this is ' Ave ' , ' Rd ' , ' Dr ' and replace the rest ?
in default python , i should use ` re.sub ( regex , replace , string )`
I then try to resample for analysis #CODE
So if we apply this we get a Series with the indices as the c3 keys we want and the values as dictionaries , and that we can turn into a dictionary using ` .to_dict() ` : #CODE
Using Pandas pd.pivot_table to pivot by date
That said , the closest thing I could find to the problem I'm encountering is here How to create pivot with totals ( margins ) in Pandas ?
I'd like to pivot this table to get output like this , where I can see things grouped first by account id and then by close date .
Account 10a has 3 rows for 2009-01-01 , but in the pivot table shows 2009-01-01 Nan .
I thought I'd try the same pivot table with margins = True .
It sounds like a group by rather than a pivot table to me - your columns are fixed .
The equivalent with pivot table is : #CODE
I changed something else earlier in my code , and now the pivot table is producing exactly what I want .
What I would like to do is transform the data frame to this : #CODE
I'm looking for a particular pandas function or idiom for column organization to cut down the code and make this more efficient .
I'm getting this error ` KeyError : ' MultiIndex Slicing requires the index to be fully lexsorted tuple len ( 2 ) , lexsort depth ( 0 )'` here ` df = df.loc [: , idx [: , ' Sept ']]` . thoughts ?
How to resample stock tick data to constant value range bars using python ( pandas )
I am relatively new to Python Pandas , I am trying to figure out how to resample stock tick data to constant price range bars ( example 1% change in price should be considered as a bar ) .
I can get the frequencies using ` df.AXLES.value_counts() / len ( df.index )` but I am not sure about how to plug this information into Seaborn's ` countplot() ` .
I tried it using ` vals = ax.get_yticks() ` and ` ax.set_yticks ( vals / len ( df ))` .
I would like to transform the dataframe to look like this : #CODE
My ultimate goal is to aggregate the data into 30 minute intervals for each day then take the mean of the actual trip time in each interval .
I am under the assumption converting to a datetimeindex i can resample the data under those assumptions .
` df [ " type "] == A ` gives you a boolean mask which you can plug back into a dataframe : ` df [ df [ " type "] == A ]`
The problem is when I apply this to a ` DataFrame ` with a large dataset it runs slow .
If you are just looking for a well formated string ( your example suggests it ) you can use a label argument to the cut function .
Then , cut using the ` breaks ` and ` labels ` .
In your example ` labels ` results in ` [ ]` which leads to the error ` Bin labels must be one fewer than the number of bin edges ` when running your cut method .
Instead of ` joint = zip ( breaks , diff )
` , try ` joint =list ( zip ( breaks , diff ))` and see if that gives you a list of tuples with beginning / end points of each interval .
but you run it with ` joint =list ( zip ( breaks , diff ))` , yes ?
You can apply the ` type ` function to the Series values : #CODE
Python Pandas sorting by multiindex and column
Given a dataframe with unique rows , where only one row will have a value for any given column , how can I merge the rows into a single row .
Pandas conditional subset for dataframe with bool values and ints
At each point , I use ` loc ` to capture ` y ` values located at the selected ` x ` value plus and minus the width .
To get back to your desired frame , drop the tracking variable and reset the index .
These DataFrames can have a lot of rows in them so I'm also concerned that the ` apply ` function might not be very efficient .
Python Pandas slice multiindex by second level index ( or any other level )
There are many postings on slicing the level [ 0 ] of a multiindex by a range of level 1 .
` t_df.columns = xrange ( 1 , len ( t_df.columns ) + 1 )` .
When you use ` drop ` on the next line , the field you are dropping is based on the index and not on your df.columns attribute .
I misunderstand the drop function .
The reason I re-define the column is I want to translate the requirement easier to the code
How to calculate rolling mean on a GroupBy object using Pandas ?
How to calculate rolling mean on a GroupBy object using Pandas ?
I want calculate rolling mean on each of my groups in my GroupBy object using Pandas ?
Edit : Do I use itergroups maybe and calculate rolling mean on each group on each group as I iterate through ?
One way to do this is set the index on ` date ` and ` city ` , and then unstack .
This is equivalent to a pivot table .
You can then perform your rolling mean in the usual fashion .
How can I apply a function for each city or for each column in this " pivot table " ?
I want to replace ` Nan ` with [ ] .
I even tried ` replace ( np.nan , [ ])` but it gives error : #CODE
You can first use ` loc ` to locate all rows that have a ` nan ` in the ` ids ` column , and then loop through these rows using ` at ` to set their values to an empty list : #CODE
We are going to populate them according to the mask , IOW . where both sales1 / sales2 are notnull , we will use the hours / minutes for that ( local ) zone #CODE
How to apply a function to each column of a pivot table in pandas ?
You should use ` apply ` function of DataFrame API .
You have to precompute : " same date and city rolling mean " , " same date and city std dev " .
You can use ` groupby ` function for it , it allows to aggregate data by city and date , after that you can calculate std dev and mean .
For putting data in dataframe use apply function .
You have all necessary data for running your check by apply function .
Does the standard deviation and the rolling mean need to be appended to the original data frame for this apply to work ?
If yes , then how can I concatenate rolling mean and std dev ?
I don't think it's as simple as adding a column because df2 is a stacked table / pivot table .
Figured out concat by pd.concat ([ dfs , you , want , to , concat ] , axis=1 ) .
This works because the iterable returned from a df are the columns which are strings so you can just ` join ` them with your separator .
You can create a pivot table on ` user_id ` and ` category ` , fill ` nan ` values with zero , stack ` category ` ( which makes the dataframe indexed on ` user_id ` and ` category `) , and then reset the index to match the desired output .
How to join a data frame to each group in a pandas data frame ?
Do I use join / merge / concat / append ?
All I needed to do was replace the print in my code with return .
If you replace print with return , it returns a real dataframe .
I am looking at the api and related stack overflow posts but I can't seem to figure it out .
How to pass multiple arguments to the apply function
However when I am passing the two parameters to the apply method it is giving the following error :
I have seen the following thread python pandas : apply a function with arguments to a series .
Modified the question for more clarity . dic is the column value that would come by default via the apply function .
` partial ` is equivalent here , lambda isn't ' better ' in any way : e.g. ` countWord = partial ( counting , strWord= ' word ')` and then ` apply ( countWord )` .
How to apply a concat function to a group by data frame using pandas ?
How to return new data frame when using a apply function on old dataframe ?
How to return new data frame when using a apply function on old dataframe ?
Is it a default return with the apply function ?
I do not think you can achieve what you are trying for with ` apply ` , as ` apply() ` with axis ` 1 ` actually runs the function for every row and replaces the row with the returned value ( as you see in your case ) .
Next line does boolean comparison of series ( which does same bool comparison for each of its elements and returns a boolean series back ) , and then it does boolean indexing .
Anyway you can call ` apply ` and pass ` axis=1 ` to process the df row-wise .
You can groupby on ' Column1 ' and ` apply ` a lambda that calls ` join ` to concatenate all the string values and then if you desire construct a list object from that result : #CODE
not sure why you get the error , note that I'm using a double subscript ` [[ ]]` to create a single column df when calling ` apply ` .
How to merge two list columns when merging DataFrames ?
When I merge them based on ` date ` as below : #CODE
For merging two lists use ` apply ` function : #CODE
You can add both columns together to get the list you are looking for , and then use ` df.drop() ` with ` axis=1 ` to drop the ` ids_x ` and ` ids_y ` columns .
Isn't there a way to drop duplicates with ` numpy.unique() ` as it is much more efficient than ` set() ` ?
The values I am using are randomly generate , my question is if there is a method for summing values already to insert as a list instead of random , or do I have to manually go through the list and sum them myself ?
Pandas warning when using map : A value is trying to be set on a copy of a slice from a DataFrame
But no examples seem to be using map , so I'm at a loss ...
If you want to apply some function to some column of dataframe , try to use function ` apply ` function of DataFrame API .
Or if this seem too long to you , you can create a mask ( boolean dataframe ) beforehand and assign to a variable , and use that in the above statement .
Demo with the mask method - #CODE
So if I first map , then slice ( so the other way round ) , it works without bitching !
I tried to reindex it , but got an error : #CODE
In pandas you can only create a row to the right , unless you do join between two dataframe .
If ' 0 ' is a placeholder for a value that was not measured ( i.e. ' NaN ') , then it might make more sense to replace all ' 0 ' occurrences
I would like to aggregate a DataFrame by ` col1 ` and ` col2 ` , summing results on ` col3 ` and ` col4 ` and averaging results on ` col5 `
By passing a dict to aggregate you can apply a different aggregation to the columns of a DataFrame .
replace non Null values in column by 1
I'de like to replace the numerical values that are not NaN by 1 , to have : #CODE
Use ` loc ` for this : #CODE
You could ` apply ` ` pd.Series.nunique ` , and then use that to select : #CODE
The fields are id , start ( date ) , end ( date ) , diff ( the number of days between start and end ) , mindiff ( the min ( diff and the last day x months from start ) .
But how do I get only the specific rolling mindiff values for months of ' start ' to be averaged within the groupby of the ' end ' year / month .
Would it be more pythonic to use resample for the months ?
are you saying your way is an improvement over resample ( ' M ') ?
However I'm not sure how to go about it with apply ( if at all possible ) .
1 , Shift the ` amp_change ` by one position ( ` shift ( 1 )`)
Especially the part " abs ( start - index ) < 0.01 "
Inner join to fetch columns from a look up table pandas
I used merge function , to do this : #CODE
first_df has 46 K rows and second_df ahs 56 K rows . but the resultant I got has 18 million records ( so a cross join has happened ) .
Here , join is not happening the way I want , any reason you can think of as to why this is gng wrong ?
You can apply string methods in a vectorized way using the ` str ` attribute of ` Series ` .
The basic issue is that I would like to set a certain row , column in the ` dataframe ` to a list using the ` ix ` function and am getting ` ValueError : setting an array element with a sequence .
so then I map over this open prices to my frame ?
Now , I would expect pandas to drop the second row ` 3 , 5 ` since it actually is a duplicate .
From the documentation it says that " by default use all of the columns " , which in my mind means " only drop a row if all the columns of one row match all the columns of another row " , but clearly this isn't the case .
I recommend using datetime64 , that is first apply ` pd.to_datetime ` on the index .
If you set this as an index then you can use resample : #CODE
Attempt 2 : When I try with ` apply ` I almost get what I need : #CODE
Attempt 3 : If I to assign the result of ` apply ` to a new column : #CODE
How did you try the transform ?
just assign a new column to the transform , e.g. ` df [ ' C_pct '] = df.C / df.groupby ( " A ") [ " C "] .transform ( " sum ")`
weirdly , even with ` as_index=False ` the apply version doesn't work ...
You can use transform , which " puts back " the aggregated results : #CODE
I tried using transform and apply without luck .
Will have a little think if I can get this with an apply ( but this will always be more efficient ) .
You can use the ` agg ` function #CODE
I need to get out a numpy matrix with shape ` ( len ( first_index ) , maxlen , num_columns )` .
` maxlen ` is some number ( likely the max of all of the ` len ( second_index )`) or just something simple like ` 1000 ` .
I can do this with ` arr = df.as_matrix ( ... )` and then ` arr.resize (( len ( first_index ) , maxlen , num_columns ))` .
` df.unstack() ` which would result in shape ` ( len ( index2 ) , maxlen * num_columns )` following your notation ; here ` columns ` will be stored as a ` MultiIndex ` .
I believe that the shape should be ` ( num_columns , len ( index1 ) , maxlen )` .
How do I drop a table in SQLAlchemy when I don't have a table object ?
I want to drop a table ( if it exists ) before writing some data in a Pandas dataframe : #CODE
The SQLAlchemy documentation all points to a ` Table.drop() ` object - how would I create that object , or equivalently is there an alternative way to drop this table ?
Note : I can't just use ` if_exists = ' replace '` as the input data is actually a dict of DataFrames which I loop over - I've suppressed that code for clarity ( I hope ) .
How to convert recurrent vertical column into rows than stack them together in Python / Pandas ?
I am generating some data vertically at first , but would like to transpose them into row data , then stack them into an array like a Pandas data frame .
Then as I loop through the rows in your file , I'd use the first value to index into the dictionary to get the array and then append the numerical value to that array .
Pandas dataframe column containing list , Get intersection of two consecutive rows
Now I want to use ` len ( filter ( y.__contains__ , x ))` to get no of same elements in two columns which contains ` set ` .
You can do this using ` DataFrame.shift() ` to shift the rows by one column and then rename the ` coulmnis ` column to something else , then reset index and merge the dataframes on ` timestmp ` and then use ` apply() ` on the DataFrame .
HI can you explain len ( x [ ' coulmnis '] & x [ ' newcol ']) part ??
It is taking intersection of the set in ` coulmnis ` and set in ` newcol ` for each row and then its length ( set in ` newcol ` is the set from next row ) .
Earlier I was trying len ( filter ( y.__contains__ , x )) for the same purpose but was not able to apply it on df
For ` .apply ` it would become - ` len ( filter ( x [ ' newcol '] .__contains__ , x [ ' coulmnis ']))` .
But set intersection would be much faster than this .
One way , is to use ` apply ` and calculate len #CODE
Efficient way to store append rows into dataframe stored in disk .
I need an efficient way to store a dataframe in the disk and append each row ( dic_id and entity freq ) one by one to the dataframe .
Pandas multiindex boolean indexing
I want the ` a ` index to dominate , so if any of the entries under a unique value of ` a ` , in this case ` a == 2 ` , have a corresponding value of ` c ` such that ` c > 1 ` , then drop all entries associated to that value of the ` a ` index .
@USER ` concatenate ` is not the solution as it merge the way I don't want to .
matplotlib boxplot xticks shifting towards y axis
I am beginner to matplotlib and trying to plot a boxplot for my data I extracted using pandas .
I am successful in getting boxplot which looks perfectly like what i want except xtics .
And below is my matplotlib boxplot code : #CODE
Transforming DataFrame ( pivot )
After turning a pivot table into a DataFrame , my data looks like this : #CODE
Assuming ` time ` and ` model color ` are forming a hierarchical index ( if they are not , you can create this index easily with ` pd.MultiIndex.from_arrays `) , the simplest solution would be to " unstack " that index : #CODE
When you use that you don't need strip .
You can use ` converters ` to strip the data as you read them in .
concatenate multiindex pandas dataframe
I have two pandas's dataframes I wish to concat over axis=1 .
` stats_df.columns ` : ` MultiIndex ( levels =[[ ' gmm ' , ' normal ' , ' regression '] , [ ' covars ' , ' diff ' , ' dof ' , ' expected ' , ' loc ' , ' means ' , ' observed ' , ' scale ' , ' weights ']] ,
` File " C :\ Users\hanans\AppData\Local\Continuum\Anaconda3\lib\ site-packages \pandas\tools\ merge.py " , line 754 , in concat
` pd.concat ` tries to merge ` DataFrame ` s based on its index .
You could do a reset index and merge based on resulting columns
In this particular case , you can pull out specific elements from that list using apply though with something like ` x.apply ( lambda x : x [ 1 ])` to pull the month , but Fabio's answer is better from a data organization perspective .
I want to drop duplicated rows for a dataframe , based on the type of values of a column .
The first aspect I'm struggling with is how to select on the second index of a multiindex .
However , how do I then replace the original values with those newly rescaled values ?
I would go through unstack to make the multi-index level to be first , and then slice it : #CODE
It's not totally the same output , and only the first version will allow you to change the values ( thanks to ` loc ` and the fact you keep all the index values ): #CODE
I looped through the dataframe to insert rows into a table in database .
If you are sure that there is one and only one row - just do without the ` sum ` , and in this case you also can remove the ` map ` operator
Pandas - Add leading " 0 " to string values so all values are equal len
I have a dataframe that contains a multiindex .
You then subset it by saying , give me all the levels in the second multiindex ( called ' script ') .
For instance : list a len is 700 and list b len is 300 .
for i in range ( 0 , len ( a ):
b.count ( a [ i ]) I apply this but received this error .
you are trying to do an inplace replace . you should do #CODE
Then you can simply replace the relative ` True ` or ` False ` values with 1 and 0 .
The grouping operation should ultimately produce a single ` grouped ` object that I can then apply aggregations on .
you might also want to apply integer division to generate time intervals : #CODE
The challenge I'm facing is creating multiple , sometimes overlapping time bins that only apply to specific ` id ` s .
You may not have two labels that map one element to several subsets .
if you really believe it is the only way , you will need to cut your infants into halves , i.e. devise a scheme to duplicate your rows .
Specifically , I have annual data and I want to resample so that there are 14 rows spaced between each current row .
What I want is a method to resample so that I get something like this #CODE
Have a look at #URL One issue -- because Pandas Dataframes are a collection of columns , one Series per column , a column can only be converted to numeric or date types if all the rows conform or there is a default value for non-conforming rows .
Also , the conversion problem has been solved before , see [ How to insert pandas dataframe into mysql ] ( #URL )
How to group a dataframe by some transform of a column
Of course , I realize that one can always create an auxiliary column to hold the result of the transform , and use this auxiliary column as the argument to ` groupby ` .
I have written a python script to create a table using a ` create table if not exists ` statement and then insert rows from dataframe into vertica database .
For the first time when I run this python script , I want it to create a table and insert the data - it works fine .
But from next time onwards , I want it to create a table only if it does not exist ( works fine ) and insert data only if that row is not contained in the database .
I use both ` insert ` statement and ` COPY ` statement to insert data .
Here in the above code , I am creating table and then inserting into tablename1 and tablename2 using COPY and insert .
Or the ` INSERT ` only when data for that row does not exist in the target ?
I am looking for both , COPY or Insert only when data for that row does not exist in the target .
Use a control table that somehow uniquely describe the set of data being loaded and insert into it and check before running the script that the data set has not been loaded .
If row not found , insert rows #CODE
Alternatively you can insert or merge data in based on a key .
If you don't want updates and data does not change once inserted , then INSERT will be better as it will not incur a delete vector .
Then you can replace actual values as appropriate given the levels of the multi-indices .
I would like to know the most efficient ( fastest computing ) way to replace the " , " between Last and First with " , " ( get rid of just the space before the comma but keep the one after it ) .
One way is to do the plain old merge then throw away the values out of the range : #CODE
Wouldn't it be more efficient to first filter and then merge ?
I'm thinking there must be a smarter or faster way to do this , a mask could have been useful except you can't fill down with this data as ` price2 ` for one row might be thousands of rows after the ` price2 ` for another row , and I can't find a way to turn a merge into a cross apply like one might in TSQL .
df [ ' Exit '] = list ( map ( ATestFcn , df.index , df.Price1 ))
It is much more straightforward to create a column , and assign periods using the ` loc ` method .
Using ` loc ` like above is recommended .
To make it more generic , compare row values on apply method .
Or , use ` df.iloc [: , 0 ]` for first column values and match it ` eq ` with ` df ` #CODE
if I wanted a ' 0 ' where there is a match and ' 1 ' where is no match , can I just change ` len ( set ( x ))= =1 ` to ` len ( set ( x ))= =0 ` ?
` len ( set ( x )) !
I have a question per below - I need to transform multiple rows of ID into one row , and let the different " output " -values become columns with binary 1 / 0 , like example .
I have been down the lines of doing a pivot like matt_s suggested below , but didnt come all the way !
You could use a pivot : #CODE
pandas replace zeros with previous non zero value
You can use ` replace ` with ` method= ' ffill '` #CODE
You'd want to first convert the date columns via ` pd.to_datetime() ` and then take the diff ?
You can first merge both dataframes #CODE
I have looked online and found the merge function , but this isn't working because my rows aren't a column .
Then to merge : #CODE
You need to pass the boolean mask and the ( two ) values columns : #CODE
will mean you have a DatetimeIndex and can do nifty things like loc by strings .
I feel like I could somehow perform this by using merge but cannot figure out ..
Is there a way to get the array to find the years , or a way to shift that first column up by one ?
I have found a way to shift a dataframe column up by one , but I need to shift the grouping , not the dataframe .
I also tried turning the new grouping into a new dataframe so that I can shift the year column up , but haven't been successful .
Use the pivot function : #CODE
it is because of the comma . remove the comma with something like ` replace ( ' , ' , '')` and it should work
I believe that changing the rows as you pass through them is not a good idea , after that you can simply replace a column and delete the other if you wish .
pandas merge dataframes by closest time
I've got two dataframes ( ` logs ` and ` failures `) , which I would like to merge so that I add in ` logs ` a column which has the value of the closest date found in ' failures ' .
You can reindex with method= " nearest " .
Using Pandas to merge 2 list of dicts with common elements
You should actually use ` groupby ` to group based on ` name ` and ` total_year ` instead of ` apply ` ( as second step ) and in the groupby you can create the list you want .
To avoid too many tiny slices in a pie chart , I need to merge / sum all elements in a series below a certain threshold .
this delivers the correct result , but the use of ` append ` bothers me and does not strike me as particularly pandas-like .
Python Pandas replace string based on format
Please , is there any ways to replace " x-y " by " x , x+1 , x+2 ,..., y " in every row in a data frame ?
For example , I want to replace every row like this :
In pandas you can use apply to apply any function to either rows or columns in a DataFrame .
( side-remark : your example does not entirely make clear if you actually have a 2-D DataFrame or just a 1-D Series . Either way , ` apply ` can be used )
You can use ` str.cat ` to join the strings in each row .
If you want to apply this method to multiple columns of a DataFrame , you'll need to use it on each column individually in turn .
Sure - to apply the method to the ' words ' column , you could write ` df [ ' words '] .str .cat ( sep= ' , ')` ( where ` df ` is the name of your DataFrame ) .
@USER the transform ( max ) trick is neat .
A completely different approach would be to use a dictionary comprehension to map the unique planets : #CODE
okay presently I was trying to use the xlsxwriter engine to open up the workbook and append the dataframe from a particular index which is after the first frame .
right way to use eval statement in pandas dataframe map function
I try to use map , where the function inside map is eval : #CODE
how can I use eval ( or a vectorized implementation ) instead of the clumsy loop above ?
try to use " apply " instead of " map "
Unfortunately , I get the same output with " apply " .
Anzel , you are right . but the is_string function should take care of that . after I do map ( is_string ) , the pd.series should only include strings .
That is out of this question scope , but if you have mixed types both non-evaluate and evaluated , then do a map or apply with a function and perform a try / except then you should be good
Using eval is generally frowned upon as it allows arbitrary python code to be run .
I agree with ** literal_eval ** for safer measures , although OP's error is probably due to mixed types of both evaluated and non-evaluated strings -- OP's original approach with just " eval " doesn't work neither .
I guess that , as a non expert , I still fight a bit with the notion of apply vs map .
After filing the nans with strings ( data [ ' organization '] = data [ ' organization '] .fillna ( ' [ ]') , both apply and map on literal_eval did the job .
btw , I know that map vs . apply is a complete different question , do not feel the need to reply it .
Ironically , I had just been using indexed keys on a standard json import a little earlier , but hadn't thought to apply it to the pandas read :)
So far I tried to apply the ` isin ` method : #CODE
You can do this efficiently using ` isin ` on a multiindex constructed from the desired columns : #CODE
Another option that avoids creating an extra column or doing a merge would be to do a groupby on df2 to get the distinct ( c , l ) pairs and then just filter df1 using that .
I want to do an inplace replace like this :
replace that value with 1
replace that value with 0
You can also create a function to check your conditions , and apply to the dataframe : #CODE
I want to add a new column which contains values based on df [ ' diff ']
When using ` DataFrame.apply ` if you use ` axis=0 ` it applies the condition through columns , to use ` apply ` to go through each row , you need ` axis=1 ` .
But given that , you can use ` Series.apply ` instead of ` DataFrame.apply ` on the `' diff '` series .
You can just set all the values that meet your criteria rather than looping over the df by calling ` apply ` so the following should work and as it's vectorised will scale better for larger datasets : #CODE
this will set all rows that meet the criteria , the problem using ` apply ` is that it's just syntactic sugar for a ` for ` loop and where possible this should be avoided where a vectorised solution exists .
You then join this to the original dataframe : #CODE
I think this may be a bug in apply / map_infer , definitely worth a github issue .
In this case , you need to strip these out first : #CODE
It seems strange to use a lambda that returns a Series in a transform !
( Rather than use an apply . )
I guess they use the same path , * but * tranform usually means that one value is spread on the group ( e.g. transform ( ' min ')) whereas apply means that the group can return anything .
Subtract the AMV column from the transform min : #CODE
` pandas.Series ` has a method called ` map ` which does this for ` dict ` objects , meaning something like this would work : #CODE
` t = [ l [ s [ i ]] for i in range ( len ( s ))]` ?
@USER no relationship between length of ` l ` and ` s ` , but ` max ( s ) < len ( l )` .
As you already have a list of ints then you can just construct a Series from this and pass this as the data param for ` map ` : #CODE
Change week definition in pandas resample
Currently , when I use the pandas resample function for days to weeks it uses a Sunday to Saturday week , but I'd like it to use a Monday to Sunday week .
Can't you just subtract a day and then resample ?
Would it suffice to drop the Saturday rows before the resample ?
The ` lambda ` function counts the number of items in the rolling group ( excluding the last item ) is greater than the last item .
Yes you can create a mask which will remove unwanted rows by combining ` df.notnull ` and ` df.shift ` : #CODE
Test whether the rows are null with notnull : #CODE
You can shift down to get whether the above row was NaN : #CODE
Note : You can shift upwards with ` shift ( -1 )` .
Use shift for that : #CODE
Then you want to figure out if anything in that row was nan , so you want to reduce this to a single boolean mask .
Then just drop the columns : #CODE
Then follow the recipe in the comment to your question , and on the columns you want to strip
Standard comment : if you insert an image , no one can copy and paste it -- they'd have to type it in .
On the other hand , if you insert * text* , we can use ` pd.read_clipboard() ` to easily reproduce your frame .
Then flatten this with stack ( which adds a MultiIndex ): #CODE
to which I apply pivot_table #CODE
I want to replace all values in ` Late ` that are 46 or 48 with the median of ` Late ` .
Bear in mind that groupby didn't really apply in your case and that it returns a ` DataFrame ` -ish object
fig.savefig ( " hist " , format= ' pdf ')`
I want to replace all values in Late that are 46 or 48 with the median of Late .
To get a specific cell use iloc ( or loc ): #CODE
Calculation between groups in a Pandas multiindex dataframe
Use a transform : #CODE
UnPivot a Pandas Data Set with Merge
How do I transform this starting data set into a flattened data set with a Python Pandas data frame ?
I tried to " stack " the data and reset the index , but this produced an undesired result .
You're looking for ` melt ` ( aka " unpivot ") : #CODE
Hey Andy , just curious why you prefer melt over setting an index and then stacking ?
( my datasets almost always have huge , complex ` MultiIndex ` indexes and columns , so those always feel more intuitive to me .
Honestly I rarely use melt , I think it's a bit space-wasteful , I prefer to just leave it as columns .
:) That said , I think melt is more efficient than ` df.set_index ([ " N " , " P "]) .stack() ` .
For one , melt doesn't drop NaNs ...
@USER the melt function worked when i was importing an Excel file but when I tried the same function with a .csv file , the entire values column is " NaN " .
can you resample your dataframe so that it is evenly spaced ?
@USER I am asking about sums , not averages , though maybe there is an obvious transform
Then a slight change in how you pick the weights from the weightspace in the rolling function : #CODE
I've figured out how to use intersection to determine how much two ranges overlap , and I figure using the length of the intersection output would be a good filter to remove overlapping ranges .
In addition , I'm also wondering what's a good way to create these intersection values for filtering , but keep the rest of the values in the rows .
Strange behavior of transform function in Pandas when using x.unique()
I do not think this is a bug , ` transform ` converts the result it gets to the same size of the group , hence when you send it a list of unique elements , it repeats the list so that it becomes the same size of the group , hence for the first group , you get ` [ ' t1 ' , ' t2 ' , ' t1 ']` , and then each element is applied at each index .
If you want a string like `' t1t2 '` , in the resulting column , you should use ` str.join ` to join the result and provide that to ` transform ` .
You mean't to index the cell in the dataframe , with ` irow ` as the index and ` col ` as the column , you cannot use simple subscript for that .
Use ` .iloc ` if you intended for ` irow ` to the the 0-indexed number of the index , if ` irow ` is the exact index of the DataFrame , you can use ` .loc ` instead of ` .iloc ` .
I would like to transform a column in my data set that has number of days from the start date of a period into a date .
Have seen similar questions on stack , but none seem to solve the issue .
Create an array with numpy that resample values over a year
For ` resample ` to work the index needs to be a ` DateTimeIndex ` type
I'm looking for method , that iterates over the rows , but apply some method only for every 20th or 30th row values
That's why I tried iterate over the rows , and apply the function of request only for every 20th or 60th row ( cause I have 7000 rows ) and not to speed the process by applying the time.sleep method
then you can apply ` mean() ` to the series : #CODE
Can you try to transpose the dataframe ( ` .T `) and then try to resample but without axis=1 ?
Personally , I would like to see that the default behaviour so I wouldn't have to remember to replace all the NaNs in every crosstab calculation .
Because the database is fairly large , my first task is to cut it down to size by removing entries which I know for a fast will not form part of the optimal draft .
you may want to read the chapters in pandas docs about ` merge ` and ` join ` .
If records are aligned , you can truncate each table by minimal length of the two ; if no alignment is assumed , you should look at ` join ` method .
Read your files into Pandas with ` pandas.read_csv ` and join on the `' gene '` column .
( i.e. a way to aggregate with specific functions per column ? ) .
I find myself often doing something like the below , starting with a dataframe which has a column of dates in string format which I want to bin by some calendar unit ( days , months , years etc . ) I resort to something like the below because I know ` resample ` only works on a DateTimeIndex series .
Another way of doing this is to put your conversion logic in a function , and to apply this function over the column .
Many Pandas functions take this argument to apply an operation across the columns or across the rows .
Use ` axis=0 ` to apply row-wise and ` axis=1 ` to apply column-wise .
Problem : Given the dataframe below , I'm trying to come up with the code that will apply a function to three distinct columns without having to write three separate function calls .
Then I apply the function to the particular column : #CODE
I want to be able to write code that will generalize and apply this function to multiple columns in one pass .
The ` lambda ` will ensure that only one input parameter of your function is dangling free when it gets ` apply ` d .
So you set default values for the * last * two parameters of the lambda , thereby effectively rendering it a single-input function for ` apply ` .
Since these are just default values , the lambda could also work in a 2 or 3-input syntax , but ` apply ` only uses a single variable , so it must have at most 1 non-default parameter .
One more ( general ) question : why pass a lambda into the apply function instead of just the function itself ?
So it's simply because ` apply ` expects a single-input function , which it then applies to the variable .
Okay , I think I understand : the use of a multi-argument ` lambda ` is your way of getting around the single-argument constraint of ` apply ` .
And this approach can be generalized when wanting to apply ** any ** multi-argument function with ` apply ` or ` map ` or ` applymap ` .
First , your data apparently does not conform with a tabular format .
The main reason I'm doing this is to be able to join data files and currently , it seems that one table has a key field that is of a different format than the other and I'm guessing that's why the join ( pd.merge ) is failing .
And yet the join worked , so for my purposes , this solved the problem .
Furthermore , the MultiIndex did not group all the ` foo ` s together under ` A ` ?
But I don't understand why the ` columns = len ( group.columns )` is necessary ...
One could as well just have used ` len ( group.columns )` directly .
How to boxplot data after different column values in pandas
What I want to make is a boxplot for each group in ` Country ` , displaying a number of boxes corresponding to the number of unique values in ` Years ` .
That gives me a boxplot for each Country , but with just one plot in it , representing all the values from that group , not separated by years .
Have you seen [ seaborn's boxplot ] ( #URL ) ?
I would use the seaborn package , in particular combining the FacetGrid with boxplot .
failed to replace column values in dataframe
I thought about using ` iloc ` ` loc ` but I'm not very strong with this methods , so if you know how better apply them to this case , it could be solution for my problem
You can use the ` loc ` property to modify a view of the data like this : #CODE
Edit some explanation of ` loc ` :
You can think of ` loc ` as just a way to index rows and columns of dataframes in a flexible way .
The row / col indicator is very flexible : it can be a boolean mask , an index , a slice , a list of indices , etc .
We first construct a boolean mask which indicates where the ID is 9 : #CODE
We can use this mask to index the dataframe , and access all rows where the mask is True : #CODE
But we don't want the whole row , we just want the `' Direction '` column , so we add this to the ` loc ` call : #CODE
Thank you ! as I supposed the ``` loc ``` is a solution for my problem , but could you explain , please , how it happens that it replaces only non-empty cells of ``` Direction ``` column ( what I needed indeed ) and not all the rows that correspond to ``` df [ ' ID '] == 10 ``` ( what I was afraid of )
I added some more info on how `` loc `` works here .
Check out the output of e.g. `` pd.Series ([ ' ABC ' , '' , ' TRUE ' , '']) .astype ( bool )`` to see this in action .
This is my first time posting on stack overflow , so bear with me .
My data is in a DataFrame of about 10378 rows and ` len ( df [ ' Full name '])` is 10378 , as expected .
But ` len ( choices )` is only 1695 .
I'm fairly certain that the issue is in the first line , with the ` to_dict() ` function , as ` len ( df [ ' Full name '] .astype ( str )` results in 10378 and ` len ( df [ ' Full name '] .to_dict() )` results in 1695 .
what is ` len ( df.index.unique() )` ?
@USER using ` choices = dict ( zip ( df [ ' n '] , df [ ' Full name '] .astype ( str )))` , where df [ ' n '] is np.arange ( len ( df )) , worked fine and got what I needed .
This is what is happening in your case , and noted from the comments , since the amount of ` unique ` values for the index are only ` 1695 ` , we can confirm this by testing the value of ` len ( df.index.unique() )` .
How do I use timestamp dates as an index in a pivot table in python ?
I have recently began working with the python.pivot_table and have encountered a challenge using timestamps properly with the pivot tables .
Is there a way to implement this with pivot tables when the date column are TimeStamp objects ?
Then use those columns to create the pivot table : #CODE
When I create a pivot table from data in Pandas ( python ) , I get an other result than when I create it with Excel .
Someone knows the difference between the pivot table in Pandas and Excel ?
Taking the sum of all these values is not really a pivot table .
Using the above as a mask : #CODE
But when I try and use this function with apply : #CODE
I'm not sure why you have this problem with ` apply ` , but you should not write the function like you did in the first place .
To answer the second part , once the dtype is a ` datetime64 ` then you can call the vectorised ` dt ` accessor methods to get just the ` day ` , ` month ` , and ` year ` portions : #CODE
Otherwise , passing a function which returns ` Series ` to ` apply ` results in ` DataFrame ` .
Then transform your original data frame into the format that you want #CODE
You can construct the lists for each continent and ` apply ` a func : #CODE
Or if you have a much larger df then you can just make successive calls using ` isin ` and mask the rows using ` loc ` : #CODE
When I drop into the debugger at that point ( one frame up from the UnicodeDecodeError ) and decode the UTF-8 string `' df2_\xce\xb2 '` into the Unicode string ` u'df2_ \u03b2 '` , and then call ` _g_get_objinfo ` with _that_ value , it works and returns `' Group '` .
if I apply ` .value_counts ` directly to ` groupby ` as #CODE
no , in fact , given the big dataframe I have for ex frequency = 198 instead of 4 for every session , and in some rows of " freq " column I have date values as " 2015-05-22
I want to append 2 dataframes : #CODE
Also tried converting the dataframes into list and then tring to append it .
` merge ` or ` concat ` work on keys .
However , why not use ` numpy append ` and create the dataframe ?
You need to change one column name , so ` append ` can detect hat you want to do : #CODE
Now i need to merge them together .
Is there a chance to merge them together in one row ?
This is by default an inner join .
would merge on column names shared in common by default .
( Otherwise , how do you know it stops after 265 rows ? ) You should be able to merge the two DataFrames with only one call to ` pd.merge ` ....
I have tried creating a new function and using ` groupby ` and ` apply ` , but this works only if rows are sorted .
You could use ` pd.rolling_sum ` with ` window=2 ` , then ` shift ` once and fill ` NaNs ` with ` 0 ` #CODE
Yes , you could do , in ` agg ` you can mention which aggregation rule ot be applied on each column .
Not sure about efficiently but a cleaner method is to call ` apply ` and pass `' , ' , join ` as the func to call : #CODE
IIUC then if you're using pandas version ` 0.17.0 ` then you can use ` merge ` and set ` indicator=True ` : #CODE
You could use ` transform ` to overwrite column ' B ' in original df and then call ` drop_duplicates ` : #CODE
no worries , it is indeed simpler than using ix !
This is ok as it is , and with some more work , I can map to results to correct month names , then plot the bar chart .
When i try to apply drop_duplicates() on a column containing 1 or 2 words , if works fine .
When i try to apply drop_duplicates() on a column containing 1 or 2 words , or on a smaller sample of comments , if works fine .
OK then you should modify the question to be explicit about what you want to do in case the start / end tags don't align .
On the append operations pass ` index=False ` ; this will turn off indexing .
How to avoiding skipped months in Python pivot table when all entires in table are 0
I have a pivot table which I sort index by year then month from a pandas dataframe column with timestamp objects .
If there are no entries for a particular month / year that particular row in the pivot table is not populated .
Curent code I have to generate the pivot table is below .
You can use pandas ` date_range ` to generate a complete range of dates ; and then ` reindex ` your dataframe with the option ` fill_value=0 ` .
The link to data @USER is added to the post for reference as well as the code I'm using to generate the pivot table in question .
You can change the `` reindex `` arguments like `` pv.reindex ( index =p d.date_range ( ' 2012-12-01 ' , ' 2016-12-01 ' , freq= ' M ') , ... )``
I've looked into ` eval() ` , but it doesn't seem to apply to hard-coded lists of index values like this .
You can try ` merge ` : #CODE
You can get a decent speedup by using ` reindex ` instead of ` loc ` : #CODE
` loc ` builds the new DataFrame by calling down to
On the other hand , the hard work in ` reindex ` appears to be done by the ` take_* ` functions in generated.pyx .
Also you're trying to use the boolean mask from the condition to index the df by doing this : #CODE
Shifting disaggregate distribution to match more aggregate level distribution
What I would like to do , then , is shift the tract-level population counts on the margins , meeting the following criteria , with the first two being most important ( I realize there are tradeoffs with respect to meeting all of these ):
Any ideas as to how to , in general terms , make a detail distribution fit a more aggregate one , beyond just sampling tracts and moving x people from ` grp4 - grp3 ` , ` grp2 - grp1 ` and whatever the difference is between the existing and the target distributions ?
In order to force your subregion distributions match the aggregate distributions you will * necessarily * have to alter the spatial distribution at the level of the subgroups .
The size of the adjustment that's needed will depend on how different the subgroup distributions are compared to their corresponding aggregate distributions , and on how closely you require them to match .
@USER , I realize there is some conflict / tradeoffs between the requirements but I don't think it is a fundamental one : You can reduce , say , the numbers in age group 4 downwards for a subregion , without fundamentally changing the underlying , within-region map .
I used ` append_to_multiple ` to append dataframe into 2 tables , and the selector is an datetime column .
A vectorized way to do this would be to normalize the series , and then add ` 12 ` hours to it using ` timedelta ` .
Pandas TimeSeries resample produces NaNs
I can't post any example data here since it is sensitive info , but I create and resample the series as follows : #CODE
Instead fillna you can use resample .
I want to use a function from an add-in in excel and apply it to some data i have simulated in python .
I need to be able to call the add-in and apply my data indexes there ... something along these lines : = add-in_name ( data_range1 , data_range2 , " GGCV ")
Sadly , if I run this code below I get ` len ( reqsb ) = 21456 ` .
Where x is length of df.output_gap_pf_sf , a is amount of entries in df.gov_debt_perct_mev under 60 and b is ( len ( df.gov_debt_perct_mev ) - a ) .
To take this further , you could use an [ apply ] ( #URL ) combined with a function to carry out the logic to remove the loop entirely .
" character will be drop automatically .
Assuming your dataframe is indexed by ` [ " Sector " , " industry "]` you need first reset_index and then pivot your dataframe and finally make the stacked plot .
You can then apply ` np.where ` as you did to find the indices where your condition is fulfilled : #CODE
You can try concat this output dataframes to one : #CODE
pandas join data frames on similar but not identical string using lower case only
I need to join data frames on columns that are similar but not identical .
So I am trying to isolate the lowercase letters from each column , create new columns to join on .
Note that this assumes collecting all ASCII characters from ` a ` to ` z ` suffices to produce values on which to join .
How to append columns based on other column values to pandas dataframe
I have the following problem : I want to append columns to a dataframe .
When you use ` apply ` , it calls your function once for each column , with that column as an argument .
Since ` apply ` is called for each column , you wind up appending the values to both columns in this way .
` apply ` is not the right choice when your computation depends only on the values of a single column .
Instead , use ` map ` .
Then you need to wrap this new row data into a DataFrame and attach it column-wise to your existing data using ` concat ` .
This approach certainly has disadvantages in that if you strip out a subset of data into a smaller df and then want to run an analysis against a column of data you left in the bigger df , you have to go back and recut stuff .
Or is it best to actually cut out smaller dfs to work with ?
Pandas Dataframe - faster apply ?
You should avoid ` apply ` and use ` to_datetime ` : ` df [ ' local_time '] = pd.to_datetime ( df [ ' local_time '])`
pandas groupby aggregate with grand total in the bottom
How do I add a grand total row at the bottom , as in a pivot table in Excel ?
Are you asking how to get rid of NA rows generally , or if there is a method for the ` agg ` method to leave them out ?
Only the second question is interesting ;) It's curious how ` groupby ` keeps one of the bins generated by ` cut ` even though the resulting dataframe does not have this particular bin .
To just drop the ` na ` records , you can use the ` .dropna() ` dataframe method .
I have a dataframe with variables that are coded as integers , which I'd like to replace with their actual value labels .
I could do it using apply and a for loop ( see below ) , but it is pretty clunky .
You can use ` apply ` and use the column's ` name ` attribute to get the key for the outer dictionary : #CODE
I created a dataframe and then used a pivot_table to transpose the data from rows to columns .
Try unstacking your data so max is a column rather than a part of a ` MultiIndex ` , and use ` ffill ` on that frame .
For an irregular time series , ` S ` , and I would like to take points that are at least ` dt ` apart , but without changing their timestamps .
So to be clear -- Start Date is a potential join field ?
Now , join the ` End Date ` from ` df2 ` to ` df1 ` .
Essentially I want to merge ` Metrics ` and ` Stage_Name ` :
Later I would want to aggregate based on ` Block_Name ` , so for a merged column , the result would be two dictionaries added together for ` Block_Name ` : ` A ` .
IIUC correctly then you use ` apply ` with a ` lambda ` : #CODE
Didn't know I can use apply and axis=1 , thanks !
I am able to generate a heatmap with quantity overlaid on the graphic as a visual of a pivot table .
` pv ` is my pivot table which I use to generate the heatmap figure .
The pandas pivot table function has a ` margins ` parameter , maybe you want to use that ?
Elegant way to mask out intervals between events containing nan in numpy / pandas
The ` -1 ` on ` searchsorted ` accounts for the shift of ` np.diff ` ; as a minor caveat , this code does not work correctly if there are ` nan ` s before the first ` 1 ` ( although this is easily remedied ) .
You may use ` np.ufunc.reduceat ` to find out which segments include nan , and mask them out : #CODE
I would hate forge ahead and apply the hack shown above to some code and then have it not supported in future releases of pandas .
It stems from the fact that if the item being assigned is a DataFrame , ` loc ` and ` ix ` assume that you want to fill the given indices with the content of the DataFrame .
You can create a list of column names and then iterate through them and apply your logic for them .
Melt the data frame , then apply the repalce and to lower function .
Pivot the data frame to get back
Pandas merge fails on large dataframes
but with large data , the merge fails here my output using large data ( two files , 100mb approx ): #CODE
Using a large file , the merge always set ` hour_y ` and ` phone_2_y ` as NaN , but is not true for all the records .
How does the merge fail exactly ?
You should rewrite your question as : Pandas merge fails on large dataframes
Pandas rolling sum with unevenly spaced index
Or could I resample ' week ' and set sales to zero for all missing rows ?
To clarify , you want the rolling sum on a shifted basis ?
You can use pivot to create a table which will auto-fill the missing values .
This works provided that there is at least one entry for each week in your original data , reindex can be used to ensure that there is a row in the table for every week .
But ` reindex ` is possible with integers .
Then df is grouped by column ` product ` and apply reindex by max values of index of each group .
I've run into a bit of a sticky problem with pandas merge functionality .
If I now want to left merge these based on column A in the dataframe and the index in the series , i.e. : #CODE
Do you need to use ` merge ` ?
Didn't realise merge needed 2 dataframes .
This is done with Pandas ` merge ` .
You can then apply it straightforwardly : #CODE
I have manage to take out the values I want by doing a simple merge which gives the values I want from table A , given references .
The strange thing is that when I apply this logic to a bigger table that im working on , I get a " True " for all boolean values , despite me having different " time " -columns and the same ID-number ?
OK I understand what you're after , you can't use ` isin ` like this because for your data set it returns the rows with values 201511 because this still satisfies the condition that ID is in [ 1 , 2 ] and Time1 and Time2 are also in those row values , you have to use merge to get row matches like you desire : ` df.merge ( df2 )`
I have dataframe with floats as data , and I'd like to normalize the data , so first I convert it into int ( otherwise I have error ` ValueError : Input contains NaN , infinity or a value too large for dtype ( ' float64 ') . `)
And onto that I would like to join #CODE
IIUC then you want to ` pivot ` ` Frame2 ` and ` merge ` this with ` Frame1 ` : #CODE
I can map it back I guess .
use ` str.replace ` to replace a specific character for a column : #CODE
IIUC then you ` groupby ` on ` level=0 ` of your index and ` apply ` a ` lambda ` to ` join ` the values : #CODE
Use ` nfl_frame [ ' Rank ']` instead , or strip the spaces off the column names .
If so , I will increment the number of events associated with a road and append it to a new column " num_events " for the roads dataframe .
Issue is with the way you join two data frame , you should do something thing like this , #CODE
In addition , we can use above method to join two data frames .
Firstly , you can create the column ` ID ` from your ` col1 ` , and then drop ` col1 ` .
I have a pandas dataframe that I groupby , and then perform an aggregate calculation to get the mean for : #CODE
You could use ` str.contains ` and sum over bool values in list comprehension of ` words ` #CODE
Then use regex operation ` exactmatch = re.compile ( r ' \b ( ? : %s ) \b ' % ' | ' .join ( word_list ))` then ` df.comment.map ( lambda x : re.findall ( exactmatch , x ))` returns lists of words join the list to count .
Regex are usually faster but I'm not sure when map it as function to pandas df ..
One way would be to use ` apply ` by constructing column name for each row based on year like `' w ' + str ( x.year )` .
you can rename columns names to be the same as year columns using replace #CODE
You can ` groupby ` on ` country ` and resample on week #CODE
As mentioned , i tried Resampling to weekly data and the output gave the similar one like above.But how can i access the ' Country ' and ' Date ' column , as this is a multiindex dataframe.I tried ** droplevel ( 0 ) ** to remove the first level , but it didn't work .
sorry to mention that I use python 2.7 , may u translate the code into 2.7 ?
I want to apply the condition to check the equality of a string .
How to align dfs in pandas
How to join the two so that I have the following : #CODE
So , first I need to align by file column ( and I can do this ) , and then if one line has an id I have to add it to the ID column , if it has more than one , add it to the first token and second one to the second token and so on .
Note that this will be a left join .
Putting how= ' right ' in the join will do it the other way .
I didn't know about the stack function .
Merge two rows in the same Dataframe if their index is the same ?
The construction of the dataframe wasn't simple as I had to do it in parts , using the concat function to add new columns to the data set as they were pulled from the database .
Is it possible for me to merge lines with the same index ?
I have searched online for solutions but I always come across examples trying to merge two separate dataframes instead of merging rows within the same dataframe .
You could just merge the result of the groupby sum with your original df but select the missing columns and then call drop_duplicates if required , can you post your actual data or representative data ?
aggregate or average the values across all rows
So you just want add hour component , increment it and drop the ` .0 ` correct ?
I have a SparseDataFrame with a multiindex on both the ` columns ` and ` index ` indices : #CODE
When I convert the DataFrame to a dense matrix the ` index ` multiindex names are kept intact : #CODE
However the ` columns ` multiindex names are lost and replaced with ` None ` #CODE
While keeping the multiindex structure .
pandas rolling functions with time groupby
The function could groupby the dataframe by the time unit such as ' D ' , ' M ' , ' Y ' , and then rolling the dataframe through the number before the time unit such as 1 , 3 ...
I need a rolling function such like this ?
I need to implement such a function which could calculate rolling standard deviation of every day , not resampled by month , but the window step unit is weighed by month .
However , no matter what how = ' xxx ' method in the resample function , the result is not right .
The standard deviation is one of my functions which I want to implement rolling ...
I think the ` freq ` param of ` rolling_mean ` is what you're after but you need a datetimeindex for this to work
I'd recommend using ` pandas ` , specifically the ` resample ` function : #CODE
Then resample over day-long periods , note this defaults to ' mean ' of the samples in each day : #CODE
I am trying to write some code that splits a string in a dataframe column at comma ( so it becomes a list ) and removes a certain string from that list if it is present . after removing the unwanted string I want to join the list elements again at comma .
simply you can apply the regex ` b ,?
` , which means replace any value of ` b ` and ` , ` found after the ` b ` if exists #CODE
I don't think you can do an ` agg ` and pass ` list ` as a function , in case you wanted to get count and list in the same op , you'd have to merge both results unfortunately
Merge values split into different columns
I took a look at both pandas functions , ` merge ` and ` join ` , but I could not find the right one for my case .
Is there a way to merge some columns by summing their values and not have to delete the remaining columns afterwards ?
I don't know how to avoid the ` drop ` though .
What about if the names of the column names differ at all and you want to merge them .
Padding python pivot tables with 0
I have a pivot table which has an index of dates ranging from 01-01-2014 to 12-31-2015 .
I would like the index to range from 01-01-2013 to 12-31-2016 and do not know how without modifying the underlying dataset by inserting a row in my pandas dataframe with those dates in the column I want to use as my index for the pivot table .
Could you provide the code how you generated the pivot table ?
At the end of this workflow , ` sales ` should include data from ` dt [ ' Sales ']` AND placeholders for dates that are not within the data frame .
Next , I would iterate through ` dates ` and check to see if each date is in ` dt [ ' Date ']` .
If ` date ` is in ` dt [ ' Date ']` , I would append the ` Sales ` data for that date into ` sales ` .
So , to append the corresponding ` Sales ` data into the list , I would use this command ` sales.append ( df [ df [ ' Date '] == date ] [ ' Sales ']` .
If ` date ` is NOT in ` dt [ ' Date ']` , I would append a placeholder into ` sales ` ( i.e. ` sales.append ( 0 )` .
How to merge two data frames based on nearest date
I want to merge two data frames based on two columns : " Code " and " Date " .
It is straightforward to merge data frames based on " Code " , however in case of " Date " it becomes tricky - there is no exact match between Dates in df1 and df2 .
Should I first use " searchsorted " and then put " mask = idx > = 0 & ...
call a standard merge on these
Now let's write an ` apply ` function that adds a column of nearest dates to ` df1 ` using scikit-learn : #CODE
Finally , we can merge these together with a straightforward call to ` pd.merge ` : #CODE
this will drop the rows where you have duplicate ID and Value input .
Next I apply data = ` np.asarray() ` on the DataFrame : #CODE
@USER , yes I know I'll have to specify the order manually , I'm just wondering how to apply those specified orders .
I'm trying to get weekly rolling technical indicators using pandas and talib .
By " weekly rolling " I mean that if for example today is thursday , then the ADX weekly value of today is going to be calculated using only this thursday , the previous thursday and so forth .
So question is , how can I join these calculations back to the original dataframe respecting the indexes of df ?
Had to transpose and then re-assign the original index .
To track the duplicates I use ` df [ ' duplicate '] = df.duplicated ( ' id ' , keep=False )` However , I would like to keep the ones with the highest ` value ` and either mark or drop the other duplicates .
In order to normalize data in a pandas DataFrame I wrote the following functions : #CODE
Why not drop them first ?
@USER I don't want to drop them ( yet ) because I'm also filtering on another column .
Only if this is False and another column is True will i want to drop them
How to merge rows with same index on a single data frame ?
You can ` groupby ` on ' A ' and ' C ' seeing as their relationship is the same , cast the ' B ' column to str and ` join ` with a comma : #CODE
Why not just read in a single line , get the list of columns and then drop the last entry ?
If you have multiple conditions besides this example you can use ` apply ` : #CODE
pandas : calculate over multiindex
You can use ` loc ` to select all rows that correspond to one label in the first level like this : #CODE
To answer my Question 1 , I was thinking about using the ' reindex ' and ' reset_index ' functions , however haven't managed to make them work .
Q1 : fill ` X ` using ` reindex ` , and others using ` fillna `
Probably you want to use a merge to bring in the column ` ben ` into your dataframe : #CODE
I tried using append , merge and concat and none worked and then tried simply using : #CODE
If the row counts differ , you'll need to use ` df.merge ` and let it know which columns it should be using to join the two frames .
I'm new to stack overflow , so how can I post the code in that format so I can show you what it looks like ?
Does the store_min_buy apply to just the one thing or across the sum of all the things you buy at this store ?
You can transform that to a legal constraint format the obvious way .
The linear phase will abuse that M as much as it can , and probably trivially satisfy the last two constraints by choosing ` u ` tiny but nonzero .
Python xray - is it possible to append a table ?
Xray doesn't have an append method because its data structures are built on top of NumPy's non-resizable arrays , so we cannot append new elements without copying the entire array .
Hence , we don't implement an ` append ` method .
For what it's worth , pandas has the same limitation : the ` append ` method does indeed copy entire dataframes each time it is used .
I suggest MultiIndex : #CODE
Now I use pandas.groupy to aggregate periods #CODE
I would like that better , because I would like to apply many different aggregation functions at once ( amin , amax , first , ... ) .
An alternative is to simply resample and use OHLC ` ( open=first , close=last , high=max , low=min )` #CODE
Pandas drop rare entries
Pandas - How to replace string with zero values in a DataFrame series ?
What's the best way to replace these " $- " strings with zeros ?
Or more generally , how can I replace all the strings in a series ( which is predominantly numerical ) , with a numerical value , and convert the series to a floating point type ?
You just need to apply an aggregation operation to the object to return a DataFrame or Series .
Group series using mapper ( dict or key function , apply given function
Use ` Series.value_counts ` to count the number of occurrences for each city in ` US [ ' city ']` , and then use ` Series.map ` to apply those counts to corresponding values in ` UK [ ' city ']` : #CODE
When I try to run the file with %run ( or by just selecting the file from a drop down menu ) , the function has the problem I described .
I posted on stack overflow a few days ago with a similar problem ( which was solved ) , and I'm not sure what the proper etiquette is here , but I'm making a new post .
I would want to append ` 0.813857557031 ` into an array ` res_gamma_init `
replace this #CODE
I have tried using ` apply ` but it is pretty slow : #CODE
` get_dummies ` and other Categorical operations don't apply because they operate on a per row level .
Since Hamming distance doesn't care about magnitude differences , I can get about a 40-60 % speedup just replacing ` df.apply ( lambda x : np.array ([ mapping [ char ] for char in x ]))` with ` df.apply ( lambda x : map ( ord , x ))` on made-up datasets .
This is a bit like a multi-column factorize : #URL
Python Pandas join aggregated tables
I'm trying to join two tables .
You need ` transform ` : #CODE
How to resample starting from the first element in pandas ?
I did the following to resample it into 100milliseconds bin time :
Pandas merge pivot_table and a dataframe
how to merge a pandas pivot table and a data frame where the combined column in pivot table is in index and in data frame is in column label
pivot table is #CODE
i had to merge both data frame using pd.merge ( df1 , df2 , on =[ ' bts_name '] , how= ' left ') but i am getting an error
Apply a function to each of the subsequent rows of the dataframe that will give the depreciation relative to the base-year , base-price .
What this really boiled down to was how to sequentially apply a function to rows of a dataframe .
Now I can apply a function : #CODE
So is there anyway that we can transform each row of pandas into an object of my datastructure .
So i want to transform each row into an object
I created this pivot table , but I need to figure out how to apply a function within each day on the ' is_match ' values only .
could post few records of original data frame before applying pivot .?
Reason is pivot you created made Date as columns so it makes harder to calculate the average for each date .
Using a function with ` apply ` is slower than the list comprehension : #CODE
But maybe it's possible to make it trough pandas apply / map function ?
You can use ` apply ` ( see updated answer above ) .
As far as I know , you would have to read the entire ` h5 ` file into a DataFrame ( or to do so in chunks using the ` chunksize ` parameter ) and then to write it out or append to a different ` h5 ` file in ` table ` format .
Off the top of my head , extract the format of the date , check against the format you want to keep , and drop all records from the dataframe or series that doesn't match your desired format .
If you do , try the [ ` dt ` accessor ] ( #URL ) .
python pandas - groupby unstack pivot - count len - mixed values and export
i'd tryed with groupby , unstake , pivot but nothing worked ...
For the ` aggfunc ` argument , use ` len ` .
however I want there to be an easy way to specify that I want to join ` df1.actorName ` and ` df2.directorName ` together , and ` actorID / directorID ` .
Rename column headers in both data frame and use ` append ` or ` concat ` :)
Yep , added a new option above that will apply the same logic to the entire df .
Take that back , no need for groupby , see my edit though , create dataframe of isnull and can avoid valid_dates part .
Can you give an example of the function and how to apply to the groupby ?
AttributeError : ' Series ' object has no attribute ' dt '
@USER : that ` replace ` method can only replace entire entries in a Series or DataFrame that match a particular value or pattern with a new value .
I used ` replace ` before , though not with regex .
I'd use the Pandas ` replace ` function to replace $ and ) by nothing , replace - by 0 , and then finally replace ( by - .
@USER That's exactly what I was looking for ( and it's faster ) , I didn't know the use of ` transform ` .
Huge sparse dataframe to scipy sparse matrix without dense transform
But for working with scikit algos and xgboost it's necessary transform dataframe to sparse matrix .
I tried df.as_matrix() and df.values , but all of first transform data to dense what arise MemoryError :(
I had to transform the test dataset values for sqft living from row to column .
I had to transform X_test using np.reshape .
How to get the union of two MultiIndex DataFrames ?
How do I merge two MultiIndexed DataFrames ?
I want to merge ` df1 ` and ` df2 ` to produce : #CODE
Pandas dataframe apply refer to previous row to calculate difference
I want to add a column diff which represents the time difference in days per player .
The first row has ` 0 ` for diff , because there is no earlier date .
I know , that I have to make a ` groupby ` first ( ` df.groupby ( ' player ')`) and then use ` apply ` ( or maybe ` transform ` ? ) .
( I've used the name " difference " instead of " diff " to distinguish the name from the method ` diff ` . )
this looks good , however , i get this error : ` File " " , line 17 , in diff / ValueError ` .
I get ` raise ValueError ( " cannot reindex from a duplicate axis ")` .
I suggest that you apply the function on a subset of data for example the first ` 100 ` row if it worked then increase the subset until you get the error and know which rows specifically in your data set causes the issue
can you post the complete error message and stack trace
You can get all the rows where ` 2 ` is in ` a ` using a map , e.g. #CODE
Once you have either of these results , you can use , e.g. ` result [ ' a '] = 2 ` to replace the values in the ` a ` column .
After we used ` resample ` #CODE
@USER the ` resample ` lets you do much more than that .
use of apply function when you need to pass ' self ' as argument
I need to do a merge on Timestamps columns but the behaviour depends on wether the timezone is set or not .
w / o tz #CODE
with tz #CODE
How to resample pandas series monthy ( not end , not start ) ?
I know how to resample a dataframe but feel frustrated by rule parameter not accepting DateOffset ( months=1 ) .
How can I resample with monthly frequency but from any give date .
I'm still learning pandas and must have missed the stack / unstack .
python - pass dataframe column as argument in apply function
You have to ` apply ` over the other axis .
Another way you could do this is by using a set intersection to calculate the size .
I tried apply command , but I think I do not know how to use it properly .
Or you can apply a ` lambda ` function onto the groups : #CODE
Because if there is no data in the columns , Pandas will insert NaN there .
If that is equal to 10 , just map true / false values to your desired 0 / 1 pairs : #CODE
Does somebody know what I have to do to replace my ' substring ' ?
I didn't understand the relation between your data and your line that is supposed to replace the substring
` numpy.diff ` may be useful here : calculate the diff per column , and where the diff !
( Perhaps there's a ` diff ` function / method in Pandas as well , but I'm more familiar with numpy . )
IIUC you can use ` diff ` : #CODE
So this calls ` diff ` to subtract the rows against the previous rows and filters them where this diff is ` 0 `
The output from ` diff ` : #CODE
I was unaware of the shift function .
AttributeError : map object has no attribute ' lower ' #CODE
Also , the ` AttributeError : map object has no attribute ' lower '` tells you that you're dealing with a ` map ` object and not a ` string ` object as you assumed you had .
Edit : Sorry for Unclear Question ... the merge criterion would be that for every ` data_lvl1_TIMESTAMP ` file there is a ` spektrum_TIMESTAMP ` file in the folder .
The Timesstamp ist ` %Y%m%d ` . both files have rows for every minute and I want to merge the data rows .
What exactly is the merge criterion , please ?
You can use merge and if you want remove columns with ` NaN ` , use function dropna .
pandas merge dataframe based on same value in columns
i want to join the 2 dataframe in a way to produce result as below .
i have try pandas merge which can base on one same name column .
Use ` merge ` and pass the columns to merge on for ` left_param ` and ` right_param ` respectively : #CODE
i am google-ing the pandas merge details , but if u can enlighten me abit more . thank you .
Take last value of the prior grouping in a rolling sum function ?
I am trying to write a function that will sum / average according to specific indexing over a rolling window .
For example , to calculate the rolling sum for the period , #CODE
My solution was to use an expanding window , groupby level 0 , then take the tail and sum : #CODE
One way could have been to use pd.exanding_apply() , but it doesn't preserve the dataframe to apply the function on , so there is no way to have the correct groupyby index ..
I have a ` pandas.Series ` of datetime and need to replace the tzinfo for every element in it .
I know how to do it using ` apply ` with python function but it's very slow : ~16s for 1M elements on a MacBookPro #CODE
pandas drop row below each row containing an ' na '
How do I drop all these rows ?
IIUC correctly you can use ` notnull ` with ` all ` to mask off any rows with ` NaN ` and any rows that follow ` NaN ` rows : #CODE
Can you not drop the dups before writing to db ?
` df.drop_duplicates() ` you can optionally pass your ID column ` df.drop_duplicates ( ' ID ')` , it unclear what you're asking specifically , are you wanting to insert all rows and ensire ` ID ` is unique or you actually want to drop duplicate rows ?
STEP 5 : convert the spark dataframe into a pandas dataframe and replace any Nulls by 0 ( with the fillna ( 0 )) #CODE
Or rolling mean per each type ( for me it looks like more sophisticated task , but might be it's easy to implement ) .
Assuming your dataframe is sorted by ` date ` , you can write a aggregate function to do this : #CODE
Pandas merge over missing values
merge on ' a ' and ' b ' will result into data frame containing only index 2 .
Sorry can't you drop the ` NaN ` values prior to merging ?
how to unstack a pandas dataframe with two sets of variables
For each run , it will append all ' n ' rows below the columns to the clean DataFrame to give you a consistent result .
The idea is to do two melts , one for each set of columns , then concat them .
@USER : I think ` merge ` is somewhat more flexible and allows different kinds of joins .
` concat ` is just for piling up multiple DFs next to each other .
Preserve None values when using pandas apply
I'm trying to speed up the creation of ` c3 ` , since there is a lot of data , by using ` apply ` .
Is there any way to tell ` apply ` that I want to preserve the ` None ` values ?
Your apply function is weird because you don't use ` x ` , instead you extract the two whole columns of your dataframe on each row .
Apply time shift on Pandas DataFrame from another column
Do you have an idea how I can drop NaN values in that conversion ?
How do I use pandas groupby function to apply a formula based on the groupby value
You could also create a special function and pass it to the groupby ` apply ` method : #CODE
The problem with using ` agg ` is that the dict of functions are column-specific where here you need to combine two columns .
Writing a named funtion and using ` apply ` works : #CODE
How do I merge together two columns from two dataframes in Python
I guess it sorts all the dfs indices so that it's easier to align them on index , there is probably not much call for preserving the index order when performing this operation
This seems to be by design , the only thing I'd suggest is to call ` reindex ` on the concatenated df and pass the index of ` df ` : #CODE
I want to normalize these values by stripping spaces and uppercasing them .
Thus , you need to join the path and filename together to get the absolute path for each file .
Perhaps using applymap or map ?
Now i want merge two data frames in such way that the second time stamp is always closer or equal to the first timestamp
If you set these as ` DatetimeIndex ` s can then use ` reindex ` with the ' nearest ' method : #CODE
Then add these columns / join back to df1 .
I want to truncate the date so that i have only last monday of each timestamp #CODE
Whenever I merge / concat / join , I get NaN instead of the right data .
Whenever I merge / concat / join , I get NaN instead of the right
and after that apply #CODE
Speeding up a grouby sum function on a custom rolling time interval window in Pandas Python ?
I am trying to calculate the rolling sum of values according to a multindex over a predefined interval such as year ( basically , a rolling annual sum ) .
Pandas groupby apply performing slow
The bottleneck seems to be the apply function , even when I remove the for loop in the function it remains slow ( ~ 4.25s per loop ) .
I am wondering if there is another way to apply the function ( without the apply command ) .
I perform some other procedures on the data in this code using the agg command .
This works much faster , but I don't know if its possible to perform this check ( full_coverage ) using the agg command .
So far I tried to mask the dataframe as : #CODE
But since I have to apply several filters keeping the original ` H ` column for non-filtered values I'm getting confused .
If I understand correctly he wants to drop columns that have ` NaN ` in them .
before reset_index it's a Series and the index is a MultiIndex ( of age and name ) .
How to change order of plots in pandas hist command
If you have a ` groupby ` object , you should use the ` apply ` , ` agg ` , ` filter ` or ` transform ` methods .
In your case ` apply ` is appropriate .
Now , let's ` apply ` that to each group of your real dataframe : #CODE
You apply the function ` fill_seq ` to the H / K sequence columns using the values from H / K sequence as input .
However , if you had to consider multiply columns see my code with boolean mask .
Pandas DataFrame with MultiIndex to Numpy Matrix
( MultiIndex ) I want to get out a Numpy Matrix with something like ` df.as_matrix ( ... )` but this matrix has shape ` ( n_rows , 1 )` .
I think what you want is to unstack the multiindex , e.g. #CODE
Here's a full example of ` unstack ` .
Now we unstack the multiindex : we'll show the first 4x4 slice of the data : #CODE
I get ` ValueError : Index contains duplicate entries , cannot reshape ` when trying to unstack .
Id have to unstack millions of indexes .
In that case , you need to do some sort of aggregation to get the result you want ( and you'll have to decide which aggregation is appropriate for your data ) A pivot table would be a good approach : see my edit above .
You can call ` apply ` on the df pass ` axis=1 ` to apply row-wise and use the column values to slice the str : #CODE
Make a list of list of your desired row values and construct a df and ` concat ` them : #CODE
You'd have to construct a new df and merge on the index to do that and I advise against it for any large dfs , besides adding a row at a time is non-performant .
It's better to construct the entire df and concat them as I've shown
How do I replace all the 1s in all the columns except the ' ratings ' column with the corresponding ' ratings ' column value ?
How to aggregate some rows based on their column value in Python
I wan to aggregate the values in in some columns if they share the same value in a specific column of the data frame ?
The ` anagram_list ` column contains tuples , which is not supported by sqlite to insert in a dataframe .
The ` anagram_list ` column contains tuples , which is not supported by sqlite to insert in a dataframe .
So what I want to do is , to align the two dataframes based on count .
You then merge these counts to your original dataframe and do some formatting operations to get the data in the desired shape .
Based around google searching , I have tried to ` resample ` the data using this code ` df.resample ( ' 5min ' , how=sum )` Here I get the error ` TypeError : Only valid with DatetimeIndex , TimedeltaIndex or PeriodIndex ` .
You should then be able to resample .
Then you can set the index and resample as above .
While in this case we have ` notnull ` , ` ~ ` can come in handy in situations where there's no special opposite method .
How to apply a function on every row on a dataframe ?
Now I want to apply the formula to every row on the dataframe and return it as an extra row ' Q ' .
( returns only ' map ' types )
Getting Data of a boxplot - Pandas
I tried the following query to draw the boxplot .
Following are the median values but it repeated . why is that ?
However for my issue I could sort and drop duplicates and reindex , which performs a lot better .
the w variable will not surpass len ( seq ) .
For example instead of looping trough every element in a numpy array to do some processing you can apply a numpy function directly on the array and get the results in seconds rather than hours . as an example : #CODE
Computing ` len ( seq )` inside the loop is not necessary , since its value is not changing .
You don't really need the ` if ` statement , since in your code it always evaluate to true ( ` w in range ( len ( seq ))` means ` w ` maximium value will be ` len ( seq ) -1 `) .
the ` f ` function is more complicated that this one , of course , and I want to apply a sequence of functions to transform the data frame .
There are basically string parsers to normalize , transform to unicode , remove characters , split into components , etc , so so far I can t see a way to do it without ` applymap ` .
Can you give more insight in what you want to apply ?
output from the merge : #CODE
you can use the left merge to obtain values that exist in both frames ` + ` values that exist in the first data frame only #CODE
It would be better if it could be wrapped into , say , a list comprehension ( or apply function ) since each column is independent
I am trying to append 12 files into a single file .
I loaded all the 12 files into dataframe and append it into an empty dataframe one by one in a loop .
Since you know there will only be five elements you can make a tmp list initialized with 5 zeros and just replace the ones that are non-zero
use ` str.contains ` and join the keys using ` | ` to create a regex pattern and negate the boolean mask ` ~ ` to filter your df : #CODE
In the event that you had unique column and row labels , you could use a combination of ` melt ` with ` pivot ` like so : #CODE
` pivot ` gets us the rest of the way , but will not work in this case because the headers are not unique ( again , the issue with ` Subject 2 ` and ` test C `) .
Are you need to replace your hours minutes and seconds from original to 13:00 : 00 ?
Use replace method of pandas timestamp objects : #CODE
You could , for example , for each column : copy the series ; count the ` NaN ` s ; strip the ` NaN ` s keeping the values in order , append the same number of ` NaN ` s at the end .
If you also want to extract correlation values from the DataFrame , replace ` .tolist() ` by ` .to_dict() ` .
However , then you will have to be careful to append new data coming from the next chunk to ` pairs ` .
You can mask out the off upper-triangular values with ` np.triu ` : #CODE
You can use the same trick in pandas but with stack to get the index / columns natively : #CODE
if i want to insert that ' columnF ' from df1 to the column after ' columnA ' in df , would i do something like ` df [ ' columnA ' +1 ] = df [ ' columnF ']` ?
You'll need ` insert ` for that .
I have recently upgraded to pandas ` 0.17.0 ` ( numpy : ` 1.10.1 ` , Python : 2.7.10 ) and I receive the warning shown in the block below when i try to reshape a dataframe with an unstack operation .
A similar unstack exists somewhere in the code of a bigger application written with the Pycharm editor .
The result of the unstack call looks to be fine , its just the warning that puzzles me and whether it is expected behaviour .
Also passing and argument when unstack is called looks to prevent the warning from appearing .
Since you have just mentioned a specific condition , the answer is accordingly framed to align with your requirements .
If you want in different column I would suggest to do transpose , #CODE
The problem with ` reindex ` is it'll use the existing index and column values of your df so your passed in new column values don't exist hence why you get all ` NaN ` s
this error indicates that there is no argument with name ` inplace ` for the method ` drop ` which of course is not the case , I'm not sure about this , are you sure you followed the same steps ?
The only way I found so far is to transform it into a dictionnary so I have : #CODE
There is a limited amount of manipulation needed to the data ( sql joins to merge linked tables and create properties , change names of some properties , deal with empty columns etc ) .
I wouldn't worry about the performance of operations like ` join ` inside the database versus in memory at the scale you're talking about .
I'd like to mask elements in a DataFrame / Series that are greater / less than elements in another DataFrame / Series .
For instance , the following doesn't replace elements greater than the mean
I'm able to work around this problem by using transpose : #CODE
I thought there would be this kind of functions but I couldn't find by googling and I must say the docstrings of ` gt ` and ` lt ` aren't very helpful ..
As pointed out by @USER , there a potential workaround , that is to not just append the groupby results to a new csv and read that back in and perform the aggregation again until the df size doesn't change .
Could you not just append the groupby results to a new csv and read that back in and perform the aggregation again until the df size doesn't change ?
Then read csv by chunks , concat chunks by subset of id and groupby . better explain .
But I don't want to drop any of the columns , as the results of the sum aggregation will be altered if I do so .
you drop some columns only for choose constants .
I have pretty big dataframe in pandas and want to transform all index rows into column headers und all column headers into index rows .
Are you wanting to transpose or rename ?
transpose .
Just transpose it using ` .T ` : #CODE
If this isn't the case , you may need to pass an explicit ordering to the ` reindex ` method .
Color seaborn boxplot based in DataFrame column name
The column names contain strings that indicate an experimental condition based on which I want the box of the boxplot colored .
Now I want to give every boxplot that has ' prog + , DMSO+ ' in its name a red color , leaving the rest as blue .
If I could somehow put these boxplot nicely in one plot below each other I'd be almost at what I want .
Performance implications of using an outer merge as opposed to concat / append
I have noticed that our system has been using outer merges as opposed to using concat / merge to combine data frames .
However if I uncomment the line ` df.Date = pd.to_datetime ( df.Date )` to do the date type conversion and run it again it will drop me the overflow error mentioned in the title of this post .
Python pandas groupby object apply method adds index
I have this question is an extension after reading the " Python pandas groupby object apply method duplicates first group " .
reindex ( remapping ) column's value
I'd like to divide them , index by index , without have to interpolate .
Actually , I have data for several days , and interpolate one by one is a dificult option .
Maybe using ` apply ` but I dont know how .
If I take the mean across my columns first and resample second , then the columns with values will be weighted .
If I resample first and then take the mean across columns , then certain timesteps will be weighted ( for example : if there is only one value for a whole day that will be taken as the mean of that day ) .
Why can't you insert these two lines above ` df.to_csv() ` ?
If so , Redshift's copy will insert the arbitrary int as that record's field value .
If it is like ' \N ' in SQL , you can convert the column to varchar and replace all null values with ' \\N ' ...
For one thing this is very inefficient ( O ( n ) for each append so O ( n^2 ) in total .
The preferred way is to concat some objects together in one pass .
Finally , append doesn't act in place , so the variable ` dataf ` is discarded once the callback is finished !! and no changes are made to the parent ` dataf ` .
You can concat these ( and ` ignore_index `) to get the desired result : #CODE
` .groups ` will return the groups for a ` groupby ` object , you can then find the intersection of these and then get the specific group calling ` .get_group `
You can get the groups from a ` groupby ` object using ` .groups ` this returns a dict , the keys are the group values , you can then find the common columns using ` set ` and ` intersection ` and then get the common groups using ` .get_group ` : #CODE
` for i in range ( 1 , len ( fi.index )):
I'm starting to have some success using this code ` for i in range ( 1 , len ( fi.index )):
The solution will then be extracting the index , from ` between_time ` and using that to append into DF1 directly .
I use twice function ` merge ` .
thanks for the solution , i'm just trying to make it work , when you drop the index the time column reverts to a pandas object rather than ` datetime64 ` , so ` timedelta ` doesn't work .
One issue i've found is using the merge function on ` i ` takes a very long time and lot's of memory .
Can you suggest an alternate joining method that would create the same join ?
I've tried ` concat ` and ` append ` .
I changed the above code to merge on the ID field and it is a million times faster ... roughly .
How to calculate the count of column values less than 95 on each row on pandas pivot table
I am new to pandas pivot tables , how to get the count of column values less than 95 for a row on pandas pivot table #CODE
Just compare the entire df , this will produce a boolean mask against the whole array , you can then ` sum ` row-wise , this will convert ` True ` to ` 1 ` and ` False ` to ` 0 ` : #CODE
Here is what the boolean mask looks like : #CODE
Also , ` drop ` can be configured to drop columns which may or may not be there by passing ` errors= ' ignore '`
I can transpose the file with " bash+grep+awk " and then read it as csv but it's not practical for users who has only Python and Windows .
This looks like something you might do with cumulative sums ` w.cumsum() ` might give you something you can just multiply by the range you're trying to interpolate .
Here's an example of weighted interpolate with only ufuncs ( eg numpy functions ) .
Below a plot of the known points , the linear interpolate and the weighted interpolation
You should accept my answer and ask this as another #URL That said I think unstack that level first , no need for groupby etc .
You can use ` apply ` on the column to generate a boolean mask describing the desired columns , and then filter the DataFrame by this mask : #CODE
What I've got so far is printing out the correct expanded dataframes for each row , but I don't know how to consolidate the results of apply : #CODE
You could , of course , do all this within a function that you apply on a ` groupby ` , but it would be superfluous in this case .
I don't really get stack and unstack yet , so I guess this will be a good opportunity to dig in a grok it .
Then I removed it and got an error : "' Series ' object has no attribute ' stack '"
You can use a mask on the date , e.g. #CODE
python pandas nested loop : to apply a function to each element of e.g. column 2 involving compounding same elements in previous column 1s
I tried to do this by using shift ( bins ) , e.g. shift ( 10 ) , but the problem is that this is not cumulative , i.e. I need to shift by 10 every result as its a cumulative product .
In fact , it seems to me , that one could apply a similar method to calculations involving n nested loops .
As long as one has pre-calculated each of the n loops , and ordered them using sort , one can apply a function to the result by using groupby on the nth bin directly .
I feel like ` df.pct_change() ` would be helpful , but I can't figure out how to apply it in the way I'm trying to describe .
I always use ` ser.plot ( xticks=range ( 0 , len ( ser ) , step ) , ..
or apply the patch in the pandas code ( see the link above , it's only a one line change )
You can use the shift function for create new columns and after you can compare them to the initial columns .
I have seen here a similar question for scatterplots , but it doesn't seem I can apply the same solution to a time series line chart .
How do I apply PCA on this sparse matrix to reduce dimensionality ?
Tried doing by creating mask #CODE
How can I replace values in the datatable ` data ` with information in ` filllist ` if a value is in ` varlist ` ?
Use the ` replace ` method of the dataframe .
replace method documentation
I think it'll be easier to reindex after deleting the cols : ` df.reindex ( columns =[ " name " , " a " , " b " , " c " , " department " , " d " , " e " , " f " , " date " , " g " , " h "]) .to_csv ( ' outfile.csv ' , index=False , header=False )`
It'll be easier in my opinion to ` reindex ` your df , this will put the cols in the order you desire and where columns don't exist put ` NaN ` values there : #CODE
How can I do this without using groupby and apply function ?
The normal op here is to ` groupby ` on ' item ' and call ` sum ` on the ' grade ' column no need to call ` apply ` here
Note that if you have to perform ` urljoin ` then using ` map ` or ` apply ` would be fine here
This will be much quicker than calling ` map `
For pure string concatenation , this will be vectorised , using ` map ` and ` apply ` this is just a ` for ` loop so this approach will be much faster for large datasets
For instance if I only want to replace if in the last two years and in the next two years they are the same .
You can use function merge on column ` L ` .
However , I would like to replace them with actual string names .
Then simply use ` map ` .
Python-pandas Replace NA with the median or mean of a group in dataframe
to replace the NA , we can use df [ " B "] .fillna ( df [ " B "] .median() ) , but it will fill NA with the median of all data in " B "
Is there any way that we can use the median of a certain A to replace the NA ( like below ): #CODE
` with ( dd , ifelse ( is.na ( B ) , ave ( B , A , FUN = function ( x ) median ( x , na.rm = TRUE )) , B ))`
In ` R ` , can use ` na.aggregate / data.table ` to replace the ` NA ` by ` mean ` value of the group .
We convert the ' data.frame ' to ' data.table ' ( ` setDT ( df )`) , grouped by ' A ' , apply the ` na.aggregate ` on ' B ' .
In pandas you may use ` transform ` to obtain null-fill values : #CODE
However I am wondering if there is a way I can write my function and apply it to my grouped object such that I can specify when applying it , which column I want to calculate for ( or both ) .
You can apply and return both averages : #CODE
Pandas dataframe : how to apply describe() to each group and add to new columns ?
You could try to merge these two dataframes , then drop DPAvPos and rename DKP / Game as DPAvPos : #CODE
Let me explain the question briefly , I wanted to resample the following data : #CODE
And I wanted to resample it into 100L bin time .
Note : resample without a DatetimeIndex / PeriodIndex / TimedeltaIndex raises an error in recent pandas , so you should convert to DatetimeIndex before doing this .
I have tried using the cut function in pandas but the results are not promising .
I'd put these in another dict of ( course , semester , section ) -> DataFrame and then concat and work out where to go from the big MultiIndex dataFrame ...
How do I concat a list of columns to a DataFrame .
Another option is to just concat the entire dict : #CODE
Using yours , I was able to get frames , but using concat isn't exactly what I want to do .
Then , shifting this ` xy ` column by 1 ( use ` shift `) , get a new column ` xyshift ` .
Then apply a difference function in the row-axis in this ` xyshift ` column ; and finally merge it iteratively into your dataframe or compose a new one
you can calculate the difference between ` x ` and ` y ` by using the ` diff ` function which will produce ` na ` for the first columns but you can drop it easily using ` dropna ` function .
plot line over boxplot using pandas DateFrame
I'm trying plot a boxplot and a line with 2015 values in same fig in order to compare , using this code : #CODE
In 2015 column I have data from January until September , but I get a shift line plot , from yline until september :
df.plot starts the labelling at 0 and boxplot at 1 I think ( which is annoying especially when the index is the same ) .
I've been trying to replace missing values in a Pandas dataframe , but without success .
I created boolean mask and return subset with ` NaN ` values .
If I drop columns using the label , all columns headed by that label are dropped : #CODE
For now , I'd like to drop all but the first column in the group ( I stated this in the OP ) .
This does do what I specified ( drop all but the first column in each group ) .
It seems there should be some way to drop a column by specifying its ( numeric ) index , and not have all the like-labelled columns be dropped .
actually the drop function doesn't take index as a parameter , I tried to pass ` int ` to ` drop ` but it did nothing , I'm not sure why do you prefer to drop column by it's index
It's not that I prefer to use the index , I just want to be able to say " drop the third column " rather than " drop all the columns with the label on the third column " .
Pandas rename / transpose column using value from another column
Move the index level values into a column index using ` unstack ` .
Note that you may prefer to skip that step , as the MultiIndex provides richer structure which may be useful at later stages if you need to select columns based on ` appName ` or ` count ` or ` RT ` .
The ` Id ` column was also placed in the index to " protect " it from getting split up by the ` unstack ` operation .
There should be a total of 3 block|concentration|name combos that are the same , so I need to assign ' 1 , 2 , 3 ' to each to separate them for later when I use pivot table .
Instead of function ` cumcount() +1 ` can be used rolling count with ` moving window=3 ` : #CODE
I am trying to connect to a local mariadb database and insert some data into a table .
My code to insert the dataframe into the database is : #CODE
How do I tell pandas python to insert into every other field in the table except that one ( because I want it to auto increment ?
You can just have a dataframe without this stock_id columns ( but with all others ) and append this .
pandas pivot table descending order python
then i organized everthing with pivot table #CODE
the Pivot table looks like this ( the actual table's format is a little bit different than this , i typed the table in manually here : #CODE
I tried but i got error , i do'nt think i can do that to a pivot table .
i want my pivot table output for ' Conc ' in descending order .
@USER : were you able to pivot your table ?
yes , the table above is from my pivot table , i dont know how to copy paste table here , so i typed it in manually , that's why the format looks a bit different
The output of your pivot_table is a MultiIndex .
How to plot aggregate results after groupby in Pandas ?
I've recently started learning Pandas and I'm having some trouble on how to plot results after using groupby and agg .
Then I selected one specific column ( ' results ') from the group to calculate the sem and mean .
is there anyway to generate a sparse vector row by row and stack then together ?
pandas map function returning ' NaN '
I have manually added a ' sex ' column onto the DataFrame , and I am trying to replace ' Male ' with 0 and ' Female ' with 1 however it does not seem to work .
Finding the median of a histogram
I want to find the median value in the histogram , but I am unsure how .
The median should be where half of the area of the histogram lies to the left , and half of the area to the right .
and the code I am using to find the median is this : #CODE
and the median value is this :
Aren't you simply calculating the median of the bin edges and discard the information about the histogram ?
Again , I want to find the median where half the area lies to the right and half to the left .
2 ) Iteratively sum the normalized counts until you have accumulated more than 0.5 , return the average between the last and this value as approximation to the median .
What you are looking for is the " weighted median " since you have the summarized distribution with frequencies .
And the correct way to find the median is to take the cumulative frequencies .
The interval where the cumulative frequency exceeds %50 of the total contains the median ( i.e. go from the minimum value to the maximum by summing the frequencies ) .
I understand , I think I understand what you were hinting at , find the cumsum and then write a function to iterate through the value column until .51 of the cum sum is met and that should be approximately the median
np.average has a weighted argument , I don't think there's an equivalent for median #URL
I think , your need add reindex function with list ` data2 ` to the end of your script and then fill missing data ` NaN ` to ` 1 ` .
- to replace all values in label column with 0 : num_class index ( a with 0 , b with 1 , c with 2 etc )
For simple conversion you can use map method on your data frame , #CODE
Here's a solution with apply #CODE
You can use ` regex ` and strip first and last spaces by ` strip ` : #CODE
Or regex without strip : #CODE
if you put \s * outside both ends of the group there's no need to strip .
Use the vectorised ` str.split ` this will be much faster than using ` apply ` on a large dataset : #CODE
How to maintain Pandas DataFrame index order when using stack / unstack ?
After using the ` stack ` and ` unstack ` methods on the given ` df ` DataFrame object , the index is automatically sorted lexicographically ( alphabetically ) so that one loses the original order of the rows .
Is it possible to maintain the original ordering after the ` unstack / stack ` operations above ?
Notice that the stack and unstack methods implicitly sort the index levels involved .
Hence a call to stack and then unstack , or viceversa , will result in a sorted copy of the original DataFrame or Series
you can keep a copy of the original index and reindex to that ... otherwise I don't think so .
You can keep a copy of the original ` index ` and reindex to that , thanks Andy Hayden .
I'm trying to replace and add some values in pandas dataframe object .
How can I find the cell values in column A which contains ' - ' sign , replace this value with '' and add for these values a trailing ' _0 ' ?
I tried to replace empty string with ` np.nan `
If you try to replace `''` with ` np.nan ` in the above example , you get the desired result : #CODE
It looks all you're doing is grouping on email addresses then calculating ` diff ` on those groups and then assigning a count if the diff is larger than ` 10 ` correct ?
How to efficiently map entries in a dataframe to dictionary
I have tried to conform to best practices when it comes to posting a queston on here .
I've tried to merge and concatenate them but no joy .
how to design agg funtion for pandas groupby
How should I write a ` agg ` function to complete this task ?
If anyone could explain what the ` loc ` does , that would be a lot of help as well .
Then glue them up with a concat with axis=1 ( i.e. add as more columns rather add as more rows ) .
Aside : it would be nice if concat didn't require that the column name be passed in explicitly ( as the key kwarg ) when they exist ...
You can map the columns to det1 , det2 , etc . before doing the concat , for example if you had the mapping as a dictionary .
@USER If that doesn't help for the final join / concat aspect you might get more activity via asking a new question about that specific part ( otherwise I'll try and answer next time I'm online ) .
@USER but you don't have to do that if you use exclusively crosstab and concat .
how to group and pivot the table ?
how to merge the table ?
what is ` loc ` doing ?
` concat `
` merge `
` loc `
first create the df using ` from_dict ` , then call ` stack ` and ` reset_index ` to get the shape you desire , you then need to rename the cols , sort and reset the index : #CODE
ensure the array is unque to start and mask out values which are equal ( to themselves / maybe add them back in as a special case ) ?
I have tried using string functions rsplit and replace in a new column creation statement , but I get an error that the string function is expecting a string and is receiving a Series .
To extract just the month-year piece from the resulting list created by the split , apply ` map ` and a lambda to the result : #CODE
Or as @USER suggests apply ` str ` again instead of the map-lambda : #CODE
Instead of the map / lambda , you could use ` .str [ 0 ]` directly .
I will suggest you create a new column of ' Type ' and then use ` pivot ` : #CODE
However when I use TimeGrouper and set freq to be " 2D " : #CODE
For simple case like this , ` shift ` will suffice : #CODE
More generally , it is a case of ` rolling ` apply , ` min_periods ` control the minimal window that will be considered as valid .
Thanks CT , rolling is exactly what I want
I have a few values with Timestamps in my Data Frame , and I want to aggregate the values into a single value with the sum of all values and the weighted mean of the appropriate Timestamps
And I want to aggregate the data using the ID index .
Pandas dataframe - transform column values into individual columns
IIUC you want to ` pivot ` : #CODE
` pivot ` doesn't support multi-index df's which was one method I was considering , what you could do is add a new column which is a composite of the 2 columns and use this as the index to ` pivot ` on : #CODE
You can also use multi Index and unstack like this : #CODE
This where I like to use multi-level indexes and stack / unstack .
I load the html tabular data into a dataframe and am now trying to replace the blank spaces with underscores .
it does return a list of data frames so you just need to iterate through the dataframes and call replace on your iterations
Is unnecessary as diff is unchanged ( since it's already unique via the previous groupby ) .
I would be tempted to unstack here , as the data is really two dimensional : #CODE
Python pandas asof join on groups
I would like to run asof join of Y on X , i.e. we take the prevailing price of Y each time we see an update on X .
asof is a Series method , rather than a DataFrame one .
When you do the apply , it's across each row ( which is a Series ) .
But how would I get the as of join on the entire data frame .
Also , why would Y_asof = Y.apply ( lambda x : x.asof ( X.index )) work if asof is method on pd.Series only .
I believe pandas throws an error because ` Y.groupby ( ' Date ')` creates a ` GroupBy ` object which does not have the method ` asof ` .
I'm guessing the slow part is the apply ( rather than the split , or the stack ) ?
@USER how would you apply the answer in your link to my question ?
There is a built-in method for this ` clip ` : #CODE
You can do that but to me this defeats the whole point of using pandas which provides vectorised methods , if you're going to do that then use ` apply ` to process an element
struggling to get to grips with update / merge / concat etc to achieve what I thought was a simple merge or update procedure .
I have df ( recorded values ) and df2 ( times series with 0s ) , and would like to update / replace the values in df2 with the corresponding recorded values of df , matching on date_time .
Tried so far : Re-indexing to Date_time and df2.update ( df ) with variations , multiple merging / join / concat variations , an adapted definition ( below ) with apply ... and now wondering if I need to use iterrows ( see below ? ) .
Thought this would work as well , but it results in ( ValueError : cannot reindex from a duplicate axis ) #CODE
I have tried simpler approaches , even just making a df [ ' newcol '] and try to strip the left four characters from date . like df [ ' newcol '] = df [ ' sqldate '] [ 0:4 ] but that fails .
I want to iterate over first multiindex dataframe , and every time add value in created column [ ' session '] corresponding to interval in the second dataframe to get this result : #CODE
i'm juggling with periodindex and merge method ,
Why Join key is lost if is the same rule i've used to make the merge ?
This looks like a bug with ` reset_index ` if you do this it returns ` False ` for all index values : ` df2.reset_index() [ ' index '] == df2.index ` it would be better to keep the comparisons between the same type e.g. merge both sides on ` Index ` objects or on ` Series ` , note that technically the second merge is still correct as you've reset the index so the index becomes a default ` Int64Index `
I tried to replace ` idx ` values with ` numbers ` and ` strings ` and it worked fine with both , so it turns out that problem is with ` PeriodIndex `
@USER : with reset_index i've " defaulted " an input dataframe , but in all cases the output dataframe must have the index defined in the join rules .
I don't understand what ` but in all cases the output dataframe must have the index defined in the join rules .
you can join on index or column , it unnecessary to specify as the default is ` False ` for left and right indices
Merge dataframes without header
I want to merge them based on Col_A in df_A and 1st column in df_B .
A simple merge would have been : #CODE
TO CLARIFY : I want to merge two dataframes .
How do i specify the join column on a dataframe that does not have a header ?
Well it looks like you want to merge like so : ` df_A.merge ( df_B , left_on= ' Col_A ' , right_on= ' 23 ')`
Well you've answered my question then , you've either imported the file incorrectly or you'll have to merge using my code
I want to merge two dataframes .
How do i specify the join column on a dataframe that does not have a header ?
Then you can merge dataframes .
If you ensure the ' years ' are different columns ( eg with ` unstack `) , then you can specify ` stacked=True ` to say that the different columns should be stacked : #CODE
How to apply a function to rows of two pandas DataFrame
I want to apply a function to the corresponding rows of these two DataFrame .
Then I do not know how to apply ` agg ` in the proper way .
how to replace the header row when reading from Excel- Python
I then try to merge this back to the group ( basically I want to know what each name / number correspond to ) #CODE
@ user3120266 Do you want to concatenate ` Billing ` + ` len ( Billing )` ?
You can find more details in pandas concat API documentation
also you can get the number of characters in any word easily by using the ` len ` function ` df.Group.str.len() ` #CODE
I need to aggregate the data into Quarterly and Annual series , but I do not want to report data for quarters / years with missing data .
You could create a boolean mask which is True for each group which has exactly 3 elements : #CODE
I want to replace the missing values ( indicated by -999 ) with the mean of that column taken over non-missing values .
I don't think you would have to worry anyways though , I believe ` a.index ` converts your dataframe to ` Ints ` so ` drop ( 6 )` should always work .
Instead , I tried counting unique days in the timestamp series using ` map ` and ` apply ` , and both take a considerable amount of time for 3,000,000 rows : #CODE
To get the unique dates you should first ` normalize ` ( to get the time at midnight that day , note this is fast ) , then use ` unique ` : #CODE
To get the counts could use ` normalize ` and then use ` value_counts ` : #CODE
but perhaps the cleaner option is to resample ( though I'm not sure if this is less efficient ): #CODE
` %timeit len ( df [ ' TIMESTAMP '] .dt .normalize() .unique() )` gives me 281 ms , 148 times faster than the map approach !
I would like to mergeone data frame twice to another data frame , first using one column and then another for the left data frame in the merge .
If you pass the ` keys ` parameter to ` concat ` , the columns of the resulting dataframe will be comprised of a multi-index which keeps track of the original dataframes : #CODE
Since the underlying arrays now have the same shape , you can now use ` == ` to broadcast your comparison and use this as a mask to return all matching values : #CODE
If you just want the indices that are different , you can do : ` different_indices = [( i , j ) for i in range ( len (( df1 ! = df2 ) .columns )) for j in range ( len ( df1 ! = df2 )) if ( df1 ! = df2 ) [ i ] [ j ]]`
Note that I'm calling the ` nonzero ` method in order to just output the coordinates of the nonzero entries in the underlying sparse matrix since I don't care about the actual values which are all ` True ` .
You can iterate through each row in the dataframe , create a mask of null values , and output their index ( i.e. the columns in the dataframe ) .
@USER do you mean doing the apply ?
I can see the apply part being slow , but as I say I don't think you really need that part .
MultiIndex pivot table pandas python
Pivot table looks like this ( only a part of the large table ): #CODE
but it doesn't work in my case , it doesn't work with pivot table , im not sure why , i'm confused .
Is a pivot table multiindex ??
Assuming each ` Location ` has a unique ` Identifier ` , a join using the `' inner '` strategy should do what you're asking for : #CODE
I came up with a combination of ` resample ` and ` groupby ` : #CODE
From here , how can I access different components of this multiIndex object to create a bar plot for the following conditions ?
I think you need concat chunks to df , because type of output of function : #CODE
I think is necessary add parameter ignore index to function ` concat ` , because avoiding duplicity of indexes .
you've got a couple of problems here . the first involves repopulating your dataframe from a subset . the other , more computational problem involved not ` for ` loops , but ` groupby ` and ` transform ` ( or ` apply ` ? ) operations .
Consider using groupby apply functions to dataset .
Can't drop NAN with dropna in pandas
You can use ` melt ` from pandas : #CODE
Never knew about ' melt ' , works beautifully .
i wanted to count rainfall events in the timeseries data , where we define a rainfall event as a subdataframe of the main dataframe which contains nonzero values between zero rainfall values
i managed to get the start of the dataframe by getting the index of rainfall value before the first nonzero value : #CODE
to go on to the last zero value before the nonzero .
Series f is large ( 30000 ) so looping over the elements ( with ` map `) is probably not very efficient .
The first column is just a counter between 0-200 - I usually drop it when I load the data since it is the same as the row index .
Bug between asfreq and resample in pandas ?
I am trying to understand the difference between resample and asfreq but I can't sem to wrap my head around it , in the example below I took the same dataframe and used resample and asfreq but when I count the values I get different results , how is this possible ?
howto get multiindex from pandas read_csv
I want to get a multiindex on the columns , but I don't get what I expect .
You can either define 2 boolean conditions as a mask and use with ` .loc ` : #CODE
just note that I'm importing ` datetime ` module as ` dt ` #CODE
Using pandas loc in hy
Box 6 is the implementation of apply , and box 7 is what my data looks like .
I think because you strip incorrectly : ` 11 / 14 / 2015 00:00 : 00 ` should be `' %Y / %m / %d %H : %M : %S '` but not with `' - '` symbols between month and day
If you'd like to work with ` df.columns ` you could use ` map ` function : #CODE
You could use apply method for pd.Series of dataframe
So you could convert your ` df.columns ` to ` pd.Series ` with ` pd.Series ( df.columns )` and then use apply method .
I am a longtime R user trying to make the shift to Python ...
For example for ' ind ' = ' la ' and the ' diff ' column : #CODE
You may obtain within groups normalized weights by using ` transform ` : #CODE
I'm thinking I kind of need to do a reverse pivot .
Use panda's ` melt ` function #CODE
thanks @USER , I did not know ` melt ` , but it seems to be exactly the way to go .
How to shift the column headers in pandas
I'm not used to working with ` lists ` in columns of Pandas and don't know how to get the intersection of ` lists ` from two columns in a ` dataframe ` , then get the index of where the words appear , then apply plus signs to the front of each found index .
First one needs to stack the data using ; .unstack ( level=-1 )
It turns out , however , that when ` a.columns ` is a ` MultiIndex ` , the indexer for ` a.loc [ 1 , ' b ']` is : #CODE
Notice that if you change the index of ` b ` to also be a MultiIndex : #CODE
You can directly assign ` b ` to a column in ` a ` , because ` b ` is not a multiindex Series .
I tried with apply and groupby function , did not work .
I improved function ` calctot ` - delete columns and then append sum row .
Just recently I had a situation at work where I set up a big join operation essentially to take a couple hundred files ( ~4GB in total , 600k rows per each file ) using pandas .
Django is built on a database ( usually mySQL or Postgres ) and pandas even has a function to directly insert your dataframe into the database ` dataframe.to_sql ( ' database_connection_str ' )` !
I need to aggregate ` A ` and ` B ` every two consecutive days for each ID .
Can ` resample ` be used for this ?
You can easily resample weekly or monthly as follows ( weekly shown ): #CODE
@USER -Reina This is more efficient , mainly as it doesn't require the unstack > op > stack ( which creates lots of copies , and from 1d to 2d to 1d ) , but also as there's misdirection by grouping by date then resampling rather than resampling directly .
Getting Scalar Value with pandas loc / iloc / at / iat / ix
You can use map and a zip / join over the column levels : #CODE
@USER hmmm , maybe python2 doesn't allow the multi-argument join , try `"" .join ( zip ( *x ))`
I am confused with insert counter ( collections ) into a dataframe :
But I don't know what the proper way to apply str.contains() to pd.read_csv ' s na_values .
Pandas Group By Split , get user input , add column to subset , merge back to df
Then I want to merge it back to the original df ( I realize that the merge may not be right , I am taking this one step at a time ) .
I am not sure why it says I am making a copy , and I am using .iloc to declare a new df , and merge it to the original .
pandas to_csv comma were replace with a
and I am using ` append ` mode in ` to_csv ` function , but the previous data were cleared , I am confused !
The ` a ` does not refer to append mode ( I assume thats what you're talking about when you say append mode ) .
I'm not sure what you want your resulting file ( s ) to look like , but you'll want to make sure you either use a different filename for each dataframe , or join all your dataframes together first and then write the new larger dataframe to a single csv file .
I could do that with diff and followed by loop .
You can use ` diff ` and ` zip ` to get the start and end indexes : #CODE
I'm trying to merge row in a dataframe where I have different inputs for one ID , so I'd like to have a single row for each ID with a weight .
and I would need it to merge the A , B columns for I D= 2 into a weighted average ( 0.3 * 0.35 + 0.6 * 0.55 for A , 3* 0.35 +5* 0.55 for B ) .
Firstly as the dtype are ` datetime64 ` then you have the ` dt ` accessors to return just the ` hour ` component you can use this to perform a comparison .
So now it complains that the `' Series ' object has no attribute ' dt '` .
I'm sure there is still a better way but as I easily fit within memory I just did a SQL type merge .
Use ` reindex ` to expand the data to include all dates
Use ` shift ` to shift 7 spots
I'm only interested in the content of the column , but I think that ` for x in xrange ( 0 , len ( tweets_data )):
To do that just use ` read_csv ` , ` concat ` the files , then loop through them .
How to get just the value of the values column on python pandas pivot table
How to get just the value of the values column on python pandas pivot table #CODE
i want for each block to add 3 new ' 0 ' concentrations for each Name other than the name ' PB ' , and then append the values from ' PB ' to the newly added ' 0 ' concentrations .
If the format of the dataframe is consistent with the example you gave above , then you can apply the function to the groupby object .
I am trying to get rid of the initial NaNs and shift all values to the top leaving NaNs at the bottom .
You iterate over the cols and using ` first_valid_index ` and ` get_loc ` ` shift ` the col values : #CODE
Another method using ` apply ` : #CODE
If ' STRIP ' is a column then you don't need ` get_loc ` : #CODE
I think you can just stack the valid numbers and nan's back together : #CODE
Use ` loc ` with a mask to assign : #CODE
I've updated the first solution also , you'd have to call ` fillna ( 0 )` to replace the ` NaN ` values
Then replace the NaN's with zeroes with ` df.fillna ( inplace=True )`
I would like some help to figure out how to pivot a pandas dataframe into a table with a given list of indices and columns ( instead of the default behavior where the indices and columns are picked automatically by pandas ) .
I'd reindex after pivoting : #CODE
the code below would replace a given column with the same values with another value #CODE
I would not suggest to use ` int ( ma )` as you pointed out in your comment , because that would cut away the decimals .
I could merge the datasets together , but then I don't think there's a good way to extract one column and plot it differently than the others .
You just need to create your ` MultiIndex ` and replace your column index with the one you created :
I am able to add a new column in Panda by defining user function and then using apply .
df [ ' c '] = df.apply ( lambda x : len ( x [ ' a ']) if len ( x [ ' a ']) > len ( x [ ' b ']) else len ( x [ ' b ']))
You can use function map and select by function ` np.where ` more info #CODE
Next solution is with function apply with parameter ` axis=1 ` :
axis = 1 or columns : apply function to each row
So what I'm trying to do is reindex a DataFrame that has a bunch of separate groups within the frame .
I want to take the two groups in the ' A ' column , and reindex each to include month ends for each group from the min until the month end after the max of the index .
This avoids the apply so will be more efficient .
If you want this new c column to propogate over df , you can use transform : #CODE
I know there is a group by rank function but the challenge so far is to combine that with logical operator of a different column and then apply to all rows .
Now I need a new column which will contain the diff ( in days ) between two consecutive date field values ( transaction dates ) .
like thi s a new column conataing the date diff in days for two consecutive date field value for a perticular groupby group.As i have to calculate avg of each groups days_between_transaction .
pandas - groupby and mask levels by ratio of sizes
I would like to group ` U ` , ` G ` , ` C ` according to ` Datetime.hour ` , and mask only ( U , G , C ) values that match a particular function .
@USER Can you give a full list of non-numeric characters you'd expect or do you just want to strip any characters that aren't numeric ?
call ` apply ` and pass func ` len ` : #CODE
I'm trying to read a csv and translate one column that is written in French to English with the TextBlob package in Python ( 2.7.10 Mac OS X Yosemite ) .
But how can I make pandas to read the content field and have textblob translate this to English .
I would like to use the interpolate function , but only between known data values in a pandas DataFrame column .
You can try ` apply ( pandas.Series.interpolate )` instead of ` fillna ` .
I have this simple multiindex dataframe ` df ` obtained after performing some ` groupby.size() ` operations : #CODE
I would like to mask only the rows having the maximum value of the last column with respect to the ` U ` index column .
How can I reset in some way the multiindex dataframe ?
You can use ` groupby ( ' ID ')` followed by ` aggregate ` with specified function per column : #CODE
Almost there , you just need to call ` transform ` to return a series with an index aligned to your orig df : #CODE
@USER first transpose your dataframe , then apply my solution : #URL
Python pandas groupby transform / apply function operating on multiple columns
Trying to use apply-split-combine pandas transform .
With the twist that the apply function needs to operate on multiple columns .
In this case , you can't use transform since the function returns multiple values / array / series : #CODE
Note it's important to return a series or it won't align : #CODE
Use join to explicitly add a separator , which you can't do with sum : #CODE
The string join results in the following : #CODE
Which you then use ` eval ` to evaluate , effectively : #CODE
One column has 80M unique elements ( emails ) , the other column 5M uniques ( anonymized senders ) to which the 80M map many-to-one .
For every sender , it goes over all 80M messages to match the sender , and join the matched emails .
I get a factor of ~5 speed improvement on a simple N= 10,000 test case by using a pandas groupby / apply and writing the csv from the resulting dataframe : #CODE
I checked the type of b is list type and len ( b ) is 6035980 .
But c = len ( x ) = 68516 , much smaller than 6 millions .
Again , if I did not append , but only print , it worked : #CODE
You can use apply with ` to_json ` : #CODE
As far as the ` drop_duplicates() ` , what you're seeing is a known bug and should be fixed in ` 0.17.1 ` through merge #11403 .
What is the best way to read this into a Pandas dataframe with multiindex columns for those attributes in the sub dictionaries ?
I loop through the columns , find those that are made of dicts and then divide it into multiple columns and merge it to the dataframe .
To apply functions to this object , you can do a few things :
If you want to pass a custom function , you can call ` grouped.apply ( func )` to apply that function to each group .
The last question about the average over a range of columns relies on pandas understanding of how it should apply functions .
I'm trying to group by multiple columns , and aggregate them so that they become a list after grouping .
However , when I apply this in IPython , it gives me this error : #CODE
The error message says that you need to give a reducing function to ` aggregate ` , i.e. returning a scalar for each group ( and not a list or array )
I've read this post as well as the pandas 0.17 documentation , but I can't figure out how to use it to selectively replace the column names in a way that doesn't require me to assign new column names manually like this post .
I want to interpolate column B using ` method= ' values '` .
Also , take a look at [ ` ast.literal_eval `] ( #URL ) to handle all Python literals ( without using unsafe ` eval `) .
You need the -1 for the first row in which the ` shift ( 1 )` will return ` NaN `
We use ` str.get_dummies ` and join it to the original dataframe : #CODE
You can easily drop the original column if you wish .
Maybe ` df1 [ map ( lambda x : len ( x ) ! = 1 , df1 )]` but I think it returns a Series .
Based on your answer i used the following ` dataf [ ' C '] .apply ( lambda x : len ( x ) ! = 1 )` but i get True and False as results .
Generate a mask #CODE
@USER The idea is to generate a ` mask ` array i.e. , collection of True , False values corresponding to rows you wish to select .
Try this instead in your multiindex creation : #CODE
For what it's worth , I made a csv with a lot of columns and got this : ` print ( ' chart1.size {} ' .format ( len ( chart1.axes [ 1 ])))` ` chart1.size 1619 ` .
Right now my approach is to plot the map at each step , then use some other tools to make a little movie from all the static images ( pretty simple ) .
That's all fine for now , but ultimately I'm aiming for some interactive functionality where a user can put in some parameters and see their map converge in real time .
Seems like updating the map on the fly would be best in that case .
The map updates in real time .
how to shift all the non empty cells all the way to the left in a pandas dataframe
Is it possible to shift all non empty cells as far as possible to the left so that the column and row immediately to their left is a non empty data item .
You could use ` apply ` to achieve that .
You could use Pandas ` apply ` function , which allows you to traverse rows or columns and apply your own function to them .
instead of ` x1 = dataf [ ' business_id '] [ newdf ]` ? in that case i got the error ` AttributeError : ' Series ' object has no attribute ' join '` @USER
join is a bit strange .
So only strings have a join function .
Have a look at the python docs to join ( sry , I have no time to Google it and give you a link ) @USER
I have a large pandas dataframe , with time series data and a rather large multiindex .
I think @USER M Duarte's answer might be the more correct one , given that it uses the multiindex as intended .
However , for my real data , it would require either a lot of reworking , or a lot of typing , for a 7 levels deep multiindex and 50 data series , which is very error prone .
@USER Hisham ignored my request to stay in my multiindex , but it requires just the quick , easy and easy to check writing of a simple row of numbers ( saving me a lot of time ) , that I can then remove after sorting .
In this case the desired ` MultiIndex ` has three levels , therefore each tuple in the list of tuples has ` len == 3 ` , just like the ` names ` list .
Here ` index.get_values() ` gives us a handy representation of the original ` MultiIndex ` as a list of ` tuples ` .
Yes , but it is an index like ` a ` , ` b ` and so on , instead of being part of the multiindex such as ` loc ` and ` S ` .
Notice that I used a dictionary to obtain the value of ` dist ` from the values of ` loc ` and ` S ` .
Somehow a naive way to do it without regex and assuming that the value you want to replace is contained in your desired value or the other way around .
and apply it column-wise : #CODE
` .T ` at the end for transpose .
How to replace part of a string in Pandas ?
I'm looking at some financial time series data , and am having trouble adding a new column to a pivot table that I created using a dataFrame .
I went ahead and created a pivot table that would give me the values in " DlyReturn " first by indexed by " Factor " and then indexed by " DataDate " using the following #CODE
the new object pivot now has the following format : #CODE
I'm now trying to add a column to this pivot table that gives me the rolling standard deviation for BETA , MOMENTUM , and SIZE across the entire date range contained in " DataDate " .
Does this return inaccurate values : ` pivot [ ' rolling_std '] = pd.rolling_std ( pivot , window=252 )` ?
Try converting your pivot_table result into a dataframe type : ` pivot = pd.DataFrame ( pivot )` .
We can group by the User Id , and then for each group apply a function to evaluate the difference between the rows .
Now define a function that will operate on each group , and apply it .
The way to think about this in terms of a vectorised operation is to use ` shift ` to offset the timestamp column , and then just use a subtraction ( this will be broadcast along the array ) #CODE
Thanks for introducing the shift function .
I'm trying to transform a panda data frame , its original structure is #CODE
and I'm trying to transform it into #CODE
You are looking for the pivot operation , which looks like reshaping the table from long to wide : #CODE
And you want to add a column containing the mean ( or any other aggregate function ) of column A for each distinct value of column B .
You may use ` transform ` : #CODE
You're probably better off making a dict with the mapping and then just using ` map ` to assign it : #CODE
Your original version would work if you used ` transform ` , which broadcasts the result back up to the original indices : #CODE
Once you have those you can map them into unique random numbers safely however you like .
What is the SQL equivalent to pandas ' transform ' ?
And you want to add / show a column containing the mean ( or any other aggregate function ) of column A for each distinct value of column B .
Why would you avoid join ?
Even in background pandas does join :)
@USER Aren't FuzzyTree's queries using a subquery or a window function more efficient than doing a join ?
You can join to a derived table that contains the aggregate value for each grouping of b #CODE
First , I forgot to say that I was trying to avoid a join , sorry .
@USER mysql doesn't support window functions , so I don't think there's a way to do it w / o a subquery or join
Find the rank for desired datetime , and shift ( i.e. , subtract ) the column by that much .
I don't think drop duplicates is needed .
Aggregate will silently drop columns which it can't add , in this case the original ` Genres ` column .
Resetting the index after the groupby and aggregate gives you a new column titled ' Customers ' with the standard numeric index .
You can call ` apply ` and convert your dict values into a set can convert the ` intersection ` to a list : #CODE
I am running pandas v17.0.0 on Anaconda stack ( 2.4 ) of Python 3.4.3 .
Running to_numeric via apply on the dataframe iloc selection , ie #CODE
So your approach of using ` apply ` is the correct one if you want to use this functions on several columns at the same time .
Merge dataframe columns that contain a list
and i want to merge the cells like this #CODE
You can apply difference of sets converted from lists of columns and then convert to list .
You have to use ` axis=1 ` , because apply function to each row .
I want to replace the values in column B with a string , if the respective values in column A are equal to ' a ' .
More efficient way to import and apply a function to text data in a Pandas Dataframe
I know the key to speed when working with Dataframes is to avoid for-loops and to apply functions to the whole data set .
My question is , is there a quicker way to apply a function to a string when reading in a text file , other than line by line and appending it to a dataframe ?
Multiindex to different columns in Pandas Dataframe
After I've read in a ` netCDF ` file via ` xray ` , I get a pandas ` dataframe ` with a ` multiindex ` like this : #CODE
Now I want to convert the ` multiindex ` into different columns to do statistical manipulations .
If you wish to join the lat_lon values , you can do that as follows : #CODE
Now that I have all these lists , I can create a new dataframe ' results ' merge the date back with the original dataframe using the ix as the merge key to make sure everything stays lined up
I need to loop through each line and check the latitude and longitude data I have ( 2 separate columns ) and append a code ( 0 , 1 or 2 ) to the the same line depending on whether the lat , long data falls within a certain range .
I'd create a boolean mask of your 2 conditions and pass these to ` .loc ` to mask the rows that meet the criteria and set these to the values .
You apply jsonification to it twice .
Calling set_index() in __init__() with multiindex
The index doesn't get set if I try to set a multiindex in init ( ) .
@USER len ( wordslist ) shows 6672 which means all words are in the wordlist .
You should probably also strip punctuation etc . to get what you really want otherwise this will happen : #CODE
replace #CODE
Also how to strip punctuations ?
inner join
I will like to avoid merge if possible
I have to do some research about mask function but this works great .
Pandas.apply with dependency on previous value ( not shift )
I am trying to apply a function to each row in a dataframe .
I am aware I can shift the df to get sampleprime and deltats values .
It is not feasible to resample / interpolate and run over this expanded dataset for each window .
It looks like you want to apply a recursive function .
If you use the transpose of this matrix so the columns sum to 1 instead of the rows , you just have to right multiply then , i.e. above change ` state = np.dot ( start , Tmult )` to ` state = np.dot ( Tmult , start )` .
We could replace the elements in ' fruit ' that are not ' mango ' to ' others ' , then ` groupby ` the variables ( ' customer ' , ' fruit ') , get the ` sum ` and ` unstack ` .
Try grouping certain columns and then apply sum() like this : #CODE
, Thanks.But is there any to rename those False True to spent_on mango and spent_on others.or how will i get these in two diff columns .
Generally if you see a stack / unstack you should instead try " pivot " :) .
then in order to add your time delta to your date , you need to fulfill two conditions , first you need to create a ` timeseries ` of your date so that later you can add time delta to it , the second thing is that newly created ` timeseries ` must have the same number of elements of your ` timedelta ` , and this can be achieved by ` repeat ( len ( sample_size )` #CODE
In my code here ( json data source there ) I'd like to calculate the rolling mean of timedelta objects : #CODE
You can use ` groupby ` after using ` cut ` to bin the index of the DataFrame .
` np.arange ( 20 , 60 , 10 )` defines the bins that will be used ; you can adjust these according to max / min values in your ' freq ' column .
Replace string values in a dataframe based on regex match
I want to replace the whole text and just put : " Not accredited "
The first argument passed to ` replace ` , e.g. ` r ' ( ? i ) .
` Series.str.replace ` Cythonizes the calls to ` re.sub ` ( which makes it faster than what you could achieve using ` apply ` since ` apply ` uses a Python loop . )
I want to perform a " join " on both of the data in a such way that my resulting dataframe would contain : #CODE
More generally you can use a MultiIndex e.g. generated ` from_product ` : #CODE
I know for this small and fixed ` DataFrame ` I can just use ` ix ` method with the desired position , as follow :
Yeah , don't you want to replace values in col2 with zero values and with index a to be any number ?
The answer may come down to ' calculate some intermediate columns until you can do a simple merge ' or ' just use ` apply() ` , but I could swear that there was a way to do what I've described above .
Not sure it's all that much tidier than the intermediate column / merge : #CODE
One approach is to use a ` merge ` in which the ` on_left ` is not a column , but a vector of keys .
I have the following dataframe which is the result of doing a groupby + aggregate sum : #CODE
Because of the groupby and agg , the column headings are now tuples .
If you use sum the " agg function name " won't be created as part of the columns ( as a MultiIndex ): #CODE
To remove them , you can drop the level : #CODE
append multiple pandas data frames to single csv , but only include header on first append
I need to create a .csv file and append subsets of multiple dataframes into it .
All the dataframes are structured identically , however I need to create the output data set with headers , and then append all the subsequent data frames without headers .
I know I could just create the output file using the headers from the first data frame and then do an append loop with no headers from there , but I'd really like to learn how to do this in a more efficient way .
To do it efficiently , you can use one of the Merge , join , and concatenate so you have two complete dataframe ( ` yankdf ` and ` metsdf `) , then write to csv using ` to_csv ` as you have been doing .
Using append #CODE
Add data to dataframe using ` append ` instead of re-assigning everytime
No worries , you're only looping to read the files then as you're reading you append to dataframe .
try using apply function .
Imagine that my ` store ` ` SFrame ` has 8 columns which I want to append to ` train ` .
You could do this using a ` join ` operation .
I am simply trying to replace the index of a particular dataframe based on the index from a another dataframe .
Then i have a second DataFrame which reads from an xlsx file , my aim is to simply replace the index of this file from the index of nai_data using this technique A.index = B.index .
( ` len ( A.index ) , len ( B.index )`)
Pandas apply but only for rows where a condition is met
pandas concat / merge and sum one column
I would like to merge these two data frames and take the sum of one of the columns if the row ( index value ) exists .
What I want to do is merge these two , and sum the ' b ' column , but keep all rows whether they exist in one or the other dataframe : #CODE
Columns B-F should apply to all null fields below them , then once all nulls are filled the next set of periods should be filled by the next values populated in B-F .
but the problem is this will fail as the indices don't align .
I meant to also comment , when I do that add I get a ` ValueError : cannot join with no level specified and no overlapping names ` .
Is this where people apply normalization ?
It would be great if someone could explain how to apply normalization in such a situation .
You could do it with pandas using groupby and aggregate to appropriate column : #CODE
Group series using mapper ( dict or key function , apply given function to group , return result as series ) or by a series of columns
len ( self.values ) , len ( self.mgr_locs )))
Try this ` result = df2.groupby ( by= ' A ') [ ' B '] .unique() .map ( lambda x : len ( x ))`
As a workaround for now , you can easily use the ` nunique ` Series method through ` apply ` instead of calling it directly on the groupby object : #CODE
and then append this to master dataframe with #CODE
You may ` groupby ` on ` A ` and ` df [ ' C '] 0 ` , and ` unstack ` the result : #CODE
if you want to add these to the original frame corresponding to values of ` groupby ` key , i.e. ` A ` , it would require a left ` join ` : #CODE
I am trying to convert it to a bool dataframe where True means that the value is greater than the threshold , False means that it is less , and NaN values are maintained .
Try to replace your ` func ` with this line : ` return x > = threshold if x is not None else x ` , it might be faster .
Combine the output of ` isnull ` with ` df = threshold ` using bitwise or : #CODE
This is not pretty and it is still slow ( but faster than apply ):
But that is different from what you will get using the apply method .
Here your mask has float dtype containing NaN , 0.0 and 1.0 .
In the apply solution you get ` object ` dtype with NaN , False , and True .
Neither are OK to be used as a mask because you might not get what you want .
IEEE says that any NaN comparison must yield False and the apply method is implicitly violates that by returning NaN !
A Pandas DataFrame with ` bool ` dtype cannot have missing values , and a DataFrame with ` object ` dtype holding a mix of Python bool and float objects is not efficient .
but after that one can apply ` .reset_index ( drop=True )` ( mind drop=True here ) and that returns #CODE
Just the usual apply warning , this can be slow ( that's why we special cased groupby head to use cumcount under the hood , at least originally not 100% sure if it still does ) .
My aproach is convert ` Datetime ` column to period by months , then shift column ` a0 ` to 2 new columns .
as_type has casting rules , none of which seems to apply ( you'd think it'd be casting= ' safe ' #URL )
Trying merge with help of the pandas documentation didn't get me very far , mostly endlessly repeating rows of duplicate data ..
You group on ` user.id ` , and then use ` agg ` to apply a custom aggregation function to each column .
How can I ( or should I ) transform this DataFrame of DataFrames into a MultiIndexed DataFrame ?
How exactly can I transform the data structure I already have ?
Regarding how to convert your data structure to the hierarchically indexed dataframe , first , you need merge your dataframes for each state together to form a horizontal structure , then you can use transpose and stack to rearrange into a vertical structure .
In your post , you say , " first , you need merge your dataframes for each state together to form a horizontal structure " - how do I do that ?
The transpose and stack seems to work all by itself , although accessing elements appears to be really slow .
How to find and replace a single value in a dataframe in python
If the above were to work my next step would have been to find and replace as follows : #CODE
Now we compute a boolean mask using ` isin() ` and change these results to `' Extended '` : #CODE
and then make a pivot table based on this column
In the current implementation apply calls func twice on the first group to decide whether it can take a fast or slow code path .
Pythonic / efficient way to strip whitespace from every Pandas Data frame cell that has a stringlike object in it
I need to strip whitespace from all the stringlike cells , leaving the other cells unchanged in Python 2.7 .
I've tried searching for a definitive answer , but most questions on this topic seem to be how to strip whitespace from the column names themselves , or presume the cells are all strings .
If there aren't any downsides maybe you could remove the check and replace it with a try and except block .
@USER - attempting to call .strip() on a non-stringlike object ( or thing that doesn't have a strip method , such as a numeric data type ) will raise an exception .
If I understand your question , it seems to me that the easiest solution would be to pick the columns from your dataframe first , then apply a function that concatenates all columns .
Pandas : Getting a rolling sum while grouping by a column
How do I get a 30 day ( or x day ) rolling sum of these values broken out by whoever is in the ' Name ' column ?
to get the rolling sum overall .
But how do I return a dataframe with that rolling sum grouped by the ' Name ' column ?
Just a few days ago there was a [ bounty question ] ( #URL ) for a similar ( nearly exact ) python rolling sum by group but for last 7 days .
Specifically , I am interested in how an Index is actually used when slicing via ` loc ` with particular attention to ` MultiIndex ` .
In particular it looks like there is some intention of a minimal API so that ` loc ` dispatch works , but the docs are a little sparse here .
How is it that you want to aggregate ` Vge ` ?
Probably not ideal , but this can be done using ` groupby ` and apply a function which returns the expanded DataFrame for each row ( here the time difference is assumed to be fixed at 2.0 ): #CODE
For each phrase in the phrase_list , I want to get the aggregate score if that phrase is found in each row .
For each phrase , it creates of mask of those rows in the dataframe that contain that phrase .
The mask is ` df.Description.str.contains ( phrase )` .
This mask is then applied to the scores which are in turn summed , effectively ` df.Score [ mask ] .sum() ` .
So I want to replace that string with None and then drop NA .
So you could use apply and pd.to_numeric methods : #CODE
Panda Pivot Table : calculate percentage with respect to two columns
The ` aggfunc ` determines how to aggregate multiple rows into one row .
But your pivot table has one index , ` ssname ` , and your DataFrame has no duplicates in the ssname column , so there aren't multiple matching rows for the pivot table's index , so in the pivot_table no aggregation into one row will take place .
I want to append ( merge ) all the csv files in a folder using Python pandas .
Prevent scientific notation in seaborn boxplot
I'm using pandas version 0.17.0 , matplotlib version 1.4.3 and seaborn version 0.6.0 to create a boxplot .
try to replace ` x.set_index ([ 0 ]) .T ` with ` x.set_index ([ ' column_name ']) .T `
Your solution is so elegant , it stimulates me to learn deeper in groupby and apply methods .
Across the many files there are going to be many , many , repeated strings that I want to normalise as I insert them into the database .
When it goes to insert the second row it should look at the Experimenters table to realise that John Smith already exists , and has the id 1 .
Jane Green doesn't exist yet , so it inserts Jane Green into the Experimenters table , and then takes the id value assigned to her to insert into the second column .
Then for the group column it does the same thing , to insert the value 1 .
My best solution so far is to read the Experimenters table and Groups table separate DataFrames , perform merges , insert rows into Experimenters and Groups , and then write to Data .
I want to transform it into a dataframe where the first part of the tuple is the row header , and the second part of the tuple is the column header .
or you could convert it to a NumPy array and transpose that .
here ` apply ` will call ` nunique ` on each column
Use ` isin ` to test for membership and ` ~ ` to negate the boolean mask : #CODE
How can I do the best fitting , taking into account my hist and bins arrays ?
I am trying to use ` param= spy.expon.fit ( distance ); a = np.linspace ( 0 , bins [ -1 ] , 1000 , dtype= ' f ') ; pdf_fitted= spy.expon.pdf ( a , loc =p aram [ 0 ] , scale =p aram [ 1 ]); plt.plot ( a , pdf_fitted , ' r- ')`
For this you'd first define a fitting model ` myexp=lambda x , l , A : A* np.exp ( -l*x )` , then use it as ` popt , pcov= spy.optimize.curve_fit ( myexp , bins [ 1 :] +bins [: -1 ]) / 2 , hist )` .
Don't ` plt.plot ( stats.expon.pdf ( np.arange ( len ( hist )) , popt ) , ' - ')` , rather ` plt.plot (( x [ 1 :] +x [: -1 ]) / 2 , myexp (( x [ 1 :] +x [: -1 ]) / 2 , *popt ) , ' - ')` ( or with any ` x ` array you prefer ) .
Your ` distance ` will replace ` X ` here .
All attempts at iloc , loc , slice , sliceindex , and ix have thus far failed .
It's possible to use ` loc ` to slice but the index needs to be ` sorted first ` : #CODE
KeyError : ' MultiIndex Slicing requires the index to be fully lexsorted
tuple len ( 2 ) , lexsort depth ( 0 )'
3d bar chart with dataframe in pivot table format
I have a csv with data like the below , which I have had to convert to a pivot table to normalise the data , and ensure that response times where a server does not have a value is filled with the integer 0 #CODE
I am now attempting to draw a 3d bar chart with this information in matplotlib , but I am having issues being able to unravel the pivot data , and see the count as something that could be seen as values of the z axis .
Indexing new column for boxplot of pandas dataframe
Then plot the difference in prices on a boxplot ( based on the time grouping they belong in )
This function is a merge of StratifiedKFold and ShuffleSplit , which returns stratified randomized folds .
There is multiple space in string , so I replace all of them by ` ; ` .
I'm having trouble finding how to normalize data in long form in pandas .
In R , I would cast the data , normalize , then melt .
I have 74 relatively large Pandas DataFrames ( About 34,600 rows and 8 columns ) that I am trying to insert into a SQL Server database as quickly as possible .
The problem is that insert is not getting any values -- they appear as a bunch of empty parenthesis and I get this error : #CODE
The database is remote , so writing to CSV files and then doing a bulk insert via raw sql code won't really work either in this situation .
The idea is to write an anonymous function that operates on each of your groups and feed this to your groupby using ` apply ` : #CODE
Assuming that your ' Time ' column is already a ` datetime64 ` then you want to ` groupby ` on ' ID ' column and then call ` transform ` to apply a lambda to create a series with an index aligned with your original df : #CODE
is dt a new feature of pandas -- I get an error when that functionality is called ?
Given a directory containing CSV files named with the pattern Prefix-Year.csv , create a new set of CSV files named Prefix-aggregate.csv where each aggregate file is the combination of all CSV files with the same Prefix .
Also , while anisotropic could have worded the problem better , it is very clear what he wants to accomplish : ' Given a directory containing CSV files named with the pattern Prefix-Year.csv , create a new set of CSV files named Prefix-aggregate.csv where each aggregate file is the combination of all CSV files with the same Prefix .
I've included a CSV merge method as well based on MaxNoe's answer .
Keep getting : KeyError : ' cannot use a single bool to index into setitem ' on this line of code in the second chunk I posted .
apply a function to a dataframe column ( datetime.date )
I'm trying to apply this formula on a dataframe column ' Days ' ( datetime.date type ): #CODE
From here you can use Datetimelike Properties and call the month by doing ` salesPandas [ ' date '] .dt .month ` , then for day and hour just replace it accordingly .
to replace the indexes 0 , 1 , 2 , 3 with the feature corresponding with your bar-pairs ?
No merge wont work because both of the dataframe are of different length ...
you can then easily drop ` Gene_id ` if you want
Also Merge wont work on this kind of data frame as the both dataframes Up ( df ) and annon ( df2 ) are of diifferent size , also they are are of different order .. to do a ' merge ' on it
Looks like you didn't copy my answer , actually on Up dataframe the column you want to join on is Gene_Id instead of Gene_id columns are case sensitive , make sure you typed columns correctly
So I think merge is not a right solution is for this issue
You've misinterpreted the function , the passed values are the values to replace , not the index positions
Python Pandas fuzzywuzzy ' join ' of two datasets on string columns
I am following the answer in this question that uses fuzzywuzzy to ' join ' two data sets on string columns .
You could do this using ` map ` : #CODE
First you merge df2 , then add the two columns .
function won't apply to pandas data frame , getting syntax error
I'm trying to apply this function to a pandas data frame in order to see if a taxi pickup or dropoff time falls within the range that I created using the arrivemin , arrive max variable below .
If it's outside the range I want to drop it from the dataframe .
If you wanted to translate those list items into strings without the braces / brackets , you could use this function to convert the culprit column into a column that would work : #CODE
Let's say forecast is the function I have created that I want to apply : #CODE
How can I apply this function to the dataframes ?
I have tried the apply function as follows : #CODE
The ` apply ` function takes in a function and its args .
At this point , you can join that result onto ` df2016 ` if you need this to appear in the ` df2016 ` data set : #CODE
In general though , you need to be careful about this , and it can be more general to perform the join explicitly ( as I did above by using the ` merge ` function ) .
How do I apply datetime.date() and datetime.time() to the whole series
Pivot in ID in Python as change column name
Then ` pivot ` and rename columns .
You could set ' s.no ' as the index first ( if it isn't already ) and unstack to get the columns into a Series .
You can then use ` get_dummies ` and sum the level of the multiindex to get the result : #CODE
Python 2.7 groupby and then join
My question is about how to join 2 dataframes , which were created by the ` groupby ` method and the ` sum() ` and ` max() ` functions .
Then I try to join them : #CODE
When you aggregate using ` groupby ` the columns you're grouping by get moved into the index .
You should be able to reset the index with ` tr_bin_dep_grouped.reset_index ( inplace=True )` and then merge the two dataframes together .
It's certainly the nice , normal , readable thing to use serieswise operations rather than apply in these sorts of cases , though .
A crude work around is simply backfill each list so that all of each list are the same length , then simply pass it to the DataFrame constructor .
I just used the list function arbitrarily in this example and am more interested generally in how to apply a function to adjacent rows .
You can write your own function that accepts a subdataframe in the groupby ... what function do you want to apply ?
Quintessence of the SQL transpose / pivot query .
check the documentation for ` pivot_table ` , and ` melt `
However , when I want to resample using a multiple of a base frequence , #CODE
I've done some debugging and everytime I get this error is when apply duplicates the first group .
So are there any better ways to do it without using apply ?
Note : I tried transform but it gets rid of the groups and outputs a different dataset which is definitely not what I want , I want to keep the groups and the format .
Consider simply creating an absolute value column through a defined function , apply the function on a groupby , and then sorting item ascending and absolute value descending .
Un-pivoting / Stacking a pivot table with Python Pandas
I have a pandas dataframe df and I would like to drop columns which have a mean greater than 10 and less than 2 .
You can't use drop , but you can index ...
your answer is correct but i think , axis=0 since i want to drop column instead .
Actually , when i apply it to some dataFrame where some entries are nan , and some with unequal length , I have this error : Unalignable boolean Series key provided
( Some other backtraces on similar ( but not identical ) issues on pandas-related github bugs put index.py in the middle of a healthy call stack . )
Rename the original one in your installation and replace it for this one .
Basically , we use ` np.column_stack ` to stack ` column-1 ` with ` column-2 ` and then again ` column-1 ` with ` column-3 ` to give us a 2D NumPy array ` arr2D ` of shape ` N x 4 ` .
It's not really a bug , and it's just when you use ` apply ` .
You're not really even using the data frame on which you're calling ` apply ` other than the needless print statement .
This is typically a case the Pandas doc warns about when using the apply method ( " side-effect ") ...
` apply ` works fine for that .
Merging median from one column by a key column - SFrame / Pandas
To get the median Sales per store , I can do the following to attach a new column for the median sales per store using ` graphlab ` : #CODE
This extracts the median sales per store as per the graphlab code , but when i try to merge it back to the train matrix with : #CODE
How could I merge the median of the " Sales " column using " Store " as the key using ` pandas ` ?
You may do this in one-stage using ` transform ` : #CODE
the merge error simply says that you have duplicated column names across left and right frame , so either you need to provide suffixes to distinguish columns or rename the columns : #CODE
I tried an inner join , but I couldn't find a way to subtract it from df2 .
You can use a list comprehension together with ` join ` to create unique keys of each table consisting of the the first name , last name and the date field ( I assumed date of birth ) .
You can append the two frames and use ` drop_duplicates ` to get the unique rows , then as suggested by @USER you can use ` iloc ` to get the rows you want : #CODE
What is the best way to reshape / collapse / unstack the multilevel DataFrame so we can make use of all the data for our regression ?
I can easily transform this series into a regular one using ` pd.Series.asfreq ` , e.g. ` series.asfreq ( ' MS ')` gives : #CODE
I think you're missing the fundamentals of apply , when passed the Series ` clasif ` , your function should do something with ` clasif ` ( at the moment , the function body makes no mention of it ) .
You have to pass the function to ` apply ` .
Explicit map solved my issue .
End to end solution will be to create a function to map non str columns to strings during extraction .
Python - replace exec for dynamic variable creation
for example , i'm looping through a dataset . after the first loop , i get a subset of my dataframe ( rows 1-10 for example ) . for the next loop , i want it to start iterating on ` index = 11 ` , and then apply whatever alg i have . the thing is , i do it by truncating the dataframe .
so if the last item in the previous run was at ` index=10 ` i truncate the dataframe to ` df = df [ 11 :] `
is there a way for me to just " point " at the row in the dataframe so that i don't have to truncate it ?
which should be avoided by using ` loc ` : #CODE
Python Pandas pivot table how to handle ' \xc2\xa0 ' ?
@USER How do I replace ?
Next , I want to add one more cut : I want to select all the targets that have ` 4.0 logg 5.0 ` .
I also tried ` if catalog.loc [ i , ' logg '] == -1 : ` and now the error I get is ` AttributeError : ' DataFrame ' object has no attribute ' loc '`
AFAIU You need to select all ` ( catalog.rp ! = -1 ) & ( catalog.mp ! = -1 )` then replace all rows where ` df.logg == -1 ` to ` df.ix [ i , ' mp '] / df.ix [ i , ' rp ']` and then choose all rows from modified ` df ` where ` ( df.logg > 4 ) & ( df.logg < 5 )` .
Second , IF ` catalog.logg == -1 ` , replace ` -1 ` by ` catalog.mp / catalog.rp ` .
I presume it has something to do with the fact that the labels are quite long and are getting cut off by the bar chart plotted 2nd ,... but im not sure .
Date ` lastdayfrom ` is used for selecting last 30 days of ` DataFrame ` by function loc .
I am trying to iterate over groups ( produced by group.by in Pandas ) in order to apply a function ( create a chart in MatPlotLib ) and get a result for each group in the DataFrame .
You can use the apply function .
You can filter the columns first to get the cols of interest and then call ` apply ` and use the boolean mask to mask the cols : #CODE
Bokeh heat map not displaying correct results
I am trying to make a chloropleth map based off of bokeh's map of US unemployment , and I am not very familiar with this library , although I am somewhat familiar with pandas .
try to replace your loops with something like that : #CODE
and then replace source by #CODE
But now I don't know how to merge it with the original ` A ` .
You can apply function ` f ` for each group .
I want to interpolate my data between the known points .
TypeError : Cannot interpolate with all NaNs .
make a shift by index with a pandas dataframe
I create a new column based on a shift of an other column , but that's not a shift in number of columns .
That's a shift based on an index ( here the index is a timestamp )
have you tried ` shift ` ?
the answer was to resample so I won't have any hole , and then apply the answer for this question : How do you shift Pandas DataFrame with a multiindex ?
In Pandas , there is a new styler option for formatting CSS ( #URL ) .
You can reindex the two dataframes so their index / column match , and then multiply them : #CODE
So this converts the ` DateTimeIndex ` to a ` Series ` so that we can call ` isin ` to test for membership , using ` .dt .dayofweek ` and passing ` 0 , 1 ` ( this corresponds to Monday and Tuedsay ) , we use the boolean mask to mask the index
Another way is to construct a boolean mask without converting to a ` Series ` : #CODE
I can't reproduce this , I get ` bool ` as the ` dtype ` when all elements are boolean
You can also forcibly cast the ` dtype ` ` df [ ' b '] .astype ( bool )`
Because you have a mixed ` dtype ` initially even after calling ` dropna ` then you can coerce the dtype , seeing as all you're interested in is preserving numeric and bool types then calling ` convert_objects ` or ` to_numeric ` will correctly convert the ` dtype ` : #CODE
If yes , you can use the Linux terminal to strip quotes from the ends of the rows quickly .
below replace ' in the first name with _
Well for dates you can do ` pd.to_datetime ( df [ ' date '] , coerce=True )` this wil l generate ` NaT ` where the data format is incorrect and you can use those locations to mask the df and place them in a separate df
I see in the python documentation the ability to resample and synchronize two timeseries .
Note that the time series are already read into a pandas DataFrame , so I need to be able to synchronize ( and resample ? ) with already created DataFrames .
Note that they will have ` NaN ` s in all of their new entries ( presumably this is the same behaviour as in Matlab ) , it's up to you if you want to fill these , for example ` fillna ( method= ' pad ')` will fill in null values using the last known value , or you could use ` interpolate ( method= ' time ')` to use linear interpolation based on the timestamps .
The Matlab version of union appears to also resample ?
" Union Resample timeseries objects using a time vector that is a union of the time vectors of ts1 and ts2 on the time range where the two time vectors overlap .
I would like to transform the foll . dataframe : #CODE
You can simply create a slice of the data you want , set the index and then unstack as follows : #CODE
Should I use apply ?
Then I use apply : #CODE
It is more efficient to assign the boolean indexing to a mask as follows : #CODE
dropped vectorization and list-comprehension tags - they don't really apply here
I said , let me use a lambda to apply the .hour to every " row " .
Using Pandas 0.16.2 , I didn't have a problem getting local US Eastern time from tz aware timestamps .
I knew about the dt command but it wasn't working .
It turns out that when making the time tz-aware by using ' America / New York ' , the dt does not work .
I know I can get histogram values with ` np.histogram ` , but I want to use pandas hist method .
However , if you really want to use the pandas ` hist ` function , you can get this information , from the ` patches ` that are added to the ` hist ` ` AxesSubplot ` when you call ` serie.hist ` .
IIUC then you can combine your boolean conditions and use ` shift ` to test the previous row value : #CODE
So this combines your 2 boolean conditions using ` ` wrapped in parentheses due to operator precedence and tests the previous row using ` shift ` , as you can see entry ` 27 2007-04-10 13:33 : 00 off True ` is set to ` True ` as desired , you can modify the statement to this : #CODE
I knew that was the direction but I kept tripping myself up with where the ` shift ` goes .
2.i need to append df1 to df and only columns from df should only remain and it should exclude if any new column from df1 getting appended .
1.is there way that i will read the small sized csv only and append to pickle file ...
2.can it be done like converting the csv to pickle and merge two pickles . by load , dump method ( actually never used that )
use ` .loc ` with a boolean mask : #CODE
I'm trying to create a pivot table that has 3 measures on the same value being aggregated :
What I'm interested is to know if you can do it during the creation of the pivot table , by passing the right aggfuncs .
I've created a DataFrame in my desired date order , however , when I put this into a pivot table the order changes .
I wanted to sort the pivot table base on the newest date of any of the rows within a given level #CODE
You can remember old ` multiindex ` before pivoting and then reindex output dataframe by old ` multiindex ` .
Its ` multiindex ` is : #CODE
Output dataframe has ` multiindex ` sorted by ` colour ` : #CODE
And you can sorted by level ` date ` , but multiindex and output is : #CODE
So solution is ` reindex ` by original ` multiindex ` .
call ` set_index ` on ` df2 ` to column ' B ' and then call ` map ` on ` df1 [ ' A ']` : #CODE
This would give me the most recent change in ` a ` and ` b ` namely ` x.loc [ dt ( 2015 , 1 , 16 )]` .
The pandas asof function is meant for this : #CODE
I want to resample the index column ( ` Timestamp `) in 100-millisecond bin time and check in which bin times the signs belong .
Sum columns of array subject to mask
" I am attempting to use a mask to accomplish this .
My question is : given this mask , how can I group all the features which do not meet the criteria ( eg , are ' false ') into " All Others " and use that to create a stacked bar chart with all the other ' True ' features ?
IIUC you can use [ ` cut `] ( #URL ) for this
not sure how to apply this suggestion to the problem above .
Or would I do array [: , mask ] which doesn't seem to work ?
To do this , I'll need to select the column names corresponding to the ' True ' elements of the array and append ' all others ' which reflect the new column .
After they are done , merge the two frames together : #CODE
Another solution ( slightly harder ): Merge the columns ` transcript_id ` , ` gene_id ` and ` gene_name ` in another column , say ` merged_id ` and ` groupby ` on ` merged_id ` .
In your particular case , this is probably working fine on ` df ` as a whole because ` df ` likely has an integer index ( ` [ 0 , 1 , 2 , ..., len ( df ) -1 ]`) , which is the default row index in a ` DataFrame ` .
It would be great if you could include a bit more of the stack trace in your original post so that others can see exactly where the error comes from .
Otherwise , you can get all the column names , strip the numbers from the end ( ` [ name [: -1 ] for name in df.columns ]`) and then use [ ` sets ` to give you unique names ] ( #URL ) .
You can ` groupby ` the ' id ' and call ` nth ( -1 )` to get the last entry for each group , then use this to mask the df and set the ' indicator ' to ` 1 ` and then the rest with ` 0 ` using ` fillna ` : #CODE
Then if I print averages after averages = averages.concat ( df.groupby ( ' Player_ID ') .mean() .unstack() ) , I get : AttributeError : ' DataFrame ' object has no attribute ' concat '
About the ` AttributeError ` I actually wanted to write ` append ` , I don't know why I wrote ` concat ` .
Pandas merge only if condition is true
I have two DataFrames , I need to merge both , and I need to add a column that specifies if it is accepted or not .
Can I use the apply method to this task ?, can someone help me to do in the right way using pandas .
( Python Pandas ) MemoryError : skiplist failed when using rolling median and apply
This line once computed a rolling median by group but now it produces an error :
I'm trying to figure out the best way to apply a function to groups within a Pandas dataframe where the function depends on the group .
You can map a function on the index .
This works because transform supplies the function the arguments of the series as well the ` name ` of the grouped-by variable ( in the case the value of ' Year ') .
For any cell that was False in at least one dataframe , I want to assign it a string ( e.g. " False in Dataframe1 ") and if multiple , append both ( e.g. " False in Dataframe1 , Dataframe2 ") .
Or do I need to concat the dataframes so I can compare the columns to each other ?
it seems that you ` x ` should start from 2008-01-30 , after that , you can simply inner join two dataframes : #CODE
I can use ` groupby ` and ` apply ` , but ` apply ` runs the ` shift ` function over each column independently , and it doesn't seem to like receiving an ` [ nrow , 2 ]` shaped dataframe in return .
Is there perhaps a function like ` apply ` that acts on the whole group sub-frame ?
IIUC , you can simply use ` level= " grp "` and then shift by -1 : #CODE
Great , thanks , I can't remember why I thought I needed to do it with ` apply ` - maybe it'll come to me later .
Part of the solution , because you'll have duplicated rows with slightly different names so you couldn't apply drop_duplicates method of dataframes : #CODE
Thus , I would propose to transpose DF first , and then just sum columns
Also I think you may want to replace them by `' '` rather than `''` since now you merged the last word on every line with the first word on the next line , which doesn't seem a good idea .
Depends on how you define the last week of the previous month ( final 7 days ?, final 5 trading days , etc ) , it should be easy to compute month-by-month the number of buys in the last week , then just shift this series to align with the subsequent month .
Here's an implementation that uses a MultiIndex and an outer join to handle the cross join .
You can do this with a group-by , followed by a join , followed by a sort .
Looking at your example output , perhaps what you want is to replace ` sum() ` above with ` max() ` it's not entirely clear to me from your question .
Python Pandas ( read xlsx and append )
i want to read some columns of an excel-file and finally append them to a csv-Data .
python merge dataframes while choosing the keys conditionally
And I want to merge the lookup table with another table ` table B ` which also has missing values under ` id_B ` : #CODE
I want to merge these two dataframes on ` left_on = ' id_A ' , right_on = ' id_B '` if possible .
And for those with missing ` id_A `' s or ` id_B `' s , I want to merge the two dataframes on ` left_on =[ ' first ' , ' last '] , right_on = [ ' fName ' , ' lName ']` .
The overall result would be like a left join , so that I only keep the data with keys shown in my lookup table .
Please post what the desired merge df looks like , I sorta understand but you need to post this so that there are no ambiguities
Are you after [ ` interpolate `] ( #URL ) ?
You are right about ` interpolate ` method currently not working with ` Timestamp ` .
One solution is to convert it to float , interpolate it and covert it back to ` Timestamp ` : #CODE
eq : #CODE
What I was expecting is to map the unique column values into a dict eg : { " Lip_G D: " 1 , " Jac+Jac " : 2 } and map this int values into the dataframe column .
@USER see my proposed answer below that uses sklearn to map categorical data to numerical values .
Within each iteration , apply the function .
As a general idea , you can use ` apply ` on your grouped data to take compute the ratio for each state : ` sum ( bads ) / sum ( goods )`
Note that you cannot use ' loc ' with the tuple index : #CODE
I believe the hash table is rebuilt when you replace the index .
Then you can apply ` cumsum() ` : #CODE
Oh for some reason sort_values() is working now . but how do I drop all the 0 entries in the resulting column ?
Pandas dataframes reindex on the union of their respective datetime timestamp indexes
I am trying to get them to be the same length and resample the missing data so that the two timeseries are the same length .
If I try to reindex with #CODE
In reality I need to apply this function on 40K over rows .
very slow using Pandas ' apply ' : #CODE
Is there a way to actually append 15 sheets while loading or I have to iterate over keys to append 15 DataFrames ?
I mean while loading specifying to append instead of creating dictionary ?
you can simplify to this : ` df [ ' C '] = df.A.where ( df.B.isnull() , df.B )` as ` isnull ` is available for df and series , also I wouldn't encourage the practice of accessing columns as attributes as it can lead to strange behaviour , better to do this ` df [ ' C '] = df [ ' A '] .where ( df [ ' B '] .isnull() , df [ ' B '])`
how to concat two csv file and then sort by python
First I have concat this two csv files by pandas , and then save the data into a new file named join_cv_common.csv by pandas .
I want to rewrite these two functions of pandas - concat and sort by pure python ( 2.6 and 3.4 ) .
Pandas concat function #CODE
By my knowledge it can be done by reading both the files using the file i / o after that just join and convert the string to a list after that sort the newly created list and put it in the final output csv by converting the list to a string .
Is there a way to either flatten the header rows of the pivot table into one header row , and to keep the order of the value columns ?
I think if use ` summary =p d.pivot_table ( table , index =[ ' a ' , ' b '] , values =[ ' c '])` get multiindex ( because 2 columns ` a ` and ` b ` are set to index ) and how let sorted df custom way explain [ this ] ( #URL ) .
One approach would be to add a column using ` groupby ` and ` transform ` to flag which fruit is the most popular , you can then filter the df : #CODE
Without trying this I can't be entirely sure that the returned value of ` func ` will be acceptable for use with ` apply ` , so you might need to play around with that a little .
How do I replace cells in a Panda dataframe column based on a condition
This post consists of two questions , touching on issues I have encountered when trying to replace elements in a Panda dataframe based on a given condition .
I want to append the integer 0 to each cell that is of length less than four .
I want to replace cells in column A that take on the value 5 , with the values on the same row in B .
So what I want to do is , I want to resample the START_TIME into 100milliseconds bin itme and interpolate the variables ( TRIAL_No and itemnr ) between each START_TIME and END_TIME .
After creating new dataframe by ` concat ` dataframes I can group it by row and apply ` resample ` on each of these groups ( with method ` ffill ` to forward fill ) .
numpy and pandas are not needed here although you need to apply the strip function to every element in each row to remove excess spaces ( ` map ( str.strip , row )`) and also pass ` delimiter= ' | '` into ` csv.reader ` because the default delimiter is a comma .
You can do ` len ( result )` to get the count you're after .
To open the files , parse some data and append it to a dataframe I think pandas is needed .
How can I drop the first value ?
I'd like to change a value in a dataframe by addressing the rows by using integer indexing ( using iloc ) , and addressing the columns using location indexing ( using loc ) .
Mixed integer and label based access is supported by ` ix ` .
Is your use of ` apply ` a recommended method to access pandas functionality that isn't present in dask ( assuming the block can fit into memory ) ?
There is no direct equivalent in graphlab to ` DataFrame.iloc ` ( previously ` irow `) .
And i want to merge them so each frame has its column :
I'm trying to merge them together so each of the 10 Dataframes has its own column which I can rename .
For the record ` applymap ` is used to apply a lambda function elementwise ( documentation )
Then all dataframes are joined by concat to one .
I found a similar question on SO for plotting a boxplot here , so I tried modifying it for line plot .
You can add the column " Type " to the index , and unstack it so as to have the values of C1 split in two columns according to the value of Type , and then plot them , e.g. : #CODE
IIUC you need ` resample ` ` df1 ` , because you have an irregular ` frequency ` and you need regular frequency : #CODE
You can use function ` asfreq ` instead of ` resample ` - doc , ` resample vs asfreq ` .
First I think that ` resample ` didn't work , because after resampling the ` Result ` is the same as ` df1 ` .
So I think ` resample ` works well .
Thanks for the suggestion @USER , I've tried your method , but still have the same issue using ` asfreq ` or ` resample ` .
You've made it clear for me to see that ` resample ` was working .
Expanding a daily roster in pandas to join with other files
You can ` groupby ` by ` EmpID ` and ` resample ` by ` days ` and fill rows by method ` how=first ` .
Then I reset multiindex - first drop first duplicity level and then reset again .
How to aggregate the large date of status change
A slight change to my question , is it possible to normalize this sort in any way ?
So replace ` sum ` with ` mean ` .
You could use standard method of strings ` isnumeric ` and apply it to each value in your ` id ` column : #CODE
I then would like to apply a function to each of these groups .
Shouldn't apply iterate over the results of the grouping , and the ` group ` argument refers to the current group in consideration ?
The syntax of the operation is incorrect , replace the above split with the following .
I'd suggest using ` join ` .
Pandas merge two dataframes by time column
I have been struggling to merge data frames .
I tried ` append ` and ` merge ` but I am struggling to find an appropriate solution .
It appears that you want to ` append ` the dataframes to each other .
Also , this blog post is a good resource for ` join ` and ` merge ` in pandas .
Next , you can use a dictionary comprehension together with ` loc ` to select the relevant ` group_no ` dataframe .
You can then use ` loc ` to fetch the value at that row from the product ID column : #CODE
I am trying use pandas to resample vessel tracking data from seconds to minutes using how= ' first ' .
The code to resample : #CODE
I am familiar with the page . but I dont know how i can apply this to my data and plot , as they are using only one variable with an arithmetic operation , while I have a dataset with different categories .
I know how to loop over the each row to append 1 to matching elements of each columns .
Using get_dummies() , you can specify which columns to transform into dummy variables .
I tried something like this and it's 500+ times faster than yours on my machine when ` len ( df ) = 100000 ` .
iii ) There are no NaNs : notnull ( df.var ) .all() returns True
v ) a period index is used - and that forms the index for pivot table .
The pandas docs give an example of ` concat ` that combines indices ( ` axis=0 `) , by concatenating along the columns ( ` axis=1 `) : #CODE
` concat ` has not duplicated the shared indices , but it has duplicated the columns .
I'm essentially asking for an " upsert " ( insert , update ) operation .
First , the " insert " , of rows that don't currently exist in ` df1 ` : #CODE
Yet when I assign a name to the ` Series ` and convert it into JSON by using an ` index ` orientation , it somehow " gets " that the year is named ` year ` and the aggregate is named ` amount ` .
I am trying to merge tsv files using pandas but cannot get pandas to return the file contents correctly .
insert rows from Pandas dataframe into mongodb collection as individual documents using Python
I have been attempting to insert the rows of a pandas dataframe into a mongodb collection as individual documents .
The last step will be to insert the rows into a special collection on the mongoDB database as individual documents but I am completely stuck .
so you can later apply : #CODE
I parsed the dates on the original and the merge copied the data type automatically .
I assume now , I can use the same time difference logic on merge to filter only the time difference in seconds to reduce my df size too .
I can create a basic pivot table from this using : #CODE
Note : I think you have to do this after the pivot as an aggregation function doesn't have access to the other groups .
You will need to use a join operation : #CODE
I want to consolidate this information in a postgres database .
It's possible that the same ` person_id ` will speak to the same ` customer_id ` multiple times in a day so I just drop all of those rows from my joins to be safe .
As far as setting up your tables -- if your customer data contains repeats , I'd consider piping them into a temp table and then running a function to insert non-duplicates into your actual table .
I read about 3 months of data into memory , groupby at a date+person_id+customer_id level and take unique values and then join the files .
I then append the non-unique values ( this prevents Cartesian products duplicating my rows ) .
Pandas merge leads to scientific number
Imagine I have two dataframes almost the same and I want to merge them .
Although technically correct we should avoid posting answers that use ` apply ` where a vectorised solution exists as this confuses users
You can call ` sum ` to generate a ` Series ` that gives the sum of each column , then use this to generate a boolean mask against your column array and use this to filter the df .
` replace ` method of dataframe or series doesn't work in your case because it's trying to find string that match whole pattern which you are set in the ` to_replace ` parameter .
If you want to use the same ` replace ` method for Series for particular column you could use : #CODE
The replace method regex=True : TypeError : replace() got an unexpected keyword argument ' regex ' .Pyhton 2.7 My python version is too old .
I would like to split that file into files of len ( index ) = 2 , using linux : #CODE
' , so I had to wrangle it out first by dropping those rows - then I can apply .astype
Is there some way i can apply a lambda function to all members of the list columns in order to speed the following up ?
How to insert df into mongodb with a date.index field in ` isoDate ` format but not datatime64 [ ns ] / timestamps ?
Adding plot arguments to plots created with a pivot table in matplotlib
I'd like to do the same for a plot created via pivot table : #CODE
` so you can just filter these out using ` apply ` : #CODE
First I append row with max value of column ` odate ` to each group ( grouping by column ` tech `) .
Then you can use custom resample ` how ` method .
You can ` concat ` all the rows into one the result ` dataframe ` .
Also , I learned that making DataFrame from list can be easily don by concat function .
The transform method returns an object that is indexed the same ( same size ) as the one being grouped .
Thus , the passed transform function should return a result that is the same size as the group chunk .
Updated answer , needed to use ` transform ` to get back object with same index .
I could try to save each column as a separate key with index , then when I want to load data I simply load a couple of keys and join them on the index .
Is there a way to apply a ` math ` function to a whole column ?
Here I read the data , strip out the target variables and split the data into testing and training datasets ( this all works fine ): #CODE
Then create a boolean mask to find when the diff is more than 60 seconds : #CODE
To only keep the rows of interest , this creates a boolean mask for each condition and uses ` ` to ` and ` them together , you need parentheses due to operator precedence
You could use apply : #CODE
You can divide the balance by the ` transform ` of the ` groupby ` ( which keeps the same dimension as the original dataframe ): #CODE
Output ( after groupby ( ' a ') replace NaN by mean of group ) #CODE
IIUC then you can call ` fillna ` with the result of ` groupby ` on ' a ' and ` transform ` on ' b ' : #CODE
Index levels doubled when using groupby / apply on a multiindexed dataframe
I have a problem when using a groupby / apply chain on multiindexed pandas data frames : The resulting data frame contains the grouped level ( s ) twice !
When I group by both levels , the whole multiindex in doubled : #CODE
Do I have to remove it by hand every time I do a groupby / apply operation ?
My actual function that I apply looks different .
I just used the sum here to show the effect of a function that takes a dataframe and returns a dataframe ( " case 2 " in the documentation of apply ) .
#URL RuntimeWarning : Invalid value encountered in median
Short of going through every median function call is there a way to print the line number ?
exec ( compile ( scripttext , filename , ' exec ') , glob , loc )
Contact tracing may be easier if I interpolate so that every user has information for every time point ( say ever hour ) though that would increase the amount of data by a huge amount .
Also if the time increment is large for a particular person you probably want to interpolate or the probability of infection will be relatively low :) [ made a mistake and posted this as an answer . sorry ]
First solution with interpolate : #CODE
Second solution without interpolate : #CODE
I have added a version without interpolate .
It seems that you would have quite a few columns to aggregate - assume ' date ' is your timestamp , there seem to be seven , ie , data1 - data7 .
If you apply three aggregation functions to these seven columns ( mean , min , max ) you'll get 7 x 3 columns with a hierarchical ` MultiIndex ` ( where ` .agg ( dict )` works differently as for ' ordinary ' columns ) .
AttributeError : Cannot access callable attribute ' to_csv ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method But I still get this error while printing to csv file
You can use reshape strategies ( ` pivot `) so that you can naturally subtract the result .
You are looking for the melt functionality , which will allow you to do this in basically one line : #CODE
Then you can use ` apply ` ( or there probably is something better ) , to get the output you have above : #CODE
Between data points , I'm assuming people move linearly , so that if we know the location at 8:31 : 10 and 8:31 : 50 , then at 8:31 : 30 they should be exactly halfway between the two locations , and at 8:31 : 11 they should be 1 / 40th of the way between the points ( so interpolating as described here : Pandas data frame : resample with linear interpolation )
interpolate to figure out where they are at that second
Other answers suggest using the resample and interpolate functions , but that will take way too long with the size of data I have , and does a lot of unnecessary calculations seeing as I only need one point .
So I'm thinking I need to get the data just before 10:30 : 14 and just after 10:30 : 14 and interpolate .
So this sounds like you need a ` MultiIndex ` containing the gene and organs columns ( the latter called ' celltype ' in the first row ) .
I got this error : ` No numeric types to aggregate `
Then you could drop row with NA values with ` dropna ` or fill them with zeros with ` fillna `
Removing this with a find and replace worked .
How to normalize multiindex dataframe with its sum values in Pandas
I'd like to transform a Pandas series in case its dtype is ` np.timedelta64 ` .
I want to normalize this matrix with this formula :
Problems come with the isnull #CODE
Actually , playing with this , ` None ` is working with the ` isnull ` method .
You can call ` unstack ` and pass ` level=1 ` to unstack a specific level : #CODE
The result for ` agg ` must be a scalar .
So ` .aggregate ( lambda x : len ( np.unique ( x ))` probably would work too .
However , if you really want to use an anonymous function you can use apply : #CODE
In xlswriter , once a format is defined , how can you apply it to a range and not to the whole column or the whole row ?
In xlswriter , once a format is defined , how can you apply it to a range and not to the whole column or the whole row ?
You will need to loop over the range and apply the data and formatting to each cell .
I have 2 files I try to merge into 1 .
Do you want to use them for further data handling or just in order to merge two files ?
If you just want to merge two files there might be other ways to do this ...
Or if that is not what you want , and you wish to join on the index , initialise df as the first log file .
If you know what the header startswith : #CODE
pandas inner join on data frames without merging data
I would like to perform an inner join like in the following SQL : #CODE
I tried the pandas merge , but it gives me the result from both data frames !
which defaults to inner join according to the docs .
rather than a merge you want to just filter your second df by testing membership of the values against the other df using ` isin ` : #CODE
You need to specify that you want an inner merge ( the default is to do an outer ) .
You also need to first restrict ` df1 ` to the merge columns : #CODE
See also the pandas docs for a " brief primer on merge methods " .
@USER My understanding is that a join is going to be a * lot * faster than an isin call ...
That wasn't my point , only that the output seems to me to want a filter rather than merge the op may be after that in reality but there's been no response as yet
Well , an inner join ** is ** a filter , especially in the case where there is no additional columns to merge in .
What I need is normalize the rating column below by the following process :
In this case , grouping by `' user '` , and then for each group subtract the mean of that group ( the function you supply to ` transform ` is applied to each group , but the result keeps the original index ): #CODE
The code should read the column names from a csv file and I want to append the column names to %%column name .
I've written some code to clean up the tweet for some machine learning applications and I would like to apply the cleaning function to the entire Tweet column .
Could also create a set to store ` stopwords.words ( " english ") + [ u'rt ']` ( for faster ` in ` lookups ) and define it outside ` clean_tweet ` to make sure the append operation isn't being run each time .
If this is the reason , you would need to apply ` .fillna ( value )` prior to ` .groupby() ` with a value of your choice - for instance 0 .
How Numpy two-dimensional array ( or pandas DataFrame ) take intersection when there is a column has same value ?
I have tried to pick the intersection of A [ ' time '] and B [ ' time '] , and use np.where to select the rows : #CODE
Sorry are you just asking for the intersection like an ` inner ` ` merge ` ?
Thanks.I tried merge and isin , but there is a error , .Do you get a error two ?
How to loop through a dataframe , create a new column and append values to it in python
Then I want to append this unique list of value in a new column , at the same index position as the row value the changed value is derived from .
IIUC you want to pivoting dataframe by ` pivot ` .
With rolling apply is the window size time based ?
Merge on one key after the other
I want to merge on id1 , like this
However for the failed lookups ( that is when id1 is not available ) I would like to merge on id2 , so the result looks like #CODE
Conditionally aggregate ?
Are you after [ ` resample `] ( #URL ) ?
it means that ` resample ` only works when the index is a ` datetime ` like index , so you'd need to set the index first#
I concatenate rows as follows ( I do not need to join neither merge rows with respect to some column , just put rows together ): #CODE
You can use a function to map . like the example .
I wanna know how to code fast in stack overflow , are there some one can tell me the tricks ?
At first , you can transform the first column to a set or list as you base columns .
The results should be a dataframe of shape ` ( 647054 , 5196 )` , which I then plan to ` concat ` to the original dataframe .
How can I append an extra row of string to the dataframe , for example : the average is : ( numerical value of the average of third column )
I tried to append a extra row , data.append ([ ' Average ' + w [ n ] *1 8]) .
pandas version 0.13 , you can use loc #CODE
You have to use loc as radar suggested .
This is how I apply your code : featuresA = [ col + ' _x ' for col in group.to_frame() .columns ]
How to merge two data frames with repeated values in pandas
I want to join these two dataframes in pandas .
Then join with message 1 , like : Desired Message List #CODE
Then I would just use pandas.concat ( list , 0 ) which would merge all the different arrays .
Pandas aggregate fails on function that uses an index
Aggregating functions are ones that reduce the dimension of the returned objects , for example : mean , sum , size , count , std , var , sem , describe , first , last , nth , min , max .
You can extract ` feature ` column from pandas dataframe and apply it : #CODE
Another approach which might be cleaner if you have a lot of conditions to apply would to be to chain your filters together with reduce or a loop : #CODE
... and DO NOT merge the cells containing the years across multiple columns .
My problem is that when i try to convert it to a sparse using the code ` s = sparse.csr_matrix ( A )` i get this error ` TypeError : Cannot cast array data from dtype ( ' O ') to dtype ( ' bool ') according to the rule ' safe '`
and I want to replace ` NaN ` within ` SP2 ` only if there is a non ` NaN ` value in ` SP1 ` .
If this happens I want to replace it with the value in ` SP1 ` .
Use ` loc ` and a boolean mask to overwrite the values , it's irrelevant if ' SP1 ' is already ` NaN ` as the result is the same : #CODE
Groupby with mask and transform
and for each uniqe ` POLY_KEY_I ` if ` Class ` == ` Meadow ` and ` SP_Percent ` > = ` 20 ` I want to transform ` MTGP ` into ` WMTGP ` .
mask = final.groupby ([ ' POLY_KEY_I ']) [ ' mask '] .transform ( ' MTGP ')
File " C :\ Users\Stefano\Anaconda2\lib\ site-packages \pandas\core\ groupby.py " , line 2439 , in transform
` mask = df.groupby ([ ' POLY_KEY_I ']) [ ' mask '] .transform ( ' MTGP ')`
` mask = df.groupby ([ ' POLY_KEY_I ']) [ ' mask '] .transform ( ' any ')`
I completely change your solution to ` groupby ` with ` apply ` custom function ` f ` .
However , I have two dataframes that I would like to join into one .
In this one , the values are the same , but after the joining , you may end up with different values based on how you will join them .
If you only want to merge on ` index ` , ` pd.concat ` would be the way to go ( docs ) : #CODE
If you want to merge based on multiple column conditions , you should look at ` .merge() ` ( docs ) .
Using ` merge ` , you can also combine ` index ` and ` column ` conditions like so : #CODE
You could use ` merge ` for that 2 dataframes on ` id ` and ` sample ` column .
Then output join to original ` df ` and drop rows with ` NaN ` values .
pandas - mask dataframe by column name
I would like to apply a boolean ` mask ` in function of the name of the column .
mask : #CODE
Now I would like to obtain a boolean mask based on the column name , something like : #CODE
mask #CODE
Well you can just do ` df.columns == ' col1 '` which will return a boolean mask
@USER yes but it returns a mask only over the column names ` [ True False ]` , not on the corresponding values .
As noted in the comments , situations where you would need to get a " mask " like that seem rare ( and chances are , you not in one of them ) .
As noted in the comments , if your data frame contains nulls , the mask computed this way will always be ` False ` at the corresponding locations .
Thanks , but it returns a wrong mask , populated only with ` False `
You could transpose your dataframe than compare it with the columns and then transpose back .
Let's play more with transpose : #CODE
Actually , come to think of it , your transpose method only works by coincidence too .
I would like to populate a pandas dataframe from attributes of a list of classes generated via ' append ' .
Are there other columns that you need to unwrap at the same time , or other columns you could append to the index ?
If you have a lot of columns , it may be easier to take the ' set ' column with a single column that has unique values so you can merge back into the original frame after unwrapping .
Updated to unwrap separately and then merge back .
I'd like to drop the dates from 2015-09-11 to 2015-09-15 , so my df would look like : #CODE
I guess you could replace all these ` elif ...
I am sure that could be do with aggregates ( lambdas func ) or apply .
Or did you mean join / merge operations ?
IIUC you can use ` merge ` and ` combine_first ` .
How to apply drop_duplicates to grouped dataframe ?
I'm trying to drop the duplicate rows in each chunk of a grouped dataframe .
Then sort the array and the image in the same manner e.g. get the sort indices from the original array and apply them to the image array .
how to iterate a Series with multiindex in pandas
I have about 25 data frames with identical column headers that I need to append to one another .
Merge 1 results in 2000 rows , merge 2 results in 2700 rows , merge 3 results in 3892 rows ...
What i'm trying to do is where the timestamps in ` times ` fit into the resampled ` Bin_time ` , append the ` Bin_time ` and ` ave_knots ` to the end of the row .
In the top dataframe , why did you merge ` Bin_time ` and ` ave_knots ` with the other columns ?
Then you could reindex it to the appropriate " times " with ffill .
This works by setting the index on ` df2 ` to the ` Bin_time ` column , so that ` reindex ` using " forward fill " will automatically put the bins in the right place .
If it's not too much trouble can you explain how the reindex ( ffill method works ) ?
At a high level , ` reindex ` just samples the current columns at new index positions ( very handy for date / time functionality ) .
I am looking for the best way to join them on their common indexes .
2 ) Joining either of the multiindex dataframes with the single index ` DataFrame ` works : ` df1.join ( df2 )` or ` df0.join ( df2 )` .
If you transpose your data so that the ` index ` you show below makes up the ` columns ` and your ` xticks ` make up the new ` index ` , you should get what you are looking for .
I will actually use a very similar structure to insert such field into a database .
I found an issue about a similar error when using the apply method and that bug was fixed .
Since they fixed for apply I used the apply function to do what I want .
I used the apply method on the grouped data , and the easiest way to get the ' identifier ' was to get the groups keys .
tz-aware datetime series producing UTC-based .date() output in pandas series apply ( lambda ) operation
Pandas apply function - comparing each row to entire column
The ` groupby ` function makes sense , but I'm having trouble defining the appropriate apply function .
EDIT : I think something along these lines may work , but I'm not sure what the appropriate way to reference the time columns to replace " XXXXXX " should be .
But you can access a column inside an ` apply ` function .
This functionality makes sense - you can use an apply to compare each element to a fixed value ( in your example , ` x < 12 ` and ` 6 < x < 12 `) .
I had hoped using a grouping criterion that produces a greater number of smaller segments to apply the function to would speed up the calculation , but this doesn't seem to be the case in practice .
I then want to join to message_A for a single column message .
With the final result just been a single column string ( drop Nan ) .
( I do not want to drop full rows )
@USER I did check that after I got the variable to insert into the query properly .
( not very familiar with pandas ) so maybe you should find the location of the parameter where you want to insert the value using string.replace() or regex and then pass it to pandas ?
The easiest way I found to insert the variable into the statement was to utilize the read_sql function in pandas and set the variable as a parameter .
This will insert the variable ( s ) into the string query .
To get around this , I found that you can apply a date function to the column in sqlalchemy , similar to this issue .
This works because we can use a boolean array ( ` row.astype ( bool )`) to select only the elements of ` df.columns ` where the row has True .
When you do ` row.astype ( bool )` you get something like ` array ([ False , True , True , False ] , dtype =b ool )` .
How to use one date-indexed pandas DataFrame to map over another one ?
I want to map over the prices so that they are all deflated .
I made a function for comparing days to quarters but I'm new to pandas and I have no idea how to use it to map over my DataFrame #CODE
You could resample the quarterly data to daily frequency so that you have corresponding dates for both series .
For the missing values that result from the upsampling to higher frequency , you would presumably forward-fill past values , but you could also backfill future values .
In ` pandas ` , using the resample method , this would imply - assuming you have created ` DateTimeIndexes ` for both ` DataFrames ` : #CODE
You may need to use ' ffill ' or ' bfill ' for the reindex depending on your deflator time series .
You then could drop ' Cash ' column or edit your initial table #CODE
How to get Pandas column multiindex names as a list
Use ` apply ` and pass ` axis=1 ` to call ` describe ` row-wise : #CODE
Based on this post on stack i tried the value counts function like this
You have to remove first and last ` [ ]` from column ` genres ` by function ` str.strip ` and then replace spaces by empty string by function ` str.replace ` #CODE
Then at second level 2 or multiple sub / child nodes can be found based on language filter condition.so after filtering i need to drop the main dataframe at level1 .
How can I convert ` result ` into a row with columns , each time I append ` flights ` ?
Eventually I think I will write a function to replace the hardcoded list assigned to xlsAlpha , so that it can be a virtually unlimited amount of columns .
How to concat a dataframe in pandas ?
count = len ( data_dish ) ## counting dishes ordered
You should be able to get a list of dataframes in a list and then concat them .
Loop and append dataframes #CODE
Then concat the list into the full dataframe : #CODE
But when I apply the code , I get the following Error : #CODE
However , when I try to apply that function in order to write the output to a new column in a pandas data frame , it's returning None .
You can just create a temporary ` DataFrame ` from the records in ` df.dish ` and join it back to original ` df ` .
You could read the file with ` header=None ` , drop the duplicate rows ( which keeps the first per default ) , and then set the remaining first row as header like so : #CODE
How to transform values into columns after group by in pandas ?
Method #2 : add a dummy column and then pivot : #CODE
Is there a straightforward way to " insert " zero records for missing sample data ( like this question , but I have two columns functioning as indices , and I don't know how to adapt the solution to my problem ) or have Matplotlib plot with holes ?
Python append value to list of unique objects
I would like to append the new value / value-name for each object .
It sounds like you are attempting to merge two dataframes based on a column , though it's not entirely clear .
I was attempting to join two dataframes on a column ( example from above link ): #CODE
formatter function to apply to columns elements if they are floats , default None .
Can you insert a ` print os.getcwd() ` into your python script and rerun it using both methods to see what it outputs ?
I want to append a column to df , so that it looks like this : #CODE
One option is to use an apply : #CODE
Another which may be more efficient ( if you have larger groups ) is to use a transform : #CODE
You can also use ` map ` : #CODE
It's interesting that map doesn't require the ` .get ` , any idea why that is ?
My understanding is that this just uses the keys to perform the lookup , same as if you pass a ` Series ` and similarly it will bork and generate a ` KeyError ` if the label / key doesn't exist , in that case doing ` apply ( lambda x : other_dict.get ( x , other_val ))` would at least not go mental if the key doesn't exist .
Oh , so you want to interpolate for ` NaN ` , rather than ignore them .
If you interpolate , using linear interpolation , you will get the same regression , but with as many points as you have .
I want to apply a function to groups of ` x1 ` based on the columns in ` x2 ` .
Also this ` x1.groupby ( list ( zip ( x2 [ ' e '] , x2 [ ' f ']))) [ " b "] .transform ( " mean ")` gives me a ` GroupByError : len ( index ) !
= len ( labels )` which looks incorrect to me .
That said , it may be better to go ahead and do the join ...
How to use groupby transform across multiple columns
I have a big dataframe , and I'm grouping by one to n columns , and want to apply a function on these groups across two columns ( e.g. foo and bar ) .
But ` transform ` apparently isn't able to combine multiple columns together because it looks at each column separately ( unlike apply ) .
e.g. I could use ` apply ` and then create ` df [ ' new_col ']` by using ` pd.match ` , but that would necessitate matching over sometimes multiple groupby columns ( col1 and col2 ) which seems really hacky / would take a fair amount of code .
I also want to replace these missing values , I have used ` pandas.filna() ` but it doesn't effect .
: Guess need to use the groupby function , but because of lack of experience , do not understand how to apply it to my problem .
I've also done my Google searches but I do not feel found results apply to my issue .
using the ` apply ` method of the DataFrame , with something like :
Actually apply produces the ` < built-in method values of dict object at 0x00 ...
I also tend to not use apply but it can be convenient ( for readability or when using certain functions for example ) .
Anyway in my test ( on python 3.4 / pandas 0.17 ) the following statment ``` df [ ' word count '] = df.apply ( lambda x : dict ( Counter ( x [ ' test '] .split ( " ")) .items() ) , axis=1 )``` make a new column with the word count ( not sure why you use join * and * split on your text ) .
You can also use list comprehension to avoid apply , like ``` df [ ' word count '] = [ dict ( Counter ( i [ 1 ] [ ' text '] .split ( " ")) .items() ) for i in df.iterrows() ]```
You are right , join isn't needed .
` Quick timer test shows apply is bit faster than direct method : ** apply method ** ` 1 loops , best of 3 : 14.6 s per loop ` ** direct method ** ` 1 loops , best of 3 : 18.2 s per loop ` .
Then you can append new Series with range from 1 to length of ` NaN ` values in first row .
I think you need concat all chunks to dataframe by function ` concat ` and then apply function : #CODE
The problem is exactly this : if I do a concat I would not have enough memory to do anything .
Thanks again , but I don't have a lot of constants on my dataframe and even one column would surpass what I have of RAM ( as I said in the question ) so any concat wouldn't help .
Finding unique elements in cells of pandas DF and expanding DF to include columns with the names of those unique elements
And then we can pivot this in many ways .
Try defining the fuinction and then applying it using [ ` map `] ( #URL ) or [ ` applymap `] ( #URL ) from pandas .
You can use the ` apply ` and ` applymap ` from pandas .
Function to drop duplicate columns in pandas dataframe
But I don't how to apply that to my code either .
Many of those values we don't want , but that's okay , because we're only going to insert the ones we need .
We should clip to 0 , though : #CODE
and finally we can use the original ones where mask is False , and the new values where mask is True : #CODE
( Note : this assumes the rows we need to fill look like the ones in your example . If they're messier we'd have to do a little more work , but the same techniques will apply . )
Especially because Pandas ` .apply() ` now runs through the first apply twice , to find out if it can take a shortcut approach .
It's been doing that for as long a I can remember ( a few years ( ! ) at least ) :) The apply still happens in python so if you have a lot of small groups the dummy_column + groupby sum will blow apply out of the water .
@USER - I am new to pandas.I am guessing from you explanation and needing to apply some some map reduce type of thing is going on ?
But it is more similar to a database style group by and aggregate , than the Hadoop style .
Pandas expects to see the result from the aggregate to be a singularity , be it a list , an array , number or string , it should be one .
I'm trying to do multiple aggragations over a pandas dataframe , the problem is that I want to keep the column over I aggregate #CODE
Then you can resample and just need to decide whether you want to forwardfill the value corresponding to the start date or backfill value corresponding to the end date .
See resample docs .
I think , if I can make this new dataframe , I'll be able to append it to the old one .
Then I ` pivot ` and ` fillna ` dataframe .
I try add all columns as recommended output by function ` merge ` : #CODE
Force pandas groupby transform to return floats instead of ints
If I use groupby apply , everything works fine : #CODE
But if I use transform , the output is rounded ( bad ): #CODE
The result of ` transform ` has the same data type as the columns in the original data frame .
My approach don't use ` map ` , but functions ` stack ` , ` groupby ` , ` pivot_table ` , ` merge ` and ` cumsum ` : #CODE
I tried those ideas and failed , so maybe I am just missing how to apply them in my situation :(
Then you can apply the numpy / scipy vectorised methods for computing cosine similarity as in Whats the fastest way in Python to calculate cosine similarity given sparse matrix data ?
I tried changing it to boxplot , but it made no difference , it still returns the same plot .
In fact , it doesn't seem to matter what I use for the ` kind ` parameter , I'm always getting the same graph ( tried pie , hist , hexbin , etc . ) .
Change string encoding of pandas MultiIndex
#URL when applying ` groupby ` and ` aggregate ` to the result of the Fourier Transform .
I need to transform it into datetime format , but when I do it like this : #CODE
This calls ` rsplit ` and takes the last element and then calls ` zfill ` on these , we then join it back with the rest of the string to get back what we want .
Aha , the use of ' shift ' , together with ' cumsum ' .
What I would like to do is interpolate prices for the fruit over a consistent time frame for each fruit type .
The only question I have is that rather than extend the price estimates throughout the whole range of dates in the pivot table , I would like to have the first and last dates for each fruit be the first and last dates in the original dataframe .
For example , the value of the second fruit gets cut as the interpolation breaks the weekly period .
How to create new column and insert row values while iterating through pandas data frame
It's because ` apply ` method works for column by default , change ` axis ` to 1 if you'd like through rows :
0 or index : apply function to each column
1 or columns : apply function to each row
Like indicated by Anton you should execute the apply function with ` axis=1 ` parameter .
the ` apply ` documentation mentions :
How to apply a function to two columns of Pandas dataframe
Pandas pivot table fixing value columns
I pivot it and write in db among otherthings , #CODE
Then my pivot will fail I mean invalid , #CODE
I guess in that case I would not use pivot table for this .
How to strip and uppercase a column in a dataframe derived from Excel file -- avoiding unicode / str error message
Now what I want to do is to strip the ` Gene.Symbol ` column and upper case it with the following command : #CODE
Sorry why do you want to strip here ?
@USER : I need to strip it for ' normalizing ' .
Because later I need to perform join with another table .
So it would be possible to have a unique constraint where I would just overwrite the old data with the new data ( for example , sometimes the roster information is not updated so I occasionally need to read old data and replace it ) .
If you're using ` apply ` , the speed difference is minimal ; you should feel free to use ` iteritems ` .
If I am focusing on a single dataframe , and want to use function to do calculations to each column , is iteritems() also better than apply ( lambda x ) ?
For me , ` apply ` is cleaner , but you don't get the name .
I often have MultiIndex indices and I'd like to iterate over groups where higher level indices are equal .
to apply various aggregation functions as described in the docs .
If you provide some detail on what you'd like to aggregate and how , happy to add an example .
Or you can group by day and aggregate by sum : #CODE
Or group by year , month and day and aggregate by sum : #CODE
` ( a [ -1 ] -b [ -1 ]) / len ( b )` is one " chunk " and one more of them is added in each " iteration " ( year ) via multiplication with a ` numpy.arange() ` array .
Generally speaking , you'd apply an " easing function " over some range .
I want to apply a function to every group in a ` groupby ` object , so that the function operates on multiple columns of each group , and returns a 1 x n " row vector " as result .
Thanks for the help with unstack and combining the year field with the quarter row .
I am looking to extract out information from spreadsheet 2 and merge that information into spreadsheet 1 .
I know how to extract the information in excel , by utilizing an " index-match-match " function , but I am having trouble figuring this out in Python since it doesn't seem to fit a merge or join method ... or maybe I'm just missing the connection .
What happens is the program will insert a new column and will insert the data based on indexing the column and row to get to the single data point .
it will look in spreadsheet two across the top row until it gets to Product 1 and then down to until it finds Fresno and the insert 5,060 .
I have been unable to figure out any documentation for a merge / join scenario that i am experiencing .
IIUC you can ` stack ` dataframe ` df2 ` created from ` Spreadsheet 2 ` .
If you need ` merge ` by columns ` Region ` , ` Master Product Name ` and ` Branch ` , you have to lowercase both ` Branch ` columns for matching .
I was unfamiliar with the stack method and helps solve my dilemma .
But pandas is optimized for vector working and use high optimized functions like ` stack ` and ` merge ` and this is the best practice .
In the second branch , minimum is less than zero , so adding the abs ( min ) shifts the data up to zero ...
If you believe that input parameter without checking , then OK , I thought this was a feature to normalize the data .
Considering looking up [ reshaping data ] ( #URL ) , [ ` melt ` function ] ( #URL ) and [ ` merge `] ( #URL )
Can you show me how to use merge or melt ?
You can use ` loc ` to locate the rows and columns that match you criteria ( e.g. whether each element in the ` LOT ` column ` isin ` the series of ` lots of interest `) .
I want rename the Unnamed Columns with the prefix as the Previous column's name then append ' Count ' to that name .
You could use ` apply ` with ` rsplit ` .
I could of course write some sort of parser with if / else statements to translate ' > 2 ' into code , but I was wondering if there is a more elegant way ?
I'm generating an overlay for a map using pandas and used : #CODE
this uses ` abs ` to get the absolute value to generate a boolean mask and filter all the duplicate and near duplicate values out .
I'm trying to map ` total_seconds ` for that series hovewer I've got an error : #CODE
The question why it's not working with ` map ` method and why types are different ?
So the ` dtype ` gets converted to ` np.timedelta64 ` using ` map ` but you have no ` totalseconds ` attribute when this happens , but if you access a specific scalar value then it remains a pandas ` Timedelta ` which does
But I thought ` map ` method access directrly to each value instead of ` apply ` which for whole Series .
` apply ` would fail here also , ` apply ` is also a ` for ` loop it just allows you operate either column-wise or row-wise when called on a df , the type conversion is being doing by pandas here
Your DataFrame has ( probably due to the pivoting ) a MultiIndex of 3 levels and only 1 column .
but this will filter the data frame several times , one value at a time , and not apply all filters at the same time .
How can I merge the two DataFrames to get #CODE
use ` .loc ` with a boolean mask #CODE
Another method is do define a dict and call ` map ` : #CODE
It is easy to interpolate values in a ` Pandas.DataFrame ` using ` Series.interpolate ` , how can extrapolation be done ?
But when I do this with pandas ' implementation of hist , as follows : #CODE
Does anyone know a way to get the histogram values off a histogram generated by pandas ' hist method ?
The ` pandas ` implementation of ` hist ` does not return the counts , bins and patches that you get from ` matplotlib `' s version .
The column A1 can be used to join .
I tried using merge and join but that is not what I am looking for .
When doing your merge , you want to specify an ' outer ' merge so that you can see these items with an A1 key in one dataframe but not the other .
seems like just what i needed . slight issue . when i try it on my data i dont see the missing rows . there are a few records that are present in one of the dataframes and they dont show up after the merge . however , your solution works perfectly with the test data that you have provided here .
each group and perform some aggregate operation on all the pairs of a group .
Merging two pandas data frames in python using outer merge not identifying identical values
I am trying to merge two pandas data frames using a common column ( I imported both data frames from csv files ) .
The common column ( key column ) should have some identical values , but the majority are different , so I used an outer merge .
I realized after performing the merge that my output was actually just data frame 2 added on to data frame 1 , without the identical values from the key column actually merging into one row .
With your example data , the outer join renders your desired result .
pandas two dataframe cross join
I can't find anything about cross join include the merge / join or some other .
if used apply i will be how to implement them thx ; ^^
Essentially , you have to do a normal merge but give every row the same key to join on , so that every row is joined to each other across the frames .
Basically I would replace the empty brakets of lists with ` NaN ` and then dropping according to your rule .
your second line will replace nothing as there are no values ' [ ]' in the dataFrame .
After I sort by the rank field , what would be the best way to drop / delete all rows after the first occurrence of an " open " value ?
You can simply reindex the data frame when you sort it and then find the index location of the first instance of ' Open ' and slice the data to there ....
Using pandas resample / rolling_sum to calculate seconds time intervals
Consider a series sum function and apply it to a ` groupby() ` : #CODE
The basic pivot command is like this : #CODE
I had been looking for answers everywhere , and had been trying methods like `' unstack ' , ' droplevel ' , ' reset_index '` , etc , but wasn't able to make them work myself .
Here's a [ take ] ( #URL ) on pivot sort of operations .
pandas replace dataframe cell values
I would like to iterate over the entire dataframe , and for any cells that contain the less than sign , I would like to replace the entire cell with 0.0 .
You could try ` iloc ` or ` ix ` methods of dataframe from column : #CODE
If I apply ` train_y.ravel() ` , then it becomes a row vector and no error message appears , through the prediction step takes very long time ( actually it never finishes ... ) .
Merging Pandas DataFrames ( LEFT join style ) produces strange results
I'm trying to merge two Pandas DataFrames with a many-to-one relationship .
This is normally a rather straightforward SQL-like procedure using a ` LEFT ` join , however I'm getting a rather unexpected result .
To do the merge I simply do the following : #CODE
In an attempt to find out where this might be going wrong , I tried to do the merge on only a subset of both DataFrames using ` head() ` : #CODE
If I then perform the merge I get the desired output without a problem , just only 100 rows .
This too will give me the expected merge output .
Why does the merge produce such strange results , unless I pull the DataFrames through ` head() ` first ?
However , if I do use ` head() ` , even with a great enough number to include all rows on both sides , the join works as expected .
A custom function should apply to a series of the dataframe , a boolean operator for example : #CODE
I'm not familiar with NumPy nor Pandas , but the error is saying that one of the objects in the comparison ` if len ( self ) !
= len ( other )` does not have a ` __len__ ` method and therefore has no length .
How can I reindex pandas dataframe to reset the starting index value to zero ?
I want to reindex the rows of my dataframe , so I run : #CODE
Did you try the reindex functionality ?
I currently want to apply several machine learning models on this data .
For the test data you can use the same set of categories using transform .
@USER -if you have any idea like list iteration way , then checking first char ( if startswith sub_ or sub_cond1 like this ) .Then also i am fine with that.please suggest .
Example : divide the dataset into N chunks , read each one in , do your filtering , write csv files , delete dataframes in memory , read next chunk , filter , append output to the csv files , etc .
@USER -so one more thing , can i apply chunksize filtering ( taking a chunk of dataframe like in the case of pandas dataframe , then doing some Op . then merging them -- sending back ) on dask dataframe ???
@USER -while """ import dask.dataframe as dd """ i am getting an error "" AttributeError : type object ' Series ' has no attribute ' dt '""" .Is there any module that should be imported before dask.COZ i have successfully installed dask and successfull in import dask ??
OK , see my edited comment above ( try using ` agg ( np.mean )` .
Do I have to normalize the third column of the resulting ` z ` with the maximum value ?
would give you the mean for the second row anyway , to operate row-wise you can use ` apply ` and pass a ` lambda ` : #CODE
However I'd like to melt the dataframe depending on the value in the Output column , i.e. I'll have two crosstabs one where the Output is only zero and the other when the Output is 1 .
Essentially I want len ( x ) slope_x values .
( There is no * i * in ` if abs ( df.START-df.END )= =20 : `)
I had to do " if abs ( df.ix [ i [ 0 ] , ' START ' - df.ix [ i [ 0 ] , ' END ') .
is broken , because abs ( df.START-df.END )= =20 returns a Series .
this would produce the desired result , because abs ( x.START-x.END )= =20 returns a boolean , not a series .
Well you overwrite ` dish ` on each iteration you should append to a list or similar container and return this , also this is very similar to your previous question , what's wrong with ` ( s / s.sum() ) * 100 ` ?
Use the loop to generate a list of the values you want to return - start with an empty list and append each new value .
That is , you don't overwrite the result in every iteration but append it to the list .
Or use the merge method : #CODE
First I started by using ` pd.rank() ` on the data and then I planned on then using ` pd.cut() ` to cut the data into bins , but it does not seem like this accepts top N% , rather it accepts explicit bin edges .
Is there an easy way to do this in pandas , or do I need to create a lambda / apply function which calculates which bin each of the ranked items should be placed in .
When I apply this to my data set it says there are 419 posts in the top 0-5 % percentile , when in actuality in my data set of 1674 samples , there should only be 84 samples within the top 5%
Is there any convenient way to normalize each row by row's maximum ( divide by row's max )
You can use ` apply ` and apply a lambda row-wise : #CODE
Bear in mind that this will replace all values in the group with the first value , not just ` NaN ` values ( this seems to be what you're looking for here though ) .
where the index column represents a value , and the ` freq ` column represents the frequency of occurance of that value , as in a frequency table .
Prior Research : This so post on getting max values using groupby and transform and this other so post on finding and selecting most frequent data are informative but don't work on series ( the result of count_values() ) or just trip me up ...
I think you can use ` join ` , ` sort_values ` .
` df.to_sql ( con=cnx , name= ' some_table_name ' , if_exists= ' replace ' , flavor= ' mysql ' , index=False )`
I only want the two new columns and to drop the rest of the file so that it looks like :
Now I need to transform that to :
The issue now is that if I try to do this : df [ ' dif '] = df.groupby ( pd.TimeGrouper ( ' D ')) [ ' dif '] .cumsum() , I get : ' ValueError : cannot reindex from a duplicate axis ' .
now i need groupby the dfn_grouped by " key1_x " and concat to dict like A_x : A_y #CODE
g_dics = dfg.groupby ([ ' key1_y ' , ' key1_x ']) .apply ( lambda x : dict ( sum ( map ( dict.items , [ d for d in x.dic ]) , [ ])))
Wonder if it would help to pivot / unstack the data ?
Had to pivot the data and create NaNs .
If list of functions passed , the resulting pivot table will have hierarchical columns whose top level are the function names ( inferred from the function objects themselves )
Can you please advise how can i rename hierarchical columns , say " len " to " n "
pandas pivot dataframe structure
pivot table
how can I replace industryName with tradeDate and remove that blank row ?
I am trying to transform the categorical values into corresponding integer values and further building decisio trees
I am having trouble getting the dict to append even though this seems to be supported in pandas
Merge DataFrames and discard duplicates values
2 ) More generally , is there a faster / more clever way with ` pandas ` to read and merge multiple csv files than doing manually : #CODE
I have a problem , where I'm trying to replace sql-case when statements with pandas
You can just do group again and pass a dict of funcs using ` agg ` : #URL
You can use ` apply ` with a ` lambda ` to return the name of the column , here we compare the value row-wise against the max , this produces a boolean mask we can use to mask the columns : #CODE
Here is the boolean mask : #CODE
drop duplicates from a dataframe by a date field in python
2.And i checked with drop.duplicates ([ ' dt ']) , and drop.duplicates ([ ' other columns ']) also .
By other columns works fine except dt field
3.my question is , if this is due to datetime.date field i am passing to column dt .....
4.In case of converting to str ( currentdate ) it works fine.The comparision works great and drop duplicate is fine.why such behavior is avoided by datetime ( dt field ) in 1st run and followed from 2nd run onwards .
When you write and then re-read the csv you don't parse the date column as a datetime so it comes in as ` str ` dtype , you need to pass to ` read_csv ` ` parse_dates =[ ' dt ']` to read the strings as datetime
After removal , I join the list back together into a string and write that to a new column of df , at the same index position as the corresponding row .
Or just use ` replace ` : #CODE
I already tried replace , but that is not an option as I have to count the list elements later . with df.column.replace ( ' x ,? ' , '') I stll add an element to the list , meaning it is still counted even when ' removed ' from the list .
but here the solution does not use replace but sub ...
I tried your way , but as I said , when I do ` df [ ' columnNewNew '] =d f [ ' new '] .apply ( lambda x : len ( x.split ( ' , ')))` I still get the length of ` col2 ` , not the " new " length
I already provided two different answer solving the original problem ( with ` apply ` and a ` vectorized ` one )
I meant the difference between sub and replace .
So i group and aggregate the count for per user-product pair
Merge text file with a csv database file using pandas
I'd like to merge the text file with a csv database file based on column E .
yes , change to the same column name then using merge to combine two files .
I want to transform the previous data frame to a new one that looks like this : #CODE
You can pivot your original dataframe ` df ` setting the correct values for index , columns and values .
The ` fillna ( 0 )` replace ` NaN ` with ` 0 ` values .
I basically want a column to be sorted from smallest to largest , but I don't actually want to sort these , I want to replace values that are " wrong " in the sort .
The DataFrame is already sorted how it needs to be , I just need to replace some values that this ' id ' column has wrong .
You can use ` diff ` to find where the values are not equal or increasing , set these to ` NaN ` and then call ` ffill ` : #CODE
You can use double square brackets to force ` apply ` to be called on a ` df ` , this allows you operate row-wise , then use a user defined func to compare the current row value against all row values prior to current row , this generates a boolean mask to select the invalid rows and assign ` NaN ` to these and then ` ffill ` : #CODE
Ok , I'll fix in a couple hours when back at my mac , the pivot step can be done differently .
Updated & simplified without pivot .
Unclear why you have duplicates but you can use a boolean condition to mask the df and return the col of interest : #CODE
The mask on your original df : #CODE
I would like to merge year and month to one column in padas datetime format .
So you need to use ` loc ` method in your case : #CODE
Finally i gave up on ` ix ` and decided to make ` loc ` work , as ` Anton ` recommended i try : #CODE
Got me thinking that loc only accepts strings which finally worked : #CODE
replace each column of pandas dataframe with each value of array
I want to replace non-null value in each column with each value in array , and the result will be , #CODE
What am I getting wrong in my pandas lambda map ?
which I read as for each ` x ` in the series ` df1.var1 ` , apply the function ` np.percentile ( df2.var1 , x )` , which finds the percentile of ` x ` in the series ` df2.var1 ` .
For you case you could try ` loc ` : #CODE
Your loc solution with @USER ' s improvement solved my issue .
You can use ` loc ` : #CODE
This allows me to control the layout , but I can't apply it to bar charts .
I have looked at this Pandas : add a column to a multiindex column dataframe and pandas dataframe select columns in multiindex .
If so , the below should get you there - if you first apply ` set_index ` and then ` unstack ` , ` pandas ` creates the ` MultiIndex ` automatically , which you can then ` swap ` by ` level ` and ` sort ` as you wish : #CODE
How to apply a expanding window formula that restarts with change in date in Pandas dataframe ?
So in your example , do ` df.set_index ( ' Date Time ')` and then ` groupby ` and ` apply ` .
Pandas loc automatically converting datetime to long
If override_col isn't null I want to replace the corresponding time_col record with the override_col record .
You could use ` reindex ` to expand ` series1.index ` to include ` series2.index ` , filling missing values with 0 .
Conditionally insert rows into pandas DataFrame
I need to then call the Identity Apple , find its min and max dates and insert rows ie months in order to interpolate between the two points so the end result becomes #CODE
The problem is that although I can loop through a list of identities and get all rows associated I cant seem to find a way to then insert extra rows , especially without a nasty for loop . essentially I need to bridge the date gap and fill the associated Identity values with zeros .
My suggestion is create the full theoretical DF , merge with data and fillna : #CODE
This is good for one Identity and year , loop over years and Identities , append all created DF's into a list and pd.concat ( list )
pandas - insert values from one data frame into another
You could chose ` inner ` join instead of the default ` outer ` .
If you want more fine-grained control ( ` right ` or ` left ` join ) , ` pandas.merge ` is also well documented .
Say for instance you wanted to drop any row that had ' c ' in a column #CODE
You can use ` apply ` together with a lambda expression to check for the target word in each column .
I was looking at concat and merge before but it didn't make sense to me because I completely did not realize there can be groupby function used .
I use ` reindex_like ` for align dataframes and then ` where ` and ` loc ` for filling column ` Value ` of new dataframe ` df ` : #CODE
if you find a subset of your data that reproduced the problem , feel free to edit your question and append it , and I'll take a look .
Without seeing the data and assuming you want the resultant prediction as a column in the second data ( df2 ) frame you can apply the kn.predict() using the .apply() function and specifying the vertical axis .
Heres the info on apply .
Interval-matching PeriodIndex with DatetimeIndex during merge
But if you want ` merge ` : #CODE
I decided to align all ` df_other2.period ` with ` df2.period ` ( and make them ` DatetimeIndex `) and merge as usual .
pick a person to join the team
otherwise let him join the team and update ` met ` variable of the whole team
I understand all the error messages , and I also have a potential fix which involves pre-creating the DataFrame ` cov [ tpd ]` the way I want it , then use indexing to insert the output from ` cov_var() ` .
But that is a few extra lines of code to create the multiindex for ` cov [ tpd ]` and then inserting the data .
Essentially you are looking to do a lookup for that we can use ` map ` on the boolean series so the following will add a boolean flag : #CODE
can we have two conditions in map ?
From your question I guessed you do want to set all rows where the group aggregate ratio is less than 50 .
I'd overwrite the ' b ' column using ` transform ` and then drop the duplicate ' a ' row using ` drop_duplicates ` : #CODE
You'd have to provide data so that I can reproduce , one possibility is you have ` NaN ` values perhaps , ` transform ` is designed to return a Series aligned with the original df , it's unclear how it can generate more rows that the original df
I want to replace all ` NaN ` in a particular row by the corresponding value of ` average_emission ` of the same row .
lexsort error when slicing pandas multiindex dataframe
` KeyError : ' MultiIndex Slicing requires the index to be fully lexsorted tuple len ( 3 ) , lexsort depth ( 0 )'`
= data.sign() .shift() ` should work rather than use ` apply `
Currently dask.dataframe does not implement the ` shift ` operation .
In principle this is not so dissimilar from rolling operations that dask.dataframe does support , like ` rolling_mean ` , ` rolling_sum ` , etc ..
Actually , if you were to create a Pandas function that adheres to the same API as these ` pandas.rolling_foo ` functions then you can use the ` dask.dataframe.rolling.wrap_rolling ` function to turn your pandas style rolling function into a ` dask.dataframe ` rolling function .
Python pandas merge keyerror
Consistently getting a keyerror when I try to merge two data frames .
Output ( of transpose ) #CODE
HINT : Use the Theano flag ' exception_verbosity=high ' for a debugprint and storage map footprint of this apply node .
how to extract two different pandas series element and map to rows in dataframe's column
python pandas : computing argmax of column in matrix subset
Now lets find argmax of colA for each frame #CODE
I think it's better to use method ` argmax ` instead of ` np.argmax ` : #CODE
How to replace NaN in a row by first element of that row of a data frame using Pandas
I have random NaN values and I want to replace it by the nearest element in the row .
Since you want to do this for a row , you can transpose your data frame first and use the corresponding index ( a column after transposition ) with the ` fillna() ` method .
You have to use apply .
You're going to have to iterate over your list , get copies of them filtered and then concat them all together #CODE
A solution without loop but ` merge ` : #CODE
I want to transform this to #CODE
You can use ` factorize ` : #CODE
Here ` factorize ` returns a tuple containing array pairs : #CODE
I want to add a new column to my dataframe and map it to a dictionary .
I was about to reindex the df data frame and have it indexed with 0 , 1 , 2 , 3 , ...
I am trying to merge two pandas dataframes together on index but i am getting error ...
When i try to merge , i get this error - `" [ ' TP121 ' ' TP135 ' ' TP283 ' ..., ' TP251072 ' ' TP251178 ' ' TP251355 '] not in index "` when they are clearly in both the indexes .
Wonder why the merge didn't work .
For your task ` pd.concat ` is better then merge .
But with ` merge ` you could merge two dataframe with ` reset_index ` on column ` rs# ` : #CODE
why I need to apply nth to the whole grouped dataframe
Because you need first apply function ` nth ` for all group and then get first rows of group .
It is together ` df.groupby ([ ' a ' , ' b ']) [ ' c ']` and then apply function ` nth ` .
In your 2nd example , I don't understand why I can't just do ` g [ ' c '] .nth ( 0 )` and why I need to apply ` nth ` to the whole grouped dataframe and _then_ select ` c ` .
How to find and replace specific values in pandas dataframe
Ipython histogram - replace old histogram with new one
How to replace a value in dataframe column with series index
Simply passing the ` freq ` column to ` plt.hist ` does the trick .
But I don't currently have a way to insert all the ` functionA.__name__ , functionB.__name__ , etc .
IIUC , given your ` concat ` dataframe ` df ` you can : #CODE
Pandas replace nan with mean value for a given grouping
I am trying to replace the NaN values with the gic_industry_id median / mean value for that time period .
However , I am struggling to figure out how to map the NaNs to these means .
If you want to apply it to all columns , do ` df [ df 0 ]` with ` dropna() ` : #CODE
If you know what columns to apply it to , then do for only those cols with ` df [ df [ cols ] 0 ]` : #CODE
Pivot Pandas Dataframe with a Mix of Numeric and Text Fields
I am able to use MySQL auto increment functionality to get the race counts as a separate column to pivot on in pandas , but it does not solve the whole problem .
Then the desired DataFrame can be expressed as the result of a ` set_index / unstack ` operation : #CODE
The ` unstack ` operation moves the ` race ` index level into a column level .
using pandas in python to append csv files into one
You can ` concat ` .
Actually 0 is the default value for ` axis ` in ` concat ` .
No worries , pandas is a great tool if you actually want to do some computation on the data , it is not the tool to use to concat a few files into one
Then I used function ` concat ` for joining this list to final dataframe .
I add parameter ` header=None ` to ` read_csv ` and ` to_csv ` and add parameter ` ignore_index=True ` to ` append ` .
How to merge two grouped-by Pandas Dataframes by a common column ( ID ) together ?
You could use ` join ` #CODE
or ` merge ` by using ` reset_index ` of ` df1 ` , ` df2 ` #CODE
But , I could see your aggregate function don't have reducer .
Basically above function is not a aggregate function .
Few example of aggregate functions are , #CODE
access previous rows in python dataframe apply method
You might want to your attempt with ` shift ` or ` rolling_apply ` methods .
Next solution is use ` map ` : #CODE
How can I plot a Python Pandas multiindex dataframe as a bar chart with group labels ?
This [ SO post ] ( #URL ) shows how to add the labels leveraging the multiindex , but still requires calculation .
` periods = map ( lambda i : calc_time_delta ( df , i , i + 1 ) .seconds , range ( 7 ))`
I have tried ` hist ` and ` kind =b ar ` , but neither was able to sort by age , instead graphing both age and response separately .
To generate a multiple bar plot , you would first need to group by age and response and then unstack the dataframe : #CODE
For this dataset if I delete the first 3 rows the fit is greater than 0.995 , I have tested this but I want this to be a general expression so I can apply it to other datasets .
Merge csv's with some common columns and fill in Nans
Linearly interpolate missing rows in pandas dataframe
You could convert your ` JD ` values to a ` DateTimeIndex ` and ` resample ` to daily frequency ( ( see docs ) .
Use pandas merge : #CODE
` join ` works on the index , which you don't appear to have set .
Just set the index to ` ID ` , join ` df ` after also setting its index to ` ID ` , and then reset your index to return your original dataframe with the new column added .
Pairwise matrix multiplication with MultiIndex
You can use groupby size and then unstack : #CODE
However whenever you do a groupby followed by an unstack you should think : pivot_table : #CODE
Maybe there's a better way involving ` pivot ` , ` pivot_table ` or even ` get_dummies ` ?
Having ` user_id ` as the index seems appropriate to me , but if you'd like to drop the ` user_id ` , you could use ` reset_index ` : #CODE
I resample it like so : #CODE
See Linearly interpolate missing rows in pandas dataframe for an explanation of where the dataframe is coming from .
Have you tried simply print ( df ) before resample ?
I did , before resample it is fine .
The ` Val ` columns will probably not have a numerical dtype for some reason , and all non-numerical ( eg ` object ` dtype ) columns are removed in ` resample ` .
Use ` merge ` .
I think if you don't transpose then maybe an unstack will help ?
if ` axis=0 ` is thought of as column-wise , why then does ` drop ( 1 , axis=0 )` drop a row ?
So when you use ` df.drop ( 1 , axis=1 )` you are saying drop column number 1 .
Unexpected difference between loc and ix
I've noticed a strange difference between ` loc ` and ` ix ` when subsetting a DataFrame in Pandas .
If not , why are ` loc ` and ` ix ` designed this way ?
ix can very subtly give wrong results ( use an index of say even numbers )
you can use whatever function you want ; ix is still there , but it doesn't provide the guarantees that loc provides , namely that it won't interpret a number as a location
Instead ` ix ` returns ` NaN ` when given a list
As @USER says , this is ( at least for ` loc `) the intended and documented behaviour , and not a bug .
The documentation on ` loc ` / selection by label , gives the rules on this ( #URL ):
This means using ` loc ` with a single label ( eg ` df.loc [[ 7 ]]`) will raise an error if this label is not in the index , but when using it with a list of labels ( eg ` df.loc [[ 7 , 8 , 9 ]]`) will not raise an error if at least one of those labels is in the index .
For ` ix ` I am less sure , and this is not clearly documented I think .
But in any case , ` ix ` is much more permissive and has a lot of edge cases ( fallback to integer position etc ) , and is rather a rabbit hole .
But in general , ` ix ` will always return a result indexed with the provided labels ( so does not check if the labels are in the index as ` loc ` does ) , unless it falls back to integer position indexing .
In most cases it is advised to use ` loc ` / ` iloc `
Plotting a DataFrame using seaborn isn't difficult , nor is sorting the rows of the DataFrame to align with the cluster assignments .
I want to drop the all the rows which don't satisfy a given condition , in this case the condition is that the column can't contain the word ` secure `
I want to drop the row at the place , not to have a function that return ` None ` if the condition isn't meet .
Which return a boolean array , but I don't know how to used it to drop the row .
Now , how can I use this array to drop the columns ?
As for your question , you can either use the ` drop ` method or do it yourself : #CODE
` os.getcwd() ` gives you the path C :\ Users\name\folderName and then we join the filename to make the complete path .
I have a Pandas dataframe containing Latitude and Longitude data for an abundance of points , and I would like to clip them to a shapefile ; ie : everything outside the boundaries of the shapefile is dropped .
I have searched on Google , but ' clipping ' seems to refer to data being cut off by map legends / axis rather than what I'm describing .
I want to drop the first item in the list so that I am left with the last value without it being in a list .
` apply ` a lambda to access the last element : #CODE
But am stack on how to output the data from this loop : x
You may need to strip the filename down to what you really want but this is trivial
EDIT3 : If you include the boolean mask generation time in the benchmark , the ` groupby ` method outperforms the others by nearly an order of magnitude #CODE
pandas pivot table with average time
I have been working with pandas to do analysis on time series data and have become stuck with integrating them into pivot tables .
I want to create a pivot table with this time information but when I try : #CODE
DataError : No numeric types to aggregate
Another method would include ` pivot ` .
and then pivot the table according to your values : #CODE
You're using the mask generated from the comparison so where it's ` False ` it returns ` NaN ` , to get rid of those call ` dropna ` : #CODE
The mask here : #CODE
I have looked at both ` Transform ` and ` map ` but without much success .
Pandas - Merge and Groupby different dataframes and create new columns
I want to apply a step function to transform ` d ` based on values in ` d ` and ` num_original_introns ` , like so : #CODE
I know that this is invalid , and it is not possible to apply a pair of conditionals like this , but I can't seem to find an alternative from googling .
Individually replace NaN in pandas.dataframe
Why you couldn't use ` apply ` with ` axis=1 ` then ?
IIUC you could use ` apply ` with ` axis=1 ` and ` fillna ` with your custom function : #CODE
If you want to get column names for missing values you could apply or use that function for processing : #CODE
Even after taking out the categorical explanatory variable , you still have other variables that shift the predictions .
I am using a mixture of both lists and pandas dataframes to accomplish a clean and merge of csv data .
I then append this data frame to a list " dummydata " and loop to the next api
Maybe you meant to append to the lists each time like you do to DateList but all the other ones you just rewrite over every time you get a hit in the second loop .
I think you're looking for transform : #CODE
How to create mask with multiple alternative values ( Pandas DataFrame )
I am trying to create a mask to select all rows with value ' a ' or ' b ' in column ' xx ' ( I would like to select out row 0 , 1 , 3 , 4 ) .
If you really wanted a mask you could use or ` | ` : #CODE
When looking online , I tend to see examples of ' hardcoded ' variables , but I don't get how to apply this to a dataframe column - I found that I should use strptime to identify what format my date column is , but I don't know if this has any effect ( I get the same error if I comment out the convert_dates apply method ) .
In this example , what would be the best way to drop the columns that start with ' c ' , ' d ' , ' e ' , and ' f ' , knowing that they will all include an ' f ' somewhere .
Later I join all the strings in each row into one value but it seems like it should be easier to manipulate the data at this stage where everything is in different columns .
python pandas- apply function with two arguments to columns
I failed using replace and dictionary , so I used a merge .
Next , I use ` transform ` on the ` groupby ` to take the min value of these quarters we just calculated .
Using ` transform ` keeps this value in the same shape as the original dataframe .
Continuing off of part 1 , you can merge the values back on to the original dataframe .
At that point , you can write a custom function to subtract your date strings and then apply it to each row .
pandas : concat data frame with different column name
I want to concat x and y into a single column like this keeping their ids #CODE
And how if I want to give a new name for the concat column ?
Updated to hopefully make the steps more transparent - I did in fact set ` id ` as ` index ` before ` concat ` .
If ordering of rows is not important , you can use ` stack ` : #CODE
Next solution is use ` map ` : #CODE
I have tried a few things with pandas pivot and pandas unstack but it seems that I am missing something as none of it worked .
I guess I also would need to use pandas cut at one point for the binning but I failed to reach this stage in my analysis .
I want a something that gives me the same values of ` res ` but without having to create a whole ` Series ` object ( or ` DF `) only to resample it sater .
` len ( df.resample ( ' W-MON ') .index ) = 886 ` and
` len ( df.resample ( ' W-TUE ') .index ) = 885 ` ,
I'm calculating the variance ratio t-statistic for each day of the week on a rolling basis ( that is each " i ") but then need to add it back to this original df .
The data that I am using has some missing days and I am trying to interpolate that data .
Now I have two questions ; while using ` inter_lin_nan ` function , it does interpolate data but it also changes the next day data and the next data is totally different from the one available in the excel file .
Use as frequency to add the required frequency of timestamps to your time series , and uses interpolate() to interpolate nan values only .
I need to aggregate by one field , but the other field has to be the maximum of all the values for each group .
" cond " is the condition , " measurement " are the different replicates ( 2 per condition ) , and " t " is the time dimension . to average across time in each replicate , i use ` agg ` : #CODE
how can you flatten the dataframe returned by ` agg ` so it can be plotted with seaborn ?
I want to transform this to be equal to : #CODE
` data = data.astype ( bool )` should give what you want .
0s and 1s are what I'm trying to replace .
Maybe ` map ` can do it : #CODE
resample gives different final dates in pandas
I'm having trouble to understand whats going on when I resample into weekly data this way , specifically into mondays .
resample defaults to mean .
@USER resample does the mean : for values between 12-07 ( inclusive ) up to 12-13 ( not-inclusive ) ...
@USER that wouldn't be a resample , resample needs an aggfunc which defaults to mean ( you can also sum etc . ) .
You can reindex to just slice out a particular date_range #URL
I am using the pandas.DataFrame.dropna method to drop rows that contain NaN .
Then we can easly transform matrix using zip function , sort result rows by first element of tuple and take N first items : #CODE
Nevertheless , any idea why your code give me this error : AttributeError : ' Series ' object has no attribute ' items ' n the " pd.DataFrame ( map ( it ( 0 ) , sorted ( row [ 1 :] .items() , key=it ( 1 ) , reverse=1 ) [: n ])
thank you , it looks a lot better with the explicit loop ... not sure whats going on , probably a version thing ; Just to abuse your will a lottle longer , can you explain what does the " map ( it ( 0 )" is doing in this example ?
I see that " row [ 1 :] .order ( ascending=0 ) [: n ] .iteritems() " returns an " itertools.izip " object , but not sure what " map " is doing .
` map ( itemgetter ( 0 ) ..
append to data to from df2 to stock and result of conditional .
Since these DataFrames share the same index you can index using the mask : #CODE
I think , i worked it out using - result = pd.concat ([ df3 , df2 ] , axis = 1 , join = ' inner ')
@USER that or a plain old inner join : ` df3.join ( df2 , how= " inner ")` .
You can ` groupby ` dataframe by column ` Name ` , ` apply ` custom function ` f ` and then select dataframes ` df_A ` and ` df_B ` : #CODE
I.e. , all my y-labels are cut off , and margins are too big .
If I have a pandas Series composed of strings , say something like [ " ab " , " ac " , " bc " , " ab " , " ab " , " abc "] , is there a built in function to replace each of the strings by some integer corresponding to the string ?
Merge seems a little bit overkill there , you can do it base ` python ` with list comprehension in one line : #CODE
I went to the link and now I understand the short cut .
This is modified from this answer , which uses the same binary search method for inserted values to find a rolling cumulative sum , but it looks into the past ( lag ) rather than the future ( lead ) .
I think the drop might be modifying the indexes somehow so that merge can't read ' uid ' but I don't know how to fix it .
Because pandas ' read_csv method will not strip spaces in the first line for you .
So you have to use ' uid ' as merge key or change the first lines in your file1.csv or file2.csv .
You may look at this question to save a little strip work by hands
I have a dataframe and I'd like to apply a function to each 2 columns ( or 3 , it's variable ) .
For example with the following ` DataFrame ` , I'd like to apply the mean function to columns 0-1 , 2-3 , 4-5 , ....
this should run through all the rows by expanding based on range until the value on ` StartDate ` matches ` EndDate ` .
Instead of creating it , we can append it to initial StartDate .
Then use apply and return a series indexed on the expanded set of dates for each row ( Series of Series = DataFrame ) .
You can select by ` loc ` with ` any ` and specify ` axis ` : #CODE
You can ` apply ` function to ` groupby ` where use another ` apply ` with ` replace ` ` 0 ` to ` NaN ` : #CODE
You can use this apply function : #CODE
IIUC you can use ` stack ` : #CODE
What I don't understand is that if I do ` len ( df [ ' branded ']` I see 8173 , but if I do ` df [ ' branded '] .describe() ` the first line of output is ` count 5158 ` .
So what does ` len ( df [ ' branded '] .convert_objects ( convert_numeric=True ))` show ?
` print len ( df [ ' branded_items '] .convert_objects ( convert_numeric=True ))` shows 8173 , but then ` print df [ ' branded_items '] .describe() [ ' count ']` shows 5158.0 .
Ah - I see the problem now - ` df.fillna ( 0.0 )` does not replace in-place , I needed to do ` df.replace ( ' None ' , 0.0 , inplace=True )` first , then ` df = df.fillna ( value= 0.0 )` .
Tried to replace the code with ` loc ` : #CODE
Another option is to use ` df [ ' D '] .apply ` to expand the items in the list into different columns , and then use ` stack ` to expand the rows : #CODE
Although this does not use explicit ` for-loop ` s or a list comprehension , there is an implicit for-loop hidden in the call to ` apply ` .
Yes and no -- there is a way to do it using ` apply / stack ` which avoids the * explicit * double for-loops , but it is actually much slower than the list comprehension-based solution shown above .
Cannot interpolate in pandas dataframe
How can I interpolate the time series like this ?
This should be quick , but none of the pivot / groupby work I'm doing is coming up with what I need .
I want to pivot it , so that the Index is basically YrMonth and Letter , the Period are the columns , and the Amount are the values .
I understand Pivot in general , but am getting errors when I try to do it with multiple indexes .
and I would like to transform it to count views that belong to certain bins like this : #CODE
But it only gives aggregate counts and not counts by user .
The aggregate counts ( using my real data ) looks like this : #CODE
Do I then save that as a function and apply it to the dataframe or could I just run that on it's own and have it append the column to the original df ?
How would I translate that to a standalone function ?
Please , explain , how to extract features in that sample case , how to train a model and so on , or provide a good tutor for that case ( I'm not able to translate sklearn tutor to my case ) .
If you simply want to fill the NaNs , append [ ` .fillna ( args )`] ( #URL ) to your concatenation .
Are you sure you want to drop the index ?
The NaNs may be telling you that you have predictions for observations that don't appear in one of your resample folds .
How do aggregate functions in pandas work ?
` agg ( F )` simply calls ` aggregate ( F )` .
If so , ` aggregate ( F )` becomes a call to the ` .F ` method of the ` groupby ` object , which is an optimized ` cython ` method .
` apply ( F )` calles ` _python_apply_general ` .
Finally , ` apply ( lambda x : F ( x ))` will be slightly slower than ` apply ( F )` due to the additional ` lambda ` function .
I then pivot : #CODE
In the numpy array above , I would like to replace every value that matches the column ` country_codes ` in the dataframe ( df_A ) with the value from the column ` continent_codes ` in df_A .
Right now , I loop through dataframe and replace using numpy indexing notation .
Hmm , one method would be to construct a df from your array and then call ` map ` on each column : ` a = pd.DataFrame ( arr ) a.apply ( lambda x : x.map ( df_A.set_index ( ' country_codes ') [ ' continent_codes '])` or something like this
Approach #2 : As a variant , you can also create the mask with a comparison between ` np.searchsorted ( oldval , arr , ' left ')` and ` np.searchsorted ( oldval , arr , ' right ')` as discussed in the solutions for ` this question ` and re-use ` np.searchsorted ( oldval , arr , ' left ')` again later on while putting values into ` arr ` for a more efficient solution , like so - #CODE
align on both row and column labels .
Then you can use a list comprehension to cycle through each label of that level and map the random number .
reindex to add missing dates to pandas dataframe
In Pandas , how to apply a customized function using Group mean on Groupby Object
Then I use apply method on the Groupby Obj .
I want to apply a group by on a pandas dataframe .
I am attempting to map this dictionary over various cases of self reported Twitter location that I have in a pandas dataframe to look for partial matches .
In the following example , I transform " A " , which is a column / variable of strings into numbers through a mapping , but it looks like a dirty hack to me #CODE
A simple map on a transposed dictionary should get you what you want .
How about using ` factorize ` ?
Never heard of ` factorize ` .
How to merge two dataframes based on the closest ( or most recent ) timestamp
I would like to fuzzy ` merge ` the dataframes with a join on the ` timestamp ` .
However , if the timestamps don't match ( which they most likely don't ) , I would like it to merge on the closest entry before the timestamp in ' A ' that it can find in ' C ' .
` numpy.searchsorted() ` finds the appropriate ` index ` positions to ` merge ` on ( see docs ) - hope the below get you closer to what you're looking for : #CODE
I think native apply is the best , but not .
You can use apply with lambda with is faster than your solution : #CODE
With len ( df ) = 5 #CODE
With len ( df ) = 10000 #CODE
Try to transform to_keep in a dataframe and then merge it with the original , like in Compare Python Pandas DataFrames for matching rows
I would like to merge two Pandas dataframes together and control the names of the new column values .
I want to merge them together to get a final data frame , joining on the ` org ` and ` name ` values , and then prefixing all other columns with an appropriate prefix .
This seems to merge correctly and result in the right number of columns : #CODE
The ` suffixes ` option in the merge function does this .
Pandas : merge multiple dataframes and control column names ?
I would like to merge nine Pandas dataframes together into a single dataframe , doing a join on two columns , controlling the column names .
I want to join them into a single dataframe with the following columns : #CODE
I can currently merge two datasets together like this : #CODE
` merge ` only seems to accept two at a time , and if I do it sequentially , my column names are going to end up very messy .
Just found this #URL but I'm not sure it works for my example - I guess I need to concatenate , then merge somehow ?
You could use ` functools.reduce ` to iteratively apply ` pd.merge ` to each of the DataFrames : #CODE
To pass the ` on =[ ' org ' , ' name ']` argument , you could use ` functools.partial ` define the merge function : #CODE
I would like to append 7 new series formed by taking the difference of each of series3 through 9 and series2 ( i.e. series10 is series3 - series2 , series 11 is series3 - series2 , and so on ) .
The following code snippet worked fine until I added a couple of lines of code that referenced date but does not append or change it above it . with the simple case of setting #CODE
you don't even need to use ` apply ` : ` df [[ ' AccX ' , ' AccY ']] .values `
But if I want to do some operation on each row , I still need ` apply ` correct ?
when using groupby with multiple columns , how to NOT drop an unobserved combination of columns
I wonder if there is a way to NOT drop this combination and instead give it a count of 0
Minor , but " drop " seems the wrong word , given that the combination isn't there in the first place .
This works because the new MultiIndex contains the cartesian product of the levels : #CODE
i need to format the contents of a Json file in a certain format in a pandas DataFrame so that i can run pandassql to transform the data and run it through a scoring model .
In order to transform the data with pandasql queries , the data in the columns need to be in various data types ( i.e ' int ' , ' str ' , ' timestamp ' , etc . ) , what is best way to do this ?
following up on my comment above , ended up using cast ( variable_x as integer ) to transform the data in pandasql
This code gives me an attribute error when I apply the seasonal_decompose method :
Add observations to a pandas dataframe by expanding dates
Then I use ` melt ` to get all of the quarters in the same column .
Creating Period objects is expensive , so let's identify the unique quarters and then apply the period mapping .
Now we can map the ` qtr ` column in the initial dataframe and use a list comprehension to extract each of the four quarters .
I'm trying to determine the intersection of two lines .
The intersection was found near ` ( 0.96 , 37.19 )` : #CODE
If Date were an index / DatetimeIndex , you could resample : #CODE
@USER yes , like I say for the resample you need a DatetimeIndex .
Do you really need ` transform ` ?
Consider a groupby apply function with sort : #CODE
pandas dataframes merge with same column names , give priority to one
Any idea how it could be converted with cat or merge or some other function ( without having to manually " drop " the common columns before or after the merge , and merge must be still inner ) into the following output ( assumes " priority " is given to ` df1 [ ' A ']`) ?
You can transpose your dataframe , drop duplicates , and transform again .
You can just ` groupby ` a list of length ` len ( df )`
If you have 50 ` Series ` , you can use ` concat ` and ` groupby ` by ` index ` with ` sum ` : #CODE
You can trandform the series to dataframes and then merge them together using the date as key : #CODE
While there is a very nice example on obtaining the median of a distribution , I'd like to see whether this can be generalized for multimodal distributions for 1D data and particularly in the 2D case .
I have searched for a method to insert a Row into a Pandas DataFrame in Python , and I have found this :
How can I make the code insert a row WITHOUT overriding it ?
For example , if I have a 50 row DataFrame , insert a row in position 25 ( Just as an example ) , and make the DataFrame have 51 rows ( Being the 25 a extra NEW row ) .
Is it possible to insert a row at an arbitrary position in a dataframe using pandas ?
Possible duplicate of [ Is it possible to insert a row at an arbitrary position in a dataframe using pandas ? ] ( #URL )
With a small ` DataFrame ` , if you want to insert a line , at position ` 1 ` : #CODE
You can split column ` HISTOGRAM ` to new ` DataFrame ` and ` concat ` it to original .
or to drop the columns will all nana : #CODE
But , when I try to loop sequentially over the data and apply the same conversion it doesn't give any error .
I discovered that some values are just missing , the non obvious thing is why the conversion gives an error when trying to convert an empty string to float when ` map ( float , [ ])` doesn't .
Unlike with , for example , the ` cframe [ " tz "]` Series for which the ` name ` attribute will be set to " tz " , ` by_tz_os.size() ` does not have a name attached to it , most probably due to there being no obvious way to name groupby() results in general .
` by_tz_os.size() .index .name ` returns a ` None ` simply due to the index being a MultiIndex .
Python Pandas Dataframe filter and replace
But if it happens it should take two of these highest numbers and replace them with 1
Finally , convert the boolean mask to integers .
I want to interpolate for missing hours in the month to create the desired dataframe with 744 " dates " and 744 " values " .
Use the panda interpolate funtion
One idea is to use pandas.Series.fillna with ' backfill ' after the interpolation .
Also , do not set fill_value to 0 whe you call reindex
then use ` df.interpolate ` to replace the NaNs with ( linearly ) interpolated values based on the DatetimeIndex and the nearest non-NaN values : #CODE
Group on the column , get the count of each , and then unstack the results .
You could probably skip the ` fillna() ` step if you later drop the ` 0 ` values again ( that's why it doesn't say ` df_datafile2 `) .
Just three more things : 1 ) how do I completely drop the variables that have no values for that participant ?
I want to transform both of these to ` 2014-01-31 ` .
If the desired result is to group only on ' A ' and then insert a new column ' B ' that has ' one ' in each entry that can be done also .
In pandas v0.17.1 ( anaconda python v3.4.3 ) the replace function on ` datetime ` is broken .
I am trying to replace a string value in my ` DataFrame ` with new value .
The replace function fails #CODE
line 594 , in replace
line 3110 , in replace
line 2870 , in replace
return self.apply ( ' replace ' , ** kwargs ) File " / home / xxx / anaconda / envs / py3 / lib / python3.4 / site-packages / pandas / core / internals.py " ,
line 2823 , in apply
line 607 , in replace
if not mask.any() : UnboundLocalError : local variable ' mask ' referenced before assignment
As a workaround ( assuming you have no duplicately named columns ) , the Series replace still works fine in 0.17.1 : #CODE
merge every two columns on pandas.DataFrame
Possible duplicate of [ How to merge 2 columns in 1 within same dataframe ( Python , Pandas ) ? ] ( #URL )
Generator - join items of tuples : #CODE
Iterate through ` ArrivalDate ` and append the equivalent rows as follows , then Expected output :
( I need to check under DepartureDate if it matches with ArrivalDate , if it does match , I want to append its ID to the Departure otherwise " Arrival " will only be listed #CODE
The condition to have any ID appended to " Departure " in the JSON format , since i'm considering ArrivalDate , I want to check if the DepartureDate matches with Arrival on a given row and it does match , I want to append it
The problem is that I've got no values to merge it back with .
` str.contains ` return bool values .
Pandas DataFrame stack multiple column values into single column
You can melt your dataframe : #CODE
After trying various ways , I find the following is more or less intuitive , provided ` stack `' s magic is understood : #CODE
use ` factorize ` : #CODE
I can't seem to apply ` to_datetime ` to a pandas dataframe column , although I've done it dozens of times in the past .
I would like to resample this DataFrame : #CODE
How to do this with pandas , i.e. resample based on a condition on a column ?
I thought this was a common data-esque task : to resample at certain points , each time some amount B increases : here each time B increases of 4 units .
I would first remove the ones that are > 4 , then resample with the last : #CODE
If you don't want all the times in the middle , you have to use a groupby rather than resample : #CODE
I have an ` apply ` function that operates on each row in my dataframe .
The result of that ` apply ` function is a new value .
If you need to use other arguments , you can pass them to the ` apply ` function , but sometimes it's easier ( for me ) to just use a lambda : #CODE
How do I pass arguments to the ` apply ` function ?
Another thought was SQLalchemy ; I could shift the whole thing out of Pandas and in to the SQL realm and build functions that make use of the or_ function and SQLalchemy filtering ( like this : Using OR in SQLAlchemy ) .
It doesn't get more complex than this , I was only trying to get an answer that I can apply to many rows instead of just manually inserting that specific row .
max_len = max ( map ( len , rows ))
@USER weird , can you paste a gist of the entire stack trace #URL ?
What if I want it to replace based on ` y ` and not ` x ` .
However , if I replace all the `"` in a.csv to `'` , then the ` f_1 ` could be correctly infered into `' object '` .
can I add a columns this way df [ ' freq '] = df.groupby ([ ' a ' , ' b ']) .size() and delete the duplicates ?
argmax gets the first True .
Use argmax on the reversed Series : #CODE
@USER one thing is a little annoying is that reversing creates a copy ( IIUC ) ... you could drop to the values and use the numpy reversed view which may be slightly faster ( but IMO much less readable ) as essentially O ( 1 ) .
@USER I guess that it's not immediately obvious why ` argmax ` works here , still it's quicker which is what usually counts
You can use ` idxmax ` what is the same as argmax of Andy Hayden answer : #CODE
I've tried Boolean indexing but can't get it to work , and maybe there is a way to tell Pandas to skip the NaN results ... but the best approach seems to be a qualifier that says " apply this code , starting at the second row .
IIUC you can skip first row of column ` A ` of ` df_source ` by selection all rows without first by ` ix ` : #CODE
I think I understand your problem and in these cases , I usually find it easier to make a list and append it to the existing dataframe .
IIRC , iloc is being deprecated in future builds of pandas in favor of ix
Now , I would like to resample this time series by listing for each week the number of times a certain rail company departed from this station by origin .
Or you can select column ` Train ` and ` resample ` it by ` count ` : #CODE
Select one column ` Train ` , where is ` BritishRail ` using ` isin ` and resample it with ` count ` instead of ` sum ` : #CODE
After searching thoroughly on the material available and the tutorials , I guess I can replace all the ' ?
` map ` iterates over the values in the column to pass them to the ` lambda ` function one at a time .
This is called by ` map ` on each iteration : #CODE
So instead I used a dummy / hackish condition like ` if len ( data.split ( ' : ')) > 1 : ` .
I've looked into ` groupy ` and ` transpose ` but have not been able to produce anything close to what I'd like .
I have tried merge , but I need to only add one specific column ( ' addthiscolumn ') not all of the columns .
Renaming the ` columns ` you are merging on avoids having both in the resulting ` DataFrame ` , but you could of course just as well ` drop ` the redundant ` column ` post merge .
Because you just want to merge a single column , you can select as follows : #CODE
And want to insert it into an email .
How to create dummy variable and then aggregate using scikit-learn ?
OneHotEncoder requires integers , so here is one way to map your items to a unique integer .
Now create a OneHotEncoder and map your values .
You can merge ` tf ` with ` df ` using ` tf.merge ( df )` , example with results below : #CODE
Note how the result above differs from a simple merge , which does an inner join by default .
The method ` transform ` takes your grouped object and creates a series : #CODE
pandas : merge several dataframes
You can adapt the join method by setting ` how ` :
left : use only keys from left frame ( SQL : left outer join )
right : use only keys from right frame ( SQL : right outer join )
outer : use union of keys from both frames ( SQL : full outer join )
inner : use intersection of keys from both frames ( SQL : inner join )
Merge do note preserve the index ( as you can see in the above results )
Melt the Upper Triangular Matrix of a Pandas Dataframe
How can I ` melt ` only the upper triangle to get #CODE
First I convert lower values of ` df ` to ` NaN ` by ` where ` and ` numpy.triu ` and then ` stack ` , ` reset_index ` and set column names : #CODE
I the only thing to watch out for is if you have any ` NaN ` values that you want to preserve in the upper triangle ( ` stack ` will drop them all ) .
You might have to explicitly construct the multi-index and then reindex if that is the case .
As you asked , you can do this efficiently using ` isin ` ( without resorting to expensive ` merge ` s ) .
You can use merge if data size is big : #CODE
You can merge them and keep only the lines that have a NaN .
pandas : convert index type in multiindex dataframe
Hi have a multiindex dataframe : #CODE
I have a data frame and a series that I would like to return a rolling correlation as a new data frame .
So I have 3 columns in df1 , I would like to return a new data frame that is the rolling correlation of each of these columns with a ` Series ` object .
I am trying to interpolate data for some missing days .
As you can see 2014-12-28 is missing , so I tried to interpolate it using both Numpy and Pandas .
The problem is , both of these method does interpolate for the missing day , but they also change original data for 2014-12-29 .
You can use custom aggregation by ` agg ` and then convert column ` document_id ` to list : #CODE
I cannot use `' document_id ' : lambda x : list ( x )` in ` agg ` , because error :
I am trying to merge two geodataframes ( want to see which polygon each point is in ) .
Are you saying transform the subset of the data ( as per code snippet two ) , and then update df with this ? or transform the entire dataset ?
@USER The transform is the expensive part .
Then I can apply the max transformation to that grouping , and then recombine the two into one .
I'm still struggling with this . doing g.size() produces an effective count of the number of duplicates , and is very , very , fast , so I am trying to use this to get the unique_id and period_id pairs where size > 2 , then apply the max idea above to those which should be much faster , and then I'll need to recombine with the original data frame .
If so will append to answer .
I'm not familiar with transform , but I used .size() ?
Pandas DataFrame apply Specific Function to Each column
I don't know my way around the pandas plotting code to check , but I bet there is a ` if stacked : ` conditional someplace in the code and ` bool ( ' false ') is True `
Then I am trying to next column header v and trying to append with underscore with unique values of v .
You might get away with some batch insert operation , but why bother since pandas ' ` to_sql ` already does that for us .
I am not so familiar with stack exchange ' etiquette ' .
if you have a list of indices already then you can use ` loc ` to perform label ( row ) selection , you can pass the new column name , where your existing rows are not selected these will have ` NaN ` assigned : #CODE
I've tried using replace : #CODE
I can use map : #CODE
e.g. Pandas - replacing column values , pandas replace multiple values one column , Python pandas : replace values multiple columns matching multiple columns from another dataframe
You can use the replace method : #CODE
My only solution as of now is to replace the original NaNs with a unique string , concatenate the csv's , replace the new NaNs with a second unique string , replace the first unique string with NaN .
And append #CODE
In case you have a core set of columns , as here represented by ` df1 ` , you could apply ` .fillna() ` to the ` .difference() ` between the core set and any new columns in more recent ` DataFrames ` .
The issue with this is that it will replace all NaN values in that column .
How to replace infinity in PySpark DataFrame
Or do I have to take the painful route : convert PySpark DataFrame into pandas DataFrame , replace infinity values , and convert it back to PySpark DataFrame
Actually it looks like a Py4J bug not an issue with ` replace ` itself .
How to append on a dataframe with timezone aware timestamp column ?
I am able to append a new row to it if the timestamp column is timezone naive .
But if I set timezone for the timestamp column , and then try to append new rows , I get error .
Any help on how can I append new rows to a dataframe having timezone aware timespamp column will be greatly appreciated .
As an aside , rather than doing apply ( pd.to_datetime ) , just do pd.to_datetime ( df ) .
For instance , I would like to apply ( lambda x : x+ 273.15 ) on each columns which contain C data .
You don't need to use drop in case you using ` read_csv ` with ` header =[ 0 , 1 ]` because first row already read as header .
First you need to do 2 level header , then drop 1st row and cast ` reset_index ` to make it from 0 ( or you could omit that if you are fine with index starting from 1 ): #CODE
Is it possible to get that in the final output by changing the " how " in merge ?
I think ` on ` in function ` merge ` is for matching - better example with pictures is [ here ] ( #URL ) .
Works similar to merge function in R .
Hmmm , I think if you merge ` df1 ` and ` df2 ` by ` Stock ` , you get all other columns from df1 , so another columns too .
I have a panel of data that contains a rolling daily correlation matrix of two identical dataframes with multiple columns .
The replace method doesn't seem to be working for me .
If I call map or ` mapPartition ` and my function receives rows from PySpark what is the natural way to create either a local PySpark or Pandas DataFrame ?
Which dataframe operation do you want to apply if you had the rows combined into a dataframe ?
That doesn't answer my question I need it to operate inside a map call on a partition .
If there's a map that passes dataframe that would be good too .
Sorry , I couldn't understand ` map that passes dataframe ` .
These are different structures , with different properties and in general you cannot replace one with another .
How to reindex a a pandas dataframe by date range without deleting values
then to resample .
If you reindex , you're going to lose the rows with duplicate dates .
Would I resample the date_index or CEITest first ?
Could you give me an example of how to resample these data frames ?
Look up how to resample separately .
I think you need set index from column ` Order_DateC ` before reindex : #CODE
And final you can check ` notnull ` values by ` isnull ` with ` any ` : #CODE
I want to transform the data with year and month as separate columns .
I want to transform this into 13 columns ( 1 column for year and 12 for months ) .
One easy-fix method would be to replace the Month string with its equivalent number .
Pivoting a Pandas Dataframe containing strings - ' No numeric types to aggregate ' error
I'm trying to pivot a data frame with strings , to get some row data to become columns , but not working out so far .
What I would like to pivot to #CODE
DataError : No numeric types to aggregate
What is the way to pivot a data frame with string values ?
You can index the pivot in whatever manner you think matches the level of aggregation you desire .
A higher level question could be , is Pandas the best Python tool to pivot tables with string data ; it seems it might be ...
Edit : I have found a way to do that : I apply ` ast.literal_eval ` to each line .
You can use ` join ` with ` rsuffix ` : #CODE
Apply function to each column that returns the value associated with the ` index ` of the ` min ` ` abs ` value like so : #CODE
It looks like the dates are index values , so the abs and min functions aren't applied to the index .
You can use diff : #CODE
You could pass an argument to ` apply ` : #CODE
As the first way need ` join ` , so should way 2 be faster than way 1 ?
@USER , inplace parameters expects only a bool while ascending and by are allowing a list .
Using apply to go through row by row and test whether the value is numeric of string is the quickest way separate them .
I have 2 time-series files I wanted to merge both .
When I tried to merge I'm getting below error ( of course it's expected one because of the timestamp issue ): #CODE
I want to normalize my both categorical and numeric values .
1 . how correctly replace categorical columns ?
2 how normalize numeric columns in this case properly ?
Pandas-style transform of grouped data on pyspark DataFrame
As far as I understand , spark dataframes do not directly offer this group-by / transform operation ( I am using pyspark on spark 1.5.0 ) .
I have tried using a group-by / join as follows : #CODE
See DataFrame groupBy behaviour / optimization A slower part is ` join ` which requires sorting / shuffling .
If this is an exact code you use it is slow because you don't provide a join expression .
I wasn't aware of the Cartesian product behavior in df.join() ; I had assumed incorrectly that the default behavior was to join on any columns that shares the same name .
The most common issues ignoring configuration are related to Cartesian products and even if you provide a join expression it may not be optimized .
I have tried expressing it in terms of join or merge but have failed so far .
Is there any simple way to express that or will I have to use apply and create a new DataFrame ?
Then create a new dataframe ` df2 ` which uses ` ix ` to index column ` B ` of each group based on the value ` n ` from column ` A ` .
I'd also recommend using ` groupby ` but I think we can use ` pivot ` to simplify things .
First , we create a new C column with the column labels we want to use , and then we call ` pivot ` : #CODE
I am attempting to insert / update a df into a mysql table .
This is the schema of the tables I am attempting to insert / update data .
You could use ` apply ` with ` axis=1 ` to apply for rows with ` any ` method , if you have only one valid value and all other are ` NaN ` ( using @USER example ): #CODE
Depending where you want to insert the table , maybe ` to_html ` , ` to_json ` or ` to_latex ` are better options than plotting with seaborn .
using pandas and numpy to parametrize stack overflow's number of users and reputation
For the moment , let's drop ` pandas ` entirely .
We'll just need to normalize it by the total number of users in our sample to get an estimate of the CCDF .
So you want to find ` NaN ` in particular column and drop them ?
No I don't want to drop them .
You could do that with ` isnull ` and ` any ` methods : #CODE
If you want to subset your dataframe you could use mask with your columns and apply it to the whole dataframe : #CODE
Pandas median over grouped by binned data
I'd like to calculate for each user the median of the scores .
and then use groupBy and apply to calculate the median somehow ?
I believe you may be interested in weighted median #URL
I believe you need weighted median .
and I need to aggregate this data ( like a pivot table ) .
As suggested , look into the pandas package , put the data in a DataFrame , it can do pivot tables , or just use the groupby function .
and apply a function to the groups .
` team ` is the grouper , and we apply the function ` count() ` on column ` [ ' trips ']` .
We use 2 columns ` [ ' team ' , ' player ']` as the grouper , and apply the function ` sum() ` on column ` [ ' time ']` .
Python - how select / drop elements in data frames that have multiple ( 2 ) indices
How do I select all non CASH related positions ( there can be many ) - i.e. what is the command to drop the cash positions in this df ?
seems like it was just use the drop statement differently #CODE
How to append data to a DataFrame like this ?
To append a single row using an index .
Still same concept but instead of appending to lists in loop and then a bulk append to df out of loop , each iteration appends to df in loop .
As you can see the two methods that I'm using is " rename " and " insert " .
The real problem is how to transform / filter the dataframe
One would assume these are ` np.nan ` which may cause ` matplolib ` to interpolate hence the ` .dropna ` suggestion , but perhaps you are getting a constant for some other reason .
I think you are missing the step in line 3 above to convert the remaining ` timestamps ` to ` strings ` , otherwise ` pandas ` will interpolate to create an even-space series .
Merge pandas DataFrame with MultiIndex
I also tried to merge the frames like so : #CODE
IIUC you need groupby by ` Feed ` from multiindex and apply ` pct_change ` .
Then you can use subset of ` df3 ` , where column ` Rate_Return ` is ` notnull ` #CODE
In an other question some people are trying to insert a Pandas DataFrame into MongoDB using Python internal structures ( ` dict ` , ` list `)
I wonder if we can't insert instead a NumPy ` rec.array ` ( ` numpy.recarray `) to MongoDB using PyMongo .
but I faced some errors at insert #CODE
But this feels wrong and clunky as I have to drop the column levels ( which removes useful structure from my data ) and I have to manually rename the columns .
I tried to use the shift method like this #CODE
You can replace it with ` main_frame ` in your case .
You need to pivot your data .
For doing fractional calculations before hand , you might use ` groupby() ` then ` transform ( ' sum ')` .
You need to pivot the data frame .
I know how to do it via ` SQL ` using ` left join ` and aggregate functions but I do not know how to replicate this using ` Python ` / ` pandas ` .
So one awful solution is replace ` NaN ` to some string and after ` agg ` replace back to ` NaN ` : #CODE
I'm trying to replace all the commas with an empty space in a single column in a csv .
If something goes wrong with ` _try_coerce_args ` or ` mask_missing ` and it raises a TypeError or a ValueError , then we fall into the ` except ` and try to use ` mask ` even though it isn't defined .
IIUC you can use ` transform ` and ` mean ` .
Try a combination of transform ( ' sum ') , mean #CODE
Does this apply here and is it inclusive for rows but not for columns then ?
, ix method is primarily label based with fallback to indexing ... from docs online ...
For guaranteed position based slicing , you can use the ` loc ` indexer .
The ` ix ` indexer you are using , is more flexible ( not strict in type of indexing ) .
For this reason , it is recommended to always use ` loc ` / ` iloc ` instead of ` ix ` ( unless you need mixed label / position based indexing ) .
` seasonal_decompose() ` requires a ` freq ` that is either provided as part of the ` DateTimeIndex ` meta information , can be inferred by ` pandas.Index.inferred_freq ` or else by the user as an ` integer ` that gives the number of periods per cycle .
I'm trying to plot a simple line plot and insert a background image to a plot .
Now , I want to replace the values in ` a ` and ` b ` by the maximum value seen thus far .
I guess I can apply a rolling max on each group but is there a better way to do this ?
You can ` apply ` custom function , where find index of first 1 by ` idxmax ` and set rows to the end of group to ` 1 ` : #CODE
Merge / Concat Issue
Is there a way to just concat the comments onto each other with some sort of character ?
I think Merge would work well for your case .
This sounds like you want to do a ' left ' merge ?
You can specify to do ` inner ` ( default ) , ` right ` , ` left ` , or ` outer ` join , following ` sql ` logic .
My knowledge isn't that great of Pandas ( yet ) , but I'm guessing it's an " apply " or an agg() function but so far , syntactically , I'm banging my head from the syntax errors , but I appreciate any pointers in the right direction . ..
Also a similar approach is to use a pivot table with margins .
Merge rows with same id and time in pandas
pandas rank by column aggregate
Use of loc to update a dataframe python pandas
As you could see from the warning you should use ` loc [ row_index , col_index ]` .
My suggestion is merge the X data to Y data on axis 1 , calculate and rebuild .
Once you unstack , column ' X ' will also be NaN .
If you mean it shouldn't exist at all you could always drop the rows where ` X ` is ` Nan ` .
This has me looking at vectorization , and I've come across ` numpy.vectorize ` ( ( docs ) , but can't figure out how to apply it in this context .
Dataframe append a row
How do I append only one row to a new dataframe ?
I am using iterrows to so I have the index of the row I want to append to a new dataframe .
Inside this if statement I want to append that row .
Do you wan to append one of the rows from your DataFrame to the same DataFrame ?
If you wan to append a row to the DataFrame you have to assign it to a variable first .
For example if you want to append the first row in your DataFrame to the same DataFrame : #CODE
then , you can append a specific row in a new DataFrame .
You could groupby aggregate to list and join the list as below .
The main part of the solution is the pivot table , where we fill missing values with ` N ` .
I REALLY like this pivot table solution .
replace ` datetime ` with what ever your column name is
I used " to_datetime " to convert the original date format to a datetime object , in order to apply " tz_localize " to convert to another time zone .
Maybe help strip last 6 chars : #CODE
how compute rolling difference between open year stock price and closing price of that stock with a frequency one Day
what I want to achieve is for example after entering the date e.g 2015-12-22 , the rolling function should compute whether through the year starting from 2015-12-22 to 2014-12-22 the value ` open_price ` ( at 2015-12-2 ) - ` close_price ` ( 2014-12-22 ) increased or reduced and return the value then from 2014-12-22 to 2013-12-22 all the way to 1997 .
One of the ideas is use itterrows() and apply harvesine() function , if rows ' sequence ' parameter is not 0 and row's ' track_id ' is equal to previous row's ' track_id '
So , basically , apply haversine() function to all rows whose ' sequence ' !
I normally populate new columns using " apply , axis = 1 " so I would really appreciate any solution based on that .
I found that " apply " works fine when for each row , computation is done across columns using values at the same row level .
However , I don't know how an " apply " function can involve different rows , which is what this problem requires . the only exception I have seen so far is " diff " , which is not useful here .
@USER ` time_condition ` is just a mask that selects the right time window , which is subsequently used to create sub_df ; @USER Z I take 100 rows to not spend the night waiting for the output but the objective is to have the output for the entire dataset
Given your small sample data , I used a two day rolling average instead of 60 days .
By taking a pivot of the data with dates as your index and stores as your columns , you can simply take a rolling average .
You then need to stack the stores to get the data back into the correct shape .
Here is some sample output of the original data prior to the final stack : #CODE
Assuming the above is named ` df ` , you can then merge it back into your original data as follows : #CODE
In any case , you probably want your rolling average to be in multiples of seven to represent even weeks .
In order to avoid losing data on stores that just open ( and those at the start of the time period ) , you can specify ` min_periods=1 ` in the rolling mean function .
In our example , the NaN rows of the pivot table do not get merged back into ` df ` because we did a left join and all the rows in ` df ` have one or more Customers .
You can view a chart of the rolling data as follows : #CODE
You then need to merge it back it .
I , however , want to use the rolling mean to create a feature to feed into a ML model .
Therefore , I loop over the 1115 stores and apply your solution ( without min_periods ) , which is still much faster than my initial attempt .
It's pretty trivial to truncate the table before I write the data to it , but I feel like there should be a more elegant solution to this problem .
You can try ` loc ` : #CODE
Is there a way to do it without having to use the join ?
Every time I attempt to apply ` pd.offsets.Day ` or ` pd.Timedelta ` I get an error stating that Series are an unsupported type .
This also occurs when I use ` apply ` .
When I use ` map ` I receive a runtime error saying " maximum recursion depth exceeded while calling a Python object " .
What is a proper idiom in pandas for creating a dataframes from the output of a apply function on a df ?
Once I've done the apply I get feed objects back : #CODE
So , now I'm stuck with the feed entries in the " entries " column , I'd like to create a two new data frames in one apply method , and concatenate the two frames immediately .
How can I create pandas dataframe with column dtype being bool ( or int for that matter ) with support for Nan / missing values .
I have seen panda's ' melt ' function but it seems to only work if there is a single group .
This is not a typical application for reshape / melt type functions , so you're probably going to have to roll your own .
Build dictionary to rename ` columns ` for proper application of ` concat ` : #CODE
If you want to eke out some more performance , you're going to want to do this concat in numpy and then repeat the index ( though I'm not convinced it's worth the small gain that will give you ) .
Is it possible to create a brand new column , say ' N_index ' , within the concat function and assign it the value of str ( i ) ?
In some sense the index you wanted wasn't really coming from anywhere and I don't think it can be done during concat .
Another thread I was reading says it's possible to store more than one value per key , though ( using .append() ) but I don't know how to apply it to this situation .
eval ( ' ticker_ %s ' % fname = {} )
Yes , it would be a really bad , unexpected and unusual practice , and yes , you can use eval to do this ( I guess ) , and no , it would not affect performance or memory consumption too much
Now we can use numpy's argmax : #CODE
Pandas : apply returns list
I have the following function that I want to apply to each group : #CODE
How do I make the results of the apply operation the values of the " mean_to_date " column ?
you're getting lists back because your function ` previous_mean ` , when fed a dataframe , returns a list -- it has nothing to do with ` apply ` .
You said in another comment that you want one mean per row , therefore the function you apply should return a single value .
IIUC you can use ` expanding_mean ` , shift data by ` shift ` to ` 1 ` , fill ` NaN ` to ` 0 ` by ` fillna ` and return column ` mean_to_date ` : #CODE
query pandas dataframe with integer in first level of multiindex
I'm having trouble with pandas MultiIndex , if the first index is an integer .
Use ` loc ` instead of ` ix ` : #CODE
For ix it's strange why it's not working because from docs :
But they recommended to use ` loc ` in such cases to be more explicit .
That is indeed confusing and the source of the motivation that created ` loc ` and ` iloc ` .
How can I drop the date from this output ?
You could use ` dropna ` of ` fillna ` methods to drop or fill with values what you like
You could use ` groupby ` method for your hashes , then ` transform ` method for ` date ` column and ` iloc ` to get the first element .
What I want is a rolling count like this : #CODE
comparing dtyped [ float64 ] array with a scalar of type [ bool ] in Pandas DataFrame
You can transform either column ' a ' or ' b ' so they are both either float64 or bool .
How to apply a function ( BigramCollocationFinder ) to Pandas DataFrame
I am aware of the apply function for Pandas Dataframes , but can't manage to get it work .
If you want to apply ` BigramCollocationFinder.from_words() ` to each ` value ` in the ` Body ` ` column , you'd have to do : #CODE
In essence , ` apply ` allows you to loop through the ` rows ` and provide the corresponding ` value ` of the ` Body ` ` column ` to the applied function .
Pandas I want to drop rows with NaT based on another column's value
You can try ` notnull ` : #CODE
` notnull ` is more readable than a negated ` isnull ` IMO
hmmm , is possible replace ` -- ` to ` AA ` and then compare ?
You could also do apply : #CODE
It relates to a big data analytics course project and demonstrating the use of Spark / Bluemix and map / reduce is a requirement .
Tell ` value_counts ` not to drop NaN values by setting ` dropna=False ` ( added in 0.14.1 ): #CODE
Or even more efficiently , just check ` len ( dfd )` .
You can use ` diff ` , ` astype ` and ` cumsum ` : #CODE
Test ` len ( df ) = 10 ` : #CODE
Test ` len ( df ) = 10000 ` : #CODE
I have a dataframe which was converted into a multiIndex dataframe after doing ` groupby() ` and aggregation .
Use ` reset_index() ` to drop the team index column and make the player index column as part of the dataframe .
When printing we have to map the integers to a string , and use the end parameter of the print function to print a semicolon instead of printing a new line at the end .
How can I map each date ` d ` to the start day of the week containing ` d ` ?
Do I need to find the difference between each random number generated and store only the sets where the abs diff is 0.5 ?
Given the date_range , r , that data frame can be created as : ` DataFrame ( index=r , data=randn ( len ( r )) , columns =[ ' Random Number Generated '])` .
Can you please clarify me if you have any idea on the abs different thing
To see what's going on , try ` hist ( bins=1 )` , you should get 72 on the y-axis , the length of the data .
Thanks for the explanation stefen :) I tried the same but when I tried to create the bar : df_new = diff [ abs ( diff [ ' Random Number Generated ']) < 0.5 ] , df_new.diff() .hist ( bins =10 ) .
My chart above was for the 72 values , ie the full series , before dropping the values with ` abs ( diff < 0.5 )` .
Dataframe : shift expanding mean with groupby
I want to have expanding mean give the result excluding the current item , ie , the items prior average .
Before I had : ` df2 [ ' homewin_at_home '] = df2.groupby ( ' home ') [ ' hw '] .apply ( pd.expanding_mean ) .shift ( 1 )` which didn't shift the calculation .
I am trying to learn about rolling statistics .
Now I am trying to find the rolling mean of the series over the last 3 hours in a new column on a DataFrame .
I tried to find the rolling mean first : #CODE
If you want to take the rolling mean over the last 3 hours , ` rolling_mean ( df_new , window=5 )` should be ` rolling_mean ( df_new , window=3 )`
The results by ` rolling_mean ` is consistent with manually calculated rolling mean values .
Another way to confirm the validity is looking at the plots of calculated rolling mean . pandas.DataFrame prepares ` plot ` method to draw graph easily .
That's because of the definition of the rolling mean .
As long as your index is a timestamp ( as it currently is ) , you can just use resample : #CODE
IIUC you do that with ` interpolate ` method with ` method ` parameter equals to ` linear ` to do linear interpolation or ` nearest ` if you'd like to fill gaps with the closes values for your resampled dataframe : #CODE
For your resampling , ` resample ` method works only valid with DatetimeIndex , TimedeltaIndex or PeriodIndex .
So you could convert your column to ` timedelta ` then set it as index , resample , ` reset_index ` to get back to your original dataframe .
Maybe you could merge and then fill na's , for instance :
Merge , sort , Forward-fill calculate average : #CODE
But I think if I do this , it's better do a rolling mean or median ?
Additionally you don't need to name variables , what you want to do is just append each df to a list and then at the end concat all the dfs in the list
Consider using merge with ` right_index ` and ` left_index ` arguments : #CODE
When you set right and left index as TRUE , does this mean that it will only merge if all the indexes are the same ?
You can use ` loc ` and ` isnull ` : #CODE
Another approach is to use the property that ` NaN ` isn't positive or negative so if we negate the positive mask you also return the ` NaN ` values and we can set these to ` 0 ` also : #CODE
1- Is it possible to build a ` Pipeline ` in order to aggregate these three steps ?
If not , what is the closest plot that I can get , to plot the min , max , mean , median , std-dev etc .
Inputting 2 data frames then trying to use data merge or join : #CODE
I know how to merge to data frames if it has same # of rows and index , but i couldnt do it if i only want to do it using first file as a standard index .
I know how to do it in R using the merge function then by.x and by.y , but R messes up all my header names ( the ones up are just an example ) .
You can use ` join ` to merge on the indices : #CODE
For some reason , join is giving me error : ValueError : columns overlap but no suffix specified : Index ([ u'ID '] , dtype= ' object ') .
@USER if ID is the index then join works , if it's not you need to use merge .
Reading your files with ` sep= ' \t '` didn't parse properly for , but ` sep= ' \s+ '` did for your sample lines , and then the standard ` merge ` gives your desired result : #CODE
You can of course also move ' ID ' to the ` index ` and use ` .join() ` , ` .concat() ` , or ` .merge ( left_index=True , right_index=True )` with the appropriate settings for ` left ` merge for each .
See the documentation which is quite specific as to allowed inputs for the three basic selection methods : ` loc ` , ` iloc ` , and ` ix ` .
I feel like the pandas ` apply ` function may be able to do what I need , but I'm at a loss as to how to implement it .
What I would like to do is replace whatever is not a date , with a date based on a variable that represents the last_update + 1 / 2 the update interval , so the items are not filtered out by later functions .
I realized the ' None ' was actually a None object , which just let me do a bit of alteration to your suggestion ( if x ! = None ) , then the other replace solution .
` apply ` calls ` tuple_to_timestamp ` for each row of ` df [ ' orig ']` .
The first operation can be done with an ` apply ` function returning a series ( see the accepted answer to this question ) , followed by a horizontal ` concat ` operation ( i.e. , with ` axis=1 `) .
The second operation is simply pandas's own ` stack ` .
You could use ` groupby / cumcount ` to assign column numbers and then call ` pivot ` : #CODE
Using ` reshape ` is quicker than calling ` groupby / cumcount ` and ` pivot ` , but it
bool operator in for Timestamp in Series does not work
There are duplicate times , which should map to duplicate indices .
loc can take a list or array of labels to look up : #CODE
I have a pandas data frame ( result ) df with n ( variable ) columns that I generated using the merge of two other data frames : #CODE
Values missing : Overlaying points on boxplot subplots from a pandas dataframe
I would like to plot each column on a separate subplot as a boxplot with values from df1 , and overlay it with scatter plots ( just one value per boxplot ) from df2 .
The following code does not give me any error but only one value on the second boxplot is visible ..
Getting boxplot subplots only is easy using #CODE
use ` diff ` with ` fillna ` : #CODE
I've written the following code to give the ` UpThisPeriod ` column but I can't see how I would aggregate that to get the ` ConsecPeriodsUp ` column , or whether there is way to do it in a single calculation that I'm missing .
This can be done by adapting the ` groupby ` , ` shift ` and ` cumsum ` trick described in the Pandas Cookbook , Grouping like Python s itertools.groupby .
I have a groupby object I apply expanding mean to .
Next , stack so you can follow each team : #CODE
Then , define what's a ' win ' , calculate overall record and apply ` expanding_mean ` : #CODE
Since you have references for both games and teams , you could ` merge ` and ` filter ` to get your preferred layout .
You could ` stack ` the frame like so : #CODE
Is there a shortcut not to create the MultiIndex ?
I have already asked a similar question here but couldn't manage to apply the solution for my problem with two columns .
You can extract from columns ` values ` and ` datetime ` new ` Series ` and then merge them with original dataframe ` df ` by ` concat ` : #CODE
You can use a nested list comprehension together with ` concat ` to extract the core part of your dataframe that repeats each row the desired number of times ( based on the length of its ` values ` list .
Then you could do your calculations with that subset dataframe and then using ` loc ` you could assign it to the original dataframe : #CODE
You could use ` dropna ` method to drop all rows which contain ` NaN ` in any of the columns .
You could also check ` NaNs ` with ` notnull() ` and ` isnull ` methods .
How would I stack the table such that I have one column for country name , one column for year , and one column for gdp figures ?
