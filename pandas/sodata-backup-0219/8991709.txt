Title:
Why are pandas merges in python faster than data.table merges in R?

Q:
I recently came across the pandas library for python, which according to this benchmark performs very fast in-memory merges. It's even faster than the data.table package in R (my language of choice for analysis). 
Why is `pandas` so much faster than `data.table`? Is it because of an inherent speed advantage python has over R, or is there some tradeoff I'm not aware of? Is there a way to perform inner and outer joins in `data.table` without resorting to `merge(X, Y, all=FALSE)` and `merge(X, Y, all=TRUE)`? 
Here's the R code and the Python code used to benchmark the various packages.

A1:
The reason pandas is faster is because I came up with a better algorithm, which is implemented very carefully using a fast hash table implementation - klib and in C/ Cython to avoid the Python interpreter overhead for the non-vectorizable parts. The algorithm is described in some detail in my presentation: A look inside pandas design and development . 
The comparison with `data.table` is actually a bit interesting because the whole point of R's `data.table` is that it contains pre-computed indexes for various columns to accelerate operations like data selection and merges. In this case (database joins) pandas' DataFrame contains no pre-computed information that is being used for the merge, so to speak it's a "cold" merge. If I had stored the factorized versions of the join keys, the join would be significantly faster - as factorizing is the biggest bottleneck for this algorithm. 
I should also add that the internal design of pandas' DataFrame is much more amenable to these kinds of operations than R's data.frame (which is just a list of arrays internally).

A2:
It looks like Wes may have discovered a known issue in `data.table` when the number of unique strings ( levels ) is large: 10,000. 
Does `Rprof()` reveal most of the time spent in the call `sortedmatch(levels(i[[lc]]), levels(x[[rc]])`? This isn't really the join itself (the algorithm), but a preliminary step. 
Recent efforts have gone into allowing character columns in keys, which should resolve that issue by integrating more closely with R's own global string hash table. Some benchmark results are already reported by `test.data.table()` but that code isn't hooked up yet to replace the levels to levels match. 
Are pandas merges faster than `data.table` for regular integer columns? That should be a way to isolate the algorithm itself vs factor issues. 
Also, `data.table` has time series merge in mind. Two aspects to that: i) multi column ordered keys such as (id,datetime) ii) fast prevailing join (`roll=TRUE`) a.k.a. last observation carried forward. 
I'll need some time to confirm as it's the first I've seen of the comparison to `data.table` as presented. 
UPDATE from data.table v1.8.0 released July 2012 
Internal function sortedmatch() removed and replaced with chmatch()
when matching i levels to x levels for columns of type 'factor'. This
preliminary step was causing a (known) significant slowdown when the number
of levels of a factor column was large (e.g. >10,000). Exacerbated in
tests of joining four such columns, as demonstrated by Wes McKinney
(author of Python package Pandas). Matching 1 million strings of which
of which 600,000 are unique is now reduced from 16s to 0.5s, for example. 
also in that release was : 
character columns are now allowed in keys and are preferred to
factor. data.table() and setkey() no longer coerce character to
factor. Factors are still supported. Implements FR#1493, FR#1224
and (partially) FR#951. 
New functions chmatch() and %chin%, faster versions of match()
and %in% for character vectors. R's internal string cache is
utilised (no hash table is built). They are about 4 times faster
than match() on the example in ?chmatch. 
As of Sep 2013 data.table is v1.8.10 on CRAN and we're working on v1.9.0. NEWS is updated live. 
But as I wrote originally, above : 
`data.table` has time series merge in mind. Two aspects to that: i)
multi column ordered keys such as (id,datetime) ii) fast prevailing
join (`roll=TRUE`) a.k.a. last observation carried forward. 
So the Pandas equi join of two character columns is probably still faster than data.table. Since it sounds like it hashes the combined two columns. data.table doesn't hash the key because it has prevailing ordered joins in mind. A "key" in data.table is literally just the sort order (similar to a clustered index in SQL; i.e., that's how the data is ordered in RAM). On the list is to add secondary keys, for example. 
In summary, the glaring speed difference highlighted by this particular two-character-column test with over 10,000 unique strings shouldn't be as bad now, since the known problem has been fixed.

A3:
Since both of the other answers are from the respective authors of the two packages, I thought I'd add a users view on this, since I've used both packages quite a bit. 
Headline message for users: don't choose between these packages based on speed or benchmarks, that will be second order in your work. If you care that much about speed, write you application in assembly/C for your specific case, and buy a faster computer. 
Looking at the implementation of both, they should generally come out similar in speed, and tests I've run now generally do. (As explained above the big difference found in an early benchmark was a bug). For certain tasks one or other can come out a tad faster, but never by much. 
The reason for this, is the heavy lifting part is "factoring" the join keys, and both packages seem to have found the same solution to this problem , to use something very similar to a Counting Sort . Matthew (data.table) generally calls this a Radix Sort (since it is incorrectly named such in R) and Wes (pandas) is using a klib hash table to achieve a similar end. Thus the only real difference in approach that I can see is 
To hash or not
Pandas gives everything to klib to hash, and relies on klib being smart (which it is), but this must still cause at least one opcode! 
data.table will just use the object itself if it can, e.g. integer, and uses Rs internal hash tables for strings. Other data-types things get a bit less optimal. An advantage to this is you often get things in "natural" order. 
Build the index in advance or not (although it appears pandas does some caching here for future calls?) 
And these two effect can mess with benchmarks quite badly. Leave out the `set.index` part of the `data.table` and `data.table` looks good. Use keys that are "ugly" in their raw form and pandas can look better. I've not found the pattern but I'm sure non-unique key values will also cause differences. So you can see, in the wild you are likely to be unsure where on the spectrum you will actually be. For example, you may have a situation where the join favours pandas slightly, but later on you need to have the table sorted by that key column, and the "unhashed" approach of data.table can make that sort faster (since it will already be in order). 
I haven't gone through every line in the source of both packages, so I can't be certain I haven't missed anything.
