The	O
above	O
entire	O
expression	O
is	O
therefore	O
evaluating	O
to	O
an	O
array	O
of	O
truth	O
values	O
,	O
rather	O
than	O
a	O
single	O
`	O
True	O
`	O
/	O
`	O
False	O
`	O
.	O

For	O
the	O
multiprocessing	O
:	O
You	O
can	O
distribute	O
the	O
data	O
sets	O
across	O
cores	O
,	O
do	O
`	O
partial_fit	O
`	O
,	O
get	O
the	O
weight	O
vectors	O
,	O
average	O
them	O
,	O
distribute	O
them	O
to	O
the	O
estimators	O
,	O
do	O
partial	O
fit	O
again	O
.	O

the	O
size	O
of	O
Y	O
is	O
100e6	O
x	O
1	O

Put	O
I	O
think	O
that	O
following	O
this	O
route	O
would	O
lead	O
to	O
an	O
inefficient	O
solution	O
.	O

Your	O
example	O
come	O
at	O
a	O
good	O
time	O
for	O
me	O
,	O
so	O
I	O
now	O
have	O
something	O
concrete	O
to	O
train	O
with	O
.	O

One	O
thing	O
I	O
find	O
very	O
confortable	O
with	O
Numpy	O
is	O
the	O
vectorization	O
of	O
operations	O
with	O
arrays	O
(	O
ie	O
.	O
the	O
absence	O
of	O
any	O
explicit	O
looping	O
)	O
,	O
and	O
the	O
implicit	O
element-by-element	O
behavior	O
of	O
operations	O
.	O

I	O
suggest	O
to	O
set	O
it	O
to	O
some	O
reasonable	O
upper	O
limit	O
,	O
though	O
.	O

Gonna	O
try	O
to	O
find	O
another	O
solution	O
.	O

Is	O
it	O
possible	O
to	O
construct	O
a	O
`	O
numpy	O
`	O
matrix	O
from	O
a	O
function	O
?	O

I	O
have	O
2D	O
numpy	O
array	O
,	O
with	O
example	O
shape	O
:	O
#CODE	O

`	O
grid	O
[	O
0	O
]`	O
can	O
be	O
used	O
as	O
a	O
proxy	O
for	O
the	O
index	O
`	O
i	O
`	O
,	O
and	O

Thank	O
you	O
Martijn	O
:)	O
-	O
your	O
are	O
BIG	O
help	O
,	O
and	O
just	O
one	O
thing	O
confuses	O
me	O
,	O
how	O
do	O
I	O
tell	O
python	O
to	O
read	O
all	O
CDR	O
records	O
if	O
record	O
is	O
907	O
bytes	O
long	O
.	O

I	O
want	O
to	O
get	O
the	O
elements	O
of	O
a	O
`	O
numpy	O
`	O
array	O
using	O
an	O
index	O
array	O
like	O
so	O
#CODE	O

I	O
can	O
weight	O
them	O
how	O
I	O
want	O
to	O
as	O
long	O
as	O
sum	O
of	O
their	O
weights	O
adds	O
to	O
1	O
.	O

I	O
wanted	O
to	O
try	O
to	O
duplicate	O
those	O
performance	O
gains	O
when	O
solving	O
the	O
distance	O
between	O
two	O
equal	O
sized	O
arrays	O
.	O

Even	O
if	O
it	O
worked	O
,	O
I	O
would	O
not	O
expect	O
any	O
speed-up	O
from	O
this	O
compared	O
to	O
an	O
ordinary	O
loop	O
,	O
since	O
it	O
needs	O
to	O
call	O
a	O
Python	O
function	O
for	O
every	O
entry	O
.	O

@USER	O
,	O
you're	O
right	O
,	O
if	O
you	O
have	O
to	O
convert	O
everything	O
to	O
ndarrays	O
it's	O
often	O
not	O
worth	O
it	O
.	O

An	O
example	O
implementation	O
without	O
recalculating	O
the	O
distance	O
array	O
would	O
be	O
this	O
:	O
#CODE	O

I	O
need	O
to	O
return	O
all	O
of	O
the	O
points	O
within	O
a	O
distance	O
of	O
X	O
units	O
from	O
every	O
point	O
.	O

EDIT	O
:	O
Actually	O
renaming	O
my	O
package	O
does	O
not	O
fix	O
it	O
.	O

2	O
)	O
look	O
at	O
the	O
lengths	O
distance	O
(	O
point	O
,	O
centre	O
,	O
metric=	O
...	O
)	O
of	O
all	O
the	O
rays	O
.	O

Sorry	O
,	O
all	O
are	O
positive	O
values	O
greater	O
than	O
0	O
.	O

After	O
that	O
I	O
convert	O
the	O
image	O
to	O
BGR	O
model	O
:	O
#CODE	O

How	O
do	O
I	O
standardize	O
a	O
matrix	O
?	O

Speed	O
can	O
probably	O
be	O
increased	O
by	O
ensuring	O
that	O
the	O
record	O
array	O
you	O
pass	O
to	O
Cython	O
is	O
contiguous	O
.	O

fid	O
is	O
the	O
file	O
currently	O
being	O
looked	O
at	O

I'm	O
guessing	O
it's	O
opening	O
TWO	O
filehandles	O
per	O
iteration	O
,	O
just	O
based	O
on	O
the	O
498	O
(	O
a	O
bit	O
less	O
than	O
half	O
1024	O
,	O
and	O
Python	O
would	O
have	O
some	O
files	O
open	O
itself	O
(	O
maybe	O
25-odd	O
?	O
)	O
.	O

The	O
idea	O
is	O
to	O
count	O
the	O
number	O
of	O
occurrences	O
of	O
each	O
transition	O
,	O
and	O
use	O
the	O
counts	O
in	O
a	O
vectorized	O
update	O
of	O
the	O
matrix	O
.	O

I	O
kept	O
them	O
in	O
to	O
distinguish	O
them	O
from	O
the	O
`	O
math	O
`	O
ones	O
,	O
which	O
won't	O
work	O
for	O
this	O
approach	O
.	O

Powers	O
of	O
two	O
are	O
simple	O
to	O
compute	O
,	O
but	O
mixed	O
radix	O
sizes	O
can	O
be	O
faster	O
and	O
use	O
less	O
memory	O
.	O

The	O
stars	O
/	O
dots	O
are	O
the	O
`	O
X	O
`	O
and	O
`	O
Y	O
`	O
plotted	O
with	O
two	O
modifications	O
,	O
I	O
removed	O
the	O
first	O
position	O
and	O
added	O
a	O
false	O
one	O
to	O
make	O
this	O
a	O
full	O
example	O
of	O
the	O
sought	O
algorithm	O
.	O

Please	O
look	O
at	O
my	O
EDIT	O
2	O
,	O
where	O
I	O
described	O
my	O
problem	O
with	O
input	O
data	O
...	O
and	O
why	O
I	O
can't	O
get	O
matrix	O
..	O

pyqt	O
:	O
Convert	O
numpy	O
array	O
to	O
QImage	O

To	O
find	O
the	O
difference	O
between	O
your	O
data	O
and	O
a	O
point	O
,	O
you'd	O
just	O
do	O
`	O
data	O
-	O
point	O
`	O
.	O

Unfortunately	O
when	O
numpy	O
reads	O
the	O
19-digit	O
number	O
as	O
a	O
floating	O
point	O
number	O
,	O
there	O
is	O
not	O
enough	O
precision	O
to	O
get	O
all	O
the	O
significant	O
digits	O
,	O
so	O
there	O
is	O
a	O
rounding	O
error	O
.	O

The	O
exceptions	O
are	O
very	O
rare	O
,	O
if	O
any	O
.	O

I	O
can't	O
reproduce	O
your	O
problem	O
on	O
Linux	O
using	O
the	O
same	O
versions	O
of	O
numpy	O
and	O
python	O
and	O
a	O
quickly	O
made	O
test	O
file	O
(	O
with	O
dos	O
line	O
endings	O
,	O
even	O
)	O
...	O

I	O
imagine	O
I	O
would	O
have	O
to	O
use	O
the	O
uncompiled	O
source	O
provided	O
from	O
each	O
of	O
these	O
three	O
projects	O
.	O

However	O
,	O
I	O
am	O
checking	O
optimization	O
routine	O
result	O
,	O
and	O
sometimes	O
power	O
is	O
negative	O
,	O
sometimes	O
it	O
is	O
positive	O
.	O

What	O
about	O
array	O
of	O
arrays	O
that	O
contains	O
some	O
structures	O
?	O

The	O
y	O
data	O
takes	O
the	O
shape	O
of	O
the	O
triangle	O
wave	O
below	O
.	O

There	O
are	O
some	O
algorithm	O
to	O
calculate	O
faster	O
the	O
results	O
for	O
low	O
valued	O
matrix	O
,	O
but	O
just	O
google	O
for	O
this	O
.	O

Those	O
are	O
not	O
random	O
replacements	O
by	O
any	O
means	O
.	O

I	O
would	O
suggest	O
to	O
make	O
the	O
library	O
use	O
an	O
(	O
NumPy-	O
)	O
array	O
you	O
allocate	O
in	O
Python	O
and	O
pass	O
on	O
to	O
the	O
library	O
.	O

For	O
the	O
simple	O
case	O
of	O
"	O
remove	O
column	O
3	O
"	O
,	O
`	O
delete	O
`	O
makes	O
more	O
sense	O
;	O
for	O
a	O
more	O
complicated	O
case	O
,	O
`	O
take	O
`	O
probably	O
makes	O
more	O
sense	O
.	O

I	O
have	O
an	O
array	O
of	O
points	O
in	O
numpy	O
:	O
#CODE	O

I	O
have	O
done	O
7	O
of	O
the	O
problems	O
on	O
Project	O
Euler	O
(	O
nothing	O
to	O
brag	O
about	O
,	O
but	O
it	O
might	O
give	O
you	O
a	O
better	O
idea	O
of	O
where	O
I	O
stand	O
in	O
skills	O
)	O
.	O

How	O
do	O
I	O
find	O
the	O
length	O
(	O
or	O
dimensions	O
,	O
size	O
)	O
of	O
a	O
numpy	O
matrix	O
in	O
python	O
?	O

It's	O
longer	O
than	O
the	O
other	O
answer	O
but	O
is	O
more	O
generic	O
(	O
can	O
be	O
used	O
with	O
values	O
that	O
are	O
not	O
strings	O
)	O
.	O

I	O
coded	O
my	O
own	O
routine	O
with	O
Python	O
/	O
Numpy	O
,	O
and	O
it	O
is	O
giving	O
me	O
a	O
little	O
bit	O
different	O
results	O
from	O
the	O
MATLAB	O
code	O
somebody	O
else	O
did	O
,	O
and	O
I	O
am	O
having	O
hard	O
time	O
finding	O
out	O
where	O
it	O
is	O
coming	O
from	O
because	O
of	O
different	O
random	O
draws	O
.	O

How	O
can	O
1,000,000	O
4-byte	O
ints	O
be	O
compressed	O
any	O
smaller	O
?	O

If	O
this	O
number	O
is	O
less	O
than	O
a	O
third	O
of	O
the	O
total	O
I'll	O
use	O
my	O
answer	O
above	O
.	O

I	O
have	O
done	O
7	O
of	O
the	O
problems	O
on	O
Project	O
Euler	O
(	O
nothing	O
to	O
brag	O
about	O
,	O
but	O
it	O
might	O
give	O
you	O
a	O
better	O
idea	O
of	O
where	O
I	O
stand	O
in	O
skills	O
)	O
.	O

Apologies	O
if	O
this	O
is	O
a	O
wrongly	O
framed	O
question	O
or	O
if	O
this	O
question	O
was	O
already	O
asked	O
earlier	O
(	O
I	O
couldn't	O
find	O
it	O
)	O

If	O
you	O
can	O
choose	O
,	O
I	O
strongly	O
recommend	O
pandas	O
:	O
it	O
has	O
"	O
column	O
indexing	O
"	O
built-in	O
plus	O
a	O
lot	O
of	O
other	O
features	O
.	O

But	O
this	O
will	O
iterate	O
through	O
the	O
entire	O
array	O
and	O
allocate	O
a	O
new	O
array	O
in	O
memory	O
containing	O
the	O
all	O
the	O
results	O
,	O
and	O
only	O
then	O
check	O
to	O
see	O
if	O
it	O
is	O
empty	O
.	O

Since	O
some	O
askers	O
and	O
some	O
answers	O
both	O
avoid	O
that	O
constraint	O
,	O
I	O
encourage	O
anyone	O
who's	O
here	O
and	O
doesn't	O
mind	O
having	O
PIL	O
to	O
look	O
below	O
,	O
and	O
any	O
non-PIL	O
answers	O
(	O
new	O
or	O
old	O
)	O
to	O
mention	O
that	O
they're	O
a	O
PIL-is-used	O
type	O
of	O
answer	O
,	O
to	O
distinguish	O
themselves	O
from	O
answers	O
meeting	O
the	O
original	O
constraint	O
.	O

As	O
I	O
understand	O
your	O
question	O
,	O
you	O
have	O
a	O
2D	O
array	O
of	O
"	O
z	O
"	O
values	O
that	O
ranges	O
from	O
some	O
xmin	O
to	O
xmax	O
,	O
and	O
ymin	O
to	O
ymax	O
in	O
each	O
direction	O
.	O

The	O
covariance	O
matrix	O
of	O
a	O
dataset	O
A	O
is	O
:	O
1	O
/(	O
N-1	O
)	O
*	O
AA^T	O

I	O
have	O
a	O
large	O
(	O
500k	O
by	O
500k	O
)	O
,	O
sparse	O
matrix	O
.	O

I	O
can't	O
comment	O
on	O
a	O
numpy	O
array	O
as	O
I	O
haven't	O
used	O
one	O
before	O
,	O
but	O
for	O
using	O
a	O
list	O
of	O
lists	O
Python	O
already	O
has	O
built	O
in	O
support	O
.	O

If	O
`	O
finer_fxy	O
`	O
is	O
stored	O
in	O
the	O
probably-default	O
`	O
float64	O
`	O
s	O
,	O
this	O
would	O
take	O
about	O
64	O
GiB	O
of	O
memory	O
;	O
not	O
surprising	O
that	O
you're	O
running	O
out	O
.	O

Sebastian's	O
solution	O
for	O
a	O
way	O
around	O
the	O
integer-values-only	O
restriction	O
and	O
big-values	O
problem	O
.	O

This	O
allows	O
the	O
column	O
to	O
hold	O
float	O
values	O
at	O
first	O
,	O
and	O
strings	O
later	O
.	O

Efficient	O
slicing	O
of	O
matrices	O
using	O
matrix	O
multiplication	O
,	O
with	O
Python	O
,	O
NumPy	O
,	O
SciPy	O

Is	O
there	O
a	O
more	O
compact	O
way	O
to	O
operate	O
on	O
array	O
elements	O
,	O
without	O
having	O
to	O
use	O
the	O
standard	O
for	O
loop	O
.?	O

Please	O
look	O
at	O
this	O
answer	O
:	O
#URL	O

When	O
I	O
tried	O
this	O
,	O
I	O
got	O
sort	O
of	O
similar	O
shaped	O
"	O
tiles	O
"	O
of	O
different	O
colors	O
rather	O
than	O
3	O
Gaussian	O
humps	O
.	O

I	O
created	O
the	O
first	O
array	O
like	O
this	O
:	O

(	O
Note	O
that	O
I	O
can't	O
imagine	O
any	O
reason	O
why	O
this	O
should	O
be	O
necessary	O
.	O
)	O

SOLUTION	O
:	O
i	O
have	O
some	O
scattered	O
points	O
(	O
i	O
don't	O
know	O
how	O
many	O
)	O
and	O
i	O
want	O
to	O
reduce	O
it	O
to	O
a	O
8	O
meaning	O
point	O
.	O
one	O
of	O
the	O
technique	O
i	O
can	O
use	O
is	O
to	O
clusterize	O
them	O
with	O
some	O
cluster	O
algorithms	O
.	O

that	O
blas	O
is	O
reference	O
blas	O
from	O
netlib	O
-	O
the	O
slowest	O
blas	O
around	O
.	O
install	O
atlas	O
or	O
mkl	O
instead	O
.	O

EDIT	O
:	O
Answer	O
updated	O
for	O
a	O
2D	O
array	O
.	O

But	O
you	O
lose	O
a	O
lot	O
of	O
NumPy	O
power	O
that	O
way	O
.	O

Because	O
I	O
view	O
doesn't	O
really	O
have	O
to	O
do	O
with	O
filtering	O
,	O
but	O
rather	O
with	O
different	O
representation	O
of	O
the	O
same	O
data	O
.	O

@USER	O
,	O
not	O
sure	O
what	O
you	O
mean	O
by	O
"	O
changing	O
original	O
values	O
"	O
.	O

this	O
could	O
also	O
be	O
achieved	O
elegantly	O
with	O
numpy's	O
`	O
where	O
`	O
function	O

I	O
need	O
to	O
specify	O
datatypes	O
for	O
all	O
numerical	O
types	O
since	O
I	O
care	O
about	O
int	O
8/	O
16	O
/	O
32	O
,	O
etc	O
,	O
but	O
I	O
would	O
like	O
to	O
benefit	O
from	O
the	O
auto	O
string	O
length	O
detection	O
that	O
works	O
if	O
I	O
don't	O
specify	O
datatypes	O
.	O

In	O
the	O
following	O
trivial	O
function	O
,	O
I	O
have	O
declared	O
the	O
numpy	O
array	O
argument	O
`	O
arr	O
`	O
using	O
the	O
buffer	O
syntax	O
.	O

I	O
remember	O
that	O
there	O
was	O
a	O
smart	O
trick	O
about	O
turning	O
on	O
and	O
off	O
the	O
right	O
intersections	O
of	O
rows	O
and	O
columns	O
to	O
turn	O
off	O
one-by-one	O
all	O
the	O
lightbulbs	O
,	O
but	O
it	O
wont	O
come	O
back	O
to	O
my	O
mind	O
...	O

However	O
this	O
code	O
is	O
to	O
slow	O
in	O
the	O
current	O
version	O
,	O
and	O
I	O
am	O
wondering	O
wheater	O
there	O
is	O
a	O
faster	O
solution	O
.	O
thanks	O
!	O

This	O
would	O
probably	O
be	O
the	O
most	O
efficient	O
way	O
to	O
access	O
a	O
numpy	O
array	O
stored	O
on	O
disk	O
.	O

hmmmmm	O
,	O
probably	O
it	O
will	O
help	O
some	O
others	O
to	O
sort	O
dictionarys	O
or	O
to	O
prevent	O
from	O
using	O
commands	O
like	O
sorted	O
=	O
sorted	O
(	O
...	O
)	O
.	O

The	O
ticket	O
simply	O
spoke	O
of	O
random	O
number	O
seeding	O
with	O
64-bit	O
,	O
perhaps	O
its	O
referring	O
to	O
a	O
different	O
random	O
number	O
generator	O
.	O

Not	O
really	O
elegant	O
at	O
all	O
but	O
you	O
can	O
get	O
close	O
to	O
what	O
you	O
want	O
using	O
a	O
tuple	O
to	O
store	O
pointers	O
to	O
the	O
arrays	O
.	O

For	O
example	O
I	O
am	O
looking	O
for	O
4.2	O
but	O
I	O
know	O
in	O
the	O
array	O
there	O
is	O
no	O
4.2	O
but	O
I	O
want	O
to	O
return	O
the	O
index	O
of	O
the	O
value	O
4.1	O
instead	O
of	O
4.4	O
.	O

Print	O
'	O
Length	O
of	O
together	O
'	O
goes	O
just	O
before	O
the	O
matrix	O
line	O
.	O

solve	O
a	O
nonlinear	O
equation	O
at	O
several	O
intermediate	O
points	O
of	O
a	O
calculation	O
,	O
not	O
just	O
as	O
the	O
final	O
result	O
.	O

Find	O
where	O
they're	O
located	O
at	O
(	O
assumes	O
the	O
data	O
is	O
sorted	O
!!	O
):	O
#CODE	O

You	O
need	O
Python	O
to	O
keep	O
track	O
of	O
your	O
vector	O
so	O
that	O
it	O
can	O
be	O
deleted	O
*	O
after	O
*	O
the	O
numpy	O
array	O
.	O

I	O
find	O
that	O
I	O
have	O
to	O
first	O
build	O
a	O
list	O
and	O
then	O
cast	O
it	O
(	O
using	O
"	O
array	O
")	O
to	O
an	O
array	O
.	O

I	O
have	O
an	O
numpy	O
one	O
dimensional	O
array	O
c	O
that	O
is	O
supposed	O
to	O
be	O
filled	O
with	O
the	O
contents	O
of	O

but	O
the	O
issue	O
now	O
,	O
when	O
I	O
am	O
trying	O
to	O
save	O
the	O
name	O
of	O
the	O
file	O
as	O
well	O
in	O
the	O
csv	O
file	O
like	O
this	O
:	O
#CODE	O

After	O
you	O
do	O
this	O
no	O
matter	O
where	O
the	O
template	O
object	O
is	O
in	O
a	O
calculation	O
.	O

So	O
the	O
easiest	O
thing	O
to	O
do	O
would	O
be	O
to	O
take	O
a	O
sample	O
of	O
say	O
,	O
1000	O
points	O
,	O
from	O
your	O
data	O
:	O
#CODE	O

Your	O
array	O
consists	O
of	O
:	O
#CODE	O

The	O
final	O
DF	O
should	O
have	O
as	O
many	O
columns	O
as	O
all	O
the	O
df	O
columns	O
added	O
together	O
,	O
so	O
it	O
grow	O
additively	O
and	O
not	O
be	O
combinatorial	O
.	O

I'm	O
sorting	O
the	O
cells	O
of	O
the	O
matrix	O
by	O
the	O
float	O
value	O
,	O
producing	O
a	O
list	O
of	O
`	O
(	O
row	O
,	O
col	O
,	O
value	O
)`	O
tuples	O
.	O

Is	O
it	O
essential	O
that	O
you	O
need	O
a	O
numpy	O
array	O
?	O

Mh	O
.	O
but	O
look	O
at	O
this	O
:	O

All	O
variables	O
are	O
dependent	O
on	O
each	O
other	O
and	O
I	O
am	O
only	O
looking	O
for	O
local	O
minima	O
from	O
the	O
initial	O
guess	O
.	O

The	O
basic	O
idea	O
is	O
to	O
simply	O
run	O
all	O
the	O
usual	O
steps	O
of	O
a	O
root	O
finder	O
in	O
parallel	O
on	O
a	O
vector	O
of	O
variables	O
,	O
using	O
a	O
function	O
that	O
can	O
be	O
evaluated	O
on	O
a	O
vector	O
of	O
variables	O
and	O
equivalent	O
vector	O
(	O
s	O
)	O
of	O
parameters	O
that	O
define	O
the	O
individual	O
component	O
functions	O
.	O

Hence	O
,	O
with	O
NetworkX	O
,	O
you	O
can	O
put	O
in	O
an	O
adjacency	O
matrix	O
and	O
find	O
out	O
which	O
authors	O
are	O
clustered	O
together	O
.	O

The	O
issue	O
your	O
having	O
more	O
likely	O
is	O
a	O
python	O
mmap	O
issue	O
,	O
since	O
python	O
mmaps	O
handle	O
all	O
the	O
memory	O
mapping	O
and	O
file	O
closing	O
for	O
numpy	O
memmaps	O
.	O

So	O
far	O
,	O
I'm	O
sticking	O
with	O
C++	O
-	O
on	O
my	O
tests	O
,	O
at	O
least	O
2	O
orders	O
of	O
magnitude	O
faster	O
!	O

Sorting	O
ends	O
up	O
being	O
the	O
slowest	O
step	O
but	O
it's	O
still	O
faster	O
if	O
m	O
is	O
large	O
because	O
the	O
n*log	O
(	O
n	O
)	O
sort	O
is	O
faster	O
than	O
(	O
n*m	O
)	O
.	O

Basically	O
,	O
I	O
am	O
getting	O
a	O
memory	O
error	O
in	O
python	O
when	O
trying	O
to	O
perform	O
an	O
algebraic	O
operation	O
on	O
a	O
numpy	O
matrix	O
.	O

Surely	O
there	O
must	O
be	O
a	O
way	O
to	O
populate	O
a	O
boost	O
::	O
python	O
::	O
numeric	O
::	O
array	O
with	O
data	O
from	O
a	O
simple	O
std	O
::	O
vector	O
without	O
having	O
to	O
get	O
some	O
3rd	O
party	O
library	O
.	O

Here	O
again	O
a	O
if	O
statement	O
could	O
do	O
,	O
but	O
I	O
am	O
wondering	O
if	O
there	O
is	O
a	O
workarouns	O
and	O
a	O
Python	O
library	O
where	O
negative	O
exposant	O
is	O
allowed	O
.	O

The	O
key	O
point	O
here	O
is	O
that	O
Tabular	O
and	O
NumPy	O
set	O
certain	O
standards	O
for	O
what	O
counts	O
as	O
"	O
fast	O
"	O
or	O
"	O
slow	O
"	O
--	O
and	O
then	O
,	O
force	O
you	O
to	O
be	O
explicit	O
about	O
operations	O
that	O
are	O
going	O
to	O
be	O
slow	O
.	O

Asume	O
that	O
your	O
numpy	O
module	O
is	O
located	O
at	O
/	O
Users	O
/	O
Me	O
/	O
python	O
/	O
modules	O
directory	O
.	O

I	O
am	O
not	O
responsible	O
from	O
any	O
brain	O
damage	O
resulting	O
from	O
attempting	O
to	O
understand	O
this	O
code	O
.	O

There	O
a	O
plenty	O
of	O
places	O
where	O
you're	O
inadvertently	O
creating	O
additional	O
temporary	O
arrays	O
,	O
but	O
they're	O
mostly	O
irrelevant	O
,	O
as	O
they're	O
overwhelmed	O
by	O
what	O
goes	O
on	O
during	O
the	O
call	O
to	O
`	O
select	O
`	O
.	O

The	O
fact	O
that	O
you	O
are	O
using	O
`	O
object	O
`	O
arrays	O
(	O
not	O
very	O
common	O
and	O
not	O
very	O
memory-efficient	O
)	O
presents	O
a	O
particular	O
problem	O
when	O
trying	O
to	O
determine	O
the	O
index	O
of	O
non-None	O
array	O
items	O
.	O

where	O
things	O
improve	O
as	O
the	O
number	O
of	O
bits	O
increases	O
.	O

Really	O
,	O
4D	O
arrays	O
are	O
just	O
1D	O
arrays	O
in	O
memory	O
anyway	O
(	O
Unless	O
you	O
really	O
have	O
view	O
objects	O
,	O
but	O
it	O
should	O
still	O
work	O
with	O
those	O
as	O
well	O
)	O

I'll	O
add	O
comments	O
to	O
explain	O
things	O
in	O
a	O
bit	O
.	O

I	O
was	O
assuming	O
that	O
the	O
rgb	O
and	O
ycc	O
matrices	O
were	O
just	O
a	O
matrix	O
that	O
had	O
as	O
many	O
rows	O
as	O
pixels	O
and	O
a	O
column	O
per	O
colour	O
component	O
.	O

For	O
example	O
,	O
suppose	O
`	O
a	O
=	O
ones	O
((	O
3	O
,	O
3	O
))`	O
.	O

Therefore	O
,	O
n	O
and	O
m	O
correspond	O
to	O
indices	O
in	O
the	O
array	O
,	O
but	O
I'm	O
not	O
sure	O
how	O
?	O

Update	O
:	O
As	O
mentioned	O
in	O
my	O
comment	O
below	O
,	O
I	O
should	O
have	O
stated	O
that	O
I'm	O
trying	O
to	O
do	O
this	O
on	O
2D	O
arrays	O
,	O
and	O
therefore	O
get	O
a	O
set	O
of	O
2D	O
indices	O
back	O
.	O

Need	O
to	O
add	O
a	O
check	O
for	O
that	O
,	O
but	O
otherwise	O
thanks	O
!	O

I	O
think	O
you	O
just	O
want	O
`	O
label	O
==	O
num	O
`	O
where	O
`	O
num	O
`	O
is	O
the	O
number	O
of	O
the	O
object	O
in	O
`	O
label	O
`	O
(	O
the	O
labeled	O
array	O
)	O
.	O

My	O
question	O
is	O
how	O
can	O
I	O
go	O
thru	O
the	O
array	O
to	O
access	O
the	O
object	O
in	O
the	O
array	O
?	O

The	O
matrix	O
in	O
the	O
example	O
above	O
is	O
singular	O
(	O
determinant	O
~	O
0	O
)	O
.	O

See	O
the	O
note	O
at	O
#URL	O

would	O
turn	O
into	O
either	O
this	O
array	O
:	O
#CODE	O

Note	O
that	O
this	O
is	O
a	O
bit	O
more	O
sophisticated	O
than	O
the	O
simple	O
do-it-yourself	O
convolve-method	O
,	O
since	O
it	O
tries	O
to	O
handle	O
the	O
problems	O
at	O
the	O
beginning	O
and	O
the	O
end	O
of	O
the	O
data	O
by	O
reflecting	O
it	O
(	O
which	O
may	O
or	O
may	O
not	O
work	O
in	O
your	O
case	O
...	O
)	O
.	O

Usually	O
,	O
in	O
numpy	O
,	O
you	O
keep	O
the	O
string	O
data	O
in	O
a	O
separate	O
array	O
.	O

Any	O
idea	O
what	O
might	O
be	O
happening	O
?	O

but	O
the	O
size	O
is	O
wrong	O
because	O
i've	O
assigned	O
1000	O
as	O
the	O
period	O
size	O
.	O

This	O
may	O
not	O
be	O
perfectly	O
pythonic	O
(	O
perhaps	O
someone	O
can	O
think	O
of	O
a	O
nicer	O
implementation	O
using	O
generators	O
or	O
itertools	O
?	O
)	O
but	O
it	O
is	O
hard	O
to	O
imagine	O
any	O
method	O
that	O
relies	O
on	O
searching	O
one	O
point	O
at	O
a	O
time	O
beating	O
this	O
in	O
speed	O
.	O

Thanks	O
,	O
your	O
post	O
helped	O
me	O
solve	O
this	O
problem	O
.	O

Now	O
imagine	O
that	O
the	O
next	O
time	O
step	O
some	O
values	O
change	O
,	O
so	O
should	O
this	O
picture	O
.	O

Since	O
get_probability	O
is	O
a	O
function	O
,	O
so	O
what	O
value	O
is	O
being	O
passed	O
to	O
count	O
parameter	O
here	O
???	O

taking	O
the	O
sum	O
for	O
each	O
column	O
.	O

You	O
should	O
be	O
able	O
to	O
just	O
load	O
the	O
entire	O
thing	O
into	O
memory	O
on	O
a	O
modern	O
machine	O
.	O

What	O
I	O
want	O
to	O
do	O
is	O
to	O
calculate	O
the	O
geographic	O
distances	O
between	O
rows	O
(	O
with	O
the	O
special	O
condition	O
that	O
the	O
first	O
element	O
is	O
always	O
zero	O
,	O
at	O
the	O
starting	O
point	O
)	O
.	O

We	O
can	O
simply	O
use	O
the	O
leastsq	O
function	O
to	O
find	O
the	O
best	O
coefficients	O
.	O

If	O
the	O
list	O
of	O
python	O
objects	O
doesn't	O
grow	O
at	O
all	O
from	O
frame	O
to	O
frame	O
,	O
the	O
leak	O
is	O
probably	O
in	O
the	O
C	O
code	O
or	O
the	O
python-to-C	O
link	O

Any	O
and	O
all	O
advice	O
is	O
greatly	O
appreciated	O
.	O

Numpy	O
:	O
Is	O
there	O
an	O
array	O
size	O
limit	O
?	O

Then	O
do	O
this	O
after	O
each	O
calculation	O
:	O
for	O
i	O
in	O
range	O
(	O
len	O
(	O
array	O
)):	O
array	O
[	O
i	O
]	O
[	O
i	O
]=	O
0	O

I	O
know	O
the	O
random	O
functions	O
and	O
numbers	O
seem	O
odd	O
,	O
but	O
conceptually	O
this	O
still	O
should	O
work	O
,	O
as	O
it	O
worked	O
when	O
both	O
were	O
set	O
to	O
variables	O
individually	O
.	O

@USER	O
are	O
your	O
numbers	O
in	O
the	O
range	O
of	O
-128	O
to	O
127	O
before	O
you	O
convert	O
them	O
to	O
8b	O
it	O
?	O

In	O
the	O
future	O
,	O
how	O
should	O
I	O
go	O
about	O
trying	O
to	O
find	O
routines	O
like	O
this	O
?	O

At	O
20,000	O
elements	O
,	O
your	O
method	O
is	O
about	O
25%	O
faster	O
.	O

I'll	O
fix	O
it	O
just	O
for	O
you	O
:P	O

Then	O
I	O
convert	O
it	O
to	O
a	O
numpy	O
array	O
:	O
#CODE	O

Just	O
throwing	O
in	O
my	O
two	O
cents	O
you	O
could	O
do	O
this	O
pretty	O
simply	O
using	O
list	O
comprehension	O
if	O
it's	O
always	O
a	O
2d	O
array	O
like	O
that	O
#CODE	O

While	O
its	O
expected	O
value	O
here	O
is	O
zero	O
,	O
the	O
particular	O
realizations	O
will	O
fluctuate	O
around	O
that	O
expected	O
value	O
.	O

Then	O
if	O
each	O
item	O
is	O
weighted	O
with	O
weight	O
w_i	O
,	O
the	O
"	O
summed	O
histogram	O
"	O
would	O
have	O
weight	O
sum	O
(	O
i	O
in	O
items	O
)	O
w_i	O
D_ij	O
.	O

This	O
approach	O
will	O
take	O
an	O
overhead	O
because	O
of	O
crating	O
a	O
new	O
array	O
in	O
memory	O
.	O

"	O
Eric's	O
suggestion	O
for	O
revising	O
this	O
question	O
is	O
a	O
good	O
start	O
,	O
but	O
I	O
think	O
the	O
question	O
"	O
Given	O
a	O
Cartesian	O
plane	O
,	O
how	O
to	O
discretize	O
it	O
in	O
a	O
matrix	O
form	O
?	O

it	O
is	O
the	O
same	O
as	O
long	O
as	O
you	O
ignore	O
precision	O
issue	O
-	O
which	O
matters	O
quite	O
often	O
when	O
you	O
start	O
taking	O
exponential	O
of	O
numbers	O
.	O

Google	O
Protocol	O
Buffers	O
support	O
self-describing	O
too	O
,	O
are	O
pretty	O
fast	O
(	O
but	O
Python	O
support	O
is	O
poor	O
at	O
present	O
time	O
,	O
slow	O
and	O
buggy	O
)	O
.	O

Not	O
all	O
people	O
can	O
install	O
NumPy	O
(	O
or	O
even	O
Python	O
:D	O
)	O
as	O
many	O
Blender	O
users	O
are	O
just	O
artists	O
.	O

All	O
possible	O
solutions	O
are	O
mentioned	O
in	O
the	O
comments	O
.	O

I've	O
also	O
refined	O
your	O
approach	O
to	O
allow	O
zooming	O
in	O
over	O
a	O
section	O
of	O
the	O
data	O
and	O
to	O
produce	O
better	O
results	O
at	O
the	O
borders	O
.	O

I	O
need	O
to	O
specify	O
datatypes	O
for	O
all	O
numerical	O
types	O
since	O
I	O
care	O
about	O
int	O
8/	O
16	O
/	O
32	O
,	O
etc	O
,	O
but	O
I	O
would	O
like	O
to	O
benefit	O
from	O
the	O
auto	O
string	O
length	O
detection	O
that	O
works	O
if	O
I	O
don't	O
specify	O
datatypes	O
.	O

I	O
would	O
appreciate	O
any	O
assistance	O
you	O
can	O
offer	O
.	O

Let's	O
say	O
for	O
example	O
I	O
have	O
a	O
matrix	O
X	O
which	O
is	O
my	O
input	O
.	O

@USER	O
-	O
By	O
the	O
way	O
,	O
indexing	O
returns	O
a	O
view	O
(	O
essentially	O
a	O
pointer	O
)	O
into	O
the	O
array	O
.	O

Note	O
that	O
`	O
view	O
`	O
holds	O
the	O
same	O
data	O
as	O
the	O
original	O
array	O
!	O

EDIT	O
:	O
What	O
sort	O
of	O
sequence	O
is	O
it	O
you're	O
making	O
?	O

The	O
relative	O
error	O
is	O
less	O
than	O
2	O
-24	O
,	O
which	O
is	O
1	O
/	O
2	O
ULP	O
divided	O
by	O
the	O
smallest	O
the	O
value	O
could	O
be	O
(	O
the	O
smallest	O
value	O
in	O
the	O
interval	O
for	O
a	O
particular	O
ULP	O
,	O
so	O
the	O
power	O
of	O
two	O
that	O
bounds	O
it	O
)	O
.	O

This	O
is	O
called	O
matrix	O
transposition	O
.	O

@USER	O
The	O
solutions	O
there	O
all	O
make	O
use	O
of	O
the	O
fact	O
that	O
only	O
a	O
3x3	O
sliding	O
window	O
is	O
needed	O
,	O
but	O
I	O
need	O
something	O
that	O
works	O
for	O
all	O
sizes	O
of	O
templates	O
.	O

(	O
0008	O
,	O
103e	O
)	O
Series	O
Description	O
LO	O
:	O
'	O
Screen	O
Save	O
'	O

@USER	O
khanSever	O
20k	O
wouldn't	O
be	O
a	O
problem	O
for	O
modern	O
computers	O
,	O
if	O
you	O
are	O
really	O
thresholded	O
by	O
speed	O
in	O
this	O
kind	O
of	O
computation	O
,	O
I	O
would	O
say	O
that	O
you	O
shouldn't	O
have	O
had	O
an	O
inhomogenous	O
data	O
array	O
to	O
begin	O
with	O
.	O

@USER	O
Eweiwi	O
:	O
Did	O
you	O
find	O
my	O
answer	O
anyway	O
useful	O
?	O

You	O
can	O
now	O
compute	O
the	O
function	O
`	O
f	O
(	O
x	O
)`	O
at	O
any	O
point	O
`	O
x	O
`	O
.	O

BTW	O
:	O
this	O
is	O
a	O
neat	O
workaround	O
,	O
but	O
if	O
it	O
were	O
possible	O
to	O
use	O
the	O
`	O
in	O
`	O
operator	O
would	O
have	O
preferred	O
,	O
as	O
in	O
my	O
"	O
real	O
case	O
"	O
I	O
have	O
a	O
pool	O
of	O
roughly	O
10	O
values	O
,	O
non	O
only	O
`	O
(	O
6	O
,	O
8)	O
`	O
.	O

In	O
this	O
example	O
I	O
want	O
to	O
return	O
an	O
array	O
of	O
[	O
202	O
203	O
206	O
210	O
]	O

So	O
f	O
(	O
x	O
,	O
y	O
)	O
=	O
0	O

I	O
present	O
below	O
a	O
sample	O
silhouette	O
implementation	O
in	O
both	O
MATLAB	O
and	O
Python	O
/	O
Numpy	O
(	O
keep	O
in	O
mind	O
that	O
I	O
am	O
more	O
fluent	O
in	O
MATLAB	O
):	O

Python	O
import	O
Column	O
Data	O
from	O
MySQL	O
as	O
Array	O

This	O
is	O
just	O
the	O
partial	O
count	O
due	O
to	O
the	O
34	O
1-chips	O
.	O

I	O
want	O
to	O
know	O
how	O
I	O
should	O
index	O
/	O
access	O
some	O
data	O
programmatically	O
in	O
python	O
.	O

There	O
is	O
a	O
short	O
comment	O
at	O
the	O
end	O
of	O
the	O
introduction	O
to	O
SciPy	O
documentation	O
:	O

What	O
about	O
the	O
maximum	O
value	O
in	O
the	O
array	O
?	O

If	O
you	O
use	O
a	O
list	O
of	O
`	O
True	O
/	O
False	O
`	O
,	O
NumPy	O
will	O
interpret	O
that	O
as	O
a	O
list	O
of	O
`	O
1	O
/	O
0	O
`	O
as	O
integers	O
,	O
that	O
is	O
,	O
indices	O
,	O
meaning	O
that	O
you	O
'	O
either	O
get	O
the	O
second	O
or	O
first	O
element	O
of	O
your	O
array	O
.	O

But	O
it's	O
still	O
an	O
array	O
and	O
there	O
is	O
no	O
difference	O
in	O
asymptotic	O
complexity	O
.	O

Here's	O
one	O
way	O
(	O
same	O
matrix	O
as	O
before	O
):	O
#CODE	O

assume	O
i	O
have	O
100	O
points	O
whose	O
coordinates	O
are	O
random	O
,	O

If	O
you	O
just	O
want	O
the	O
first	O
one	O
,	O
use	O
next	O
with	O
the	O
list	O
comprehension	O
as	O
a	O
generator	O
expression	O
.	O

So	O
I	O
am	O
able	O
to	O
plot	O
what	O
I	O
want	O
onto	O
my	O
matrix	O

By	O
X3D	O
,	O
are	O
you	O
referring	O
to	O
the	O
x3d	O
standard	O
for	O
3d	O
content	O
,	O
as	O
at	O
#URL	O
If	O
so	O
,	O
I	O
would	O
very	O
much	O
like	O
to	O
learn	O
more	O
of	O
what	O
you	O
are	O
doing	O
--	O
thanks	O

Would	O
it	O
be	O
prohibitvely	O
wasteful	O
to	O
save	O
them	O
with	O
a	O
fixed	O
width	O
?	O

BSD-licensed	O
Python	O
source	O
code	O
for	O
surface	O
fits	O
can	O
be	O
found	O
at	O

...	O
which	O
returned	O
`	O
True	O
`	O
on	O
each	O
value	O
of	O
the	O
array	O
.	O

I	O
have	O
two	O
ordered	O
numpy	O
arrays	O
and	O
I	O
want	O
to	O
interleave	O
them	O
so	O
that	O
I	O
take	O
one	O
item	O
from	O
the	O
first	O
array	O
,	O
then	O
another	O
from	O
the	O
second	O
,	O
then	O
back	O
to	O
the	O
first	O
-	O
taking	O
the	O
next	O
item	O
that	O
is	O
larger	O
than	O
the	O
one	O
I	O
just	O
took	O
from	O
the	O
second	O
and	O
so	O
on	O
.	O

Did	O
you	O
look	O
at	O
the	O
link	O
in	O
my	O
answer	O
to	O
the	O
SciPy	O
page	O
on	O
Performance	O
Python	O
.	O

If	O
you	O
want	O
the	O
column	O
indices	O
instead	O
of	O
the	O
resulting	O
square	O
matrix	O
,	O
just	O
replace	O
`	O
return	O
B	O
`	O
with	O
`	O
return	O
colset	O
`	O
.	O

At	O
the	O
end	O
of	O
it	O
all	O
:	O
#CODE	O

Is	O
there	O
no	O
equivalent	O
function	O
that	O
gets	O
the	O
index	O
of	O
the	O
last	O
occurrence	O
?	O

I	O
want	O
to	O
get	O
a	O
cartesian	O
product	O
of	O
a	O
[:	O
:	O
i	O
]	O
and	O
b	O
[:	O
:	O
j	O
]	O
from	O
c	O
.	O

python	O
/	O
numpy	O
:	O
how	O
to	O
get	O
2D	O
array	O
column	O
length	O
?	O

Your	O
example	O
works	O
for	O
me	O
if	O
I	O
sample	O
around	O
2**6	O
points	O
.	O

NumPy's	O
main	O
object	O
is	O
the	O
homogeneous	O
multidimensional	O
array	O
.	O

Pythonic	O
way	O
to	O
import	O
data	O
from	O
multiple	O
files	O
into	O
an	O
array	O

The	O
only	O
thing	O
I	O
was	O
going	O
to	O
add	O
was	O
this	O
:	O
#URL	O
Indicated	O
that	O
this	O
is	O
not	O
likely	O
to	O
change	O
.	O

i	O
have	O
a	O
numpy	O
array	O
like	O
the	O
following	O
#CODE	O

I	O
want	O
to	O
write	O
a	O
Boost-Python	O
program	O
to	O
take	O
a	O
symbolic	O
python	O
function	O
from	O
user	O
and	O
evaluate	O
its	O
derivative	O
in	O
my	O
program	O
.	O

Is	O
there	O
a	O
way	O
around	O
this	O
?	O

Not	O
sure	O
if	O
I	O
explained	O
this	O
all	O
really	O
well	O
,	O
but	O
just	O
print	O
out	O
a_strided	O
and	O
you'll	O
see	O
what	O
the	O
result	O
is	O
and	O
how	O
easy	O
this	O
makes	O
the	O
operation	O
.	O

But	O
when	O
I	O
start	O
calling	O
columns	O
by	O
their	O
field	O
names	O
,	O
screwy	O
things	O
happen	O
.	O

all	O
I	O
get	O
is	O
very	O
high	O
or	O
inf	O
numbers	O
.	O

If	O
you're	O
iterating	O
through	O
,	O
and	O
applying	O
the	O
function	O
to	O
_each_	O
item	O
,	O
then	O
,	O
yeah	O
,	O
the	O
numpy	O
functions	O
will	O
be	O
slower	O
.	O

Slicing	O
does	O
not	O
copy	O
the	O
array	O
into	O
new	O
memory	O
(	O
unlike	O
delete	O
)	O
.	O

And	O
here's	O
the	O
filled	O
version	O
:	O
#CODE	O

This	O
is	O
a	O
little	O
bit	O
annoying	O
to	O
do	O
,	O
but	O
at	O
least	O
you	O
can	O
remove	O
that	O
annoying	O
`	O
==	O
`	O
easily	O
,	O
using	O
sorting	O
(	O
and	O
thats	O
probably	O
your	O
speed	O
killer	O
)	O
.	O

I	O
still	O
haven't	O
found	O
an	O
entirely	O
satisfactory	O
solution	O
,	O
but	O
nevertheless	O
there	O
is	O
something	O
one	O
can	O
do	O
to	O
obtain	O
the	O
pointer	O
with	O
a	O
lot	O
less	O
overhead	O
in	O
CPython	O
.	O

I	O
also	O
tried	O
using	O
NumPy	O
masked	O
arrays	O
,	O
with	O
NaN	O
fill_value	O
,	O
which	O
also	O
did	O
not	O
work	O
.	O

cartesian	O
(	O
split	O
(	O
a	O
,	O
3	O
))`	O
.	O

I	O
did	O
a	O
little	O
further	O
experimenting	O
and	O
found	O
a	O
numpy	O
specific	O
way	O
to	O
solve	O
this	O
:	O
#CODE	O

When	O
you	O
need	O
to	O
deal	O
with	O
exponential	O
,	O
you	O
quickly	O
go	O
into	O
under	O
/	O
over	O
flow	O
since	O
the	O
function	O
grows	O
so	O
quickly	O
.	O

Long	O
story	O
short	O
,	O
not	O
only	O
does	O
tabular	O
not	O
act	O
like	O
a	O
spreadsheet	O
out	O
of	O
the	O
box	O
,	O
I	O
can't	O
find	O
a	O
way	O
to	O
make	O
it	O
work	O
.	O

What	O
do	O
you	O
mean	O
"	O
two	O
significant	O
figures	O
"	O
?	O

We	O
put	O
it	O
in	O
a	O
list	O
and	O
double	O
it	O
.	O

For	O
example	O
for	O
value	O
255	O
the	O
coordinates	O
of	O
the	O
box	O
around	O
the	O
value	O
255	O
will	O
be	O
upper	O
left	O
(	O
0	O
,	O
0	O
)	O
and	O
lower	O
right	O
(	O
4	O
,	O
6	O
)	O
.	O

Like	O
in	O
a	O
java	O
program	O
,	O
you	O
can	O
choose	O
to	O
start	O
it	O
up	O
with	O
,	O
say	O
,	O
5GB	O
of	O
memory	O
.	O

However	O
,	O
due	O
to	O
the	O
way	O
the	O
data	O
points	O
lie	O
it	O
does	O
not	O
give	O
me	O
a	O
y-axis	O
interception	O
at	O
0	O
.	O

I'd	O
like	O
to	O
sort	O
it	O
such	O
that	O
my	O
points	O
are	O
ordered	O
by	O
x-coordinate	O
,	O
and	O
then	O
by	O
y	O
in	O
cases	O
where	O
the	O
x	O
coordinate	O
is	O
the	O
same	O
.	O

Of	O
course	O
this	O
will	O
slow	O
the	O
program	O
down	O
,	O
but	O
at	O
least	O
it'll	O
finish	O
.	O

Im	O
writing	O
it	O
here	O
because	O
i	O
cant	O
put	O
image	O
in	O
comment	O
.	O

In	O
looking	O
at	O
`	O
fill	O
`	O
,	O
I	O
saw	O
that	O
`	O
repeat	O
`	O
suits	O
my	O
needs	O
even	O
better	O
.	O

Note	O
that	O
an	O
array's	O
base	O
will	O
be	O
another	O
array	O
,	O
even	O
if	O
it	O
is	O
a	O
subset	O
:	O
#CODE	O

If	O
you	O
have	O
float	O
data	O
,	O
or	O
data	O
spread	O
over	O
a	O
huge	O
range	O
you	O
can	O
convert	O
it	O
to	O
integers	O
by	O
doing	O
:	O
#CODE	O

@USER	O
,	O
plaes	O
recommend	O
using	O
a	O
generator	O
(	O
parenthesis	O
)	O
instead	O
of	O
a	O
list	O
(	O
brackets	O
)	O
in	O
order	O
to	O
save	O
memory	O
and	O
gain	O
speed	O
when	O
managing	O
high	O
amounts	O
of	O
data	O
.	O

I	O
want	O
to	O
divide	O
this	O
array	O
into	O
3	O
blocks	O
of	O
size	O
2x4	O
,	O
and	O
then	O
find	O
the	O
mean	O
of	O
all	O
three	O
blocks	O
(	O
so	O
that	O
the	O
shape	O
of	O
the	O
mean	O
is	O
2x4	O
.	O

(	O
Have	O
a	O
look	O
at	O
the	O
comments	O
above	O
the	O
code	O
for	O
that	O
portion	O
.	O
)	O

That	O
is	O
because	O
`	O
fsolve	O
`	O
thinks	O
it	O
is	O
looking	O
for	O
an	O
array	O
of	O
length	O
17	O
that	O
solves	O
`	O
p	O
`	O
.	O

When	O
there's	O
a	O
choice	O
between	O
working	O
with	O
NumPy	O
array	O
and	O
numeric	O
lists	O
,	O
the	O
former	O
are	O
typically	O
faster	O
.	O

Wait	O
...	O
why	O
do	O
you	O
need	O
the	O
negative	O
?	O

But	O
if	O
a	O
dense	O
3d	O
array	O
representation	O
isn't	O
that	O
much	O
bigger	O
,	O
storing	O
it	O
as	O
a	O
chuncked	O
and	O
compressed	O
hdf5	O
array	O
is	O
probably	O
the	O
way	O
to	O
go	O
.	O

Index	O
datetime	O
in	O
numpy	O
array	O

Is	O
there	O
an	O
"	O
expandable	O
"	O
matrix	O
data	O
structure	O
available	O
in	O
a	O
well	O
tested	O
module	O
?	O

You	O
can	O
make	O
this	O
one-liner	O
reusable	O
if	O
you	O
are	O
going	O
to	O
repeat	O
it	O
a	O
lot	O
:	O
#CODE	O

Here's	O
my	O
array	O
(	O
rather	O
,	O
a	O
method	O
of	O
generating	O
representative	O
test	O
arrays	O
):	O
#CODE	O

We	O
need	O
more	O
information	O
on	O
your	O
array	O
.	O

@USER	O
:	O
If	O
the	O
code	O
all	O
F77	O
,	O
why	O
is	O
the	O
question	O
tagged	O
Python	O
?	O

It	O
does	O
that	O
without	O
densifying	O
the	O
matrix	O
right	O
?	O

To	O
speed	O
up	O
the	O
program	O
,	O
I	O
want	O
to	O
pass	O
the	O
index	O
through	O
a	O
subroutine	O
,	O
but	O
I	O
cannot	O
pass	O
`	O
[	O
index	O
[	O
0	O
]	O
,	O
:	O
,	O
index	O
[	O
1	O
]	O
,	O
index	O
[	O
2	O
]]`	O
through	O
a	O
subroutine	O
because	O
I	O
cannot	O
pass	O
the	O
colon	O
'	O
:	O
'	O
.	O

Any	O
thoughts	O
on	O
what	O
I'm	O
doing	O
wrong	O
?	O

This	O
will	O
be	O
far	O
,	O
far	O
faster	O
than	O
constantly	O
reallocating	O
the	O
array	O
inside	O
the	O
loop	O
.	O

How	O
can	O
I	O
get	O
a	O
new	O
array	O
containing	O
the	O
values	O
of	O
specific	O
attributes	O
of	O
those	O
objects	O
?	O

Seriously	O
,	O
at	O
least	O
leave	O
a	O
note	O
,	O
but	O
given	O
the	O
"	O
complexity	O
"	O
of	O
your	O
actual	O
request	O
I'd	O
say	O
that	O
you'll	O
have	O
better	O
chances	O
with	O
a	O
new	O
question	O
.	O

and	O
find	O
the	O
roots	O
with	O
numpy	O
:	O
#CODE	O

I	O
need	O
to	O
create	O
a	O
numpy	O
array	O
of	O
N	O
elements	O
,	O
but	O
I	O
want	O
to	O
access	O
the	O

I	O
have	O
allocated	O
a	O
chunk	O
of	O
double	O
in	O
a	O
C	O
library	O
and	O
I	O
would	O
like	O
to	O
create	O
a	O
numpy	O
1D	O
array	O
based	O
on	O
that	O
data	O
;	O
ideally	O
I	O
would	O
like	O
two	O
versions	O
one	O
which	O
only	O
wraps	O
the	O
c_ptr	O
readonly	O
-	O
letting	O
the	O
C	O
layer	O
retain	O
ownership	O
of	O
the	O
data	O
,	O
and	O
one	O
which	O
copies	O
the	O
data	O
.	O

The	O
code	O
included	O
in	O
pypy	O
is	O
a	O
new	O
array	O
class	O
which	O
tries	O
to	O
be	O
compatible	O
with	O
numpy	O
,	O
IOW	O
,	O
it	O
is	O
a	O
reimplementation	O
from	O
scratch	O
,	O
without	O
many	O
features	O
from	O
numpy	O
.	O

Like	O
I	O
say	O
,	O
I'm	O
honestly	O
struggling	O
,	O
any	O
help	O
would	O
be	O
much	O
appreciated	O
.	O

with	O
array	O
.	O

I'm	O
not	O
sure	O
that	O
I	O
understand	O
the	O
difference	O
between	O
copying	O
the	O
matrix	O
(	O
example	O
1	O
)	O
and	O
copying	O
the	O
data	O
(	O
example	O
2	O
)	O
.	O

Does	O
anybody	O
know	O
of	O
a	O
(	O
common	O
case	O
)	O
faster-than-linear	O
way	O
to	O
find	O
the	O
endpoints	O
of	O
a	O
boolean	O
property	O
of	O
an	O
array	O
.	O

Any	O
unrecognized	O
type	O
will	O
work	O
this	O
way	O
,	O
so	O
you	O
might	O
want	O
to	O
use	O
`	O
myclass	O
`	O
instead	O
of	O
`	O
object	O
`	O
.	O

Iterate	O
over	O
vectors	O
in	O
a	O
multidimensional	O
numpy	O
array	O

This	O
way	O
you	O
can	O
load	O
a	O
large	O
dataset	O
from	O
a	O
textfile	O
memory-efficiently	O
while	O
retaining	O
all	O
the	O
convenient	O
parsing	O
features	O
of	O
the	O
two	O
functions	O
.	O

you	O
may	O
win	O
few	O
cycles	O
if	O
you	O
multiply	O
by	O
inverse	O
instead	O
of	O
dividing	O
in	O
floating-point	O
performance	O
.	O

Without	O
knowing	O
the	O
size	O
or	O
quantity	O
of	O
the	O
images	O
or	O
the	O
application	O
of	O
the	O
algorithm	O
(	O
computer	O
vision	O
?	O
)	O
,	O
I	O
can't	O
say	O
how	O
big	O
a	O
deal	O
that	O
kind	O
of	O
speedup	O
is	O
.	O

Is	O
there	O
an	O
easy	O
way	O
to	O
sort	O
these	O
eigenvalues	O
(	O
and	O
associated	O
vectors	O
)	O
in	O
order	O
?	O

You	O
can	O
pass	O
a	O
numpy	O
array	O
or	O
matrix	O
as	O
an	O
argument	O
when	O
initializing	O
a	O
sparse	O
matrix	O
.	O

(	O
For	O
most	O
common	O
applications	O
of	O
quadratic	O
forms	O
q	O
A	O
,	O
the	O
matrix	O
A	O
is	O
symmetric	O
,	O
or	O
even	O
symmetric	O
positive	O
definite	O
,	O
so	O
feel	O
free	O
to	O
assume	O
that	O
either	O
one	O
of	O
these	O
is	O
the	O
case	O
,	O
if	O
it	O
matters	O
for	O
your	O
answer	O
.	O
)	O

I	O
think	O
you	O
might	O
find	O
the	O
`	O
flat	O
`	O
method	O
useful	O
.	O

Now	O
that	O
we	O
have	O
both	O
the	O
starting	O
and	O
ending	O
values	O
,	O
we	O
can	O
use	O
the	O
indices	O
function	O
from	O
this	O
question	O
to	O
get	O
an	O
array	O
of	O
selector	O
indices	O
:	O
#CODE	O

10	O
(	O
i	O
?	O
1	O
)	O
K	O
,	O
where	O
K	O
=	O
k	O
/	O
(	O
n	O
?	O
1	O
)	O
.	O

This	O
identifies	O
which	O
rows	O
have	O
any	O
element	O
which	O
are	O
True	O
#CODE	O

Broadcasting	O
is	O
a	O
more	O
general	O
way	O
to	O
fill	O
an	O
array	O
and	O
I	O
would	O
guess	O
is	O
slower	O
or	O
equal	O
to	O
the	O
very	O
narrow	O
use	O
case	O
of	O
`	O
fill	O
`	O
.	O

By	O
"	O
not	O
replicating	O
data	O
"	O
I	O
am	O
assuming	O
you	O
mean	O
"	O
not	O
allocating	O
more	O
memory	O
"	O
.	O

Can	O
you	O
post	O
all	O
/	O
more	O
of	O
the	O
data	O
?	O

The	O
scoring	O
matrix	O
would	O
be	O
trivial	O
,	O
as	O
the	O
"	O
distance	O
"	O
between	O
two	O
numbers	O
is	O
just	O
their	O
difference	O
.	O

Contours	O
around	O
scipy	O
labeled	O
regions	O
in	O
a	O
2D	O
grid	O

Why	O
doesn't	O
the	O
shape	O
of	O
my	O
numpy	O
array	O
change	O
?	O

Mind	O
also	O
the	O
indexing	O
starts	O
at	O
`	O
0	O
`	O

I	O
have	O
a	O
vague	O
feeling	O
that	O
I	O
might	O
have	O
seen	O
a	O
question	O
addressing	O
this	O
problem	O
,	O
but	O
I	O
can't	O
find	O
it	O
now	O
.	O

If	O
you	O
know	O
which	O
rows	O
are	O
to	O
be	O
deleted	O
,	O
just	O
extract	O
the	O
other	O
rows	O
(	O
you	O
need	O
)	O
and	O
create	O
a	O
new	O
array	O
.	O

If	O
there	O
is	O
any	O
other	O
way	O
I	O
guess	O
I	O
have	O
to	O
do	O
that	O
.	O

I	O
have	O
a	O
matrix	O
,	O
say	O
#CODE	O

You	O
might	O
find	O
out	O
the	O
distribution	O
information	O
using	O
`	O
cat	O
/	O
etc	O
/	O
*-release	O
`	O
;)	O

I	O
was	O
wondering	O
if	O
anyone	O
found	O
a	O
good	O
workaround	O
,	O
as	O
my	O
real-world	O
problem	O
of	O
iterating	O
over	O
the	O
Cartesian-product	O
of	O
the	O
rows	O
in	O
very	O
large	O
arrays	O
is	O
so	O
slow	O
it's	O
impeding	O
progress	O
.	O

I	O
have	O
an	O
array	O
of	O
x	O
,	O
y	O
,	O
z	O
distances	O
and	O
I	O
need	O
to	O
find	O
the	O
differences	O
between	O
each	O
vector	O
from	O
one	O
another	O
.	O

The	O
code	O
above	O
finds	O
parts	O
where	O
there	O
are	O
at	O
least	O
MIN_SILENCE	O
consecutive	O
elements	O
smaller	O
than	O
SILENCE_THRESHOLD	O
.	O

The	O
list	O
of	O
indices	O
will	O
always	O
be	O
ascending	O
,	O
never	O
have	O
duplicates	O
,	O
but	O
may	O
have	O
gaps	O
like	O
the	O
example	O
.	O

Any	O
ideas	O
?	O

sum	O
function	O
in	O
python	O

All	O
globals	O
hold	O
either	O
values	O
referenced	O
by	O
those	O
tuples	O
or	O
are	O
lists	O
of	O
tuples	O
.	O

You	O
can	O
pass	O
a	O
list	O
or	O
an	O
array	O
as	O
indexes	O
to	O
any	O
np	O
array	O
.	O

The	O
array	O
I'm	O
using	O
is	O
quite	O
large	O
(	O
3500x3500	O
)	O
,	O
so	O
I'm	O
wondering	O
where	O
the	O
best	O
place	O
to	O
load	O
it	O
is	O
for	O
repeated	O
use	O
.	O

Basically	O
,	O
it	O
comes	O
down	O
to	O
checking	O
before	O
you	O
add	O
.	O

I	O
have	O
a	O
simple	O
function	O
called	O
get_gradient	O
which	O
takes	O
a	O
numpy	O
array	O
of	O
[[	O
x	O
,	O
y	O
,	O
Vx	O
,	O
Vy	O
]]	O
and	O
returns	O
(	O
should	O
return	O
)	O
an	O
array	O
of	O
[[	O
Vx	O
,	O
Vy	O
,	O
Ax	O
,	O
Ay	O
]]	O
.	O

I	O
found	O
this	O
post	O
:	O
Python	O
:	O
finding	O
an	O
element	O
in	O
an	O
array	O

So	O
,	O
are	O
VBOs	O
simply	O
not	O
meant	O
to	O
be	O
that	O
big	O
(	O
I	O
somehow	O
doubt	O
that	O
VBOs	O
could	O
only	O
have	O
around	O
17k	O
triangles	O
each	O
)	O
?	O

`	O
flags	O
`	O
parameter	O
leads	O
to	O
`	O
TypeError	O
`	O
if	O
input	O
array	O
is	O
not	O
contiguous	O
.	O

I	O
then	O
have	O
a	O
2nd	O
array	O
similar	O
to	O
#CODE	O

Convert	O
a	O
list	O
of	O
2D	O
numpy	O
arrays	O
to	O
one	O
3D	O
numpy	O
array	O
?	O

I'm	O
currently	O
a	O
grad	O
student	O
at	O
Harvard	O
and	O
a	O
good	O
friend	O
of	O
mine	O
went	O
there	O
(	O
he	O
would	O
have	O
graduated	O
two	O
or	O
three	O
years	O
ago	O
,	O
as	O
he	O
is	O
currently	O
a	O
second-year	O
grad	O
student	O
here	O
at	O
Harvard	O
with	O
me	O
)	O
.	O

I'm	O
not	O
clear	O
on	O
how	O
you	O
are	O
wanting	O
to	O
plot	O
it	O
,	O
but	O
it	O
sound	O
like	O
you'll	O
need	O
to	O
select	O
some	O
values	O
of	O
a	O
column	O
.	O

The	O
issue	O
I	O
am	O
running	O
in	O
to	O
is	O
that	O
the	O
array	O
can	O
be	O
larger	O
than	O
3gb	O
in	O
size	O
(	O
these	O
are	O
huge	O
images	O
)	O
and	O
I	O
need	O
to	O
segment	O
them	O
prior	O
to	O
ingesting	O
them	O
.	O

The	O
latter	O
might	O
be	O
faster	O
because	O
it	O
doesn't	O
produce	O
the	O
intermediate	O
`	O
x**2	O
`	O
array	O
.	O

Any	O
suggestions	O
?	O

"	O
A	O
copy	O
of	O
arr	O
with	O
the	O
elements	O
specified	O
by	O
obj	O
removed	O
.	O

is	O
not	O
it	O
another	O
copy	O
?	O

NOTE	O
:	O
the	O
row	O
has	O
"	O
:	O
"	O
,	O
but	O
the	O
"	O
:	O
"	O
does	O
mean	O
the	O
dict	O
'	O
:	O
'	O
.	O

If	O
,	O
for	O
some	O
reason	O
,	O
I	O
would	O
only	O
save	O
one	O
dictionary	O
then	O
every	O
script	O
loading	O
this	O
file	O
with	O
pickle	O
would	O
mess	O
up	O
the	O
order	O
of	O
the	O
stored	O
variables	O
.	O

You	O
might	O
also	O
want	O
to	O
take	O
a	O
look	O
at	O
Anvil	O
,	O
announcement	O
here	O
.	O

The	O
other	O
way	O
that	O
I	O
know	O
is	O
to	O
convert	O
Y	O
to	O
list	O
iteratively	O
.	O

This	O
is	O
especially	O
helpful	O
since	O
it	O
includes	O
the	O
import	O
commands	O
and	O
info	O
on	O
how	O
to	O
write	O
to	O
file	O
.	O

But	O
actually	O
I	O
am	O
not	O
so	O
sure	O
that	O
from	O
where	O
you	O
are	O
now	O
,	O
using	O
sparse	O
matrices	O
will	O
gain	O
you	O
any	O
speed-up	O
.	O

Upon	O
deeper	O
examination	O
of	O
the	O
relationship	O
between	O
the	O
python	O
printout	O
and	O
the	O
structure	O
of	O
my	O
underlying	O
data	O
,	O
I	O
see	O
that	O
the	O
python	O
print	O
command	O
is	O
saying	O
that	O
there	O
are	O
two	O
empty	O
columns	O
at	O
the	O
end	O
of	O
the	O
array	O
.	O

How	O
to	O
convert	O
a	O
simple	O
list	O
of	O
lists	O
into	O
a	O
numppy	O
array	O
?	O

Django	O
has	O
a	O
library	O
for	O
encapsulating	O
all	O
the	O
database	O
work	O
into	O
Python	O
classes	O
,	O
so	O
you	O
don't	O
have	O
to	O
mess	O
with	O
raw	O
SQL	O
until	O
you	O
have	O
to	O
do	O
something	O
really	O
clever	O
.	O

So	O
I	O
got	O
numpy	O
,	O
scipy	O
,	O
IPython	O
,	O
and	O
matplotlib	O
working	O
(	O
I	O
can	O
import	O
all	O
four	O
with	O
"	O
import	O
_	O
)"	O
.	O

@USER	O
`	O
new	O
type	O
not	O
compatible	O
with	O
array	O
.	O

Is	O
there	O
any	O
way	O
to	O
do	O
this	O
in	O
Python	O
?	O

Then	O
you	O
can	O
choose	O
many	O
methods	O
to	O
visualize	O
it	O
.	O

Numpy	O
Array	O
to	O
base64	O
and	O
back	O
to	O
Numpy	O
Array	O
-	O
Python	O

In	O
each	O
iteration	O
of	O
Gibbs	O
sampling	O
,	O
we	O
remove	O
one	O
(	O
current	O
)	O
word	O
,	O
sample	O
a	O
new	O
topic	O
for	O
that	O
word	O
according	O
to	O
a	O
posterior	O
conditional	O
probability	O
distribution	O
inferred	O
from	O
the	O
LDA	O
model	O
,	O
and	O
update	O
word-topic	O
counts	O
,	O
as	O
follows	O
:	O
#CODE	O

I	O
am	O
getting	O
weird	O
errors	O
when	O
I	O
try	O
to	O
convert	O
a	O
black	O
and	O
white	O
PIL	O
image	O
to	O
a	O
numpy	O
array	O
.	O

Numpy	O
arrays	O
have	O
a	O
`	O
copy	O
`	O
method	O
which	O
you	O
can	O
use	O
for	O
just	O
this	O
purpose	O
.	O

Actually	O
I	O
could	O
not	O
test	O
with	O
big	O
K	O
,	O
d	O
and	O
N	O
as	O
I	O
was	O
going	O
out	O
of	O
memory	O
.	O

With	O
all	O
of	O
these	O
options	O
you	O
have	O
to	O
pay	O
a	O
JNA	O
tax	O
...	O
all	O
of	O
your	O
data	O
has	O
to	O
be	O
copied	O
before	O
it	O
can	O
be	O
processed	O
.	O

Useless	O
because	O
it	O
ignores	O
the	O
"	O
cross	O
platform	O
issues	O
,	O
proprietary	O
tool	O
chains	O
,	O
certification	O
gates	O
,	O
licensed	O
technologies	O
,	O
and	O
stringent	O
performance	O
requirements	O
on	O
top	O
of	O
the	O
issues	O
with	O
legacy	O
codebases	O
and	O
workforce	O
availability	O
"	O
(	O
John	O
Carmack	O
)	O
that	O
op	O
is	O
probably	O
facing	O
.	O

And	O
that	O
the	O
values	O
of	O
all	O
(	O
x	O
,	O
y	O
)	O
pairs	O
are	O
given	O
.	O

Is	O
is	O
possible	O
to	O
have	O
a	O
3-D	O
record	O
array	O
in	O
numpy	O
?	O

However	O
,	O
the	O
evidence	O
suggests	O
that	O
you've	O
encountered	O
an	O
issue	O
of	O
this	O
sort	O
.	O

There's	O
_way_	O
less	O
overhead	O
this	O
way	O
.	O

I'm	O
having	O
trouble	O
figuring	O
out	O
what	O
kind	O
of	O
test	O
I	O
need	O
here	O
,	O
and	O
the	O
best	O
numpy	O
/	O
scipy	O
/	O
R	O
function	O
to	O
use	O
for	O
these	O
kinds	O
of	O
issues	O
.	O

I	O
have	O
see	O
people	O
using	O
dictionaries	O
,	O
but	O
the	O
arrays	O
are	O
large	O
and	O
filled	O
with	O
both	O
positive	O
and	O
negative	O
floats	O
.	O

How	O
can	O
I	O
speed	O
up	O
iteration	O
through	O
this	O
transformed	O
numpy	O
array	O
?	O

This	O
is	O
may	O
not	O
be	O
the	O
best	O
way	O
to	O
solve	O
this	O
but	O
have	O
a	O
look	O
at	O
the	O
following	O
...	O

All	O
in	O
all	O
,	O
I	O
would	O
go	O
with	O
the	O
#CODE	O

This	O
is	O
not	O
a	O
matter	O
of	O
style	O
.	O
without	O
the	O
list	O
(	O
_	O
)	O
it	O
does	O
not	O
even	O
work	O
at	O
last	O
for	O
the	O
case	O
i	O
have	O
that	O
y	O
is	O
an	O
array	O
itself	O

(	O
at	O
least	O
it	O
gives	O
me	O
an	O
error	O
stating	O
that	O
the	O
'	O
as	O
'	O
is	O
reserved	O
in	O
python	O
2.6	O
)	O
Am	O
I	O
correct	O
?	O

Did	O
you	O
try	O
looking	O
at	O
numpy	O
for	O
matlab	O
users	O
manuals	O
,	O
like	O
:	O
#URL	O

I	O
would	O
not	O
try	O
to	O
process	O
`	O
arr	O
`	O
in	O
place	O
-	O
it	O
seems	O
that	O
a	O
new	O
array	O
is	O
created	O
under	O
the	O
hood	O
in	O
most	O
cases	O
anyway	O
.	O

Now	O
you	O
must	O
initialize	O
each	O
element	O
of	O
the	O
numpy	O
array	O
to	O
be	O
an	O
1-d	O
numpy	O
array	O
:	O
#CODE	O

The	O
easiest	O
way	O
around	O
this	O
is	O
to	O
just	O
use	O
a	O
numpy	O
array	O
,	O
instead	O
of	O
a	O
numpy	O
matrix	O
:	O
#CODE	O

I	O
am	O
trying	O
to	O
create	O
an	O
affinity	O
matrix	O
for	O
an	O
image	O
.	O

to	O
handle	O
the	O
error	O
cases	O
and	O
the	O
return	O
value	O
,	O
they	O
are	O
not	O
related	O
to	O
the	O
array	O
assignment	O
.	O

Saving	O
a	O
Numpy	O
array	O
as	O
an	O
image	O
(	O
instructions	O
)	O

Using	O
this	O
,	O
I	O
know	O
I	O
am	O
calculating	O
r-squared	O
correctly	O
for	O
linear	O
best-fit	O
(	O
degree	O
equals	O
1	O
)	O
.	O

No	O
expert	O
on	O
the	O
topic	O
,	O
but	O
this	O
is	O
some	O
kind	O
of	O
adjency	O
matrix	O
(	O
#URL	O
)	O
.	O

about	O
15	O
times	O
faster	O
using	O
broadcast	O

Arrays	O
to	O
Matrix	O
numpy	O

but	O
it	O
appears	O
to	O
only	O
take	O
square	O
matrices	O
.	O

Any	O
idea	O
how	O
that	O
can	O
be	O
done	O
?	O

In	O
your	O
code	O
,	O
`	O
a	O
[	O
condition	O
]	O
[	O
index	O
]`	O
returns	O
the	O
value	O
in	O
a	O
,	O
but	O
I	O
want	O
the	O
INDEX	O
in	O
a	O
,	O
so	O
that	O
`	O
a	O
[	O
INDEX	O
]	O
=	O
a	O
[	O
condition	O
]	O
[	O
index	O
]`	O
.	O

Any	O
database	O
that	O
can	O
create	O
an	O
index	O
will	O
provide	O
relatively	O
fast	O
look-ups	O
(	O
depending	O
on	O
how	O
many	O
millions	O
of	O
records	O
you're	O
storing	O
)	O
.	O

Actually	O
,	O
the	O
best	O
way	O
to	O
manage	O
packages	O
on	O
OS	O
X	O
is	O
[	O
Homebrew	O
]	O
(	O
#URL	O
)	O
(	O
not	O
Fink	O
or	O
MacPorts	O
:))	O
-	O
which	O
unfortunately	O
lists	O
neither	O
NumPy	O
now	O
SciPy	O
at	O
the	O
current	O
time	O
.	O

I	O
would	O
like	O
to	O
keep	O
`	O
xcoords	O
`	O
a	O
numpy	O
array	O
if	O
possible	O
.	O
what	O
do	O
you	O
mean	O
'	O
adding	O
them	O
to	O
the	O
object	O
before	O
it	O
is	O
returned	O
'	O
?	O

But	O
I	O
just	O
need	O
to	O
sort	O
out	O
which	O
points	O
to	O
send	O
for	O
a	O
complete	O
graph	O
.	O

how	O
do	O
I	O
calculate	O
that	O
an	O
array	O
of	O
python	O
numpy	O
or	O
me	O
of	O
all	O
the	O
calculate	O
decimals	O
and	O
not	O
skip	O
like	O
.	O

It	O
will	O
support	O
it	O
on	O
the	O
next	O
release	O
.	O

Python	O
lists	O
are	O
defined	O
with	O
square	O
brackets	O
,	O
and	O
we	O
want	O
to	O
generate	O
a	O
list	O
of	O
lists	O
(	O
where	O
each	O
piece	O
contains	O
one	O
of	O
your	O
defined	O
segments	O
)	O
.	O

The	O
biggest	O
gotcha	O
for	O
me	O
was	O
that	O
almost	O
every	O
standard	O
operator	O
is	O
overloaded	O
to	O
distribute	O
across	O
the	O
array	O
.	O

I	O
want	O
to	O
combine	O
the	O
two	O
into	O
a	O
mutli-dimensional	O
numpy	O
array	O
.	O

where	O
`	O
nlooks	O
`	O
and	O
`	O
dfactor	O
`	O
are	O
scalars	O
and	O
`	O
Ic	O
`	O
is	O
the	O
unfiltered	O
array	O
.	O

In	O
a	O
10x5x5	O
matrix	O
with	O
`	O
x	O
[	O
0	O
,	O
:	O
,	O
:]	O
=	O
0	O
`	O
I	O
would	O
expect	O
a	O
result	O
of	O
:	O
#CODE	O

For	O
example	O
:	O
I	O
have	O
a	O
=	O
array	O
([	O
123	O
,	O
412	O
,	O
444	O
])	O

While	O
it	O
often	O
results	O
in	O
a	O
massive	O
speedup	O
to	O
eliminate	O
for	O
loops	O
and	O
take	O
advantage	O
of	O
numpy	O
built-ins	O
/	O
vectorization	O
.	O

If	O
I	O
understand	O
correctly	O
you	O
have	O
a	O
three	O
dimensional	O
array	O
,	O
something	O
like	O
:	O
#CODE	O

@USER	O
:	O
where	O
is	O
a	O
new	O
array	O
created	O
?	O

array	O
([	O
41	O
,	O
32	O
,	O
41	O
,	O
33	O
,	O
42	O
,	O
32	O
,	O
42	O
,	O
33	O
])	O

Any	O
idea	O
when	O
it	O
will	O
be	O
ready	O
?	O

I	O
see	O
you've	O
taken	O
care	O
of	O
my	O
edge	O
issues	O
,	O
although	O
your	O
filter	O
size	O
is	O
hardcoded	O
;)	O
.	O

If	O
you	O
open	O
idle	O
and	O
type	O
`	O
import	O
matplotlib	O
`	O
it	O
shouldn't	O
return	O
an	O
error	O

No	O
expert	O
on	O
the	O
topic	O
,	O
but	O
this	O
is	O
some	O
kind	O
of	O
adjency	O
matrix	O
(	O
#URL	O
)	O
.	O

Edit	O
:	O
If	O
it's	O
a	O
floating	O
point	O
issue	O
,	O
what	O
sort	O
of	O
floating	O
point	O
error	O
mistakes	O
a	O
number	O
much	O
less	O
than	O
1	O
as	O
one	O
around	O
8	O
?	O

The	O
question	O
was	O
about	O
how	O
to	O
slice	O
if	O
the	O
rank	O
is	O
not	O
known	O
at	O
the	O
time	O
I	O
write	O
the	O
code	O
.	O

I	O
think	O
a	O
typical	O
method	O
is	O
to	O
always	O
double	O
the	O
size	O
,	O
when	O
you	O
really	O
don't	O
know	O
how	O
large	O
things	O
will	O
be	O
.	O

This	O
script	O
is	O
mainly	O
intended	O
to	O
demonstrate	O
building	O
an	O
independent	O
python	O
in	O
your	O
home	O
directory	O
,	O
and	O
assumes	O
the	O
system	O
you're	O
building	O
on	O
has	O
the	O
proper	O
dependencies	O
already	O
installed	O
,	O
but	O
it	O
at	O
least	O
points	O
you	O
in	O
the	O
right	O
direction	O
.	O

and	O
use	O
the	O
information	O
on	O
the	O
size	O
inclued	O
in	O
the	O
filename	O
to	O
restore	O
the	O
initial	O
shape	O

Hmm	O
I	O
added	O
for	O
first	O
example	O
,	O
did	O
you	O
know	O
how	O
to	O
copy	O
from	O
IDE	O
exactly	O
with	O
commas	O
and	O
everything	O
..?	O

@USER	O
:	O
Your	O
answer	O
will	O
give	O
false	O
positives	O
in	O
the	O
event	O
that	O
one	O
or	O
more	O
(	O
but	O
not	O
all	O
)	O
of	O
the	O
elements	O
in	O
B	O
matches	O
with	O
one	O
of	O
the	O
rows	O
in	O
A	O
.	O

I	O
would	O
like	O
to	O
average	O
the	O
2	O
different	O
arrays	O
contained	O
within	O
`	O
record	O
`	O
.	O

I	O
need	O
to	O
constrained	O
minimization	O
of	O
some	O
data	O
(	O
ie	O
so	O
that	O
I	O
get	O
the	O
minimum	O
value	O
within	O
a	O
certain	O
range	O
)	O
.	O

In	O
this	O
case	O
,	O
I	O
would	O
like	O
to	O
return	O
the	O
index	O
2	O
(	O
2nd	O
row	O
)	O
.	O

a	O
32	O
bits	O
process	O
can	O
only	O
access	O
around	O
4	O
GB	O
of	O
memory	O
.	O

How	O
do	O
I	O
find	O
out	O
,	O
if	O
the	O
numpy	O
BLAS	O
libraries	O
are	O
availalbe	O
as	O
dynamically-loadable	O
?	O

(	O
they	O
are	O
at	O
same	O
scale	O
)	O

Now	O
simply	O
create	O
a	O
new	O
array	O
and	O
multiply	O
:	O
#CODE	O

Take	O
a	O
look	O
at	O
this	O
Project	O
Euler	O
problem	O
:	O
#URL	O

Python	O
:	O
how	O
to	O
store	O
a	O
numpy	O
multidimensional	O
array	O
in	O
PyTables	O
?	O

How	O
can	O
i	O
load	O
all	O
24	O
joblib	O
files	O
in	O
one	O
program	O
without	O
any	O
errors	O
?	O

Where	O
I'm	O
stuck	O
is	O
what	O
the	O
wrapper	O
code	O
should	O
then	O
look	O
like	O
to	O
pass	O
a	O
MxN	O
numpy	O
array	O
to	O
the	O
**	O
coords1	O
and	O
**	O
coords2	O
arguments	O
.	O

I	O
have	O
created	O
a	O
numpy	O
2d	O
array	O
of	O
type	O
string	O
called	O
'	O
minutes_array	O
'	O
with	O
the	O
first	O
column	O
as	O
unix	O
timestamps	O
rounded	O
to	O
the	O
nearest	O
minute	O
covering	O
every	O
minute	O
from	O
the	O
start	O
of	O
the	O
sensor	O
timeseries	O
to	O
the	O
end	O
with	O
three	O
empty	O
columns	O
to	O
be	O
filled	O
with	O
data	O
from	O
each	O
of	O
the	O
3	O
sensors	O
where	O
available	O
.	O

Which	O
can	O
be	O
done	O
in	O
O	O
(	O
n	O
)	O
,	O
but	O
your	O
answer	O
requires	O
O	O
(	O
mn	O
)	O
,	O
where	O
m	O
is	O
size	O
of	O
window	O
.	O

Somehow	O
I	O
always	O
thought	O
you	O
can	O
load	O
the	O
shared	O
library	O
compiled	O
with	O
any	O
compiler	O
.	O

`	O
array	O
=[	O
'	O
NaN	O
'	O
,	O
'	O
20	O
'	O
,	O
'	O
383.333	O
'	O
,	O
'	O
NaN	O
'	O
,	O
'	O
NaN	O
'	O
,	O
'	O
NaN	O
'	O
,	O
'	O
5	O
'	O
,	O
'	O
100	O
'	O
,	O
'	O
129	O
'	O
,	O
'	O
122.5	O
'	O
,	O
'	O
NaN	O
'	O
,	O
'	O
NaN	O
']`	O

array	O
,	O
and	O
then	O
use	O
`	O
view	O
`	O
to	O
turn	O
it	O
into	O
a	O
structured	O
array	O
,	O
and	O
then	O
use	O

and	O
so	O
all	O
we	O
need	O
to	O
do	O
is	O
:	O
#CODE	O

Any	O
clue	O
to	O
why	O
this	O
is	O
happening	O
?	O

I	O
think	O
the	O
definition	O
used	O
in	O
the	O
field	O
of	O
statistics	O
is	O
the	O
value	O
in	O
the	O
middle	O
of	O
your	O
data	O
array	O
after	O
it	O
has	O
been	O
sorted	O
.	O

Dense	O
covariance	O
matrices	O
of	O
that	O
size	O
suggest	O
operations	O
that	O
run	O
forever	O
!	O

In	O
this	O
case	O
,	O
I'd	O
like	O
it	O
to	O
return	O
a	O
density	O
that's	O
essentially	O
peaked	O
completely	O
at	O
a	O
difference	O
of	O
0	O
,	O
with	O
no	O
mass	O
everywhere	O
else	O
.	O

If	O
the	O
array	O
is	O
doubles	O
(	O
remember	O
python	O
floats	O
are	O
C	O
doubles	O
by	O
default	O
)	O
then	O
you	O
have	O
to	O
think	O
a	O
bit	O
harder	O
as	O
==	O
is	O
not	O
really	O
safe	O
or	O
what	O
you	O
want	O
for	O
floating	O
point	O
values	O
.	O

They	O
all	O
have	O
their	O
strengths	O
and	O
weaknesses	O
.	O

numpy	O
array	O
of	O
chars	O
to	O
string	O

matrix	O
rank	O
:	O
#CODE	O

This	O
slows	O
down	O
for	O
large	O
sigma	O
,	O
at	O
which	O
point	O
using	O
FFT-based	O
smoothing	O
might	O
be	O
faster	O
.	O

What	O
is	O
the	O
fastest	O
way	O
to	O
iterate	O
through	O
all	O
one	O
dimensional	O
sub-arrays	O
of	O
an	O
n	O
dimensional	O
array	O
in	O
python	O
.	O

This	O
works	O
,	O
but	O
it's	O
really	O
slow	O
.	O

If	O
I	O
create	O
a	O
simple	O
array	O
like	O
this	O
in	O
Python	O
I'm	O
able	O
to	O
read	O
the	O
values	O
in	O
the	O
C	O
code	O
:	O

In	O
an	O
ideal	O
world	O
,	O
the	O
function	O
or	O
class	O
would	O
support	O
overlap	O
between	O
the	O
divisions	O
in	O
the	O
input	O
matrix	O
too	O
.	O

My	O
problem	O
is	O
different	O
because	O
I	O
need	O
to	O
find	O
**	O
all	O
**	O
the	O
roots	O
of	O
my	O
function	O
,	O
on	O
a	O
given	O
interval	O
.	O

How	O
can	O
I	O
create	O
a	O
PyArrayObject	O
from	O
this	O
structure	O
,	O
specially	O
how	O
I	O
can	O
create	O
a	O
numpy	O
array	O
that	O
hold	O
3	O
object	O
(	O
off	O
course	O
3	O
is	O
an	O
example	O
here	O
)	O
(	O
each	O
of	O
them	O
is	O
an	O
array	O
)	O

x	O
:	O
a	O
numpy	O
2d	O
array	O

Thanks	O
for	O
the	O
info	O
.	O

How	O
would	O
you	O
avoid	O
the	O
loop	O
in	O
the	O
case	O
that	O
all	O
entries	O
in	O
`	O
repl	O
`	O
are	O
the	O
same	O
?	O

Pulling	O
data	O
from	O
a	O
numpy	O
array	O

There's	O
no	O
effective	O
difference	O
(	O
they	O
both	O
return	O
views	O
into	O
the	O
original	O
array	O
)	O
.	O

Thanks	O
for	O
all	O
the	O
tips	O
!	O

remove	O
zero	O
lines	O
2-D	O
numpy	O
array	O

Instead	O
of	O
using	O
`	O
PyInt_AsLong	O
`	O
,	O
use	O
the	O
`	O
PyArray_*	O
`	O
functions	O
provided	O
by	O
Numpy's	O
C	O
API	O
to	O
access	O
the	O
data	O
;	O
in	O
particular	O
,	O
see	O
section	O
Array	O
API	O
.	O

Well	O
,	O
I	O
tried	O
dividing	O
by	O
the	O
largest	O
place	O
value	O
.	O

All	O
of	O
those	O
numpys	O
are	O
linked	O
to	O
the	O
system	O
Accelerate	O
framework	O
:	O
#CODE	O

and	O
I	O
wish	O
to	O
create	O
a	O
third	O
array	O
with	O
each	O
element	O
from	O
`	O
b	O
`	O
appearing	O
`	O
a	O
`	O
times	O
in	O
the	O
new	O
array	O
,	O
as	O
:	O
#CODE	O

I	O
can	O
imagine	O
a	O
number	O
of	O
approaches	O
to	O
storing	O
both	O
of	O
these	O
data	O
formats	O
,	O
ranging	O
from	O
storing	O
the	O
metadata	O
with	O
the	O
`	O
AttributeSet	O
`	O
class	O
for	O
each	O
`	O
Array	O
`	O
/	O
`	O
CArray	O
`	O
to	O
using	O
a	O
`	O
Table	O
`	O
for	O
all	O
of	O
the	O
metadata	O
.	O

I	O
want	O
to	O
calculate	O
the	O
average	O
of	O
four	O
neighbors	O
in	O
a	O
huge	O
array	O
.	O

Suppress	O
Scientific	O
Notation	O
in	O
Numpy	O
When	O
Creating	O
Array	O
From	O
Nested	O
List	O

I	O
want	O
to	O
find	O
the	O
vector	O
x	O
'	O
such	O
that	O
Ax	O
'	O
is	O
as	O
close	O
as	O
possible	O
to	O

And	O
the	O
dataset	O
in	O
question	O
is	O
beyond	O
doubt	O
particular	O
:	O
There	O
certainly	O
is	O
an	O
upper	O
bound	O
and	O
a	O
precision	O
.	O

Only	O
integers	O
can	O
be	O
used	O
as	O
array	O
or	O
matrix	O
indices	O
.	O

I	O
can't	O
find	O
it	O
online	O
anywhere	O
.	O

I	O
will	O
try	O
your	O
code	O
,	O
but	O
I	O
am	O
also	O
going	O
to	O
try	O
writing	O
a	O
simple	O
C	O
extension	O
to	O
simply	O
do	O
the	O
reading	O
,	O
math	O
,	O
and	O
drawing	O
all	O
in	O
one	O
place	O
.	O

Are	O
there	O
any	O
good	O
greedy	O
implementations	O
to	O
solve	O
this	O
or	O
am	O
I	O
on	O
my	O
own	O
to	O
implement	O
this	O
?	O

The	O
problem	O
is	O
that	O
for	O
the	O
array	O
input	O
,	O
SWIG	O
complains	O
that	O
there	O
is	O
no	O
typemap	O
.	O

Is	O
`	O
column_array_to_add	O
`	O
another	O
2D	O
array	O
,	O
or	O
is	O
it	O
a	O
1D	O
column	O
array	O
,	O
as	O
the	O
name	O
implies	O
?	O

the	O
sum	O
of	O
a	O
triple-product	O
(	O
element-wise	O
)	O
.	O

I	O
ran	O
a	O
simple	O
speed	O
test	O
comparing	O
numpy	O
and	O
python	O
list	O
comprehension	O
,	O
and	O
apparently	O
list	O
comprehension	O
was	O
faster	O
.	O

That	O
is	O
why	O
your	O
sample	O
loop	O
has	O
been	O
collapsed	O
to	O
read	O
in	O
the	O
full	O
sample	O
for	O
the	O
receiver	O
and	O
channel	O
in	O
one	O
large	O
read	O
.	O

Something	O
like	O
the	O
following	O
iterator	O
should	O
get	O
around	O
both	O
of	O
these	O
problems	O
:	O
#CODE	O

I	O
appreciate	O
any	O
input	O
on	O
this	O
...	O

Do	O
you	O
really	O
need	O
to	O
find	O
such	O
a	O
weird	O
thing	O
?	O

Any	O
particular	O
reason	O
you	O
don't	O
want	O
to	O
use	O
a	O
straightforward	O
approach	O
?	O

The	O
advantage	O
of	O
numpy	O
is	O
the	O
support	O
of	O
slicing	O
at	O
different	O
levels	O
.	O

An	O
implementation	O
,	O
however	O
,	O
is	O
not	O
really	O
open	O
to	O
interpretation	O
.	O

Python	O
numpy	O
masked	O
array	O
initialization	O

You	O
can	O
further	O
optimize	O
by	O
exploiting	O
array-order	O
alignment	O
to	O
reduce	O
excess	O
memory	O
consumption	O
caused	O
by	O
copying	O
the	O
original	O
arrays	O
.	O

For	O
example	O
,	O
any	O
vector	O
(	O
of	O
the	O
appropriate	O
dimension	O
)	O
can	O
be	O
an	O
eigenvector	O
of	O
the	O
identity	O
matrix	O
.	O

The	O
normal	O
64-bit	O
double-precision	O
floating	O
point	O
has	O
least	O
positive	O
normal	O
value	O
2.2E-308	O
;	O
storing	O
logs	O
gives	O
you	O
an	O
effective	O
least	O
positive	O
normal	O
1E-	O
(	O
1.7E308	O
)	O
.	O

index	O
set	O
for	O
each	O
position	O
in	O
the	O
index	O
arrays	O
.	O

I	O
am	O
wondering	O
if	O
reassigning	O
temp	O
[	O
]	O
to	O
a	O
1-element	O
shorter	O
vector	O
each	O
time	O
is	O
slow	O
,	O
would	O
it	O
be	O
faster	O
to	O
pre-allocate	O
a	O
96-3	O
length	O
list	O
of	O
vectors	O
of	O
length	O
96	O
,	O
95	O
,	O
94	O
...	O
to	O
3	O
?	O

What	O
would	O
we	O
do	O
,	O
if	O
we	O
wanted	O
to	O
change	O
values	O
at	O
indexes	O
which	O
are	O
multiple	O
of	O
given	O
n	O
,	O
like	O
a	O
[	O
2	O
]	O
,	O
a	O
[	O
4	O
]	O
,	O
a	O
[	O
6	O
]	O
,	O
a	O
[8	O
]	O
.....	O
for	O
n=2	O
?	O

Thanks	O
for	O
all	O
the	O
python	O
guidance	O
!	O

I'm	O
not	O
really	O
pro	O
Matlab	O
,	O
but	O
surely	O
Stata	O
can't	O
be	O
so	O
bad	O
as	O
to	O
require	O
`	O
adoedit	O
`	O
just	O
to	O
know	O
what	O
algorithm	O
it	O
is	O
using	O
?	O

This	O
can	O
be	O
found	O
relatively	O
easily	O
by	O
just	O
looking	O
at	O
points	O
where	O
the	O
potential	O
exceeds	O
a	O
certain	O
threshold	O
.	O

Negative	O
indices	O
are	O
interpreted	O
as	O
counting	O
from	O
the	O
end	O
of	O
the	O
array	O

I	O
was	O
using	O
unsigned	O
int	O
indices	O
to	O
speed	O
up	O
access	O
according	O
to	O
:	O
#URL	O

I've	O
tried	O
to	O
vectorise	O
it	O
using	O
numpy	O
but	O
I'm	O
not	O
really	O
sure	O
how	O
to	O
do	O
it	O
given	O
that	O
the	O
matrix	O
/	O
2D	O
array	O
gets	O
changed	O
on	O
each	O
iteration	O
.	O

Numpy	O
slicing	O
x	O
,	O
y	O
,	O
z	O
array	O
for	O
variable	O
z	O

I	O
would	O
like	O
to	O
convert	O
(	O
a	O
more	O
complicated	O
form	O
of	O
)	O
the	O
follwing	O
Matlab	O
code	O
#CODE	O

I	O
have	O
a	O
NumPy	O
array	O
'	O
boolarr	O
'	O
of	O
boolean	O
type	O
.	O

If	O
you	O
have	O
only	O
integers	O
that	O
are	O
between	O
0	O
and	O
n	O
(	O
if	O
not	O
its	O
no	O
problem	O
to	O
generalize	O
to	O
any	O
integer	O
range	O
unless	O
its	O
very	O
sparse	O
)	O
,	O
the	O
most	O
efficient	O
way	O
is	O
the	O
use	O
of	O
take	O
/	O
fancy	O
indexing	O
:	O
#CODE	O

Instead	O
of	O
2D	O
coordinates	O
,	O
I	O
use	O
index	O
for	O
every	O
elements	O
in	O
the	O
matrix	O
.	O

I	O
already	O
tried	O
converting	O
the	O
cols	O
to	O
int	O
but	O
that	O
didn't	O
solve	O
it	O
.	O

Although	O
I'm	O
sure	O
there	O
are	O
methods	O
for	O
applying	O
RK	O
to	O
an	O
equation	O
such	O
as	O
this	O
,	O
I	O
didn't	O
find	O
any	O
evidence	O
of	O
them	O
in	O
_Numerical	O
Recipes_	O
,	O
which	O
I	O
think	O
qualifies	O
that	O
topic	O
as	O
relatively	O
obscure	O
;-)	O

When	O
facing	O
a	O
big	O
computation	O
,	O
it	O
will	O
run	O
tests	O
using	O
several	O
implementations	O
to	O
find	O
out	O
which	O
is	O
the	O
fastest	O
one	O
on	O
our	O
computer	O
at	O
this	O
moment	O
.	O

Use	O
an	O
array	O
of	O
floating	O
point	O
numbers	O
instead	O
.	O

`	O
numpy	O
`	O
slicing	O
operations	O
probably	O
involve	O
`	O
for	O
`	O
loops	O
at	O
some	O
level	O
,	O
but	O
they're	O
implemented	O
in	O
c	O
,	O
and	O
provide	O
a	O
linear	O
time	O
solution	O
for	O
this	O
.	O

I	O
have	O
one	O
question	O
:	O
Is	O
there	O
only	O
one	O
way	O
to	O
do	O
addition	O
of	O
two	O
matrix	O
?	O

@USER	O
It	O
is	O
now	O
supported	O
,	O
at	O
least	O
in	O
my	O
version	O
(	O
1.7.1	O
)	O
.	O

I	O
know	O
I	O
could	O
start	O
a	O
number	O
of	O
times	O
at	O
random	O
locations	O
but	O
I'm	O
not	O
able	O
to	O
do	O
that	O
with	O
what	O
I	O
am	O
currently	O
working	O
on	O
and	O
have	O
to	O
use	O
on	O
of	O
these	O
minimisers	O
out	O
of	O
the	O
box	O
.	O

For	O
small	O
displacements	O
of	O
around	O
4-5	O
pixels	O
,	O
the	O
direction	O
of	O
vector	O
calculated	O
seems	O
to	O
be	O
fine	O
,	O
but	O
the	O
magnitude	O
of	O
the	O
vector	O
is	O
too	O
small	O
(	O
that's	O
why	O
I	O
had	O
to	O
multiply	O
u	O
,	O
v	O
by	O
3	O
before	O
plotting	O
them	O
)	O
.	O

However	O
,	O
I	O
will	O
need	O
to	O
access	O
all	O
waveforms	O
at	O
some	O
point	O
.	O

I've	O
find	O
this	O
:	O
#URL	O
but	O
when	O
I	O
try	O
to	O
install	O
this	O
I	O
get	O
an	O
error	O
:	O
#CODE	O

yes	O
,	O
I	O
can	O
assume	O
either	O
that	O
I	O
have	O
g	O
explicitly	O
or	O
that	O
I	O
can	O
sample	O
x	O
according	O
to	O
g	O
.	O

`	O
example	O
`	O
is	O
a	O
structured	O
array	O
consisting	O
of	O
two	O
elements	O
(	O
`	O
(	O
1	O
,	O
2	O
,	O
3	O
)`	O
and	O
`	O
(	O
4	O
,	O
5	O
,	O
6	O
)`)	O
,	O
each	O
element	O
(	O
or	O
'	O
record	O
')	O
having	O
3	O
fields	O
.	O

If	O
i	O
have	O
two	O
variables	O
-	O
where	O
they	O
either	O
are	O
a	O
1d	O
array	O
of	O
values	O
length	O
n	O
,	O
or	O
are	O
a	O
single	O
value	O
,	O
how	O
do	O
i	O
loop	O
through	O
them	O
so	O
that	O
I	O
get	O
n	O
values	O
returned	O
.	O

For	O
each	O
point	O
in	O
array	O
A	O
,	O
I	O
need	O
to	O
find	O
how	O
many	O
points	O
in	O
array	O
B	O
are	O
within	O
a	O
certain	O
distance	O
of	O
it	O
.	O

It	O
does	O
,	O
but	O
somehow	O
it	O
is	O
8	O
times	O
slower	O
than	O
copying	O
to	O
numpy	O
array	O
:(	O
I	O
suppose	O
the	O
regular	O
python	O
overhead	O
slows	O
things	O
down	O
much	O
more	O
than	O
a	O
copy	O
...	O

It	O
all	O
depends	O
on	O
its	O
dependencies	O
.	O

Is	O
there	O
a	O
way	O
to	O
make	O
an	O
array	O
of	O
such	O
strings	O
?	O

`	O
grid	O
[	O
1	O
]`	O
can	O
be	O
used	O
as	O
a	O
proxy	O
for	O
the	O
index	O
`	O
j	O
`	O
.	O

After	O
doing	O
so	O
,	O
I	O
discovered	O
that	O
if	O
I	O
tried	O
to	O
open	O
the	O
IPython	O
HTML	O
Notebook	O
I	O
got	O
the	O
error	O
message	O
:	O
#CODE	O

(	O
the	O
new	O
matrix	O
would	O
have	O
n-2	O
rows	O
m-2	O
columns	O
)	O
.	O

and	O
duplicate	O
index	O
values	O
at	O
the	O
correpsonding	O
sites	O
within	O

I	O
found	O
that	O
the	O
best	O
way	O
to	O
produce	O
small	O
pdf	O
files	O
is	O
to	O
save	O
as	O
eps	O
in	O
matplotlib	O
and	O
then	O
use	O
epstopdf	O
.	O

You	O
could	O
rearrange	O
the	O
image	O
to	O
put	O
the	O
(	O
0	O
,	O
0	O
)	O
in	O
the	O
middle	O
with	O
some	O
matrix	O
manipulation	O
.	O

Please	O
,	O
see	O
the	O
next	O
example	O
:	O

A	O
function	O
that	O
broadcasts	O
a	O
scalar	O
operation	O
over	O
an	O
array	O
is	O
called	O
a	O
universal	O
function	O
,	O
or	O
ufunc	O
.	O

may	O
not	O
exist	O
until	O
the	O
datasets	O
get	O
quite	O
big	O
(	O
maybe	O
you'll	O
need	O
at	O
least	O
10,000	O
rows	O
per	O
data	O
set	O
)	O
.	O

Magic	O
answers	O
like	O
this	O
are	O
not	O
really	O
helpful	O
because	O
they	O
don't	O
solve	O
the	O
problem	O
.	O

I	O
think	O
what	O
I	O
was	O
missing	O
is	O
that	O
I	O
really	O
have	O
a	O
3	O
dimensional	O
array	O
,	O
48x365x3	O
.	O

I	O
load	O
a	O
some	O
machine	O
learning	O
data	O
from	O
a	O
csv	O
file	O
.	O

So	O
I	O
have	O
it	O
running	O
(	O
or	O
at	O
least	O
that	O
assignment	O
isn't	O
throwing	O
an	O
error	O
and	O
it's	O
compiling	O
)	O
!	O

@USER	O
The	O
first	O
function	O
is	O
taking	O
chunks	O
of	O
200	O
items	O
from	O
your	O
huge	O
array	O
,	O
and	O
copying	O
those	O
chunks	O
to	O
a	O
new	O
,	O
even	O
more	O
ginormous	O
array	O
.	O

@USER	O
:	O
With	O
`	O
where	O
`	O
it	O
looks	O
definitely	O
nice	O
,	O
but	O
have	O
you	O
consider	O
also	O
the	O
implications	O
to	O
performance	O
when	O
implementing	O
with	O
`	O
where	O
`	O
?	O

Anyone	O
any	O
idea	O
what	O
this	O
means	O
?!	O

Assuming	O
you	O
are	O
using	O
g++	O
to	O
compile	O
...	O
have	O
you	O
had	O
different	O
results	O
in	O
any	O
way	O
when	O
experimenting	O
with	O
compiler	O
optimization	O
flags	O
?	O

With	O
the	O
overhead	O
of	O
the	O
data	O
structure	O
you	O
could	O
be	O
looking	O
at	O
usage	O
much	O
higher	O
than	O
that	O
--	O
I	O
can't	O
say	O
how	O
much	O
because	O
I	O
don't	O
know	O
the	O
memory	O
model	O
behind	O
SciPy	O
/	O
numpy	O
.	O

I	O
have	O
serious	O
doubt	O
that	O
adding	O
two	O
numpy	O
arrays	O
is	O
a	O
bottleneck	O
that	O
you	O
can	O
solve	O
rewriting	O
things	O
in	O
C	O
.	O

Where	O
exactly	O
is	O
the	O
error	O
occurring	O
?	O

I	O
frequently	O
convert	O
16-bit	O
grayscale	O
image	O
data	O
to	O
8-b	O
it	O
image	O
data	O
for	O
display	O
.	O

Reduce	O
it	O
to	O
a	O
1	O
/	O
10	O
resolution	O
,	O
find	O
the	O
one	O
white	O
pixel	O
,	O
and	O
then	O
you	O
have	O
a	O
precise	O
idea	O
of	O
where	O
to	O
search	O
for	O
the	O
centroid	O
.	O

I	O
ran	O
a	O
test	O
to	O
compare	O
the	O
times	O
,	O
and	O
found	O
that	O
my	O
method	O
is	O
faster	O
by	O
quite	O
a	O
bit	O
,	O
but	O
Freddie	O
Witherdon	O
'	O
s	O
suggestion	O
is	O
even	O
faster	O
.	O

I	O
couldn't	O
find	O
it	O
in	O
the	O
OLS	O
recipe	O
(	O
#URL	O
)	O
.	O

convert	O
binary	O
string	O
to	O
numpy	O
array	O

How	O
to	O
know	O
where	O
warning	O
come	O
from	O
in	O
Python	O
