How	O
do	O
I	O
add	O
a	O
title	O
to	O
my	O
MatPlotLib	O
basemap	O
?	O

It	O
might	O
help	O
issue	O
of	O
things	O
refusing	O
to	O
update	O
,	O
and	O
at	O
the	O
least	O
it	O
will	O
be	O
faster	O
.	O

For	O
future	O
use	O
you	O
should	O
really	O
find	O
out	O
why	O
it	O
is	O
installed	O
in	O
a	O
location	O
that	O
isn't	O
searched	O
by	O
python	O
by	O
default	O
.	O

I've	O
done	O
that	O
and	O
I'm	O
still	O
getting	O
the	O
same	O
error	O
-	O
it	O
can't	O
find	O
the	O
sip	O
module	O
.	O

ImportError	O
:	O
DLL	O
load	O
failed	O
:	O
The	O
specified	O
module	O
could	O
not	O
be	O
found	O
.	O

This	O
has	O
happened	O
previously	O
(	O
and	O
I	O
was	O
not	O
able	O
to	O
get	O
around	O
it	O
)	O
.	O

@USER	O
SObolev	O
:	O
Histograms	O
of	O
the	O
values	O
falling	O
within	O
a	O
range	O
of	O
keys	O
where	O
each	O
bin	O
is	O
of	O
size	O
0.1	O

Also	O
,	O
now	O
that	O
I	O
look	O
at	O
it	O
,	O
the	O
y-axis	O
limits	O
haven't	O
been	O
set	O
right	O
.	O

I've	O
looked	O
for	O
various	O
solutions	O
to	O
my	O
problem	O
but	O
can't	O
get	O
any	O
help	O
.	O

I	O
normally	O
use	O
something	O
akin	O
to	O
the	O
following	O
for	O
this	O
purpose	O
and	O
the	O
result	O
is	O
a	O
very	O
nice	O
plot	O
where	O
x1	O
,	O
x2	O
and	O
x3	O
are	O
defined	O
as	O
follows	O

And	O
I	O
have	O
all	O
of	O
them	O
installed	O
.	O

When	O
I	O
save	O
these	O
plots	O
to	O
pdf	O
,	O
it	O
takes	O
a	O
long	O
time	O
to	O
write	O
,	O
and	O
reading	O
the	O
pdf	O
is	O
even	O
worse	O
.	O

Seems	O
like	O
a	O
security	O
setting	O
on	O
OS	O
X	O
is	O
preventing	O
it	O
,	O
and	O
the	O
quick	O
fix	O
seems	O
to	O
be	O
to	O
run	O
apache	O
as	O
root	O
.	O

Any	O
ideas	O
how	O
to	O
deal	O
with	O
this	O
problem	O
?	O

If	O
I	O
remove	O
the	O
function	O
and	O
put	O
its	O
body	O
back	O
under	O
the	O
"	O
if	O
name	O
==	O
'	O
main	O
'"	O
block	O
,	O
it	O
works	O
as	O
expected	O
(	O
like	O
in	O
the	O
original	O
code	O
)	O
.	O

Seems	O
like	O
a	O
bit	O
of	O
searching	O
would	O
have	O
turned	O
up	O
[	O
how	O
to	O
set	O
custom	O
ticks	O
]	O
(	O
#URL	O
)	O
,	O
where	O
the	O
defined	O
`	O
t11	O
`	O
is	O
where	O
you	O
would	O
write	O
out	O
A	O
,	O
B	O
,	O
C	O
,	O
and	O
D	O
.	O

@USER	O
,	O
that	O
would	O
require	O
to	O
have	O
root	O
access	O
(	O
sudo	O
)	O
which	O
i	O
dont	O
have	O
!	O

Any	O
suggestions	O
how	O
this	O
can	O
work	O
?	O

Any	O
help	O
would	O
be	O
appreciated	O
in	O
understanding	O
what	O
I	O
am	O
missing	O
.	O

How	O
can	O
I	O
add	O
the	O
`	O
P_g	O
`	O
values	O
from	O
each	O
section	O
to	O
their	O
respective	O
column	O
?	O

Is	O
there	O
any	O
other	O
python	O
package	O
that	O
would	O
help	O
?	O

I'm	O
having	O
a	O
problem	O
,	O
where	O
I	O
can't	O
do	O
a	O
step	O
graph	O
of	O
2	O
lists	O
I	O
have	O
,	O
in	O
which	O
I	O
need	O
list	O
`	O
x	O
`	O
to	O
be	O
the	O
x	O
values	O
,	O
in	O
which	O
each	O
`	O
x	O
[	O
j	O
]`	O
value	O
will	O
add	O
with	O
`	O
x	O
[	O
j+1	O
]`	O
value	O
for	O
each	O
step	O
.	O

The	O
best	O
thing	O
would	O
we	O
if	O
one	O
could	O
integrate	O
it	O
into	O
the	O
label	O
of	O
the	O
colorbar	O
(	O
to	O
say	O
the	O
color	O
means	O
A	O
in	O
units	O
of	O
B	O
where	O
B	O
includes	O
the	O
order	O
of	O
magnitude	O
)	O
.	O

ive	O
tried	O
to	O
fix	O
it	O
with	O
setting	O
ax	O
limits	O
but	O
that	O
does	O
not	O
work	O
.	O

@USER	O
:	O
I	O
added	O
some	O
more	O
info	O
and	O
also	O
a	O
code	O
used	O
.	O

If	O
I	O
wanted	O
to	O
make	O
a	O
combined	O
image	O
like	O
the	O
one	O
shown	O
below	O
(	O
original	O
source	O
here	O
)	O
,	O

When	O
you	O
**	O
add	O
**	O
two	O
**	O
random	O
variables**	O
,	O
the	O
resulting	O
density	O
is	O
the	O
**	O
convolution	O
**	O
of	O
their	O
**	O
densities**	O
.	O

This	O
made	O
the	O
program	O
work	O
just	O
as	O
in	O
Windows	O
,	O
without	O
any	O
other	O
modifications	O
necessary	O
.	O

Using	O
PySide	O
in	O
place	O
of	O
PyQt4	O
results	O
in	O
the	O
same	O
behavior	O
.	O

However	O
,	O
the	O
first	O
item	O
appears	O
to	O
be	O
a	O
float	O
#CODE	O

The	O
code	O
below	O
loads	O
the	O
image	O
data	O
hosted	O
on	O
github	O
and	O
plots	O
it	O
:	O
#CODE	O

I	O
reduce	O
the	O
values	O
to	O
months	O
using	O
:	O
#CODE	O

That	O
would	O
solve	O
Question	O
1	O
.	O

I	O
modified	O
you	O
code	O
,	O
and	O
it	O
can	O
product	O
the	O
same	O
ticks	O
now	O
.	O

`'	O
$\xi$	O
(	O
r	O
)	O
M$_{200}$	O
13.4	O
'`	O
:	O
several	O
`	O
$	O
`	O
,	O
where	O
you	O
should	O
have	O
only	O
one	O
.	O

I	O
just	O
completed	O
a	O
brute	O
force	O
test	O
running	O
through	O
all	O
permutations	O
of	O
[	O
0	O
,	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
,	O
5	O
]	O
for	O
the	O
tmp_planes	O
,	O
a	O
total	O
of	O
720	O
different	O
arrangements	O
.	O

The	O
way	O
to	O
fix	O
it	O
is	O
using	O
pip	O
,	O
as	O
mentioned	O
by	O
@USER	O
.	O

Do	O
you	O
mean	O
a	O
scatterplot	O
of	O
your	O
dataset	O
instead	O
?	O

Plotting	O
subplots	O
with	O
secondary	O
continuous	O
y-axis	O
across	O
all	O
subplots	O

However	O
,	O
I	O
want	O
to	O
be	O
able	O
to	O
call	O
something	O
or	O
write	O
some	O
code	O
which	O
then	O
skips	O
any	O
value	O
in	O
the	O
array	O
which	O
would	O
cause	O
this	O
error	O
,	O
then	O
ignoring	O
that	O
row	O
altogether	O
.	O

Any	O
help	O
would	O
be	O
appreciated	O
.	O

You	O
are	O
typing	O
your	O
code	O
in	O
at	O
the	O
bash	O
(	O
terminal	O
)	O
prompt	O
,	O
not	O
the	O
Python	O
interpreter	O
prompt	O
.	O

Also	O
,	O
if	O
you	O
take	O
a	O
look	O
at	O
Rob's	O
answer	O
,	O
its	O
far	O
simpler	O
than	O
the	O
example	O
shown	O
on	O
the	O
website	O
.	O

MPL	O
is	O
a	O
big	O
project	O
with	O
lots	O
of	O
moving	O
parts	O
,	O
any	O
help	O
keep	O
them	O
up	O
is	O
appreciated	O
.	O

I	O
am	O
less	O
certain	O
if	O
I	O
am	O
solving	O
the	O
correct	O
problem	O
.	O

I	O
am	O
having	O
a	O
problem	O
getting	O
matplotlib	O
to	O
work	O
well	O
with	O
interactive	O
plotting	O
...	O
what	O
I	O
see	O
is	O
that	O
after	O
displaying	O
a	O
few	O
frames	O
of	O
my	O
simulated	O
data	O
matplotlib	O
hangs-and	O
doesn't	O
display	O
any	O
more	O
.	O

If	O
anyone	O
has	O
any	O
idea	O
,	O
I	O
appreciate	O
the	O
help	O
.	O

You	O
can	O
populate	O
a	O
general	O
2d	O
array	O
by	O
:	O
Zhu	O
shows	O
in	O
the	O
answer	O
below	O
.	O

Have	O
a	O
look	O
at	O
this	O
example	O
:	O
#CODE	O

That	O
way	O
,	O
it	O
is	O
possible	O
to	O
see	O
2	O
dots	O
on	O
the	O
same	O
place	O
.	O

This	O
error	O
shows	O
up	O
when	O
the	O
size	O
of	O
a	O
and	O
b	O
(	O
taking	O
from	O
above	O
example	O
)	O
is	O
not	O
the	O
same	O
-	O
so	O
,	O
128	O
x-values	O
here	O
should	O
be	O
plotted	O
against	O
128	O
y-values	O
.	O

[	O
These	O
should	O
all	O
be	O
installed	O
into	O
/	O
usr	O
/	O
local	O
/	O
Cellar	O
,	O
which	O
is	O
the	O
default	O
install	O
location	O
for	O
homebrew	O
.	O
]	O

I'm	O
trying	O
to	O
make	O
an	O
exponential	O
fit	O
to	O
find	O
the	O
Lyapunov	O
Exponent	O
of	O
this	O
data	O
,	O
however	O
,	O
I	O
keep	O
getting	O
this	O
error	O
:	O
#CODE	O

So	O
I	O
wrote	O
an	O
argument	O
`	O
origin=	O
'	O
lower	O
'`	O
.	O

Note	O
that	O
I	O
don't	O
reduce	O
to	O
2	O
,	O
I	O
reduce	O
to	O
10000	O
,	O
and	O
then	O
extract	O
the	O
first	O
2	O
.	O

which	O
you	O
can	O
run	O
with	O
this	O
sample	O
code	O
:	O
#CODE	O

Try	O
converting	O
your	O
the	O
pandas	O
data	O
series	O
to	O
lists	O
or	O
numpy	O
array	O
before	O
plotting	O
.	O

If	O
I	O
de-homogenize	O
only	O
the	O
last	O
column	O
I	O
can	O
find	O
the	O
center	O
of	O
where	O
the	O
ellipse	O
was	O
projected	O
,	O
but	O
I	O
would	O
like	O
to	O
see	O
some	O
shape	O
information	O
as	O
well	O
.	O

Ignoring	O
a	O
problem	O
doesn't	O
fix	O
it	O
.	O

Will	O
add	O
the	O
code	O
.	O

I	O
found	O
that	O
in	O
the	O
folder	O
C	O
:\	O
Program	O
Files\Python27\DLLs	O
I	O
cann't	O
find	O
the	O
_socket	O
file	O

The	O
index	O
is	O
zero-based	O
.	O

The	O
image	O
ggd4	O
for	O
the	O
test	O
can	O
be	O
downloaded	O
from	O
:	O

Load	O
the	O
data	O
sets	O
individually	O
,	O
and	O
then	O
plot	O
each	O
one	O
individually	O
.	O

Have	O
you	O
looked	O
at	O
mplot3d	O
on	O
matplotlib	O
?	O

The	O
columns	O
are	O
floats	O
,	O
I'm	O
not	O
using	O
the	O
index	O
of	O
integers	O
.	O

Incidentally	O
(	O
for	O
someone	O
reading	O
this	O
in	O
the	O
future	O
as	O
you	O
know	O
yours	O
)	O
if	O
you	O
need	O
to	O
find	O
your	O
current	O
Python	O
version	O
you	O
can	O
simply	O
type	O
`	O
python	O
-V	O
`	O
in	O
the	O
command	O
line	O
and	O
it'll	O
return	O
the	O
details	O
,	O
for	O
example	O
mine	O
returns	O
:	O

The	O
code	O
is	O
located	O
at	O

find	O
the	O
file	O
from	O
the	O
link	O

@USER	O
:	O
What	O
do	O
you	O
mean	O
?	O

Thank	O
you	O
in	O
advance	O
for	O
any	O
help	O
you	O
can	O
give	O
me	O

how	O
i	O
can	O
add	O
vector	O
arrows	O
starting	O
from	O
0	O
,	O
0	O
and	O
end	O
at	O
the	O
points	O
that	O
i	O
want	O
?	O

If	O
you	O
want	O
the	O
sum	O
(	O
not	O
the	O
integral	O
)	O
to	O
be	O
one	O
#CODE	O

As	O
I	O
have	O
no	O
idea	O
how	O
long	O
`	O
data_all	O
`	O
is	O
,	O
I	O
created	O
a	O
(	O
random	O
)	O
array	O
,	O
which	O
in	O
my	O
case	O
turned	O
out	O
to	O
give	O
a	O
2D	O
array	O
with	O
512	O
columns	O
(	O
lucky	O
shot	O
)	O
.	O

Do	O
you	O
mean	O
something	O
like	O
this	O
:	O

Can	O
you	O
copy	O
and	O
paste	O
the	O
code	O
lines	O
described	O
in	O
your	O
comment	O
,	O
please	O
?	O

When	O
I	O
print	O
this	O
in	O
A3	O
(	O
good	O
size	O
)	O
it	O
still	O
almost	O
unreadable	O
because	O
the	O
letters	O
are	O
stacking	O
in	O
each	O
other	O
.	O

I	O
mean	O
my	O
grid	O
is	O
20	O
units	O
in	O
x	O
direction	O
however	O
,	O
my	O
resulting	O
plot	O
is	O
10	O
units	O
in	O
x	O
direction	O
with	O
replica	O
plot	O
from	O
10	O
to	O
20	O
units	O
.	O

Now	O
how	O
to	O
plot	O
[	O
M	O
,	O
F	O
+	O
other	O
]	O

To	O
specify	O
that	O
you	O
want	O
the	O
first	O
interval	O
filled	O
with	O
"	O
pure	O
"	O
black	O
,	O
set	O
`	O
vmin=mean	O
(	O
lvls	O
[:	O
1	O
])`	O
in	O
your	O
example	O
.	O

Your	O
Data	O
looks	O
pretty	O
good	O
without	O
any	O
filtering	O
,	O
your	O
"	O
outlieres	O
"	O
all	O
have	O
a	O
very	O
big	O
error	O
and	O
won't	O
affect	O
the	O
fit	O
much	O
.	O

This	O
is	O
a	O
terribly	O
unclear	O
answer	O
,	O
at	O
least	O
some	O
comment	O
in	O
the	O
code	O
or	O
explanation	O
of	O
why	O
this	O
helps	O
floating	O
point	O
error	O
would	O
be	O
nice	O
.	O

I	O
produce	O
a	O
histogram	O
where	O
I	O
put	O
the	O
weights	O
in	O
the	O
parameter	O
#CODE	O

Besides	O
,	O
I	O
see	O
that	O
you	O
use	O
`	O
numpy	O
`	O
in	O
your	O
code	O
,	O
so	O
you	O
can	O
save	O
and	O
load	O
file	O
by	O
`	O
numpy	O
`	O
with	O
a	O
more	O
compact	O
codes	O
like	O
this	O
without	O
using	O
the	O
`	O
csv	O
`	O
module	O
.	O

Usually	O
these	O
sorts	O
of	O
simple	O
array	O
reshaping	O
operations	O
can	O
be	O
done	O
in	O
a	O
couple	O
of	O
lines	O
with	O
Numpy	O
,	O
so	O
I	O
feel	O
like	O
I'm	O
missing	O
something	O
.	O

I	O
don't	O
see	O
any	O
way	O
I	O
could	O
do	O
that	O
.	O

Look	O
at	O
the	O
docs	O
:	O
#CODE	O

To	O
fix	O
that	O
we	O
turn	O
the	O
frame	O
off	O
on	O
`	O
ax1	O
`	O
(	O
so	O
we	O
can	O
see	O
`	O
ax2	O
`)	O
.	O

At	O
one	O
point	O
I	O
was	O
able	O
to	O
trick	O
it	O
into	O
working	O
from	O
the	O
console	O
by	O
installing	O
some	O
thing	O
in	O
the	O
virtualenv	O
,	O
but	O
other	O
things	O
only	O
in	O
the	O
global	O
namespace	O
,	O
but	O
I	O
forgot	O
how	O
I	O
did	O
it	O
.	O

(	O
Technically	O
we	O
could	O
skip	O
the	O
`	O
FigureCanvas	O
`	O
,	O
but	O
it	O
will	O
be	O
needed	O
as	O
soon	O
as	O
we	O
want	O
to	O
save	O
the	O
plot	O
to	O
an	O
image	O
,	O
etc	O
.	O
)	O
#CODE	O

But	O
how	O
can	O
I	O
make	O
plt	O
do	O
all	O
this	O
stuff	O
?	O

From	O
your	O
piece	O
of	O
code	O
it	O
seems	O
to	O
me	O
that	O
`	O
clust_data	O
`	O
is	O
already	O
a	O
list	O
of	O
lists	O
with	O
the	O
correct	O
shape	O
and	O
that	O
`	O
cellText	O
`	O
after	O
being	O
filled	O
is	O
going	O
to	O
be	O
the	O
same	O
of	O
`	O
clust_data	O
`	O
.	O

Anaconda	O
doesn't	O
by	O
default	O
load	O
pylab	O
into	O
IPython	O
so	O
you	O
can	O
choose	O
the	O
backend	O
after	O
launching	O
IPython	O
.	O

Most	O
likely	O
I'll	O
have	O
graphs	O
with	O
longer	O
labels	O
and	O
if	O
I	O
put	O
them	O
near	O
lines	O
,	O
it'll	O
look	O
messy	O
.	O

The	O
lines	O
are	O
random	O
within	O
ranges	O
that	O
cause	O
clustering	O
of	O
lines	O
;	O
a	O
behavior	O
I	O
wanted	O
to	O
verify	O
.	O

have	O
those	O
lists	O
all	O
the	O
same	O
length	O
aka	O
would	O
it	O
be	O
directly	O
compatible	O
witha	O
2d	O
numpy	O
array	O
?	O

Any	O
assistance	O
will	O
be	O
appreciated	O
.	O

I	O
have	O
another	O
working	O
project	O
where	O
the	O
toolbar	O
is	O
in	O
the	O
same	O
panel	O
as	O
the	O
canvas	O
,	O
and	O
I	O
think	O
that	O
might	O
be	O
the	O
key	O
difference	O
between	O
the	O
two	O
.	O

any	O
of	O
#CODE	O

Which	O
way	O
is	O
the	O
easiest	O
to	O
load	O
all	O
txt	O
files	O
?	O

(	O
Note	O
that	O
F	O
is	O
not	O
given	O
in	O
the	O
data	O
,	O
but	O
you	O
gave	O
it	O
as	O
an	O
example	O
)	O

Now	O
I	O
need	O
to	O
put	O
this	O
plot	O
in	O
a	O
PyQt	O
form	O
.	O

Where	O
is	O
the	O
URL	O
API	O
documented	O
?	O

Are	O
there	O
any	O
other	O
option	O
?	O

You	O
need	O
some	O
criteria	O
to	O
select	O
out	O
the	O
"	O
best	O
"	O
in	O
your	O
context	O
.	O

What	O
do	O
you	O
mean	O
when	O
you	O
say	O
`	O
the	O
first	O
histogram	O
will	O
be	O
the	O
first	O
column	O
as	O
the	O
x	O
values	O
plotted	O
against	O
the	O
second	O
column	O
as	O
the	O
y-values	O
`	O
.	O

Unfortunately	O
I	O
don't	O
have	O
any	O
experience	O
doing	O
this	O
so	O
won't	O
be	O
able	O
to	O
help	O
.	O

A	O
sample	O
of	O
my	O
data	O
to	O
plot	O
:	O
#CODE	O

Also	O
,	O
I	O
do	O
not	O
want	O
user	O
intermediation	O
at	O
all	O
!	O

Rather	O
than	O
put	O
together	O
an	O
example	O
from	O
scratch	O
,	O
there's	O
an	O
excellent	O
example	O
of	O
this	O
written	O
by	O
Paul	O
Ivanov	O
in	O
the	O
matplotlib	O
examples	O
(	O
It's	O
only	O
in	O
the	O
current	O
git	O
tip	O
,	O
as	O
it	O
was	O
only	O
committed	O
a	O
few	O
months	O
ago	O
.	O
It's	O
not	O
on	O
the	O
webpage	O
yet	O
.	O
)	O
.	O

what	O
to	O
do	O
next	O
?	O

I	O
think	O
you'll	O
find	O
you	O
get	O
better	O
help	O
if	O
you	O
describe	O
*	O
what	O
*	O
you're	O
trying	O
to	O
do	O
,	O
rather	O
than	O
*	O
how	O
*	O
you	O
want	O
to	O
do	O
it	O
.	O

(	O
ie	O
save	O
etc	O
)	O
.	O
ill	O
have	O
a	O
look	O
at	O
the	O
toolkits	O
:D	O

in	O
place	O
of	O
#CODE	O

You	O
could	O
cast	O
the	O
array	O
to	O
a	O
list	O
:	O
#CODE	O

Add	O
as	O
many	O
symlinks	O
to	O
the	O
directory	O
as	O
you	O
wish	O
,	O
it	O
gives	O
you	O
fine-grained	O
control	O
over	O
which	O
packages	O
you	O
want	O
to	O
make	O
accessible	O
.	O

In	O
the	O
end	O
,	O
I	O
found	O
out	O
that	O
the	O
problem	O
was	O
simply	O
that	O
matplotlib	O
could	O
not	O
find	O
any	O
backend	O
for	O
plotting	O
.	O

Please	O
change	O
your	O
title	O
to	O
describe	O
your	O
_actual_	O
issue	O
.	O

If	O
you	O
did	O
not	O
create	O
one	O
when	O
you	O
installed	O
matplotlib	O
,	O
you	O
can	O
get	O
this	O
template	O
from	O
the	O
matplotlib	O
source	O
,	O
or	O
from	O
the	O
matplotlib	O
website	O
.	O

I	O
am	O
sorry	O
to	O
be	O
so	O
needy	O
,	O
but	O
I	O
really	O
cant	O
find	O
any	O
useful	O
information	O
anywhere	O
.	O

Or	O
is	O
there	O
another	O
way	O
to	O
change	O
the	O
size	O
?	O

I	O
did	O
find	O
the	O
way	O
to	O
make	O
my	O
images	O
almost	O
what	O
I	O
wanted	O
with	O
:	O
#CODE	O

any	O
idea	O
?	O

@USER	O
:	O
I	O
suppose	O
the	O
specific	O
way	O
I	O
have	O
it	O
set	O
up	O
would	O
work	O
better	O
if	O
you	O
had	O
,	O
say	O
,	O
a	O
1000x1000	O
matrix	O
,	O
as	O
it	O
lets	O
numpy	O
pick	O
how	O
the	O
tics	O
should	O
be	O
spaced	O
.	O

As	O
you	O
can	O
see	O
it	O
only	O
covers	O
half	O
of	O
the	O
matrix	O
.	O

You	O
iterate	O
the	O
items	O
in	O
`	O
glass	O
`	O
,	O
but	O
then	O
compare	O
to	O
the	O
whole	O
list	O
instead	O
of	O
to	O
the	O
current	O
`	O
item	O
`	O

If	O
that	O
helps	O
,	O
this	O
is	O
the	O
process	O
that	O
I	O
am	O
doing	O
to	O
save	O
and	O
plot	O
:	O
#CODE	O

Please	O
consider	O
answering	O
your	O
own	O
question	O
or	O
removing	O
it	O
all	O
together	O
if	O
you	O
feel	O
it	O
does	O
not	O
add	O
.	O

Now	O
"	O
print	O
zi	O
"	O
gives	O
me	O
an	O
array	O
like	O
before	O
but	O
with	O
nan	O
everywhere	O
.	O

Im	O
not	O
by	O
computer	O
to	O
test	O
.	O

I	O
want	O
to	O
put	O
my	O
custom	O
datetime	O
format	O
but	O
when	O
I	O
tried	O
my	O
custom	O
date	O
format	O
is	O
overlapped	O
over	O
the	O
date	O
given	O
by	O
default	O
by	O
pandas	O
plot	O
.	O

Change	O
formatting	O
on	O
datetime	O
ticks	O
when	O
plotting	O
daily	O
mean	O
with	O
Pandas	O
/	O
matplotlib	O

Any	O
other	O
way	O
to	O
get	O
a	O
similar	O
effect	O
?	O

If	O
you	O
see	O
the	O
right	O
side	O
,	O
the	O
size	O
is	O
changed	O
.	O

How	O
can	O
I	O
select	O
multiple	O
data	O
points	O
with	O
matplotlib	O
and	O
export	O
it	O
?	O

Two	O
steps	O
to	O
sketch	O
an	O
idea	O
to	O
solve	O
this	O
:	O

This	O
is	O
not	O
necessarily	O
the	O
nicest	O
possible	O
way	O
of	O
doing	O
things	O
,	O
and	O
you	O
may	O
bump	O
into	O
resolution	O
problems	O
,	O
as	O
well	O
(	O
check	O
the	O
size	O
of	O
the	O
`	O
mayavi	O
`	O
window	O
)	O
.	O

Is	O
there	O
a	O
way	O
I	O
can	O
get	O
y=x	O
to	O
show	O
up	O
even	O
if	O
I	O
don't	O
have	O
a	O
list	O
of	O
all	O
the	O
points	O
that	O
I	O
plotted	O
?	O

Knowing	O
all	O
that	O
,	O
your	O
example	O
becomes	O
:	O
#CODE	O

Any	O
Ideas	O
what	O
might	O
be	O
the	O
problem	O
?	O

It	O
appear	O
that	O
these	O
disappear	O
when	O
I	O
add	O
the	O
`	O
facecolors=UWR	O
(	O
heatmap	O
)`	O
property	O
to	O
the	O
`	O
surf	O
(	O
...	O
)`	O
.	O

Can	O
you	O
post	O
the	O
full	O
traceback	O
?	O

The	O
full	O
list	O
of	O
positions	O
is	O
given	O
here	O
:	O
#URL	O

unutbu's	O
answer	O
should	O
fix	O
it	O
everywhere	O
.	O

how	O
can	O
i	O
insert	O
the	O
plot	O
returned	O
by	O
create_predef_plot	O
into	O
f2	O
#CODE	O

And	O
the	O
formatted	O
ones	O
(	O
Used	O
as	O
_id	O
of	O
the	O
documents	O
after	O
using	O
aggregation	O
):	O
#CODE	O

Any	O
packages	O
you	O
accidentally	O
installed	O
for	O
other	O
Python	O
installations	O
,	O
just	O
reinstall	O
them	O
for	O
MacPorts	O
.	O

However	O
,	O
other	O
programs	O
seem	O
to	O
be	O
ok	O
with	O
filling	O
almost	O
all	O
the	O
memory	O
before	O
the	O
swap	O
is	O
used	O
.	O

I'm	O
not	O
sure	O
how	O
to	O
tackle	O
this	O
problem	O
and	O
any	O
help	O
or	O
direction	O
would	O
be	O
very	O
appreciated	O
.	O

i	O
looked	O
at	O
sage	O
,	O
the	O
source	O
code	O
is	O
huge	O
,	O
does	O
it	O
support	O
python	O
out	O
of	O
the	O
box	O
?	O

If	O
any	O
one	O
know	O
about	O
it	O
then	O
please	O
reply	O
me	O
.	O

I'll	O
have	O
a	O
look	O
at	O
it	O

The	O
main	O
issue	O
is	O
that	O
you	O
can't	O
make	O
UI	O
changes	O
from	O
a	O
separate	O
process	O
from	O
the	O
main	O
UI	O
thread	O
(	O
the	O
one	O
that	O
all	O
of	O
your	O
Qt	O
calls	O
are	O
in	O
)	O
.	O

I	O
say	O
"	O
quick	O
fix	O
"	O
because	O
there	O
are	O
more	O
changes	O
that	O
could	O
be	O
recommended	O
.	O

Maybe	O
look	O
at	O
the	O
scikit-learn	O
docs	O
for	O
inspiration	O
?	O

Labels	O
are	O
located	O
at	O
the	O
right	O
place	O
,	O
but	O
label	O
is	O
not	O
correct	O
.	O

I	O
ran	O
the	O
following	O
code	O
to	O
get	O
two	O
plots	O
next	O
to	O
each	O
other	O
(	O
it	O
is	O
a	O
minimal	O
working	O
example	O
that	O
you	O
can	O
copy	O
):	O
#CODE	O

here	O
is	O
the	O
code	O
where	O
I	O
create	O
the	O
plot	O
and	O
a	O
picture	O
of	O
this	O
:	O

Admittedly	O
,	O
I'm	O
new	O
to	O
pandas	O
but	O
it	O
seems	O
like	O
at	O
every	O
turn	O
I	O
am	O
stopped	O
,	O
by	O
my	O
ignorance	O
no	O
doubt	O
,	O
but	O
it's	O
getting	O
tiresome	O
.	O

Are	O
all	O
of	O
your	O
modes	O
needed	O
to	O
demonstrate	O
the	O
problem	O
?	O

Note	O
that	O
the	O
explicit	O
masking	O
is	O
no	O
longer	O
necessary	O
in	O
matplotlib	O
master	O
as	O
arrays	O
are	O
now	O
masked	O
automatically	O
internally	O
.	O

(	O
The	O
disadvantage	O
is	O
that	O
this	O
method	O
is	O
much	O
less	O
flexible	O
.	O
):	O
#CODE	O

Python	O
&	O
Matplotlib	O
:	O
creating	O
a	O
chart	O
first	O
and	O
showing	O
it	O
at	O
a	O
second	O
moment	O

Put	O
#CODE	O

If	O
you	O
require	O
a	O
completely	O
manual	O
installation	O
,	O
you	O
should	O
give	O
those	O
instructions	O
a	O
thorough	O
read	O
.	O

Take	O
a	O
look	O
at	O
this	O
example	O
.	O

I	O
tried	O
something	O
like	O
that	O
already	O
,	O
but	O
when	O
I	O
plot	O
each	O
data	O
point	O
individually	O
in	O
a	O
for-loop	O
,	O
I	O
see	O
no	O
way	O
to	O
use	O
a	O
colormap	O
and	O
colorbar	O
for	O
all	O
of	O
them	O
.	O

Here's	O
how	O
to	O
do	O
it	O
:	O
if	O
you	O
find	O
that	O
an	O
answer	O
has	O
been	O
helpful	O
to	O
you	O
,	O
then	O
move	O
your	O
mouse	O
over	O
the	O
upper	O
left-hand	O
corner	O
of	O
that	O
answer	O
(	O
where	O
the	O
answer	O
"	O
score	O
"	O
appears	O
between	O
two	O
gray	O
triangles	O
)	O
.	O

Can	O
you	O
recommend	O
any	O
tutorial	O
for	O
MATLAB	O
speakers	O
?	O

The	O
second	O
plot	O
keeps	O
rescaling	O
the	O
x-axis	O
for	O
me	O
,	O
losing	O
the	O
overview	O
of	O
the	O
full	O
column	O
1	O
plot	O
.	O

Thanks	O
for	O
the	O
answer	O
,	O
I	O
searched	O
for	O
event	O
handling	O
examples	O
but	O
could	O
not	O
find	O
it	O
(	O
I	O
am	O
newbie	O
to	O
python	O
)	O
,	O
can	O
you	O
please	O
send	O
me	O
link	O
to	O
at	O
least	O
one	O
such	O
example	O
.	O

You	O
can	O
find	O
it	O
here	O
.	O

(	O
I	O
know	O
I	O
add	O
nothing	O
new	O
,	O
but	O
the	O
straightforward	O
answer	O
should	O
be	O
visible	O
)	O
.	O

So	O
,	O
I	O
don't	O
even	O
know	O
where	O
to	O
start	O
looking	O

When	O
I	O
try	O
to	O
run	O
the	O
example	O
script	O
at	O

Thanks	O
for	O
your	O
comment	O
,	O
I	O
have	O
put	O
everything	O
,	O
I	O
wanna	O
plot	O
(	O
years	O
,	O
Frequency_accidents_years	O
,	O
regions	O
)	O

Via	O
`	O
apt-get	O
`	O
,	O
I've	O
tried	O
installing	O
ffmpeg	O
,	O
every	O
codec	O
imaginable	O
,	O
and	O
even	O
tried	O
to	O
compile	O
ffmpeg	O
from	O
source	O
.	O

@USER	O
This	O
isn't	O
a	O
bug	O
,	O
but	O
a	O
design	O
choice	O
.	O

The	O
left-hand	O
side	O
can	O
,	O
however	O
,	O
contain	O
any	O
number	O
of	O
elements	O
,	O
and	O
provided	O
it	O
is	O
a	O
tuple	O
of	O
variables	O
the	O
unpacking	O
will	O
take	O
place	O
.	O

You	O
want	O
to	O
look	O
at	O
the	O
OO	O
interface	O
,	O
not	O
the	O
state	O
machine	O
interface	O
for	O
matplotlib	O
.	O

This	O
would	O
be	O
an	O
interesting	O
thing	O
to	O
add	O
to	O
the	O
emerging	O
thoughts	O
about	O
semantic	O
objects	O
in	O
mpl	O
.	O

Maybe	O
you	O
could	O
post	O
a	O
sample	O
of	O
your	O
data	O
array	O
?	O

I	O
believe	O
this	O
is	O
because	O
I'm	O
plotting	O
the	O
image	O
wrong	O
and	O
so	O
it	O
is	O
plotting	O
all	O
the	O
data	O
as	O
0	O
.	O

Try	O
running	O
with	O
`	O
python	O
-X	O
faulthandler	O
`	O
and	O
see	O
if	O
you	O
get	O
a	O
useful	O
traceback	O
at	O
the	O
segfault	O
.	O

Notice	O
how	O
only	O
one	O
body	O
has	O
been	O
coloured	O
,	O
all	O
other	O
disjoint	O
polygons	O
remain	O
white	O
:	O

Thanks	O
@USER	O
,	O
All	O
I	O
want	O
set	O
a	O
plot	O
title	O
based	O
on	O
the	O
what	O
user	O
is	O
selected	O
on	O
the	O
plot	O
,	O
for	O
example	O
if	O
the	O
plot	O
shows	O
a	O
data	O
from	O
'	O
Jan	O
2012	O
'	O
to	O
'	O
July	O
2014	O
'	O
dynamically	O
want	O
to	O
set	O
the	O
title	O
to	O
'	O
Years	O
'	O
and	O
if	O
the	O
user	O
selects	O
from	O
'	O
May	O
2014	O
'	O
to	O
'	O
July	O
2014	O
'	O
then	O
dynamically	O
want	O
to	O
set	O
the	O
title	O
to	O
'	O
Months	O
'	O
and	O
if	O
the	O
user	O
selects	O
from	O
'	O
July	O
2014	O
'	O
to	O
'	O
July	O
2104	O
'	O
then	O
dynamically	O
want	O
to	O
set	O
the	O
title	O
to	O
'	O
Days	O
'	O
.	O

I	O
would	O
like	O
to	O
know	O
if	O
I	O
have	O
a	O
triangular	O
marker	O
,	O
is	O
it	O
possible	O
to	O
control	O
its	O
orientation	O
?	O

I	O
like	O
to	O
sort	O
these	O
points	O
row	O
by	O
row	O
.	O

Before	O
comparing	O
between	O
these	O
two	O
PDFs	O
,	O
I	O
would	O
like	O
to	O
compare	O
original	O
time	O
series	O
and	O
the	O
series	O
of	O
Gaussian	O
distributed	O
numbers	O
with	O
the	O
standard	O
deviation	O
and	O
mean	O
from	O
original	O
wind	O
velocity	O
series	O
.	O

I	O
will	O
like	O
create	O
a	O
alpha	O
channel	O
,	O
for	O
explanation	O
I	O
put	O
a	O
example	O
:	O

The	O
function	O
is	O
supposed	O
to	O
be	O
smooth	O
and	O
connect	O
at	O
0	O
and	O
2	O
pi	O
in	O
the	O
y	O
range	O
of	O
(	O
0	O
,	O
2pi	O
)	O
not	O
touching	O
0	O
and	O
2pi	O
.	O

I	O
have	O
run	O
a	O
similar	O
script	O
before	O
,	O
and	O
I	O
think	O
I've	O
imported	O
all	O
the	O
relevant	O
modules	O
.	O

The	O
sample	O
image	O
makes	O
things	O
clear	O
.	O

Take	O
Care	O
,	O
George	O

I	O
don't	O
know	O
of	O
any	O
movie	O
format	O
that	O
is	O
vector	O
based	O
(	O
but	O
I	O
don't	O
know	O
a	O
lot	O
about	O
video	O
codecs	O
)	O
.	O

I	O
can't	O
find	O
anything	O
like	O
that	O
in	O
the	O
galery	O
and	O
all	O
my	O
attempts	O
do	O
deal	O
with	O
the	O
colorbar	O
failed	O
.	O

I	O
encourage	O
you	O
to	O
try	O
all	O
of	O
the	O
great	O
ideas	O
people	O
are	O
telling	O
you	O
,	O
and	O
I'm	O
interested	O
to	O
see	O
if	O
any	O
of	O
them	O
work	O
!	O

Have	O
you	O
tried	O
setting	O
your	O
backend	O
to	O
SVG	O
so	O
that	O
all	O
of	O
your	O
images	O
are	O
in	O
vector	O
format	O
?	O

I	O
appreciate	O
any	O
help	O
:)	O

Would	O
be	O
better	O
to	O
change	O
the	O
title	O

My	O
csv	O
is	O
like	O
:	O
id	O
,	O
author	O
,	O
title	O
,	O
language	O

After	O
making	O
all	O
of	O
the	O
changes	O
listed	O
in	O
steps	O
2	O
and	O
3	O
(	O
and	O
correcting	O
the	O
spelling	O
of	O
the	O
title	O
)	O
,	O
I	O
get	O
the	O
following	O
very-reasonable-looking	O
plot	O
:	O

When	O
I	O
look	O
at	O
the	O
plot	O
,	O
the	O
x	O
range	O
ends	O
up	O
being	O
from	O
0	O
to	O
12000	O
.	O

It	O
was	O
still	O
at	O
-1	O
when	O
I	O
saw	O
it	O
.	O

when	O
the	O
bad	O
points	O
are	O
deleted	O
,	O
the	O
user	O
can	O
click	O
on	O
different	O
buttons	O
to	O
classify	O
the	O
source	O
(	O
lets	O
say	O
"	O
star	O
"	O
,	O
"	O
galaxy	O
"	O
,	O
"	O
whatever	O
")	O
.	O

Your	O
`	O
Z	O
`	O
'	O
array	O
'	O
is	O
actually	O
a	O
list	O
of	O
numpy	O
arrays	O
of	O
different	O
lengths	O
.	O

when	O
the	O
2D	O
array	O
is	O
viewed	O
as	O
a	O
color	O

I	O
looked	O
through	O
the	O
examples	O
in	O
MatPlotLib	O
or	O
R	O
and	O
they	O
all	O
seem	O
to	O
already	O
start	O
with	O
random	O
data	O
to	O
generate	O
the	O
image	O
.	O

Filtering	O
spreads	O
out	O
the	O
information	O
and	O
makes	O
the	O
inner	O
contours	O
disappear	O
.	O

This	O
is	O
probably	O
related	O
to	O
upgrading	O
the	O
library	O
,	O
and	O
having	O
old	O
versions	O
of	O
the	O
javascript	O
hanging	O
around	O
.	O

can	O
you	O
explain	O
your	O
data	O
format	O
in	O
a	O
little	O
more	O
detail	O
and	O
give	O
us	O
some	O
copy	O
/	O
paste-able	O
code	O
to	O
create	O
a	O
toy	O
version	O
of	O
it	O
?	O

'	O
To	O
open	O
"	O
Python	O
,	O
"	O
you	O
need	O
to	O
install	O
X11	O
'	O
.	O

@USER	O
:	O
You	O
can	O
certainly	O
change	O
the	O
exact	O
shape	O
by	O
modifying	O
the	O
factor	O
"	O
3	O
"	O
in	O
front	O
of	O
the	O
lower	O
branch	O
.	O

Downvotes	O
are	O
due	O
to	O
the	O
lack	O
of	O
research	O
effort	O
(	O
if	O
you	O
look	O
at	O
the	O
reasons	O
for	O
downvotes	O
,	O
it	O
says	O
"	O
This	O
question	O
does	O
not	O
show	O
any	O
research	O
effort	O
")	O
.	O

As	O
for	O
the	O
matplotlib	O
version	O
1.1.0	O
,	O
it's	O
a	O
really	O
easy	O
install	O
,	O
at	O
least	O
on	O
my	O
by-now-quite	O
old	O
Lucid	O
it's	O
no	O
problem	O
at	O
all	O
.	O

Numpy	O
index	O
out	O
of	O
range	O
error	O

Though	O
I	O
want	O
to	O
add	O
that	O
manually	O
saving	O
the	O
chart	O
from	O
Qt	O
Widget	O
is	O
OK	O
.	O

Without	O
seeing	O
any	O
of	O
the	O
existing	O
plotting	O
functions	O
,	O
how	O
am	O
I	O
supposed	O
to	O
know	O
how	O
similar	O
or	O
different	O
they	O
are	O
?	O

You	O
can	O
use	O
spline	O
to	O
fit	O
the	O
[	O
blue	O
curve	O
-	O
peak	O
/	O
2	O
]	O
,	O
and	O
then	O
find	O
it's	O
roots	O
:	O
#CODE	O

I	O
had	O
to	O
add	O
`	O
colorbar	O
(	O
im2	O
)`	O
right	O
after	O
the	O
definition	O
of	O
im2	O
...	O

Creating	O
sample	O
data	O
#CODE	O

The	O
numbers	O
choose	O
now	O
many	O
decimal	O
places	O
you	O
want	O
and	O
the	O
e	O
chooses	O
scientific	O
notation	O
.	O

To	O
clear	O
up	O
the	O
code	O
,	O
I	O
put	O
the	O
individual	O
plotting	O
functions	O
for	O
the	O
six	O
or	O
so	O
groups	O
of	O
subplots	O
into	O
separate	O
submodules	O
and	O
call	O
them	O
with	O
the	O
arrays	O
they	O
need	O
for	O
plotting	O
purposes	O
.	O

You	O
have	O
given	O
us	O
a	O
list	O
of	O
requirements	O
,	O
but	O
what	O
have	O
you	O
tried	O
to	O
solve	O
your	O
own	O
problem	O
?	O

For	O
that	O
purpose	O
,	O
I	O
have	O
put	O
the	O
following	O
command	O
in	O
a	O
for	O
loop	O
:	O
#CODE	O

I	O
work	O
at	O
a	O
large	O
company	O
and	O
do	O
not	O
have	O
admin	O
rights	O
.	O

Note	O
that	O
it	O
only	O
detect	O
truly	O
equal	O
placements	O
,	O
if	O
you	O
would	O
have	O
values	O
of	O
`	O
[	O
0	O
,	O
0.00001	O
,	O
2	O
,	O
10	O
]`	O
,	O
they	O
would	O
probably	O
still	O
overlap	O
.	O

Suppose	O
I	O
have	O
a	O
triangle	O
mesh	O
defined	O
by	O
X	O
,	O
Y	O
,	O
and	O
Z	O
,	O
the	O
3D	O
coordinates	O
of	O
a	O
point	O
cloud	O
,	O
each	O
a	O
vector	O
of	O
length	O
n	O
,	O
and	O
UVW	O
,	O
a	O
2D	O
m-x-3	O
matrix	O
in	O
which	O
each	O
row	O
is	O
a	O
triplet	O
of	O
indices	O
into	O
the	O
point	O
cloud	O
.	O

I	O
am	O
trying	O
to	O
create	O
a	O
plot	O
with	O
vertical	O
bars	O
equal	O
to	O
score	O
length	O
,	O
AND	O
with	O
a	O
radiobutton	O
which	O
enables	O
the	O
user	O
to	O
choose	O
which	O
level	O
scores	O
he	O
/	O
she	O
wants	O
to	O
see	O
,	O
as	O
such	O
:	O

The	O
2nd	O
set	O
of	O
data	O
will	O
also	O
include	O
the	O
first	O
as	O
written	O
in	O
this	O
snippet	O
(	O
looking	O
for	O
good	O
way	O
to	O
delete	O
it	O
that	O
doesn't	O
cause	O
problems	O
later	O
down	O
the	O
line	O
)	O
.	O

So	O
you	O
need	O
to	O
add	O
`	O
u	O
`	O
:	O
#CODE	O

Regardless	O
,	O
once	O
you	O
get	O
used	O
to	O
it	O
,	O
I	O
find	O
it	O
much	O
easier	O
to	O
write	O
reusable	O
plotting	O
functions	O
in	O
python	O
with	O
matplotlib	O
than	O
in	O
Matlab	O
.	O

Now	O
,	O
you	O
should	O
be	O
able	O
to	O
get	O
all	O
the	O
libraries	O
you	O
need	O
just	O
as	O
easily	O
,	O
using	O
,	O
too	O
,	O
Macports	O
.	O

I	O
believe	O
that	O
I'm	O
using	O
the	O
TK	O
backend	O
;	O
I'm	O
not	O
married	O
to	O
that	O
choice	O
,	O
but	O
it's	O
not	O
clear	O
how	O
I	O
should	O
install	O
other	O
backends	O
.	O

What	O
is	O
this	O
float	O
,	O
and	O
how	O
can	O
I	O
convert	O
it	O
to	O
a	O
datetime	O
?	O

xtics	O
and	O
ytics	O
labels	O
because	O
they	O
appear	O
for	O
all	O
values	O
of	O
xPoints	O
and	O
yPoints	O
(	O
they	O
overlap	O
)	O
.	O

matplotlib	O
(	O
equal	O
unit	O
length	O
)	O

Is	O
there	O
any	O
reason	O
why	O
I	O
can't	O
do	O
this	O
?	O

I	O
am	O
using	O
the	O
following	O
codes	O
to	O
generate	O
the	O
square	O
wave	O
format	O
[	O
Eg	O
:	O
from	O
0	O
till	O
5	O
]	O
using	O
for	O
loop	O
.	O

Can	O
you	O
reduce	O
the	O
extent	O
of	O
you	O
problem	O
by	O
calculating	O
something	O
akin	O
to	O
a	O
density	O
,	O
reducing	O
millions	O
of	O
line	O
segments	O
to	O
maybe	O
a	O
few	O
hundred	O
or	O
a	O
few	O
thousand	O
data	O
points	O
?	O

array	O
erroneously	O
filled	O
with	O
the	O
same	O
value	O

You	O
can	O
change	O
the	O
`	O
reduce_C_function	O
`	O
option	O
if	O
you	O
want	O
something	O
different	O
than	O
a	O
simple	O
average	O
.	O

The	O
whole	O
traceback	O
is	O
rather	O
large	O
but	O
seems	O
to	O
revolve	O
around	O
this	O
part	O
(	O
if	O
I	O
read	O
it	O
correctly	O
):	O
#CODE	O

Matplotlib	O
:	O
WebAgg	O
backend	O
doesn't	O
show	O
any	O
figures	O

While	O
this	O
seems	O
what	O
I	O
was	O
looking	O
for	O
,	O
I'm	O
not	O
lucky	O
finding	O
any	O
proper	O
documentation	O
for	O
webagg	O
.	O

If	O
I	O
were	O
you	O
,	O
I'd	O
start	O
from	O
this	O
example	O
to	O
see	O
where	O
do	O
these	O
lines	O
start	O
showing	O
up	O
.	O

According	O
to	O
efiring	O
,	O
matplotlib	O
does	O
not	O
support	O
NumPy	O
datetime64	O
objects	O
(	O
at	O
least	O
not	O
yet	O
)	O
.	O

At	O
least	O
on	O
my	O
system	O
(	O
OSX	O
,	O
Python	O
2.7	O
,	O
mpl	O
1.1.0	O
)	O
,	O
I	O
don't	O
have	O
any	O
issues	O
with	O
panning	O
,	O
etc	O
.	O

Yeah	O
,	O
I	O
understand	O
why	O
Pylab	O
exists	O
,	O
and	O
why	O
it	O
seems	O
so	O
attractive	O
to	O
people	O
moving	O
over	O
from	O
MATLAB	O
,	O
but	O
over	O
the	O
long	O
term	O
I	O
feel	O
like	O
it's	O
a	O
net	O
negative	O
because	O
it	O
makes	O
it	O
harder	O
to	O
share	O
code	O
with	O
and	O
get	O
help	O
from	O
everyone	O
who	O
isn't	O
using	O
it	O
.	O

How	O
can	O
I	O
get	O
around	O
this	O
?	O

Function	O
of	O
Numpy	O
Array	O
with	O
if-statement	O

By	O
default	O
it	O
sets	O
the	O
(	O
0	O
,	O
0	O
)	O
element	O
of	O
the	O
array	O
in	O
the	O
upper	O
left	O
corner	O
.	O

I	O
appreciate	O
any	O
help	O
with	O
this	O
.	O

At	O
the	O
moment	O
I	O
do	O
a	O
for	O
loop	O
and	O
plot	O
each	O
point	O
:	O
#CODE	O

What	O
I	O
currently	O
have	O
to	O
convert	O
it	O
:	O
#CODE	O

You	O
should	O
post	O
your	O
new	O
code	O
and	O
the	O
**	O
full	O
**	O
traceback	O
in	O
another	O
question	O
.	O

I	O
don't	O
want	O
to	O
make	O
any	O
translations	O
/	O
shifts	O
to	O
the	O
data	O
itself	O
(	O
it's	O
a	O
large	O
dataset	O
,	O
etc	O
)	O
,	O
so	O
I'd	O
be	O
looking	O
to	O
do	O
this	O
with	O
some	O
matplotlib-trickery	O
.	O

Raster	O
to	O
Numpy	O
Array	O
-	O
how	O
to	O
change	O
a	O
default	O
color-scheme	O
of	O
a	O
matplotlib	O
plot	O

or	O
invert	O
the	O
Y-axis	O
:	O
#CODE	O

So	O
,	O
a	O
solution	O
is	O
simply	O
to	O
create	O
a	O
list	O
of	O
all	O
objects	O
that	O
will	O
be	O
refreshed	O
,	O
and	O
to	O
call	O
this	O
`	O
remove	O
`	O
method	O
for	O
everyone	O
of	O
them	O
:	O

The	O
best	O
choice	O
is	O
to	O
make	O
your	O
widgets	O
in	O
the	O
order	O
you	O
wish	O
them	O
to	O
appear	O
.	O

Python	O
:	O
How	O
to	O
generate	O
a	O
power	O
law	O
graph	O

Unfortunately	O
the	O
broken	O
command	O
is	O
also	O
preventing	O
me	O
from	O
tryig	O
this	O
,	O
so	O
I'll	O
have	O
to	O
fix	O
that	O
first	O
.	O

(	O
`	O
1	O
`	O
is	O
the	O
the	O
start	O
index	O
,	O
`	O
2	O
`	O
is	O
the	O
increment	O
)	O
In	O
contrast	O
,	O
`	O
x	O
[:	O
:	O
-1	O
]`	O
just	O
specifies	O
an	O
increment	O
of	O
`	O
-1	O
`	O
,	O
thus	O
reversing	O
the	O
array	O
.	O

One	O
way	O
around	O
this	O
is	O
not	O
to	O
use	O
the	O
pandas	O
`	O
plot	O
`	O
method	O
,	O
but	O
to	O
directly	O
the	O
matplotlib's	O
`	O
plot	O
`	O
function	O
.	O

where	O
`	O
numctrl_ccm90	O
`	O
and	O
`	O
numctrl_ucp90	O
`	O
are	O
the	O
widgets	O
corresponding	O
to	O
the	O
collimated	O
counts	O
at	O
-90deg	O
and	O
the	O
uncollimated	O
counts	O
+90deg	O
etc	O
.	O

Any	O
tips	O
?	O

I	O
would	O
appreciate	O
any	O
comment	O
or	O
suggestion	O
.	O

I	O
looked	O
at	O
`	O
masked_array	O
`	O
which	O
is	O
different	O
.	O

Without	O
having	O
all	O
the	O
code	O
listed	O
,	O
I	O
had	O
to	O
make	O
some	O
assumptions	O
about	O
the	O
data	O
and	O
about	O
what	O
you	O
wanted	O
.	O

You	O
mean	O
`	O
Axes3D	O
`	O
is	O
different	O
from	O
`	O
axes3d	O
`	O
?	O

If	O
you	O
already	O
have	O
python	O
and	O
gfortran	O
and	O
such	O
,	O
jump	O
in	O
at	O
the	O
point	O
where	O
you	O
need	O
.	O

Maybe	O
you	O
know	O
a	O
fix	O
for	O
this	O
?	O

How	O
can	O
I	O
fix	O
this	O
?	O

The	O
background	O
is	O
that	O
the	O
array's	O
values	O
represent	O
the	O
water	O
depth	O
of	O
a	O
square	O
grid	O
.	O

And	O
try	O
debugging	O
one	O
step	O
at	O
a	O
time	O
:	O
make	O
sure	O
you	O
have	O
good	O
data	O
before	O
you	O
try	O
to	O
plot	O
it	O
.	O

What	O
do	O
you	O
mean	O
the	O
"	O
label	O
"	O
?	O

To	O
do	O
this	O
we	O
ll	O
use	O
the	O
standard	O
y=mx+b	O
line	O
equation	O
where	O
m	O
is	O
the	O
line	O
s	O
slope	O
and	O
b	O
is	O
the	O
line	O
s	O
y-intercept	O
.	O

If	O
you	O
check	O
the	O
Matplotlib	O
example	O
for	O
the	O
function	O
you	O
are	O
using	O
,	O
you	O
will	O
notice	O
they	O
use	O
a	O
similar	O
methodology	O
:	O
build	O
empty	O
matrix	O
and	O
fill	O
it	O
with	O
strings	O
built	O
with	O
the	O
interpolation	O
method	O
.	O

But	O
the	O
question	O
still	O
remains-	O
how	O
do	O
I	O
fix	O
it	O
?	O

Based	O
on	O
the	O
plot	O
selection	O
I	O
want	O
to	O
set	O
the	O
plot	O
title	O
,	O
so	O
I	O
need	O
to	O
know	O
the	O
current	O
x	O
-axis	O
value	O
after	O
the	O
plot	O
is	O
shown	O
.	O

I	O
have	O
tried	O
the	O
following	O
code	O
but	O
this	O
creates	O
2	O
separate	O
charts	O
but	O
I	O
would	O
like	O
this	O
all	O
on	O
one	O
chart	O
.	O

Any	O
advice	O
would	O
be	O
greatly	O
appreciated	O
.	O

How	O
can	O
I	O
add	O
colorbar	O
to	O
this	O
code	O
?	O

I'm	O
reading	O
in	O
a	O
file	O
with	O
several	O
columns	O
,	O
each	O
of	O
which	O
contains	O
around	O
100	O
values	O
between	O
1	O
and	O
10	O
.	O

But	O
for	O
the	O
all	O
three	O
plots	O
We	O
have	O
same	O
temperature	O
T=4K	O
.	O

I've	O
downloaded	O
a	O
number	O
of	O
nifty	O
programs	O
and	O
utilities	O
(	O
and	O
info	O
sites	O
)	O
/	O
quantum	O
gis	O
,	O
virtual	O
terrain	O
project	O
,	O
grass	O
,	O
I'm	O
looking	O
to	O
get	O
an	O
xyz	O
csv	O
file	O
into	O
a	O
format	O
that	O
these	O
tools	O
can	O
use	O
.	O

I	O
have	O
all	O
the	O
plots	O
as	O
several	O
modules	O
in	O
python	O
/	O
matplotlib	O
already	O
coded	O
,	O
so	O
that's	O
perfect	O
.	O

At	O
some	O
level	O
,	O
this	O
is	O
purely	O
a	O
matter	O
of	O
taste	O
and	O
depends	O
a	O
bit	O
on	O
what	O
you	O
are	O
doing	O
.	O

I	O
really	O
want	O
to	O
plot	O
all	O
the	O
points	O
in	O
my	O
matrix	O
instead	O
of	O
the	O
(	O
M-1	O
)	O
x	O
(	O
N-1	O
)	O
.	O

I	O
want	O
to	O
set	O
a	O
colour	O
scheme	O
for	O
my	O
python	O
plots	O
,	O
so	O
that	O
they	O
don't	O
repeat	O
the	O
same	O
colour	O
like	O
they	O
are	O
for	O
A	O
and	O
H	O
in	O
the	O
top	O
plot	O
shown	O
below	O
.	O

I've	O
got	O
a	O
2D	O
numpy	O
array	O
with	O
1.0e6	O
as	O
the	O
no	O
data	O
value	O
.	O

Is	O
this	O
what	O
you	O
mean	O
?	O

This	O
is	O
somewhat	O
based	O
on	O
guesswork	O
,	O
since	O
I	O
can't	O
test	O
it	O
with	O
`	O
matplotlib	O
`	O
,	O
but	O
something	O
along	O
these	O
lines	O
might	O
work	O
:	O
#CODE	O

Here	O
is	O
some	O
sample	O
code	O
resembling	O
my	O
case	O
:	O
#CODE	O

This	O
is	O
the	O
result	O
for	O
printing	O
every	O
item	O
in	O
the	O
xyz	O
list	O
and	O
piping	O
it	O
into	O
a	O
file	O
.	O

Or	O
is	O
your	O
plot	O
supposed	O
to	O
be	O
an	O
average	O
or	O
something	O
?	O

I	O
just	O
wasn't	O
sure	O
if	O
changing	O
some	O
plotting	O
functions	O
around	O
would	O
help	O
.	O

I	O
agree	O
with	O
you	O
,	O
but	O
I	O
just	O
tried	O
to	O
make	O
code	O
which	O
uses	O
as	O
few	O
less	O
well-known	O
features	O
of	O
`	O
matplotlib	O
`	O
.	O

You	O
could	O
add	O
a	O
URL	O
to	O
the	O
image	O
,	O
so	O
someone	O
with	O
enough	O
reputation	O
can	O
upload	O
it	O
to	O
your	O
post	O
.	O

'	O
plot	O
'	O
shows	O
me	O
a	O
line	O
interconnecting	O
all	O
data	O
points	O
,	O
but	O
not	O
the	O
data	O
points	O
themselves	O
(	O
unless	O
there's	O
a	O
way	O
I	O
don't	O
know	O
of	O
)	O
.	O

Did	O
you	O
add	O
matplotlib	O
to	O
your	O
PYTHONPATH	O
?	O

I	O
have	O
successfully	O
edited	O
and	O
put	O
it	O
in	O
my	O
code	O
and	O
works	O
fine	O
,	O
but	O
my	O
problem	O
is	O
that	O
I	O
can't	O
show	O
multiple	O
lines	O
of	O
data	O
in	O
the	O
plot	O
,	O
I	O
want	O
to	O
show	O
up	O
to	O
10	O
lines	O
in	O
my	O
graph	O
but	O
I'm	O
getting	O
a	O
lot	O
of	O
errors	O
when	O
I	O
try	O
to	O
add	O
them	O
.	O

That	O
is	O
correct	O
,	O
it	O
draws	O
a	O
line	O
from	O
the	O
lower	O
left	O
to	O
the	O
upper	O
right	O
of	O
the	O
plot	O
as	O
stated	O
in	O
my	O
answer	O
.	O

The	O
problem	O
arises	O
when	O
I	O
select	O
a	O
subset	O
of	O
the	O
data	O
based	O
on	O
the	O
time	O
,	O
ie	O
:	O
#CODE	O

Do	O
you	O
have	O
any	O
suggestion	O
to	O
fix	O
this	O
?	O

I	O
think	O
I'm	O
going	O
to	O
start	O
from	O
scratch	O
so	O
that	O
I	O
know	O
what	O
I'm	O
doing	O
but	O
I	O
need	O
help	O
on	O
where	O
to	O
start	O
.	O

It	O
was	O
a	O
simplified	O
example	O
since	O
I	O
didn't	O
have	O
your	O
code	O
at	O
the	O
time	O
.	O

The	O
question	O
does	O
not	O
define	O
matrix	O
very	O
well	O
:	O
"	O
matrix	O
of	O
values	O
"	O
,	O
"	O
matrix	O
of	O
data	O
"	O
.	O

The	O
algorithm	O
returns	O
an	O
array	O
of	O
ranges	O
,	O
then	O
I	O
create	O
several	O
histograms	O
for	O
the	O
same	O
dataset	O
using	O
different	O
ranges	O
.	O

(	O
Since	O
all	O
you're	O
saying	O
now	O
is	O
"	O
I	O
have	O
this	O
equation	O
and	O
it	O
doesn't	O
work	O
"	O
,	O
we	O
don't	O
have	O
enough	O
info	O
to	O
answer	O
this	O
and	O
it's	O
not	O
a	O
question	O
that	O
can	O
be	O
answered	O
.	O
)	O

This	O
method	O
generalizes	O
to	O
any	O
number	O
of	O
rows	O
(	O
or	O
dimensions	O
)	O
.	O

Another	O
possibility	O
would	O
be	O
to	O
put	O
a	O
tuple	O
into	O
flist	O
:	O
#CODE	O

I	O
downloaded	O
the	O
100meg	O
datafiles	O
from	O
the	O
full	O
Basemap	O
distro	O
.	O

Do	O
you	O
have	O
any	O
other	O
ideas	O
to	O
handle	O
that	O
?	O

This	O
controls	O
whether	O
the	O
data	O
limits	O
or	O
the	O
bounding	O
rectangle's	O
shape	O
are	O
changed	O
when	O
the	O
aspect	O
/	O
limits	O
are	O
changed	O
.	O

Could	O
you	O
be	O
able	O
to	O
give	O
me	O
some	O
hints	O
how	O
to	O
solve	O
this	O
?	O

like	O
you	O
could	O
in	O
MATLAB	O
,	O
as	O
well	O
as	O
having	O
all	O
the	O
features	O
of	O

I	O
tried	O
setting	O
all	O
x-values	O
to	O
'	O
1	O
'	O
,	O
but	O
it	O
turns	O
out	O
as	O
kind	O
of	O
a	O
2d-diagram	O
,	O
anyway	O
:	O

This	O
is	O
the	O
hacky	O
way	O
to	O
fix	O
your	O
aspect	O
ratio	O
on	O
the	O
image	O
.	O

Is	O
Anaconda's	O
main	O
advantage	O
then	O
that	O
one	O
has	O
essentially	O
all	O
one	O
would	O
need	O
,	O
without	O
needing	O
continuously	O
to	O
pip	O
this	O
and	O
pip	O
that	O
?	O

Just	O
for	O
the	O
record	O
,	O
there's	O
no	O
need	O
to	O
jump	O
through	O
all	O
of	O
those	O
hoops	O
with	O
PIL	O
.	O

I'll	O
play	O
with	O
Dermen's	O
link	O
as	O
well	O
,	O
but	O
at	O
the	O
moment	O
the	O
most	O
pressing	O
problem	O
is	O
not	O
having	O
the	O
data	O
points	O
be	O
all	O
squished	O
together	O
because	O
the	O
spacing	O
between	O
the	O
ticks	O
is	O
very	O
very	O
small	O
.	O

This	O
is	O
perfect	O
,	O
though	O
I'm	O
not	O
sure	O
exactly	O
where	O
I	O
went	O
wrong	O
...	O

you	O
can	O
manually	O
set	O
the	O
size	O
of	O
the	O
colorbar	O
using	O
:	O
#CODE	O

Plotting	O
values	O
From	O
a	O
Array	O
in	O
Real	O
time	O
using	O
matplotlib	O
in	O
Python	O

I	O
will	O
also	O
gladly	O
accept	O
any	O
examples	O
where	O
someone	O
has	O
built	O
up	O
any	O
other	O
distribution	O
.	O

How	O
sure	O
are	O
you	O
that	O
the	O
code	O
*	O
runs	O
at	O
all*	O
?	O

I	O
was	O
temporarily	O
disappeared	O
at	O
my	O
day	O
job	O
!	O

In	O
addition	O
,	O
you'll	O
find	O
that	O
`	O
pylab	O
`	O
leaves	O
a	O
generous	O
,	O
often	O
undesirable	O
,	O
whitespace	O
around	O
the	O
image	O
.	O

Force	O
plot	O
to	O
have	O
a	O
special	O
size	O
or	O
to	O
increase	O
the	O
distance	O
between	O
subplots	O

If	O
you	O
do	O
all	O
the	O
data	O
processing	O
beforehand	O
,	O
and	O
provide	O
the	O
(	O
nearly	O
)	O
finished	O
product	O
.	O

To	O
make	O
it	O
clearer	O
I	O
want	O
to	O
get	O
the	O
x-axis	O
and	O
the	O
y-axis	O
range	O
so	O
as	O
to	O
find	O
the	O
mid	O
point	O
of	O
the	O
grid	O
.	O

and	O
you	O
might	O
want	O
to	O
add	O
the	O
ax	O
to	O
the	O
colorbar	O
.	O

The	O
question	O
is	O
:	O
Why	O
does	O
the	O
`	O
matplotlibrc	O
`	O
file	O
have	O
an	O
invalid	O
backend	O
(	O
`	O
pyside	O
`)	O
value	O
in	O
the	O
first	O
place	O
?	O

Is	O
that	O
a	O
bug	O
(	O
could	O
not	O
find	O
any	O
reports	O
to	O
this	O
)	O
,	O
a	O
feature	O
(	O
?!	O
)	O
or	O

I'm	O
not	O
sure	O
how	O
you	O
want	O
to	O
convert	O
your	O
data	O
,	O
but	O
if	O
you	O
have	O
irregular	O
line	O
lengths	O
,	O
the	O
easiest	O
way	O
would	O
be	O
something	O
like	O
this	O
:	O
#CODE	O

No	O
worries	O
,	O
I	O
was	O
coming	O
at	O
this	O
from	O
a	O
radio	O
perspective	O
not	O
a	O
gamma-ray	O
one	O
.	O

How	O
can	O
I	O
fix	O
this	O
?	O

Is	O
not	O
there	O
any	O
wx	O
example	O
in	O
your	O
iPython	O
distribution	O
?	O

Hmm	O
,	O
sorry	O
,	O
maybe	O
I	O
wasn't	O
clear	O
-	O
colours	O
[	O
li	O
,	O
:]	O
only	O
substitutes	O
the	O
tuple	O
with	O
index	O
"	O
li	O
"	O
,	O
but	O
I	O
need	O
to	O
substitute	O
all	O
tuples	O
with	O
indices	O
in	O
this	O
list	O
called	O
"	O
indices	O
"	O
(	O
and	O
li=length	O
(	O
indices	O
))	O

I've	O
also	O
tried	O
installing	O
1.3.0	O
through	O
pip	O
as	O
well	O
as	O
source	O
.	O

Any	O
help	O
would	O
be	O
much	O
appreciated	O
.	O

Maybe	O
you	O
need	O
to	O
rescale	O
with	O
1	O
/	O
30000	O
to	O
be	O
at	O
the	O
right	O
place	O
on	O
your	O
graph	O
.	O

Changing	O
this	O
would	O
require	O
low-level	O
tweaking	O
.	O

I've	O
also	O
refined	O
your	O
approach	O
to	O
allow	O
zooming	O
in	O
over	O
a	O
section	O
of	O
the	O
data	O
and	O
to	O
produce	O
better	O
results	O
at	O
the	O
borders	O
.	O

Any	O
help	O
would	O
be	O
appreciated	O
.	O

Can	O
someone	O
help	O
me	O
sort	O
out	O
the	O
issue	O
resulting	O
to	O
such	O
an	O
error	O
.	O

Is	O
there	O
any	O
way	O
,	O
I	O
can	O
save	O
a	O
magnified	O
version	O
for	O
proper	O
visualisation	O
later	O
?	O

So	O
let's	O
say	O
I	O
only	O
need	O
`	O
4402	O
x	O
4410	O
`	O
but	O
I	O
don't	O
know	O
the	O
index	O
.	O

I	O
had	O
to	O
upgrade	O
to	O
most	O
recent	O
MPL	O
to	O
find	O
the	O
subplots	O
method	O
of	O
pylab	O
.	O

averaging	O
elements	O
in	O
a	O
matrix	O
with	O
the	O
corresponding	O
elements	O
in	O
another	O
matrix	O
(	O
in	O
python	O
)	O

We	O
consider	O
now	O
that	O
`	O
--	O
pylab	O
`	O
was	O
a	O
mistake	O
,	O
but	O
that	O
it	O
was	O
still	O
really	O
usefull	O
at	O
the	O
beginning	O
of	O
IPython	O
.	O

Is	O
there	O
a	O
way	O
to	O
increase	O
the	O
thickness	O
and	O
size	O
of	O
ticks	O
in	O
matplotlib	O
without	O
having	O
to	O
write	O
a	O
long	O
piece	O
of	O
code	O
like	O
this	O
:	O
#CODE	O

Or	O
you	O
could	O
define	O
column	O
'	O
date_obj	O
'	O
as	O
the	O
index	O
of	O
your	O
data	O
:	O
#CODE	O

Am	O
I	O
missing	O
something	O
,	O
or	O
have	O
you	O
left	O
out	O
all	O
the	O
plot	O
commands	O
?	O

This	O
is	O
my	O
first	O
attempt	O
at	O
python	O
,	O
however	O
,	O
and	O
am	O
still	O
finding	O
out	O
how	O
it	O
works	O
,	O
so	O
please	O
put	O
me	O
right	O
if	O
this	O
idea	O
is	O
fundamentally	O
flawed	O
!	O

Finally	O
,	O
here's	O
a	O
sample	O
portion	O
of	O
an	O
old	O
(	O
bad	O
)	O
data	O
file	O
:	O
#CODE	O

If	O
you	O
you	O
have	O
any	O
questions	O
,	O
please	O
leave	O
a	O
comment	O
.	O

Where	O
exactly	O
is	O
the	O
problem	O
?	O

The	O
last	O
dutch	O
sentence	O
does	O
mean	O
something	O
like	O
:	O
The	O
system	O
can't	O
find	O
the	O
specified	O
file	O
.	O

This	O
is	O
nice	O
of	O
them	O
,	O
as	O
not	O
all	O
Windows	O
users	O
have	O
C++	O
compilers	O
available	O
.	O

The	O
root	O
of	O
the	O
problem	O
is	O
that	O
matplotlib	O
tries	O
to	O
put	O
all	O
bins	O
on	O
the	O
plot	O
.	O

This	O
question	O
has	O
probably	O
a	O
totally	O
simple	O
solution	O
but	O
I	O
just	O
can't	O
find	O
it	O
.	O

Any	O
suggestions	O
would	O
be	O
awesome	O
.	O

In	O
non-interactive	O
mode	O
,	O
display	O
all	O
figures	O
and	O
block	O
until	O
the	O

This	O
is	O
all	O
very	O
new	O
to	O
me	O
.	O

1	O
)	O
Let's	O
assume	O
I	O
take	O
1	O
sample	O
`	O
x	O
=	O
[	O
1	O
,	O
2	O
]`	O
from	O
the	O
dataset	O

I	O
have	O
a	O
4d	O
or	O
higher	O
ellipsoid	O
in	O
matrix	O
form	O
(	O
which	O
form	O
is	O
important	O
)	O
.	O

I	O
didn't	O
look	O
into	O
the	O
documentation	O
,	O
simply	O
used	O
the	O
dot	O
completion	O
on	O
the	O
handle	O

Here	O
is	O
your	O
example	O
with	O
those	O
modifications	O
and	O
also	O
with	O
a	O
single	O
layout	O
computed	O
at	O
the	O
beginning	O
that	O
is	O
reused	O
in	O
the	O
loop	O
.	O

I	O
now	O
understand	O
that	O
I	O
need	O
to	O
generate	O
all	O
the	O
triplets	O
across	O
the	O
three	O
tables	O
(	O
to	O
cover	O
all	O
the	O
combinations	O
)	O
.	O

i	O
dont	O
know	O
how	O
array	O
concatenation	O
works	O
on	O
numpy	O
arrays	O

I	O
can't	O
seem	O
to	O
get	O
my	O
head	O
around	O
why	O
it	O
doesn't	O
show	O
the	O
value	O
128	O
as	O
grey	O
with	O
I	O
have	O
chosen	O
the	O
cmap	O
to	O
be	O
gray-scale	O
.	O

what	O
do	O
you	O
mean	O
with	O
%s	O
?	O

If	O
that	O
isn't	O
what	O
you	O
want	O
,	O
I	O
would	O
recommend	O
doing	O
something	O
where	O
you	O
create	O
a	O
PNG	O
image	O
of	O
what	O
you	O
do	O
want	O
,	O
create	O
a	O
child	O
QLabel	O
,	O
and	O
move	O
the	O
child	O
widget	O
around	O
in	O
your	O
mouseMoveEvent	O
.	O

Any	O
suggestion	O
is	O
welcome	O
.	O

I	O
was	O
trying	O
to	O
add	O
a	O
custom	O
widget	O
into	O
qtdesginer	O
using	O
following	O
code	O
#CODE	O

source	O
:	O
#URL	O

As	O
has	O
been	O
pointed	O
out	O
above	O
,	O
this	O
will	O
get	O
more	O
complicated	O
if	O
the	O
both	O
the	O
y	O
graphs	O
were	O
defined	O
at	O
different	O
x	O
points	O
.	O

@USER	O
Simoes	O
,	O
what	O
happens	O
is	O
you	O
have	O
Nan's	O
in	O
various	O
places	O
in	O
your	O
array	O
?	O

An	O
example	O
script	O
with	O
generated	O
data	O
rather	O
than	O
the	O
real	O
data	O
is	O
:	O
#CODE	O

Also	O
,	O
it's	O
harder	O
to	O
reproduce	O
across	O
multiple	O
machines	O
(	O
even	O
if	O
that	O
just	O
means	O
"	O
my	O
current	O
machine	O
,	O
and	O
the	O
new	O
laptop	O
I'll	O
have	O
18	O
months	O
from	O
now	O
when	O
I've	O
forgotten	O
how	O
to	O
solve	O
it	O
")	O
.	O

The	O
words	O
'	O
non-default	O
'	O
and	O
'	O
normal	O
'	O
are	O
mine	O
-	O
I'm	O
not	O
sure	O
what	O
they	O
mean	O
yet	O
.	O

(	O
think	O
of	O
it	O
as	O
a	O
unit	O
square	O
)	O

This	O
code	O
uses	O
a	O
Counter	O
to	O
count	O
the	O
number	O
of	O
instances	O
that	O
an	O
object	O
occurs	O
in	O
an	O
array	O
(	O
in	O
this	O
case	O
counting	O
the	O
number	O
of	O
times	O
an	O
integer	O
is	O
in	O
your	O
list	O
)	O
.	O

In	O
the	O
image	O
above	O
I'm	O
trying	O
to	O
extract	O
iso-value	O
25	O
from	O
the	O
scalar	O
field	O
of	O
f	O
(	O
x	O
,	O
y	O
)=	O
x^3+y^3	O
.	O

Is	O
there	O
any	O
other	O
way	O
I	O
can	O
plot	O
my	O
result	O
continuously	O
?	O

at	O
lag	O
0	O
,	O
the	O
ACF	O
is	O
0.5	O
.	O

The	O
example	O
talks	O
about	O
a	O
fit	O
to	O
power	O
law	O
,	O
but	O
clearly	O
a	O
straight	O
line	O
could	O
be	O
done	O
as	O
well	O
.	O

As	O
I	O
see	O
it	O
,	O
I	O
could	O
resolve	O
and	O
make	O
it	O
work	O
if	O
I	O
could	O
adjust	O
the	O
line	O
spacing	O
/	O
height	O
to	O
be	O
less	O
than	O
what	O
the	O
font	O
requests	O
.	O

tells	O
the	O
writer	O
to	O
temporarily	O
save	O
the	O
frames	O
to	O
disc	O
before	O
composing	O
the	O
movie	O
,	O
which	O
side-steps	O
the	O
problem	O
.	O

Select	O
starting	O
color	O
in	O
matplotlib	O
colormap	O

I	O
know	O
this	O
question	O
is	O
a	O
couple	O
years	O
old	O
,	O
but	O
since	O
there	O
is	O
no	O
accepted	O
answer	O
,	O
I'll	O
add	O
what	O
works	O
for	O
me	O
.	O

include	O
the	O
piece	O
of	O
the	O
code	O
where	O
you	O
make	O
call	O
to	O
`	O
mpl_connect	O
`	O

Is	O
there	O
any	O
ready	O
to	O
use	O
function	O
based	O
on	O
python's	O
matplolib	O
?	O

-2	O
is	O
high	O
of	O
indices	O
range	O
and	O
then	O
low	O
of	O
indices	O
range	O

Any	O
insight	O
you	O
could	O
provide	O
that	O
would	O
help	O
get	O
me	O
started	O
the	O
proper	O
way	O
would	O
be	O
fantastic	O
.	O

(	O
I	O
can	O
open	O
a	O
new	O
question	O
if	O
that's	O
perferable	O
)	O

For	O
example	O
,	O
here's	O
a	O
simple	O
bit	O
of	O
code	O
that	O
sets	O
up	O
a	O
list	O
of	O
colors	O
based	O
on	O
whether	O
the	O
data	O
is	O
positive	O
or	O
negative	O
:	O
#CODE	O

With	O
that	O
example	O
data	O
I	O
was	O
getting	O
"	O
shape	O
mismatch	O
"	O
errors	O
.	O

Then	O
,	O
I'd	O
use	O
POV-Ray	O
or	O
Blender	O
to	O
model	O
the	O
planes	O
at	O
whatever	O
angles	O
,	O
spheres	O
for	O
the	O
round	O
things	O
(	O
planets	O
?	O
)	O
.	O

So	O
,	O
thus	O
far	O
,	O
all	O
I	O
produce	O
is	O
:	O

I'm	O
hoping	O
I	O
can	O
find	O
a	O
funciton	O
(	O
or	O
build	O
one	O
)	O
that	O
does	O
new_ids_arr	O
=	O
new_ids	O
*	O
new_ids^Transpose	O
...	O
or	O
similar	O

Any	O
help	O
much	O
appreciated	O
.	O

But	O
there	O
are	O
still	O
white	O
edges	O
around	O
(	O
I	O
have	O
added	O
a	O
screenshot	O
to	O
my	O
original	O
question	O
)	O

How	O
could	O
I	O
use	O
`	O
matplotlib	O
`	O
to	O
represent	O
such	O
a	O
matrix	O
as	O
a	O
grid	O
of	O
red	O
and	O
black	O
squares	O
?	O

The	O
thick	O
grey	O
line	O
in	O
the	O
middle	O
of	O
the	O
violin	O
is	O
the	O
bootstrapped	O
99%	O
confidence	O
interval	O
of	O
the	O
mean	O
,	O
which	O
is	O
the	O
white	O
horizontal	O
line	O
,	O
both	O
from	O
pointplot	O
.	O

@USER	O
I	O
up	O
voted	O
you	O
but	O
add	O
`	O
import	O
numpy	O
as	O
np	O
`	O

So	O
,	O
when	O
you	O
reduce	O
to	O
2	O
dimensions	O
,	O
the	O
power	O
is	O
the	O
sum	O
of	O
the	O
first	O
2	O
eigen	O
values	O
.	O

Indexing	O
starts	O
with	O
`	O
0	O
`	O
in	O
Python	O
(	O
and	O
most	O
any	O
other	O
programming	O
language	O
I	O
know	O
)	O
.	O

I	O
have	O
a	O
matrix	O
file	O
of	O
Hurricane	O
data	O
and	O
it	O
plots	O
with	O
the	O
command	O
:	O
#CODE	O

If	O
they	O
enter	O
30	O
,	O
they	O
will	O
look	O
at	O
a	O
30-minute	O
time	O
window	O
,	O
if	O
they	O
type	O
5	O
,	O
matplotlib	O
picks	O
this	O
up	O
,	O
data	O
gets	O
trimmed	O
,	O
and	O
only	O
5	O
minutes	O
worth	O
of	O
data	O
gets	O
shown	O
.	O

In	O
the	O
meanwhile	O
does	O
any	O
one	O
has	O
a	O
better	O
suggestion	O
?	O

I	O
want	O
it	O
at	O
y=	O
10.5	O
,	O
specifically	O
.	O

I	O
then	O
plot	O
each	O
of	O
these	O
values	O
,	O
but	O
to	O
get	O
a	O
more	O
sensible	O
result	O
,	O
I	O
need	O
to	O
take	O
the	O
mean	O
of	O
all	O
of	O
these	O
files	O
for	O
each	O
cell	O
,	O
and	O
plot	O
that	O
with	O
the	O
standard	O
deviation	O
obtained	O
as	O
error	O
bars	O
.	O

I'm	O
using	O
data	O
of	O
the	O
form	O
:	O
`	O
[	O
num1	O
,	O
num2	O
,...,	O
numk	O
]`	O
(	O
an	O
array	O
of	O
integers	O
)	O
.	O

For	O
the	O
time	O
steps	O
where	O
convergence	O
is	O
reached	O
Id	O
like	O
to	O
have	O
the	O
x-axis	O
green	O
and	O
for	O
non-convergence	O
starting	O
from	O
light	O
red	O
for	O
not	O
so	O
bad	O
to	O
red	O
for	O
bad	O
.	O

For	O
instance	O
you	O
can	O
add	O
#CODE	O

I	O
don't	O
want	O
to	O
get	O
a	O
newer	O
one	O
just	O
incase	O
any	O
of	O
the	O
practice	O
problems	O
/	O
lecture	O
problems	O
are	O
incompatible	O
with	O
a	O
newer	O
version	O
of	O
IDLE	O
.	O

All	O
of	O
these	O
frames	O
are	O
loaded	O
at	O
the	O
start	O
of	O
the	O
application	O
.	O

if	O
you	O
suspect	O
one	O
specific	O
builtin	O
,	O
try	O
checking	O
it's	O
type	O
(	O
`	O
type	O
(	O
dict	O
)`)	O
,	O
or	O
look	O
at	O
the	O
properties	O
/	O
functions	O
it	O
has	O
(	O
`	O
dir	O
(	O
dict	O
)`)	O
.	O

In	O
some	O
work	O
I	O
have	O
done	O
we	O
used	O
a	O
simple	O
approximation	O
of	O
the	O
derivative	O
,	O
when	O
this	O
changes	O
sign	O
you	O
have	O
a	O
peak	O
(	O
in	O
1D	O
data	O
)	O
,	O
one	O
can	O
then	O
add	O
some	O
parameters	O
to	O
remove	O
peaks	O
due	O
to	O
noise	O
.	O

Here's	O
a	O
working	O
code	O
sample	O
that	O
shows	O
you	O
how	O
to	O
get	O
it	O
working	O
.	O

How	O
can	O
I	O
change	O
this	O
behavior	O
and	O
use	O
Arial	O
fonts	O
for	O
all	O
of	O
my	O
plots	O
?	O

How	O
to	O
divide	O
ytics	O
to	O
a	O
certain	O
number	O
in	O
matplotlib	O
?	O

Matplotlib	O
cannot	O
find	O
facefile	O
,	O
is	O
using	O
old	O
Python	O
interpreter	O
location	O

do	O
you	O
get	O
any	O
message	O
when	O
trying	O
plotting	O
it	O
?	O

I	O
have	O
mac	O
os	O
x	O
10.5.8	O
and	O
was	O
wondering	O
if	O
anyone	O
could	O
recommend	O
where	O
I	O
could	O
find	O
the	O
proper	O
package	O
.	O

Well	O
,	O
this	O
isn't	O
a	O
wxPython	O
answer	O
but	O
I've	O
used	O
Chaco	O
for	O
this	O
sort	O
of	O
thing	O
and	O
it's	O
pretty	O
straight	O
forward	O
.	O

The	O
point	O
IS	O
at	O
its	O
position	O
namely	O
(	O
1	O
,	O
1	O
,	O
1	O
)	O
.	O

All	O
is	O
fine	O
again	O
.	O

The	O
next	O
plot	O
to	O
show	O
has	O
the	O
same	O
clusters	O
and	O
colors	O
but	O
a	O
different	O
location	O
.	O

@USER	O
The	O
first	O
approach	O
would	O
help	O
only	O
if	O
the	O
internal	O
representation	O
was	O
the	O
problem	O
,	O
but	O
it	O
doesn't	O
solve	O
the	O
underlying	O
size	O
issue	O
.	O

After	O
hours	O
analyzing	O
the	O
source	O
code	O
of	O
basemap's	O
`	O
tissot	O
`	O
function	O
,	O
learning	O
some	O
properties	O
of	O
ellipses	O
and	O
lot's	O
of	O
debugging	O
,	O
I	O
came	O
with	O
a	O
solution	O
to	O
my	O
problem	O
.	O

You'd	O
need	O
to	O
refactor	O
the	O
code	O
to	O
break	O
your	O
worker	O
method	O
up	O
into	O
smaller	O
pieces	O
that	O
multiple	O
processes	O
can	O
work	O
on	O
at	O
the	O
same	O
time	O
,	O
and	O
then	O
pull	O
it	O
back	O
together	O
once	O
all	O
the	O
pieces	O
are	O
ready	O
.	O

are	O
you	O
just	O
trying	O
to	O
open	O
a	O
separate	O
window	O
or	O
display	O
that	O
window	O
inside	O
of	O
your	O
main	O
window	O
?	O

The	O
one	O
catch	O
:	O
if	O
you	O
embed	O
a	O
PDF	O
inside	O
a	O
textbox	O
,	O
Word	O
will	O
rasterize	O
this	O
when	O
you	O
save	O
it	O
to	O
a	O
PDF	O
.	O

How	O
can	O
I	O
address	O
a	O
specific	O
line	O
(	O
by	O
name	O
,	O
by	O
number	O
,	O
by	O
reference	O
)	O
throughout	O
the	O
program	O
and	O
delete	O
that	O
line	O
?	O

And	O
also	O
I	O
need	O
to	O
do	O
that	O
for	O
all	O
variables	O
simultaneously	O
.	O

`	O
newtitle	O
=	O
"	O
%s\n$%s$	O
"	O
%	O
(	O
'	O
text\	O
below\	O
the\	O
main\	O
title	O
'	O
,	O
newtitle	O
)`	O

Have	O
you	O
had	O
a	O
look	O
at	O
the	O
`	O
webagg	O
`	O
matplotlib	O
backend	O
?	O

find	O
the	O
file	O
from	O
the	O
link	O

If	O
you	O
want	O
separate	O
figures	O
,	O
as	O
the	O
title	O
of	O
your	O
question	O
suggests	O
the	O
`	O
subplots=True	O
`	O
argument	O
might	O
get	O
the	O
job	O
done	O
.	O

I	O
suggest	O
removing	O
all	O
the	O
domain	O
specific	O
stuff	O
.	O

Just	O
change	O
`	O
shape	O
`	O
to	O
`	O
(	O
10	O
,	O
10	O
,	O
10	O
,	O
10	O
,	O
10	O
)`	O
and	O
see	O
.	O

Any	O
help	O
would	O
be	O
appreciated	O
.	O

However	O
,	O
(	O
at	O
least	O
what	O
I	O
have	O
seen	O
is	O
that	O
)	O
sometimes	O
after	O
the	O
plot	O
has	O
been	O
displayed	O
,	O
it	O
goes	O
into	O
a	O
"	O
Not	O
Responding	O
"	O
mode	O
and	O
you	O
just	O
have	O
to	O
kill	O
the	O
process	O
anyways	O
.	O

It	O
consists	O
in	O
a	O
floating-point	O
parameter	O
P	O
,	O
with	O
dimension	O
NxM	O
,	O
and	O
each	O
pixel	O
is	O
geolocated	O
by	O
the	O
fields	O
latitude	O
and	O
longitude	O
(	O
each	O
of	O
size	O
NxM	O
)	O
.	O

In	O
order	O
to	O
change	O
the	O
order	O
of	O
the	O
bars	O
,	O
you	O
should	O
change	O
the	O
order	O
in	O
the	O
index	O
.	O

This	O
does	O
not	O
work	O
when	O
I	O
replace	O
the	O
random	O
plot	O
data	O
with	O
an	O
actual	O
color	O
PNG	O
image	O
.	O

For	O
example	O
,	O
say	O
you	O
had	O
data	O
between	O
0	O
and	O
1	O
but	O
didn't	O
like	O
the	O
colors	O
used	O
at	O
the	O
extremes	O
of	O
the	O
colormap	O
for	O
0	O
and	O
1	O
.	O

After	O
searching	O
through	O
multiple	O
posts	O
,	O
I	O
basically	O
want	O
to	O
use	O
a	O
generic	O
colormap	O
(	O
rainbow	O
)	O
and	O
multiply	O
my	O
third	O
array	O
by	O
the	O
colormap	O
in	O
order	O
to	O
display	O
different	O
colors	O
for	O
each	O
of	O
the	O
xy	O
points	O
.	O

Hi	O
,	O
Thanks	O
Tobold	O
,	O
I	O
will	O
check	O
the	O
source	O
code	O
you	O
mention	O
.	O

This	O
next	O
block	O
is	O
what	O
does	O
what	O
you	O
want	O
with	O
the	O
`	O
for	O
`	O
loop	O
.	O

Say	O
that	O
I	O
have	O
6	O
variables	O
that	O
I	O
want	O
to	O
sort	O
into	O
these	O
3	O
groups	O
and	O
plot	O
like	O
a	O
venn	O
diagram	O
.	O

ValueError	O
:	O
setting	O
an	O
array	O
element	O
with	O
a	O
sequence	O
.	O

My	O
need	O
is	O
that	O
I	O
want	O
to	O
plot	O
these	O
nominal	O
values	O
of	O
both	O
actual	O
and	O
predicted	O
ones	O
on	O
the	O
same	O
plot	O
so	O
that	O
It	O
will	O
be	O
easy	O
to	O
compare	O
how	O
good	O
was	O
the	O
prediction	O
as	O
compared	O
to	O
actual	O
values	O
.	O

So	O
,	O
open	O
up	O
your	O
favorite	O
Terminal	O
emulator	O
and	O
enter	O
#CODE	O

Also	O
,	O
the	O
lists	O
need	O
to	O
be	O
equal	O
length	O
.	O

Because	O
matplotlib	O
will	O
convert	O
things	O
to	O
numpy	O
arrays	O
regardless	O
,	O
there	O
are	O
more	O
efficient	O
ways	O
to	O
do	O
it	O
.	O

And	O
here	O
is	O
where	O
I	O
grab	O
the	O
data	O
and	O
I	O
try	O
to	O
update	O
the	O
limits	O
of	O
the	O
plot	O
#CODE	O

Any	O
help	O
is	O
appreciated	O
.	O

Which	O
is	O
the	O
why	O
,	O
conveniently	O
,	O
`	O
colors	O
`	O
keyword	O
exists	O
.	O

How	O
can	O
I	O
find	O
the	O
code	O
to	O
support	O
multiple	O
plotting	O
scales	O
and	O
X-axis	O
on	O
seperate	O
panels	O
using	O
sizer	O
routines	O
?	O

I'm	O
trying	O
to	O
build	O
Matplotlib	O
from	O
source	O
with	O
Tkinter	O
.	O

This	O
means	O
that	O
you	O
can	O
install	O
matplotlib	O
locally	O
in	O
your	O
`	O
virtualenv	O
`	O
and	O
it	O
will	O
find	O
all	O
of	O
its	O
backend	O
dependencies	O
in	O
the	O
system-wide	O
`	O
site-packages	O
`	O
.	O

I	O
am	O
trying	O
to	O
create	O
a	O
simple	O
GUI	O
using	O
tkinter	O
which	O
will	O
just	O
open	O
a	O
window	O
displaying	O
the	O
plot	O
.	O

Web	O
convinces	O
me	O
using	O
this	O
is	O
no	O
good	O
idea	O
after	O
all	O
:	O
For	O
example	O
#URL	O
"	O
Avoid	O
3-D	O
charts	O
at	O
all	O
costs	O
.	O

Using	O
pure	O
Python	O
,	O
you	O
can	O
extract	O
columns	O
`	O
1	O
`	O
,	O
`	O
5	O
`	O
and	O
`	O
7	O
`	O
by	O
using	O
the	O
following	O
nested	O
list	O
comprehension	O
:	O
#CODE	O

Is	O
there	O
any	O
way	O
of	O
getting	O
some	O
input	O
from	O
a	O
user	O
in	O
matplotlib	O
?	O

`	O
theta	O
`	O
is	O
in	O
radians	O
instead	O
of	O
degrees	O
.	O

'	O
plot	O
'	O
shows	O
me	O
a	O
line	O
interconnecting	O
all	O
data	O
points	O
,	O
but	O
not	O
the	O
data	O
points	O
themselves	O
(	O
unless	O
there's	O
a	O
way	O
I	O
don't	O
know	O
of	O
)	O
.	O

I	O
could	O
divide	O
the	O
`	O
peak_top	O
`	O
by	O
2	O
to	O
find	O
the	O
half	O
height	O
and	O
then	O
try	O
and	O
find	O
y-values	O
corresponding	O
to	O
the	O
half	O
height	O
,	O
but	O
then	O
I	O
would	O
run	O
into	O
trouble	O
if	O
there	O
are	O
no	O
x-values	O
exactly	O
matching	O
the	O
half	O
height	O
.	O

why	O
can't	O
it	O
find	O
the	O
file	O
?	O

However	O
,	O
the	O
resulting	O
image	O
always	O
has	O
the	O
same	O
resolution	O
,	O
around	O
(	O
250x250	O
)	O
.	O

Installing	O
those	O
packages	O
from	O
the	O
source	O
can	O
be	O
a	O
pain	O
,	O
especially	O
on	O
Windows	O
and	O
OS	O
X	O
.	O

Sort	O
arrays	O
by	O
two	O
criteria	O

However	O
,	O
I	O
am	O
kinda	O
confused	O
,	O
any	O
help	O
would	O
be	O
appreciated	O
.	O

Unfortunately	O
if	O
you	O
search	O
this	O
array	O
for	O
all	O
subnodes	O
given	O
some	O
index	O
you	O
only	O
get	O
some	O
of	O
the	O
subnodes	O
contained	O
by	O
the	O
original	O
triangle	O
because	O
most	O
nodes	O
belong	O
to	O
several	O
triangles	O
and	O
they	O
are	O
only	O
added	O
to	O
one	O
of	O
these	O
.	O

If	O
I	O
set	O
shrink	O
1.0	O
and	O
fraction	O
to	O
anything	O
,	O
it	O
shrinks	O
the	O
graph	O
,	O
not	O
affecting	O
the	O
colorbar	O
size	O
at	O
all	O
,	O
until	O
changing	O
fraction	O
causes	O
it	O
to	O
be	O
exactly	O
what	O
I	O
already	O
have	O
,	O
at	O
which	O
point	O
changing	O
them	O
stops	O
doing	O
anything	O
.	O

It	O
doesn't	O
add	O
anything	O
else	O
.	O

Now	O
I	O
just	O
wish	O
they'd	O
add	O
some	O
kind	O
of	O
drop	O
down	O
menu	O
to	O
matplotlib	O
...	O

Thanks	O
for	O
any	O
help	O

The	O
source	O
data's	O
list	O
is	O
called	O
xyz	O
.	O

I	O
have	O
been	O
looking	O
around	O
and	O
maybe	O
it	O
could	O
be	O
a	O
version-of-Pyhton	O
problem	O
.	O

(	O
I	O
want	O
to	O
reduce	O
the	O
size	O
of	O
the	O
outlier	O
)	O
#CODE	O

This	O
should	O
work	O
for	O
any	O
number	O
of	O
columns	O
and	O
length	O
of	O
data	O
.	O

Any	O
assistance	O
would	O
be	O
greatly	O
appreciated	O
,	O
this	O
has	O
been	O
doing	O
my	O
head	O
in	O
.	O

where	O
axes_square	O
is	O
simply	O
:	O
#CODE	O

I	O
can	O
put	O
together	O
a	O
more	O
detailed	O
answer	O
based	O
on	O
that	O
package	O
if	O
desired	O
.	O

It's	O
fairly	O
easy	O
to	O
speed	O
it	O
up	O
by	O
making	O
an	O
approximation	O
--	O
just	O
take	O
the	O
2D	O
histogram	O
and	O
blur	O
it	O
with	O
a	O
guassian	O
filter	O
of	O
the	O
right	O
radius	O
and	O
covariance	O
.	O

I've	O
tried	O
the	O
solutions	O
posted	O
by	O
Marek	O
and	O
qarma	O
attempting	O
to	O
obtain	O
the	O
coordinates	O
of	O
the	O
bins	O
rather	O
than	O
the	O
index	O
of	O
them	O
,	O
like	O
so	O
:	O
#CODE	O

What	O
do	O
you	O
mean	O
by	O
make	O
sure	O
the	O
'	O
backend	O
setting	O
for	O
pylab	O
is	O
not	O
set	O
to	O
inline	O
?	O

sets	O
the	O
initial	O
x	O
,	O
v	O
of	O
every	O
particle	O
in	O
an	O
array	O

Post	O
more	O
information	O
on	O
structure	O
of	O
your	O
array	O
.	O

Here	O
is	O
a	O
sample	O
code	O
if	O
anyone	O
wants	O
to	O
play	O
with	O
it	O
.	O

Are	O
all	O
these	O
settings	O
really	O
necessary	O
to	O
achieve	O
the	O
result	O
I'm	O
looking	O
for	O
,	O
or	O
is	O
there	O
perhaps	O
a	O
more	O
compact	O
way	O
to	O
accomplish	O
my	O
goal	O
:	O
#CODE	O

I	O
want	O
to	O
select	O
(	O
in	O
the	O
SQL	O
sense	O
)	O
just	O
columns	O
`	O
b0	O
`	O
through	O
`	O
b9	O
`	O
from	O
the	O
array	O
,	O
giving	O
the	O
structure	O
#CODE	O

I	O
can't	O
find	O
any	O
method	O
to	O
tell	O
mpl	O
to	O
plot	O
all	O
of	O
the	O
data	O
,	O
it	O
seems	O
that	O
there	O
are	O
only	O
methods	O
for	O
drawing	O
every	O
n'th	O
element	O
or	O
similar	O
(	O
and	O
passing	O
1	O
does	O
not	O
help	O
)	O
.	O

My	O
question	O
concerns	O
iterating	O
through	O
the	O
rows	O
of	O
a	O
data	O
frame	O
and	O
on	O
each	O
row	O
setting	O
a	O
field	O
based	O
on	O
information	O
in	O
a	O
different	O
data	O
frame	O
.	O

EDIT	O
:	O
Adding	O
logic	O
to	O
default	O
empty	O
strings	O
to	O
`	O
0	O
`	O
,	O
use	O
a	O
different	O
value	O
if	O
you	O
want	O
to	O
handle	O
empty	O
strings	O
in	O
`	O
years	O
`	O
colomn	O
differently	O
#CODE	O

I	O
would	O
suggest	O
that	O
you	O
use	O
2-dimensional	O
numpy	O
array	O
.	O

I	O
renamed	O
them	O
to	O
aa	O
,	O
ab	O
and	O
ac	O
but	O
still	O
get	O
the	O
same	O
error	O
.	O

In	O
this	O
last	O
case	O
,	O
RAM	O
usage	O
fits	O
the	O
equivalent	O
`	O
chunk	O
`	O
size	O
#CODE	O

`	O
pandas	O
`	O
,	O
like	O
`	O
numpy	O
`	O
and	O
many	O
other	O
modules	O
,	O
is	O
not	O
written	O
in	O
pure	O
Python	O
-	O
it	O
has	O
components	O
written	O
in	O
C	O
and	O
Cython	O
that	O
get	O
compiled	O
into	O
version-	O
and	O
platform-specific	O
libraries	O
during	O
the	O
build	O
process	O
.	O

It	O
gave	O
me	O
the	O
error	O
:	O
cqid	O
=	O
row	O
[	O
'	O
ClearQuest	O
ID	O
']	O
TypeError	O
:	O
string	O
indices	O
must	O
be	O
integers	O
,	O
not	O
str	O
...........	O

The	O
use	O
case	O
is	O
that	O
I	O
have	O
different	O
time	O
series	O
coming	O
from	O
different	O
data	O
sources	O
.	O

How	O
can	O
I	O
get	O
pandas	O
Timestamp	O
offset	O
by	O
certain	O
amount	O
of	O
months	O
?	O

I	O
managed	O
to	O
get	O
the	O
stats	O
by	O
placing	O
everything	O
in	O
nested	O
dictionary	O
,	O
but	O
I	O
feel	O
that	O
there	O
may	O
be	O
a	O
much	O
easier	O
way	O
to	O
the	O
approach	O
by	O
using	O
pandas	O
dataframes	O
and	O
groubpy	O
.	O

Just	O
to	O
get	O
a	O
sense	O
of	O
what	O
I'm	O
trying	O
to	O
achieve	O
.	O

Which	O
is	O
suspect	O
is	O
due	O
to	O
my	O
data	O
range	O
,,	O
but	O
it	O
may	O
well	O
be	O
that	O
I	O
don't	O
understand	O
the	O
other	O
parameters	O
.	O

Your	O
second	O
one	O
doesn't	O
really	O
make	O
sense	O
as	O
an	O
aggregation	O
.	O

How	O
can	O
I	O
get	O
the	O
index	O
of	O
certain	O
element	O
of	O
a	O
Series	O
in	O
python	O
pandas	O
?	O

(	O
Very	O
,	O
very	O
late	O
reply	O
-	O
apologies	O
.	O
)	O
That's	O
true	O
,	O
you'd	O
use	O
the	O
method	O
EdChum	O
suggested	O
for	O
longer	O
lists	O
of	O
columns	O
.	O

If	O
actual_sum	O
and	O
expected_to_date	O
are	O
equal	O
,	O
put	O
a	O
0	O

`	O
ts	O
[	O
ts	O
[	O
'	O
values	O
']	O
0	O
]`	O
should	O
produce	O
the	O
output	O
you	O
are	O
looking	O
for	O
.	O

And	O
I	O
get	O
the	O
counts	O
:	O
#CODE	O

The	O
standard	O
deviation	O
differs	O
between	O
pandas	O
and	O
numpy	O
.	O

I	O
would	O
like	O
to	O
get	O
rid	O
of	O
the	O
loops	O
,	O
if	O
that	O
is	O
possible	O
.	O

If	O
I	O
change	O
the	O
names	O
then	O
there	O
is	O
nothing	O
to	O
reference	O
.	O

I	O
even	O
tried	O
building	O
from	O
the	O
git	O
,	O
but	O
whatever	O
I	O
seem	O
to	O
do	O
,	O
I	O
get	O
the	O
same	O
error	O
:	O
#CODE	O

I	O
want	O
to	O
do	O
the	O
following	O
operations	O
on	O
the	O
data	O
storage	O
:	O

How	O
do	O
I	O
get	O
it	O
to	O
actually	O
show	O
the	O
graph	O
?	O

#URL	O
shows	O
a	O
way	O
to	O
get	O
the	O
number	O
of	O
days	O
in	O
a	O
month	O
,	O
making	O
the	O
rest	O
more	O
or	O
less	O
trivial	O
as	O
they	O
don't	O
vary	O
.	O

to	O
create	O
average	O
values	O
with	O
an	O
equidistant	O
time-vector	O
.	O

I	O
get	O
something	O
where	O
all	O
"	O
newlines	O
"	O
are	O
escaped	O
.	O

Reproducing	O
without	O
a	O
data	O
file	O
,	O
using	O
Jeff's	O
suggestion	O
:	O
#CODE	O

However	O
,	O
I	O
also	O
want	O
to	O
get	O
it	O
on	O
the	O
basis	O
of	O
the	O
`	O
Group	O
`	O
variable	O
,	O
which	O
means	O
I	O
don't	O
want	O
to	O
get	O
`	O
Bob	O
`'	O
s	O
`	O
Value	O
`	O
based	O
on	O
the	O
`	O
Jared	O
`'	O
s	O
`	O
Value	O
`	O
,	O
since	O
those	O
two	O
records's	O
`	O
Group	O
`	O
value	O
is	O
different	O
-	O
I	O
only	O
compute	O
it	O
within	O
each	O
specific	O
`	O
Group	O
`	O
variable	O
.	O

I	O
try	O
to	O
use	O
jsonlint	O
to	O
validate	O
these	O
json	O
files	O
but	O
encounter	O
some	O
error	O
messages	O
.	O

The	O
logic	O
to	O
arrive	O
at	O
that	O
database	O
is	O
an	O
intricate	O
mix	O
of	O
Python	O
processing	O
and	O
SQL	O
joins	O
done	O
in	O
sqlite3	O
.	O

I	O
want	O
to	O
take	O
advantage	O
of	O
the	O
`	O
str	O
`	O
accessor	O
to	O
split	O
the	O
data	O
into	O
two	O
columns	O
,	O
such	O
that	O
the	O
first	O
column	O
is	O
,	O
Name	O
,	O
contains	O
the	O
actual	O
name	O
(	O
first	O
name	O
last	O
name	O
)	O
,	O
and	O
the	O
second	O
column	O
,	O
Email	O
,	O
contains	O
the	O
email	O
address	O
)	O
.	O

In	O
fact	O
the	O
only	O
really	O
relevant	O
data	O
needed	O
for	O
the	O
plot	O
is	O
the	O
first	O
and	O
second	O
column	O
,	O
namely	O
:	O
`	O
Compression	O
Force	O
`	O
and	O
`	O
Compression	O
Velocity	O
`	O
.	O

How	O
to	O
get	O
special	O
characters	O
from	O
Excel	O
to	O
screen	O
using	O
pandas	O
?	O

And	O
replace	O
`'	O
Month	O
'`	O
with	O
`'	O
Day	O
'`	O
below	O
.	O

But	O
if	O
you	O
have	O
a	O
huge	O
amount	O
to	O
data	O
,	O
it	O
*	O
might	O
*	O
be	O
interesting	O
to	O
think	O
of	O
a	O
more	O
complex	O
data	O
model	O
.	O

What	O
are	O
you	O
trying	O
to	O
do	O
where	O
this	O
is	O
the	O
bottleneck	O
?	O

How	O
can	O
I	O
change	O
that	O
and	O
use	O
insted	O
the	O
first	O
line	O
of	O
output	O
code	O
as	O
a	O
column	O
(	O
In	O
this	O
case	O
line	O
10	O
:	O
Sub-Data	O
Item	O
...	O
)	O

My	O
objective	O
was	O
to	O
have	O
a	O
DTM	O
like	O
the	O
one	O
you	O
get	O
in	O
R	O
tm	O
.	O

So	O
right	O
now	O
all	O
the	O
data	O
comes	O
from	O
each	O
iteration	O
group	O
,	O
and	O
all	O
of	O
its	O
is	O
transformed	O
into	O
one	O
column	O
vector	O
.	O

I	O
am	O
trying	O
to	O
get	O
to	O
the	O
point	O
where	O
I	O
can	O
run	O
#CODE	O

While	O
I	O
don't	O
get	O
that	O
warning	O
with	O
#CODE	O

Thanks	O
@USER	O
-	O
I	O
mean	O
that	O
,	O
if	O
we	O
do	O
`	O
A-B	O
`	O
we	O
should	O
only	O
get	O
the	O
NaNs	O
in	O
`	O
A	O
`	O
,	O
and	O
not	O
the	O
NaNs	O
in	O
`	O
B	O
`	O
.	O

I	O
have	O
three	O
columns	O
in	O
my	O
data	O
set	O
,	O
namely	O
"	O
age	O
"	O
,	O
"	O
race	O
"	O
,	O
"	O
sex	O
"	O
,	O
that	O
I	O
care	O
about	O
.	O

Cannot	O
get	O
the	O
average	O
date	O
using	O
pandas	O

Any	O
suggestions	O
?	O

All	O
values	O
ought	O
to	O
be	O
integers	O
,	O
no	O
floats	O
.	O

Note	O
:	O
this	O
will	O
get	O
tripped	O
up	O
by	O
some	O
strings	O
,	O
so	O
use	O
with	O
caution	O
.	O

The	O
purpose	O
of	O
all	O
these	O
stuff	O
is	O
a	O
geographical	O
representation	O
of	O
data	O
on	O
a	O
spatial	O
grid	O
.	O

Since	O
Name	O
`	O
C	O
`	O
does	O
not	O
have	O
`	O
3	O
`	O
or	O
`	O
5	O
`	O
in	O
the	O
column	O
`	O
Activity	O
`	O
,	O
I	O
do	O
not	O
want	O
to	O
get	O
this	O
data	O
frame	O
.	O

Data	O
has	O
to	O
be	O
collected	O
before	O
local	O
data	O
frame	O
is	O
created	O
.	O

PANDAS	O
:	O
Extracting	O
values	O
from	O
a	O
column	O
by	O
applying	O
a	O
condition	O
on	O
other	O
columnns	O

If	O
you	O
try	O
to	O
produce	O
the	O
groups	O
from	O
my	O
example	O
you'll	O
see	O
what	O
I	O
mean	O
.	O

`	O
pandas	O
`	O
,	O
like	O
`	O
numpy	O
`	O
and	O
many	O
other	O
modules	O
,	O
is	O
not	O
written	O
in	O
pure	O
Python	O
-	O
it	O
has	O
components	O
written	O
in	O
C	O
and	O
Cython	O
that	O
get	O
compiled	O
into	O
version-	O
and	O
platform-specific	O
libraries	O
during	O
the	O
build	O
process	O
.	O

Not	O
sure	O
how	O
to	O
get	O
around	O
this	O
...	O
pretty	O
new	O
to	O
pandas	O
.	O

Here's	O
the	O
product	O
:	O
#CODE	O

However	O
,	O
as	O
the	O
data	O
became	O
large	O
,	O
we	O
played	O
with	O
SQLAlchemy	O
/	O
SQLite3	O
.	O

But	O
this	O
time	O
I	O
get	O
another	O
error	O
:	O
#CODE	O

Makes	O
the	O
change	O
the	O
idea	O
of	O
trying	O
to	O
use	O
this	O
approach	O
all	O
together	O
.	O

For	O
a	O
generalized	O
scenario	O
where	O
there	O
are	O
many	O
different	O
combinations	O
of	O
values	O
under	O
'	O
COL1	O
'	O
and	O
'	O
COL3	O
'	O
,	O
this	O
works	O
but	O
is	O
probably	O
not	O
nearly	O
as	O
efficient	O
as	O
it	O
can	O
be	O
:	O
#CODE	O

Similarly	O
in	O
your	O
example	O
where	O
you	O
plot	O
`	O
col1	O
,	O
col2	O
`	O
differently	O
based	O
on	O
`	O
col3	O
`	O
,	O
what	O
if	O
there	O
are	O
NA	O
values	O
that	O
break	O
the	O
association	O
between	O
`	O
col1	O
,	O
col2	O
,	O
col3	O
`	O
?	O

For	O
example	O
,	O
I	O
want	O
to	O
take	O
values	O
from	O
`	O
col_3	O
`	O
and	O
`	O
col_4	O
`	O
and	O
use	O
them	O
to	O
generate	O
a	O
single	O
values	O
.	O

The	O
speed	O
difference	O
is	O
astonishing	O
.	O

The	O
summation	O
in	O
one	O
group	O
won't	O
reduce	O
the	O
size	O
of	O
the	O
result	O
,	O
the	O
summation	O
I	O
want	O
to	O
do	O
is	O
across	O
different	O
groups	O
.	O

If	O
you	O
really	O
prefer	O
`	O
1	O
`'	O
s	O
and	O
`	O
0	O
`'	O
s	O
replace	O
the	O
last	O
line	O
with	O
:	O
#CODE	O

So	O
traverse	O
the	O
data	O
once	O
and	O
generate	O
both	O
arrays	O
would	O
be	O
preferred	O
.	O

Im	O
not	O
fully	O
adjusted	O
to	O
how	O
Pandas	O
is	O
using	O
matplotlib	O
so	O
i	O
often	O
switch	O
to	O
matplotlib	O
myself	O
if	O
plots	O
get	O
more	O
complicated	O
,	O
eg	O
:	O
#CODE	O

The	O
table	O
that	O
gives	O
this	O
message	O
contains	O
a	O
few	O
columns	O
,	O
none	O
of	O
them	O
have	O
data	O
in	O
them	O
.	O

so	O
yes	O
later	O
i	O
have	O
open	O
the	O
file	O
but	O
thanks	O
to	O
pandas	O
i	O
can	O
use	O
the	O
`	O
chunksize	O
`	O
command	O
to	O
get	O
the	O
information	O
i	O
need	O
.	O

create	O
column	O
names	O
by	O
joining	O
two	O
labels	O
of	O
different	O
levels	O
with	O
pandas	O

@USER	O
so	O
how	O
should	O
i	O
write	O
it	O
so	O
that	O
the	O
program	O
gives	O
seq	O
to	O
'	O
Hsequence	O
'	O
column	O
when	O
'	O
Hcolumn	O
'	O
contains	O
the	O
title	O
from	O
fasta	O
file	O
?	O

Also	O
,	O
in	O
my	O
larger	O
directory	O
,	O
this	O
is	O
taking	O
forever	O
-	O
as	O
in	O
,	O
about	O
a	O
gig	O
of	O
CSVs	O
is	O
timing	O
out	O
for	O
me	O
(	O
by	O
my	O
hand	O
)	O
at	O
around	O
20	O
minutes	O
.	O

The	O
key	O
was	O
unstacking	O
the	O
data	O
first	O
:	O
#CODE	O

I	O
want	O
to	O
get	O
the	O
latitude	O
and	O
longitude	O
coordinates	O
for	O
any	O
one	O
of	O
the	O
columns	O
in	O
the	O
data	O
frame	O
below	O
.	O

Option	O
values	O
are	O
restored	O
automatically	O
when	O
you	O
exit	O
the	O
`	O
with	O
`	O
block	O
.	O

I	O
am	O
finding	O
difficulty	O
to	O
plot	O
reason	O
every	O
csv	O
file	O
starts	O
with	O
different	O
date	O
,	O
that's	O
the	O
reason	O
I	O
was	O
trying	O
to	O
convert	O
into	O
no	O
.	O
of	O
days	O
,	O
so	O
that	O
I	O
can	O
plot	O
all	O
in	O
one	O
go	O
with	O
starting	O
day	O
-	O
1	O
,	O
for	O
example	O
:	O
-	O
csv	O
file	O
2	O
fall	O
short	O
as	O
compared	O
to	O
csv	O
file	O
1	O
.	O

Most	O
of	O
the	O
time	O
you	O
can	O
get	O
away	O
with	O
using	O
something	O
else	O
...	O

In	O
that	O
case	O
the	O
index	O
is	O
composed	O
of	O
integers	O
from	O
0	O
to	O
n	O
:	O
#CODE	O

You	O
have	O
a	O
difference	O
between	O
a	O
mac	O
and	O
a	O
pc	O
,	O
and	O
*	O
presumably	O
*	O
the	O
same	O
code	O
.	O

Suppose	O
you	O
want	O
to	O
find	O
the	O
row	O
or	O
rows	O
where	O
`	O
beef	O
`	O
production	O
was	O
the	O
highest	O
.	O

The	O
number	O
of	O
columns	O
may	O
differ	O
and	O
so	O
does	O
the	O
column	O
names	O
.	O

How	O
do	O
I	O
avoid	O
that	O
and	O
rather	O
generate	O
it	O
in	O
a	O
sparse	O
matrix	O
CSR	O
format	O
?	O

I	O
download	O
and	O
scrape	O
a	O
webpage	O
for	O
some	O
data	O
in	O
TSV	O
format	O
.	O

You	O
can	O
set	O
parameter	O
`	O
labels=False	O
`	O
to	O
get	O
the	O
integer	O
representation	O
#CODE	O

it's	O
not	O
too	O
much	O
of	O
a	O
stretch	O
to	O
insert	O
NaN's	O
into	O
the	O
data	O
using	O
reindexing	O
so	O
that	O
i	O
get	O
this	O
:	O
#CODE	O

Any	O
suggestions	O
?	O

Data-driven	O
DOM	O
manipulation	O
(	O
maybe	O
the	O
hardest	O
thing	O
to	O
wrap	O
one's	O
head	O
around	O
):	O
your	O
data	O
gets	O
transformed	O
into	O
DOM	O
elements	O
.	O

Your	O
regex	O
is	O
matching	O
on	O
all	O
`	O
-	O
`	O
characters	O
:	O
#CODE	O

1	O
)	O
create	O
additional	O
columns	O
with	O
clock	O
time	O
headings	O
for	O
5	O
minute	O
intervals	O
between	O
9:30	O
and	O
4:00	O
pm	O
,	O
so	O
the	O
headings	O
of	O
the	O
data	O
frame	O
look	O
like	O
:	O

`	O
Index	O
([	O
u'id	O
opinion	O
']	O
,	O
dtype=	O
'	O
object	O
')`	O
Thanks	O
for	O
the	O
response	O

The	O
end	O
product	O
would	O
be	O
ten	O
timeseries	O
plots	O
with	O
charted	O
lines	O
over	O
time	O
for	O
each	O
TID	O
.	O

And	O
get	O
the	O
result	O
:	O
#CODE	O

However	O
,	O
I	O
still	O
don't	O
get	O
why	O
`	O
iconv	O
`	O
messes	O
it	O
up	O
.	O

If	O
you	O
have	O
huge	O
CSV	O
data	O
,	O
NYSOL's	O
mcmd	O
is	O
the	O
best	O
.	O

I	O
get	O
#CODE	O

If	O
I	O
use	O
a	O
tweaked	O
version	O
of	O
@USER	O
'	O
s	O
suggestion	O
below	O
,	O
I	O
get	O
this	O
error	O
:	O
#CODE	O

ValueError	O
:	O
Unknown	O
format	O
code	O
'	O
f	O
'	O
for	O
object	O
of	O
type	O
'	O
str	O
'	O
-	O
why	O
do	O
I	O
get	O
this	O
the	O
second	O
time	O
but	O
not	O
the	O
first	O
time	O
?	O

Any	O
suggestion	O
about	O
the	O
reason	O
?	O

I	O
have	O
a	O
data	O
set	O
which	O
has	O
multiple	O
columns	O
,	O
strings	O
and	O
integers	O

is	O
the	O
condition	O
,	O
returning	O
a	O
booleans	O
array	O
of	O
True	O
/	O
False	O
for	O
all	O
values	O
meeting	O
the	O
condition	O
or	O
not	O
,	O
and	O
then	O
the	O
corresponding	O
A	O
values	O
are	O
selected	O

I	O
fixed	O
this	O
bug	O
in	O
0.11-dev	O
in	O
any	O
event	O
,	O
see	O
here	O
:	O
#URL	O
thanks	O
!	O

To	O
split	O
`	O
my_data2	O
`	O
into	O
two	O
arrays	O
of	O
roughly	O
equal	O
size	O
:	O
#CODE	O

to	O
get	O
a	O
`	O
Series	O
`	O
of	O
`	O
list	O
`	O
s	O
of	O
strings	O
.	O

For	O
example	O
,	O
you	O
can't	O
sum	O
a	O
mix	O
of	O
strings	O
and	O
floats	O
in	O
pandas	O
but	O
Excel	O
would	O
silently	O
drop	O
the	O
string	O
value	O
and	O
sum	O
the	O
floats	O
.	O

Notice	O
how	O
the	O
values	O
in	O
the	O
second	O
column	O
are	O
no	O
longer	O
integers	O
,	O
as	O
they	O
were	O
originally	O
.	O

I	O
have	O
a	O
large	O
but	O
very	O
sparse	O
matrix	O
(	O
50,000	O
rows*	O
100,000	O
columns	O
,	O
only	O
10%	O
of	O
the	O
values	O
are	O
known	O
)	O
.	O

In	O
python	O
normally	O
you	O
don't	O
need	O
and	O
you	O
shouldn't	O
use	O
a	O
semicolon	O
at	O
the	O
end	O
of	O
the	O
line	O
.	O

That's	O
all	O
data	O
python	O
is	O
reading	O
in	O
,	O
apparently	O
:	O
the	O
16	O
first	O
lines	O
,	O
or	O
at	O
least	O
I	O
am	O
not	O
able	O
to	O
get	O
the	O
rest	O
of	O
data	O
in	O
.	O

The	O
problem	O
is	O
to	O
find	O
average	O
values	O
of	O
temp1	O
,	O
temp2	O
and	O
temp3	O
for	O
a	O
period	O
of	O
time	O
(	O
say	O
,	O
2	O
days	O
)	O
over	O
the	O
same	O
intervals	O
(	O
for	O
that	O
example	O
-	O
15	O
minutes	O
)	O
.	O

In	O
generally	O
I	O
wonder	O
if	O
pandas	O
should	O
not	O
at	O
least	O
throw	O
a	O
warning	O
,	O
afterall	O
broadcasting	O
the	O
result	O
to	O
both	O
columns	O
should	O
be	O
almost	O
never	O
what	O
is	O
wanted	O
.	O

I	O
get	O
pandas	O
error	O
when	O
I	O
try	O
to	O
read	O
HDF5	O
format	O
files	O
that	O
I	O
have	O
created	O
with	O
h5py	O
.	O

Additionally	O
you	O
can	O
use	O
numpys	O
matrix	O
#CODE	O

I	O
updated	O
pandas	O
'	O
sudo	O
pip	O
install	O
--	O
upgrade	O
pandas	O
'	O
,	O
between	O
both	O
of	O
these	O
fixes	O
,	O
everything	O
worked	O
.	O

Sorry	O
can't	O
reproduce	O
nor	O
understand	O
your	O
real	O
problem	O
,	O
please	O
post	O
what	O
you	O
see	O
in	O
your	O
question	O

When	O
I	O
used	O
'	O
ethnicity	O
'	O
or	O
'	O
veteran	O
'	O
as	O
a	O
value	O
my	O
results	O
came	O
out	O
really	O
strange	O
and	O
didn't	O
match	O
my	O
value	O
counts	O
numbers	O
.	O

`	O
post_start	O
`	O
is	O
the	O
date	O
that	O
the	O
employee	O
started	O
in	O
the	O
post	O
,	O
and	O
`	O
change_date	O
`	O
is	O
the	O
date	O
that	O
the	O
post	O
title	O
was	O
changed	O
.	O

How	O
do	O
I	O
replace	O
the	O
ints	O
with	O
the	O
float	O
values	O
from	O
another	O
column	O
(	O
by	O
same	O
row	O
)	O
,	O
but	O
leave	O
all	O
the	O
nulls	O
?	O

There	O
may	O
be	O
a	O
more	O
foolproof	O
,	O
cleaner	O
way	O
of	O
computing	O
date	O
time	O
differences	O
in	O
pandas	O
.	O

However	O
,	O
to	O
get	O
the	O
row	O
sum	O
,	O
one	O
needs	O
to	O
specify	O
axis=1	O
.	O

Using	O
the	O
second	O
method	O
I	O
get	O
the	O
following	O
error	O
:	O
#CODE	O

Filter	O
data	O
to	O
get	O
only	O
first	O
day	O
of	O
the	O
month	O
rows	O

(	O
FYI	O
if	O
i	O
insert	O
a	O
print	O
print	O
(	O
vals	O
)	O
in	O
the	O
middle	O
of	O
that	O
loop	O
,	O
it	O
prints	O
#CODE	O

For	O
days	O
in	O
a	O
month	O
(	O
'	O
2015-07	O
'	O
say	O
)	O
You	O
could	O
change	O
#CODE	O

Doesnt	O
the	O
frame	O
variable	O
get	O
overwritten	O
during	O
each	O
iteration	O
in	O
the	O
loop	O
?	O

Any	O
other	O
advice	O
I	O
can	O
leverage	O
in	O
the	O
meantime	O
?	O

If	O
`	O
Change	O
Closing	O
Date	O
`	O
is	O
True	O
,	O
I	O
would	O
like	O
to	O
add	O
`	O
Closing	O
Date2	O
`	O
column	O
into	O
my	O
new	O
column	O
with	O
adding	O
1	O
year	O
.	O

If	O
you	O
REALLY	O
want	O
to	O
get	O
by	O
a	O
group	O
individually	O
#CODE	O

but	O
I	O
get	O
the	O
error	O
:	O
#CODE	O

I	O
am	O
new	O
to	O
pandas	O
for	O
data	O
analysis	O
and	O
I	O
just	O
installed	O
pandas	O
with	O
required	O
dependencies	O
(	O
NumPy	O
,	O
python-dateutil	O
,	O
pytz	O
,	O
numexpr	O
,	O
bottleneck	O
and	O
matplotlib	O
)	O
.	O

What	O
do	O
you	O
get	O
if	O
you	O
print	O
that	O
?	O

Can't	O
you	O
use	O
sets	O
and	O
intersections	O
?	O

is	O
there	O
a	O
way	O
to	O
insert	O
`	O
s	O
`	O
into	O
`	O
df	O
`	O
without	O
creating	O
a	O
reindexed	O
copy	O
of	O
`	O
df	O
`	O
first	O
?	O

I'm	O
using	O
python	O
2.7.5	O
(	O
with	O
all	O
the	O
packages	O
in	O
the	O
python	O
(	O
x	O
,	O
y	O
)	O
bundle	O
)	O
,	O
and	O
running	O
files	O
from	O
the	O
command	O
prompt	O
.	O

Any	O
suggestions	O
??	O

This	O
will	O
never	O
get	O
the	O
similar	O
graph	O
as	O
the	O
kernel	O
estimate	O
base	O
of	O
the	O
original	O
data	O
,	O
result	O
:	O

The	O
working	O
version	O
I	O
have	O
is	O
this	O
one	O
,	O
but	O
I	O
feel	O
there	O
is	O
potential	O
for	O
improvement	O
,	O
as	O
I	O
find	O
my	O
solution	O
unreadable	O
and	O
I	O
am	O
unsure	O
about	O
how	O
it	O
would	O
generalize	O
to	O
multiindexes	O
#CODE	O

Also	O
,	O
once	O
you	O
get	O
to	O
15	O
points	O
,	O
you'll	O
be	O
able	O
to	O
upvote	O
as	O
well	O
.	O

You	O
can	O
then	O
get	O
the	O
last	O
first	O
value	O
by	O
forward	O
filling	O
`	O
first_values	O
`	O
,	O
reindexing	O
like	O
`	O
second_values	O
`	O
,	O
stacking	O
again	O
and	O
indexing	O
into	O
the	O
result	O
using	O
the	O
original	O
`'	O
time	O
'	O
,	O
'	O
second	O
'`	O
pairs	O
:	O
#CODE	O

how	O
do	O
i	O
avoid	O
creating	O
so	O
many	O
variables	O
as	O
I	O
add	O
columns	O
together	O
?	O

Any	O
suggestion	O
on	O
how	O
to	O
efficiently	O
achieve	O
this	O
?	O

I	O
get	O
:	O
#CODE	O

For	O
instance	O
,	O
I	O
can	O
compute	O
the	O
value	O
for	O
data	O
record	O
3	O
by	O
taking	O
`	O
len	O
(	O
set	O
([	O
4	O
,	O
4	O
,	O
6	O
,	O
12	O
]))`	O
which	O
gives	O
3	O
.	O

@USER	O
That's	O
a	O
great	O
suggestion	O
(	O
for	O
some	O
use-cases	O
)	O
it	O
should	O
be	O
its	O
own	O
answer	O
(	O
so	O
I	O
can	O
upvote	O
it	O
)	O
Though	O
it	O
does	O
need	O
tweak	O
to	O
multiply	O
by	O
100	O
.	O

python	O
how	O
to	O
sum	O
together	O
all	O
values	O
within	O
a	O
time	O
interval	O
in	O
datetime64	O
?	O

was	O
trying	O
to	O
do	O
a	O
"	O
for	O
i	O
in	O
range	O
(	O
len	O
(	O
results	O
))"	O
before	O
the	O
"	O
for	O
item	O
in	O
results	O
[	O
i	O
]"	O
that	O
you	O
did	O
but	O
not	O
working	O
for	O
me	O
...	O

But	O
,	O
on	O
the	O
other	O
hand	O
,	O
if	O
your	O
columns	O
aren't	O
in	O
the	O
same	O
order	O
,	O
then	O
my	O
suggestion	O
won't	O
work	O
.	O

When	O
I	O
execute	O
the	O
program	O
for	O
the	O
data	O
of	O
the	O
same	O
day	O
,	O
processor	O
time	O
becomes	O
long	O
from	O
the	O
same	O
point	O
.	O

I'm	O
new	O
to	O
pandas	O
,	O
python	O
,	O
and	O
scripting	O
in	O
general	O
,	O
so	O
am	O
still	O
getting	O
my	O
head	O
around	O
the	O
basics	O
.	O

You	O
can	O
,	O
for	O
example	O
,	O
use	O
interpolation	O
to	O
get	O
equally	O
spaced	O
datapoints	O
out	O
off	O
your	O
timeseries	O
.	O

What	O
I	O
was	O
hoping	O
for	O
was	O
to	O
add	O
up	O
all	O
of	O
the	O
frequencies	O
across	O
the	O
websites	O
and	O
to	O
create	O
two	O
columns	O
:	O
Column	O
A	O
with	O
the	O
word	O
,	O
and	O
Column	O
B	O
with	O
all	O
of	O
the	O
frequencies	O
added	O
together	O
.	O

It	O
does	O
not	O
work	O
without	O
dropping	O
index	O
.	O

Now	O
I	O
was	O
wondering	O
how	O
I	O
could	O
subtract	O
my	O
multi-year	O
timeseries	O
from	O
this	O
standard	O
year	O
,	O
in	O
order	O
to	O
get	O
a	O
timeseries	O
that	O
show	O
which	O
days	O
were	O
below	O
or	O
above	O
it's	O
standard	O
.	O

I	O
may	O
try	O
installing	O
an	O
older	O
version	O
to	O
find	O
out	O
what	O
was	O
actually	O
getting	O
calculated	O
.	O

Is	O
there	O
any	O
disadvantage	O
?	O

The	O
length	O
of	O
the	O
frame	O
is	O
over	O
2	O
million	O
rows	O
and	O
looping	O
to	O
extract	O
the	O
elements	O
I	O
need	O
is	O
a	O
poor	O
choice	O
.	O

edit	O
I	O
believe	O
'	O
endog	O
'	O
as	O
defined	O
is	O
incorrect-I	O
should	O
be	O
passing	O
the	O
values	O
for	O
which	O
I	O
want	O
to	O
predict	O
;	O
therefore	O
I've	O
created	O
a	O
date	O
range	O
of	O
12	O
periods	O
past	O
the	O
last	O
recorded	O
value	O
.	O

@USER	O
It	O
should	O
be	O
a	O
little	O
quicker	O
with	O
a	O
boolean	O
index	O
like	O
that	O
,	O
but	O
it	O
does	O
do	O
a	O
cast	O
(	O
timedelta	O
)	O
so	O
I'm	O
not	O
100%	O
sure	O
on	O
that	O
.	O

I	O
still	O
get	O
the	O
same	O
TypeError	O
message	O
using	O
the	O
line	O
you	O
suggest	O
.	O

Use	O
regex	O
with	O
`	O
python	O
`	O
engine	O
#CODE	O

(	O
it's	O
pretty	O
clear	O
that	O
`	O
id	O
`	O
maps	O
to	O
`	O
individual	O
`	O
,	O
but	O
I	O
would	O
clean	O
that	O
up	O
too	O
)	O
.	O

Being	O
able	O
to	O
quickly	O
determine	O
the	O
time	O
difference	O
between	O
Order	O
1	O
and	O
Order	O
2	O
(	O
per	O
PersonID	O
)	O
would	O
be	O
great	O
too	O
.	O

Thus	O
,	O
if	O
there	O
is	O
an	O
update	O
to	O
some	O
value	O
on	O
a	O
memory	O
page	O
,	O
that	O
page	O
is	O

and	O
make	O
this	O
a	O
Series	O
,	O
mapping	O
names	O
to	O
their	O
respective	O
numbers	O
:	O
#CODE	O

That	O
is	O
,	O
for	O
each	O
second	O
there	O
is	O
a	O
value	O
and	O
they	O
should	O
not	O
be	O
averaged	O
,	O
just	O
grouped	O
together	O
to	O
a	O
new	O
series	O
..	O

Specifically	O
,	O
in	O
this	O
case	O
,	O
I'd	O
only	O
like	O
to	O
drop	O
row	O
with	O
Indices	O
'	O
1991-12-31	O
'	O
and	O
'	O
1992-01-31	O
'	O
.	O

Or	O
read	O
it	O
in	O
directly	O
as	O
a	O
csv	O
,	O
by	O
appending	O
'	O
na	O
'	O
to	O
the	O
list	O
of	O
values	O
to	O
be	O
considered	O
NaN	O
:	O
#CODE	O

I	O
fail	O
to	O
see	O
the	O
corelation	O
between	O
"	O
John	O
"	O
and	O
the	O
dates	O
in	O
the	O
target	O
.	O

I	O
get	O
:	O

The	O
question	O
is	O
,	O
how	O
can	O
I	O
remove	O
or	O
filter	O
out	O
all	O
entries	O
that	O
have	O
frequency	O
1	O
?	O

For	O
all	O
the	O
other	O
names	O
that	O
are	O
not	O
in	O
the	O
top	O
ten	O
frequencies	O
I	O
want	O
to	O
combine	O
their	O
number	O
of	O
occurences	O
together	O
under	O
say	O
the	O
name	O
"	O
other	O
"	O
.	O

You	O
should	O
get	O
the	O
following	O
result	O
:	O

Which	O
indeed	O
is	O
longer	O
(	O
50	O
)	O
than	O
my	O
number	O
of	O
columns	O
/	O
indices	O
(	O
25	O
)	O
.	O

I	O
am	O
new	O
to	O
Python	O
(	O
and	O
programming	O
in	O
general	O
!	O
)	O
,	O
trying	O
to	O
conduct	O
some	O
data	O
analysis	O
using	O
Pandas	O
.	O

I	O
would	O
like	O
to	O
combine	O
these	O
columns	O
into	O
start	O
time	O
(	O
index	O
)	O
and	O
length	O
in	O
actual	O
seconds	O
.	O

I'm	O
looking	O
to	O
find	O
,	O
for	O
each	O
Census	O
Block	O
centroid	O
,	O
the	O
distance	O
to	O
it's	O
closest	O
restaurant	O
.	O

You	O
will	O
get	O
the	O
exception	O
"	O
appended	O
items	O
do	O
not	O
match	O
existing	O
items	O
in	O
table	O
!	O

Honestly	O
-	O
we	O
were	O
going	O
to	O
originally	O
do	O
visualizations	O
with	O
it	O
(	O
heatmaps	O
)	O
-	O
but	O
for	O
a	O
lot	O
of	O
reasons	O
we're	O
now	O
going	O
to	O
use	O
D3	O
...	O

For	O
example	O
,	O
if	O
I	O
say	O
year	O
,	O
the	O
entire	O
column	O
needs	O
to	O
be	O
appended	O
into	O
a	O
list	O
like	O
[	O
1	O
year	O
,	O
3	O
minutes	O
,	O
2	O
hours	O
]	O
.	O

Anyone	O
have	O
any	O
suggestions	O
for	O
how	O
to	O
accomplish	O
this	O
?	O

Yeah	O
I	O
know	O
it	O
gives	O
NaN	O
padding	O
,	O
but	O
only	O
on	O
the	O
indices	O
the	O
joining	O
is	O
done	O
over	O
.	O

The	O
paired	O
measurements	O
should	O
have	O
the	O
same	O
month	O
,	O
just	O
different	O
years	O
.	O

You	O
can	O
get	O
started	O
on	O
debugging	O
this	O
by	O
just	O
adding	O
a	O
line	O
to	O
your	O
code	O
and	O
running	O
again	O
:	O
#CODE	O

When	O
I	O
run	O
the	O
solution	O
I	O
get	O
the	O
error	O
.	O

Then	O
let's	O
add	O
a	O
helper	O
column	O
,	O
called	O
Safe	O
,	O
that	O
will	O
be	O
a	O
concatenation	O
of	O
all	O
the	O
Safex	O
columns	O
.	O

product	O
1111	O
non-null	O
object	O

In	O
R	O
,	O
using	O
the	O
car	O
package	O
,	O
there	O
is	O
a	O
useful	O
function	O
`	O
some	O
(	O
x	O
,	O
n	O
)`	O
which	O
is	O
similar	O
to	O
head	O
but	O
selects	O
,	O
in	O
this	O
example	O
,	O
10	O
rows	O
at	O
random	O
from	O
x	O
.	O

The	O
separator	O
(	O
between	O
cells	O
)	O
is	O
defined	O
by	O
the	O
operating	O
system	O
(	O
at	O
least	O
under	O
Windows	O
)	O
,	O
and	O
when	O
the	O
system	O
wide	O
list	O
separator	O
differs	O
from	O
comma	O
,	O
pandas	O
(	O
or	O
anything	O
else	O
I	O
tried	O
)	O
cannot	O
determine	O
what	O
separator	O
should	O
be	O
used	O
.	O

Setting	O
up	O
a	O
histogram	O
with	O
a	O
range	O
and	O
an	O
appropriate	O
bin	O
size	O
is	O
an	O
unknown	O
.	O

Thanks	O
TravisJ	O
,	O
I	O
guess	O
I	O
was	O
just	O
struggling	O
to	O
get	O
the	O
(	O
...	O
something	O
involving	O
group	O
...	O
)	O
in	O
when	O
i	O
was	O
using	O
the	O
ax=fig1	O
....	O
method	O
.	O

I	O
am	O
optimising	O
the	O
span	O
of	O
an	O
exponential	O
moving	O
average	O
and	O
the	O
number	O
of	O
lagged	O
variables	O
that	O
I	O
use	O
in	O
the	O
regression	O
.	O

The	O
error	O
message	O
that	O
I	O
get	O
is	O
:	O
#CODE	O

It	O
doesn't	O
however	O
take	O
advantage	O
of	O
the	O
psql	O
package	O
in	O
Pandas	O
.	O

On	O
a	O
much	O
larger	O
data	O
set	O
,	O
this	O
runs	O
in	O
790	O
ms	O
compared	O
to	O
1345	O
ms	O
for	O
ajcr's	O
and	O
Primer's	O
solutions	O
.	O

I've	O
put	O
together	O
one	O
approach	O
to	O
that	O
solution	O
that	O
should	O
scale	O
relatively	O
well	O
.	O

I	O
was	O
hoping	O
there	O
was	O
an	O
easy	O
way	O
to	O
get	O
the	O
set	O
of	O
B	O
values	O
per	O
each	O
A	O
value	O
like	O
`	O
{	O
'	O
one	O
'	O
:[	O
'	O
A	O
'	O
,	O
'	O
B	O
']	O
,	O
'	O
two	O
'	O
:[	O
'	O
A	O
']	O
,	O
'	O
three	O
'	O
:[	O
'	O
B	O
']	O
}	O
`	O
but	O
I	O
don't	O
see	O
anything	O
like	O
that	O
in	O
the	O
pandas	O
documentation	O

To	O
avoid	O
chained	O
indexing	O
,	O
you	O
need	O
to	O
get	O
all	O
your	O
conditions	O
into	O
a	O
single	O
set	O
of	O
brackets	O
.	O

But	O
trying	O
to	O
parse	O
the	O
column	O
name	O
and	O
hierarchy	O
and	O
auto-generate	O
the	O
insertable	O
thing	O
with	O
matching	O
index	O
is	O
unpleasant	O
.	O

The	O
seaborn	O
package	O
will	O
allow	O
you	O
to	O
plot	O
long	O
form	O
data	O
like	O
you	O
have	O
without	O
pivoting	O
but	O
pandas	O
requires	O
shared	O
index	O
and	O
one	O
column	O
per	O
plotted	O
line	O
by	O
default	O
so	O
your	O
solution	O
is	O
the	O
correct	O
one	O
.	O

Unable	O
to	O
filter	O
out	O
missing	O
(	O
NaN	O
)	O
location	O
data	O
while	O
using	O
Pandas	O
and	O
Geocoder	O
modules	O
in	O
Python	O

problem	O
is	O
the	O
sum	O
i	O
now	O
get	O
is	O
lined	O
up	O
in	O
week	O
intervals	O
but	O
not	O
in	O
the	O
right	O
sequence	O
.	O
this	O
wouldn't	O
be	O
a	O
problem	O
but	O
i	O
need	O
to	O
get	O
to	O
the	O
month	O
of	O
each	O
date	O
in	O
order	O
to	O
do	O
the	O
next	O
step	O
i	O
guess	O
.	O

How	O
could	O
I	O
sum	O
consecutive	O
day	O
values	O
here	O
,	O
so	O
I	O
would	O
get	O
something	O
like	O
this	O
?	O

I	O
changed	O
this	O
to	O
use	O
\t	O
as	O
the	O
separator	O
.	O

It's	O
possible	O
,	O
but	O
if	O
your	O
data	O
is	O
organized	O
it's	O
very	O
quick	O
with	O
shifting	O
it	O

@USER	O
fixed	O
,	O
was	O
a	O
typo	O
;	O
this	O
take	O
a	O
full	O
uri	O

Just	O
get	O
rid	O
of	O
it	O
and	O
reindent	O
the	O
loop	O
body	O
to	O
the	O
left	O
one	O
level	O
.	O

However	O
,	O
I	O
still	O
get	O
the	O
warning	O
.	O

For	O
any	O
x	O
in	O
dataset2	O
it	O
has	O
mapped	O
value	O
in	O
col2	O
.	O

but	O
you	O
then	O
need	O
to	O
store	O
these	O
dfs	O
somewhere	O
which	O
means	O
either	O
in	O
a	O
list	O
or	O
tuple	O
or	O
some	O
other	O
container	O
or	O
use	O
a	O
generator	O

and	O
so	O
on	O
for	O
the	O
remaining	O
location	O
categories	O
.	O

@USER	O
do	O
**	O
all	O
**	O
the	O
columns	O
in	O
the	O
DF	O
require	O
that	O
same	O
replacement	O
?	O

did	O
you	O
get	O
any	O
warnings	O
while	O
installing	O
numpy	O
or	O
pandas	O
?	O

This	O
`	O
df	O
`	O
consist	O
of	O
volume	O
observations	O
at	O
every	O
10	O
second	O
for	O
22	O
non-consecutive	O
days	O
.	O

I've	O
also	O
included	O
a	O
section	O
to	O
immediately	O
identify	O
any	O
redundant	O
genes	O
that	O
don't	O
have	O
any	O
SNPs	O
that	O
fall	O
within	O
their	O
range	O
.	O

Using	O
some	O
string	O
formatting	O
to	O
get	O
the	O
index	O
,	O
but	O
works	O
for	O
any	O
combination	O
of	O
months	O
(	O
as	O
long	O
as	O
the	O
first	O
month	O
is	O
explicitly	O
included	O
)	O
.	O

Any	O
suggestions	O
please	O
?	O

I	O
get	O
:	O
#CODE	O

I	O
understand	O
that	O
we	O
can	O
line	O
them	O
all	O
together	O
side	O
by	O
side	O
so	O
their	O
dates	O
match	O
and	O
loop	O
row	O
by	O
row	O
,	O
but	O
then	O
when	O
i	O
have	O
100k	O
different	O
securities	O
,	O
this	O
is	O
slow	O
in	O
memory	O
.	O

I	O
would	O
like	O
to	O
automate	O
this	O
table	O
so	O
If	O
I	O
change	O
my	O
parameters	O
in	O
my	O
code	O
,	O
I	O
get	O
a	O
new	O
table	O
with	O
that	O
new	O
data	O
.	O

But	O
I	O
get	O
,	O
which	O
I	O
cannot	O
understand	O
,	O
#CODE	O

Here's	O
the	O
product	O
:	O
#CODE	O

(	O
My	O
actual	O
problem	O
involves	O
parsing	O
strings	O
into	O
lists	O
,	O
then	O
checking	O
for	O
presents	O
of	O
a	O
1	O
or	O
0	O
in	O
one	O
list	O
and	O
if	O
so	O
marking	O
the	O
cosponsoring	O
element	O
in	O
the	O
other	O
list	O
with	O
a	O
asterix	O
,	O
but	O
I	O
didn't	O
want	O
to	O
put	O
that	O
in	O
my	O
example	O
and	O
it	O
is	O
long	O
and	O
harder	O
to	O
follow	O
.	O

What	O
is	O
the	O
error	O
you	O
get	O
?	O

So	O
first	O
chunk	O
is	O
stored	O
as	O
integer	O
and	O
in	O
second	O
chunk	O
gets	O
NaN	O
values	O
and	O
store	O
cannot	O
convert	O
NaN	O
to	O
integer	O

Just	O
play	O
around	O
with	O
it	O
to	O
get	O
it	O
right	O
.	O

I	O
get	O
the	O
following	O
error	O
:	O
#CODE	O

Edit	O
:	O
Here's	O
an	O
example	O
of	O
generating	O
a	O
close-enough	O
data	O
set	O
,	O
so	O
you	O
can	O
get	O
some	O
idea	O
of	O
what	O
I	O
mean	O
:	O
#CODE	O

@USER	O
that	O
means	O
you	O
have	O
non	O
string	O
values	O
mixed	O
in	O
;	O
you	O
need	O
to	O
specify	O
`	O
na=True	O
`	O
or	O
`	O
na=False	O
`	O
depending	O
on	O
what	O
those	O
values	O
are	O
.	O
see	O
my	O
edits	O
.	O

Preferably	O
,	O
use	O
Pandas	O
for	O
the	O
data	O
structure	O
and	O
Python	O
for	O
the	O
language	O
.	O

Then	O
as	O
in	O
the	O
first	O
point	O
,	O
I	O
would	O
like	O
to	O
calculate	O
the	O
number	O
of	O
continuous	O
up	O
and	O
down	O
sequences	O
from	O
the	O
previous	O
point	O
.	O

0	O
can	O
be	O
changed	O
to	O
1	O
or	O
other	O
values	O
later	O
in	O
the	O
code	O

and	O
in	O
the	O
instance	O
when	O
i	O
am	O
able	O
to	O
set	O
the	O
index	O
of	O
the	O
df	O
to	O
the	O
range	O
,	O
the	O
data	O
in	O
the	O
4	O
columns	O
change	O
to	O
NaN	O
since	O
they	O
have	O
no	O
data	O
that	O
matches	O
the	O
new	O
index	O
.	O

(	O
Note	O
that	O
this	O
produces	O
an	O
unrealistically	O
high	O
number	O
of	O
flooding	O
events	O
,	O
but	O
that's	O
just	O
because	O
of	O
how	O
the	O
sample	O
data	O
is	O
set	O
up	O
and	O
not	O
reflective	O
of	O
a	O
typical	O
pond	O
,	O
though	O
I'm	O
not	O
an	O
expert	O
on	O
pond	O
flooding	O
!	O
)	O
#CODE	O

What	O
output	O
do	O
you	O
get	O
from	O
this	O
?	O

`	O
Ideally	O
,	O
for	O
the	O
pages	O
that	O
have	O
multiple	O
groups	O
of	O
34	O
,	O
i'd	O
like	O
to	O
add	O
a	O
suffix	O
of	O
_1	O
,	O
_2	O
,	O
_3	O
,	O
etc	O
.	O

That	O
means	O
duplicating	O
values	O
from	O
cols	O
`	O
product_id	O
`	O
and	O
tem_name	O
`	O
as	O
long	O
as	O
there	O
are	O
items	O
in	O
list	O
`	O
prices	O
`	O
.	O

cool	O
,	O
but	O
I	O
get	O
a	O
syntax	O
error	O
for	O

problem	O
is	O
the	O
sum	O
i	O
now	O
get	O
is	O
lined	O
up	O
in	O
week	O
intervals	O
but	O
not	O
in	O
the	O
right	O
sequence	O
.	O
this	O
wouldn't	O
be	O
a	O
problem	O
but	O
i	O
need	O
to	O
get	O
to	O
the	O
month	O
of	O
each	O
date	O
in	O
order	O
to	O
do	O
the	O
next	O
step	O
i	O
guess	O
.	O

For	O
empty	O
date	O
cells	O
I	O
am	O
inserting	O
a	O
NaT	O
,	O
which	O
I	O
would	O
have	O
thought	O
would	O
be	O
fine	O
,	O
but	O
in	O
Oracle	O
that	O
is	O
becoming	O
some	O
weird	O
invalid	O
time	O
that	O
displays	O
as	O
"	O
0001-255-255	O
00:00	O
:	O
00	O
"	O
(	O
Something	O
like	O
MAXINT	O
or	O
0	O
being	O
converted	O
to	O
a	O
timestamp	O
I'm	O
guessing	O
?	O
)	O
#CODE	O

I	O
would	O
like	O
to	O
get	O
the	O
following	O
result	O
:	O
#CODE	O

so	O
in	O
all	O
both	O
suggestions	O
below	O
worked	O
for	O
me	O
:	O

I	O
get	O
the	O
following	O
error	O
message	O
:	O
#CODE	O

I	O
would	O
suggest	O
using	O
the	O
duplicated	O
method	O
on	O
the	O
Pandas	O
Index	O
itself	O
:	O
#CODE	O

I	O
would	O
like	O
to	O
group	O
the	O
x	O
values	O
into	O
equal	O
size	O
bins	O
,	O
and	O
for	O
each	O
bin	O
take	O
the	O
average	O
value	O
of	O
both	O
x	O
and	O
y	O
.	O

For	O
this	O
data	O
set	O
the	O
two	O
numbers	O
are	O
always	O
equal	O
.	O

Do	O
you	O
want	O
to	O
check	O
if	O
the	O
value	O
is	O
in	O
the	O
provided	O
bounds	O
and	O
return	O
a	O
boolean	O
True	O
/	O
False	O
array	O
,	O
or	O
you	O
want	O
to	O
represent	O
your	O
values	O
in	O
categories	O
represented	O
by	O
those	O
bounds	O
?	O

The	O
series	O
I'd	O
like	O
to	O
get	O
would	O
contain	O
:	O
#CODE	O

The	O
error	O
I	O
get	O
:	O
#CODE	O

How	O
do	O
you	O
deal	O
with	O
apparently	O
overlapping	O
date	O
ranges	O
?	O

There	O
are	O
more	O
columns	O
in	O
the	O
data	O
that	O
are	O
not	O
shown	O
above	O
,	O
and	O
using	O
this	O
code	O
causes	O
the	O
non-numeric	O
columns	O
to	O
drop	O
off	O
.	O

Any	O
suggestions	O
?	O

python	O
-	O
trying	O
to	O
get	O
a	O
new	O
pandas	O
release	O

In	O
the	O
process	O
of	O
creating	O
an	O
example	O
with	O
code	O
,	O
I	O
managed	O
to	O
get	O
it	O
working	O
.	O

Can	O
you	O
post	O
raw	O
data	O
and	O
example	O
code	O
that	O
demonstrates	O
this	O
'	O
cutting	O
'	O
off	O

Running	O
your	O
code	O
on	O
the	O
sample	O
data	O
produces	O
the	O
same	O
result	O
.	O

I	O
want	O
to	O
take	O
advantage	O
of	O
sortedness	O
since	O
with	O
very	O
large	O
series	O
merging	O
when	O
we	O
know	O
it's	O
sorted	O
should	O
be	O
linear	O
in	O
total	O
length	O
of	O
the	O
arrays	O
,	O
whereas	O
any	O
sort	O
will	O
be	O
non-linear	O
.	O

What	O
if	O
you	O
just	O
changed	O
the	O
index	O
from	O
date	O
/	O
status	O
to	O
date	O
/	O
var1	O
/	O
status	O
?	O

which	O
when	O
imported	O
into	O
pandas	O
data	O
frames	O
and	O
each	O
joined	O
to	O
a	O
common	O
timestamp	O
,	O
with	O
a	O
day	O
of	O
year	O
field	O
added	O
,	O
so	O
looking	O
something	O
like	O
:	O
#CODE	O

This	O
works	O
only	O
if	O
your	O
object	O
is	O
table	O
format	O
(	O
rather	O
than	O
fixed	O
format	O
)	O
.	O

Drop	O
range	O
of	O
columns	O
by	O
labels	O

You	O
could	O
put	O
```	O
[	O
'	O
col1	O
']```	O
at	O
the	O
end	O
to	O
get	O
an	O
int	O
.	O

Hence	O
,	O
the	O
width	O
of	O
each	O
bin	O
over	O
the	O
interval	O
from	O
[	O
-1	O
,	O
1	O
]	O
should	O
be	O
`	O
2	O
/	O
10=	O
0.20	O
`	O
;	O
however	O
,	O
the	O
graph	O
does	O
not	O
have	O
any	O
bins	O
with	O
a	O
width	O
of	O
0.20	O
.	O

For	O
encoding	O
training	O
data	O
you	O
can	O
use	O
fit_transform	O
which	O
will	O
discover	O
the	O
category	O
labels	O
and	O
create	O
appropriate	O
dummy	O
variables	O
.	O

I	O
think	O
you're	O
confusing	O
how	O
to	O
filter	O
here	O
,	O
if	O
you're	O
looking	O
for	O
a	O
specific	O
value	O
then	O
`	O
stock	O
[	O
stock	O
[	O
'	O
Whs	O
']	O
==	O
'	O
VKO	O
']`	O
will	O
return	O
only	O
the	O
rows	O
where	O
that	O
condition	O
is	O
satisfied	O
,	O
for	O
your	O
last	O
part	O
the	O
reason	O
you	O
get	O
an	O
empty	O
row	O
is	O
that	O
you're	O
slicing	O
the	O
first	O
3	O
rows	O
and	O
then	O
comparing	O
the	O
first	O
value	O
with	O
your	O
string	O
but	O
the	O
first	O
string	O
value	O
is	O
'	O
VKO	O
'	O
and	O
not	O
'	O
ZZZ	O
'	O
,	O
you	O
should	O
do	O
this	O
:	O
`	O
stock	O
[	O
stock	O
[	O
'	O
Whs	O
']	O
==	O
'	O
ZZZ	O
']`	O
to	O
filter	O
the	O
resuls	O
first	O

I	O
had	O
changed	O
the	O
data	O
on	O
the	O
local	O
file	O
.	O

Note	O
that	O
this	O
is	O
slightly	O
different	O
as	O
we	O
are	O
returning	O
the	O
first	O
index	O
here	O
(	O
and	O
not	O
the	O
normally	O
returned	O
last	O
,	O
youy	O
could	O
do	O
either	O
)	O
.	O

I'm	O
not	O
sure	O
how	O
the	O
archive	O
block	O
reading	O
works	O
and	O
how	O
much	O
data	O
it	O
loads	O
into	O
memory	O
,	O
but	O
it's	O
clear	O
that	O
you	O
will	O
have	O
to	O
somehow	O
control	O
the	O
size	O
of	O
the	O
chunks	O
.	O

Regarding	O
nesting	O
of	O
functions	O
:	O
if	O
you	O
believe	O
a	O
function	O
has	O
a	O
general	O
purpose	O
or	O
is	O
reusable	O
,	O
then	O
it	O
should	O
be	O
defined	O
at	O
the	O
top	O
level	O
of	O
a	O
module	O
or	O
some	O
place	O
where	O
other	O
functions	O
can	O
call	O
it	O
.	O

I	O
get	O
an	O
error	O
saying	O
:	O
lambda	O
(	O
)	O
takes	O
exactly	O
1	O
argument	O
(	O
3	O
given	O
)	O

You	O
can	O
get	O
the	O
list	O
of	O
columns	O
with	O
:	O
#CODE	O

One	O
is	O
I	O
only	O
wanted	O
to	O
get	O
the	O
mean	O
of	O
the	O
next	O
rows	O
that	O
relate	O
to	O
the	O
same	O
group	O
.	O

By	O
default	O
this	O
will	O
add	O
a	O
column	O
of	O
integers	O
(	O
because	O
R	O
factors	O
are	O
encoded	O
as	O
integers	O
)	O
.	O

I	O
am	O
trying	O
to	O
calculate	O
the	O
percent	O
change	O
by	O
month	O
for	O
each	O
product	O
.	O

If	O
i	O
want	O
only	O
USA	O
Equities	O
vs	O
all	O
other	O
equity	O
and	O
not	O
the	O
enitre	O
89	O
columns	O
how	O
do	O
i	O
do	O
it	O
?	O

Pandas	O
:	O
Efficient	O
way	O
to	O
get	O
first	O
row	O
with	O
element	O
that	O
is	O
smaller	O
than	O
a	O
given	O
value	O

I	O
first	O
thought	O
this	O
was	O
a	O
spacing	O
issue	O
in	O
the	O
columns	O
values	O
,	O
so	O
I	O
replaced	O
them	O
with	O
underscores	O
,	O
but	O
it	O
also	O
doesn't	O
work	O
in	O
columns	O
which	O
only	O
contain	O
a	O
single	O
word	O
and	O
no	O
spaces	O
?	O

Are	O
you	O
getting	O
the	O
values	O
from	O
the	O
GUI	O
ok	O
,	O
but	O
your	O
calculations	O
are	O
returning	O
nothin	O
?	O

This	O
is	O
not	O
precisely	O
what	O
I'm	O
after	O
,	O
but	O
I	O
think	O
I'll	O
have	O
to	O
write	O
a	O
loop	O
top	O
get	O
that	O
.	O

Hence	O
,	O
the	O
width	O
of	O
each	O
bin	O
over	O
the	O
interval	O
from	O
[	O
-1	O
,	O
1	O
]	O
should	O
be	O
`	O
2	O
/	O
10=	O
0.20	O
`	O
;	O
however	O
,	O
the	O
graph	O
does	O
not	O
have	O
any	O
bins	O
with	O
a	O
width	O
of	O
0.20	O
.	O

I'm	O
having	O
a	O
problem	O
trying	O
to	O
get	O
a	O
character	O
count	O
column	O
of	O
the	O
string	O
values	O
in	O
another	O
column	O
,	O
and	O
haven't	O
figured	O
out	O
how	O
to	O
do	O
it	O
efficiently	O
.	O

I	O
think	O
I	O
get	O
the	O
idea	O
.	O

How	O
can	O
I	O
get	O
the	O
position	O
of	O
index	O
`	O
18	O
`	O
?	O

When	O
using	O
`	O
groupy-apply	O
`	O
,	O
instead	O
of	O
dropping	O
the	O
group	O
key	O
index	O
using	O
:	O
#CODE	O

No	O
real	O
advantage	O
if	O
there	O
are	O
just	O
two	O
categories	O
:	O
#CODE	O

@USER	O
I	O
suspect	O
something	O
else	O
is	O
going	O
on	O
then	O
,	O
because	O
when	O
I	O
memory	O
profile	O
this	O
way	O
(	O
with	O
`	O
drop	O
`)	O
using	O
the	O
snippet	O
that	O
Michael	O
Laszlo	O
posted	O
,	O
I	O
do	O
not	O
see	O
memory	O
growth	O
.	O

Of	O
course	O
you	O
may	O
not	O
like	O
the	O
index	O
as	O
tuples	O
;	O
you	O
could	O
reset	O
the	O
index	O
within	O
the	O
list	O
comprehension	O
to	O
get	O
the	O
following	O
if	O
you	O
prefer	O
(	O
for	O
example	O
,	O
this	O
if	O
for	O
part	O
1	O
):	O
#CODE	O

Here	O
is	O
how	O
I	O
am	O
trying	O
to	O
get	O
the	O
output	O
to	O
look	O
like	O
:	O
#CODE	O

memory	O
efficient	O
Python	O
(	O
pandas	O
)	O
aggregates	O
of	O
categories	O
from	O
one	O
csv	O
file	O
per	O
period	O

But	O
this	O
last	O
line	O
generate	O
the	O
error	O
message	O
:	O
`	O
no	O
item	O
named	O
timestamp	O
`	O
.	O

The	O
problem	O
here	O
,	O
well	O
the	O
biggest	O
one	O
,	O
is	O
that	O
your	O
`	O
data	O
`	O
is	O
string	O
,	O
not	O
valid	O
data	O
structure	O
,	O
same	O
thing	O
with	O
dictionary	O
inside	O
it	O
,	O
you	O
creating	O
strings	O
,	O
not	O
data	O
structures	O
.	O

I	O
also	O
want	O
to	O
create	O
a	O
new	O
column	O
that	O
shows	O
the	O
difference	O
in	O
days	O
between	O
the	O
end	O
and	O
begin	O
dates	O
.	O

In	O
the	O
next	O
column	O
(	O
B	O
)	O
,	O
I	O
want	O
to	O
create	O
an	O
indexed	O
series	O
that	O
begins	O
at	O
1000	O
based	O
on	O
the	O
percent	O
changes	O
.	O

I	O
can't	O
use	O
fixed	O
position	O
to	O
slice	O
it	O
.	O

This	O
gets	O
you	O
to	O
where	O
I	O
am	O
.	O

I	O
am	O
using	O
this	O
to	O
generate	O
nodes	O
in	O
a	O
graph	O
,	O
if	O
x1	O
,	O
x2	O
are	O
not	O
exactly	O
equal	O
,	O
networkx	O
recognizes	O
them	O
as	O
different	O
nodes	O
,	O
if	O
x1=x2	O
,	O
i	O
get	O
a	O
recombinant	O
tree	O
which	O
is	O
what	O
i	O
want	O
.	O

My	O
example	O
was	O
not	O
good	O
enough	O
,	O
as	O
your	O
script	O
smartly	O
took	O
the	O
size	O
from	O
the	O
length	O
of	O
the	O
df	O
.	O

iPython's	O
Rmagic	O
is	O
already	O
able	O
to	O
perform	O
an	O
automagic	O
conversion	O
between	O
the	O
two	O
in	O
a	O
number	O
of	O
situations	O
,	O
and	O
might	O
be	O
a	O
good	O
way	O
to	O
get	O
familiar	O
with	O
Python	O
.	O

So	O
if	O
your	O
dataset	O
is	O
really	O
large	O
,	O
perhaps	O
store	O
them	O
first	O
in	O
on-disk	O
database	O
/	O
HDF	O
rather	O
than	O
csv	O
file	O
and	O
sort	O
them	O
there	O
,	O
and	O
then	O
query	O
.	O

One	O
had	O
no	O
problem	O
at	O
all	O
(	O
the	O
xlsx	O
file	O
,	O
example	O
2	O
)	O
and	O
the	O
other	O
(	O
xls	O
,	O
example	O
1	O
)	O
had	O
a	O
difference	O
between	O
the	O
columns	O
.	O

then	O
with	O
the	O
`	O
sorted	O
`	O
function	O
and	O
`	O
datetime	O
`	O
module	O
(	O
remember	O
the	O
`	O
sorted	O
`	O
function	O
change	O
the	O
`	O
data	O
`	O
it	O
self	O
)	O
#CODE	O

BUT	O
,	O
I	O
get	O
the	O
"	O
SettingWithCopyWarning	O
"	O
:	O

This	O
is	O
machine	O
generated	O
data	O
.	O

You	O
can't	O
use	O
`	O
or	O
`	O
with	O
arrays	O
,	O
if	O
you	O
try	O
this	O
you	O
get	O
an	O
error	O
`	O
ValueError	O
:	O
The	O
truth	O
value	O
of	O
a	O
Series	O
is	O
ambiguous	O
.	O

Incidentally	O
,	O
this	O
is	O
the	O
same	O
result	O
that	O
you	O
would	O
get	O
with	O
the	O
Spearman	O
R	O
coefficient	O
as	O
well	O
.	O

How	O
could	O
I	O
do	O
to	O
have	O
exactly	O
one	O
calendar	O
year	O
between	O
dates	O
in	O
spite	O
of	O
leap	O
years	O
?	O

"	O
However	O
,	O
we	O
still	O
have	O
one	O
large	O
difference	O
.	O

All	O
that	O
remains	O
is	O
to	O
merged	O
the	O
contents	O
of	O
the	O
second-level	O
dictionaries	O
:	O
#CODE	O

I	O
am	O
trying	O
to	O
create	O
a	O
single	O
image	O
with	O
heatmaps	O
representing	O
the	O
correlation	O
of	O
features	O
of	O
data	O
points	O
for	O
each	O
label	O
separately	O
.	O

I	O
have	O
two	O
Series	O
which	O
have	O
a	O
format	O
equal	O
to	O
this	O
:	O
#CODE	O

However	O
,	O
when	O
I	O
do	O
the	O
following	O
,	O
the	O
error	O
does	O
not	O
show	O
up	O
and	O
I	O
get	O
the	O
expected	O
result	O
:	O
#CODE	O

As	O
brackets	O
are	O
part	O
of	O
the	O
regex	O
syntax	O
if	O
you're	O
trying	O
to	O
match	O
literal	O
brackets	O
you	O
need	O
to	O
escape	O
them	O
:	O
#CODE	O

Yeah	O
,	O
the	O
best	O
idea	O
I've	O
had	O
this	O
whole	O
time	O
was	O
to	O
actually	O
sign	O
up	O
to	O
SO	O
so	O
I	O
can	O
post	O
my	O
own	O
questions	O
instead	O
of	O
trying	O
to	O
funble	O
my	O
way	O
through	O
problems	O
by	O
patching	O
together	O
answers	O
to	O
other	O
peoples	O
questions	O
-	O
sometimes	O
what	O
I	O
need	O
just	O
isn't	O
covered	O
in	O
other	O
people's	O
questions	O
.	O

But	O
when	O
I	O
try	O
and	O
import	O
pandas	O
I	O
get	O
:	O
#CODE	O

However	O
,	O
I	O
can't	O
get	O
the	O
column	O
to	O
fill	O
up	O
with	O
the	O
appropriate	O
user	O
inputted	O
value	O
.	O

Also	O
I'm	O
not	O
getting	O
any	O
traceback	O
messages	O
but	O
I	O
think	O
I	O
have	O
an	O
idea	O
of	O
where	O
my	O
problem	O
may	O
be	O
.	O

Basically	O
I'm	O
trying	O
to	O
get	O
at	O
this	O
:	O
#CODE	O

So	O
this	O
is	O
not	O
a	O
fully	O
working	O
answer	O
,	O
but	O
maybe	O
it	O
can	O
be	O
extended	O
to	O
ultimatively	O
get	O
you	O
there	O
.	O

I	O
have	O
a	O
pandas	O
Series	O
holding	O
one	O
numpy	O
array	O
per	O
entry	O
(	O
same	O
length	O
for	O
all	O
entries	O
)	O
and	O
I	O
would	O
like	O
to	O
convert	O
this	O
to	O
a	O
2D	O
numpy	O
array	O
.	O

For	O
example	O
,	O
let's	O
say	O
I	O
want	O
to	O
select	O
50%	O
(	O
but	O
this	O
could	O
change	O
)	O
.	O

For	O
this	O
purpose	O
I	O
would	O
like	O
to	O
find	O
the	O
soonest	O
date	O
(	O
month	O
)	O
and	O
from	O
there	O
start	O
counting	O
months	O
and	O
their	O
averages	O
.	O

You've	O
changed	O
your	O
data	O
,	O
so	O
the	O
script	O
as	O
written	O
doesn't	O
work	O
.	O

Hopefully	O
you'll	O
get	O
an	O
answer	O
soon	O
.	O

This	O
should	O
get	O
you	O
to	O
the	O
point	O
in	O
your	O
code	O
where	O
you	O
start	O
dropping	O
columns	O
and	O
start	O
concatenating	O
.	O

So	O
,	O
if	O
k	O
were	O
equal	O
to	O
2	O
,	O
and	O
this	O
were	O
my	O
data	O
frame	O
:	O
#CODE	O

It	O
seems	O
as	O
though	O
the	O
second	O
approach	O
,	O
using	O
"	O
where	O
"	O
is	O
only	O
returning	O
data	O
from	O
the	O
last	O
few	O
appended	O
files	O
,	O
while	O
the	O
first	O
approach	O
is	O
returning	O
much	O
more	O
data	O
.	O

I	O
have	O
to	O
improve	O
this	O
to	O
get	O
rid	O
of	O
redundancy	O
,	O
and	O
I	O
am	O
not	O
sure	O
how	O
to	O
go	O
about	O
this	O
.	O

Are	O
you	O
able	O
to	O
post	O
raw	O
data	O
and	O
code	O
to	O
reproduce	O
this	O
issue	O
?	O

The	O
relative	O
size	O
between	O
consecutive	O
levels	O
.	O

This	O
will	O
potentially	O
cater	O
the	O
corner	O
cases	O
if	O
you	O
happen	O
to	O
have	O
conditions	O
like	O
:	O
"	O
value	O
"	O
360	O
then	O
+360	O
else	O
-360	O
but	O
the	O
sequence	O
of	O
the	O
update	O
will	O
cause	O
the	O
results	O
reapply	O
,	O
ie	O
.	O

I'll	O
put	O
an	O
example	O
of	O
what	O
I'm	O
suggesting	O
in	O
my	O
answer	O
.	O

But	O
you	O
said	O
you	O
want	O
only	O
the	O
time	O
points	O
from	O
the	O
longest	O
`	O
csv	O
`	O
.	O

replacing	O
this	O
in	O
code	O
just	O
drop	O
those	O
whole	O
rows	O
...	O

Thank	O
you	O
for	O
response	O
and	O
for	O
helping	O
me	O
get	O
to	O
next	O
level	O
of	O
pyhton	O
,	O
great	O
stuff	O
!	O

For	O
instance	O
,	O
you	O
can	O
insert	O
new	O
values	O
into	O
the	O
index	O
(	O
and	O
even	O
choose	O
what	O
value	O
it	O
should	O
have	O
):	O
#CODE	O

What	O
I	O
need	O
to	O
do	O
is	O
to	O
compute	O
the	O
average	O
temperature	O
for	O
every	O
run	O
,	O
averaging	O
all	O
the	O
temperature	O
measurements	O
belonging	O
to	O
a	O
run	O
.	O

@USER	O
I	O
get	O
`	O
Type	O
Error	O
:	O
'	O
bool	O
'	O
object	O
is	O
not	O
callable	O
`	O
when	O
I	O
do	O
that	O

For	O
your	O
specific	O
request	O
of	O
entries	O
between	O
12:00	O
to	O
13:00	O
for	O
every	O
single	O
day	O
,	O
you	O
can	O
fetch	O
the	O
rows	O
with	O
:	O
#CODE	O

Also	O
,	O
their	O
order	O
matters	O
(	O
they	O
are	O
sorted	O
by	O
decreasing	O
standard	O
deviation	O
across	O
rows	O
and	O
should	O
appear	O
in	O
this	O
order	O
in	O
the	O
heatmap	O
.	O
)	O

I'm	O
not	O
averse	O
the	O
reformatting	O
the	O
data	O
in	O
Pandas	O
-->	O
dumping	O
to	O
CSV	O
-->	O
importing	O
to	O
NetworkX	O
,	O
but	O
it	O
seems	O
as	O
if	O
I	O
should	O
be	O
able	O
to	O
generate	O
the	O
edges	O
from	O
the	O
index	O
and	O
the	O
nodes	O
from	O
the	O
values	O
.	O

What	O
is	O
the	O
simplest	O
way	O
to	O
get	O
a	O
sum	O
of	O
only	O
numbers	O
across	O
the	O
entire	O
frame	O
?	O

I	O
think	O
you	O
mean	O
a	O
Lorenz	O
plot	O
:	O
#URL	O
This	O
would	O
make	O
sense	O
then	O
as	O
it	O
requires	O
a	O
specific	O
preordering	O
of	O
the	O
data	O
.	O

Any	O
suggestions	O
?	O

Despite	O
the	O
title	O
,	O
similar	O
problems	O
can	O
occur	O
with	O
other	O
operating	O
systems	O
if	O
you	O
mix	O
32-bit	O
and	O
64-fit	O
versions	O
.	O

Use	O
`	O
select_as_coordinates	O
`	O
to	O
actually	O
execute	O
your	O
query	O
;	O
this	O
returns	O
an	O
`	O
Int64Index	O
`	O
of	O
the	O
row	O
number	O
(	O
the	O
coordinates	O
)	O
.	O

Somehow	O
create	O
a	O
mapping	O
so	O
that	O
instead	O
of	O
the	O
labels	O
being	O
29	O
,	O
30	O
etc	O
,	O
they	O
say	O
"	O
week	O
29	O
"	O
,	O
"	O
Week	O
30	O
"	O
etc	O
.	O

The	O
fix	O
you	O
describe	O
would	O
work	O
,	O
of	O
course	O
,	O
but	O
then	O
I	O
could	O
skip	O
pandas	O
all-together	O
and	O
directly	O
plot	O
the	O
results	O
of	O
my	O
individual	O
simulations	O
.	O

These	O
two	O
timezones	O
have	O
different	O
names	O
but	O
represent	O
the	O
same	O
thing	O
,	O
however	O

which	O
would	O
just	O
change	O
the	O
last	O
data	O
point	O
.	O

What	O
do	O
you	O
mean	O
by	O
reproducible	O
example	O
?	O

But	O
thought	O
i'd	O
make	O
it	O
clear	O
what	O
my	O
next	O
objective	O
was	O
,	O
in	O
case	O
someone	O
could	O
illuminate	O
a	O
better	O
method	O
to	O
get	O
there	O
.	O

What	O
output	O
do	O
you	O
get	O
when	O
you	O
just	O
enter	O
`	O
pd	O
`	O
in	O
the	O
console	O
?	O

When	O
I	O
train	O
on	O
each	O
label	O
I	O
get	O
et	O
better	O
than	O
73%	O
on	O
each	O
label	O
.	O

but	O
what	O
value	O
does	O
it	O
grab	O
when	O
indexing	O
?	O
in	O
other	O
words	O
,	O
if	O
i'm	O
just	O
testing	O
one	O
side	O
i'll	O
get	O
the	O
value	O
corresponding	O
to	O
that	O
row	O
if	O
true	O
.	O
since	O
both	O
sides	O
could	O
be	O
true	O
and	O
one	O
of	O
them	O
is	O
always	O
true	O
,	O
which	O
row's	O
values	O
will	O
be	O
selected	O
?	O

However	O
,	O
this	O
is	O
a	O
bug	O
as	O
you	O
should	O
get	O
an	O
error	O
.	O

guys	O
at	O
least	O
tell	O
me	O
why	O
i	O
am	O
getting	O
downvoted	O
?	O

In	O
some	O
cases	O
the	O
data	O
might	O
be	O
out	O
of	O
sync	O
which	O
makes	O
direct	O
comparisons	O
difficult	O
.	O

I	O
would	O
like	O
to	O
generate	O
a	O
matrix	O
which	O
contains	O
the	O
output	O
of	O
the	O
function	O
for	O
every	O
combination	O
of	O
X	O
and	O
Y	O
.	O

I'm	O
trying	O
to	O
get	O
the	O
data	O
to	O
in	O
the	O
column	O
"	O
Structure	O
"	O
to	O
repeat	O
the	O
row	O
labels	O
so	O
it	O
look	O
like	O
this	O
:	O
#CODE	O

decision	O
for	O
single	O
rows	O
to	O
get	O
converted	O
into	O
a	O
series	O
-	O
why	O
not	O
a	O

I	O
get	O
#CODE	O

More	O
info	O
as	O
requested	O
#CODE	O

[	O
Their	O
product	O
page	O
]	O
(	O
#URL	O
)	O
holds	O
the	O
answer	O
.	O

Next	O
,	O
you	O
wish	O
to	O
get	O
the	O
specific	O
groups	O
from	O
this	O
`	O
grouped	O
`	O
object	O
.	O

To	O
get	O
datetime64	O
that	O
uses	O
seconds	O
directly	O
:	O
#CODE	O

The	O
only	O
option	O
you	O
may	O
have	O
is	O
to	O
setup	O
your	O
data	O
structures	O
to	O
be	O
light	O
weight	O
so	O
each	O
worker	O
isn't	O
boated	O
by	O
redundant	O
copies	O
of	O
the	O
same	O
data	O
or	O
excessive	O
amounts	O
of	O
data	O
which	O
might	O
be	O
better	O
off	O
split	O
across	O
different	O
workers	O
.	O

Inplace	O
dropping	O
seems	O
more	O
like	O
idiomatic	O
pandas	O
to	O
me	O
than	O
making	O
a	O
copy	O
only	O
to	O
instantly	O
garbage	O
collect	O
the	O
now-defunct	O
original	O
.	O

And	O
fortunately	O
,	O
these	O
days	O
,	O
`	O
-pylab	O
`	O
has	O
been	O
deprecated	O
and	O
using	O
`	O
--	O
matplotlib	O
`	O
and	O
importing	O
`	O
pylab	O
`	O
manually	O
is	O
encouraged	O
.	O

Are	O
you	O
sure	O
you	O
don't	O
mean	O
`	O
range	O
(	O
1	O
,	O
len	O
(	O
DF	O
)):	O
`	O
?	O

Where	O
"	O
timeblock	O
"	O
#1	O
will	O
include	O
the	O
first	O
4:59	O
minutes	O
of	O
observation	O
period	O
#1	O
,	O
#2	O
will	O
include	O
5:00	O
to	O
9:59	O
minutes	O
...	O
through	O
to	O
25:00	O
and	O
over	O
,	O
for	O
each	O
observation	O
period	O
.	O

It	O
seems	O
like	O
I'm	O
maybe	O
getting	O
confused	O
between	O
the	O
underlying	O
data	O
and	O
views	O
on	O
it	O
.	O

When	O
I	O
try	O
specifying	O
index_col=0	O
,	O
as	O
some	O
examples	O
in	O
the	O
documentation	O
do	O
,	O
I	O
get	O
a	O
"	O
IndexError	O
:	O
list	O
index	O
out	O
of	O
range	O
"	O
error	O
,	O
which	O
was	O
a	O
solution	O
to	O
several	O
related	O
questions	O
but	O
for	O
some	O
reason	O
isn't	O
working	O
for	O
me	O
.	O

That's	O
what	O
I	O
thought	O
about	O
my	O
original	O
code	O
but	O
for	O
some	O
reason	O
when	O
I	O
check	O
len	O
(	O
Sframe	O
)	O
at	O
the	O
end	O
in	O
the	O
main	O
code	O
,	O
it	O
still	O
has	O
the	O
duplicate	O
values	O
even	O
if	O
the	O
conditional	O
statement	O
applies	O
option	O
2	O
to	O
remove	O
duplicates	O
.	O

The	O
main	O
thing	O
I	O
need	O
to	O
do	O
is	O
to	O
group	O
the	O
days	O
by	O
week	O
such	O
that	O
I	O
can	O
get	O
sum	O
of	O
the	O
data	O
to	O
be	O
by	O
week	O
.	O

To	O
move	O
the	O
third	O
row	O
to	O
the	O
first	O
,	O
you	O
can	O
create	O
an	O
index	O
moving	O
the	O
target	O
row	O
to	O
the	O
first	O
element	O
.	O

So	O
using	O
your	O
approach	O
,	O
how	O
can	O
I	O
:	O
1	O
)	O
plot	O
the	O
scores	O
as	O
a	O
histogram	O
in	O
a	O
memory-conscious	O
way	O
,	O
and	O
2	O
)	O
extract	O
the	O
scores	O
belonging	O
to	O
the	O
certain	O
cell	O
types	O
to	O
plot	O
those	O
as	O
well	O
?	O

I	O
have	O
a	O
CSV	O
file	O
,	O
I	O
wanted	O
to	O
filter	O
it	O
where	O
I	O
keep	O
just	O
rows	O
where	O
I	O
have	O
values	O
in	O
row	O
"	O
d	O
"	O
bigger	O
then	O
0	O
.	O

Simulations	O
can	O
be	O
repeated	O
for	O
different	O
scenarios	O
and	O
each	O
one	O
of	O
these	O
scenarios	O
will	O
produce	O
a	O
different	O
hourly	O
set	O
of	O
data	O
for	O
each	O
room	O
and	O
each	O
variable	O
.	O

To	O
get	O
around	O
this	O
,	O
I'm	O
passing	O
in	O
a	O
large	O
number	O
for	O
the	O
max_results	O
parameter	O
and	O
specifying	O
a	O
chunksize	O
.	O

You	O
get	O
back	O
a	O
float	O
because	O
each	O
row	O
contains	O
a	O
mix	O
of	O
`	O
float	O
`	O
and	O
`	O
int	O
`	O
types	O
.	O

Should	O
I	O
use	O
something	O
different	O
.	O

The	O
best	O
would	O
be	O
to	O
convert	O
that	O
one	O
file	O
to	O
a	O
an	O
actual	O
comma	O
(	O
semicolon	O
or	O
other	O
)	O
separated	O
file	O
or	O
make	O
sure	O
that	O
compound	O
values	O
are	O
quoted	O
(	O
"	O
Alabama	O
County	O
")	O
and	O
then	O
specify	O
the	O
quotechar	O
:	O
#CODE	O

I	O
would	O
like	O
to	O
calculate	O
the	O
average	O
of	O
time	O
per	O
org	O
per	O
cluster	O
.	O

I	O
found	O
a	O
way	O
which	O
seems	O
very	O
inefficient	O
(	O
stacking	O
and	O
unstacking	O
which	O
will	O
create	O
many	O
many	O
columns	O
in	O
case	O
of	O
millions	O
of	O
categories	O
)	O
.	O

I	O
was	O
going	O
to	O
suggest	O
cumcount	O
and	O
tail	O
(	O
1	O
)	O
,	O
but	O
you're	O
after	O
something	O
else	O
(	O
these	O
would	O
be	O
much	O
faster	O
)	O
.	O

This	O
is	O
because	O
working	O
with	O
dictionaries	O
is	O
so	O
easy	O
and	O
thinking	O
of	O
them	O
like	O
simple	O
dicts	O
often	O
means	O
you	O
can	O
find	O
a	O
solution	O
to	O
an	O
issue	O
without	O
having	O
to	O
get	O
too	O
deep	O
into	O
pandas	O
.	O

If	O
you're	O
trying	O
to	O
slice	O
each	O
string	O
to	O
get	O
the	O
substring	O
from	O
5	O
to	O
7	O
,	O
you	O
need	O
a	O
`	O
:	O
`	O
,	O
not	O
a	O
`	O
,	O
`	O
:	O
#CODE	O

However	O
,	O
since	O
each	O
of	O
your	O
new	O
`	O
DataFrames	O
`	O
is	O
a	O
summary	O
of	O
a	O
single	O
customer	O
,	O
I	O
would	O
suggest	O
writing	O
one	O
function	O
that	O
can	O
return	O
all	O
of	O
your	O
desired	O
results	O
in	O
a	O
single	O
`	O
Series	O
`	O
.	O

I	O
want	O
to	O
get	O
statistics	O
of	O
debt_ratio	O
based	O
on	O
subgroups	O
of	O
market_capitalization	O
.	O

Any	O
ideas	O
why	O
this	O
error	O
might	O
be	O
showing	O
up	O
so	O
I	O
can	O
know	O
what	O
to	O
go	O
after	O
to	O
fix	O
?	O

I	O
believe	O
it	O
is	O
getting	O
at	O
what	O
I	O
want	O
.	O

I	O
haven't	O
done	O
any	O
stress	O
testing	O
but	O
I'd	O
imagine	O
this	O
could	O
get	O
slow	O
on	O
very	O
large	O
DataFrames	O
.	O

Is	O
there	O
a	O
quick	O
way	O
to	O
sort	O
my	O
data	O
by	O
a	O
given	O
column	O
that	O
only	O
takes	O
chunks	O
into	O
account	O
and	O
doesn't	O
require	O
loading	O
entire	O
datasets	O
in	O
memory	O
?	O

Take	O
a	O
look	O
at	O
the	O
regex	O
docs	O
.	O

Any	O
logic	O
requirements	O
(	O
like	O
comparing	O
elem+1	O
to	O
elem	O
)	O
should	O
be	O
in	O
your	O
question	O
so	O
there	O
is	O
no	O
confusion	O
.	O

I'd	O
like	O
to	O
get	O
a	O
list	O
as	O
`	O
[	O
'	O
abcd	O
'	O
,	O
'	O
ddse	O
'	O
,	O
'	O
123d	O
'	O
,	O
'	O
aaaaa*	O
']`	O
.	O

but	O
get	O
the	O
following	O
error	O
:	O

I	O
get	O
#CODE	O

Ideally	O
the	O
question	O
would	O
provide	O
a	O
self-contained	O
piece	O
of	O
code	O
generating	O
the	O
data	O
structure	O
,	O
or	O
even	O
just	O
something	O
like	O
`	O
df	O
=	O
[[	O
1	O
,	O
2	O
]	O
,	O
[	O
2	O
,	O
3	O
]	O
,	O
[	O
4	O
,	O
5	O
]]`	O
,	O
enough	O
to	O
try	O
to	O
get	O
an	O
answer	O
without	O
diving	O
into	O
panda	O
.	O

I'm	O
running	O
daily	O
simulations	O
in	O
a	O
batch	O
:	O
I	O
do	O
365	O
simluations	O
to	O
get	O
results	O
for	O
a	O
full	O
year	O
.	O

I	O
edited	O
my	O
answer	O
to	O
use	O
capwords	O
per	O
your	O
suggestion	O
,	O
that	O
fixed	O
the	O
problem	O
I	O
missed	O
where	O
it	O
capitalized	O
the	O
'	O
s	O
'	O
in	O
Guy's	O
Name	O
.	O

Does	O
not	O
work	O
exactly	O
right	O
since	O
categories	O
can	O
be	O
mixed	O
like	O
that	O
so	O
it	O
will	O
produce	O
more	O
duplications	O

Essentially	O
,	O
I	O
want	O
to	O
look	O
at	O
quintiles	O
(	O
since	O
there	O
are	O
5	O
days	O
in	O
a	O
business	O
week	O
)	O
rank	O
1	O
and	O
5	O
and	O
see	O
how	O
they	O
change	O
from	O
week	O
to	O
week	O
.	O

Doing	O
this	O
transformation	O
for	O
500,000	O
file	O
takes	O
time	O
.	O

The	O
requirement	O
isn't	O
easy	O
to	O
wrap	O
once	O
mind	O
around	O
,	O
so	O
sorry	O
If	O
I	O
am	O
not	O
being	O
clear	O
.	O

My	O
question	O
is	O
,	O
from	O
`	O
result	O
`	O
how	O
can	O
I	O
get	O
the	O
column	O
index	O
of	O
the	O
first	O
level	O
as	O
list	O
:	O
#CODE	O

This	O
function	O
will	O
then	O
work	O
out	O
the	O
maximum	O
,	O
minimum	O
,	O
and	O
return	O
rages	O
of	O
values	O
based	O
on	O
the	O
fact	O
I	O
want	O
5	O
categories	O
:	O
(	O
1	O
,	O
2	O
)	O
,	O
(	O
3	O
,	O
4	O
)	O
,	O
(	O
5	O
,	O
6	O
)	O
,	O
(	O
7	O
,	O
8)	O
,	O
(	O
9	O
,	O
10	O
)	O
.	O

Any	O
suggestions	O
of	O
a	O
better	O
way	O
?	O

In	O
addition	O
this	O
is	O
unlikely	O
to	O
be	O
only	O
time	O
I	O
have	O
to	O
do	O
this	O
so	O
being	O
able	O
to	O
change	O
the	O
numbers	O
and	O
columns	O
i'm	O
interested	O
in	O
would	O
be	O
good	O
.	O

And	O
at	O
that	O
time	O
i	O
think	O
the	O
state	O
becomes	O
bad	O
,	O
so	O
subsequent	O
calls	O
will	O
lead	O
to	O
exceptions	O
like	O
these	O
:	O
#CODE	O

Let	O
me	O
fix	O
that	O
and	O
return	O
to	O
this	O
issue	O
,	O
as	O
I	O
have	O
read	O
something	O
about	O
format	O
changes	O
between	O
10	O
and	O
12	O
.	O

All	O
the	O
data	O
,	O
columns	O
222	O
and	O
333	O
are	O
offset	O
as	O
required	O
,	O
but	O
it	O
isn't	O
even	O
the	O
same	O
size	O
as	O
the	O
output	O
in	O
the	O
first	O
order	O
.	O

But	O
I	O
can	O
only	O
get	O
this	O
:	O
#CODE	O

I	O
have	O
summary-level	O
data	O
of	O
the	O
count	O
of	O
people	O
by	O
age	O
group	O
,	O
city	O
,	O
income	O
and	O
the	O
industry	O
in	O
which	O
they	O
work	O
,	O
or	O
in	O
this	O
case	O
four	O
dimensions	O
.	O

@USER	O
:	O
Yes	O
,	O
it	O
can	O
as	O
I	O
said	O
,	O
"	O
They	O
all	O
suck	O
in	O
different	O
ways	O
"	O
.	O

Error	O
:	O
list	O
indices	O
must	O
be	O
integers	O
,	O
not	O
Series	O

Just	O
so	O
that	O
it	O
does	O
not	O
fail	O
silently	O
if	O
the	O
wrong	O
kind	O
of	O
data	O
is	O
passed	O
in	O
.	O

If	O
you	O
want	O
to	O
take	O
advantage	O
of	O
NumPy	O
/	O
Pandas	O
to	O
perform	O
fast	O
(	O
er	O
)	O
calculations	O
you	O
must	O
keep	O
the	O
data	O
in	O
a	O
NumPy	O
array	O
or	O
Pandas	O
NDFrame	O
.	O

Your	O
benchmark	O
is	O
actually	O
too	O
small	O
to	O
show	O
the	O
real	O
difference	O
.	O

I	O
was	O
able	O
to	O
resolve	O
this	O
by	O
opening	O
/	O
closing	O
a	O
connection	O
each	O
time	O
i	O
need	O
to	O
execute	O
a	O
query	O
.	O

I	O
used	O
a	O
chunksize	O
of	O
4	O
to	O
make	O
the	O
grouping	O
noticeable	O
on	O
the	O
small	O
dataset	O
;	O
you'll	O
want	O
to	O
change	O
it	O
to	O
90000	O
for	O
your	O
real	O
dataset	O
.	O

It's	O
mostly	O
trial	O
and	O
error	O
for	O
me	O
,	O
plus	O
knowledge	O
that	O
you	O
can	O
do	O
a	O
lot	O
just	O
with	O
```	O
rank	O
```	O
and	O
```	O
count	O
```	O
,	O
which	O
are	O
both	O
pretty	O
fast	O
.	O

And	O
those	O
columns	O
which	O
have	O
differing	O
values	O
:	O
#CODE	O

(	O
I	O
will	O
use	O
logistic	O
regression	O
and	O
random	O
forest	O
to	O
do	O
the	O
prediction	O
,	O
which	O
support	O
sparse	O
matrix	O
.	O
)	O
Is	O
there	O
anyway	O
to	O
efficiently	O
slicing	O
a	O
sparseDataFrame	O
or	O
for	O
the	O
whole	O
process	O
I	O
am	O
doing	O
,	O
it	O
should	O
be	O
improved	O
in	O
anyway	O
?	O

I	O
looked	O
here	O
,	O
but	O
when	O
I	O
ran	O
that	O
in	O
iPython	O
Notebook	O
,	O
I	O
don't	O
get	O
anything	O
.	O

does	O
not	O
produce	O
any	O
difference	O
in	O
terms	O
of	O
file	O
size	O
than	O
...	O

You	O
will	O
either	O
have	O
to	O
split	O
the	O
table	O
up	O
or	O
choose	O
another	O
storage	O
format	O
.	O

To	O
get	O
the	O
number	O
of	O
groups	O
you	O
can	O
use	O
the	O
ngroups	O
attribute	O
:	O
#CODE	O

this	O
does	O
not	O
seem	O
to	O
work	O
and	O
I	O
am	O
not	O
sure	O
if	O
it	O
just	O
is	O
not	O
possible	O
,	O
I	O
can	O
always	O
generate	O
separate	O
dictionaries	O
from	O
a	O
master	O
dictionary	O
to	O
get	O
around	O
having	O
to	O
update	O
data	O
in	O
multiple	O
locations	O

I	O
want	O
to	O
compare	O
these	O
two	O
data	O
frames	O
by	O
row	O
based	O
on	O
column	O
Value	O
and	O
keep	O
the	O
row	O
from	O
first	O
or	O
second	O
depending	O
where	O
the	O
Value	O
is	O
bigger	O
.	O

Then	O
,	O
you	O
can	O
run	O
`	O
sudo	O
port	O
install	O
py27-pandas	O
`	O
to	O
get	O
Python	O
and	O
all	O
of	O
the	O
dependencies	O
installed	O
.	O

See	O
the	O
shape	O
method	O
of	O
the	O
input	O
array	O
,	O
and	O
you	O
should	O
get	O
something	O
like	O
`	O
(	O
N	O
,	O
)`	O
and	O
not	O
`	O
(	O
N	O
,	O
1	O
)`	O
.	O

So	O
,	O
will	O
the	O
lower	O
value	O
always	O
come	O
first	O
,	O
or	O
could	O
that	O
change	O
?	O

and	O
get	O
the	O
following	O
error	O
:	O

In	O
order	O
to	O
have	O
the	O
index	O
the	O
exact	O
same	O
as	O
the	O
first	O
example	O
you'd	O
need	O
to	O
change	O
to	O
int	O
from	O
float	O
.	O

The	O
graph	O
bit	O
is	O
sorted	O
but	O
the	O
part	O
i'm	O
finding	O
hard	O
is	O
the	O
fact	O
the	O
column	O
headers	O
can	O
change	O
so	O
picking	O
up	O
their	O
data	O
without	O
manually	O
calling	O
them	O
is	O
something	O
I'm	O
unable	O
to	O
do	O
.	O

If	O
pytables	O
used	O
msgpack	O
it	O
would	O
be	O
easier	O
for	O
other	O
languages	O
to	O
read	O
the	O
data	O
but	O
obviously	O
their	O
target	O
is	O
python	O
.	O

The	O
company	O
name	O
may	O
be	O
variable	O
length	O
,	O
it	O
will	O
however	O
always	O
be	O
after	O
the	O
first	O
`	O
\s	O
`	O

To	O
get	O
the	O
same	O
form	O
of	O
broadcasting	O
to	O
occur	O
like	O
the	O
diagram	O
above	O
shows	O
we	O
have	O
to	O
decompose	O
to	O
numpy	O
arrays	O
which	O
then	O
become	O
anonymous	O
data	O
:	O
#CODE	O

I	O
want	O
to	O
generate	O
a	O
plot	O
showing	O
these	O
dates	O
graphically	O
.	O

How	O
is	O
it	O
possible	O
to	O
get	O
the	O
label	O
of	O
value	O
'	O
12	O
'	O
?	O

You	O
shall	O
note	O
that	O
`	O
and	O
`	O
and	O
`	O
or	O
`	O
are	O
not	O
appropriate	O
for	O
a	O
vector	O
of	O
booleans	O
,	O
use	O
`	O
`	O
and	O
`	O
|	O
`	O
instead	O
.	O

No	O
,	O
this	O
table	O
is	O
used	O
by	O
a	O
lot	O
of	O
other	O
code	O
(	O
mostly	O
C#	O
)	O
,	O
I	O
am	O
just	O
doing	O
some	O
data	O
analysis	O
on	O
it	O
from	O
Python	O
,	O
so	O
I'm	O
not	O
in	O
a	O
position	O
to	O
change	O
the	O
semantics	O
/	O
data	O
structure	O
.	O

#	O
Valid	O
positions	O
in	O
output	O
array	O
to	O
be	O
changed	O

Is	O
there	O
a	O
bug	O
in	O
my	O
code	O
or	O
is	O
there	O
another	O
reason	O
for	O
the	O
huge	O
computation	O
speed	O
difference	O
between	O
those	O
two	O
lines	O
of	O
code	O
?	O

I	O
also	O
used	O
a	O
longer	O
window	O
because	O
there	O
were	O
only	O
15	O
values	O
per	O
array	O
but	O
you	O
seemed	O
to	O
be	O
planning	O
on	O
using	O
the	O
last	O
50	O
days	O
.	O

I	O
am	O
trying	O
to	O
get	O
the	O
data	O
into	O
the	O
following	O
shape	O
:	O
#CODE	O

I	O
then	O
get	O
the	O
following	O
error	O
:	O
#CODE	O

I	O
want	O
to	O
:	O
plot	O
a	O
heatmap	O
of	O
x	O
,	O
y	O
and	O
the	O
colour	O
is	O
the	O
z	O
value	O
..	O

I'm	O
also	O
running	O
python	O
3.4	O
and	O
I	O
didn't	O
get	O
any	O
warning	O
when	O
I	O
ran	O
your	O
code	O
exactly	O
as	O
is	O
.	O

This	O
does	O
it	O
in	O
a	O
one	O
liner	O
but	O
is	O
not	O
so	O
readable	O
,	O
basically	O
it	O
tests	O
where	O
the	O
value	O
counts	O
for	O
each	O
column	O
is	O
equal	O
to	O
1	O
,	O
filters	O
the	O
resultant	O
list	O
out	O
and	O
uses	O
the	O
index	O
as	O
a	O
boolean	O
indec	O
:	O
#CODE	O

I	O
still	O
think	O
pandas	O
is	O
not	O
correctly	O
handling	O
your	O
empty	O
column	O
and	O
you	O
end	O
up	O
with	O
either	O
with	O
5	O
columns	O
,	O
or	O
with	O
6	O
columns	O
,	O
but	O
shifted	O
one	O
to	O
the	O
left	O
.	O

However	O
,	O
I	O
am	O
receiving	O
this	O
warning	O
`	O
/	O
usr	O
/	O
local	O
/	O
lib	O
/	O
python2.7	O
/	O
dist-packages	O
/	O
pandas	O
/	O
core	O
/	O
index	O
.	O

It	O
looks	O
like	O
this	O
changed	O
at	O
some	O
point	O
;	O
maybe	O
he	O
has	O
an	O
old	O
version	O
of	O
pandas	O
where	O
S	O
and	O
Sec	O
are	O
no	O
good	O
.	O

I	O
have	O
also	O
heard	O
of	O
Orange	O
library	O
for	O
imputation	O
,	O
but	O
haven't	O
had	O
a	O
chance	O
to	O
use	O
it	O
yet	O
.	O

I	O
managed	O
to	O
find	O
how	O
this	O
is	O
almost	O
done	O
:	O
#CODE	O

For	O
this	O
purpose	O
you	O
need	O
pandas	O
-	O
most	O
popular	O
python	O
package	O
for	O
working	O
with	O
timeseries	O
and	O
another	O
analytic	O
data	O
.	O

well	O
,	O
what	O
I	O
am	O
trying	O
to	O
do	O
is	O
to	O
have	O
all	O
my	O
results	O
ready	O
out	O
of	O
mysql	O
,	O
and	O
then	O
do	O
different	O
types	O
of	O
merging	O
to	O
get	O
my	O
plots	O
.	O

I	O
agree	O
with	O
Chang	O
that	O
it	O
would	O
help	O
to	O
have	O
a	O
very	O
clear	O
example	O
of	O
how	O
the	O
exact	O
alignment	O
should	O
be	O
.	O

You	O
mention	O
in	O
update	O
2	O
above	O
that	O
you	O
want	O
to	O
get	O
the	O
columns	O
and	O
the	O
only	O
way	O
is	O
opening	O
the	O
hdf	O
.	O

So	O
where	O
those	O
indices	O
don't	O
match	O
up	O
(	O
50	O
,	O
and	O
51	O
)	O
,	O
you	O
get	O
`	O
NaN	O
`	O
as	O
I	O
would	O
hope	O
.	O

However	O
,	O
if	O
I	O
save	O
it	O
as	O
a	O
csv	O
file	O
and	O
reload	O
it	O
again	O
,	O
I	O
got	O
error	O
message	O
and	O
the	O
plot	O
is	O
not	O
quite	O
right	O
either	O
,	O
#CODE	O

My	O
main	O
goal	O
is	O
to	O
match	O
the	O
index	O
value	O
from	O
`	O
ds2	O
`	O
into	O
`	O
ds1	O
`	O
and	O
replace	O
it	O
with	O
corresponding	O
value	O
,	O
so	O
the	O
output	O
would	O
look	O
like	O
#CODE	O

That	O
would	O
be	O
a	O
possibility	O
but	O
the	O
problem	O
is	O
that	O
each	O
frame5	O
has	O
a	O
different	O
index	O
.	O

First	O
,	O
you	O
need	O
some	O
kind	O
of	O
mapping	O
of	O
what	O
makes	O
up	O
each	O
level	O
.	O

PyTables	O
3.1	O
was	O
just	O
released	O
that	O
changes	O
the	O
file	O
caching	O
mechanism	O
at	O
least	O
on	O
a	O
lower	O
HDF5	O
version	O
,	O
do	O
to	O
see	O
your	O
version	O
:	O
#CODE	O

backfilling	O
data	O
from	O
one	O
column	O
into	O
another	O

Also	O
,	O
I	O
know	O
I	O
am	O
missing	O
patterns	O
that	O
may	O
be	O
useful	O
because	O
if	O
a	O
pattern	O
exists	O
between	O
Variable_1	O
and	O
Variable_2	O
and	O
Variable_3	O
and	O
Variable_4	O
are	O
missing	O
completely	O
at	O
random	O
,	O
then	O
concatenating	O
them	O
as	O
strings	O
will	O
not	O
capture	O
the	O
pattern	O
between	O
Variable_1	O
and	O
Variable_2	O
.	O

Do	O
you	O
know	O
what	O
is	O
the	O
difference	O
in	O
this	O
case	O
between	O
both	O
?	O

The	O
goal	O
is	O
to	O
take	O
the	O
2x2	O
piece	O
of	O
df	O
with	O
index	O
(	O
4	O
,	O
5	O
)	O
and	O
columns	O
(	O
'	O
date	O
'	O
,	O
'	O
val	O
')	O
and	O
replace	O
it	O
with	O
a	O
same-shaped	O
,	O
same-typed	O
2x2	O
block	O
.	O

Convert	O
Matrix	O
format	O
to	O
Column	O
in	O
Pandas	O

`	O
nan	O
`	O
is	O
commonly	O
used	O
for	O
this	O
purpose	O
,	O
but	O
here	O
I'm	O
actually	O
just	O
using	O
the	O
time	O
that	O
was	O
already	O
there	O
if	O
there	O
isn't	O
a	O
new	O
one	O
defined	O
for	O
it	O
in	O
the	O
`	O
time_map	O
`	O
`	O
dict	O
`	O
.	O

I	O
want	O
to	O
drop	O
all	O
values	O
after	O
index	O
`	O
5	O
`	O
because	O
it	O
has	O
no	O
values	O
,	O
but	O
not	O
index	O
`	O
2	O
`	O
,	O
`	O
3	O
`	O
.	O

You	O
should	O
be	O
able	O
to	O
uninstall	O
Anaconda	O
(	O
it	O
is	O
only	O
a	O
directory	O
)	O
to	O
reverse	O
any	O
changes	O
.	O

Since	O
it	O
is	O
a	O
very	O
large	O
data	O
frame	O
,	O
I	O
think	O
it	O
might	O
be	O
inefficient	O
to	O
do	O
a	O
loop	O
and	O
row	O
by	O
row	O
extraction	O
.	O

Since	O
the	O
NumPy	O
array	O
has	O
no	O
index	O
,	O
there	O
should	O
be	O
no	O
"	O
Unalignable	O
boolean	O
Series	O
"	O
problem	O
.	O

My	O
guess	O
is	O
that	O
I	O
am	O
either	O
not	O
applying	O
the	O
functions	O
correctly	O
for	O
a	O
column	O
or	O
the	O
values	O
I	O
am	O
getting	O
arent	O
integers	O
.	O

which	O
gives	O
you	O
your	O
date	O
as	O
a	O
list	O
arranged	O
in	O
the	O
order	O
of	O
importance	O
...	O

I'm	O
not	O
sure	O
what	O
the	O
difference	O
was	O
.	O

The	O
mongodb	O
collection	O
contains	O
sensor	O
values	O
tagged	O
with	O
date	O
and	O
time	O
.	O

This	O
should	O
not	O
make	O
any	O
difference	O
.	O

I	O
would	O
like	O
to	O
split	O
that	O
file	O
into	O
files	O
of	O
len	O
(	O
index	O
)	O
=	O
2	O
,	O
using	O
linux	O
:	O
#CODE	O

You	O
can	O
coerce	O
the	O
response	O
into	O
a	O
data	O
frame	O
after	O
you	O
get	O
it	O
.	O

Oh	O
wait	O
,	O
your	O
matrix	O
must	O
already	O
be	O
in	O
the	O
form	O
of	O
differences	O
from	O
the	O
mean	O
(	O
by	O
column	O
)	O
?	O

Any	O
suggestions	O
?	O

I	O
get	O
the	O
following	O
error	O
:	O
#CODE	O

I	O
am	O
curious	O
why	O
doing	O
`	O
unique_df	O
[	O
i	O
]	O
=	O
"	O
AAA	O
"`	O
no	O
longer	O
modifies	O
the	O
data	O
frame	O
values	O
.	O

Unfortunately	O
I	O
get	O
an	O
error	O
,	O
and	O
the	O
shading	O
doesn't	O
work	O
.	O

The	O
range	O
of	O
the	O
values	O
in	O
`	O
megaball	O
`	O
are	O
from	O
1	O
to	O
25	O
,	O
and	O
this	O
line	O
of	O
code	O
:	O
#CODE	O

yes	O
it	O
works	O
fine	O
but	O
I	O
need	O
to	O
drop	O
'	O
2014-07-16	O
14:24	O
'	O
thnx	O

If	O
you	O
have	O
more	O
pressing	O
things	O
to	O
do	O
,	O
you	O
could	O
temporarily	O
rename	O
it	O
out	O
of	O
the	O
way	O
to	O
get	O
through	O
your	O
installs	O
,	O
then	O
rename	O
it	O
back	O
.	O

As	O
you	O
can	O
see	O
,	O
the	O
lines	O
overlap	O
perfectly	O
for	O
the	O
days	O
where	O
there	O
is	O
data	O
:	O
no	O
original	O
data	O
is	O
'	O
changed	O
'	O
.	O

I	O
have	O
a	O
massive	O
file	O
with	O
per	O
timestamp	O
survey	O
data	O
from	O
about	O
thousands	O
of	O
different	O
people	O
and	O
over	O
20	O
different	O
locations	O
.	O

So	O
if	O
you	O
have	O
a	O
million	O
items	O
that	O
,	O
on	O
average	O
,	O
belong	O
to	O
three	O
categories	O
each	O
,	O
then	O
you	O
need	O
storage	O
for	O
the	O
million	O
items	O
plus	O
three	O
million	O
references	O
.	O

Given	O
how	O
the	O
sample	O
was	O
built	O
,	O
there	O
was	O
a	O
need	O
to	O
weight	O
adjust	O
the	O
respondent	O
data	O
so	O
that	O
not	O
every	O
one	O
is	O
deemed	O
as	O
"	O
equal	O
"	O
when	O
performing	O
the	O
analysis	O
.	O

What	O
I'm	O
hoping	O
to	O
achieve	O
is	O
knowing	O
where	O
the	O
first	O
/	O
last	O
row	O
of	O
trimmed	O
data	O
is	O
located	O
so	O
I	O
can	O
set	O
up	O
a	O
for-loop	O
to	O
go	O
through	O
the	O
data	O
and	O
perform	O
mathematical	O
calculations	O
with	O
the	O
values	O
and	O
then	O
send	O
those	O
results	O
back	O
into	O
new	O
columns	O
attached	O
directly	O
to	O
the	O
same	O
date	O
as	O
the	O
date	O
in	O
question	O
.	O

Ultimately	O
I	O
want	O
to	O
be	O
able	O
to	O
loop	O
through	O
the	O
json	O
to	O
display	O
the	O
dates	O
and	O
corresponding	O
values	O
,	O
but	O
I	O
cant	O
do	O
that	O
until	O
this	O
error	O
is	O
no	O
longer	O
happening	O
.	O

They	O
are	O
however	O
extremely	O
useful	O
once	O
you	O
get	O
to	O
grips	O
with	O
them	O
.	O

I	O
know	O
the	O
values	O
within	O
the	O
CSV	O
are	O
not	O
all	O
"	O
NaN	O
"	O
so	O
why	O
does	O
the	O
output	O
looks	O
like	O
this	O
and	O
how	O
can	O
I	O
get	O
the	O
correct	O
output	O
with	O
the	O
numbers	O
in	O
reach	O
of	O
the	O
rows	O
?	O

I've	O
filtered	O
my	O
data	O
as	O
suggested	O
here	O
:	O
With	O
Pandas	O
in	O
Python	O
,	O
select	O
the	O
highest	O
value	O
row	O
for	O
each	O
group	O
#CODE	O

I	O
was	O
able	O
to	O
get	O
the	O
more	O
precise	O
value	O
in	O
my	O
previous	O
environment	O
by	O
doing	O
the	O
incremental	O
update	O
to	O
cumulative	O
mean	O
instead	O
of	O
taking	O
a	O
batch	O
sum	O
and	O
divide	O
.	O

Then	O
use	O
the	O
`	O
~	O
`	O
to	O
flip	O
the	O
bools	O
.	O

I	O
haven't	O
checked	O
any	O
of	O
the	O
details	O
and	O
you	O
should	O
therefore	O
not	O
rely	O
on	O
it	O
to	O
be	O
correct	O
.	O

You	O
can	O
access	O
members	O
and	O
slices	O
of	O
the	O
array	O
as	O
you	O
would	O
with	O
normal	O
numpy	O
arrays	O
.	O

Running	O
1000	O
simulations	O
at	O
the	O
same	O
time	O
might	O
be	O
a	O
bit	O
expensive	O
though	O
.	O

I	O
need	O
to	O
look	O
at	O
other	O
formats	O
that	O
allow	O
create	O
.	O

finding	O
element	O
of	O
numpy	O
array	O
that	O
satisfies	O
condition	O

All	O
the	O
results	O
(	O
percentage	O
)	O
are	O
the	O
comparison	O
between	O
the	O
described	O
condition	O
and	O
the	O
reference	O
which	O
here	O
is	O
the	O
packaged	O
ATLAS	O
library	O
.	O

how	O
do	O
you	O
use	O
`	O
slicing	O
`	O
to	O
extract	O
something	O
from	O
`	O
x	O
`	O
?	O

Mapping	O
a	O
numpy	O
array	O
pairwise	O

If	O
that	O
is	O
all	O
you	O
want	O
to	O
do	O
,	O
it	O
should	O
just	O
work	O
otherwise	O
look	O
at	O
#URL	O
since	O
subclassing	O
an	O
array	O
is	O
not	O
that	O
simple	O
.	O

If	O
you	O
want	O
to	O
access	O
an	O
individual	O
element	O
using	O
2D	O
notation	O
,	O
you	O
can	O
create	O
a	O
view	O
and	O
work	O
with	O
that	O
.	O

Regarding	O
3	O
,	O
I	O
don't	O
think	O
this	O
would	O
be	O
necessary	O
since	O
the	O
above	O
trick	O
can	O
also	O
be	O
applied	O
to	O
an	O
array	O
allocated	O
by	O
`	O
shmarray	O
`	O
.	O

And	O
here	O
a	O
list	O
with	O
the	O
names	O
of	O
all	O
distribution	O
functions	O
available	O
in	O
Scipy	O
0.12.0	O
(	O
VI	O
):	O
#CODE	O

I	O
get	O
this	O
error	O
whenever	O
i	O
try	O
to	O
use	O
any	O
functions	O
of	O
matplotlib	O
such	O
as	O
graph	O
etc	O
...	O

Numpy	O
is	O
designed	O
for	O
repeated	O
application	O
of	O
the	O
exact	O
same	O
operation	O
in	O
parallel	O
across	O
an	O
array	O
.	O

Each	O
B	O
i	O
is	O
a	O
`	O
k	O
`	O
-by-	O
`	O
n	O
`	O
matrix	O
.	O

I	O
created	O
a	O
copy	O
of	O
the	O
initial	O
array	O
in	O
the	O
hope	O
that	O
it	O
would	O
sort	O
it	O
out	O
but	O
it	O
still	O
doesn't	O
work	O
!	O

So	O
far	O
,	O
all	O
the	O
solutions	O
I	O
found	O
required	O
converting	O
to	O
IPLImage	O
.	O

However	O
,	O
I	O
haven't	O
tested	O
this	O
very	O
systematically	O
,	O
and	O
it's	O
likely	O
that	O
for	O
smaller	O
matrices	O
the	O
additional	O
overhead	O
would	O
outweigh	O
the	O
performance	O
benefit	O
from	O
a	O
higher	O
thread	O
count	O
.	O

but	O
why	O
dimA+dimB	O
to	O
begin	O
with	O
,	O
that	O
just	O
leaves	O
you	O
0s	O
at	O
the	O
end	O
.	O

If	O
you	O
select	O
a	O
list	O
of	O
actual	O
fitness	O
entries	O
,	O
`	O
indices	O
`	O
,	O
the	O
corresponding	O
points	O
are	O
given	O
by	O
`	O
A	O
[	O
'	O
point	O
']	O
[	O
indices	O
]`	O
,	O
which	O
is	O
a	O
simple	O
`	O
(	O
n	O
,	O
3	O
)`	O
array	O
.	O

It's	O
also	O
possible	O
to	O
generate	O
an	O
array	O
of	O
indices	O
without	O
using	O
`	O
enumerate	O
`	O
.	O

If	O
you	O
want	O
to	O
keep	O
the	O
array	O
allocated	O
,	O
and	O
with	O
the	O
same	O
size	O
,	O
you	O
don't	O
need	O
to	O
clear	O
the	O
elements	O
.	O

It'll	O
print	O
out	O
all	O
methods	O
and	O
properties	O
of	O
the	O
object	O
.	O

Search	O
numpy	O
array	O
inside	O
numpy	O
array	O

Assigning	O
a	O
view	O
to	O
a	O
structured	O
array	O
also	O
copies	O
the	O
data	O
,	O
soyour	O
suggestion	O
doesnt	O
work	O
.	O

If	O
you	O
know	O
there	O
are	O
not	O
many	O
different	O
values	O
(	O
relative	O
to	O
the	O
size	O
of	O
the	O
input	O
"	O
itemArray	O
")	O
,	O
something	O
like	O
this	O
could	O
be	O
efficient	O
:	O
#CODE	O

When	O
I	O
import	O
numpy	O
in	O
a	O
script	O
,	O
I	O
don't	O
have	O
all	O
the	O
functions	O
of	O
numpy	O
available	O
,	O
only	O
few	O
of	O
them	O
(	O
not	O
a	O
lot	O
,	O
and	O
not	O
array	O
)	O
?	O

It	O
allows	O
to	O
have	O
your	O
custom	O
system	O
inside	O
the	O
home	O
directory	O
accessible	O
via	O
proot	O
and	O
,	O
therefore	O
,	O
you	O
can	O
install	O
any	O
packages	O
without	O
root	O
privileges	O
.	O

Hey	O
,	O
is	O
there	O
an	O
optimal	O
size	O
for	O
a	O
block	O
?	O

Numpy	O
array	O
,	O
how	O
to	O
select	O
indices	O
satisfying	O
multiple	O
conditions	O
?	O

I'm	O
still	O
learning	O
git	O
and	O
this	O
whole	O
open	O
source	O
thing	O
.	O

A	O
further	O
problem	O
is	O
that	O
a	O
list	O
index	O
can't	O
contain	O
duplicates	O
-	O
I	O
couldn't	O
simultaneously	O
read	O
pixels	O
`	O
[	O
1	O
,	O
2	O
]`	O
and	O
`	O
[	O
1	O
,	O
3	O
]`	O
,	O
since	O
my	O
list	O
of	O
pixel	O
x-coordinates	O
would	O
contain	O
`	O
[	O
1	O
,	O
1	O
]`	O
.	O

Your	O
interpretation	O
is	O
,	O
of	O
course	O
,	O
quite	O
correct	O
:	O
`	O
count	O
`	O
refers	O
to	O
the	O
number	O
of	O
elements	O
in	O
the	O
`	O
float*	O
`	O
"	O
array	O
"	O
.	O

This	O
all	O
works	O
.	O

Of	O
course	O
it	O
may	O
_not_	O
be	O
acceptable	O
,	O
but	O
it	O
seemed	O
like	O
it	O
was	O
at	O
least	O
trying	O
.	O

I	O
think	O
what	O
should	O
happen	O
is	O
for	O
R	O
to	O
be	O
overwritten	O
with	O
an	O
upper	O
triangular	O
matrix	O
.	O

@USER	O
Is	O
there	O
a	O
way	O
to	O
get	O
it	O
to	O
return	O
all	O
possible	O
solutions	O
?	O

Thank	O
you	O
all	O
for	O
some	O
great	O
insight	O
!	O

as	O
numpy	O
array	O
of	O
0	O
and	O
1	O
values	O
.	O

By	O
construction	O
of	O
the	O
problem	O
,	O
there	O
can	O
not	O
be	O
any	O
non-unique	O
values	O
lying	O
within	O
one	O
another	O
.	O

Both	O
functions	O
are	O
kind	O
of	O
equal	O
,	O
but	O
are	O
different	O
.	O

For	O
a	O
sparse	O
csr	O
matrix	O
(	O
X	O
)	O
and	O
a	O
list	O
of	O
indices	O
to	O
drop	O
(	O
index_to_drop	O
):	O
#CODE	O

This	O
will	O
be	O
relatively	O
slow	O
,	O
and	O
requires	O
you	O
to	O
have	O
twice	O
the	O
free	O
memory	O
space	O
required	O
to	O
store	O
the	O
array	O
.	O

I	O
need	O
numpy	O
for	O
this	O
because	O
I	O
don't	O
want	O
to	O
loop	O
through	O
the	O
array	O
n-times	O
for	O
n	O
groups	O
,	O
since	O
my	O
array	O
sizes	O
can	O
be	O
arbitrarily	O
large	O
.	O

I	O
wonder	O
what	O
the	O
speed	O
delta	O
between	O
native	O
Numpy	O
arrays	O
and	O
an	O
`	O
mpfr	O
`	O
array	O
,	O
seeing	O
as	O
`	O
mpfr	O
`	O
is	O
a	O
relatively	O
low-level	O
C	O
wrapper	O
class	O
.	O

In	O
this	O
particular	O
case	O
,	O
each	O
1D	O
column	O
(	O
`	O
column	O
=	O
myarray	O
[	O
i	O
,	O
j	O
,	O
:]	O
`)	O
of	O
the	O
3D	O
array	O
can	O
be	O
treated	O
independently	O
.	O

Any	O
ideas	O
?	O

Then	O
take	O
element	O
3	O
and	O
1	O
from	O
second	O
row	O
,	O
etc	O
.	O

print	O
len	O
(	O
index	O
)	O

I	O
don't	O
know	O
how	O
to	O
label	O
rows	O
and	O
columns	O
in	O
numpy	O
,	O
so	O
I	O
just	O
made	O
a	O
dict	O
mapping	O
the	O
row	O
label	O
to	O
the	O
row	O
index	O
and	O
another	O
doing	O
the	O
same	O
for	O
the	O
columns	O
.	O

It's	O
the	O
same	O
as	O
a	O
normal	O
solve	O
except	O
I	O
know	O
some	O
of	O
the	O
solution	O
to	O
begin	O
with	O
.	O

yeah	O
,	O
this	O
is	O
the	O
best	O
answer	O
anywhere	O
where	O
edge	O
cases	O
are	O
important	O
.	O

With	O
this	O
,	O
I	O
solve	O
the	O
problem	O
(	O
may	O
help	O
you	O
):	O
#CODE	O

Teach	O
a	O
man	O
to	O
fish	O
and	O
all	O
that	O
.	O

Looks	O
like	O
this	O
would	O
be	O
a	O
good	O
place	O
to	O
use	O
a	O
context	O
manager	O
,	O
so	O
you	O
can	O
say	O
"	O
with	O
fullprint	O
"	O
.	O

What	O
do	O
you	O
mean	O
by	O
'	O
random	O
sort	O
'	O
?	O

However	O
,	O
it	O
indeed	O
does	O
not	O
allow	O
you	O
to	O
exponentiate	O
any	O
matrix	O
directly	O
:	O
#CODE	O

The	O
first	O
one	O
would	O
probably	O
be	O
slow	O
for	O
large	O
data	O
,	O
and	O
the	O
second	O
one	O
does	O
not	O
seem	O
to	O
offer	O
any	O
other	O
interpolation	O
method	O
except	O
splines	O
.	O

EDIT	O
:	O
For	O
the	O
record	O
,	O
here	O
is	O
example	O
code	O
that	O
demonstrates	O
the	O
issue	O
:	O
#CODE	O

In	O
terms	O
of	O
functionality	O
,	O
it's	O
not	O
a	O
metaclass	O
at	O
all	O
,	O
it's	O
a	O
function	O
which	O
takes	O
a	O
class	O
(	O
along	O
with	O
some	O
others	O
stuff	O
)	O
and	O
returns	O
a	O
new	O
class	O
.	O

or	O
from	O
source	O
#CODE	O

Didn't	O
do	O
any	O
timings	O
here	O
,	O
but	O
it's	O
possible	O
this	O
version	O
has	O
reasonable	O
performance	O
.	O

I'll	O
just	O
test	O
whether	O
or	O
not	O
there's	O
a	O
significan	O
space	O
gain	O
on	O
my	O
data	O
with	O
protocol	O
2	O
.	O

Just	O
wanted	O
to	O
add	O
that	O
the	O
moving	O
average	O
function	O
has	O
been	O
extracted	O
into	O
the	O
[	O
Bottleneck	O
]	O
(	O
#URL	O
)	O
library	O
if	O
pandas	O
seems	O
too	O
heavyweight	O
as	O
a	O
dependency	O
.	O

`	O
numpy	O
`	O
can	O
use	O
`	O
malloc	O
`	O
/	O
`	O
realloc	O
`	O
for	O
creating	O
an	O
array	O
of	O
objects	O
of	O
`	O
sizeof	O
(	O
int	O
)`	O
.	O

If	O
you	O
read	O
through	O
the	O
documentation	O
of	O
those	O
engines	O
you	O
will	O
often	O
find	O
statements	O
saying	O
that	O
they	O
are	O
optimized	O
for	O
speed	O
(	O
30fps	O
-	O
60fps	O
)	O
.	O

I	O
know	O
I	O
have	O
to	O
look	O
at	O
each	O
row	O
,	O
but	O
I	O
don't	O
want	O
to	O
do	O
it	O
with	O
loops	O
.	O

I'm	O
hoping	O
this	O
can	O
at	O
least	O
save	O
someone	O
a	O
few	O
hours	O
of	O
hopeless	O
research	O
for	O
this	O
topic	O
.	O

What	O
does	O
"	O
doesn't	O
work	O
"	O
mean	O
?	O

You	O
might	O
look	O
at	O
your	O
code	O
that	O
generates	O
the	O
`	O
y	O
`	O
values	O
,	O
and	O
see	O
if	O
that	O
would	O
benefit	O
from	O
the	O
use	O
of	O
additional	O
numpy	O
or	O
scipy	O
functions	O
.	O

For	O
example	O
,	O
give	O
the	O
list	O
[	O
(	O
.3	O
,	O
'	O
a	O
')	O
,	O
(	O
.4	O
,	O
'	O
b	O
')	O
,	O
(	O
.3	O
,	O
'	O
c	O
')]	O
I'd	O
like	O
to	O
sample	O
'	O
b	O
'	O
40%	O
of	O
the	O
time	O
.	O

The	O
square	O
bracket	O
idea	O
you	O
mentioned	O
works	O
for	O
my	O
current	O
problem	O
.	O

However	O
,	O
when	O
I	O
try	O
to	O
use	O
a	O
weighted	O
average	O
#CODE	O

If	O
I	O
scale	O
that	O
to	O
a	O
2000	O
X	O
2000	O
np	O
array	O
,	O
here	O
is	O
what	O
I	O
get	O
:	O
#CODE	O

Unfortunately	O
,	O
I'm	O
not	O
aware	O
of	O
a	O
numpy	O
implementation	O
,	O
but	O
I	O
did	O
find	O
this	O
:	O
#URL	O

We	O
can	O
do	O
this	O
quite	O
neatly	O
using	O
numpy	O
,	O
without	O
having	O
to	O
worry	O
about	O
the	O
channels	O
at	O
all	O
!	O

It's	O
not	O
exactly	O
the	O
most	O
efficient	O
way	O
,	O
but	O
it	O
will	O
work	O
,	O
and	O
not	O
require	O
keeping	O
a	O
copy	O
of	O
the	O
file	O
in	O
memory	O
(	O
or	O
two	O
)	O
.	O

I'm	O
trying	O
to	O
find	O
the	O
fastest	O
way	O
to	O
find	O
the	O
first	O
non-zero	O
value	O
for	O
each	O
row	O
of	O
a	O
two	O
dimensional	O
sorted	O
array	O
.	O

Convert	O
the	O
numpy	O
array	O
into	O
a	O
list	O
first	O
.	O

@USER	O
do	O
you	O
mean	O
that	O
grid	O
dictonary	O
holds	O
the	O
results	O
in	O
memory	O
or	O
something	O
else	O
?	O

@USER	O
:	O
This	O
solution	O
is	O
indeed	O
designed	O
to	O
give	O
the	O
set	O
of	O
all	O
the	O
numbers	O
found	O
in	O
the	O
array	O
.	O

This	O
way	O
at	O
most	O
one	O
line	O
is	O
in	O
memory	O
at	O
any	O
one	O
time	O
.	O

It's	O
low	O
efficiency	O
,	O
I	O
want	O
to	O
know	O
are	O
there	O
any	O
builtin	O
functions	O
that	O
can	O
do	O
this	O
in	O
NumPy	O
.	O

The	O
raw	O
hardware	O
data	O
is	O
32-bit	O
signed	O
integer	O
,	O
which	O
becomes	O
float	O
when	O
I	O
convert	O
it	O
to	O
normal	O
physics	O
units	O
(	O
m	O
/	O
s	O
)	O

Calculate	O
subset	O
of	O
matrix	O
multiplication	O

EDIT	O
:	O
as	O
to	O
what	O
DSM	O
pointed	O
out	O
,	O
OP	O
is	O
infact	O
using	O
a	O
numpy	O
array	O
.	O

As	O
for	O
the	O
second	O
question	O
,	O
`	O
delete	O
`	O
has	O
been	O
suggested	O
before	O
:	O
#CODE	O

The	O
remaining	O
rows	O
of	O
the	O
matrices	O
are	O
all	O
linear	O
combinations	O
(	O
in	O
fact	O
exact	O
copies	O
for	O
almost	O
all	O
submatrices	O
)	O
of	O
these	O
rows	O
.	O

I'm	O
printing	O
the	O
contents	O
of	O
an	O
array	O
with	O
a	O
header	O
.	O

Edit	O
:	O
If	O
you	O
need	O
to	O
save	O
memory	O
try	O
radix	O
sort	O
,	O
which	O
is	O
much	O
faster	O
on	O
integers	O
than	O
quicksort	O
(	O
which	O
I	O
believe	O
is	O
what	O
numpy	O
uses	O
)	O
.	O

The	O
data	O
was	O
in	O
following	O
order	O
(	O
with	O
sample	O
data	O
)	O
#CODE	O

definitely	O
a	O
good	O
solution	O
,	O
nevertheless	O
I'd	O
like	O
to	O
solve	O
this	O
without	O
a	O
range	O
,	O
but	O
the	O
nearest	O
neighbour	O
.	O

For	O
my	O
recent	O
project	O
that	O
works	O
on	O
the	O
order	O
of	O
20000x20000	O
matrix	O
entries	O
,	O
I	O
will	O
quickly	O
and	O
disastrously	O
use	O
up	O
all	O
of	O
my	O
workstation's	O
8GB	O
of	O
RAM	O
and	O
more	O
.	O

Well	O
,	O
this	O
might	O
give	O
a	O
small	O
speed-up	O
just	O
because	O
it	O
uses	O
less	O
memory	O
.	O

Try	O
using	O
`	O
all	O
`	O
(	O
edited	O
to	O
return	O
`	O
int	O
`)	O
:	O
#CODE	O

:	O
I	O
don't	O
care	O
if	O
the	O
statement	O
modifies	O
array	O
or	O
not	O
.	O

Rather	O
than	O
doing	O
nans	O
I	O
put	O
in	O
-1	O
and	O
then	O
filtered	O
on	O
match	O
=	O
b	O
>	O
=	O
0	O
.	O

Removing	O
duplicate	O
columns	O
and	O
rows	O
from	O
a	O
NumPy	O
2D	O
array	O

arrays	O
to	O
extract	O
arrays	O
of	O
the	O
same	O

as	O
i	O
have	O
to	O
test	O
if	O
it	O
works	O
or	O
not	O
so	O
minimum	O
i	O
have	O
to	O
try	O
with	O
10-15	O
different	O
types	O
of	O
images	O
.	O
its	O
not	O
specific	O
images	O
it	O
can	O
be	O
any	O
image	O
of	O
people	O
.	O

So	O
having	O
a	O
python	O
loop	O
,	O
and	O
having	O
to	O
sum	O
all	O
the	O
results	O
together	O
,	O
is	O
taking	O
390	O
ms	O
more	O
than	O
200	O
times	O
what	O
it	O
takes	O
to	O
solve	O
each	O
of	O
the	O
200	O
systems	O
that	O
have	O
to	O
be	O
solved	O
.	O

You	O
could	O
then	O
sample	O
pixels	O
at	O
the	O
locations	O
of	O
the	O
new	O
points	O
.	O

I	O
could	O
use	O
an	O
idea	O
about	O
either	O
how	O
to	O
fix	O
the	O
compilation	O
errors	O
or	O
another	O
way	O
to	O
convert	O
my	O
python	O
objects	O
.	O

I	O
tried	O
every	O
possible	O
combination	O
resulting	O
with	O
a	O
0	O
bytes	O
file	O
if	O
the	O
extension	O
was	O
mpg	O
,	O
and	O
5.5kb	O
if	O
it	O
was	O
avi	O
.	O

Fortran	O
90	O
DOES	O
support	O
arbitrary	O
lower	O
bounds	O
on	O
arrays	O
,	O
and	O
borrowing	O
from	O
that	O
paradigm	O
sounds	O
quite	O
plausible	O
.	O

I	O
want	O
to	O
perform	O
an	O
operation	O
on	O
a	O
that	O
increments	O
all	O
the	O
values	O
inside	O
it	O
that	O
are	O
less	O
than	O
0	O
and	O
leaves	O
the	O
rest	O
alone	O
.	O
for	O
example	O
,	O
if	O
I	O
had	O
:	O
#CODE	O

I	O
like	O
how	O
this	O
one	O
uses	O
straight	O
python	O
slicing	O
and	O
doesn't	O
require	O
numpy	O

I	O
select	O
the	O
first	O
value	O
using	O
`	O
ys	O
[	O
0	O
]`	O
.	O

While	O
waiting	O
for	O
the	O
next	O
buffer	O
to	O
fill	O
,	O
I'd	O
like	O
to	O
process	O
the	O
most	O
recent	O
buffer	O
with	O
numpy	O
and	O
save	O
the	O
result	O
.	O

(	O
They	O
all	O
do	O
the	O
same	O
thing	O
,	O
in	O
this	O
case	O
.	O
)	O

As	O
of	O
PIL	O
1.1.6	O
,	O
the	O
"	O
proper	O
"	O
way	O
to	O
convert	O
between	O
images	O
and	O
numpy	O
arrays	O
is	O
simply	O
#CODE	O

So	O
first	O
you	O
need	O
to	O
construct	O
an	O
array	O
that	O
represents	O
the	O
rows	O
you	O
wish	O
to	O
select	O
.	O

Any	O
ideas	O
how	O
to	O
improve	O
this	O
?	O

If	O
this	O
is	O
the	O
case	O
,	O
you	O
can	O
easily	O
plot	O
a	O
known	O
asymmetric	O
shape	O
and	O
the	O
plot	O
will	O
tell	O
you	O
everything	O
.	O

Apart	O
from	O
the	O
compression	O
part	O
,	O
this	O
shouldn't	O
be	O
any	O
slower	O
than	O
normal	O
.	O

Acquiring	O
the	O
Minimum	O
array	O
out	O
of	O
Multiple	O
Arrays	O
by	O
order	O
in	O
Python	O

first	O
copy	O
#CODE	O

Getting	O
all	O
points	O
where	O
y=2	O
.	O

moving	O
average	O
function	O
on	O
numpy	O
/	O
scipy	O
?	O

for	O
a	O
N	O
dimensional	O
array	O
:	O
#CODE	O

From	O
looking	O
at	O
#URL	O
it	O
seems	O
that	O
it	O
was	O
originally	O
required	O
because	O
indexing	O
with	O
`	O
...	O

In	O
other	O
words	O
...	O
yes	O
you	O
can	O
trust	O
it	O
to	O
be	O
faster	O
...	O
but	O
don't	O
think	O
python	O
is	O
that	O
slow	O
that	O
you	O
cannot	O
do	O
a	O
for	O
loop	O
over	O
3	O
items	O
without	O
waiting	O
for	O
ages	O
...	O

And	O
all	O
that	O
while	O
you're	O
getting	O
lunch	O
.	O

If	O
you	O
do	O
want	O
to	O
raise	O
some	O
sort	O
of	O
exception	O
for	O
invalid	O
data	O
(	O
not	O
type	O
checking	O
)	O
,	O
either	O
let	O
an	O
existing	O
exception	O
propagate	O
,	O
or	O
wrap	O
it	O
in	O
your	O
own	O
exception	O
type	O
.	O

casting	O
are	O
used	O
in	O
Numexpr	O
,	O
in	O
contrast	O
with	O
NumPy	O
,	O
where	O
array	O
types	O

I	O
check	O
mathexchange	O
and	O
while	O
making	O
the	O
tags	O
for	O
the	O
post	O
,	O
it	O
didn't	O
have	O
any	O
the	O
ones	O
that	O
would	O
seem	O
relevant	O
like	O
scipy	O
and	O
numpy	O
or	O
even	O
sparse	O
.	O

Any	O
help	O
would	O
be	O
appreciated	O
.	O

Say	O
not	O
that	O
efficiency	O
is	O
a	O
secondary	O
priority	O
;	O
say	O
instead	O
that	O
I	O
want	O
to	O
perform	O
bivariate	O
optimization	O
:	O
pythonicity	O
+	O
efficiency	O
(	O
hence	O
the	O
post	O
title	O
)	O
.	O

Does	O
numpy	O
have	O
any	O
constructs	O
to	O
make	O
this	O
easier	O
?	O

If	O
you	O
view	O
`	O
P	O
`	O
as	O
a	O
rank-2	O
tensor	O
then	O
only	O
three	O
options	O
exist	O
for	O
the	O
product	O
of	O
`	O
P	O
`	O
with	O
itself	O
,	O
1	O
)	O
either	O
all	O
the	O
indexes	O
cancel	O
leaving	O
you	O
with	O
a	O
rank-0	O
tensor	O
(	O
a	O
scalar	O
)	O
,	O
2	O
)	O
1	O
set	O
of	O
indexes	O
cancels	O
and	O
you	O
are	O
left	O
with	O
a	O
rank-2	O
tensor	O
(	O
a	O
matrix	O
)	O
,	O
or	O
3	O
)	O
none	O
of	O
them	O
cancel	O
and	O
you're	O
left	O
with	O
a	O
rank-4	O
tensor	O
.	O

At	O
the	O
moment	O
I	O
made	O
a	O
custom	O
iterator	O
class	O
that	O
builds	O
a	O
list	O
of	O
lists	O
.	O

This	O
basically	O
sees	O
whether	O
two	O
circles	O
(	O
with	O
coordinates	O
that	O
correspond	O
to	O
the	O
indices	O
n	O
and	O
m	O
)	O
connect	O
.	O

It	O
would	O
mean	O
Gaussian	O
quadrature	O
using	O
points	O
along	O
the	O
line	O
that	O
are	O
easily	O
evaluated	O
.	O

I	O
see	O
one	O
potential	O
problem	O
-	O
beta	O
is	O
defined	O
as	O
1-dimensional	O
,	O
but	O
its	O
value	O
is	O
given	O
as	O
2-dimensional	O
(	O
dimensions	O
of	O
size	O
1	O
still	O
count	O
as	O
dimensions	O
)	O

Can	O
you	O
provide	O
an	O
algorithm	O
for	O
computing	O
them	O
based	O
on	O
block	O
size	O
?	O

If	O
`	O
X	O
`	O
is	O
your	O
array	O
,	O
#CODE	O

You	O
could	O
cast	O
the	O
array	O
to	O
a	O
list	O
:	O
#CODE	O

But	O
if	O
you	O
for	O
example	O
self	O
compiled	O
from	O
the	O
development	O
version	O
an	O
update	O
may	O
fix	O
most	O
of	O
it	O
.	O

In	O
VTK	O
I	O
am	O
able	O
to	O
use	O
the	O
following	O
snippet	O
to	O
save	O
the	O
render	O
window	O
as	O
an	O
image	O
.	O

In	O
other	O
words	O
,	O
the	O
4th	O
row	O
in	O
A_sorted	O
was	O
the	O
1st	O
row	O
in	O
the	O
original	O
array	O
,	O
A	O
,	O
etc	O
.	O

Add	O
a	O
`	O
return	O
`	O
statement	O
at	O
the	O
end	O
of	O
the	O
method	O
.	O

However	O
,	O
with	O
different	O
input	O
sizes	O
,	O
using	O
fft's	O
to	O
do	O
a	O
convolution	O
can	O
be	O
considerably	O
faster	O
(	O
Though	O
I	O
can't	O
seem	O
to	O
come	O
up	O
with	O
a	O
good	O
example	O
,	O
at	O
the	O
moment	O
...	O
)	O
.	O

That	O
is	O
a	O
shallow	O
copy	O
...	O

@USER	O
The	O
solutions	O
there	O
all	O
make	O
use	O
of	O
the	O
fact	O
that	O
only	O
a	O
3x3	O
sliding	O
window	O
is	O
needed	O
,	O
but	O
I	O
need	O
something	O
that	O
works	O
for	O
all	O
sizes	O
of	O
templates	O
.	O

A	O
masked	O
array	O
is	O
useful	O
here	O
:	O
#CODE	O

Second	O
I	O
would	O
like	O
it	O
to	O
be	O
easily	O
expandable	O
,	O
that	O
I	O
can	O
add	O
new	O
functions	O
easily	O
.	O

If	O
i	O
have	O
two	O
variables	O
-	O
where	O
they	O
either	O
are	O
a	O
1d	O
array	O
of	O
values	O
length	O
n	O
,	O
or	O
are	O
a	O
single	O
value	O
,	O
how	O
do	O
i	O
loop	O
through	O
them	O
so	O
that	O
I	O
get	O
n	O
values	O
returned	O
.	O

I	O
think	O
masked	O
arrays	O
have	O
been	O
in	O
numpy	O
for	O
a	O
few	O
years	O
now	O
.	O

I	O
have	O
an	O
array	O
defined	O
in	O
this	O
way	O
(	O
extracting	O
the	O
third	O
column	O
of	O
a	O
dataset	O
):	O
#CODE	O

I	O
wasn't	O
aware	O
of	O
the	O
option	O
of	O
using	O
`	O
data	O
[	O
list	O
]`	O
to	O
select	O
multiple	O
columns	O
.	O

Beware	O
that	O
if	O
the	O
type	O
of	O
the	O
output	O
array	O
is	O

Any	O
ideas	O
how	O
I	O
would	O
do	O
this	O
calculation	O
in	O
a	O
simpler	O
way	O
?	O

Those	O
take	O
up	O
about	O
15MB	O
of	O
space	O
which	O
,	O
considering	O
that	O
I	O
get	O
about	O
1000	O
result	O
files	O
in	O
a	O
series	O
,	O
is	O
unacceptable	O
to	O
save	O
.	O

I'm	O
trying	O
to	O
implement	O
a	O
logic	O
where	O
I'm	O
trying	O
to	O
subtract	O
each	O
element	O
of	O
an	O
array	O
from	O
every	O
other	O
element	O
of	O
the	O
array	O
and	O
then	O
find	O
the	O
minimum	O
difference	O
of	O
the	O
result	O
.	O

My	O
final	O
matrix	O
should	O
have	O
5416	O
rows	O
with	O
500	O
000	O
column	O
each	O
.	O

I	O
am	O
attempting	O
to	O
process	O
data	O
saved	O
to	O
CSV	O
that	O
may	O
have	O
missing	O
values	O
in	O
an	O
unknown	O
number	O
of	O
columns	O
(	O
up	O
to	O
around	O
30	O
)	O
.	O

Actually	O
the	O
surprise	O
is	O
still	O
hidden	O
in	O
a	O
way	O
,	O
because	O
in	O
your	O
examples	O
`	O
a	O
[	O
indices	O
]`	O
is	O
the	O
same	O
as	O
`	O
a	O
[	O
indices	O
[	O
0	O
]	O
,	O
indicies	O
[	O
1	O
]]`	O
but	O
`	O
a	O
[	O
indicies	O
,	O
:]	O
`	O
is	O
`	O
a	O
[(	O
indicies	O
[	O
0	O
]	O
,	O
indicies	O
[	O
1	O
])	O
,	O
:]	O
`	O
which	O
is	O
not	O
a	O
big	O
surprise	O
that	O
it	O
is	O
different	O
.	O

Slice	O
numpy	O
array	O
wth	O
list	O
of	O
wanted	O
rows	O

Here	O
is	O
a	O
slightly	O
more	O
complex	O
version	O
that	O
always	O
returns	O
a	O
view	O
into	O
the	O
original	O
array	O
(	O
of	O
course	O
provided	O
that	O
you	O
don't	O
do	O
any	O
advanced	O
indexing	O
;	O
this	O
should	O
be	O
guaranteed	O
by	O
your	O
specification	O
of	O
valid	O
indices	O
):	O
#CODE	O

If	O
the	O
array	O
is	O
one-dimensional	O
,	O
this	O
means	O
it	O
has	O
no	O
effect	O
.	O

The	O
problem	O
with	O
using	O
`	O
view	O
`	O
,	O
however	O
,	O
is	O
that	O
a	O
32-bit	O
integer	O
becomes	O
viewed	O
as	O
4	O
8-b	O
it	O
integers	O
,	O
and	O
we	O
only	O
care	O
about	O
the	O
value	O
in	O
the	O
last	O
8-b	O
its	O
.	O

So	O
this	O
won't	O
solve	O
OP's	O
question	O
(	O
unless	O
his	O
nD	O
is	O
2	O
or	O
3	O
)	O
.	O

Note	O
:	O
this	O
was	O
meant	O
as	O
a	O
comment	O
,	O
not	O
an	O
answer	O
...	O
just	O
needed	O
more	O
room	O
to	O
put	O
in	O
the	O
example	O
above	O
.	O

If	O
I	O
wanted	O
to	O
change	O
the	O
data	O
type	O
of	O
a	O
numpy	O
array	O
permanently	O
,	O
is	O
reassignment	O
the	O
best	O
way	O
?	O

The	O
shifted	O
shm-allocated	O
array	O
is	O
indeed	O
accessible	O
from	O
other	O
processes	O
.	O

The	O
second	O
solution	O
you	O
propose	O
is	O
better	O
from	O
that	O
point	O
of	O
view	O
.	O

It	O
checks	O
for	O
nans	O
and	O
empty	O
input	O
strings	O
.	O

The	O
ability	O
to	O
extract	O
columns	O
and	O
rows	O
by	O
header	O
name	O
(	O
as	O
in	O
the	O
above	O
example	O
)	O

With	O
Python	O
2.6.5	O
or	O
Python	O
2.7	O
,	O
and	O
Numpy	O
1.5.0	O
,	O
I	O
don't	O
get	O
any	O
error	O
.	O

Returns	O
a	O
boolean	O
array	O
the	O
same	O
length	O
as	O
`	O
ar1	O
`	O
that	O
is	O
True	O

If	O
the	O
objects	O
in	O
the	O
array	O
are	O
not	O
fixed	O
size	O
(	O
such	O
as	O
your	O
MultiEvent	O
)	O
the	O
operations	O
can	O
become	O
much	O
slower	O
.	O

@USER	O
:	O
True	O
the	O
code	O
I	O
gave	O
would	O
not	O
be	O
all	O
the	O
efficient	O

You	O
print	O
`	O
Length	O
of	O
together	O
2708000000	O
`	O
-	O
where	O
is	O
that	O
`	O
print	O
`	O
statement	O
in	O
your	O
code	O
?	O

Is	O
there	O
a	O
way	O
to	O
copy	O
just	O
the	O
reference	O
to	O
b	O
,	O
so	O
that	O
when	O
I	O
change	O
b	O
,	O
the	O
change	O
is	O
reflected	O
in	O
`	O
a	O
[	O
'	O
B	O
']	O
[	O
i	O
]`	O
?	O

If	O
the	O
matrix	O
is	O
not	O
symmetric	O
be	O
careful	O
about	O
the	O
order	O
in	O
dot	O
.	O

So	O
I	O
changed	O
the	O
"	O
array	O
"	O
to	O
matrix	O
.	O

config	O
paths	O
,	O
cflags	O
.	O

gnibbler	O
:	O
That	O
completely	O
misses	O
the	O
point	O
of	O
the	O
algorithm	O
,	O
this	O
is	O
a	O
simple	O
example	O
,	O
what	O
I	O
am	O
doing	O
is	O
Gauss-Seidel	O
iteration	O
,	O
which	O
infers	O
information	O
about	O
a	O
location	O
in	O
a	O
matrix	O
by	O
using	O
data	O
that	O
has	O
already	O
been	O
inferred	O
in	O
previous	O
entries	O
.	O

how	O
do	O
i	O
find	O
the	O
smallest	O
then	O
if	O
this	O
gives	O
me	O
the	O
n	O
greatest	O
values	O
?	O

@USER	O
:	O
you	O
can	O
wrap	O
the	O
insides	O
the	O
prange	O
loop	O
in	O
`	O
with	O
nogil	O
:	O
`	O
to	O
use	O
any	O
Python	O
constructs	O
.	O

For	O
each	O
`	O
Xi	O
`	O
greater	O
than	O
`	O
lower_limit_X	O
`	O
and	O
less	O
than	O
`	O
upper_limit_X	O
`	O
,	O
I	O
would	O
like	O
to	O
get	O
the	O
number	O
of	O
`	O
Yi	O
`'	O
s	O
that	O
are	O
greater	O
than	O
`	O
lower_limit_Y	O
`	O
and	O
less	O
than	O
`	O
upper_limit_Y	O
`	O
.	O

and	O
find	O
the	O
roots	O
?	O

I'd	O
like	O
to	O
generalize	O
this	O
to	O
an	O
ellipsoid	O
,	O
that	O
could	O
ideally	O
have	O
any	O
rotation	O
.	O

Is	O
there	O
any	O
way	O
to	O
integrate	O
the	O
entire	O
array	O
at	O
once	O
,	O
or	O
do	O
I	O
need	O
to	O
integrate	O
element-by-element	O
?	O

I	O
need	O
to	O
sum	O

with	O
'	O
f	O
'	O
and	O
'	O
F	O
'	O
,	O
or	O
before	O
and	O
after	O
the	O
decimal	O
point	O
for	O
a	O
floating	O

You	O
can	O
write	O
that	O
as	O
a	O
matrix	O
:	O
#CODE	O

In	O
fact	O
,	O
if	O
you	O
use	O
tuples	O
as	O
Justin	O
suggested	O
and	O
iterate	O
directly	O
over	O
the	O
rows	O
of	O
the	O
array	O
(	O
`	O
for	O
row	O
in	O
data	O
:	O
`)	O
,	O
it's	O
actually	O
faster	O
than	O
my	O
method	O
below	O
.	O

Using	O
the	O
`	O
ctypedef	O
`	O
keyword	O
in	O
Cython	O
will	O
make	O
it	O
add	O
the	O
C	O
/	O
C++	O
`	O
typedef	O
`	O
statement	O
with	O
the	O
given	O
types	O
in	O
the	O
compiled	O
Cython-code	O
.	O

Here	O
is	O
how	O
I	O
would	O
compute	O
a	O
subset	O
of	O
the	O
elements	O
of	O
C	O
given	O
a	O
list	O
of	O
tuples	O
of	O
C	O
index	O
values	O
.	O

In	O
my	O
specific	O
problem	O
`	O
A	O
,	O
B	O
`	O
are	O
slices	O
out	O
of	O
a	O
bigger	O
3-dimensional	O
array	O
`	O
Z	O
`	O
,	O

The	O
solution	O
to	O
get	O
all	O
the	O
data	O
you	O
need	O
as	O
you	O
build	O
the	O
list	O
,	O
by	O
using	O
the	O
accessor	O
on	O
each	O
iteration	O
.	O

Edit	O
:	O
Actually	O
you	O
could	O
also	O
sort	O
both	O
arrays	O
into	O
one	O
(	O
and	O
remember	O
which	O
one	O
belongs	O
to	O
which	O
class	O
)	O
,	O
then	O
go	O
from	O
there	O
by	O
checking	O
where	O
two	O
of	O
the	O
different	O
class	O
are	O
next	O
to	O
each	O
other	O
.	O

How	O
can	O
I	O
integrate	O
it	O
from	O
a	O
given	O
value	O
`	O
a	O
`	O
to	O
another	O
value	O
`	O
b	O
`	O
so	O
that	O
the	O
output	O
is	O
a	O
corresponding	O
array	O
?	O

I	O
want	O
to	O
multiply	O
a	O
sparse	O
matrix	O
A	O
,	O
with	O
a	O
matrix	O
B	O
which	O
has	O
0	O
,	O
-1	O
,	O
or	O
1	O
as	O
elements	O
.	O

If	O
you	O
turn	O
a	O
`	O
dict	O
`	O
into	O
an	O
array	O
,	O
you'll	O
get	O
an	O
object	O
array	O
.	O

`	O
Holy	O
CPU	O
cycles	O
batman	O
!	O

Note	O
:	O
Xarray	O
and	O
Yarray	O
are	O
each	O
single-column	O
vectors	O
with	O
data	O
at	O
each	O
index	O
that	O
links	O
the	O
two	O
arrays	O
as	O
sets	O
of	O
x	O
,	O
y	O
coordinates	O
.	O

Yeah	O
,	O
my	O
rule-of-thumb	O
is	O
**	O
numpy	O
**	O
for	O
anything	O
that	O
can	O
handle	O
small	O
amounts	O
of	O
latency	O
but	O
has	O
the	O
potential	O
to	O
be	O
very	O
large	O
,	O
**	O
lists	O
**	O
for	O
smaller	O
data	O
sets	O
where	O
latency	O
critical	O
,	O
and	O
of	O
course	O
**	O
real	O
benchmarking	O
**	O
FTW	O
:)	O

You	O
need	O
to	O
know	O
what	O
kind	O
of	O
information	O
is	O
stored	O
in	O
each	O
field	O
for	O
the	O
`	O
struct	O
`	O
module	O
to	O
make	O
sense	O
of	O
each	O
field	O
.	O

Once	O
you	O
have	O
this	O
array	O
,	O
you	O
can	O
get	O
your	O
sums	O
for	O
each	O
vertex	O
as	O
#CODE	O

Where	O
are	O
`	O
plot	O
`	O
and	O
`	O
show	O
`	O
coming	O
from	O
?	O

But	O
it	O
might	O
be	O
easier	O
(	O
and	O
more	O
understandable	O
when	O
you	O
look	O
at	O
the	O
code	O
in	O
the	O
future	O
)	O
to	O
just	O
drop	O
into	O
Cython	O
to	O
get	O
this	O
done	O
.	O

I	O
can't	O
comment	O
on	O
the	O
Perl	O
code	O
,	O
I	O
simply	O
don't	O
know	O
it	O
at	O
all	O
.	O

I	O
updated	O
the	O
answer	O
with	O
a	O
full	O
example	O
.	O

There	O
you	O
will	O
find	O
C	O
code	O
which	O
provides	O
the	O
same	O
functionality	O
as	O
`	O
fmincon	O
`	O
.	O

so	O
for	O
instance	O
,	O
for	O
an	O
array	O
the	O
code	O
looked	O
like	O
#CODE	O

I've	O
been	O
trying	O
to	O
find	O
a	O
solution	O
for	O
hours	O
.	O

Note	O
that	O
as	O
implemented	O
here	O
,	O
the	O
first	O
method	O
gives	O
incorrect	O
results	O
according	O
to	O
my	O
test	O
.	O

This	O
is	O
my	O
test	O
code	O
:	O
#CODE	O

This	O
will	O
short-circuit	O
if	O
you	O
just	O
want	O
to	O
determine	O
if	O
any	O
match	O
exists	O
.	O

It	O
also	O
helps	O
to	O
know	O
that	O
a	O
resource	O
like	O
Wolfram	O
Alpha	O
is	O
available	O
to	O
you	O
at	O
all	O
times	O
.	O

What	O
I	O
meant	O
was	O
if	O
we	O
had	O
three	O
arrays	O
`	O
X	O
=[	O
0	O
1	O
2	O
]	O
,	O
Y	O
=[	O
0	O
1	O
1	O
]	O
,	O
and	O
Z	O
=[	O
0	O
2	O
2	O
]`	O
there	O
would	O
be	O
six	O
values	O
in	O
the	O
range	O
of	O
greater	O
than	O
or	O
equal	O
1	O
and	O
and	O
less	O
than	O
or	O
equal	O
to	O
2	O
.	O

You	O
should	O
really	O
split	O
this	O
into	O
two	O
separate	O
questions	O
since	O
each	O
part	O
is	O
distinct	O
.	O

This	O
doesn't	O
answer	O
your	O
problem	O
exactly	O
,	O
but	O
I	O
think	O
especially	O
with	O
the	O
sum	O
issue	O
that	O
you	O
should	O
see	O
significant	O
speedups	O
with	O
these	O
changes	O
.	O

Basically	O
you're	O
iterating	O
through	O
each	O
item	O
in	O
`	O
Xa	O
`	O
and	O
omitting	O
the	O
ones	O
that	O
don't	O
fall	O
with	O
the	O
range	O
.	O

This	O
code	O
has	O
been	O
written	O
following	O
the	O
tips	O
(	O
and	O
copy	O
/	O
pasting	O
):	O
#URL	O

I	O
would	O
look	O
at	O
this	O
question	O
:	O
#URL	O

As	O
@USER	O
-Anderson	O
asked	O
,	O
why	O
not	O
avoid	O
making	O
an	O
array	O
?	O

to	O
create	O
such	O
an	O
array	O
.	O

In	O
short	O
,	O
I	O
find	O
class	O
C	O
to	O
provide	O
an	O
implementation	O
that	O
is	O
over	O
60x	O
faster	O
than	O
the	O
method	O
in	O
the	O
original	O
post	O
.	O

I	O
suggest	O
allocating	O
an	O
array	O
of	O
the	O
correct	O
size	O
up-front	O
,	O
then	O
populating	O
it	O
with	O
data	O
in	O
each	O
iteration	O
.	O

edited	O
my	O
solution	O
to	O
include	O
what	O
I	O
would	O
do	O
.	O
let	O
me	O
know	O
if	O
this	O
work	O
,	O
as	O
i	O
cannot	O
fully	O
test	O
because	O
i	O
dont	O
have	O
test	O
data	O
.	O

The	O
problem	O
is	O
if	O
i	O
don't	O
want	O
0:10	O
,	O
but	O
an	O
arbitrary	O
set	O
of	O
indices	O
.	O

and	O
it	O
occurs	O
at	O
the	O
line	O
:	O
`	O
del	O
innerAry	O
[	O
j	O
]`	O

Do	O
you	O
have	O
any	O
references	O
to	O
back	O
up	O
that	O
`	O
NaN	O
handling	O
is	O
much	O
slower	O
than	O
"	O
normal	O
"	O
float	O
at	O
the	O
CPU	O
level	O
`	O
?	O

What	O
do	O
you	O
mean	O
by	O
the	O
line	O
?	O

For	O
example	O
if	O
we	O
stick	O
with	O
a	O
linear	O
search	O
we	O
can	O
at	O
least	O
start	O
at	O
the	O
appropriate	O
end	O
(	O
search	O
backwards	O
to	O
find	O
last	O
value	O
matching	O
a	O
condition	O
)	O
.	O

Yes	O
,	O
but	O
you	O
don't	O
get	O
a	O
numpy	O
array	O
out	O
,	O
do	O
you	O
?	O

This	O
seems	O
like	O
a	O
simple	O
question	O
,	O
but	O
I	O
haven't	O
been	O
able	O
to	O
find	O
a	O
good	O
answer	O
.	O

@USER	O
:	O
it	O
could	O
also	O
mean	O
they	O
are	O
both	O
strictly	O
periodic	O
and	O
sinusoidal	O
,	O
but	O
their	O
frequencies	O
are	O
integer-independent	O
(	O
ie	O
.	O
the	O
interference	O
wave	O
is	O
not	O
periodic	O
)	O

Python	O
:	O
How	O
to	O
rotate	O
an	O
array	O
?	O

@USER	O
Your	O
edited	O
sample	O
input	O
is	O
still	O
not	O
consistent	O
with	O
your	O
expected	O
output	O
.	O

You	O
can	O
calculate	O
the	O
variance	O
yourself	O
using	O
the	O
mean	O
,	O
with	O
the	O
following	O
formula	O
:	O
#CODE	O

Populate	O
numpy	O
matrix	O
from	O
the	O
difference	O
of	O
two	O
vectors	O

where	O
#CODE	O

Interleaving	O
two	O
numpy	O
index	O
arrays	O
,	O
one	O
item	O
from	O
each	O
array	O

I	O
was	O
worried	O
about	O
the	O
performance	O
,	O
but	O
the	O
difference	O
in	O
load	O
time	O
is	O
tiny	O
for	O
me	O
.	O

I	O
looked	O
for	O
an	O
online	O
reference	O
but	O
couldn't	O
find	O
one	O
.	O

Numpy	O
/	O
Python	O
:	O
Array	O
iteration	O
without	O
for-loop	O

If	O
you	O
want	O
it	O
printed	O
with	O
commas	O
,	O
you	O
could	O
convert	O
it	O
to	O
a	O
Python	O
list	O
:	O
#CODE	O

I	O
have	O
included	O
my	O
code	O
to	O
see	O
if	O
you	O
can	O
help	O
me	O
implement	O
some	O
kind	O
of	O
'	O
fminsearch	O
'	O
to	O
find	O
the	O
optimal	O
parameter	O
values	O
k0	O
and	O
k1	O
that	O
will	O
fit	O
my	O
data	O
.	O

Any	O
ideas	O
what	O
this	O
could	O
be	O
all	O
about	O
?	O

What	O
does	O
"	O
IIUC	O
"	O
mean	O
?	O

I	O
can't	O
seem	O
to	O
find	O
examples	O
that	O
don't	O
rely	O
on	O
the	O
former	O
syntax	O
.	O

Any	O
help	O
would	O
be	O
greatly	O
appreciated	O
.	O

Where	O
blocks	O
is	O
a	O
3	O
dimensional	O
numpy	O
array	O
.	O

The	O
science	O
/	O
engineering	O
application	O
I'm	O
working	O
on	O
has	O
lots	O
of	O
linear	O
algebra	O
matrix	O
multiplications	O
,	O
therefore	O
I	O
use	O
Numpy	O
matrices	O
.	O

when	O
I	O
print	O
Chao	O
,	O
the	O
product	O
of	O
this	O
loop	O
I	O
currently	O
have	O
this	O
:	O

pyopengl	O
buffer	O
dynamic	O
read	O
from	O
numpy	O
array	O

I	O
also	O
have	O
an	O
array	O
which	O
is	O
my	O
desired	O
subset	O
of	O
ages	O
.	O

It	O
does	O
automatically	O
expand	O
the	O
array	O
,	O
but	O
now	O
every	O
item	O
in	O
the	O
array	O
is	O
`	O
None	O
`	O
and	O
cannot	O
be	O
changed	O
.	O

I'm	O
liking	O
fortran	O
more	O
at	O
the	O
moment	O
because	O
by	O
the	O
time	O
you	O
add	O
all	O
the	O
required	O
type	O
annotations	O
in	O
cython	O
I	O
think	O
it	O
ends	O
up	O
looking	O
less	O
clear	O
than	O
the	O
fortran	O
.	O

(	O
There	O
are	O
also	O
chances	O
that	O
Python	O
stores	O
the	O
number	O
on	O
the	O
heap	O
and	O
all	O
you	O
get	O
is	O
a	O
pointer	O
to	O
it	O
,	O
approximately	O
doubling	O
the	O
footprint	O
,	O
without	O
even	O
taking	O
in	O
account	O
metadata	O
but	O
that's	O
slippery	O
grounds	O
,	O
I'm	O
always	O
wrong	O
when	O
I	O
talk	O
about	O
Python	O
internals	O
,	O
so	O
let's	O
not	O
dig	O
it	O
too	O
much	O
.	O
)	O

Do	O
you	O
have	O
any	O
clue	O
?	O

your	O
method	O
works	O
,	O
@USER	O
.	O
put	O
it	O
into	O
an	O
answer	O
to	O
get	O
some	O
acceptance	O
points	O
.	O

that	O
blas	O
is	O
reference	O
blas	O
from	O
netlib	O
-	O
the	O
slowest	O
blas	O
around	O
.	O
install	O
atlas	O
or	O
mkl	O
instead	O
.	O

The	O
`	O
not	O
`	O
operator	O
implicitly	O
tries	O
to	O
convert	O
its	O
operand	O
to	O
`	O
bool	O
`	O
,	O
and	O
then	O
returns	O
the	O
opposite	O
truth	O
value	O
.	O

The	O
recommended	O
way	O
to	O
do	O
this	O
is	O
to	O
preallocate	O
before	O
the	O
loop	O
and	O
use	O
slicing	O
and	O
indexing	O
to	O
insert	O
#CODE	O

If	O
the	O
simple	O
sort	O
solution	O
is	O
good	O
enough	O
,	O
clearly	O
go	O
for	O
that	O
.	O

Basic	O
idea	O
being	O
,	O
I	O
know	O
the	O
actual	O
value	O
of	O
that	O
should	O
be	O
predicted	O
for	O
each	O
sample	O
in	O
a	O
row	O
of	O
N	O
,	O
and	O
I'd	O
like	O
to	O
determine	O
which	O
set	O
of	O
predicted	O
values	O
in	O
a	O
column	O
of	O
M	O
is	O
most	O
accurate	O
using	O
the	O
residuals	O
.	O

Fill	O
in	O
missing	O
values	O
with	O
nearest	O
neighbour	O
in	O
Python	O
numpy	O
masked	O
arrays	O
?	O

Any	O
tips	O
on	O
what	O
I'm	O
doing	O
wrong	O
?	O

SO	O
I	O
have	O
a	O
file	O
having	O
three	O
columns	O
;	O
Frequency	O
,	O
Power	O
spec	O
.	O

In	O
numpy	O
,	O
your	O
array	O
is	O
2	O
x	O
5	O
,	O
isn't	O
it	O
?	O

For	O
small-ish	O
problems	O
,	O
I	O
would	O
certainly	O
just	O
create	O
the	O
new	O
array	O
.	O

If	O
each	O
element	O
takes	O
up	O
4	O
bytes	O
,	O
it	O
would	O
require	O
4,000,000,000,000	O
bytes	O
of	O
memory	O
.	O

TypeError	O
:	O
must	O
be	O
str	O
,	O
not	O
bytes	O

Error	O
while	O
computing	O
probabilities	O
of	O
an	O
array	O
list	O

This	O
produces	O
the	O
array	O
,	O
but	O
I	O
don't	O
know	O
which	O
row	O
corresponds	O
to	O
which	O
year-disease	O
.	O

I'm	O
not	O
sure	O
what	O
you	O
mean	O
by	O
"	O
all	O
from	O
numpy	O
"	O
,	O
but	O
you	O
should	O
never	O
need	O
to	O
use	O
more	O
than	O
one	O
form	O
of	O
`	O
import	O
`	O
at	O
a	O
time	O
.	O

The	O
problem	O
is	O
that	O
you	O
have	O
an	O
array	O
of	O
strings	O
,	O
not	O
an	O
array	O
of	O
numbers	O
.	O

how	O
can	O
i	O
effectively	O
check	O
items	O
of	O
a	O
list	O
of	O
tuples	O
against	O
all	O
the	O
items	O
of	O
another	O
using	O
numpy	O
or	O
tabular	O
?	O

Replace	O
part	O
of	O
numpy	O
1D	O
array	O
with	O
shorter	O
array	O

Python	O
2.6	O
numpy	O
interaction	O
array	O
objects	O
error	O

So	O
for	O
square	O
matrices	O
it	O
is	O
basically	O
syntactic	O
sugar	O
for	O
the	O
exact	O
same	O
operation	O
.	O

If	O
we	O
evaulate	O
the	O
ij	O
th	O
element	O
of	O
the	O
matrix	O
U*A*V	O
,	O
then	O
it	O
must	O
equal	O
both	O
#CODE	O

How	O
can	O
I	O
get	O
new	O
array	O
`	O
B	O
`	O
such	O
as	O
if	O
`	O
row_set	O
=	O
[	O
0	O
,	O
2	O
,	O
5	O
]`	O
,	O
then	O
`	O
B	O
=	O
[	O
A_row	O
[	O
0	O
]	O
,	O
A_row	O
[	O
2	O
]	O
,	O
A_row	O
[	O
5	O
]]`	O
?	O

I	O
have	O
tried	O
the	O
following	O
to	O
fix	O
it	O
:	O

The	O
weighted	O
sum	O
result	O
is	O
approximated	O
by	O
the	O
multiple	O
passes	O
and	O
actually	O
after	O
very	O
few	O
of	O
them	O
the	O
output	O
is	O
already	O
smooth	O
.	O

Basically	O
,	O
it	O
comes	O
down	O
to	O
checking	O
before	O
you	O
add	O
.	O

Any	O
ideas	O
?	O

Do	O
you	O
really	O
have	O
matrix	O
type	O
or	O
just	O
list	O
of	O
lists	O
from	O
python	O
?	O

zI	O
[	O
N-1	O
]	O
=	O
f	O
(	O
xI	O
[	O
N-1	O
]	O
,	O
yI	O
[	O
N-1	O
])	O
.	O

The	O
actual	O
size	O
of	O
the	O
numpy	O
array	O
is	O
514	O
by	O
504	O
and	O
of	O
the	O
list	O
is	O
8	O
.	O

`	O
array	O
([[	O
a	O
,	O
a	O
,	O
a	O
,	O
a	O
,	O
0	O
,	O
0	O
,	O
0	O
,	O
0	O
,	O
0	O
,	O
b	O
,	O
b	O
,	O
b	O
,...	O
]	O
,	O

What's	O
wrong	O
with	O
just	O
separating	O
it	O
out	O
into	O
real	O
and	O
imaginary	O
parts	O
?	O

I	O
have	O
an	O
N-dimensional	O
array	O
and	O
a	O
set	O
N	O
index	O
arrays	O
,	O
who's	O
values	O
I	O
want	O
to	O
increment	O
.	O

As	O
@USER	O
suggests	O
in	O
a	O
comment	O
,	O
if	O
you	O
really	O
want	O
a	O
3D	O
array	O
--	O
which	O
is	O
not	O
entirely	O
clear	O
to	O
me	O
from	O
your	O
code	O
sample	O
--	O
you	O
can	O
use	O
:	O
#CODE	O

In	O
another	O
question	O
,	O
other	O
users	O
offered	O
some	O
help	O
if	O
I	O
could	O
supply	O
the	O
array	O
I	O
was	O
having	O
trouble	O
with	O
.	O

The	O
other	O
thing	O
is	O
changing	O
the	O
size	O
of	O
the	O
ticklabels	O
in	O
the	O
colorbar	O
which	O
I	O
haven't	O
figured	O
out	O
.	O

Unfortunately	O
,	O
these	O
lines	O
are	O
fast	O
already	O
,	O
but	O
I	O
will	O
take	O
any	O
speedup	O
to	O
offset	O
the	O
IO	O
issues	O
I	O
have	O
using	O
GDAL	O
.	O

First	O
you	O
need	O
to	O
write	O
a	O
function	O
that	O
when	O
given	O
an	O
array	O
of	O
values	O
,	O
with	O
the	O
middle	O
one	O
being	O
the	O
element	O
currently	O
examined	O
,	O
will	O
return	O
some	O
computation	O
of	O
those	O
values	O
.	O

Thanks	O
in	O
advance	O
for	O
any	O
help	O
.	O

so	O
what	O
you	O
want	O
is	O
some	O
sort	O
of	O
recursive	O
assignment	O
--	O
but	O
i	O
don't	O
believe	O
there	O
is	O
any	O
guarantee	O
that	O
this	O
will	O
settle	O
down	O
into	O
a	O
constant	O
value	O
.	O
sure	O
it	O
does	O
in	O
your	O
case	O
,	O
but	O
not	O
in	O
general	O
--	O
for	O
example	O
:	O
`	O
a	O
[:	O
]	O
=	O
2*a	O
[:	O
]`	O
would	O
loop	O
forever	O
.	O

Just	O
initialize	O
the	O
array	O
of	O
`	O
float*	O
`	O
to	O
point	O
to	O
each	O
of	O
the	O
rows	O
in	O
the	O
2-D	O
array	O
.	O

Do	O
things	O
go	O
wrong	O
gradually	O
and	O
more	O
and	O
more	O
,	O
or	O
all	O
at	O
once	O
?	O

If	O
you	O
don't	O
find	O
anything	O
useful	O
then	O
try	O
"	O
R	O
"	O
.	O

Unfortunately	O
,	O
when	O
I	O
tried	O
it	O
I	O
got	O
the	O
error	O
:	O
"	O
ValueError	O
:	O
array	O
is	O
too	O
big	O
.	O

EDIT	O
2	O
:	O
This	O
raises	O
another	O
question	O
-	O
What	O
is	O
`	O
env	O
`	O
and	O
why	O
does	O
`	O
make	O
`	O
add	O
it	O
?	O

However	O
it	O
doesn't	O
give	O
you	O
negative	O
overflows	O
,	O
probably	O
because	O
`	O
uint32	O
`	O
fits	O
inside	O
the	O
positive	O
values	O
of	O
the	O
`	O
int64	O
`	O
.	O

Consider	O
for	O
example	O
the	O
array	O
:	O
#CODE	O

One	O
difference	O
could	O
be	O
the	O
result	O
of	O
python	O
having	O
to	O
take	O
extra	O
steps	O
to	O
resolve	O
the	O
float64	O
types	O
.	O

How	O
do	O
I	O
turn	O
this	O
into	O
a	O
numpy	O
matrix	O
?	O

I	O
want	O
to	O
rotate	O
an	O
array	O
but	O
not	O
as	O
a	O
whole	O
,	O
only	O
small	O
portion	O
of	O
it	O
.	O

is	O
so	O
much	O
more	O
readable	O
than	O
any	O
dot	O
(	O
a	O
,	O
b	O
)	O
equivalent	O
.	O

The	O
array	O
looks	O
like	O
:	O
#CODE	O

Constructing	O
an	O
n-by-n	O
matrix	O
in	O
Numpy	O
is	O
easy	O
and	O
fairly	O
efficient	O
.	O

Numpy	O
array	O
broadcasting	O
with	O
vector	O
parameters	O

It	O
only	O
works	O
like	O
this	O
for	O
numpy	O
`	O
array	O
`	O
s	O
.	O

@USER	O
:	O
Also	O
there's	O
a	O
mistake	O
in	O
your	O
code	O
I	O
think	O
:	O
because	O
when	O
you	O
set	O
elements	O
to	O
NaN	O
in	O
each	O
iteration	O
,	O
the	O
elements	O
are	O
not	O
restored	O
to	O
their	O
pre-NaN	O
values	O
for	O
the	O
next	O
iteration	O
!	O

Quick	O
question	O
:	O
is	O
there	O
any	O
reason	O
why	O
you	O
use	O

Now	O
the	O
question	O
is	O
,	O
which	O
equal	O
area	O
projection	O
shall	O
I	O
choose	O
in	O
order	O
to	O
have	O
comparable	O
area	O
sizes	O
for	O
the	O
polygons	O
.	O

numpy	O
array	O
multiplication	O
issue	O

(	O
2	O
)	O
When	O
I	O
change	O
the	O
connection	O
keywords	O
to	O
check_same_thread=False	O
,	O
then	O
the	O
full	O
pool	O
of	O
workers	O
is	O
used	O
,	O
but	O
then	O
only	O
some	O
queries	O
succeed	O
and	O
some	O
queries	O
fail	O
.	O

It	O
also	O
has	O
the	O
advantage	O
of	O
being	O
able	O
to	O
load	O
and	O
store	O
transparently	O
with	O
HDF5	O
.	O

Bah	O
:	O
"	O
operates	O
on	O
two	O
n-dimensional	O
arrays	O
"	O
should	O
be	O
"	O
operates	O
on	O
an	O
n-dimensional	O
array	O
"	O
above	O
.	O

Instead	O
,	O
they	O
expect	O
the	O
user	O
to	O
either	O
pass	O
an	O
array	O
of	O
shape	O
`	O
(	O
r	O
,	O
c	O
)`	O
exactly	O
,	O
or	O
for	O
the	O
user	O
to	O
pass	O
a	O
1-D	O
array	O
that	O
broadcasts	O
up	O
to	O
shape	O
`	O
(	O
r	O
,	O
c	O
)`	O
.	O

Though	O
,	O
it	O
isn't	O
so	O
straight	O
forward	O
because	O
I	O
don't	O
necessarily	O
know	O
how	O
many	O
duplicates	O
of	O
each	O
lon	O
or	O
lat	O
there	O
are	O
which	O
determines	O
the	O
shape	O
of	O
the	O
array	O
.	O

geom	O
function	O
takes	O
an	O
n+1	O
X	O
2	O
array	O
and	O
n	O
as	O
input	O
,	O
i	O
guess	O
i'm	O
doing	O
something	O
really	O
stupid	O
(	O
which	O
i	O
think	O
i	O
am	O
)	O
or	O
i	O
don't	O
understand	O
this	O
behavior	O
#CODE	O

When	O
I	O
tried	O
this	O
,	O
I	O
got	O
sort	O
of	O
similar	O
shaped	O
"	O
tiles	O
"	O
of	O
different	O
colors	O
rather	O
than	O
3	O
Gaussian	O
humps	O
.	O

There	O
was	O
a	O
comment	O
here	O
saying	O
that	O
the	O
Apple	O
version	O
of	O
python	O
2.7	O
comes	O
with	O
numpy	O
so	O
you	O
shouldn't	O
have	O
to	O
install	O
it	O
at	O
all	O
.	O

For	O
more	O
general	O
solution	O
,	O
you	O
could	O
use	O
somekind	O
of	O
edge	O
detection	O
method	O
to	O
find	O
only	O
the	O
edge	O
points	O
.	O

If	O
you	O
see	O
any	O
errors	O
;	O
provide	O
a	O
link	O
to	O
the	O
code	O
that	O
can	O
be	O
run	O
.	O

What	O
I	O
am	O
looking	O
for	O
is	O
a	O
quick	O
and	O
easy	O
way	O
to	O
find	O
the	O
closest	O
(	O
nearest	O
neighbor	O
)	O
of	O
some	O
multidimensional	O
query	O
point	O
in	O
an	O
2D	O
(	O
numpy	O
)	O
array	O
of	O
multidimensional	O
points	O
(	O
also	O
numpy	O
arrays	O
)	O
.	O

How	O
do	O
I	O
get	O
the	O
`	O
Image	O
`	O
part	O
only	O
and	O
how	O
do	O
I	O
convert	O
it	O
to	O
Numpy	O
Array	O
?	O

Maybe	O
it'll	O
save	O
you	O
some	O
frustration	O
=)	O

Is	O
there	O
any	O
way	O
to	O
rewrite	O
this	O
functions	O
with	O
Numpy	O
?	O

Is	O
there	O
any	O
easy	O
way	O
to	O
speed	O
my	O
calculation	O
up	O
?	O

A	O
variable	O
in	O
Python	O
is	O
just	O
a	O
label	O
for	O
an	O
object	O
;	O
giving	O
the	O
object	O
a	O
new	O
label	O
doesn't	O
change	O
the	O
object	O
itself	O
at	O
all	O
.	O

But	O
this	O
is	O
actually	O
where	O
the	O
doc	O
belongs	O
.	O

So	O
given	O
the	O
sorted	O
version	O
,	O
you	O
can	O
reconstruct	O
the	O
original	O
by	O
"	O
putting	O
items	O
back	O
where	O
they	O
came	O
from	O
"	O
:	O
#CODE	O

Note	O
that	O
this	O
all	O
assumes	O
that	O
your	O
values	O
are	O
normally	O
distributed	O
.	O

(	O
Bounding	O
box	O
intersections	O
are	O
actually	O
a	O
rather	O
poor	O
way	O
of	O
deciding	O
where	O
to	O
place	O
labels	O
.	O
What's	O
the	O
point	O
in	O
writing	O
a	O
ton	O
of	O
code	O
for	O
something	O
that	O
will	O
only	O
work	O
in	O
one	O
case	O
out	O
of	O
1000	O
?	O
)	O

The	O
python	O
code	O
outputs	O
eleven	O
0's	O
,	O
eleven	O
1's	O
all	O
the	O
way	O
to	O
eleven	O
39's	O
.	O

@USER	O
;	O
in	O
above	O
example	O
z	O
is	O
(	O
5	O
,	O
2	O
)	O
array	O
created	O
by	O
another	O
function	O
,	O
with	O
first	O
dimension	O
from	O
the	O
number	O
of	O
True	O
(	O
at	O
least	O
one	O
True	O
in	O
x	O
>	O
y	O
)	O
,	O
here	O
5	O
,	O
and	O
second	O
dimention	O
as	O
the	O
first	O
dimension	O
of	O
x	O
,	O
here	O
2	O
.	O

I	O
have	O
a	O
NumPy	O
array	O
of	O
values	O
.	O

I	O
just	O
need	O
the	O
total	O
of	O
all	O
the	O
values	O
instead	O
of	O
the	O
actual	O
values	O
themselves	O
.	O

Didn't	O
think	O
about	O
multiplying	O
my	O
array	O
of	O
number	O
by	O
an	O
array	O
of	O
booleans	O
to	O
extract	O
my	O
data	O
.	O

not	O
to	O
convert	O
floats	O
to	O
floats	O
would	O
be	O
the	O
first	O
step	O
.	O

make	O
a	O
list	O
of	O
all	O
the	O
days	O

This	O
way	O
,	O
you	O
could	O
access	O
all	O
the	O
`	O
A	O
`	O
through	O
`	O
result	O
[	O
'	O
label	O
']	O
[	O
'	O
A	O
']`	O
...	O

picking	O
out	O
elements	O
based	O
on	O
complement	O
of	O
indices	O
in	O
Python	O
pandas	O

Joran	O
,	O
could	O
you	O
please	O
explain	O
what	O
you	O
mean	O
more	O
?	O

Anyone	O
have	O
any	O
clues	O
for	O
what	O
I	O
can	O
do	O
,	O
or	O
approaches	O
I	O
should	O
research	O
?	O

Appending	O
data	O
to	O
an	O
existing	O
array	O
is	O
a	O
natural	O
thing	O
to	O
want	O
to	O
do	O
for	O
anyone	O
with	O
python	O
experience	O
.	O

Note	O
that	O
you	O
get	O
a	O
sorte	O
copy	O
of	O
the	O
array	O
.	O

I'm	O
not	O
sure	O
that	O
this	O
is	O
the	O
way	O
that	O
you	O
should	O
do	O
things	O
as	O
I'd	O
expect	O
numpy	O
to	O
have	O
a	O
much	O
more	O
efficient	O
method	O
of	O
going	O
about	O
it	O
,	O
but	O
do	O
you	O
just	O
mean	O
something	O
like	O
this	O
?	O

`	O
KMID	O
`	O
is	O
a	O
function	O
,	O
not	O
an	O
array	O
,	O
so	O
you	O
can't	O
index	O
it	O
with	O
`	O
:	O
`	O
.	O

Insert	O
to	O
original	O
code	O
in	O
question	O
:	O
#CODE	O

Multiple	O
conditions	O
using	O
'	O
or	O
'	O
in	O
numpy	O
array	O

Usually	O
,	O
it's	O
best	O
to	O
avoid	O
the	O
matrix	O
class	O
(	O
see	O
docs	O
)	O
.	O

I	O
have	O
tried	O
two	O
different	O
methods	O
but	O
both	O
of	O
them	O
are	O
slow	O
.	O

Not	O
really	O
,	O
you	O
can	O
construct	O
the	O
Counter	O
from	O
any	O
iterable	O
.	O

(	O
I	O
have	O
the	O
code	O
ready	O
,	O
but	O
as	O
i'm	O
new	O
to	O
stackoverflow	O
,	O
i	O
don't	O
know	O
where	O
to	O
put	O
it	O
.	O
Here	O
,	O
in	O
this	O
comment	O
field	O
?	O
Or	O
rather	O
making	O
a	O
new	O
answer	O
??	O
)	O

For	O
example	O
,	O
one	O
simple	O
method	O
to	O
generate	O
at	O
most	O
rank	O
`	O
k	O
`	O
(	O
when	O
`	O
m	O
`	O
is	O
`	O
k+1	O
`)	O
is	O
to	O
get	O
a	O
random	O
valid	O
B	O
0	O
,	O
keep	O
rotating	O
all	O
rows	O
of	O
this	O
matrix	O
up	O
to	O
get	O
B	O
1	O
to	O
B	O
m-2	O
,	O
set	O
first	O
row	O
of	O
B	O
m-1	O
to	O
all	O
1	O
,	O
and	O
the	O
remaining	O
rows	O
to	O
all	O
0	O
.	O

Glad	O
you	O
saw	O
around	O
it	O
!	O

@USER	O
:	O
basically	O
I	O
am	O
converting	O
some	O
matlab	O
code	O
into	O
python	O
,	O
I	O
can	O
not	O
write	O
actual	O
code	O
because	O
that	O
is	O
confidential	O
,	O
(	O
1+float	O
(	O
100	O
))	O
Here	O
100	O
is	O
coming	O
from	O
two	O
dimension	O
string	O
matrix	O
,	O
that	O
why	O
I	O
have	O
written	O
float	O
to	O
convert	O
string	O
variable	O
.	O

instead	O
of	O
call	O
plot	O
(	O
test	O
[	O
"	O
x	O
"]	O
[	O
5:10	O
])	O
,	O
you	O
can	O
call	O
the	O
plot	O
method	O
of	O
Series	O
object	O
:	O
#CODE	O

The	O
size	O
of	O
a	O
slice	O
with	O
`	O
0:5	O
`	O
is	O
not	O
6	O
as	O
you	O
say	O
:	O
it's	O
5	O
.	O

This	O
ensures	O
proper	O
display	O
and	O
syntax	O
highlighting	O
-	O
right	O
now	O
someone	O
who	O
would	O
usually	O
fix	O
your	O
formatting	O
is	O
likely	O
to	O
not	O
do	O
it	O
because	O
he'd	O
have	O
to	O
remove	O
all	O
the	O
HTML	O
linebreaks	O
on	O
his	O
own	O
.	O

I	O
believe	O
you've	O
reduced	O
the	O
problem	O
to	O
a	O
one	O
of	O
finding	O
roots	O
.	O

In	O
order	O
to	O
make	O
sure	O
it	O
is	O
still	O
multiprocessor	O
safe	O
,	O
I	O
believe	O
you	O
will	O
have	O
to	O
use	O
the	O
`	O
acquire	O
`	O
and	O
`	O
release	O
`	O
methods	O
that	O
exist	O
on	O
the	O
`	O
Array	O
`	O
object	O
,	O
`	O
a	O
`	O
,	O
and	O
its	O
built	O
in	O
lock	O
to	O
make	O
sure	O
its	O
all	O
safely	O
accessed	O
(	O
though	O
I'm	O
not	O
an	O
expert	O
on	O
the	O
multiprocessor	O
module	O
)	O
.	O

So	O
,	O
given	O
your	O
matrix	O
M	O
,	O
your	O
problem	O
asks	O
to	O
maximize	O
the	O
PB	O
function	O
#CODE	O

I	O
would	O
like	O
to	O
find	O
all	O
elements	O
within	O
a	O
specific	O
range	O
.	O

I	O
cannot	O
seem	O
to	O
find	O
how	O
to	O
do	O
that	O
.	O

Any	O
thoughts	O
?	O

If	O
you	O
want	O
the	O
PRNGs	O
to	O
be	O
independent	O
,	O
do	O
not	O
seed	O
them	O
with	O
anything	O
.	O

In	O
the	O
end	O
I'll	O
probably	O
take	O
n	O
randomly	O
selected	O
samples	O
.	O

This	O
is	O
a	O
mystery	O
to	O
me	O
,	O
though	O
I	O
would	O
guess	O
that	O
there	O
must	O
be	O
more	O
overhead	O
associated	O
with	O
accessing	O
an	O
array	O
element	O
than	O
with	O
appending	O
to	O
a	O
list	O
.	O

If	O
it's	O
not	O
reasonable	O
,	O
you	O
can	O
always	O
decompose	O
the	O
matrix	O
multiplication	O
yourself	O
.	O

If	O
the	O
vectors	O
do	O
not	O
have	O
equal	O
dimension	O
,	O
or	O
if	O
you	O
want	O
to	O
avoid	O
numpy	O
,	O
then	O
perhaps	O
,	O
#CODE	O

In	O
particular	O
,	O
you	O
can't	O
index	O
a	O
2D	O
matrix	O
with	O
a	O
single	O
integer	O
,	O
because	O
--	O
well	O
--	O
it's	O
two	O
dimensional	O
and	O
you	O
need	O
to	O
specify	O
two	O
integers	O
,	O
hence	O
the	O
need	O
for	O
the	O
extra	O
0	O
index	O
in	O
the	O
second	O
example	O
.	O

nearly	O
all	O
of	O
which	O
the	O
author	O
responded	O
to	O
and	O
in	O
some	O
cases	O
,	O

I	O
added	O
the	O
slow	O
Python	O
code	O
to	O
the	O
description	O
.	O

The	O
easy	O
way	O
-	O
pick	O
a	O
random	O
number	O
q	O
[	O
0	O
,	O
1	O
]	O
.	O

will	O
be	O
the	O
number	O
of	O
bytes	O
which	O
the	O
pattern	O
of	O
streams	O
will	O
repeat	O
after	O
.	O

Any	O
idea	O
how	O
I	O
can	O
later	O
make	O
1.6.2	O
in	O
/	O
usr	O
/	O
local	O
/	O
lib	O
work	O
with	O
python-dbg	O
?	O

Because	O
I'm	O
still	O
not	O
quite	O
grasping	O
the	O
method	O
and	O
there	O
seems	O
to	O
be	O
simpler	O
ways	O
to	O
solve	O
the	O
problem	O
,	O
I'm	O
just	O
going	O
to	O
put	O
this	O
here	O
:	O
#CODE	O

how	O
do	O
I	O
fix	O
that	O
?	O

The	O
usual	O
shape	O
is	O
(	O
xx	O
,	O
)	O
.	O

what	O
does	O
`	O
[	O
0	O
,	O
1:3	O
]`	O
mean	O
?	O

@USER	O
he	O
basically	O
considers	O
x	O
as	O
my	O
array	O
and	O
defines	O
a	O
function	O
that	O
gives	O
the	O
range	O
of	O
each	O
row	O
..	O
works	O
fine	O
.	O

I	O
was	O
just	O
curious	O
to	O
find	O
a	O
*	O
best	O
practice	O
*	O
in	O
this	O
instance	O
I	O
suppose	O
.	O

I'm	O
guessing	O
there's	O
an	O
easy	O
way	O
to	O
do	O
this	O
without	O
iterating	O
through	O
the	O
full	O
array	O
.	O

I	O
don't	O
know	O
how	O
efficient	O
`	O
Image	O
`	O
is	O
at	O
storing	O
images	O
,	O
but	O
a	O
list	O
of	O
bytes	O
uses	O
four	O
bytes	O
of	O
space	O
for	O
each	O
element	O
in	O
the	O
list	O
,	O
so	O
you'll	O
burn	O
more	O
than	O
8GB	O
of	O
(	O
virtual	O
)	O
memory	O
...	O
and	O
a	O
lot	O
of	O
patience	O
.	O

They	O
are	O
all	O
floats	O

I	O
want	O
to	O
efficiently	O
iterate	O
through	O
the	O
two	O
columns	O
,	O
a	O
[:	O
,	O
0	O
]	O
and	O
a	O
[:	O
,	O
1	O
]	O
and	O
remove	O
any	O
pairs	O
that	O
meet	O
a	O
certain	O
condition	O
,	O
in	O
this	O
case	O
if	O
they	O
are	O
NaN	O
.	O

I	O
turned	O
`	O
color_array	O
`	O
into	O
a	O
two-dimensional	O
array	O
,	O
since	O
this	O
seems	O
more	O
appropriate	O
.	O

An	O
important	O
caveat	O
is	O
that	O
the	O
array	O
must	O
be	O
*	O
contiguous	O
*	O
in	O
memory	O
--	O
otherwise	O
the	O
view	O
fails	O
.	O

You	O
might	O
want	O
to	O
also	O
make	O
sure	O
that	O
you	O
use	O
a	O
method	O
of	O
generating	O
your	O
random	O
numbers	O
that	O
causes	O
them	O
to	O
fall	O
into	O
the	O
range	O
of	O
16	O
bit	O
ints	O
.	O

I	O
have	O
a	O
100x200	O
2D	O
array	O
expressed	O
as	O
a	O
numpy	O
array	O
consisting	O
of	O
black	O
(	O
0	O
)	O
and	O
white	O
(	O
255	O
)	O
cells	O
.	O

If	O
you	O
know	O
the	O
shape	O
of	O
the	O
array	O
(	O
to	O
find	O
discover	O
the	O
edges	O
)	O
you	O
can	O
use	O
simple	O
math	O
...	O
annoying	O
,	O
time	O
consuming	O
(	O
to	O
write	O
and	O
process	O
)	O
,	O
but	O
pretty	O
straight	O
forward	O
.	O

And	O
yet	O
another	O
way	O
,	O
possibly	O
faster	O
(	O
thanks	O
for	O
all	O
the	O
comments	O
,	O
guys	O
!	O
):	O
#CODE	O

Now	O
,	O
you	O
can	O
construct	O
another	O
matrix	O
,	O
B	O
,	O
that	O
has	O
1s	O
at	O
certain	O
locations	O
to	O
pick	O
the	O
elements	O
from	O
the	O
set	O
E	O
.	O

But	O
overhead	O
of	O
all	O
the	O
object	O
creation	O
it's	O
true	O
,	O
plus	O
the	O
time	O
of	O
Numpy	O
array	O
creation	O
.	O

I	O
was	O
trying	O
to	O
calculate	O
the	O
curvature	O
of	O
a	O
surface	O
given	O
by	O
array	O
of	O
points	O
(	O
x	O
,	O
y	O
,	O
z	O
)	O
.	O

You	O
also	O
don't	O
have	O
any	O
speed	O
improvement	O
,	O
when	O
you	O
try	O
to	O
do	O
a	O
function	O
return	O
in	O
C	O
instead	O
of	O
doing	O
that	O
in	O
python	O
,	O
also	O
array	O
access	O
is	O
neglectable	O
compared	O
to	O
the	O
cost	O
of	O
the	O
function	O
call	O
.	O

I	O
want	O
to	O
view	O
the	O
file	O
outside	O
the	O
python	O
environment	O
also	O
.	O

The	O
following	O
code	O
demonstrates	O
how	O
to	O
get	O
the	O
indices	O
of	O
some	O
vertex	O
(	O
number	O
17	O
,	O
in	O
this	O
example	O
):	O
#CODE	O

@USER	O
:	O
the	O
idea	O
is	O
possibly	O
that	O
two	O
calculation	O
that	O
both	O
arrive	O
at	O
positive	O
infinity	O
could	O
get	O
there	O
via	O
**	O
wildly	O
**	O
different	O
calculations	O
.	O

Given	O
a	O
wavelength	O
(	O
or	O
band	O
number	O
)	O
,	O
I	O
would	O
like	O
to	O
extract	O
the	O
2D	O
image	O
corresponding	O
to	O
that	O
wavelength	O
.	O

(	O
d	O
/	O
dx	O
)	O
^n	O
(	O
d	O
/	O
dy	O
)	O
^m	O
f	O
(	O
x	O
,	O
y	O
)	O

The	O
`	O
a	O
[:	O
,	O
0	O
]	O
<3	O
`	O
will	O
create	O
an	O
array	O
of	O
`	O
bool	O
`	O
with	O
the	O
same	O
dimension	O
as	O
`	O
a	O
`	O
,	O
which	O
says	O
which	O
elements	O
go	O
into	O
the	O
selection	O
.	O

(	O
Maybe	O
I	O
should	O
add	O
this	O
to	O
this	O
answer	O
.	O
)	O

Count	O
elements	O
in	O
a	O
box	O

The	O
problem	O
may	O
cook	O
down	O
to	O
mean	O
being	O
subtracted	O
over	O
all	O
numbers	O
,	O
but	O
in	O
the	O
next	O
step	O
they	O
are	O
not	O
used	O
anymore	O
...	O

I	O
wrote	O
a	O
function	O
that	O
works	O
,	O
but	O
it	O
is	O
extremely	O
slow	O
.	O

The	O
plot	O
commands	O
split	O
the	O
arrays	O
in	O
two	O
halfs	O
and	O
swap	O
them	O
to	O
get	O
a	O
nicer	O
picture	O
.	O

Any	O
help	O
would	O
be	O
appreciated	O
!	O

`	O
be	O
replaced	O
with	O
if	O
`	O
d	O
`	O
should	O
become	O
`	O
array	O
([[	O
1	O
,	O
1	O
]	O
,	O
[	O
2	O
,	O
2	O
]	O
,	O
[	O
3	O
,	O
3	O
]])`	O
?	O

How	O
big	O
is	O
the	O
triangular	O
matrix	O
?	O

Program	O
is	O
to	O
add	O
two	O
signals	O
using	O
Matlab	O

This	O
tax	O
makes	O
sense	O
when	O
you	O
are	O
doing	O
CPU	O
bound	O
operations	O
like	O
matrix	O
decompositions	O
and	O
whose	O
processing	O
time	O
takes	O
longer	O
than	O
the	O
time	O
it	O
takes	O
to	O
copy	O
the	O
data	O
.	O

Numpy	O
object	O
array	O
of	O
numerical	O
arrays	O

After	O
looking	O
at	O
these	O
page	O
and	O
other	O
I	O
got	O
it	O
working	O
using	O
raw	O
uncoded	O
buffers	O
using	O
mencoder	O
(	O
ffmpeg	O
solution	O
still	O
wanted	O
)	O
.	O

thanks	O
seberg	O
.	O
but	O
I	O
can	O
create	O
an	O
array	O
of	O
objects	O
using	O
numpy	O
,	O
then	O
I	O
can	O
assign	O
an	O
arbitrary	O
array	O
to	O
each	O
element	O
of	O
array	O
(	O
in	O
python	O
interpreter	O
)	O
.	O
what	O
do	O
you	O
think	O
of	O
this	O
?	O

Check	O
if	O
values	O
in	O
a	O
set	O
are	O
in	O
a	O
numpy	O
array	O
in	O
python	O

Would	O
you	O
know	O
of	O
a	O
way	O
to	O
**	O
average	O
**	O
the	O
duplicates	O
instead	O
of	O
sum	O
them	O
?	O

I	O
think	O
that	O
I	O
phrased	O
the	O
question	O
badly	O
;	O
what	O
I	O
meant	O
is	O
I	O
want	O
a	O
list	O
when	O
I	O
input	O
a	O
list	O
and	O
I	O
want	O
a	O
numpy	O
array	O
when	O
I	O
input	O
a	O
numpy	O
array	O
.	O

I	O
have	O
a	O
list	O
of	O
numbers	O
and	O
I	O
need	O
to	O
split	O
into	O
to	O
corresponding	O
arrays	O
of	O
different	O
sizes	O
,	O
but	O
that	O
make	O
up	O
all	O
the	O
combinations	O
of	O
the	O
array	O
splitting	O
up	O
.	O

Yes	O
,	O
it	O
is	O
true	O
that	O
before	O
you	O
ask	O
any	O
question	O
on	O
stackoverflow	O
,	O
you	O
should	O
perform	O
a	O
minimum	O
of	O
research	O
which	O
includes	O
looking	O
for	O
existing	O
answers	O
on	O
stackoverflow	O
.	O

This	O
is	O
probably	O
the	O
thing	O
that	O
is	O
most	O
important	O
for	O
numpy	O
to	O
know	O
...	O
although	O
there	O
may	O
be	O
other	O
subtle	O
differences	O
(	O
if	O
numpy	O
is	O
doing	O
some	O
dirty	O
hacking	O
to	O
get	O
at	O
information	O
stored	O
in	O
a	O
common	O
block	O
for	O
instance	O
)	O
.	O

You	O
might	O
want	O
to	O
add	O
this	O
link	O
to	O
the	O
response	O
:	O
#URL	O

A	O
list	O
takes	O
up	O
significantly	O
more	O
memory	O
than	O
a	O
comparably	O
sized	O
numpy	O
array	O
(	O
factor	O
of	O
~2	O
)	O
--	O
So	O
why	O
are	O
you	O
going	O
back	O
to	O
using	O
lists	O
?	O

From	O
my	O
understanding	O
,	O
when	O
you	O
use	O
python	O
standard	O
operators	O
,	O
these	O
are	O
fairly	O
similar	O
to	O
using	O
the	O
ones	O
from	O
"	O
import	O
operator	O
.	O

This	O
gives	O
you	O
4000	O
samples	O
of	O
your	O
function	O
,	O
sampled	O
at	O
33.33	O
Hz	O
,	O
representing	O
120	O
seconds	O
of	O
data	O
.	O

Before	O
,	O
what	O
you	O
were	O
doing	O
was	O
having	O
SWIG	O
convert	O
the	O
C++	O
`	O
std	O
::	O
vector	O
`	O
objects	O
into	O
Python	O
tuples	O
,	O
and	O
then	O
passing	O
those	O
tuples	O
back	O
down	O
to	O
`	O
numpy	O
`	O
-	O
where	O
they	O
were	O
converted	O
again	O
.	O

Currently	O
I	O
am	O
generating	O
a	O
list	O
of	O
7	O
numbers	O
with	O
random	O
values	O
from	O
[	O
0-1	O
)	O
then	O
multiplying	O
by	O
[	O
X1	O
..	O
X7	O
]	O

Introducing	O
the	O
transposing	O
of	O
the	O
matrix	O
everything	O
is	O
working	O
correctly	O
.	O

So	O
I	O
did	O
everything	O
,	O
but	O
I	O
feel	O
the	O
slicing	O
method	O
I	O
used	O
(	O
and	O
the	O
initialisation	O
of	O
`	O
b	O
`)	O
is	O
not	O
pythonic	O
at	O
all	O
:	O
#CODE	O

The	O
python	O
extension	O
calls	O
this	O
function	O
and	O
places	O
it	O
into	O
the	O
module-level	O
numpy	O
array	O
variable	O
`	O
arr	O
`	O
(	O
and	O
I	O
make	O
it	O
read-only	O
for	O
good	O
measure	O
):	O
#CODE	O

Depending	O
on	O
the	O
rig	O
you	O
have	O
at	O
the	O
moment	O
,	O
optimizing	O
your	O
SciPy	O

Examples	O
would	O
be	O
the	O
git	O
commit	O
id	O
of	O
the	O
current	O
version	O
,	O
and	O
the	O
input	O
parameters	O
used	O
to	O
generate	O
the	O
data	O
so	O
that	O
later	O
I	O
can	O
look	O
at	O
the	O
data	O
and	O
know	O
exactly	O
how	O
I	O
created	O
it	O
.	O

There	O
are	O
other	O
ways	O
to	O
do	O
this	O
(	O
you	O
may	O
want	O
to	O
avoid	O
storing	O
a	O
reference	O
to	O
a	O
specific	O
numpy	O
array	O
in	O
each	O
`	O
point	O
`	O
,	O
for	O
example	O
)	O
,	O
but	O
I	O
hope	O
it's	O
a	O
useful	O
example	O
.	O

I	O
downloaded	O
a	O
new	O
version	O
of	O
`	O
pip	O
`	O
and	O
installed	O
locally	O
in	O
my	O
home	O
directory	O
but	O
the	O
system	O
wide	O
old	O
one	O
is	O
still	O
used	O
when	O
I	O
type	O
`	O
pip	O
`	O
at	O
the	O
prompt	O
.	O

I	O
need	O
to	O
iterate	O
over	O
the	O
matrix	O
summing	O
all	O
elements	O
in	O
rows	O
0	O
,	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
only	O

If	O
you	O
compare	O
point	O
i	O
to	O
point	O
i+1	O
and	O
remove	O
all	O
for	O
which	O
the	O
distance	O
is	O
less	O
than	O
your	O
threshold	O
,	O
then	O
you	O
will	O
miss	O
the	O
cumulative	O
effect	O
of	O
many	O
small	O
steps	O
in	O
the	O
same	O
direction	O
.	O

In	O
my	O
output	O
array	O
I	O
only	O
want	O
to	O
include	O
rows	O
for	O
position	O
values	O
which	O
appear	O
in	O
all	O
3	O
input	O
arrays	O
.	O

Perhaps	O
you	O
should	O
change	O
the	O
title	O
of	O
this	O
question	O
to	O
more	O
accurately	O
reflect	O
your	O
specific	O
problem	O
.	O

Perhaps	O
there	O
is	O
a	O
good	O
choice	O
for	O
this	O
special	O
case	O
.	O

it	O
must	O
be	O
a	O
record	O
in	O
syntax	O
boycotting	O
!	O

Addressing	O
ranges	O
in	O
a	O
Scipy	O
sparse	O
matrix	O

Still	O
,	O
I	O
would	O
like	O
something	O
much	O
more	O
computationally	O
efficient	O
,	O
like	O
relying	O
on	O
builtin	O
numpy	O
functions	O
,	O
but	O
I	O
cannot	O
find	O
relevant	O
functions	O
,	O
can	O
someone	O
help	O
me	O
?	O

You	O
can	O
run	O
the	O
test	O
suite	O
to	O
see	O
if	O
the	O
build	O
works	O
.	O

The	O
bit	O
where	O
you	O
index	O
into	O
the	O
original	O
CSR	O
matrix	O
is	O
surely	O
expensive	O
.	O

I	O
don't	O
believe	O
there's	O
a	O
way	O
to	O
do	O
what	O
you're	O
asking	O
(	O
it	O
would	O
require	O
unaligned	O
access	O
,	O
which	O
is	O
highly	O
inefficient	O
on	O
some	O
architectures	O
)	O
.	O

This	O
should	O
give	O
a	O
large	O
speedup	O
compared	O
with	O
iterating	O
over	O
each	O
row	O
in	O
the	O
array	O
...	O

Not	O
sure	O
what	O
addition	O
you're	O
talking	O
about	O
nor	O
what	O
random	O
replacements	O
you	O
mean	O
.	O

I	O
have	O
a	O
counter	O
for	O
c	O
,	O
and	O
if	O
it	O
hits	O
50	O
I	O
want	O
it	O
to	O
print	O
out	O
that	O
You	O
have	O
been	O
mauled	O
by	O
a	O
bear	O
,	O
and	O
then	O
on	O
the	O
screen	O
have	O
a	O
bear	O
pop	O
up	O
,	O
whether	O
the	O
bear	O
image	O
is	O
just	O
a	O
file	O
in	O
the	O
same	O
folder	O
,	O
or	O
if	O
it	O
links	O
to	O
a	O
webpage	O
that	O
I	O
have	O
the	O
bear	O
image	O
hosted	O
at	O
.	O

I	O
have	O
yet	O
to	O
find	O
anything	O
in	O
the	O
python	O
documentation	O
that	O
states	O
how	O
a	O
list	O
of	O
list	O
is	O
assembled	O
,	O
is	O
it	O
like	O
:	O
#CODE	O

I'm	O
trying	O
to	O
use	O
leastsq	O
for	O
this	O
,	O
but	O
I'm	O
unsure	O
how	O
to	O
adjust	O
the	O
line	O
to	O
be	O
below	O
all	O
points	O
instead	O
of	O
the	O
line	O
of	O
best	O
fit	O
.	O

I	O
do	O
not	O
want	O
to	O
store	O
the	O
array	O
objects	O
,	O
I	O
want	O
to	O
store	O
the	O
lists	O
of	O
numbers	O
in	O
a	O
record	O
list	O
.	O

I	O
have	O
tried	O
two	O
different	O
methods	O
but	O
both	O
of	O
them	O
are	O
slow	O
.	O

Where	O
can	O
I	O
find	O
information	O
regarding	O
the	O
`	O
L	O
`	O
option	O
and	O
other	O
options	O
?	O

The	O
essential	O
problem	O
was	O
that	O
when	O
the	O
array	O
is	O
1d	O
you	O
call	O
it	O
array	O
[	O
i	O
]	O
when	O
it	O
is	O
2d	O
you	O
call	O
it	O
array	O
[	O
i	O
]	O
[	O
j	O
]	O
,	O
without	O
two	O
separate	O
cases	O
I	O
don't	O
know	O
how	O
to	O
handle	O
this	O
.	O

You	O
can	O
iterate	O
through	O
your	O
input	O
file	O
(	O
in	O
chunks	O
if	O
possible	O
)	O
and	O
convert	O
the	O
incoming	O
data	O
and	O
insert	O
them	O
as	O
rows	O
into	O
a	O
memory-mapped	O
numpy	O
array	O
.	O

This	O
all	O
works	O
great	O
when	O
I	O
run	O
the	O
Python	O
(	O
command	O
line	O
)	O
tool	O
that	O
comes	O
with	O
Python	O
.	O

The	O
standard	O
linear	O
algebra	O
methods	O
you	O
seem	O
to	O
be	O
thinking	O
of	O
cannot	O
(	O
to	O
the	O
best	O
of	O
my	O
knowledge	O
)	O
be	O
used	O
to	O
solve	O
this	O
sort	O
of	O
constrained	O
integer	O
problem	O
.	O

You	O
will	O
probably	O
find	O
answers	O
to	O
all	O
your	O
questions	O
regarding	O
NumPy	O
and	O
parallel	O
programming	O
on	O
the	O
official	O
wiki	O
.	O

Also	O
the	O
size	O
will	O
increase	O
as	O
I	O
process	O
data	O
with	O
higher	O
and	O
higher	O
horizontal	O
resolution	O
.	O

Where	O
each	O
row	O
(	O
of	O
a	O
fixed	O
arbitrary	O
width	O
)	O
is	O
shifted	O
by	O
one	O
.	O

ATLAS	O
3.10	O
doesn't	O
know	O
how	O
to	O
handle	O
a	O
lower	O
number	O
of	O
cores	O
than	O
the	O
number	O
it	O
had	O
at	O
build	O
time	O
and	O
generate	O
an	O
exception	O
.	O

This	O
doesn't	O
result	O
in	O
a	O
`	O
NaN	O
`	O
in	O
the	O
last	O
place	O
of	O
your	O
array	O
though	O
.	O

I've	O
tried	O
to	O
add	O
-march=i486	O
to	O
the	O
gcc	O
line	O
,	O
as	O
suggested	O
in	O
this	O
post	O
:	O

I	O
tried	O
indexing	O
it	O
before	O
but	O
i	O
didn't	O
get	O
the	O
trick	O
of	O
adding	O
1	O
to	O
the	O
size	O
of	O
label	O
1	O
before	O
.	O

Merging	O
ND	O
array	O
into	O
single	O
array	O
and	O
list	O

Its	O
not	O
complaning	O
any	O
error	O
,	O
so	O
I	O
don't	O
know	O
what	O
to	O
do	O
in	O
this	O
case	O
.	O

Here	O
is	O
a	O
sample	O
implementation	O
:	O
#CODE	O

And	O
then	O
proceed	O
with	O
the	O
split	O
of	O
each	O
shuffled	O
array	O
as	O
in	O
HYRY's	O
answer	O
.	O

This	O
is	O
quite	O
a	O
generic	O
solution	O
:	O
#CODE	O

List	O
comprehensions	O
and	O
generator	O
expressions	O
are	O
fairly	O
fast	O
,	O
and	O
they	O
don't	O
suffer	O
any	O
overhead	O
from	O
trying	O
to	O
guess	O
what	O
the	O
type	O
or	O
size	O
of	O
the	O
returned	O
item	O
should	O
be	O
,	O
as	O
they	O
don't	O
care	O
.	O

Examples	O
and	O
further	O
explanation	O
of	O
this	O
40-year-old	O
algorithm	O
at	O
the	O
matplotlib	O
FAQ	O
.	O

The	O
copy	O
will	O
only	O
copy	O
the	O
memory	O
page	O
on	O
which	O
the	O
refcount	O
integer	O
resides	O
.	O

What	O
is	O
the	O
cleanest	O
way	O
to	O
add	O
a	O
field	O
to	O
a	O
structured	O
numpy	O
array	O
?	O

Is	O
there	O
any	O
way	O
to	O
redefine	O
my	O
function	O
so	O
it	O
will	O
pass	O
without	O
warnings	O
?	O

@USER	O
:	O
the	O
reference	O
count	O
might	O
be	O
needed	O
(	O
though	O
I'm	O
not	O
entirely	O
sure	O
in	O
this	O
case	O
)	O
because	O
no	O
other	O
thread	O
must	O
deallocate	O
the	O
array	O
.	O

I	O
looked	O
around	O
but	O
couldn't	O
find	O
anything	O
syntax	O
wise	O
regarding	O
this	O
specific	O
scenario	O
.	O

While	O
this	O
involves	O
copying	O
,	O
do	O
you	O
do	O
this	O
often	O
enough	O
for	O
the	O
cost	O
of	O
the	O
copy	O
to	O
be	O
a	O
problem	O
?	O

I	O
think	O
you	O
can	O
see	O
where	O
this	O
is	O
going	O
.	O

Here	O
is	O
a	O
sample	O
function	O
(	O
you	O
need	O
to	O
have	O
the	O
pyopencv	O
module	O
installed	O
):	O
#CODE	O

I	O
have	O
learned	O
now	O
that	O
numpy	O
does	O
internally	O
create	O
a	O
temporary	O
array	O
for	O
the	O
output	O
and	O
in	O
the	O
end	O
copies	O
this	O
array	O
,	O
that	O
is	O
why	O
it	O
fails	O
for	O
the	O
values	O
that	O
are	O
zero	O
in	O
the	O
original	O
array	O
.	O

If	O
your	O
list	O
is	O
sorted	O
,	O
you	O
can	O
achieve	O
very	O
quick	O
search	O
of	O
index	O
with	O
the	O
'	O
bisect	O
'	O
package	O
.	O

How	O
to	O
save	O
a	O
boolean	O
matrix	O
?	O

I	O
have	O
need	O
to	O
slice	O
an	O
array	O
where	O
I	O
would	O
like	O
zero	O
to	O
be	O
assumed	O
for	O
every	O
dimension	O
except	O
the	O
first	O
.	O

will	O
compile	O
machine	O
code	O
that	O
will	O
execute	O
fast	O
and	O
with	O
minimal	O
memory	O
overhead	O
,	O
taking	O
care	O
of	O
memory	O
locality	O
stuff	O
(	O
and	O
thus	O
cache	O
optimization	O
)	O
if	O
the	O
same	O
array	O
occurs	O
several	O
times	O
in	O
your	O
expression	O
,	O

First	O
,	O
note	O
that	O
for	O
most	O
everyday	O
uses	O
,	O
you	O
probably	O
won't	O
require	O
an	O
array	O
(	O
which	O
is	O
a	O
special	O
datatype	O
in	O
Numpy	O
)	O
.	O

Not	O
sure	O
if	O
it	O
helps	O
but	O
if	O
you	O
add	O
another	O
array	O
of	O
a	O
different	O
shape	O
,	O
it	O
converts	O
back	O
to	O
the	O
types	O
you	O
want	O
:	O
#CODE	O

plus	O
lists	O
of	O
known	O
objects	O
falling	O
inside	O
the	O
field	O
of	O
view	O
.	O

Look	O
at	O
the	O
documentation	O
here	O
and	O
here	O
to	O
see	O
how	O
to	O
set	O
your	O
backend	O
.	O

numpy	O
array	O
plot	O
matrix	O
matplotlib	O

After	O
the	O
records	O
are	O
loaded	O
,	O
I	O
can	O
access	O
them	O
in	O
the	O
normal	O
NumPy	O
fashion	O
.	O

Eventually	O
,	O
I'm	O
looking	O
for	O
a	O
way	O
of	O
creating	O
graphs	O
similar	O
to	O
the	O
one	O
below	O
using	O
naive	O
Python	O
(	O
with	O
any	O
"	O
standard	O
"	O
library	O
such	O
as	O
numpy	O
,	O
matplotlib	O
etc	O
,	O
but	O
without	O
using	O
R	O
or	O
other	O
external	O
tools	O
)	O
.	O

On	O
a	O
general	O
note	O
however	O
,	O
in	O
numpy	O
it	O
is	O
better	O
to	O
always	O
use	O
base	O
class	O
arrays	O
unless	O
you	O
are	O
doing	O
a	O
lot	O
of	O
matrix	O
multiplications	O
,	O
etc	O
.	O

Exact	O
,	O
in	O
optimization	O
context	O
,	O
and	O
as	O
far	O
as	O
Powell	O
Badly	O
Scaled	O
function	O
is	O
used	O
for	O
test	O
,	O
I	O
impose	O
some	O
box	O
constraints	O
.	O

At	O
the	O
moment	O
I	O
have	O
the	O
lines	O
in	O
a	O
list	O
of	O
lists	O
then	O
looping	O
through	O
and	O
testing	O
for	O
identity	O
with	O
the	O
previous	O
value	O
at	O
index	O
0	O
in	O
the	O
list	O
but	O
this	O
is	O
very	O
clumsy	O
.	O

Calculating	O
the	O
exact	O
dimensions	O
of	O
this	O
array	O
:	O
I	O
tried	O
len	O
(	O
x	O
[	O
0	O
])	O
and	O
len	O
(	O
x	O
)	O
to	O
get	O
the	O
both	O
dimensional	O
coordinates	O
,	O
but	O
this	O
way	O
seems	O
a	O
bit	O
hackish	O
.	O

If	O
you	O
know	O
the	O
size	O
,	O
you	O
might	O
want	O
to	O
consider	O
pre-allocating	O
the	O
array	O
and	O
parsing	O
it	O
yourself	O
.	O

What	O
does	O
the	O
autocorrelation	O
array	O
look	O
like	O
if	O
you	O
plot	O
it	O
?	O

Where	O
it	O
says	O
:	O
#CODE	O

The	O
above	O
entire	O
expression	O
is	O
therefore	O
evaluating	O
to	O
an	O
array	O
of	O
truth	O
values	O
,	O
rather	O
than	O
a	O
single	O
`	O
True	O
`	O
/	O
`	O
False	O
`	O
.	O

For	O
the	O
multiprocessing	O
:	O
You	O
can	O
distribute	O
the	O
data	O
sets	O
across	O
cores	O
,	O
do	O
`	O
partial_fit	O
`	O
,	O
get	O
the	O
weight	O
vectors	O
,	O
average	O
them	O
,	O
distribute	O
them	O
to	O
the	O
estimators	O
,	O
do	O
partial	O
fit	O
again	O
.	O

the	O
size	O
of	O
Y	O
is	O
100e6	O
x	O
1	O

Put	O
I	O
think	O
that	O
following	O
this	O
route	O
would	O
lead	O
to	O
an	O
inefficient	O
solution	O
.	O

Your	O
example	O
come	O
at	O
a	O
good	O
time	O
for	O
me	O
,	O
so	O
I	O
now	O
have	O
something	O
concrete	O
to	O
train	O
with	O
.	O

One	O
thing	O
I	O
find	O
very	O
confortable	O
with	O
Numpy	O
is	O
the	O
vectorization	O
of	O
operations	O
with	O
arrays	O
(	O
ie	O
.	O
the	O
absence	O
of	O
any	O
explicit	O
looping	O
)	O
,	O
and	O
the	O
implicit	O
element-by-element	O
behavior	O
of	O
operations	O
.	O

I	O
suggest	O
to	O
set	O
it	O
to	O
some	O
reasonable	O
upper	O
limit	O
,	O
though	O
.	O

Gonna	O
try	O
to	O
find	O
another	O
solution	O
.	O

Is	O
it	O
possible	O
to	O
construct	O
a	O
`	O
numpy	O
`	O
matrix	O
from	O
a	O
function	O
?	O

I	O
have	O
2D	O
numpy	O
array	O
,	O
with	O
example	O
shape	O
:	O
#CODE	O

`	O
grid	O
[	O
0	O
]`	O
can	O
be	O
used	O
as	O
a	O
proxy	O
for	O
the	O
index	O
`	O
i	O
`	O
,	O
and	O

Thank	O
you	O
Martijn	O
:)	O
-	O
your	O
are	O
BIG	O
help	O
,	O
and	O
just	O
one	O
thing	O
confuses	O
me	O
,	O
how	O
do	O
I	O
tell	O
python	O
to	O
read	O
all	O
CDR	O
records	O
if	O
record	O
is	O
907	O
bytes	O
long	O
.	O

I	O
want	O
to	O
get	O
the	O
elements	O
of	O
a	O
`	O
numpy	O
`	O
array	O
using	O
an	O
index	O
array	O
like	O
so	O
#CODE	O

I	O
can	O
weight	O
them	O
how	O
I	O
want	O
to	O
as	O
long	O
as	O
sum	O
of	O
their	O
weights	O
adds	O
to	O
1	O
.	O

I	O
wanted	O
to	O
try	O
to	O
duplicate	O
those	O
performance	O
gains	O
when	O
solving	O
the	O
distance	O
between	O
two	O
equal	O
sized	O
arrays	O
.	O

Even	O
if	O
it	O
worked	O
,	O
I	O
would	O
not	O
expect	O
any	O
speed-up	O
from	O
this	O
compared	O
to	O
an	O
ordinary	O
loop	O
,	O
since	O
it	O
needs	O
to	O
call	O
a	O
Python	O
function	O
for	O
every	O
entry	O
.	O

@USER	O
,	O
you're	O
right	O
,	O
if	O
you	O
have	O
to	O
convert	O
everything	O
to	O
ndarrays	O
it's	O
often	O
not	O
worth	O
it	O
.	O

An	O
example	O
implementation	O
without	O
recalculating	O
the	O
distance	O
array	O
would	O
be	O
this	O
:	O
#CODE	O

I	O
need	O
to	O
return	O
all	O
of	O
the	O
points	O
within	O
a	O
distance	O
of	O
X	O
units	O
from	O
every	O
point	O
.	O

EDIT	O
:	O
Actually	O
renaming	O
my	O
package	O
does	O
not	O
fix	O
it	O
.	O

2	O
)	O
look	O
at	O
the	O
lengths	O
distance	O
(	O
point	O
,	O
centre	O
,	O
metric=	O
...	O
)	O
of	O
all	O
the	O
rays	O
.	O

Sorry	O
,	O
all	O
are	O
positive	O
values	O
greater	O
than	O
0	O
.	O

After	O
that	O
I	O
convert	O
the	O
image	O
to	O
BGR	O
model	O
:	O
#CODE	O

How	O
do	O
I	O
standardize	O
a	O
matrix	O
?	O

Speed	O
can	O
probably	O
be	O
increased	O
by	O
ensuring	O
that	O
the	O
record	O
array	O
you	O
pass	O
to	O
Cython	O
is	O
contiguous	O
.	O

fid	O
is	O
the	O
file	O
currently	O
being	O
looked	O
at	O

I'm	O
guessing	O
it's	O
opening	O
TWO	O
filehandles	O
per	O
iteration	O
,	O
just	O
based	O
on	O
the	O
498	O
(	O
a	O
bit	O
less	O
than	O
half	O
1024	O
,	O
and	O
Python	O
would	O
have	O
some	O
files	O
open	O
itself	O
(	O
maybe	O
25-odd	O
?	O
)	O
.	O

The	O
idea	O
is	O
to	O
count	O
the	O
number	O
of	O
occurrences	O
of	O
each	O
transition	O
,	O
and	O
use	O
the	O
counts	O
in	O
a	O
vectorized	O
update	O
of	O
the	O
matrix	O
.	O

I	O
kept	O
them	O
in	O
to	O
distinguish	O
them	O
from	O
the	O
`	O
math	O
`	O
ones	O
,	O
which	O
won't	O
work	O
for	O
this	O
approach	O
.	O

Powers	O
of	O
two	O
are	O
simple	O
to	O
compute	O
,	O
but	O
mixed	O
radix	O
sizes	O
can	O
be	O
faster	O
and	O
use	O
less	O
memory	O
.	O

The	O
stars	O
/	O
dots	O
are	O
the	O
`	O
X	O
`	O
and	O
`	O
Y	O
`	O
plotted	O
with	O
two	O
modifications	O
,	O
I	O
removed	O
the	O
first	O
position	O
and	O
added	O
a	O
false	O
one	O
to	O
make	O
this	O
a	O
full	O
example	O
of	O
the	O
sought	O
algorithm	O
.	O

Please	O
look	O
at	O
my	O
EDIT	O
2	O
,	O
where	O
I	O
described	O
my	O
problem	O
with	O
input	O
data	O
...	O
and	O
why	O
I	O
can't	O
get	O
matrix	O
..	O

pyqt	O
:	O
Convert	O
numpy	O
array	O
to	O
QImage	O

To	O
find	O
the	O
difference	O
between	O
your	O
data	O
and	O
a	O
point	O
,	O
you'd	O
just	O
do	O
`	O
data	O
-	O
point	O
`	O
.	O

Unfortunately	O
when	O
numpy	O
reads	O
the	O
19-digit	O
number	O
as	O
a	O
floating	O
point	O
number	O
,	O
there	O
is	O
not	O
enough	O
precision	O
to	O
get	O
all	O
the	O
significant	O
digits	O
,	O
so	O
there	O
is	O
a	O
rounding	O
error	O
.	O

The	O
exceptions	O
are	O
very	O
rare	O
,	O
if	O
any	O
.	O

I	O
can't	O
reproduce	O
your	O
problem	O
on	O
Linux	O
using	O
the	O
same	O
versions	O
of	O
numpy	O
and	O
python	O
and	O
a	O
quickly	O
made	O
test	O
file	O
(	O
with	O
dos	O
line	O
endings	O
,	O
even	O
)	O
...	O

I	O
imagine	O
I	O
would	O
have	O
to	O
use	O
the	O
uncompiled	O
source	O
provided	O
from	O
each	O
of	O
these	O
three	O
projects	O
.	O

However	O
,	O
I	O
am	O
checking	O
optimization	O
routine	O
result	O
,	O
and	O
sometimes	O
power	O
is	O
negative	O
,	O
sometimes	O
it	O
is	O
positive	O
.	O

What	O
about	O
array	O
of	O
arrays	O
that	O
contains	O
some	O
structures	O
?	O

The	O
y	O
data	O
takes	O
the	O
shape	O
of	O
the	O
triangle	O
wave	O
below	O
.	O

There	O
are	O
some	O
algorithm	O
to	O
calculate	O
faster	O
the	O
results	O
for	O
low	O
valued	O
matrix	O
,	O
but	O
just	O
google	O
for	O
this	O
.	O

Those	O
are	O
not	O
random	O
replacements	O
by	O
any	O
means	O
.	O

I	O
would	O
suggest	O
to	O
make	O
the	O
library	O
use	O
an	O
(	O
NumPy-	O
)	O
array	O
you	O
allocate	O
in	O
Python	O
and	O
pass	O
on	O
to	O
the	O
library	O
.	O

For	O
the	O
simple	O
case	O
of	O
"	O
remove	O
column	O
3	O
"	O
,	O
`	O
delete	O
`	O
makes	O
more	O
sense	O
;	O
for	O
a	O
more	O
complicated	O
case	O
,	O
`	O
take	O
`	O
probably	O
makes	O
more	O
sense	O
.	O

I	O
have	O
an	O
array	O
of	O
points	O
in	O
numpy	O
:	O
#CODE	O

I	O
have	O
done	O
7	O
of	O
the	O
problems	O
on	O
Project	O
Euler	O
(	O
nothing	O
to	O
brag	O
about	O
,	O
but	O
it	O
might	O
give	O
you	O
a	O
better	O
idea	O
of	O
where	O
I	O
stand	O
in	O
skills	O
)	O
.	O

How	O
do	O
I	O
find	O
the	O
length	O
(	O
or	O
dimensions	O
,	O
size	O
)	O
of	O
a	O
numpy	O
matrix	O
in	O
python	O
?	O

It's	O
longer	O
than	O
the	O
other	O
answer	O
but	O
is	O
more	O
generic	O
(	O
can	O
be	O
used	O
with	O
values	O
that	O
are	O
not	O
strings	O
)	O
.	O

I	O
coded	O
my	O
own	O
routine	O
with	O
Python	O
/	O
Numpy	O
,	O
and	O
it	O
is	O
giving	O
me	O
a	O
little	O
bit	O
different	O
results	O
from	O
the	O
MATLAB	O
code	O
somebody	O
else	O
did	O
,	O
and	O
I	O
am	O
having	O
hard	O
time	O
finding	O
out	O
where	O
it	O
is	O
coming	O
from	O
because	O
of	O
different	O
random	O
draws	O
.	O

How	O
can	O
1,000,000	O
4-byte	O
ints	O
be	O
compressed	O
any	O
smaller	O
?	O

If	O
this	O
number	O
is	O
less	O
than	O
a	O
third	O
of	O
the	O
total	O
I'll	O
use	O
my	O
answer	O
above	O
.	O

I	O
have	O
done	O
7	O
of	O
the	O
problems	O
on	O
Project	O
Euler	O
(	O
nothing	O
to	O
brag	O
about	O
,	O
but	O
it	O
might	O
give	O
you	O
a	O
better	O
idea	O
of	O
where	O
I	O
stand	O
in	O
skills	O
)	O
.	O

Apologies	O
if	O
this	O
is	O
a	O
wrongly	O
framed	O
question	O
or	O
if	O
this	O
question	O
was	O
already	O
asked	O
earlier	O
(	O
I	O
couldn't	O
find	O
it	O
)	O

If	O
you	O
can	O
choose	O
,	O
I	O
strongly	O
recommend	O
pandas	O
:	O
it	O
has	O
"	O
column	O
indexing	O
"	O
built-in	O
plus	O
a	O
lot	O
of	O
other	O
features	O
.	O

But	O
this	O
will	O
iterate	O
through	O
the	O
entire	O
array	O
and	O
allocate	O
a	O
new	O
array	O
in	O
memory	O
containing	O
the	O
all	O
the	O
results	O
,	O
and	O
only	O
then	O
check	O
to	O
see	O
if	O
it	O
is	O
empty	O
.	O

Since	O
some	O
askers	O
and	O
some	O
answers	O
both	O
avoid	O
that	O
constraint	O
,	O
I	O
encourage	O
anyone	O
who's	O
here	O
and	O
doesn't	O
mind	O
having	O
PIL	O
to	O
look	O
below	O
,	O
and	O
any	O
non-PIL	O
answers	O
(	O
new	O
or	O
old	O
)	O
to	O
mention	O
that	O
they're	O
a	O
PIL-is-used	O
type	O
of	O
answer	O
,	O
to	O
distinguish	O
themselves	O
from	O
answers	O
meeting	O
the	O
original	O
constraint	O
.	O

As	O
I	O
understand	O
your	O
question	O
,	O
you	O
have	O
a	O
2D	O
array	O
of	O
"	O
z	O
"	O
values	O
that	O
ranges	O
from	O
some	O
xmin	O
to	O
xmax	O
,	O
and	O
ymin	O
to	O
ymax	O
in	O
each	O
direction	O
.	O

The	O
covariance	O
matrix	O
of	O
a	O
dataset	O
A	O
is	O
:	O
1	O
/(	O
N-1	O
)	O
*	O
AA^T	O

I	O
have	O
a	O
large	O
(	O
500k	O
by	O
500k	O
)	O
,	O
sparse	O
matrix	O
.	O

I	O
can't	O
comment	O
on	O
a	O
numpy	O
array	O
as	O
I	O
haven't	O
used	O
one	O
before	O
,	O
but	O
for	O
using	O
a	O
list	O
of	O
lists	O
Python	O
already	O
has	O
built	O
in	O
support	O
.	O

If	O
`	O
finer_fxy	O
`	O
is	O
stored	O
in	O
the	O
probably-default	O
`	O
float64	O
`	O
s	O
,	O
this	O
would	O
take	O
about	O
64	O
GiB	O
of	O
memory	O
;	O
not	O
surprising	O
that	O
you're	O
running	O
out	O
.	O

Sebastian's	O
solution	O
for	O
a	O
way	O
around	O
the	O
integer-values-only	O
restriction	O
and	O
big-values	O
problem	O
.	O

This	O
allows	O
the	O
column	O
to	O
hold	O
float	O
values	O
at	O
first	O
,	O
and	O
strings	O
later	O
.	O

Efficient	O
slicing	O
of	O
matrices	O
using	O
matrix	O
multiplication	O
,	O
with	O
Python	O
,	O
NumPy	O
,	O
SciPy	O

Is	O
there	O
a	O
more	O
compact	O
way	O
to	O
operate	O
on	O
array	O
elements	O
,	O
without	O
having	O
to	O
use	O
the	O
standard	O
for	O
loop	O
.?	O

Please	O
look	O
at	O
this	O
answer	O
:	O
#URL	O

When	O
I	O
tried	O
this	O
,	O
I	O
got	O
sort	O
of	O
similar	O
shaped	O
"	O
tiles	O
"	O
of	O
different	O
colors	O
rather	O
than	O
3	O
Gaussian	O
humps	O
.	O

I	O
created	O
the	O
first	O
array	O
like	O
this	O
:	O

(	O
Note	O
that	O
I	O
can't	O
imagine	O
any	O
reason	O
why	O
this	O
should	O
be	O
necessary	O
.	O
)	O

SOLUTION	O
:	O
i	O
have	O
some	O
scattered	O
points	O
(	O
i	O
don't	O
know	O
how	O
many	O
)	O
and	O
i	O
want	O
to	O
reduce	O
it	O
to	O
a	O
8	O
meaning	O
point	O
.	O
one	O
of	O
the	O
technique	O
i	O
can	O
use	O
is	O
to	O
clusterize	O
them	O
with	O
some	O
cluster	O
algorithms	O
.	O

that	O
blas	O
is	O
reference	O
blas	O
from	O
netlib	O
-	O
the	O
slowest	O
blas	O
around	O
.	O
install	O
atlas	O
or	O
mkl	O
instead	O
.	O

EDIT	O
:	O
Answer	O
updated	O
for	O
a	O
2D	O
array	O
.	O

But	O
you	O
lose	O
a	O
lot	O
of	O
NumPy	O
power	O
that	O
way	O
.	O

Because	O
I	O
view	O
doesn't	O
really	O
have	O
to	O
do	O
with	O
filtering	O
,	O
but	O
rather	O
with	O
different	O
representation	O
of	O
the	O
same	O
data	O
.	O

@USER	O
,	O
not	O
sure	O
what	O
you	O
mean	O
by	O
"	O
changing	O
original	O
values	O
"	O
.	O

this	O
could	O
also	O
be	O
achieved	O
elegantly	O
with	O
numpy's	O
`	O
where	O
`	O
function	O

I	O
need	O
to	O
specify	O
datatypes	O
for	O
all	O
numerical	O
types	O
since	O
I	O
care	O
about	O
int	O
8/	O
16	O
/	O
32	O
,	O
etc	O
,	O
but	O
I	O
would	O
like	O
to	O
benefit	O
from	O
the	O
auto	O
string	O
length	O
detection	O
that	O
works	O
if	O
I	O
don't	O
specify	O
datatypes	O
.	O

In	O
the	O
following	O
trivial	O
function	O
,	O
I	O
have	O
declared	O
the	O
numpy	O
array	O
argument	O
`	O
arr	O
`	O
using	O
the	O
buffer	O
syntax	O
.	O

I	O
remember	O
that	O
there	O
was	O
a	O
smart	O
trick	O
about	O
turning	O
on	O
and	O
off	O
the	O
right	O
intersections	O
of	O
rows	O
and	O
columns	O
to	O
turn	O
off	O
one-by-one	O
all	O
the	O
lightbulbs	O
,	O
but	O
it	O
wont	O
come	O
back	O
to	O
my	O
mind	O
...	O

However	O
this	O
code	O
is	O
to	O
slow	O
in	O
the	O
current	O
version	O
,	O
and	O
I	O
am	O
wondering	O
wheater	O
there	O
is	O
a	O
faster	O
solution	O
.	O
thanks	O
!	O

This	O
would	O
probably	O
be	O
the	O
most	O
efficient	O
way	O
to	O
access	O
a	O
numpy	O
array	O
stored	O
on	O
disk	O
.	O

hmmmmm	O
,	O
probably	O
it	O
will	O
help	O
some	O
others	O
to	O
sort	O
dictionarys	O
or	O
to	O
prevent	O
from	O
using	O
commands	O
like	O
sorted	O
=	O
sorted	O
(	O
...	O
)	O
.	O

The	O
ticket	O
simply	O
spoke	O
of	O
random	O
number	O
seeding	O
with	O
64-bit	O
,	O
perhaps	O
its	O
referring	O
to	O
a	O
different	O
random	O
number	O
generator	O
.	O

Not	O
really	O
elegant	O
at	O
all	O
but	O
you	O
can	O
get	O
close	O
to	O
what	O
you	O
want	O
using	O
a	O
tuple	O
to	O
store	O
pointers	O
to	O
the	O
arrays	O
.	O

For	O
example	O
I	O
am	O
looking	O
for	O
4.2	O
but	O
I	O
know	O
in	O
the	O
array	O
there	O
is	O
no	O
4.2	O
but	O
I	O
want	O
to	O
return	O
the	O
index	O
of	O
the	O
value	O
4.1	O
instead	O
of	O
4.4	O
.	O

Print	O
'	O
Length	O
of	O
together	O
'	O
goes	O
just	O
before	O
the	O
matrix	O
line	O
.	O

solve	O
a	O
nonlinear	O
equation	O
at	O
several	O
intermediate	O
points	O
of	O
a	O
calculation	O
,	O
not	O
just	O
as	O
the	O
final	O
result	O
.	O

Find	O
where	O
they're	O
located	O
at	O
(	O
assumes	O
the	O
data	O
is	O
sorted	O
!!	O
):	O
#CODE	O

You	O
need	O
Python	O
to	O
keep	O
track	O
of	O
your	O
vector	O
so	O
that	O
it	O
can	O
be	O
deleted	O
*	O
after	O
*	O
the	O
numpy	O
array	O
.	O

I	O
find	O
that	O
I	O
have	O
to	O
first	O
build	O
a	O
list	O
and	O
then	O
cast	O
it	O
(	O
using	O
"	O
array	O
")	O
to	O
an	O
array	O
.	O

I	O
have	O
an	O
numpy	O
one	O
dimensional	O
array	O
c	O
that	O
is	O
supposed	O
to	O
be	O
filled	O
with	O
the	O
contents	O
of	O

but	O
the	O
issue	O
now	O
,	O
when	O
I	O
am	O
trying	O
to	O
save	O
the	O
name	O
of	O
the	O
file	O
as	O
well	O
in	O
the	O
csv	O
file	O
like	O
this	O
:	O
#CODE	O

After	O
you	O
do	O
this	O
no	O
matter	O
where	O
the	O
template	O
object	O
is	O
in	O
a	O
calculation	O
.	O

So	O
the	O
easiest	O
thing	O
to	O
do	O
would	O
be	O
to	O
take	O
a	O
sample	O
of	O
say	O
,	O
1000	O
points	O
,	O
from	O
your	O
data	O
:	O
#CODE	O

Your	O
array	O
consists	O
of	O
:	O
#CODE	O

The	O
final	O
DF	O
should	O
have	O
as	O
many	O
columns	O
as	O
all	O
the	O
df	O
columns	O
added	O
together	O
,	O
so	O
it	O
grow	O
additively	O
and	O
not	O
be	O
combinatorial	O
.	O

I'm	O
sorting	O
the	O
cells	O
of	O
the	O
matrix	O
by	O
the	O
float	O
value	O
,	O
producing	O
a	O
list	O
of	O
`	O
(	O
row	O
,	O
col	O
,	O
value	O
)`	O
tuples	O
.	O

Is	O
it	O
essential	O
that	O
you	O
need	O
a	O
numpy	O
array	O
?	O

Mh	O
.	O
but	O
look	O
at	O
this	O
:	O

All	O
variables	O
are	O
dependent	O
on	O
each	O
other	O
and	O
I	O
am	O
only	O
looking	O
for	O
local	O
minima	O
from	O
the	O
initial	O
guess	O
.	O

The	O
basic	O
idea	O
is	O
to	O
simply	O
run	O
all	O
the	O
usual	O
steps	O
of	O
a	O
root	O
finder	O
in	O
parallel	O
on	O
a	O
vector	O
of	O
variables	O
,	O
using	O
a	O
function	O
that	O
can	O
be	O
evaluated	O
on	O
a	O
vector	O
of	O
variables	O
and	O
equivalent	O
vector	O
(	O
s	O
)	O
of	O
parameters	O
that	O
define	O
the	O
individual	O
component	O
functions	O
.	O

Hence	O
,	O
with	O
NetworkX	O
,	O
you	O
can	O
put	O
in	O
an	O
adjacency	O
matrix	O
and	O
find	O
out	O
which	O
authors	O
are	O
clustered	O
together	O
.	O

The	O
issue	O
your	O
having	O
more	O
likely	O
is	O
a	O
python	O
mmap	O
issue	O
,	O
since	O
python	O
mmaps	O
handle	O
all	O
the	O
memory	O
mapping	O
and	O
file	O
closing	O
for	O
numpy	O
memmaps	O
.	O

So	O
far	O
,	O
I'm	O
sticking	O
with	O
C++	O
-	O
on	O
my	O
tests	O
,	O
at	O
least	O
2	O
orders	O
of	O
magnitude	O
faster	O
!	O

Sorting	O
ends	O
up	O
being	O
the	O
slowest	O
step	O
but	O
it's	O
still	O
faster	O
if	O
m	O
is	O
large	O
because	O
the	O
n*log	O
(	O
n	O
)	O
sort	O
is	O
faster	O
than	O
(	O
n*m	O
)	O
.	O

Basically	O
,	O
I	O
am	O
getting	O
a	O
memory	O
error	O
in	O
python	O
when	O
trying	O
to	O
perform	O
an	O
algebraic	O
operation	O
on	O
a	O
numpy	O
matrix	O
.	O

Surely	O
there	O
must	O
be	O
a	O
way	O
to	O
populate	O
a	O
boost	O
::	O
python	O
::	O
numeric	O
::	O
array	O
with	O
data	O
from	O
a	O
simple	O
std	O
::	O
vector	O
without	O
having	O
to	O
get	O
some	O
3rd	O
party	O
library	O
.	O

Here	O
again	O
a	O
if	O
statement	O
could	O
do	O
,	O
but	O
I	O
am	O
wondering	O
if	O
there	O
is	O
a	O
workarouns	O
and	O
a	O
Python	O
library	O
where	O
negative	O
exposant	O
is	O
allowed	O
.	O

The	O
key	O
point	O
here	O
is	O
that	O
Tabular	O
and	O
NumPy	O
set	O
certain	O
standards	O
for	O
what	O
counts	O
as	O
"	O
fast	O
"	O
or	O
"	O
slow	O
"	O
--	O
and	O
then	O
,	O
force	O
you	O
to	O
be	O
explicit	O
about	O
operations	O
that	O
are	O
going	O
to	O
be	O
slow	O
.	O

Asume	O
that	O
your	O
numpy	O
module	O
is	O
located	O
at	O
/	O
Users	O
/	O
Me	O
/	O
python	O
/	O
modules	O
directory	O
.	O

I	O
am	O
not	O
responsible	O
from	O
any	O
brain	O
damage	O
resulting	O
from	O
attempting	O
to	O
understand	O
this	O
code	O
.	O

There	O
a	O
plenty	O
of	O
places	O
where	O
you're	O
inadvertently	O
creating	O
additional	O
temporary	O
arrays	O
,	O
but	O
they're	O
mostly	O
irrelevant	O
,	O
as	O
they're	O
overwhelmed	O
by	O
what	O
goes	O
on	O
during	O
the	O
call	O
to	O
`	O
select	O
`	O
.	O

The	O
fact	O
that	O
you	O
are	O
using	O
`	O
object	O
`	O
arrays	O
(	O
not	O
very	O
common	O
and	O
not	O
very	O
memory-efficient	O
)	O
presents	O
a	O
particular	O
problem	O
when	O
trying	O
to	O
determine	O
the	O
index	O
of	O
non-None	O
array	O
items	O
.	O

where	O
things	O
improve	O
as	O
the	O
number	O
of	O
bits	O
increases	O
.	O

Really	O
,	O
4D	O
arrays	O
are	O
just	O
1D	O
arrays	O
in	O
memory	O
anyway	O
(	O
Unless	O
you	O
really	O
have	O
view	O
objects	O
,	O
but	O
it	O
should	O
still	O
work	O
with	O
those	O
as	O
well	O
)	O

I'll	O
add	O
comments	O
to	O
explain	O
things	O
in	O
a	O
bit	O
.	O

I	O
was	O
assuming	O
that	O
the	O
rgb	O
and	O
ycc	O
matrices	O
were	O
just	O
a	O
matrix	O
that	O
had	O
as	O
many	O
rows	O
as	O
pixels	O
and	O
a	O
column	O
per	O
colour	O
component	O
.	O

For	O
example	O
,	O
suppose	O
`	O
a	O
=	O
ones	O
((	O
3	O
,	O
3	O
))`	O
.	O

Therefore	O
,	O
n	O
and	O
m	O
correspond	O
to	O
indices	O
in	O
the	O
array	O
,	O
but	O
I'm	O
not	O
sure	O
how	O
?	O

Update	O
:	O
As	O
mentioned	O
in	O
my	O
comment	O
below	O
,	O
I	O
should	O
have	O
stated	O
that	O
I'm	O
trying	O
to	O
do	O
this	O
on	O
2D	O
arrays	O
,	O
and	O
therefore	O
get	O
a	O
set	O
of	O
2D	O
indices	O
back	O
.	O

Need	O
to	O
add	O
a	O
check	O
for	O
that	O
,	O
but	O
otherwise	O
thanks	O
!	O

I	O
think	O
you	O
just	O
want	O
`	O
label	O
==	O
num	O
`	O
where	O
`	O
num	O
`	O
is	O
the	O
number	O
of	O
the	O
object	O
in	O
`	O
label	O
`	O
(	O
the	O
labeled	O
array	O
)	O
.	O

My	O
question	O
is	O
how	O
can	O
I	O
go	O
thru	O
the	O
array	O
to	O
access	O
the	O
object	O
in	O
the	O
array	O
?	O

The	O
matrix	O
in	O
the	O
example	O
above	O
is	O
singular	O
(	O
determinant	O
~	O
0	O
)	O
.	O

See	O
the	O
note	O
at	O
#URL	O

would	O
turn	O
into	O
either	O
this	O
array	O
:	O
#CODE	O

Note	O
that	O
this	O
is	O
a	O
bit	O
more	O
sophisticated	O
than	O
the	O
simple	O
do-it-yourself	O
convolve-method	O
,	O
since	O
it	O
tries	O
to	O
handle	O
the	O
problems	O
at	O
the	O
beginning	O
and	O
the	O
end	O
of	O
the	O
data	O
by	O
reflecting	O
it	O
(	O
which	O
may	O
or	O
may	O
not	O
work	O
in	O
your	O
case	O
...	O
)	O
.	O

Usually	O
,	O
in	O
numpy	O
,	O
you	O
keep	O
the	O
string	O
data	O
in	O
a	O
separate	O
array	O
.	O

Any	O
idea	O
what	O
might	O
be	O
happening	O
?	O

but	O
the	O
size	O
is	O
wrong	O
because	O
i've	O
assigned	O
1000	O
as	O
the	O
period	O
size	O
.	O

This	O
may	O
not	O
be	O
perfectly	O
pythonic	O
(	O
perhaps	O
someone	O
can	O
think	O
of	O
a	O
nicer	O
implementation	O
using	O
generators	O
or	O
itertools	O
?	O
)	O
but	O
it	O
is	O
hard	O
to	O
imagine	O
any	O
method	O
that	O
relies	O
on	O
searching	O
one	O
point	O
at	O
a	O
time	O
beating	O
this	O
in	O
speed	O
.	O

Thanks	O
,	O
your	O
post	O
helped	O
me	O
solve	O
this	O
problem	O
.	O

Now	O
imagine	O
that	O
the	O
next	O
time	O
step	O
some	O
values	O
change	O
,	O
so	O
should	O
this	O
picture	O
.	O

Since	O
get_probability	O
is	O
a	O
function	O
,	O
so	O
what	O
value	O
is	O
being	O
passed	O
to	O
count	O
parameter	O
here	O
???	O

taking	O
the	O
sum	O
for	O
each	O
column	O
.	O

You	O
should	O
be	O
able	O
to	O
just	O
load	O
the	O
entire	O
thing	O
into	O
memory	O
on	O
a	O
modern	O
machine	O
.	O

What	O
I	O
want	O
to	O
do	O
is	O
to	O
calculate	O
the	O
geographic	O
distances	O
between	O
rows	O
(	O
with	O
the	O
special	O
condition	O
that	O
the	O
first	O
element	O
is	O
always	O
zero	O
,	O
at	O
the	O
starting	O
point	O
)	O
.	O

We	O
can	O
simply	O
use	O
the	O
leastsq	O
function	O
to	O
find	O
the	O
best	O
coefficients	O
.	O

If	O
the	O
list	O
of	O
python	O
objects	O
doesn't	O
grow	O
at	O
all	O
from	O
frame	O
to	O
frame	O
,	O
the	O
leak	O
is	O
probably	O
in	O
the	O
C	O
code	O
or	O
the	O
python-to-C	O
link	O

Any	O
and	O
all	O
advice	O
is	O
greatly	O
appreciated	O
.	O

Numpy	O
:	O
Is	O
there	O
an	O
array	O
size	O
limit	O
?	O

Then	O
do	O
this	O
after	O
each	O
calculation	O
:	O
for	O
i	O
in	O
range	O
(	O
len	O
(	O
array	O
)):	O
array	O
[	O
i	O
]	O
[	O
i	O
]=	O
0	O

I	O
know	O
the	O
random	O
functions	O
and	O
numbers	O
seem	O
odd	O
,	O
but	O
conceptually	O
this	O
still	O
should	O
work	O
,	O
as	O
it	O
worked	O
when	O
both	O
were	O
set	O
to	O
variables	O
individually	O
.	O

@USER	O
are	O
your	O
numbers	O
in	O
the	O
range	O
of	O
-128	O
to	O
127	O
before	O
you	O
convert	O
them	O
to	O
8b	O
it	O
?	O

In	O
the	O
future	O
,	O
how	O
should	O
I	O
go	O
about	O
trying	O
to	O
find	O
routines	O
like	O
this	O
?	O

At	O
20,000	O
elements	O
,	O
your	O
method	O
is	O
about	O
25%	O
faster	O
.	O

I'll	O
fix	O
it	O
just	O
for	O
you	O
:P	O

Then	O
I	O
convert	O
it	O
to	O
a	O
numpy	O
array	O
:	O
#CODE	O

Just	O
throwing	O
in	O
my	O
two	O
cents	O
you	O
could	O
do	O
this	O
pretty	O
simply	O
using	O
list	O
comprehension	O
if	O
it's	O
always	O
a	O
2d	O
array	O
like	O
that	O
#CODE	O

While	O
its	O
expected	O
value	O
here	O
is	O
zero	O
,	O
the	O
particular	O
realizations	O
will	O
fluctuate	O
around	O
that	O
expected	O
value	O
.	O

Then	O
if	O
each	O
item	O
is	O
weighted	O
with	O
weight	O
w_i	O
,	O
the	O
"	O
summed	O
histogram	O
"	O
would	O
have	O
weight	O
sum	O
(	O
i	O
in	O
items	O
)	O
w_i	O
D_ij	O
.	O

This	O
approach	O
will	O
take	O
an	O
overhead	O
because	O
of	O
crating	O
a	O
new	O
array	O
in	O
memory	O
.	O

"	O
Eric's	O
suggestion	O
for	O
revising	O
this	O
question	O
is	O
a	O
good	O
start	O
,	O
but	O
I	O
think	O
the	O
question	O
"	O
Given	O
a	O
Cartesian	O
plane	O
,	O
how	O
to	O
discretize	O
it	O
in	O
a	O
matrix	O
form	O
?	O

it	O
is	O
the	O
same	O
as	O
long	O
as	O
you	O
ignore	O
precision	O
issue	O
-	O
which	O
matters	O
quite	O
often	O
when	O
you	O
start	O
taking	O
exponential	O
of	O
numbers	O
.	O

Google	O
Protocol	O
Buffers	O
support	O
self-describing	O
too	O
,	O
are	O
pretty	O
fast	O
(	O
but	O
Python	O
support	O
is	O
poor	O
at	O
present	O
time	O
,	O
slow	O
and	O
buggy	O
)	O
.	O

Not	O
all	O
people	O
can	O
install	O
NumPy	O
(	O
or	O
even	O
Python	O
:D	O
)	O
as	O
many	O
Blender	O
users	O
are	O
just	O
artists	O
.	O

All	O
possible	O
solutions	O
are	O
mentioned	O
in	O
the	O
comments	O
.	O

I've	O
also	O
refined	O
your	O
approach	O
to	O
allow	O
zooming	O
in	O
over	O
a	O
section	O
of	O
the	O
data	O
and	O
to	O
produce	O
better	O
results	O
at	O
the	O
borders	O
.	O

I	O
need	O
to	O
specify	O
datatypes	O
for	O
all	O
numerical	O
types	O
since	O
I	O
care	O
about	O
int	O
8/	O
16	O
/	O
32	O
,	O
etc	O
,	O
but	O
I	O
would	O
like	O
to	O
benefit	O
from	O
the	O
auto	O
string	O
length	O
detection	O
that	O
works	O
if	O
I	O
don't	O
specify	O
datatypes	O
.	O

I	O
would	O
appreciate	O
any	O
assistance	O
you	O
can	O
offer	O
.	O

Let's	O
say	O
for	O
example	O
I	O
have	O
a	O
matrix	O
X	O
which	O
is	O
my	O
input	O
.	O

@USER	O
-	O
By	O
the	O
way	O
,	O
indexing	O
returns	O
a	O
view	O
(	O
essentially	O
a	O
pointer	O
)	O
into	O
the	O
array	O
.	O

Note	O
that	O
`	O
view	O
`	O
holds	O
the	O
same	O
data	O
as	O
the	O
original	O
array	O
!	O

EDIT	O
:	O
What	O
sort	O
of	O
sequence	O
is	O
it	O
you're	O
making	O
?	O

The	O
relative	O
error	O
is	O
less	O
than	O
2	O
-24	O
,	O
which	O
is	O
1	O
/	O
2	O
ULP	O
divided	O
by	O
the	O
smallest	O
the	O
value	O
could	O
be	O
(	O
the	O
smallest	O
value	O
in	O
the	O
interval	O
for	O
a	O
particular	O
ULP	O
,	O
so	O
the	O
power	O
of	O
two	O
that	O
bounds	O
it	O
)	O
.	O

This	O
is	O
called	O
matrix	O
transposition	O
.	O

@USER	O
The	O
solutions	O
there	O
all	O
make	O
use	O
of	O
the	O
fact	O
that	O
only	O
a	O
3x3	O
sliding	O
window	O
is	O
needed	O
,	O
but	O
I	O
need	O
something	O
that	O
works	O
for	O
all	O
sizes	O
of	O
templates	O
.	O

(	O
0008	O
,	O
103e	O
)	O
Series	O
Description	O
LO	O
:	O
'	O
Screen	O
Save	O
'	O

@USER	O
khanSever	O
20k	O
wouldn't	O
be	O
a	O
problem	O
for	O
modern	O
computers	O
,	O
if	O
you	O
are	O
really	O
thresholded	O
by	O
speed	O
in	O
this	O
kind	O
of	O
computation	O
,	O
I	O
would	O
say	O
that	O
you	O
shouldn't	O
have	O
had	O
an	O
inhomogenous	O
data	O
array	O
to	O
begin	O
with	O
.	O

@USER	O
Eweiwi	O
:	O
Did	O
you	O
find	O
my	O
answer	O
anyway	O
useful	O
?	O

You	O
can	O
now	O
compute	O
the	O
function	O
`	O
f	O
(	O
x	O
)`	O
at	O
any	O
point	O
`	O
x	O
`	O
.	O

BTW	O
:	O
this	O
is	O
a	O
neat	O
workaround	O
,	O
but	O
if	O
it	O
were	O
possible	O
to	O
use	O
the	O
`	O
in	O
`	O
operator	O
would	O
have	O
preferred	O
,	O
as	O
in	O
my	O
"	O
real	O
case	O
"	O
I	O
have	O
a	O
pool	O
of	O
roughly	O
10	O
values	O
,	O
non	O
only	O
`	O
(	O
6	O
,	O
8)	O
`	O
.	O

In	O
this	O
example	O
I	O
want	O
to	O
return	O
an	O
array	O
of	O
[	O
202	O
203	O
206	O
210	O
]	O

So	O
f	O
(	O
x	O
,	O
y	O
)	O
=	O
0	O

I	O
present	O
below	O
a	O
sample	O
silhouette	O
implementation	O
in	O
both	O
MATLAB	O
and	O
Python	O
/	O
Numpy	O
(	O
keep	O
in	O
mind	O
that	O
I	O
am	O
more	O
fluent	O
in	O
MATLAB	O
):	O

Python	O
import	O
Column	O
Data	O
from	O
MySQL	O
as	O
Array	O

This	O
is	O
just	O
the	O
partial	O
count	O
due	O
to	O
the	O
34	O
1-chips	O
.	O

I	O
want	O
to	O
know	O
how	O
I	O
should	O
index	O
/	O
access	O
some	O
data	O
programmatically	O
in	O
python	O
.	O

There	O
is	O
a	O
short	O
comment	O
at	O
the	O
end	O
of	O
the	O
introduction	O
to	O
SciPy	O
documentation	O
:	O

What	O
about	O
the	O
maximum	O
value	O
in	O
the	O
array	O
?	O

If	O
you	O
use	O
a	O
list	O
of	O
`	O
True	O
/	O
False	O
`	O
,	O
NumPy	O
will	O
interpret	O
that	O
as	O
a	O
list	O
of	O
`	O
1	O
/	O
0	O
`	O
as	O
integers	O
,	O
that	O
is	O
,	O
indices	O
,	O
meaning	O
that	O
you	O
'	O
either	O
get	O
the	O
second	O
or	O
first	O
element	O
of	O
your	O
array	O
.	O

But	O
it's	O
still	O
an	O
array	O
and	O
there	O
is	O
no	O
difference	O
in	O
asymptotic	O
complexity	O
.	O

Here's	O
one	O
way	O
(	O
same	O
matrix	O
as	O
before	O
):	O
#CODE	O

assume	O
i	O
have	O
100	O
points	O
whose	O
coordinates	O
are	O
random	O
,	O

If	O
you	O
just	O
want	O
the	O
first	O
one	O
,	O
use	O
next	O
with	O
the	O
list	O
comprehension	O
as	O
a	O
generator	O
expression	O
.	O

So	O
I	O
am	O
able	O
to	O
plot	O
what	O
I	O
want	O
onto	O
my	O
matrix	O

By	O
X3D	O
,	O
are	O
you	O
referring	O
to	O
the	O
x3d	O
standard	O
for	O
3d	O
content	O
,	O
as	O
at	O
#URL	O
If	O
so	O
,	O
I	O
would	O
very	O
much	O
like	O
to	O
learn	O
more	O
of	O
what	O
you	O
are	O
doing	O
--	O
thanks	O

Would	O
it	O
be	O
prohibitvely	O
wasteful	O
to	O
save	O
them	O
with	O
a	O
fixed	O
width	O
?	O

BSD-licensed	O
Python	O
source	O
code	O
for	O
surface	O
fits	O
can	O
be	O
found	O
at	O

...	O
which	O
returned	O
`	O
True	O
`	O
on	O
each	O
value	O
of	O
the	O
array	O
.	O

I	O
have	O
two	O
ordered	O
numpy	O
arrays	O
and	O
I	O
want	O
to	O
interleave	O
them	O
so	O
that	O
I	O
take	O
one	O
item	O
from	O
the	O
first	O
array	O
,	O
then	O
another	O
from	O
the	O
second	O
,	O
then	O
back	O
to	O
the	O
first	O
-	O
taking	O
the	O
next	O
item	O
that	O
is	O
larger	O
than	O
the	O
one	O
I	O
just	O
took	O
from	O
the	O
second	O
and	O
so	O
on	O
.	O

Did	O
you	O
look	O
at	O
the	O
link	O
in	O
my	O
answer	O
to	O
the	O
SciPy	O
page	O
on	O
Performance	O
Python	O
.	O

If	O
you	O
want	O
the	O
column	O
indices	O
instead	O
of	O
the	O
resulting	O
square	O
matrix	O
,	O
just	O
replace	O
`	O
return	O
B	O
`	O
with	O
`	O
return	O
colset	O
`	O
.	O

At	O
the	O
end	O
of	O
it	O
all	O
:	O
#CODE	O

Is	O
there	O
no	O
equivalent	O
function	O
that	O
gets	O
the	O
index	O
of	O
the	O
last	O
occurrence	O
?	O

I	O
want	O
to	O
get	O
a	O
cartesian	O
product	O
of	O
a	O
[:	O
:	O
i	O
]	O
and	O
b	O
[:	O
:	O
j	O
]	O
from	O
c	O
.	O

python	O
/	O
numpy	O
:	O
how	O
to	O
get	O
2D	O
array	O
column	O
length	O
?	O

Your	O
example	O
works	O
for	O
me	O
if	O
I	O
sample	O
around	O
2**6	O
points	O
.	O

NumPy's	O
main	O
object	O
is	O
the	O
homogeneous	O
multidimensional	O
array	O
.	O

Pythonic	O
way	O
to	O
import	O
data	O
from	O
multiple	O
files	O
into	O
an	O
array	O

The	O
only	O
thing	O
I	O
was	O
going	O
to	O
add	O
was	O
this	O
:	O
#URL	O
Indicated	O
that	O
this	O
is	O
not	O
likely	O
to	O
change	O
.	O

i	O
have	O
a	O
numpy	O
array	O
like	O
the	O
following	O
#CODE	O

I	O
want	O
to	O
write	O
a	O
Boost-Python	O
program	O
to	O
take	O
a	O
symbolic	O
python	O
function	O
from	O
user	O
and	O
evaluate	O
its	O
derivative	O
in	O
my	O
program	O
.	O

Is	O
there	O
a	O
way	O
around	O
this	O
?	O

Not	O
sure	O
if	O
I	O
explained	O
this	O
all	O
really	O
well	O
,	O
but	O
just	O
print	O
out	O
a_strided	O
and	O
you'll	O
see	O
what	O
the	O
result	O
is	O
and	O
how	O
easy	O
this	O
makes	O
the	O
operation	O
.	O

But	O
when	O
I	O
start	O
calling	O
columns	O
by	O
their	O
field	O
names	O
,	O
screwy	O
things	O
happen	O
.	O

all	O
I	O
get	O
is	O
very	O
high	O
or	O
inf	O
numbers	O
.	O

If	O
you're	O
iterating	O
through	O
,	O
and	O
applying	O
the	O
function	O
to	O
_each_	O
item	O
,	O
then	O
,	O
yeah	O
,	O
the	O
numpy	O
functions	O
will	O
be	O
slower	O
.	O

Slicing	O
does	O
not	O
copy	O
the	O
array	O
into	O
new	O
memory	O
(	O
unlike	O
delete	O
)	O
.	O

And	O
here's	O
the	O
filled	O
version	O
:	O
#CODE	O

This	O
is	O
a	O
little	O
bit	O
annoying	O
to	O
do	O
,	O
but	O
at	O
least	O
you	O
can	O
remove	O
that	O
annoying	O
`	O
==	O
`	O
easily	O
,	O
using	O
sorting	O
(	O
and	O
thats	O
probably	O
your	O
speed	O
killer	O
)	O
.	O

I	O
still	O
haven't	O
found	O
an	O
entirely	O
satisfactory	O
solution	O
,	O
but	O
nevertheless	O
there	O
is	O
something	O
one	O
can	O
do	O
to	O
obtain	O
the	O
pointer	O
with	O
a	O
lot	O
less	O
overhead	O
in	O
CPython	O
.	O

I	O
also	O
tried	O
using	O
NumPy	O
masked	O
arrays	O
,	O
with	O
NaN	O
fill_value	O
,	O
which	O
also	O
did	O
not	O
work	O
.	O

cartesian	O
(	O
split	O
(	O
a	O
,	O
3	O
))`	O
.	O

I	O
did	O
a	O
little	O
further	O
experimenting	O
and	O
found	O
a	O
numpy	O
specific	O
way	O
to	O
solve	O
this	O
:	O
#CODE	O

When	O
you	O
need	O
to	O
deal	O
with	O
exponential	O
,	O
you	O
quickly	O
go	O
into	O
under	O
/	O
over	O
flow	O
since	O
the	O
function	O
grows	O
so	O
quickly	O
.	O

Long	O
story	O
short	O
,	O
not	O
only	O
does	O
tabular	O
not	O
act	O
like	O
a	O
spreadsheet	O
out	O
of	O
the	O
box	O
,	O
I	O
can't	O
find	O
a	O
way	O
to	O
make	O
it	O
work	O
.	O

What	O
do	O
you	O
mean	O
"	O
two	O
significant	O
figures	O
"	O
?	O

We	O
put	O
it	O
in	O
a	O
list	O
and	O
double	O
it	O
.	O

For	O
example	O
for	O
value	O
255	O
the	O
coordinates	O
of	O
the	O
box	O
around	O
the	O
value	O
255	O
will	O
be	O
upper	O
left	O
(	O
0	O
,	O
0	O
)	O
and	O
lower	O
right	O
(	O
4	O
,	O
6	O
)	O
.	O

Like	O
in	O
a	O
java	O
program	O
,	O
you	O
can	O
choose	O
to	O
start	O
it	O
up	O
with	O
,	O
say	O
,	O
5GB	O
of	O
memory	O
.	O

However	O
,	O
due	O
to	O
the	O
way	O
the	O
data	O
points	O
lie	O
it	O
does	O
not	O
give	O
me	O
a	O
y-axis	O
interception	O
at	O
0	O
.	O

I'd	O
like	O
to	O
sort	O
it	O
such	O
that	O
my	O
points	O
are	O
ordered	O
by	O
x-coordinate	O
,	O
and	O
then	O
by	O
y	O
in	O
cases	O
where	O
the	O
x	O
coordinate	O
is	O
the	O
same	O
.	O

Of	O
course	O
this	O
will	O
slow	O
the	O
program	O
down	O
,	O
but	O
at	O
least	O
it'll	O
finish	O
.	O

Im	O
writing	O
it	O
here	O
because	O
i	O
cant	O
put	O
image	O
in	O
comment	O
.	O

In	O
looking	O
at	O
`	O
fill	O
`	O
,	O
I	O
saw	O
that	O
`	O
repeat	O
`	O
suits	O
my	O
needs	O
even	O
better	O
.	O

Note	O
that	O
an	O
array's	O
base	O
will	O
be	O
another	O
array	O
,	O
even	O
if	O
it	O
is	O
a	O
subset	O
:	O
#CODE	O

If	O
you	O
have	O
float	O
data	O
,	O
or	O
data	O
spread	O
over	O
a	O
huge	O
range	O
you	O
can	O
convert	O
it	O
to	O
integers	O
by	O
doing	O
:	O
#CODE	O

@USER	O
,	O
plaes	O
recommend	O
using	O
a	O
generator	O
(	O
parenthesis	O
)	O
instead	O
of	O
a	O
list	O
(	O
brackets	O
)	O
in	O
order	O
to	O
save	O
memory	O
and	O
gain	O
speed	O
when	O
managing	O
high	O
amounts	O
of	O
data	O
.	O

I	O
want	O
to	O
divide	O
this	O
array	O
into	O
3	O
blocks	O
of	O
size	O
2x4	O
,	O
and	O
then	O
find	O
the	O
mean	O
of	O
all	O
three	O
blocks	O
(	O
so	O
that	O
the	O
shape	O
of	O
the	O
mean	O
is	O
2x4	O
.	O

(	O
Have	O
a	O
look	O
at	O
the	O
comments	O
above	O
the	O
code	O
for	O
that	O
portion	O
.	O
)	O

That	O
is	O
because	O
`	O
fsolve	O
`	O
thinks	O
it	O
is	O
looking	O
for	O
an	O
array	O
of	O
length	O
17	O
that	O
solves	O
`	O
p	O
`	O
.	O

When	O
there's	O
a	O
choice	O
between	O
working	O
with	O
NumPy	O
array	O
and	O
numeric	O
lists	O
,	O
the	O
former	O
are	O
typically	O
faster	O
.	O

Wait	O
...	O
why	O
do	O
you	O
need	O
the	O
negative	O
?	O

But	O
if	O
a	O
dense	O
3d	O
array	O
representation	O
isn't	O
that	O
much	O
bigger	O
,	O
storing	O
it	O
as	O
a	O
chuncked	O
and	O
compressed	O
hdf5	O
array	O
is	O
probably	O
the	O
way	O
to	O
go	O
.	O

Index	O
datetime	O
in	O
numpy	O
array	O

Is	O
there	O
an	O
"	O
expandable	O
"	O
matrix	O
data	O
structure	O
available	O
in	O
a	O
well	O
tested	O
module	O
?	O

You	O
can	O
make	O
this	O
one-liner	O
reusable	O
if	O
you	O
are	O
going	O
to	O
repeat	O
it	O
a	O
lot	O
:	O
#CODE	O

Here's	O
my	O
array	O
(	O
rather	O
,	O
a	O
method	O
of	O
generating	O
representative	O
test	O
arrays	O
):	O
#CODE	O

We	O
need	O
more	O
information	O
on	O
your	O
array	O
.	O

@USER	O
:	O
If	O
the	O
code	O
all	O
F77	O
,	O
why	O
is	O
the	O
question	O
tagged	O
Python	O
?	O

It	O
does	O
that	O
without	O
densifying	O
the	O
matrix	O
right	O
?	O

To	O
speed	O
up	O
the	O
program	O
,	O
I	O
want	O
to	O
pass	O
the	O
index	O
through	O
a	O
subroutine	O
,	O
but	O
I	O
cannot	O
pass	O
`	O
[	O
index	O
[	O
0	O
]	O
,	O
:	O
,	O
index	O
[	O
1	O
]	O
,	O
index	O
[	O
2	O
]]`	O
through	O
a	O
subroutine	O
because	O
I	O
cannot	O
pass	O
the	O
colon	O
'	O
:	O
'	O
.	O

Any	O
thoughts	O
on	O
what	O
I'm	O
doing	O
wrong	O
?	O

This	O
will	O
be	O
far	O
,	O
far	O
faster	O
than	O
constantly	O
reallocating	O
the	O
array	O
inside	O
the	O
loop	O
.	O

How	O
can	O
I	O
get	O
a	O
new	O
array	O
containing	O
the	O
values	O
of	O
specific	O
attributes	O
of	O
those	O
objects	O
?	O

Seriously	O
,	O
at	O
least	O
leave	O
a	O
note	O
,	O
but	O
given	O
the	O
"	O
complexity	O
"	O
of	O
your	O
actual	O
request	O
I'd	O
say	O
that	O
you'll	O
have	O
better	O
chances	O
with	O
a	O
new	O
question	O
.	O

and	O
find	O
the	O
roots	O
with	O
numpy	O
:	O
#CODE	O

I	O
need	O
to	O
create	O
a	O
numpy	O
array	O
of	O
N	O
elements	O
,	O
but	O
I	O
want	O
to	O
access	O
the	O

I	O
have	O
allocated	O
a	O
chunk	O
of	O
double	O
in	O
a	O
C	O
library	O
and	O
I	O
would	O
like	O
to	O
create	O
a	O
numpy	O
1D	O
array	O
based	O
on	O
that	O
data	O
;	O
ideally	O
I	O
would	O
like	O
two	O
versions	O
one	O
which	O
only	O
wraps	O
the	O
c_ptr	O
readonly	O
-	O
letting	O
the	O
C	O
layer	O
retain	O
ownership	O
of	O
the	O
data	O
,	O
and	O
one	O
which	O
copies	O
the	O
data	O
.	O

The	O
code	O
included	O
in	O
pypy	O
is	O
a	O
new	O
array	O
class	O
which	O
tries	O
to	O
be	O
compatible	O
with	O
numpy	O
,	O
IOW	O
,	O
it	O
is	O
a	O
reimplementation	O
from	O
scratch	O
,	O
without	O
many	O
features	O
from	O
numpy	O
.	O

Like	O
I	O
say	O
,	O
I'm	O
honestly	O
struggling	O
,	O
any	O
help	O
would	O
be	O
much	O
appreciated	O
.	O

with	O
array	O
.	O

I'm	O
not	O
sure	O
that	O
I	O
understand	O
the	O
difference	O
between	O
copying	O
the	O
matrix	O
(	O
example	O
1	O
)	O
and	O
copying	O
the	O
data	O
(	O
example	O
2	O
)	O
.	O

Does	O
anybody	O
know	O
of	O
a	O
(	O
common	O
case	O
)	O
faster-than-linear	O
way	O
to	O
find	O
the	O
endpoints	O
of	O
a	O
boolean	O
property	O
of	O
an	O
array	O
.	O

Any	O
unrecognized	O
type	O
will	O
work	O
this	O
way	O
,	O
so	O
you	O
might	O
want	O
to	O
use	O
`	O
myclass	O
`	O
instead	O
of	O
`	O
object	O
`	O
.	O

Iterate	O
over	O
vectors	O
in	O
a	O
multidimensional	O
numpy	O
array	O

This	O
way	O
you	O
can	O
load	O
a	O
large	O
dataset	O
from	O
a	O
textfile	O
memory-efficiently	O
while	O
retaining	O
all	O
the	O
convenient	O
parsing	O
features	O
of	O
the	O
two	O
functions	O
.	O

you	O
may	O
win	O
few	O
cycles	O
if	O
you	O
multiply	O
by	O
inverse	O
instead	O
of	O
dividing	O
in	O
floating-point	O
performance	O
.	O

Without	O
knowing	O
the	O
size	O
or	O
quantity	O
of	O
the	O
images	O
or	O
the	O
application	O
of	O
the	O
algorithm	O
(	O
computer	O
vision	O
?	O
)	O
,	O
I	O
can't	O
say	O
how	O
big	O
a	O
deal	O
that	O
kind	O
of	O
speedup	O
is	O
.	O

Is	O
there	O
an	O
easy	O
way	O
to	O
sort	O
these	O
eigenvalues	O
(	O
and	O
associated	O
vectors	O
)	O
in	O
order	O
?	O

You	O
can	O
pass	O
a	O
numpy	O
array	O
or	O
matrix	O
as	O
an	O
argument	O
when	O
initializing	O
a	O
sparse	O
matrix	O
.	O

(	O
For	O
most	O
common	O
applications	O
of	O
quadratic	O
forms	O
q	O
A	O
,	O
the	O
matrix	O
A	O
is	O
symmetric	O
,	O
or	O
even	O
symmetric	O
positive	O
definite	O
,	O
so	O
feel	O
free	O
to	O
assume	O
that	O
either	O
one	O
of	O
these	O
is	O
the	O
case	O
,	O
if	O
it	O
matters	O
for	O
your	O
answer	O
.	O
)	O

I	O
think	O
you	O
might	O
find	O
the	O
`	O
flat	O
`	O
method	O
useful	O
.	O

Now	O
that	O
we	O
have	O
both	O
the	O
starting	O
and	O
ending	O
values	O
,	O
we	O
can	O
use	O
the	O
indices	O
function	O
from	O
this	O
question	O
to	O
get	O
an	O
array	O
of	O
selector	O
indices	O
:	O
#CODE	O

10	O
(	O
i	O
?	O
1	O
)	O
K	O
,	O
where	O
K	O
=	O
k	O
/	O
(	O
n	O
?	O
1	O
)	O
.	O

This	O
identifies	O
which	O
rows	O
have	O
any	O
element	O
which	O
are	O
True	O
#CODE	O

Broadcasting	O
is	O
a	O
more	O
general	O
way	O
to	O
fill	O
an	O
array	O
and	O
I	O
would	O
guess	O
is	O
slower	O
or	O
equal	O
to	O
the	O
very	O
narrow	O
use	O
case	O
of	O
`	O
fill	O
`	O
.	O

By	O
"	O
not	O
replicating	O
data	O
"	O
I	O
am	O
assuming	O
you	O
mean	O
"	O
not	O
allocating	O
more	O
memory	O
"	O
.	O

Can	O
you	O
post	O
all	O
/	O
more	O
of	O
the	O
data	O
?	O

The	O
scoring	O
matrix	O
would	O
be	O
trivial	O
,	O
as	O
the	O
"	O
distance	O
"	O
between	O
two	O
numbers	O
is	O
just	O
their	O
difference	O
.	O

Contours	O
around	O
scipy	O
labeled	O
regions	O
in	O
a	O
2D	O
grid	O

Why	O
doesn't	O
the	O
shape	O
of	O
my	O
numpy	O
array	O
change	O
?	O

Mind	O
also	O
the	O
indexing	O
starts	O
at	O
`	O
0	O
`	O

I	O
have	O
a	O
vague	O
feeling	O
that	O
I	O
might	O
have	O
seen	O
a	O
question	O
addressing	O
this	O
problem	O
,	O
but	O
I	O
can't	O
find	O
it	O
now	O
.	O

If	O
you	O
know	O
which	O
rows	O
are	O
to	O
be	O
deleted	O
,	O
just	O
extract	O
the	O
other	O
rows	O
(	O
you	O
need	O
)	O
and	O
create	O
a	O
new	O
array	O
.	O

If	O
there	O
is	O
any	O
other	O
way	O
I	O
guess	O
I	O
have	O
to	O
do	O
that	O
.	O

I	O
have	O
a	O
matrix	O
,	O
say	O
#CODE	O

You	O
might	O
find	O
out	O
the	O
distribution	O
information	O
using	O
`	O
cat	O
/	O
etc	O
/	O
*-release	O
`	O
;)	O

I	O
was	O
wondering	O
if	O
anyone	O
found	O
a	O
good	O
workaround	O
,	O
as	O
my	O
real-world	O
problem	O
of	O
iterating	O
over	O
the	O
Cartesian-product	O
of	O
the	O
rows	O
in	O
very	O
large	O
arrays	O
is	O
so	O
slow	O
it's	O
impeding	O
progress	O
.	O

I	O
have	O
an	O
array	O
of	O
x	O
,	O
y	O
,	O
z	O
distances	O
and	O
I	O
need	O
to	O
find	O
the	O
differences	O
between	O
each	O
vector	O
from	O
one	O
another	O
.	O

The	O
code	O
above	O
finds	O
parts	O
where	O
there	O
are	O
at	O
least	O
MIN_SILENCE	O
consecutive	O
elements	O
smaller	O
than	O
SILENCE_THRESHOLD	O
.	O

The	O
list	O
of	O
indices	O
will	O
always	O
be	O
ascending	O
,	O
never	O
have	O
duplicates	O
,	O
but	O
may	O
have	O
gaps	O
like	O
the	O
example	O
.	O

Any	O
ideas	O
?	O

sum	O
function	O
in	O
python	O

All	O
globals	O
hold	O
either	O
values	O
referenced	O
by	O
those	O
tuples	O
or	O
are	O
lists	O
of	O
tuples	O
.	O

You	O
can	O
pass	O
a	O
list	O
or	O
an	O
array	O
as	O
indexes	O
to	O
any	O
np	O
array	O
.	O

The	O
array	O
I'm	O
using	O
is	O
quite	O
large	O
(	O
3500x3500	O
)	O
,	O
so	O
I'm	O
wondering	O
where	O
the	O
best	O
place	O
to	O
load	O
it	O
is	O
for	O
repeated	O
use	O
.	O

Basically	O
,	O
it	O
comes	O
down	O
to	O
checking	O
before	O
you	O
add	O
.	O

I	O
have	O
a	O
simple	O
function	O
called	O
get_gradient	O
which	O
takes	O
a	O
numpy	O
array	O
of	O
[[	O
x	O
,	O
y	O
,	O
Vx	O
,	O
Vy	O
]]	O
and	O
returns	O
(	O
should	O
return	O
)	O
an	O
array	O
of	O
[[	O
Vx	O
,	O
Vy	O
,	O
Ax	O
,	O
Ay	O
]]	O
.	O

I	O
found	O
this	O
post	O
:	O
Python	O
:	O
finding	O
an	O
element	O
in	O
an	O
array	O

So	O
,	O
are	O
VBOs	O
simply	O
not	O
meant	O
to	O
be	O
that	O
big	O
(	O
I	O
somehow	O
doubt	O
that	O
VBOs	O
could	O
only	O
have	O
around	O
17k	O
triangles	O
each	O
)	O
?	O

`	O
flags	O
`	O
parameter	O
leads	O
to	O
`	O
TypeError	O
`	O
if	O
input	O
array	O
is	O
not	O
contiguous	O
.	O

I	O
then	O
have	O
a	O
2nd	O
array	O
similar	O
to	O
#CODE	O

Convert	O
a	O
list	O
of	O
2D	O
numpy	O
arrays	O
to	O
one	O
3D	O
numpy	O
array	O
?	O

I'm	O
currently	O
a	O
grad	O
student	O
at	O
Harvard	O
and	O
a	O
good	O
friend	O
of	O
mine	O
went	O
there	O
(	O
he	O
would	O
have	O
graduated	O
two	O
or	O
three	O
years	O
ago	O
,	O
as	O
he	O
is	O
currently	O
a	O
second-year	O
grad	O
student	O
here	O
at	O
Harvard	O
with	O
me	O
)	O
.	O

I'm	O
not	O
clear	O
on	O
how	O
you	O
are	O
wanting	O
to	O
plot	O
it	O
,	O
but	O
it	O
sound	O
like	O
you'll	O
need	O
to	O
select	O
some	O
values	O
of	O
a	O
column	O
.	O

The	O
issue	O
I	O
am	O
running	O
in	O
to	O
is	O
that	O
the	O
array	O
can	O
be	O
larger	O
than	O
3gb	O
in	O
size	O
(	O
these	O
are	O
huge	O
images	O
)	O
and	O
I	O
need	O
to	O
segment	O
them	O
prior	O
to	O
ingesting	O
them	O
.	O

The	O
latter	O
might	O
be	O
faster	O
because	O
it	O
doesn't	O
produce	O
the	O
intermediate	O
`	O
x**2	O
`	O
array	O
.	O

Any	O
suggestions	O
?	O

"	O
A	O
copy	O
of	O
arr	O
with	O
the	O
elements	O
specified	O
by	O
obj	O
removed	O
.	O

is	O
not	O
it	O
another	O
copy	O
?	O

NOTE	O
:	O
the	O
row	O
has	O
"	O
:	O
"	O
,	O
but	O
the	O
"	O
:	O
"	O
does	O
mean	O
the	O
dict	O
'	O
:	O
'	O
.	O

If	O
,	O
for	O
some	O
reason	O
,	O
I	O
would	O
only	O
save	O
one	O
dictionary	O
then	O
every	O
script	O
loading	O
this	O
file	O
with	O
pickle	O
would	O
mess	O
up	O
the	O
order	O
of	O
the	O
stored	O
variables	O
.	O

You	O
might	O
also	O
want	O
to	O
take	O
a	O
look	O
at	O
Anvil	O
,	O
announcement	O
here	O
.	O

The	O
other	O
way	O
that	O
I	O
know	O
is	O
to	O
convert	O
Y	O
to	O
list	O
iteratively	O
.	O

This	O
is	O
especially	O
helpful	O
since	O
it	O
includes	O
the	O
import	O
commands	O
and	O
info	O
on	O
how	O
to	O
write	O
to	O
file	O
.	O

But	O
actually	O
I	O
am	O
not	O
so	O
sure	O
that	O
from	O
where	O
you	O
are	O
now	O
,	O
using	O
sparse	O
matrices	O
will	O
gain	O
you	O
any	O
speed-up	O
.	O

Upon	O
deeper	O
examination	O
of	O
the	O
relationship	O
between	O
the	O
python	O
printout	O
and	O
the	O
structure	O
of	O
my	O
underlying	O
data	O
,	O
I	O
see	O
that	O
the	O
python	O
print	O
command	O
is	O
saying	O
that	O
there	O
are	O
two	O
empty	O
columns	O
at	O
the	O
end	O
of	O
the	O
array	O
.	O

How	O
to	O
convert	O
a	O
simple	O
list	O
of	O
lists	O
into	O
a	O
numppy	O
array	O
?	O

Django	O
has	O
a	O
library	O
for	O
encapsulating	O
all	O
the	O
database	O
work	O
into	O
Python	O
classes	O
,	O
so	O
you	O
don't	O
have	O
to	O
mess	O
with	O
raw	O
SQL	O
until	O
you	O
have	O
to	O
do	O
something	O
really	O
clever	O
.	O

So	O
I	O
got	O
numpy	O
,	O
scipy	O
,	O
IPython	O
,	O
and	O
matplotlib	O
working	O
(	O
I	O
can	O
import	O
all	O
four	O
with	O
"	O
import	O
_	O
)"	O
.	O

@USER	O
`	O
new	O
type	O
not	O
compatible	O
with	O
array	O
.	O

Is	O
there	O
any	O
way	O
to	O
do	O
this	O
in	O
Python	O
?	O

Then	O
you	O
can	O
choose	O
many	O
methods	O
to	O
visualize	O
it	O
.	O

Numpy	O
Array	O
to	O
base64	O
and	O
back	O
to	O
Numpy	O
Array	O
-	O
Python	O

In	O
each	O
iteration	O
of	O
Gibbs	O
sampling	O
,	O
we	O
remove	O
one	O
(	O
current	O
)	O
word	O
,	O
sample	O
a	O
new	O
topic	O
for	O
that	O
word	O
according	O
to	O
a	O
posterior	O
conditional	O
probability	O
distribution	O
inferred	O
from	O
the	O
LDA	O
model	O
,	O
and	O
update	O
word-topic	O
counts	O
,	O
as	O
follows	O
:	O
#CODE	O

I	O
am	O
getting	O
weird	O
errors	O
when	O
I	O
try	O
to	O
convert	O
a	O
black	O
and	O
white	O
PIL	O
image	O
to	O
a	O
numpy	O
array	O
.	O

Numpy	O
arrays	O
have	O
a	O
`	O
copy	O
`	O
method	O
which	O
you	O
can	O
use	O
for	O
just	O
this	O
purpose	O
.	O

Actually	O
I	O
could	O
not	O
test	O
with	O
big	O
K	O
,	O
d	O
and	O
N	O
as	O
I	O
was	O
going	O
out	O
of	O
memory	O
.	O

With	O
all	O
of	O
these	O
options	O
you	O
have	O
to	O
pay	O
a	O
JNA	O
tax	O
...	O
all	O
of	O
your	O
data	O
has	O
to	O
be	O
copied	O
before	O
it	O
can	O
be	O
processed	O
.	O

Useless	O
because	O
it	O
ignores	O
the	O
"	O
cross	O
platform	O
issues	O
,	O
proprietary	O
tool	O
chains	O
,	O
certification	O
gates	O
,	O
licensed	O
technologies	O
,	O
and	O
stringent	O
performance	O
requirements	O
on	O
top	O
of	O
the	O
issues	O
with	O
legacy	O
codebases	O
and	O
workforce	O
availability	O
"	O
(	O
John	O
Carmack	O
)	O
that	O
op	O
is	O
probably	O
facing	O
.	O

And	O
that	O
the	O
values	O
of	O
all	O
(	O
x	O
,	O
y	O
)	O
pairs	O
are	O
given	O
.	O

Is	O
is	O
possible	O
to	O
have	O
a	O
3-D	O
record	O
array	O
in	O
numpy	O
?	O

However	O
,	O
the	O
evidence	O
suggests	O
that	O
you've	O
encountered	O
an	O
issue	O
of	O
this	O
sort	O
.	O

There's	O
_way_	O
less	O
overhead	O
this	O
way	O
.	O

I'm	O
having	O
trouble	O
figuring	O
out	O
what	O
kind	O
of	O
test	O
I	O
need	O
here	O
,	O
and	O
the	O
best	O
numpy	O
/	O
scipy	O
/	O
R	O
function	O
to	O
use	O
for	O
these	O
kinds	O
of	O
issues	O
.	O

I	O
have	O
see	O
people	O
using	O
dictionaries	O
,	O
but	O
the	O
arrays	O
are	O
large	O
and	O
filled	O
with	O
both	O
positive	O
and	O
negative	O
floats	O
.	O

How	O
can	O
I	O
speed	O
up	O
iteration	O
through	O
this	O
transformed	O
numpy	O
array	O
?	O

This	O
is	O
may	O
not	O
be	O
the	O
best	O
way	O
to	O
solve	O
this	O
but	O
have	O
a	O
look	O
at	O
the	O
following	O
...	O

All	O
in	O
all	O
,	O
I	O
would	O
go	O
with	O
the	O
#CODE	O

This	O
is	O
not	O
a	O
matter	O
of	O
style	O
.	O
without	O
the	O
list	O
(	O
_	O
)	O
it	O
does	O
not	O
even	O
work	O
at	O
last	O
for	O
the	O
case	O
i	O
have	O
that	O
y	O
is	O
an	O
array	O
itself	O

(	O
at	O
least	O
it	O
gives	O
me	O
an	O
error	O
stating	O
that	O
the	O
'	O
as	O
'	O
is	O
reserved	O
in	O
python	O
2.6	O
)	O
Am	O
I	O
correct	O
?	O

Did	O
you	O
try	O
looking	O
at	O
numpy	O
for	O
matlab	O
users	O
manuals	O
,	O
like	O
:	O
#URL	O

I	O
would	O
not	O
try	O
to	O
process	O
`	O
arr	O
`	O
in	O
place	O
-	O
it	O
seems	O
that	O
a	O
new	O
array	O
is	O
created	O
under	O
the	O
hood	O
in	O
most	O
cases	O
anyway	O
.	O

Now	O
you	O
must	O
initialize	O
each	O
element	O
of	O
the	O
numpy	O
array	O
to	O
be	O
an	O
1-d	O
numpy	O
array	O
:	O
#CODE	O

The	O
easiest	O
way	O
around	O
this	O
is	O
to	O
just	O
use	O
a	O
numpy	O
array	O
,	O
instead	O
of	O
a	O
numpy	O
matrix	O
:	O
#CODE	O

I	O
am	O
trying	O
to	O
create	O
an	O
affinity	O
matrix	O
for	O
an	O
image	O
.	O

to	O
handle	O
the	O
error	O
cases	O
and	O
the	O
return	O
value	O
,	O
they	O
are	O
not	O
related	O
to	O
the	O
array	O
assignment	O
.	O

Saving	O
a	O
Numpy	O
array	O
as	O
an	O
image	O
(	O
instructions	O
)	O

Using	O
this	O
,	O
I	O
know	O
I	O
am	O
calculating	O
r-squared	O
correctly	O
for	O
linear	O
best-fit	O
(	O
degree	O
equals	O
1	O
)	O
.	O

No	O
expert	O
on	O
the	O
topic	O
,	O
but	O
this	O
is	O
some	O
kind	O
of	O
adjency	O
matrix	O
(	O
#URL	O
)	O
.	O

about	O
15	O
times	O
faster	O
using	O
broadcast	O

Arrays	O
to	O
Matrix	O
numpy	O

but	O
it	O
appears	O
to	O
only	O
take	O
square	O
matrices	O
.	O

Any	O
idea	O
how	O
that	O
can	O
be	O
done	O
?	O

In	O
your	O
code	O
,	O
`	O
a	O
[	O
condition	O
]	O
[	O
index	O
]`	O
returns	O
the	O
value	O
in	O
a	O
,	O
but	O
I	O
want	O
the	O
INDEX	O
in	O
a	O
,	O
so	O
that	O
`	O
a	O
[	O
INDEX	O
]	O
=	O
a	O
[	O
condition	O
]	O
[	O
index	O
]`	O
.	O

Any	O
database	O
that	O
can	O
create	O
an	O
index	O
will	O
provide	O
relatively	O
fast	O
look-ups	O
(	O
depending	O
on	O
how	O
many	O
millions	O
of	O
records	O
you're	O
storing	O
)	O
.	O

Actually	O
,	O
the	O
best	O
way	O
to	O
manage	O
packages	O
on	O
OS	O
X	O
is	O
[	O
Homebrew	O
]	O
(	O
#URL	O
)	O
(	O
not	O
Fink	O
or	O
MacPorts	O
:))	O
-	O
which	O
unfortunately	O
lists	O
neither	O
NumPy	O
now	O
SciPy	O
at	O
the	O
current	O
time	O
.	O

I	O
would	O
like	O
to	O
keep	O
`	O
xcoords	O
`	O
a	O
numpy	O
array	O
if	O
possible	O
.	O
what	O
do	O
you	O
mean	O
'	O
adding	O
them	O
to	O
the	O
object	O
before	O
it	O
is	O
returned	O
'	O
?	O

But	O
I	O
just	O
need	O
to	O
sort	O
out	O
which	O
points	O
to	O
send	O
for	O
a	O
complete	O
graph	O
.	O

how	O
do	O
I	O
calculate	O
that	O
an	O
array	O
of	O
python	O
numpy	O
or	O
me	O
of	O
all	O
the	O
calculate	O
decimals	O
and	O
not	O
skip	O
like	O
.	O

It	O
will	O
support	O
it	O
on	O
the	O
next	O
release	O
.	O

Python	O
lists	O
are	O
defined	O
with	O
square	O
brackets	O
,	O
and	O
we	O
want	O
to	O
generate	O
a	O
list	O
of	O
lists	O
(	O
where	O
each	O
piece	O
contains	O
one	O
of	O
your	O
defined	O
segments	O
)	O
.	O

The	O
biggest	O
gotcha	O
for	O
me	O
was	O
that	O
almost	O
every	O
standard	O
operator	O
is	O
overloaded	O
to	O
distribute	O
across	O
the	O
array	O
.	O

I	O
want	O
to	O
combine	O
the	O
two	O
into	O
a	O
mutli-dimensional	O
numpy	O
array	O
.	O

where	O
`	O
nlooks	O
`	O
and	O
`	O
dfactor	O
`	O
are	O
scalars	O
and	O
`	O
Ic	O
`	O
is	O
the	O
unfiltered	O
array	O
.	O

In	O
a	O
10x5x5	O
matrix	O
with	O
`	O
x	O
[	O
0	O
,	O
:	O
,	O
:]	O
=	O
0	O
`	O
I	O
would	O
expect	O
a	O
result	O
of	O
:	O
#CODE	O

For	O
example	O
:	O
I	O
have	O
a	O
=	O
array	O
([	O
123	O
,	O
412	O
,	O
444	O
])	O

While	O
it	O
often	O
results	O
in	O
a	O
massive	O
speedup	O
to	O
eliminate	O
for	O
loops	O
and	O
take	O
advantage	O
of	O
numpy	O
built-ins	O
/	O
vectorization	O
.	O

If	O
I	O
understand	O
correctly	O
you	O
have	O
a	O
three	O
dimensional	O
array	O
,	O
something	O
like	O
:	O
#CODE	O

@USER	O
:	O
where	O
is	O
a	O
new	O
array	O
created	O
?	O

array	O
([	O
41	O
,	O
32	O
,	O
41	O
,	O
33	O
,	O
42	O
,	O
32	O
,	O
42	O
,	O
33	O
])	O

Any	O
idea	O
when	O
it	O
will	O
be	O
ready	O
?	O

I	O
see	O
you've	O
taken	O
care	O
of	O
my	O
edge	O
issues	O
,	O
although	O
your	O
filter	O
size	O
is	O
hardcoded	O
;)	O
.	O

If	O
you	O
open	O
idle	O
and	O
type	O
`	O
import	O
matplotlib	O
`	O
it	O
shouldn't	O
return	O
an	O
error	O

No	O
expert	O
on	O
the	O
topic	O
,	O
but	O
this	O
is	O
some	O
kind	O
of	O
adjency	O
matrix	O
(	O
#URL	O
)	O
.	O

Edit	O
:	O
If	O
it's	O
a	O
floating	O
point	O
issue	O
,	O
what	O
sort	O
of	O
floating	O
point	O
error	O
mistakes	O
a	O
number	O
much	O
less	O
than	O
1	O
as	O
one	O
around	O
8	O
?	O

The	O
question	O
was	O
about	O
how	O
to	O
slice	O
if	O
the	O
rank	O
is	O
not	O
known	O
at	O
the	O
time	O
I	O
write	O
the	O
code	O
.	O

I	O
think	O
a	O
typical	O
method	O
is	O
to	O
always	O
double	O
the	O
size	O
,	O
when	O
you	O
really	O
don't	O
know	O
how	O
large	O
things	O
will	O
be	O
.	O

This	O
script	O
is	O
mainly	O
intended	O
to	O
demonstrate	O
building	O
an	O
independent	O
python	O
in	O
your	O
home	O
directory	O
,	O
and	O
assumes	O
the	O
system	O
you're	O
building	O
on	O
has	O
the	O
proper	O
dependencies	O
already	O
installed	O
,	O
but	O
it	O
at	O
least	O
points	O
you	O
in	O
the	O
right	O
direction	O
.	O

and	O
use	O
the	O
information	O
on	O
the	O
size	O
inclued	O
in	O
the	O
filename	O
to	O
restore	O
the	O
initial	O
shape	O

Hmm	O
I	O
added	O
for	O
first	O
example	O
,	O
did	O
you	O
know	O
how	O
to	O
copy	O
from	O
IDE	O
exactly	O
with	O
commas	O
and	O
everything	O
..?	O

@USER	O
:	O
Your	O
answer	O
will	O
give	O
false	O
positives	O
in	O
the	O
event	O
that	O
one	O
or	O
more	O
(	O
but	O
not	O
all	O
)	O
of	O
the	O
elements	O
in	O
B	O
matches	O
with	O
one	O
of	O
the	O
rows	O
in	O
A	O
.	O

I	O
would	O
like	O
to	O
average	O
the	O
2	O
different	O
arrays	O
contained	O
within	O
`	O
record	O
`	O
.	O

I	O
need	O
to	O
constrained	O
minimization	O
of	O
some	O
data	O
(	O
ie	O
so	O
that	O
I	O
get	O
the	O
minimum	O
value	O
within	O
a	O
certain	O
range	O
)	O
.	O

In	O
this	O
case	O
,	O
I	O
would	O
like	O
to	O
return	O
the	O
index	O
2	O
(	O
2nd	O
row	O
)	O
.	O

a	O
32	O
bits	O
process	O
can	O
only	O
access	O
around	O
4	O
GB	O
of	O
memory	O
.	O

How	O
do	O
I	O
find	O
out	O
,	O
if	O
the	O
numpy	O
BLAS	O
libraries	O
are	O
availalbe	O
as	O
dynamically-loadable	O
?	O

(	O
they	O
are	O
at	O
same	O
scale	O
)	O

Now	O
simply	O
create	O
a	O
new	O
array	O
and	O
multiply	O
:	O
#CODE	O

Take	O
a	O
look	O
at	O
this	O
Project	O
Euler	O
problem	O
:	O
#URL	O

Python	O
:	O
how	O
to	O
store	O
a	O
numpy	O
multidimensional	O
array	O
in	O
PyTables	O
?	O

How	O
can	O
i	O
load	O
all	O
24	O
joblib	O
files	O
in	O
one	O
program	O
without	O
any	O
errors	O
?	O

Where	O
I'm	O
stuck	O
is	O
what	O
the	O
wrapper	O
code	O
should	O
then	O
look	O
like	O
to	O
pass	O
a	O
MxN	O
numpy	O
array	O
to	O
the	O
**	O
coords1	O
and	O
**	O
coords2	O
arguments	O
.	O

I	O
have	O
created	O
a	O
numpy	O
2d	O
array	O
of	O
type	O
string	O
called	O
'	O
minutes_array	O
'	O
with	O
the	O
first	O
column	O
as	O
unix	O
timestamps	O
rounded	O
to	O
the	O
nearest	O
minute	O
covering	O
every	O
minute	O
from	O
the	O
start	O
of	O
the	O
sensor	O
timeseries	O
to	O
the	O
end	O
with	O
three	O
empty	O
columns	O
to	O
be	O
filled	O
with	O
data	O
from	O
each	O
of	O
the	O
3	O
sensors	O
where	O
available	O
.	O

Which	O
can	O
be	O
done	O
in	O
O	O
(	O
n	O
)	O
,	O
but	O
your	O
answer	O
requires	O
O	O
(	O
mn	O
)	O
,	O
where	O
m	O
is	O
size	O
of	O
window	O
.	O

Somehow	O
I	O
always	O
thought	O
you	O
can	O
load	O
the	O
shared	O
library	O
compiled	O
with	O
any	O
compiler	O
.	O

`	O
array	O
=[	O
'	O
NaN	O
'	O
,	O
'	O
20	O
'	O
,	O
'	O
383.333	O
'	O
,	O
'	O
NaN	O
'	O
,	O
'	O
NaN	O
'	O
,	O
'	O
NaN	O
'	O
,	O
'	O
5	O
'	O
,	O
'	O
100	O
'	O
,	O
'	O
129	O
'	O
,	O
'	O
122.5	O
'	O
,	O
'	O
NaN	O
'	O
,	O
'	O
NaN	O
']`	O

array	O
,	O
and	O
then	O
use	O
`	O
view	O
`	O
to	O
turn	O
it	O
into	O
a	O
structured	O
array	O
,	O
and	O
then	O
use	O

and	O
so	O
all	O
we	O
need	O
to	O
do	O
is	O
:	O
#CODE	O

Any	O
clue	O
to	O
why	O
this	O
is	O
happening	O
?	O

I	O
think	O
the	O
definition	O
used	O
in	O
the	O
field	O
of	O
statistics	O
is	O
the	O
value	O
in	O
the	O
middle	O
of	O
your	O
data	O
array	O
after	O
it	O
has	O
been	O
sorted	O
.	O

Dense	O
covariance	O
matrices	O
of	O
that	O
size	O
suggest	O
operations	O
that	O
run	O
forever	O
!	O

In	O
this	O
case	O
,	O
I'd	O
like	O
it	O
to	O
return	O
a	O
density	O
that's	O
essentially	O
peaked	O
completely	O
at	O
a	O
difference	O
of	O
0	O
,	O
with	O
no	O
mass	O
everywhere	O
else	O
.	O

If	O
the	O
array	O
is	O
doubles	O
(	O
remember	O
python	O
floats	O
are	O
C	O
doubles	O
by	O
default	O
)	O
then	O
you	O
have	O
to	O
think	O
a	O
bit	O
harder	O
as	O
==	O
is	O
not	O
really	O
safe	O
or	O
what	O
you	O
want	O
for	O
floating	O
point	O
values	O
.	O

They	O
all	O
have	O
their	O
strengths	O
and	O
weaknesses	O
.	O

numpy	O
array	O
of	O
chars	O
to	O
string	O

matrix	O
rank	O
:	O
#CODE	O

This	O
slows	O
down	O
for	O
large	O
sigma	O
,	O
at	O
which	O
point	O
using	O
FFT-based	O
smoothing	O
might	O
be	O
faster	O
.	O

What	O
is	O
the	O
fastest	O
way	O
to	O
iterate	O
through	O
all	O
one	O
dimensional	O
sub-arrays	O
of	O
an	O
n	O
dimensional	O
array	O
in	O
python	O
.	O

This	O
works	O
,	O
but	O
it's	O
really	O
slow	O
.	O

If	O
I	O
create	O
a	O
simple	O
array	O
like	O
this	O
in	O
Python	O
I'm	O
able	O
to	O
read	O
the	O
values	O
in	O
the	O
C	O
code	O
:	O

In	O
an	O
ideal	O
world	O
,	O
the	O
function	O
or	O
class	O
would	O
support	O
overlap	O
between	O
the	O
divisions	O
in	O
the	O
input	O
matrix	O
too	O
.	O

My	O
problem	O
is	O
different	O
because	O
I	O
need	O
to	O
find	O
**	O
all	O
**	O
the	O
roots	O
of	O
my	O
function	O
,	O
on	O
a	O
given	O
interval	O
.	O

How	O
can	O
I	O
create	O
a	O
PyArrayObject	O
from	O
this	O
structure	O
,	O
specially	O
how	O
I	O
can	O
create	O
a	O
numpy	O
array	O
that	O
hold	O
3	O
object	O
(	O
off	O
course	O
3	O
is	O
an	O
example	O
here	O
)	O
(	O
each	O
of	O
them	O
is	O
an	O
array	O
)	O

x	O
:	O
a	O
numpy	O
2d	O
array	O

Thanks	O
for	O
the	O
info	O
.	O

How	O
would	O
you	O
avoid	O
the	O
loop	O
in	O
the	O
case	O
that	O
all	O
entries	O
in	O
`	O
repl	O
`	O
are	O
the	O
same	O
?	O

Pulling	O
data	O
from	O
a	O
numpy	O
array	O

There's	O
no	O
effective	O
difference	O
(	O
they	O
both	O
return	O
views	O
into	O
the	O
original	O
array	O
)	O
.	O

Thanks	O
for	O
all	O
the	O
tips	O
!	O

remove	O
zero	O
lines	O
2-D	O
numpy	O
array	O

Instead	O
of	O
using	O
`	O
PyInt_AsLong	O
`	O
,	O
use	O
the	O
`	O
PyArray_*	O
`	O
functions	O
provided	O
by	O
Numpy's	O
C	O
API	O
to	O
access	O
the	O
data	O
;	O
in	O
particular	O
,	O
see	O
section	O
Array	O
API	O
.	O

Well	O
,	O
I	O
tried	O
dividing	O
by	O
the	O
largest	O
place	O
value	O
.	O

All	O
of	O
those	O
numpys	O
are	O
linked	O
to	O
the	O
system	O
Accelerate	O
framework	O
:	O
#CODE	O

and	O
I	O
wish	O
to	O
create	O
a	O
third	O
array	O
with	O
each	O
element	O
from	O
`	O
b	O
`	O
appearing	O
`	O
a	O
`	O
times	O
in	O
the	O
new	O
array	O
,	O
as	O
:	O
#CODE	O

I	O
can	O
imagine	O
a	O
number	O
of	O
approaches	O
to	O
storing	O
both	O
of	O
these	O
data	O
formats	O
,	O
ranging	O
from	O
storing	O
the	O
metadata	O
with	O
the	O
`	O
AttributeSet	O
`	O
class	O
for	O
each	O
`	O
Array	O
`	O
/	O
`	O
CArray	O
`	O
to	O
using	O
a	O
`	O
Table	O
`	O
for	O
all	O
of	O
the	O
metadata	O
.	O

I	O
want	O
to	O
calculate	O
the	O
average	O
of	O
four	O
neighbors	O
in	O
a	O
huge	O
array	O
.	O

Suppress	O
Scientific	O
Notation	O
in	O
Numpy	O
When	O
Creating	O
Array	O
From	O
Nested	O
List	O

I	O
want	O
to	O
find	O
the	O
vector	O
x	O
'	O
such	O
that	O
Ax	O
'	O
is	O
as	O
close	O
as	O
possible	O
to	O

And	O
the	O
dataset	O
in	O
question	O
is	O
beyond	O
doubt	O
particular	O
:	O
There	O
certainly	O
is	O
an	O
upper	O
bound	O
and	O
a	O
precision	O
.	O

Only	O
integers	O
can	O
be	O
used	O
as	O
array	O
or	O
matrix	O
indices	O
.	O

I	O
can't	O
find	O
it	O
online	O
anywhere	O
.	O

I	O
will	O
try	O
your	O
code	O
,	O
but	O
I	O
am	O
also	O
going	O
to	O
try	O
writing	O
a	O
simple	O
C	O
extension	O
to	O
simply	O
do	O
the	O
reading	O
,	O
math	O
,	O
and	O
drawing	O
all	O
in	O
one	O
place	O
.	O

Are	O
there	O
any	O
good	O
greedy	O
implementations	O
to	O
solve	O
this	O
or	O
am	O
I	O
on	O
my	O
own	O
to	O
implement	O
this	O
?	O

The	O
problem	O
is	O
that	O
for	O
the	O
array	O
input	O
,	O
SWIG	O
complains	O
that	O
there	O
is	O
no	O
typemap	O
.	O

Is	O
`	O
column_array_to_add	O
`	O
another	O
2D	O
array	O
,	O
or	O
is	O
it	O
a	O
1D	O
column	O
array	O
,	O
as	O
the	O
name	O
implies	O
?	O

the	O
sum	O
of	O
a	O
triple-product	O
(	O
element-wise	O
)	O
.	O

I	O
ran	O
a	O
simple	O
speed	O
test	O
comparing	O
numpy	O
and	O
python	O
list	O
comprehension	O
,	O
and	O
apparently	O
list	O
comprehension	O
was	O
faster	O
.	O

That	O
is	O
why	O
your	O
sample	O
loop	O
has	O
been	O
collapsed	O
to	O
read	O
in	O
the	O
full	O
sample	O
for	O
the	O
receiver	O
and	O
channel	O
in	O
one	O
large	O
read	O
.	O

Something	O
like	O
the	O
following	O
iterator	O
should	O
get	O
around	O
both	O
of	O
these	O
problems	O
:	O
#CODE	O

I	O
appreciate	O
any	O
input	O
on	O
this	O
...	O

Do	O
you	O
really	O
need	O
to	O
find	O
such	O
a	O
weird	O
thing	O
?	O

Any	O
particular	O
reason	O
you	O
don't	O
want	O
to	O
use	O
a	O
straightforward	O
approach	O
?	O

The	O
advantage	O
of	O
numpy	O
is	O
the	O
support	O
of	O
slicing	O
at	O
different	O
levels	O
.	O

An	O
implementation	O
,	O
however	O
,	O
is	O
not	O
really	O
open	O
to	O
interpretation	O
.	O

Python	O
numpy	O
masked	O
array	O
initialization	O

You	O
can	O
further	O
optimize	O
by	O
exploiting	O
array-order	O
alignment	O
to	O
reduce	O
excess	O
memory	O
consumption	O
caused	O
by	O
copying	O
the	O
original	O
arrays	O
.	O

For	O
example	O
,	O
any	O
vector	O
(	O
of	O
the	O
appropriate	O
dimension	O
)	O
can	O
be	O
an	O
eigenvector	O
of	O
the	O
identity	O
matrix	O
.	O

The	O
normal	O
64-bit	O
double-precision	O
floating	O
point	O
has	O
least	O
positive	O
normal	O
value	O
2.2E-308	O
;	O
storing	O
logs	O
gives	O
you	O
an	O
effective	O
least	O
positive	O
normal	O
1E-	O
(	O
1.7E308	O
)	O
.	O

index	O
set	O
for	O
each	O
position	O
in	O
the	O
index	O
arrays	O
.	O

I	O
am	O
wondering	O
if	O
reassigning	O
temp	O
[	O
]	O
to	O
a	O
1-element	O
shorter	O
vector	O
each	O
time	O
is	O
slow	O
,	O
would	O
it	O
be	O
faster	O
to	O
pre-allocate	O
a	O
96-3	O
length	O
list	O
of	O
vectors	O
of	O
length	O
96	O
,	O
95	O
,	O
94	O
...	O
to	O
3	O
?	O

What	O
would	O
we	O
do	O
,	O
if	O
we	O
wanted	O
to	O
change	O
values	O
at	O
indexes	O
which	O
are	O
multiple	O
of	O
given	O
n	O
,	O
like	O
a	O
[	O
2	O
]	O
,	O
a	O
[	O
4	O
]	O
,	O
a	O
[	O
6	O
]	O
,	O
a	O
[8	O
]	O
.....	O
for	O
n=2	O
?	O

Thanks	O
for	O
all	O
the	O
python	O
guidance	O
!	O

I'm	O
not	O
really	O
pro	O
Matlab	O
,	O
but	O
surely	O
Stata	O
can't	O
be	O
so	O
bad	O
as	O
to	O
require	O
`	O
adoedit	O
`	O
just	O
to	O
know	O
what	O
algorithm	O
it	O
is	O
using	O
?	O

This	O
can	O
be	O
found	O
relatively	O
easily	O
by	O
just	O
looking	O
at	O
points	O
where	O
the	O
potential	O
exceeds	O
a	O
certain	O
threshold	O
.	O

Negative	O
indices	O
are	O
interpreted	O
as	O
counting	O
from	O
the	O
end	O
of	O
the	O
array	O

I	O
was	O
using	O
unsigned	O
int	O
indices	O
to	O
speed	O
up	O
access	O
according	O
to	O
:	O
#URL	O

I've	O
tried	O
to	O
vectorise	O
it	O
using	O
numpy	O
but	O
I'm	O
not	O
really	O
sure	O
how	O
to	O
do	O
it	O
given	O
that	O
the	O
matrix	O
/	O
2D	O
array	O
gets	O
changed	O
on	O
each	O
iteration	O
.	O

Numpy	O
slicing	O
x	O
,	O
y	O
,	O
z	O
array	O
for	O
variable	O
z	O

I	O
would	O
like	O
to	O
convert	O
(	O
a	O
more	O
complicated	O
form	O
of	O
)	O
the	O
follwing	O
Matlab	O
code	O
#CODE	O

I	O
have	O
a	O
NumPy	O
array	O
'	O
boolarr	O
'	O
of	O
boolean	O
type	O
.	O

If	O
you	O
have	O
only	O
integers	O
that	O
are	O
between	O
0	O
and	O
n	O
(	O
if	O
not	O
its	O
no	O
problem	O
to	O
generalize	O
to	O
any	O
integer	O
range	O
unless	O
its	O
very	O
sparse	O
)	O
,	O
the	O
most	O
efficient	O
way	O
is	O
the	O
use	O
of	O
take	O
/	O
fancy	O
indexing	O
:	O
#CODE	O

Instead	O
of	O
2D	O
coordinates	O
,	O
I	O
use	O
index	O
for	O
every	O
elements	O
in	O
the	O
matrix	O
.	O

I	O
already	O
tried	O
converting	O
the	O
cols	O
to	O
int	O
but	O
that	O
didn't	O
solve	O
it	O
.	O

Although	O
I'm	O
sure	O
there	O
are	O
methods	O
for	O
applying	O
RK	O
to	O
an	O
equation	O
such	O
as	O
this	O
,	O
I	O
didn't	O
find	O
any	O
evidence	O
of	O
them	O
in	O
_Numerical	O
Recipes_	O
,	O
which	O
I	O
think	O
qualifies	O
that	O
topic	O
as	O
relatively	O
obscure	O
;-)	O

When	O
facing	O
a	O
big	O
computation	O
,	O
it	O
will	O
run	O
tests	O
using	O
several	O
implementations	O
to	O
find	O
out	O
which	O
is	O
the	O
fastest	O
one	O
on	O
our	O
computer	O
at	O
this	O
moment	O
.	O

Use	O
an	O
array	O
of	O
floating	O
point	O
numbers	O
instead	O
.	O

`	O
numpy	O
`	O
slicing	O
operations	O
probably	O
involve	O
`	O
for	O
`	O
loops	O
at	O
some	O
level	O
,	O
but	O
they're	O
implemented	O
in	O
c	O
,	O
and	O
provide	O
a	O
linear	O
time	O
solution	O
for	O
this	O
.	O

I	O
have	O
one	O
question	O
:	O
Is	O
there	O
only	O
one	O
way	O
to	O
do	O
addition	O
of	O
two	O
matrix	O
?	O

@USER	O
It	O
is	O
now	O
supported	O
,	O
at	O
least	O
in	O
my	O
version	O
(	O
1.7.1	O
)	O
.	O

I	O
know	O
I	O
could	O
start	O
a	O
number	O
of	O
times	O
at	O
random	O
locations	O
but	O
I'm	O
not	O
able	O
to	O
do	O
that	O
with	O
what	O
I	O
am	O
currently	O
working	O
on	O
and	O
have	O
to	O
use	O
on	O
of	O
these	O
minimisers	O
out	O
of	O
the	O
box	O
.	O

For	O
small	O
displacements	O
of	O
around	O
4-5	O
pixels	O
,	O
the	O
direction	O
of	O
vector	O
calculated	O
seems	O
to	O
be	O
fine	O
,	O
but	O
the	O
magnitude	O
of	O
the	O
vector	O
is	O
too	O
small	O
(	O
that's	O
why	O
I	O
had	O
to	O
multiply	O
u	O
,	O
v	O
by	O
3	O
before	O
plotting	O
them	O
)	O
.	O

However	O
,	O
I	O
will	O
need	O
to	O
access	O
all	O
waveforms	O
at	O
some	O
point	O
.	O

I've	O
find	O
this	O
:	O
#URL	O
but	O
when	O
I	O
try	O
to	O
install	O
this	O
I	O
get	O
an	O
error	O
:	O
#CODE	O

yes	O
,	O
I	O
can	O
assume	O
either	O
that	O
I	O
have	O
g	O
explicitly	O
or	O
that	O
I	O
can	O
sample	O
x	O
according	O
to	O
g	O
.	O

`	O
example	O
`	O
is	O
a	O
structured	O
array	O
consisting	O
of	O
two	O
elements	O
(	O
`	O
(	O
1	O
,	O
2	O
,	O
3	O
)`	O
and	O
`	O
(	O
4	O
,	O
5	O
,	O
6	O
)`)	O
,	O
each	O
element	O
(	O
or	O
'	O
record	O
')	O
having	O
3	O
fields	O
.	O

If	O
i	O
have	O
two	O
variables	O
-	O
where	O
they	O
either	O
are	O
a	O
1d	O
array	O
of	O
values	O
length	O
n	O
,	O
or	O
are	O
a	O
single	O
value	O
,	O
how	O
do	O
i	O
loop	O
through	O
them	O
so	O
that	O
I	O
get	O
n	O
values	O
returned	O
.	O

For	O
each	O
point	O
in	O
array	O
A	O
,	O
I	O
need	O
to	O
find	O
how	O
many	O
points	O
in	O
array	O
B	O
are	O
within	O
a	O
certain	O
distance	O
of	O
it	O
.	O

It	O
does	O
,	O
but	O
somehow	O
it	O
is	O
8	O
times	O
slower	O
than	O
copying	O
to	O
numpy	O
array	O
:(	O
I	O
suppose	O
the	O
regular	O
python	O
overhead	O
slows	O
things	O
down	O
much	O
more	O
than	O
a	O
copy	O
...	O

It	O
all	O
depends	O
on	O
its	O
dependencies	O
.	O

Is	O
there	O
a	O
way	O
to	O
make	O
an	O
array	O
of	O
such	O
strings	O
?	O

`	O
grid	O
[	O
1	O
]`	O
can	O
be	O
used	O
as	O
a	O
proxy	O
for	O
the	O
index	O
`	O
j	O
`	O
.	O

After	O
doing	O
so	O
,	O
I	O
discovered	O
that	O
if	O
I	O
tried	O
to	O
open	O
the	O
IPython	O
HTML	O
Notebook	O
I	O
got	O
the	O
error	O
message	O
:	O
#CODE	O

(	O
the	O
new	O
matrix	O
would	O
have	O
n-2	O
rows	O
m-2	O
columns	O
)	O
.	O

and	O
duplicate	O
index	O
values	O
at	O
the	O
correpsonding	O
sites	O
within	O

I	O
found	O
that	O
the	O
best	O
way	O
to	O
produce	O
small	O
pdf	O
files	O
is	O
to	O
save	O
as	O
eps	O
in	O
matplotlib	O
and	O
then	O
use	O
epstopdf	O
.	O

You	O
could	O
rearrange	O
the	O
image	O
to	O
put	O
the	O
(	O
0	O
,	O
0	O
)	O
in	O
the	O
middle	O
with	O
some	O
matrix	O
manipulation	O
.	O

Please	O
,	O
see	O
the	O
next	O
example	O
:	O

A	O
function	O
that	O
broadcasts	O
a	O
scalar	O
operation	O
over	O
an	O
array	O
is	O
called	O
a	O
universal	O
function	O
,	O
or	O
ufunc	O
.	O

may	O
not	O
exist	O
until	O
the	O
datasets	O
get	O
quite	O
big	O
(	O
maybe	O
you'll	O
need	O
at	O
least	O
10,000	O
rows	O
per	O
data	O
set	O
)	O
.	O

Magic	O
answers	O
like	O
this	O
are	O
not	O
really	O
helpful	O
because	O
they	O
don't	O
solve	O
the	O
problem	O
.	O

I	O
think	O
what	O
I	O
was	O
missing	O
is	O
that	O
I	O
really	O
have	O
a	O
3	O
dimensional	O
array	O
,	O
48x365x3	O
.	O

I	O
load	O
a	O
some	O
machine	O
learning	O
data	O
from	O
a	O
csv	O
file	O
.	O

So	O
I	O
have	O
it	O
running	O
(	O
or	O
at	O
least	O
that	O
assignment	O
isn't	O
throwing	O
an	O
error	O
and	O
it's	O
compiling	O
)	O
!	O

@USER	O
The	O
first	O
function	O
is	O
taking	O
chunks	O
of	O
200	O
items	O
from	O
your	O
huge	O
array	O
,	O
and	O
copying	O
those	O
chunks	O
to	O
a	O
new	O
,	O
even	O
more	O
ginormous	O
array	O
.	O

@USER	O
:	O
With	O
`	O
where	O
`	O
it	O
looks	O
definitely	O
nice	O
,	O
but	O
have	O
you	O
consider	O
also	O
the	O
implications	O
to	O
performance	O
when	O
implementing	O
with	O
`	O
where	O
`	O
?	O

Anyone	O
any	O
idea	O
what	O
this	O
means	O
?!	O

Assuming	O
you	O
are	O
using	O
g++	O
to	O
compile	O
...	O
have	O
you	O
had	O
different	O
results	O
in	O
any	O
way	O
when	O
experimenting	O
with	O
compiler	O
optimization	O
flags	O
?	O

With	O
the	O
overhead	O
of	O
the	O
data	O
structure	O
you	O
could	O
be	O
looking	O
at	O
usage	O
much	O
higher	O
than	O
that	O
--	O
I	O
can't	O
say	O
how	O
much	O
because	O
I	O
don't	O
know	O
the	O
memory	O
model	O
behind	O
SciPy	O
/	O
numpy	O
.	O

I	O
have	O
serious	O
doubt	O
that	O
adding	O
two	O
numpy	O
arrays	O
is	O
a	O
bottleneck	O
that	O
you	O
can	O
solve	O
rewriting	O
things	O
in	O
C	O
.	O

Where	O
exactly	O
is	O
the	O
error	O
occurring	O
?	O

I	O
frequently	O
convert	O
16-bit	O
grayscale	O
image	O
data	O
to	O
8-b	O
it	O
image	O
data	O
for	O
display	O
.	O

Reduce	O
it	O
to	O
a	O
1	O
/	O
10	O
resolution	O
,	O
find	O
the	O
one	O
white	O
pixel	O
,	O
and	O
then	O
you	O
have	O
a	O
precise	O
idea	O
of	O
where	O
to	O
search	O
for	O
the	O
centroid	O
.	O

I	O
ran	O
a	O
test	O
to	O
compare	O
the	O
times	O
,	O
and	O
found	O
that	O
my	O
method	O
is	O
faster	O
by	O
quite	O
a	O
bit	O
,	O
but	O
Freddie	O
Witherdon	O
'	O
s	O
suggestion	O
is	O
even	O
faster	O
.	O

I	O
couldn't	O
find	O
it	O
in	O
the	O
OLS	O
recipe	O
(	O
#URL	O
)	O
.	O

convert	O
binary	O
string	O
to	O
numpy	O
array	O

How	O
to	O
know	O
where	O
warning	O
come	O
from	O
in	O
Python	O
