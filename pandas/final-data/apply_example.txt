@USER Pennington : So I was able convert my ~ 1,000 line program easily to using pandas . great call ... any idea how I might get my compound return series created ? I was was thinking of using a lambda function in the apply method of DataMatrix but I'm having some challenges ... Thanks
I tested with apply , it seems that when there are many sub groups , it's very slow . the groups attribute of grouped is a dict , you can choice index directly from it : #CODE
Just as a small addition , you can also do an apply if you have a complex function that you apply to a single column :
probably x is a confusing name for the column name and the row variable , though I agree apply is easiest way to do it :)
just to add , ` apply ` can also be applied to multiple columns :
Can apply take in a function defined elsewhere in code ? this is so that we can introduce a more complicated function
How to apply slicing on pandas Series of strings
I'm playing with pandas and trying to apply string slicing on a Series of strings object .
` apply ` first tries to apply the function to the whole series . Only if that fails it maps the given function to each element . ` [: 2 ]` is a valid function on a series , ` + ' qwerty '` apparently isn't , that's why you do get the implicit mapping on the latter . If you always want to do the mapping you can use ` s.map ` .
` apply `' s source code for reference : #CODE
To clarify : The ` any ( 1 )` approach wouldn't work if you had other values in the table that you didn't want to filter . Suppose there are many columns and you only want the ` any ` to apply to a subset of them ( you know the subset's labels ) .
Then , using the ability to apply multiple aggregation functions following a groupby , you can say : #CODE
That's OK , although it has the problem that I don't even know the number of columns beforehand . I think I will continue converting the dataframe after loading with the apply method .
I see that Pandas does not allow duplicate time series indexes yet ( #URL ) , but will be added soon . I am wondering if there is a good way to apply rolling window means to a dataset with duplicate times by a multi-index tag / column
However mine uses the apply function of a dataframe instead of the aggregate .
and apply agg() with it : #CODE
I tried all manner of ` strftime ` methods on cdiff.DATE with no success . It wants to apply the to strings , not series object .
I am trying do use a pandas multiindex to select a partial slice at the top level index ( date ) , and apply a list to the second level index ( stock symbol ) . I.e. below I want the data for AAPL and MSFT in the range #URL
AttributeError : Cannot access callable attribute ' reset_index ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method
I think the same concepts apply to an index of floats . You just need to write your own method to group samples into a period group and time step within the group . Hope that helps
I don't think underlying mathematics apply that sum of interpolation equal to interpolation of sum . it only holds at special case
Is there a more performant and / or more idiomatic way to do this ? I know about apply , but sometimes it's more convenient to use a for loop . Thanks in advance .
Thanks . Is apply more efficient than iterrows ?
Returning multiple values from pandas apply on a DataFrame
Now , supposing I have " a " and " b " as one group , and " c " and " d " at the other , I'm performing the t-test row-wise . This is fairly trivial with pandas , using ` apply ` with axis=1 . However , I can either return a DataFrame of the same shape if my function doesn't aggregate , or a Series if it aggregates .
Why are you using ` apply ` in the first place ? Your result is a new ` DataFrame ` with a shape different from the input ( both rows and columns ) , therefore it's a completely new obj . You could just have ` t_test_and_mean ` accept your input dataframe ( and the columns to group by ) and return a 1-row-2-columns dataframe , without using ` apply ` .
Some of the tables I'm displaying would be much easier to read with a little bit of formatting . I'd really like something like " zebra tables " where every other row is shaded . I read here about how this formatting can be implemented via css . Is there a really straight forward way to apply a css to an IPython Notebook and then have tables rendered using the style sheet ?
If you just stick that in one of your markdown cells , then it will apply to everything on the page .
How to optimally apply a function on all items of a dataframe using inputs from another dataframe ?
I am new in python and I am currenlt struggly to do simple things with pandas . I would like to apply the same function to each item of a given dataset but using a time-dependent parameter .
I'm having a bit of trouble altering a duplicated pandas DataFrame and not having the edits apply to both the duplicate and the original DataFrame .
Then I assign the ' d ' dataframe to variable ' e ' and apply some arbitrary math to column ' a ' using apply : #CODE
The problem arises in that the apply function apparently applies to both the duplicate DataFrame ' e ' and original DataFrame ' d ' , which I cannot for the life of me figure out : #CODE
So , apply this function to each of those 3 columns : #CODE
You want to use the apply function and a lambda : #CODE
I don't believe you can avoid iteration 100% with what you are trying to do . You can possibly duplicate the ` quote ` column twice shifting it by one each direction and apply it to your dataset to create a pivot table based on entries where ` quote ` is ! = to ` quote_next ` and ` quote_prev ` .
I'm using the excellent ` pandas ` package to deal with a large amount of varied meteorological diagnostic data and I'm quickly running out of dimensions as I stitch the data together . Looking at the documentation , it may be that using the ` MultiIndex ` may solve my problem , but I'm not sure how to apply it to my situation - the documentation shows examples of creating MultiIndexes with random data and DataFrames , but not Series with pre-existing timeseries data .
I've run out of dimensions ( up to 3-D with a Panel ) and I'm also not able to use things like ` dropna ` to remove empty columns once everything is aligned in the Panel ( this has led to several bugs when plotting summary statistics ) . Reading about using pandas with higher-dimensional data has led to reading about the ` MultiIndex ` and its use . I've tried the examples given in the documentation , but I'm still a little unclear how to apply it to my situation . Any direction would be useful . I'd like to be able to :
Once I have the frame given by this routine , I can easily apply the various operations suggested below - of particular utility is being able to use the ` names ` field when I
How can I iterate and apply a function over a single level of a DataFrame with MultiIndex ?
This works for lists in general and I am familiar with it . How do I apply it to a pandas DataFrame ?
I would like to roll through my data by date and on each date take a time slice in the past apply a function to every time series so I get a result such as this where X is the output of the function of timeslice . #CODE
Also is there some other way to do the following.Using Apply function seems to be very slow for large dataset .
Similar to this R question , I'd like to apply a function to each item in a Series ( or each row in a DataFrame ) using Pandas , but want to use as an argument to this function the index or id of that row . As a trivial example , suppose one wants to create a list of tuples of the form [( index_i , value_i ) , ..., ( index_n , value_n )] . Using a simple Python for loop , I can do :
But there must be a more efficient way to do this ? Perhaps something more Panda-ish like Series.apply ? In reality , I'm not worried ( in this case ) about returning anything meaningful , but more for the efficiency of something like ' apply ' . Any ideas ?
If you use the apply method with a function what happens is that every item in the Series will be mapped with such a function . E.g. #CODE
A more complex usage of apply would be this one : #CODE
Following the OP's question for clarifications : Don't confuse Series ( 1D ) with DataFrames ( 2D ) #URL - as I don't really see how you can talk about rows . However you can include indices in your function by creating a new series ( apply wont give you any information about the current index ): #CODE
grouped pandas DataFrames : how do I apply scipy.stats.sem to them ?
I know that I can apply numpy methods by doing the following :
The trick here is to use the ` axis=1 ` option in the ` apply ` to pass elements to the lambda function row by row , as opposed to column by column .
Why map instead of apply ?
Also , you don't really need the lambda here , just feeding ` np.mean ` would work too , but I left the lambda in to illustrate how you would solve this when more general functions that you want to apply aren't working in their default ways . The ` .apply ` function is very powerful in Pandas .
I went ahead and did a tiny benchmark in IPython . First is for ` vtype ` above , then for the ` apply ` route . I repeated it a dozen or so times , and this example run is pretty typical on my machine .
I often need to apply a function to the groups of a very large ` DataFrame ` ( of mixed data types ) and would like to take advantage of multiple cores .
Thanks to the help of this forum i managed to solve a similar question using groupBy and the apply function but i would love to also use the cool resample function .
A combination of boolean indexing and apply can do the trick .
Quite neat . However , I think that you can get away with ` .max ( axis=1 )` instead of ` apply ( ... )` .
` max() ` is ok too of course , i think i got biased towards ` apply ` by the way you asked the question :-)
The problem in your code is that you want to apply the operation on every row . The way you've written it though takes the whole ' bar ' and ' foo ' columns , converts them to strings and gives you back one big string . You can write it like : #CODE
Most operations in ` pandas ` can be accomplished with operator chaining ( ` groupby ` , ` aggregate ` , ` apply ` , etc ) , but the only way I've found to filter rows is via normal bracket indexing #CODE
If you would like to apply all of the common boolean masks as well as a general purpose mask you can chuck the following in a file and then simply assign them all as follows : #CODE
and than apply it by passing the function and the args to ` agg ` : #CODE
Should I use a lambda with ` apply ` ? ( If so , how do I get a reference to the given column , as opposed to a whole row ) .
python pandas : apply a function with arguments to a series
I want to apply a function with arguments to a series in python pandas : #CODE
The documentation describes support for an apply method , but it doesn't accept any arguments . Is there a different method that accepts arguments ? Alternatively , am I missing a simple workaround ?
The documentation explains this clearly . The apply method accept a python function which should have a single parameter . If you want to pass more parameters you should use ` functools.partial ` as suggested by Joel Cornett in his comment .
For a DataFrame apply method accepts ` args ` argument , which is a tuple holding additional positional arguments or ** kwds for named ones . I created an issue to have this also for Series.apply() #URL
I want to apply a groupby operation that computes cap-weighted average return across everything , per each date in the " yearmonth " column .
This still requires me to save out the groupby computation , rather than having the assignment directly on the LHS on the line where I perform the groupby operation . Apply might be a bit better than the loop in my hack at the bottom of the question , but they are basically the same idea .
While I'm still exploring all of the incredibly smart ways that ` apply ` concatenates the pieces it's given , here's another way to add a new column in the parent after a groupby operation . #CODE
Is there an efficient way to apply this disaggregation map to get a new dataframe at a State level ?
There might be a slick vectorized way to do this , but I'd just apply the obvious per-entry function to the values and get on with my day : #CODE
The function to apply is like : #CODE
I don't suppose you have nny ideas on the second part , viz referencing neighbouring rows in the dataframe from within the map / apply function ?
The exact code will vary for each of the columns you want to do , but it's likely you'll want to use the ` map ` and ` apply ` functions . In some cases you can just compute using the existing columns directly , since the columns are Pandas Series objects , which also work as Numpy arrays , which automatically work element-wise for usual mathematical operations . #CODE
If you need to use operations like max and min within a row , you can use ` apply ` with ` axis=1 ` to apply any function you like to each row . Here's an example that computes ` min ( A , B ) -C ` , which seems to be like your " lower wick " : #CODE
For the second part , I would recommend introducing a column indicating the pattern for each row and writing a family of functions which deal with each pattern . Then groupby the pattern and apply the appropriate function to each group .
Note that a simple ` apply ` will not work here , since it won't know how to make sense of the possibly differently-sized result arrays for each group .
What problems are you running into with ` apply ` ? It works for this toy example here and the group lengths are different : #CODE
Python Pandas : How to broadcast an operation using apply without writing a secondary function
It seems logical to use the ` apply ` function for this , but it doesn't work like expected . It does not even seem to be consistent with other uses of ` apply ` . See below . #CODE
Based on this , it appears that ` apply ` does nothing but perform the NumPy equivalent of whatever is called inside . That is , ` apply ` seems to execute the same thing as ` arr + " cat "` in the first example . And if NumPy happens to broadcast that , then it will work . If not , then it won't .
But this seems to break from what ` apply ` promises in the docs . Below is the quotation for what pandas.Series.apply should expect :
Is there some way of using ` apply ` that I am missing here ?
and I verified that this version does work with Pandas ` apply ` . But this is beside the point . It would be easier to write something that operated externally on top of a Series object than to have to constantly write wrappers that use list comprehensions to effectively loop over the contents of the Series . Isn't this specifically what ` apply ` is supposed to abstract away from the user ?
and use this in ` apply ` : #CODE
This works , but I consider it a workaround as well , since it doesn't address the fact that ` apply ` isn't working as promised . Can you verify that ` map ` will work in all the same situations where ` apply ` will work ? I also don't like the inconsistency in going from ` map ` for a Series to ` applymap ` for a DataFrame .
That contradicts the docs for ` apply ` , as well as its 0.8.1 behavior , in which it successfully performs the elementwise version of my example above , whereas version 0.7.3 seems to use the logic you describe . Since ` apply ` should work in 0.7.3 as it does in 0.8.1 ( according to the docs ) , that's why I think it's a workaround . ` map ` is fine , but ` apply ` should work .
I'm on github master and it does not work ; it probably worked in 0.8.1 by accident . ` apply ` is designed so that you can apply a ufunc and get back a Series with the index intact . Take a look at the source code , it tries to call func ( self ) and wraps that in a try / except block and then calls map_infer in the except . In your example , the function you gave * can * take a Series and return a Series but doesn't do element-wise operations so the code cannot know to trigger the element-wise case . To be explicit that you want the input function to be applied element-wise , you have to use ` Series.map ` .
Though I do agree with you the docstring for apply is very unclear about this aspect . We can improve the documentation for apply .
In fact , by saying that ` apply ` can take any function that expect a * single * argument , it's not just unclear , but plain misleading . I'm glad you confirmed by hunch about that try-except block . So to be clear , we should use ` apply ` whenever we have a vectorized / ufunc already , and ` map ` when we literally want to apply an elementwise operation to a series ?
Yup , that's exactly right on ` apply ` vs ` map ` .
The inner syntax ` ( df ! =0 ) .any() ` doesn't work . A DataFrame object doesn't have the ` any ` function , at least not in 0.7.3 . You'd have to map that to the columns using ` map ` or ` apply ` or something .
How to groupby the first level index and apply function to the second index in Pandas
And I want to apply a function ` func ` ( exp : `' lambda x : x*10 '`) to ` second ` , somewhat like : #CODE
This way , the index column is not dropped and still accessible for your ` apply ` .
PS : but if you really just want the last column , ` apply ` would suffice : #CODE
( I think it can be some problem with ` lambda ` When I want to apply my function to the column I have an error : ` TypeError : only length-1 arrays can be converted to Python scalars `)
On top of a dodgy converter , i think you apply the converter to the wrong column ( look at the exception you get ) .
You can use ` apply ` for this , and it's a bit neater : #CODE
If you want that done on every row in the dataframe , you can use apply ( with axis=1 to select rows instead of columns ): #CODE
At the moment for conversion I use as below , but need remove unwanted rows first to apply it to all df . #CODE
is there an existing built-in way to apply two different aggregating functions to the same column , without having to call ` agg ` multiple times ?
N / M I didn't see the extra call to ` returns ` in there . So this is the Series version of aggregate ? I'm looking to do the DataFrame version of aggregate , and I want to apply several different aggregations to each column all at once .
An alternative slightly more flexible way , might be to use ` apply ` ( or equivalently ` map ` ) to do this : #CODE
First , I think you have to either specify named parameters or use ` args ` to pass additional arguments to ` apply ` . Your second argument is probably being interpreted as an axis . But if you use #CODE
because ` apply ` doesn't act elementwise , it acts on entire Series objects . Other approaches include using ` applymap ` or boolean indexing , i.e. #CODE
One way to do this is to use apply : #CODE
If you want to change the values in only one column you can still use ` apply ` : #CODE
Note : since ` my_fun2 ` returns a single value , this time ` apply ` return a Series , so we need to slightly change the way we apply apply .
Looks like this is going to work . Thanks for your help ! In general , though , is there a prefered approach to Split-Apply-Combine where Apply returns a dataframe of arbitrary size ( but consistent for all chunks ) , and Combine just vstacks the returned DFs ?
Pandas DataFrame : apply function to all columns
Is there a more pythonic way to apply a function to all columns or the entire frame ( without a loop ) ?
I have found a workaround which is listed at the end of this post , but its not at all ' panda-style ' and prone to errors . The apply or transform function on a group seems like the right way to go but after hours of trying i still do not succeed . I figured the correct way should be something like : #CODE
Having the apply / transform mechanism be able to output structured values and those broadcast into colums ( i.e. if a tuple is produced by the applied function , the components go in separate columns instead of the tuple becoming an atomic element in a single column ) would be a fantastic feature , even if it is only syntactic sugar . Probably with another method name , to make intent clear ( applyfork or something like that , or a keyword splitseq=True in apply ) .
It is important to say that I already have a function that returns a distance between two points ( two coordinate pairs ) , but I don't know how to apply it with a single array operation instead of looping through row pairs .
pandas : apply function to DataFrame that can return multiple rows
One possibility might be to allow ` DataFrame.applymap ` function return multiple rows ( akin ` apply ` method of ` GroupBy `) . However , I do not think it is possible in pandas now .
Thanks for reporting . Is there any work around I could apply before it is fixed ?
I notice Pandas can apply different function to different column by passing a dict . But I have a long column list and just want parameters to set or tip to simply tell Pandas to bypass some columns and apply ` my_func() ` to rest of columns ? ( Otherwise I have to build a long dict )
One simple ( and general ) approach is to create a view of the dataframe with the subset you are interested in ( or , stated for your case , a view with all columns except the ones you want to ignore ) , and then use APPLY for that view . #CODE
Apply your function to that view . ( Note this doesn't yet change anything in df . ) #CODE
Starting with row number 2 , or in this case , I guess it's 250 ( PS - is that the index ? ) , I want to calculate the difference between 2011-01-03 and 2011-01-04 , for every entry in this dataframe . I believe the appropriate way is to write a function that takes the current row , then figures out the previous row , and calculates the difference between them , the use the ` pandas ` ` apply ` function to update the dataframe with the value .
How to apply condition on level of pandas.multiindex ?
I.e. , I would like to apply np.mean over all counts of the detectors of 1 channel at each time separately .
Thank you @ root , that's very helpful ! As a follow-up question , how would you go about to apply the same function on groups ? ( See updated question ) . Thanks again !
It is only applied to a time series , so you would have to apply ` reset_index ` to your ` DataFrame `
This can be accomplished quite simply with the DataFrame method ` apply ` . #CODE
Now that we have our ` DataFrame ` and ` Series ` we need a function to pass to ` apply ` . #CODE
` df.apply ` acts column-wise by default , but it can can also act row-wise by passing ` axis=1 ` as an argument to ` apply ` . #CODE
This could be done more concisely by defining the anonymous function inside ` apply ` #CODE
I know no easy solution to get to align to the closest and I find the current version quite logical . But with ` label= ' left '` you can achieve what you want with the current data , still it doesn't align to the closest , so overall you probably have to figure out something else ( like using apply to change the dates so they would conform as you wish ) . #CODE
Since resample() requires a TimeSeries-indexed frame / series , setting the index during creation eliminates the need to set the index for each group individually . GroupBy objects also have an apply method , which is basically syntactic sugar around the " combine " step done with pd.concat() above . #CODE
I would like to apply a function to a dataframe and receive a single dictionary as a result . pandas.apply gives me a Series of dicts , and so currently I have to combine keys from each . I'll use an example to illustrate .
However , my goal is to be able to use a row-wise function in the ` DataFrame.apply() ` method ( so I can apply the desired functionality to other functions I build ) . I've tried : #CODE
Row-wise functionality should be possible with apply . For example , ` df.apply ( lambda x : sum ( x**2 ) , axis = 1 )`
The " problem " is that the chaining breaks the fillna ability to update the original dataframe . I put " problem " in quotes because there are good reasons for the design decisions that led to not interpreting through these chains in certain situations . Also , this is a complex example ( though I really ran into it ) , but the same may apply to fewer levels of indexes depending on how you slice .
It's one line , reads reasonably well ( sort of ) and eliminates any unnecessary messing with intermediate variables or loops while allowing you to apply fillna to any multi-level slice you like !
How to apply a function to two columns of Pandas dataframe
Now I want to apply the ` f ` to ` df `' s two columns `' col_1 ' , ' col_2 '` to element-wise calculate a new column `' col_3 '` , somewhat like : #CODE
can you apply f directly to columns : df [ ' col_3 '] = f ( df [ ' col_1 '] , df [ ' col_2 '])
Here's an example using ` apply ` on the dataframe , which I am calling with ` axis = 1 ` .
Depending on your use case , it is sometimes helpful to create a pandas ` group ` object , and then use ` apply ` on the group .
Yes , i tried to use apply , but can't find the valid syntax expression . And if each row of df is unique , still use groupby ?
i provide a detail sample in question . How to use Pandas ' apply ' function to create ' col_3 ' ?
Use apply on the whole dataframe , passing in rows with df.apply ( f , axis=1 ) . Then rewrite your function ` get_sublist ( x )` to index the col values like this ` start_idx = x [ 1 ] , end_idx = x [ 2 ]` .
Pandas rolling apply with missing data
I think a partial answer to this question is probably via using the keyword argument min_periods in the rolling apply function . Ex : pandas.rolling_apply ( x2 , 3 , foo , min_periods=1 ) helps .
You can use groupby and then apply to achieve what you want : #CODE
I've already explored Panda's fillna , but it doesn't seem to meet my needs . I've also considered the np.where method , but I'm not sure how'd it work in this situation . I'm pretty new to Pandas , but maybe the map / apply function are what I need ? This can probably be accomplished a thousand different ways , but looking for something that won't crawl given the size of the data .
The paired dict has the city as the key and the borough as the value . Now the last step is to apply / map it back to the borough column ... how do I do that ?
I'm also puzzled why the ` apply ` version along ` axis=1 ` is so much slower . It should literally be just a shortening of the syntax , no ?
Currently I think you need to create a custom subclass . You'd need to override the ` apply ` and ` onOffset ` methods to take into account your holiday calendar .
Update : A useful workaround is to just smash this with the DatetimeIndex constructor ( which is usually much faster than an apply ) , for example : #CODE
With more complicated selections like this one you can use ` apply ` : #CODE
problems with apply function in pandas after update
I tried to apply ' manually ' the function recursively to see if some of the dates passed as the x parameter in the lambda definition where wrong , but managed to get correct results any time . But the ` apply ` method just seem not to work anymore , and cannot understand why .
You need to use ` | ` instead of ` or ` . The ` and ` and ` or ` operators are special in Python and don't interact well with things like numpy and pandas that try to apply to them elementwise across a collection . So for these contexts , they've redefined the " bitwise " operators ` ` and ` | ` to mean " and " and " or " .
Actually , many of DataFrameGroupBy object methods such as ( apply , transform , aggregate , head , first , last ) return a DataFrame object . I used the method ` filter ` in [ one ] ( #URL ) of my blog posts .
Define the function you want to apply . #CODE
Then , apply it . #CODE
Efficient way to apply multiple filters to pandas DataFrame or Series
I have a scenario where a user wants to apply several filters to a Pandas DataFrame or Series object . Essentially , I want to efficiently chain a bunch of filtering ( comparison operations ) together that are specified at run-time by the user .
I want to take a dictionary of the following form and apply each operation to a given Series object and return a ' filtered ' Series object . #CODE
Your right , boolean is more efficient since it doesn't make a copy of the data . However , my scenario is a bit more tricky than your example . The input I receive is a dictionary defining what filters to apply . My example could do something like ` df [( ge ( df [ ' col1 '] , 1 ) & le ( df [ ' col1 '] , 1 )]` . The issue for me really is the dictionary with the filters could contain lots of operators and chaining them together is cumbersome . Maybe I could add each intermediate boolean array to a big array and then just use ` map ` to apply the ` and ` operator to them ?
Similar to @USER suggestion , you can apply ` difflib ` ' s ` get_closest_matches ` to ` df2 `' s index and then apply a ` join ` : #CODE
If these were columns , in the same vein you could apply to the column then ` merge ` : #CODE
As a heads up , this basically works , except if no match is found , or if you have NaNs in either column . Instead of directly applying ` get_close_matches ` , I found it easier to apply the following function . The choice of NaN replacements will depend a lot on your dataset . #CODE
I used this and DataFrame.apply to apply it to all major columns in the dataframe . After thinking about it a bit more , I think this is the intended design , and it perfectly accomplishes the goal .
You can either load the file and then filter using ` df [ df [ ' field '] constant ]` , or if you have a very large file and you are worried about memory running out , then use an iterator and apply the filter as you concatenate chunks of your file e.g. : #CODE
Think of np.datetime64 the same way you would about np.int8 , np.int16 , etc and apply the same methods to convert beetween Python objects such as int , datetime and corresponding numpy objects .
In this case they're equivalent . Apply can also do aggregation and other things
You are looking for ` apply ` ( ` merge ` is like a database join . ): #CODE
Update : if you're doing this to a DatetimeIndex / datetime64 column a better way is to use ` np.round ` directly rather than via an apply / map : #CODE
Hence you can apply this to the entire index : #CODE
@USER you're right of course ! I forgot about milli-seconds ... whoops ! I have corrected this and added how to apply this to the entire dt_index .
The ` for ` loops and ` append ` s will not be efficient and should be avoided . Try rewrting these using numpy functions and / or the DataFrame ` apply ` method ...
Also , would you agree then , using your suggestion , if we want to apply a function / algorithm restricted every unique date in the file one should just groupby the ' datetime ' object ?
When you do ` len ( df [ ' column name '])` you are just getting one number , namely the number of rows in the DataFrame ( i.e. , the length of the column itself ) . If you want to apply ` len ` to each element in the column , use ` df [ ' column name '] .map ( len )` . So try #CODE
` transform ` is not that well documented , but it seems that the way it works is that what the transform function is passed is not the entire group as a dataframe , but a single column of a single group . I don't think it's really meant for what you're trying to do , and your solution with ` apply ` is fine .
So basically , you don't need to use transform here . ` apply ` is the appropriate function here , because ` apply ` really does operate on each group as a single DataFrame , while ` transform ` operates on each column of each group .
If it's already in the DataFrame you could use ` apply ` to convert those strings which are numbers into integers ( using ` str.isdigit ` ): #CODE
@USER uncommented , and commented the ` to_dict() ` . Although , ` apply ` is the important bit of my answer ( weirdly no other answers seem to use it ) .
Many thanks ! I was under the impression that apply was preferable to loops . Looking at the source , I'm not that sure , as it is just regular python with more functionality than the one I need . Moreover , I assumed that Pandas indexes provided enough performance . A dict is just perfect -- I just have to find the way to put the data in the files as a dict instead than as df , since originally it was precisely a dict --
apply on group replicating complete MultiIndex
I would like to fill gaps in a column in my DataFrame using a cubic spline . If I were to export to a list then I could use the numpy's ` interp1d ` function and apply this to the missing values .
OK thanks for your help ! and last thing I hope . I have multiple columns each containing NaN data . Sol the df.dropna() drops too many rows . How do you apply that to one column only ( i.e. ' data1 ')
Apply function on Pandas dataframe
I'm a newbie to pandas dataframe , and I wanted to apply a function to each column so that it computes for each element x , x / max of column .
Pandas DataFrame : apply function to all columns
How to apply function to date indexed DataFrame
Then ` apply ` this to each state in the DataFrame : #CODE
The real issue is -- and now comes a threefold question : how can it be that just importing pandas broke matplotlib's ability to handle datetime objects , when just two lines earlier pandas was clearly not even involved in that same operation ? Does pandas upon import silently alter other modules in the top level namespace to force them to make use of pandas methods ? And is this acceptable behavour for a python module ? Because I need to be able to rely on it that importing , say , a random number module , won't silently change , say , the pickle module to apply a random salt to everything it writes ..
I have a time series object ` grouped ` of the type ` pandas.core.groupby.SeriesGroupBy object at 0x03F1A9F0 ` . ` grouped.sum() ` gives the desired result but I cannot get rolling_sum to work with the ` groupby ` object . Is there any way to apply rolling functions to ` groupby ` objects ? For example : #CODE
I'm not sure of the mechanics , but this works . Note , the returned value is just an ndarray . I think you could apply any cumulative or " rolling " function in this manner and it should have the same result .
Pass the ` axis ` option to the ` apply ` function : #CODE
Great . Does apply pass the columns including item1 , item2 when I use axis=0 ? What happens when there is a hierarchical indexing in the columns and the rows ?
You can use the DataFrame ` apply ` method : #CODE
You may find it faster to extract the index as a column and use ` apply ` and ` bfill ` .
Here's one ( slow ! ) workaround to do it using ` apply ` , not ideal but it works : #CODE
It seems like a bug ( that you can't do ` apply ( lambda x : x.month )`) , perhaps worth adding as an issue on github . As Wes would say : " welcome to hell " .
This happens when using apply as well #CODE
How to apply quantile to pandas groupby object ?
I ran a formula on the price col of the csv file . Indeed , they are all " numbers " . apply ( float ) for some reason was rejected w / ValueError : could not convert string to float : price .
` applymap() ` can be used to apply a function to every element of a ` dataframe ` #CODE
Have you tested the ` calcvol ` function separately ? It's good practice to do that first before you apply . ( I don't suppose this is a simple as a forgotten axis argument : ` optionsData.apply ( calcvol , axis=1 )` ?
You could create a function which takes an entry in ` df.D ` columns and returns a Series . Then you can use Series ` apply ` with this function : #CODE
Subset the ` dataframe ` to only those records with the desired Status . ` Groupby ` the ID and apply the lambda function ` diff() .sum() ` to each group . Use ` transform ` instead of ` apply ` because ` transform ` returns an indexed series which you can use to assign to a new column ' diff ' .
The modeling process requires that I analyze every column , look for interesting relationships with some outcome variable , and create new compound columns that describe those relationships . The columns that I explore are usually done in small sets . For example , I will focus on a set of say 20 columns just dealing with property values and observe how they relate to defaulting on a loan . Once those are explored and new columns are created , I then move on to another group of columns , say college education , and repeat the process . What I'm doing is creating candidate variables that explain the relationship between my data and some outcome . At the very end of this process , I apply some learning techniques that create an equation out of those compound columns .
You say that the best way is to plot each condition ( like ` subset_a ` , ` subset_b `) separately . What if you have many conditions , e.g. you want to split up the scatters into 4 types of points or even more , plotting each in different shape / color . How can you elegantly apply condition a , b , c , etc . and make sure you then plot " the rest " ( things not in any of these conditions ) as the last step ?
" AssertionError when using apply after GroupBy " . It's since been fixed .
Is there any workaround on 0.10 ? In some cases I can get ` apply ` working after ` groupby ` and in other cases not .
Now groupby both columns and apply the lambda function : #CODE
I actually think it won't always make sense to apply ` reshape ` to a Series ( do you ignore the index ? ) , and that you're correct in thinking it's just numpy's reshape :
You can groupby the index and apply a function that returns one value per index group . Here , I take the first value : #CODE
In SQL , this is standard set logic , accomplished differently depending on the dialect , but a standard function . How do I elegantly apply this in Pandas ? I would love to input some code , but nothing I have is even remotely correct . It's a situation in which I don't know what I don't know ..... Pandas has set logic for intersection and union , but nothing for disjoint .
The ` Target ` is just a constant , so instead of trying to find the root for ` f ( x ) = 0 ` , you'd define ` g ( x ) = f ( x ) - Target ` and apply ` newton ` to ` g ` .
EDIT : Thanks for the two responses , I can reproduce those with no problem . However when I use the apply function to my case I get an ' unhashable type ' error . #CODE
You are just printing these and not ` apply ` -ing them to the DataFrame , here's one way to do it :
If I understand you right , you're looking for the ` apply ` method : #CODE
apply a function to a pandas Dataframe whose retuned value is based on other rows
I want to apply the same process to the whole quantity column . I don't know how to approach this problem with the pandas library other than looping through the Dataframe row by row .
Here , we groupby ` [ ' item ' , ' price ']` and apply the function above . The output is a series of relative weights for the unique combinations of item and price . #CODE
After building basic class with ` __str__ ` and plotData() methods I would like to apply some filters and build a new class where additional column is the filter . I would like to do that in ` __init__ ` but keep everything what already was done . In another words I don't want to re-write the whole ` __init__ ` only want to add new column to the basic dataframe .
However , when stored ( and retrieved ) dates are ` unicode ` rather than ` Timestamp ` . To convert back to what we started with we could ` apply ` ` Timestamp ` to the column and ` set_index ` : #CODE
I try to apply exactly the same logic to my original problem with large dataframe inside a class . The code is : #CODE
I found in here that there could be a problem with type of the columns but Depth is type ` numpy.float64 ` Hper is type ` float ` Vper is type ` float ` so I understand how it can apply to my problem .
Construct the index as desired and apply it to the dataframe
Now create the desired index and apply it . Here are several approaches for the index . #CODE
Apply multiple functions to multiple groupby columns
The docs show how to apply multiple functions on a groupby object at a time using a dict with the output column names as the keys : #CODE
What I want to do is apply multiple functions to several columns ( but certain columns will be operated on multiple times ) . Also , some functions will depend on other columns in the groupby object ( like sumif functions ) . My current solution is to go column by column , and doing something like the code above , using lambdas for functions that depend on other rows . But this is taking a long time , ( I think it takes a long time to iterate through a groupby object ) . I'll have to change it so that I iterate through the whole groupby object in a single run , but I'm wondering if there's a built in way in pandas to do this somewhat cleanly .
@USER -- My first thought was also that it worked the same , but I think DataFrame tries to apply it to the columns ( without the ix ) .
I need to apply some function for every columns and create new columns in this DataFrame with special name . #CODE
I would skip the ` apply ` method and just define the columns directly . #CODE
Not as elegant as DSM's solution . But for whatever reason I avoid ` apply ` unless I really need it .
add column with time rounded to millisec and groupby it , apply cumsum within each group
Apply a lambda function that indexes the current time from the ts series . The function returns the sum of all ts entries between ` x - ms and x ` . #CODE
But it seems inefficient to compute cumsum on each call . Is there a way to first compute the cumsums and then apply ' ohcl ' to the data ? #CODE
Here I create a dictionary of dictionaries . The outer key references the columns you want to apply the functions to . The inner key contains the names of your aggregation functions and the inner values are the functions you want to apply : #CODE
@USER You can probably create a two-level index ` [ ' date ' , ' time ']` and then apply time filtering for the second level , but that is beyond my current level of pandas-fu now .
Then we apply this to a slice of the ` _rt ` columns : #CODE
All the values which we're using are within 3 standard deviations , so this cut isn't very interesting , but we can apply it anyhow : #CODE
I'm new to pandas ( and python ) and have been slowly working my way trying to apply things learned to my own datasets .
How to apply conditional logic to a Pandas DataFrame .
I could apply a loop and do re-construct the DataFrame ... but that would be ' un-pythonic '
You want to apply a function that conditionally returns a value based on the selected dataframe column . #CODE
I thought it was from the calculation . But if I apply ( np.float64 ) it changes the numbers to what I need . Thanks .
And ` apply ` it ( row-wise ): #CODE
but now , I need to apply this ( multiparameters ) function along 0-axis .
Then use ` apply ` across each row , to replace each NaN with its groups mean : #CODE
How to apply linregress in Pandas bygroup
I would like to apply a scipy.stats.linregress within Pandas ByGroup . I had looked through the documentation but all I could see was how to apply something to a single column like #CODE
But how do I apply a linregress which has TWO inputs X and Y ?
and if using a groupby you can similarly ` apply ` ( to each group ): #CODE
Thanks Andy , Yes it can accept it . The question is how to do it BYGROUP . For example I have datetime that I have GROUPED into Year and month . I want to do the linear regression for each of the groups then return the values from the lin regression . Also I have a DataFram so how can I apply that using two columns in the DF ? Thanks Jason
but when i apply a function from scikit-learn i loose the informations about columns : #CODE
Is there a way to apply scikit or numpy function to DataFrames without loosing the information ?
How to Apply an equation to a Pandas dataframe ByGroup
I have been reading all day by cant find and exact solution . Now my question is how do apply this back to the original data frame . I would like a new column in the DF that applies the linear regression y=mx+c to each line in the original data using the column 3 as the input BUT to do that using the specific coefficients ( slope , intercept ) that are different for each YEAR and MONTH . Any ideas most welcomed :)
So I would like to apply that relationship back to all the Year=2010 and Month=1 to look like this . Then for the rest of the DF apply the same approach for each month of each year . #CODE
I don't think it's entirely clear ( to me ) what you are asking , perhaps it would help to provide an example DF and what you want it to be ? Perhaps you want an ` apply ` which refers to ` Corr_grouped ` ( ? )
Okat , I think I got it . Instead of resampling , you can ` groupby ` where the group is a unit of time . To this group you can apply a function of your choice , for example your directionAverage function .
Apply a lambda testing for the conditions you want to drop : #CODE
You can use ` applymap ` to apply your function to the elements of the ` DataFrame ` : #CODE
` applymap ` was the key to getting the ` re.match ` function to work for me . It failed when using just ` apply `
One of my favorite aspects of using the ` ggplot2 ` library in R is the ability to easily specify aesthetics . I can quickly make a scatterplot and apply color associated with a specific column and I would love to be able to do this with python / pandas / matplotlib . I'm wondering if there are there any convenience functions that people use to map colors to values using pandas dataframes and Matplotlib ? #CODE
I wondering if I can apply the ` pandas.ols ` model to a data frame of multiple response variables against one independent variable at one time .
@USER whoops typo :) . Yes , it does seem silly using a dummy ( tbh I could've been more clever with my apply [ 12 ] to do it in one , and it may well be more efficient , but I decided I wouldn't like to be the person reading it ... ) . Like I say , I think there is a clever way to do this kind of comlex sort : s
I want to replace df.ix [ 0 ] [ ' date2 '] with df.ix [ 1 ] [ ' date2 '] for each symbol -- the symbol changes through the dataframe so I can't just apply this through the whole dataframe .
plus 1 for using .apply() where my solution used a for loop . I always forget about apply .
1 E.g. , any approach I can think of involving the ` filter ` built-in is probably ineffiencient , since it would apply the criterion ( some lambda function ) by iterating , " in Python " , over the panda ( or numpy ) object ...
How to apply " first " and " last " functions to columns while using group by in pandas ?
In some sense there's three types of mapping here : aggregation , apply and filter ( the above is kind of a filter , although it uses the agg verb ) . This is complicated thing is that you can use ** either ** agg or apply to get the ` .iloc [ 0 ]` job done , not sure why I used agg , apply is probably a better description . Since this post I fixed nth to work better so IMO that's the preferred solution here .
In the above illustration the result of the ` apply() ` function is a Pandas Series . And it lacks the groupby columns from the ` df.groupby ` . The essence of what I'm struggling with is how do I create a function which I apply to a groupby which returns both the result of the function AND the columns on which it was grouped ?
In the example you've appended , what's the purpose of the groupby ( it'll just find dupes ) , you can just do an apply to df itself and add that as a column : ` df [ ' func3 '] = df.apply ( lambda row : row [ ' col2 '] ** 2 , axis=1 )` . ?
I don't can't see an example where it makes sense to groupby all columns and apply , rather than just apply ( DataFrames apply can be very non-trivial and save to multiple columns ) . ( Also you don't need to create a dfout return variable , you can just return the calculation e.g. ` return df [ ' col3 '] **2 ` :) )
example updated ... and now it works ! Geesh . It appears that when the apply is on every row it does not return the keys , but if the apply results in aggregation it does return the keys
The best way to understand how your apply is going to work is to inspect each action group-wise : #CODE
I am interested in this question in the context of the ` groupby ` operation . If we apply this operation to a data frame , as a result we do not get another data frame . I wonder why not . Why not to have another data frame that has lists as values for some cells ?
The index option has a format method that lets you apply a formatter in the form of a function : #CODE
how to apply functions to grouped dataframes in Python pandas ?
I would like to apply a function per group that does something specific with a subset of the columns in ` grouped_iris ` . How could I apply a function that for each group ( each value of ` Name `) sums ` PetalLength ` and ` PetalWidth ` and puts it in a new column called ` SumLengthWidth ` ? I know that I can sum all the columns per group with ` agg ` like this : #CODE
and apply set intersection : #CODE
Apply a function that returns the group row that has the index of the minimum ' q ' value .
I have written a function ( below ) that works with ` apply ` to perform this , but it is unacceptably slow . Instead , is there a way to use ` pandas.ols ` to directly perform this sort of cumulative regression ?
Here is the function I am able to use with ` apply ` on the identifier-grouped object : #CODE
pandas does offer cumsum ( cumulative sum ) and cumprod ( cumulative product ) that you could apply to a series . If you can break down you reduce your function into products and sums you could achieve what you are trying to do ...
Following on the advice in the comments , I created my own function that can be used with ` apply ` and which relies on ` cumsum ` to accumulate all the individual needed terms for expressing the coefficient from an OLS univariate regression vectorially . #CODE
Is it possible to apply user defined functions to series in pandas ?
If we have two series ` s1 ` and ` s2 ` we can apply arithmetic operations to them : ` s1 + s2 ` or ` s1*s2 ` . The arithmetic operation will be applied pairwise ( assuming that the two series have the same length ) as a result we get a new series . This feature makes a lot of things much more easier .
Now , I try to define my own operator and apply it to two series : #CODE
And I try to apply it to two series : ` f ( s1 , s2 )` . It does not work . It is expectable , to a certain extent , since the user-defined function doers not know how to treat series . So , my question is if there is an elegant way to do what I want to do ?
Apply function to each row of pandas dataframe to create two new columns
I want to create two new columns for this dataframe based on applying a function to each row of the dataframe . I don't want to have to call the function multiple times ( eg . by doing two separate ` apply ` calls ) as it is rather computationally intensive . I have tried doing this in two ways , and neither of them work :
Using ` apply ` :
Trying to apply this to the DataFrame gives an error : #CODE
I was then going to assign the values returned from ` apply ` to two new columns using the method shown in this question . However , I can't even get to this point ! This all works fine if I just return one value .
To make the first approach work , try returning a Series instead of a tuple ( apply is throwing an exception because it doesn't know how to glue the rows back together as the number of columns doesn't match the original frame ) . #CODE
The solution to the second approach works - thanks :-) . However , I can't get the first approach to work . Returning a series works , and I get a ' mini-df ' returned , but I can't seem to get the values returned from the ` apply ` function into the original dataframe . Using ` st [ ' a '] , st [ ' b '] = st.apply ( calculate , axis=1 )` doesn't work , and neither does wrapping the right-hand side in ` zip ( * )` . Any ideas about what I'm doing wrong here ?
Apply pandas function which returns multiple values ?
The general idea now is to apply these calibration data to the measurements .
Pass ` numpy.argsort ` to the ` apply ` method instead of using it directly . This way , NaNs / NaTs persist . For your example : #CODE
@USER , The originals are datetime64 [ ns ] and the return index after the apply are objects . I ma using version 0.11.0.dev-3790f16 .
To complement unutbu's answer , here's an approach using ` apply ` on the groupby object . #CODE
Apply different functions to different items in group object : Python pandas
I want to group a duplicate data at time ` 14:42 : 10 ` and apply different functions to ` exe_price ` and ` exe_vol ` ( e.g. , sum the ` exe_vol ` and compute volume weighted average of ` exe_price `) . I know that I can do #CODE
Is there a way to group and then apply different ( written by me ) functions to values in different column ?
Thank you for your quick response . I wonder since my ' grouped ' is now a panda DataFrameGroupBy object , I cannot really apply your fucntion directly can I ?
Apply your own function : #CODE
I was surprised to see that there was no " rolling " function built into pandas for this , but I was hoping somebody could help with a function that I can then apply to the df [ ' Alpha '] column using pd.rolling_apply .
We can apply column operations and get boolean Series objects : #CODE
Alternative , you could use ` apply ` . ` apply `' s callable is passed a sub-DataFrame which gives you access to all the columns : #CODE
Using ` idxmax ` and ` loc ` is typically faster than ` apply ` , especially for large DataFrames . Using IPython's %timeit : #CODE
Due to the very large nature of the problem , I'm using the pandas as the main Database API as its very easy to apply function to column .
Apply function to pandas groupby
This code throws an error , ' DataFrame object has no attribute ' size ' . How can I apply a function to calculate this in Pandas ?
` apply ` takes a function to apply to each value , not the series , and accepts kwargs .
I have a pandas DataFrame that includes a pipe-separated string in one of the fields . I've split this into a list inside an ` apply ` and added it to the DataFrame . The number and content of the values in the pipe-separated string vary . #CODE
But A ) I'm not sure how to apply the conditional logic and B ) I have to apply the logic to each column iteratively rather than to the dataframe as a whole .
How can I apply conditional logic to the non-null values of a dataframe , preserving the nullity of the other fields ?
There isn't , but if you want to only apply to unique values , just do that yourself . Get ` mySeries.unique() ` , then use your function to pre-calculate the mapped alternatives for those unique values and create a dictionary with the resulting mappings . Then use pandas ` map ` with the dictionary . This should be about as fast as you can expect .
In this case , how can we execute the groupby on values of A , then apply this computation to each individual group , and finally plot the D values for the two groups ?
Rolling apply question
For each group in the groupby object , we will want to apply a function : #CODE
We want to take the Times column , and for each time , apply a function . That's done with ` applymap ` : #CODE
#URL will provide an error for your code here ( because you are trying to min_itemsize with a column that is not queryable ) . Also , you can use lib.max_len_string_array ( s.values ) for a quick max in your apply ( faster and you don't need to test for object type )
I don't understand what is your question . Also , what is that ` apply ` method ?
and the " reshape " feature ( does not apply to 3-d case though ... ): #CODE
How to apply custom column order to boxplot ?
How can I apply my custom column order to the boxplot columns ? ( other than ugly kludging the column names with a prefix to force ordering )
not where I was headed . I typically just use ` apply ` with a hard-coded lookup table . see my edited response for a different approach , though .
Then apply the groupby function and add a column City : #CODE
I have finally decided to use apply which I understand is more flexible .
I finally decided to use apply .
apply is more flexible than agg and transform because you can define your own function .
The ` rename ` function should convert the the dictionary to a mapper and apply it to each index . However , for the ` MultiIndex ` case , it only walk through each tuple but not each index .
Should I apply the ` df.sortlevel ( level= ' Transition ')` after each ` df.set_value() ` call ?
Group by k1 , select column k2 and apply a lambda function . The lambda gets frequency counts for each level of k2 within k1 and then we divide by the count of k1 : #CODE
First using ` apply ` you could add a column with the signed shares ( positive for Buy negative for Sell ): #CODE
Note that ` numpy.rollaxis ` brings the specified axis to the first dimension and then let's us iterate over arrays with the remaining dimensions , i.e. , if we want to shuffle along the first dimension ( columns ) , we need to roll the second dimension to the front , so that we apply the shuffling to views over the first dimension .
Still not sure what you're asking , but if you have a function for one value , you can then use Series / DataFrame ` apply ` or index's ` map ` , e.g. ` df.index.map ( lambda t : t.value )` ?
You can also use the standard time functions yearmon() or yearqtr() , or custom functions for both split and apply . This method is as syntactically sweet as that of pandas .
can u give a small example of what kind of functions u r going to apply with the group ? and a small example frame would be helpful .
It goes a bit against Pandas ' philosophy , which seems to see ` Series ` as a one-dimensional data structure . Therefore you have to create the ` Series ` by hand , tell them that they have data type `" object "` . This means don't apply any automatic data conversions .
I'm not sure there's a vectorized hook , but you can use ` apply ` , anyhow : #CODE
In essence you are flattening the blink frame to a series that you then can apply to each of the trial
Group them and then apply our customized function
Then you can ` apply ` this ( row-wise ): #CODE
apply changes to the column names of a dataframe
There are a few approaches . Using ` apply ` : #CODE
Is there a TimeSeries method to apply ` .replace ( tzinfo=None )` to each date in the index ?
You can use ` apply ` to do this : #CODE
@USER are you saying the above worked on newer or older pandas ? There are a few edge cases in pandas ' apply which have been tweaked over last few releases so this could be one of them !
You can convert this column to integers by ` apply ` -ing ` int ` : #CODE
Thanks I guess I just have to iterate over each of the hierarchy and apply pivot_table to get my desire output .
It's less complicated and faster than using ` apply ` or ` map ` . Something like ` np.dstack ` is twice as fast as ` zip ` , but wouldn't give you tuples .
The midpoint formula that I wish to apply is dependent on the bid / ask spread of the instrument . If the current spread is wider than the minimum tick increment , the midpoint will be the simple average of bid and ask prices at that moment . If the spread is equal to the minimum , the midpoint is weighted based on the bid and ask quantity .
I'm not sure this used to work . If I'm reading the code right , the intent is to apply this function to corresponding elements of the four ` DataFrames ` . But that's not what it did in olden times -- previously , I'm pretty sure the first branch would have been taken , because ` ( ask_price - bid_price ) > tick_increment ` was non-empty , and thus truthlike . So I suspect this code was buggy in the past . We can write a vectorized version of this which can work , at the cost of doing twice the work , but if there's a multi-DataFrame version of ` applymap ` I'm not sure I've used it .
@USER it definitely ran without errors , though I'm not certain it was getting the right result . I may try installing an older version to find out what was actually getting calculated . I think you are understanding what I am aiming to do - essentially apply the midpoint formula for each symbol , and at each timestamp , in the same way that I could get a DataFrame of bid / ask spreads using ` spread = ap - bp ` .
You need this odd apply at the end because not yet full support for timedelta64 [ ns ] scalars ( e.g. like how we use Timestamps now for datetime64 [ ns ] , coming in 0.12 )
PS : the same dict-based approach will work if you only want to apply the replace to certain columns , but you'd have to restrict the application . For example , if you wanted to go the other way , you probably wouldn't want the ` 2 ` in the weight column to become ` fou ` .
But , to answer your question , you can apply ` list ` . #CODE
Pass a function to ` apply ` and specify ` axis=1 ` .
This still does not work . I think the na_values option does not apply to the columns that is being parsed as dates . The problem is really parse_dates does not work for columns with missing values .
Apply pandas function to column to create multiple new columns ?
However an apply within an apply still isn't going to be particularly efficient ...
How to apply a function to several columns of a GroupBy object ?
If you dont specify a function per column , all columns will be passed to the function ( for both apply and agg ) . So : #CODE
This can also be done using apply , no need to sort . #CODE
As a workaround , in earlier pandas you can use apply : #CODE
@USER tranform expects one result to all the things in the group , whereas apply expects a value for each row in the group . Although both act of the groups ( sub DataFrames ) so it is a little confusing .
That makes sense , but doesn't seem to be very clearly documented . For example [ here ] ( #URL ) it starts by describing transform as a form of apply , and later makes them sound almost equivalent : " ... For these , use the apply function , which can be substituted for both aggregate and transform in many standard use cases . However , apply can handle some exceptional use cases , for example ... "
But I can't see any easy way of doing the same thing with my ' recd ' or ' ship ' date fields . For example , generate a similar table of counts broken down by ( say ) monthly buckets of recd and ship . It seems like resample() has all of the machinery to bucket into periods , but I can't figure out how to apply it here . The buckets ( or levels ) in the ' date cut ' would be equivalent to a pandas.PeriodIndex , and then I want to label each value of df [ ' recd '] with the period it falls into ?
The second line uses the ` apply ` method on groupby to replace the dataframe of near-duplicate rows , ` g ` , with a new dataframe ` g.apply ( lambda row : g.irow ( 0 ) , axis=1 )` . That uses the ` apply ` method on dataframes to replace each row with the first row of the group .
You can create a column in your ` DataFrame ` based on your Days Late column by using the ` map ` or ` apply ` functions as follows . Let's first create some sample data . #CODE
Group your ` DataFrame ` by ` country ` and ` countrycode ` and then apply your own function : #CODE
Pandas : How to use apply function to multiple columns
I have some problems with the Pandas apply function , when using multiple columns with the following dataframe #CODE
When I try to apply this function with : #CODE
If you just want to compute ( column a ) % ( column b ) , you don't need ` apply ` , just do it directly : #CODE
@USER following [ 53-54 ] allow you to apply more complex functions .
This is obviously using the data generated below , but you can easily apply to your example .
You probably need ` apply ` , so something like : #CODE
Wrapping it in a Series in the apply returns a DataFrame : #CODE
You could also ` apply ` ` np.prod ` , which is what I'd originally done , but usually when available the direct methods are faster . #CODE
I've seen a few solutions which map / do list comprehension to ' manually ' put the dataframe together . Is that the only way ? I was hoping pandas had some basic function to magically do this kind of thing ... apply ? join ?
To restrict to Business days within business hours ( apply these in either order ) .
I am generating some delimited files from hive queries into multiple HDFS directories . As the next step , I would like to read the files into a single pandas dataframe in order to apply standard non-distributed algorithms .
You could just apply the Series constructor to that column : #CODE
Hmmm , a simple is to just apply something like ` make_series = lambda x : pd.Series ( x ) if x == nan else x ` , there's probably a more efficient way though .
Have a look at #URL more specifically at the apply and transform sections
After you've read in the DataFrame ( without restricting dtype ) you can then convert it ( using technique from this post ) with ` apply ` : #CODE
You shouldn't edit the question , but rather ask a new one :) . It's essentially the same trick in both cases ( just define a function which does it to a single string and then apply it the column ) .
You do not need groupby / apply to compute the diff column . df4 [ ' Diff '] = abs ( df4 [( ' acctual ' , ' Quantity ')] - df4 [( ' trend ' , ' Quantity ')]) . From this aggregated data can be computed with groupby e.g df4.groupby ([ df4 [ ' Start '] .map ( lambda x : x.year ) , df4 [ ' Start '] .map ( lambda x : x.week ) , df4 [ ' Product ']]) .sum() . Not that the latter does not have a Trader column . Are you sure that the groupby you are doing is what you want ? Some funky mapping Trader <-> week .
Is there a way to do this ? I think the easiest way is to apply a function of dividing by index1 values by 3 , but not sure how you apply a function to an index . Perhaps though pandas has it's own methods for redefining index values to have groupings like this which are still unique when you consider both indexes ?
your data is 2-d , how do you want to make it 1-d ? e.g. take a single column for example , or apply a function across all the columns in a reduction operation , or concatenate the data
Hmm ... why the downvote , gang ? This seems like a well stated use case that may apply to others .
However , this doesn't carry over directly to pandas Series . I seems that map / apply / lambda seems the way to go . I've arrived at this piece of code , but getting an invalid syntax error . #CODE
You can just use an apply with the same method suggested there : #CODE
Hmm not too sure , think you may be better off just with the apply , but something using match could be possible : ` s.str.findall ( r ' ( ? <= \ ( ) [ ^ ( ] * ( ? =\ ))')`
The issue came up with apply in fact . Findall works ! Last thing - how do I get rid of the square brackets around the results other than stripping them after the fact ?
pandas - apply function to current row against all other rows
Interesting . I replaced my list comprehension with a slightly nicer nested apply . But this is even more compact . I wonder if `` np.equal `` can be worked into it ....
When subsequently doing an apply / map , you'll usually want the function to return a Series ...
When I do ' g = df.groupby ( ' author_id ')' g is then just ' ' and I cant seem to be able to apply the function ...
Yes , it returns a groupby object . You apply the function using ` g.apply ( some_function )` , whether you can apply it depends on the function ...
The output of the unique function is a numpy array , which doesn't provide the apply method . You can create a ` Series ` by that array and then apply your function : #CODE
Error when trying to apply log method to pandas data frame column in Python
Pandas doesn't complain , because now you have an array of Python objects . [ but this is really just cheating around the typecheck ] . And if you want to convert back to array , just apply ` np.array ` to it . #CODE
Apply iterates through every item in the dataframe .
Then just apply the aggregation function and boom you're done : #CODE
I wonder whether ` to_datetime ` is faster that ` apply ( pd.Timestamp )` ? Certainly that'll be the only choice come 0.11.1 :)
`` pd.to_datetime `` should be faster if the cython doesn't raise ( in which case it essentially falls back on `` apply ( Timestamp )`` .
Apply ` histogram ` to each group . #CODE
Here is the canonical way of doing it , while not necessarily more concise , is more flexible ( in that you can apply this to arbitrary columns ) #CODE
I am on window 7 , python 2.7.2 , pandas 0.11.0 , django 1.4 , wsgi and apache 2.2 . I have a pandas script that works fine if I run it directly with python and also works in ipython with %run . However , when I run pandas in my view i get " LookupError : unknown encoding : cp0 " . This only happens when using ols in pandas within the view . I'm also a little confused why py3compat.py is entering the picture as i'm using python 2.7 . Also , I have seen some posts about wrapping a printed variable in a str() , but I'm not sure how that would apply here . The whole traceback is : #CODE
I kind of think that the first one is incorrect behaviour here . It's like the popular saying : " Writing aggregation functions is hard ... let's go write apply functions . "
If you pass a ` dict ` or ` list ` to apply , you will have item-by-item agg , IOW , you will get a ` Series `
I know i should do df.groupby ( ' channel ') and then apply function to each group .
In this case the apply value already returns the exact same df with only different cost values . So you can do : ` df = df.groupby ( ' channel ') .apply ( myfunc )` . But if you insist on only modifying the cost column this would also work : ` df [ ' cost '] = df.groupby ( ' channel ') .apply ( myfunc ) [ ' cost ']` . But i wouldnt use the latter since a change in the index might cause misalignment , even though it would work in this case .
( Some operating systems provide record-oriented files that have more complex internal structure than the common flat file . The above does not apply to them . )
@USER give an index to the Series when you apply ; they will become column names
I'm having a little trouble with the amount of memory that this method consumes and I'm wondering if you could give me a little advice . I have a DataFrame that contains about 8000 rows , each with a string containing 9216 space delimited 8-b it integers . This is roughly 75MB , but when I apply the last solution verbatim , Python eats 2GB of my memory . Can you point me in the direction of some source that would tell me why this is , and what I can do to get around it ? Thanks .
Now you can apply the respective ` fillna ` s , one cheeky way : #CODE
You get one string per line and the other cells are ` NaN ` , then the math to apply is to ask for the ` max ` value : #CODE
I knew there was something , I just couldn't find it ! FWIW the ` isin ` method is about 10 times faster than using ` apply ` on my particular dataset .
then apply this function across each group : #CODE
Awesome , thanks @USER . Knew it was going to be a groupby , but couldn't figure out how to apply it properly .
here cols is df [ ' C '] and values is df [ ' D '] . We group those two things by cols and then apply the aggregating function , which in this case is np.size . Each row looks like #CODE
How can I apply the testfunction on x which returns the two c [ 1 ] values ?
Do you mean to say , you need to apply testfunction on df1 and df2 individually and store the result somewhere ?
Like ` y = map ( testfunction , x )` ? This will apply testfunction to every item of the iterable you pass and return a list of the results .
Thank you very much Brian for your answer ! I just spent 2.5 hours on this problem . I thought that I need to apply a " for loop " ...
This will apply the function ` testfunction ` to every item in ` x ` and return a list of the results .
Is there any equivalent function like " apply " for rows . Since apply seems to work on " columns " only .
@USER ` apply ` takes an ` axis ` kwarg to toggle between rows and columns
The result you currently have is a ` Series ` with a ` MultiIndex ` , so all the usual rules will apply . If your result is called ` res ` , then ` res.ix [ False ]` gives you just the Falses , indexed now by only ` user_id ` . Likewise for ` res.ix [ True ]` . See the docs .
Hmm ... I'm running into a bit of trouble here . The matrix includes 1.00 identities ( perfect matches ) across the diagonal . However , when I apply the " unpack " function , all of those turn into a value 0.939085 , for which I'm not sure how it's happening . In less than a few minutes , I will update the original question with the data set I'm working with .
Well I don't know anything about robot_detection module , but a DataFrame with 50000 lines doesn't sound that much , really . And looking at your for-loop it seems like the function ` is_robot ` is not applied to the whole DataFrame but to each line , which I assume to contain one entry of the log . Thus I suggested to use ` apply ` instead .
I see , I guess then you can split the DataFrame , apply ` value_counts() ` to each split , and then sum all the splits to get the final result .
Thanks Very Much ! In addition to this I want to apply some function to each group ? How to access groups one by one ?
I'm splitting the dataframe since it is too large . I want to take the first group and apply the function , then the second group and apply function etc . so how do I access each group ?
subset before you apply #CODE
2nd part , row-wise apply , returning ' sum ' of elements in A and B columns . You can
pretty much what what you want in apply . #CODE
This is cool - how would I do a row-wise apply taking two values from different columns but in the same row into the apply function ? Say two columns were first / name and family / name strings and I wanted to take value1 and value2 and do stuff to them .
neat apply tricks #CODE
To select rows with one item you're interested in , you can ` apply ` a ` lambda ` function . For example : #CODE
thanks for mentioning ` grouper ` , as it is not documented ! I finally found a solution to change rows in original dataframe while iterating over its grouped object , using ` grouped.grouper.indices ` . I had to use it because I have duplicate DateTime indices in the dataframe . Also the transformation is too complicated to fit a ` grouped ` then ` apply ` paradigm , it involves clustering and filling in multiple dataframes at once while going through each group .
You could use ` apply ` , e.g. : #CODE
Essentially an apply , but with control over how exactly its combined . #CODE
You can use ` groupby ` , ` apply ` , ` reset_index ` to create a multiindex Series , and then call ` unstack ` : #CODE
I have a DataFrame with an index called ` city_id ` of cities in the format ` [ city ] , [ state ]` ( e.g. , ` new york , ny ` containing integer counts in the columns . The problem is that I have multiple rows for the same city , and I want to collapse the rows sharing a ` city_id ` by adding their column values . I looked at ` groupby() ` but it wasn't immediately obvious how to apply it to this problem .
So its not necessary to groupby month first , unless you have other reasons to do so . If so , you can also apply the last groupby on the monthly df as well .
Ah . thanks tshauck , I guess that was actually what i was trying to do . The result to that test actually printed the count for each field i.e.Field1 = 10 and next line Field2 = 10 . I guess you could also apply the count to one particular Field ?
How to apply to_datetime or with sort_index so that the dates are sorted ?
I don't know how to apply to_datetime , because the output of the dates are not sorted . #CODE
At the moment you can do this with an apply or the delta attribute : #CODE
` copy() ` doesnt prevent that . Using ` copy ` seems to only apply on the data , not the index . Which is a bit strange , reading your spreadsheet twice might be the best option , altough not terribly efficient .
@USER It's not really clear what you expect that to do , but using groupby ensures each groups is a DataFrame and then , for example , you can apply a function to that .
@USER if you really wanted to do it manually you could use : ` g = df1.groupy ( ' time ') ; [ g.get_group ( x ) for x in g.groups ]` , but I recommend apply .
Thanks falsetrue ! this has been SOO helpful I can apply this to everything now ! :) I can't tell you how long this has taken me !
I have been doing Pandas , in which I apply a ` .groupby ( ' Subtype ')` on the dataframe , but after I do that , I'm not sure how to proceed further . Any help would be appreciated !
Then you can use this in a groupby ` apply ` : #CODE
@USER it looks like you've tried to apply shift to a Timestamp rather than a column / Series ( not sure how you did that though ) .
[ Using Python3 ] I'm using pandas to read a csv file , group the dataframe , apply a function to the grouped data and add these results back to the original dataframe .
Basically I'm trying to group by ` cc ` and calculate the percentile rank for each value in ` total_value ` within that group . Secondly I want to apply a flow statement to these results . I need these results to be added back to the original / parent DataFrame . Such that it would look something like this : #CODE
Another option is to use an apply : #CODE
Another option is promote the ' day ' level to a column and then use an apply .
Note you can apply strftime directly from a Timestamp object e.g. ` rng.map ( lambda t : t.strftime ( ' %Y-%m-%d '))` .
2 ) I tried filtering using the apply function : #CODE
How to apply condition on level of pandas.multiindex ?
You should check out ` scipy.sparse ` ( link ) . You can apply operations on those sparse matrices just like how you use a normal matrix .
Here's one way ( though it feels this should work in one go with an apply , I can't get it ) . #CODE
but I'm not able to apply this on each " name " group of my dataframe
Your desired result suggests that you actually want to normalize the values in each name group with reference to the elements in ` value1 ` and ` value2 ` . For something like that , you can apply a function to each group individually , and reassemble the result . #CODE
I read about the .stack method but couldn't figure out how to apply it for this case .
I'm not aware of a method on DataFrame to do this . Do you expect things like count and the quantiles to change ? Or just the mean and standard deviation ? Can you apply the weighting first and then call describe on the resulting series ?
Thanks TomAuspurger ... that was my suspicion , but I was hoping to avoid that extra coding ... I'd expect it to apply to all of the metrics .
Assume you have 70% of the population with revenue 0 , and 30% with revenue 1 . You'd want median revenue weighted by population to be 0 . If you multiply revenue by weight and apply describe ... you probably get a median of 0.15 ( vector 0 , 0.3 ) which is irrelevant .
I have a pandas series of booleans and was wondering what the best way is to apply " or " or " and " to the whole series . I am thinking something along the lines of a Haskell #CODE
will apply a function to each element in the series so doesn't seem to do what I need .
` replace ` seems to apply to DataFrame not to a Serie
As you say , looping ( iterrows ) is a last resort . Try this , which uses ` apply ` with ` axis=1 ` instead of iterating through rows . #CODE
Thanks a lot . Very helpful . I've been trying to apply that to a larger DataFrame and keep on getting this error
If the criteria I apply return a single row , I'd expect to be able to set the value of a certain column in that row in an easy way , but my first attempt doesn't work : #CODE
A nested apply will do it #CODE
suppose I have a dataframe with index as monthy timestep , I know I can use ` dataframe.groupby ( lambda x : x.year )` to group monthly data into yearly and apply other operations . Is there some way I could quick group them , let's say by decade ?
I have tried several variants on : ( I assume I'll need to apply the limits to each plot .. but since I can't get one working ... From the Matplotlib doc it seems that I need to set ylim , but can't figure the syntax to do so . #CODE
You are using an indexing short-cut which doesn't apply , see here : #URL
To do this over each group you have to groupby category first and then apply this function : #CODE
Apply the function ( you could group by uuid , site if you want as well ) #CODE
When you apply your own function , there is not automatic exclusions of non-numeric columns . This is slower , though ( that the applicatino of ` .sum() ` to the groupby #CODE
Thanks Jeff . How could I apply different functions on several columns in one go as well , e.g. sum on column " B " and set on column " C " ?
each group get's passed a `` DataFrame `` ( called x ) , so x [ ' A '] is a `` Series `` , just like regular indexing ( but its just the rows in that group ) . the `` x [ ' A '] .sum() `` thus reduces to a scalar value , as do the other terms . Net you are returning a `` Series `` with values for the `` index =[ ' A ' , ' B ' , ' C ']`` . These are stacked ( row-wise ) to form the result frame at the very end of the apply . You can do bare strings for the keys when you use `` dict `` , equiv is `` { ' A ' : x [ ' A '] .sum() } ``
great ! just to point out though , you should still do the aggregation on A and B via a direct `` .sum() `` rather than apply because these are cythonized . The apply going to be slower , so do only where you really need it .
Great answer ! A slightly more direct way to apply different functions on several columns is to use the ` agg ` function , so ` df.groupby ( ' A ') .agg ( dict ( A = ' sum ' , B = ' sum ' , C = lambda x : ' {%s} ' % ' , ' .join ( x )))`
Thanks a lot , I have been looking into the ` agg ` function . Any ideas how that would compare performance-wise to apply ?
they are the same ; apply is a bit more flexible in how it looks at the output ( just slightly )
You can use the ` apply ` method to apply an arbitrary function to the grouped data . So if you want a set , apply ` set ` . If you want a list , apply ` list ` . #CODE
If you want something else , just write a function that does what you want and then ` apply ` that .
Why am I unable to apply my function elementwise on my ` Series ` object ?
You could write a function and ` apply ` it to each DataFrame in the panel you get from Yahoo .
I have calibration factors that need to be applied after specific dates and for certain ranges of instrument readings ie . a higher reading will require a different calibration factor . I am way I am trying to apply a lookup table that is based on the time and also the raw instrument reading through the use of a nested python dictionary .
is there a way to iterate through my dataframe to apply these calibration factors with the nested dict as a form of a lookup table ?
Till ` pandas ` is not officially implemented in ` plt.fill_between ` function , you can still apply ` pd.Series ` or ` pd.DataFrame ` as ` pd.Series() .values ` and ` pd.DataFrame() .values ` to make ` fill_between ` plots .
`` apply `` would return a shorter Series , with one entry per group . Instead , we want a Series of the same length as the original one , with each group's entire contents mapped to `` True `` or `` False `` as a block . Then we can use that boolean Series to mask the original Series . See the [ documentation ] ( #URL ) for more .
And then use ` apply ` : #CODE
Alternatively you could use ` apply ` ( but this will usually be slower ): #CODE
How to apply a function to two columns of Pandas dataframe
Pandas : How to use apply function to multiple columns
And apply the function like this : #CODE
Any Ideas on how to get around the axis parameter error ? Or a more elegant way to calculate the pct change ? The kicker with my problem is that I needs be able to apply this function across several different column pairs , so hard coding the column names like the answer in 2nd question is undesirable . Thanks !
The confusion stems from two different ( but equally named ) ` apply ` functions , one on Series / DataFrame and one on groupby . #CODE
The DataFrame apply method takes an axis argument : #CODE
The groupby apply doesn't , and the kwarg is passed to the function : #CODE
Thanks for your answer Andy . If I stick with the groupby apply and remove the axis param , I get a key error ` KeyError : u'no item named 0 '` for accessing the elements as ` row [ 0 ]` ect . Is there a way to use the groupby apply and still use a notation that keeps it easy to apply to several differently named column pairs ?
@USER updated , you can use the groupby with axis=1 , you can apply pct_change to entire dataframe . Or perhaps you want to do this one each group using an apply ( ` lambda x : x.pct_change() `) .
@USER you need to tweak delta a bit , see my last example ( assuming that's from the apply )
@USER not sure I have a good reference , but a good trick is to set up a break point in the function you're going to apply and then see how you can access things you want
You could define a function to subtract the quarterly totals from the annual number , and then apply the function to each row , storing the result in a new column . #CODE
Assuming these are just strings you could simply add them together ( with a space ) , allowing you to apply ` to_datetime ` : #CODE
A purely pandas way might be to apply the Series constructor to put this into one DataFrame and stack into a Series ( so you can use value_counts ) ... if you didn't care about the index / timestamp you could use collections ( which may be faster ): #CODE
` Apply ` Series constructor to get a DataFrame and ` stack ` it into a Series : #CODE
The result of ` df.groupby ( ... )` is not a DataFrame . To get a DataFrame back , you have to apply a function to each group , transform each element of a group , or filter the groups .
I came out with the following solution . As preparation steps , I ll group by speaker name and set the file name as index by the set_index method . I will then iterate over the groupbyObj and apply the calculation function , which will return the selected speaker and the files to be marked as used .
1 . How can I approach a specific group by a groupby object ? bcz I thought maybe instead of setting the files as indexed , grouping by a file , and the using that groupby obj to apply a changing function to all of its occurrences . But I didn t find a way to approach a specific group and passing the group name as parameter and calling apply on all the groups and then acting only on one of them seemed not " right " to me .
` apply ` on a series returns a DataFrame if the function wich is applied returns a Series for each element of the series , where the different elements of the returned Series become the values of the different columns of one row . So in this case first the ` list ` function converts the string in BINDATA to a list , which is then converted to a Series ( see also the answer of @USER , which does actually the same but is written a little bit different )
I'm not sure I follow exactly what aggregation you want , but you should be able to apply ` groupby ` however you like . For example : #CODE
How can I apply formatting to the secondary Y-axis ( the one that displays on the right ) ?
Actually , you can't just apply Series to a set ( which is annoying ) ` TypeError : Set value is unordered ` , seems unnecessary restriction / not very duck .
@USER exactly right ; apply DOES call things twice ( on purpose ) to see if there are modifies in place ( in which case slow path is taken ); otherwise a faster path can be taken .
How to merge two DataFrame columns and apply pandas.to_datetime to it ?
What would be a more pythonic way to merge two columns , and apply a function into the result ?
Just apply the ` min ` function along the axis=1 . #CODE
Then use ` groupby ` and ` apply ` : #CODE
I'm trying to apply simple functions to groups in pandas . I have this dataframe which I can group by ` type ` : #CODE
I want to apply a function like ` np.log2 ` only to the groups before taking the mean of each group . This does not work since ` apply ` is element wise and ` type ` ( as well as potentially other columns in ` df ` in a real scenario ) is not numeric : #CODE
is there a way to apply ` np.log2 ` only to the groups prior to taking the mean ? I thought ` transform ` would be the answer but the problem is that it returns a dataframe that does not have the original ` type ` columns : #CODE
The first proposal gives ` ( ' Not implemented for this type ' , u'occurred at index type ')` . The second one works but it drops the ` type ` , so you can't group afterwards . ` _get_numeric_data() ` can't be used with groups I believe . So can't think of how to use the second one to apply ` np.log2 ` to numeric data only and then group or group first and then apply only to groups
I think you'll be able to ` apply ` a different function here ( rather than sum ) to achieve the desired result .
@USER then sum or more complicated analysis e.g. via apply :)
Thanks for helping . Your suggestion works just fine for the those dfs . I'm trying to modify your code to apply lambda function for each MAPINFO at once . Acctually , those df are huge.Best .
Apply function to a MultiIndex dataframe with pandas / python
I have the following ` DataFrame ` that I wish to apply some date range calculations to . I want to select rows in the date frame where the the date difference between samples for unique persons ( from sample_date ) is less than 8 weeks and keep the row with the oldest date ( i.e. the first sample ) .
I normally use R for the procedure and generate a list of dataframes based on the name / dob combination and sort each dataframe by sample_date . I then would use a list apply function to determine if the difference in date between the fist and last index within each dataframe to return the oldest if it was less than 8 weeks from the most recent date . It takes forever .
pandas groupby apply on multiple columns
I am trying to apply the same function to multiple columns of a groupby object , such as : #CODE
What is the correct way to apply the function to multiple columns at once ?
Apply by the columns of the object you want to map ( df2 ); find the rows that are not in the set ( ` isin ` is like a set operator ) #CODE
` apply ` should generally be much faster than a for loop .
This example you provided is : ` ( df [ ' A '] == 999 ) & ( df [ ' B '] == 999 )` , But if you have a branches with else statement also you should use ` apply ` along the asix .
I added an example to the answer that covers that case ( using ` apply `) .
In cases where you have multiple branching statements it's best to create a function that accepts a row and then apply it along the ` axis=1 ` . This is usually much faster then iteration through rows . #CODE
I have tried using ` pd.cut ` or ` np.digitize ` with different combinations of ` map ` , ` apply ` , but without success .
This is a good candidate for .apply . You can do this in the same line where you compute column values by putting .apply ( lambda x : x / np.timedelta64 ( 1 , ' D ')) at the end to apply the conversion at the column level . e.g. s3 =( s1-s2 ) .apply ( lambda x : x / np.timedelta64 ( 1 , ' D ')) .
I understand now the source for most of my problems using groupby . To my taste , the groupby mechanism is logically too ambiguous and the design encourages the user to use it in the wrong why . The way I saw this , the whole idea behinds data analysis with pandas is to group and apply . I thought grouping is the most expensive task , so I imagined the proper use would be to group only once , and then do what ever you want with the groups . as long as the group members dont change , you shouldn't regroup a dataframe . This idea is also implied from the design , as you can save a groupby object , which for me implies that the author of pandas wanted as to create a groupby object only once .
If Im right , the linkage between a groupby element to a dataframe is quite weak , and the results of several dataframe modifications and groupby operations cannot be fully anticipated . So what is the solution ? running groupby for each and every apply operation ? that seems redundant ..
You just need to return the frame in your function . Apply takes the output of the function and creates a new frame ( of the applied data ); if you return ` None ` in your function then it uses the original ( and if you don't return a value , then you are implicity returning ` None `) #CODE
Sorry , didn't get it completely .. what do you mean by " if you return NONE n your function then it uses the original " you mean that the underlying dataframe is determined according to my apply function returning or not returning a value ? Im still confused about what going on under the hood here . I thought that a groupby object simply holds the index of all gruop and their members . but this is clearly not so . So what , groupby object holds a copy of the original dataframe ? Please elaborate more and be more clear ..
also , this implies regrouping the dataframe for each apply operation , which can be expensive and redundent ( see my edits .. )
Maybe you can explain in a simple example what your problem is . Optimizing is often the last step . Make sure you have correctness first . Profile , THEN optimize if needed . Groupby is a cheap operation ; the apply can be more expensive . But unless you have LOTS of groups this shouldn't matter .
why don't you group by both columns then ? If your process is iterative then just groupby ( and apply after each iteration )
I was writing up a version using ` groupby ` and ` apply ` , but this works too . : ^ )
You should apply your function along the axis=1 . Function will receive a row as an argument , and anything it returns will be collected into a new series object #CODE
As for the second part of the question : row wise operations , even optimised ones , using pandas ` apply ` , are not the fastest solution there is . They are certainly a lot faster than a python for loop , but not the fastest . You can test that by timing operations and you'll see the difference .
Some operation could be converted to column oriented ones ( one in my example could be easily converted to just ` df [ ' a '] + df [ ' b ']`) , but others cannot . Especially if you have a lot of branching , special cases or other logic that should be perform on your row . In that case , if the ` apply ` is too slow for you , I would suggest " Cython-izing " your code . Cython plays really nicely with the NumPy C api and will give you the maximal speed you can achieve .
@USER I saw that you rarely use apply along ` axis=1 ` . Is there any specific performance reason ? Shouldn't that be the fastest way to itterate over the array row wise ?
Well , I thought about something similar to your solution , however I didn't pursue it further because I want to apply it to millions of rows , so I'm afraid this won't scale well .
Wonder if there should be a native way to do apply across multiple dataframes ...
and I want to apply a function which uses the index of the row : #CODE
I don't believe ` apply ` has access to the index ; it treats each row as a numpy object , not a Series , as you can see : #CODE
To get around this limitation , promote the indexes to columns , apply your function , and recreate a Series with the original index . #CODE
In my case ( a dataframe , with axis=1 ) , x.name() returns the value of the index when I apply a function lambda x : x ...
so when calling ` apply ` on ` DataFrame ` its index will be accessible through ` name ` of each series ? I see this also is true for ` DateTimeIndex ` but it is a little weird to use something similar to ` x.name == Time ( 2015-06-27 20:08 : 32.097333 + 00:00 )`
You may find it faster to use ` where ` rather than ` apply ` here : #CODE
I recommend testing for speed ( as efficiency against apply will depend on the function ) . Although , I find that ` apply ` s are more readable ...
Excellent point , did not think about that ! There is filter I apply that filters out about 90% of the data . ( However the rest of the calculations have to be done over the concatenated DataFrame . ) Thanks a lot !
I guess I could write a function using ` try ` and then use pandas ` apply ` or ` map ` , but that seems like an inelegant solution . This must be a fairly common problem , right ?
In fact , you can apply this to the entire DataFrame : #CODE
If you really wanted to do this you could use a groupby apply : #CODE
and apply the function to each group , and then ` unstack ` from a Series to a DataFrame : #CODE
@USER Not sure I do fully either , I think I should probably put it up as an issue on github to discuss / fix , suspect it is untested behaviour , but it seemed that you can only apply aggfunc to columns which were going to be in your pivot_table ( IIRC the rest are not passed to the function so couldn't be used ) , also I was seeing SNDArrays which was confusing ... I need to investigate a further .
Thanks ! what if you don't want to apply a rolling mean but an arbitrary function in rolling overlapped windows ?
Assuming you have indexed by datetime can use groupby apply : #CODE
It's also present in apply though ( with both our answers ) . ?
( see also Pandas : How to use apply function to multiple columns ) .
This means you can invert np.percentile using map , and then apply a shift and a subtract to get the " percentage if array in interval " you're after .
It feels like there ought to be a neat way using stack ( essentially separating into away / home , enabling a groupby apply ) .
Create a ` defualtdict ` ( with default value 0 ) where you will keep the current scores of the teams , and apply along the ` axis=1 ` a function that updates this dictionary and returns a tuple of results . Then just concatenate your DataFrame and the resulting DataFrame from the ` apply ` function along ` axis=1 ` . #CODE
stack / groupby / apply is * significantly * faster than this method ( once there's more than 40 or so rows ) !
@USER how is it independent when in one case it takes 32x more memory and in the other it takes only 4x more memory ? And the global can be passed as an argument to apply , but that is not the an issue :)
You could use ` pd.to_numeric ` method and apply it for the dataframe with arg ' coerce ' . #CODE
If your final game plan involves doing on of these ... just use these . If it's something else , consider writing it as a generic rolling apply .
You can simply use an ` apply ` to do this : #CODE
I am trying to transform the data points in a DataFrame to a two dimensional array group by group and then transform the same data to a different one dimensional array . For example , the two dimensional array can be the independent variables of a regression while the one dimensional array can be the dependent variable . I could have written two functions , one for each array , and apply them separately , but that would be very slow . So traverse the data once and generate both arrays would be preferred .
You don't need to use apply here , and unless you are using a cythonized function which can operate on a frame / series , it doesn't make any difference in perf .
I'm working on replacing an Excel financial model into Python Pandas . By financial model I mean forecasting a cash flow , profit loss statement and balance sheet over time for a business venture as opposed to pricing swaps / options or working with stock price data that are also referred to as financial models . It's quite possible that the same concepts and issues apply to the latter types I just don't know them that well so can't comment .
If not , could you suggest how to modify Pandas in order to provide irregular duration ` Period ` objects ? ( this comment suggests that it might be possible " using custom DateOffset classes with appropriately crafted onOffset , rollforward , rollback , and apply methods ")
Since i tried tu find the prediction values of my variables ... For that , i'm developping this code and i tried to apply what i found in this link #URL
The ` apply ` method applies a function to each of the aforementioned subsets and concatenates the result ( if needed ) .
One thing to note is my use of ` g.name ` . This doesn't normally exist as an attribute on ` DataFrame ` s ( you can of course define it yourself if you want to ) , but is there so you can perform computations that may require the group name . In this case that's the " current " fruit you're looking at when you apply your function .
When I apply the below code pandas is considering NaN as Zero and returing the sum of remaining days . #CODE
You are looking for apply . Your example would look like this : #CODE
Apply function to values and index of series
I like the idea and is along the lines of what I was thinking about . The trouble is I can't make it work , if I apply your exact code ( plus my def of dates ) I get a Type Error : ` TypeError : ( " unsupported operand type ( s ) for /: ' buffer ' and ' int '" , u'occurred at index 2012-04-01 00:00 : 00 ')`
And you / are working on a ` DataFrame ` object inside of ` apply ` indeed .
Actually inside ` apply ` we seem to be working with a ` Series ` object , the name of which is the index of the ` DataFrame ` we're iterating through . This is why your name suggestion works for accessing the value of the index . It seems ` apply ` collapses a " dimension " of the initiating object so this is why working on a ` TimeSeries ` directly only left me with a value object and no way to access the value of the index . Not a perfect solution then but def better than where I was . Thanks again for all the help .
That doesn't work ( although it looks like it should ! ) . When I apply your solution I get ` s ` where ` s.index.month % 3 == 1 ` and ` s / 4 ` where ` s.index.month % 3 ! = 1 ` ; I wanted ` s / 4 ` and ` 0 ` respectively
Just use the ` apply ` function along the ` axis=1 ` and pass the ` pattern ` parameter as an additional argument to the function . #CODE
Like regexp ;) I agree , I just always give the simplest , predictable in speed solution to the OP , and live the exotic ones to you :) ` apply ` should always be ` O ( N )` if I'm not wrong . I don't even know how to calculate how ` melt ` with ` groupby ` behaves ? ` O ( ? )`
Just apply the Timestamp ` time ` method to items in the date column : #CODE
Using the method Andy created on Index is faster than apply #CODE
And then apply the function along the ` axis=1 ` . #CODE
Now ` apply ` needs to return a ` Series ` , not a ` DataFrame ` . One way to turn a ` DataFrame ` into a ` Series ` is to use ` stack ` . Look at what happens if we
Wow , awesome walkthrough ! ` apply ` is now among my top 5 functions to always remember . Concerning the ` pivot_table ` solution : At which point am I supposed to enter the line ? No matter when in my attempt above , I always get ` no item named Edge ` .
I would like to divide my data table called ( my_data2 ) in two samples called ( learning sample and test sample ) . How to apply the logistic regression on the first part of my table ( the first sample ) , then apply predict on the second part ? Thank you .
One solution could be to use get dummies ( which should be more efficient that apply ): #CODE
You could use an apply with a couple of locs : #CODE
You can apply to_json to any DataFrame : #CODE
you can what I do in the apply , its operating on the numpy arrays directly . I don't think readsav can do this conversion .
I tried to make this algorithm : random draw between 0 and 1 ( tir ) .si tir ' ' pred then Xestime2= 1 else Xestime2=0 . I wish apply this algorithm in df [ ' X3 '] but I had 0 in all the values ?? of X3 columns . Which explains thats i have an error in my code .
I understand your code but when I try to apply it to my code it does not work !!
I tried to apply what you made but i have an error !! My coding
I'm hoping there's something I've overlooked in the pandas library / documentation that allows one to know the progress of a split-apply-combine . A simple implementation would maybe look at the total number of data frame subsets upon which the ` apply ` function is working and report progress as the completed fraction of those subsets .
have u done a %prun ( profile ) on the code ? sometimes you can do operations on the whole frame before you apply to eliminate bottlenecks
I did the above in my answer , also cheeky percentage update . Actually I couldn't get yours working ... I think with the wraps bit . If your using it for the apply it's not so important anyway .
Note : the apply progress percentage updates inline . If your function stdouts then this won't work . #CODE
@USER DataFrame's apply is row-wise so I don't * think * dropna can be written in terms of apply ( needed to use this answer ) . Also , dropna is written in cython ( not pure python ) so doing something like this will be much slower .
I came across what looks like the perfect solution on another SO answer ( #URL ) but when trying to apply to this series , I'm getting an exception : #CODE
I really want to be able to apply the changes within the dataframe to apply and reapply groupby conditions and perform the plots efficiently - and would love to learn more about how the .apply() method works .
As far as the ` apply ` method goes , it does slightly different things for different objects . For example , ` DataFrame.apply() ` will apply the passed in callable across the columns by default , but you can pass ` axis=1 ` to apply it along the rows .
You can reproduce it with dupes in the findall ( see my answer ) , I think Jeff's apply solution should work fine with dupes in the DatetimeIndex .
Thanks @USER - your answer did help remove my confusion around the duplicate index , as I know that a pandas index will take duplicates in the index , so I now understand it's the index within the .apply() that was kicking out the error . Your explanation of how apply works as well is really useful , so even though it's not a solution to the original question it's really appreciated .
If you try to return multiple values from the function that is passed to ` apply ` , and the DataFrame you call the ` apply ` on has the same number of item along the axis ( in this case columns ) as the number of values you returned , Pandas will create a DataFrame from the return values with the same labels as the original DataFrame . You can see this if you just do : #CODE
To fix the index , you could just add the X as this index , you could first apply set_index : #CODE
If you want to preserve some of the original timestamps ( but you have to choose which if you are binning them ) , you can specify a function to apply on the individual columns . For example take the first : #CODE
` as_index ` and ` apply ` will only work on pandas git master . If you're not using master , then you'll get the following : #CODE
How to apply a complex formula using Pandas in Python ?
I need to apply a specially designed moving average filter on a traffic dataset ( NGSim ) . This process is very tedious in Excel not only because dataset is very large but also because the formula has to look into columns for getting some values and sum them up . I was wondering if there are any examples like this or any other complicated formulas used in Pandas , Python . Kindly provide any example resources .
Apply per-column the mean of that columns and fill #CODE
hmmm , apologies for being a novice at this , but I can't figure it out . I can get the ' or ' logic to work using numpy comparison functions , but ultimately the logic I need is not ' or ' ... I'm trying to apply a case-statement-like function element wise that works on groups : if element above 1.2 * group_median , then assign ' h ' , if it's below .8 *group_median , then assign ' l ' , the third logical possibility ( it's within .2 of the group median ) is ignored or assigned ' n ' . Any thoughts ?
To expand a little on what's going on here , during the apply the function is called on each group ( in this case there are two , one for B= ' c ' and one for B= ' d ') , here's the c group : #CODE
Apply then outputs this together with the B= ' d ' group to get the desired result .
You could groupby person and then apply a shift to the values : #CODE
You can apply ` value_counts ` to the Series groupby : #CODE
It seems cool , and an approach I had not concieved of . They are all time stamps , I'm just slow on the uptake ,, need to think about it , I'm sure you are right I just need to catch up ! And apply the code ,
You could reset_index . Also , perhaps use a df.index.map , rather than apply . ?
And if you wan't a list of the prices per day then just do a groupby per day and return the list of all the prices from every group using the ` apply ` on the grouped object : #CODE
In R , it is easy to aggregate values and apply a function ( in this case , ` sum `) #CODE
You can use groupby , which can apply a function to the index values ( in this case looking at the first element ): #CODE
I want to group by the columns : date , textA and textB - but want to apply " sum " to numberA , but " min " to numberB . #CODE
... but I cannot see how to then apply two different aggregate functions , to two different columns ?
You can apply a ` join ` operation between your original dataframe and the resulting aggregated data : #CODE
Now that we have the grouping , we can apply the aggregator : #CODE
And you can do an apply over the major axis : #CODE
Hmm , it doesn't look like this lets me store it the way I want , but it does let me access the data in a way which may let me do what I need . I clearly had not looked close enough at Panels . I will have to look into how apply more carefully , I don't understand why you would have the function return 1 . Thanks though !
Hmm , well trying it out doesn't quite seem to do what I want . The return value needs to be of length of the panel , where if I want to operate on the whole array to compute a single object it won't let me return that . Apply doesn't let me reduce the panel . For example as a test I wanted to apply ` np.reshape ( x , ( 2 , 2 ))` on a panel of 4 dfs and it fails . I guess I need to do all my work within the apply , and flatten it back out . This has me on the right track now I think so thanks again .
So then I tried applying with axis=0 ( per column basis ) and modifying the function so it only applies this to the ' Date ' column ( I can't see how to apply this to just one column ) #CODE
That doesn't work because the empty ' Date ' values are not ' NaN ' they are empty strings . Also , ideally , I want to do this using ` apply ` as there are other things I want to do within that function which I removed to simplify this question .
then show what you actually want ; this is the efficient way of doing it , apply is essentially a loop in python space
the shift index looks like a better fix , still would like to know if there is a simple date add function , which is how I'd do it in sql , that could apply ?
I see that , and I like it .. I'd still like to know if there is a simple DateAdd type function that I could use that might also apply for use elsewhere if needed ?
Each list in this column can be passed to ` set.update ` function to get unique values . Use ` apply ` to do so : #CODE
If it's already in your DataFrame , you could use an apply : #CODE
I would like to add additional calculated columns to each DataFrame that is inside the Panel , preferably without a for-loop . I'm attempting to use the apply function to the panel and name the new columns based on the original column name appended with a ' p ' ( for easier indexing later ) . Below is the code I am currently using . #CODE
The code above currently duplicates each DataFrame with the calculated columns instead of appending them to each DataFrame . The apply function I'm using operates on a Series object , which in this case would be a passed column . My question is how can I use the apply function on a Panel such that it calculates new columns and appends them to each DataFrame ?
If you want to add a new column via ` apply ` simply assign the output of the apply operation to the column you desire : #CODE
Or , in this case , simply `` apply ( newCalculation )`` .
Assume you would like to evaluate a time series ts on a different datetime_index . This index and the index of ts may overlap . I recommend to use the following groupby trick . This essentially gets rid of dubious double stamps . I then forward interpolate but feel free to apply more fancy methods #CODE
Pandas Dataframe , Apply Function , Return Index
Then I can apply the function to my dataframe , grouped by I D: #CODE
I'm a newbie to Pandas and I'm trying to apply it to a script that I have already written .
Apply ` tolist ` on each of the group's column B : #CODE
Unfortunately you can't just do an apply ( since it fits it back to a DataFrame ): #CODE
I have a dataset created with pytables that I am trying to import into a pandas dataframe . I can't apply a ` where ` filter to the ` read_hdf ` step . I'm on pandas ' 0.12.0 '
I'm having trouble with Pandas ' groupby functionality . I've read the documentation , but I can't see to figure out how to apply aggregate functions to multiple columns and have custom names for those columns .
To understand why your approach didn't work , remember that the function ` round ` needs two arguments , the number of decimal places and the data to be rounded . In general , to apply functions that take two arguments , you can " curry " the function like so : #CODE
For a modestly sized ` DataFrame ` , ` applymap ` will be horrendously slow , since it is applying a Python function element by element in Python ( i.e. , there's no Cython speeding this up ) . It's faster to use ` apply ` with ` functools.partial ` : #CODE
You could even make a function that returns a partial function that you can apply : #CODE
Why not use ` numpy.round ` and pass the ` DataFrame ` as an argument ? ` 100 * np.round ( df , 2 )` seems to solve the problem for me . If some columns have type inappropriate for ` round ` , just exclude them before passing to the ` round ` function . This should avoid overhead from ` apply ` -like things .
This link is useful , although I cannot figure out how to apply it to my situation .
I use Pandas in IPython Notebook rather than IPython as a terminal shell , I don't see any options in ` set_option ` that supports the colouring , it maybe something that could be done as a plugin to apply some css or output formatting . This is the only way I think you could achieve this
Sounds like you're looking for the ` DataFrame.apply() ` method . The ` apply ` method is a very general way to apply a function across either the columns or rows of a ` DataFrame ` : #CODE
By default it applies a function to the columns , but by passing ` axis=1 ` you can apply a function to each row : #CODE
You can apply the swap_axes method after construction : #CODE
@USER wasn't aware of that I knew that ` apply ` did iterate but thought ` map ` didn't
You can just use ` apply ` and assign direct to the column like so ` df [ ' start_time '] = df [ ' start_time '] .apply ( lambda x : dt.datetime.fromtimestamp ( x ))` , this is better than a list comprehension
Use ` apply ` #CODE
Python using lambda to apply pd.DataFrame instead for nested loop is it possible ?
I'm trying to avoid nested loop in python here by using lambda apply to create a new column
Your apply doesn't work as by default it works column wise , plus you misunderstand what the lambda parameters actually represent so your lambda func does not map to the columns as you expected . If you wanted it to work row wise you need to do something like this ` df [ ' c '] = df.apply ( lambda row : row.A + row.B , axis=1 )` but @USER ' s answer will achieve what you want and is simpler
As @USER points out in the comment , the argument to the function in ` apply ` is a series , by default on axis ` 0 ` which are rows ( axis ` 1 ` means columns ): #CODE
Notice the overlap between IDs 0 and 1 and 1 and 2 at the edges ( I dont want that , messes up my calculations ) . One possible way to get around this is to using groupby on IDs and then loop through that groupby and then apply a rolling_sum
The data look roughly like this , where I only want this to apply to columns starting with T_ : #CODE
This gets the job done , except that I need to apply it to only 6 of the many columns in my dataframe . I should have made it more clear above that there are additional columns containing strings that I don't need to replace . How would I do that ?
Update : I've combined this solution with using a dict to apply the fillna method as suggested here . I wouldn't have known how to use zip to create a handy dict , though , so the answer is rather split between the two of you . Either way , learned a ton from this so thanks .
If you perform an operation on a single column the return will be a series with multiindex and you can simply apply ` pd.DataFrame ` to it and then reset_index .
Also , you might instead want to look at using ` apply ` , which lets you return an entire DataFrame . This way you could , instead of collapsing the items into a list , actually return a new grouped table with one row for each unique value in the source column .
Where the argument x is the Column int he DF ? Thanks this is helpful . It seems I don't need to actually iterate over the index within each group . How would one do that if it were necessary ? Apply seems like it could be useful as well , and that seems to work in prety much the same way . I'll have a look now . thanks a lot .
I am trying to apply Logistic Regression in Python using statsmodel.api.Logit .
When I want to resample my time series data , it is very straightforward to apply the arithmetic mean function .
` df.date = df.date.apply ( lambda d : datetime.strptime ( d , " %Y-%m-%d "))` here which doesn't work since I'm working with integers , not strings . I think I need to use ` datetime.date.fromtimestamp ` but I'm not quite sure how to apply this to the whole of df.date . Thanks .
I think I have finally solved the problem . I suspect that yemu's answer is good , but I prefer this as it was an exercise in finally learning how to apply my own functions : #CODE
Working and tuning your data from the stacked dataframe above is straightforward . You can follow by resetting the index , split it into year month day columns , and apply the math on the non NaN data that are in a single column now .
So you need to apply an aggregation operation ( e.g. ` sum ` , or use ` apply `) to your grouped frame , which will then create a new frame , which you can ` to_excel ` .
This makes sense . I suppose I should be using code that just sorts if I want to organize by groups for the output , but not apply any aggregation operations . Thanks for the clear explanation - the fact that some output did make it to the excel file made me think it ought to work the way I supposed , but obviously not .
But for a larger set of replaces you would want to use one of the two other methods or use " apply " with a lambda function ( for value transformations ) . Last but not least : you can use .fillna ( ' bla ') to rapidly fill up NA values .
Your question is a little unclear , but you seem to be trying to apply a function to each row of the DataFrame . Try #CODE
I would consider using a [ Pandas DataFrame ] ( #URL ) . You could have an index for the ID ( dict key in your example ) . Then perform an outer join operation and apply a function that does the comparison to the result . Of course there will be ways to do it with ` dict ` , but it seems like you really need a bit of relational logic on the IDs followed by application of an arbitrary piece of code ( the comparison operations ) . That's what Pandas is good at .
One of the distance metrics ( jaro , perhaps ) in the ` jellyfish ` library would probably apply . Here is an [ example answer ] ( #URL ) that prints out words of a certain distance , but you could just as easily print out the distance itself ...
Iv'e tried to use the groupby mechanism , but with no success . using the simple apply mechanism is ok , but seems a little cumbersome ( I'll need to keep a dictionary containing a counter of appearances for each ID )
Can you explain what is happing here ? because i was trying to use groupby and apply and what i got back was a series with the ID as index and the modified ID's as lists for each index . what is going on here under the hood ? what is the translation into natural language of the code above ?
` apply ` and ` transform ` do similar things . ` apply ` is a complicated beast because it behaves differently depending on the type of object the function returns . I have not attempted to memorize [ the rules which govern this behavior ] ( #URL ) , I simply try a few plausible variations until I find the one that works . In this case , since I knew transform is intended for changing a Series to another Series * of equal length* , I tried transform .
Possibly useful , but this doesn't say how to apply the transformation to the date index ...
On ` groupby ` object , the ` agg ` function can take a list to apply several aggregation methods at once . This should give you the result you need : #CODE
Python Pandas : Groupby and Apply multi-column operation
It doesn't usually make sense to perform ` value_counts ` on a DataFrame , though I suppose you could apply it to every entry by flattening the underlying values array : #CODE
I know about the apply function but it is too easy in my case ..
In the dataframe above I would like to apply the qcut function to B while partitioning on A to return C .
To apply your custom function to each row , use ` apply ` with the keyword argument ` axis=1 ` . #CODE
I have a long dataframe with daily dates starting from 1999 . I apply a filter to the original_dataframe to create a new_dataframe_1 and another filter to create new_dataframe_2 .
You can apply value_counts to the SeriesGroupby ( for the column ): #CODE
Interesting , I will give it a go . I have been looking up normalization on the unicodedata module . I was not sure if that would apply to this situation however . I also discovered from zope.component import getUtility
Hey this is awesome . Thanks for exposing me to the fuzzywuzzy article . I will take some time to check it out and see how I can apply it .
The 5 in the lambda above comes from the correct width . You'd need to select out all the columns that need leading zeros and apply the function ( with the correct width ) to each .
The result obtained , I need to apply the following conditions #CODE
You can make your own aggregate functions to apply to grouped data #URL . So for your case you could try something like : #CODE
yes .... the issue is the Series vector ( e.g. `` df.D > 1 ``) * looks * like it should work , but its ambiguous how it should broadcast , e.g. should that Series named D apply to all of the other columns ( in which case what should it do ? ) , or should it effectively have no name which means it SHOULD broadcast . You problem could also be solved by using `` df > 1.0 `` because I think that is what you intend ( e.g. that it DOES broadcast )
Problem ! Though this works , it ends up being really memory inefficient . I'm working with a 13 million row dataframe , and attempting to run the apply ( sequence_id ) bit ends maxing out the 20gb of ram I have available . I've worked with bigger dataframes , so it must be something to do with this particular operation . Any thoughts on how we could optimize it ?
@USER : You could apply any of the above methods to the Series ` df1 [ ' col ']` and ` df2 [ ' col ']` . For example , @USER ' s answer would look like this : ` pd.concat (( df1 [ ' col '] , df2 [ ' col ']) , axis=1 ) .mean ( axis=1 )` .
you can use apply function : #CODE
very close ! I had to convert dates to ' 12 10 ' instead of ' 12 Oct ' because pandas crosstab alphabetizes and the 3-letter months messes that up . So if I can isolate the two-digit month , I can apply calendar.month_abbr [ ## ] to get that 3-letter month . Thanks !
Why they're being converted like that : I'm not sure . Might be a bug , but it should be simple enough to ` apply ` something keep it all straight .
So how can I apply this to the full dataframe , ffill-ing the observations ( but also the item_id index ) such that each item_id has properly filled rows for all the dates in baseDateRange ?
Essentially for each group you want to reindex and ffill . The apply gets passed a data frame that has the item_id and date still in the index , so reset , then set and reindex with filling .
answer to your first part is yes that is reasonable , you can drop on the reset_index to not have a dup . for the second , I think the apply is a bit confused by the index because of the way you are aggregating ( so it's dropping the name of the index ) . You can do another reset_index inside the apply , then at the very end ( after the apply , `` .reset_index ( drop=True ) .set_index ([ ' date ' , ' item_id '])`` , so reset the mi
It you want to apply these to all your pandas tables you can use css . A ( not recommended ) way is put the following into a markdown cell of the active notebook . #CODE
I need the apply function that returns several value from several complex calculations . I can return those values in a tuple , and thus the outcome of the groupby-apply action would be a Series with group name as indexes and the tuple as values . I would like it to return a DataFrame instead , So I could keep all the pandas functionality and flexibility .
In general , The outcome of a groupby-apply operation would be a Series In the case apply returning 1 value . In the case of apply returning 2 or more values , I would like the outcome to be a dataframe . so my question is how to do that . See the original Q for more details and examples
Based on the edited question , maybe this is what you are looking for . Returning a series in the apply call results being collated into a dataframe ( guessing that is what you are looking for )
That's definitely answer the example I gave , but not my problem . which means that I gave a bad example .. Imagine that I need the apply function to retrieve few complex calculations . A function that is not a built-in function . In the case of apply returning 1 value , the outcome is a series . In the case of apply returning 2 or more values , I would like the outcome to be a dataframe . so my question is how to do that .
On 12+ dev of Pandas .. so I'll try apply ( int ) I still don't get why the Dtype={ does not work on read_csv ?
The second command causes the following error which I do not understand . Any thoughts on what might be going on here ? I replaced map with apply and that didn't help matters . #CODE
Perfect Jeff , thank you ! Once I got rid of the rows with missing data , I could directly apply this to the new problem . ;-)
I'm exploring Pandas - trying to learn and apply it . Currently I have a csv file populated with a financial timeseries data of following structure :
now apply the lambda function , doing what the parser should have done : #CODE
datetime.date creating many problems with set_index , groupby , and apply in Pandas 0.8.1
Now here is what happens when I try to work with these using ` groupby ` and ` apply ` : #CODE
If I save the ` groupby ` object and attempt to apply ` foo ` myself , then in the straightforward way , this also fails : #CODE
I can simplify the problem just to the ` set_index ` call within the ` apply ` function . But this is getting really weird . Here's an example with a simpler test DataFrame , just with ` set_index ` . #CODE
By adding a call to reset the index inside of the function to be applied with ` apply ` , it gets rid of the problem : #CODE
Thanks , that seems to work well . I have some back up questions if you don't mind . 1 ) What is being passed to the function f when it is called with apply ? Is it each groupe of data sequentially ? I assume it must be . 2 ) How can the function be called with multiple columns so people2 = Grouped.apply ( f ( ' a ' , ' b ' , ' c ')) ? Clearly the fucntion would have to be changed , but in your example the function is not very abstract . I would want to write def f ( df , col1 , col2 , col3 ) - so that it could be used beyond the columns referenced inside the function .
+1 , the main part of answer I think is to use apply instead of transform
Would it be right to say then that calling transform passes only the named column , or each column in the DF to the function individually and it is not possible to pass more than one column , whereas apply passes the whole data frame and then column values can be used within the function ? I think that was where I was getting it wrong ...
ok , I think apply as in @USER answer is more appropriate here ?
You're welcome ! If you really want to thank me , look up the functions I'm using there and figure out how each of those things is implemented , play around with them and apply it in your own code :)
did you try to use ` axis=0 ` ? This should be the case since you want to apply the function for each row ...
I just tried it and it didn't work . According to #URL axis=1 is to apply to each row .
I also experienced this error . It turned out that the pandas Time Series data type was causing the problem . When I applied the function with the time expressed in epoch ( or anything ) success , but with the time converted to pandas Time Series , there was this error . So my suggestion would be to convert to Time Series after you apply the function , which obviously is contingent that you don't need your time variable in the function being applied .
* apply function not tested with pandas Time Spans .
You could ` apply ` the conversion on the appropriate column : #CODE
Hi Jeff , thank you for your reply and this is a great idea , but it somehow does not really solve the complete problem . The standard grouping method of " resample " is " how= ' mean '" . Is it defined anywhere how I can change that to something more useful ? I have to apply a custom function . The point is that I also have multiple entries per day and your resample-solution is only correct with the " mean " method when there is only one entry per day .
` argmin() ` is not an agg function , you can use apply to get the closest index of every group : #CODE
Are you trying to apply a function to each row by taking arguments from different columns ? This has already been [ answered here ] ( #URL ) .
EDIT : I originally start with a dataframe that hase one column . I add 4 columns in 4 difference apply steps , and then when I try to add another column I get this error .
what are you actually trying to do ? using apply with a function that returns a list will try to coerce this to a Series , thus it needs the same length as the original lenght , OR a scalar ( including None ) .
Output in your question is not the one you get from apply . Your output in first case is DataFrame with 4 columns , as @USER said , it's coersed list into rows .
@USER I think that the output is the output from apply because apply will run each row through func=random , and that func will print out [ 1 , 2 , 3 , 4 ] . I am not sure what you are pointing out .
I had this issue , and my solution was just to join my list into a string ... then split it after the apply .
and then use apply : #CODE
This doesn't work on my example because there is one line of code that is different . It is missing df [ ' E '] = 1 . I add the column ' E ' and then I do apply . I think that that is throwing it all off . The problem that I am working on starts with a dataframe with one column and then I keep doing apply to the dataframe to add columns . I add 4 columns and then when I try to add a fifth column , I get that error .
EDIT : To clarify , I'm not just doing this for subtracting the mean , it was just a simple example . A more realistic example would be linearly filtering the array along axis 0 . I'd like to use the scipy.signal filtfilt function to filter my array . This is quite easy if I can just pass it a tpts x feats matrix , but right now it seems that the only way to do it is column-wise using " apply "
apply also works on entire dataframes . If you want to subtract 5 ( or an avg number ) from every item in the data frame , you can do that as well by excluding the axis argument .
So , it's not that I want to apply a function to every item in the dataframe , it's that I want to pass the entire contents of a dataframe to a function . For example , the hilbert transform in scipy takes a timepoints x features array , and computes the transform along the first axis . It is faster to pass a 10,000 by 100 matrix to this function than it is to pass 100 separate 10,000 length columns to this function , which is what would happen if I used " apply " . I'm trying to get around this .
Am I stuck with using a for loop to apply the boolean ' and ' operation on all columns ( i.e. from column A to column Z ) ? To reiterate my question in another manner is there an efficient way or built-in pandas function to ' AND ' a pandas Series on all the columns of a pandas dataframe ?
Hi @USER , I am getting ` OverflowError : Python int too large to convert to C long ` when I apply your solution . Any suggestion about the reason ?
My question , then , is how I can accomplish this while reducing my memory overhead . I think the problem is trying to perform the reindexing with the groupby / apply method , but I don't kow what the alternative is . It seems like there should be way I could do something similar iteratively that would require less memory , but I'm not sure how to go about it .
Or you can use apply : #CODE
I now want to apply some aggregate functions to the records in each of my bin groups ( An aggregate funcitn is something like sum , mean or count ) .
Now I want to apply three aggregate functions to the records in each of my bins : the mean of ' col11 ' , the number of records in each bin , and the number of records in each bin that have ' col7 ' equal to one . The mean is easy ; numpy already has a function to calculate the mean . If I was just doing the mean of ' col11 ' I would write : ` dfg = df [[ ' bin ' , ' col7 ' , ' col11 ']] .groupby ( ' bin ') .agg ( { ' col11 ' : [ np.mean ] } )` . The number of records is also easy ; python's ` len ` function ( It's not really a function but a property of lists etc . ) will give us the number of items in list . So I now have ` dfg = df [[ ' bin ' , ' col7 ' , ' col11 ']] .groupby ( ' bin ') .agg ( { ' col11 ' : [ np.mean ] , ' col7 ' : [ len ] } )` . Now I can't think of an existing function that counts the number of ones in a numpy array ( it has to work on a numpy array ) . I can define my own functions that work on a numpy array , hence my function ` count_ones ` .
Now I'll deconstruct the ` count_ones ` function . the varibale ` x ` passed to the function is always going to be a 1d numpy array . In our specific case it will be all the ' col7 ' values that fall in bin #1 , all the ' col7 ' values that fall in bin #2 etc .. The code ` x == 1 ` will create a boolean ( TRUE / FALSE ) array the same size as x . The entries in the boolean array will be True if the corresponding values in x are equal to 1 and false otherwise . Because python treats True as 1 if I sum the values of my boolean array I'll get a count of the values that == 1 . Now that I have my ` count_ones ` function I apply it to ' col7 ' by : ` dfg = df [[ ' bin ' , ' col7 ' , ' col11 ']] .groupby ( ' bin ') .agg ( { ' col11 ' : [ np.mean ] , ' col7 ' : [ count_ones , len ] } )`
I find this helpful to see what is actually passed to the apply , which in this case is a frame #CODE
The above works just fine , but I can't understand why I have to wrap the function in a lambda . Based upon the syntax used with transform and apply it seems to me that the following should work just fine : #CODE
Passing arguments to ` apply ` just happens to work , because ` apply ` passes on all arguments to the target function .
and apply this custom converter #CODE
and apply this conversion function : #CODE
I apply some functions and generate a new column values to a existing column of Pandas dataframe . However ` df [ ' col1 '] = new_list ` does not work to assign new list to the column . Is it the wrong way and what is the accurate way to apply such operation ?
I don't know why the index method has inconsistent behavior while doing column-wise apply function .
And I want to apply lambda to the second columns , it it saying the Series object can not be apply ? #CODE
When you index with `' B '` you get a series . When you index with ` 1:2 ` or with ` [ ' B ']` , you get a DataFrame with one column . When you use ` apply ` on a series , your function is called on each element . When you use ` apply ` on a DataFrame , your function is called on each column .
and then you can use apply ( you don't have to use ` lambda ` , BTW ): #CODE
If you want to apply ` upper ` to DataFrame , you can use pandas.applymap() : #CODE
Here's a groupby way ( and you could do an arbitrary apply rather than sum ) #CODE
Difference between map , applymap and apply methods in Pandas
Can you tell me when to use these vectorization methods with basic examples ? I see that ` map ` is a ` Series ` method whereas the rest are ` DataFrame ` methods . I got confused about ` apply ` and ` applymap ` methods though . Why do we have two methods for applying a function to a DataFrame ? Again , simple examples which illustrate the usage would be great !
Another frequent operation is applying a function on 1D arrays to each column or row . DataFrame s apply method does exactly this :
so using apply is not necessary .
Summing up , ` apply ` works on a row / column basis of a DataFrame , ` applymap ` works element-wise on a DataFrame , and ` map ` works element-wise on a Series .
strictly speaking , applymap internally is implemented via apply with a little wrap-up over passed function parameter ( rougly speaking replacing ` func ` to ` lambda x : [ func ( y ) for y in x ]` , and applying column-wise )
@USER mentioned that apply works on row / columns , while applymap works element-wise . But it seems you can still use apply for element-wise computation .... #CODE
Good catch with this . The reason this works in your example is because np.sqrt is a ufunc , i.e. if you give it an array , it will broadcast the sqrt function onto each element of the array . So when apply pushes np.sqrt on each columns , np.sqrt works itself on each of the elements of the columns , so you are essentially getting the same result as applymap .
Adding to the other answers , in a ` Series ` there are also map and apply .
Apply can make a DataFrame out of a series ; however , map will just put a series in every cell of another series , which is probably not what you want . #CODE
Also if I had a function with side effects , such as " connect to a web server " , I'd probably use ` apply ` just for the sake of clarity . #CODE
In Pandas version 0.13 and greater the index level names are immutable ( type ` FrozenList `) and can no longer be set directly . You must first use ` Index.rename() ` to apply the new index level names to the Index and then use ` DataFrame.reindex() ` to apply the new index to the DataFrame . Examples :
Good improvement . I think `` apply ( ... )`` could be achieved more simply by `` replace ( dir_dict )`` . I haven't tested that , but I think that's how replace works .
You can apply a function that tests row-wise your ` DataFrame ` for the presence of strings , e.g. , say that ` df ` is your ` DataFrame ` #CODE
and then you apply it row by row .
I was able to do this in the DataFrame using a lambda function with map ( lambda x : x.lower() ) . I tried to use a lambda function with pd.series.apply() but that didn't work . Also when I try to isolate the column in series with something like series [ ' A '] should it return the index ( although I guess this makes sense ) because I get a float error even though the values that I want to apply the lower method to are strings . Any help would be appreciated .
You can also do ` groupby ( ..., as_index=False )` , though buggy with apply in 0.12 , fixed in 0.13 .
To add multiple columns , you could use ` groupby / apply ` . Make sure the function you apply returns a DataFrame with the same index as its input . For example , #CODE
Maybe this is not what agg was intended for . Maybe I should be using apply ...
Note : to force the dtype to object ( and have mixed dtypes , ints and floats , rather than all floats ) you can use an apply . I would recommend against this if you're doing any analysis ! #CODE
You can apply this per column , but much easier just to check the dtype . in any event pandas operations exclude non-numeric when needed . what are you trying to do ?
` ix ` index access and ` mean ` function handle this for you . Fetch the two tuples from ` df.ix ` and apply the mean function to it : non existing keys are returned as nan values , and mean ignores nan values by default : #CODE
This answer solves this toy example and will be enough for me to rewrite my actual function , but it does not address how to apply a previously defined function without rewriting it to reference columns .
You can go with @USER example , if it's possible for you to rewrite your function . But if you don't want to rewrite your function , you can wrap it into anonymous function inside apply , like this : #CODE
would calling pd.to_datetime from apply allow for easier parralelization after import or would you still have to manually split up the data frame into N / M parts ( N = num rows , M = num logical procs ) and execute afterward ? I'm really hoping for some of the straightforward parallelization cases pandas gets n_jobs type support that some scikit-learn functions have ( like gridsearch ) .
Use apply : #CODE
How about not calling add_area_column if the DataFrane is emtpy ? ( e.g. Take the ` if ` out of the ` add_area_column ` and put it where you would call ` apply `)
these edge cases for apply are pretty tricky ... to fix your issue , don't use apply : df [ ' width '] * df [ ' height ']
Use ternary operator in apply function in pandas dataframe , without grouping columns
How can I use ternary operator in the lambda function within ` apply ` function of ` pandas ` dataframe ?
select from hdf5 apply function ( e.g. mean )
I want to combine ` MEETING DATE ` and ` MEETING TIME ` into one column . datetime.combine seems to do what I want , however , I need to apply this function column-wise somehow . How can I achieve this ?
perhaps you could ` apply ` the function ( or anyfunction you want ) to MEETING DATE and MEETING TIME #URL
You can use apply method , and apply combine like this : #CODE
call groupby and apply to get the begin and end datetime for every group .
Apply Function Along DataFrame Index
What is the best way to apply a function over the index of a Pandas ` DataFrame ` ?
And after that you can use pandas.DataFrame.apply function , with axis=1 ( means apply function to each row ): #CODE
Thanks but its not working , Its throwing a traceback and which says : AttributeError : DictReader instance has no attribute ' apply ' , is it because i am reading this as a dictonary ?
@USER oh , it will in 0.13 - there was a bug in apply maybe .
This will be significantly more efficient than an apply or using lists ...
If you use ` apply ` on the groupby , the function you pass is called on each group , passed as a DataFrame . So you can do : #CODE
However , this will raise an error if the group doesn't have at least two rows . If you want to exclude groups with fewer than two rows , that could be trickier . I'm not aware of a way to exclude the result of ` apply ` only for certain groups . You could try filtering the group list first by removing small groups , or return a one-row ` nan ` -filled DataFrame and do ` dropna ` on the result .
pandas group by n seconds and apply arbitrary rolling function
The accelerometer data is not uniformly sampled , and I want to group data by every 10 or 20 or 30 seconds and apply a custom function to the data group .
If the data was uniformly sampled , it would have been easy to apply a rolling function .
However , since it is not , I want to apply groupby using timestamp interval .
However , I cannot figure out how to group by an arbitary number of seconds and then apply a function to it .
Or have a look at the resampling-functions here . Maybe you could apply a custom resampling-function instead of using the groupby-method . #CODE
yes , this isn't working ` df.ix [ ' bar ' , ' two ']` . Or wasn't actually , apparently your code would not work if [ ' Trial '] was already set as an indey when it was run . Strangely enough , after I run you code , df [ ' Trial '] no longer works :( which is a pity because I wanted to do this in order to better apply the same function to multiple trials ( I want to downsample all of the measurements in every trial to two - just two ) .
I know pandas has a resample function , but I have no idea how to apply it to my second-level index while keeping the data in discrete categories based on the first-level index :(
if it's relevant that you have Timestamp columns , e.g. you're resampling or something , then be explicit and apply ` pd.to_datetime ` to them for good measure** . #CODE
Now just apply usual pandas transformations and delete unneseccary columns : #CODE
You can apply a specific function to a specific column by passing in a dict . #CODE
you can use apply : #CODE
brilliant ! I'm still looking at what the combination of apply , lambda , pd.Series and stack does , but it works exactly as intended . thanks !
Pandas groupby apply function that combines some groups but not others
I'm using pandas ` groupby ` on my DataFrame ` df ` which has columns ` type ` , ` subtype ` , and 11 others . I'm then calling an ` apply ` with my ` combine_function ` ( needs a better name ) on the groups like : #CODE
Have you tried just using an apply ?
So we can use helper function like this and apply it to each group to get desired results . #CODE
When apply a function to a list , it occurs " TypeError : ' Int64Index ' object is not callable "
I have tried on some simple list like : ` x =[ 0 , 1 , 2 ] , titleNot0 ( x )` . It works . But if I apply the function to the groupby , it returns " TypeError " . Please help me to fix it . Thank you !
maybe it's because when you apply this to a list ` ls [ x ]` returns an integer , when you apply this to a DataFrame , ` ls [ x ]` returns a Series .
Use this with a groupby apply : #CODE
apply a function to a groupby function
I want to count how many consistent increase , and the difference between the first element and the last element , on a groupby . But I can't apply the function on the groupby . After groupby , is it a list ? And also what's the difference between " apply " and " agg " ? Sorry , I just touched the python for a few days . #CODE
The ` apply ` method calls ` foo ` once for every group . It can return a Series or a DataFrame with the resulting chunks glued together . It is possible to use ` apply ` when ` foo ` returns an object such as a numerical value or string , but in such cases I think using ` agg ` is preferred . A typical use case for using ` apply ` is when you want to , say , square every value in a group and thus need to return a new group of the same shape .
The ` transform ` method is also useful in this situation -- when you want to transform every value in the group and thus need to return something of the same shape -- but the result can be different than that with ` apply ` since a different object may be passed to ` foo ` ( for example , each column of a grouped dataframe would be passed to ` foo ` when using ` transform ` , while the entire group would be passed to ` foo ` when using ` apply ` . The easiest way to understand this is to experiment with a simple dataframe and the generic ` foo ` . )
The ` agg ` method calls ` foo ` once for every group , but unlike ` apply ` it should return a single number per group . The group is aggregated into a value . A typical use case for using ` agg ` is when you want to count the number of items in the group .
you could jus use lambda in apply like that :
if you subtract values of a particular cells , there's no difference between agg and apply , they both create a one value for each group #CODE
SQL ( actually , SQL Server ) way would be to use ` outer apply ` : #CODE
I would like to apply it to the " col1 " column of a dataframe similar to : #CODE
@USER : Stay away from ` apply ` ( especially with ` lambda `) if you can help it . It is likely to be the slowest solution available .
Does this apply it to the columns or rows ? I tried df =d f.apply ( lambda col : col.interpolate ( ' linear ') , axis=1 ) , yet it's still not interpolating all the columns .
Normally different columns in a pandas DataFrame contain different type of information , so an interpolation method may not apply or you may need different methods depending on the data .
You can use a combination of groupby and apply : #CODE
Python Pandas : Using Aggregate vs Apply to define new columns
But switching aggregate for apply seems to work .
Why does apply work and not aggregte ?
Good question . Actually , if you define some test function like ` def test ( x ): print x ; return x.sum() ` and call ` aggregate ` in both cases , you'll see that in first case ` x ` is a DataFrame and in second case ` x ` is a Series ( and when you call ` apply ` , it's always DataFrame ) . I don't have time to dig into the code at the moment , and I'm sure some pandas developers will show up and explain this behaviour :)
I have struggled to work out what is going on exactly with these groupby operations . As Roman points out , the first argument passed to agg is a series , therefore if you want to agg based on values in multiple columns you have to call the second column in the function based upon the index values of the series that is passed automatically . apply always passes as data frame as he points out . If you want to see some really strange behaviour check out transform , it seems to pass series and dataframes as the first argument to the function . Quite confusing IMO
@USER Hayden it sounds like the best approach is just to switch to apply or attempt to use cythonized functions when aggregate() fails ? Also , I imagine the groupby code being ' hairy ' doesn't mean you think its unreliable ? Seems to consistently match results I get in SQL . Thanks .
Hm , I can't see other problems in your data besides the lat / long at the moment . You could read the header line and replace lat / long with your approach above , `' LatD ' , ' LatM ' , ' LatS ' , ' LonD ' , ' LonM ' , ' LonS '` , read the file from just below the header , using whitespace as a delimiter and apply the previously read and amended header line as an index to the new dataframe .
Pandas clean column and apply optional multiplier
So , you can apply a function to your data frame to do this ... #CODE
As far as it goes , it looks like ` std() ` is calling ` aggregation() ` on the ` groupby ` result , and a subtle bug ( see here - Python Pandas : Using Aggregate vs Apply to define new columns ) . To avoid this , you can use ` apply() ` : #CODE
Why does function behavior used within pandas apply change ?
Now , using ` apply ` and ` to_integer ` with ` df1 ` : #CODE
But if I apply it to this ` df2 ` : #CODE
make dict to apply same function to all columns #CODE
@USER yeah , mean ` apply ` with ` count ` but you've already added that .
Apply SequenceMatcher to DataFrame
You have to apply a function , not a float which expression ` SequenceMatcher ( None , str ( m.ITEM_NAME_x ) , str ( m.ITEM_NAME_y )) .ratio() ` is .
Update : I realised what you were actually asking , and I think this ought to be an option in sortlevels , but for now I think you have to reset_index , groupby and apply : #CODE
but it tells me there is no attribute ' first ' when I apply the same thing to the index . #CODE
You could use ` groupby / apply ` : #CODE
The exact usage depends on how you wrote your function ` myfun ` . Where the column used is static ( e.g. always ` x `) I write ` myfun ` to take the full ` DataFrame ` and subset inside the function . However if your function is written to accept a vector ( or a pandas ` Series `) , you can also select the column and ` apply ` your function to it : #CODE
the way you are making the dataframe , the other column is index , pandas apply the functions to columns not the index
Your operation doesn't make sense as a DataFrame . The index labels in your expected result don't match up with the labels in the original ` data ` . You'll want to take the Series method and ` apply ` it to each column in ` data ` .
Apply it to your data set : #CODE
I thought apply treated each group as a sub-dataframe , which i can then manipulate and then return . I believe my understanding of the structure is flawed , and I've had trouble finding anything to help correct myself .
After running this function , I was hoping to be able to reaccess each subgroup and perform further analysis on it . But I'm curious about the resulting format . After I perform my groupby function , I can use the describe() function , and it will return a table subindexed by each grouped name , with the statistics . After my apply function , I want to look at the same type of table , but it congests it down to one , with the rows being describe parameters , without the level of group indexing
I think there's some alignment magic that happens at the end ( rather than just a concat ) , often I find groupby apply a dark art .
@USER : I still don't really understand what you're trying to do , but if you want to " perform further analysis " on each group , why don't you just do * that * analysis in the groupby function ? That is , make a function that actually does the analysis you want done , and apply that with ` groupby ( ... ) .apply ( ... )` , so it just returns the results of your analysis .
@USER its answered my question about how to add the data I wanted to my DataFrame without copying it and possibly messing up the order . I'm still not sure why the ` groupby ` / ` apply ` is failing when I do it one way and not the other , but it seemed easier to go after the more general question .
How do we apply a function to an entire group in pandas and python ?
How can we apply a function to an entire group in pandas dataframe in python ? This is the code that I have so far : #CODE
Could you be more specific about how ` magic_apply ` will differ from ` apply ` ? Maybe give an example of ` myfunc ` ?
As @USER points out the " magic apply " is simply called ... apply . It's a groupby method : #CODE
Beautiful .. I thought something like ix index quarter existed just could not apply it !....
Thanks . But how do i apply that formatting to a dataframe ? like --- print dfTotalv3.format ( {0 } { 1 } { 2 : ,. 2f } { 3 : ,. 2f } { 4 : ,. 2f } ) or how could i just format one column by referencing its field name and the format i want
see here : #URL you need to set using : `` df.loc [ row , column ] = value `` , and not chained assignment . In addition , you are better off using a vectorized method or apply if you cannot vectorize .
Use apply : #CODE
Now I apply the cut : ` up3 = up2 [: cut_loc-1 ]` , which should just shorten the ` DataFrame ` . However , when I go to plot it ` up3.plot ( x= ' Field ' , y= ' Moment ' , color= ' red ' , label= ' Up '` I get the error `' numpy.ndarray ' object has no attribute ' find '`
Perhaps not the most fancy way , but you can always ` groupby ` your time frequency and apply a custom function returning what you want .
Then groupby the month frequency and apply the function : #CODE
For example , let's say I'm looking to find and categorize transients using some moving window process ( e.g. wavelet analysis ) or apply a FIR filter . How do I handle the boundaries , either at the end or beginning of a file or at chunk boundaries ? I would like the data to appear as one continuous data set .
Based upon the helpful hints I built a iterator that steps over files and returns chunks of arbitrary size --- a moving window that hopefully handles file boundaries with grace . I've added the option of padding the front and back of each of the windows with data ( overlapping windows ) . I can then apply a succession of filters to the overlapping windows and then remove the overlaps at the end . This , I hope , gives me continuity .
This can very easily get quite complicated . For instance , if you apply an operation that does a reduction that you can fit in memory , Results can simpley be a pandas.Series ( or Frame ) . Hoever ,
Thanks for your insight . I looked at your ENH module . I built a iterator that steps over the files but that allows for data padding on either end . By overlapping the data chunks , I can apply a filter and then cast aside the padding at the end , thus preserving continuity .
You can do this directly with an apply instead of last ( and get the -1th row of each group ): #CODE
But most likely you can do a transform or apply ( depending on what something is ): #CODE
Thanks - the refactor makes sense . Couldn't figure out the transform apply approach when I looked into it at first . I was hoping there was some nice syntactic sugar .
@USER you need something to be a function for transform / apply , worth checking out the docs : #URL
@USER completely depends on the something whether or not you can do apply / transform ! :)
I'll give this the check mark because it's taught me quite a bit . But it still seems odd to me that there isn't a more natural way to do this with groupby . For example , I get weird behavior if I take the original dataframe and try : ` df.sort ([ ' date ']) .groupby ([ ' ticker ']) .transform ( lambda x : x.diff() )` I would have hoped pandas would be able to figure out that it should ignore text columns and then apply the diff function to the numerical columns . In general , is there a way to use a different function per column in ` transform ` ( like you can with ` agg `) ?
This will accomplish everything I want . And what I really like is that it can be generalized to cases where you want to apply a function more intricate than ` diff ` . In particular , you could do things like ` lambda x : pd.rolling_mean ( x , 20 , 20 )` to make a column of rolling means where you don't need to worry about each ticker's data being corrupted by that of any other ticker ( ` groupby ` takes care of that for you ... ) .
Then how do I apply clustering to this to determine a cut-off threshold ?
I think methods with apply are going to need some annoying sorting at the end ... : s
Say my dataframe had two values columns : value_1 and value_2 . I can do : ` diffs_df = data3.groupby ([ ' ticker ']) [[ ' value_1 ' , ' value_2 ']] .transform ( lambda x : x.diff() )` and that works fine . But ` diffs_df = data3.groupby ([ ' ticker ']) [[ ' value_1 ' , ' value_2 ']] .transform ( pd.DataFrame.diff )` blows chunks . So is it correct to say that ` transform ` always operates on a series ( even if it means it has to work on multiple series in succession ) , while ` apply ` works on multiple series all at once as a DataFrame ?
What determines the type of object passed to the function ... is it the # of columns , or is it ` transform ` vs ` apply ` , or is it ` [ ' colname ']` vs ` [[ ' colname ']]` ?
What Pandas data type is passed to transform or apply in a groupby
Is there any way to force transform to pass the multi-column dataframe ( i.e. not the individual series from the columns ) to the function ? I basically want the same behavior of ` apply ( my_func , axis =1 )` but forcing it to return a result with the same index ( i.e. what transform is supposed to do , but rather than working column by column , I want to be able to access multiple columns at the same time ) .
On the Left Hand Side , we get the new column names from the keys of the element of the stats column . Each element in the stats column is a dictionary . So we are doing a bulk assign . On the Right Hand Side , we break up the ' stats ' column using apply to make a data frame out of each key / value pair .
Does such a function exist ? If I build my own function , how can I apply it to the DataFrame columns ?
Pandas apply to data frame groupby
If I groupby ( g object below ) and then apply following function to first 1000 rows of df , it works . But if I apply it to entire df , I get this exception : #CODE
First reset the index , then group and apply . You can recover your original index by then setting the index at the end . The reset index is turned into a column called ' index ' ( which set_index then drops ) .
And there's always ` apply ` : #CODE
In pandas 0.13 ( in development ) this is fixed ( #URL ) . It is for this reason the ` as_index=False ` is used in the groupby call , so the column ` L1 ` ( fow which you group ) is not added to the index ( creating a MultiIndex ) , so the original index is retained and the result can be appended to the original frame . But it seems the ` as_index ` keyword is ignored in 0.12 when using ` apply ` .
@USER Ah , yes , indeed this does also work . But with ` apply ` it doesn't . Do you know what is the difference in this case between both ?
The answer of @USER is indeed the solution to your question , although I think you misunderstand the groupby . You still need to apply a function or aggregation on the ` groupby() ` call , in your case to sum all items in a group ` data.groupby ( .. ) .sum() ` .
I have a dataframe which I want to split into 5 chunks ( more generally n chunks ) , so that I can apply a groupby on the chunks .
then in the apply , do your calculation , which in this case is another groupby . #CODE
For filling the NaNs , you can apply this on all columns in one line as follows : #CODE
Glad I could help ! Most functions on Serieses you can also apply on a DataFrame , and if you can't , you can always apply a Series functions on all columns at once like this : ` df.apply ( lambda x : x.seriesmethod() )`
However I wish to plot create a facet-wrapped histogram , and have each facet share the same xlim . The command I use seems to apply the xlim only to the last of the facets . ( Also you can see that the labels are applied to the last of the facets only ) . Is there a way to specificy a global xlim ? ( And global labels ) ?
@USER apply answer from question then " 1 " the found rectangle and recurse .
If you do not want to add a column to your original DataFrame , you could create an independent ` Series ` and apply the ` groupby ` method to the ` Series ` instead : #CODE
Apply function to sets of columns in pandas , ' looping ' over entire data frame column-wise
I have tried using ` df.groupby ` and ` df.filter ` to loop-over the columns but I cannot really get it to work , because I am not at all sure how I apply effectively the same function to chunks of the data-frame , all in one go ( as apparently one is to avoid looping over rows ) . I have tried doing #CODE
Note : I am a bit frustrated that I needed to thrown in the two transposes . I just couldn't get ` groupby ` and ` apply ` to play nicely with ` axis=1 ` . If someone could show me how to do that , I'd be very grateful . The trick here was knowing that when you call ` groupby ( lambda x : f ( x ))` that ` x ` is the value of the index for each row . So ` groupby ( lambda x : x [ 0 ])` groups by the first letter of the row index . After doing the transposition , this was ` A ` or ` B ` .
Thanks @USER yeah i kind of got your logic to work in my dataset . I am having a couple of problems though . One is I only wanted to get the mean of the next rows that relate to the same group . i.e. i need to apply a groupby to your example in my case it would be by ' Country '
However , when I do ` apply ` , I'm getting dataframes only : #CODE
Internally , ` apply ` and ` filter ` try different ways of looping through the data : a " slow path " that is sure to work for any function , and a " fast path " that only works for some functions . These paths can operate on whole chucks of the data ( as a DataFrame ) or one row at a time ( as Series ) .
Thanks @USER . The problem is , after the ` apply ( f )` , I cannot do something like ` nth ( 1 )` to return the 2nd value of each group . Any ideas ? In other words , the following fails : ` dd.groupby ( ' user_id ') .apply ( f ) .nth ( 1 ) .dropna ( how= ' all ')`
in `` f `` you could do `` x.head ( 2 ) .tail ( 1 )`` to do that ( you could also do `` x.iloc [ 1 ]`` , but if you have a groupsize < 2 that will fail . alternatively , you can do a second group / apply ( `` nth `` is a groupby operation ) , e.g. `` dd.gropuby ( ' user_id ') .apply ( f ) .gropuby ( ' user_id ') .nth ( 1 )``
Hence how do I get my function to apply to the data frame in a row-wise fashion ?
I do step 1 once , then repeat step 2-3 many ( ~100 ) times . In the future I may need to pre-process ` emission ` ( apply ` cumsum ` or other functions ) before computing ` counts ` .
@USER : That's odd . I just tested this on ` pd.concat ([ data ] *1000 )` and found ` str.contains ` to be 2x slower than ` apply ( lambda x : ' Fruit ' in x )` . Perhaps my version ` 0.12.0-933-g281dc4e ` is too old ?
And then set it as the index , groupby on ` [ ' Code ' , ' ID ']` and then apply a ` resample ` on each group : #CODE
in a pandas dataframe how can I apply a sort of excel left ( ' state ' , 2 ) to only take the first two letters . Ideally I want to learn how to use left , right and mid in a dataframe too . So need an equivalent and not a " trick " for this specific example . #CODE
For last two that would be ` df [ ' state '] .str [ -2 :] ` . Don't know what exactly you want for middle , but you can apply arbitrary function to a column with ` apply ` method : #CODE
I have another example where i am try to apply the first two digits of an 8 digit number . then i get the error . ' invalid index to scalar variable ' how can i apply the above to take the last 2 numbers in ' year ' ?
Basically , there's no need to apply the set of functions to the two groups separately and append the results together . That's essentially what group by is doing : split , apply ( separately ) and combine .
I have tried to look at Pandas ` apply ` and ` groupby ` methods , but can not come up with something that generates the desired overlapping groups .
thanks but how do I apply that to a dataframe .
when apply this concept to my larger working file I seem to lose the order of my columns . so month comes before product and month10 comes before month2 etc . why would this happen or rather how can i avoid losing my shape ?
" Google or learn " is a generic attitude that could apply to all forums , especially Q& A-style forums . However , I'm obviously here for a reason . " Vote Down requires 125 reputation . "
Perfect for what I wanted . I just needed to understand how to apply the string operations to the dataframes / series , but I guess that's as simple as it is for strings . For row [ 0 ] - row [ 0 ] .capwords() should solve several problems at once , I expect !
I have tried ` apply ( lambda x : set ( x ))` but it only works on individual lists as opposed to the entire column .
this will raise starting in 0.13 , you need to `` apply `` to your groupby to get back a dataframe , see : #URL
I would like to use the ` pandas.rolling_apply ` function to apply my own custom function on a rolling window basis .
I have run into , what I think , is a fairly simple problem yet again . I would like to apply the following function to a pandas data frame . #CODE
In general , it's better to avoid looping over your frame's rows , if you can avoid it . If I understand your problem correctly , you want to look at a single column from your frame , and apply a function on each element of that column . Then you want to put the result of all those function calls into a column of the original frame . Maybe a new column , maybe in place of the old column . This sounds like a job for ` pd.Series.map ` . #CODE
And now , instead of ` value_counts() ` , use ` apply ( f )` . Here is an example : #CODE
Furthermore , you normally don't need to ' iterate over the df ' as you do here . To apply a function to all groups , you can do that directly on the groupby result , eg ` df.groupby() .apply ( .. )` or ` df.groupby() .aggregate ( .. )` .
Can you give a more specific example of what kind of function you want to apply to the ratios ?
` data [ ' ID ']` will give you the ` ID ` column , so you cannot use it as a key . You want one specific value of that column . To apply a function on each row of a dataframe , you can use ` apply ` : #CODE
You can apply an ` expanding_mean ` ( see docs ) to each group : #CODE
I tried using " apply " but I can't figure out how to return a correct data frame . For example #CODE
Apply that function to the rows : #CODE
The problem is that you are concatenating a pandas Series ` Fx [ ' File ']` with the string representation of a pandas Series ` str ( Fx [ ' Date '])` , what you need to do is apply the ` str ` cast function to the elements of ` Fx [ ' Date ']` like this : #CODE
First , groupby code and colour and then apply a customized function to format id and amount : #CODE
Apply read_csv instead of read_clipboard to handle your actual data : #CODE
I guess you are assuming that you can select stuff within the groupby object . As far as I know , you can't . But you can do that in the resulting object , after you apply your aggregation function .
I'd like to show the scatter plots with data points for one group of data , let's say , in green and the other group in red in the very same scatter matrix . The same should apply for the density plots on the diagonal .
How to apply hierarchy or multi-index to panda columns
You never mentioned you had single unpaired Series ( es ) instead of a DataFrame . You need to provide an example that's faithful to your context so that our solutions apply . Can you re-state your problem ?
Groupby ` Group ` first and then apply your customized function : #CODE
@USER Not sure I understand your point . Do you mean that when you use an apply on a groupby mapping , you can treat the object received by apply as a portion of the original df ?
@USER , yes . Series.apply() will apply a function to every element of the Series . Every object is a portion of the entire Series . Groupby will apply a function to every portion of the DataFrame that matches a value in the grouping Series ( es ) . Together all groups make the whole DataFrame .
If you wanted this as columns as days of each week , you could do the groupby within the apply : #CODE
You could use a groupby apply for this : #CODE
Also using python 2.7.5 and pandas 0.12.0 . Also worth mentioning I would like to apply this to datasets of up to 1 million rows . Forgot to mention this !
@USER : Using ` apply ` to do this with a ` lambda ` expression is a worse solution than using the builtin ` idxmin ` directly on the transposed data . For one , ` idxmin ` automatically skips NaN . For two , ` idxmin ` is already optimized to function as an array operation , whereas your ` lambda ` incurs the cost of a function call across the rows , needlessly . For three , relying on the Pandas API preserves the modularity and readability of the code . Reading that ` lambda ` is needless extra work for anyone using your code . For four , ` idxmin ` is already tested and documented , whereas the ` lambda ` isn't .
python pandas : apply a function with arguments to a series . Update
I would like to apply a function with argument to a pandas series : I have found two different solution of SO :
python pandas : apply a function with arguments to a series
Passing multiple arguments to apply ( Python )
if you're using pandas and assuming the date are in datetime format , you can group by on ' code ' and then apply a min , max function to that .
@USER thanks ! So ` apply ` gets called once for the column or once per element ? ` x.astype ( float )` is the whole column or just one element ? and ` x.sum() ` is the group by group sum ? How do these rules work ? :)
I just can't make it , no matter what I try ( I tried ` apply ` with ` axis=1 ` and have it return a tuple , a list , a Series object .. neither worked ) .
I saw that I can create a DataFrame and set the dtype to ' object ' and then I can put tuples in a cell . How do I do it with ` apply ` ?
@USER you could do a groupy apply to return just those rows ...
One way to get the desired result is to use an apply e.g. via the following function : #CODE
Basically you just have the function that does ` row / row.sum() ` , and you use ` apply ` with ` axis=1 ` to apply it by row .
Another option is to use div rather than apply : #CODE
The problem with the first approach is that I have no way of accessing my categorical data ( i.e. the ` subject ` , ` stimuli ` , and ` resp ` columns , amongst others I've left out here ) , while the problem with the second is that I end up with a ` DataFrame ` thousands of columns wide ( and wider again for each transformation I apply : velocity at each step , angle at each step , etc ) , and no useful way of accessing specific time serieses ( i.e. what I've been currently calling as ` data.rx.mean() .plot() ` .
Apply numpy functions to pandas DataFrame
I have a DataFrame where each element is a numpy array and I would like to apply to them numpy functions .
now let's try to apply ` np.dot ` along ` axis=1 `
Why doesn't my apply function return the length of the string ?
Just for the sake of trying something , does the same error happen if you replace your use of ` apply ` with ` map ` since you're looking to spray the operation onto a single ` Series ` object ?
and you can also use ` map ` instead of ` apply ` since you're operating along the values of a ` Series ` .
Is the issue that result of [ 11 ] is set not a list ( just apply set to it ) . Atm this feels like the [ XY Problem ] ( #URL ) ...
Now , I ` apply ` it to the dataframe : #CODE
The thing you have to realize about apply is you need to write functions that operate on scalar values and return the result that you want . With that in mind : #CODE
Just want to clarify that when using ` apply ` on a series , you should write function that accept scalar values . When using ` apply ` on a DataFrame , however , the functions should accept either full columns ( when ` axis=0 ` -- the default ) or full rows ( when ` axis=1 `) .
OK - I think I just figured it out - to use functions on a dataframe , you have to use ( should use ) apply . So , I can chain together functions by using apply inside of the main function . Is that right ( does that make sense ) ?
It's worth noting that you can do this ( without using apply , so more efficiently ) using ` str.contains ` : #CODE
I also have some states which are not present in the dictionary . If I apply map() , the corresponding values in the new series are missing . Can I somehow specify that I want to apply identity function for the values not present in the dictionary , i.e. leave them as is ?
If you can have a NaN in the FK column , then could you replace the NaN's with some other random number , let's say if all current FK's are positive integers , then use a negative int like -999 to distinguish empty values . Another option is to only include the rows that have a value : " df [ df [ ' FK_COL '] .notnull() ]" . Then apply the filling on NaN values , and save this dataframe to a new variable for loading into the database .
using apply you can make new value and assign it to new column
E.g. you could use groupby apply with ` def f ( x ): return ( 1 . * x [ ' weight '] * x [ ' jobs ']) .sum() / x [ ' jobs '] .sum() ` but it will probably be less efficient than the above .
You could do this as a one line apply ( the first column being negative , the second positive ): #CODE
then use apply to get your " Type " #CODE
original Apply : #CODE
revised Apply : #CODE
This will be faster than the apply soln ( and the looping soln )
Apply #CODE
An easier way to describe your function is as x -> 1 - x , this will be more efficient that apply / map . #CODE
I am looking to ' smooth ' regularly-sampled 30-sec time series data using the pandas ` rolling_window ` function , with a window type other than ` boxcar ` - ideally ` hamming ` . However , so far all windows which I have tried to apply , over varying window lengths from 2 to 100 , appear to offset the smoothed data to lower values , e.g. :
To verify correctness apply the rolling window on a step function . If there were an offset , it would show up
However their IS a way to do this . Here is the sketch . Use ` select_as_coordinates ` to actually execute your query ; this returns an ` Int64Index ` of the row number ( the coordinates ) . Then apply an iterator to that where you select based on those rows .
This is the function you'll apply to each of the lists in ` groups ` . Just like before we hand of the pair to ` SequenceMatcher ` to get the ratio . Only now we need to keep the name around . So in that function ` x ` is a tuple like ` ( ' maria ' , ' mary ')` . We need to know the name in the best match and the ratio of the best match , so I threw them in a dict with ` {name : ratio} ` . The other thing here is that ` max ` takes a second argument . This time it's just saying the thing to maximize is ` x [ 1 ]` , the ratio .
You don't need to if you simply pass a `` min_itemsize=40 `` ( or whatever number is ' big enough ') , this will apply to all object columns , alternatively , you can use : `` df.dtypes `` to see which are object ( the values are the dtype )
Why do pyplot methods apply instantly and subplot axes methods do not ?
I'm editing my graphs step by step . Doing so , ` plt ` functions from ` matplotlib.pyplot ` apply instantly to my graphical output of pylab . That's great .
Apply function to a specific number of rows in a DataFrame
I have a weather data and I would need to apply a function to a specific number of rows . For example , to calculate mean values of every 10 or 15 rows . The number of rows is important because there are quite many missing values in dates and I don't want to rely on it .
I tried ` groupby ` but there I can only specify hours or minutes . Anyway I would like to apply any function independent from ` DateTime index ` .
I think slicing ` DF ` would be an option ` df [: 9 ]` but I don't know how to apply this to all rows ?
Also what is the function you want to apply ? Does ` df.resample ( ' 10min ' , how= )` work ?
4 : I apply the transaction level criteria to the datafarme
I want to apply the expanding mean , such that ` ptsA ` and ` ptsB ` for each player get counted in ( and are not left ) to the net result . Final output should make it more clear : #CODE
Python pandas groupby object apply method duplicates first group
I am confused about this behavior of apply method of groupby in pandas ( 0.12.0-4 ) , it appears to apply the function TWICE to the first row of a data frame . For example : #CODE
Then I try to do something similar using apply on the groupby object and I get the first row output twice : #CODE
Edit : @USER provides the answer below . I am dense and did not understand it immediately , so here is a simple example to show that despite the double printout of the first group in the example above , the apply method operates only once on the first group and does not mutate the original data frame : #CODE
This is checking whether you are mutating the data in the apply . If you are then it has to take a slower path than otherwise . It doesn't change the results .
@USER : Could the result of the first call be saved so it is not called again ? This might help if the function called by apply takes a long time ... ( along with being more intuitive , since this question comes up a lot . )
The ` apply ` function needs to know the shape of the returned data to intelligently figure out how it will be combined . To do this it calls the function ( ` checkit ` in your case ) twice to achieve this .
Depending on your actual use case , you can replace the call to ` apply ` with ` aggregate ` , ` transform ` or ` filter ` , as described in detail here . These functions require the return value to be a particular shape , and so don't call the function twice .
What can I do if I have several DataFrames and I want to apply the same set of operations to each with operations that do not support ` inplace=True ` ? Is there a way to change the original DataFrame in a for-loop ? For example , instead of ` df_train [ df_train > 1 ] = 1 ` and ` df_test [ df_test > 1 ] = 1 ` iterating over the two frames and changing the content of the DataFrames in the for-loop .
I think a nicer way to do this , assuming you were planning on apply it to an entire column , is to use one of the vectorised string methods : ` str.split ` : #CODE
You can group and apply an user-defined function : #CODE
Are you trying to apply two different types of equations based on the value in serialNumber ?
After the merge between the object_list and percentages , you could " query " the dataframe based on the value in serialNumber and apply the correct formula ; #CODE
The apply function is similar to pythons builtin " map " . You can ' apply ' the same function over the rows or columns ( where axis=1 is for row-wise [ top to bottom ] where the indexes will be the column names , and axis=0 is column-wise [ left to right ] where the row indexes are the indexes )
Apply function to pandas Series with argument ( which varies for every element )
I have a pandas Series and a function that I want to apply to each element of the Series . The function have an additional argument too . So far so good : for example
python pandas : apply a function with arguments to a series . Update
I had to face this problem in my code and I have found a straightforward solution but it is quite specific and ( even worse ) do not use the apply method .
That's an unusual broadcast rule , and not one that will be widely desired . So the Pandas API doesn't directly handle it for you . Your best bet would be to write a function that maps the ` t ` vector into a correctly-sized column in the data frame , using whatever mapping convention you'd like , and after that is created , * then * you can just use a simple ` apply ` or ` map ` or basic array function to operate on them . But you shouldn't want Pandas to support arbitrary ways of broadcasting elements . That interface would be so wide open it would necessitate that the data structure was meaningless .
can you show `` data.info()`` before this ? you should have `` float64 `` dtypes already . secondarily , you don't need the apply , you can do something like : `` data [ data [ ' currency '] ! = ' A ' , ' amount '] =d ata [ ' qty '] *data [ ' rate ']``
Apply function then Filter DataFrame
You can [ groupby an index level ] ( #URL ): ` site.groupby ( level= ' DK ' , axis=1 )` , and then iterate through that like a normal groupby object . It may be cleaner to use and ` apply ` after grouping , instead of iterating over the groups .
I am struggling with indexing and bools but i can't solve this . I strongly suspect that i need to use a lambda function , but i don't know how to apply it . So please have mercy it's too long that i'm trying on this . Hope i've been clear enough .
You would need to use ` applymap ` instead of ` apply ` to do it that way . But more generally , working with lists inside DataFrames can be somewhat awkward , and working with columns where some values are lists and some are numbers is also likely to be awkward .
You can apply your values to your matrice by looping through the list with values that you
want to apply and append them to the matrice in the loop . #CODE
how to apply a function which takes the caller as its arugment
When calling apply , add group keys to index to identify pieces ` .
Reindexing error makes no sense does not seem to apply as my old index is unique .
Probably better to split your data using regexp and then apply some date parsing using strptime IMO , I can't think of an easier method
Use ` apply ` for row-wise methods : #CODE
Well , it does select the number of rows I want but I can't apply ` size() ` method on this new object . #CODE
I don't want to redefine each and every method of a class to pass through . This is just one example . I'd like to apply this elsewhere .
`` df == DataFrame ( np.tile ( rowmax , len ( df )) .reshape ( df.shape ) .T , index =d f.index , columns =d f.columns )`` will get your boolean frame ( kind of like a broadcasted comparison operator ); faster , but prob not more clear than the `` apply ``
This means we don't easily know how to build our grouping . It would be much better to just operate on the first level , but then I'm stuck on how to then apply the grouping I actually want . #CODE
I think you are doing a row / column-wise operation so can use ` apply ` : #CODE
@USER Thank you , I understand the difference and see that both selections are not the same ; this can be confirmed using the ` is ` operator for comparison . But is there a simple rule ? The documentation looks rather complicated in this point . How far does the following statement apply to one but not the other select ? " Whenever an array of labels or a boolean vector are involved in the indexing operation , the result will be a copy . " In both versions ` df [ " col "] == 3 ` is a boolean vector for selection , but in the former version the condition is used first ; in the latter the Series is selected first .
Why does pandas apply calculate twice
I'm using the apply method on a panda's DataFrame object . When my DataFrame has a single column , it appears that the applied function is being called twice . The questions are why ? And , can I stop that behavior ?
Also , calling it four times when you apply on the column is normal . When you get one columnm you get a Series , not a DataFrame . ` apply ` on a Series applies the function to each element . Since your column has four elements in it , the function is called four times .
doing a nested a nested apply / grouping like this is not the answer . This is almost pure python code , you are not leveraging any of pandas strengths . You prob want to groupby at the top level ( or construct a multi-index ) , select the values that you want to include , then use a cythonized function to apply it . You only want to do 1 level of groupby and apply ( except in some very very rare cases ) . You are ultimately are doing some vectorized operations , but you are doing them backwards at the lowest level .
You can do the apply and groupby by one multilevel groupby , here is the code : #CODE
Pandas : how to apply function to only part of a dataframe and append result back to dataframe ?
I could then figure out how to append these lists back to the original list ( though I have no idea how to go from a dataframe to a list again to do this ) . Is there more of a direct way to only apply the function to the numeric variables ? Also , how would you change the pandas dataframe back to its original list form anyway ?
You can use ` apply ` and use a lambda to subtract the list values column-wise : #CODE
I was trying to remember what the correct / better method was and couldn't remember this one so i posted ` apply ` as an answer .
Ed , in this case , ' sub ' was what I was looking for , but I'm certainly keeping the ' apply lambda ' method in my back pocket -- the next problem in queue isn't a straight subtraction . Thanks !
I want to apply filters based on the following pattern from ' CAT1 ' ' CAT2 ' ;
You could just define a function and pass this to ` apply ` and set ` axis=1 ` would work , not sure I can think of an operation that would give you what you want
Then apply it to your dataframe passing in the ` axis=1 ` option : #CODE
Trying other user-defined functions produces similar errors . In all these cases , it's pretty clearly trying to apply peak_to_peak() or np.mean() ( or whatever ) to the ( subsets of the ) ' key2 ' column from df , whereas for the built-in methods and predefined functions , it ( correctly ) ignores the ' key2 ' column subsets .
Also check out #URL which covers ` apply ` , ` map ` as well as ` applymap ` .
I would like to maintain the data types from the original data frame as I need to apply other operations to the total row , something like : #CODE
That's a little too complex to discuss without something concrete to work with . I suggest you make some short toy examples and open a new question . Or see if [ this old answer of mine ] ( #URL ) gets you close enough . You'll have to split your sentences into columns of words and then apply .
That article used ` id ` s to achieve different formatting for each table but you could just apply the desired CSS styles directly to the appropriate HTML tags . For instance , your image is from their " Box 3 " example , which used the ids ` box-table-a ` and ` box-table-b ` and the corresponding formatting is
Some of these are documented [ here ] ( #URL ) , but some are missing . However , it looks like many of the missing ones are fairly clear because they just apply a mathematical function of the same name to each group ( e.g. , ` cummin `) .
these ultimately just call the same named DataFrame method ( or an optimized for groupby version ) ( asside from the specific methods `` transform / apply / agg / groups ``)
You'll need to mess with the ` b ` column to get things flipped . I'd say multiply by -1 , apply the sub and div , then multiply by -1 again .
If I find a more elegant way ( for example , using the column index : ( 0 or 1 ) mod 2 - 1 to select the sign in the apply operation so it can be done with just one apply command , I'll let you know .
Using the apply function , we can compare entire columns to the empty string and then aggregate down with the ` .all() ` method . #CODE
Can't figure out how to apply the solution to my code ,, when I try to just run the code as provided I get module not callable and when I try to place the code in the " with " clause it blows up .. ( so I have not translated that correctly . ) I tried to just use the Family=clause inside the " with " gives KeyError : 0
The ` converters ` parameter tells ` read_csv ` to apply the given
Appreciate the quick answer @USER . However , I seem to get conversion error when trying to apply the .loc .
Moreover , as this is just an example , do you think there is a way to apply the " *= -1 " to all columns , as ' 1Y ' : ' 1Y4M ' ?
What is the difference between pandas agg and apply function ?
Using ` apply ` : #CODE
` apply ` applies the function to each group ( your ` Species `) . Your function returns 1 , so you end up with 3 groups .
So for all 4.000 locations you apply the distance function to all 11.000 towers ? That seems rather wasteful , as most towers are * not * near . You could already greatly reduce the work by binning all the towers on certain lat / long combinations , such that you only have to iterate over a small subset of all the towers to find your best match .
Does this mean that it recognizes each string as a word ? so when I apply the filter it will filter against each of the words in my List2 ? #CODE
I usually read everything as string , then apply a helper function that includes try / except to convert each individual string . Or you can validate your strings with regex , and substitute all the values that aren't a date with '' .
I have the following problem , I have a Panda data frame and I want to process each row ny using the apply method . Each row should be processed by using a function ( static method ) within the same class .. #CODE
You can group by `' Currency '` and apply ` diff ` but first you need to convert the data to ` float ` , try this : #CODE
Hi again @USER . Your code-update is very much appreciated . However , when applying " diff " to my groupby-function , it seems that Python mess up the original order of the currencys . What's probably happening is that when I apply groupby currencies , Python also SORTS the data accordingly -> so e.g. if my original data is stored in order " EUR , CHF , DKK " the diff-command makes the data " CHF , DKK , EUR " , i.e. when putting back the currency-labels , they obviously will be mis-labeled . Is there a way to maybe tell Python , * not * to order by currency , but leave the order as is ? Thanks .
It could be an groupby's apply will do it , like I say in other question , depends what you're doing !
Note : most of the time you don't need to do this , apply , aggregate and transform are your friends !
I think it may be simpler to use an apply here : #CODE
@USER this seems to work with apply : s ( Thanks for editing , makes it much easier , if I hadn't already +1d , I would again ! ) :)
When you do an apply each column is realigned with the other results , since every value between 1 and 5 is seen it's aligned with ` range ( 1 , 6 )` : #CODE
When you do the apply , it concats the result of doing this for each column : #CODE
I would then like to apply some vba formatting to the results - but i'm not sure which dll or addon or something I would need to call excel vba using python to format headings as bold and add color etc .
Doesn't ` apply ` call my lambda function , once for each column ?
Use ` where ` instead of ` apply ` and add days with ` np.timedelta64 ` #CODE
Just put your code in a function an use ` apply ` : #CODE
If I just group the object and apply the interval function , it looks like this : #CODE
Apply custom function to the temporary column
But I cannot work out how to use the group by functions ( transform , apply , etc ) to achieve the same result . How can I do this in a concise way using pandas ?
@USER it's the for loop which is slow , as is apply ( to a lesser extent ) . Although it's possible this could be made faster !
Apply then calls the function on each group and assimilates the results
The result of apply is a list of index values that represent rows with B == 1 if more than one row in the group else the default row for given A
The data is sparse but I do need them . I am gonna try to apply on a small set of data first . Thanks a lot !
Is there a vectorized way to apply that formatting command in either context ?
Ok , then how would I apply that in my case ?
You can just apply this to each case / group : #CODE
how to apply a function to multiple columns in a pandas dataframe at one time
Question : 1 - what if I have a dataframe with 50 columns , and want to apply that formatting to multiple columns , etc column 1 , 3 , 5 , 7 , 9 ,
Is there also any way to programatically create that string ( which would change depending on the number of columns you had ) and apply the format_number function ? I.e. the above would work fine if I knew exactly how many columns were in the sheet every time , but If I didn't know the number of columns , and wanted to apply the same function to every column , is there a better way of doing it ?
@USER : If you just want to apply it to all the columns , just do ` df.applymap ( format_number )` .
You could use ` apply ` like this : #CODE
@USER ignoring my code example , if you perform ` apply ` to a dataframe then the dataframe itself is modified by any changes in your function so you would not need to assign to the column , you may still need to depending on what your function is doing . The point being that you just need to call ` df.apply ` and not need to say do ` df [[ ' col1 ' , ' col2 ' , ' col3 ']] =d f.apply ( lambda row : format_number ( row ) , axis=1 ))` , in my code the assignment is done by the ` format_number ` function so I guess the assignment is implicit rather than explicit like BrenBarn's answer
Alternatively you could simply use the apply function on all rows of df . #CODE
and hence I can apply ` timedelta64 ` conversions . For microseconds #CODE
You can go by using the power of apply function : #CODE
You can use ` apply ` and test your column like so : #CODE
what would be the most efficient way to use groupby and in parallel apply a filter in pandas ?
Python -- Pandas : How to apply aggfunc to data in currency format ?
I have a table above . Want to apply groupby function to the data and apply sum ( over revenue_total ) . Pandas gives an NA value since revenue_total is an object data type . Any help #CODE
Apply Different Resampling Method to the Same Column ( pandas )
I have a time series and I want to apply different functions to the same column .
I have 2 pandas data frames ` df ` and ` df_min ` . I apply some filters to ` df ` , which results in a single row of data , and I'd like to append that row to ` df_min ` . I tried using a loop to traverse ` df ` , and tried using ` loc ` to append the row to ` df_min ` . I keep getting a ` Incompatible indexer with DataFrame ` ValueError for the line where I use ` loc ` . I guess I am not using ` loc ` correctly . What would be the best way to accomplish what I am trying to do ? #CODE
Pandas how to apply multiple functions to dataframe
Is there a way to apply a list of functions to each column in a DataFrame like the DataFrameGroupBy.agg function does ? I found an ugly way to do it like this : #CODE
Assuming these were datetime columns ( if they're not apply ` to_datetime `) you can just subtract them : #CODE
But when I apply it to the DataFrame , I get an error . #CODE
Thanks to @USER for pointing out the sweet apply syntax to avoid the lambda .
Thanks , Andy . Apply actually passes the values correctly , so you don't need the lambda . I edited the answer to use the simpler syntax , and avoid the lambda .
Then apply a lookup from ` df2 ` to each element of the lists in ` vals ` : #CODE
Note : that's a really bad way to iterate over the rows , either use iterrows or apply . Using range like that creates a huge python list ( in python 2 ) , xrange is slightly better .
I believe that you are operating on copies of the dataframe . I think you should use ` apply ` : #CODE
I don't think so map operates on each element in a series , if you want to pass do something with multiple arguments on a row-wise basis then you could use ` apply ` and set ` axis=1 ` like so ` mn.apply ( lambda row : getTag ( row ) , axis=1 )` in ` getTag ` you can select the columns like so : ` row [ ' fld1 ']` and ` row [ ' fld2 ']` . This should achieve what you want
And to answer the general question : Yes , there is a way to pass extra arguments -- use apply instead of map ( Thanks to Andy Hayden for pointing this out ): #CODE
Still , I don't recommend using ` apply ` for this particular problem since ` pd.cut ` is be faster , easier to use , and avoids the non-deterministic order of dict keys problem . But knowing that ` apply ` can take additional positional arguments may help you in the future .
How to apply a long set of conditions on a pandas dataframe efficiently - stock backtesting
I'm attempting to apply a long set of conditions and operations onto a pandas dataframe ( see the dataframe below with VTI , upper , lower , etc ) . I attempted to use apply , but I was having a lot of trouble doing so . ) . My current solution ( which works perfectly ) relies on a for loop iterating through the dataframe . But my sense is that this is an inefficient way to complete my simulation . I'd appreciate help on the design of my code . #CODE
Generally speaking , you need to assess the way in which the desired result for a given row depends on the data that is " higher up " in your set . If a given row's output can be created based on * input * data in higher rows , you can save yourself some time and effort using something like ` apply ` . But if your desired output for a given row depends on the * output * from earlier rows , then your problem is inherently ordered and as a result , you're unlikely to be able to do much better than a ` for ` loop even if you wind up with cleaner code .
How do I apply a count function ? like how= ??? ) Hmmm , I figured out , it's np.size
You can [ set up a progress meter for apply ] ( #URL ) , but this obviously slows down whatever it is you're doing . Generally a bad idea to return different types of data in an apply ( here a string or a Series ) , it's unclear what you want the apply to return ...
@USER Your comment suggests to me that I may not understand ` apply ` properly . My understanding was that my function would return the string ' RARE_VALUE ' if the condition were met but keep the existing string / null if it weren't . Is this incorrect ?
Ah wait , I see what you're saying , I mistook this for a DataFrame apply . No you're correct , but boolean masking at each step is ** slow** !!
Or , if these are numpy arrays you need to apply tolist to each item first : #CODE
I have a DataFrame with multi-index [ ' timestamp ' , ' symbol '] that contains timeseries data . I merging this data with other samples and my apply function that uses asof is similar to : #CODE
pandas groupby apply function that take N-column frame and returns object
Is there a ' transform ' method of something like that to apply a function to groups ( all columns at once ) and return an object ? Anything I try seems to return one object per column in the group .
and suppose I do a groupby on Date and apply some function to the groups labeled by ( Term , Month , s ) . The result should be something like #CODE
You could apply the function and then aggregate each group manually . For example , assuming the aggregation is a mean and the function is the sum of the column , you could : #CODE
Then we can apply the fit() on all the columns at once for each group of rows : #CODE
I should have mentioned that I can use the ' apply ' or ' agg ' function but it returns the same ( redundant ) object for each column . So it appears that my function is receiving access to the full group each time ( which is what I want ) but that it is being called once for each column in the group ( or at least that's how the result is populated ) .
Hmmm , I think you are correct for the usage of ` agg ` , but the ` apply ` method should normally receive all the columns at once for each group . I edited my answer above to illustrate that .
You can convert all elements of id to ` str ` using ` apply ` #CODE
You really want to return a bool to indicate rather than a string . Also avoiding apply where possible .
The data is a bit distorted that is why when I initially read with pd.read_csv , the column is mixed with string and int . It is my wish to remove the string part to apply aggregation functions on that columns .
My basic question is can I efficiently apply this structure to HDF ? Specifically :
apply hierarchy or multi-index to panda columns
Honestly I'm lost as to how to do it . Do I need to append data from the original reviews back onto my sorted dataframe ? Do I need to make a function to apply onto the groupby function ? Tips or suggestions would be very helpful !
As DanB says , groupby() just splits your DataFrame into groups . Then , you apply some number of functions to each group and pandas will stitch the results together as best it can -- indexed by the original group identifiers . Other than that , as far as I understand , there's no " memory " for what the original group looked like .
Instead , you have to specify what you want to output to contain . There are a few ways to do this -- I'd look into ' agg ' and ' apply ' . ' Agg ' is for functions that return a single value for the whole group , whereas apply is much more flexible .
Suppose you want to return a dataframe of the first and last review by each reviewer . We can use ' apply ' , which works with any function that outputs a pandas object . So we'll write a function that takes each group and a dataframe of just the first and last row :
Pandas has a lot of built-in functionality to apply functions in a vectorized way over Series and DataFrames . When that fails , you can use ` map ` or ` apply ` . Here ` map ` will applies a function element-wise on a Series .
For more on map and apply , see this answer .
This is better than having multiple DataFrames , because you can apply fast numpy / pandas operations to the entire DataFrame whereas , if you had a list of DataFrames you would be forced to use a Python loop to operate on the sub-DataFrames individually ( assuming you want to perform the same operation on each sub-DataFrame ) . Doing so is generally always slower .
I know I can do it by converting the DataFrame to a list of lists , or by using a row-wise ` apply ` to grab each item one by one , but isn't there any way to do it without that amount of overhead ? How can I do the equivalent of ` Series.map ` on multiple columns at once ?
My timing results are somewhat different from yours . I get 900us for the MultiIndex solution and only 90us for your ` apply ` . But that ` apply ` doesn't actually do the indexing ; timing ` d.apply ( lambda r : s.ix [ r ] , axis=1 )` gives a much slower result of about 5.5ms . However , I thought of another way : ` d.apply ( tuple , axis=1 ) .map ( s )` . This seems to be even faster at about 580us . Even so , doesn't it seem like there should be a built-in way to do this without creating a new data structure ? The values I want to index with are already sitting right there .
If it is a DatetimeIndex the apply won't work .
the date_format argument does not apply to timedelta dtypes . Easist to simply convert them to strings first , e.g. `` df [ timedelta_field '] = df [ ' timedelta_fields '] .apply ( str )``
You can do an apply on the LgRnk column : #CODE
I found this presentation , which is going about SQLALchemy and GeoAlchemy2 . And where it is mentioned that it support PostGIS Raster as well . It seems to be very interesting ! But using the documentation I don't see how I can apply this to Raster data
Using apply , as in ` df.x.apply ( tuple , axis=1 )` will work , but then I'd somehow need to iterate over the first level of the index . ` df.x = df.x.apply ( tuple , axis=1 )` sort-of works , but the index is still unchanged ( i.e. still has two levels ) .
Now I also have a function ` func ` that takes ` id , group ` as input . I would like to apply ` func ` to each ` id , group ` in the groupby object . Currently I use a loop : #CODE
Is there any better ( faster ) way of doing this using an apply or similar ?
You can use ` apply ` on the groupby object to apply a function to each group . Since the function will need to accept the group as its argument , you can use the following : #CODE
Using pandas built in functions you would have to apply ` notnull() ` over all series and then call a numpy function to the DataFrame anyway .
And can you apply it to selected columns , instead of the whole dataframe ? - because If I have a text column like someones name , I can foresee it throwing an error ?
no you cannot just group , you have to apply / transform .
Otherwise you could just filter the NaN ( I'm assuming that is what you want to do , difficult to tell without sample data and code ) and apply the map : ` df [ ' flag '] = filtered.notnull() .map ( ' N ')`
One way is to use an apply : #CODE
I need to combine different functions into one and use the apply function ( of those individual functions ) within the main function itself . My case is something more complex so i'll use a basic example for this .
The above function would give me all values in a single column separated by commas . is there a way to apply each one in such a way that they come under different columns ??
The ` apply ` returns a dataframe with True / False values ( the ` ` expression is evaluated for each column where ` x.name [ 2 ]` selects the third level of that column name ) , and the where replaces the False values with NaN .
Apply to each element in a Pandas dataframe
Is there anyway to apply this simple function to each element in the data frame ?
I use 0.12.0 ( added to the question as well ) . ( I am trying to apply your solution and compare times . )
python pandas strange error when concat the return values of ' apply '
The apply function is trying to return a whole dataframe in your case . You can't really do that . You can use apply to map a column or a row of a frame to a row / column or scalar . Printing is fine , but that's no surprise ...
In pandas , you can use ` apply ` to do similar thing #CODE
The reason the rolling apply does not work is because 1 ) you provided it a GroupBy object and not a series , and 2 ) it only works with numerical values .
@USER Yes , thinking of the combine was the hard part :-) . For set difference , ` s - s2 ` does work , but intersection as ` s & s2 ` seems not to work for pandas serieses . I was looking for a way to apply a function on the elements of two serieses , but didn't find an obvious one .
There hopefully will be some support for parallel ` apply ` in 0.14 , see here : #URL
also , there's no reason to use apply in this case .
Apply upper and lower bounds to Pandas Dataframe
You could iterate over each column / bounds-list and apply the same filter . #CODE
I apply a custom function on the DataFrame column ( convert_time ) #CODE
I encounterd the same question and I used a same way like you to solve it . ( apply a function to remove the unnecessary data )
Only better in terms of simpler syntax : ` df.rain_column.map ( d )` , and perhaps faster performance-wise , it depends on data size and type for a dataframe with 100 rows then ` apply ` is marginally faster ( apply 228 us vs map 287us ) , for one with 10000 rows then map is 26 times faster ( map is 512 us vs apply 13 ms )
Alright , this makes a lot of sense , since apply is more general purpose than map .
How can I use apply with pandas rolling_corr()
Also how do we do the same using some applymap or apply / map method of df ?
Ok . I understand this is not how Pandas works . I was trying to relate it's functionality with other data analysis tools . But in R , I could do a similar thing using apply method and then using the same if-else statement . It creates the right flag . Isn't pandas an add on of R dataframe with more better features ? Or do you think we also have a similar way to do this in pandas using some applymap or apply method and if-else construct ? This is the R code which creates the flag properly :
c$Flag -> apply ( c , 1 , function ( X ) {
Why not just use apply with these functions .
@USER obviously you have to elaborate . Anyway , like I commented usually it's better to use groupby methods e.g apply .
Thanks DSM ! @USER that would be a sign that the apply did not create a dataframe . Are you sure there are lists in your series ?
Now comes the third part - Let's apply same reasoning . ` reindex ` is not defined in ` MyDataFrame ` . Where should we look next ? Class hierarchy , that means ` pandas.DataFrame ` . Now ` reindex ` is indeed defined by this class and it returns a pandas.DataFrame object ! . ( See this : #URL ) So , no wonder ` y ` is a ` pandas DataFrame ` .
Apply method of DataFrame vs List Comprehension
I can do it with list comprehension but I would like to understand if I can do that with the apply method of DataFrame . Here is a toy model : #CODE
Probably it is just because of my shallow knowledge of pandas , but when I use the apply method I imagine the serie as a list or so end hence I do not have any idea on how to " put " the index attribute .
The ` ( df1-df2 ) .dropna() ` call creates a slice of the dataframe . When you apply a new operation , this triggers a ` SettingWithCopy ` check because it could be a copy ( but often is not ) .
slices aren't conditional , you'll have to apply a filter .
You could use ` apply ` , there is probably a better way than this : #CODE
One way is to merge index values as well - pd.merge ( df1.reset_index() , df2.reset_index() , ... ) . Then run apply twice to return indices of df1 and df2 as appropriate . You could then select appropriate rows from the two and concatenate pd.concat ([ df1.ix [ ix1 ] , df2.ix [ ix2 ]] , ignore_index=True ) . This might make it faster
I have a dataframe to start with , with that dataframe I want to apply some function . I want to repeat this many times and build / stack the reults from the operations in a new larger dataframe . I was thinking of doing this with a for loop . Here is a simplified example that I can not get to work : #CODE
Clearly the ` apply ` method isn't a disaster . But just seems weird that I couldn't figure out the syntax for doing this directly across all the columns with ` mul ` . Is there a more direct way to handle this ? If not , is there an intuitive reason the ` mul ` syntax shouldn't be enhanced to work this way ?
Jeff , thanks for the reply . And sorry for highlighting something that's already been flagged as an issue . For what it's worth , I think my ` apply ` column-wise solution , above , is easier to read than the ` pd.concat ... ` method in your ` [ 129 ]` . What do you think ?
using `` apply `` will be much slower .
Both options works very nice :) Do you know hot to apply your method to use secondary ` y_axis ` [ example ] ( #URL ) ?
I know I can loop through and apply regex #CODE
whats the best way to apply it to the column in the dataframe ? so I have df [ ' pricing '] do I just loop row by row ?
It is like a dataframe , but requires ` apply ` to generate a new structure ( either reduced or an actual DataFrame ) .
Doing something like : ` df.groupby ( ... ) .sum() ` is syntactic sugar for using ` apply ` . Functions which are naturally applicable to using this kind of sugar are enabled ; otherwise they will raise an error .
Their are ways to apply some operations to multiple slabs of a n-dim ( esp . via new ` apply ` in 0.13.1 , see here .
Using partial with groupby and apply in Pandas
I am having trouble using partial with groupby and apply in Pandas . Perhaps I am not using this right ? #CODE
There is no need to use ` functools.partial ` here , as you can provide arguments to the function inside the ` apply ` call .
If your function has as first argument the group ( so switch the order of the arguments ) , then the other arguments in ` apply ` are passed to the function and in this way you can specify the ` columnName ` in the apply : #CODE
The reason it does not work with partial , is that ` functools.wraps ` does not seem to work together with ` functools.partial ` ( ` wraps ` is used inside the apply ) .
except that the top n logic should be embedded ( head ( n ) does not work with n depends on my data-set - I guess I need to use " apply " ? - and post this the Object , which is a "" object needs to be identified by matplotlib with its own labels ( top n " name " here )
Still not following how the cluster name is chosen . Also , you don't need to apply list in definition of cn1 since you're iterating through it .
Thanks to user1827356 , I sped it up by a factor of 100 by taking the operation out of the apply . For some reason first was dropping by Group column , so I used idxmax instead . #CODE
Then using the built-in apply function in pandas . But then , I realize none of my dates are in fact strings , so that does not really solve the problem . #CODE
no , you need to do this on the entire frame , selecting out individual elements is general VERY slow . Try doing what I suggegted AFTER a groupby ( e.g. in the apply function itself , which only has a time-index and only has the elements for which group you need )
You could also use ` shift ` to accomplish this within a ` groupby ` / ` apply ` : #CODE
this is pretty awesome , never thought about shift . I did try to use groupby and apply with a vlookup and couldn't quite crack it . this works perfectly , but just for my own educational use , if there were some years skipped and a vlookup was needed , would you know what to do ?
Why is this printing the print statement in the function 4 times ? The way I would have thought it works is to group ` df ` into 3 dataframes ( for each machine ) and apply ` func ` on each of those grouped dataframes . But this is not what I observe ...
For example if three values in a row are all higher than the previous , and the fourth is lower , then I want to add the first three . Does that make sense . I want the capability that excels cell.offset property or array element provides . I need to be able to apply calculations on multiple values from a dataframe column , rather than a single value .
I want to apply my_func ( a custom created function ) to each row of a dataframe . #CODE
You need to set ` axis=1 ` in ` apply ` : #CODE
If you're only passing in the row , you can just do ` df.apply ( my_func , axis=1 )` . Atlernatively , you can use the ` args ` kwarg or a ` lambda ` to pass in more arguments . ` apply ( my_funx , axis=1 , args =( par1 , par2 ))` or ` apply ( lambda row : my_func ( row , par1 , par2 ) , axis=1 )`
However , it is really slow . I looked around and found ` apply ` can do the work too : #CODE
It's even faster . With my larger dataset ( about 90k rows ) , the ` transform ` method takes about 44 secs on my computer , ` apply ` takes ~2 secs and the ` for loop ` takes only ~1 secs . I need to work on much larger dataset so even the time difference between the ` apply ` and ` for loop ` makes a difference to me . However , the ` for loop ` looks ugly and may not be easily applied if I need to create other group-based variables .
Above " apply_by_multiprocessing " can execute Pandas Dataframe apply in parallel . But when I make it to Celery task , It raised AssertionError : ' Worker ' object has no attribute ' _config ' . #CODE
You can apply ` strip ` to each element in a column this way : #CODE
But a groupby operation doesn't actually return a DataFrame sorted by group . The ` .head() ` method is a little misleading here -- it's just a convenience feature to let you re-examine the object ( in this case , ` df `) that you grouped . The result of ` groupby ` is separate kind of object , a ` GroupBy ` object . You must ` apply ` , ` transform ` , or ` filter ` to get back to a DataFrame or Series .
Why am I getting an empty row in my dataframe after using pandas apply ?
I'm fairly new to Python and Pandas and trying to figure out how to do a simple split-join-apply . The problem I am having is that I am getting an blank row at the top of all the dataframes I'm getting back from Pandas ' apply function and I'm not sure why . Can anyone explain ?
The groupby / apply operation returns is a new DataFrame , with a named index . The name corresponds to the column name by which the original DataFrame was grouped .
Ah , okay , so the reason it is there is that the groupby / apply operation _replaces_ the usual indexing with indexing by the grouping value ? Is that correct ?
Yes , the result of the groupby / apply is a new DataFrame , with a named index , and the name corresponds to the column name by which the original DataFrame was grouped .
This way reduce the timings by half on my side . By problem is I have values for hours initialy in 24 columns and date in first one . So I have to be more efficient as I can because after , I have to stach columns and do an apply to do a relativedelta on the datetime ( which is take actually 2 min more ) ...
I have done one for this typical problem { too long dataframe apply row functions} . I am very suprising if I am the first person to deal with this problem .
too long dataframe apply row functions
Rather than do a apply here , I would probably check each column for whether it's numeric with a simple list comprehension and separate these paths and then concat them back . This will be more efficient for larger frames . #CODE
Ah , I was talking about calling a method ( ` fillna `) on an object wrapped by a curly bracket , which I am usually not a fan of . I prefer using ` apply ` as suggested by Karl .
Or you could use shift within a ` groupby ` / ` apply ` : #CODE
Thanks . I like the ` apply ` solution !
Pandas Groupby apply function to count values greater than zero
Pandas Groupby apply function to count values greater than zero
You might want consider using apply : #CODE
Groupem ( can also use an apply here ) #CODE
Apply is only useful when you cannot vectorize .
Before even stored would be great . Would an apply be the fastest way for after the fact ?
Writing an apply right now . Is there a way to get the width of a column of type object ?
Just apply ` to_datetime ` : #CODE
Presumably you want to real part of the complex numbers ( numpy.real ) , unfortunately this doesn't play super nicely with pandas Series / DataFrame , so you need to apply back the indexes : #CODE
I have messed around with groupby , and calculated actual epoch values at the start and end of 5 minute time blocks for each of the observation periods , in a separate dataframe . But I can't see how to bring these to bear in a function that applies per observation period in the original dataframe above , where there are multiple values for each observational period . I suspect the answer lies in a better comprehension of groupby and the apply method , but I'm having trouble getting this off the ground . ( Also , maybe I am not using the right search terms , but I'm not finding much on this posted to the forum already . I'm only able to find info on working with timeseries ) . Two options I've considered but can't figure out how to program :
[ Edit ] Update - I think you can use the apply function to subtract the right min time #CODE
Okay , this is making some progress . But each of my observation periods starts at a different time ( the dates & times are spread out over 3 months ) . So , I can't just do step 1 across all my ' epoch ' data - I need to apply it separately for each ' observation ' group . Similarly , I'd need to do step 2 by ' observation ' group . So I think the plan of action will be to use the sort of approach you've provided , but apply it via groupby ?
I've updated the answer . Based on the dataframe you created you can get the right min value and apply to the epoch column
A few caveats apply :
Given that the time periods in the data are non-uniform and contain overlap , there are a few approaches possible . If you're alright with linearly averaging entries and exits , you can take each time period and calculate how many entries and exits occur per hour on average , then , given an hour , you could iterate through all data points , find how much a data point overlaps with that hour ( i.e. 15 minutes or the whole hour ) , and apply the data point's average entries / exits per hour modified by the percentage of overlap to an accumulator .
Struggling with pandas ' rolling and shifting concept . There are many good suggestions including in this forum but I failed miserably to apply these to my scenario .
If you manage to get the data in ine single Dataframe it should be possible using ' apply ' .
and then you can group by the ' i ' column and apply an arbitrary function to the subgroup . #CODE
or ` apply ` : #CODE
I have a MultiIndex pandas DataFrame in which I want to apply a function to one of its columns and assign the result to that same column . #CODE
I managed to apply the function slicing the dataframe with ` .loc ` as the warning recommended : #CODE
Thank you , removing the indexing is speeding up the process substantially . However , I still get very large file sizes : for every 300Mb table that I append to the merged store , I get an increase in size of 1Gb which eventually is going to fill in my disk . This should not be due to compression , as I did not apply compression to the 300Mb files .
But I'm looking for generalization for arbitrary number of columns . I tried to apply these methods as reduction function , but couldn't make it work .
Third question : I think that the code is quite pythonic , but I am not proud of that because of the last list comprehension which is running over the series of the dataframe : using the method apply would look better to my eyes ( but I'm not sure how to do it ) . Nontheless is there any real reason ( apart from elegance ) I should work to do the changes ?
I'm not sure there is a better method than using apply in this case
Unfortunately it isn't currently possible to apply Excel formatting when writing data with Pandas ` to_excel() ` .
You can apply column formatting when using XlsxWriter as the Excel writer engine . See Working with Python Pandas and XlsxWriter .
Use a function and apply to whole column : #CODE
pandas apply filter for boolean type
` pd.tools.plotting.scatter_matrix ` returns an array of the axes it draws ; The lower left boundary axes corresponds to indices ` [: , 0 ]` and ` [ -1 , :] ` . One can loop over these elements and apply any sort of modifications . For example : #CODE
You can apply a multiindex to a dataframe with #CODE
Once you tested ` flatten ` on one OrderedDict , it is straight-forward to apply it to a list of OrderedDict
Yeah , thanks a lot @USER Zhu . Quick follow-up : I have a couple hundred columns , so I can't call a column the way you called the column Val . How would I apply the method you suggested on across every column ?
Inconsistent behavior of apply with operator.itemgetter v.s. applymap operator.itemgetter
` apply ` gives wrong result #CODE
apply is being passed an entire row which is a series of 2 elements which are lists ; the last list is returned and coerced to a series . embedded lists as elements are not a good idea in general .
apply custom function / method to the groups ( sort within group on col ' A ' , filter elements ) . #CODE
If you wish to select many rows per group , you could use ` groupby / apply ` with a function that returns sub-DataFrames for
each group . ` apply ` will then try to merge these sub-DataFrames for you .
Another way is to use ` groupby / apply ` to return a Series of index values . Again ` apply ` will try to join the Series into one Series . You could then use ` df.loc ` to select rows by index value : #CODE
Also , the reason is because the entire data frame is read in externally but that one column happens to be all JSON . I apply pd.io.json.read_json() to the column but that leaves me with a column of DataFrames that I need to expand out .
But when I apply the conditions to obtain the final dataframe : #CODE
I'm using Ubuntu , so this might not apply that accurately , but here's how I'll do it . #CODE
However , also note that Pandas has string operators builtin . Using them will be far faster than using ` apply ` to call a custom Python function for each item in the Series . #CODE
No need to go via ` apply ` ; ` 100 * df2 / df2.sum() ` should work .
For looping over a groupby object , you can try ` apply ` . For example , #CODE
Pandas : Timing difference between Function and Apply to Series
Why is the apply ( lambda ) method ~ 3.5 x slower . In more complex dataframes , I have noticed a larger difference ( ~10 x ) .
So in this case it looks like most of the performance difference is related apply converting each column to a ` Series ` and passing each series separately to rolling_mean . Having it use ` Raw=True ` has it just pass ndarrays .
Aftermath : I ended up being a coward and preprocessing the data before importing into pandas . But I'm still curious if there's a general solution , using something like apply or map .
@USER . Yes I am sure . ` sweep ` is based on ` apply ` . if you read the documentation for apply , the definition is amply clear IMHO . Before you ask , yes apply is mentioned on the docs page for sweep ( look under see also ) .
Pandas has an apply method too , apply being what R's sweep uses under the hood . ( Note that the MARGIN argument is " equivalent " to the axis argument in many pandas functions , except that it takes values 0 and 1 rather than 1 and 2 ) . #CODE
You can use an apply with a function which is called against each row : #CODE
Note : that axis=0 would apply against each column , this is the default as data is stored column-wise and so column-wise operations are more efficient .
You can do the same in numpy ( ie ` data.values ` here ) , either multiplying directly , this will be faster as it doesn't worry about data-alignment , or using vectorize rather than apply .
Great answer . I am playing around with this approach now and had a question . Can you please comment on the use of ` lambda ` in ` apply ` ? Any reason to prefer it over a function declared using ` def ` ? Thanks much .
Obviously I could write plain Python that , given the period I re-sampled to in Pandas , could give me the Series I need , but I'd like to know if there is a trick within Pandas that helps me with this , or something I could do in Numpy , as I want to apply this to largish datasets ( hundreds of users , thousands of days , multiple login / logouts a day per user ) .
How to apply a function to a mixed type Pandas DataFrame in place ?
This is how I apply a function to Pandas dataframe , it works in place and modifies the original data frame . #CODE
But if I try the same on this data frame ( it has ints and floats instead of just ints ) , then it fails to apply inplace and always returns a dataframe . But I have a huge dataframe , so I want to do it inplace . #CODE
So no way to apply a function in-place to a DataFrame in pandas ?
But of course it'll return a frame , apply returns a frame , this seems to work inplace for me in 0.13.1 ( even with floats ) . Generally you'll want to vectorize rather than use apply , obviously here , as mentioned above , you'd use ` x [ ' b '] +=1 ` . Also , I think using iterrows is preferable to using apply like this .
I realise this does answer the question ( well ) , but using apply with side-effects seems hacky / wrong / I really dislike it !
If you want to iterate across your database and apply a function to each row , you might also want to consider the apply function #CODE
If a column contains only strings , we can apply ` len ` on it like what you did should work fine : #CODE
Continuing the above example , let us convert ` strange ` to strings and check if ` apply ` works : #CODE
Then I tried to apply the same solution to ` pandas ` : #CODE
Use ` groupby-shift ` to apply the shift to each group individually : ( Thanks to Jeff for pointing out this simplification . ) #CODE
datetime won't operate on a pandas Series ( column of a dataframe ) . You can use ` to_datetime ` or you could use ` datetime ` within ` apply ` . Something like the following should work : #CODE
Or use apply : #CODE
Yeah , I did the ` to_datetime ` answer a while ago and the came back and added the ` apply ` answer at about the same time as you .
not a speedfreak myself , but i tested yours and my apply method . Youe is about 50% faster as mine has the overhead of creating a new dataframe before doing the apply . All in all your is probably better . Also handles the 1 for day better than mine .
Because questions describing your requirements and asking someone to write the code for you or explain how to write the code are considered off-topic for Stack Overflow , but none of the standard close reasons apply . Some people seem to think that " too broad " , " unclear what you're asking " , or " lacks sufficient information to diagnose the problem " are always sufficient to cover these kinds of questions , but this case illustrates why they often don't get the right message across .
I've already searched posts on stackoverflow , and looked at the documentation for convert_objects , but it is unfortunately pretty sparse . I wouldn't have known to even attempt to apply it this way if not for the previous post ( linked above ) .
In order to apply a method on a DataFrame that is grouped ; your need to use a loop as follows : #CODE
I have checked and the ` apply lambda ` part gives the expected results if I ` debug print ` it .
The problem seems to be assigning the apply lambda construct back to the DataFrame .
Note : it could be there are some NaNs causing this float upcasting , in which case you may have to reconsider your approach ( since you won't be able to convert to int ! ) , one option might be to do the string formatting and then apply ` to_datetime ` : #CODE
I am not sure that I understand your question , but in the last part you say that you want to make sure that your data is not correlated . You apply Principal Component Analysis ( PCA ) to any dataset , the resulting principal components are not correlated by definition .
I don't fully understand how to get my apply function to actually apply to the row to change it .
In that case merge them and then depending on the complexity of your function either use a lambda or define your function and just apply it row-wise so ` merged = df.merge ( df1 , on= ' Date ')` then ` merged.apply ( myfunc , axis=1 )` or ` merged.apply ( lambda row : myfunc ( row ) , axis=1 )` I'd need to see your function first though before deciding the best approach , also it's getting late here in blighty so I may not answer
Yes that is correct , then apply a function row wise or if you're just looking for values larger than some threshold then do boolean masking using max() that will be very quick . Sorry love to help further but it's getting late here .
Note : This is not the same as this question ; and I have read the documentation , but am not enlightened by it . I've also read through the " Related " questions on this topic , but I'm still missing the simple rule Pandas is using , and how I'd apply it to for example modify the values ( or a subset of values ) in a dataframe that satisfy a particular query .
Then I used the pandas ` apply ` method ( as was suggested here ) to implement the ` pyproj.Geod.inv ` calculation , looping through slices of the ` pandas ` ` DataFrame ` for each individual in the population . #CODE
Now use ` apply ` and ` cut ` to create a new dataframe that replaces the percentile with the decile bin it is in ( apply is iterating over each column ): #CODE
Use apply once again to get a frequency count : #CODE
You should use the apply function which applies a function on either each column ( default ) or each row efficiently : #CODE
and then call apply like this : #CODE
linear fit by group in apply takes too long using pandas
Normally , we can use either ` map ` or ` apply ` , but it seems that neither of them allows the access to values in the previous row .
Thanks for your suggestions ! I used a combination of the two responses to change the original text into a list , saved matches to a list , then joined the list and saved it to a new variable using the format @USER stated . Now to format , scale , and apply to the actual data set . It's significantly less verbose as well .
you're right , but how does that apply here ?
This is a bug , slated to be fixed for 0.14 ( releasing soon ) , see here . The bug is that non-cythonized routines are calling ` apply ` rather than `` agg ` effectively .
As expected this apply works over the groupby object : #CODE
However , when specifying options for apply it throws up an error : #CODE
Pandas : Timing difference between Function and Apply to Series
There are different ` apply ` methods for ` DataFrame ` s and ` GroupBy ` objects . Only ` DataFrame.apply ` has a ` raw ` argument : #CODE
My mistake you can pass that param on a groupby apply , could you post some sample data so I can see what your df looks like prior to the groupby
@USER So if you reset_index and then do groupby and apply does it work ?
Suppose I want to calculate how many days ago each observation occurred , and return that as a simple integer . I know I can just use ` apply ` twice , but is there a vectorized / cleaner way to do it ? #CODE
I think you want to use the ` apply ` method of the ** DataFrame** , using axis = 1 , e.g. df.apply ( lambda row : print row , axis=1 ) . This method will generate a series , which you could add to your ** DataFrame** . I'll write this up into an answer for you .
You can use the ` apply ` method of the DataFrame , using ` axis = 1 ` to work on each row of the DataFrame to build a Series with the same Index .
pandas groupby add column from apply operation
Awesome , a ` transform() ` was exactly what I needed ! But do you mind explaining what's the difference between ` transform ` and ` apply ` ?
` apply ` we will work just fine too . If you replace ` transform ` with ` apply ` , you should get the same output . ` apply ` is the more general method ; ` transform ` is appropriate when you want to return something like indexed .
I tried creating a function that would filter on table B and then using apply like below . #CODE
For comparison , here's the timeit of ` apply ` on the same frames : #CODE
I know how to create a new column with ` apply ` or ` np.where ` based on the values of another column , but a way of selectively changing the values of an existing column is escaping me ; I suspect ` df.ix ` is involved ? Am I close ?
When I apply a numeric function to the group , such as max() or mean() , I get a DataFrame with type ` object ` returned #CODE
When I select only numeric columns first and then apply a numeric function to the group , such as max() or mean() , I get a DataFrame with a numeric type #CODE
More generally , if you want to transform the groups of a ` GroupBy ` object with any arbitrary function , use the methods apply , transform , or filter . See the docs linked by Jeff above to understand the distinctions between these three .
i've had no success using transform , apply , or aggregate to accomplish my goal . :-/
@USER or maybe a nested ` apply ` 0_o
Though some locations has 4 non nan values , the whole process is stopped , saying the ' cubic ' method requires at least 4 non nan values . How can I make it conditional to apply the ' cubic ' method to change values for those locations which can run ' cubic ' method ?
But it is extremely slow . Would it be quicker to do this in list comprehension , or with an apply function ? What is best practice for this kind of operation ?
For the aggregation methods , the list is fairly short so I can easily make a list of ` if ` statements . However , by definition the ` apply ` ` lambda ` functions are bespoke for each definition . Here's an example which takes a couple of columns to derive a percentage : #CODE
While this works , iterating over columns feels like it goes against the spirit of python and ` numpy ` . Is there a more natural way I'm not seeing ? I would normally use ` apply ` , but I don't know how to use ` apply ` and still get the unique population value for each row .
What's the correct way to apply ` zscore ` ( or an equivalent function not from scipy ) to a column of a pandas dataframe and have it ignore the ` nan ` values ? I'd like it to be same dimension as original column with ` np.nan ` for values that can't be normalized
Iterate and apply function over level ( s ) of MultiIndex dropping the iteration level
Can't you just apply a lambda that strips out the first index level ?
Agree that A and B are all in memory . I am reading a file ( .csv ) into memory performing the rename and only then saving it to HDF5 . As soon as I apply the rename method to the frame , it doubles the output size . If I omit the rename method the file size is half . Since all operations are performed in memory with the writing to HDF only happening at the end , I can't seem to understand why the rename method would seem to cause the frame to double in size and thus create a double-sized HDF file .
Is there a way to do this without the for loop or using where ( ) or apply ( ) functions .
` apply ` should work well for you : #CODE
It just depends on how you want to treat NaNs . If you return ` NaN ` when ` row [ ' C ']` is ` NaN ` , then you won't even need this case , since ` x * NaN ` is ` NaN ` . If you want to return 0 , you can do a ` fillna ( 0 )` after ` apply ` ing ` fund ` . Also , for various reasons ` np.nan == np.nan ` is * always * False , so your way wouldn't quite work . Pandas gives the ` pd.isnull ` function to check for NaNs .
Thanks . I actually have two indices ( one is ` datetime ` , the other one is a string ) . Is ` to_datetime() ` an ` Index ` method ?? If so , how can I apply it to only one of the index levels ?
Apply resampling to each group in a groupby object
And I would like to apply this function to every dataframe in a groupby object with something like the following : #CODE
How do I create functions like the above and have them properly apply to a groupby object ?
am i just using pandas incredibly badly ? i am used to being able to groupby and apply a transformation without difficulty . am i supposed to do things differently for this application for some reason ?
Now , you can apply interpolation methods on the ` NaN ` values as described in the docs .
To apply to a groupby then you can do this : #CODE
Thanks . What if want to apply the result within a group , respecting the boundaries of the group ? Sorry for the last minute twist , but it turns out that I need to make sure this happens .
Problem is I cannot specify the index position inside the ix function as I plan to iterate by row using apply function . Any suggestion ? #CODE
Thanks Jeff , it sounds like this is a question for Enthought . I've been able to work around the problem for now by using ` apply ` , but am still concerned .
Try " Apply completion on . " or try to play with " AutoCompletion delay " .
I need a method to get the selected features , ( and preferably something to drop the unselected ones , for when I apply the models and selected features on new " test " data ) .
Hi Gank . The " field " was supposed to show you can apply the " .values " method to various fields of the dataframe such as columns or a selected column . " .index " is an example of replacing " field " with an actual field that is available :) I guess that could be clearer ...
Well , you can avoid the apply and do it vectorized ( I think that makes it a bit nicer ): #CODE
Actually it's not a complete duplicate ... the question is asking specifically for Pandas . The response below shows how to assign an axis to a plot function call from ` pandas.DataFrame.plot ` which makes it possible to apply the ` matplotlib.pyplot ` refinements .
Figured it out . Selecting out the Series in the Dataframe effectively allows me to assign to it and the original dataframe . this allows me to use the slicing syntac to apply logic influencing the results : #CODE
Return multiple columns from apply pandas
Is there anyway I can make this faster ? For example , can I instead of returning one column at a time from apply and running it 3 times , can I return all three columns in one pass to insert back into the original dataframe ?
You could try using DataFrame's ` apply ` . Write a function that includes an exception handler and apply it to the DataFrame . #CODE
you should avoid `` apply `` if you can vectorize , e.g `` df [ ' diff '] .where ( df [ ' diff '] .abs() > = 0.3 )`` will be much faster
The basic idea is to group data based on `' Localization '` and to apply a function on group . #CODE
I would like to group a ` DataFrame ` then apply ` myfunc ` along columns of each individual frame ( in each group ) and then paste together the results . There are hacky ways to do it , but I wonder it seems like there is some simple kwarg I'm missing .
ok , revised the answer ; you can do almost anything inside the apply FYI
As far as the unpacking goes , maybe you could let the function take an argument for whether to add or subtract , and then apply it twice .
You could perform a groupby / apply ( shift ) operation : #CODE
The obvious way to find all such matches is to iterate over each row and apply a filter to the data frame : #CODE
In Pandas , is there an easy way to apply a function only on columns of a specific type ?
What is an easy way to apply a function only on columns of a certain type ?
And now you can use that list with an apply or whatever .
I'm trying to drop rows of a dataframe based on whether they are duplicates , and always keep the more recent of the rows . This would be simple using ` df.drop_duplicates() ` , however I also need to apply a ` timedelta ` . The row is to be considered a duplicate if the ` EndDate ` column is less than 182 days earlier than that of another row with the same ID .
Groupby the ' level ' on the columns and apply f ; don't use apply directly , but just concat the results as rows ( this is the ' unstacking ' part ) .
( 3 ) Or you could do it all in a function that was called by the ` groupby / apply ` : #CODE
When using ` count ` , state isn't a nuisance column ( it can count strings ) so the ` resample ` is going to apply count to it ( although the output is not what I would expect ) . You could do something like ( tell it only to apply ` count ` to ` value_a `) , #CODE
Or more generally , you can apply different kinds of ` how ` to different columns : #CODE
pandas apply function that returns multiple values to rows in pandas dataframe
I would like to apply a transformation to each row that also returns a vector #CODE
I end up with a Pandas series whose elements are tuples . This is beacause apply will take the result of myfunc without unpacking it . How can I change myfunc so that I obtain a new df with 3 columns ?
pandas , apply string operation to column should be string type , but has missing values ( np.nan )
groupby the ' c ' column , and consider all the columns that you want passed to the ` apply ` EXCEPT for c ( this is what ` df.columns - [ ' c ']` does , as normally the grouping column IS passed to the apply .
Try using apply with a custom function over axis=1 : #CODE
@USER Actually , it does . Just specify the columns when you apply the function , like this : ` df [[ ' col1 ' , ' col2 ']] .apply ( ... )`
This should still be pretty fast because it uses ` pandas `' vectorized string methods for each of the columns ( the apply is across the columns , not an iteration over the rows ) .
Well it looks like within groups , there is one observation per month and you want the percent change from one month to the next . You can do that with a ` groupby / apply ` by grouping on ' product_desc ' and then using the built in ` pct_change() ` method : #CODE
Thanks for your help on this . The pct_change function was exactly what I needed . However , when I apply the sort they don't sort the same way . Did you convert the activity_month into dates with strptime first ? My results are sorted by product_desc but have the activity_month out of order .
Now I find that the class of ` a ` and the class of ` b ` are different ; ` b ` is a ` pandas.core.series.Series ` object and therefore you can not apply the method ` append2 ` to it .
but still can't quite get the output to the formats I need . I'm not quite sure how to apply the df.groupby syntax or the df.apply syntax to what I'm working with .
Here is a neat apply trick . I'll create a function and print out what is incoming ( and maybe even debug in their ) . Then easy to see what's happening . #CODE
Perfect ! This is exactly what I was looking for . I guess my confusion stems from the fact that the Series.value_counts doesn't seem to fit into the arguments required by the df.apply method . How does it know which axis to apply the value_counts to ?
You can reformat the values by passing a reformatting function into the ` apply ` method as follows : #CODE
Apply an operation to the value of a series ( 2 and 3 ) #CODE
Then apply : #CODE
what does the function in df.groupby ( ... ) .apply ( lambda x : ... ) apply to ? what is the form of x ? list ?
groupby , apply , and set not behaving as expected ... is this a bug ?
[ @USER ] ( #URL ) , why does ` dat.groupby ([ ' names ']) [[ ' letters ']]` pass all the columns through to apply ( letters , numbers , names ) ? Why not just the ' letters ' column as a dataframe instead of a series ? Is it ignoring ` [[ ' letters ']]` ( syntatic sugar that's not allowed ) ?
You don't need the final apply , see here : #URL you can simply `` astype ( ' timedelta64 [ D ]')`` or divide by `` np.timedelta64 ( 1 , ' D ')`` ( they are sligthly different in how they round .
Based your code ( your ` groupby / apply `) , it looks like ( despite your example ... but maybe I misunderstand what you want and then what Andy did would be the best idea ) that you're working with a ' date ' column that is a ` datetime64 ` dtype and not an ` integer ` dtype in your actual data . Also it looks like you want compute the change in days as measured from the first observation of a given ` group / stage ` . I think this is a better set of example data ( if I understand your goal correctly ): #CODE
Given that you should get some speed-up from just modifying your apply ( as Jeff suggests in his comment ) by dividing through by the ` timedelta64 ` in a vectorized way after the apply ( or you could do it in the apply ): #CODE
But you can also avoid the ` groupby / apply ` given your data is in group , stage , date order . The first date for every ` [ ' group ' , ' stage ']` grouping happens when either the group changes or the stage changes . So I think you can do something like the following : #CODE
Apply method : #CODE
So I think avoiding the apply could give some significant speed-ups
Yeah @USER , I thought about ` transform ` but at least for 0.13.1 I usually find transform no faster than a generic ` apply ` so I didn't include it . But I will update the answer with that as an alternative .
Of course you don't show your data so this may or may not apply and I apologize if not , but for a large dataframe I'm dealing with I see a speedup of , well , 24 million times !
Thanks . Is this technically a ` transform ` or ` apply ` operation ? I never quite understood the difference .
` apply ` is the most general category of operation on a group , so lots of things fall under its umbrella . What distinguishes transform operations is that they produce something indexed like the input , and that happens here , so I guess you could think of it as a transform .
Well , one approach is the following : ( 1 ) do a ` groupby / apply ` with ' id ' as grouping variable . ( 2 ) Within the apply , ` resample ` the group to a daily time series . ( 3 ) Then just using ` rolling_sum ` ( and shift so you don't include the current rows ' x ' value ) to compute the sum of your 70 day lookback periods . ( 4 ) Reduce the group back to only the original observations : #CODE
You are going to need your data sorted by ` [ ' id ' , ' dates ']` . Now we can do the ` groupby / apply ` : #CODE
It just becomes the next parameter in the ` apply ` . See me edit for details .
No need for the ` lambda ` on the first one : ` apply ( ' { : 0 > 15} ' .format )` should work too .
as the apply function creates only one column with tuples in it .
You can put the two values in a Series , and then it will be returned as a dataframe from the apply ( where each series is a row in that dataframe ) . With a dummy example : #CODE
Use ` to_datetime ` to convert to a string to a datetime , you can pass a formatting string but in this case it seems to handle it fine , then if you wanted a date then call ` apply ` and use a lambda to call ` .date() ` on each datetime entry : #CODE
It is . Thanks . In R , we could write something such as grepl ( " date " , colnames ( df )) , which creates a logical index by which we subset . Maybe that same logic does not apply here , although I have seen people do that for selecting rows .
Using ' apply ' in Pandas ( externally defined function )
What am I doing wrong ? I think I'm not really understanding how apply ( and its cousins , aggregate and agg ) works . If someone could explain , I'd be ever so grateful !
You could group by year , isolate the prop column , apply ` argmax ` , and use ` loc ` to select desired rows : #CODE
AttributeError : Cannot access callable attribute ' info ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method
No worries , the given solution still applies , you just have to call it inside of an apply of your groupby : ` df.groupby ( bla ) .apply ( lambda df_grouped : df_grouped.groupby ( level =[ 0 , 1 , 2 ]) .apply ( fancy_func ))`
how to apply Functions on numpy arrays using pandas groupby function
@USER I get this error when trying to apply . why might this be ?
I want to apply a weighted sum to a DataFrame . In the past I have used #CODE
I want to apply a weighted average to the sum where the most recent is multiplied by 0.6 , 2nd by 0.2 , 3rd and 4th by 0.1 .
I think you can do it will a ` rolling_apply ` within a function called by a normal ` groupby / apply ` . So something like the following : #CODE
@USER Thanks . I'm new to Python Is this documented somewhere ? I could not find it in the Pandas documentation . I did find something about loglog plots in the matplotlib documentation , but I don't know how to apply this to a Pandas dataframe .
It is not updating +1 count as I wanted . I believe it is some kind of type issue , but not sure how to force the type . I am using DataFrame for my data as I want to use group function to split the data and apply the above function . Any suggestions ?
Use indexing instead of apply , it's much faster : #CODE
Since you've got to do the ` apply ` anyway , I think it is cleaner to move the ` json.loads ` and column creation all into the apply ( don't use the ` converters ` for ` read_csv `) : ` df [ ' field3 '] .apply ( lambda x : pd.Series ( json.loads ( x )))`
By searching this subject , I've known basic syntax of replace() . But I couldn't apply to my specific problem .
Although unwieldy using ` loc ` will scale better with larger dataframes as the apply here is called for every row whilst using boolean indexing will be vectorised .
That is only true if the column already exists I think as I get no warning unless the column exists , I did some timings and using ` loc ` method is 1.66ms for a data frame size of 3000 rows versus the apply method which takes 60.2 ms
How do I apply a function to a pandas dataframe ?
I have tried to apply a function to a pandas dataframe like this #CODE
So the first problem in your code is that you are calling apply and setting param ` axis=1 ` this applies your function row-wise which is fine .
Currently , I am using ` df_sub =d f [ df.ID.isin ( ID_list )]` to do it . But it takes a lot time . ` ID ` s contained in ` ID_list ` doesn't have any pattern , so it's not within certain range . ( And I need to apply the same operation to many similar dataframes . I was wondering if there is any faster way to do this . Will it help a lot if make ` ID ` as the index ?
If you're working with small datasets , you get different behaviors and it actually becomes faster to use a list comprehension or apply against a dictionary than using ` isin ` .
Thank you all for help . I've try out these different methods and get back with results . But can how can I apply Cython to pandas ? It's hard to declare type .
Summarizing the comments , for a dataframe of this size , using ` apply ` will not differ much in performance compared to using vectorized functions ( working on the full column ) , but when your real dataframe becomes larger , it will .
Indeed , I get 201us ( np ) vs 208us ( math ) , so almost the same for this dataframe , but for a larger one ( this one 100 times repeated ) , numpy is clearly faster than using apply .
Also for the concatenation , for this dataframe , using apply is not slower ( even a bit faster 500 vs 700 us ) , but for larger dataframes ( 7000 rows ) it is again clearly slower ( 200 vs 80 ms ) .
Regarding to the performance , I just notice if I use the vectorized functions , I may cause a memoryError ( I have 3G ram ) but apply does not have such a problem . So I think the vectorized functions are reading everything in memory right ? The original file is about 12M in size , above is just a sample section of the file
When using ` apply ` the id generation is performed per row resulting in minimal overhead in memory allocation .
yeah , this way works , but in this thread , #URL it is said the vectorized function is faster than using apply call , and from my experiments it seems true . The vectorized functions tend to use more memory than apply call , but the confusion is that I still have lots of memory left when the memory error occurs
I can reproduce the memory problem . Still investigating further but no luck on faster approaches . Also timed the apply solution and it takes about 5 minutes with 500k rows
Sorry , I can also reproduce it on 0.13.1 , but the issue does not occur in 0.12 or in 0.14 ( released yesterday ) , so it seems a bug in 0.13 . So , maybe try to upgrade your pandas version , as the vectorized way is much faster as the apply ( 5s vs > 1min on my machine ) , * and * using less peak memory ( 200Mb vs 980Mb , with ` %memit `) on 0.14 .
So , maybe try to upgrade your pandas version , as the vectorized way is much faster as the apply ( 5s vs > 1min on my machine ) , and using less peak memory ( 200Mb vs 980Mb , with %memit ) on 0.14
Can I apply a function that uses ' shift ' on a grouped data frame , and return a simple data frame from pandas ?
You can use the ` TimeGrouper ` function in a ` groupy / apply ` . With a ` TimeGrouper ` you don't need to create your period column . I know you're not trying to compute the mean but I will use it as an example : #CODE
Or an example with an explicit ` apply ` : #CODE
It works because the groupby here with as_index=False actually returns the period column you want as the part of the multiindex and I just grab that part of the multiindex and assign to a new column in the orginal dataframe . You could do anything in the apply , I just want the index : #CODE
Yes , you can do it in an apply . Just do a groupby in a function that the first groupby / apply calls
@USER you could use a function and apply it to the dataframe but I'm not sure you would save much time , it would depend on how many unique ids there were , once you have created the dicts then using ` map ` is really fast
It's a reasonable way to do it . You could change the apply a little to only return a ` ranks ` Series . That would allow you to just assign a new ` ranks ` column to the original dataframe as the result of the ` groupby / apply ` . But your way works just fine .
I am struggling to set xlim for each histogram and create 1 column of graphs so the x-axis ticks are aligned . Being new pandas , I am unsure of how to apply answer applies : Overlaying multiple histograms using pandas . #CODE
The layout option is very helpful . However , the bin setting seems to only apply to the range of data not the entire interval that we display using xlim . For example , say that I'd like to bucket the counts over [ -1 , 1 ] with a total of 10 buckets ; then the values from 0 to 0.2 should be in a single bucket , but that's not the case with bins=10 . Any idea why not ?
jeff , i getting the output i want . I just want to see if another way exist with pandas , like apply or shift . I tried but i couldn't figure out
Once you understood what happened here , I'm sure you can apply this to find the maximum of ' t1 ' .
Udate : If you don't want to depend on the column order , you can also specify the values to use to fill for each row ( like ` .fillna ( value =d f [ ' D ']`) . The only problem is that this only works for Series ( when it is a dataframe , it tries to map the different values to fill to the different columns , not the rows ) . So with an apply to do it column by column , it works : #CODE
` Cannot access callable attribute ' reset_index ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method `
Thanks a lot ! My fault was , that I did not realize , that I have to apply some function to the groupby dataframe , like ` .size() ` , to work with it ... #CODE
Then you can select the rows you want in an apply call on the grouped object : #CODE
If you have one large dataframe and only a few update values I would use apply like this : #CODE
OK , first problem is you have embedded spaces causing the function to incorrectly apply :
So you can call ` replace ` instead of calling ` apply ` : #CODE
@USER so is ` replace ` faster than calling ` map ` or ` apply ` and passing a dict now ? Wasn't aware of ` factorize ` also , when was this introduced ?
My tentative solution is to apply this function after reading : #CODE
Now you can use an apply with zip : #CODE
I have applied dropna ( how= ' all ') , which makes my example misleading . However , I am not willing to apply dropna ( how= ' any ') , since I do not want to lose valid data just because a NaN sits in the next column over . Your suggestion is good . That being said , there still seems to be a fundamental problem with quantile ( or so I think ! ) .
the order within group apply function
order is preserved within a group and to the subframe that is passed to apply or a reduction function . you should show what you are doing and why this matters .
@USER it matters for apply functions like x - x.shift ( 1 ) . If order is not preserved I may get wrong answer .
If you are using apply not only is the order not guaranteed , but as you've found it can trigger the function for the same group a couple of times ( to decide which " path " to take / what type of result to return ) . So if your function has side-effects don't do this !
Apply styles while exporting to ' xlsx ' in pandas with XlsxWriter
When using the pure XlsxWriter I can apply formats to cells what also works nice .
Based on this new dataframe , I can group it by ` ticker ` and ` row ` , and apply a daily ` resample ` on each of these groups and ` fillna ` ( with method ' pad ' to forward fill ) #CODE
@USER : Happy001 asked to see the dtype of ` realtime ` . You showed the * type * of what you get when you select a column from a groupby object ( ` SeriesGroupBy `) , and the * type * of an unrelated Series after you apply ` pd.to_datetime ` to its elements , which by construction is ` Series ` . So far , nothing you've shown is incompatible with the error message ` pandas ` gave , which says that you're trying to subtract a ` timedelta ` from a ` unicode ` string . Instead , look at ` df [ ' realtime '] .dtype ` , and ` df [ " realtime "] .apply ( type )` .
How to apply OLS from statsmodels to groupby
So how can I go through my dataframe and apply sm.OLS() for each product_desc ?
If I understand you correctly , I think you can do it with a ` groupby / apply ` . It's a bit tricky . So I think you have data like the following : #CODE
bypass read_csv and apply to_datetime after :
I think the problem is ` apply ` expects to return the same number of rows as the input .
You could also do it with a ` groupby / apply ` since it is more flexible . So something like the following : #CODE
You can use the pandas groupby-apply combo . Group the dataframe by " Item " and apply a function that calculates the process time . Something like : #CODE
@USER still I think BrenBarn is correct . If you are just updating existing data then the performance hit may not be an issue , it sounds like all you'd be doing would some stats on the updated values , note that groupby itself does nothing only when you apply a function does it do something . If you know which group is to be updated then you can call ` get_group ( ' updated_item )` can call apply on just that group see : #URL
You could use the ` apply ` method : #CODE
Thanks !! I found my stupid mistake while using apply() . I did apply ( wordnet.synsets() )
For example , say that I know what slices I want to apply on each level name , e.g. as a dictionary : #CODE
you could use an ` apply ` statement to select the values from the correct columns .
I now would like to combine the ' Day ' and ' Hour ' columns into one ' Date ' index column . I did a lot of searching and so far I have only seen solutions that are based on pd.read_csv and pd.read_table . However , as this is a series ( not a dataframe / csv / excel ) , these solutions do not seem to apply .
use `` apply `` ONLY as a last resort ( e.g. you can't do vectorized things ) . even if you have a very complicated function to do , you can often do vectorized calculations on most of it , saving the last for `` apply `` , which is essentially a loop .
Using apply took 172ms versus 39ms using Jeff's method , I can also confirm that it made negligle difference whether the apply was called inside or outside the function but it does modify the df so you didn't need to return the df as it was being modified inside the function
@USER : thanks for your clarifications once again , very glad you helped me with it . As a new user like me to python / pandas , the problem mostly is , that I can only search / google for solutions as the libraries contain so many classes and functions that I don't know what to look for ( in this case the ` DatetimeIndex ` class ) . And then sometimes different solutions ( in this case using ` apply `) come up on google / stackoverflow and yet again I can NOT verify that there is no better solution as I dont have the insight into the library . But I keep learning heavily each day , thanks :)
Pandas : apply a function to a multiindexed series
Now I want to apply any function to each series indexed by numbers.hash only , e.g. summing the values in each time series that is made up of local_time and the value . I guess I can get the number.hash indices and iterate over them , but there must be a more efficient and clean way to do it .
Or groupby and apply an arbitrary function #CODE
gives " AttributeError : rint " if I insert values and then apply np.round() . If I copy df.describe() , change some values and then do np.round() it works fine . Both are DataFrames so I don't see why the behaviour could be different .
Thanks . For some weird reason that didn't work . I keep checking that I have dupes by using df.duplicated() .value_counts() and it does show as many rows as True but then when I apply ` df.sort ( df.columns.tolist() )` as you suggest , it still is not sorting all of the duplicated rows .
No , I have tried that and received this : AttributeError : Cannot access attribute ' values ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method
You need to apply some kind of aggregation to the GroupBy object to return a DataFrame . Once you have that , you can use ` .values ` to extract the numpy arrary .
In DF1 are a few hundred thousand records with columns lat1 and lon1 and there are 50,000 records in DF2 with columns lat2 , lon2 and zip . I want to apply a function f ( lat1 , lon1 , lat2 , lon2 ) which calculates the distance between two points ( defined using lat1 , lon1 , lat2 , lon2 ) . I ultimately want to add zip from DF2 into Df1 corresponding to the record in D2 corresponding to the smallest distance between that row in Df1 and all rows in D2 .
For all 110k+ records in ` df1 ` do you want to apply your distance function for every record in ` df2 ` ?
pandas apply function to multiple columns and multiple rows
I have a dataframe with consecutive pixel coordinates in rows and columns ' xpos ' , ' ypos ' , and I want to calculate the angle in degrees of each path between consecutive pixels . Currently I have the solution presented below , which works fine and for teh size of my file is speedy enough , but iterating through all the rows seems not to be the pandas way to do it . I know how to apply a function to different columns , and how to apply functions to different rows of columns , but can't figure out how to combine both .
I compared the time of three solutions for my df ( the size of the df is about 6k rows ) , the iteration is almost 9 times slower than apply , and about 1500 times slower then doing it without apply :
execution time of the solution without iteration , with apply : 0.17s
execution time of accepted answer by EdChum using diff() , without iteration and without apply : 0.001s
Suggestion : do not use iteration or apply and always try to use vectorized calculation ;) it is not only faster , but also more readable .
Also if possible avoid using ` apply ` , as this operates row-wise , if you can find a vectorised method that can work on the entire series or dataframe then always prefer this .
NOTE : I selected the ' size ' column because all the functions apply to that column . If you wanted to do a different set of functions for different columns , you can use ` agg ` with a dictionary with a list of functions e.g. ` agg ( { ' size ' : [ np.sum , np.average ] } )` . This results in ` MultiIndex ` columns , which means that when getting the IDs for the maximum size in each group you need to do : #CODE
I've tried using the ` apply ` function across the column , but to no avail . So , I took a very naive ( but not very concise ) approach to create these columns : #CODE
Rather than fill as an empty column , you can simply populate this with an apply : #CODE
Perform a ` groupby ` by ' Country ' and use ` transform ` to apply a function to that group which will return an index aligned to the original df #CODE
I'm aware of df.where function but apparently it's not possible to apply for columns , it works just for all DataFrame .
Why not use the built in string method , rather than apply . ` df [ 0 ] .str .count ( ' : ')`
@USER using that function is actually slower than using ` apply ` ( see my answer ) .
Looking for a better method to achieve desired output as shown . I have messed with lambda , apply , aggregrate commands can't quite get anything to work . #CODE
I realize there are different settings one can apply that will change the look and feel of either individually ( matplotlib savefig() plots different from show() ) , however I haven't been able to find any easy to follow documentation that shows how to set the default fonts while using matplotlib backend : MacOSX . Can someone show how to make the font that appears in the shown figure also appear in the saved figure ?
BUT so far we can't apply the right DatetimeIndex to ts.resample()
I think to use ` apply ` in this case will be difficult as it is conditional ( based on the surrounding cells ) . I think you many want to generate separate ` DataFrame ` s for ` Around_A , B .... ` . Once you get those , you can use ` dropna() ` to get rid of the rows containing ` nan ` s , which will make the dataset much smaller and may avoid the memory issue altogether .
Basically I want to create a new column " Ratio " that divides Price / Buy or Price / Sell , depending on which abs ( buy ) or abs ( sell ) is greater . I am not really sure how to do this ... would I use an apply function ?
Here is the solution using apply - First define a function operating in rows of the DataFrame . #CODE
Finally , set the ` Ratio ` column appropriately using apply .
Thanks this works ! But is there a way to do it with an apply function or something as well ?
i am looking to apply multiply masks on each column of a pandas dataset ( respectively to it's properties ) in python .
how can i apply the concat_mask on df , so that i select rows , in which all Boolean criteria are matched ( are True ) ?
Thanks for Your answer . In the proper code i actually iterate throw all columns and apply various of diffenrent conditions to mask each column . This is all what the code ment to express .
not yet #URL ( but you can do it in the apply ; this is for using an aggregate function ) .
Since we're using views here , this should be more efficient / faster than the apply ...
Pandas rolling apply with variable window length
I'm trying to reduce meterological data using ` pandas ` 0.13.1 . I have a large dataframe of floats . Thanks to this answer I have grouped the data into half-hour intervals most efficiently . I am using ` groupby ` + ` apply ` instead of ` resample ` because of the need to examine multiple columns . #CODE
I want to use ` math.atan2 ` on the ' Ux / Uy ' columns and am having trouble successfully ` apply ` ing any function . I get tracebacks about attribute ` ndim ` : #CODE
My original question was : what kind of value should be returned from my ` apply ` ed function so that a groupby-apply operation results in a 1-column DataFrame or Series with a length equal to number of groups and group names ( e.g. Timestamps ) used as index values ?
this will be much more efficient to not use apply at all , rather compute the mean aggregates first , then use np.atan2 . I'll put up an example tomorrow
Just looking at your exception , looks like you're trying to apply function to each row but didn't specify axis=1 e.g. df.apply ( f , axis=1 ) #apply function to each row
Pandas uses the index " line up " operations in that the operation will apply only to the common indices . So if you want to subtract one row from all in a DataFrame then you need to convert that to a numpy array first as shown in the answer .
then apply the filter like so :
well you can simply access the subfeatures field in the apply then .
It doesn't affect you as its not visible to you . This tests whether you are mutating the input in the apply or not . Just use `` apply `` or iterate over the groups .
Thus in a non-trivial computation it is essential that you use the built in functions . Using ( apply / aggregate ) is nice for a generalized function evaluation , but pandas cannot make too many assumptions about what is going on in the user function , and these are evaluated in python space . #CODE
if you combine this with another apply , you'll get info for the total columns #CODE
I've come up with this , using itertools , to find mid-day timestamps and group them by date , and now I'm coming up short trying to apply imap to find the means . #CODE
There might be , but AFAIK , there is no way to use bracket indexing ` df1 [ ... ]` or any of the indexers like ` .loc [ ]` , ` .at [ ]` , or ` .ix [ ]` to accomplish this easily . Each of these returns " rectangular " DataFrames , whereas you want to select a * sequence * of individual values using row and column label coordinates . There is a way to use ` df.apply ` -- there almost always is -- but I think ` apply ` should be avoided when possible because it is often a relatively slow alternative . ( Under the hood it uses a Python loop which calls a Python function for each row or each column ... a recipe for slowness . )
and when I try to apply pct_change : #CODE
So , I have created a Series with C1 , C2 and C3 as the values - one way top count this is to loop over the rows and columns of the DataFrame and then over this Series and increment the counter if it matches . But is there an ` apply ` approach that can achieve this in a compact fashion ?
You could apply ` value_counts ` : #CODE
You can just apply ` dropna ` to ` a12 ` before ` merge ` : #CODE
Pandas : apply tupleize_cols to dataframe without to_csv() ?
pandas apply np.histogram to reshape dataframe
` np.histogram ` is neither a reducer ( returns a single value ) , nor a transformer ( returns the same number as the input ) . So ` apply ` doesn't know how to map the return values .
Here is another way ( and conceptually how to think about apply ) #CODE
Perfect , thanks ! I knew I was missing something about the internals of ` apply `
The code above obviously does not work . It is not clear to me how to correctly pass the fixed ` y ` to the function while having ` apply ` iterating through the ` x ` columns ( ` x1 ` , ` x2 ` , ... ) . I suspect there might be a very clever one-line solution to do this . Any idea ?
The function you pass to ` apply ` must take a ` pandas.DataFrame ` as a first argument , you can pass additional keyword or positional arguments to ` apply ` that get passed to the applied function . So , your example would work with a small modification . Change ` ols_res ` to #CODE
Then , you can use ` groupby ` and ` apply ` like this #CODE
Use ` groupby ` and we can pass a dict of functions to apply to each column , for ` WL ` column we apply ` count ` from ` pandas.Series ` , the ` all ` applies a test on all values and returns ` True ` if all values in the series are ` True ` and ` False ` otherwise . #CODE
In order to assign these values back to the original dataframe you can use ` transform ` , unfortunately I couldn't figure out how to apply different functions to different columns as transform won't accept ` agg ` function or a user defined function .
Pandas groupby apply how to speed up
rolling apply for a binary ( or n-ary ) function in pandas
To do this , I have the following function that I would like to use a rolling apply with - all this does is calculate covariance assuming zero mean if not centered and calculate the usual covariance when it is centered . #CODE
You can just call ` apply ` and access the ` time ` function on the datetime object create the column initially like this without the need for post processing : #CODE
it does not apply operator by elements , but returns a 2*n DataFrame of NaNs : #CODE
this uses indexing by column name , and doesn't use logical operators on columns , rather than that it traverses rows with apply function : #CODE
thanks for the reference although I haven't been able to apply it to my specific problem .
There's also evidence that statsmodels supports timeseries from pandas . You may be able to apply this to linear models as well :
It worked fine with the test data ( 200 lines ) but gives me the following error when I apply it to the real data ( 20 million lines ): #CODE
use a lamda apply to pass groups to the function
This will work by calling ` apply ` and passing param ` axis=1 ` to apply it row-wise : #CODE
You simply ` groupby ` your ` time ` column and then apply the ` mean ` method to each element . See documentation here . #CODE
You won't be able to merge using a partial match , you'd have to merge what you can and then perform a lookup for the other rows , I've done this before where there were inexact matches . You have to write some function and then apply it row-wise to your merged dataframe
and I'm trying to apply a transformation in order to get a dataframe that looks like the following #CODE
My question is how can I apply this function to several classifiers and append their result as a long data frame like #CODE
` d.index = d.index.apply ( lambda x : x.time() )` won't work . I finally managed to do it by doing a reset of the index , apply , and set again :
A simple method would be ` df.loc [: (( df [ ' A '] == 0 ) & ( df [ ' C '] == 0 )) .idxmax() ]` but this doesn't apply for every id
@USER you can just groupby id then apply idxmax ( filter before the groupby )
I've been trying to apply your code to my dataset but keep running into MemoryErrors . My local machine only runs python 32 so i switched to an Amazon m3.medium ( 1 VCPU 3.75GB Mem 64-bit python ) but even this instance Kills the job . How can you achieve these kinds of runtimes ?
I have a pandas dataframe with mixed type columns , and I'd like to apply sklearn's min_max_scaler to some of the columns . Ideally , I'd like to do these transformations in place , but haven't figured out a way to do that yet . I've written the following code that works : #CODE
I know that I can do it just in pandas , but I may want to eventually apply a different sklearn method that isn't as easy to write myself . I'm more interested in figuring out why applying to a series doesn't work as I expected than I am in coming up with a strictly simpler solution . My next step will be to run a RandomForestRegressor , and I want to make sure I understand how Pandas and sklearn work together .
Using apply is always the last operation to try . Vectorized methods are much faster . #CODE
@USER That works for the entire dataframe easily . How would you apply this to a single column ? ` df [( ' date ' , '' , '')] .swaplevel ( 0 , 2 )` did not work .
How can I apply this process to all the columns I want at once and produce a dataframe of it all ? Sorry if this is a repeat ; the pandas questions I've found that seem to be about related topics are all over my head .
I have 3,000 .dat files that I am reading and concatenating into one pandas dataframe . They have the same format ( 4 columns , no header ) except that some of them have a description at the beginning of the file while others don't . In order to concatenate those files , I need to get rid of those first rows before I concatenate them . The ` skiprows ` option of the ` pandas.read_csv() ` doesn't apply here , because the number of rows to skip is very inconsistent from one file to another ( btw , I use ` pandas.read_csv() ` and not ` pandas.read_table() ` because the files are separated by a coma ) .
I thought I could apply a list of strings , but it does not work either , because it is ` not in the ColumnDataSource ` . If I zoom in deeper , the numbers are getting even less meaningfull . Then it might say ` 03 ` , but 03 of what ? At which minute , which hour ? Is there a solution for this ?
python pandas : groupby apply function looks at prior rows
so I've updated my code below , maybe I'm not understanding how apply works , but I thought this would execute twice ( once for each group ) . Then , my function would loop over each row within those executions . I'm still puzzled as to why it's going 3 times ... I thought " executed " would print 5 times . Thoughts on this ?
its not specific to apply , but more general in groupby , nor is ever point mentioned in the doc-string .
I tried to apply this method to each subset of data with a nested-loop script : #CODE
Grouping data frames and applying a function is essentially done in one statement , using the ` apply ` -functionality of pandas : #CODE
pandas : apply a function to the many columns of a large DataFrame to return multiple rows
Taking the idea from From this answer : pandas : apply function to DataFrame that can return multiple rows
If you have " oddly-shaped " json , then you can either ` json_normalize ` when reading , or parse the columns which contain multiple columns after reading in the DataFrame ( e.g. using a Series string method or apply ) .
just use an ` apply ` with a function that creates a dictionary based on the ` str.count ` of the substrings
Well , I am creating somewhat of a randomizer for an experiment . In order to counterbalance appropriately , I want to be able to randomize the rows and the columns independently from each other , but the data inside the table isn't all ints , but rather , lists of strings , dictionaries , and such . That said , I am trying to find out if there is a way to basically do what was done in the link I posted ( randomize column-wise ) and apply that to rows . I was able to make this work , but only if the dataframe contains numbers only , though I want to extend the possibility to strings and such .
Tried using : apply ( pd.Series.interpolate ( method= ' linear ')) however I get the following error :
My idea was to apply this function ( or similar ) column wise . Have played with .apply() but because its a double ( or triple ) function call i.e. f1 . ( ) .f2 ( x , y ) or f1 . ( ) .f2 ( x , y ) .f3 ( x , y ) it gives me an error . Any ideas would be greatly appreciated and I think this would be a very useful bit of code to have out there !
how to apply preprocessing methods on several columns at one time in sklearn
My question is I have so many columns in my pandas data frame and I am trying to apply the sklearn preprocessing using dataframe mapper from sklearn-pandas library such as #CODE
Typically everything that you do within ` groupby ` should be group independent . So , within any ` groupby.apply() ` , you will only get the group itself , not the context . An alternative is to compute the ` index ` value for the whole sample ( following , ` index `) out of the indices for the groups ( here , ` selected `) . Note that the dataset is sorted by groups , which you need to do if you want to apply the following .
Thanks @USER for the quick reply . Unfortunately it does not work for me : I get ` ValueError : level > 0 only valid with MultiIndex ` when trying to apply your line ` c = test.groupby ( level=1 ) .count() ` . I think it must be due to the fact that my original data is not indexed as yours ( 0 , 1 , 2 , 0 , 1 , 2 ) but ( 0 , 1 , 2 , 3 , 4 ,... ) . Did you already group text by ' name ' before applying your solution here ?
Sounds like you are on the right track . Creating two groups is a good approach . Since your precip_avg starts as " the average precip level for all Jan 1st 2pm across all years " this average will not change when you apply other filters . The average would be the same if you looked at the past 50 years or the past 5 years . This may be desirable or it may not be .
Previously answered questions don't seem to apply . Someone good with lambda functions or the .asfreq method might be able to come up with something . #CODE
Using StackOverflow and the documentation I have only been able to find how to apply a function dependent on a single variable to more than one column ( using the axis option ) . Please help .
This will be much faster than performing an apply operation as it is vectorised .
@USER you're welcome , you can accept this as answer , there will be a tick mark underneath the voting buttons . Using apply and iterating should always be the last choice , if possible find a method that operates on the whole dataframe
This means that all you have to do to group by year is leverage the apply function and re-work the syntax
Dynamically creating variables , while doing map / apply on a dataframe in pandas to get key names for the values in Series object returned
It looks like THIS loop is the killer here.Also , intutively , looping on a dataframe is a BAD practice . How can I rewrite this , perhaps using Map / Apply ? #CODE
I also know that ` df2.apply ( funct1 , axis=1 )` contains part of mycustom " names " ( ie feature values ) , how would I then build these names using map / apply ?
Ie . I will have the values , but how would I create the " key " `' P_ ' +feature_name+ ' _ ' +feature_value+ ' _C '` , since feature value post apply is returned as a series object .
Gregor - +1 for making the time & effort . I have to admit that itertuples() struck me , omly when I saw the tuples in your code . I am not accepting , since I am still looking at a map / apply way ( possibly ) to solve this .
itertuples() is what worked for me ( worked at lightspeed ) - though It is still not using the map / apply approach that I so much wanted to see . Itertuples on a pandas dataframe returns the whole row , so I no longer have to do ` df2 [ df2 [ feature_name ]= =feature_value ] [ ' click ']` - be aware that this matching by value is not only expensive , but also undesired , since it may return a series , if there were duplicate rows . itertuples solves that problem were elegantly , though I need to then access the individual objects / columns by integer indexes , which means less re-usable code . I could abstract this , but It wont be like accessing by column names , the status-quo . #CODE
Then , pass that into the ` groupby ` object with apply . #CODE
the final line uses the apply method , with the paramater key set to 1 , which applies the method -first parameter - row wise along the DataFrame and Returns a Series which is appended to the DataFrame .
How can I accomplish this concisely and efficiently ? I've tried using combinations of ` groupby ` and ` apply ` , but I'm new to PANDAS and keep throwing Exceptions .
next you just apply a lambda function that finds the union between columns . Had trouble finding a quick method for the union but this works #CODE
Pandas apply and lambda function efficiency
The ` apply ` operation of pandas is quite expressive , I could first ` group ` , and then do the Cartesian product on each group using ` apply ` , and then aggregate the result using ` sum ` . The problem with this approach , however , is that ` apply ` is not lazy , it will compute all the intermediate results before the aggregation , and the intermediate results ( Cartesian production on each group ) is very large .
Intermediate result after the ` apply ` ( can be large ) #CODE
Could you give a small self-contained example with fake data to show what you want ? ( E.g. I don't see why you can't move a summation into the apply to avoid expanding more than one group at a time , but maybe that doesn't work in a real case for some reason . )
Why are vectorized operations like apply so much quicker ? I imagine there must be some row by row iteration going on there too .
`` apply `` is NOT vectorized . `` iterrows `` is even worse as it boxes everything ( that ' the perf diff with `` apply ``) . You should only use `` iterrows `` in very very few situations . IMHO never . Show what you are actually doing with `` iterrows `` .
3 ) Apply involves can usually be done by an iterator in cython space ( this is done internally in pandas ) ( this is a ) case .
This is dependent on what is going on inside the apply expression . e.g. ` df.apply ( lambda x : np.sum ( x ))` will be executed pretty swiftly ( of course ` df.sum ( 1 )` is even better ) . However something like : ` df.apply ( lambda x : x [ ' b '] + 1 )` will be executed in python space , and consequently is slower .
The examples I've seen using ' map ' or ' apply ' generally show one datatable which seems intuitive enough . However , I am working across two tables and they are large ( T1 is 2.5million rows , T2 is 96000 rows ) .
Unfortunatley not . I get this message : " ValueError : array is too big . " I'm pretty sure that I will get 5 billion rows having looked into the data ( I agree it is not creating a cartesian product ) . I plan on trying itertools with the groupby feature . I might make two grouped objects , one for each table , to start . Then iterate to find the " matching " groups . I will then merge and apply on each as you have done , aggregating to a new table . If you know how to do that I'd be grateful to see it on this ( tiny ) example . If I succeed , I'll post it myself :)
I cannot apply a rolling window because this would first be daily and secondly I need to specify the number of values ( a rolling window does not aggregate by time frame , some posts addressed this issue but they are not relevant to my problem as the rolling would still be for each new day ) .
I cannot apply resampling , because then the sample would be every 5 months , e .. g I would only have values for May 2012 , Oct 2012 , March 2013 ... Finally , as the function is not linear I cannot reconstruct it by first doing a monthly sample and then applying a 5 period rolling window on it .
To clarify : I am looking for 5 calendar months ( data is not necessarily evenly spaced ) , including the current month , so for May 2012 I go from Jan 2012 to May 2012 ( the length of the windows is 5 months , regardless if I have only one day per month or 20 ) . User @USER is correct , in addition I only care of a monthly result , so I need to apply the same for June 2012 , July 2012 , etc .
If pandas has imported you date and time data , you should be able to get select data from given months using the syntax ` dft [ datetime ( 2013 , 1 , 1 ): datetime ( 2013 , 6 )]` . Just program a loop or equivalent to cycle the start and end month values and apply your function to the values in the resulting dataframes . ( Sorry , I don't have a date stamped data set handy to test this myself right now )
I would consider yours a dupe of [ this ] ( #URL ) question , but the accepted answer there is not what I would use . Still , the [ higher voted answers ] ( #URL ) there apply to your situation .
Call ` apply ` on the dataframe ( note the double square brackets ` df [[ ' A ']]` rather than ` df [ ' A ']`) and call the string method ` isdigit() ` , we then set param ` axis=1 ` to apply the lambda function row-wise . What happens here is that the index is used to create a boolean mask . #CODE
This will be considerably faster that using ` apply ` on a larger frame as this is all implemented in cython . #CODE
Do you want ` transform ` so that it returns an object with its index aligned to the original dataframe ? like ` b = stock_data_df.groupby ( ' stock id ') .transform ( apply ( lambda x : x [ ' price '] > = pd.rolling_max ( x [ ' price '] , 20 )))`
Not sure if you so the edit , but I tried ` b = stock_data_df.groupby ( ' stock id ') .transform ( apply ( lambda x : x [ ' price '] > = pd.rolling_max ( x [ ' price '] , 20 )))` and then I get ` TypeError : ( ) takes exactly 1 argument ( 0 given )`
edit : I shoudl add that I already know what the 95%CI values are . This is just a plotting question ( how to apply the axvline to each of these subplots ) . Thx .
Note that when setting properties of an axis using its methods , most of the ` plt ` attributes become ` set_X ` . For example , instead of ` plt.ylabel ( ' my_y ')` you do ` ax1.set_ylabel ( ' my_y ')` . You can still use the ` plt ` methods , but they will apply to whatever the current plot is . The variables ` ax1 ` and ` ax2 ` give you a little more freedom about when you do things .
You have the right idea with groupby . It has the ability to split up your data by the day then give you access to those groups . The trick here is using the apply method on the Series df [ ' date_time '] . Apply on a series applies the input method element wise and returns a new Series . You can use this to split up by days and then again to split up by hours .
But in the code below , it seems there are just too many lines . That is , the rank() function in pandas is super convenient . Seems to me there should be some parameter somewhere that says to the data frame , " Hey , apply this function you already know about , but instead of doing to the original column itself , as you do it , make it a new column at the end of the data frame "
Please explain exactly what you want to achieve , in general you want to avoid any form of iteration and using apply if the calculation can be vectorised . It looks like you are just adding a new column , in which case just do df [ ' new_col '] = some_calc_on_df . If you want to append another dataframe which has the same index then use append or concat , no need to merge unless the order is different and you want to join on some id column
You are getting this error because the function you are passing to apply doesn't return anything . If all you care about is the printed output , you could just return the df back , like this . #CODE
Then the apply will run through without error . #CODE
For example , if the value in float_col is greater than 5 , I want to multiply the value in in_col ( in the same row ) by 2 . I'm guessing I'm supposed to use one of the ` map ` ` apply ` or ` applymap ` functions , but I'm not sure which , or how .
Apply regex replace to python pandas data frame
For some reason , the function runs properly on strings , but when running it on the data frame with apply it returns empty strings , and not the first three octets .
Turns out that the errors raised on version 0.12 * should * be raised on 0.14.1 . The bug here is that ` Grouby.filter ` should apply to the entire subframe , not rows within the subframe .
I am calculating a series by multiplying two columns of a dataframe . I apply groupby on that series . Get ` ValueError : Buffer has wrong number of dimensions ( expected 1 , got 2 )`
python pandas apply function group by group
My question is : can I avoid to iterate group by group to apply my_stat_function , and is there exist something faster , maybe applying the function apply ? I would really like something more " pandas-ish ' and faster .
You can apply functions to groups : ` df.groupby ( ' user_id ') .apply ( my_stat_function )` or similar , have you tried this ?
Thank you for your help , my problem is that I don't know how to define my_stat_function in order to apply it like this , because I need the full data of each group , It is not a row by row execution . Do you see what I mean ?
You could groupby user and apply the function , you'd have to rewrite your function though
generally speaking you don't want to iterate through a dataframe . look into the ` apply ` method : #URL Also , the point of stackoverflow is to be a resource for future readers who might have a similar question . There are currently over 7100 pandas questions . Your title , as it current reads , will not at all help future readers understand what the topic of this question is .
You can then apply some boolean logic to find the count which you're interested in : #CODE
Python : Pandas : Speeding up an Apply Function
I am trying to do a pandas apply function on a 33 MB dataframe ( in CSV form ) and it is going incredibly slow . And I am trying out figure out why . I was doing an apply on a much bigger dataframe ( 16 GB ) and it finished in about 6 hours . This function is operating on a much , much smaller dataframe and I let it run for 1.5 hours and still nothing .
PS- also if someone knows how to add a progress bar on an apply function that would be a great added bonus :) Thanks again ! #CODE
Then , rather than using apply , you could merge your existing data against the stock df , something like this : #CODE
If you want to fill missing values , rather than having custom logic in apply , it's much faster use ` fillna ` #CODE
You need to combine the functions that apply to the same column , like this : #CODE
Then apply to the columns and assign : #CODE
Why ? I thought I could pass any function to ` apply ` on a group .
@USER I agree about ` transform ` , although I thought you can mimic the behavior of ` transform ` with apply ( which is more generic )
If you want to compute stats across each set of rows with the same index in the two datasets , you can use ` .groupby() ` to group the data by row index , then apply the mean , median etc . : #CODE
much faster than my general function , you can also use apply on this for more general functions
Is this something I would use ` groupby ` for and then apply a function to it ? I tried doing a ` groupby ` for the cust_id and date columns , but I was given an object so I'm not sure if it is formatted properly .
You can just get a list of the columns you want to multiply the scores by and then an apply function ... #CODE
Both of you have the desired answer . I really appreciated it . In fact I have a more generalised question . I have a function which takes a couple of column values as input and outputs an value and I wanna apply that function to each row in a dataframe . Is that possible not to use for loop to achieve that ? thanks in advance .
This roundabout method of converting my ` array ` to a nested ` list ` and then converting it back to an array via ` apply ` is bothersome . Is there a more straightforward way that I'm just not aware of ?
I don't think that's a supported use case for DataFrames ; while you can cram nonscalar data into a cell , there's not much you can do with it after that . You'll have a column dtype of object , which is slow to begin with , and you can't really do any fast aggregation ops , so you'll have to fall back to relatively slow apply ops . Depending on preference you might be more interested in using a MultiIndex or a Panel instead of this approach .
You can perform a ` groupby ` on ' Product ID ' , then apply ` idxmax ` on ' Sales ' column .
And a function , which I want to apply to each row , storing the result into a new column . #CODE
You can use ` apply ` with the ` axis=1 ` argument to apply by row .
The more general apply will be slower . It's better to find a way to vectorize the operations . When you have more data a general apply will not scale very well especially the row by row version since each row is converted to a series of uniform type which if you have mixed types will be very annoying to use and inefficient .
Actually most of those are implemented in Cython which speeds up loops considerably . By vectorization I simply meant applying operations on whole sequences rather than single elements at a time , which is unrelated to the use of BLAS . What I'm saying is that spending a bit of time trying to avoid apply will probably yield reusable and more performant code .
Try the simplest of operations : string concatenation , with two Series of length 1,000,000 . Do this by adding them together directly ` a + b ` then try putting them in a ` DataFrame ` and calling ` df.apply ( lambda x : x.a + x.b , axis=1 )` . The latter takes an unbearably long time ( about 22 seconds ) where as the former takes about 60 milliseconds on my machine . Trust me , the string methods in pandas are orders of magnitude faster than calling ` apply ` .
What you are looking for is ` apply ( func , axis=1 )` This will apply a function row wise through your dataframe .
I have a function which returns a list of length 2 . I would like to apply this function to one column in my dataframe and assign the result to two columns .
That's too bad , since performance is much better when you can apply functions to whole Series rather than to individual values one-at-a-time .
2 ) group it up and apply a function to index your values . #CODE
How to apply a function ( numpy ufunc ) with two array arguments to one pandas Series ?
As for ` apply ` , you're looking at the wrong documentation . You're looking at ` Dataframe.apply ` , but you have a series , so you should be looking at ` Series.apply ` . ` Series.apply ` doesn't take a ` raw ` argument .
You might think that removing the ` raw ` argument would fix your attempt , but ` Series.apply ` has a peculiar behavior where if ` f ` is a ufunc and no keyword arguments to ` f ` are supplied , it completely ignores ` args ` . I think this is actually a bug . The workaround is to not use ` apply ` for this ; the broadcasting rules make ` apply ` redundant for your situation .
Beautiful . I still don't know how apply should work if I need it again , but this solves the problem at hand . Thanks .
Actually , note that this is not really an answer for the question as stated . I would give you credit , but this would mislead others finding the question for an answer on ` apply . ` Sorry
@USER szl : Well , this * is * how you apply a NumPy ufunc in the way you want . I've expanded the answer explaining why your attempt failed ; does that answer your question ?
Thanks , I accepted the answer . Maybe you could be even clearer about no use of apply could broadcast ( if that's the right word ) the second argument if the ufunc expects an array .
2 ) apply it to your Series after converting to dataframe #CODE
to individually query each column and find the information I'm looking for but this is tedious even if I figure out how to use the abs function to combine the two queries into one.How can I apply this filtration to the whole dataframe ?
You don't have to apply the filtration to columns , you can also do #CODE
I use this function with pandas to apply it to each month of a historical record : #CODE
and when I apply the command pd.to_datetime() to these columns I get fields resulting that look like : #CODE
I would like to create a new column ` time_hour ` . I can create it by writing a short function as so and using ` apply() ` to apply it iteratively : #CODE
You can apply a lambda , e.g .
how to apply different functions to each group of pandas groupby ?
I want to group the dataframe by the column ' type ' and apply different function to each group , say , ` min ` for group with type A , ` max ` for group with type B and ` mean ` for group with type C .
I think you might be misunderstanding the intent behind ` groupby ` . No worries , it happens to the best of us too . The intent behind ` groupby ` is such that you can apply the same operations to subgroups of your data , as grouped by the ` groupby ` operation .
My system has 16gb of RAM and is running Debian ( Mint ) . After creating the dataframe I was using ~600mb of RAM . As soon as the apply method began to execute , that value started to soar . It steadily climbed up to around 7gb ( ! ) before finishing the command and settling back down to 5.4gb ( while the shell was still active ) . The problem is , my work requires doing more than the ' do_nothing ' method and as such while executing the real program , I cap my 16gb of RAM and start swapping , making the program unusable . Is this intended ? I can't see why Pandas should need 7gb of RAM to effectively ' do_nothing ' , even if it has to store the grouped object .
That example is somewhat pathological . The groupby creates a separate group for each distinct value . Since you generated the values as random floats , it's likely that they are all distinct , which means there are 3 million groups . Each group passed to your ` do_nothing ` is a DataFrame , so you are creating 3 million DataFrames ( which ` apply ` then has to aggregate into a single result ) . Even if each has only one row , this is a lot of overhead . It might be more illuminating to create an exmaple whose " groupiness " ( i.e. , number of distinct groups ) is more in line with your actual data .
I need to expand this as matrix . How to do that ? My first thought was iterate through the rows and apply numpy.hstack for joining , store it and numpy.vstack the stored rows , but it doesn't work as intended .
I had the same issue : This does it all in place using pandas apply function . Should be the fastest method . #CODE
A more memory efficient way to do this . The key is to apply ` usecols ` in ` pd.read_csv ` . #CODE
Looking at your code , it seems you could use pandas built in moving average / sliding windows functionality , combined with a group by and apply .
this makes me think that there must be an easier , perhaps vectorized way ... perhaps using some kind of " apply " , however i'm not sure how to do that when each column needs to be shifted down as a function of its position in the array .
Apply still uses loops by the way
Just apply the function directly - I guess this will take more CPU as it's calculating all the maxes , then just getting the ones you want , but doesn't create a new variable . #CODE
Then apply a groupby on the first level of the MultiIndex to apply the operation you want . #CODE
That isn't fully vectorized , though , because of the ` apply ` . Something like ` np.isfinite ( df ) .sum ( axis=1 ) -1 ` should bypass all Python loops .
I am trying to understand how to apply function within the ' groupby ' or each groups of the groups in a dataframe . #CODE
I'm having some trouble figuring out what I'm doing wrong here , trying to append columns to an existing pd.DataFrame object . Specifically , my original dataframe has n-many columns , and I want to use apply to append an additional 2n-many columns to it . The problem seems to be that doing this via apply() doesn't work , in that if I try to append more than n-many columns , it falls over . This doesn't make sense to me , and I was hoping somebody could either shed some light on to why I'm seeing this behaviour , or suggest a better approach .
In general you want apply to return either :
why are you showing using apply anyhow ? this should just be column assignment , no ?
Here are solutions using apply . #CODE
Can't think of anything great for unique . This uses apply , but may be faster , depending on the shape of the data . #CODE
apply function on dataframe involving two rows
I want to apply a function to calculate distance based on the longitude and latitude . Basically I need a way to express the function can handle two adjacent rows in dataframe
I know there are ways to apply function along axis 1 and 0 , but they seem only apply to single row or column . How can I express something involving several rows or columns .
Apply won't be able to this , but you can do something simple like the following : #CODE
I would like to group the series by hours or days and apply a function group-wise which calculate the ratio #CODE
Why does the second block of code not work ? Doesn't DataFrame.apply() default to inplace ? There is no inplace parameter to the apply function . If it doesn't work in place , doesn't this make pandas a terrible memory handler ? Do all pandas data frame operations copy everything in situations like this ? Wouldn't it be better to just do it inplace ? Even if it doesn't default to inplace , shouldn't it provide an inplace parameter the way replace() does ?
No , apply does not work inplace* .
In general apply is slow ( since you are basically iterating through each row in python ) , and the " game " is to rewrite that function in terms of pandas / numpy native functions and indexing . If you want to delve into more details about the internals , check out the BlockManager in core / internals.py , this is the object which holds the underlying numpy arrays . But to be honest I think your most useful tool is ` %timeit ` and looking at the source code for specific functions ( ` ?? ` in ipython ) .
* apply is not usually going to make sense inplace ( and IMO this behaviour would rarely be desired ) .
Just apply it to every column : #CODE
There's nothing built-in , so you'll need to calculate it with apply . For example , for an easy ' how many 7 day periods have passed ' measure . #CODE
JohnB - Yes I know how to group-by based on the location . That it actually included as part of the question . But when you group by you end up with a group-by object which you can iterate over . for a specific group is there a way to apply a shift to it . Or do I need to iterate over it . Also once you have the groups there seems to be options to process the data but not add another column to the group . This is kind of where I'm stuck .
Use transform ( instead of apply ) , as chrisb suggests , and assign the result to ` portfolios [ columname ]` : #CODE
Thanks @USER , thanks for code , I have small problem now , two months of cvs1 has 50,000 data Value and two months of csv2 has 4,000 data Value and so on , it's really hard to fit all the graph for two months at same size , [ link ] ( #URL ) from the graph , only two months of data is shown but all the signals doesn't fit . Python is plotting based on the data points but I want to plot based on days ( for ex : 2 months / 60 days ) . Is there any logic to apply , or methods , any suggestions are highly appreciable , since I am in this for long time . Thank you very much .
and then apply the above technique . ( I sometimes toss in a ` reset_index ( drop=True )` , but that's up to you . )
I am trying to run the ` scipy.stats.entropy ` function on two arrays . It is being run on each row of a Pandas DataFrame via the apply function : #CODE
Turns out if any column in the original dataframe is an object , the series created via apply is of object type . Thanks again . #URL
Then define my group operation and apply it : #CODE
Then , using the approach from this answer , pivot into columns , and drop the sentinel . This won't be ultra performant because of the apply , but probably reasonable on data of your size . #CODE
How do I apply filters and functions to previous rows in pandas ?
The proper use of apply is a bit unclear to me .
Further , you should not repeatedly using ` get_group ` , instead use the cythonized functions , ` apply ` , or iteration , see docs here
Say I have a large dataframe and that I want to apply one operation to every element in a column .
I was looking for an option like example 2 , except it won't apply the formatting to future dataframes . Thanks !
You can apply any operator or across a column . To mutate it in-place , just multiply the column by -1 : #CODE
You don't need the apply , just do ` - frame.abs() `
The online docs show how to apply an operation element wise , as does the excellent book
The typical trick is to write a general mathematical operation to apply to the whole column , but then use indicators to select rows for which we actually apply it : #CODE
pandas apply with inputs from multiple rows
I need to do an apply on a dataframe using inputs from multiple rows . As a simple example , I can do the following if all the inputs are from a single row : #CODE
However , if I need ' a ' from the current row , and ' b ' from the previous row , is there a way to do that with apply ? I could add a new ' bshift ' column and then just use df [[ ' a ' , ' bshift ']] but it seems there must be a more direct way .
Point taken about apply not being magically faster . Come to think of it I suppose doing multiple applies could well end up being slower than a single large for loop . The first answer to #URL made me want to try it .
Added a little background . Considering your point about apply and speed , I think what I'll do is try to vectorize as much as I can , and see what I'm left with , and then come back with more questions as I have them . If I tried to post all my code as it is it'd just be overload . Appreciate your advice .
This should directly answer your question and let you use apply , although I'm not sure it's ultimately any better than a two-line solution . It does avoid creating extra variables at least .
Thanks . I timed your first method vs adding a shifted column and then deleting it after I finished the apply , and interestingly they were virtually identical ( and not very fast ) for any df size I tried , so I guess the time is all spent in the apply itself . Indeed the 2nd method ( vectorized ) is much faster and I'll try to do as much of that as possible but unfortunately I don't think it's possible to vectorize all of it .
@USER It's not just the apply , it's also somewhat expensive to concatenate / merge or create a new variable . In my example data , about half the time cost is due to concat and half due to apply . Just replace the apply ( mean ) with mean() and you'll see it's still slower .
Apply function to pandas dataframe that returns multiple rows
I would like to apply a function to a pandas DataFrame that splits some of the rows into two . So for example , I may have this as input : #CODE
Then I would simply apply the extract function with a regex : #CODE
That is , the first two lines together constitute the headers . Is there any way to apply ` read_csv() ` to this without any major hassle ?
I would like to map the values in the Probability column to ' s ' for ' success ' for the first 10 values , and ' f ' for ' fail ' for the rest . To do this , I create a dummy column called Index , apply a transformation , and then drop the dummy column . #CODE
If you want to use ` apply ` it can be done , but you will loose the column names : #CODE
See edit , if you want to use ` apply ` , it can be done but you will loose the column names . Basically the ` apply ` function should return a ` Series ` of length of 3 , compare the two new edits .
Thanks ! Probably the most readable way to do it , although I generally try to avoid apply / lambda if possible for speed reasons if there is a good alternative ( though speed is not a concern here ) .
I'd apply ` value_counts ` columnwise rather than doing ` groupby ` : #CODE
Thanks a lot Jeff , I updated the OP with an example . I basically want to apply the solution that you described to ** each item ** in the series ( i.e. get the offset with respect to ** each timestamp** ' s hour in ` ms `)
See the docs on apply . Pandas will call the function twice on the first group ( to determine between a fast / slow code path ) , so the side effects of the function ( IO ) will happen twice for the first group .
Yeah , or you could even still use apply on the main df , but then change item_grouper to iterate . I didn't step through all the code , but if possible , your life might be easier if you avoid any IO in the groupby and use standard pandas IO ( i.e. ` df.to_csv ( sep= ' \t ')` to write your tab separated file .
So I have a dataset consisting of several million rows of trading data that I am trying to apply a filter to as in the following code . The function ` trade_quant_filter ` looks for outliers and then adds the index of all the outliers to a list to deal with later . #CODE
I'm really struggling with the Pandas ` rolling_apply function ` . I'm trying to apply a filter to some time series data like below and make a new series for outliers . I want the value to return ` True ` when the value is an outlier . #CODE
` group_user= store.select ( ' clean_input ' , where =[ ' user_id == %s ' %user ])` is too heavy in time complexity since I have really a lot of groups , and I am sure there is a lot of redundant sorting in the routine of ` store.select ` if I apply it 10 millions times .
But you have not introduced the variable sub_group_chunk , and If i try chunk.groupby ( sub_group_hash ) I get an error of the form : long object have no attribute _____get.item____ because It tries to apply the function to the index .
In my experience , this approach seems slower than using an approach like ` apply ` or ` map ` , but as always , it's up to you to decide how to make the performance / ease of coding tradeoff .
You could apply ` f ` first , and pass the return value to ` groupby ` : #CODE
pandas apply function to corresponding cells of multiple frames
I'm trying to apply a function to corresponding cells of two identically sized dataFrames to create a new frame .
Did you test if looping is actually too slow ? ( it will depend on the timing of the applied function if the looping will be determinant for speed ) And can the function you want to apply be vectorized ?
_ " What fast methods could I use to load / save the data from disk ? " _ I don't know if this can apply to your use case , but have you investigated [ PyTables ] ( #URL ) ? Its blazing fast at loading data and interfaces nicely with ` numpy ` . Don't know about ` Panda ` though .
You're looking for a groupby with an apply . #CODE
Hi zerovector , I am getting an error when I try to apply that idea : def func ( x ):
Thank you zerovector -- your help put me on the right path to solve the problem ! An apply function was what I was looking for !
I am wondering if there is a way to do a pandas dataframe apply function in parallel . I have looked around and haven't found anything . At least in theory I think it should be fairly simple to implement but haven't seen anything . This is practically the textbook definition of parallel after all .. Has anyone else tried this or know of a way ? If no one has any ideas I think I might just try writing it myself .
That is a good idea . I was planning on doing something much more dynamic but much more complicated . I think your way is much better and simpler though . I will give it a try with the apply function and report back .
( Note that there is no .fit function for OLS in Pandas ) Could somebody shed some light on how I might get future predictions from my OLS model in either pandas or statsmodel-I realize I must not be using .predict properly and I've read the multiple other problems people have had but they do not seem to apply to my case .
You can use numpy where and apply to do it for all columns in a DataFrame : #CODE
I start by chunking the CSV file , and apply a ` groupby ` ( on the two last figures of the ` user_id `) on the chunks files so I can store a total of 100 files containing groups of users , and storing them in a HDF5 store .
The difference lies in the fact that ` grouped.median() ` calls an optimized ( cythonized ) ` median ` aggregation function , while ` grouped.quantile() ` calls a generic wrapper to apply the function on the groups .
So ` grouped.quantile() ` does a general apply and not an aggregation . The reason for this is that ` quantile ` can also return a DataFrame ( and thus is not always a pure aggregation ) , if you calculate multiple quantiles at once , eg with ` grouped.quantile ([ 0.1 , 0.5 , 0.9 ])` : #CODE
In terms of how you make a new column based on other columns - if you absolutely need to iterate , then you can assign using ` loc ` , as you did in one example . But you should always look for a vectorized solution , then look at ` apply ` , and only then think about iterating . See this answer for some more background .
This give yous a boolean mask for groupby of each store for each date where there are exactly two unique ` Item_Id ` present . From this you can now apply the function that concatenates your prices : #CODE
@USER Not quite . ` join ` is shorthand for merging on index with both frames , so the indices need only be consistent ( which it will be here as the apply and col selection don't affect it ) . I'll edit the answer .
You could use lambda function and ` apply ` .
In this case , I made the new index level by taking the first character of the original columns , but of course you can apply another function here if wanted .
As user @USER has pointed out using ` map ` or ` apply ` should be a last resort if a vectorised solution can be applied .
What you wrote is incorrect , you are calling apply on the df but the column as a label does not exist , see below : #CODE
in general I wouldn't show a map / apply based soln if the vectorized one works ( it's confusing and much slower )
I've managed to do this with apply , like this : #CODE
Is there a more efficient way to compare every column in every row in one DF to every column in every row of another DF ? This feels sloppy to me , but my loop / apply attempts have been much slower . #CODE
I forgot to mention , my actual DF's have millions of rows and dozens of columns to compare . With that size , the apply attempts were taking hours .
I would go for apply : #CODE
You can then use apply to concatenate : #CODE
` drop_duplicates() ' is the right function . Coming from sql I immediately thought to GROUP BY . If the fierst column is intger type you suggest to cast to string and the to use apply ?
You can groupby the Label column , apply the list constructor . Here is an minimal example . #CODE
if you want to convert it to strings , you can apply ` strfitme ` ( ` df [ ' timestamp '] .apply ( lambda x : x.strftime ( ' %Y-%m-%d '))`) . Or if it is to write it as strings to csv , use the ` date_format ` keyword in ` to_csv `
The reason I say this is that Jeff has always commented to me that ` map ` and ` apply ` are last resort methods so I thought ` update ` would perform better
I have a large dataframe . I want to groupby three columns in the dataframe , and then apply a function to each group . However , I'm also interested in some groups and keys that are NOT in the dataframe . How do I add those to the groupby object , so I may use ` groupby.apply() ` uniformly on all groups ?
The reason that ` df [ ' column_with_times '] .apply ( lambda x : x.days )` does not work is that apply is given the ` timedelta64 ` values ( and not the ` Timedelta ` pandas type ) , and these don't have such attributes .
I would assume to_excel() would only try to apply the parameter to float-formatted columns ( or even specific cells ) rather than to every piece of data , so I'm not sure what I'm missing . If need be I'll post a cleaned version of the specific table that reproduces the error , but I thought perhaps someone would recognize what I'm facing .
I think you are looking for a rolling apply ( rolling mean in this case ) ? See the docs : #URL . But then applied for each weekday seperately , this can be achieved by combining ` rolling_mean ` with grouping on the weekday with ` groupby ` .
Cleaner pandas apply with function that cannot use pandas.Series and non-unique index
The use case : I want to apply a function to each row via a parallel map in IPython . It doesn't matter which rows go to which back-end engine , as the function calculates a result based on one row at a time . ( Conceptually at least ; in reality it's vectorized . )
Pandas - Create a new column with apply for float indexed dataframe
No worries . As a general rule avoid using ` apply ` if there is a vectorised operation , for basic operations such as add / subtract / div and multiply there are built in operator support for these that are many orders of magnitude faster .
Returning multiple columns with pandas apply and user-defined functions
And I want to use ` df [ ' x '] .apply ( lambda x : fn ( x ))` to return both ` y ` and ` z ` in separate columns . Is there a good way to do this by still using ` fn ( x )` ? In reality , my function will be much more complicated - so I only want to run it once within the apply and assign ` output [ 0 ]` , ` output [ 1 ]` , etc to individual columns .
How about this method ? ( n.b. , I edited this answer in light of the comment below ) so the apply step could take a single function with shared calculations and return the required series for the merge step . #CODE
` apply ( Series )` gives me a DataFrame with two columns . To join them into one while keeping the original index , I use ` unstack ` . ` reset_index ` removes the first level of the index , which basically holds the index of the value in the original list which was in C . Then I join it back into the df .
Anyway , you can do this using apply : #CODE
The function in the apply works by first seeing if the ID is in the dictionary ( and tuples won't be ) and if it is , go for that , and if it isn't find the first one that is ...
This was a bit more annoying to code , basically we can apply a custom function that performs the lookup for you : #CODE
OK but generally using ` apply ` should be a last resort , there are vectorised functions in pandas and numpy that will perform math operations on the whole df , please check the numbers I can't guarantee anything
Compared to the apply function which took 4.3s so nearly 250 times quicker , something to note in the future
Use ` axis=0 ` to apply a method down each column , or to the row labels ( the index ) .
Use ` axis=1 ` to apply a method across each row , or to the column labels .
You can call the ` str ` method and apply a slice , this will be much quicker than the other method as this is vectorised ( thanks @USER ): #CODE
Just out of interest how would I go about apply this to the index column ? As I can't seem to get that to work - I can always just reset_index() the column and do it then ..
Furthermore , the mean ( or whatever other function I want to use to generate a value ) must be based on the original data and applied to new data . Imagine the situation where I calculate mean bad rates for a continuous variable on a training data set , build a model and then have to apply that same transformation logic to new data .
yes , thank you for the suggestions . I should clarify ( and will do so in an edit ) that I need to be able to apply whatever transformation I devise to new data . So I have to save the lookup information somehow so I can use it later .
Thanks for your comments . The documentation I linked says that the ` data ` argument may be a " numpy ndarray ( structured or homogeneous ) , dict , or DataFrame " , and follows this line with one that says " *** Dict can contain *** Series , arrays , constants , or list-like objects " ( my emphasis ) . I interpret this to mean that * when the ` data ` argument is a ` dict ` * , its values can be Series , arrays , etc . IOW , the " list-like objects " bit is not referring to the ` data ` argument itself . I am specifically looking for a ` data ` argument that is a Python list , not a dict , so this clause does not apply .
I'm looking for a method to perform an ANOVA and HSD tests from a dataframe in Python . I tried to read some examples on forums and tutorials but i didn't achieve to apply it to my work .
but i can't achieve to apply them to my example
For pairwise comparison for only some effects , we would need the pairwise comparison after estimating the multiway ANOVA with OLS . This is currently not available in statsmodels . The critical values and p-values of Tukey-HSD would not apply in that case .
What would be possible in this case is to estimate the full model with OLS , define all desired pairwise contrasts , use the ` t_test ` to get the raw p-values for the comparisons , and then apply one of the multiple p-value corrections that are available .
The reason it overwrites is because the indexing on the left hand side is defaulting to the entire dataframe , if you apply the mask to the left hand also using ` loc ` then it only affects those rows where the condition is met : #CODE
How to apply Pandas Groupby with multiple conditions for split and apply multiple calculations ?
I could possibly live without floats and use strings , but curiously the things in my Dataframe appear to BE strings , since when I try to apply the round() function on any value extracted from there , it will protest that the input is not a float ...
I would like to apply dummy-coding contrasting on it so that I get : #CODE
But I need to time where those peaks occur as well . I know I could iterate over the output and find where in the original dataset those values occur , but that seems like a rather brute-force way to do it . I also could write a different function to apply to the grouped object that returns both the max and the time where that max occurs ( at least in theory - haven't tried to do this , but I assume it's pretty straightforward ) .
As an alternative you could index the group by using the ` argmin() ` function . I tried to do this with transform but it was just returning the entire dataframe . I'm not sure why that should be , it does however work with ` apply ` : #CODE
In my opinion the ` transform ` and ` apply ` functions are very opaque and the docs are not a great help . They are however extremely useful once you get to grips with them .
is incorrect , as you have 18:00 : 00 twice for the same date , and in your initial data , they apply to different dates .
it is probably better to do datetime conversion doing ` df [ ' t '] = pd.to_datetime ( df [ ' t '])` rather than call ` apply `
we need to write a apply function first and then use group by . This is my latest guess
There maybe actually something a little cleaner . The solution above is just a general solution for quickly merging a bunch of dataframes . Your particular problem might be more cleanly ( and quickly ? ) solved with a cross-product solution ( #URL ) , but I'd have to think more about how something like this would apply .
I think split is a little more clear than regex but you can ` apply ` any function you choose to a series . #CODE
Sorry , I'm not sure about that . I misread and thought the data was more regular . Perhaps resample() and then apply the above method ? If your data is consistently spaced except for some missing rows , then resampling ought to work fine and be easy to do . I'm sure you could do something with groupby but that could be a lot slower . Maybe someone else will have a better idea though .
Where the " 0 " column is no longer the index for rows . Then we can apply ` df1 = df1 [ df1 [ 3 ] .isin ( df2 [ 0 ])]` . NOTE : application of ` df1 = df1 [ df1 [ 3 ] == df2 [ 0 ]]` will raise the error message ` Series lengths must match to compare `
2 ways , define a func and call apply #CODE
@USER you can upvote too ;) , the thing to take from this is to avoid loops and using apply unless it is not possible , what you want to do is to find if you vectorise your operation , that is perform your operation on the entire dataframe or series rather than a row at a time .
apply is just a loop and should be avoided where possible , your solution is fine , there are many ways of doing what you want . It depends on the size of the data , your sample code could be simplified : ` df.loc [ df [ ' C '] == ' a ' , ' new '] = df [ ' A ']` and likewise for the other condition
You create a lookup function and call ` apply ` on your dataframe row-wise , this isn't very efficient for large dfs though #CODE
There is a built in ` lookup ` function that can handle this type of situation ( looks up by row / column ) . I don't know how optimized it is , but may be faster than the apply solution . #CODE
On the toy dataset apply takes 470us , lookup takes 531us
Hmm for some reason timeit gets a memory error when I try this on even a modest sized df of say 4000 rows , for 400 rows I get 8.17ms using apply and 3.05ms using lookup , so I expect lookup to scale better
You can create a function that determines if the value column ends in `' _regen '` and then apply it your values : #CODE
Yes but how to you actually pass the arguments within the apply function . You can't simply have a.loc [ a [ ' value '] .apply ( has_substring ( s , " regen ")) , ' key '] += ' _regen ' . Within the apply function arguments are passed with the " args= " parameter , something like this a.loc [ a [ ' value '] .apply ( func=has_substring args= " regen " , ' key '] += ' _regen ' How ever I can't get this to work .
What you have will work , args just needs to be an array within the apply function : ` a.loc [ a [ ' value '] .apply ( has_substring , args =[ ' regen ']) , ' key '] += ' _regen '`
You can use ` apply ` to check whether each row satisfy the condition , and use the resulting boolean Series to do the slicing : #CODE
Thank you ! ` apply ` does the trick .
I need the try statement to filter some NaNs ... 1st problem : I couldnt figure out how to use ' apply ' or something instead of the for loop . Does anyone know a more efficient way ? Second problem : #CODE
I use ` np.vectorize ` to apply this function on a ` DataFrame ` - ` dataFrame ` - that has about 22 million rows . #CODE
plz show the related code where you apply ` getData ` over the data-frame
Warning : In the current implementation apply calls func twice on the
@USER I agree that this should raise for now . I wasn't sure if I was using this method wrong but an obvious workaround would be to iterate through each column and apply fillna() with axis=0 . Would it be so hard to incorporate this for when the method is applied to dataframes ?
Hi CT -- so a bit of a problem / update that I am struggling with here . This solution works .. sorta . I tried to apply this solution to my larger data frame and at first glance it worked wonders , but now taking a closer look there are several instances of " looped phrases " that slipped through the cracks . Any idea why this could be ?
You could sort the values in columns ` A ` and ` B ` so that for each row the value in ` A ` is less than or equal to the value in ` B ` . Once the values are ordered , then you could apply ` groupby-transform-max ` as usual : #CODE
I first tried using apply but it's not possible to return multiple Series as far as I know . iterrows seems to be the trick . But the code below gives me an empty dataframe ... #CODE
no need for apply ; you can directly subtract a Timestamp from a column to yield a timedelta64 dtype
Desired output - A new DataFrame of grouped / aggregated coordinates in an array so that I can apply a fuction to each array : #CODE
Distance calculation I wish to apply ... #CODE
note that to apply your distance function you have to do : #CODE
One method could be to apply a lambda to the column and use the boolean index returned this to index against : #CODE
FYI - no need to use apply here ( tz_localize / convert are methods on index and series )
Thanks @USER . I tried that in the past without luck . I opened this question separately : [ Unable to apply methods on timestamps in Pandas using Series built-ins ] ( #URL )
Unable to apply methods on timestamps using Series built-ins
In norm.ppf ( probability , mean , standard deviation ) so 10 is mean and 5 is std . My code will apply the norm.ppf for every element of lts .
Using Apply Map to Remove Unwanted Phrases from DF ( Pandas , Python 3 )
What do you mean by " remove cells " ? I think you mean remove rows right ? Then this will be an ` apply ` since you'll consider things rowwise . #CODE
You can vectorize ' Words1 ' into a series and then apply a regex : #CODE
The second part of my code is function I am trying to apply #CODE
Pandas Groupby Apply Function to Level
Assuming you have a series ` s ` , with a MultiIndex andthe two levels you have shown , you can groupby the first level and apply the `' first '` / `' last '` aggregations to get the values you want . #CODE
What have you tried here ? You could apply your function row-wise which would look something like ` df [ ' ls '] = df.apply ( lambda row : checker ( x.Review ) , axis=1 )` but ideally you want to vectorise your function so that it can be done on the whole column , at the moment your function looks incomplete so it's hard to suggest what improvements can be made
The easiest way would be to specific a single column of the groupby ( doesn't matter which one ) , and use ` transform ` instead of ` apply ` , like this . #CODE
The reason this didn't work while your first did is that your function returns a single value , rather than an array of values , so ` transform ` broadcasts back to the original frame's shape , while ` apply ` is more flexible and generally passes back whatever shape your function returns .
If you just want the column headings , you can apply the mask to itself . #CODE
But what if I want to get the percentage of the vote each candidate got ? Would I have to apply some sort of function on each data object ? Ideally I would like the final data object to look like : #CODE
We can perform a groupby on ' A ' and then apply a function ( lambda in this case ) where we join the desired delimiter ` ; ` with a list comprehension of the B values .
In Python you can join things by using ` some_delimiter.join ( things_you_want_to_join )` , e.g. `' , ' .join ( " abc ") == ' a , b , c '` . We can apply that to the ` B ` column after grouping on ` A ` : #CODE
I wouldn't worry about it , it seems ok but I think you have to show what you've tried at the very least plus any code and approaches , it would help also to show what you've tried that works on simple data i.e. not in a pandas dataframe and are asking how to apply that to pandas would probably help . I guess at this point this looks like an open exercise without demonstrating your efforts
since , per the docs , " [ a ] dditional keyword arguments [ to ` apply `] will be passed as keywords to the function " .
hmmm this gets me sort of the way here but not quite -- how can I apply that if I need to actually CALL the dataframe in the arguement ? Edited OP above
One way would be to create a custom apply function and check each datum's YMD and look up the corresponding low frequency data , but that seems pretty inefficient .
It does what you describe , go row-wise ( so apply over ` axis=1 `) along ` df ` and use the entries as index for selecting in ` P ` .
Parallelize apply after pandas groupby
I have used rosetta.parallel.pandas_easy to parallelize apply after group by , for example : #CODE
I have a hack I use for getting parallelization in Pandas . I break my dataframe into chunks , put each chunk into the element of a list , and then use ipython's parallel bits to do a parallel apply on the list of dataframes . Then I put the list back together using pandas ` concat ` function .
This is not generally applicable , however . It works for me because the function I want to apply to each chunk of the dataframe takes about a minute . And the pulling apart and putting together of my data does not take all that long . So this is clearly a kludge . With that said , here's an example . I'm using Ipython notebook so you'll see ` %%time ` magic in my code : #CODE
write a silly function to apply to our data #CODE
I will try this with my code , thank you . Can you explain to me why apply does not automatically parallelize operations ? It seems like the whole benefit of having the apply function is to avoid looping , but if it is not doing that with these groups , what gives ?
There's a long story about parallelization being hard in Python because of the GIL . Keep in mind that apply is usually syntactic sugar and underneath it's doing the implied loop . Using parallelization is somewhat tricky because there are runtime costs to parallelization which sometimes negate the benefits of parallelization .
By doing small modification to the function it can be made to return the hierarchical index that the regular apply returns :
pandas : how to apply scipy.stats test on a groupby object ?
How to use groupby to apply multiple functions to multiple columns in Pandas ?
Since you are aggregating each grouped column into one value , you can use ` agg ` instead of ` apply ` . The ` agg ` method can take a list of functions as input . The functions will be applied to each column : #CODE
ok , I can combine the function ` value_counts ` and ` groupby ` through ` apply ` function . Thank you very much !
@USER I tried with a 100,000 row dataframe and the difference becomes 11.4ms vs . 8.9ms for my implementation versus yours . There is probably some battle / tradeoff between the vectorised ` value_counts ` and calling ` apply ` , there maybe a better way but I've not figured out a better way yet
Panda's DataFrame dup each row , apply changes to the duplicate and combine back into a dataframe
I need to create a duplicate for each row in a dataframe , apply some basic operations to the duplicate row and then combine these dupped rows along with the originals back into a dataframe .
I'm trying to use apply for it and the print shows that it's working correctly but when I return these 2 rows from the function and the dataframe is assembled I get an error message " cannot copy sequence with size 7 to array axis with dimension 2 " . It is as if it's trying to fit these 2 new rows back into the original 1 row slot . Any insight on how I can achieve it within apply ( and not by iterating over every row in a loop ) ? #CODE
The ` apply ` function of pandas operates along an axis . With ` axis=1 ` , it operates along every row . To do something like what you're trying to do , think of how you would construct a new row from your existing row . Something like this should work : #CODE
But I don't understand how to apply it to my case .
Any idea how to solve this problem in a efficient pandas approach ? ( using apply , map or rolling ? )
What you needed to do from the answer you linked to was to turn the index into a series so you can then call apply on it . The other key thing here is that you also have to index the constructed series the same as your df index as the default is to just create an index from scratch like 0 , 1 , 2 , 3 ... #CODE
You can apply ` merge ` to a list of DataFrames using reduce : #CODE
Probably the simplest solution is to use the APPLYMAP or APPLY fucntions which applies the function to every data value in the entire data set .
Apply Docs
So I have data that I am outputting to an excel file using pandas ' ExcelWriter . After the entire data is outputted to the Excel file , what is the easiest way to apply conditional formatting to it programmatically using Python ?
After the data is written , I need a way to apply the conditional formatting using python . To make it simple , I want the colors to be darker shades of blue the more positive ( > 0 ) the values are and to be darker shades of red the more negative the values are ( 0 ) and the cell to be white if the value is 0 .
Here is an example of how to apply a conditional format to the XlsxWriter Excel file created by Pandas : #CODE
I need to split a dataframe into groups , and for those groups that have odd number of lines , i need to pull in the first line whose column matches a certain condition and then i need to assemble back all such first lines ( so only the first ones in odd numbered groups matching a condition ) . I can do it in a loop like below ( it works ) but can't rework it into a groupby with apply . Could you help ? #CODE
Interesting problem which I would solve by writing a function which you then pass to apply .
Note that when calling passing a function to apply , the fist argument passed is the DataFrame itself and this is done so automatically . That is why you don't need to specify the ' df ' argument when passing the function to apply . In fact if you do you get an error saying you have passed too many arguments . Also somewhat strangely in my view when passing the function the arguments are supplied after commas rather than in parenthese . This I find confusing to look at , but it is what it is ....
Is this expected ? I know I can apply ` dropna() ` on this output , but isn't the above already supposed to filter out the values I ask for ? ( it typically works on dataframes wihtout having to call ` dropna `)
I use this to get the vote totals and I apply a function to the group to get the number of precincts reporting and the total number . #CODE
I think you can use the select method to apply a filter to the index : ` df_raw.select ( lambda r : r.lower in my_list_of_rows )`
Compare what happens when you call apply with ` mean ` : #CODE
How do I define the function which calculates the percentage columns and how to apply that function to my two columns namely ` Qd ( cb )` and ` Autopass ( cb )` to give me additional calculated columns
How to apply a custom formula over a group element in a grouped.apply() .unstack() method ?
Looks like when you group the dataframe it returns a bunch of series due to your original only having two columns . ` applymap ` is a Dataframe method that applies a method element wise . It looks like you are looking for apply in this instance . Try the following ... #CODE
and then apply a method to the groups or whatever you want to do . If you just want these groups separated call ` grouped.groups `
If you want to apply additional filtering , with this index , you can even select a specific year using ` df [ ' 2013 ']` .
It's still quite misleading . Imagine if this happened when you called ` to_dict ` . You could just as well say that you are " leaving Pandas " and going back to " pure Python " , and then apply some type conversion on the values that will be ` dict ` values . Then ` o.head() .datetime .to_dict() [ 0 ]` would be different than ` o.head() .datetime [ 0 ]` . In any of these cases , if you are asking for some iterable thing that has values in it as a sequence ( whether dict , Series , or ndarray ) , you expect the entries to be references to a single value in memory . You don't expect to get a different value .
To do this , you would group the data and use the ` apply ` method to apply a function that does the above .
And then pass that function to ` apply ` using the groupby object as follows : #CODE
In Pandas , how to apply 2 custom formulas in a groupby.agg() method ?
The apply functionality is probably what your are looking for : #CODE
How to concatenate within ' apply ' on a grouped object
I have a dataframe where I wish to edit the information in columns a and b , within groups defined by columns d and e . The procedure to apply to a and b is : set all rows equal to the row where c is a minimum . Columns c , d and e must remain unchanged .
I am using an apply function on a grouped object . I use ' reindex ' to change a and b . The difficulty comes with concatenating , " cannot concatenate a non-NDFrame object "
Either apply a regex pattern or apply a function that returns the characters you want , even if length did work , it'll just return the length of each row which is no different to doing nothing . You've not clarified the requirement , are you just wanting numbers only ? Is the number length fixed etc ..
docs , so I'd use numpy to get a " windowing " view on the array and apply a ufunc
Group this new dataframe on ID , and use the ` shift() ` method to get the differences in the stock prices using the ` apply ` method
No , of that you'd have to apply a regex or a lambda to test each word for that character and strip it
You can apply a post-processing step that first converts the string to a datetime and then applies a lambda to keep just the date portion : #CODE
Create a new column , then just apply simple ` datetime ` functions using ` lambda ` and ` apply ` . #CODE
That error should only occur if you have a date whose year is below 1900 . Otherwise , ` strftime ` is smart enough to convert it as shown above without errors . Are you sure your values are correct ? Try exporting to CSV right before doing the ` apply ` . A date might have been incorrectly converted from ` Created Date ` .
You can pass a function to apply , but it shouldn't return a dict . If you want to add a sum column your way - do it like this : #CODE
I've designed this method so I can apply any function I wish to the table at any point . It needs to stay this flexible , but it just seems horrible ! Is there a more efficient way to do something like this ? Is iterating over groups + appending better ?
Thanks . I was aware of this type of solution . But the problem is : I may need to apply an aggregate function which needs access to more than one column ( table manipulation ) . And I will need to apply more than one of these in batch to different data !
Any chance of a little spoonfeeding ? Not sure how to apply this to my dataframe ...
there is an apply function , e.g. frame.apply ( f , axis=1 ) where f is a function that does something with a row ...
It seems to cast all values to ` bool ` , unless I " touch " the ` DataFrame ` by adding a new column . This happens regardless of whether I use row or column-based ` apply ` ( i.e. ` axis=0 ` or ` axis=1 `) .
The problem comes from the ` samples = data [ idx ]` line . I suspect I need to be more specific than using ' grouped ' for the data field in bootstrap , but I am unsure how to do this . Do I need to apply this as a lambda function ? Or maybe with a for loop ? Any suggestions would be much appreciated .
Pandas : apply different functions to different columns
At this point , I should be able to use something like an " apply " function on ` df ` using ` months ` , but am a bit lost ...
I think a way should exist to avoid the use of apply but I didn't find it .
I think ' apply ' is a good way to go .
Is there a solution that works for all three cases ? Or is there some way to apply a condition in string splits , without iterating over each row in the data frame ?
How can I use the ` drop_duplicates ` method to remove all the duplicate dates for each persons set of records ? Or is there another way - such as apply ? #CODE
I made something that works , but there is probably a better way to do this . For each column in my dataframe ` df [ column ] = df [ column ] .map ( mapdict )` . ` apply ` doesn't seem to work on a dataframe . Thank you !
Very slow single pandas apply / groupby call
I have a small dataframe ( 200 * 19 ) . I want to apply a function to each row . There's no sub-loops . I've tried using groupby and row apply : #CODE
Incidentally , I don't see the use of the groupby version . If you want to apply a function to * every * row , groupby doesn't make sense .
was just using it for clarity / possible future abstraction . at the time , each row is it's own round , so each group is a df with one row . This might not always be the case , so the row apply would only be a temporary fix ( but it's not faster , anyway , so I guess not )
As a work-around I'm trying to do the same thing filter does with groupby and apply but it doesn't work as expected . Any suggestions ? #CODE
The problem with your example code is that the ` apply ` doesn't know what to do with the ` None ` when putting the dataframe back together . Your ` apply ` function needs to output the same type of object every time . If you return ` pd.DataFrame() ` instead of None you should get what you're looking for .
I did not play around with that approach long enough to figure out how to " enlarge " batches of indexes all at once . But , if you figure that out , you can just " enlarge " the original data frame with all NaN values ( at index values from ` DatesEOY `) , and then apply the function about to ` YTDSales ` instead of bringing ` output ` into it at all .
I came up with this function , using ` apply ` : #CODE
then I apply the multi-indexing and unstacking so I can plot the yearly data on top of each other like this : #CODE
AttributeError : Cannot access callable attribute ' to_csv ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method
@USER -Hypothesis my original answer was incorrect , have corrected it now , I now apply row-wise dividing each item by the row sum , the values seem correct to me now
If I apply the rule in the df example , the output should be : #CODE
Thanks for the example . In my case , there are around 30 columns : ' measure1 ' , ' measure2 ' ... ' measure30 ' . However , I only want to apply sum() to ' measure1 ' ... ' measure20 ' . is there a way df.groupby ([ ' dim1 ']) [ ' measure1 ' , ' measure2 '] .sum() can be written without having to write all 20 column names . This is important because some of my column names are generated programatically , and I do not know their names beforehand .
I need to apply the formula above for each chunk . So following this recipe I tried : #CODE
You just need to add the param ` axis=1 ` : apply ( lambda row : func ( row ) , axis=1 )` then whatever your func does access the columns of interest : ` def func ( x ): return x [ ' A '] + x [ ' C ']` as an example
using right_on : result of function f apply to col ' Y ' from df2 .
Can anyone suggest a way to apply a function sequentially , so not only using the last calculated value of the column being computed but also using the present and past values of other columns in a pandas dataframe .
@USER : Thanks for responding , I have added clarification as asked for . The crux of what I am trying to do is to be able to apply a function such that it has access to current values of all columns and past calculated values of the current column .
Resample everything to 5 minute data and then apply a rolling average . Something like that is apllied here : Pandas : rolling mean by time interval
Resample everything to 5 minute data and then apply linear interpolation . This method is close to method 3 . Pandas data frame : resample with linear interpolation
Apply the function to the array like this : #CODE
groupby the user and apply a lambda : #CODE
Python pandas apply function if a column value is not NULL
I want to apply a simple function for rows that does not contain NULL values in a specific column . My function is as simple as possible : #CODE
And my apply code is the following : #CODE
Just apply this : #CODE
Thought this would be straight forward but had some trouble tracking down an elegant way to search all columns in a dataframe at same time for a partial string match . Basically how would I apply ` df [ ' col1 '] .str .contains ( ' ^ ')` to an entire dataframe at once and filter down to any rows that have records containing the match ?
You can do that using ` apply ` to traverse and apply function on every element , and lambda to write a function to replace the key with the value of in your dictionary .
You can use this to go through the dates that you have classified as " year-month " and apply cretiria on it to get related data . #CODE
But those have to be ints . Add a separate issue on Github to have the str.slice method take series objects and apply element-wise .
getting the index of a row in a pandas apply function
I can apply it like so : #CODE
Aside : is there a reason you need to use ` apply ` ? It's much slower than performing vectorized ops on the frame itself . ( Sometimes apply * is * the simplest way to do something , and performance considerations are often exaggerated , but for your particular example it's as easy * not * to use it . )
` apply ` is too slow .
@USER : I have a script that I ( would like to ) run fairly frequently that spent about 300 seconds in this computation when using ` apply ` ( more like 100 with the awful hack above ) . It goes through tens of thousands of rows , tens of times . Pandas 0.14.1 .
I was not able to find a way without at least using an ` apply ` for setup but assuming that is okay : #CODE
Note that you must use the ` datetime ` from the ` datetime ` module rather than the ` numpy ` one or the ` pandas ` one . Since you are only creating the delta with the apply I would hope you experience a speedup .
Here is a way to do it ( by adding NumPy datetime64s with timedelta64s ) without calling ` apply ` : #CODE
` array_split ` accepts any array-like argument ( including ` pandas.DataFrame ` objects ) , but only returns guarantees that it return a ` numpy.ndarray ` ( which DataFrames are not ) . Of course , ndarrays don't have an ` apply ` method , which is exactly the error you're seeing . I'm actually surprised that this works in any scenario . You'll either need to split the dataframe into sub-frames or apply a function that operations on ndarrays .
Since Limit varies on each row , you should use , for example , apply like following : #CODE
Since you want to apply the operation generically , to any given ` foo ` function , you have no choice but to call that function ` na ` -times- ` nb ` times . That part is not likely to be further optimizable .
The ` pandas ` way actively shuns looping in favor of proper indexing and selecting due to the expensive overhead incurred in looping . Are you sure you cannot apply indexing plus , for example , a lambda expression to apply these filters ?
You could also create a mask in a loop and apply it all at once : #CODE
set ` ts ` as index and then ` groupby ` second , and transform with ` cumsum() ` as a new column s , then apply ` reset_index ` , like this : #CODE
Apply the ranker function on each group separately :
Another option , inspired by HYRY's solution , would be to hide the common columns in the index , and then apply HYRY's ` stack ` ing trick : #CODE
I want to apply one single function call on " df " . Sometimes the function will call member functions of " df " . Sometimes it will just print " df " . I dont want a switch case . I am a Python newbie , so I dont really understand how my question is unclear . As I know so little , I tend to skip over crucial details . What are those crucial details you need to know ?
In Pandas version 0.14 and older , you can use ` apply ` to extract the dates from the ` datetime ` values : #CODE
I think you should wrap the dict in a Series , and then this will already expand in the groupby call ( but then using ` apply ` instead of ` agg ` as it is not an aggregated ( scalar ) result anymore ): #CODE
Using apply in pandas data frame gives ValueError
I have a vector that I want to apply a pearson correlation to all rows of a pandas data frame . I am trying the following : #CODE
Apply func simply takes two ` numpy ` arrays and calculates the correlation #CODE
python pandas : apply a function with arguments to a series . Update
I ensure that args is a tuple which is what the ` apply ` function is expecting and I get the result I was expecting
` df.apply ` is to each value in array , so ` df.apply ( list , axis=1 )` is equivalent to : apply ` list() ` on each value in the array , ie . ` 81 = [ 81 ] , 88 = [ 8 8] , ... ` individually . So it will have no effects .
Actually no . apply works with entire row ( if axis parameter = 1 ) . If you make df.apply ( sum , axis=1 ) you will receive sum of entire row . Additionally , try df.apply ( lambda r : ' , ' .join ([ str ( e ) for e in r ]) , axis=1 ) and you will get one result for each row .
@USER , I can't see why ` sum ` is applicable here . The OP is asking for a method of aggregating values into a list structure , not a numerical operation . From your first comment , it looks like you're arguing that ` list() ` in Python takes more than one parameter . It doesn't . The behavior you're attributing to apply , element-wise operations , is what df.applymap does . Can you clarify what you mean , please ?
I think my presentation skills need improving , I only refer to ` sum ` as @USER mentioned in comment about why ` sum ` works on ` apply ` but ` list ` doesn't . my first comment was trying to make a point how ` list ` doesn't change the element in the row array , I guess I didn't explain it in a clear way .
Just assign another column as a ` cumsum ` of ` indicator ` , then apply ` groupby ` , this should do the trick : #CODE
I'm trying to use multiprocessing with pandas dataframe , that is split the dataframe to 8 parts . apply some function to each part using apply ( with each part processed in different process ) .
there is a space in the ` res = df.apply ( process apply , axis=1 )` , is that right ?
currently apply only saturates one core of the CPU . I want to use multiprocess and use all cores to decrease processing time
Iam trying to get the row with maximum value based on another column of a groupby , I am trying to follow the solutions given here Python : How can I get the Row which has the max value in goups making groupby ? , however it doesn't work when you apply #CODE
Alternatively , you could use ` apply ` to split each variant on commas : #CODE
Thus , to avoid possibly complicated regex or a relatively slow call to ` apply ` , I think your best bet is to build the DataFrame with one integer variant per row .
This is slow , but I am not sure how to translate it into something using apply . Any hints ?
You can use ` apply ` function , but you need to do extra work here ( just to simplify the work ) .
It is not ` map ` that is fast , but ` iterrows ` that is very slow . You can use ` itertuples ` and will get something almost as fast as ` map ` , but as @USER says , you should try to see if you can apply this function on the whole columns .
Dear JD , I dont want to create the custom column in the dataframe gain.I want to apply the where condition for the custome column
Then go through the ` rno_cd ` column , and apply a function that transform the data . You can use ` apply ` and function ` tranform ` where you can verify whether x is a key so you get the values using your dictionary ` D [ x ]` if it's not the case , you just return `" unknown "` #CODE
Is there a way that I can apply a function to a pandas dataframe that returns a list for each row that it is applied to , and then take that list and put each item into new columns of that existing dataframe ?
Right now the return of the apply function is a list of lists each of the inner lists is a 9 item list like that shown above . I am fine putting the response into a new dataframe such as the below , but I haven't figured out how to get the apply function to write to each new row for each return or get the list of lists in to the right form . #CODE
I like this structure of data before ` apply ` ing , before I am usually able to just ` df [ ' someNewColumn '] = df.apply ( ... )` . But strangely , this time , I'm not able to instantly remerge the results .
@USER , ` level=0 ` will give the same result as above as you have only 1 index by the time you do the ` apply ` . the ** ... ** is exactly from the output as you're trying to apply the ` ... / x.coef.mean() ` on a group level . However , I think JD Long's suggestion is more likely what you're trying to achieve . And my pandas's version is ** 0.14.1 **
It's not obvious to me which version of pandas you're using , but your apply does not work for me at all .
whoops . You are correct . I read his code too quickly and didn't notice the apply was on the grouped data . I'll fix my comments above .
I already know about the np.isfinite and pd.notnull commands from this question but I do not know how to apply them to combinations of columns .
You can use ` apply ` and lambda function where you choose non-Nan value . You can verify if it's Nan value using ` Numpy.isNan ( .. )` . #CODE
What do your data actually look like ? Also , are you sure you want a groupby and apply together ( i.e. not ` agg ` instead ) ? When you say " they all fail " , a specific error message would be helpful , as would a description of what exactly you are trying to do by doing this operation .
So if apply a simple function , ` mean ` , to the grouped data we get the following : #CODE
This works just fine ( i.e. ` ctt_ask ( example_data )` yields 2.90 ) for the above example but my real dataset has several stocks and many date times ( it has a ` MultiIndex `) . When I use ` groupby ` and ` agg ` to apply this function to every stock-date time combination ( ` full_book_ask.groupby ( level =[ 0 , 1 ]) .agg ( ctt_ask )`) I get an error : ` KeyError : ' avail_shares '` . This is strange because I do have a column named avail_shares in my actual dataset . I have also tried the same with the ` apply ` functionality but this raises the error message ` Exception : cannot handle a non-unique multi-index !
Thank you for the comments . @USER H , as I state in the description there is a column called avail_shares in my dataset so this can't be the issue . @USER Pride , you are correct , I understand now why I can't use ` agg ` . However , both ` apply ` and ` transform ` give me an error as well : ` Exception : cannot handle a non-unique multi-index ! ` . So there must be something else going on . I guess there is a problem with the use of ` ix ` but I don't know why or what I should do to solve it .
OP asked for a way to apply multiple aggregate functions at the same time . A short answer is still an answer .
There ought to be a metric you can apply that takes a baseline picture of memory usage prior to creating the object under inspection , then another picture afterwards . Comparison of the two memory maps ( assuming nothing else has been created and we can isolate the change is due to the new object ) should provide an idea of whether a view or copy has been produced .
Next , use the apply function in pandas to apply the function - e.g. #CODE
Finally we must replace the obtained Series with ` value ` if ` col == " E "` and ` value == False ` . You can't apply a condition on the index of a Series , thats why you need the ` reset_index ` first .
I would like to use the to_datetime method to convert the recognized string date formats into datetimes in the dataframe column , leaving the unrecognized strings in excel format which I can then isolate and correct off line . But unless I apply the method row by row ( way too slow ) , it fails to do this .
My idea was to then apply the rolling mean on this time period .
How do I apply a lambda function on pandas slices , and return the same format as the input data frame ?
I want to apply a function to row slices of dataframe in pandas for each row and returning a dataframe with for each row the value and number of slices that was calculated .
What I want is to apply lambda function f from column 0 to 5 and from column 5 to 10 .
let's see .. I want to apply the function to the slice on columns 0 , 1 , 2 , 3 , 4 and then also on 5 , 6 , 7 , 8 , 9 . However , I want the function to run on the original data where the mean is taken only from the first 5 in the first round and then on the last 5 in the second round . Does that make sense ?
hmmmm .... it's so weird .. it worked and now it doesn't apply any kind of calculation , even when I take exactly your code .. >>> When I use ` df1 =d f.copy() ` it works , but not with ` df1 =d f ` .. don't know how that makes sense ..
I want to apply a function f to many slices within each row of a pandas DataFrame .
If you're applying the same function to all of the groups , why not just apply it to the whole dataframe ? does the function aggregate the values in some why ? ( all of these questions I have could be avoided if you simply included some example output that you would like to see )
First , I want to say that I've studied basic python and had an intro into pandas , but I'm overwhelmed by pandas a bit .. So , I found it hard for me to breakdown my eventual goals into one question , so I thought it made more sense to go in little steps and build on top of each other to finally get the full picture , but I was worried that it would seem like I'm repeating myself . But yes , one of the things I want to do I guess is transform matrices into same-size output matrices , not so much aggregating- but mostly I need to apply functions to slices of the rows of my input matrix ..
it'll take me a bit to digest your answer , but this is very useful info for me at this point . I do want eventually be fluent in applying these things as such , but didn't know how to step into the whole indexing and grouping with pandas . Delicate balancing between learning code and getting my project done ... I'll be trying to apply your answer to my needs . Thank you !
Read my data from file and trying to apply indexes to the ( 43 , 49 ) df . I put all my indexes as a list of tuples ( idx_tuple ) and then created multi-index by ` index = pd.MultiIndex.from_tuples ( idx_tuple , names =[ ' nr ' , ' date_sample ' , ' month ' , ' conc ' , ' time '])` . Now tried to update my df as such :
Sorry but how would I apply this for a dataframe ? df.values.apply ( lambda x : round ( x )) ??
This code generates the error ' Int64Index ' object has no attribute ' apply '
Index types don't have an ` apply ` method , but ` Series ` does .
To apply a function to your index , you can convert it to a series first , using its ` to_series ` method : #CODE
and then pass it to ` apply ` on data grouped by s_id : #CODE
So ` apply ` passes each chunk of grouped data to the function and the the pieces are glues back together once this has been done for each group of data .
One possible method without using ` regex ` is to write your own function and just ` apply ` it to the column / Series of your choosing .
You could apply that regular expression to the elements in the data . However , the solution of mapping your own function over the data works well . Thought you might want to see how you could approach this using your original idea .
I'm having problems when trying to use apply on the result of a groupby operation .
And I now would like to use apply , but I need to feed it id1 , which is part of the index , so I get an error when I try to do the following : #CODE
The ` DataFrame ` object doesn't have ` nunique ` . You have to pick out which column you want to apply ` nunique() ` on . You can do this with a simple dot operator : #CODE
To answer your question about why your recursive lambda prints the ` A ` column as well , it's because when you do a ` groupby ` / ` apply ` operation , you're now iterating through three ` DataFrame ` objects . Each ` DataFrame ` object is a sub- ` DataFrame ` of the original . Applying an operation to that will apply it to each ` Series ` . There are three ` Series ` per ` DataFrame ` you're applying the ` nunique() ` operator to .
I am using groupby and apply , so I am not explicitly pulling the groups , which is why i need to do this .
Apply will break the dataframe into multiple smaller dataframes by the groupby columns . The columns you group by are still inside the smaller dataframes . Is that what you are after ?
so you can see in the resulting printed output that each iteration of the ` apply ` gets all columns of the input dataframe .
I'm not sure how to grab a tuple of keys from an ` apply ` but I can from a loop : #CODE
so are you asking how to write a function which , when you apply it to grouped data , can see the keys ?
I get it now . not sure how to do this from apply , but I added an example of how to do it from a loop
Parallelizing apply function in pandas python . worked on groupby
The same " apply " pattern works for SFrames as well . You could do : #CODE
I'm guessing that I can't apply a sort method to the returned groupby object .
so that fills in all the missing dates with zeros . Now we can apply the rolling sum . #CODE
You can define a function which returns your different states " Full " , " Partial " , " Empty " , etc and then use ` df.apply ` to apply the function to each row . Note that you have to pass the keyword argument ` axis=1 ` to ensure that it applies the function to rows . #CODE
Then using apply : #CODE
I am creating the dataframe by concatenating two other frames immediately before trying to apply the filter .
an intuitive way to understand the pandas groupby is to treat the return obj of DataFrame.groupby() as a list of dataframe . so when u try to using filter to apply the lambda function upon x , x is actually one of those dataframes : #CODE
What I'd like to do is : for each day , apply a function that takes the sum of all logvol between 14:40 : 00 and 15:00 : 00 .
` apply ` that function to each row , save in the original dataframe #CODE
Apply multiple functions to multiple groupby columns
You might start by looking for conditional ` apply ` - there are plenty examples on how this can be done . Alternatively ` numpy.where ` can do ` if ... else ` replacement / assignment . And for comparing current value ( s ) with previous ones ` pandas ` has ` .shift ` method , which you could use with any of the first two approaches .
It's a list ... In [ 81 ]: type ( parsedSeries.ix [ 0 ]) Out [8 1 ]: list . I apply str.split to a df to create that list
The " best " solution probably involves not finding yourself in this situation in the first place . Most of the time when you have non-scalar quantities in a Series or DataFrame you've already taken a step in the wrong direction , because you can't really apply vector ops .
This tells u that the datetime format of variable { b } is wrong . so two choices here . the first one is to correct the str format ( modify " 24 " to " 00 ") , then apply the { pd.to_datetime } func : #CODE
I know about ` scipy.interpolate ` mentioned in this article ( which is where I got the images from ) , but how can I apply it for Pandas time series ?
Your ` apply ` approach would work too if you used ` x ` instead of ` df.radon ` : #CODE
I struggled with this problem for several hours to little avail . Ultimately , I wound up writing a nested for loop and solved the problem iteratively . Unfortunately , that solution is painfully slow and I'd much prefer something that utilizes nice features in Pandas such as groupby or apply .
I have found workaround which is extremely slow due to the " in python " apply : #CODE
Apply a function to a specific row using the index value
How can i apply a function to the dataframes index ? I want to round the numbers for every column where the index is " c " . #CODE
I think your title is a bit misleading , what you are saying really is you want to apply a function to a specific row using the index value
I think what you want to do won't work due to the shape of the returned values and expected return type . Another way would be to apply a lambda and concatenate the result : #CODE
Is there any magic I can apply to the code below ? #CODE
One way I could conceive a solution would be to groupby all duplicated columns and then apply a concatenation operation on unique values : #CODE
Now , I've made a little method that will take an input string and do this , spitting back the value I desire . However , it seems to be horribly inefficient . I'm using pandas for data manipulation and this method gets applied to a whole column of timeseries string data in the above string format . Calling the apply method via interactive shell finished execution in ~2sec , but strangely , letting the code run as compiled / interpreted on the same dataframe takes more like 15-20 seconds . Why is that ? This is how I'm calling it for the dataframe / series :
Pandas apply with argument that varies by row
I am attempting to apply a function to each row , where the function takes a ' size ' argument . #CODE
but is there a better way using apply functions ?
Why don't you make ` size ` another column in your data , so it is passed as part of the ` apply ` ?
You can create function and ` apply ` it to your dataset : #CODE
` read_csv ` accepts an argument named ` converters ` . This can be used to apply functions to particular columns as a file is read in . ` converters ` should be passed in as a dictionary of the following form : #CODE
You could use this to apply a function to the third column . All you need to do is set the function to get a value from a dictionary ` d ` which maps `" male "` to ` 0 ` and `" female "` to ` 1 ` : #CODE
Can someone point me to a link or provide an explanation of the benefits of indexing in pandas ? I routinely deal with tables and join them based on columns , and this joining / merging process seems to re-index things anyway , so it's a bit cumbersome to apply index criteria considering I don't think I need to .
This needs to be done to several million rows of data . Any thoughts on how to speed up the process ? I am using pandas data frame's map function to apply the function ` toTheExp ` to my column of data already . This step is still pretty slow though . Currently I'm trying something like : #CODE
A little unclear what you want , ` applymap ` is for a dataframe it doesn't make much sense to call ` applymap ` on a series when ` apply ` is specifically for this . You can get a df by using double square brackets : ` pd.DataFrame ( df [[ ' col ']]) .applymap ( isnan )`
Apply FROM_UNIXTIME on column , c
If you want sequential index , you can apply ` reset_index ( drop=True )` to the result .
the code below perfectly fine , but i need to create a function that will do that for me rather than creating lambda functions for every block of columns that i like to merge . ( I still have other similar columns that i like to apply the same logic on them . ) #CODE
I don't get how I can use groupby and apply some sort of concatenation of the strings in the column " text " . Any help appreciated !
You can groupby the `' name '` and `' month '` columns , then call ` transform ` which will return data aligned to the original df and apply a lambda where we ` join ` the text entries : #CODE
EDIT actually I can just call ` apply ` and then ` reset_index ` : #CODE
I have been messing around with groupby / transform / apply but haven't gotten anything to work so far . If I groupby and then shift , it shifts each group giving the output of : #CODE
Is there anyway to randomly apply changes of stings by row to a Pandas data frame .
Pandas use groupby to apply a different function for each value of the groupby variable
I'd like to use groupby , but instead of applying the same functions to each group , I want to specify which function to apply to which group value . I'm providing a very simple example here to illustrate the point , but in reality there are many values of my groupby variable , and my functions are all user-defined and fairly complex -- so solutions that involve selecting each group separately or apply the same functions to all groups will not be practical . ( Answers of that sort were provided to this very similar question : how to apply different functions to each group of pandas groupby ? but they don't address my question ) #CODE
This makes sense , but how do I specify function_map so that it contains functions in valid python syntax ? Or , alternatively , if I store the names of the functions as strings , how do I then pass them as functions to apply ?
i.e. to compute the value of C I need the previously computed value of C . This can be done by a simple for loop , but I would like to use map , apply or some other pandas functionality . Can this be done i a simple manner ?
Thanks @USER Paulo - Is there a way of specifying the size of the title when it is inside the df.plot ( title=MyTitle ) . if I move the plt.title ( MyTitle , size=20 ) outside of the PLOT brackets then in my for loop it creates a title above every subplot and I don't want that . I want a single title for a group of subplots - but I don't know how to apply the size property when it's in the brackets plot ( title=MyTitle ) .
I can tell you from experience that this will need to be performed in some kind of for loop / apply method . As there are no exact matches you have to find the appropriate index value to set the new column values , I would use ` numpy.searchsorted ` or you could use a filter , in ` 0.15.1 ` I think you can do some fancy filtering if you pass a range but not sure it applies to dates , worth a try though .
I want to apply this function on my data , stored in a data frame . However , the data frame consists many experiment subjects and 4 experiment conditions , while the outlier detection function should be applied on the level and for each subject + trialcode .
Is there a way to apply this function on groups of subject+trialcode ?
Groupby and apply ` .mean ` multiplying by 30 : #CODE
My problem is that I'm having trouble getting pandas to create a date column which I can then apply a timedelta to . Here's my offending line : #CODE
Yes , this is the output I want to have . However , I want to apply this procedure several times with different filter criteria to df1 . Using the above code snippet would overwrite older inserted values in ' eins ' everytime with NaN . Therefor I look for joining only those rows of df1 , which fulfill the filter criteria .
When you apply a function on the groupby , in your example ` df.groupby ( ... ) .agg ( ... )` ( but this can also be ` transform ` , ` apply ` , ` mean ` , ... ) , you combine the result of applying the function to the different groups together in one dataframe ( the apply and combine step of the ' split-apply-combine ' paradigm of groupby ) . So the result of this will always be again a DataFrame ( or a Series depending on the applied function ) .
Now you can apply the condition like this : #CODE
You need to ` apply ` your logic to each row , like this : #CODE
This works , except that it's very slow , presumably due to the nested ` apply ` calls : one on each group , and then one for each column in each group . I tried getting rid of the second ` apply ` by computing quantiles for all columns at once , but got stuck trying to threshold each column by a different value . Is there a faster way to accomplish this procedure ?
I have data stored in a DataFrameGroupBy object . Therefore , I would like to apply the function to the entire column .
Also , the ` apply ` method of a GroupBy object doesnt have the ` axis ` keyword , die ` apply ` method of a DataFrame does .
@USER : I think I read in the linked stackoverflow question that ` axis ` has been added to the ` apply ` method of the ` GroupBy ` object . But this is obviously not the case , given the error message I get back from the Python console .
You could set ` files ` as the index in ` df1 ` and then apply a function which uses ` loc ` to look up the ` pkid ` value corresponding to the index : #CODE
A faster method than @USER ' s is to use ` map ` here because you have a unique index then this will be much faster than calling apply which is essentially a for loop : #CODE
yep , this is MUCH better than using apply as its fully vectorized
It is possible that pandas gets confused if your function sometimes returns a list and sometimes a single value , since different dtypes would be used for those two cases . It is probably better not to do it that way . The calling-twice behavior could be related to the issue described [ here ] ( #URL ) for ` apply ` : it calls the function twice on the first group in order to check whether the function mutates the existing data .
The strangest thing is , im reuse this code all the time with no issues . I know apply and transform pass different packets of data such that it is quite hard to ascertain from print statements what is going on , but agh is fairly straightforward . Were you able to recreate the error ?
I cannot send the data frame to the function row by row because there is a rank by group aspect to the algorithm . So I have to send at least one group of data at a time to the function . I tried groupby.apply but there were unexpected results due to the apply calling the function twice on the first group . So now I am using a lambda like this . #CODE
In the current implementation apply calls func twice on the first
My recommendation would be to concat the list of dataframes using pd.concat . This will allow you to use the standard group-by / apply . In this example , multi_df is a MultiIndex which behaves like a standard data frame , only the indexing and group by is a little different : #CODE
By " Finding it hard " I mean that strptime on the x [ ' datex '] doesn't work because those are series and not values and I can't apply it to the x in " lambda x " or use %Y%M%d instead of %s . An example would be date1 = datetime.datetime ( 2014 , 1 , 1 ) and date2 = datetime.datetime ( 2014 , 1 , 3 ) .
Well , since you're already using ` apply ` , you're dealing with two values ( not columns ) , so you can call the ` date ` method on each : #CODE
python / numpy / pandas fastest way apply algorithm for expanding calculations
In other words I'd like a function ` f ` so that I can apply ` f ` to a series ( or multiple series ) as well as applying ` f ` to a float ( or multiple floats ) , and ideally apply ` f ` to a combination of floats and series .
However I am struggling to figure out how to apply one function ` convertToMeters ` to the first column and ` convertToNewtons ` for the second column .
it will apply the respective function to each column and not just the desired column .
Can you show how ` convertToMeters ` looks like ? Probably you can write this function so that you can just do ` df [ ' col_meters '] = convertToMeters ( df [ ' col '])` without using the apply .
Sorry I was a little bit busy when writting this question . Data are correct now . Well I mean just to delete rows from groups and keep these groups as they are - I need to apply several filters and after each apply is needed new groupby .
You just need to use ` apply ` on the ` groupby ` object . I modified your example data to make this a little more clear : #CODE
Similarly if I create a column to store the int day value and then perform the apply then it works also : #CODE
I guess you could easily turn this into a function to apply on a dataframe inplace .
Please see edit in the answer to see the fix for this error ( since you added more columns ` transform ` was being applied on a GroupbyDataframe and not on the Series ) - all it takes is to pass name of the column after ` groupby ` to apply ` transform ` on .
It seems I can apply some functions without problems to a DataFrame , but other give a Value Error . #CODE
The first apply works fine , the second one generates a :
I know I can generate the " max ( df , 0 )" in other ways ( e.g. by df [ df 0 ]= 0 ) , so I'm not looking for a solution to this particular problem . Rather , I'm interested in why the apply above doesn't work .
Actually it's quicker to convert the type to string and then convert the entire series to a datetime rather than calling apply on every value : #CODE
How can i apply do_calcuations without loops like this . Loops like that is discourage in panda because slow , right ?
groupby these event numbers and apply ` do_calculations ` to each group .
Apply vs transform on a group object
In other words , I thought that transform is essentially a specific type of apply ( the one that does not aggregate ) . Where am I wrong ?
X and Y are actually pairs of coordinates and the function I would like to apply is the vincenty distance from the geopy package .
And another problem : Even if I just want to apply the vincenty formula to the series in my dataframe , I receive an error message : #CODE
The problem is that I have not clue how I get these errors . I can give four single values as coordinates to the vincenty formula or a list or a string and it will work . But the only way I can apply the formula to several entries is using lists .
Gives you a generator with the values , which you can reshape and print as you need . Substitute operator.mul with the pertinent function you need to apply .
IIUC , your function probably doesn't support sequences of strings as input , only strings . You can use ` apply ` to pass the values individually : #CODE
It works for the first row if all the strings are inside one double quotes . But it doesn't apply for the third and second row if there're commas outside the quotes ( single or double )
I have a pandas DataFrame with a mix of numerical ( float ) and text columns . Some of the numerical values seem to be off by a factor 10 and I want to modify them in place . I can do with apply , but I was wondering if there is any way to only indexing instead .
This works of course , it is similar to the apply solution , it just make the loop over the columns explicit . I am still surprised that this is a faster than a solution with no ( apparent ) loop . Thanks for the performance numbers .
I may be doing something wrong , but on my machine I got the following timing information : 1.58 ms for apply on subset of columns , 62.3 ms for fillna , and 2.65 ms for the explicit loop .
Second , we're going to use the dataframe method ` apply ` . What ` apply ` does is it takes a function and runs every row ( axis=1 ) or column ( axis=0 ) through it , and builds a new pandas object with all of the returned values . So we need to set up ` haversine ` totake row of a dataframe and unpack the values . It becomes : #CODE
So now we can ` apply ` the Haversine function : #CODE
I have a pandas series ` series ` . If I want to get the element-wise floor or ceiling , is there a built in method or do I have to write the function and use apply ? I ask because the data is big so I appreciate efficiency . Also this question has not been asked with respect to the Pandas package .
Sorry , forgot to mention the first column ( which I used as label and really do not need the index ) is str . Can I ignore that while apply the dropna or use that column as axis ? take a look at the example file at the link ? #URL
Is it feasible to apply your techniques when no_row=1600000 and no_colors=230000 ?
Yon convert a unixtimestamp by using pandas to_datetime . You can read in the timecode from the csv as an integer and then apply pd.to_datetime #CODE
Then ( since False == 0 and True == 1 ) we can apply a cumulative sum to get a number for the groups : #CODE
There are lots of ways to apply this to your problem , but if I understand your approach correctly -- a straightforward application that follows your structure would be something like this : #CODE
I think I need to use the apply method to trim the column data . So if there is anything after the period keep the data unchanged but if there is nothing after the period then return just the letters without the period at the end . I know I can probably use a lambda function and maybe a string split or something to do this but have not much of an idea to make it happen .
But that does not work , is there some other way to do what I need ? I think my issue is that method is for a series and not a column of values . I tried apply and could not seem to get that to work either .
I also tried using the apply function , but very likely I am doing it wrong . My guess is that I am either not applying the functions correctly for a column or the values I am getting arent integers . I have been trying for days so now am breaking down to ask for help ....
I see . By messed up , I realized that what it did was change the ordering around . I'll just apply a sort again and get it in the correct order . This answer worked , thanks a bunch ! I spent 6 hours trying different methods . :(
Unclear if it should work at all but you could apply the function : ` s.apply ( pd.DataFrame.mean )`
I have to calibrate a distance measuring instrument which gives capacitance as output , I am able to use ` numpy polyfit ` to find a relation and apply it get distance . But I need to include limits of detection 0.0008 m as it is the resolution of the instrument .
For example If I have a capacitance value of 3044 and if you look into calibration data the distance should be between 0.4 m to 1 m and If I do the present method I get distance like 0.8967892678 m ( for example ) , instead something like 0.8008 ( example ) . Because the instrument will only able to differentiate 0.0008 m . I need to apply a correction like if the value is between the two numbers it is rounded and shows the limits of detection
use your raw data of known distances ( your calibration set ) and apply the fit . you can then see how much variation you have from the actual values ( range ) and standard deviation ( 1 sigma )
@USER I mean the logic behind the fact that I can apply the comparison to the whole DataFrame only if the rhs is a Timedelta , although it works just fine with separate columns and ints . Sorry for being unclear .
How to efficiently apply a function to each DataFrame of a Pandas Panel
I am trying to apply a function to every DataFrame in a Pandas Panel . I can write it as a loop but the indexing seems to take a long time . I am hoping a builtin Pandas function might be faster .
I looked at ` mypanel.apply ( condenser , axis = ' items ' )` but this loops over each column of my DataFrames separately . Is there something which would apply a function to each DataFrame ?
apply is correct , but the usage is :
@USER : Thanks ! Also good to see a solution using ` apply ` ; it could be handy for implementing more exotic concatenation functions .
try to use str.cat over apply whenever you can . feels a gazillion times faster .
You can use ` pandas.apply ( args )` to apply a function to each row in the ` transdf ` data frame if you know that the rules set in the ` segmentdf ` are static and don't change . Perhaps the following code snippet may help you . I haven't tested this so be wary , but I think it should get you started in the right direction . #CODE
Basically , the cumulative sum operation ( with a factor ) is done using ` numpy.convolve ` . The rest is straight forward : just ` groupby ` the data into groups , apply the ` convolve ` and then ` concat ` the resultants together .
You can use ` apply ` to force all your objects to be immutable . Try #CODE
However , I think the ` apply ` function does not allow for ` inplace ` modification , right ? So what I basically ended up doing is : #CODE
You can apply a ` lambda ` to only the relevant column , instead of the whole row : #CODE
I believe that str() or the int() in the 1st line of your function wouldn't like a NaN . You could change your apply to df.new_var = df.ID.dropna() .apply ( checker ) or test for NaN in your function
I think using ` nunique ` is better than calling ` apply ( len )` though ;) +1
@USER check out timings , ` nunique() ` is worse than ` apply ( len )` :)
Now add another column for the week and year ( one way is to use ` apply ` and generate a string of the week / year numbers ): #CODE
However , if you need / want date strings with 3-letter months like `' NOV '` converted to ` -11- ` , then you can convert the Timestamps with ` strftime ` and ` apply ` : #CODE
To answer your question literally , in order to use ` Series.str.replace ` you need a column with the month string abbreviations all by themselves . You can arrange for that by first calling ` Series.str.extract ` . Then you can join the columns back into one using ` apply ` : #CODE
what column are you calling your ` apply ` on ? A sample of your data would help you get an answer much quicker .
I would like to apply the function to the new column and get the results by referencing the other two columns . The data is a bit messy and also confidentially , i will try and knock together some simple data for the question .
also avoid using ` apply ` where possible as this is just going to loop over the values , ` np.where ` is a vectorised method and will scale much better .
@USER no worries , the key thing to take away from this is to look for a vectorised method that will operate on the whole df or series rather than calling apply which loops over the values
You'r lambda is operating on the 0 axis which is columnwise . Simply add ` axis=1 ` to the ` apply ` arg list . This is clearly documented . #CODE
How is a DataFrameGroupBy structured to then apply functions to it ( like a for loop ) ?
Applying a cumulative sum on this DataFrame is easy , just using e.g. ` df.cumsum() ` . But is it possible to apply a cumulative sum every ` X ` days ( or data points ) say , yielding only the cumulative sum of the last ` Y ` days ( data points ) .
I suppose you could always define a function ` f ` that accepts the parameters of column ` A ` , column ` C ` , and column ` L ` and then ` apply ` it to your data frame . See --> #URL
You need to explicitly read the data from the table . Table.read will pull in the entire table , and Table.read_where allows you to apply a conditional statement to filter the data that is returned . #CODE
Going around , I have found also this solution based on apply method
So for the short example I give df.index has shape ( 3 , ) and df.T has shape ( 2 , 3 ) . I think numpy broadcasting ( which I thought should apply here ) goes from last dimension to the first . So the 3's should match and the operation should be successful . That's why df.T.values * df.index.values will always work .
Is there any smart way to do this or to apply gensim from pandas data ?
Until I learn how to use map / apply , looping through a pandas dataframe is good enough .
` df [ ' ids '] .str ` allows us to apply vectorized string methods ( e.g. , ` lower ` , ` contains `) to the Series
Here comes my problem , I would like to apply a PCA on the table which requires the whole DataFrame to be loaded but I don't have enough memory to do that .
The PCA function takes a numpy array or a pandas DataFrame as input , is there another way to apply a PCA that would directly use an object stored on disk ?
If you look in the [ documentation ] ( #URL ) , you can see the ` rolling_apply ` function , which allows you to apply any function in a rolling way . Your function must take the data inside the " rolling window " as an argument . It's not clear how your hodgesLehmannMean involves a window . What is ` x ` ? Is it the window or the whole data set ?
Pandas apply to dateframe produces ' < built-in method values of ... '
The weirdest part is , when I directly call the function ( i.e. ` make_geojson ( data.loc [ 0 ])` I do in fact get the dictionary I'm expecting . Perhaps even weirder is that , when I call the functions I'm getting from the apply ( e.g. ` data.output [ 0 ] ( )` , ` data.loc [ 0 ] [ ' output '] ( )`) I get the equivalent of the following list :
@USER I am following the example [ here ] ( #URL ) , but my equivalent of ` f() ` is returning a ` dict ` . Same issue as this question . Yet it's possible to store a ` dict ` in a ` DataFrame ` . I don't know quite what you mean by " a branch is taken " -- does that mean : ` apply ` with a returned ` dict ` is not possible at all ? Is there another way to operate on each row while storing the ` dict ` result in a new column ?
You can pass a function to a ` groupby ` object using ` apply ` : #CODE
Then you can apply a function to each subset . It sounds like you want either ` rolling_mean ` or ` expanding_mean ` , both of which are already available in ` pandas ` : #CODE
I'm not sure how you can do things within the function , it's just not really how groupby and apply work . ` frame.groupby ( ' year ') [[ ' gate ' , ' pop ']]` is almost the same as ` frame.groupby ( ' year ')` , it just excludes the state column .
You need to use ` pandas.DataFrame.apply ` . The code below will apply the lambda function to each row of ` df ` . You could , of course , define a separate function ( if you need to do more something more complicated ) . #CODE
Apply a lambda with a shift function in python pandas were some null elements are to be replaced
for the last part , in row 2 period 3 I was hoping this would also subsequently be filled with the same formula i.e. period 2 ( 0.425 ) x 0.94 . I am guessing I would need to step through each row or repeat the lambda until there are no more NaNs ? I was assuming a lambda function would automatically apply the function in a sort of iterrows fashion .
( I'm not very familiar with Pandas , but this describes a very generic idea - you should be able to apply it . If necessary , adapt the Pandas-specific functions . )
The ` apply ` method calls the lambda function once for each row of the Series ,
using Pandas , only use the ` apply ` method if there is no other option .
space-efficient . So ` zip ` , like ` apply ` , should be avoided here if possible .
Both functions works , but they are very slow if I use them on thousands of csv files . I think the main bottleneck is the apply method . Is there anyway to speed it up ? Thank you
You are right that using ` apply ` here is also a potential bottleneck , since it is calling a Python function once for each row of the dataframe . Instead of parsing the time strings using ` to_timestamp ` , you could instead use ` pd.read_csv `' s built-in date string parsing ability : #CODE
Oh just noticed that you mentioned you're using ` set_context ` , you can also pass that information to the ` rc ` parameter in that function and it will apply to all figures .
The main idea behind ` groupby ` and similar functions is " Split - Apply - Combine " whereby , in general , you :
Apply some aggregate function to each of the individual groups ,
Finally in pandas you need to apply some aggregate function to your groups ( the apply stage ) , we're going to use ` count() ` to count the amount of results . This line then becomes : #CODE
You can use the ` xlsxwriter ` engine from Pandas to apply a conditional format to data in an Excel worksheet . See this answer to [ Easiest way to create a color gradient on excel using python / pandas ? ] ( #URL ) . That may be close to what you want to do .
I've been through lots of questions in stack overflow but still can't figure this out . I understand it's returning a Bool etc but basically I want to apply multiple conditionals to a DataFrame ( If And Else , Else if ... ) But continue to get Ambiguous Error asking to you use any() , all()
I don't know how to use apply , or whatever else , to figure this out by group and return a dataframe of only those groups .
I just figured out the answer . I was looking at apply but I needed to use filter #CODE
Create a group by on the column you want to reduce over and then apply a function that returns the results of the group by an a list per group . Note this returns a series .
Note that the type on the date is now Timestamp , not datetime . Down the other portion of my code it stays datettime ( which is the correct chain of events , yes ? ) and so now they both reference the same date but test as not equal so I can't apply DataFrame.update to push data from one to the other . Unfortunately both paths have strong data-driven reasons why they should be done the way they are .
` df.columns.levels [ 1 ] [ 0 ] .to_datetime() ` will convert the timestamps back to datetime . I can't find away to apply this to the whole level of the index at once . List comprehension will work but i guess it isn't very ` pandas `
It should be quite straightforward to get time objects into a dataframe ( load them as string , then use an apply to transform into a time object )
@USER , as a side note , although current method is very quick , it could benefit even more if you apply ** compiled ** regexp .
A simple ` apply ` can solve this . If you can weather a few seconds of processing , I think this is the simplest method available to you without venturing outside ` pandas ` . #CODE
Another way to achieve this is to use pd.Series.isin() with map and apply , with your sample it will be like : #CODE
@USER , I think speed-wise , using regexp will be faster as my method requires 3 x ** apply ** to transform the string content , efficiency wise , using ** map ** will yield each ** lambda ** whereas ** findall ** may eventually max out memory
This works , but be very wary of applies as they slow things down as your data grows in size ... Regexs are so fast because they are basically just lexical parsing . If you really are that concerned about memory usage ( which shouldn't be the case here as you are already holding a DF in memory that is bigger or equal in size to the resulting df ) and want to use apply , then at least use regexs in your lambda function .
This can be accomplished with a one line solution using Pandas ' boolean indexing . The one-liner also employs some other tricks : Pandas ' ` map ` and ` diff ` methods and a ` lambda ` function . ` map ` is used to apply the ` lambda ` function to all rows . The ` lambda ` function is needed to create a custom less-then comparison that will evaluate NaN values to True .
Please let me know the command , I am trying with apply but it ll only given the boolean expression . I want the entire row with latest year .
If you are intending to apply some sorting on the result of ` transform ` then sort the df first : #CODE
I would like to add a new column , ` d ` , where I apply a rolling function , on a fixed window ( 6 here ) , where I somehow , for each row ( or date ) , fix the value ` c ` . One loop in this rolling function should be ( pseudo ): #CODE
I am trying to apply a filter on a series of values stored in a pandas series object . The desired output is the value itself if it meets the criterion otherwise zero . I can only get it to half work : #CODE
Agree with @USER . ` to_datetime ` is another option , though for more esoteric formats , ` strptime ` and ` apply ` work wonders .
What does work is if you apply the same mask to the left hand side like so : #CODE
We can see that my combined code is marginally faster than yours so there's not much saved by doing this , normally you can apply multiple aggregation functions so that you can return multiple columns , but the problem here is that you are grouping by different columns so we have to perform 2 expensive groupby operations .
This obviously means that I wanted to search for words like rigour and rigour s , en demeanour and demeanour s , centre and centre s , h arbour and arbour , and fulfil . So the keywords list I have is a mix of complete and partial strings to find . I would like to apply the search on this DataFrame " df " : #CODE
I think you've neglected to pass the ` axis=1 ` param to apply , so it's operating column-wise hence the error . Try : ` holdings [ ' wt '] = holdings.groupby ([ ' holdings.portfolio ' , ' holdings.date ']) .apply ( lambda x : x [ ' mv '] / sum ( x [ ' mv ']) , axis=1 )`
I'm looking into using the ` sklearn.linear_model.LinearRegression ` module but am unsure on the syntax to use the ` LinearRegression ` in the ` apply ` function that BrenBarn suggested .
I thought about doing an ` apply ` of some sorts to the ` Category ` column after ` groupby ` but I am having trouble figuring out the right function .
As requested by OP , if you want to implement an ` apply ( lambda ... )` to all the columns then you can either explicitly set each column with a line that looks like the one above replacing `' col1 '` with each of the column names you wish to alter or you can just loop over the columns like this :
but how can I apply this to my entire data frame ?? can you edit your answer to my question , it would be really helpful
It varies ; always a good idea to use ` timeit ` yourself to find out , if it's really a bottleneck . For longer frames it'll be much faster than ` apply ` , but for smaller ones you won't be able to amortize the startup cost and soit might be a little slower .
Use ` any ` and pass param ` axis=1 ` which tests row-wise this will produce a boolean array which when converted to int will convert all ` True ` values to ` 1 ` and ` False ` values to ` 0 ` , this will be much faster than calling ` apply ` which is going to iterate row-wise and will be very slow : #CODE
Using the ` %timeit ` module running in IPython Notebook the ` for ` loop compared to the ` apply . ( lambda ... )` is a little over 3 times as fast . #CODE
@USER is there a reason you don't want to use a loop ? According to the ` %timeit ` module running in IPython Notebook , the for loop implementation is over 3 times as fast as a lambda apply .
I thought the for loop is less efficient as l thought the dataframe apply is optimized for matrix type operation like MATLAB . I probably use your approach and stitch the result back to the main dataframe . What's is the most efficient to stitch that column vector back to the data frame
I know that this very possible to do with an apply command but I would like to keep this as vectorized as possible so that is not what I am looking for . So far I haven't found any solutions anywhere else on stack overflow .
I knew there had to be a way to do this without resorting to an apply function . Thank you so much .
If you want to mutate a series ( column ) in pandas , the pattern is to ` apply ` a function to it ( that updates on element in the series at a time ) , and to then assign that series back into into the dataframe #CODE
While this will work , ` apply ` is the method of last resort : it tends to be pretty slow . In this case , we can reach for the vectorized ` dt ` accessor , and get the times via ` df [ " date "] .dt .time ` ( after we've ensured the column is datetimelike , anyway . )
but can't figure out how to apply this to my problem ?
@USER I am getting an Nan value if I apply this map function directly to my data_org data frame .
@USER Sounds like the apply isn't working . Can you post the exact code you have ? If you're using a full dataframe , you would have to do something like : DF [ ' COL '] = DF [ ' COL '] .apply ( lambda x : .......
apply to the entire dataframe a user-defined function involving another dataframe in pandas
Also , I am always confused by apply and applymap , what is the difference and when should use one over the other ?
Never used ' where ' before , thank you very much ! is there any other method to do it ? maybe using apply ?
@USER apply won't be as efficient / fast as vectorizing or using where , since basically it has to loop through within python just like you do in your example code ( rather than using much faster numpy / C ) .
Thanks @USER could I just check if you have any thoughts on the first question ? Are there more efficient ways to pick items from the date index other than the four I've described above ? .. and thanks for explaining the get_loc_level . Do you any use case examples you could share . Where might I apply it ?
Some timings indicate that both my solution as that of @USER is much faster than the apply ( pd.Series ) approach ( and the difference between both is negligible ): #CODE
Python pandas apply on more columns
How can I generate more columns in a dataframe using apply with more columns ? My df is : #CODE
But what if I want to use more than two columns at apply ? #CODE
Using the Series constructor within the apply usually does the trick : #CODE
Thank you this is very helpful reference . I had tried similar methods but was too narrow minded on solely using the Datetimeindex where groupby , map , and apply were running into errors .
I see this option when I want to apply a scalar result , but I couldn't figure out how to put df into lambda like this : #CODE
you can use ` apply ` column-wise on the whole dataframe . #CODE
@USER you might be able to use slicing to index a range of columns in a single operation and an ` apply ` . Check out the Pandas documentation as well as the numerous SO questions for better guidance .
@USER Hello , I have actually came across groupby in the documentation / cookbook / tutorials . I felt like it is what I am looking for , however I was not able to apply it on my problem .
You can pass a dictionary to ` aggfunc ` with what functions you want to apply for each column like this :
Why can't I apply shift from within a pandas function ?
Try passing the frame to the function , rather than using ` apply ` ( I am not sure why ` apply ` doesn't work , even column-wise ): #CODE
have not seen this anywhere in the pandas documentation ! will pursue further , but is this performant on par with apply ?
My assumption is that this is more performant than apply as ( if ? ) the shift and sum are vectorized .
Apply a function to a DataFrame that is intended to operate
Where is it that I have to apply ` toarray ` or ` todense ` ?
This is one method , I'm trying to figure out a vectorised method , basically you define a function that takes your row and then call apply , passing the function name and param ` axis=1 ` to apply row-wise . The color_cols is just a list of your color column names defined by : ` color_cols = [ col for col in df if ' color ' in col ]` #CODE
The mask method is over 2x faster than the query and eval method for this sample dataset . The ` apply ` method is actually the fastest method but it will not scale as well as the other methods as this essentially loops over each row .
Your method works well . But am struggling to apply it when there are multiple columns in the df1 .
and now the muzz function . EDIT : Added choices= right [ match_col_name ] line and used choices in the apply per Brenbarn suggestion . I also , per Brenbarn suggestion , ran some tests with the extractOne() without the apply and it it appears to be the bottleneck . Maybe there's a faster way to do the fuzzy matching ? #CODE
One possibility is to pull the ` right [ match_col_name ]` outside of the ` apply ` , so that you don't recalculate it every time . You should try profiling your code and testing it with different inputs to see if the bottleneck is really in the apply or in the fuzzy matching itself .
Yes , but that is inside the function you apply , which is called once for each element in the series you apply it on . Your code retrieves ` right [ match_col_name ]` repeatedly , once for each element in the Series . If you extract this once to a variable and then use the variable in the call , you will avoid all those redundant lookups .
Why the mismatch in second doing grouping one by one in A then C vs doing them together . Is it a bug or feature ? Normally multi grouping should proceed in this fashion . First take out elements satisfying predicate ' A ' and then use those groups to apply ' C ' grouping .
that should be one of the behaviours of groupby . apply qcut to subset of values in column C . Coz already it's been cut into 2 parts via ' A ' cut
is it possible to do grouping via ' A ' , and then for each of those groups apply grouping to column ' C ' via map / lambda's ?
But I can't find how to apply this to multiple columns
An alternative is to apply : #CODE
Thanks @USER . It's interesting that you sort entries within apply ( e.g. as opposed to sorting them * before * running groupby and apply ) . Is this because ` groupby ` is not guaranteed to preserve the original ordering ?
@USER -Reina in situations like this ( when the function doesn't " reduce ") then transform and apply are the same . In retrospect , I think that sorting globally may be faster ... I mistakenly thought that was the issue causing the most slow down . I think I have a better solution .
One way is to use ` functools.partial ` to partially apply the merge function . #CODE
Hi , thanks , but I don't think this answers my question though . Those methods don't create the rank over a window . I have read through the documentation which led me to ` rolling_apply ` . However this appears to simply apply the function a fresh to each window and over a large dataset in can take a long time to iteratively apply that function . Using the pandas roll function was far to slow . argsort was faster , closer to the bottleneck method above , but I still believe it shouldn't be to difficult to implement a much more efficient way , either by using online windows , or a method I am missing ?
Apply will only return more rows than it gets with a groupby , so we're going to use groupby artificially ( i.e. groupby a column of unique values , so each group is one line ) . #CODE
If you want to stay in pure pandas you can throw in a tricky ` groupby ` and ` apply ` which ends up boiling down to a one liner if you don't count the column rename . #CODE
We want the date to become the single index for the new rows so we use ` groupby ` which puts the desired row value into an index . Then inside that operation I want to split only this list for this date which is what ` apply ` will do for us .
I'm passing ` apply ` a pandas ` Series ` which consists of a single list but I can access that list via a ` .values [ 0 ]` which pushes the sole row of the ` Series ` to an array with a single entry .
Speed wise this tends to be pretty good and since it relies on ` apply ` any parallelization tricks that work with ` apply ` work here .
I've checked out map , apply , mapapply , and combine , but can't seem to find a simple way of doing the following :
I want to apply this and create a new column in the dataframe with the result . #CODE
@USER you not allowing pandas to do anything with your UDF here . You are doing way too much in the apply of the groupby . I am not exactly sure what you are trying to achieve , but using try except blocks , loc , and mutation of the passed in data INSIDE OF A GROUPBY is quite inefficient . A combination of filter and / or indexing will achieve what you want in a much more efficient way . Pls read the docs #URL and possibly provide a self-reproducing example ( in a new question ) if you still have concerns .
Pandas to_html() : Apply CSS style to < td > tag
Easy way to apply transformation from ` pandas.get_dummies ` to new data ?
Then apply the rule to the datatime index , and to a datetime object using ' to_period ` for filtering : #CODE
Pandas on Apply passing wrong value
Apply function to Dataframe GroupBy Object and return dataframe
Using ` groupby ` with ` apply ` will give you a Series mapping old to new IPs : #CODE
Can you show how you were using pd.to_datetime ? That should work fine . If for some reason it won't work , you can apply a strptime function to the str series . #URL
How would I apply it on a specific slice of the data frame , e.g. I want to run the script for each row , but only using columns 6 through 12 ?
Both of these methods are 10 times more efficient than the previous one , iterate on rows which is good and work perfectly on my " debug " table ` df ` . But , when I apply it to my " test " table of 18k x 40k , it leads to a ` MemoryError : ` ( I have 60% of my 32GB of RAM occupied after reading the corresponding csv file ) .
I don't understand your question if you did this : ` df.groupby ( ' A ') [ ' B ']` then you explicitly select just column ' B ' and then you can still apply your functions to this only
If I apply an operation to the Value column I would then like to recalculate the groupby operation : #CODE
I think the calculated groupby MultiIndex should be re-usable to re-calculate the new agg functions ( np.sum is an example ) , but I can't work out how to apply it . How would one most efficiently reuse a groupby method on a dataframe of the same shape and columns structure multiple times ?
True and I often do , but in this instance I want to be able to use the groupby data / construct to apply the operation on a fresh data frame . In actual fact I would want to save the groupby to a hdf and reload it in separate threads , but it doesn't seem possible to save a groupby object to a file .
Pandas speedup apply on max()
I got some fantasy football data and I'm trying to sort it out so I can later apply on it , the full force of scikit-learn .
Here's solution for you test data , I think you can easily apply it to your real data #CODE
You don't need to use ` where ` . Just use ` isin ` and apply your condition directly to the columns : #CODE
This will apply it to only the columns you desire and assign the result back to those columns . Alternatively you could set them to new , normalized columns and keep the originals if you want .
Otherwise you can call ` apply ` like so : #CODE
In the case where the above won't work as it can't generate a Series to align with you df you can apply row-wise passing ` axis=1 ` to the df : #CODE
pandas apply over a single column
how can I apply this function over a single column of pandas ? In pandas documentation , the structure of the function is given as
but I don't see any ability to apply it to a column of the dataframe .
1 . I'd just convert the columns to datetime and then access the date attribute , so ` df [ ' time '] = pd.to_datetime ( df [ ' time ']) .date ` 2 . You can apply to a series also but if you want to use the axis param then you can force a df with a single column using double sqaure brackets : ` df [[ col_name ]] .apply ( func , axis=0 )` 3 . to check if an element is nan you can use the top-level ` isnull ` method so ` pd.isnull ( x )` will return True or False
If you can write this as a function that takes in a 1d array ( list , numpy array etc ... ) , you can use df.apply to apply it to any column , using ` df.apply() ` .
Pandas already knows that it must apply the equation to every row and return each value to its proper index . I didn't realize it would be this easy and was looking for more explicit code .
apply the function to the dataframe : #CODE
I would like to create a function that does this for one imo , and then I can apply it to all of them but I am unfortunately stuck .
Apply a function data frame column
But note ! my test data was evenly sampled . Looks like ` rolling_* ` don't apply to irregular time series yet , though there are some workarounds : Pandas : rolling mean by time interval
I want to loop through and apply a function to the dataframes within ` groups ` that have more than one row in them . My code is below , here each dataframe is the ` value ` in the key , value pair : #CODE
Then you can apply the following logic . #CODE
Your original issue was caused because your individual loops did not contain the same amount of each element you were looping through ( i.e. - 15 stars , v . 20 prices ) . The best way to avoid this type of issue is to firstly have one loop and then to apply a try except value to each item you're scaping . That way if there are any issues with the constant presence of the items you want , you can still collect what is present . #CODE
Is there a way that I can apply this to the entire dataframe at once , rather than looping through rows ? Or other suggestions to speed this up ?
How to apply rolling functions in a group by object in pandas
Apply rolling mean function on data frames with duplicated indices in pandas
You can check this question and its answer to see how to apply a function row by row : #URL Otherwise , in order to fully answer the question i.e. be able to have all desired fields , we need to know exactly what's in the data ( not just one line ) . Could you for instance split on " two blanks or more " ( typically no if there are missing values ) ... or do columns have the same position in the string in each row etc .
Python Pandas : Apply function to dataframe in place
Although apply doesn't offer an inplace , you could do something like the following ( which I would argue was more explicit anyway ): #CODE
How can I apply a function that transforms it into a dataframe like this : #CODE
I could use a list comprehension to apply the selected result on the ` get_loc ` function , but perhaps there's some Pandas-built-in function .
Apply permutation matrix to pandas DataFrame
I have two identically sized DataFrames ( call them ` A ` and ` B `) with the same set of index and column names . ` A ` and ` B ` have a different ordering of their ( row / column ) labels and I want them to be identically labeled so I can directly manipulate the matrices in other programs . Mathematically , there is a permutation matrix ` P ` that reshuffles one matrix labels to another , so I can apply this transformation by constructing the matrix . I feel however , that this is overkill and a solution should exist within pandas itself .
Calculate the weights you'll need to apply to achieve your target age / gender distribution : #CODE
If anyone has experience with this I'd love to see what you wrote . There are examples online for using a .csvreader with python which loops through the rows and adds them as they are being read but I can't find any example of how to take data stored in a dataframe and apply it to a table defined as part of an extract .
I've read all the documentation I'm just not sure how to apply it . There is no clear instruction as to moving data from a dataframe to a virtual table in python and I know this is a problem for other people using different type of target table ( #URL ) . when I try to insert the dataframe to the table as shown in the tutorials , an error returns : " dataframe not callable "
I then want to apply poisson sampling noise to all data in the abundance frame . #CODE
You have to call ` apply ` and pass the data to ` strftime ` : #CODE
you can apply a lambda function to the column ` a ` in your dataframe that returns the lowercase of the string contained , if your correction is just making the string lowercase .
the ` apply function ` method can be extended for other more specific replacements .
Also you can probably generate the new rows by calling ` apply ` which would be much easier to read than what you're doingnow
so I tried replacing the row [ ' J '] and row [ ' K '] with row.loc ( ' J ') and row.loc ( ' K ') but that ended up in some error messages . Am I just going about this all wrong by the for looping through index and row ? Should I just apply a function directly to row.loc ( ' I ') ?
For the email bit we can just use the same regex and call ` findall ` , for the other bit we just pass the func as a param to ` apply : #CODE
@USER OK , I've updated my code , we can use your email regex directly as a param to ` findall ` , for the name bit we can just pass that as the param to ` apply `
I am trying to apply this function to each row in player_points_position and create a new column ` zscore ` . However , the entire data set is returning the same value . #CODE
I can group by id and apply a function per group to define wich data is the right one . I can unstack the dataframe and put ' group ' as columns and apply a function . Is there a simpler more elegant way to do this ?
Pandas apply exponential decay + dataset to function
I'd like to make a function in pandas that calculates the resulting , continuous dataset for my activation function , but I don't know which functions to apply .
pandas - apply time and space functions to groupby
This may be an excellent opportunity to highlight the " Split , Apply , Combine " premise and with a simple case use ?
If you have a DataFrame where all columns are booleans ( like the slice you mention at the end of your question , you could apply ` all ` to it row-wise : #CODE
Usually , you can apply the function in one of the following ways : #CODE
If the dataframe is empty , or has only one entry , these methods no longer work . A Series does not have an ` iterrows() ` method and ` apply ` applies the function to each column ( not rows ) .
Is there a cleaner built in method to iterate / apply functions to DataFrames of variable length ? Otherwise you have to constantly write cumbersome logic . #CODE
I realize there are methods to ensure you form length 1 DataFrames , but what I am asking is for a clean way to apply / iterate on the various pandas data structures when it could be like-formatted dataframes or series .
Instead of doing either of those things , I think it is better to make sure you create the right type of object before calling ` apply ` . For example , instead of using ` df.iloc [ 0 ]` which returns a Series , use ` df.iloc [: 1 ]` to select a DataFrame of length 1 . As long as you pass a slice range instead of a single value to ` df.iloc ` , you'll get back a DataFrame . #CODE
Python Pandas : Using ' apply ' to apply 1 function to multiple columns
Essentially , I'd like to know if I could apply ` function ` to ` df ` to get the following output : #CODE
you need to apply the function on each row , for this you need to specify axis=1 #CODE
Use apply / map for pandass dataframe column
But id does not work with ` apply ` method : #CODE
What am I doing wrong and why apply method takes this int index as parameter ?
After reading your data and puting in a dataframe , you can groupby values based on one of the columns ` groupby ([ ' month '])` , and then apply a function on these values , Pandas includes a number of common ones such as mean() , max() , median() , etc . : you can use ` sum() ` for example . #CODE
I only know I could use ` concat() ` to combine columns and use ` apply ( lambda xxx ... )` to set up a suitable function .
That works perfectly . I was apply to modify the format a bit to help me out on another file as well ! Thanks !
Ideally , I want to store the output in a new DataFrame , where each row corresponds to the rows in the original DataFrame , and there are 3 columns . I was asked not to use a loop to do this , and to use apply , but I cannot figure out the syntax ( see attempts below ) . What are my options for getting the desired output ? ( I'm new to python and pandas , so please excuse my ignorance and let me know if I've left out any necessary info ) . #CODE
if you want to store the results in your data frame , I would define a function and then apply it to the data frame like this : #CODE
Apply function over relative rows in Pandas
of course , one alternative is writing a for loop employing df.loc [ i , col ] and df.loc [ i-1 , col ] , but I generally find apply or transform with functions computationally faster
You could use a mask and apply it to the dataframe #CODE
The above is then groupby'd on customer and then we can apply a filter where the number of unique ( nunique ) customers is equal to 2
First , I would like to add 3 extra columns with order numbers , sorting on sum , sum_sq and max , respectively . Next , these 3 columns should be combined into one column - the mean of the order numbers - but I do know how to do that part ( with apply and axis=1 ) .
Then apply the rolling_mean to ` result ` , so you get two columns of rolling means : #CODE
Hmmm , I guess I should have transposed the data ... well that was a relatively simple fix . Instead of using groupby and apply , #CODE
Thanks for the reply ! It works when I just pass a single string to that function but not when I apply that function to a column ----> df [ ' Duration '] = df [ ' Avg . Session Duration '] .apply ( convertTime ) . It returned this error = TypeError : expected string or buffer
This avoids the need for a bespoke Python function and the use of ` apply ` . Testing it for the small example DataFrame in your question showed that it was around 8 times faster .
I am really sorry for being that naive , I promise to learn as much as I can if someone could guide me towards the right direction ( regarding the theory and technologies to apply ) .
Do I really need to ` apply ` and iterate through each row , or is there a more efficient alternative ?
` map ` can take a dictionary , Series or function and return a new Series with the mapped values . It is also very efficiently implemented ( much more so than ` apply ` , for example ) .
What is a better way to apply the function to each row ?
I have looked into ways of creating a function to do this , but confused as to how to map and / or apply it in my case , especially the part returning the result as a new column .
Just trying to find the most elegant way to apply a really simple transformation to values in different columns with each column having it's own condition . So given a dataframe like this : #CODE
I was thinking the where function in pandas would come in handy here but wasn't sure how to apply it . I could do the below but does not seem very efficient and I would have to create a different function for each col : #CODE
You can't just just stick your expression in brackets onto the groupby like that . What you need to do is use ` apply ` to apply a function that calculates what you want . What you want can be calculated more simply using the ` diff ` method : #CODE
However , it is good to be aware of how to do it with ` apply ` because you'll need to do things that way if you want to do a more complex operation on the groups ( i.e. , an operation for which there is no predefined one-shot method ) .
You don't need a regex here , just create a lookup table and apply to your DataFrame's column based on that column's first character , eg : #CODE
To apply this to all columns , then loop over the columns : #CODE
@USER then just apply it to all relevant columns ? Could you [ edit ] ( #URL ) your question to clarify exactly what you do have ... ?
You can groupby the user_id column and then call ` apply ` and pass a lambda which filters the results where the start time is equal to the max value , we want to generate a boolean index from this . We can then call ` reset_index ` but due to the way the groupby was filtered we will get an error with duplicate columns so we have to drop this duplicate column : #CODE
thank you , the ` apply ` version actually worked but not the direct ` unique() ` version .
The solution below uses a lambda function to apply a regex to remove non-digit characters .
Solution is great . but when I apply this to my original data frame I am getting an error " invalid literal for int() with base 10 : ' 16a '"
pandas - show results of apply next to original dataframe
I have a pandas DataFrame , then I apply a function to a bunch of columns and I get a new result column . Then I want to be able to check the results of the new column with the original column values . #CODE
One way to calculate this is to use ` apply ` on the ` groupby ` object : #CODE
When I apply this on my real data frame ( problem set ) . I am getting an error ` TypeError : Argument ' values ' has incorrect type ( expected numpy.ndarray , got Series )` what would be the reason ?
Ok I am working on it right now . will let you know after I apply it to my code !!
How do you apply a function to one of several columns in a DataFrame ?
I would like to apply a function to data in one of the columns and return the same DataFrame but with new values in the column to which the function was applied to .
Sorry it's unclear why you need to apply on several columns when the following does what you want : ` df [ ' Numbers '] = df [ ' Numbers '] .apply ( lambda x : int ( ' 1 ' +str ( x )))` can you explain what you are trying to do ?
I only want to apply on one column . In your example I would get a DataFrame or Series with one column only , which I would further have to merge with the the initial DataFrame . I hoped I could avoid this by applying a function to one column and leave everything else untouched .
I saw that it's possible to convert the column into the datetime format by DF = pd.to_datetime ( DF , ' %Y-%m-%d %H : %M : %S ') but when I try to then apply datetime.datetime.year ( DF ) it doesn't work . I will also need to parse the timestamps to months and combinations of years-months and so on ...
No need to apply a function for each row there is a new datetime attribute you can call to access the year attribute : #CODE
Thanks , @USER , but it doesn't work , giving AttributeError : ' Series ' object has no attribute ' year ' , although I converted the original DF into datetime64 [ ns ] ... I tried to apply DF.year and it is not working ...
Thanks for your effort . I posted another question but I think I am getting close .... using the apply method with a lambda function seems to be headed in the right direction .
I'd like to apply the model to column ` c ` , but a naive attempt to do so doesn't work : #CODE
I would like to select certain rows from a DataFrame and apply a result from lambda from it , and I am not able to assign it correctly , either all the other columns become NaN or the DataFrame not changed at all ( I believe this is related to DataFrame returning a copy , read that caveat )
I am using the .to_excel method in pandas to write dataframe in an excel file . However i want to change the default formatting . The answer at Apply styles while exporting to xlsx in pandas with XlsxWriter helps with the basic formatting .
Apply function on cumulative values of pandas series
Is there an equivalent of ` rolling_apply ` in pandas that applies function to the cumulative values of a series rather than the rolling values ? I realize ` cumsum ` , ` cumprod ` , ` cummax ` , and ` cummin ` exist , but I'd like to apply a custom function .
@USER , you started with a different input ( a string that formats as a list , I start from a list ) , but I am not sure what the OP wants . Apart from that , you did the ` get_dummies ` within the apply ( so for each row instead of once on all ) , which made it slower as the approach above .
I have a dataframe ' clicks ' created by parsing CSV of size 1.4G . I'm trying to create a new column ' bought ' using apply function . #CODE
` apply ` is essentially just syntactic sugar for a ` for ` loop over the rows of a column . There's also an explicit ` for ` loop over a NumPy array in your function ( the ` for row in boughtSessions ` part ) . Looping in this ( non-vectorised ) way is best avoided whenever possible as it impacts performance heavily .
First use ` groupby ` to group the rows of ` buys ` by the values in ' session ' . ` apply ` is used to join up the strings for each value : #CODE
` groupby ` means that only one pass through the DataFrame is needed and is pretty well-optimised in Pandas . The use of ` apply ` to join the strings is unavoidable here , but only one pass through the grouped values is needed .
To match each string in ` boughtSessions ` to the approach value in ` clicks [ ' session ']` you can use ` map ` . Unlike ` apply ` , ` map ` is fully vectorised and should be very fast : #CODE
Using apply ( or some other vectorisation ) to perform calculation involving two ( or more ) data frames ?
So , I'm not sure how to go about vectorising this . Naively I could split this into two apply statements , each effectively replacing each iterrows call . But is there some other clever approach , as there will still be significant looping overhead with that solution .
python apply function to list and return data frame
I am new to python . I wrote a function that returns a pandas data frame . I am trying to apply this function to a list and I would like to merge all the results to one data frame . For example , if my function looks like : #CODE
I want to apply it to list ` [ 1 , 2 , 3 , 4 , 5 ]` , and get the result as a data frame which looks like : #CODE
This works for me in pandas 0.12 . Can you check which part throws the error ? ( the ` to_datetime ` part , or the ` apply ` part )
Probably it is due to some missing values . If you use ` dropna ` before using ` to_datetime ` and ` apply ( ... strftime() )` , this will work . A small example : #CODE
Yes . I want to apply that function in Time column of data frame .
I think your problem maybe that you're not assigning the result of your ` apply ` back : #CODE
and then scale the count array to apply kde() to it ?
One pandas method would be to call apply on the df column to perform the conversion : #CODE
One method , so long as datetime is already a datetime column is to apply ` datetime.strftime ` to get the string for the weekday : #CODE
Then I have to apply a function to the dataframe to create a new column based on some values : #CODE
Yes thanks ! So , should be better to concatenate the dataframe inside the loop instead of build the whole dataframe outside and then apply the function ? The RAM consumption is proportional to the size of the chunk , no matter the final dimension of the whole concatenate DataFrame ?
The " dumb " way would be to cycle through the frame ( iterrows ) and compare one by one . There must be a smarter , Pandas way like using something like apply / join / whatever .
Here's one way of doing it , using ` groupby ` and ` apply ` : #CODE
Trouble passing in lambda to apply for pandas DataFrame
I'm trying to apply a function to all rows of a pandas DataFrame ( actually just one column in that DataFrame )
Can you show ` df.info()` and which columns you are trying to perform the calculations on , you will not be able to pass 2 columns row-wise if you are calling apply on a series
I am unable to apply the last operation across all of the DataFrames . It seems I can only get it to apply to the last DataFrame in my list . Once I get past this point I will have to append all of the DataFrames to form one large DataFrame .
I need to create an index on a specific frequency , however I need to apply that frequency only for certain months . This is the frequency : #CODE
I suppose the issue is that VALUE does not have an upper level . A similar operation is not described in the manual . Isn't there a way to apply ` stack ` only to some columns ?
how to apply ceiling to pandas DateTime
I was considering this as well , but I need to slice several dataframes in the same way , so I would need to do it multiple times . I was looking to have the function take the locationargument and apply it to all dataframes in one call . Thanks !
I will get different groups that do not coincide . Is there some way to get the ` datetime ` related groups from the first grouping , and apply them to the second grouping instead ? Or how could I achieve this else wise ?
this won't work because you are calling apply on the df , naturally this will iterate over the columns and you are trying to check the probability column it's unclear to me what you are trying to do here , are you checking just the probability column or all columns ?
So it would be great if there's some option to do the job with a * .csv with two rows . On the left all " Industry Category " items and on the right the desired " Parent Category " I like to apply to the dataset .
I think I got the idea . I have to create a csv and apply the the2nd step of this [ link ] ( #URL ) . Once I have the dict , I have to map the df with .map ( category_list.get )
I do this quite a lot . I would create a dictionary and use ` apply ` and ` lambda ` . #CODE
I am trying to read a certain DF from file and add to it two more columns containing , say , the year and the week from other columns in DF . When i apply the code to generate a single new column , all works great . But when there are few columns to be created , the change does not apply . Specifically , new columns are created but their values are not what they are supposed to be .
it has to do with not changing all data values , but i don't understand why the change does not apply - after all , before the second iteration begins , the DF seems to be updated and then ` tbl [ tmp_col_name ] = ' No Week '` for the second iteration " deletes " the changes made in the first iteration , but only partially - it leaves the new column created but filled with ' No Week ' values ...
I'm struggling to find a way to iterate over Df , and for each row , apply a definition that iterates to search for the nearest match in Df1 ( with the aim to add data from Df1 to Df ) . Read and tried a lot of methods found here , but not winning . Would appreciate some pointers , especially if I'm going the wrong route :
You can use ` apply ` to call a lambda function that splits the string and then joins on the unique values : #CODE
Basically we can drop the ` NaN ` rows first and then call ` apply ` and use ` datetime.strftime ` to apply a new format : #CODE
Can you post what the final df values should be and give examples of the calculations you are trying to apply , thanks
@USER , unfortunately I think there is no general answer that I know of for which option to opt for . It really depends on both your data and the specific machine learning / statistical algorithm you would like to apply . However ; I would generally suggest opting for 1 ) as base case unless you have specific concerns about correlations between your features and how your ML algorithm deals with that .
One method is to apply a function to your df to split the ' cc ' column and create a new dict that contains each split country and their associated count , you can then construct a new df from this , groupby the country and perform the sum on the count : #CODE
Python PANDAS : New Column , Apply Unique Value To All Rows
My confusion seems to be about which function to utilize ( apply , mapapply , etc . ) , if I need to reference an index value in the original df to apply a completely unrelated value to the initial df , and the most optimized , pythonic way to accomplish this .
` monthrange ` returns a tuple with the first and last days of the month , so ` [ 1 ]` references the last day of the month . You can also use @USER ' s method for finding the date of the last day , and assign it directly to the column instead of ` apply ` ing it .
Columns are fixed I would extract the column calculations and vectorize the real , child and other normalizations . Use apply rather than iterating ( for zfill ) . #CODE
Looks like it did in first test @USER . I'm doing some more tests for confirmation so I can apply the bounty with a fair judgement .
You could define a function and call ` apply ` passing the function name , this will create a df with min and max as the index names : #CODE
Use the datetime attribute to filter on date and then ` apply ` a function to replace just the day component : #CODE
I've found apply and map great for speeding up calculations on particular rows in a DataFrame .
The question I've got is : Is it possible to return a value with apply or map functions which refer to a previous row ?
Apply and map are great at vectorising row by row calculations - is it possible to refer to the previous row so I can make that calculation ?
My second suggestion to specify this in the groupby call with ` as_index=False ` , seems not to work as desired in this case with ` apply ` ( but it does work when using ` aggregate `)
Apply function row wise on pandas data frame on columns with numerical values
To apply an arbitrary function , ` func ` , to each row : #CODE
So you see here that apply as it is iterating row-wise scales poorly compared to the other two methods which are vectorised but ` map ` is still the fastest .
I would like to have the company symbols in their own seperate column instead of inside the Company Name column . Right now I just have it iterate over the company names , and a RE pulls the symbols , puts it into a list , and then I apply it to the new column , but I'm wondering if there is a cleaner / easier way .
but it will be faster to use the inbuilt ` to_datetime ` rather than call ` apply ` which essentially just loops over your series .
Using apply will be substantially slower than your first method by the way
Not at the moment , cumsum is a vectorised method apply will not beat this .
Here's a one-liner , but with cumsum : ``` ( df.Volume * ( df.High + df.Low + df.Close ) / 3 ) .cumsum() / df.Volume.cumsum() ``` . As @USER notes , cumsum is going to beat apply . I doubt you're going to improve speed by avoiding cumsum in pandas . Why do you want to avoid cumsum ? Beyond this , I'd guess you can improve speed slightly by doing in numpy , and even more by doing in numba .
Now , suppose I have multiple data frames outside the function , say ` df1 ` , ` df2 ` , ` df3 ` on which I want to apply ` myfunc ` . I want to ensure that the function ` myfunc ` uses a local copy of the dataframe - I want to avoid the situation that the ` df1 ` outside the function doesn't get changed / modified by the operations inside the dataframe .
Apply Across Dynamic Number of Columns
In order to solve this big problem , I find read_fwf in pandas module and apply it but failed . #CODE
Then I want apply fill_between() on area between A and B series : #CODE
@USER use ` pd.to_datetime ` rather than apply . If you use the apply it may not create a Datetime column . I think you may have to update to 0.15.X for the dt accessor . If you want month-year then use the ` to_period ` part of the answer above ?
Whether this is more efficient than a groupby / resample apply solution will depend on the data . For very sparse data ( with lots of starting up NaN , assuming you want to drop these ) I suspect it won't be as fast . If the data is dense ( or you want to keep the initial NaN ) I suspect this solution should be faster .
Python Pandas ' apply ' returns series ; can't convert to dataframe
OK , I'm at half-wit ' s end . I'm geocoding a dataframe with geopy . I've written a simple function to take an input - country name - and return the latitude and longitude . I use apply to run the function and it returns a Pandas series object . I can't seem to convert it to a dataframe . I'm sure I'm missing something obvious , but I'm new to python and still RTFMing . BTW , the geocoder function works great . #CODE
The goal is to geocode 166 unique countries , then join it back to the 188K addresses in df_addr . I'm trying to be pandas-y in my code and not write loops if possible . But I haven't found the magic to convert series into dataframes and this is the first time I've tried to use apply .
Thanks Ed ! What if I would like to filter Users with occurrences only in some months ? Can I apply some dt.month == june conditions ?
If you actually have strings that look like tuples , you can parse them first and then apply the same pattern as above : #CODE
If it is actually strings , you can first convert it to lists like so , then apply the above operation : #CODE
To create multiple columns when using ` apply ` , I think it's best to return a ` Series ` rather than a list . You can set the column names by setting them as the index for the ` Series ` . So you can do : #CODE
I have a pandas TimeSeries and would like to apply the argmax function to a rolling window . However , due to casting to float from rolling_apply , if I apply ` numpy.argmax() ` , I only obtain the index of the slice of the ndarray . Is there a way to apply a rolling argmax to a Series / DataFrame ?
Here is a work-around , essentially doing the apply ' manually ' , should be pretty efficient actually . #CODE
You can also use the ` apply ` method for a one-liner , which is simpler and clearer but also slower even than your approach : #CODE
Apply formula to Multi-index column Python
Want to apply formula and label result to multi-index dataframe .
Gut tells me I need to group-by and then apply formula , I can handle that ( I think ) , but how do I bring label ( ' pattern ') along ?
Apply where function [ SQL like ] on datatime Pandas
pandas - apply datetime functions
Optimizing pandas filter inside apply function
I now need to apply this function to several million rows and it's impossibly slow so I'm trying to figure out the best way to speed it up . I've heard that Cython can increase the speed of functions but I have no experience with it ( and I'm new to both pandas and python ) . Is it possible to pass two rows of a dataframe as arguments to the function and then use Cython to speed it up or would it be necessary to create new columns with "` diff `" values in them so that the function only reads from and writes to one row of the dataframe at a time , in order to benefit from using Cython ? Any other speed tricks would be greatly appreciated !
Geopy error : GeocoderServiceError : HTTP Error 500 : Internal Server Error using pandas apply function with str concat
Working function ( see code Python Pandas apply returns series ; cant convert to dataframe ) has stopped working . Only difference is I'm passing it a string concatenation . #CODE
Ed , I see what you are saying - that I'm passing a series - but am not sure how to fix it . Short of ditching apply and iterating through the table , or passing 5 parameters and then iterating through the table . I thought apply() did this for me - calling the function once for each row . The debug code seems to indicate this as it says that x is a str type , not series . Hmmm ... I believe you , I'm just trying to wrap my head around it and decide what to do next . And I manually confirmed the geocoder is insensitive to white spaces . Any Further advice ?
What you're doing is a little perverse to be honest , you're calling ` apply ` on a series and then trying to construct a str from lots of columns , this is the wrong way to go about this , you can call apply on the df and pass ` axis=1 ` so that the row is passed and either access each column in a lambda func and pass them to ` locate ` or in ` locate ` extract each column value , or just create a series from the concatenation of all the columns and call apply on this : #CODE
Your DataFrame column contains a mixture of strings and tuples . I don't think you can avoid iterating the column . But you can iterate efficiently with the apply method . Example code follows . #CODE
Then I am able to process it using apply , for example : #CODE
The solution to this problem is to apply ` reset_index() ` to " end " the group-by operation .
Just apply a ` filter ` : #CODE
The point is that I don't think I've understood it , sorry . Ok , no ANOVA . But then , you will run 24x2 t-tests ?? Maybe I still haven't understood ... Anyway , if you want to apply a value to a multiindex , it would be something like ` df.loc [ ' 0hr '] .loc [ ' 0.01um '] [ ' t '] = xxx ` , assuming that ' t ' is a column . For your dataframe , I've tried it straight in ipython as ` df.loc [ ' 0hr '] .loc [ ' 0.01um '] .loc [ 0 ] [ ' a '] = 3 ` . But if again I haven't understood , just tell me !
apply sort to a pandas groupby operation
How do I apply sort to a pandas groupby operation ? The command below returns an error saying that ' bool ' object is not callable #CODE
Normally the sort is performed on the groupby keys and as you've found out you can't call ` sort ` on a groupby object , what you could do is call ` apply ` and pass the ` DataFrame.sort ` function and pass the column as the kwarg param : #CODE
It doesn't matter for your mean calculation because you're just generating a boolean that pandas interprets as 0 / 1 . But for the correlation coeffient you need to provide numbers . You also need to use ` apply ` rather than ` agg ` here : #CODE
Thanks Ed and Alex , never used ' .gt ' before . Just out of curiosity , is it possible to use ' apply ' to realize the same function ?
Yes you could but I'd advise against it because apply is slow whilst this will be vectorised , apply should be a last resort always
thank you John , I was very curious about how to use apply to realize it .
Python How to find average of columns using dataframes apply method
Using the dataframe's apply method , create a new Series called ` avg_medal_count ` that indicates the average number of gold , silver , and bronze medals earned amongst countries who earned at least one medal of any kind at the 2014 Sochi Olympics .
I played around with this for a couple of minutes . I think v1 / v2 are empty for some data values , in line with what Andy suggests . Also coef doesn't seem to have any purpose in the code , just fyi . Anyway , your groupby / apply is probably OK , you just have something wrong with the function itself .
And , Now , let's see what was the error that was happening , when you apply np.argsort() after first group on series object . Lets take the second group values . Which is - #CODE
Ok , it solves the example . The problem is that I do not have a string of data as " data " in your example . I only have csv-file , and i tried to apply your solution but could not make it work . Do I have to convert the csv-file to a string of data first maby ?
Alternatively , you can represent ` sym ` column as a q generic list containing strings . You can also apply type conversion to other columns : #CODE
How can I apply a search , such that the result would be the index for each value , if it exists , in an efficient way ( since I know the column `' A '` has uniqu values ) to get the following results : #CODE
Yeah the euclidian approximation will work fine for small enough distances . You shouldn't even need to do an ` apply ` for that , can directly just use the columns in the dataframe .
You can do this by ` apply ( pd.Series )` on that column : #CODE
That's quite a long lambda function in your apply , I recommend writing as a function . For one thing it'll be easier to debug ( clearer which line is causing the error ) . I could be wrong but it looks like ` search ( Origin_Zip , stop=stop , pause= 5.0 )` doesn't always return a list ( but an int ) .
I'd reset the index so that it becomes a column , this allows you to call ` apply ` on it , then for each datetime apply a lambda which calls ` replace ` and null the minute and seconds attributes , then drop the duplicates and set the index back : #CODE
Something like this should work . It will be faster than ` apply ` since it uses vectorized operations . Further , rather than hard-coding the ` apple ` result , it gives you counts and percentages for all purchases , no matter how many you might have . #CODE
@USER , right off my head I will say filter the selection of the columns , then apply the same on the values copy
simply multiply the number by 100 to scale it in the range ( 1 , 100 ) , and then apply the same algo , however you can play with numbers and find out your own way of doing the same .
I have a time-series data in " stacked " format and would like to compute a rolling function based on two columns . However , as shown in my example below , the ` groupby ` is concatenating my results horizontally instead of vertically . I can apply ` stack ` at the end to get back to tall format . However , I thought the correct behavior should be to concatenate vertically to allow assignment back to the original dataframe ( something like ` x [ ' res '] = df.groupby ( ... ) .apply ( func )`) . Does anyone know why ` groupby ` is not behaving as expected or am I doing something wrong ? #CODE
@USER Also calling ` apply ` should be the last resort when working with arrays , it is not vectorised and therefore will not scale well
apologies if this is a silly question , but I am not quite sure as to why this behavior is the case , and / or whether I am misunderstanding it . I was trying to create a function for the ' apply ' method , and noticed that if you run apply on a series the series is passed as a np.array and if you pass the same series within a dataframe of 1 column , the series is passed as a series to the ( u ) func .
mode isn't a groupby method , though it is a Series ( and DataFrame ) method , so you have to pass it to apply : #CODE
For each set_up in set_ups I want to apply ' value ' to ' set_up ' and groupby ( level=0 ) or df.A and df.B . #CODE
First , we can perform a groupby / apply operation to obtain the Protein / Peptide pairs with the two largest Peptide counts for each Protein : #CODE
Apply function to multilevel columns
I renamed your columns to ' author ' and ' citations ' here , we can groupby the authors and then apply a lambda , here the lambda is comparing the number of citations against the value , this will generate a 1 or 0 if true , we can then sum this : #CODE
Pandas : column of type str converted to tslib.Timestamp after using apply function
Thank you . That worked ! Is this a implementation problem of pandas or the apply function that I was using ?
You can apply multiple functions to multiple fields : #CODE
So what are you trying to achieve here ? for squaring the values this is trivial to perform and doesn't require the use of ` apply ` in this case
I am trying to create arbitrarily many fields and assign them values simultaneously using the apply method . The simple functions in the example just represent any arbitraryfunction I may want to use for generating a new field . The issue is when trying to create more than 1 field in the above example it errors . Once I can do this I can replace the trivial function with any value .
This will certainly fix the problem . But it will involve having the apply return only a single field at once . What I was curious about was being able to use apply to return arbritrarily many fields . By being able to do this it should allow for cleaner code .
The original approach you suggested is correct , although you have to use a ` transform ` on the groups ( by ` date ` AND ` source `) instead of an ` apply ` . ` transform ` return the group information with the same structure of the original dataframe . #CODE
It splits up the DataFrame by store_nbr , calls is_good on each each group ( apply ) to determine the rows you want to keep , puts everything back together in the right order , and then takes a subset of rows from the original frame .
If you want to apply it on a full column , you can also do : #CODE
Yes there is a concise and efficient way to solve this . You were on the right track with ` df.dropna() ` , just that you need to ` unstack ` your data before you apply it . #CODE
The last expression would apply to your case if ` res_tmp.fittedvalues ` are the predicted or fitted values of your winsorized model , and ` y_orig ` is your original unchanged response variable . This definition of R squared applies if there is a constant in the model .
By now , you'd have a sense of the pattern . Create a ` distance ` method . Then apply it pairwise to every column using #CODE
Since ` count_dic ` is actually a ` dict ` , then you can apply ` len ` to get the number of keys , eg : #CODE
Python DataFrame - apply different calculations due to a column's value
I am setting the values to NaN and then apply the fillna method .
I think the main problem here , is because I'm trying to apply the ` pool.map ` to ` rpy2 ` function and not a Python predefined function . Probably there is some workaround solution for this without using the multiprocessing library , but I can't see any .
maybe the .map() was trying to apply both the mask and the operation : #CODE
You can use the apply function : #CODE
And , then ` apply ` ` split_cumsum ` over ` df.groupby ( ' Group ')` #CODE
Thanks John , I didn't quite appreciate what circumstances would make sense to create a function and apply it to a DataFrame , but this solution is definitely a cleaner approach .
That is not so surprising but ` apply ` does not scale well , I just did timings on a 600 row df and the timings were 6.24ms vs 33.3ms comparing my method against yours , I expect the performance difference to increase significantly on much larger datasets
@USER Absolutely , ` apply ` doesn't perform well on larger datasets . So , I mentioned * expensive for this smaller data . * =)
Is there some sort of ` apply ` equivalent ( like in ` pandas `) that would make this more efficient ?
Pandas Apply ( axis=1 ): produce more than one row
I have a function I want to apply by row like so : #CODE
As you can tell , this function is meant to take a list of items and create a row for each item that duplicates the rest of the remaining data . Unfortunately , my current method isn't the correct usage of the apply method : #CODE
This question is similar to pandas : apply function to DataFrame that can return multiple rows that Wes McKinney has answered .
Let , ` df ` the dataframe with two columns , apply conditional absolute minimum over rows using ` axis=1 `
OK , after reading and understanding your question and not being able to find a vectorised approach , we can define a custom function and call ` apply ` and pass each row .
` df.values ` returns a NumPy array containing the values in ` df ` . You could then apply ` np.std ` to that array : #CODE
I believe I can achieve this by using apply on the first to filter the second based on these criterion and then combining the results but apply has in practice been a horribly slow way to go about things .
Set column name for apply result over groupby
What I'd like to do is assign a name to the result of ` apply ` ( or ` lambda `) . Is there anyway to do this without moving ` lambda ` to a named function or renaming the column after running the last line ?
Yes , that was what I was planning to do . However , I know realize it is not the correct way to do it since they are not independent . Your answer give me a better understanding how to use pandas groupby and apply . Thank you very much !
What you want to do is ` groupby ` on the index levels and apply a function that calls ` mannwhitneyu ` , passing the two columns ` course1 ` and ` course2 ` . Suppose this is your data : #CODE
Yeah . this works . I wrongly assumed sum() would not apply to lists . Thanks a lot
apply if statement within sort.head()
Am I not allowed to use a series in a pd.apply function ? If so how can I apply a function row by row and assign the output to a new column ?
You don't need to use apply when calling the function . Just use : #CODE
I reworked the formulae to apply to series : #CODE
Because the parameters lon2 and lat2 are Pandas Series , dlon and dlat will both be Series objects as well . You then need to use apply on the series to apply the function to each element in the list .
Okay , I just restarted the kernel in my ipython notebook and now with the above i get this error - ` AttributeError : ' numpy.float64 ' object has no attribute ' apply '` for the a= code that you gave above
Do you just want to print it or actually DO something with it or to it ? E.g. if you want to apply a function to every element , see ``` applymap() ```
When I apply ` plt.xcorr ( df.Val1 , df.Val2 )`
Note : Before you apply ` factorize() ` you need to ` fill ` your ` NaNs `
And I want to calculate the following information " How many days was each account active ? " , I understand that I could simply do a count to get this information , but I want to apply the following restriction , " If there are n days between activity dates , only count the days before that gap " .
It seems more logical to apply the filter to the time column as it is being read in then to operate on it later . Is there a way to do this by telling the read function what function to call as it reads the column , before storing the object in memory ?
Below , john-galt gives an extremely helpful answer . However , I've found one case where it's not immediately obviously how to apply his solution : using a custom grouping function .
But unlike most Pandas Cython tutorials or examples I am not apply functions so to speak , more manipulating data using slices , sums and division ( etc ) .
@USER in apply / agg the function needs to take the subDataFrame / each group . tbh I'm a little confused at what you're trying to do : s
You could all ` df.filter ( regex= ' HW ')` to return column names like ' HW ' and then apply sum row-wise via ` sum ( axis-1 )` #CODE
I have a big ` DataFrame ` in pandas with three columns : `' col1 '` is string , `' col2 '` and `' col3 '` are ` numpy.int64 ` . I need to do a ` groupby ` , then apply a custom aggregation function using ` apply ` , as follows : #CODE
If you want the values themselves , you can ` groupby ` ' Column1 ' and then call ` apply ` and pass the ` list ` method to apply to each group .
You could ` groupby ` on ` Column1 ` and then take ` Column3 ` to ` apply ( list )` and call ` to_dict ` ? #CODE
@USER this is incredible .. clever use of apply ! what a great principle .. can be applied as a function for any kind of dict look up .
Same code should also apply to a binary confusion matrix like : #CODE
Don't use ` apply ` you can achieve the same result much faster using 3 ` .loc ` calls : #CODE
df [ ' column_of_ints '] is a Series not a DataFrame , there is no ` axis=1 ` for ` apply ` method for a Series , you can force this to a DataFrame using double square brackets : #CODE
@USER No it gives me the same warning even on using apply instead of map
One way to do this apply ` clip_upper() ` on 90 percentile value ` np.percentile ( x , 90 )` for each column #CODE
I had imagined @USER elegant solution would faster than ` apply ` . But ,
Thanks for the timings ! I had thought the same thing but , having just checked timings on my machine , it appears ` apply ` can be surprising sometimes :-)
Not sure why , but you would notice that in the benchmarks posted , this method seems slower than apply method . +1 for cleaner single-liner .
Note that these days you can use ` expand=True ` instead of ` apply ( pd.Series )` : #CODE
You can't pass a Series as a param to a function unless it understands what a pandas Series or the array type is so you can instead call ` apply ` and pass the function as the param which will call that function for every value in the Series as shown above .
You could call ` apply ` and use ` datetime.strptime ` : #CODE
A function can be applied to a ` groupby ` with the ` apply ` function . The passed function in this case ` linregress ` . Please see below : #CODE
How do I iterate over each row and column in ` rectangle ` to run my function ` MakeBoolDictOfSearchTermsAndProducts() ` on it and fill in the correct element with the result ? Should I use apply ? or map ? or perhaps apply_map ?
You can use groupby and apply : #CODE
You could ` apply ` on dataframe and get ` argmax() ` of each row via ` axis=1 ` #CODE
Here's a benchmark to compare how slow ` apply ` method is to ` idxmax() ` for ` len ( df ) ~ 20K ` #CODE
Pandas DataFrame apply function doubling size of DataFrame
I am trying to create this boolean mask using the ` apply ` method , where ` df ` is a DataFrame with numeric data of size a * b , as follows . #CODE
Why is the ` apply ` method doubling the size of the DataFrame ? Unfortunately , the Pandas apply documentation does not offer helpful clues .
Ability to apply different stats to different columns ( for now just count , sum , mean , weighted mean )
` data.apply ( math.log10 )` did not work because ` apply ` tries to pass an entire column ( a Series ) of values to ` math.log10 ` . ` math.log10 ` expects a scalar value only .
And can I apply tight_layout to data.hist somehow ?
I'm not sure I can give you a great explanation for that warning beyond what's in the documentation , but it appears what you did works fine and that warning doesn't always apply even when it appears .
Call ` apply ` on ' B ' and pass a lambda which just accesses the single key in the dict : #CODE
Wow this works for all cases unless I use ` apply ` and a lambda function
Apply a weighted average function to a dataframe without grouping it , as if it was a single group
I want to apply a function that computes something similar to a weighted average absolute deviation of all the elements of my data frame .
If I don't use groupby , pandas would apply this function to every row of the dataframe , which is not my goal .
Okay , so in your post when you say " If I don't use groupby , pandas would apply this function to every row of the dataframe " , that's not necessarily true . You should try to read up on the way operations on ` numpy ` arrays are " vectorized " . So , like people have pointed out in the comments , your function works fine without having to do the groupby : #CODE
Use the older ` openpyxl ` engine to apply formats one cell at a time . This is the approach with which I've had the most success . But it means writing loops to apply formats cell-by-cell , remembering offsets , etc .
This is currently not possible in openpyxl . As you rightly point out applying formats to individual cells is extremely inefficient . This will hopefully improve in forthcoming releases when we add support for named styles you'll still have to apply these individually as resolving all the possible styles for an individual cell ( built-in , row , column , individual ) is an expensive operation which will be much less complex than it currently is .
A single-liner - you could extract numbers from via regex and ` apply ` on the ` duration ` column like split into multilines for readability #CODE
And , then apply on #CODE
What's REALLY confusing me is , when I try to step through the function ( not using apply ) with just one row , I get the DataFrames that I expect i.e. , not the Series and then Timestamp . Really appreciate any insight into what's going on !
Is there a way to have a counter variable in the function called through a pandas groupby apply ? #CODE
Note : this is an implementation detail , the number of times the function in an apply is called may depend on the return type / whether the apply takes the slow or fast path ...
Isn't the first call to apply the initialisation of the groups though , I thought I saw this as an explanation in a previous answer somewhere ...
Oops didn't see this answer was the first . Great I should have thought of using apply . I ended up having a few issues with integers and special character ( like spanish letters ) . The answer bellow solves my issue with integers but waiting for an answer on how to deal with special characters like ` u ' \xf3 '` .
We then apply another function to this that converts the str numbers to ints , puts these in a list and returns the smallest value : #CODE
Thanks ! this is an approach that I hadn't thought about and one that I'm likely to employ down the road . for age , I wanted the series [ 62 , 55 , 67 ] at the end , and the problem I'm having now is that I can't target just row2 when I apply split ( ' ') .
I can't pick the values from the list based on min and max because the expression format from which they came matters ( I think it's not very clear from the examples I gave for sake of simplicity ) . I want to apply df [ ' age '] =d f [ ' e0 '] [( df [ ' e0 '] .str .match ( pattern7 )= =1 )] .apply ( lambda x : str ( x ) .split ( ' ') [ 1 ]) to only rows for which df [ ' e0 '] .str .match ( pattern7 )= =1 ) so as to not overwrite what was already in the age column ...
If you make these a list you can apply loc ( which gets you the desired result ): #CODE
You could use ` apply ` like this : #CODE
Surprisingly , applying ` str ` seem to be taking longer than ` apply ` : #CODE
Alternatively , you could also use ` apply ` #CODE
The thing that I'm wanting to do is take the actual score value and apply a color map to it . So that worse scores are more blue and good scores are more red . Is there any way to do this within a radviz graph ? How would you input the score values into the equation ?
Summarizing Dataframes with ambiguous columns with apply function
The code works for almost all cases except for ` apply ` functions that count specific cases inside a column : #CODE
Is there a way to incorporate the apply function into the dictionary ` sumdict ` ?
As an alternative , you can create ` gene ` in pure python ( rather than using apply ): #CODE
Apply the function .
pandas - apply UTM function to dataframe columns
I'm working with this python package called UTM , which converts WGS84 coordinates to UTM and vice versa . I would like to apply this function to a pandas dataframe . The function works as follows : #CODE
You could use ` apply ` method over the columns like
You can do this by ` apply ` ing a ` rolling_sum ` after we ` groupby ` the Type . For example : #CODE
AttributeError : Cannot access attribute ' index ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method
Here's on approach to do it using one ` apply `
And , ` apply ` and store the result to ` df [[ ' hour ' , ' weekday ' , ' weeknum ']]` #CODE
Depending on the task that is performed by ` lambdafun ` , you may get some speedup by storing the result of ` apply ` in a new ` DataFrame ` and then joining with the original : #CODE
One way is to groupby and apply function to take list , and then convert to dict . #CODE
Pandas dataframe apply function
3 ) How to use apply function on the above dataframe ` temp ` ?
Then ` apply ` lambda function . #CODE
It's not the most elegant , but life is short , so I'd apply ` list ` to get the values and then ` pd.Series ` to expand them into columns : #CODE
Thanks . Is there a way I can apply this to all values in a column ?
Then I want to save that factorization and apply it to other ` DataFrame ` ( look input doesn't have c values in column A ):
As wroted in the question - I know about ` get_dummies ` - but this doesn't resolve my problem to apply the same mapping to the other series object . You don't need to use ` map ` on columns - ` get_dummies ` have an optional parameter ` prefix `
( Although this doesn't apply to the question , it may help someone searching later : If you're using Python 2.x , make sure to explicitly use pickle format 2 ; IIRC , NumPy is very bad at the default pickle format 0 . In Python 3.0 + , this isn't relevant , because the default format is at least 3 . )
Apply function with pandas dataframe - POS tagger computation time
I'm very confused on the apply function for pandas . I have a big dataframe where one column is a column of strings . I'm then using a function to count part-of-speech occurrences . I'm just not sure the way of setting up my apply statement or my function . #CODE
So basically I have a function similar to the above where I use a POS tagger on a column that outputs a single number ( number of nouns ) . I may possibly rewrite it to output multiple numbers for different parts of speech , but I can't wrap my head around ` apply ` .
I'm pretty sure I don't really have either part arranged correctly . For instance , I can run ` noun_count [ row ]` and get the correct value for any index but I can't figure out how to make it work with apply how I have it set up . Basically I don't know how to pass the row value to the function within the apply statement . #CODE
How do I apply a function designed for one number to an entire dataframe in pandas ?
Seems like there might be an easier way but this isn't too bad . Mostly the work is done by ` reindex ` but I had to loop with a groupby rather than apply directly due to the index not being unique . #CODE
What should I use if I want to update ` some_series ` in place from ` other_series ` , but also have the NA values apply ?
The reason I'm not sure if this is the best way is because ` apply ` tends to be slow . Something like #CODE
maybe show a timeit for apply vs using the str ops :)
Series ( and dictionaries ) can be used just like functions with map and apply : #CODE
Ok , from what I understand , the problem at its most simple is that you have a ` pd.Series ` of values ( i.e. ` a [ " key "]` , which let's just call ` keys `) , which correspond to the rows of a ` pd.DataFrame ` ( the df called ` b `) , such that ` set ( b [ " key "]) .issuperset ( set ( keys ))` . You then want to apply some function to each group of rows in ` b ` where the ` b [ " key "]` is one of the values in ` keys ` .
There are a few built in methods on the ` groupby ` object that are useful . For example , check out ` valid_rows.groupby ( " key ") .sum() ` or ` valid_rows.groupby ( " key ") .describe() ` . Under the covers , these are really similar uses of ` apply ` . The shape of the returned ` summary ` is determined by the applied function . The unique grouped-by values -- those of ` b [ " key "]` -- always constitute the index , but if the applied function returns a scalar , ` summary ` is a ` Series ` ; if the applied function returns a ` Series ` , then ` summary ` constituted of the return ` Series ` as rows ; if the applied function returns a ` DataFrame ` , then the result is a multiindex ` DataFrame ` . This is a core pattern in Pandas , and there's a whole , whole lot to explore here .
How do you check a condition of several pandas DataFrame.Series element-wise and apply the result to a new column ?
I have managed to do this , but it's slower than I would like ( takes 2 mins for a single 60mb file ; mostly in the apply part as seen below ) and I'm thinking that there must be a better way of doing it
There is no ` str ` accessor for datetimes and you can't do ` dates.astype ( str )` either , you can call ` apply ` and use ` datetime.strftime ` : #CODE
To apply the same condition to to dozens of columns I could use ` isin ` , but it seems not to work if I need to substitute `' first '` with a regex , as in ` regex = ' ( ? = . *first ) ( ? = . *second )'` .
Why don't we use ` applymap ` on the entire data frame . This will be different than working the columns but would make it easier for your to apply if-else conditions to ( I hope ): #CODE
The whole point of ` applymap ` is that you can apply a function on every cell of the data frame . So , I think your two drawbacks are covered if you expand the function further . Need help with that ?
But how do I apply this function on each element of a pandas data frame ?
Pass the ` hash ` function to ` apply ` on the ` str ` column : #CODE
I then try using apply to run it on a dataframe to create a new column . #CODE
For a start there is a built in ` str.split() ` which is vectorised so you could eliminate that from your code so ` df [ ' word_split '] = df [ ' string '] .str .split() ` and then call apply on this column and change your line in your func to this ` listoflists = st.tag ( x )`
You could iterate through them and apply the ` to_datetime ` function OR
It's generally expensive to do it this way , as you're losing the vector speed advantage when you ` apply ` a user defined function . Instead , how about using the numpy version of the ternary operator : #CODE
I have long list of date in dataframe that need to be converted into datetime , the date is in the form " %d%m%Y " , and I apply ` datetime.strptime ( x , ' %d%m%Y ')` , which works fine until meet the date " 3122012 " , which should be datetime.datetime ( 2012 , 12 , 3 , 0 , 0 ) , but instead it throw the error message : #CODE
This type of thing always feels somewhat clunky to me . Is there a preferred way to apply a function to only the nonnull rows of a column ?
Thank you for this , I had initially tried something like this but didn't realise that you only have to apply the filter on the RHS of the assignment and not the LHS .
Pandas : How to use apply to create new dataframe
and appy that function using apply to the created DataFrame using #CODE
Can you reconstruct this problem ? Did I get anything wrong regarding the use of the apply function ?
Because you're passing your ` data ` df as a reference and assigning directly to it each time by calling ` apply ` in your func then it overwrites with the last operation : #CODE
@USER : You'll have to explain what you mean . Each column has only one dtype . You can subset based on any criterion you can apply to each item in the column .
You can use ` df . column .map ` to apply a function to each element in a column : #CODE
Why don't you use ` apply ` and on a modified dictionary lookup : #CODE
And , apply it like this - #CODE
And use it with apply #CODE
Once this is created , you can create a function to split the categories column by the " , " and count the length of the resulting list . Use lambda and apply .
Assuming that Category is actually a list , you can use ` apply ` ( per @USER ' s suggestion ): #CODE
You can groupy the ' ITEM ' and ' CATEGORY ' columns and then call ` apply ` on the df groupby object and pass the function ` mode ` . We can then call ` reset_index ` and pass param ` drop=True ` so that the multi-index is not added back as a column as you already have those columns : #CODE
How to reference groupby index when using apply , transform , agg - Python Pandas ?
The question is that neither aggregate , apply , nor transform can reference to the index . Any idea how to work around this ?
After some tinkering around , I wrote a function that can be used with the ` apply ` method on a ` groupby ` . #CODE
You can use ` apply ` to extract the numerical values , and do the counting there : #CODE
If not already you need to convert to datetime , then you can call ` apply ` and use ` datetime.strftime ` to do the formatting : #CODE
You can use ` apply ` : #CODE
When I try to plot a histogram , I apply : #CODE
But indexing with your boolean can't be summarized in those few numbers . Either it has to carry the ` index ` array all the way through , or copy selected items from the ` x ` data buffer . ` numpy ` chooses to copy . You have choice of when to apply the ` index ` , now or further down the calling stack .
I also tried with apply and I think it works , but I need to reset the index , which is something I'd rather avoid ( I have a large dataset and I need to do this repeatedly ) #CODE
Trying to create a new column with the groupby calculation . In the code below , I get the correct calculated values for each date ( see group below ) but when I try to create a new column ( df [ ' Data4 ']) with it I get NaN . So I am trying to create a new column in the dataframe with the sum of ' Data3 ' for the all dates and apply that to each date row . For example , 2015-05-08 is in 2 rows ( total is 50+5 = 55 ) and in this new column I would like to have 55 in both of the rows . #CODE
Pandas groupby : apply vs agggregate with missing categories
I'm running into an issue where panda's ` GroupBy.apply ` and ` GroupBy.aggregate ` give different-shaped results when categorical data has missing values . ` aggregate ` retains all " known " categories , but ` apply ` only keeps the categories that are present in the data .
Note that the last data frame is missing the ` NaN ` rows where ` missing = b ` . I understand why ` apply ` might do this ( it chooses not to pass a group full of ` NaN ` s to the reduction function ) . The above snippet is just a toy example : I actually need to use ` apply ` to get the result I want .
Question : What's the best way to use ` apply ` but create an output shape matching the one returned by ` aggregate ` ?
Here's a fairly general solution you can apply to multiple columns . The ' To ' column doesn't need to be rounded , I just included it for the generality of two columns rather than one : #CODE
Use this ` func ` and apply over the the ` dff.groupby ( ' Group ')` #CODE
With the ` func ` function you wrote in your updated answer ( i.e. your solution ) , you should be able to use the ` DataFrame.apply ` method with parameter ` axis=1 ` . ( I haven't tested it , but perhaps you could try to apply it and report the error message , if any )
The key to this answer is that the ` apply ` method accepts arbitrary positional keyword arguments and passes them to the function .
You can use ` apply ` with option ` axis=1 ` . Then your solution is pretty concise . #CODE
How to apply Cython to Pandas DataFrame
After reading the whole Dataframe , I tried to apply function on one Serie : #CODE
While doing preprocessing of data , I first try to remove all the punctuations and also the most common stop words . After doing that , I want to apply the Porter Stemming algorithm which is readily available in nltk.stem .
You could ` apply ` and construct the datetime using your desired date values and then copying the time portion to the constructor : #CODE
Use the ` apply ` method . #CODE
With Pandas it's often a good idea to try and use ` apply ` together with an anonymous function to perform your calculation on every row . Does this work for you ? : #CODE
@USER The problem is that the ` ExcelWriter ` isn't created by ` pd.ExcelWriter() ` , but through ` xlsxwriter.Workbook() ` to just write some arbitrary ( non-pandas ) data . Unfortunately , the approach from your link doesn't apply here .
when you open csv file from excel , it will convert your data to any type it should be in this case your data converted to date type then excel apply default date format to that data . Also , you can't control the date format of excel file since csv files is only a text file , no meta or hidden data to advise excel to proceed .
Using ` apply `
You could use ` pandas `' s ` apply ` for this . #CODE
How can I now apply this test on a slice of the dataframe ? #CODE
Hi @USER ! Thanks for your help . I upvoted your answer because it was helping me getting the job done . Thanks a lot for it ! I am not sure whether it is really the answer to my question , though ( How to apply the test to the dataframe ) . Perhaps someone else can answer that question ?
One follow-up question : while this works great for " year " , I would like to do a similar thing for " yearmonth " , i.e. a combination of YYYYMM values ( so that each month of each year gets a specific label ) , As there is no attribute to directly extract " yearmonth " from index , what can I do in this case ? Could I define a lambda function and apply this to the index values , for example ?
How to avoid return twice the first groupby object after to apply it in other function ? In this case , I'm just printing , but python return the first group twice ( see ' word ' = ' a ') . This occur with other function more elaborated too . Why ? There are any solution for this ? I would continue to use DataFrame() + groupby() + apply() + def f() if possible .
You need to avoid for-loops and use the ' apply ' methods . See #URL
but when I try to write a function in pandas to apply this to every cell of a column , it either fails because of an attribute error or I get a warning that a value is trying to be set on a copy of a slice from a DataFrame #CODE
how can I apply this code to each element of a Series ?
The problem seems like you are trying to access and alter ` row [ ' text ']` and return the row itself when doing the apply function , when you do ` apply ` on a ` DataFrame ` , it's applying to each Series , so if changed to this should help : #CODE
Alternatively you might use ` lambda ` as below , and directly apply to only ` text ` column : #CODE
You can apply on ` df.groupby ( ' Sym ') [ ' close ']` using ` pd.rolling_max ( x , 2 )` instead #CODE
We do it with an apply on axis=1 :: #CODE
Well this works ` df.assign ( ts4= np.where ( df.a * df.b - df.c > 1 , ' XS ' , ' L '))` , the problem with expression ` ts4=lambda x : ' XS ' if x.a * x.b - x.c > 1 else ' L '` is that it wonly works in the apply because you're using ` axis=1 ` so you're comparing single scalar values , your expression isn't so it's not valid
What I am hoping to achieve , is to apply the logic of ` conditions ` to ` indicators ` in order to produce a new dataframe called ` signals ` . To give you an idea of what I'm looking for , see below . This looks only at the first condition in ` conditions ` and the fifth value in ` indicator ` ( because it evaluates to True ): #CODE
Just define a function that fits your needs then apply it . It can be quite complicated as well : #CODE
This would apply the desired operation to all the rows and is considerably faster
This gives ` AttributeError : ' list ' object has no attribute ' apply '`
Use ` apply ` and pass your func to it : #CODE
The correct way as EdChum pointed out is to use ` apply ` on the ' location ' column . You could compress that code in one line : #CODE
I then apply the following filter to ` ORD_ticks ` to get ` ORD_prices ` : #CODE
possible duplicate of [ Parallelize apply after pandas groupby ] ( #URL )
No it won't see my edit , when you call ` dropna() ` on a series ( which is what we're doing here when calling ` apply ` on a df ) it drops an entry in the series not an entire row
For the expanding product , there's ` cumprod() ` . For the rolling version , I think you'll have to use ` rolling_apply ` to apply ` prod() ` to each window .
Hello . Great : adopted to the " real " dataframe where I have to apply this loop , it worked out perfectly ! To groupby w.r.t the first groupby object in that way is something I should have figured out myself . Apologize for my ignorance ;-)
I normally use ` apply ` for this kind of thing ; it's basically the DataFrame version of map ( the axis parameter lets you decide whether to apply your function to rows or columns ): #CODE
To do that , you can use ` apply ` with ` axis=1 ` . However , instead of being called with three separate arguments ( one for each column ) your specified function will then be called with a single argument for each row , and that argument will be a Series containing the data for that row . You can either account for this in your function : #CODE
Note the double brackets . ( This doesn't really have anything to do with ` apply ` ; indexing with a list is the normal way to access multiple columns from a DataFrame . )
However , it's important to note that in many cases you don't need to use ` apply ` , because you can just use vectorized operations on the columns themselves . The ` combine ` function above can simply be called with the DataFrame columns themselves as the arguments : #CODE
As above , there are two basic ways to do this : a general but non-vectorized way using ` apply ` , and a faster vectorized way . Suppose you have a DataFrame like this : #CODE
You can define a function that returns a Series for each value , and then ` apply ` it to the column : #CODE
data = ascii.read ( table ) ( and apply the formatters )
Id assume you want to keep the mapping dictionaries around for later use , which you would loose with function calls .. But that depends on the overall purpose and extent of the code . You can use use ``` enumerate ``` to eliminate the need to have the ``` u_rows ``` and ``` u_cols ``` variables . Also , you can apply the mappings directly in the argument of ``` coo_matrix ``` to save space , but that is a bit messy .
The series produced by ` ( df [ " a "] | df [ " b "])` is of type ` bool ` . This surprised me because ` | ` is a bitwise operator , so I expected the series to be of type ` int ` . Thus , I have to do the ` apply ( lambda ... )` to get the desired ` int ` column .
I have been trying with a function and apply . here is my data set and code #CODE
An alternative would be to apply strip to the columns to ensure they don't have leading spaces : #CODE
Then apply ` maxminbid ` function on ` Auction_id ` grouped objects #CODE
And I apply the following group-by code #CODE
This is on a Windows 7 Enterprise Service Pack 1 machine and it seems to apply to every CSV file I create . In this particular case the binary from location 55 is 00101001 and location 54 is 01110011 , if that matters .
How to apply group by on data frame with neglecting NaN values in Pandas ?
You may also choose to use ` map ` instead of ` apply ` since upon accessing a column , you'll be working with a Series object .
works great thanks , but why would I use applymap and apply the function on all the df cells , there is different types of data and all other stuff ?
I expanded a bit in the middle to highlight why simply creating a new column of data makes more sense here than using ` join ` . On the other part , note that I'm not suggesting to use ` applymap ` which is a DataFrame method , rather to use plain ` map ` which is a Series method . ` Series.map ` is meant specifically for element-wise operations , whereas ` apply ` has some extra checking for functions that can vectorially operate on the whole ` .values ` data object in one go .
You can use any function within the ` apply ` . More examples here - #URL
df.applymap() dont apply .map() on each Series of the DataFrame , put map .apply() on each Series . See Series .apply() here : [ link ] ( #URL )
Thanks ! The example showing how map produces a series and apply produces a dataframe also explains some results I'd gotten in the past and not understood . My understanding of all this is still a bit less than 100% , but this helps .
I am trying to apply this related topic [ Merge pandas DataFrames based on irregular time intervals ] by adding start_time and end_time columns to df1 denoting 3 months ( start_time ) to 6 months ( end_time ) after DATADATE , then using np.searchsorted() , but this case is a bit trickier because I'd like to merge on a company-by-company basis .
Apply condition of opposite values ...
So anyone who wants to assign values in the column of one dataframe based on values from another . I used .ix [ ] to drill down to the value , then .apply() to apply a function across each row ( axis=1 ) finding the line's values just as you would a dataframe . ( ' line.element ' / line [ ' element '])
You could do a transpose of the df and then using ` apply ` call ` nunique ` row-wise : #CODE
You ` groupby ` first on the feature , and second on the Iteration variable . On each group you apply the ` mean() ` function , and you get the group whose index is ` 1 ` , which correspond to the ` Feature Active == 1 ` group .
So a span of 60 obviously wouldn't apply here , as Pandas would just interpret that as every 60 datapoints rather than every 60 seconds . Are there any solutions beyond the obvious ? The " obvious " being inserting datapoints for every second in the gaps , and extrapolating the values . I should note that the Date column is a proper Python datetime64 object .
You could add an ordinal variable for which group-of-60-seconds each row belongs to , then group by that ordinal value and apply the averaging function across the entire group , on a group-by-group basis . An easy way to do this would be to convert the datetime value to a number of seconds since the minimum datetime value , then do integer division by 60 .
Pandas : Get grouping-index in apply function
What are you actually trying to do with " mytest " , it looks like what you're looking for is actually a df.groupby and then an agg which can view the index , or a df.resample and then an apply which can also reference the index
I want to apply the mapping to df1 . The working version I have is this one , but I feel there is potential for improvement , as I find my solution unreadable and I am unsure about how it would generalize to multiindexes #CODE
My purpose is to apply two functions at the same time . Basically , I want to cut my dataset for extreme values by looking for the 5% quantile at the lowest part of the dataset and the top % at the other end . #CODE
So here is a mask that ought to solve the problem . Just ` interpolate ` and then apply the mask to reset appropriate values to NaN . Honestly , this was a bit more work than I realized it would be because I had to loop through each column but then groupby didn't quite work without me providing some dummy columns like ' ones ' .
Let's say you have a list of ` datetime ` objects and you want to group them by the ` .month ` attribute . So , First of all you need to sort them , then you can apply ` itertools.groupby() ` function which returns : a group value and an iterator . #CODE
This avoids apply . Link to docs
This is briefly mentioned in the docs on multi-indexing , although obviously that doesn't quite apply in your case , I'm not sure where to go for a good overview of how slicing works for sorted / unsorted indices .
How to apply multiple formats to one column with XlsxWriter
In the below code I apply number formatting to each of the columns in my excel sheet . However , I can't seem to figure out to apply multiple formattings to a specific column , either the centering or the numbering end up being over written . Is it even possible to apply two types of formatting to one column ? #CODE
Is it even possible to apply two types of formatting to one column ?
I then call ` apply ` on that list to turn it into a Series , this will auto generate the names of the columns 0 .. 4 .
Are there modules to do something like this hourly without human intervention ? I would read all of the files in a directory , append them into a single file , drop duplicates , apply some changes ( add some columns , perform some calculations on timestamps ) , and then save the consolidated new file in another directory .
I ended up doing it using a groupby apply as follows ( and coded to work forwards or backwards ): #CODE
You can use ` DataFrame.apply ` with ` axis=0 ` to apply a function to every column of a dataframe . In your case you want to check whether ` all ( col == 1 )` for each column .
I would suggest using ` all ` on the boolean condition on the entire df rather than use ` apply ` : #CODE
Although maxymoo's answer is correct generally one should avoid using ` apply ` if there is a method that is vectorised and can operate on the entire df which this does
But when I tried to do something like this on my ` DataFrameGroupBy ` object , it threw an error : ` Cannot access callable attribute ' astype ' of ' SeriesGroupBy ' objects , try using the ' apply ' method `
@USER I think he needs to unstack the `` industry `` column in order to apply `` rolling_corr `` on the result .
@USER if you apply df.corr() to the current structure , all you get is a correlation of all the columns , eg : correlation between mean and max . That is not what is required . I need to find the correlation between each industry . In other words the industries need to become columns ( along with the existing columns ) , and the rows will be dates . I haven't had any luck doing that .
Then I'd ` groupby ` df1 on name and apply a ` transform ` to calculate the total weight change for each person . ` transform ` returns a Series aligned to the orig df so you can add an aggregated column back to the df .
I have tried using groupby and apply in several different ways but I cant get it to work .
Though I was wondering if you could do it immediately using lambda , apply and groupby .
This function is mapped on every row of the ( Pandas ? ) dataframe ( actually , only on filtered columns `' humidity '` and `' workingday '`) and the result is stored in `' sticky '` column . That said , you can translate the same expression in R using an anonymous ` function ` and ` apply ` : #CODE
I have to say this is weird way to apply a function to a pandas df , anyway this is an example which shows what it does : #CODE
The lambda expression is calling ` apply ` and passing ` axis=1 ` which means row-wise and test each named column for whether the expression is True or False , the ` ( 0 , 1 )` casts this to an ` int ` , otherwise you'd get a boolean dtype returned . #CODE
I then apply this #CODE
and use ` groupby / apply ` : #CODE
As to why your method failed , you were calling ` apply ` on a Series ( ` df [ ' ID ']` is a Series and not a df ) and there is no ` axis ` param so the following works : #CODE
Working on the database of matches I would like to retrive the rating of both players and apply two functions ( i already have them defined ) predicted_result ( rating1 , rating2 ) , and updated_rating ( rating1 , rating2 ) . The first one gives me the expected result of the match given the ratings , the second one gives me the updated ratings . Finally I need to record the updated ratings in the player database .
It looks like it's binding the function object as the column value rather than unpacking it to a dict , what I'm doing above is to return the ` value_counts ` as a list and then call ` apply ` again to unpack the single element list . This forces the dict to be unpacked into a single element list in the initial ` apply ` call : #CODE
I'm trying to do apply simple functions to mostly numeric data in pandas . the data is a set of matrices indexed by time . I wanted to use hierarchical / multilevel indices to represent this and then use a split-apply-combine like operation to group the data , apply an operation , and summarize the result as a dataframe . I'd like the result of these operations to be dataframes and not Series objects .
1 ) ` groupby ` on ' id ' and call ` apply ` on the ' vehicle ' column and pass method ` nunique ` , you have to subtract 1 as you are looking for changes rather than just an overall unique count : #CODE
2 ) ` apply ` a lambda that tests whether the current vehicle does not equal the previous vehicle using ` shift ` , this is more semantically correct as this detects changes rather than just the overall unique count , calling ` sum ` on booleans will convert ` True ` and ` False ` to ` 1 ` and ` 0 ` respectively : #CODE
Are just for ilustrate the use of map function : ` l ` is a list containing the integer values 1 , 2 and 3 and ` ml ` is a list we have obtained as the result of apply the ` set_negative ` lambda to ` l ` . In others words we have applied a function to each element of the sequence ` l ` without using a ` for ` o ` while ` loop .
It seems like pandas either expects apply to return a scalar for each column , or a vector of the same length as the column . Is there a way to return vectors of different length to the original data ?
Another version that works the same as EdChum's answer , but splits within ` apply ` , instead of within ` np.percentile ` : #CODE
You can pass param ` axis=1 ` to ` apply ` so that it process each row rather than the entire column : #CODE
In the current implementation apply calls func twice on the first column / row to decide whether it can take a fast or slow code path .
In the current implementation apply calls func twice on the first group to decide whether it can take a fast or slow code path . This can lead to unexpected behavior if func has side-effects , as they will take effect twice for the first group .
In my opinion you should read the entire csv as a df , then ` apply ` your crawl method on column2 and create the new column and then write the df to your output : #CODE
using apply . #CODE
apply pandas qcut function to subgroups
Python Pandas DataFrame If Index Contains Any String Values , Apply Label , Else Apply Different Label
If the ' Search term ' ( index ) CONTAINS `' american brewing '` or `' americanbrewing '` , apply the label `' Brand '` , else apply `' Non-brand '` to a column with the header ` Label ` . #CODE
How do I output the ` Label ` column in the ` result ` ` dataframe ` based on if the ` Search term ` ( index ) contains any of several possible string values ? Where ` True ` , apply ` Brand ` , Else , apply ` Non-brand ` to the ` Label ` column .
You could convert the ` index ` to ` Series ` and apply transformations . #CODE
No matter which of the given alternatives I apply - it just doesn't work . Do you have any ideas ? #CODE
I can't figure out a way to do this in a single loop , the problem here is that you want some kind of rolling apply that can then look at the previous row , the problem here is that the previous row update will not be observable until the ` apply ` finishes so for instance the following works because we in run the apply 3 times . This isn't great IMO : #CODE
Hi Many thanks for all the answers , I have tried to apply the df.ix [: , : ' 4 '] .apply ( lambda x : x.isin ( df [ ' 1 '])) .all ( axis=1 ) .sum() to the last 4 columns of my example ( columns 2 , 3 , 4 and 5 ) by using df.ix [: , : ' 5 '] .apply ( lambda x : x.isin ( df [ ' 2 '])) .all ( axis=1 ) .sum() and have checked by hand ( with a dataset that provides me an answer greater than 0 , but keep getting a different result . Have I applied the logic correctly please ?
In a Python Pandas ` DataFrame ` , I'm trying to apply a specific label to a row if a ' Search terms ' column contains any possible strings from a joined , pipe-delimited list . How can I do conditional if , elif , else statements with Pandas ?
It doesn't look like ` str.contains ` supports multiple patterns , so you may just have to apply over the rows : #CODE
Apply a lambda function on the rows and test if A is in B . #CODE
If you're looking to perform some kind of timestamp calculation using your index you can call ` apply ` and access the index using the ` name ` attribute : #CODE
Is there a way to apply to a pandas dataframe while threading state ?
My goal is to be ably to do an apply , but one which maintains some state . Now , I know that I can just have a variable ( and be aware that apply is called twice , I believe ) , but I'm wondering if there is a more idiomatic way to do this ?
" maintains some state " <--- what does this mean and how does ` apply ` not currently achieve this ? post some more code showing what you're trying to accomplish ( and example output )
How to apply different aggregation functions to same column by using pandas Groupby
@USER I'd say that question is different , the OP here is asking how to apply multiple different functions at once , not to generate multiple columns from a single function
What I got so far is the code below and it works fine and brings the results it should : It fills ` df [ ' c ']` with the calculation ` previous c * b ` if there is no ` c ` given . The problem is that I have to apply this to a bigger data set ` len ( df.index ) = ca . 10.000 ` , so the function I have so far is inappropriate since I would have to write a couple of thousand times : ` df [ ' c '] = df.apply ( func , axis =1 )` . A ` while ` loop is no option in ` pandas ` for this size of dataset . Any ideas ? #CODE
@USER Cunningham : How would I apply this ?
But I'm a bit stumped at how to do this pandas . As far as I can tell , aggregate only applies a function on a given grouped column , and I don't know how to get it to apply a function that involves multiple columns .
This operation will only apply to groups that are larger that 2 in the original dataframe .
I tried an alternate solution which involved ` apply ` ing a ` lambda ` function to each element of the ` Series ` but that took longer .
Now get the distance from points to lines and only save the minimum distance for each point ( see below for a version with apply ) #CODE
( taken from a github issue ) Using ` apply ` is nicer and more consistent with how you'd do it in ` pandas ` : #CODE
2 ) How do I apply a function to a set of columns to remove SettingWithCopyWarning when reformatting DATA columns .
I know I can do it with a custom apply , but I'm wondering if anyone has any fun ideas ? ( Also this is slow when there are many groups . ) Here's one solution : #CODE
@USER all of this soln is completely vectorized or in cython . Using an apply ( even with a fast lambda ) will be orders of magnitude slower on any real dataset . As the apply is essentially a python loop .
The problem is , that I am getting this error form the last line of code , where I try to apply the function with ` df.apply ( flex_relative , axis =1 )`
The only thing I found so far was the link below , but calling a R function won't work for me because I need to apply that to quite big datasets and I may also implement an optimization in this function , so it definitely needs to be built in python . Here is the link anyway : Finance Lib with portfolio optimization method in python
The following should be more optimised , basically I'd ` groupby ` on the team , apply a boolean test of whether the difference in the datetime is equal to a timedelta of 1 day .
Then for where this is True then apply a ` cumsum ` on this and add 1 .
Background- I'm trying to extract unsynchronised dual-doppler measurements from a scanning ` LiDAR ` which is taking PPI scans . I have the data ( from MySQL ) loaded into pandas dataframes , and now need to apply some matching function where the rows are matched if the time of measurement is within some limit ( time 8s apart ) .
So it seems to have collapsed the groups , but I've now lost data ? Or how is the object now stored ? I realize I haven't done the apply stage , which is probably how I will generate new rows and new columns , but I don't know the next step or if there's a cookbook example for something like this .
One method would be to convert the time strings to datetime but only take the time portion and then call ` apply ` and call ` datetime.combine ` to produce your datetime for both columns : #CODE
Return multiple objects from an apply function in Pandas
I'm practicing with using ` apply ` with Pandas dataframes .
So , I'd like to use the 2nd dataframe , ` DFa ` , and get the dates from each row ( using apply ) , and then find and sum up any dates in the original dataframe , that came earlier : #CODE
Obviously I'm new to ` apply ` and I'm eager to get away from loops . I just don't understand how to return values from apply .
I don't think apply is best option for this . If I understand correctly why not DFa [ DF.index ] .sum() ?
I agree , it's a pretty lousy example . My main problem is trying to return from the apply . I would really like to see how I could return 3 different dataframes , and sum them up elsewhere ( but I didn't mention that in the question appropriately ) .
@USER ' Brien : The performance of DF.apply ( func , axis=1 ) is comparable to calling func in a loop . apply is useful when you want to align the output into a single DataFrame . If you need to return 3 disparate DataFrames , go ahead and loop over DF.iterrows() . For better performance you'll have to think of a better way to calculate the result ( such as doing a sorted cumsum for the toy example above ) or perhaps use Cython .
Also , note that ` apply ` returns a ` DataFrame ` . So your current function would return a ` DataFrame ` for each row in ` DFa ` , so you would end up with a ` DataFrame ` of ` DataFrames `
There's a bit of a mixup the way you're using ` apply ` . With ` axis=1 ` , ` foo ` will be applied to each row ( see the docs ) , and yet your code implies ( by the parameter name ) that its first parameter is a DataFrame .
Once you make the changes , as ` foo ` returns a scalar , then ` apply ` will return a series : #CODE
Is there a faster / more elegant way to accomplish this ? For example , is there a way to apply ` dateParser ` directly to the index ( perhaps inplace ) so I don't have to ` reset_index ` first ?
My current approach is to create an array of the indices where the markers occur , iterating over this array using the values to slice the dataframe , and then appending these slices to a list . I end up with a list of numpy arrays that I can then apply a function to : #CODE
Apply a value to a ` sold_at_same_place ` column base on the value in ` place ` : #CODE
apply custom function on pandas dataframe on a rolling window
You want to apply a risk calculation function ( let's say VaR ) named compute_var() on last 90 closing prices , on a rolling basis
Wait , are both functions meant to apply to individual strings instead of a whole row , or just ` perform_function1 ` ? Maybe it would help to include your functions ( if they're not too complicated ) .
The functions are kind of complicated . They're meant to apply to individual strings .
If you want to apply function to certain columns in a dataframe #CODE
How to apply a function to the elements of a pandas dataframe
I want to apply a lambda function to the elements of a dataframe , in the same way as np.sqrt returns a dataframe with the sqrt of each element . However pd.DataFrame.apply apply the function to an row or an column . Is there a similar comand that apply a lambda function on each element ?
This is really quite similar to what you are doing except that the loop is replaced by ` apply ` . The ` pd.Series ( x.values )` has an index which by default ranges over integers starting at ` 0 ` . The index values become the column names ( above ) . It doesn't matter that the various groups may have different lengths . The ` apply ` method aligns the various indices for you ( and fills missing values with ` NaN `) . What a convenience !
I'm trying to apply one function ` f1 ` to rows ` [ ' Utah , ' Texas ']` and ` f2 ` to other rows . I don't want to create separate DF for each function .
Apply function to column in pandas dataframe that takes two arguments
You can do this with the ` map ` method without writing a function or using ` apply ` at all : #CODE
then the syntax would be ` df.apply ( func , axis = 1 )` to apply the function func to each row .
Apply function to each row of pandas dataframe to create two new columns
Because NaT is technically a datetime this condition wasn't covered by that function . Since isnull will handle this , I wrote this function to apply to data [ col_name ]: #CODE
pandas find max value in groupby and apply function
How may I set my maximum H value ( 4 for Dublin and 5 for Madrid ) as a constant / city in order to apply the function all over the DataFrame ? The expected df would appear as : #CODE
Many thanks for your answer :) Just wondering what the sub_df.columns = range ( 12 ) does ? Does it just rename the columns 1 to 12 or apply some sort of indexing ?
How to apply functions with multiple arguments on Pandas selected columns data frame
What I want to do is to apply a function : #CODE
The ` DataFrame.apply ` method takes a parameter ` axis ` which when set to 1 sends the whole row into the apply function . This makes it a lot slower than a normal apply function since it is no longer a proper monoid lambda function . But it does work .
The error message is telling you that you cannot cast a pandas Series to a ` float ` , whilst you could call ` apply ` to call your method row-wise . You should look at rewriting your method so that it can work on the entire ` Series ` , this will be vectorised and be much faster than calling ` apply ` which is essentially a ` for ` loop .
I guess I will have to ` apply ` or map a ` split ( " , ")` to the ` Term ` column , but what do I do after that ? #CODE
You can use ` str.split ` to do the splitting ( instead of apply and split approach , but similar ): #CODE
To filter out some rows , we need the ' filter ' function instead of ' apply ' . #CODE
in excel what I'm trying to do would be " =IF ( AND ( A2=0 , B1=-1 ) , -1 , A2 ) so that I could then drag down column ' B ' and that would apply . In essence , based on the prior data point of column B , and the current value of column A , I need to update the current value of B .
I have to do that for 15 columns though . I have read that lambda is a ' throwaway ' function . Is it better to define a function ( to split and then to turn the values into minutes ) and apply it to each column instead ? Would I loop through the columns ( not all columns , but 15 of about 30 ) ?
I am hoping someone will provide a function I can use to apply / map everything to several columns at once efficiently . I am not too familiar with def and return functions ( I learned basic pandas before I learned basic python , just for practical purposes .. slowly learning though ) . Need to turn those timestamps into minutes
You could just measure this but generally ` apply ` should be the last resort as it doesn't scale as well as its a for loop and if called on a series it executes per row . With respect to turning it into a function so you can apply to 15 columns you put your code for you last method into a function and then call ` apply ` on a df , this will call it for each column but it will try to execute the function on the whole Series
Also , what about .map vs .apply ? And is it better to have a separate line of code for each column I am doing this to , or is it better to define a function and apply that to each column ? It looks a bit messy to have this split lambda function 15 times in a row ( 15 timestamp columns to be converted to minutes )
just wrap it up in a small function , and either apply it across the columns , or since you prob don't have too many of these , just do it per column .
Note : executing the cubic spline interpolation via the apply function takes quite a mount of time ( about 2 minutes in my PC ) . It interpolates from about 100 points to 300 points , row by row ( 2638 in total ) . #CODE
I'm trying to apply a weighted filter on data rather the use raw data before calculating stats , mu , std and covar . But the results clearly need adjusting . #CODE
pd.DataFrame.groupby.apply really gives us a lot of flexibility ( unlike agg / filter / transform , it allows you to reshape each subgroup to any shape , in your case , from 538 x 122 to N_categories x 122 ) . But it indeed comes with a cost : apply your flexible function one-by-one and lacks of vectorization .
But I'm not sure how to proceed . How do I apply the date subtraction operation , then combine ?
You can use apply like this : #CODE
FWIW , using ` transform ` can often be simpler ( and usually faster ) than ` apply ` . ` transform ` takes the results of a groupby operation and broadcasts it up to the original index : #CODE
@USER - I referenced the post you suggest as duplicative , and tried to apply the .map() function it recommends . My results from doing so are listed above under " My best effort so far " . I realize I must be missing something ; can you help my understand how to use the .map() function in this example ?
The following snippet should work after your ` apply ( crawl )` . #CODE
From what I understand I am following the steps to apply PCA as they should be . But my results are not similar with the ones in the tutorial ( or maybe they are and I can't interpret them right ? ) . With n_components=4 I obtain the following graph n_components4 . I am probably missing something somewhere , I've also added the code I have so far .
If you have too many levels for this to work , or you want to consider the individual words in ` catB ` as well as the bigrams , you could apply your ` CountVectorizer ` separately to each column , and then use and use ` hstack ` to concatenate the resulting output matrices : #CODE
Using ` apply ` check if value is list ` isinstance ( x , list )` and take the value , and then ` apply ( pd.Series , 1 )` to split as columns #CODE
I would like to change the value of a ` Pandas ` DataFrame based on index and column . I am getting an error ` A value is trying to be set on a copy of a slice from a DataFrame ` . I searched around and found similar questions / answers , but none that I was able to apply . #CODE
You can apply a lambda function to the column of the data frame , extracting the date from the dictionary via ` x [ ' $date ']` , and then just take the date / time portion ( ignoring the time offset ) . As this is a ' datetime naive ' object , Python wouldn't know what to do with any timezone adjustment . Use this stripped date / time string ( e.g. ' 2014-11-04T17 : 27:50 .000 ') as the input to ` strptime ` . #CODE
apply function throws an error = DataFrame ' object has no attribute ' datetime ' Checked Pandas version , its Also dt.datetime should be df.datetime ( just a typo ) right ?
What the ` apply ` function does , is that for each row value of ` df [ ' A ']` , it calls the ` applyFunc ` function with the parameter as the value of that row , and the returned value is put into the same row for ` df [ ' B ']` , what really happens behind the scene is a bit different though , the value is not directly put into ` df [ ' B ']` but rather a new ` Series ` is created and at the end , the new Series is assigned to ` df [ ' B ']` .
You can define an explicit function to apply to the entire ` Name ` Series . #CODE
@USER Shouldn't I apply ` split ` as : ` df [ ' Description '] = df [ ' Name '] .map ( split )` to run it ? Or similar
Applymap is for the whole df , is you ; re doing just a single column ( series ) , then just use apply .
Sorry you're trying to apply to a groupby object ? Does it work after calling ` reset_index() ` ? really you should post a new question as chaning the requirements loses context for the changes .
Interested to apply multivariate hexagonal binning to this and different color hexagoan for each unique column " ball , mouse ... etc " . scikit offers hexagoanal binning but cant figure out how to render different colors for each hexagon based on the unique data point . Any other visualization technique would also help in this .
interested to apply hexagonal binning to this #URL
Converting multiple columns to categories in Pandas . apply ?
Why does ` apply ` ( ` axis=0 `) return a Series even though it is supposed to act on the columns one by one ?
I'd suggest that you use ` groupby ` / ` apply ` returning a ` Series ` ( see Returning Multiple Values From Apply ) .
I think the way you did it is probably best . I'm not sure ' vectorized ' has any real meaning in your situation . To the extent where you would gain speed , I think you'd have to already have a square dataframe with rows and column in the same order . Given that neither of those conditions apply here , I don't see a reason to change what you have .
You could also do it with apply , but it will be slower than the ` np.where ` approach ( but approximately the same speed as what you are currently doing ) , though much simpler . That's probably a good example of why you should always avoid ` apply ` if possible , when you care about speed . #CODE
You could also do this , which is faster than ` apply ` but slower than ` np.where ` : #CODE
Pandas Apply function on Column
You don't have to call ` grouped [ ' B ']` ` grouped [ ' C ']` one by one , simply pass your entire groupby object and pandas will apply the aggregate functions to all columns . #CODE
This is a little messed up , firstly split the ' codes ' column on the separator and ` apply ` ` map ` to each element and pass the other df with the index set to ' code ' , this will perform a lookup , we reduce this to just the codes that match .
Is this really what you want to do ? Ideally you want to avoid using ` apply ` if there is a vectorised method , so just ` df [ ' x '] + df [ ' y ']` would work
You can apply a function row-wise by setting ` axis=1 ` #CODE
` groupby / apply ` to a columns of ` df ` unless the columns or levels you group by
Select your columns from the DataFrame and then apply your function ( possibly a ` lambda ` expression depending on usage ) . #CODE
If you have pandas version > = ` 0.17.0 ` you could try to apply ` pandas.to_numeric ` for each column ( or may be you know suspicious columns ): #CODE
We could see that it's slower then convert_object . Let's pass ` raw=True ` for ` apply ` : #CODE
` apply ` might work well for you here : #CODE
I have a list of servers , and each server has a number of patches that apply to that server . The excel looks a bit like this : #CODE
I realise there have been a lot of similar questions on here , but I am new to pandas and can't seem to apply the right syntax to my problem using the documentation - can anyone help ?
I'm trying to figure out how to apply a lambda function to multiple dataframes simultaneously , without first merging the data frames together . I am working with large data sets ( > 60MM records ) and I need to be extra careful with memory management .
My hope is that there is a way to apply lambda to just the underlying dataframes so that I can avoid the cost of stitching them together first , and then dropping that intermediary dataframe from memory before I move on to the next step in the process .
However , it is slower since the ` groupby / apply ` is doing an addition and division once for each group , whereas #CODE
However , it is slower since the ` groupby / apply ` is doing an addition and division once for each group , whereas #CODE
So far I can think of ` apply ` followed by ` itertools.chain ` , but I am wondering if there is a one-step solution .
Group by uid and apply ` value_counts ` to the msg column : #CODE
Apply ` groupby ` on both ` id ` and ` msg ` , and then sum the ` count ` of each : #CODE
How did you check if anything was written ? The ` if os.path.isfile() ` is not going to cause much overhead and is easy to apply the logic
I am not sure what I am doing wrong here , I am simply trying to call a function with a if-then-else filter in it and apply to a dataframe . #CODE
You want to apply it to each row , this will do what you want using apply and a lambda : #CODE
` df.apply ` has performance comparable to a Python ` for-loop ` . Sometimes using apply or a for-loop to compute row-by-row is unavoidable , but in this case a quicker alternative would be express the calculation as one done on whole columns .
thanks for the added color . . . .I just checked , in my application , this is 5.3 times faster than my " def " solution and 2.8 times faster than the df , apply approach . Thank you
Apply curve_fit within a loop
Now , I would like to apply a fit to each frequency group ( 400 , 800 and 1200 ) and do this efficiently within a loop . The first attempt is : #CODE
I hope , that there is someone , who could explain , if it is possible to apply the curv_fit routine within a loop and if so - how .
You should be able to do this using ` apply ` which is for applying a function to every row or every column of a data frame . #CODE
I am completely new to python and Pandas of course . I am trying to run a function " get url " which is function to get the complete extended url from small Url . I have a data frame in python consists all the short URLs . Now I am trying to do with following ways . One is to use " for " loop which loops and apply function on all the elements and will create a another series of extended URL but I am not able to , dont know why , I tried to write it like #CODE
If the short urls are a column in the pandas dataFrame , you can use the ` apply ` function ( though I am not sure if they would resume on error , most probably not ) .
Hi Anand - Thanks for help . but it is working same as apply function . as soon as it faces a error it pass completely . error is like HTTPError : HTTP Error 404 : Not Found ....
@USER " Misses the point " is a little harsh ! ;-) I guess if you want me to explicitly state it , I don't believe it is possible to do what you want without making a copy . Btw you might mean * set * rather than * select * in your question title ? Anyway , you're trying to apply a numpy concept to pandas and it doesn't work like that . My edit was merely an attempt to find something faster . I'll edit the answer to make this more obvious .
The following works but this really is a loop as it uses ` apply ` : #CODE
That's correct ` apply ` will not scale well , one thing you could do is assuming that all values are not unique you could create a dict of the unique values as keys and the row position values as the values and merge this , this should perform slightly better , your issue here is that you are trying to compare a scalar against the entire df for every row value , there isn't a built in method for doing this so you have to call ` apply ` here
@USER is there code not listed above that's altering the data frame ? For example , how are you apply a Month ` Series ` to the table ?
I think you're asking how to apply a function to all columns of a Data Frame : To do this call the ` apply ` method of your dataframe : #CODE
Why not try ` np.where ` . It's column-wise vectorized operation and it is much faster than row-wise apply . #CODE
I've created a function that looks for rows that have a set of column values that are the same . It's ugly though : nested apply functions . Is there a better way to test if ` n ` number columns have the same value , and if so apply a function to them or add them to a dictionary ? #CODE
Ah , yes , that was a bug in 0.15 preventing you to use such an instantiated class ( which is the case when providing a keyword ) , see #URL You need 0.16 for that fix ( or you can apply it yourself as it is only a 2 line fix , see the linked PR )
An easy workaround would be to apply ` .values ` to the series first and then apply ` std ` to these values ; in this case ` numpy's ` ` std ` is used : #CODE
By default , when you ` groupby ` - ` sum ` a DataFrame , pandas doesn't assume that you want to do so for all the columns that are not of the classic numeric types . If you'd have a column of strings , it wouldn't try to apply the sum to them too .
Thanks ! Was the error with my code because datetime.date works on a single row , and with functions working on rows one at a time I hvae to do apply ( fun() ) rather than fun ( apply() ) ?
Here is another alternative that might appear simpler ( though learning to apply functions to groups is a great idea ! ) #CODE
Here is one way to do it by defining your own rolling apply function . #CODE
Here is one way of approaching it , using the apply method to subtract the first item from all the other obs . #CODE
Use ` groupby / apply ` to sort each group individually , and pick off just the top three rows : #CODE
At a high-level , this indicates that we would like to look at each country differently . Now our goal is to determine the top 3 metric counts and report the corresponding channel . To do this , we will apply a sort to the resulting data-frame and then only return the top 3 results . We can do this by defining a sort function that returns only the top 3 results and use the apply function in pandas . This indicates to panda that " I want to apply this sort function to each of our groups and return the top 3 results for each group " .
you want to access the index inside the ` apply ` function , and I don't think you can
and then use the apply function , #CODE
You need ` apply ( your_func , axis=1 )` to work on a row-by-row basis . #CODE
This could be done in 2 steps , generate a new column that creates the expanded str values , then ` groupby ` on ' A ' and ` apply ` ` list ` to this new column : #CODE
@USER sorry what do you mean , do you mean the fact I added an intermediate column ? The intermediate step is necessary as I can't figure out how to get the ` apply ` and ` lambda ` to not try to expand the list and remain 1-dimensional
Another way to do it . First reshape the ` df ` using ` pivot_table ` and then ` apply ` ` np.repeat() .tolist() ` . #CODE
You can ` apply ` ` round ` : #CODE
If you want to apply to a specific number of places : #CODE
You can ` groupby ` on ' Symbol ' and then call ` apply ` passing a lambda and use ` shift ` : #CODE
To calculate the new ' Skew ' column , you can do a ` groupby ` and define your customized ` apply ` function . To calculate pct_change , you can use the ` .shift() ` operator . #CODE
I am using scikit learning's StandardScaler() and notice that after I apply a transform ( xtrain ) or fit_transform ( xtrain ) , it also changes my xtrain dataframe . Is this supposed to happen ? How can I avoid the StandardScaler from changing my dataframe ? ( I have tried using copy=False ) #CODE
How to apply line attribute at each group ?
` 1 / 2 ID ` is the column head that need to apply UPPERCASE .
How can I apply upper case to the first three letters in the column of the DataFrame ` df ` ?
@USER It's due to the current implementation of pandas ` groupby ` . See this Warning Message : ` Warning In the current implementation apply calls func twice on the first group to decide whether it can take a fast or slow code path . This can lead to unexpected behavior if func has side-effects , as they will take effect twice for the first group . ` from #URL
` df [ i ]` gets each column . ` apply ( Series )` applys ` Series ` function to the column , which creates ` Series ` from the inner ` dict ` . ` join ` append created ` Series ` to the end of column , and ` drop ` deletes the original column .
Python apply a func to two lists of lists , store the result in a Dataframe
I am trying to do a dataframe transformation that I cannot solve . I have tried multiple approaches from stackoverflow and the pandas documentation : apply , apply ( lambda : ... ) , pivots , and joins . Too many attempts to list here , but not sure which approach is the best or if maybe I tried the right approach with the wrong syntax .
I came pretty close with the melt() function , and then taking the former column numbers and added the offset to them . However , I've had a lot of problems trying to reform the dataframe using pivot . No luck with apply or apply ( lambda ) !
You can use ` groupby ` on column ' ts ' , and then ` apply ` using ` .any() ` to determine whether any of ` val ` is ` True ` in the cluster / group . #CODE
@USER Yes . We can just first divide the full dataset using ` groupby ` , and then apply the above procedure within each group . I've updated the code . The ` NaN ` at the bottom is because the 2nd group just 29 rows whereas 1st group has 71 rows .
This is a general question about how to apply a function efficiently in pandas . I often encounter situations where I need to apply a function to a ` pd.Series ` and it would be faster to apply the function only to unique values .
But for large data sets , this can take a while . So to speed it up , I'll extract the unique values of ` date ` , apply the function to those , and then merge it back in to the original data : #CODE
And , would it make sense and be feasible to add a feature to pandas that would take this unique / apply / merge approach automatically ? ( It wouldn't work for certain functions , such as those that rely on rolling data , so presumably the user would have to explicitly request this behavior . )
@USER Doesn't the ` apply ` need to be ` transform ` if you are going to be back to your full size data and not a collapsed version ? I.e. don't you still need to merge here ? Also , when I time this , it's actually slower than either of the original methods and I thought the point of this whole thing was speed ? ( Perhaps my timing was off though , you can check yourself , of course ) But to be clear , I do think the ` groupby ` with ` transform ` is probably the most clear and readable approach . It's just not any sort of speed improvement that I can see .
How to apply a function on a Series
I would like a apply a function to rename the values . #CODE
Your error occurred because you called apply on a single column df : #CODE
So this is different to a Series where ` apply ` iterates over each value which can be hashed but here it's passing the entire ` Series ` which cannot be hashed , it would work if you did this : #CODE
We can generated the edge list using ` groupby / apply ` and
Basically , I want to apply the same transformation to a huge data set I'm working on now , but I'm getting an error message : #CODE
Why do I get an object type when apply group by with size instead of an integer type ?
Since you want to retrieve ` category ` column as well , a standard ` .agg ` on column ` val ` won't give you what you want . ( also , since there are two values in author3 are 7 , the approach by @USER Cunningham using ` .max() ` will only return one instance instead of both ) You can define a customized ` apply ` function to accomplish your task . #CODE
python dask DataFrame , support for ( trivially parallelizable ) row apply ?
Edit : Thanks @USER for the map function . It seems to be slower than plain pandas apply . Is this related to pandas GIL releasing issue or am I doing it wrong ? #CODE
You can apply your function to all of the partitions of your dataframe with the ` map_partitions ` function . #CODE
Note that func will be given only part of the dataset at a time , not the entire dataset like with ` pandas apply ` ( which presumably you wouldn't want if you want to do parallelism . )
But avoid ` apply `
However , you should really avoid ` apply ` with custom Python functions , both in Pandas and in Dask . This is often a source of poor performance . It could be that if you find a way to do your operation in a vectorized manner then it could be that your Pandas code will be 100x faster and you won't need dask.dataframe at all .
Thanks ! I tried the map method and it seems to be slower than pandas apply . Could you comment on the edit of original post please ?
@USER Slightly off topic regarding pandas ; i try to use map over apply because I've heard it's faster , but I'm not sure why it's faster . Any clarification or links to clarification would be greatly appreciated .
Speeding up Pandas apply function
For a relatively big Pandas DataFrame ( a few 100k rows ) , I'd like to create a series that is a result of an apply function . The problem is that the function is not very fast and I was hoping that it can be sped up somehow . #CODE
No , not for this particular problem . But I think the main problem is the number of calls to the apply function , so ` cython ` , ` numba ` , ` numexpr ` , etc . won't help much to alleviate this .
The trick is not to use ` apply ` , but to do smart selections . #CODE
We have to apply the ` map ( func )` to the series in the dataframe : #CODE
DataFrame.applymap ( func ): Apply a function to a DataFrame that is intended to operate elementwise , i.e. like doing map ( func , series ) for each series in the DataFrame
You can convert ` datetime64 ` into whatever string format you like using the ` strftime ` method . In your case you would apply it like this : #CODE
@USER Yes , it's possible . You just need to first groupby ` Id ` and then move all the processing into an apply function ` my_func ` . I've updated the code . Please have a look .
The ` groupby / apply ` above returns a ` pd.Series ` . To make this a DataFrame , we can make the index level values into columns by calling ` reset_index() ` , and then assign column names to the columns : #CODE
apply function to a DataFrame GroupBy and return fewer columns
I want to group my ` DataFrame ` then apply a function of several columns which returns a single result . #CODE
This can be fixed using ` apply ` rather than ` agg ` , as ` apply ` has no constraint on the returned shape . #CODE
I put a condition for one column in pandas dataframe , hopefully it works . But it doesn't work when I apply it for all columns .
Yes that works when we are looking for any in the set that is of value 1 , 2 . But , we are looking for all of the records B = 1 and B = 2 where A = A . In our set we sometimes could have this : B = 2 but no B = 1 . We thus want to exclude these . As such the isin() doesn't seem to apply ?
You can use ` .max() ` and apply it row-wise by specifying ` axis=1 ` #CODE
However I want to apply the function to each row in the df and make a new column . I've tried the following #CODE
You can call ` apply ` and pass the function you want to execute on every row like the following : #CODE
Or do it in a one liner by calling ` apply ` twice : #CODE
If you want to keep all your rows , ` group_by ` the city / state combination , apply geocoding to it the first one by calling ` head ( 1 )` , then duplicate to the remainder rows .
Thanks for this response . Incredibly useful information ! Although when I did look at [: 5 ] rows of data I received a good dataframe . When I applied the function to all ( 200,000 records ) I received a time out error . I will have to groupby and then apply . Thank you very much .
I guess it depends on the number of levels for small number of levels this will be faster than calling apply but code wise it doesn't scale to increasing levels well
You can use apply for that : #CODE
Yes , that is what apply does . df [ df [ ' Q2 '] .apply ( your_custom_function )] . You pass a function into the ** column ** of the data frame and get rows for matching criteria .
Since you only want to change the first element of each group , you can do a customized groupby apply function to do this . #CODE
After this , I will apply the resample method , and then use the reset_index() method to get back a DataFrame that looks more or less what I have before : #CODE
I have a function which I apply it on the rows of a dataframe . This function returns a list of variable length depending on a parameter .
Another possibility could be to create dataframes with only the repeated columns , apply either sum or max and then merge everything .
Edit : Prior version used apply / lambda which is not really necessary . This is a simpler version .
Why does order of comparison matter for this apply / lambda inequality ?
Assuming your logic for function ` week ( rawdate )` is correct , You can use the ` series.apply ` function to apply a function to all values in a series ( a pandas column ) , and this returns a new series , which you can assign to your new pandas column .
anyway try the ` apply ` function i specified above .
With that as a template for the desired output , we can just loop over the relevant columns of the original dataframe and use ` groupby / apply ` with ` join / unique ` to replace the values in the existing columns : #CODE
Hi Thanks for the solution . I tried to apply this on my data set . However it gives an error " T #URL item 0 : expected string , int found " .Upon close inspection .. my data set has dates and integers .. so i converted the data frame to strings using applymap ( str ) .. that is still not helping .. how can i concatenate the values ?
Where row is the dataframe ` row ` . I am assuming your ` start ` and ` end ` columns are ` datetime ` objects . Then you can use ` DataFrame.apply() ` function to apply it to each row . #CODE
Had to use map instead of apply because of pandas ' timedelda64 , which doesn't allow a simple addition to a datetime object .
Looking at the source code , ` dropna ` appears to only only apply to the values that get assign to ` hue ` . For now , simply drop the NAs yourself via the appropriate pandas methods .
There might also be a way to do this with a loop or ` apply ` , can't quite think how though .
So I am wondering whether there is a way to accelerate this process ? maybe some vectorized functions on datetime object that I was not aware of ? I think one way to slightly improve the speed is to use ` multiprocessing ` module , and maybe I could expect 4-6 time faster on a 8-core PC . Also , because I invoke python function in the ` apply ` , cython or jit does not help in this case ?
Here is one approach where you ` apply ` a function row by row to generate the two wanted columns : #CODE
yes . as stated in the question its a silly example . but its more that dplyr offers me elegant code while also allowing for a flexible syntax . i was wondering how to make this exact code work in python . its more about how to split apply and combine in a similar fashion . not so much that the end result has any meaning .
Apply multiple functions to multiple groupby columns
I have a data set which is actually a occurrence matrix of a feature vector for some numbers of items . In theory , this type of representation helps to apply machine learning algorithms to data set as its normalized . #CODE
How could I apply decision tree algorithms such as CART and Naive Bayes for this type of data sets ? ( I only checked scikit learning library )
Hi ! Jianxun ! Thanks for answering the question ! But I cannot apply this to my code ... I posted another question .. It would be great if you could please check it ..
This you should do a groupby on option and apply a sum and retrieve the count ...
For you you just need the function and to apply it to your dataframe
However , no matter how I banged my head , I could not apply ` .str .contains() ` to the object returned by ` df.columns ` - which is an ` Index ` - nor the one returned by ` df.columns.values ` - which is an ` ndarray ` . This works fine for what is returned by the " slicing " operation ` df [ column_name ]` , i.e. a ` Series ` , though .
( one could apply any of the ` str ` functions , of course )
Of course in the first solution I could have performed the same kind of regex checking , because I can apply it to the ` str ` data type returned by the iteration .
EDIT : I just found the ` Index ` method ` Index.to_series() ` , which returns - ehm - a ` Series ` to which I could apply ` .str .contains ( ' whatever ')` .
` test.max ( columns=1 )` finds the max in each column ( like R's ` apply ( test , 2 , max )`)
I'm trying to replace some NaN values in my data with an empty list [ ] . However the list is represented as a str and doesn't allow me to properly apply the len() function . is there anyway to replace a NaN value with an actual empty list in pandas ? #CODE
You have to do this using ` apply ` in order for the list object to not be interpreted as an array to assign back to the df which will try to align the shape back to the original series
But then I try to apply it to DataFrame obtained from a .csv file : #CODE
how to change the following example to apply for a column ? say , for any cell in A1 : A15 , Ai , if the cell in column C in the same row , Ci , is less than the cell in A , we put red color in the cell Ai ? I can do a loop but it seems slow for a big file , maybe due to too many I / O in loop #CODE
Ideally I would like something like ` apply_chunk() ` which is the same as apply but only works on a piece of the dataframe . I thought ` dask ` might be an option for this , but ` dask ` dataframes seemed to have other issues when I used them . This has to be a common problem though , is there a design pattern I should be using for adding columns to large pandas dataframes ?
whats about using the apply method ?
However , if your operation is highly custom ( as it appears to be ) and if Python iterators are fast enough ( as they seem to be ) then you might just want to stick with that . Anytime you find yourself using ` apply ` or ` iloc ` in a loop it's likely that Pandas is operating much slower than is optimal .
You need to modify the apply ` func ` as below to count consecutive non-zero values . #CODE
I'm trying to export ` pandas.DataFrame.describe() ` to ` LaTex ` using the ` to_latex() ` -method . This works all fine as long as I don't apply the ` groupby() ` -method beforehand . With a grouped DataFrame , the first row has no values , even though its label is ` count ` . Note that the first row of a grouped dataframe is used to mark down the variable used for grouping in iPython notebook .
You can define a func which takes the values , sorts them , slices the top 2 values ( ` [: 2 ]`) then calculates the difference and returns the second value ( as the first value is ` NaN `) . You ` apply ` this and pass arg ` axis=1 ` to apply row-wise : #CODE
This can go wrong depending on the column names . It's safer to use ` .diff() .iloc [ 1 ]` . Also , this is pretty inefficient as it uses sort and apply , neither of which are necessary . See my answer below .
Here's an elegant solution that doesn't involve sorting or defining any functions . It's also fully vectorized as it avoid use of the ` apply ` method . #CODE
Unfortunately offsets don't support operations using array like objects so you have to ` apply ` the offset for each element : #CODE
You can ` groupby ` on ' name ' and ' id ' and just ` apply ` ` len ` function : #CODE
Calling the DataFrame's ` any ` method will perform better than using ` apply ` to call Python's builtin ` any ` function once per row .
You can ` apply ` the function ` any ` along the rows of ` df ` by using ` axis=1 ` . In this case I'll only apply ` any ` to a subset of the columns : #CODE
For each station that I'm working with . I want to be able to apply these using a dictionary like so : #CODE
Looks like a nice usecase for a multi column apply . Just write a function with your mapping dict . Apply this function to slice of your columns . Finish . #URL
` df2 [ ' Date ']` first , and then apply ` pd.merge ` to sub-DataFrames of ` df1 ` and ` df2 ` which contain only those dates : #CODE
Call ` apply ` on the ' scores ' column on the ` groupby ` object and use the vectorise ` str ` method ` contains ` , use this to filter the ` group ` and call ` count ` : #CODE
Outer join will still apply , but the original answer is correct : #CODE
` pd.DataFrame.sort() ` takes a colum object as argument , which does not apply here , so how can I achieve this ?
You could use groupby / apply with a custom ( lambda ) function : #CODE
You can define a list of the cols of interest and pass this to the groupby which will operate on each of these cols via a lambda and ` apply ` : #CODE
We can do this for each row of ` dates ` by using ` apply ` : #CODE
By making ` lambda ` , above , return a Series , ` apply ` will return a DataFrame ,
How can I accomplish this in pandas so that the dataframe I get contains the statistics of each flow i.e. the columns should contain the ` ip_src ` , ` ip_dst ` , ` sport ` , ` dport ` , ` ip_proto ` , ` service ` , and the mean var values calculated as earlier . I have tried both the ` aggr ` and ` apply ` methods , but haven't been able to do it . Thanks in advance !
Just apply a lambda function on the groups like so : #CODE
You can apply a function that returns the category : #CODE
You want to avoid iterating , if possible , and instead find a function to apply , such as this : #CODE
@USER : I like this solution , but wouldn't ` .map ` be better ? Or if you're going to use apply on a single series , you need to include an ` axis ` parameter .
I am trying to delete the first two rows from my dataframe ` df ` and am using the answer suggested in this post . However , I get the error ` AttributeError : Cannot access callable attribute ' ix ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method ` and don't know how to do this with the ` apply ` method . I've shown the relevant lines of code below : #CODE
If that's too slow , you can also skip the type checking and just apply the string conversion to columns matching the numeric type .
Python pandas : can I speed up this apply statement ?
I do all of this with apply statements . They work , but seem slow to me : 7 seconds in total , whereas any SQL would take a fraction of a second , even without parallelisation , on the same machine . If this were a one-off I wouldn't invest time in speeding this up , but I must do it multiple times on multiple dataframes of similar size .
Thanks - much appreciated . The data comes from a Microsoft SQL Server . I have a feeling I should learn LINQ and how to apply it to SQL - it might make these things much faster
You can use ` apply ` , iterate over all rows and replace 0 by the maximal number of the row by using the ` replace ` function which gives you the expected output : #CODE
Here is one workaround by defining customized ` apply / map ` function to unpack the list . #CODE
I can't solve your exact problem unless you include demonstration code . You can't apply my example to your problem ?
You can use str.split on the series and apply a function to the result : #CODE
Is apply as efficient as this " vectorized " function definition ? I received this suggestion to avoid mapping , supposedly doing loops of my lambda functions : #URL
@USER I get an AttributeError : AttributeError : ' DatetimeIndex ' object has no attribute ' apply '
OK the following should work , convert your datetimeindex to a series so you can call ` apply ` and use ` strftime ` to return an array of strings : #CODE
But , again I want the output of KDE of original X not positions , so I can make first derivative on the output of KDE . That's why I did kernel ( X ) . I'd prefer if you could write an answer with your supported code to plot pdf output vs KDE of original data and save the output of KDE to apply first derivative on it
I couldn't find an answer to this in the existing ` SettingWithCopy ` warning questions , because the common ` .loc ` solution doesn't seem to apply . I'm loading a table into pandas then trying to create some mask columns based on values in the other columns . For some reason , this returns a ` SettingWithCopy ` warning even when I'm wrapping the test in a ` pd.Series ` constructor .
Data type of pandas column changes to object when it's passed to a function via apply ?
I need to use the ` dtype ` of a pandas column in a function , but for some reason when I call the function using ` apply ` , the ` dtype ` is changed to ` object ` . Does anyone know what is happening here ? #CODE
It appears to be due to an optimization in ` DataFrame._apply_standard ` . The " fast path " in the code of that method creates an output Series whose dtype is the dtype of ` df.values ` , which in your case is ` object ` since the DataFrame is of mixed type . If you pass ` reduce=False ` to your ` apply ` call , the result is correct : #CODE
As you're trying to learn categories from one DataFrame to apply to a different DataFrame , using scikit-learn might provide a more elegant solution : #CODE
Or use ` apply ` : #CODE
By the way , you can apply your logic of ` df [ df [ ' col_1 '] ! = 754 ]` also on the index . This would give ` df [ df.index ! = 754 ]` , although this would not work with a multi-index
I know that I can simply do one datetime minus the other to find the timedelta - but I don't know how to apply this to make a new column .
There is an ` apply ` for that : #CODE
The code is not that efficient , Is there a more efficient way to do this ? I would assume there must be some special function ( such as apply , roll_apply ) to iterate through these values , but I couldn't figure that out . Any help would be appreciated .
how do I apply normalize function to pandas string series ?
I would like to apply the following function to a dataframe series :
How do I apply the ` normalize ` function to all members of the series ?
If ` c ` is your string column . ` map ` is used to apply a function elementwise ( and of course you wouldn't have to chain it all together like this ) #CODE
This was written in Python 2 , but the basic idea should work for you . It uses the apply function : #CODE
Note that if you loaded the other keys into the dict , you could do the apply without the swapper function : #CODE
Could you post an array form of the data so I can play with it a bit ? Also , consider creating a new function to use in the apply for now until you can work the problem out .
Simply add axis = 1 to your apply function and it will work : #CODE
You add the column to the subgroup inside the apply function , and then when you return that subgroup it replaces the existing subgroup .
The way that I have found to do is by making a copy of the column and operating on it ( I have to do this because a ` DateTimeIndex ` has no ` apply ` method ) . I am sure there must be a way to operate on the index directly though but I could not find it : #CODE
It could be done by calling ` apply ` on the df like so : #CODE
This uses double subscripts ` [[ ]]` to call ` apply ` on a df with a single column , this allows you to pass param ` axis=1 ` so you can call you function row-wise , you then have access to the index attribute , which is ` name ` and the column name attribute , which is ` index ` , this allows you to slice your df to calculate a rolling ` std ` .
However , when I apply this method to whole length of data , I couldn't wait until the operation was done .
Not sure what you mean by your comment @USER . I did use pd.DataFrame() on each of your dict's , however , what I did was apply a dict comprehension using each of them to strip out the nested zeros . pd.concat() just took all three of the DataFrame's from dict's and concatenates them all together .
Python pandas : banal apply statements incredibly slow
apply sum calculated using pandas group by to all elements of group
Please suggest how can I apply sum on all rows of one account .
You can use the ` map ` method on your json-text column to apply a ` lambda ` function which will parse the json using ` json.loads ` then return the field that you want . #CODE
You can use the ` loc ` method of a dataframe to select certain columns based on a Boolean indexer . Create the indexer like this ( uses Numpy Array broadcasting to apply the condition to each column ): #CODE
I can see from your output that this is exactly what I want . However , I cant recreate it because our initial dataframes are created in different ways . I'm using a groupby to set the initial split and sex index . Then , if I use your code : level_group = np.where ( df.columns.str.contains ( ' 0 ') , 0 , 1 ) on my groupby ( replacing the df ) , I get an error : Cannot access attribute ' columns ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method
What I did is to apply this function to get the count of repeating elements in B #CODE
Apply async usage :
@USER I'm not sure in this case , you could use ` %timeit ` in ipython to compare . Generally speaking , the advantage of pandas is crunching numbers not strings , so it might not matter too much . But generally speaking , ` apply / lambda ` will be slower than using built in pandas methods .
Pandas : Apply function via " Column A " , simultaneously reading " Column B "
In the ` csv ` , there is also a `" Column B "` that contains values that I want to read into a variable ` x ` within the function . It should not ` apply ` from `" Column B "` this should still be done from `" Column A "` . Is this possible ?
Post-edit : This question has been identified as a possible duplicate of another question . Although the answer may be the same , the question is not the same . For future readers it is probably not apparent that ` apply ` on two columns is interchangeable with ` apply ` on one column and " reading " another column at the same time . The question should therefore remain open .
possible duplicate of [ How to apply a function to two columns of Pandas dataframe ] ( #URL )
The ` axis=1 ` argument passed to the apply method puts the whole row into the apply method as a single tuple argument .
This is a really messy way to do this , first use ` first_valid_index ` to get the valid columns , convert the returned series to a dataframe so we can call ` apply ` row-wise and use this to index back to original df : #CODE
You can do the following , this tests each row for membership of ` 1 , 2 ` using ` isin ` and if so this generates a boolean series , you can use this to index into the columns by calling ` apply ` again , we convert this to a list because the dimensions won't align if you don't do this : #CODE
output from inner ` apply ` : #CODE
performance degradation when switching from pandas column concatenation to using apply on dataframe
Generally using ` apply ` should be avoided it's essentially a for loop and not vectorised , it would be better to write a func that performed vectorised operations rather than using ` apply ` at all
Right now I have the data input , and I have more or less written the function I would like to use to analyze each column separately . However , I can't quite understand how to use a forloop or use the apply function through all of the columns in the dataset . I would prefer not to hardcode the columns because I will have 40,000 ~ 50,000 columns to analyze .
I'm attempting to use the apply function : #CODE
` axis=2 ` is not a valid param for ` apply `
I know I can make a partial function with " or " and my vector and apply it to the df , but this is probably unidiomatic and needlessly time-consuming . What is the pandas way ?
Apply curve_fit on dataframe columns
I have a ` pandas.DataFrame ` with with multiple columns and I would like to apply a ` curve_fit ` function to each of them . I would like the output to be a dataframe with the optimal values fitting the data in the columns ( for now , I am not interested in their covariance ) .
I can apply the function and get an array in return : #CODE
Can this be done with something similar to ` apply ` ?
I think the issue is that the apply of your fitting function returns an array of dim 3x3 ( the 3 fitparameters as returned by conner ) . But expected is something in the shape of 20x3 as your df .
Well all you're doing is creating a df for each column which seems unncessary , semantically they should perform the same , ` apply ` on a df calls the function on each column in turn ( as ` axis=0 ` is the default param value )
Can probably be improved using ` map ` or ` apply ` . #CODE
I've been trying to figure this out for awhile now and haven't been able to apply any of the solutions I've found online for splitting columns in pandas yet . I have to apply the column split to 90+ consistently formatted columns . I feel like the solution should be trivial , I'm just too new to programming and python to figure it out !
Maybe a ` groupby ` in conjunction with ` apply ` ? I'm not familiar with ` apply ` yet .
you can use apply on the groups , which allows you to transform a group . This means that the function inside returns something for each set of entries that has the same ID .
@USER You can also define your own apply function to achieve the goal by using the ` cumsum ` trick . See the edited part .
The extract method will create a dataframe with as many columns as groups specified in the pattern you pass , in this case two . Groups are delimited by brackets in the pattern . I've edited the question to show hoe to apply it in your case .
Passing a dataframe as an an argument in apply with pandas
Ok , the problem here is the combination of ` func ` and ` apply ` . The ` apply ` method of a dataframe applies the given function to each COLUMN in the data frame and returns the result . So the function you pass to ` apply ` should expect a pandas Series or an array as input , not a dataframe . It should give either a series / array or single value as output .
will apply the ` sum ` function to each column and give a series containing the
Secondly , the ` args ` parameter in ` apply ` is only used when the function you are passing takes additional arguments ( besides the series , which should be the first argument ) . For example , you might have a function that sums an array and then divides by some number ( again a silly example ): #CODE
The you might want to apply this to each column of a dataframe with divisor = 2 .
@USER Sorry that I misunderstood your question . For your case , just apply the same ` groupby.agg ( sum )` logic to hourly price dataframe , and then calculate dot product with daily volume data . Finally sum over ` axis=1 ` . See the edit .
You should use ` resample ` to calculate the mean price for the day , and then apply it to the volume : #CODE
Actually , it is bad to use `' columns '` in the ` query ` expression like this . In the first example , it is just happened that returned Series can be used as index , but it might not apply to the general case .
The problem with the ` apply ` / ` map ` / ` applymap ` functions is that they don't
Here is a faster code using ` apply ` , but it will provide wrong result in case there are 2 or more months in the same column with the same value , because ` np.where ` returns an ` np.array ` of the indexes that it found the value of ` x ` in , but there's no way to store it and use the next index the next time we encounter the same ` x ` value : #CODE
Sorry , that's my fault and this is not due to date conversion . I was working with a much larger ` DataFrame ` containing other columns . In this context , it was a bad idea to use the ` apply ` function . After this correction , it works fine and I obtain the same result in less time . Thanks for the solution and for the reply .
Below is the code I apply to get the fitted volatility from the regression equation y = ax^2 + x + c and the results are great . . . #CODE
I need to have the regression variables and FitVol apply to all of the original data , not the data that was filtered to have the abs ( Delta ) be between 0.01 and 0.5
You haven't to apply ` for ` loop or ` iterrows() ` at all in pandas : #CODE
Group by then apply function then flatten back to dataframe in Pandas Python
pick the columns you want to diff , and apply ` pd.DataFrame.diff `
Now apply the logic described above : #CODE
And I want an expanding apply function that identifies whether we reach a new maximum value for a given id . The resulting dataframe should look like this : #CODE
I can't seem to pass two columns to the expanding apply function .
Its been a long time since I worked with ` apply ` like a couple releases ago minimum , so my recollection may be bad , or things may have changed . However , as I remember it the grouped data is passed automatically as the first argument .
The temptation when passing your own function to ` apply ` is to do this : #CODE
In example data above , this works great but when I apply this concept to my real datasets , I get ` Exception : cannot handle a non-unique multi-index ! ` . I verified that ` df1 ` and ` df2 ` have the exact same columns . Any ideas of the cause and how to fix ?
In which columns 4 , 5 and 6 are actually the components of a vector . I want to apply a matrix multiplication in these columns , that is to replace columns 4 , 5 and 6 with the vector resulting of a the multiplication of the previous vector with a matrix .
if so i would gest that the apply method is a good hint here but like @USER -sc i would suggest you to restructure your data model or have a look at pytables etc
You can create a column with the mutation type ( A -> T , G -> C ) with a regular expression substitution then apply pandas groupby to count . #CODE
You can transpose the df and apply a lambda that drops the NaN rows , slices from 4th value onwards and returns the first valid index : #CODE
@USER then may be you can use the idea and apply to panda DataFrame
In case you need to convert existing columns in a dataframe here the solution using a helper function ` conv ` and the ` apply ` method . #CODE
I believe your data structure isn't appropriate for your problem . Especially the ` list ` in fields of a ` DataFrame ` , they make loops or ` apply ` almost unavoidable . Could you in principle re-structure the data ? ( For example one ` df ` per solar panel with columns ` date ` , ` time ` , ` energy `)
I believe your data structure isn't appropriate for your problem . Especially the list in fields of a DataFrame , they make loops or apply almost unavoidable . Could you in principle re-structure the data ? ( For example one df per solar panel with columns date , time , energy )
Python pandas : retrieve the field associated to the min of another ( cross apply equivalent )
In SQL I was used to doing this with a cross apply .
If you want to apply an arbitrary Python function , you will have to loop it .
Can you write out the formula you want to apply ?
Based on the latest suggestion of Firelynx I have a small update which makes it a bit cleaner . Still , you need to keep a list in order to prevent double counts of the label , because unique apply only to a unique ( label , side ) combination . So I now have #CODE
Is it possible to have unique() apply on the label alone ? Then I could remove the label_list to keep track of which label has been processed already
tom's answer looks good . On a column you could also do ` .map ( lambda x : min ( x , 0 ) )` to apply the standard python ` min ` to each cell , but ` np.minimum ` is probably going to be the fastest way .
OR you can apply over two columns : #CODE
Further , it is possible to select automatically all columns with a certain dtype in a dataframe using ` select_dtypes ` . This way , you can apply above operation on multiple and automatically selected columns .
Export pandas DataFrame to LaTeX and apply formatters by row
Is there any way to hack this functionality ? Only thing I thought about was to manually apply the formats converting all my columns to strings before transposing and exporting
To all the answerers : as the OP mentions pandas , it may be desirable to enclose in apply / lambda . E.g. ` df.apply ( lambda x : your_code )` . It would also be good for the OP to be more explicit : ` df =p d.DataFrame ( [ 0.5 , 4.6 , 7.2 ] )`
Thanks ! one more question , will I manually ` e = re.sub ( ' / ' , ' - ' , c )` and apply ` to_datetime ( e )` that can improve the performance ?
you already accessed the column before calling the ` .apply() ` method on it , notice that you call the apply function as - ` df [ 0 ] .apply ` , which means apply it in 0th column of ` df ` dataframe .
if I apply a groupy say with column col2 and col3 this way #CODE
One option is to use groupby and apply to end with a pandas Series : #CODE
As far as numpy is concerned , a ` list ` counts as an arbitrary Python object . numpy can only efficiently deal with arrays that have regular dimensions and contain elements of a constant size in memory ( this all has to do with numpy's [ internal representation ] ( #URL ) of the array ) . This doesn't apply to Python lists , since the length and item size can vary arbitrarily .
Ha , that would be too easy :) . That's the result of the grouping operation that I want to apply . This column is unpopulated until I run the code above .
Another option where you can control the format is using the ` strftime ` method in an apply ( this would actually be equivalent to writing a loop , but shorter ): #CODE
However , when I apply this to my full dataset , with multiple dates in utctime , the x-axis remains a time - I want it to show the dates in this case .
Groupby on level 0 ( parameter1 ) and apply ` idxmax() ` and get the values : #CODE
Go through the matrix line by line , than apply in each element ` ord() - 65 ` if it is an alphabet else use it as it is . ` 64 ` is ` ord ( " A ") -1 ` .
Use ` groupby / agg ` to aggregate the groups . For each group , apply ` set ` to find the unique strings , and `'' .join ` to concatenate the strings : #CODE
convert the dtype of the df to a ` bool ` , then call ` apply ` and use the boolean mask to mask the columns , you need to pass param ` axis=1 ` to ` apply ` the column mask row-wise : #CODE
Your code ` my_df.apply ( lambda x : colnames [ x ])` won't work because firstly when calling ` apply ` on a df without specifying the ` axis ` will call the lambda on each column in turn , secondly the ` 1 / 0 ` will interpret this as an index value rather than a boolean flag .
The issue occurs because you have three columns with only ` NaT ` values , which is causing those columns to be treated as objects when you do apply your condition on it .
You should put some kind of condition in your ` apply ` part , to default to some timedelta in case of ` NaT ` . Example - #CODE
Or a simpler way to change the ` apply ` part to directly get what you want would be - #CODE
@USER I am applying customized functions with APPLY function . It seems to me that resample or TimeGrouper fills in the gap automatically , even there is a time gap of one year . Is there a way to prevent from this ? Thanks a lot
@USER thanks for the suggestion . I have switched to resample and it almost works . Only resample takes the first column of df , does it apply to multiple columns of df at the same time ? I would reedit my function into the question . thx again
I think you want to ` agg ` ( aggregate ) , not ` apply ` , as for each of your group , you want 1 returning value : #CODE
Apply the daily frequency to weekly frequency ( eg . Monday to Sunday )
Apply daily frequency to monthly frequency ( eg . how many times I see " 2012-01 -** " in my column )
Last apply to get annotation #CODE
Pandas has an efficient ` nlargest ` operation you can use that is faster than a full sort . It will still take awhile to apply across 500,000 columns . #CODE
This should be faster than a temporary sort +1 , the problem here is that the ` apply ` is trying to return a df with the same shape as the original df which is not what can be achieved unless you take the raw values and return some other data structure like in your answer
How to apply a condition to a large number of columns in a pandas dataframe
What I would like to do now is apply a function that takes in two strings and produces a score of the similarity between them . For now , I am using the ` difflib ` library .
You can use ` groupby ` and ` apply ` scheme . #CODE
Generally your idea of trying to apply ` astype ` to each column is fine . #CODE
Note : While using ` pandas.DataFrame ` avoid using iteration using loop as this much slower than performing the same operation using ` apply ` .
You can ` groupby ` on ' id ' and then ` apply ` ` list ` to ` value ` column and then call ` reset_index ` : #CODE
I resolved this error by creating list of tuples ( i.e. apply ( tuples )) instead of list . But i am not able to explain why tuples works here & when to use tuples vs list . Please advise .
For your reference the ` apply ` method automatically passes the data frame as the first argument .
Also , as you are always going to be reducing each group of data to a single observation you could also use the ` agg ` method ( aggregate ) . ` apply ` is more flexible in terms of the length of the sequences that can be returned whereas ` agg ` must reduce the data to a single value . #CODE
I found also this question related to a similar issues , but I can't figured out how to apply that method in my case .
You can apply the same logic , just keep the rows that have the first elements the start with " AN using a generator expression
You could apply a lambda on each column group : #CODE
I've tried to use ` apply ` like this , but can't figure out the correct syntax : #CODE
Pass param ` axis=1 ` to ` apply ` to iterate row-wise : #CODE
I'm trying to convert a Pandas dataframe series to float . I do ` locale.setlocale ( locale.LC_NUMERIC , '')` and then ` df.idh.apply ( locale.atof )` , but it gives me the above mentioned error : ` AttributeError : ' float ' object has no attribute ' replace '` . I assume at some point it's getting something like a NaN , maybe or some other string and it does not recognize it . How do I tell ` apply ` to skip those ?
Firstly the conversion to decimal is really ` float ` dtype due to the resampling as this will introduce ` NaN ` values for missing values , you can fix this using ` astype ` , you can then restore your ' timeline ' column which get lost as it can't figure out how to resample a ` str ` so we can apply ` strftime ` to the index : #CODE
You can ` groupby ` on ' col1 ' and then ` apply ` a lambda that joins the values : #CODE
but this will start the index from ` 0 ` . I want to start it from ` 1 ` . How do I do that without creating any extra columns and by keeping the index / reset_index functionality and options ? I do not want to create a new dataframe , so ` inplace=True ` should still apply .
You can ` apply ` a lambda to your dates and call ` datetime.strftime ` : #CODE
referring to this link : implementing R scale function in pandas in Python ? I used the function for def scale and want to apply for it , like this fashion : #CODE
Apply a func to generate a new colum based on value of other colums in Pandas
When I ' apply ' that function to both ' Time on Page ' ' Pageviews ' columns , wouldn't it take the value from both columns as the argument and return one value , which is ' AvgTimeOnPage ' , as the output ? I don't quite understand the error msg saying ' 1 ' arg is given , instead of ' 2 '
@USER Ah , I see , apply takes ONE argument ( which is a row / Series ) and you access the column as ` x [ ' Time on Page ']` inside the apply . Passing as ( x , y ) doesn't work - I suspect the second argument is used as some kind of flag .
Using apply on a column
I'd like to somehow apply a function to each column , converting it to a list and placing it in a new DataFrame . However , apply only operates on individual entries .
Managed to figure this out from what you gave me . I simply needed to apply a split function to each string in the dataframe ! :D Thank you !
but there is no pandas method that allows you to return the minimum like ` np.miniumum ` , also ` np.minimum ` does not care about aligning columns and indices here . You'd have to define a func yourself and ` apply ` it , also you need to add all this information to your question and to pose representative code to show your desired output including a series with an index in a different order to your df
And then apply it to some function that returns a series like so : #CODE
Essentially this performs a reverse lookup , we iterate over the ingredients series using ` apply ` and then test for membership of this ingredient in the whole df using ` contains ` this will handle plurals in this case .
I have a dataframe with sporadic dates as the index , and columns = ' id ' and ' num ' . I would like to ` pd.groupby ` the ' id ' column , and apply the reindex to each group in the dataframe .
Which returns error : ` AttributeError : Cannot access callable attribute ' reindex ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method `
This appears to be successful . Now I want to apply functions based on metadata criteria : #CODE
You can use apply , like this . #CODE
I need to apply this calculation to every cell .
But then I am struggling to find a way to apply this function to each cell in the dataframe . I tried ` iterrows ` but it was very slow as the actual dataset is very large .
If you have trends in your serie , you may rather apply it on time moving window instead of the whole serie .
@USER you mean to apply the function as follows ? This produced me an error . ` results= ingredients.apply ( lambda x : where ( df [ 0 ] .str .lower() .str .contains ( x.lower() ) , True ))`
Now apply a ffill lambda as follows : #CODE
More elegant than the apply function is to use ` result [ xstring ] = tst.bla.str.contains ( xstring )`
So , I truncated my data set in the question to make it easier to read and thinking that whatever solution came would also apply .. In actuality , the groupings are 34 rows long , and replacing the ` 3 ` with ` 34 ` does not seem to work - any thoughts ?
If you want to change the display format then you need to parse as a datetime and then ` apply ` ` datetime.strftime : #CODE
` apply ` a lambda to convert to timedelta and then subtract : #CODE
That now leaves me with $23 to spend on extra presents that birthday ; the same rules as above apply on any additional presents .
apply ` df.str.contains() ` to ` s2 ` using the contents of ` s1 ` as the matching pattern
The best I could come up with is to use ` apply ` instead of manual iterations : #CODE
sorry to break this to you but ` apply ` is essentially a ` for ` loop , the code just looks cleaner
My understanding in pandas is to use a pd.rolling_sum() function but i'm not quite sure how to groupby and apply it while setting a condition . I've also tried using cumcount() to no avail #CODE
See comment @USER In short , can I " universalize " this to make it apply to every car ? Thanks .
You can apply the recipe from here #CODE
That loop works fine , but I trying to see how I could use map and apply to make this look cleaner . From reading , apply is a loop function underneath , so I'm not sure whether I will get any speed from doing this . But , it could shrink the code by a lot .
Do you know of a more general method ? This works fine with basic addition , but I was trying to find a way to apply a more general function . I've updated the question to show a more complicated example .
I cannot understand the error I see when using apply or transform :
1 2 ) ` transform ` expects something " like-indexed " , while ` apply ` is flexible . The two failing functions are adding additional columns .
4 ) The first two functions take a ` DataFrame ` with two parameters and returns data . ` InnerFoo ` actually returns another function , so it needs to be called before being passed into ` apply ` .
Hi Tom , it doesn't look like this works . It outputs just one array and is equivalent to df2 [ ' array '] .sum() . But you have given me an idea with apply . Let me see if I can figure something out .
Apply string.format() to row in Pandas DataFrame
can be used to apply the format string using the column data .
and then apply the method over it .
You can create a boolean mask by calling ` apply ` on ' type ' column to create your new df : #CODE
Apply a function to translate a column in pandas dataframe with condition on other columns
Why not just convert the whole column to English , then use a mask of non-english rows to replace only the ones you need to ? That is a bit easier than using apply with your conditionals happening in each step .
and that the function inside ` apply ` should make use of the ` isin ` method .
probably but calling ` apply ` will also be very slow as this is just a ` for ` loop
Then apply your method : #CODE
apply conditional if loop over groups
I think it apply the function to the index instead of the value .
OK , I'd ` reindex ` using your time_series , then ` groupby ` on your index and then apply ` isnull ` and call ` sum ` : #CODE
If you want to add the values then you can call ` apply ` and use the ` new_df ` values to perform a lookup from ` cos ` df : #CODE
This is by far the most efficient solution to this problem . The hard part was realizing that I could ` sort ` ` df ` by the ` Type Rank ` so the ` Criterion Type ` rows were ordered by their rank . This meant I wanted the highest ` Max CPC ` to apply to the first , the second highest ` Max CPC ` to the second , and so on .
You can use ` str.split ` , followed by a ` apply ( pd.Series ) .stack() ` ( the ` apply ( pd.Series )` makes different columns of the elements , ` stack ` is for turning this to rows ): #CODE
I have tried to apply your answers but I do not get good results as you can see below : I just have the same value 1970 - 01-01 instead of having a column with the same value stored in datetime column . Arrival column is empty instead of having the count of arrivals as needed ( from df1 ) #CODE
@USER has a pretty good answer . Thinking outside the box , you could groupby school and set indexes on the date columns one at a time . Then you can use the rolling counts because it will be sorted by date . That will be much faster than using the apply method and checking len for each row . Check out cumcount #URL
Pandas dataframe apply function to entire column
` .apply() ` is the method to apply a function to a ` Series ` on a row-by-row basis . Other than that you haven't given much information to work with .
apply a function that return a list
I know how to split a string , but I could not find a way to apply it to a series , or a Data Frame column .
For all but the last names you can apply `" " .join ( .. )` to all but the last element ( ` [: -1 ]`) of each row : #CODE
Without knowing the format of your csv files this question is hard to answer . Yes , you can probably use much less RAM than the 3.8gb text file - No you cannot use the same strategies as you would apply to a file on your disk . On the disk you only have to store the information , in memory you often have to keep this information in a form which is easy to manipulate .
The ` args ` argument given to apply function is passed ` func ` argument ( lambda function given ) . You are getting this error since two arguments are given but lambda function only accepts one argument .
You could take advantage of the vectorized string operations available under ` .str ` , instead of using ` apply ` : #CODE
` pd.read_table ( filename , usecols =[ 0 , 8 , 9 , 11 ] , parse_dates =[ 1 , 2 ] , dtype={ ' LopNr ' : np.uint32 , ' INDATUMA ' : np.uint32 , ' UTDATUMA ' : np.uint32 , ' DIAGNOS ' : np.object } )` , assuming the dtype would apply to the data before it enters the converter , hiccups on a string in some of the rows : ` ValueError : invalid literal for long() with base 10 : ' string '`
This can be done with ` groupby ` and using ` apply ` to run a simple function on each group : #CODE
and if you want to apply your custom function you can use apply where it takes your custom function as a parameter , and it passes each group to your custom function #CODE
Just to compare against using ` apply ` : #CODE
If you had not called ` apply ` on the ` groupby ` object then you could access the ` groups ` : #CODE
Including the group name in the apply function pandas python
Is there away to specify the groupby call to use the group name in the apply lambda function .
is there away to get the group name in the apply function , such as : #CODE
How can I get the group name as an argument for the apply lambda function ?
This is difficult to do in place as pandas will expand the structure after the ` apply `
If you want to apply values from other parts of the df you could put those in a dict and then pass that into your apply function . #CODE
But I recognise that my expressions contain sub-expressions that consist of only scalar values . Do the documented rules apply to sub-expressions as well then ? The code does run , but the result isn't correct .
The problem with ` apply ` is that you need to return mulitple rows and it expects only one . A possible solution : #CODE
Thanks for the advice hellpanderrr . I think what I needed to know is that it's not possible to reassign different dimensions in an apply function . I also needed a way to generically assign the remaining columns to the new groups . In the end I came up with the technique shown in my answer . Cheers
The ` groupby ` version of ` apply ` supports ` DataFrame ` as return value in the way which you intended : #CODE
Just call ` apply ` and call ` tuple ` : #CODE
How do I apply a regex substitution in a string column of a data frame ?
You should assign another DataFrame to hold the index and value of such , and apply to the original DataFrame base on the groupby field ( as index ) .
Then apply and perform the lookup : #CODE
For each of the new data frames I then apply this logic : #CODE
A hack is to set the dtype to object before doing the apply : #CODE
This solution worked if i apply group by column only on Sex field . Howver another requirement says the below format : Date Sex weight hight Salary
How to apply cubic spline interpolation over long Pandas Series ?
This approach is very slow as it involves iteration and invoking the apply method for each group .
Re transform vs apply : generally speaking you use transform to keep the same number of rows when the function would otherwise reduce the number of rows . It's not clear to me why it is needed here as expanding_mean should not be reducing anyway ...
You need to apply your function to a data frame , not a series #CODE
I would apply your operations to a copy of the ` DataFrame ` and stack back together - something like this : #CODE
You could use ` apply ` to generate the values for each range , then ` melt ` to reshape the data into long form . #CODE
Apply two operations on the sub ` DataFrame ` obtained by the ` groupby ` ( one for each year )
Alexander , this was extraordinarily helpful ! It may take me some time to evaluate and apply . I will report back . I am proactively calling this question answered . Please let me know if there are other methods for up voting and giving positive feedback !
I am new to python and pandas . How can we apply a groupby and an aggregate on multiple columns ignoring the blank / None / NaN values ?
I am trying to apply a groupby and count agregation function on these values as :
Because your are updating the same set of values in that dataframe at the same time , you can try to use a temp variable to hold the result and apply back to the column , see if this helps ridding of the warning message .
Here is the feedback I get from plugging that in : AttributeError : Cannot access callable attribute ' unstack ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method
I see you can do two conditions on one line , but I don't see how to apply it .
The second line is perfect , but how do you implement the first stage , where location id can be specified for the group by to apply to the subset ?
Discretization of continuous attributes using np.histogram - how to apply on a new data point ?
After I " learned " my bins from train data , using ` np.histogram ( A [ ' my_var '])` how do I apply it on my test set ? as in which bin is the my_var attribute of each data point ? Both my train and test data are in pandas data frames , if it matters .
Pandas : How to apply a function to different columns
I want to apply the function to just columns ` B ` and ` D ` . ( Applying it to the full DataFrame isn't the answer as that produces NaN values in the numeric columns ) .
But I cannot fathom how to select distinct columns to apply the function to . I've tried all manner of indexing by numeric position , name , etc .
How to apply a function to two columns of Pandas dataframe
Pandas : How to use apply function to multiple columns
Pandas : apply different functions to different columns
Python Pandas : Using apply to apply 1 function to multiple columns
Apply isn't inplace , it returns a new dataframe , so the question is can you return the complete dataframe in one go .
None of them seem to apply to this problem , and all of them say that in order to generate unique values from every column , you should either use a groupby function , or select individual columns . I have a very large number of columns ( over 20 ) , so it doesn't really make sense to group them together just by writing out df.unique [ ' col1 ' , ' col2 ' ... ' col20 ']
I have tried .unique() , .value_counts() , and .count , but I can't figure out how to apply any of those to work across multiple columns , rather than a groupby function or anything that was suggested in the above links .
Just call ` apply ` and pass ` pd.Series.value_counts ` : #CODE
I had understood ' converters ' specified a function to apply to the column . evidently I was wrong - thank you for pointing this out , it's very useful !
The read_excel() function has a converters argument , where you can apply functions to input in certain columns . You can use this to keep them as strings .
It won't be super-performant , but you should be able to ` apply ( pd.Series )` : #CODE
I think you may have to apply the operation to each column individually since ` factorize ` only takes a 1D array as input .
Accordingly , I followed the guidance provided elsewhere on stack overflow , but pertaining to ` re.sub ` , and attempted to apply it to ` pandas.DataFrame.replace ` ( using replace with ` regex=True , inplace=True ` and with ` to_replace ` set as either a nested dictionary , if specifying a specific column , or otherwise as two lists , per its documentation ) . My code works find without using a function call , but fails if I try to provide a function as one of the replacement values , despite doing so in the same manner as works with ` re.sub ` ( this was tested and worked correctly ) . I realize that the function is expected to accept a match object its only required parameter and return a string .
@USER That logic is not correct in the operational sense that ``` re.sub ``` allows for my expected notion of function interpolation . I am using apply right now , but this really should work with ``` pandas.DataFrame.replace ``` .
regardless , you should use apply or #URL
if you don't want to merge both frames , you can apply the same logic on ` dfV `
Thanks @USER . how can we apply the function to only the rows missing values in C and D ?
Once you have that , you can apply it to every row using the ` apply ` method on dataframes : #CODE
You can use ` apply ` to make the code more concise . For example , given this DataFrame : #CODE
The ` apply ` function applies the ` contains ` function on each column ( since by default ` axis=0 `) . The ` any ` function returns a Boolean mask , with element True indicating that at least one of the columns met the search criteria . This can then be used to perform selection on the original DataFrame .
You can make use of the DataFrame's ` apply ` method . #CODE
@USER This not a duplicate , the OP does not even use apply here .
Pandas : How to structure row-wise apply which requires previous output as input
Alternatively , you can also apply a function based on the column ` col1 ` : #CODE
The ` apply ` approach should be preferred in this case as it is much faster : #CODE
Sadly Tom I'm at a loss of where to start . I was looking at this example #URL but couldn't apply it to my own
You can ` import json ` and apply ` json.loads ` to convert the string data in your ` geojson ` column to ` dict ` . Then , you can extract data from ` dict ` directly , or use one of many Python modules that deal with GIS data . I find ` shapely ` easy to use and helpful in many cases .
Pivot table from a pandas dataframe without an apply function
I thought using the pivot function to the dataframe ( df_pivot = df.pivot ( index= ' ID ' , columns= ..., values= ' count ') but I am missing the columns index list . I thought applying a lambda function to the df to generate an additional column with the missing column names but I have 800M IDs and the apply function to a grouped dataframe is painfully slow . Is there a quick approach you might be aware off ?
Then apply the pivot method setting the new ` subindex ` as columns and fill ` NaN ` values with 0 : #CODE
Does this work only on the index of the dataframe ? I'd like the option to specify the field ( s ) to apply this to
Sorry , I want to simply identify the column that contains the text ' Measure ' in it , which I then apply the filter measure_filter too using .isnin .
Sorry for not being as clear cut , I've attempted to update my question to be more concise . I'm wanting to apply the logic to just identify the word ' measure ' within the following code ` ( df [ ' hereisalltherandomtextmeasure '] .isin ( measure_filter ))`
What is a concise way to split col3 into new , named columns ? ( perhaps using lambda and apply )
You could apply a join to the list elements to make a comma separated string and then call the vectorised ` str.split ` with ` expand=True ` to create the new columns : #CODE
A cleaner method would be to apply the ` pd.Series ` ctor which will turn each list into a Series : #CODE
You could use ` apply ` to remove the nulls and take the integer location like this . #CODE
If you change ' transform ' to ' apply ' , you'll get : #CODE
Then what you'll need is to index each column using then apply ` df.resample() ` #CODE
Are you trying to set the value of an existing column by applying a scalar function to each row ? If that's the case , instead of iterating over the rows you can consider apply , map , or applymap methods based on your need . This is a pretty good summary #URL
Assuming you have a unique-indexed dataframe ( and if you don't , you can simply do ` .reset_index() ` , apply this , and then ` set_index ` after the fact ) , you could use ` DataFrame.sample ` . [ Actually , you should be able to use ` sample ` even if the frame didn't have a unique index , but you couldn't use the below method to get ` df2 ` . ]
You can do this with groupby and apply : #CODE
You need to apply ` transform ` to the ` groupby ` , which preserves the shape of your original DataFrame . #CODE
Because that fft function changes the shape of the input you can't just apply it directly . Here would be one way to wrap it . #CODE
We want to remove rows whose values show up in all columns , or in other words the values are equal => their minimums and maximums are equal . This is method works on a ` DataFrame ` with any number of columns . If we apply the above , we remove rows 0 and 2 .
Pandas apply to multiple rows with missing dates
For a Pandas DataFrame I am looking for a vectorized way to calculate the cumulative sum of the number of views per given group , except the views from more than a week ago . I have tried all kinds apply functions , but I can't seem to go up and down 7 days to collect the data I need .
I am also not sure how to pass ` .year ` argument after I successfully convert the strings into datetimes . I could write a wrapper function that takes each row as input and then extracts the year , but I think it s useful to know how to apply pandas syntax for future reference . Thanks !
Classic case of pivot . First , let's introduce a count column , then create a pivot table . Let's ignore your regex , since that is not the issue ; just apply it to the column beforehand . #CODE
What is the Pythonic way to apply a function to multi-index multi-columns dataFrame ?
Given a multi-index multi-column dataframe below , I want to apply LinearRegression to each block of this dataframe , for example , " index ( X , 1 ) , column A " . And compute the predicted dataframe as df_result . #CODE
Some columns of the original data can contain missing value , and we cannot apply regression directly on them . For example , #CODE
` df [ df [ ' A '] 1.0 ]` : this works - But I want to apply the filter condition to all columns .
what filter condition do you want to apply , what is an example ` df ` and what are you expecting as output ? When trying it for whole df , there would surely be some rows where only some columns meet the condition ( and vice-versa ) , so for places where the condition is not met , it is substituted with ` NaN ` .
B . Create a function which transforms the " refund " orders into negative values and , then , apply it on the Series : #CODE
Why not just use an ` apply ` on the column , and do something like ` lambda lst : ' ' .join ( lst )`
what does ' ' mean in the apply function ?
I think actually it's better to test for each value and apply ` any ` : #CODE
lol .. my apologies I didn't notice you have " 2200 " columns , I jumped too quick to a conclusion . but hey same rules apply you can do : ` df [[ col for col in df.columns if col.endswith ( " _x ")]]` . But hey just follow unutbu's solution , he's GOD of pandas
Python Pandas Apply Formatting to Each Column in Dataframe Using a Dict Mapping
I want to apply very specific formatting to each column in the dataframe using a dict like the following : #CODE
I know I can use applymap for multiple columns or apply on a single column : #CODE
How can I iterate through each column in a dataframe and apply formatting using a dictionary where the ` dict ` ` key ` is the ` column ` and the ` value ` is the ` string ` formatting ?
The easiest way would be to iterate through the ` format_mapping ` dictionary and then apply on the column ( denoted by the key ) the formatting denoted by the ` value ` . Example - #CODE
With the hopes of x being a series with the values I want that I can column bind to my DF . What actually happens is that it errors out with an IndexError . In that case I made a function to apply , in place of the lambda function , so that it could except the error , but this returned all nulls .
I don't have mongo installed . Is the 1st box you show the first rows of ` data ` in your code ? If it is , I think it would be more or less easy to solve with ` apply ` , there's many questions around but probably [ this one ] ( #URL ) will help you . If you find issues please post them . Hope it helps .
To get exactly what you hoped to see , included the other columns in the group by , and apply sums to the Y variables in the frame : #CODE
The issue is that you have some columns that are int , hence when trying to apply the regex on those int values it fails with the error - #CODE
You can convert your columns to ` str ` and then apply the ` DataFrame.filter ` - #CODE
You will need to convert to ` str ` before you can apply regex on the column name , a way ( not sure if the most efficient ) to not convert the column names to ` str ` permanently and still get the required data is - #CODE
yes - however , I should have said that I want to apply the new series the requests.post method , in order to practice passing functions to series items .. for instance , I could have 100 columns that I would not want to write out .
The same principles would apply if you are using excel reader .
Note : I thought you used to be able to return a list in an apply ( to create a DataFrame which has list elements ) this no longer appears to be the case .
How to get the index and column name when apply transform to a Pandas dataframe ?
My version always replaces the positive values with 999 . It does ' apply ' over the rows , but I can't quite get how you could do this without that .
2 ) There is a much better way to apply a function than map to validate urls ?
I might do this with apply rather than eval ( especially if I didn't trust the source ): #CODE
I would like to apply something like ' text to columns ' function in excel .
You compared the result of groupby ` upper_bound ` to ` df [ ' C ']` , but they have different number of elements . Use ` transform ` to have the mean for each line existing witin each group and compare it to ` df [ ' C ']` . Apply this mask with ` loc ` : #CODE
If you really must remove the ` microsecond ` part of the datetime , you can use the ` Timestamp.replace ` method along with ` Series.apply ` method to apply it across the series , to replace the ` microsecond ` part with ` 0 ` . Example - #CODE
and got this error : ` AttributeError : Cannot access callable attribute ' drop ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method `
but when I apply it to a for loop , it shows integer type values . #CODE
You can call ` apply ` with a lambda that calls the vectorise ` str ` methods to slice your strings : #CODE
Whilst this answer is correct we should actively discourage using ` apply ` where a vectorised solution exists , of course if the version of pandas is so old that the ` .str ` methods don't exist then this would be a valid answer
I wrote the function that will compare those 2 strings and return True or False , the problem is I fail to see how to apply / applymap to the consecutive rows : #CODE
First ` NaN ` are converted to ` 0 ` , then apply function above and it return NaN instead of problematic values . So you have to find rows with NaN values and return subset of original ` df ` . #CODE
Note that ` copy() ` is required if you wish to later apply changes to that new dataframe ` dfa ` . Otherwise , if I remember correctly , you would be applying changes by pointer , much like when using dictionaries .
Function np.unique with parameter ` return_index=True ` return unique indexes of array . But I need indexes inverted , so firstly I inverted array and then subtracting them by index . Function apply cannot access to index link , so it is count from length of columns ` colD ` minus 1 . Last values of column ` colB ` are summed by indexes from list of column ` colF ` . #CODE
You can see that the ` apply ` approach is not working : #CODE
what version ? your apply code worked for me in ` 0.16.2 `
I tried this approach , however I don't know how to apply ` concat ` inside the for loop . #CODE
First , apply ` isinstance ` to determine which elements are floats , then slice your series to get the elements back .
Then just apply ` str ` and you're good . #CODE
I can do the proportion using group / apply : #CODE
sorting is slow , it's O ( n*log ( n )) . I think this may also be doing an apply , which is also slow . What's the reason you have to do this ?
For a 40,000 0 row df using ` str.replace ` is faster than using ` apply ` : #CODE
I am trying to apply something like this .... #CODE
After that I can easily count the occurrences per column . But currently I am stuck in achieving one of the two given results . I know it should work somehow with ` apply ` or ` transform ` , but I have no precise idea unfortunately .
Here's one way , with ` apply ` and ` first_valid_index ` : #CODE
Then , apply ` get ` to the column , returning the original value if it is not in the dictionary : #CODE
I have the jaccard similarity function defined as ` jaccard() ` I only want to know how to apply it to ` df ` so that I can have this type of representation matrix by the end . Thank You !
So you need apply ` jaccard ` function to ` df ` ? What is input and output of ` jaccard() ` ? Can you use function df.apply or df.applymap ? [ src ] ( #URL ) Or you need create ` jaccard_custom() ` ?
use data from two or more columns when using .map to apply a function
Thanks . Ideally I'd like to add a ` lambda ` in the ` cumsum ` . So to get to ` y ` I take ` x ` and apply some function .
... and then cast the result of " apply " to a list . #CODE
I have the dtaaframe above , where the index is the column datetime . I would like to decrease the values in the column JD ( by 1 ) from 5th Jan 2000 to 8th Jan 2000 , using the dates ( and not merely row numbers ) . Is there a pandas command to do this ? I have been playing around with apply , but not sure how to use it
Nice and simple , no need for apply .
The ` apply ` function returns a new DataFrame instead of modifying the input in-place . Therefore , in ` remove_leap_JD() ` , the code should be changed to something like : #CODE
You could write your logic as a function and then apply the function to your dataframe using applymap() . Also , note that ' 0 pound ' should probably read ' 0 pounds ' .
I am trying to apply a function which returns the latest or maximum date for a stock ( on which I have collected prices for multiple days ) .
Where , ` values ` contain numpy arrays , ` apply ` , ` lambda x : pd.Series ( x )` on ` df [ ' values ']` #CODE
Eventually , I think having the distance matrix as a pandas DataFrame may be convenient , since I may apply some ranking and ordering operations per row ( e.g. find the top N closest objects to object ` first `) .
Use ` apply ` on column to do ` df [ ' B '] .apply ( lambda x : sum ( map ( int , x.split ( ' , '))))` #CODE
If I understood correctly your issue , maybe you can just apply a filter like : #CODE
You can use the apply method :
A dictionary can only have one value associated with a key , so that syntax won't work . There are two other options that come to mind to get a similar output : you could select the column using brackets , and then pass a list of the reduction operations you want to apply : #CODE
How can I skip ( don't apply ) the filters that are None ? #CODE
Filtering in pandas - how to apply a custom method ( lambda ) ?
How can I apply ` df [ ' column2 '] .apply ( lambda x : ' str2 ' in x.split ( ' , '))` to #CODE
To apply this , simply use this to filter the DataFrame . Example - #CODE
ops , one more thing . What if the key to filter for ( ` str2 `) is an array itself ? For example : ` ..... apply ( lambda x : [ ' str2 ' , ' str4 '] in x.split ( ' , '))]` ? That won't work , but actually that's what I need -- the filter should be an array . And if any element from [ ' str2 ' , ' str4 '] contains in x.split ( ' , ') then that's True condition .
My Questions and / or things I've read about on SO but haven't / am unclear on how to apply :
So it looks like ` nextday ` is already vectorized ( i.e. operating on the whole frame at once ) . Why are you calling it via ` apply ` ?
@USER When I don't call nextday via apply , the calculations don't get applied for days 2-last day . I end up with a bunch of NaN values in the output . I tried it as well using for index , row in d.iterrows() , but that's about the same speed as the apply method . Is there another / different way I can apply the calculations without using apply ?
Can you reduce your problem down to some copy paste-able functions / data ? Likely to to get more help . Your ` nextday ` function doesn't seem to use ` row ` at all , which it doesn't make sense to use via ` apply ` ( which is for row-by-row function application ) .
It's somewhat tough to unpack without expected output , but your your function is already vectorized . e.g. when you have ` d [ ' ET_WL '] -d [ ' infilP ']` that subtracts on all the rows in ` d ` , so there isn't any reason to call it via an apply . In essence what you're doing is : #CODE
You could ` apply ` ` tuple ` on ` axis=1 ` #CODE
Apply the following function over the dataframe to generate a new column : #CODE
Are you sure you are replacing df with the result of calling ` apply ` ? Apply doesn't change the dataframe inplace , rather returns a copy of it , so you need to store it or else the results are vacuous ... that's the only thing that comes to mind
I have tried setting a new df to the results of apply via ` df2 =d f.apply ( sep_yearmonths , axis=1 )` then ` df2 =d f2.groupby ( ' month ') .sum() `
You can add a secondary axis by specifying ' secondary_y=True ' when you apply your plot function directly on your dataframe .
Pandas ' apply method
You should consider a func that is passed to apply that simply makes some calculations and returns either a scalar or array like structure to avoid ambiguous behaviour , using apply to modify a df in place is not going to work in practice as especially if you iterating row-wise yet wanting to mutate the df row-wise
Generally speaking the answer is that ` apply ` is NOT in place but you made this overly complicated . Generally you would use ` iteritems ` OR ` apply ` , not both . In this case , you have no need to use ` iteritems ` in addition to ` apply ` . In fact , just do this : ` tt.iloc [ 1 , :] * 2 ` or ` tt.iloc [ 1 , :] *= 2 `
@USER : above is a simple example to a much more complicated function . My actual function is not a simple ` multiply by 2 ` . It does other things , and uses the ` index ` of the ` Series ` as an input as well . Really I plan to have a second function which acts on every element in the ` iteritems() ` . Perhaps I should use two ` iteritems() ` instead of ` apply ` .
Generally speaking , don't use ` iteritems ` or ` iterrows ` if you can help it ( and it's rare you really need them ) . You're almost certainly better off with ` apply ` than any of the ` iter ` -options , and there are often better options than apply . I realize it's an artificial example , but still ... don't do it ! And consider posting a more realistic ( but still simple ) example if you want more specific advice .
How to apply tz_convert with different timezones to different rows in pandas dataframe
You can ` groupby ` on ' userid ' and then on ' var1 ' col call ` apply ` and pass ` list ` to create a sequence , you can rename / reset if required . #CODE
Also you should almost never need to iterate row-wise so avoid using ` for ` loops , ` apply ` , ` iterrows ` etc ...
The lines below apply to data where ` INDATUMA ` and ` UTDATUMA ` are of the format 20071231 , e.g. Date parsing seems to work for ` indate ` and ` outdate ` , those values make sense .
IIUC you can just call ` apply ` and pass ` value_counts ` : #CODE
You can use a combination of ` apply ` and this [ answer ] ( #URL ) to achieve this but why is this an issue ?
I want to apply following rules :
You could call ` apply ` and pass a lambda and call ` squeeze ` to flatten the Series into a 1-D array : #CODE
I'm not sure if this is quicker though , here we're applying the mask column-wise by calling ` apply ` on the df which is why transposing is unnecessary
So to properly fillback by date I can use groupby ( level=0 ) function . The groupby is fast but the fill function apply on the dataframe group by date is really too slow .
Then I'd apply the ` between_time ` pandas function to filter the dataframe by start and end date given by the ` bounds ` dataframe : #CODE
df.plot() correctly shows the labels , as you say . plot ( df ) was what I was doing , and does not . It seems like the ' label= ' part of the plot ( ... ) call ought to take a list or series or something and apply the values according , but I can't figure out how .
I probably should use ` apply ` , but how exactly ?
Performance is drastically improved by increasing the ` arraysize ` attribute of the Cursor - allowing me to get decent performance out of ` fetchall() ` . pandas ` read_sql() ` takes a ` Connection ` object as input and the cursor is created within the function , therefore it's not obvious to me how I can apply that same setting and still take advantage of the ` read_sql() ` function . Have I missed something ?
I've been able to construct the following code ( mostly with the help from the StackOverflow contributors ) to calculate the Implied Volatility of an option contract using Newton-Raphson method . The process calculates Vega when determining the Implied Volatility . Although I'm able to create a new DataFrame column for Implied Volatility using the Pandas DataFrame apply method , I'm unable to create a second column for Vega . Is there a way create two separate DataFrame columns when the function to returns IV Vega together ?
The two tricks I'm using here are 1 ) using %i in the format string to signify that it's an integer ( %f means a float , but it renders w / o trailing zeros ) and 2 ) the apply function on df . Make sure that axis=1 with that one .
or if s1 and s2 are columns in a pandas DataFrame df , then we can use similar logic and the apply function : #CODE
When I apply #CODE
I keep getting the following error when I apply it to real data ( in which case , data frames are of different sizes ):
Is there a way to ` apply ` a function to one column of a dataframe while leaving the other columns fixed ?
If ` apply ` is not done ` inplace ` you still have to make an assignment , so what is the difference ?
@USER my point is that if I want to compose functions ` f1 ` , ` f2 ` and ` f3 ` , the syntax would be ` d.assign ( A=f3 ( d.assign ( A=f2 ( d.assign ( A=f1 ))))` ; agree that this is better than in-place , but I would argue that this is less readable than a " forward pipe " style syntax using something like ` apply `
Apply a value to all instances of a number based on conditions
I want to apply a 5 to any ids that have a 1 anywhere in the number column and a zero to those that don't . For example , if the number " 1 " appears anywhere in the Number column for ID 1 , I want to place a 5 in the total column for every instance of that ID .
You could ` groupby ` on ' Area ' and ` apply ` ` list ` : #CODE
If you want to split the values out you can call ` apply ` and pass ` pd.Series ` ctor : #CODE
I think what you're looking for is a ` groupby ` followed by an ` apply ` which does the correct logic for each user . For example : #CODE
I know word_tokenize can for it for a string , but how to apply it onto the entire dataframe ?
You can use apply method of DataFrame API : #CODE
For finding the length of each text try to use apply and lambda function again : #CODE
You can call ` apply ` pass ` axis=1 ` to ` apply ` row-wise , then convert the dtype to ` str ` and ` join ` : #CODE
A method by which you can do this would be to apply a function on the grouped DataFrame .
Strangely the condition that removed all the ` nan ` did not work . But I could resolve it through putting the output of the condition in a new df and apply the code on that , that worked . Thanks again !
If you only want to remove certain rows within matching groups , you can write a function and then use ` apply ` : #CODE
Anyone know how to apply a method to change it ?
what does apply ( floor ) do here ? I don't really know ..
Apply unique twice in groupby dataframe
thank you ! I guess I should have mentioned my real df has about 300k rows so it wouldn't be practical this way . Also , what I'm looking for is a " query " or " method " to apply to the whole df and return the desired subset . e.g. df.method_1 = subset_1 , df.method_2 = subset_2
I'd add a new col using ` date_range ` passing the ` min ` and ` max ` date values , then call ` apply ` on a df with a single column passing param ` axis=1 ` to ` apply ` row-wise , you can then count the number of rows that meet your condition using ` sum ` ( as this will convert ` True ` to ` 1 ` and ` False ` to ` 0 `) and add this as a new column : #CODE
So I'd construct a new df with a date range , you can just call apply on this and ` sum ` the number of rows that meet your condition .
I'm assuming you want the actual index location ( zero-based ) , you can call ` apply ` on your ' date_time ' column and call ` np.searchsorted ` to find the index location of where in ` bounds ` df it falls in : #CODE
@USER has pointed out that ` apply ` is unnecessary here and of course he's right , this will be much faster : #CODE
No need to use apply here : ``` In [ 40 ]: bounds [ ' date_start '] .searchsorted ( df [ ' date_time '])
yeah some logic are same i just want to learn how to make these simple logic work , so that I can in future easily apply different things . Thanks for replying .
You should use apply method of DataFrame API : #CODE
You can find more information about apply method here .
@USER what are you talking about ? you can just do ` df [ ' usids '] = df [ ' uids '] .apply ( set )` , there is no ` inplace ` param for ` apply ` anyway you have to assign the result
@USER apply function take so long ( 19 second ) as my list includes 30,000 uids . Isn't there a better way to enhance performance ?
` apply ` is just a ` for ` loop so this will be slow unfortunately , there isn't a ` toset ` method
How to apply different aggregation functions to different columns and give the results different names ?
Fixing the order of the DataFrame columns returned by ` apply ` :
I did change it before I apply the code , and the error come out
Thinking about it , you can remove the transpose and just use axis=1 in the apply . Glad I could help .
You could ` apply ` ` value_counts ` : #CODE
` apply ` tends to be slow , and row-wise operations slow as well , but to be honest if your frame isn't very big you might not even notice the difference .
You can create a mask of your df by calling ` apply ` and call ` value_counts ` , this will produce ` NaN ` for all rows except one , you can then call ` dropna ` column-wise and pass param ` thresh=2 ` so that there must be 2 or more non- ` NaN ` values : #CODE
You could use the apply function : #CODE
You can avoid loops by using ` apply ` #CODE
C :\ Users\user\Anaconda\lib\ site-packages \pandas\core\ groupby.pyc in apply ( self , func , * args , ** kwargs )
You may use apply with regex : #CODE
Btw check into [ pandas apply / ufunc object ] ( #URL ) . You've probably found this already though . You can actually put a numpy function into the pandas apply object . So this could do the trick
You can use ` DataFrame.apply ` with ` axis=1 ` ( to apply the func to each row ) and in that function do your logic . Example - #CODE
So if we apply this we get a Series with the indices as the c3 keys we want and the values as dictionaries , and that we can turn into a dictionary using ` .to_dict() ` : #CODE
Which is the answer I'm looking for . The problem is when I apply this to a ` DataFrame ` with a large dataset it runs slow . Very slow . Is there a better way of achieving this ?
You can apply the ` type ` function to the Series values : #CODE
but this gives ` NameError : name ' ex ' is not defined ` . These DataFrames can have a lot of rows in them so I'm also concerned that the ` apply ` function might not be very efficient .
How can I apply a function for each city or for each column in this " pivot table " ?
How to apply a function to each column of a pivot table in pandas ?
You should use ` apply ` function of DataFrame API . Demo is below : #CODE
Put std dev and mean in your table , use dictionary for it : ` some_dict = { ( ' city ' , ' date ') :[ std_dev , mean ] , .. } ` . For putting data in dataframe use apply function .
You have all necessary data for running your check by apply function .
Does the standard deviation and the rolling mean need to be appended to the original data frame for this apply to work ? I think yes bc this is applying the function by row . If yes , then how can I concatenate rolling mean and std dev ? I don't think it's as simple as adding a column because df2 is a stacked table / pivot table .
How to pass multiple arguments to the apply function
I have a method called counting that takes 2 arguments . I need to call this method using the apply() method . However when I am passing the two parameters to the apply method it is giving the following error :
I have seen the following thread python pandas : apply a function with arguments to a series . Update and I do not want to use functool.partial as I do not want to import additional classes to be able to pass parameters . #CODE
Modified the question for more clarity . dic is the column value that would come by default via the apply function . The second argument is a new argument that is being passed using logic .
` partial ` is equivalent here , lambda isn't ' better ' in any way : e.g. ` countWord = partial ( counting , strWord= ' word ')` and then ` apply ( countWord )` . And yes , your understanding is correct .
How to apply a concat function to a group by data frame using pandas ?
How to return new data frame when using a apply function on old dataframe ?
How to return new data frame when using a apply function on old dataframe ?
Currently I am not returning anything ! Which is why the output confuses me . Is it a default return with the apply function ? I want to return a new dataframe with only the rows that fulfill the elif statement . However , not even the printing works in the elif so return x in the elif doesn't work either .
This is why you are getting a dataframe of all ` None ` . I do not think you can achieve what you are trying for with ` apply ` , as ` apply() ` with axis ` 1 ` actually runs the function for every row and replaces the row with the returned value ( as you see in your case ) .
Your comparison func won't work as you've found out , you're trying to compare a scalar with an array . Anyway you can call ` apply ` and pass ` axis=1 ` to process the df row-wise . Convert the dtype to ` str ` so that you can use the vectorised ` str.contains ` with ` any ` to produce a boolean series and use this as the arg for ` np.where ` and return ' yes ' or ' no ' when ` True ` or ` False ` respectively : #CODE
You can groupby on ' Column1 ' and ` apply ` a lambda that calls ` join ` to concatenate all the string values and then if you desire construct a list object from that result : #CODE
not sure why you get the error , note that I'm using a double subscript ` [[ ]]` to create a single column df when calling ` apply ` . You could iterate over each row for that column after the groupby so just take the result of my answer and iterate over ' Column2 '
For merging two lists use ` apply ` function : #CODE
If you want to apply some function to some column of dataframe , try to use function ` apply ` function of DataFrame API . Simple demo : #CODE
By passing a dict to aggregate you can apply a different aggregation to the columns of a DataFrame .
You could ` apply ` ` pd.Series.nunique ` , and then use that to select : #CODE
I have a dataframe on which I'm doing a row by row manipulation , I'm currently using iterrows() which I know is slow , and would rather use apply() . However I'm not sure how to go about it with apply ( if at all possible ) .
You can apply string methods in a vectorized way using the ` str ` attribute of ` Series ` . To find rows in the `' page_name '` column with some string `' xxx '` you can do #CODE
I recommend using datetime64 , that is first apply ` pd.to_datetime ` on the index . If you set this as an index then you can use resample : #CODE
Attempt 2 : When I try with ` apply ` I almost get what I need : #CODE
Attempt 3 : If I to assign the result of ` apply ` to a new column : #CODE
weirdly , even with ` as_index=False ` the apply version doesn't work ... I kindof think perhaps it should work ( modulo ordering ) ...
Thanks @USER . Sorry , not sure if you saw the second part of my question . I tried using transform and apply without luck .
@USER -Reina seen them thanks , I hope this way works for you . Will have a little think if I can get this with an apply ( but this will always be more efficient ) . :)
Earlier I was trying len ( filter ( y.__contains__ , x )) for the same purpose but was not able to apply it on df
One way , is to use ` apply ` and calculate len #CODE
Typically , you're better off keeping DataFrame columns as simple types rather than lists , dicts , etc . In this particular case , you can pull out specific elements from that list using apply though with something like ` x.apply ( lambda x : x [ 1 ])` to pull the month , but Fabio's answer is better from a data organization perspective .
b.count ( a [ i ]) I apply this but received this error . AttributeError : ' Int64Index ' object has no attribute ' levels '
I'd like to ` group_by ` this table according to unique combinations of ` id ` and ` timestamp range ` . The grouping operation should ultimately produce a single ` grouped ` object that I can then apply aggregations on . For example :
you might also want to apply integer division to generate time intervals : #CODE
Hi Dima , thanks for your answer . The challenge I'm facing is creating multiple , sometimes overlapping time bins that only apply to specific ` id ` s . If you try to produce the groups from my example you'll see what I mean . As a side-note , pandas has a handy convenience function for grouping on binned values called ` pd.cut ` .
I'm thinking there must be a smarter or faster way to do this , a mask could have been useful except you can't fill down with this data as ` price2 ` for one row might be thousands of rows after the ` price2 ` for another row , and I can't find a way to turn a merge into a cross apply like one might in TSQL .
To make it more generic , compare row values on apply method .
You should actually use ` groupby ` to group based on ` name ` and ` total_year ` instead of ` apply ` ( as second step ) and in the groupby you can create the list you want . Example - #CODE
In pandas you can use apply to apply any function to either rows or columns in a DataFrame . The function can be passed with a lambda , or defined separately .
( side-remark : your example does not entirely make clear if you actually have a 2-D DataFrame or just a 1-D Series . Either way , ` apply ` can be used )
The ` .str ` accessor only works on a Series or a single column of a DataFrame ( not an entire DataFrame ) . If you want to apply this method to multiple columns of a DataFrame , you'll need to use it on each column individually in turn .
Sure - to apply the method to the ' words ' column , you could write ` df [ ' words '] .str .cat ( sep= ' , ')` ( where ` df ` is the name of your DataFrame ) .
try to use " apply " instead of " map "
Anzel : thanks a lot for the answer . Unfortunately , I get the same output with " apply " .
That is out of this question scope , but if you have mixed types both non-evaluate and evaluated , then do a map or apply with a function and perform a try / except then you should be good
Andy , thanks a lot . literal_eval did solve the problem . I guess that , as a non expert , I still fight a bit with the notion of apply vs map . After filing the nans with strings ( data [ ' organization '] = data [ ' organization '] .fillna ( ' [ ]') , both apply and map on literal_eval did the job . But when is one is one preferable to the other ?
btw , I know that map vs . apply is a complete different question , do not feel the need to reply it . I will some do some research on it .
Fantastic ! Thanks so much . Ironically , I had just been using indexed keys on a standard json import a little earlier , but hadn't thought to apply it to the pandas read :)
I would like to filter ` df1 ` keeping only the values that ARE NOT in ` df2 ` . Values to filter are expected to be as ` ( A , b )` and ` ( C , a )` tuples . So far I tried to apply the ` isin ` method : #CODE
You can also create a function to check your conditions , and apply to the dataframe : #CODE
When using ` DataFrame.apply ` if you use ` axis=0 ` it applies the condition through columns , to use ` apply ` to go through each row , you need ` axis=1 ` .
You can just set all the values that meet your criteria rather than looping over the df by calling ` apply ` so the following should work and as it's vectorised will scale better for larger datasets : #CODE
this will set all rows that meet the criteria , the problem using ` apply ` is that it's just syntactic sugar for a ` for ` loop and where possible this should be avoided where a vectorised solution exists .
I think this may be a bug in apply / map_infer , definitely worth a github issue .
It seems strange to use a lambda that returns a Series in a transform ! ( Rather than use an apply . )
I guess they use the same path , * but * tranform usually means that one value is spread on the group ( e.g. transform ( ' min ')) whereas apply means that the group can return anything . But y'know I'm not sure , that was my understanding .
to which I apply pivot_table #CODE
Bear in mind that groupby didn't really apply in your case and that it returns a ` DataFrame ` -ish object
I'm looking for method , that iterates over the rows , but apply some method only for every 20th or 30th row values
Actually I try to minimize the number of requests , cause otherwise I have the timeout issue . That's why I tried iterate over the rows , and apply the function of request only for every 20th or 60th row ( cause I have 7000 rows ) and not to speed the process by applying the time.sleep method
then you can apply ` mean() ` to the series : #CODE
Another way of doing this is to put your conversion logic in a function , and to apply this function over the column . #CODE
You're looking for the ` axis ` parameter . Many Pandas functions take this argument to apply an operation across the columns or across the rows . Use ` axis=0 ` to apply row-wise and ` axis=1 ` to apply column-wise . This operation is actually traversing the columns , so you want ` axis=1 ` .
Problem : Given the dataframe below , I'm trying to come up with the code that will apply a function to three distinct columns without having to write three separate function calls .
Then I apply the function to the particular column : #CODE
This works exactly as I want it to for that one column . However , I don't want to have to rewrite this for each of the three different " spend " columns ( 30 , 90 , 365 ) . I want to be able to write code that will generalize and apply this function to multiple columns in one pass .
The ` lambda ` will ensure that only one input parameter of your function is dangling free when it gets ` apply ` d .
@USER sure :) The lambda can only use variables that are explicitly passed to it , so you pass ` col ` and ` day ` to it . It's a lazy thing to name the lambda's parameters this way , probably this would be clearer : ` lambda var1 , var2=col , var3 =d ay : annualize_spend ( var2 , var3 , var1 )` . So you set default values for the * last * two parameters of the lambda , thereby effectively rendering it a single-input function for ` apply ` . Since these are just default values , the lambda could also work in a 2 or 3-input syntax , but ` apply ` only uses a single variable , so it must have at most 1 non-default parameter .
: Wow , thanks so much for the explanation . One more ( general ) question : why pass a lambda into the apply function instead of just the function itself ? That is why df.apply ( lambda row , col=col , day =d ay : annualize_spend ( col , day , row ) instead of just df.apply ( annualize_spend ) ? What efficiency / value is gained from utilizing lambda functionality when the function has already been created ? ( I have seen this approach taken for much simpler functions , and was curious why invoking the lambda was necessary when the function had already been created ) . Thanks again , most helpful !
@USER try it without the lambda :) You should get an error that your function expects 3 arguments , and only 1 is specified , How should your function know what ` col ` and ` day ` are ? The names in function definitions are quite arbitrary , as exemplified by my use of ` lambda row , col=col , day =d ay : ... ` , so the name of the variables in the function's definition can't help in any way ( it only helps the programmer ) . So it's simply because ` apply ` expects a single-input function , which it then applies to the variable .
Okay , I think I understand : the use of a multi-argument ` lambda ` is your way of getting around the single-argument constraint of ` apply ` . And this approach can be generalized when wanting to apply ** any ** multi-argument function with ` apply ` or ` map ` or ` applymap ` . That is , first explicitly create the multi-argument function . Then specify each argument in that function as a variable in the lambda . Finally , complete the lambda by calling the function ( along with each argument ) . Again , many thanks !
I thought about using ` iloc ` ` loc ` but I'm not very strong with this methods , so if you know how better apply them to this case , it could be solution for my problem
But when I try and use this function with apply : #CODE
I'm not sure why you have this problem with ` apply ` , but you should not write the function like you did in the first place . Here is a suggestion that avoids dividing two huge numbers one by another : #CODE
Is conversion in the ` read_csv ` is mandatory ? Otherwise , passing a function which returns ` Series ` to ` apply ` results in ` DataFrame ` . #CODE
You can construct the lists for each continent and ` apply ` a func : #CODE
if I apply ` .value_counts ` directly to ` groupby ` as #CODE
I have tried creating a new function and using ` groupby ` and ` apply ` , but this works only if rows are sorted . Also it's slow and ugly . #CODE
Not sure about efficiently but a cleaner method is to call ` apply ` and pass `' , ' , join ` as the func to call : #CODE
@USER : I tried it with column name also , but it did not help . My dataframe contains one column which consists of sentences . When i try to apply drop_duplicates() on a column containing 1 or 2 words , if works fine . But not when it come to sentences . Anything that can be done ?
@USER : I tried it with column name also , but it did not help . My dataframe contains one column which consists of sentences . When i try to apply drop_duplicates() on a column containing 1 or 2 words , or on a smaller sample of comments , if works fine . But not when it come to the entire dataset ( about 300 rows ) , it does not work . Anything that can be done ?
The problem I am running in to is ` df.loc ` is running pretty slow on large DataFrames ( 2-7 million rows ) . Is there a way to speed up this operation ? I've looked into ` eval() ` , but it doesn't seem to apply to hard-coded lists of index values like this . I have also thought about using ` pd.DataFrame.isin ` , but that misses the repeat values ( only returns a row per unique element in ` selection `) .
I want to use a function from an add-in in excel and apply it to some data i have simulated in python . Is there any modules that can achieve this ?
Unfortunately , this only runs a macro . I need to be able to call the add-in and apply my data indexes there ... something along these lines : = add-in_name ( data_range1 , data_range2 , " GGCV ")
To take this further , you could use an [ apply ] ( #URL ) combined with a function to carry out the logic to remove the loop entirely . This would make the code a lot more portable ( and hopefully efficient ) and allow the output to also be a pandas object without any conversion .
You can then apply ` np.where ` as you did to find the indices where your condition is fulfilled : #CODE
When you use ` apply ` , it calls your function once for each column , with that column as an argument . So ` x ` in your NewCols will be set to a single column . When you do ` x [ string ] = list.count ( string )` , you are adding values to that column . Since ` apply ` is called for each column , you wind up appending the values to both columns in this way .
` apply ` is not the right choice when your computation depends only on the values of a single column . Instead , use ` map ` . In this case , what you need to do is write a NewCol function that accepts a single ` Column2 ` value and returns the data for a single row . You can return this as a dict , or , handily , a dict-like object such as a ` collections.Counter ` . Then you need to wrap this new row data into a DataFrame and attach it column-wise to your existing data using ` concat ` . Here is an example : #CODE
Pandas Dataframe - faster apply ?
You should avoid ` apply ` and use ` to_datetime ` : ` df [ ' local_time '] = pd.to_datetime ( df [ ' local_time '])`
I could do it using apply and a for loop ( see below ) , but it is pretty clunky . Is there a better way ? #CODE
You can use ` apply ` and use the column's ` name ` attribute to get the key for the outer dictionary : #CODE
IIUC correctly then you use ` apply ` with a ` lambda ` : #CODE
Didn't know I can use apply and axis=1 , thanks !
Should the DataFrame be used in this way ? I know that dtype object can be ultra slow for sorting and whatnot , but I am really just using the dataframe a convenient container because the column / index notation is quite slick . If DataFrames should not be used in this way is there similar alternative ? I was looking at the Panel class but I am not sure if it is the proper solution for my application . I would hate forge ahead and apply the hack shown above to some code and then have it not supported in future releases of pandas .
You can create a list of column names and then iterate through them and apply your logic for them . Example - #CODE
Melt the data frame , then apply the repalce and to lower function . Pivot the data frame to get back
Firstly column ` week ` is set to index . Then df is grouped by column ` product ` and apply reindex by max values of index of each group . Missing values are filled by ` 0 ` . #CODE
Apply function with args in pandas
You can then apply it straightforwardly : #CODE
The strange thing is that when I apply this logic to a bigger table that im working on , I get a " True " for all boolean values , despite me having different " time " -columns and the same ID-number ? Do you know if there are any cases where the boolean expression isnt evaluated over all three included colums , so it only looks on the first column ( " id ")
IIUC then you ` groupby ` on ` level=0 ` of your index and ` apply ` a ` lambda ` to ` join ` the values : #CODE
One way would be to use ` apply ` by constructing column name for each row based on year like `' w ' + str ( x.year )` . #CODE
I am facing difficulty with the correct syntax of the if condition . I want to apply the condition to check the equality of a string . But the way i am trying to do it , is giving me an error : #CODE
simply you can apply the regex ` b ,? ` , which means replace any value of ` b ` and ` , ` found after the ` b ` if exists #CODE
Now let's write an ` apply ` function that adds a column of nearest dates to ` df1 ` using scikit-learn : #CODE
Next I apply data = ` np.asarray() ` on the DataFrame : #CODE
@USER , yes I know I'll have to specify the order manually , I'm just wondering how to apply those specified orders . In reality my dataset is much larger and has several ' questions ' with the same set of responses . For example many Questions that can be ( " Yes " , " No " , " Unsure ") , many questions that can be ( " Not at all " , " A Little " , " A Lot ") , etc . I'd like to specify the orders of these responses and then have them applied to the appropriate questions ( level 0 in the index ) . Does this make sense ?
If you have multiple conditions besides this example you can use ` apply ` : #CODE
Does color mean anything special or can we treat a combination of item id and color id as a new unique item ? Does the store_min_buy apply to just the one thing or across the sum of all the things you buy at this store ? What does it even mean that there's a min_buy of 9.14 , can I buy 9 and 14% of an item somehow ?
Thanks , that's a good hint . It works , but I can't pass the first() -function to g.agg ( ... ) , can I ? I would like that better , because I would like to apply many different aggregation functions at once ( amin , amax , first , ... ) . It will be a workaround to use it and then assemble my final dataset manually , I guess .
That's not an error , just a representation of the groupby object . You just need to apply an aggregation operation to the object to return a DataFrame or Series . There's more information about this in the docs on groupby .
Group series using mapper ( dict or key function , apply given function
Use ` Series.value_counts ` to count the number of occurrences for each city in ` US [ ' city ']` , and then use ` Series.map ` to apply those counts to corresponding values in ` UK [ ' city ']` : #CODE
I have tried using ` apply ` but it is pretty slow : #CODE
` get_dummies ` and other Categorical operations don't apply because they operate on a per row level . Not within the row .
Apply a function to each of the subsequent rows of the dataframe that will give the depreciation relative to the base-year , base-price . This should be put in a set or a list . Used group.apply()
I constructed a separate dataframe with one column as " make " , another as " model " , and a third one " average yearly depreciation " . What this really boiled down to was how to sequentially apply a function to rows of a dataframe . #CODE
Now I can apply a function : #CODE
I'm new to Pandas . I created this pivot table , but I need to figure out how to apply a function within each day on the ' is_match ' values only . See img below for head of data .
Using a function with ` apply ` is slower than the list comprehension : #CODE
Thanks ! Really much faster . But maybe it's possible to make it trough pandas apply / map function ?
You can use ` apply ` ( see updated answer above ) . But it is slower due to the function calling overhead .
Yep , added a new option above that will apply the same logic to the entire df .
This also looks promising . Can you give an example of the function and how to apply to the groupby ?
Pandas dataframe apply refer to previous row to calculate difference
The problem is not calculating the day-difference between two ` datetime ` objects . I am just not sure on how to add the new column . I know , that I have to make a ` groupby ` first ( ` df.groupby ( ' player ')`) and then use ` apply ` ( or maybe ` transform ` ? ) . However , I am stuck , because for calculating the difference , I need to refer to the previous row in the apply-function , and I don't know how to do that , if possible at all .
I suggest that you apply the function on a subset of data for example the first ` 100 ` row if it worked then increase the subset until you get the error and know which rows specifically in your data set causes the issue
use of apply function when you need to pass ' self ' as argument
python - pass dataframe column as argument in apply function
You have to ` apply ` over the other axis . #CODE
does not work . Lets assume I have 3 categories in column a , for each specific on I have 5 categories of b . What I need to do is to find total number of on class of b for each class of a . I tried apply command , but I think I do not know how to use it properly . #CODE
Or you can apply a ` lambda ` function onto the groups : #CODE
One way could have been to use pd.exanding_apply() , but it doesn't preserve the dataframe to apply the function on , so there is no way to have the correct groupyby index ..
I know how to do it using ` apply ` with python function but it's very slow : ~16s for 1M elements on a MacBookPro #CODE
I have a data frame that needs a column , ` c3 ` , added . Each entry in the column depends on entries from the same row in two other columns , ` c1 ` and ` c2 ` . ` c3 ` was originally created by mapping a function over pairs of entries in ` c1 ` and ` c2 ` . I'm trying to speed up the creation of ` c3 ` , since there is a lot of data , by using ` apply ` . Here's what I have now : #CODE
However , when I do this , ' c3 ' becomes a ` float64 ` , while I need it to be of type ` object ` to preserve ` None ` values that I have for further processing of the dataframe ( rather than having them converted to ` NaN ` , which is what happens with the given line of code , since the other values generated by the function are of type ` int `) . I know one can use ` astype ` to change the type of a column , but using it on the already-created column does not work - the ` NaN ` values remain as ` NaN ` values . Is there any way to tell ` apply ` that I want to preserve the ` None ` values ? Do I need to do something special within the lambda expression or within ` my_func ` ?
Your apply function is weird because you don't use ` x ` , instead you extract the two whole columns of your dataframe on each row .
Apply time shift on Pandas DataFrame from another column
How do I use pandas groupby function to apply a formula based on the groupby value
You could also create a special function and pass it to the groupby ` apply ` method : #CODE
Writing a named funtion and using ` apply ` works : #CODE
and after that apply #CODE
Pandas groupby apply performing slow
The bottleneck seems to be the apply function , even when I remove the for loop in the function it remains slow ( ~ 4.25s per loop ) . I am wondering if there is another way to apply the function ( without the apply command ) . I perform some other procedures on the data in this code using the agg command . This works much faster , but I don't know if its possible to perform this check ( full_coverage ) using the agg command .
and that worked on the masked sample . But since I have to apply several filters keeping the original ` H ` column for non-filtered values I'm getting confused .
If you have a ` groupby ` object , you should use the ` apply ` , ` agg ` , ` filter ` or ` transform ` methods . In your case ` apply ` is appropriate .
Now , let's ` apply ` that to each group of your real dataframe : #CODE
You apply the function ` fill_seq ` to the H / K sequence columns using the values from H / K sequence as input .
You can call ` apply ` on the df pass ` axis=1 ` to apply row-wise and use the column values to slice the str : #CODE
How to apply a function on every row on a dataframe ?
And ` ch ` and ` ck ` are float types . Now I want to apply the formula to every row on the dataframe and return it as an extra row ' Q ' . An example ( that does not work ) would be : #CODE
Here you will find arrays and methods that are much faster than built-in list . For example instead of looping trough every element in a numpy array to do some processing you can apply a numpy function directly on the array and get the results in seconds rather than hours . as an example : #CODE
the ` f ` function is more complicated that this one , of course , and I want to apply a sequence of functions to transform the data frame . There are basically string parsers to normalize , transform to unicode , remove characters , split into components , etc , so so far I can t see a way to do it without ` applymap ` .
Can you give more insight in what you want to apply ? ( the simple example you give can of course easily be done without ` applymap ` , but maybe your real function as well ? )
Length of the code . As it stands , one would need to keep adding ` & ` in order to add more columns . It would be better if it could be wrapped into , say , a list comprehension ( or apply function ) since each column is independent
Python pandas groupby object apply method adds index
I have this question is an extension after reading the " Python pandas groupby object apply method duplicates first group " .
Actually , I have data for several days , and interpolate one by one is a dificult option . Maybe using ` apply ` but I dont know how .
You can use ` apply ` on the column to generate a boolean mask describing the desired columns , and then filter the DataFrame by this mask : #CODE
What I've got so far is printing out the correct expanded dataframes for each row , but I don't know how to consolidate the results of apply : #CODE
You could , of course , do all this within a function that you apply on a ` groupby ` , but it would be superfluous in this case .
python pandas nested loop : to apply a function to each element of e.g. column 2 involving compounding same elements in previous column 1s
In fact , it seems to me , that one could apply a similar method to calculations involving n nested loops . As long as one has pre-calculated each of the n loops , and ordered them using sort , one can apply a function to the result by using groupby on the nth bin directly . In the problem above , the loop was a 2-variable nested loop , with bin being at level 1 , and port being at level 0 . This may relate to indexing ?
I feel like ` df.pct_change() ` would be helpful , but I can't figure out how to apply it in the way I'm trying to describe .
or apply the patch in the pandas code ( see the link above , it's only a one line change )
I have seen here a similar question for scatterplots , but it doesn't seem I can apply the same solution to a time series line chart .
Pandas / Scikit - Apply sparse PCA while creating feature vectors
I am planning to generate a huge sparse matrix of 70000 rows and 150000 columns using Pandas get_dummies() , however I get a MemoryError . How do I apply PCA on this sparse matrix to reduce dimensionality ?
How can I do this without using groupby and apply function ? Because I need a good performance in computation .
The normal op here is to ` groupby ` on ' item ' and call ` sum ` on the ' grade ' column no need to call ` apply ` here
Note that if you have to perform ` urljoin ` then using ` map ` or ` apply ` would be fine here
For pure string concatenation , this will be vectorised , using ` map ` and ` apply ` this is just a ` for ` loop so this approach will be much faster for large datasets
In ` R ` , can use ` na.aggregate / data.table ` to replace the ` NA ` by ` mean ` value of the group . We convert the ' data.frame ' to ' data.table ' ( ` setDT ( df )`) , grouped by ' A ' , apply the ` na.aggregate ` on ' B ' . #CODE
However I am wondering if there is a way I can write my function and apply it to my grouped object such that I can specify when applying it , which column I want to calculate for ( or both ) . Rather than have ' var1 ' written into my function , I'd like to be able to specify when applying the function .
You can apply and return both averages : #CODE
Pandas dataframe : how to apply describe() to each group and add to new columns ?
Probably a convoluted way of doing it , but You could ` groupby ` ID1 and ID2 ; then * iterate over the groups * zipping ( use ` zip `) ` x ` and ` y ` into a ` xy ` column where values are points ( x , y ) . Then , shifting this ` xy ` column by 1 ( use ` shift `) , get a new column ` xyshift ` . Then apply a difference function in the row-axis in this ` xyshift ` column ; and finally merge it iteratively into your dataframe or compose a new one
Here's a solution with apply #CODE
Use the vectorised ` str.split ` this will be much faster than using ` apply ` on a large dataset : #CODE
To extract just the month-year piece from the resulting list created by the split , apply ` map ` and a lambda to the result : #CODE
Or as @USER suggests apply ` str ` again instead of the map-lambda : #CODE
More generally , it is a case of ` rolling ` apply , ` min_periods ` control the minimal window that will be considered as valid . Skipping it in this case will result in having ` nan ` for the 1st cell : #CODE
When you do the apply , it's across each row ( which is a Series ) .
I'm guessing the slow part is the apply ( rather than the split , or the stack ) ?
@USER how would you apply the answer in your link to my question ? This is my first question , I wouldn't mind if you undid your down vote .
You can do that but to me this defeats the whole point of using pandas which provides vectorised methods , if you're going to do that then use ` apply ` to process an element
Tried so far : Re-indexing to Date_time and df2.update ( df ) with variations , multiple merging / join / concat variations , an adapted definition ( below ) with apply ... and now wondering if I need to use iterrows ( see below ? ) .
How to apply a function to rows of two pandas DataFrame
There are two pandas DataFrame , say ` dfx , dfy ` of the same shape and exactly the same column and row indices . I want to apply a function to the corresponding rows of these two DataFrame .
By the following code , I make a grouped two-level indexed DataFrame . Then I do not know how to apply ` agg ` in the proper way . #CODE
However , it occured to me this may not be always correct , since there is no guarantee data was collected everyday . Instead , I tried counting unique days in the timestamp series using ` map ` and ` apply ` , and both take a considerable amount of time for 3,000,000 rows : #CODE
@USER do you mean doing the apply ?
This is very surprising , mainly because the accepted answer is O ( n^2 ) . I can see the apply part being slow , but as I say I don't think you really need that part .
you've got a couple of problems here . the first involves repopulating your dataframe from a subset . the other , more computational problem involved not ` for ` loops , but ` groupby ` and ` transform ` ( or ` apply ` ? ) operations . I recommend splitting this up into two separate questions .
Consider using groupby apply functions to dataset . The first function averages the values only for ' Print Buffer ' using ` mean() ` , leaving the others in Block zero . And then the second function maximizes the ` meanvalue ` . Finally , simply create ` newvalue ` as arithmetic difference : #CODE
UPDATE : Here is a photo to illustrate what I keep geting ! Box 6 is the implementation of apply , and box 7 is what my data looks like .
You could use apply method for pd.Series of dataframe
So you could convert your ` df.columns ` to ` pd.Series ` with ` pd.Series ( df.columns )` and then use apply method . Look to the last edit
Python Pandas Dataframe Columns of Lists , Get Intersection And Apply Function To Another Column
I'm not used to working with ` lists ` in columns of Pandas and don't know how to get the intersection of ` lists ` from two columns in a ` dataframe ` , then get the index of where the words appear , then apply plus signs to the front of each found index . Or maybe easier would be a string replacement on ` df [ ' Keyword ']` using the words from ` StemmedAG ` ?
How to do this ? Do we have any function ? I tried with apply and groupby function , did not work . Please let me know if more information is required for this .
The thing is , I know that I can set it to the Boolean false using str.contains() . But I don't know what the proper way to apply str.contains() to pd.read_csv ' s na_values . #CODE
The first step is to read the data into a Pandas DataFrame . If the format of the dataframe is consistent with the example you gave above , then you can apply the function to the groupby object . It seems some error ( s ) come out of the reading procedure .
Another method using ` apply ` : #CODE
I am able to add a new column in Panda by defining user function and then using apply . However , I want to do this using lambda ; is there a way around ?
Next solution is with function apply with parameter ` axis=1 ` :
axis = 1 or columns : apply function to each row
This avoids the apply so will be more efficient .
Thanks Andy , I will post a new question . I know there is a group by rank function but the challenge so far is to combine that with logical operator of a different column and then apply to all rows . I really appreciate your help .
call ` apply ` and pass func ` len ` : #CODE
You can try ` apply ( pandas.Series.interpolate )` instead of ` fillna ` .
@USER first transpose your dataframe , then apply my solution : #URL
Python pandas groupby transform / apply function operating on multiple columns
Trying to use apply-split-combine pandas transform . With the twist that the apply function needs to operate on multiple columns . It seems I can't get it to work using ` pd.transform ` and have to go indirect via ` pd.apply ` . There a way to do #CODE
I get a factor of ~5 speed improvement on a simple N= 10,000 test case by using a pandas groupby / apply and writing the csv from the resulting dataframe : #CODE
You can use apply with ` to_json ` : #CODE
To apply functions to this object , you can do a few things :
If you want to pass a custom function , you can call ` grouped.apply ( func )` to apply that function to each group .
The last question about the average over a range of columns relies on pandas understanding of how it should apply functions . If you take a dataframe and call ` dataframe.mean() ` , pandas returns the mean of each column . There's a default argument in ` mean() ` that is ` axis=0 ` . If you change that to ` axis=1 ` , pandas will instead take the mean of each row .
However , when I apply this in IPython , it gives me this error : #CODE
This is because you are operating a element ` x ` against series ` va2 [ ' pct_vote ']` . What you need is operation on ` va2 [ ' winner ']` and ` va2 [ ' pct_vote ']` element wise . You could use ` apply ` to achieve that .
You could use Pandas ` apply ` function , which allows you to traverse rows or columns and apply your own function to them .
and apply it column-wise : #CODE
We can group by the User Id , and then for each group apply a function to evaluate the difference between the rows .
Now define a function that will operate on each group , and apply it .
Apply a numeric rank column basis the datetime
You can call ` apply ` and convert your dict values into a set can convert the ` intersection ` to a list : #CODE
Running to_numeric via apply on the dataframe iloc selection , ie #CODE
So your approach of using ` apply ` is the correct one if you want to use this functions on several columns at the same time .
You can apply difference of sets converted from lists of columns and then convert to list . You have to use ` axis=1 ` , because apply function to each row . #CODE
More efficient way to import and apply a function to text data in a Pandas Dataframe
The code runs fine when parsing a short paragraph , but when working on larger text files the code takes a lot longer . I know the key to speed when working with Dataframes is to avoid for-loops and to apply functions to the whole data set .
My question is , is there a quicker way to apply a function to a string when reading in a text file , other than line by line and appending it to a dataframe ?
Right . The problem is the way you save it . You apply jsonification to it twice . You should do something like ` with open ( myoutfilename , ' w ') as f :\ n f.write ( dataframe.to_json() )`
I am trying to apply a function to each row in a dataframe . The problem is , the function requires output from the previous row as an input .
It looks like you want to apply a recursive function . In that case , .rolling_apply won't work . One way would be to use the series values as a list or numpy array . Then loop through the list to use the recursive function .
Try grouping certain columns and then apply sum() like this : #CODE
` Series.str.replace ` Cythonizes the calls to ` re.sub ` ( which makes it faster than what you could achieve using ` apply ` since ` apply ` uses a Python loop . )
try using apply function . #CODE
Pandas apply but only for rows where a condition is met
I have a dataframe that I created from a text file . Columns B-F should apply to all null fields below them , then once all nulls are filled the next set of periods should be filled by the next values populated in B-F . How would I go about accomplishing this ?
But what is the common approach to this problem . Is this where people apply normalization ? It would be great if someone could explain how to apply normalization in such a situation .
Group series using mapper ( dict or key function , apply given function to group , return result as series ) or by a series of columns
As a workaround for now , you can easily use the ` nunique ` Series method through ` apply ` instead of calling it directly on the groupby object : #CODE
OK I missed the ` NaN values are maintained ` part . This is not pretty and it is still slow ( but faster than apply ):
But that is different from what you will get using the apply method . Here your mask has float dtype containing NaN , 0.0 and 1.0 . In the apply solution you get ` object ` dtype with NaN , False , and True .
Neither are OK to be used as a mask because you might not get what you want . IEEE says that any NaN comparison must yield False and the apply method is implicitly violates that by returning NaN !
but after that one can apply ` .reset_index ( drop=True )` ( mind drop=True here ) and that returns #CODE
Just the usual apply warning , this can be slow ( that's why we special cased groupby head to use cumcount under the hood , at least originally not 100% sure if it still does ) . :)
as_type has casting rules , none of which seems to apply ( you'd think it'd be casting= ' safe ' #URL )
You group on ` user.id ` , and then use ` agg ` to apply a custom aggregation function to each column . In this case , we use a ` lambda ` expression and then use ` iloc ` to take the last member of each group . We then use ` count ` on the text column . #CODE
In the current implementation apply calls func twice on the first group to decide whether it can take a fast or slow code path . This can lead to unexpected behavior if func has side-effects , as they will take effect twice for the first group .
Pandas Apply Function with Multiple ** Kwarg Arguments
If I understand your question , it seems to me that the easiest solution would be to pick the columns from your dataframe first , then apply a function that concatenates all columns . This is just as dynamic , but a lot cleaner , in my opinion .
Probably not ideal , but this can be done using ` groupby ` and apply a function which returns the expanded DataFrame for each row ( here the time difference is assumed to be fixed at 2.0 ): #CODE
So you could use apply and pd.to_numeric methods : #CODE
Your solution is so elegant , it stimulates me to learn deeper in groupby and apply methods .
here ` apply ` will call ` nunique ` on each column
Apply a value to max values in a groupby
The idea is to write an anonymous function that operates on each of your groups and feed this to your groupby using ` apply ` : #CODE
Assuming that your ' Time ' column is already a ` datetime64 ` then you want to ` groupby ` on ' ID ' column and then call ` transform ` to apply a lambda to create a series with an index aligned with your original df : #CODE
apply a function to a dataframe column ( datetime.date )
I'm trying to apply this formula on a dataframe column ' Days ' ( datetime.date type ): #CODE
function won't apply to pandas data frame , getting syntax error
I'm trying to apply this function to a pandas data frame in order to see if a taxi pickup or dropoff time falls within the range that I created using the arrivemin , arrive max variable below .
Let's say forecast is the function I have created that I want to apply : #CODE
But here we are where my problem is ... How can I apply this function to the dataframes ?
I have tried the apply function as follows : #CODE
The ` apply ` function takes in a function and its args . The documentation is here .
How do I apply datetime.date() and datetime.time() to the whole series
Whether the speed of this operation is important is unknown -- it probably isn't at all . It's certainly the nice , normal , readable thing to use serieswise operations rather than apply in these sorts of cases , though .
So is the issue that I'm creating lists in my dataframe or that I'm applying a function to adjacent rows ? I just used the list function arbitrarily in this example and am more interested generally in how to apply a function to adjacent rows .
You can write your own function that accepts a subdataframe in the groupby ... what function do you want to apply ? Or is the question how to groupby adjacency ?
I've done some debugging and everytime I get this error is when apply duplicates the first group .
So are there any better ways to do it without using apply ?
Consider simply creating an absolute value column through a defined function , apply the function on a groupby , and then sorting item ascending and absolute value descending . Finally , filter out the newly created , unneeded column : #CODE
Actually , when i apply it to some dataFrame where some entries are nan , and some with unequal length , I have this error : Unalignable boolean Series key provided
It's not really a bug , and it's just when you use ` apply ` . Did you read the documentation referenced in the github function ?
Can you make a more demonstrative example ? You're not really even using the data frame on which you're calling ` apply ` other than the needless print statement . Seems like omitted that would provide the desired results .
I can imagine a use case would be : generating a column representing a state , whose value changes according to both the previous state and the other current row values . This is typically a case the Pandas doc warns about when using the apply method ( " side-effect ") ...
` apply ` works fine for that .
I think you're missing the fundamentals of apply , when passed the Series ` clasif ` , your function should do something with ` clasif ` ( at the moment , the function body makes no mention of it ) .
You have to pass the function to ` apply ` . #CODE
for example , i'm looping through a dataset . after the first loop , i get a subset of my dataframe ( rows 1-10 for example ) . for the next loop , i want it to start iterating on ` index = 11 ` , and then apply whatever alg i have . the thing is , i do it by truncating the dataframe . so if the last item in the previous run was at ` index=10 ` i truncate the dataframe to ` df = df [ 11 :] `
I am trying to iterate over groups ( produced by group.by in Pandas ) in order to apply a function ( create a chart in MatPlotLib ) and get a result for each group in the DataFrame . I thought to do something like this , but I know there's a better / functional way : #CODE
The result of the groupby function is a pandas data frame or series . You can use the apply function . See below example : #CODE
You can filter the columns first to get the cols of interest and then call ` apply ` and use the boolean mask to mask the cols : #CODE
You can apply function ` f ` for each group .
the answer was to resample so I won't have any hole , and then apply the answer for this question : How do you shift Pandas DataFrame with a multiindex ?
Apply Number formatting to Pandas HTML CSS Styling
Given the following , how can I set the NaN / None value of the B row based on the other rows ? Should I use apply ? #CODE
Apply is the way forward , it seems . ` fillna ` doesn't appear to accept custom functions . [ Reference SO question ] ( #URL )
Then I use apply : #CODE
dropped vectorization and list-comprehension tags - they don't really apply here
I said , let me use a lambda to apply the .hour to every " row " . Thus : #CODE
not sure how to apply this suggestion to the problem above . First of all - does your suggestion select the columns in the hypothetical array you suggest ? Or would I do array [: , mask ] which doesn't seem to work ?
Can I use the apply method to this task ?, can someone help me to do in the right way using pandas .
Apply group specific function to groups in Pandas
I'm trying to figure out the best way to apply a function to groups within a Pandas dataframe where the function depends on the group .
This question manages the result for a single column , but I have an arbitrary number of columns , and I want to lag all of them . I can use ` groupby ` and ` apply ` , but ` apply ` runs the ` shift ` function over each column independently , and it doesn't seem to like receiving an ` [ nrow , 2 ]` shaped dataframe in return . Is there perhaps a function like ` apply ` that acts on the whole group sub-frame ? Or is there a better way to do this ?
Great , thanks , I can't remember why I thought I needed to do it with ` apply ` - maybe it'll come to me later .
Part of the solution , because you'll have duplicated rows with slightly different names so you couldn't apply drop_duplicates method of dataframes : #CODE
Within each iteration , apply the function .
As a general idea , you can use ` apply ` on your grouped data to take compute the ratio for each state : ` sum ( bads ) / sum ( goods )`
The easiest way would probably be to set ' Date and Time ' as the index and then use ` groupby ` with ` TimeGrouper ` to group the dates . Then you can apply ` cumsum() ` : #CODE
In reality I need to apply this function on 40K over rows . And currently it runs
very slow using Pandas ' apply ' : #CODE
Without trying this I can't be entirely sure that the returned value of ` func ` will be acceptable for use with ` apply ` , so you might need to play around with that a little . But this should give you a series with the index being the description and the value being a list or dict of the last five counts .
After creating new dataframe by ` concat ` dataframes I can group it by row and apply ` resample ` on each of these groups ( with method ` ffill ` to forward fill ) . #CODE
numpy and pandas are not needed here although you need to apply the strip function to every element in each row to remove excess spaces ( ` map ( str.strip , row )`) and also pass ` delimiter= ' | '` into ` csv.reader ` because the default delimiter is a comma . Lastly you need to ` return sum ` at the end of you function .
Thanks for the great solution . Is your use of ` apply ` a recommended method to access pandas functionality that isn't present in dask ( assuming the block can fit into memory ) ? This would be a huge help to by-pass the current limitations of dask dataframes !
For the record ` applymap ` is used to apply a lambda function elementwise ( documentation )
You could use standard method of strings ` isnumeric ` and apply it to each value in your ` id ` column : #CODE
I then would like to apply a function to each of these groups . This function computes two values for each group #CODE
I am having some issues understanding the type of your ` group ` argument to ` compute_thing() ` . Shouldn't apply iterate over the results of the grouping , and the ` group ` argument refers to the current group in consideration ?
I am familiar with the page . but I dont know how i can apply this to my data and plot , as they are using only one variable with an arithmetic operation , while I have a dataset with different categories .
so you can later apply : #CODE
Although technically correct we should avoid posting answers that use ` apply ` where a vectorised solution exists as this confuses users
Figured it out ! Apparently I had some missing values denoted as ' .. ' , so I had to wrangle it out first by dropping those rows - then I can apply .astype
Is there some way i can apply a lambda function to all members of the list columns in order to speed the following up ? Thanks ! #CODE
All you're doing is skipping rows that have ` ? ` so you can just filter these out using ` apply ` : #CODE
Is there a way to apply a ` math ` function to a whole column ?
You could use apply : #CODE
Index levels doubled when using groupby / apply on a multiindexed dataframe
I have a problem when using a groupby / apply chain on multiindexed pandas data frames : The resulting data frame contains the grouped level ( s ) twice !
Is this intended behavior ? How can I avoid that another index level is created ? Do I have to remove it by hand every time I do a groupby / apply operation ?
My actual function that I apply looks different . I just used the sum here to show the effect of a function that takes a dataframe and returns a dataframe ( " case 2 " in the documentation of apply ) .
It seems that you would have quite a few columns to aggregate - assume ' date ' is your timestamp , there seem to be seven , ie , data1 - data7 . If you apply three aggregation functions to these seven columns ( mean , min , max ) you'll get 7 x 3 columns with a hierarchical ` MultiIndex ` ( where ` .agg ( dict )` works differently as for ' ordinary ' columns ) . Example follows , including saving to csv at the end . ` GroupBy ` docs and ` to_csv ` docs .
Thank you so much :) it prints the output in the way you've shown . AttributeError : Cannot access callable attribute ' to_csv ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method But I still get this error while printing to csv file
Then you can use ` apply ` ( or there probably is something better ) , to get the output you have above : #CODE
The simplest way would be to use DeepSpace answer . However , if you really want to use an anonymous function you can use apply : #CODE
In xlswriter , once a format is defined , how can you apply it to a range and not to the whole column or the whole row ?
In xlswriter , once a format is defined , how can you apply it to a range and not to the whole column or the whole row ?
There isn't a helper function to do this . You will need to loop over the range and apply the data and formatting to each cell .
I've written some code to clean up the tweet for some machine learning applications and I would like to apply the cleaning function to the entire Tweet column .
If this is the reason , you would need to apply ` .fillna ( value )` prior to ` .groupby() ` with a value of your choice - for instance 0 .
it just returns a df with all NaN values . With rolling apply is the window size time based ? Is there no other ways to reference the value in a previous row .
I get this error : TypeError : unsupported operand type ( s ) for + : ' numpy.int64 ' and ' str ' . This is how I apply your code : featuresA = [ col + ' _x ' for col in group.to_frame() .columns ]
Scikit learn's MultiLabelBinarizer creates a binary matrix from labels . You can extract ` feature ` column from pandas dataframe and apply it : #CODE
Another approach which might be cleaner if you have a lot of conditions to apply would to be to chain your filters together with reduce or a loop : #CODE
I completely change your solution to ` groupby ` with ` apply ` custom function ` f ` . For check string values is better use ` isin ` .
I would like to apply a boolean ` mask ` in function of the name of the column . I know that it is easy for values : #CODE
I am sure that could be do with aggregates ( lambdas func ) or apply .
How to apply drop_duplicates to grouped dataframe ?
You could once iterate over the array and get the colours corresponding to each value and store them in an NxMx3 ( image ) array . Then sort the array and the image in the same manner e.g. get the sort indices from the original array and apply them to the image array . Then you can display the image with ` plt.imshow `
I found an issue about a similar error when using the apply method and that bug was fixed . Since they fixed for apply I used the apply function to do what I want . #CODE
I used the apply method on the grouped data , and the easiest way to get the ' identifier ' was to get the groups keys . If you have a suggestion on how to do this more efficiently let me know !
tz-aware datetime series producing UTC-based .date() output in pandas series apply ( lambda ) operation
Pandas apply function - comparing each row to entire column
The ` groupby ` function makes sense , but I'm having trouble defining the appropriate apply function . I've seen examples using multiple columns as inputs to a function , and others using ` axis=1 ` to look at individual rows , but haven't seen a description of using both the entire column ( to look for matches ) and the entry for the row in question ( to determine the time range ) .
Not sure if I understand what you looking for . But you can access a column inside an ` apply ` function . For example , this calculates how many rows inside a group have times less than 12 : #CODE
This functionality makes sense - you can use an apply to compare each element to a fixed value ( in your example , ` x < 12 ` and ` 6 < x < 12 `) . What I am looking for would need to , for each element , compare to all the other elements and return the number matching the closeness criterion .
To continue - I think it is O ( n^2 ) , as it needs to compare each value to all other values . I had hoped using a grouping criterion that produces a greater number of smaller segments to apply the function to would speed up the calculation , but this doesn't seem to be the case in practice . Any idea whether this is a fundamental misunderstanding , or just issues with the implementation ?
To get around this , I found that you can apply a date function to the column in sqlalchemy , similar to this issue .
Use ` apply ` and pass ` axis=1 ` to call ` describe ` row-wise : #CODE
But when I apply the code , I get the following Error : #CODE
However , when I try to apply that function in order to write the output to a new column in a pandas data frame , it's returning None . See below : #CODE
formatter function to apply to columns elements if they are floats , default None . The result of this function must be a unicode string .
One option is to use an apply : #CODE
My understanding is that this just uses the keys to perform the lookup , same as if you pass a ` Series ` and similarly it will bork and generate a ` KeyError ` if the label / key doesn't exist , in that case doing ` apply ( lambda x : other_dict.get ( x , other_val ))` would at least not go mental if the key doesn't exist .
I want to apply a function to groups of ` x1 ` based on the columns in ` x2 ` . e.g. : #CODE
I have a big dataframe , and I'm grouping by one to n columns , and want to apply a function on these groups across two columns ( e.g. foo and bar ) .
But ` transform ` apparently isn't able to combine multiple columns together because it looks at each column separately ( unlike apply ) . What is the next best alternative in terms of speed / elegance ? e.g. I could use ` apply ` and then create ` df [ ' new_col ']` by using ` pd.match ` , but that would necessitate matching over sometimes multiple groupby columns ( col1 and col2 ) which seems really hacky / would take a fair amount of code .
P.S. : Guess need to use the groupby function , but because of lack of experience , do not understand how to apply it to my problem .
I've also done my Google searches but I do not feel found results apply to my issue .
using the ` apply ` method of the DataFrame , with something like :
Actually apply produces the ` < built-in method values of dict object at 0x00 ... ` output . So apply() may not be good for this particular transformation .
I also tend to not use apply but it can be convenient ( for readability or when using certain functions for example ) . Anyway in my test ( on python 3.4 / pandas 0.17 ) the following statment ``` df [ ' word count '] = df.apply ( lambda x : dict ( Counter ( x [ ' test '] .split ( " ")) .items() ) , axis=1 )``` make a new column with the word count ( not sure why you use join * and * split on your text ) . You can also use list comprehension to avoid apply , like ``` df [ ' word count '] = [ dict ( Counter ( i [ 1 ] [ ' text '] .split ( " ")) .items() ) for i in df.iterrows() ]```
You are right , join isn't needed . i see now Counter works fine on split words without needing commas to separate them . List comprehension works perfectly . Apply still outputs ` < built-in method values of dict object at 0x00 ... ` Quick timer test shows apply is bit faster than direct method : ** apply method ** ` 1 loops , best of 3 : 14.6 s per loop ` ** direct method ** ` 1 loops , best of 3 : 18.2 s per loop ` . My data has ~60k rows and mean of 314 words per row .
Type of ` df ` isn't ` dataframe ` , but ` TextFileReader ` . I think you need concat all chunks to dataframe by function ` concat ` and then apply function : #CODE
You can use the ` apply ` and ` applymap ` from pandas .
But I don't how to apply that to my code either . Please help .
( Note : this assumes the rows we need to fill look like the ones in your example . If they're messier we'd have to do a little more work , but the same techniques will apply . )
Haven't benched this , @USER , but I think the numpy approach should be pretty quick too . Especially because Pandas ` .apply() ` now runs through the first apply twice , to find out if it can take a shortcut approach . I don't know the details of under-the-hood working , but it looks similar to Numba's approach at speeding things up : run once to figure out what's happening , speed up , if possible . This is a new thing I found in their [ docs here ] ( #URL ) . Read the warning towards the end of the subsection .
It's been doing that for as long a I can remember ( a few years ( ! ) at least ) :) The apply still happens in python so if you have a lot of small groups the dummy_column + groupby sum will blow apply out of the water . Numba is a game changer however , if it were using numba it might be different ... ( Edit : [ the warning is new though ] ( #URL ) . )
@USER - I am new to pandas.I am guessing from you explanation and needing to apply some some map reduce type of thing is going on ? Is my assumption correct ?
If I use groupby apply , everything works fine : #CODE
Thanks for the links , @USER , unfortunately this is what I was saying in my intro , those approaches work like a charm when you have non complex data such as int or float , but in the case of vectors inside the dataframe , things get pretty messy . I tried those ideas and failed , so maybe I am just missing how to apply them in my situation :(
Then you can apply the numpy / scipy vectorised methods for computing cosine similarity as in Whats the fastest way in Python to calculate cosine similarity given sparse matrix data ?
It's because ` apply ` method works for column by default , change ` axis ` to 1 if you'd like through rows :
0 or index : apply function to each column
1 or columns : apply function to each row
Like indicated by Anton you should execute the apply function with ` axis=1 ` parameter . However it is not necessary to then loop through the rows as you did in the function test , since
the ` apply ` documentation mentions :
How to apply a function to two columns of Pandas dataframe
If you're using ` apply ` , the speed difference is minimal ; you should feel free to use ` iteritems ` .
Thanks a lot ! If I am focusing on a single dataframe , and want to use function to do calculations to each column , is iteritems() also better than apply ( lambda x ) ?
@USER - they are very similar from a pandas POV I think - it's just whatever you think is cleaner . For me , ` apply ` is cleaner , but you don't get the name .
to apply various aggregation functions as described in the docs . If you provide some detail on what you'd like to aggregate and how , happy to add an example .
Generally speaking , you'd apply an " easing function " over some range .
I want to apply a function to every group in a ` groupby ` object , so that the function operates on multiple columns of each group , and returns a 1 x n " row vector " as result . I want the n entries of these row vectors to form the contents of n new columns in the resulting dataframe .
What did you try ? Attach part of your dataframe . You could use ` apply ` with ` rsplit ` .
You are probably right . But I thought ` map ` method access directrly to each value instead of ` apply ` which for whole Series . If so it should access as Pandas ` Timedelta ` . Is it correct ?
` apply ` would fail here also , ` apply ` is also a ` for ` loop it just allows you operate either column-wise or row-wise when called on a df , the type conversion is being doing by pandas here
but this will filter the data frame several times , one value at a time , and not apply all filters at the same time . Is there a way to do it programmatically ?
if used apply i will be how to implement them thx ; ^^
Consider a series sum function and apply it to a ` groupby() ` : #CODE
Previously ` train_y ` was a Series , now it's numpy array ( it is a column-vector ) . If I apply ` train_y.ravel() ` , then it becomes a row vector and no error message appears , through the prediction step takes very long time ( actually it never finishes ... ) .
A custom function should apply to a series of the dataframe , a boolean operator for example : #CODE
I currently want to apply several machine learning models on this data . With some models , it is necessary to do normalization to get better result . For example , converting categorical variable into dummy / indicator variables . Indeed , pandas has a function called get_dummies for that purpose . However , this function returns the result depending on the data . So if I call get_dummies on training data , then call it again on test data , columns achieved in two cases can be different because a categorical column in test data can contains just a sub-set / different set of possible values compared to possible values in training data .
@USER -so one more thing , can i apply chunksize filtering ( taking a chunk of dataframe like in the case of pandas dataframe , then doing some Op . then merging them -- sending back ) on dask dataframe ???
would give you the mean for the second row anyway , to operate row-wise you can use ` apply ` and pass a ` lambda ` : #CODE
First I started by using ` pd.rank() ` on the data and then I planned on then using ` pd.cut() ` to cut the data into bins , but it does not seem like this accepts top N% , rather it accepts explicit bin edges . Is there an easy way to do this in pandas , or do I need to create a lambda / apply function which calculates which bin each of the ranked items should be placed in .
Not quite . When I apply this to my data set it says there are 419 posts in the top 0-5 % percentile , when in actuality in my data set of 1674 samples , there should only be 84 samples within the top 5%
You can use ` apply ` and apply a lambda row-wise : #CODE
You can use ` apply ` with a ` lambda ` to return the name of the column , here we compare the value row-wise against the max , this produces a boolean mask we can use to mask the columns : #CODE
which difference ? sorry but your are asking a lot of questions in a single question ! which is not SO working ... I already provided two different answer solving the original problem ( with ` apply ` and a ` vectorized ` one )
You can use double square brackets to force ` apply ` to be called on a ` df ` , this allows you operate row-wise , then use a user defined func to compare the current row value against all row values prior to current row , this generates a boolean mask to select the invalid rows and assign ` NaN ` to these and then ` ffill ` : #CODE
which I read as for each ` x ` in the series ` df1.var1 ` , apply the function ` np.percentile ( df2.var1 , x )` , which finds the percentile of ` x ` in the series ` df2.var1 ` . For some reason , I'm getting the error #CODE
This allows me to control the layout , but I can't apply it to bar charts .
I'm assuming you want to use all ` [ ' W ' , ' X ' , ' Y ' , ' Z ']` ` columns ` , and only one of the ` date ` columns . If so , the below should get you there - if you first apply ` set_index ` and then ` unstack ` , ` pandas ` creates the ` MultiIndex ` automatically , which you can then ` swap ` by ` level ` and ` sort ` as you wish : #CODE
How to apply a expanding window formula that restarts with change in date in Pandas dataframe ?
So in your example , do ` df.set_index ( ' Date Time ')` and then ` groupby ` and ` apply ` . You can of course assign the result back to the original ` DataFrame ` .
You can use ` apply ` together with a lambda expression to check for the target word in each column . Then use ` any ( axis=1 )` to locate any row containing that word . Finally , use boolean indexing with a tilda ( ` ~ `) to locate all rows where income is NOT in the row . #CODE
Without seeing the data and assuming you want the resultant prediction as a column in the second data ( df2 ) frame you can apply the kn.predict() using the .apply() function and specifying the vertical axis . This will give you an additional column with the predicted output .
Heres the info on apply .
` shifted = data.sign() ! = data.sign() .shift() ` should work rather than use ` apply `
HINT : Use the Theano flag ' exception_verbosity=high ' for a debugprint and storage map footprint of this apply node .
You have to use apply . Here's a toy example : #CODE
why I need to apply nth to the whole grouped dataframe
Because you need first apply function ` nth ` for all group and then get first rows of group . I try it in second approach .
It is together ` df.groupby ([ ' a ' , ' b ']) [ ' c ']` and then apply function ` nth ` . Not for all group ` df.groupby ([ ' a ' , ' b '])` .
@USER : Your first block isn't applicable to my question as the group index is not present ( I want to maintain this ) . In your 2nd example , I don't understand why I can't just do ` g [ ' c '] .nth ( 0 )` and why I need to apply ` nth ` to the whole grouped dataframe and _then_ select ` c ` .
If you want to apply it to all columns , do ` df [ df 0 ]` with ` dropna() ` : #CODE
If you know what columns to apply it to , then do for only those cols with ` df [ df [ cols ] 0 ]` : #CODE
access previous rows in python dataframe apply method
For this dataset if I delete the first 3 rows the fit is greater than 0.995 , I have tested this but I want this to be a general expression so I can apply it to other datasets .
` apply ` a lambda to access the last element : #CODE
I have two pandas tables , ` d ` and ` num_original_introns ` . They are both indexed with the same non-numeric index . I want to apply a step function to transform ` d ` based on values in ` d ` and ` num_original_introns ` , like so : #CODE
I know that this is invalid , and it is not possible to apply a pair of conditionals like this , but I can't seem to find an alternative from googling . How can I do this ?
Why you couldn't use ` apply ` with ` axis=1 ` then ?
IIUC you could use ` apply ` with ` axis=1 ` and ` fillna ` with your custom function : #CODE
If you want to get column names for missing values you could apply or use that function for processing : #CODE
When looking online , I tend to see examples of ' hardcoded ' variables , but I don't get how to apply this to a dataframe column - I found that I should use strptime to identify what format my date column is , but I don't know if this has any effect ( I get the same error if I comment out the convert_dates apply method ) .
python pandas- apply function with two arguments to columns
Continuing off of part 1 , you can merge the values back on to the original dataframe . At that point , you can write a custom function to subtract your date strings and then apply it to each row .
You can ` groupby ` dataframe by column ` Name ` , ` apply ` custom function ` f ` and then select dataframes ` df_A ` and ` df_B ` : #CODE
I have a dataframe and I'd like to apply a function to each 2 columns ( or 3 , it's variable ) .
For example with the following ` DataFrame ` , I'd like to apply the mean function to columns 0-1 , 2-3 , 4-5 , .... 28-29 #CODE
Then I push all but 1 column into the index with set_index . This leaves one column which comes back as a Series . Then use apply and return a series indexed on the expanded set of dates for each row ( Series of Series = DataFrame ) . So for each of the 7 rows in the DataFrame , I get a series indexed on the expanded date range . Then its just clever stacking , naming , and reset_index . #CODE
You can ` apply ` function to ` groupby ` where use another ` apply ` with ` replace ` ` 0 ` to ` NaN ` : #CODE
You can use this apply function : #CODE
Although this does not use explicit ` for-loop ` s or a list comprehension , there is an implicit for-loop hidden in the call to ` apply ` . In fact , it is much slower than using a list comprehension : #CODE
Yes and no -- there is a way to do it using ` apply / stack ` which avoids the * explicit * double for-loops , but it is actually much slower than the list comprehension-based solution shown above . So if you are trying to avoid ` for-loop ` s for performance , then I don't think there is a good way . You see , when you put non-native NumPy data types , such as lists , in a DataFrame , ultimately computations on those values require plain Python methods which are relatively slow ( compared to native NumPy methods ) . To break apart the items in the lists require plain Python loops no matter how you phrase it .
Do I then save that as a function and apply it to the dataframe or could I just run that on it's own and have it append the column to the original df ?
` apply ( F )` calles ` _python_apply_general ` . As the name implies , it is ageneral propose method . Under the hood , it does not attempt checking if a faster ` cython ` version of aggerate function exists . It applies ` F ` to each group and assembles the results together , which means it would run slower than the optimized ` cython ` version equivalent ( such as ` .sum `) .
Finally , ` apply ( lambda x : F ( x ))` will be slightly slower than ` apply ( F )` due to the additional ` lambda ` function .
In Pandas , how to apply a customized function using Group mean on Groupby Object
I want to create groups based on value of column A . So I slice A first . And define a function . Then I use apply method on the Groupby Obj . I am expecting the new column to be the difference between B and C over the group mean of A . #CODE
I want to apply a group by on a pandas dataframe . I want to group by three columns and calculate their count . I used the following code #CODE
I think native apply is the best , but not . I found faster approach : #CODE
You can use apply with lambda with is faster than your solution : #CODE
You could use ` functools.reduce ` to iteratively apply ` pd.merge ` to each of the DataFrames : #CODE
you don't even need to use ` apply ` : ` df [[ ' AccX ' , ' AccY ']] .values `
But if I want to do some operation on each row , I still need ` apply ` correct ? Also I kind of just want to know why the first three don't work . My whole data frame has 11 columns
This code gives me an attribute error when I apply the seasonal_decompose method :
Creating Period objects is expensive , so let's identify the unique quarters and then apply the period mapping . #CODE
Consider a groupby apply function with sort : #CODE
This works fine for some files but for some other files it raises the error : ` ValueError : could not convert string to float ` . Which naturally makes me think there is something wrong with the file . But , when I try to loop sequentially over the data and apply the same conversion it doesn't give any error . So I cannot figure out where the problem in the file is or what's the problem with the converter .
I can't seem to apply ` to_datetime ` to a pandas dataframe column , although I've done it dozens of times in the past . The following code tells me that any random value in the " Date Time " column is a string , after I try to convert it to a timestamp . The `' errors=coerce '` should convert any parsing errors to `' NaT '` , but instead I still have `' 2015-10-10 12:31 : 04 '` as a string . #CODE
I have an ` apply ` function that operates on each row in my dataframe . The result of that ` apply ` function is a new value . This new value is intended to go in a new column for that row .
If you need to use other arguments , you can pass them to the ` apply ` function , but sometimes it's easier ( for me ) to just use a lambda : #CODE
I should've mentioned this before : my function has two arguments , a row from the ` dataframe ` , and a global dictionary . I tried incorporating these like so : ` df [ ' new_column '] = df.apply ( my_fxn ( row ) , args =( ) , axis=1 )` but it seems to be breaking the global_dictionary up into individual individual K / V pairs . As a result it tells me that there are too many arguments . How do I pass arguments to the ` apply ` function ?
I'm hoping the solution will detect that there is no existing COL3= ' Y ' for COL1= ' B ' and therefore add the row while setting COL2 to 0 for the new row . The code should get the set of unique values of COL3 , check to see if all exist for all unique values of COL1 , and if not , add the row . It doesn't get more complex than this , I was only trying to get an answer that I can apply to many rows instead of just manually inserting that specific row .
There must be a way to use pandas / numpy array functions but tell it to skip the first row in the calculation . How to do that ? I've tried Boolean indexing but can't get it to work , and maybe there is a way to tell Pandas to skip the NaN results ... but the best approach seems to be a qualifier that says " apply this code , starting at the second row . "
It occurred to me that it might be much faster to identify those groups that have duplicates using count . Then I can apply the max transformation to that grouping , and then recombine the two into one .
I think the problem is that g [ " liq "] .transform ( " max ") resets the index , losing the original index in the process ? certainly df [ df [ " liq "] == g [ " liq "] .transform ( " max ") results in a memory error ... I'm still struggling with this . doing g.size() produces an effective count of the number of duplicates , and is very , very , fast , so I am trying to use this to get the unique_id and period_id pairs where size > 2 , then apply the max idea above to those which should be much faster , and then I'll need to recombine with the original data frame .
Pandas DataFrame apply Specific Function to Each column
In case you have a core set of columns , as here represented by ` df1 ` , you could apply ` .fillna() ` to the ` .difference() ` between the core set and any new columns in more recent ` DataFrames ` . #CODE
what is your pandas version . I can run this example fine in 0.16.1 . As an aside , rather than doing apply ( pd.to_datetime ) , just do pd.to_datetime ( df ) . This line : df [ 0 ]= df [ 0 ] .apply ( pd.to_datetime ) also seems to be wrong it seems you want df [ ' timestamp '] = df [ ' timestamp '] . .
Apply functon with a condition on the first row
For instance , I would like to apply ( lambda x : x+ 273.15 ) on each columns which contain C data .
Ah , now I understand . Unfortunately , I can't think of a good solution . Which dataframe operation do you want to apply if you had the rows combined into a dataframe ?
Edit : I have found a way to do that : I apply ` ast.literal_eval ` to each line .
Apply function to each column that returns the value associated with the ` index ` of the ` min ` ` abs ` value like so : #CODE
You could pass an argument to ` apply ` : #CODE
Using apply to go through row by row and test whether the value is numeric of string is the quickest way separate them . #CODE
I have tried expressing it in terms of join or merge but have failed so far . Is there any simple way to express that or will I have to use apply and create a new DataFrame ?
You could use ` apply ` with ` axis=1 ` to apply for rows with ` any ` method , if you have only one valid value and all other are ` NaN ` ( using @USER example ): #CODE
If you want to subset your dataframe you could use mask with your columns and apply it to the whole dataframe : #CODE
and then use groupBy and apply to calculate the median somehow ?
and apply a function to the groups .
Ex . 1 Find the number of trips each team went on . ` team ` is the grouper , and we apply the function ` count() ` on column ` [ ' trips ']` . #CODE
Ex . 2 ( multiple columns ) : Find the total time each player on a team spent traveling . We use 2 columns ` [ ' team ' , ' player ']` as the grouper , and apply the function ` sum() ` on column ` [ ' time ']` . #CODE
IIUC you need groupby by ` Feed ` from multiindex and apply ` pct_change ` . Then you can use subset of ` df3 ` , where column ` Rate_Return ` is ` notnull ` #CODE
We get 3 rows but only 2 columns . In the docs I find that different from standard python , label based slicing in pandas is inclusive . Does this apply here and is it inclusive for rows but not for columns then ?
Here , I am grouping by ` id ` and for each ` id ` , the df is sorted by ` time ` . Now , I want to replace the values in ` a ` and ` b ` by the maximum value seen thus far . I guess I can apply a rolling max on each group but is there a better way to do this ?
You can ` apply ` custom function , where find index of first 1 by ` idxmax ` and set rows to the end of group to ` 1 ` : #CODE
My knowledge isn't that great of Pandas ( yet ) , but I'm guessing it's an " apply " or an agg() function but so far , syntactically , I'm banging my head from the syntax errors , but I appreciate any pointers in the right direction . .. JW
But this takes quite a while given the time complexity , running at around 20s for 500 points and I have a much longer list . This has me looking at vectorization , and I've come across ` numpy.vectorize ` ( ( docs ) , but can't figure out how to apply it in this context .
Thanks . It doesn t work . The format of the date is " 2015-12-01 00:00 : 00-06 : 00 " . I used " to_datetime " to convert the original date format to a datetime object , in order to apply " tz_localize " to convert to another time zone . It seems tz_localize adds that offset and I have not found how to get rid of it .
I am not exactly sure how to go about this . One of the ideas is use itterrows() and apply harvesine() function , if rows ' sequence ' parameter is not 0 and row's ' track_id ' is equal to previous row's ' track_id '
[ EDIT ] I figured there is no need to check if ' track_id ' of row and previous row is the same , since the haversine() function is applied to two rows only , and when sequence = 0 , that row's distance == 0 , which means that the track_id has changed . So , basically , apply haversine() function to all rows whose ' sequence ' ! = 0 , ie haversine ( previous_row.lng , previous_row.lat , current_row.lng , current_row.lat ) . Still need help with that though
I normally populate new columns using " apply , axis = 1 " so I would really appreciate any solution based on that . I found that " apply " works fine when for each row , computation is done across columns using values at the same row level . However , I don't know how an " apply " function can involve different rows , which is what this problem requires . the only exception I have seen so far is " diff " , which is not useful here .
@USER : using min_period to fill in NaN is good if e.g. one wants to remove seasonality in data . I , however , want to use the rolling mean to create a feature to feed into a ML model . I can't use rolling_mean for store-dates not in the initial dataset , which min_period does . Therefore , I loop over the 1115 stores and apply your solution ( without min_periods ) , which is still much faster than my initial attempt . Thanks for your help
I have a data frame in pandas which includes number of days since an event occurred . I want to create a new column that calculates the date of the event by subtracting the number of days from the current date . Every time I attempt to apply ` pd.offsets.Day ` or ` pd.Timedelta ` I get an error stating that Series are an unsupported type . This also occurs when I use ` apply ` . When I use ` map ` I receive a runtime error saying " maximum recursion depth exceeded while calling a Python object " .
What is a proper idiom in pandas for creating a dataframes from the output of a apply function on a df ?
One of the operations I need to conduct is grabbing the latest feed entries --- the feed urls exist in a column in a data frame . Once I've done the apply I get feed objects back : #CODE
So , now I'm stuck with the feed entries in the " entries " column , I'd like to create a two new data frames in one apply method , and concatenate the two frames immediately .
But I'm not sure how this will help me . I took a look at the pandas Dataframe.to_dict() but I don't think the above code reads into a dataframe ( or , if it does , I don't understand the documentation well enough ) . It looks like it'll only store one value per key at a time . Another thread I was reading says it's possible to store more than one value per key , though ( using .append() ) but I don't know how to apply it to this situation .
Pandas : apply returns list
I have the following function that I want to apply to each group : #CODE
How do I make the results of the apply operation the values of the " mean_to_date " column ? That is , the mean_to_date for player 200 , season 21999 would be 0 and 10 , then for player 200 , season 21200 it would be 0 , 10 , and 15 , and so forth . Note that the mean_to_date value represents the mean prior to the game , so before the 1st game it is zero , and before the second game it is the total from the first game .
you're getting lists back because your function ` previous_mean ` , when fed a dataframe , returns a list -- it has nothing to do with ` apply ` .
You said in another comment that you want one mean per row , therefore the function you apply should return a single value .
Apply the function to the groups : #CODE
How to apply a function ( BigramCollocationFinder ) to Pandas DataFrame
I want to adapt this function to my Pandas Dataframe . I am aware of the apply function for Pandas Dataframes , but can't manage to get it work .
If you want to apply ` BigramCollocationFinder.from_words() ` to each ` value ` in the ` Body ` ` column , you'd have to do : #CODE
In essence , ` apply ` allows you to loop through the ` rows ` and provide the corresponding ` value ` of the ` Body ` ` column ` to the applied function .
` Apply ` function over rows and put the result in a new column . #CODE
You could also do apply : #CODE
For my data set ( with tens of thousands of rows ) , this is somewhat slow , and I understand that loops should be avoided when possible when using pandas dataframes . I feel like the pandas ` apply ` function may be able to do what I need , but I'm at a loss as to how to implement it .
` apply ` calls ` tuple_to_timestamp ` for each row of ` df [ ' orig ']` .
The first operation can be done with an ` apply ` function returning a series ( see the accepted answer to this question ) , followed by a horizontal ` concat ` operation ( i.e. , with ` axis=1 `) .
I have a groupby object I apply expanding mean to . However I want that calculation over another series / group at the same time . Here is my code : #CODE
Then , define what's a ' win ' , calculate overall record and apply ` expanding_mean ` : #CODE
I have already asked a similar question here but couldn't manage to apply the solution for my problem with two columns .