My	O
question	O
concerns	O
iterating	O
through	O
the	O
rows	O
of	O
a	O
data	O
frame	O
and	O
on	O
each	O
row	O
setting	O
a	O
field	O
based	O
on	O
information	O
in	O
a	O
different	O
data	O
frame	O
.	O

EDIT	O
:	O
Adding	O
logic	O
to	O
default	O
empty	O
strings	O
to	O
`	O
0	O
`	O
,	O
use	O
a	O
different	O
value	O
if	O
you	O
want	O
to	O
handle	O
empty	O
strings	O
in	O
`	O
years	O
`	O
colomn	O
differently	O
#CODE	O

I	O
would	O
suggest	O
that	O
you	O
use	O
2-dimensional	O
numpy	O
array	O
.	O

I	O
renamed	O
them	O
to	O
aa	O
,	O
ab	O
and	O
ac	O
but	O
still	O
get	O
the	O
same	O
error	O
.	O

In	O
this	O
last	O
case	O
,	O
RAM	O
usage	O
fits	O
the	O
equivalent	O
`	O
chunk	O
`	O
size	O
#CODE	O

`	O
pandas	O
`	O
,	O
like	O
`	O
numpy	O
`	O
and	O
many	O
other	O
modules	O
,	O
is	O
not	O
written	O
in	O
pure	O
Python	O
-	O
it	O
has	O
components	O
written	O
in	O
C	O
and	O
Cython	O
that	O
get	O
compiled	O
into	O
version-	O
and	O
platform-specific	O
libraries	O
during	O
the	O
build	O
process	O
.	O

It	O
gave	O
me	O
the	O
error	O
:	O
cqid	O
=	O
row	O
[	O
'	O
ClearQuest	O
ID	O
']	O
TypeError	O
:	O
string	O
indices	O
must	O
be	O
integers	O
,	O
not	O
str	O
...........	O

The	O
use	O
case	O
is	O
that	O
I	O
have	O
different	O
time	O
series	O
coming	O
from	O
different	O
data	O
sources	O
.	O

How	O
can	O
I	O
get	O
pandas	O
Timestamp	O
offset	O
by	O
certain	O
amount	O
of	O
months	O
?	O

I	O
managed	O
to	O
get	O
the	O
stats	O
by	O
placing	O
everything	O
in	O
nested	O
dictionary	O
,	O
but	O
I	O
feel	O
that	O
there	O
may	O
be	O
a	O
much	O
easier	O
way	O
to	O
the	O
approach	O
by	O
using	O
pandas	O
dataframes	O
and	O
groubpy	O
.	O

Just	O
to	O
get	O
a	O
sense	O
of	O
what	O
I'm	O
trying	O
to	O
achieve	O
.	O

Which	O
is	O
suspect	O
is	O
due	O
to	O
my	O
data	O
range	O
,,	O
but	O
it	O
may	O
well	O
be	O
that	O
I	O
don't	O
understand	O
the	O
other	O
parameters	O
.	O

Your	O
second	O
one	O
doesn't	O
really	O
make	O
sense	O
as	O
an	O
aggregation	O
.	O

How	O
can	O
I	O
get	O
the	O
index	O
of	O
certain	O
element	O
of	O
a	O
Series	O
in	O
python	O
pandas	O
?	O

(	O
Very	O
,	O
very	O
late	O
reply	O
-	O
apologies	O
.	O
)	O
That's	O
true	O
,	O
you'd	O
use	O
the	O
method	O
EdChum	O
suggested	O
for	O
longer	O
lists	O
of	O
columns	O
.	O

If	O
actual_sum	O
and	O
expected_to_date	O
are	O
equal	O
,	O
put	O
a	O
0	O

`	O
ts	O
[	O
ts	O
[	O
'	O
values	O
']	O
0	O
]`	O
should	O
produce	O
the	O
output	O
you	O
are	O
looking	O
for	O
.	O

And	O
I	O
get	O
the	O
counts	O
:	O
#CODE	O

The	O
standard	O
deviation	O
differs	O
between	O
pandas	O
and	O
numpy	O
.	O

I	O
would	O
like	O
to	O
get	O
rid	O
of	O
the	O
loops	O
,	O
if	O
that	O
is	O
possible	O
.	O

If	O
I	O
change	O
the	O
names	O
then	O
there	O
is	O
nothing	O
to	O
reference	O
.	O

I	O
even	O
tried	O
building	O
from	O
the	O
git	O
,	O
but	O
whatever	O
I	O
seem	O
to	O
do	O
,	O
I	O
get	O
the	O
same	O
error	O
:	O
#CODE	O

I	O
want	O
to	O
do	O
the	O
following	O
operations	O
on	O
the	O
data	O
storage	O
:	O

How	O
do	O
I	O
get	O
it	O
to	O
actually	O
show	O
the	O
graph	O
?	O

#URL	O
shows	O
a	O
way	O
to	O
get	O
the	O
number	O
of	O
days	O
in	O
a	O
month	O
,	O
making	O
the	O
rest	O
more	O
or	O
less	O
trivial	O
as	O
they	O
don't	O
vary	O
.	O

to	O
create	O
average	O
values	O
with	O
an	O
equidistant	O
time-vector	O
.	O

I	O
get	O
something	O
where	O
all	O
"	O
newlines	O
"	O
are	O
escaped	O
.	O

Reproducing	O
without	O
a	O
data	O
file	O
,	O
using	O
Jeff's	O
suggestion	O
:	O
#CODE	O

However	O
,	O
I	O
also	O
want	O
to	O
get	O
it	O
on	O
the	O
basis	O
of	O
the	O
`	O
Group	O
`	O
variable	O
,	O
which	O
means	O
I	O
don't	O
want	O
to	O
get	O
`	O
Bob	O
`'	O
s	O
`	O
Value	O
`	O
based	O
on	O
the	O
`	O
Jared	O
`'	O
s	O
`	O
Value	O
`	O
,	O
since	O
those	O
two	O
records's	O
`	O
Group	O
`	O
value	O
is	O
different	O
-	O
I	O
only	O
compute	O
it	O
within	O
each	O
specific	O
`	O
Group	O
`	O
variable	O
.	O

I	O
try	O
to	O
use	O
jsonlint	O
to	O
validate	O
these	O
json	O
files	O
but	O
encounter	O
some	O
error	O
messages	O
.	O

The	O
logic	O
to	O
arrive	O
at	O
that	O
database	O
is	O
an	O
intricate	O
mix	O
of	O
Python	O
processing	O
and	O
SQL	O
joins	O
done	O
in	O
sqlite3	O
.	O

I	O
want	O
to	O
take	O
advantage	O
of	O
the	O
`	O
str	O
`	O
accessor	O
to	O
split	O
the	O
data	O
into	O
two	O
columns	O
,	O
such	O
that	O
the	O
first	O
column	O
is	O
,	O
Name	O
,	O
contains	O
the	O
actual	O
name	O
(	O
first	O
name	O
last	O
name	O
)	O
,	O
and	O
the	O
second	O
column	O
,	O
Email	O
,	O
contains	O
the	O
email	O
address	O
)	O
.	O

In	O
fact	O
the	O
only	O
really	O
relevant	O
data	O
needed	O
for	O
the	O
plot	O
is	O
the	O
first	O
and	O
second	O
column	O
,	O
namely	O
:	O
`	O
Compression	O
Force	O
`	O
and	O
`	O
Compression	O
Velocity	O
`	O
.	O

How	O
to	O
get	O
special	O
characters	O
from	O
Excel	O
to	O
screen	O
using	O
pandas	O
?	O

And	O
replace	O
`'	O
Month	O
'`	O
with	O
`'	O
Day	O
'`	O
below	O
.	O

But	O
if	O
you	O
have	O
a	O
huge	O
amount	O
to	O
data	O
,	O
it	O
*	O
might	O
*	O
be	O
interesting	O
to	O
think	O
of	O
a	O
more	O
complex	O
data	O
model	O
.	O

What	O
are	O
you	O
trying	O
to	O
do	O
where	O
this	O
is	O
the	O
bottleneck	O
?	O

How	O
can	O
I	O
change	O
that	O
and	O
use	O
insted	O
the	O
first	O
line	O
of	O
output	O
code	O
as	O
a	O
column	O
(	O
In	O
this	O
case	O
line	O
10	O
:	O
Sub-Data	O
Item	O
...	O
)	O

My	O
objective	O
was	O
to	O
have	O
a	O
DTM	O
like	O
the	O
one	O
you	O
get	O
in	O
R	O
tm	O
.	O

So	O
right	O
now	O
all	O
the	O
data	O
comes	O
from	O
each	O
iteration	O
group	O
,	O
and	O
all	O
of	O
its	O
is	O
transformed	O
into	O
one	O
column	O
vector	O
.	O

I	O
am	O
trying	O
to	O
get	O
to	O
the	O
point	O
where	O
I	O
can	O
run	O
#CODE	O

While	O
I	O
don't	O
get	O
that	O
warning	O
with	O
#CODE	O

Thanks	O
@USER	O
-	O
I	O
mean	O
that	O
,	O
if	O
we	O
do	O
`	O
A-B	O
`	O
we	O
should	O
only	O
get	O
the	O
NaNs	O
in	O
`	O
A	O
`	O
,	O
and	O
not	O
the	O
NaNs	O
in	O
`	O
B	O
`	O
.	O

I	O
have	O
three	O
columns	O
in	O
my	O
data	O
set	O
,	O
namely	O
"	O
age	O
"	O
,	O
"	O
race	O
"	O
,	O
"	O
sex	O
"	O
,	O
that	O
I	O
care	O
about	O
.	O

Cannot	O
get	O
the	O
average	O
date	O
using	O
pandas	O

Any	O
suggestions	O
?	O

All	O
values	O
ought	O
to	O
be	O
integers	O
,	O
no	O
floats	O
.	O

Note	O
:	O
this	O
will	O
get	O
tripped	O
up	O
by	O
some	O
strings	O
,	O
so	O
use	O
with	O
caution	O
.	O

The	O
purpose	O
of	O
all	O
these	O
stuff	O
is	O
a	O
geographical	O
representation	O
of	O
data	O
on	O
a	O
spatial	O
grid	O
.	O

Since	O
Name	O
`	O
C	O
`	O
does	O
not	O
have	O
`	O
3	O
`	O
or	O
`	O
5	O
`	O
in	O
the	O
column	O
`	O
Activity	O
`	O
,	O
I	O
do	O
not	O
want	O
to	O
get	O
this	O
data	O
frame	O
.	O

Data	O
has	O
to	O
be	O
collected	O
before	O
local	O
data	O
frame	O
is	O
created	O
.	O

PANDAS	O
:	O
Extracting	O
values	O
from	O
a	O
column	O
by	O
applying	O
a	O
condition	O
on	O
other	O
columnns	O

If	O
you	O
try	O
to	O
produce	O
the	O
groups	O
from	O
my	O
example	O
you'll	O
see	O
what	O
I	O
mean	O
.	O

`	O
pandas	O
`	O
,	O
like	O
`	O
numpy	O
`	O
and	O
many	O
other	O
modules	O
,	O
is	O
not	O
written	O
in	O
pure	O
Python	O
-	O
it	O
has	O
components	O
written	O
in	O
C	O
and	O
Cython	O
that	O
get	O
compiled	O
into	O
version-	O
and	O
platform-specific	O
libraries	O
during	O
the	O
build	O
process	O
.	O

Not	O
sure	O
how	O
to	O
get	O
around	O
this	O
...	O
pretty	O
new	O
to	O
pandas	O
.	O

Here's	O
the	O
product	O
:	O
#CODE	O

However	O
,	O
as	O
the	O
data	O
became	O
large	O
,	O
we	O
played	O
with	O
SQLAlchemy	O
/	O
SQLite3	O
.	O

But	O
this	O
time	O
I	O
get	O
another	O
error	O
:	O
#CODE	O

Makes	O
the	O
change	O
the	O
idea	O
of	O
trying	O
to	O
use	O
this	O
approach	O
all	O
together	O
.	O

For	O
a	O
generalized	O
scenario	O
where	O
there	O
are	O
many	O
different	O
combinations	O
of	O
values	O
under	O
'	O
COL1	O
'	O
and	O
'	O
COL3	O
'	O
,	O
this	O
works	O
but	O
is	O
probably	O
not	O
nearly	O
as	O
efficient	O
as	O
it	O
can	O
be	O
:	O
#CODE	O

Similarly	O
in	O
your	O
example	O
where	O
you	O
plot	O
`	O
col1	O
,	O
col2	O
`	O
differently	O
based	O
on	O
`	O
col3	O
`	O
,	O
what	O
if	O
there	O
are	O
NA	O
values	O
that	O
break	O
the	O
association	O
between	O
`	O
col1	O
,	O
col2	O
,	O
col3	O
`	O
?	O

For	O
example	O
,	O
I	O
want	O
to	O
take	O
values	O
from	O
`	O
col_3	O
`	O
and	O
`	O
col_4	O
`	O
and	O
use	O
them	O
to	O
generate	O
a	O
single	O
values	O
.	O

The	O
speed	O
difference	O
is	O
astonishing	O
.	O

The	O
summation	O
in	O
one	O
group	O
won't	O
reduce	O
the	O
size	O
of	O
the	O
result	O
,	O
the	O
summation	O
I	O
want	O
to	O
do	O
is	O
across	O
different	O
groups	O
.	O

If	O
you	O
really	O
prefer	O
`	O
1	O
`'	O
s	O
and	O
`	O
0	O
`'	O
s	O
replace	O
the	O
last	O
line	O
with	O
:	O
#CODE	O

So	O
traverse	O
the	O
data	O
once	O
and	O
generate	O
both	O
arrays	O
would	O
be	O
preferred	O
.	O

Im	O
not	O
fully	O
adjusted	O
to	O
how	O
Pandas	O
is	O
using	O
matplotlib	O
so	O
i	O
often	O
switch	O
to	O
matplotlib	O
myself	O
if	O
plots	O
get	O
more	O
complicated	O
,	O
eg	O
:	O
#CODE	O

The	O
table	O
that	O
gives	O
this	O
message	O
contains	O
a	O
few	O
columns	O
,	O
none	O
of	O
them	O
have	O
data	O
in	O
them	O
.	O

so	O
yes	O
later	O
i	O
have	O
open	O
the	O
file	O
but	O
thanks	O
to	O
pandas	O
i	O
can	O
use	O
the	O
`	O
chunksize	O
`	O
command	O
to	O
get	O
the	O
information	O
i	O
need	O
.	O

create	O
column	O
names	O
by	O
joining	O
two	O
labels	O
of	O
different	O
levels	O
with	O
pandas	O

@USER	O
so	O
how	O
should	O
i	O
write	O
it	O
so	O
that	O
the	O
program	O
gives	O
seq	O
to	O
'	O
Hsequence	O
'	O
column	O
when	O
'	O
Hcolumn	O
'	O
contains	O
the	O
title	O
from	O
fasta	O
file	O
?	O

Also	O
,	O
in	O
my	O
larger	O
directory	O
,	O
this	O
is	O
taking	O
forever	O
-	O
as	O
in	O
,	O
about	O
a	O
gig	O
of	O
CSVs	O
is	O
timing	O
out	O
for	O
me	O
(	O
by	O
my	O
hand	O
)	O
at	O
around	O
20	O
minutes	O
.	O

The	O
key	O
was	O
unstacking	O
the	O
data	O
first	O
:	O
#CODE	O

I	O
want	O
to	O
get	O
the	O
latitude	O
and	O
longitude	O
coordinates	O
for	O
any	O
one	O
of	O
the	O
columns	O
in	O
the	O
data	O
frame	O
below	O
.	O

Option	O
values	O
are	O
restored	O
automatically	O
when	O
you	O
exit	O
the	O
`	O
with	O
`	O
block	O
.	O

I	O
am	O
finding	O
difficulty	O
to	O
plot	O
reason	O
every	O
csv	O
file	O
starts	O
with	O
different	O
date	O
,	O
that's	O
the	O
reason	O
I	O
was	O
trying	O
to	O
convert	O
into	O
no	O
.	O
of	O
days	O
,	O
so	O
that	O
I	O
can	O
plot	O
all	O
in	O
one	O
go	O
with	O
starting	O
day	O
-	O
1	O
,	O
for	O
example	O
:	O
-	O
csv	O
file	O
2	O
fall	O
short	O
as	O
compared	O
to	O
csv	O
file	O
1	O
.	O

Most	O
of	O
the	O
time	O
you	O
can	O
get	O
away	O
with	O
using	O
something	O
else	O
...	O

In	O
that	O
case	O
the	O
index	O
is	O
composed	O
of	O
integers	O
from	O
0	O
to	O
n	O
:	O
#CODE	O

You	O
have	O
a	O
difference	O
between	O
a	O
mac	O
and	O
a	O
pc	O
,	O
and	O
*	O
presumably	O
*	O
the	O
same	O
code	O
.	O

Suppose	O
you	O
want	O
to	O
find	O
the	O
row	O
or	O
rows	O
where	O
`	O
beef	O
`	O
production	O
was	O
the	O
highest	O
.	O

The	O
number	O
of	O
columns	O
may	O
differ	O
and	O
so	O
does	O
the	O
column	O
names	O
.	O

How	O
do	O
I	O
avoid	O
that	O
and	O
rather	O
generate	O
it	O
in	O
a	O
sparse	O
matrix	O
CSR	O
format	O
?	O

I	O
download	O
and	O
scrape	O
a	O
webpage	O
for	O
some	O
data	O
in	O
TSV	O
format	O
.	O

You	O
can	O
set	O
parameter	O
`	O
labels=False	O
`	O
to	O
get	O
the	O
integer	O
representation	O
#CODE	O

it's	O
not	O
too	O
much	O
of	O
a	O
stretch	O
to	O
insert	O
NaN's	O
into	O
the	O
data	O
using	O
reindexing	O
so	O
that	O
i	O
get	O
this	O
:	O
#CODE	O

Any	O
suggestions	O
?	O

Data-driven	O
DOM	O
manipulation	O
(	O
maybe	O
the	O
hardest	O
thing	O
to	O
wrap	O
one's	O
head	O
around	O
):	O
your	O
data	O
gets	O
transformed	O
into	O
DOM	O
elements	O
.	O

Your	O
regex	O
is	O
matching	O
on	O
all	O
`	O
-	O
`	O
characters	O
:	O
#CODE	O

1	O
)	O
create	O
additional	O
columns	O
with	O
clock	O
time	O
headings	O
for	O
5	O
minute	O
intervals	O
between	O
9:30	O
and	O
4:00	O
pm	O
,	O
so	O
the	O
headings	O
of	O
the	O
data	O
frame	O
look	O
like	O
:	O

`	O
Index	O
([	O
u'id	O
opinion	O
']	O
,	O
dtype=	O
'	O
object	O
')`	O
Thanks	O
for	O
the	O
response	O

The	O
end	O
product	O
would	O
be	O
ten	O
timeseries	O
plots	O
with	O
charted	O
lines	O
over	O
time	O
for	O
each	O
TID	O
.	O

And	O
get	O
the	O
result	O
:	O
#CODE	O

However	O
,	O
I	O
still	O
don't	O
get	O
why	O
`	O
iconv	O
`	O
messes	O
it	O
up	O
.	O

If	O
you	O
have	O
huge	O
CSV	O
data	O
,	O
NYSOL's	O
mcmd	O
is	O
the	O
best	O
.	O

I	O
get	O
#CODE	O

If	O
I	O
use	O
a	O
tweaked	O
version	O
of	O
@USER	O
'	O
s	O
suggestion	O
below	O
,	O
I	O
get	O
this	O
error	O
:	O
#CODE	O

ValueError	O
:	O
Unknown	O
format	O
code	O
'	O
f	O
'	O
for	O
object	O
of	O
type	O
'	O
str	O
'	O
-	O
why	O
do	O
I	O
get	O
this	O
the	O
second	O
time	O
but	O
not	O
the	O
first	O
time	O
?	O

Any	O
suggestion	O
about	O
the	O
reason	O
?	O

I	O
have	O
a	O
data	O
set	O
which	O
has	O
multiple	O
columns	O
,	O
strings	O
and	O
integers	O

is	O
the	O
condition	O
,	O
returning	O
a	O
booleans	O
array	O
of	O
True	O
/	O
False	O
for	O
all	O
values	O
meeting	O
the	O
condition	O
or	O
not	O
,	O
and	O
then	O
the	O
corresponding	O
A	O
values	O
are	O
selected	O

I	O
fixed	O
this	O
bug	O
in	O
0.11-dev	O
in	O
any	O
event	O
,	O
see	O
here	O
:	O
#URL	O
thanks	O
!	O

To	O
split	O
`	O
my_data2	O
`	O
into	O
two	O
arrays	O
of	O
roughly	O
equal	O
size	O
:	O
#CODE	O

to	O
get	O
a	O
`	O
Series	O
`	O
of	O
`	O
list	O
`	O
s	O
of	O
strings	O
.	O

For	O
example	O
,	O
you	O
can't	O
sum	O
a	O
mix	O
of	O
strings	O
and	O
floats	O
in	O
pandas	O
but	O
Excel	O
would	O
silently	O
drop	O
the	O
string	O
value	O
and	O
sum	O
the	O
floats	O
.	O

Notice	O
how	O
the	O
values	O
in	O
the	O
second	O
column	O
are	O
no	O
longer	O
integers	O
,	O
as	O
they	O
were	O
originally	O
.	O

I	O
have	O
a	O
large	O
but	O
very	O
sparse	O
matrix	O
(	O
50,000	O
rows*	O
100,000	O
columns	O
,	O
only	O
10%	O
of	O
the	O
values	O
are	O
known	O
)	O
.	O

In	O
python	O
normally	O
you	O
don't	O
need	O
and	O
you	O
shouldn't	O
use	O
a	O
semicolon	O
at	O
the	O
end	O
of	O
the	O
line	O
.	O

That's	O
all	O
data	O
python	O
is	O
reading	O
in	O
,	O
apparently	O
:	O
the	O
16	O
first	O
lines	O
,	O
or	O
at	O
least	O
I	O
am	O
not	O
able	O
to	O
get	O
the	O
rest	O
of	O
data	O
in	O
.	O

The	O
problem	O
is	O
to	O
find	O
average	O
values	O
of	O
temp1	O
,	O
temp2	O
and	O
temp3	O
for	O
a	O
period	O
of	O
time	O
(	O
say	O
,	O
2	O
days	O
)	O
over	O
the	O
same	O
intervals	O
(	O
for	O
that	O
example	O
-	O
15	O
minutes	O
)	O
.	O

In	O
generally	O
I	O
wonder	O
if	O
pandas	O
should	O
not	O
at	O
least	O
throw	O
a	O
warning	O
,	O
afterall	O
broadcasting	O
the	O
result	O
to	O
both	O
columns	O
should	O
be	O
almost	O
never	O
what	O
is	O
wanted	O
.	O

I	O
get	O
pandas	O
error	O
when	O
I	O
try	O
to	O
read	O
HDF5	O
format	O
files	O
that	O
I	O
have	O
created	O
with	O
h5py	O
.	O

Additionally	O
you	O
can	O
use	O
numpys	O
matrix	O
#CODE	O

I	O
updated	O
pandas	O
'	O
sudo	O
pip	O
install	O
--	O
upgrade	O
pandas	O
'	O
,	O
between	O
both	O
of	O
these	O
fixes	O
,	O
everything	O
worked	O
.	O

Sorry	O
can't	O
reproduce	O
nor	O
understand	O
your	O
real	O
problem	O
,	O
please	O
post	O
what	O
you	O
see	O
in	O
your	O
question	O

When	O
I	O
used	O
'	O
ethnicity	O
'	O
or	O
'	O
veteran	O
'	O
as	O
a	O
value	O
my	O
results	O
came	O
out	O
really	O
strange	O
and	O
didn't	O
match	O
my	O
value	O
counts	O
numbers	O
.	O

`	O
post_start	O
`	O
is	O
the	O
date	O
that	O
the	O
employee	O
started	O
in	O
the	O
post	O
,	O
and	O
`	O
change_date	O
`	O
is	O
the	O
date	O
that	O
the	O
post	O
title	O
was	O
changed	O
.	O

How	O
do	O
I	O
replace	O
the	O
ints	O
with	O
the	O
float	O
values	O
from	O
another	O
column	O
(	O
by	O
same	O
row	O
)	O
,	O
but	O
leave	O
all	O
the	O
nulls	O
?	O

There	O
may	O
be	O
a	O
more	O
foolproof	O
,	O
cleaner	O
way	O
of	O
computing	O
date	O
time	O
differences	O
in	O
pandas	O
.	O

However	O
,	O
to	O
get	O
the	O
row	O
sum	O
,	O
one	O
needs	O
to	O
specify	O
axis=1	O
.	O

Using	O
the	O
second	O
method	O
I	O
get	O
the	O
following	O
error	O
:	O
#CODE	O

Filter	O
data	O
to	O
get	O
only	O
first	O
day	O
of	O
the	O
month	O
rows	O

(	O
FYI	O
if	O
i	O
insert	O
a	O
print	O
print	O
(	O
vals	O
)	O
in	O
the	O
middle	O
of	O
that	O
loop	O
,	O
it	O
prints	O
#CODE	O

For	O
days	O
in	O
a	O
month	O
(	O
'	O
2015-07	O
'	O
say	O
)	O
You	O
could	O
change	O
#CODE	O

Doesnt	O
the	O
frame	O
variable	O
get	O
overwritten	O
during	O
each	O
iteration	O
in	O
the	O
loop	O
?	O

Any	O
other	O
advice	O
I	O
can	O
leverage	O
in	O
the	O
meantime	O
?	O

If	O
`	O
Change	O
Closing	O
Date	O
`	O
is	O
True	O
,	O
I	O
would	O
like	O
to	O
add	O
`	O
Closing	O
Date2	O
`	O
column	O
into	O
my	O
new	O
column	O
with	O
adding	O
1	O
year	O
.	O

If	O
you	O
REALLY	O
want	O
to	O
get	O
by	O
a	O
group	O
individually	O
#CODE	O

but	O
I	O
get	O
the	O
error	O
:	O
#CODE	O

I	O
am	O
new	O
to	O
pandas	O
for	O
data	O
analysis	O
and	O
I	O
just	O
installed	O
pandas	O
with	O
required	O
dependencies	O
(	O
NumPy	O
,	O
python-dateutil	O
,	O
pytz	O
,	O
numexpr	O
,	O
bottleneck	O
and	O
matplotlib	O
)	O
.	O

What	O
do	O
you	O
get	O
if	O
you	O
print	O
that	O
?	O

Can't	O
you	O
use	O
sets	O
and	O
intersections	O
?	O

is	O
there	O
a	O
way	O
to	O
insert	O
`	O
s	O
`	O
into	O
`	O
df	O
`	O
without	O
creating	O
a	O
reindexed	O
copy	O
of	O
`	O
df	O
`	O
first	O
?	O

I'm	O
using	O
python	O
2.7.5	O
(	O
with	O
all	O
the	O
packages	O
in	O
the	O
python	O
(	O
x	O
,	O
y	O
)	O
bundle	O
)	O
,	O
and	O
running	O
files	O
from	O
the	O
command	O
prompt	O
.	O

Any	O
suggestions	O
??	O

This	O
will	O
never	O
get	O
the	O
similar	O
graph	O
as	O
the	O
kernel	O
estimate	O
base	O
of	O
the	O
original	O
data	O
,	O
result	O
:	O

The	O
working	O
version	O
I	O
have	O
is	O
this	O
one	O
,	O
but	O
I	O
feel	O
there	O
is	O
potential	O
for	O
improvement	O
,	O
as	O
I	O
find	O
my	O
solution	O
unreadable	O
and	O
I	O
am	O
unsure	O
about	O
how	O
it	O
would	O
generalize	O
to	O
multiindexes	O
#CODE	O

Also	O
,	O
once	O
you	O
get	O
to	O
15	O
points	O
,	O
you'll	O
be	O
able	O
to	O
upvote	O
as	O
well	O
.	O

You	O
can	O
then	O
get	O
the	O
last	O
first	O
value	O
by	O
forward	O
filling	O
`	O
first_values	O
`	O
,	O
reindexing	O
like	O
`	O
second_values	O
`	O
,	O
stacking	O
again	O
and	O
indexing	O
into	O
the	O
result	O
using	O
the	O
original	O
`'	O
time	O
'	O
,	O
'	O
second	O
'`	O
pairs	O
:	O
#CODE	O

how	O
do	O
i	O
avoid	O
creating	O
so	O
many	O
variables	O
as	O
I	O
add	O
columns	O
together	O
?	O

Any	O
suggestion	O
on	O
how	O
to	O
efficiently	O
achieve	O
this	O
?	O

I	O
get	O
:	O
#CODE	O

For	O
instance	O
,	O
I	O
can	O
compute	O
the	O
value	O
for	O
data	O
record	O
3	O
by	O
taking	O
`	O
len	O
(	O
set	O
([	O
4	O
,	O
4	O
,	O
6	O
,	O
12	O
]))`	O
which	O
gives	O
3	O
.	O

@USER	O
That's	O
a	O
great	O
suggestion	O
(	O
for	O
some	O
use-cases	O
)	O
it	O
should	O
be	O
its	O
own	O
answer	O
(	O
so	O
I	O
can	O
upvote	O
it	O
)	O
Though	O
it	O
does	O
need	O
tweak	O
to	O
multiply	O
by	O
100	O
.	O

python	O
how	O
to	O
sum	O
together	O
all	O
values	O
within	O
a	O
time	O
interval	O
in	O
datetime64	O
?	O

was	O
trying	O
to	O
do	O
a	O
"	O
for	O
i	O
in	O
range	O
(	O
len	O
(	O
results	O
))"	O
before	O
the	O
"	O
for	O
item	O
in	O
results	O
[	O
i	O
]"	O
that	O
you	O
did	O
but	O
not	O
working	O
for	O
me	O
...	O

But	O
,	O
on	O
the	O
other	O
hand	O
,	O
if	O
your	O
columns	O
aren't	O
in	O
the	O
same	O
order	O
,	O
then	O
my	O
suggestion	O
won't	O
work	O
.	O

When	O
I	O
execute	O
the	O
program	O
for	O
the	O
data	O
of	O
the	O
same	O
day	O
,	O
processor	O
time	O
becomes	O
long	O
from	O
the	O
same	O
point	O
.	O

I'm	O
new	O
to	O
pandas	O
,	O
python	O
,	O
and	O
scripting	O
in	O
general	O
,	O
so	O
am	O
still	O
getting	O
my	O
head	O
around	O
the	O
basics	O
.	O

You	O
can	O
,	O
for	O
example	O
,	O
use	O
interpolation	O
to	O
get	O
equally	O
spaced	O
datapoints	O
out	O
off	O
your	O
timeseries	O
.	O

What	O
I	O
was	O
hoping	O
for	O
was	O
to	O
add	O
up	O
all	O
of	O
the	O
frequencies	O
across	O
the	O
websites	O
and	O
to	O
create	O
two	O
columns	O
:	O
Column	O
A	O
with	O
the	O
word	O
,	O
and	O
Column	O
B	O
with	O
all	O
of	O
the	O
frequencies	O
added	O
together	O
.	O

It	O
does	O
not	O
work	O
without	O
dropping	O
index	O
.	O

Now	O
I	O
was	O
wondering	O
how	O
I	O
could	O
subtract	O
my	O
multi-year	O
timeseries	O
from	O
this	O
standard	O
year	O
,	O
in	O
order	O
to	O
get	O
a	O
timeseries	O
that	O
show	O
which	O
days	O
were	O
below	O
or	O
above	O
it's	O
standard	O
.	O

I	O
may	O
try	O
installing	O
an	O
older	O
version	O
to	O
find	O
out	O
what	O
was	O
actually	O
getting	O
calculated	O
.	O

Is	O
there	O
any	O
disadvantage	O
?	O

The	O
length	O
of	O
the	O
frame	O
is	O
over	O
2	O
million	O
rows	O
and	O
looping	O
to	O
extract	O
the	O
elements	O
I	O
need	O
is	O
a	O
poor	O
choice	O
.	O

edit	O
I	O
believe	O
'	O
endog	O
'	O
as	O
defined	O
is	O
incorrect-I	O
should	O
be	O
passing	O
the	O
values	O
for	O
which	O
I	O
want	O
to	O
predict	O
;	O
therefore	O
I've	O
created	O
a	O
date	O
range	O
of	O
12	O
periods	O
past	O
the	O
last	O
recorded	O
value	O
.	O

@USER	O
It	O
should	O
be	O
a	O
little	O
quicker	O
with	O
a	O
boolean	O
index	O
like	O
that	O
,	O
but	O
it	O
does	O
do	O
a	O
cast	O
(	O
timedelta	O
)	O
so	O
I'm	O
not	O
100%	O
sure	O
on	O
that	O
.	O

I	O
still	O
get	O
the	O
same	O
TypeError	O
message	O
using	O
the	O
line	O
you	O
suggest	O
.	O

Use	O
regex	O
with	O
`	O
python	O
`	O
engine	O
#CODE	O

(	O
it's	O
pretty	O
clear	O
that	O
`	O
id	O
`	O
maps	O
to	O
`	O
individual	O
`	O
,	O
but	O
I	O
would	O
clean	O
that	O
up	O
too	O
)	O
.	O

Being	O
able	O
to	O
quickly	O
determine	O
the	O
time	O
difference	O
between	O
Order	O
1	O
and	O
Order	O
2	O
(	O
per	O
PersonID	O
)	O
would	O
be	O
great	O
too	O
.	O

Thus	O
,	O
if	O
there	O
is	O
an	O
update	O
to	O
some	O
value	O
on	O
a	O
memory	O
page	O
,	O
that	O
page	O
is	O

and	O
make	O
this	O
a	O
Series	O
,	O
mapping	O
names	O
to	O
their	O
respective	O
numbers	O
:	O
#CODE	O

That	O
is	O
,	O
for	O
each	O
second	O
there	O
is	O
a	O
value	O
and	O
they	O
should	O
not	O
be	O
averaged	O
,	O
just	O
grouped	O
together	O
to	O
a	O
new	O
series	O
..	O

Specifically	O
,	O
in	O
this	O
case	O
,	O
I'd	O
only	O
like	O
to	O
drop	O
row	O
with	O
Indices	O
'	O
1991-12-31	O
'	O
and	O
'	O
1992-01-31	O
'	O
.	O

Or	O
read	O
it	O
in	O
directly	O
as	O
a	O
csv	O
,	O
by	O
appending	O
'	O
na	O
'	O
to	O
the	O
list	O
of	O
values	O
to	O
be	O
considered	O
NaN	O
:	O
#CODE	O

I	O
fail	O
to	O
see	O
the	O
corelation	O
between	O
"	O
John	O
"	O
and	O
the	O
dates	O
in	O
the	O
target	O
.	O

I	O
get	O
:	O

The	O
question	O
is	O
,	O
how	O
can	O
I	O
remove	O
or	O
filter	O
out	O
all	O
entries	O
that	O
have	O
frequency	O
1	O
?	O

For	O
all	O
the	O
other	O
names	O
that	O
are	O
not	O
in	O
the	O
top	O
ten	O
frequencies	O
I	O
want	O
to	O
combine	O
their	O
number	O
of	O
occurences	O
together	O
under	O
say	O
the	O
name	O
"	O
other	O
"	O
.	O

You	O
should	O
get	O
the	O
following	O
result	O
:	O

Which	O
indeed	O
is	O
longer	O
(	O
50	O
)	O
than	O
my	O
number	O
of	O
columns	O
/	O
indices	O
(	O
25	O
)	O
.	O

I	O
am	O
new	O
to	O
Python	O
(	O
and	O
programming	O
in	O
general	O
!	O
)	O
,	O
trying	O
to	O
conduct	O
some	O
data	O
analysis	O
using	O
Pandas	O
.	O

I	O
would	O
like	O
to	O
combine	O
these	O
columns	O
into	O
start	O
time	O
(	O
index	O
)	O
and	O
length	O
in	O
actual	O
seconds	O
.	O

I'm	O
looking	O
to	O
find	O
,	O
for	O
each	O
Census	O
Block	O
centroid	O
,	O
the	O
distance	O
to	O
it's	O
closest	O
restaurant	O
.	O

You	O
will	O
get	O
the	O
exception	O
"	O
appended	O
items	O
do	O
not	O
match	O
existing	O
items	O
in	O
table	O
!	O

Honestly	O
-	O
we	O
were	O
going	O
to	O
originally	O
do	O
visualizations	O
with	O
it	O
(	O
heatmaps	O
)	O
-	O
but	O
for	O
a	O
lot	O
of	O
reasons	O
we're	O
now	O
going	O
to	O
use	O
D3	O
...	O

For	O
example	O
,	O
if	O
I	O
say	O
year	O
,	O
the	O
entire	O
column	O
needs	O
to	O
be	O
appended	O
into	O
a	O
list	O
like	O
[	O
1	O
year	O
,	O
3	O
minutes	O
,	O
2	O
hours	O
]	O
.	O

Anyone	O
have	O
any	O
suggestions	O
for	O
how	O
to	O
accomplish	O
this	O
?	O

Yeah	O
I	O
know	O
it	O
gives	O
NaN	O
padding	O
,	O
but	O
only	O
on	O
the	O
indices	O
the	O
joining	O
is	O
done	O
over	O
.	O

The	O
paired	O
measurements	O
should	O
have	O
the	O
same	O
month	O
,	O
just	O
different	O
years	O
.	O

You	O
can	O
get	O
started	O
on	O
debugging	O
this	O
by	O
just	O
adding	O
a	O
line	O
to	O
your	O
code	O
and	O
running	O
again	O
:	O
#CODE	O

When	O
I	O
run	O
the	O
solution	O
I	O
get	O
the	O
error	O
.	O

Then	O
let's	O
add	O
a	O
helper	O
column	O
,	O
called	O
Safe	O
,	O
that	O
will	O
be	O
a	O
concatenation	O
of	O
all	O
the	O
Safex	O
columns	O
.	O

product	O
1111	O
non-null	O
object	O

In	O
R	O
,	O
using	O
the	O
car	O
package	O
,	O
there	O
is	O
a	O
useful	O
function	O
`	O
some	O
(	O
x	O
,	O
n	O
)`	O
which	O
is	O
similar	O
to	O
head	O
but	O
selects	O
,	O
in	O
this	O
example	O
,	O
10	O
rows	O
at	O
random	O
from	O
x	O
.	O

The	O
separator	O
(	O
between	O
cells	O
)	O
is	O
defined	O
by	O
the	O
operating	O
system	O
(	O
at	O
least	O
under	O
Windows	O
)	O
,	O
and	O
when	O
the	O
system	O
wide	O
list	O
separator	O
differs	O
from	O
comma	O
,	O
pandas	O
(	O
or	O
anything	O
else	O
I	O
tried	O
)	O
cannot	O
determine	O
what	O
separator	O
should	O
be	O
used	O
.	O

Setting	O
up	O
a	O
histogram	O
with	O
a	O
range	O
and	O
an	O
appropriate	O
bin	O
size	O
is	O
an	O
unknown	O
.	O

Thanks	O
TravisJ	O
,	O
I	O
guess	O
I	O
was	O
just	O
struggling	O
to	O
get	O
the	O
(	O
...	O
something	O
involving	O
group	O
...	O
)	O
in	O
when	O
i	O
was	O
using	O
the	O
ax=fig1	O
....	O
method	O
.	O

I	O
am	O
optimising	O
the	O
span	O
of	O
an	O
exponential	O
moving	O
average	O
and	O
the	O
number	O
of	O
lagged	O
variables	O
that	O
I	O
use	O
in	O
the	O
regression	O
.	O

The	O
error	O
message	O
that	O
I	O
get	O
is	O
:	O
#CODE	O

It	O
doesn't	O
however	O
take	O
advantage	O
of	O
the	O
psql	O
package	O
in	O
Pandas	O
.	O

On	O
a	O
much	O
larger	O
data	O
set	O
,	O
this	O
runs	O
in	O
790	O
ms	O
compared	O
to	O
1345	O
ms	O
for	O
ajcr's	O
and	O
Primer's	O
solutions	O
.	O

I've	O
put	O
together	O
one	O
approach	O
to	O
that	O
solution	O
that	O
should	O
scale	O
relatively	O
well	O
.	O

I	O
was	O
hoping	O
there	O
was	O
an	O
easy	O
way	O
to	O
get	O
the	O
set	O
of	O
B	O
values	O
per	O
each	O
A	O
value	O
like	O
`	O
{	O
'	O
one	O
'	O
:[	O
'	O
A	O
'	O
,	O
'	O
B	O
']	O
,	O
'	O
two	O
'	O
:[	O
'	O
A	O
']	O
,	O
'	O
three	O
'	O
:[	O
'	O
B	O
']	O
}	O
`	O
but	O
I	O
don't	O
see	O
anything	O
like	O
that	O
in	O
the	O
pandas	O
documentation	O

To	O
avoid	O
chained	O
indexing	O
,	O
you	O
need	O
to	O
get	O
all	O
your	O
conditions	O
into	O
a	O
single	O
set	O
of	O
brackets	O
.	O

But	O
trying	O
to	O
parse	O
the	O
column	O
name	O
and	O
hierarchy	O
and	O
auto-generate	O
the	O
insertable	O
thing	O
with	O
matching	O
index	O
is	O
unpleasant	O
.	O

The	O
seaborn	O
package	O
will	O
allow	O
you	O
to	O
plot	O
long	O
form	O
data	O
like	O
you	O
have	O
without	O
pivoting	O
but	O
pandas	O
requires	O
shared	O
index	O
and	O
one	O
column	O
per	O
plotted	O
line	O
by	O
default	O
so	O
your	O
solution	O
is	O
the	O
correct	O
one	O
.	O

Unable	O
to	O
filter	O
out	O
missing	O
(	O
NaN	O
)	O
location	O
data	O
while	O
using	O
Pandas	O
and	O
Geocoder	O
modules	O
in	O
Python	O

problem	O
is	O
the	O
sum	O
i	O
now	O
get	O
is	O
lined	O
up	O
in	O
week	O
intervals	O
but	O
not	O
in	O
the	O
right	O
sequence	O
.	O
this	O
wouldn't	O
be	O
a	O
problem	O
but	O
i	O
need	O
to	O
get	O
to	O
the	O
month	O
of	O
each	O
date	O
in	O
order	O
to	O
do	O
the	O
next	O
step	O
i	O
guess	O
.	O

How	O
could	O
I	O
sum	O
consecutive	O
day	O
values	O
here	O
,	O
so	O
I	O
would	O
get	O
something	O
like	O
this	O
?	O

I	O
changed	O
this	O
to	O
use	O
\t	O
as	O
the	O
separator	O
.	O

It's	O
possible	O
,	O
but	O
if	O
your	O
data	O
is	O
organized	O
it's	O
very	O
quick	O
with	O
shifting	O
it	O

@USER	O
fixed	O
,	O
was	O
a	O
typo	O
;	O
this	O
take	O
a	O
full	O
uri	O

Just	O
get	O
rid	O
of	O
it	O
and	O
reindent	O
the	O
loop	O
body	O
to	O
the	O
left	O
one	O
level	O
.	O

However	O
,	O
I	O
still	O
get	O
the	O
warning	O
.	O

For	O
any	O
x	O
in	O
dataset2	O
it	O
has	O
mapped	O
value	O
in	O
col2	O
.	O

but	O
you	O
then	O
need	O
to	O
store	O
these	O
dfs	O
somewhere	O
which	O
means	O
either	O
in	O
a	O
list	O
or	O
tuple	O
or	O
some	O
other	O
container	O
or	O
use	O
a	O
generator	O

and	O
so	O
on	O
for	O
the	O
remaining	O
location	O
categories	O
.	O

@USER	O
do	O
**	O
all	O
**	O
the	O
columns	O
in	O
the	O
DF	O
require	O
that	O
same	O
replacement	O
?	O

did	O
you	O
get	O
any	O
warnings	O
while	O
installing	O
numpy	O
or	O
pandas	O
?	O

This	O
`	O
df	O
`	O
consist	O
of	O
volume	O
observations	O
at	O
every	O
10	O
second	O
for	O
22	O
non-consecutive	O
days	O
.	O

I've	O
also	O
included	O
a	O
section	O
to	O
immediately	O
identify	O
any	O
redundant	O
genes	O
that	O
don't	O
have	O
any	O
SNPs	O
that	O
fall	O
within	O
their	O
range	O
.	O

Using	O
some	O
string	O
formatting	O
to	O
get	O
the	O
index	O
,	O
but	O
works	O
for	O
any	O
combination	O
of	O
months	O
(	O
as	O
long	O
as	O
the	O
first	O
month	O
is	O
explicitly	O
included	O
)	O
.	O

Any	O
suggestions	O
please	O
?	O

I	O
get	O
:	O
#CODE	O

I	O
understand	O
that	O
we	O
can	O
line	O
them	O
all	O
together	O
side	O
by	O
side	O
so	O
their	O
dates	O
match	O
and	O
loop	O
row	O
by	O
row	O
,	O
but	O
then	O
when	O
i	O
have	O
100k	O
different	O
securities	O
,	O
this	O
is	O
slow	O
in	O
memory	O
.	O

I	O
would	O
like	O
to	O
automate	O
this	O
table	O
so	O
If	O
I	O
change	O
my	O
parameters	O
in	O
my	O
code	O
,	O
I	O
get	O
a	O
new	O
table	O
with	O
that	O
new	O
data	O
.	O

But	O
I	O
get	O
,	O
which	O
I	O
cannot	O
understand	O
,	O
#CODE	O

Here's	O
the	O
product	O
:	O
#CODE	O

(	O
My	O
actual	O
problem	O
involves	O
parsing	O
strings	O
into	O
lists	O
,	O
then	O
checking	O
for	O
presents	O
of	O
a	O
1	O
or	O
0	O
in	O
one	O
list	O
and	O
if	O
so	O
marking	O
the	O
cosponsoring	O
element	O
in	O
the	O
other	O
list	O
with	O
a	O
asterix	O
,	O
but	O
I	O
didn't	O
want	O
to	O
put	O
that	O
in	O
my	O
example	O
and	O
it	O
is	O
long	O
and	O
harder	O
to	O
follow	O
.	O

What	O
is	O
the	O
error	O
you	O
get	O
?	O

So	O
first	O
chunk	O
is	O
stored	O
as	O
integer	O
and	O
in	O
second	O
chunk	O
gets	O
NaN	O
values	O
and	O
store	O
cannot	O
convert	O
NaN	O
to	O
integer	O

Just	O
play	O
around	O
with	O
it	O
to	O
get	O
it	O
right	O
.	O

I	O
get	O
the	O
following	O
error	O
:	O
#CODE	O

Edit	O
:	O
Here's	O
an	O
example	O
of	O
generating	O
a	O
close-enough	O
data	O
set	O
,	O
so	O
you	O
can	O
get	O
some	O
idea	O
of	O
what	O
I	O
mean	O
:	O
#CODE	O

@USER	O
that	O
means	O
you	O
have	O
non	O
string	O
values	O
mixed	O
in	O
;	O
you	O
need	O
to	O
specify	O
`	O
na=True	O
`	O
or	O
`	O
na=False	O
`	O
depending	O
on	O
what	O
those	O
values	O
are	O
.	O
see	O
my	O
edits	O
.	O

Preferably	O
,	O
use	O
Pandas	O
for	O
the	O
data	O
structure	O
and	O
Python	O
for	O
the	O
language	O
.	O

Then	O
as	O
in	O
the	O
first	O
point	O
,	O
I	O
would	O
like	O
to	O
calculate	O
the	O
number	O
of	O
continuous	O
up	O
and	O
down	O
sequences	O
from	O
the	O
previous	O
point	O
.	O

0	O
can	O
be	O
changed	O
to	O
1	O
or	O
other	O
values	O
later	O
in	O
the	O
code	O

and	O
in	O
the	O
instance	O
when	O
i	O
am	O
able	O
to	O
set	O
the	O
index	O
of	O
the	O
df	O
to	O
the	O
range	O
,	O
the	O
data	O
in	O
the	O
4	O
columns	O
change	O
to	O
NaN	O
since	O
they	O
have	O
no	O
data	O
that	O
matches	O
the	O
new	O
index	O
.	O

(	O
Note	O
that	O
this	O
produces	O
an	O
unrealistically	O
high	O
number	O
of	O
flooding	O
events	O
,	O
but	O
that's	O
just	O
because	O
of	O
how	O
the	O
sample	O
data	O
is	O
set	O
up	O
and	O
not	O
reflective	O
of	O
a	O
typical	O
pond	O
,	O
though	O
I'm	O
not	O
an	O
expert	O
on	O
pond	O
flooding	O
!	O
)	O
#CODE	O

What	O
output	O
do	O
you	O
get	O
from	O
this	O
?	O

`	O
Ideally	O
,	O
for	O
the	O
pages	O
that	O
have	O
multiple	O
groups	O
of	O
34	O
,	O
i'd	O
like	O
to	O
add	O
a	O
suffix	O
of	O
_1	O
,	O
_2	O
,	O
_3	O
,	O
etc	O
.	O

That	O
means	O
duplicating	O
values	O
from	O
cols	O
`	O
product_id	O
`	O
and	O
tem_name	O
`	O
as	O
long	O
as	O
there	O
are	O
items	O
in	O
list	O
`	O
prices	O
`	O
.	O

cool	O
,	O
but	O
I	O
get	O
a	O
syntax	O
error	O
for	O

problem	O
is	O
the	O
sum	O
i	O
now	O
get	O
is	O
lined	O
up	O
in	O
week	O
intervals	O
but	O
not	O
in	O
the	O
right	O
sequence	O
.	O
this	O
wouldn't	O
be	O
a	O
problem	O
but	O
i	O
need	O
to	O
get	O
to	O
the	O
month	O
of	O
each	O
date	O
in	O
order	O
to	O
do	O
the	O
next	O
step	O
i	O
guess	O
.	O

For	O
empty	O
date	O
cells	O
I	O
am	O
inserting	O
a	O
NaT	O
,	O
which	O
I	O
would	O
have	O
thought	O
would	O
be	O
fine	O
,	O
but	O
in	O
Oracle	O
that	O
is	O
becoming	O
some	O
weird	O
invalid	O
time	O
that	O
displays	O
as	O
"	O
0001-255-255	O
00:00	O
:	O
00	O
"	O
(	O
Something	O
like	O
MAXINT	O
or	O
0	O
being	O
converted	O
to	O
a	O
timestamp	O
I'm	O
guessing	O
?	O
)	O
#CODE	O

I	O
would	O
like	O
to	O
get	O
the	O
following	O
result	O
:	O
#CODE	O

so	O
in	O
all	O
both	O
suggestions	O
below	O
worked	O
for	O
me	O
:	O

I	O
get	O
the	O
following	O
error	O
message	O
:	O
#CODE	O

I	O
would	O
suggest	O
using	O
the	O
duplicated	O
method	O
on	O
the	O
Pandas	O
Index	O
itself	O
:	O
#CODE	O

I	O
would	O
like	O
to	O
group	O
the	O
x	O
values	O
into	O
equal	O
size	O
bins	O
,	O
and	O
for	O
each	O
bin	O
take	O
the	O
average	O
value	O
of	O
both	O
x	O
and	O
y	O
.	O

For	O
this	O
data	O
set	O
the	O
two	O
numbers	O
are	O
always	O
equal	O
.	O

Do	O
you	O
want	O
to	O
check	O
if	O
the	O
value	O
is	O
in	O
the	O
provided	O
bounds	O
and	O
return	O
a	O
boolean	O
True	O
/	O
False	O
array	O
,	O
or	O
you	O
want	O
to	O
represent	O
your	O
values	O
in	O
categories	O
represented	O
by	O
those	O
bounds	O
?	O

The	O
series	O
I'd	O
like	O
to	O
get	O
would	O
contain	O
:	O
#CODE	O

The	O
error	O
I	O
get	O
:	O
#CODE	O

How	O
do	O
you	O
deal	O
with	O
apparently	O
overlapping	O
date	O
ranges	O
?	O

There	O
are	O
more	O
columns	O
in	O
the	O
data	O
that	O
are	O
not	O
shown	O
above	O
,	O
and	O
using	O
this	O
code	O
causes	O
the	O
non-numeric	O
columns	O
to	O
drop	O
off	O
.	O

Any	O
suggestions	O
?	O

python	O
-	O
trying	O
to	O
get	O
a	O
new	O
pandas	O
release	O

In	O
the	O
process	O
of	O
creating	O
an	O
example	O
with	O
code	O
,	O
I	O
managed	O
to	O
get	O
it	O
working	O
.	O

Can	O
you	O
post	O
raw	O
data	O
and	O
example	O
code	O
that	O
demonstrates	O
this	O
'	O
cutting	O
'	O
off	O

Running	O
your	O
code	O
on	O
the	O
sample	O
data	O
produces	O
the	O
same	O
result	O
.	O

I	O
want	O
to	O
take	O
advantage	O
of	O
sortedness	O
since	O
with	O
very	O
large	O
series	O
merging	O
when	O
we	O
know	O
it's	O
sorted	O
should	O
be	O
linear	O
in	O
total	O
length	O
of	O
the	O
arrays	O
,	O
whereas	O
any	O
sort	O
will	O
be	O
non-linear	O
.	O

What	O
if	O
you	O
just	O
changed	O
the	O
index	O
from	O
date	O
/	O
status	O
to	O
date	O
/	O
var1	O
/	O
status	O
?	O

which	O
when	O
imported	O
into	O
pandas	O
data	O
frames	O
and	O
each	O
joined	O
to	O
a	O
common	O
timestamp	O
,	O
with	O
a	O
day	O
of	O
year	O
field	O
added	O
,	O
so	O
looking	O
something	O
like	O
:	O
#CODE	O

This	O
works	O
only	O
if	O
your	O
object	O
is	O
table	O
format	O
(	O
rather	O
than	O
fixed	O
format	O
)	O
.	O

Drop	O
range	O
of	O
columns	O
by	O
labels	O

You	O
could	O
put	O
```	O
[	O
'	O
col1	O
']```	O
at	O
the	O
end	O
to	O
get	O
an	O
int	O
.	O

Hence	O
,	O
the	O
width	O
of	O
each	O
bin	O
over	O
the	O
interval	O
from	O
[	O
-1	O
,	O
1	O
]	O
should	O
be	O
`	O
2	O
/	O
10=	O
0.20	O
`	O
;	O
however	O
,	O
the	O
graph	O
does	O
not	O
have	O
any	O
bins	O
with	O
a	O
width	O
of	O
0.20	O
.	O

For	O
encoding	O
training	O
data	O
you	O
can	O
use	O
fit_transform	O
which	O
will	O
discover	O
the	O
category	O
labels	O
and	O
create	O
appropriate	O
dummy	O
variables	O
.	O

I	O
think	O
you're	O
confusing	O
how	O
to	O
filter	O
here	O
,	O
if	O
you're	O
looking	O
for	O
a	O
specific	O
value	O
then	O
`	O
stock	O
[	O
stock	O
[	O
'	O
Whs	O
']	O
==	O
'	O
VKO	O
']`	O
will	O
return	O
only	O
the	O
rows	O
where	O
that	O
condition	O
is	O
satisfied	O
,	O
for	O
your	O
last	O
part	O
the	O
reason	O
you	O
get	O
an	O
empty	O
row	O
is	O
that	O
you're	O
slicing	O
the	O
first	O
3	O
rows	O
and	O
then	O
comparing	O
the	O
first	O
value	O
with	O
your	O
string	O
but	O
the	O
first	O
string	O
value	O
is	O
'	O
VKO	O
'	O
and	O
not	O
'	O
ZZZ	O
'	O
,	O
you	O
should	O
do	O
this	O
:	O
`	O
stock	O
[	O
stock	O
[	O
'	O
Whs	O
']	O
==	O
'	O
ZZZ	O
']`	O
to	O
filter	O
the	O
resuls	O
first	O

I	O
had	O
changed	O
the	O
data	O
on	O
the	O
local	O
file	O
.	O

Note	O
that	O
this	O
is	O
slightly	O
different	O
as	O
we	O
are	O
returning	O
the	O
first	O
index	O
here	O
(	O
and	O
not	O
the	O
normally	O
returned	O
last	O
,	O
youy	O
could	O
do	O
either	O
)	O
.	O

I'm	O
not	O
sure	O
how	O
the	O
archive	O
block	O
reading	O
works	O
and	O
how	O
much	O
data	O
it	O
loads	O
into	O
memory	O
,	O
but	O
it's	O
clear	O
that	O
you	O
will	O
have	O
to	O
somehow	O
control	O
the	O
size	O
of	O
the	O
chunks	O
.	O

Regarding	O
nesting	O
of	O
functions	O
:	O
if	O
you	O
believe	O
a	O
function	O
has	O
a	O
general	O
purpose	O
or	O
is	O
reusable	O
,	O
then	O
it	O
should	O
be	O
defined	O
at	O
the	O
top	O
level	O
of	O
a	O
module	O
or	O
some	O
place	O
where	O
other	O
functions	O
can	O
call	O
it	O
.	O

I	O
get	O
an	O
error	O
saying	O
:	O
lambda	O
(	O
)	O
takes	O
exactly	O
1	O
argument	O
(	O
3	O
given	O
)	O

You	O
can	O
get	O
the	O
list	O
of	O
columns	O
with	O
:	O
#CODE	O

One	O
is	O
I	O
only	O
wanted	O
to	O
get	O
the	O
mean	O
of	O
the	O
next	O
rows	O
that	O
relate	O
to	O
the	O
same	O
group	O
.	O

By	O
default	O
this	O
will	O
add	O
a	O
column	O
of	O
integers	O
(	O
because	O
R	O
factors	O
are	O
encoded	O
as	O
integers	O
)	O
.	O

I	O
am	O
trying	O
to	O
calculate	O
the	O
percent	O
change	O
by	O
month	O
for	O
each	O
product	O
.	O

If	O
i	O
want	O
only	O
USA	O
Equities	O
vs	O
all	O
other	O
equity	O
and	O
not	O
the	O
enitre	O
89	O
columns	O
how	O
do	O
i	O
do	O
it	O
?	O

Pandas	O
:	O
Efficient	O
way	O
to	O
get	O
first	O
row	O
with	O
element	O
that	O
is	O
smaller	O
than	O
a	O
given	O
value	O

I	O
first	O
thought	O
this	O
was	O
a	O
spacing	O
issue	O
in	O
the	O
columns	O
values	O
,	O
so	O
I	O
replaced	O
them	O
with	O
underscores	O
,	O
but	O
it	O
also	O
doesn't	O
work	O
in	O
columns	O
which	O
only	O
contain	O
a	O
single	O
word	O
and	O
no	O
spaces	O
?	O

Are	O
you	O
getting	O
the	O
values	O
from	O
the	O
GUI	O
ok	O
,	O
but	O
your	O
calculations	O
are	O
returning	O
nothin	O
?	O

This	O
is	O
not	O
precisely	O
what	O
I'm	O
after	O
,	O
but	O
I	O
think	O
I'll	O
have	O
to	O
write	O
a	O
loop	O
top	O
get	O
that	O
.	O

Hence	O
,	O
the	O
width	O
of	O
each	O
bin	O
over	O
the	O
interval	O
from	O
[	O
-1	O
,	O
1	O
]	O
should	O
be	O
`	O
2	O
/	O
10=	O
0.20	O
`	O
;	O
however	O
,	O
the	O
graph	O
does	O
not	O
have	O
any	O
bins	O
with	O
a	O
width	O
of	O
0.20	O
.	O

I'm	O
having	O
a	O
problem	O
trying	O
to	O
get	O
a	O
character	O
count	O
column	O
of	O
the	O
string	O
values	O
in	O
another	O
column	O
,	O
and	O
haven't	O
figured	O
out	O
how	O
to	O
do	O
it	O
efficiently	O
.	O

I	O
think	O
I	O
get	O
the	O
idea	O
.	O

How	O
can	O
I	O
get	O
the	O
position	O
of	O
index	O
`	O
18	O
`	O
?	O

When	O
using	O
`	O
groupy-apply	O
`	O
,	O
instead	O
of	O
dropping	O
the	O
group	O
key	O
index	O
using	O
:	O
#CODE	O

No	O
real	O
advantage	O
if	O
there	O
are	O
just	O
two	O
categories	O
:	O
#CODE	O

@USER	O
I	O
suspect	O
something	O
else	O
is	O
going	O
on	O
then	O
,	O
because	O
when	O
I	O
memory	O
profile	O
this	O
way	O
(	O
with	O
`	O
drop	O
`)	O
using	O
the	O
snippet	O
that	O
Michael	O
Laszlo	O
posted	O
,	O
I	O
do	O
not	O
see	O
memory	O
growth	O
.	O

Of	O
course	O
you	O
may	O
not	O
like	O
the	O
index	O
as	O
tuples	O
;	O
you	O
could	O
reset	O
the	O
index	O
within	O
the	O
list	O
comprehension	O
to	O
get	O
the	O
following	O
if	O
you	O
prefer	O
(	O
for	O
example	O
,	O
this	O
if	O
for	O
part	O
1	O
):	O
#CODE	O

Here	O
is	O
how	O
I	O
am	O
trying	O
to	O
get	O
the	O
output	O
to	O
look	O
like	O
:	O
#CODE	O

memory	O
efficient	O
Python	O
(	O
pandas	O
)	O
aggregates	O
of	O
categories	O
from	O
one	O
csv	O
file	O
per	O
period	O

But	O
this	O
last	O
line	O
generate	O
the	O
error	O
message	O
:	O
`	O
no	O
item	O
named	O
timestamp	O
`	O
.	O

The	O
problem	O
here	O
,	O
well	O
the	O
biggest	O
one	O
,	O
is	O
that	O
your	O
`	O
data	O
`	O
is	O
string	O
,	O
not	O
valid	O
data	O
structure	O
,	O
same	O
thing	O
with	O
dictionary	O
inside	O
it	O
,	O
you	O
creating	O
strings	O
,	O
not	O
data	O
structures	O
.	O

I	O
also	O
want	O
to	O
create	O
a	O
new	O
column	O
that	O
shows	O
the	O
difference	O
in	O
days	O
between	O
the	O
end	O
and	O
begin	O
dates	O
.	O

In	O
the	O
next	O
column	O
(	O
B	O
)	O
,	O
I	O
want	O
to	O
create	O
an	O
indexed	O
series	O
that	O
begins	O
at	O
1000	O
based	O
on	O
the	O
percent	O
changes	O
.	O

I	O
can't	O
use	O
fixed	O
position	O
to	O
slice	O
it	O
.	O

This	O
gets	O
you	O
to	O
where	O
I	O
am	O
.	O

I	O
am	O
using	O
this	O
to	O
generate	O
nodes	O
in	O
a	O
graph	O
,	O
if	O
x1	O
,	O
x2	O
are	O
not	O
exactly	O
equal	O
,	O
networkx	O
recognizes	O
them	O
as	O
different	O
nodes	O
,	O
if	O
x1=x2	O
,	O
i	O
get	O
a	O
recombinant	O
tree	O
which	O
is	O
what	O
i	O
want	O
.	O

My	O
example	O
was	O
not	O
good	O
enough	O
,	O
as	O
your	O
script	O
smartly	O
took	O
the	O
size	O
from	O
the	O
length	O
of	O
the	O
df	O
.	O

iPython's	O
Rmagic	O
is	O
already	O
able	O
to	O
perform	O
an	O
automagic	O
conversion	O
between	O
the	O
two	O
in	O
a	O
number	O
of	O
situations	O
,	O
and	O
might	O
be	O
a	O
good	O
way	O
to	O
get	O
familiar	O
with	O
Python	O
.	O

So	O
if	O
your	O
dataset	O
is	O
really	O
large	O
,	O
perhaps	O
store	O
them	O
first	O
in	O
on-disk	O
database	O
/	O
HDF	O
rather	O
than	O
csv	O
file	O
and	O
sort	O
them	O
there	O
,	O
and	O
then	O
query	O
.	O

One	O
had	O
no	O
problem	O
at	O
all	O
(	O
the	O
xlsx	O
file	O
,	O
example	O
2	O
)	O
and	O
the	O
other	O
(	O
xls	O
,	O
example	O
1	O
)	O
had	O
a	O
difference	O
between	O
the	O
columns	O
.	O

then	O
with	O
the	O
`	O
sorted	O
`	O
function	O
and	O
`	O
datetime	O
`	O
module	O
(	O
remember	O
the	O
`	O
sorted	O
`	O
function	O
change	O
the	O
`	O
data	O
`	O
it	O
self	O
)	O
#CODE	O

BUT	O
,	O
I	O
get	O
the	O
"	O
SettingWithCopyWarning	O
"	O
:	O

This	O
is	O
machine	O
generated	O
data	O
.	O

You	O
can't	O
use	O
`	O
or	O
`	O
with	O
arrays	O
,	O
if	O
you	O
try	O
this	O
you	O
get	O
an	O
error	O
`	O
ValueError	O
:	O
The	O
truth	O
value	O
of	O
a	O
Series	O
is	O
ambiguous	O
.	O

Incidentally	O
,	O
this	O
is	O
the	O
same	O
result	O
that	O
you	O
would	O
get	O
with	O
the	O
Spearman	O
R	O
coefficient	O
as	O
well	O
.	O

How	O
could	O
I	O
do	O
to	O
have	O
exactly	O
one	O
calendar	O
year	O
between	O
dates	O
in	O
spite	O
of	O
leap	O
years	O
?	O

"	O
However	O
,	O
we	O
still	O
have	O
one	O
large	O
difference	O
.	O

All	O
that	O
remains	O
is	O
to	O
merged	O
the	O
contents	O
of	O
the	O
second-level	O
dictionaries	O
:	O
#CODE	O

I	O
am	O
trying	O
to	O
create	O
a	O
single	O
image	O
with	O
heatmaps	O
representing	O
the	O
correlation	O
of	O
features	O
of	O
data	O
points	O
for	O
each	O
label	O
separately	O
.	O

I	O
have	O
two	O
Series	O
which	O
have	O
a	O
format	O
equal	O
to	O
this	O
:	O
#CODE	O

However	O
,	O
when	O
I	O
do	O
the	O
following	O
,	O
the	O
error	O
does	O
not	O
show	O
up	O
and	O
I	O
get	O
the	O
expected	O
result	O
:	O
#CODE	O

As	O
brackets	O
are	O
part	O
of	O
the	O
regex	O
syntax	O
if	O
you're	O
trying	O
to	O
match	O
literal	O
brackets	O
you	O
need	O
to	O
escape	O
them	O
:	O
#CODE	O

Yeah	O
,	O
the	O
best	O
idea	O
I've	O
had	O
this	O
whole	O
time	O
was	O
to	O
actually	O
sign	O
up	O
to	O
SO	O
so	O
I	O
can	O
post	O
my	O
own	O
questions	O
instead	O
of	O
trying	O
to	O
funble	O
my	O
way	O
through	O
problems	O
by	O
patching	O
together	O
answers	O
to	O
other	O
peoples	O
questions	O
-	O
sometimes	O
what	O
I	O
need	O
just	O
isn't	O
covered	O
in	O
other	O
people's	O
questions	O
.	O

But	O
when	O
I	O
try	O
and	O
import	O
pandas	O
I	O
get	O
:	O
#CODE	O

However	O
,	O
I	O
can't	O
get	O
the	O
column	O
to	O
fill	O
up	O
with	O
the	O
appropriate	O
user	O
inputted	O
value	O
.	O

Also	O
I'm	O
not	O
getting	O
any	O
traceback	O
messages	O
but	O
I	O
think	O
I	O
have	O
an	O
idea	O
of	O
where	O
my	O
problem	O
may	O
be	O
.	O

Basically	O
I'm	O
trying	O
to	O
get	O
at	O
this	O
:	O
#CODE	O

So	O
this	O
is	O
not	O
a	O
fully	O
working	O
answer	O
,	O
but	O
maybe	O
it	O
can	O
be	O
extended	O
to	O
ultimatively	O
get	O
you	O
there	O
.	O

I	O
have	O
a	O
pandas	O
Series	O
holding	O
one	O
numpy	O
array	O
per	O
entry	O
(	O
same	O
length	O
for	O
all	O
entries	O
)	O
and	O
I	O
would	O
like	O
to	O
convert	O
this	O
to	O
a	O
2D	O
numpy	O
array	O
.	O

For	O
example	O
,	O
let's	O
say	O
I	O
want	O
to	O
select	O
50%	O
(	O
but	O
this	O
could	O
change	O
)	O
.	O

For	O
this	O
purpose	O
I	O
would	O
like	O
to	O
find	O
the	O
soonest	O
date	O
(	O
month	O
)	O
and	O
from	O
there	O
start	O
counting	O
months	O
and	O
their	O
averages	O
.	O

You've	O
changed	O
your	O
data	O
,	O
so	O
the	O
script	O
as	O
written	O
doesn't	O
work	O
.	O

Hopefully	O
you'll	O
get	O
an	O
answer	O
soon	O
.	O

This	O
should	O
get	O
you	O
to	O
the	O
point	O
in	O
your	O
code	O
where	O
you	O
start	O
dropping	O
columns	O
and	O
start	O
concatenating	O
.	O

So	O
,	O
if	O
k	O
were	O
equal	O
to	O
2	O
,	O
and	O
this	O
were	O
my	O
data	O
frame	O
:	O
#CODE	O

It	O
seems	O
as	O
though	O
the	O
second	O
approach	O
,	O
using	O
"	O
where	O
"	O
is	O
only	O
returning	O
data	O
from	O
the	O
last	O
few	O
appended	O
files	O
,	O
while	O
the	O
first	O
approach	O
is	O
returning	O
much	O
more	O
data	O
.	O

I	O
have	O
to	O
improve	O
this	O
to	O
get	O
rid	O
of	O
redundancy	O
,	O
and	O
I	O
am	O
not	O
sure	O
how	O
to	O
go	O
about	O
this	O
.	O

Are	O
you	O
able	O
to	O
post	O
raw	O
data	O
and	O
code	O
to	O
reproduce	O
this	O
issue	O
?	O

The	O
relative	O
size	O
between	O
consecutive	O
levels	O
.	O

This	O
will	O
potentially	O
cater	O
the	O
corner	O
cases	O
if	O
you	O
happen	O
to	O
have	O
conditions	O
like	O
:	O
"	O
value	O
"	O
360	O
then	O
+360	O
else	O
-360	O
but	O
the	O
sequence	O
of	O
the	O
update	O
will	O
cause	O
the	O
results	O
reapply	O
,	O
ie	O
.	O

I'll	O
put	O
an	O
example	O
of	O
what	O
I'm	O
suggesting	O
in	O
my	O
answer	O
.	O

But	O
you	O
said	O
you	O
want	O
only	O
the	O
time	O
points	O
from	O
the	O
longest	O
`	O
csv	O
`	O
.	O

replacing	O
this	O
in	O
code	O
just	O
drop	O
those	O
whole	O
rows	O
...	O

Thank	O
you	O
for	O
response	O
and	O
for	O
helping	O
me	O
get	O
to	O
next	O
level	O
of	O
pyhton	O
,	O
great	O
stuff	O
!	O

For	O
instance	O
,	O
you	O
can	O
insert	O
new	O
values	O
into	O
the	O
index	O
(	O
and	O
even	O
choose	O
what	O
value	O
it	O
should	O
have	O
):	O
#CODE	O

What	O
I	O
need	O
to	O
do	O
is	O
to	O
compute	O
the	O
average	O
temperature	O
for	O
every	O
run	O
,	O
averaging	O
all	O
the	O
temperature	O
measurements	O
belonging	O
to	O
a	O
run	O
.	O

@USER	O
I	O
get	O
`	O
Type	O
Error	O
:	O
'	O
bool	O
'	O
object	O
is	O
not	O
callable	O
`	O
when	O
I	O
do	O
that	O

For	O
your	O
specific	O
request	O
of	O
entries	O
between	O
12:00	O
to	O
13:00	O
for	O
every	O
single	O
day	O
,	O
you	O
can	O
fetch	O
the	O
rows	O
with	O
:	O
#CODE	O

Also	O
,	O
their	O
order	O
matters	O
(	O
they	O
are	O
sorted	O
by	O
decreasing	O
standard	O
deviation	O
across	O
rows	O
and	O
should	O
appear	O
in	O
this	O
order	O
in	O
the	O
heatmap	O
.	O
)	O

I'm	O
not	O
averse	O
the	O
reformatting	O
the	O
data	O
in	O
Pandas	O
-->	O
dumping	O
to	O
CSV	O
-->	O
importing	O
to	O
NetworkX	O
,	O
but	O
it	O
seems	O
as	O
if	O
I	O
should	O
be	O
able	O
to	O
generate	O
the	O
edges	O
from	O
the	O
index	O
and	O
the	O
nodes	O
from	O
the	O
values	O
.	O

What	O
is	O
the	O
simplest	O
way	O
to	O
get	O
a	O
sum	O
of	O
only	O
numbers	O
across	O
the	O
entire	O
frame	O
?	O

I	O
think	O
you	O
mean	O
a	O
Lorenz	O
plot	O
:	O
#URL	O
This	O
would	O
make	O
sense	O
then	O
as	O
it	O
requires	O
a	O
specific	O
preordering	O
of	O
the	O
data	O
.	O

Any	O
suggestions	O
?	O

Despite	O
the	O
title	O
,	O
similar	O
problems	O
can	O
occur	O
with	O
other	O
operating	O
systems	O
if	O
you	O
mix	O
32-bit	O
and	O
64-fit	O
versions	O
.	O

Use	O
`	O
select_as_coordinates	O
`	O
to	O
actually	O
execute	O
your	O
query	O
;	O
this	O
returns	O
an	O
`	O
Int64Index	O
`	O
of	O
the	O
row	O
number	O
(	O
the	O
coordinates	O
)	O
.	O

Somehow	O
create	O
a	O
mapping	O
so	O
that	O
instead	O
of	O
the	O
labels	O
being	O
29	O
,	O
30	O
etc	O
,	O
they	O
say	O
"	O
week	O
29	O
"	O
,	O
"	O
Week	O
30	O
"	O
etc	O
.	O

The	O
fix	O
you	O
describe	O
would	O
work	O
,	O
of	O
course	O
,	O
but	O
then	O
I	O
could	O
skip	O
pandas	O
all-together	O
and	O
directly	O
plot	O
the	O
results	O
of	O
my	O
individual	O
simulations	O
.	O

These	O
two	O
timezones	O
have	O
different	O
names	O
but	O
represent	O
the	O
same	O
thing	O
,	O
however	O

which	O
would	O
just	O
change	O
the	O
last	O
data	O
point	O
.	O

What	O
do	O
you	O
mean	O
by	O
reproducible	O
example	O
?	O

But	O
thought	O
i'd	O
make	O
it	O
clear	O
what	O
my	O
next	O
objective	O
was	O
,	O
in	O
case	O
someone	O
could	O
illuminate	O
a	O
better	O
method	O
to	O
get	O
there	O
.	O

What	O
output	O
do	O
you	O
get	O
when	O
you	O
just	O
enter	O
`	O
pd	O
`	O
in	O
the	O
console	O
?	O

When	O
I	O
train	O
on	O
each	O
label	O
I	O
get	O
et	O
better	O
than	O
73%	O
on	O
each	O
label	O
.	O

but	O
what	O
value	O
does	O
it	O
grab	O
when	O
indexing	O
?	O
in	O
other	O
words	O
,	O
if	O
i'm	O
just	O
testing	O
one	O
side	O
i'll	O
get	O
the	O
value	O
corresponding	O
to	O
that	O
row	O
if	O
true	O
.	O
since	O
both	O
sides	O
could	O
be	O
true	O
and	O
one	O
of	O
them	O
is	O
always	O
true	O
,	O
which	O
row's	O
values	O
will	O
be	O
selected	O
?	O

However	O
,	O
this	O
is	O
a	O
bug	O
as	O
you	O
should	O
get	O
an	O
error	O
.	O

guys	O
at	O
least	O
tell	O
me	O
why	O
i	O
am	O
getting	O
downvoted	O
?	O

In	O
some	O
cases	O
the	O
data	O
might	O
be	O
out	O
of	O
sync	O
which	O
makes	O
direct	O
comparisons	O
difficult	O
.	O

I	O
would	O
like	O
to	O
generate	O
a	O
matrix	O
which	O
contains	O
the	O
output	O
of	O
the	O
function	O
for	O
every	O
combination	O
of	O
X	O
and	O
Y	O
.	O

I'm	O
trying	O
to	O
get	O
the	O
data	O
to	O
in	O
the	O
column	O
"	O
Structure	O
"	O
to	O
repeat	O
the	O
row	O
labels	O
so	O
it	O
look	O
like	O
this	O
:	O
#CODE	O

decision	O
for	O
single	O
rows	O
to	O
get	O
converted	O
into	O
a	O
series	O
-	O
why	O
not	O
a	O

I	O
get	O
#CODE	O

More	O
info	O
as	O
requested	O
#CODE	O

[	O
Their	O
product	O
page	O
]	O
(	O
#URL	O
)	O
holds	O
the	O
answer	O
.	O

Next	O
,	O
you	O
wish	O
to	O
get	O
the	O
specific	O
groups	O
from	O
this	O
`	O
grouped	O
`	O
object	O
.	O

To	O
get	O
datetime64	O
that	O
uses	O
seconds	O
directly	O
:	O
#CODE	O

The	O
only	O
option	O
you	O
may	O
have	O
is	O
to	O
setup	O
your	O
data	O
structures	O
to	O
be	O
light	O
weight	O
so	O
each	O
worker	O
isn't	O
boated	O
by	O
redundant	O
copies	O
of	O
the	O
same	O
data	O
or	O
excessive	O
amounts	O
of	O
data	O
which	O
might	O
be	O
better	O
off	O
split	O
across	O
different	O
workers	O
.	O

Inplace	O
dropping	O
seems	O
more	O
like	O
idiomatic	O
pandas	O
to	O
me	O
than	O
making	O
a	O
copy	O
only	O
to	O
instantly	O
garbage	O
collect	O
the	O
now-defunct	O
original	O
.	O

And	O
fortunately	O
,	O
these	O
days	O
,	O
`	O
-pylab	O
`	O
has	O
been	O
deprecated	O
and	O
using	O
`	O
--	O
matplotlib	O
`	O
and	O
importing	O
`	O
pylab	O
`	O
manually	O
is	O
encouraged	O
.	O

Are	O
you	O
sure	O
you	O
don't	O
mean	O
`	O
range	O
(	O
1	O
,	O
len	O
(	O
DF	O
)):	O
`	O
?	O

Where	O
"	O
timeblock	O
"	O
#1	O
will	O
include	O
the	O
first	O
4:59	O
minutes	O
of	O
observation	O
period	O
#1	O
,	O
#2	O
will	O
include	O
5:00	O
to	O
9:59	O
minutes	O
...	O
through	O
to	O
25:00	O
and	O
over	O
,	O
for	O
each	O
observation	O
period	O
.	O

It	O
seems	O
like	O
I'm	O
maybe	O
getting	O
confused	O
between	O
the	O
underlying	O
data	O
and	O
views	O
on	O
it	O
.	O

When	O
I	O
try	O
specifying	O
index_col=0	O
,	O
as	O
some	O
examples	O
in	O
the	O
documentation	O
do	O
,	O
I	O
get	O
a	O
"	O
IndexError	O
:	O
list	O
index	O
out	O
of	O
range	O
"	O
error	O
,	O
which	O
was	O
a	O
solution	O
to	O
several	O
related	O
questions	O
but	O
for	O
some	O
reason	O
isn't	O
working	O
for	O
me	O
.	O

That's	O
what	O
I	O
thought	O
about	O
my	O
original	O
code	O
but	O
for	O
some	O
reason	O
when	O
I	O
check	O
len	O
(	O
Sframe	O
)	O
at	O
the	O
end	O
in	O
the	O
main	O
code	O
,	O
it	O
still	O
has	O
the	O
duplicate	O
values	O
even	O
if	O
the	O
conditional	O
statement	O
applies	O
option	O
2	O
to	O
remove	O
duplicates	O
.	O

The	O
main	O
thing	O
I	O
need	O
to	O
do	O
is	O
to	O
group	O
the	O
days	O
by	O
week	O
such	O
that	O
I	O
can	O
get	O
sum	O
of	O
the	O
data	O
to	O
be	O
by	O
week	O
.	O

To	O
move	O
the	O
third	O
row	O
to	O
the	O
first	O
,	O
you	O
can	O
create	O
an	O
index	O
moving	O
the	O
target	O
row	O
to	O
the	O
first	O
element	O
.	O

So	O
using	O
your	O
approach	O
,	O
how	O
can	O
I	O
:	O
1	O
)	O
plot	O
the	O
scores	O
as	O
a	O
histogram	O
in	O
a	O
memory-conscious	O
way	O
,	O
and	O
2	O
)	O
extract	O
the	O
scores	O
belonging	O
to	O
the	O
certain	O
cell	O
types	O
to	O
plot	O
those	O
as	O
well	O
?	O

I	O
have	O
a	O
CSV	O
file	O
,	O
I	O
wanted	O
to	O
filter	O
it	O
where	O
I	O
keep	O
just	O
rows	O
where	O
I	O
have	O
values	O
in	O
row	O
"	O
d	O
"	O
bigger	O
then	O
0	O
.	O

Simulations	O
can	O
be	O
repeated	O
for	O
different	O
scenarios	O
and	O
each	O
one	O
of	O
these	O
scenarios	O
will	O
produce	O
a	O
different	O
hourly	O
set	O
of	O
data	O
for	O
each	O
room	O
and	O
each	O
variable	O
.	O

To	O
get	O
around	O
this	O
,	O
I'm	O
passing	O
in	O
a	O
large	O
number	O
for	O
the	O
max_results	O
parameter	O
and	O
specifying	O
a	O
chunksize	O
.	O

You	O
get	O
back	O
a	O
float	O
because	O
each	O
row	O
contains	O
a	O
mix	O
of	O
`	O
float	O
`	O
and	O
`	O
int	O
`	O
types	O
.	O

Should	O
I	O
use	O
something	O
different	O
.	O

The	O
best	O
would	O
be	O
to	O
convert	O
that	O
one	O
file	O
to	O
a	O
an	O
actual	O
comma	O
(	O
semicolon	O
or	O
other	O
)	O
separated	O
file	O
or	O
make	O
sure	O
that	O
compound	O
values	O
are	O
quoted	O
(	O
"	O
Alabama	O
County	O
")	O
and	O
then	O
specify	O
the	O
quotechar	O
:	O
#CODE	O

I	O
would	O
like	O
to	O
calculate	O
the	O
average	O
of	O
time	O
per	O
org	O
per	O
cluster	O
.	O

I	O
found	O
a	O
way	O
which	O
seems	O
very	O
inefficient	O
(	O
stacking	O
and	O
unstacking	O
which	O
will	O
create	O
many	O
many	O
columns	O
in	O
case	O
of	O
millions	O
of	O
categories	O
)	O
.	O

I	O
was	O
going	O
to	O
suggest	O
cumcount	O
and	O
tail	O
(	O
1	O
)	O
,	O
but	O
you're	O
after	O
something	O
else	O
(	O
these	O
would	O
be	O
much	O
faster	O
)	O
.	O

This	O
is	O
because	O
working	O
with	O
dictionaries	O
is	O
so	O
easy	O
and	O
thinking	O
of	O
them	O
like	O
simple	O
dicts	O
often	O
means	O
you	O
can	O
find	O
a	O
solution	O
to	O
an	O
issue	O
without	O
having	O
to	O
get	O
too	O
deep	O
into	O
pandas	O
.	O

If	O
you're	O
trying	O
to	O
slice	O
each	O
string	O
to	O
get	O
the	O
substring	O
from	O
5	O
to	O
7	O
,	O
you	O
need	O
a	O
`	O
:	O
`	O
,	O
not	O
a	O
`	O
,	O
`	O
:	O
#CODE	O

However	O
,	O
since	O
each	O
of	O
your	O
new	O
`	O
DataFrames	O
`	O
is	O
a	O
summary	O
of	O
a	O
single	O
customer	O
,	O
I	O
would	O
suggest	O
writing	O
one	O
function	O
that	O
can	O
return	O
all	O
of	O
your	O
desired	O
results	O
in	O
a	O
single	O
`	O
Series	O
`	O
.	O

I	O
want	O
to	O
get	O
statistics	O
of	O
debt_ratio	O
based	O
on	O
subgroups	O
of	O
market_capitalization	O
.	O

Any	O
ideas	O
why	O
this	O
error	O
might	O
be	O
showing	O
up	O
so	O
I	O
can	O
know	O
what	O
to	O
go	O
after	O
to	O
fix	O
?	O

I	O
believe	O
it	O
is	O
getting	O
at	O
what	O
I	O
want	O
.	O

I	O
haven't	O
done	O
any	O
stress	O
testing	O
but	O
I'd	O
imagine	O
this	O
could	O
get	O
slow	O
on	O
very	O
large	O
DataFrames	O
.	O

Is	O
there	O
a	O
quick	O
way	O
to	O
sort	O
my	O
data	O
by	O
a	O
given	O
column	O
that	O
only	O
takes	O
chunks	O
into	O
account	O
and	O
doesn't	O
require	O
loading	O
entire	O
datasets	O
in	O
memory	O
?	O

Take	O
a	O
look	O
at	O
the	O
regex	O
docs	O
.	O

Any	O
logic	O
requirements	O
(	O
like	O
comparing	O
elem+1	O
to	O
elem	O
)	O
should	O
be	O
in	O
your	O
question	O
so	O
there	O
is	O
no	O
confusion	O
.	O

I'd	O
like	O
to	O
get	O
a	O
list	O
as	O
`	O
[	O
'	O
abcd	O
'	O
,	O
'	O
ddse	O
'	O
,	O
'	O
123d	O
'	O
,	O
'	O
aaaaa*	O
']`	O
.	O

but	O
get	O
the	O
following	O
error	O
:	O

I	O
get	O
#CODE	O

Ideally	O
the	O
question	O
would	O
provide	O
a	O
self-contained	O
piece	O
of	O
code	O
generating	O
the	O
data	O
structure	O
,	O
or	O
even	O
just	O
something	O
like	O
`	O
df	O
=	O
[[	O
1	O
,	O
2	O
]	O
,	O
[	O
2	O
,	O
3	O
]	O
,	O
[	O
4	O
,	O
5	O
]]`	O
,	O
enough	O
to	O
try	O
to	O
get	O
an	O
answer	O
without	O
diving	O
into	O
panda	O
.	O

I'm	O
running	O
daily	O
simulations	O
in	O
a	O
batch	O
:	O
I	O
do	O
365	O
simluations	O
to	O
get	O
results	O
for	O
a	O
full	O
year	O
.	O

I	O
edited	O
my	O
answer	O
to	O
use	O
capwords	O
per	O
your	O
suggestion	O
,	O
that	O
fixed	O
the	O
problem	O
I	O
missed	O
where	O
it	O
capitalized	O
the	O
'	O
s	O
'	O
in	O
Guy's	O
Name	O
.	O

Does	O
not	O
work	O
exactly	O
right	O
since	O
categories	O
can	O
be	O
mixed	O
like	O
that	O
so	O
it	O
will	O
produce	O
more	O
duplications	O

Essentially	O
,	O
I	O
want	O
to	O
look	O
at	O
quintiles	O
(	O
since	O
there	O
are	O
5	O
days	O
in	O
a	O
business	O
week	O
)	O
rank	O
1	O
and	O
5	O
and	O
see	O
how	O
they	O
change	O
from	O
week	O
to	O
week	O
.	O

Doing	O
this	O
transformation	O
for	O
500,000	O
file	O
takes	O
time	O
.	O

The	O
requirement	O
isn't	O
easy	O
to	O
wrap	O
once	O
mind	O
around	O
,	O
so	O
sorry	O
If	O
I	O
am	O
not	O
being	O
clear	O
.	O

My	O
question	O
is	O
,	O
from	O
`	O
result	O
`	O
how	O
can	O
I	O
get	O
the	O
column	O
index	O
of	O
the	O
first	O
level	O
as	O
list	O
:	O
#CODE	O

This	O
function	O
will	O
then	O
work	O
out	O
the	O
maximum	O
,	O
minimum	O
,	O
and	O
return	O
rages	O
of	O
values	O
based	O
on	O
the	O
fact	O
I	O
want	O
5	O
categories	O
:	O
(	O
1	O
,	O
2	O
)	O
,	O
(	O
3	O
,	O
4	O
)	O
,	O
(	O
5	O
,	O
6	O
)	O
,	O
(	O
7	O
,	O
8)	O
,	O
(	O
9	O
,	O
10	O
)	O
.	O

Any	O
suggestions	O
of	O
a	O
better	O
way	O
?	O

In	O
addition	O
this	O
is	O
unlikely	O
to	O
be	O
only	O
time	O
I	O
have	O
to	O
do	O
this	O
so	O
being	O
able	O
to	O
change	O
the	O
numbers	O
and	O
columns	O
i'm	O
interested	O
in	O
would	O
be	O
good	O
.	O

And	O
at	O
that	O
time	O
i	O
think	O
the	O
state	O
becomes	O
bad	O
,	O
so	O
subsequent	O
calls	O
will	O
lead	O
to	O
exceptions	O
like	O
these	O
:	O
#CODE	O

Let	O
me	O
fix	O
that	O
and	O
return	O
to	O
this	O
issue	O
,	O
as	O
I	O
have	O
read	O
something	O
about	O
format	O
changes	O
between	O
10	O
and	O
12	O
.	O

All	O
the	O
data	O
,	O
columns	O
222	O
and	O
333	O
are	O
offset	O
as	O
required	O
,	O
but	O
it	O
isn't	O
even	O
the	O
same	O
size	O
as	O
the	O
output	O
in	O
the	O
first	O
order	O
.	O

But	O
I	O
can	O
only	O
get	O
this	O
:	O
#CODE	O

I	O
have	O
summary-level	O
data	O
of	O
the	O
count	O
of	O
people	O
by	O
age	O
group	O
,	O
city	O
,	O
income	O
and	O
the	O
industry	O
in	O
which	O
they	O
work	O
,	O
or	O
in	O
this	O
case	O
four	O
dimensions	O
.	O

@USER	O
:	O
Yes	O
,	O
it	O
can	O
as	O
I	O
said	O
,	O
"	O
They	O
all	O
suck	O
in	O
different	O
ways	O
"	O
.	O

Error	O
:	O
list	O
indices	O
must	O
be	O
integers	O
,	O
not	O
Series	O

Just	O
so	O
that	O
it	O
does	O
not	O
fail	O
silently	O
if	O
the	O
wrong	O
kind	O
of	O
data	O
is	O
passed	O
in	O
.	O

If	O
you	O
want	O
to	O
take	O
advantage	O
of	O
NumPy	O
/	O
Pandas	O
to	O
perform	O
fast	O
(	O
er	O
)	O
calculations	O
you	O
must	O
keep	O
the	O
data	O
in	O
a	O
NumPy	O
array	O
or	O
Pandas	O
NDFrame	O
.	O

Your	O
benchmark	O
is	O
actually	O
too	O
small	O
to	O
show	O
the	O
real	O
difference	O
.	O

I	O
was	O
able	O
to	O
resolve	O
this	O
by	O
opening	O
/	O
closing	O
a	O
connection	O
each	O
time	O
i	O
need	O
to	O
execute	O
a	O
query	O
.	O

I	O
used	O
a	O
chunksize	O
of	O
4	O
to	O
make	O
the	O
grouping	O
noticeable	O
on	O
the	O
small	O
dataset	O
;	O
you'll	O
want	O
to	O
change	O
it	O
to	O
90000	O
for	O
your	O
real	O
dataset	O
.	O

It's	O
mostly	O
trial	O
and	O
error	O
for	O
me	O
,	O
plus	O
knowledge	O
that	O
you	O
can	O
do	O
a	O
lot	O
just	O
with	O
```	O
rank	O
```	O
and	O
```	O
count	O
```	O
,	O
which	O
are	O
both	O
pretty	O
fast	O
.	O

And	O
those	O
columns	O
which	O
have	O
differing	O
values	O
:	O
#CODE	O

(	O
I	O
will	O
use	O
logistic	O
regression	O
and	O
random	O
forest	O
to	O
do	O
the	O
prediction	O
,	O
which	O
support	O
sparse	O
matrix	O
.	O
)	O
Is	O
there	O
anyway	O
to	O
efficiently	O
slicing	O
a	O
sparseDataFrame	O
or	O
for	O
the	O
whole	O
process	O
I	O
am	O
doing	O
,	O
it	O
should	O
be	O
improved	O
in	O
anyway	O
?	O

I	O
looked	O
here	O
,	O
but	O
when	O
I	O
ran	O
that	O
in	O
iPython	O
Notebook	O
,	O
I	O
don't	O
get	O
anything	O
.	O

does	O
not	O
produce	O
any	O
difference	O
in	O
terms	O
of	O
file	O
size	O
than	O
...	O

You	O
will	O
either	O
have	O
to	O
split	O
the	O
table	O
up	O
or	O
choose	O
another	O
storage	O
format	O
.	O

To	O
get	O
the	O
number	O
of	O
groups	O
you	O
can	O
use	O
the	O
ngroups	O
attribute	O
:	O
#CODE	O

this	O
does	O
not	O
seem	O
to	O
work	O
and	O
I	O
am	O
not	O
sure	O
if	O
it	O
just	O
is	O
not	O
possible	O
,	O
I	O
can	O
always	O
generate	O
separate	O
dictionaries	O
from	O
a	O
master	O
dictionary	O
to	O
get	O
around	O
having	O
to	O
update	O
data	O
in	O
multiple	O
locations	O

I	O
want	O
to	O
compare	O
these	O
two	O
data	O
frames	O
by	O
row	O
based	O
on	O
column	O
Value	O
and	O
keep	O
the	O
row	O
from	O
first	O
or	O
second	O
depending	O
where	O
the	O
Value	O
is	O
bigger	O
.	O

Then	O
,	O
you	O
can	O
run	O
`	O
sudo	O
port	O
install	O
py27-pandas	O
`	O
to	O
get	O
Python	O
and	O
all	O
of	O
the	O
dependencies	O
installed	O
.	O

See	O
the	O
shape	O
method	O
of	O
the	O
input	O
array	O
,	O
and	O
you	O
should	O
get	O
something	O
like	O
`	O
(	O
N	O
,	O
)`	O
and	O
not	O
`	O
(	O
N	O
,	O
1	O
)`	O
.	O

So	O
,	O
will	O
the	O
lower	O
value	O
always	O
come	O
first	O
,	O
or	O
could	O
that	O
change	O
?	O

and	O
get	O
the	O
following	O
error	O
:	O

In	O
order	O
to	O
have	O
the	O
index	O
the	O
exact	O
same	O
as	O
the	O
first	O
example	O
you'd	O
need	O
to	O
change	O
to	O
int	O
from	O
float	O
.	O

The	O
graph	O
bit	O
is	O
sorted	O
but	O
the	O
part	O
i'm	O
finding	O
hard	O
is	O
the	O
fact	O
the	O
column	O
headers	O
can	O
change	O
so	O
picking	O
up	O
their	O
data	O
without	O
manually	O
calling	O
them	O
is	O
something	O
I'm	O
unable	O
to	O
do	O
.	O

If	O
pytables	O
used	O
msgpack	O
it	O
would	O
be	O
easier	O
for	O
other	O
languages	O
to	O
read	O
the	O
data	O
but	O
obviously	O
their	O
target	O
is	O
python	O
.	O

The	O
company	O
name	O
may	O
be	O
variable	O
length	O
,	O
it	O
will	O
however	O
always	O
be	O
after	O
the	O
first	O
`	O
\s	O
`	O

To	O
get	O
the	O
same	O
form	O
of	O
broadcasting	O
to	O
occur	O
like	O
the	O
diagram	O
above	O
shows	O
we	O
have	O
to	O
decompose	O
to	O
numpy	O
arrays	O
which	O
then	O
become	O
anonymous	O
data	O
:	O
#CODE	O

I	O
want	O
to	O
generate	O
a	O
plot	O
showing	O
these	O
dates	O
graphically	O
.	O

How	O
is	O
it	O
possible	O
to	O
get	O
the	O
label	O
of	O
value	O
'	O
12	O
'	O
?	O

You	O
shall	O
note	O
that	O
`	O
and	O
`	O
and	O
`	O
or	O
`	O
are	O
not	O
appropriate	O
for	O
a	O
vector	O
of	O
booleans	O
,	O
use	O
`	O
`	O
and	O
`	O
|	O
`	O
instead	O
.	O

No	O
,	O
this	O
table	O
is	O
used	O
by	O
a	O
lot	O
of	O
other	O
code	O
(	O
mostly	O
C#	O
)	O
,	O
I	O
am	O
just	O
doing	O
some	O
data	O
analysis	O
on	O
it	O
from	O
Python	O
,	O
so	O
I'm	O
not	O
in	O
a	O
position	O
to	O
change	O
the	O
semantics	O
/	O
data	O
structure	O
.	O

#	O
Valid	O
positions	O
in	O
output	O
array	O
to	O
be	O
changed	O

Is	O
there	O
a	O
bug	O
in	O
my	O
code	O
or	O
is	O
there	O
another	O
reason	O
for	O
the	O
huge	O
computation	O
speed	O
difference	O
between	O
those	O
two	O
lines	O
of	O
code	O
?	O

I	O
also	O
used	O
a	O
longer	O
window	O
because	O
there	O
were	O
only	O
15	O
values	O
per	O
array	O
but	O
you	O
seemed	O
to	O
be	O
planning	O
on	O
using	O
the	O
last	O
50	O
days	O
.	O

I	O
am	O
trying	O
to	O
get	O
the	O
data	O
into	O
the	O
following	O
shape	O
:	O
#CODE	O

I	O
then	O
get	O
the	O
following	O
error	O
:	O
#CODE	O

I	O
want	O
to	O
:	O
plot	O
a	O
heatmap	O
of	O
x	O
,	O
y	O
and	O
the	O
colour	O
is	O
the	O
z	O
value	O
..	O

I'm	O
also	O
running	O
python	O
3.4	O
and	O
I	O
didn't	O
get	O
any	O
warning	O
when	O
I	O
ran	O
your	O
code	O
exactly	O
as	O
is	O
.	O

This	O
does	O
it	O
in	O
a	O
one	O
liner	O
but	O
is	O
not	O
so	O
readable	O
,	O
basically	O
it	O
tests	O
where	O
the	O
value	O
counts	O
for	O
each	O
column	O
is	O
equal	O
to	O
1	O
,	O
filters	O
the	O
resultant	O
list	O
out	O
and	O
uses	O
the	O
index	O
as	O
a	O
boolean	O
indec	O
:	O
#CODE	O

I	O
still	O
think	O
pandas	O
is	O
not	O
correctly	O
handling	O
your	O
empty	O
column	O
and	O
you	O
end	O
up	O
with	O
either	O
with	O
5	O
columns	O
,	O
or	O
with	O
6	O
columns	O
,	O
but	O
shifted	O
one	O
to	O
the	O
left	O
.	O

However	O
,	O
I	O
am	O
receiving	O
this	O
warning	O
`	O
/	O
usr	O
/	O
local	O
/	O
lib	O
/	O
python2.7	O
/	O
dist-packages	O
/	O
pandas	O
/	O
core	O
/	O
index	O
.	O

It	O
looks	O
like	O
this	O
changed	O
at	O
some	O
point	O
;	O
maybe	O
he	O
has	O
an	O
old	O
version	O
of	O
pandas	O
where	O
S	O
and	O
Sec	O
are	O
no	O
good	O
.	O

I	O
have	O
also	O
heard	O
of	O
Orange	O
library	O
for	O
imputation	O
,	O
but	O
haven't	O
had	O
a	O
chance	O
to	O
use	O
it	O
yet	O
.	O

I	O
managed	O
to	O
find	O
how	O
this	O
is	O
almost	O
done	O
:	O
#CODE	O

For	O
this	O
purpose	O
you	O
need	O
pandas	O
-	O
most	O
popular	O
python	O
package	O
for	O
working	O
with	O
timeseries	O
and	O
another	O
analytic	O
data	O
.	O

well	O
,	O
what	O
I	O
am	O
trying	O
to	O
do	O
is	O
to	O
have	O
all	O
my	O
results	O
ready	O
out	O
of	O
mysql	O
,	O
and	O
then	O
do	O
different	O
types	O
of	O
merging	O
to	O
get	O
my	O
plots	O
.	O

I	O
agree	O
with	O
Chang	O
that	O
it	O
would	O
help	O
to	O
have	O
a	O
very	O
clear	O
example	O
of	O
how	O
the	O
exact	O
alignment	O
should	O
be	O
.	O

You	O
mention	O
in	O
update	O
2	O
above	O
that	O
you	O
want	O
to	O
get	O
the	O
columns	O
and	O
the	O
only	O
way	O
is	O
opening	O
the	O
hdf	O
.	O

So	O
where	O
those	O
indices	O
don't	O
match	O
up	O
(	O
50	O
,	O
and	O
51	O
)	O
,	O
you	O
get	O
`	O
NaN	O
`	O
as	O
I	O
would	O
hope	O
.	O

However	O
,	O
if	O
I	O
save	O
it	O
as	O
a	O
csv	O
file	O
and	O
reload	O
it	O
again	O
,	O
I	O
got	O
error	O
message	O
and	O
the	O
plot	O
is	O
not	O
quite	O
right	O
either	O
,	O
#CODE	O

My	O
main	O
goal	O
is	O
to	O
match	O
the	O
index	O
value	O
from	O
`	O
ds2	O
`	O
into	O
`	O
ds1	O
`	O
and	O
replace	O
it	O
with	O
corresponding	O
value	O
,	O
so	O
the	O
output	O
would	O
look	O
like	O
#CODE	O

That	O
would	O
be	O
a	O
possibility	O
but	O
the	O
problem	O
is	O
that	O
each	O
frame5	O
has	O
a	O
different	O
index	O
.	O

First	O
,	O
you	O
need	O
some	O
kind	O
of	O
mapping	O
of	O
what	O
makes	O
up	O
each	O
level	O
.	O

PyTables	O
3.1	O
was	O
just	O
released	O
that	O
changes	O
the	O
file	O
caching	O
mechanism	O
at	O
least	O
on	O
a	O
lower	O
HDF5	O
version	O
,	O
do	O
to	O
see	O
your	O
version	O
:	O
#CODE	O

backfilling	O
data	O
from	O
one	O
column	O
into	O
another	O

Also	O
,	O
I	O
know	O
I	O
am	O
missing	O
patterns	O
that	O
may	O
be	O
useful	O
because	O
if	O
a	O
pattern	O
exists	O
between	O
Variable_1	O
and	O
Variable_2	O
and	O
Variable_3	O
and	O
Variable_4	O
are	O
missing	O
completely	O
at	O
random	O
,	O
then	O
concatenating	O
them	O
as	O
strings	O
will	O
not	O
capture	O
the	O
pattern	O
between	O
Variable_1	O
and	O
Variable_2	O
.	O

Do	O
you	O
know	O
what	O
is	O
the	O
difference	O
in	O
this	O
case	O
between	O
both	O
?	O

The	O
goal	O
is	O
to	O
take	O
the	O
2x2	O
piece	O
of	O
df	O
with	O
index	O
(	O
4	O
,	O
5	O
)	O
and	O
columns	O
(	O
'	O
date	O
'	O
,	O
'	O
val	O
')	O
and	O
replace	O
it	O
with	O
a	O
same-shaped	O
,	O
same-typed	O
2x2	O
block	O
.	O

Convert	O
Matrix	O
format	O
to	O
Column	O
in	O
Pandas	O

`	O
nan	O
`	O
is	O
commonly	O
used	O
for	O
this	O
purpose	O
,	O
but	O
here	O
I'm	O
actually	O
just	O
using	O
the	O
time	O
that	O
was	O
already	O
there	O
if	O
there	O
isn't	O
a	O
new	O
one	O
defined	O
for	O
it	O
in	O
the	O
`	O
time_map	O
`	O
`	O
dict	O
`	O
.	O

I	O
want	O
to	O
drop	O
all	O
values	O
after	O
index	O
`	O
5	O
`	O
because	O
it	O
has	O
no	O
values	O
,	O
but	O
not	O
index	O
`	O
2	O
`	O
,	O
`	O
3	O
`	O
.	O

You	O
should	O
be	O
able	O
to	O
uninstall	O
Anaconda	O
(	O
it	O
is	O
only	O
a	O
directory	O
)	O
to	O
reverse	O
any	O
changes	O
.	O

Since	O
it	O
is	O
a	O
very	O
large	O
data	O
frame	O
,	O
I	O
think	O
it	O
might	O
be	O
inefficient	O
to	O
do	O
a	O
loop	O
and	O
row	O
by	O
row	O
extraction	O
.	O

Since	O
the	O
NumPy	O
array	O
has	O
no	O
index	O
,	O
there	O
should	O
be	O
no	O
"	O
Unalignable	O
boolean	O
Series	O
"	O
problem	O
.	O

My	O
guess	O
is	O
that	O
I	O
am	O
either	O
not	O
applying	O
the	O
functions	O
correctly	O
for	O
a	O
column	O
or	O
the	O
values	O
I	O
am	O
getting	O
arent	O
integers	O
.	O

which	O
gives	O
you	O
your	O
date	O
as	O
a	O
list	O
arranged	O
in	O
the	O
order	O
of	O
importance	O
...	O

I'm	O
not	O
sure	O
what	O
the	O
difference	O
was	O
.	O

The	O
mongodb	O
collection	O
contains	O
sensor	O
values	O
tagged	O
with	O
date	O
and	O
time	O
.	O

This	O
should	O
not	O
make	O
any	O
difference	O
.	O

I	O
would	O
like	O
to	O
split	O
that	O
file	O
into	O
files	O
of	O
len	O
(	O
index	O
)	O
=	O
2	O
,	O
using	O
linux	O
:	O
#CODE	O

You	O
can	O
coerce	O
the	O
response	O
into	O
a	O
data	O
frame	O
after	O
you	O
get	O
it	O
.	O

Oh	O
wait	O
,	O
your	O
matrix	O
must	O
already	O
be	O
in	O
the	O
form	O
of	O
differences	O
from	O
the	O
mean	O
(	O
by	O
column	O
)	O
?	O

Any	O
suggestions	O
?	O

I	O
get	O
the	O
following	O
error	O
:	O
#CODE	O

I	O
am	O
curious	O
why	O
doing	O
`	O
unique_df	O
[	O
i	O
]	O
=	O
"	O
AAA	O
"`	O
no	O
longer	O
modifies	O
the	O
data	O
frame	O
values	O
.	O

Unfortunately	O
I	O
get	O
an	O
error	O
,	O
and	O
the	O
shading	O
doesn't	O
work	O
.	O

The	O
range	O
of	O
the	O
values	O
in	O
`	O
megaball	O
`	O
are	O
from	O
1	O
to	O
25	O
,	O
and	O
this	O
line	O
of	O
code	O
:	O
#CODE	O

yes	O
it	O
works	O
fine	O
but	O
I	O
need	O
to	O
drop	O
'	O
2014-07-16	O
14:24	O
'	O
thnx	O

If	O
you	O
have	O
more	O
pressing	O
things	O
to	O
do	O
,	O
you	O
could	O
temporarily	O
rename	O
it	O
out	O
of	O
the	O
way	O
to	O
get	O
through	O
your	O
installs	O
,	O
then	O
rename	O
it	O
back	O
.	O

As	O
you	O
can	O
see	O
,	O
the	O
lines	O
overlap	O
perfectly	O
for	O
the	O
days	O
where	O
there	O
is	O
data	O
:	O
no	O
original	O
data	O
is	O
'	O
changed	O
'	O
.	O

I	O
have	O
a	O
massive	O
file	O
with	O
per	O
timestamp	O
survey	O
data	O
from	O
about	O
thousands	O
of	O
different	O
people	O
and	O
over	O
20	O
different	O
locations	O
.	O

So	O
if	O
you	O
have	O
a	O
million	O
items	O
that	O
,	O
on	O
average	O
,	O
belong	O
to	O
three	O
categories	O
each	O
,	O
then	O
you	O
need	O
storage	O
for	O
the	O
million	O
items	O
plus	O
three	O
million	O
references	O
.	O

Given	O
how	O
the	O
sample	O
was	O
built	O
,	O
there	O
was	O
a	O
need	O
to	O
weight	O
adjust	O
the	O
respondent	O
data	O
so	O
that	O
not	O
every	O
one	O
is	O
deemed	O
as	O
"	O
equal	O
"	O
when	O
performing	O
the	O
analysis	O
.	O

What	O
I'm	O
hoping	O
to	O
achieve	O
is	O
knowing	O
where	O
the	O
first	O
/	O
last	O
row	O
of	O
trimmed	O
data	O
is	O
located	O
so	O
I	O
can	O
set	O
up	O
a	O
for-loop	O
to	O
go	O
through	O
the	O
data	O
and	O
perform	O
mathematical	O
calculations	O
with	O
the	O
values	O
and	O
then	O
send	O
those	O
results	O
back	O
into	O
new	O
columns	O
attached	O
directly	O
to	O
the	O
same	O
date	O
as	O
the	O
date	O
in	O
question	O
.	O

Ultimately	O
I	O
want	O
to	O
be	O
able	O
to	O
loop	O
through	O
the	O
json	O
to	O
display	O
the	O
dates	O
and	O
corresponding	O
values	O
,	O
but	O
I	O
cant	O
do	O
that	O
until	O
this	O
error	O
is	O
no	O
longer	O
happening	O
.	O

They	O
are	O
however	O
extremely	O
useful	O
once	O
you	O
get	O
to	O
grips	O
with	O
them	O
.	O

I	O
know	O
the	O
values	O
within	O
the	O
CSV	O
are	O
not	O
all	O
"	O
NaN	O
"	O
so	O
why	O
does	O
the	O
output	O
looks	O
like	O
this	O
and	O
how	O
can	O
I	O
get	O
the	O
correct	O
output	O
with	O
the	O
numbers	O
in	O
reach	O
of	O
the	O
rows	O
?	O

I've	O
filtered	O
my	O
data	O
as	O
suggested	O
here	O
:	O
With	O
Pandas	O
in	O
Python	O
,	O
select	O
the	O
highest	O
value	O
row	O
for	O
each	O
group	O
#CODE	O

I	O
was	O
able	O
to	O
get	O
the	O
more	O
precise	O
value	O
in	O
my	O
previous	O
environment	O
by	O
doing	O
the	O
incremental	O
update	O
to	O
cumulative	O
mean	O
instead	O
of	O
taking	O
a	O
batch	O
sum	O
and	O
divide	O
.	O

Then	O
use	O
the	O
`	O
~	O
`	O
to	O
flip	O
the	O
bools	O
.	O
