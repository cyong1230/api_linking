"	O
Large	O
data	O
"	O
work	O
flows	O
using	O
pandas	O

I	O
have	O
tried	O
to	O
puzzle	O
out	O
an	O
answer	O
to	O
this	O
question	O
for	O
many	O
months	O
while	O
learning	O
pandas	O
.	O

I	O
use	O
SAS	O
for	O
my	O
day-to-day	O
work	O
and	O
it	O
is	O
great	O
for	O
it's	O
out-of-core	O
support	O
.	O

However	O
,	O
SAS	O
is	O
horrible	O
as	O
a	O
piece	O
of	O
software	O
for	O
numerous	O
other	O
reasons	O
.	O

One	O
day	O
I	O
hope	O
to	O
replace	O
my	O
use	O
of	O
SAS	O
with	O
python	O
and	O
pandas	O
,	O
but	O
I	O
currently	O
lack	O
an	O
out-of-core	O
workflow	O
for	O
large	O
datasets	O
.	O

I'm	O
not	O
talking	O
about	O
"	O
big	O
data	O
"	O
that	O
requires	O
a	O
distributed	O
network	O
,	O
but	O
rather	O
files	O
too	O
large	O
to	O
fit	O
in	O
memory	O
but	O
small	O
enough	O
to	O
fit	O
on	O
a	O
hard-drive	O
.	O

My	O
first	O
thought	O
is	O
to	O
use	O
`	O
HDFStore	O
`	O
to	O
hold	O
large	O
datasets	O
on	O
disk	O
and	O
pull	O
only	O
the	O
pieces	O
I	O
need	O
into	O
dataframes	B-API
for	O
analysis	O
.	O

Others	O
have	O
mentioned	O
MongoDB	O
as	O
an	O
easier	O
to	O
use	O
alternative	O
.	O

My	O
question	O
is	O
this	O
:	O

What	O
are	O
some	O
best-practice	O
workflows	O
for	O
accomplishing	O
the	O
following	O
:	O

Loading	O
flat	O
files	O
into	O
a	O
permanent	O
,	O
on-disk	O
database	O
structure	O

Querying	O
that	O
database	O
to	O
retrieve	O
data	O
to	O
feed	O
into	O
a	O
pandas	O
data	O
structure	O

Updating	O
the	O
database	O
after	O
manipulating	O
pieces	O
in	O
pandas	O

Real-world	O
examples	O
would	O
be	O
much	O
appreciated	O
,	O
especially	O
from	O
anyone	O
who	O
uses	O
pandas	O
on	O
"	O
large	O
data	O
"	O
.	O

Edit	O
--	O
an	O
example	O
of	O
how	O
I	O
would	O
like	O
this	O
to	O
work	O
:	O

Iteratively	O
import	O
a	O
large	O
flat-file	O
and	O
store	O
it	O
in	O
a	O
permanent	O
,	O
on-disk	O
database	O
structure	O
.	O

These	O
files	O
are	O
typically	O
too	O
large	O
to	O
fit	O
in	O
memory	O
.	O

In	O
order	O
to	O
use	O
Pandas	O
,	O
I	O
would	O
like	O
to	O
read	O
subsets	O
of	O
this	O
data	O
(	O
usually	O
just	O
a	O
few	O
columns	O
at	O
a	O
time	O
)	O
that	O
can	O
fit	O
in	O
memory	O
.	O

I	O
would	O
create	O
new	O
columns	O
by	O
performing	O
various	O
operations	O
on	O
the	O
selected	O
columns	O
.	O

I	O
would	O
then	O
have	O
to	O
append	O
these	O
new	O
columns	O
into	O
the	O
database	O
structure	O
.	O

I	O
am	O
trying	O
to	O
find	O
a	O
best-practice	O
way	O
of	O
performing	O
these	O
steps	O
.	O

Reading	O
links	O
about	O
pandas	O
and	O
pytables	O
it	O
seems	O
that	O
appending	O
a	O
new	O
column	O
could	O
be	O
a	O
problem	O
.	O

Edit	O
--	O
Responding	O
to	O
Jeff's	O
questions	O
specifically	O
:	O

I	O
am	O
building	O
consumer	O
credit	O
risk	O
models	O
.	O

The	O
kinds	O
of	O
data	O
include	O
phone	O
,	O
SSN	O
and	O
address	O
characteristics	O
;	O
property	O
values	O
;	O
derogatory	O
information	O
like	O
criminal	O
records	O
,	O
bankruptcies	O
,	O
etc	O
...	O

The	O
datasets	O
I	O
use	O
every	O
day	O
have	O
nearly	O
1,000	O
to	O
2,000	O
fields	O
on	O
average	O
of	O
mixed	O
data	O
types	O
:	O
continuous	O
,	O
nominal	O
and	O
ordinal	O
variables	O
of	O
both	O
numeric	O
and	O
character	O
data	O
.	O

I	O
rarely	O
append	O
rows	O
,	O
but	O
I	O
do	O
perform	O
many	O
operations	O
that	O
create	O
new	O
columns	O
.	O

Typical	O
operations	O
involve	O
combining	O
several	O
columns	O
using	O
conditional	O
logic	O
into	O
a	O
new	O
,	O
compound	O
column	O
.	O

For	O
example	O
,	O
`	O
if	O
var1	O
2	O
then	O
newvar	O
=	O
'	O
A	O
'	O
elif	O
var2	O
=	O
4	O
then	O
newvar	O
=	O
'	O
B	O
'`	O
.	O

The	O
result	O
of	O
these	O
operations	O
is	O
a	O
new	O
column	O
for	O
every	O
record	O
in	O
my	O
dataset	O
.	O

Finally	O
,	O
I	O
would	O
like	O
to	O
append	O
these	O
new	O
columns	O
into	O
the	O
on-disk	O
data	O
structure	O
.	O

I	O
would	O
repeat	O
step	O
2	O
,	O
exploring	O
the	O
data	O
with	O
crosstabs	O
and	O
descriptive	O
statistics	O
trying	O
to	O
find	O
interesting	O
,	O
intuitive	O
relationships	O
to	O
model	O
.	O

A	O
typical	O
project	O
file	O
is	O
usually	O
about	O
1GB	O
.	O

Files	O
are	O
organized	O
into	O
such	O
a	O
manner	O
where	O
a	O
row	O
consists	O
of	O
a	O
record	O
of	O
consumer	O
data	O
.	O

Each	O
row	O
has	O
the	O
same	O
number	O
of	O
columns	O
for	O
every	O
record	O
.	O

This	O
will	O
always	O
be	O
the	O
case	O
.	O

It's	O
pretty	O
rare	O
that	O
I	O
would	O
subset	O
by	O
rows	O
when	O
creating	O
a	O
new	O
column	O
.	O

However	O
,	O
it's	O
pretty	O
common	O
for	O
me	O
to	O
subset	O
on	O
rows	O
when	O
creating	O
reports	O
or	O
generating	O
descriptive	O
statistics	O
.	O

For	O
example	O
,	O
I	O
might	O
want	O
to	O
create	O
a	O
simple	O
frequency	O
for	O
a	O
specific	O
line	O
of	O
business	O
,	O
say	O
Retail	O
credit	O
cards	O
.	O

To	O
do	O
this	O
,	O
I	O
would	O
select	O
only	O
those	O
records	O
where	O
the	O
line	O
of	O
business	O
=	O
retail	O
in	O
addition	O
to	O
whichever	O
columns	O
I	O
want	O
to	O
report	O
on	O
.	O

When	O
creating	O
new	O
columns	O
,	O
however	O
,	O
I	O
would	O
pull	O
all	O
rows	O
of	O
data	O
and	O
only	O
the	O
columns	O
I	O
need	O
for	O
the	O
operations	O
.	O

The	O
modeling	O
process	O
requires	O
that	O
I	O
analyze	O
every	O
column	O
,	O
look	O
for	O
interesting	O
relationships	O
with	O
some	O
outcome	O
variable	O
,	O
and	O
create	O
new	O
compound	O
columns	O
that	O
describe	B-API
those	O
relationships	O
.	O

The	O
columns	O
that	O
I	O
explore	O
are	O
usually	O
done	O
in	O
small	O
sets	O
.	O

For	O
example	O
,	O
I	O
will	O
focus	O
on	O
a	O
set	O
of	O
say	O
20	O
columns	O
just	O
dealing	O
with	O
property	O
values	O
and	O
observe	O
how	O
they	O
relate	O
to	O
defaulting	O
on	O
a	O
loan	O
.	O

Once	O
those	O
are	O
explored	O
and	O
new	O
columns	O
are	O
created	O
,	O
I	O
then	O
move	O
on	O
to	O
another	O
group	O
of	O
columns	O
,	O
say	O
college	O
education	O
,	O
and	O
repeat	O
the	O
process	O
.	O

What	O
I'm	O
doing	O
is	O
creating	O
candidate	O
variables	O
that	O
explain	O
the	O
relationship	O
between	O
my	O
data	O
and	O
some	O
outcome	O
.	O

At	O
the	O
very	O
end	O
of	O
this	O
process	O
,	O
I	O
apply	O
some	O
learning	O
techniques	O
that	O
create	O
an	O
equation	O
out	O
of	O
those	O
compound	O
columns	O
.	O

It	O
is	O
rare	O
that	O
I	O
would	O
ever	O
add	O
rows	O
to	O
the	O
dataset	O
.	O

I	O
will	O
nearly	O
always	O
be	O
creating	O
new	O
columns	O
(	O
variables	O
or	O
features	O
in	O
statistics	O
/	O
machine	O
learning	O
parlance	O
)	O
.	O

I	O
routinely	O
use	O
tens	O
of	O
gigabytes	O
of	O
data	O
in	O
just	O
this	O
fashion	O

e.g.	O
I	O
have	O
tables	O
on	O
disk	O
that	O
I	O
read	O
via	O
queries	O
,	O
create	O
data	O
and	O
append	O
back	O
.	O

It's	O
worth	O
reading	O
the	O
docs	O
and	O
late	O
in	O
this	O
thread	O
for	O
several	O
suggestions	O
for	O
how	O
to	O
store	O
your	O
data	O
.	O

Details	O
which	O
will	O
affect	O
how	O
you	O
store	O
your	O
data	O
,	O
like	O
:	O

Give	O
as	O
much	O
detail	O
as	O
you	O
can	O
;	O
and	O
I	O
can	O
help	O
you	O
develop	O
a	O
structure	O
.	O

Size	O
of	O
data	O
,	O
#	O
of	O
rows	O
,	O
columns	O
,	O
types	O
of	O
columns	O
;	O
are	O
you	O
appending	O

rows	O
,	O
or	O
just	O
columns	O
?	O

What	O
will	O
typical	O
operations	O
look	O
like	O
.	O

E.g.	O

do	O
a	O
query	O
on	O
columns	O
to	O
select	O
a	O
bunch	O
of	O
rows	O
and	O
specific	O
columns	O
,	O
then	O
do	O
an	O
operation	O
(	O
in-memory	O
)	O
,	O
create	O
new	O
columns	O
,	O
save	O
these	O
.	O

(	O
Giving	O
a	O
toy	O
example	O
could	O
enable	O
us	O
to	O
offer	O
more	O
specific	O
recommendations	O
.	O
)	O

After	O
that	O
processing	O
,	O
then	O
what	O
do	O
you	O
do	O
?	O

Is	O
step	O
2	O
ad	O
hoc	O
,	O
or	O
repeatable	O
?	O

Input	O
flat	O
files	O
:	O
how	O
many	O
,	O
rough	O
total	O
size	O
in	O
Gb	O
.	O

How	O
are	O
these	O
organized	O
e.g.	O
by	O
records	O
?	O

Does	O
each	O
one	O
contains	O
different	O
fields	O
,	O
or	O
do	O
they	O
have	O
some	O
records	O
per	O
file	O
with	O
all	O
of	O
the	O
fields	O
in	O
each	O
file	O
?	O

Do	O
you	O
ever	O
select	O
subsets	O
of	O
rows	O
(	O
records	O
)	O
based	O
on	O
criteria	O
(	O
e.g.	O
select	O
the	O
rows	O
with	O
field	O
A	O
>	O
5	O
)	O
?	O
and	O
then	O
do	O
something	O
,	O
or	O
do	O
you	O
just	O
select	O
fields	O
A	O
,	O
B	O
,	O
C	O
with	O
all	O
of	O
the	O
records	O
(	O
and	O
then	O
do	O
something	O
)	O
?	O

Do	O
you	O
'	O
work	O
on	O
'	O
all	O
of	O
your	O
columns	O
(	O
in	O
groups	O
)	O
,	O
or	O
are	O
there	O
a	O
good	O
proportion	O
that	O
you	O
may	O
only	O
use	O
for	O
reports	O
(	O
e.g.	O
you	O
want	O
to	O
keep	O
the	O
data	O
around	O
,	O
but	O
don't	O
need	O
to	O
pull	O
in	O
that	O
column	O
explicity	O
until	O
final	O
results	O
time	O
)	O
?	O

Solution	O

Ensure	O
you	O
have	O
sure	O
you	O
have	O
pandas	O
at	O
least	O
`	O
0.10.1	O
`	O
installed	O
.	O

Read	O
iterating	O
files	O
chunk-by-chunk	O
and	O
multiple	O
table	O
queries	O
.	O

Since	O
pytables	O
is	O
optimized	O
to	O
operate	O
on	O
row-wise	O
(	O
which	O
is	O
what	O
you	O
query	O
on	O
)	O
,	O
we	O
will	O
create	O
a	O
table	O
for	O
each	O
group	O
of	O
fields	O
.	O

This	O
way	O
it's	O
easy	O
to	O
select	O
a	O
small	O
group	O
of	O
fields	O
(	O
which	O
will	O
work	O
with	O
a	O
big	O
table	O
,	O
but	O
it's	O
more	O
efficient	O
to	O
do	O
it	O
this	O
way	O
...	O
I	O
think	O
I	O
may	O
be	O
able	O
to	O
fix	O
this	O
limitation	O
in	O
the	O
future	O
...	O
this	O
is	O
more	O
intuitive	O
anyhow	O
):	O

(	O
The	O
following	O
is	O
pseudocode	O
.	O
)	O

@CODE	O

Reading	O
in	O
the	O
files	O
and	O
creating	O
the	O
storage	O
(	O
essentially	O
doing	O
what	O
`	O
append_to_multiple	O
`	O
does	O
):	O

@CODE	O

Now	O
you	O
have	O
all	O
of	O
the	O
tables	O
in	O
the	O
file	O
(	O
actually	O
you	O
could	O
store	O
them	O
in	O
separate	O
files	O
if	O
you	O
wish	O
,	O
you	O
would	O
prob	O
have	O
to	O
add	O
the	O
filename	O
to	O
the	O
group_map	O
,	O
but	O
probably	O
this	O
isn't	O
necessary	O
)	O
.	O

This	O
is	O
how	O
you	O
get	O
columns	O
and	O
create	O
new	O
ones	O
:	O

@CODE	O

When	O
you	O
are	O
ready	O
for	O
post_processing	O
:	O

@CODE	O

About	O
data_columns	O
,	O
you	O
don't	O
actually	O
need	O
to	O
define	O
ANY	O
data_columns	O
;	O
they	O
allow	O
you	O
to	O
sub-select	O
rows	O
based	O
on	O
the	O
column	O
.	O

E.g.	O
something	O
like	O
:	O

@CODE	O

They	O
may	O
be	O
most	O
interesting	O
to	O
you	O
in	O
the	O
final	O
report	O
generation	O
stage	O
(	O
essentially	O
a	O
data	O
column	O
is	O
segregated	O
from	O
other	O
columns	O
,	O
which	O
might	O
impact	O
efficiency	O
somewhat	O
if	O
you	O
define	O
a	O
lot	O
)	O
.	O

You	O
also	O
might	O
want	O
to	O
:	O

create	O
a	O
function	O
which	O
takes	O
a	O
list	O
of	O
fields	O
,	O
looks	O
up	O
the	O
groups	O
in	O
the	O
groups_map	O
,	O
then	O
selects	O
these	O
and	O
concatenates	O
the	O
results	O
so	O
you	O
get	O
the	O
resulting	O
frame	O
(	O
this	O
is	O
essentially	O
what	O
select_as_multiple	O
does	O
)	O
.	O

This	O
way	O
the	O
structure	O
would	O
be	O
pretty	O
transparent	O
to	O
you	O
.	O

indexes	O
on	O
certain	O
data	O
columns	O
(	O
makes	O
row-subsetting	O
much	O
faster	O
)	O
.	O

enable	O
compression	O
.	O

Let	O
me	O
know	O
when	O
you	O
have	O
questions	O
!	O

This	O
is	O
the	O
case	O
for	O
pymongo	O
.	O

I	O
have	O
also	O
prototyped	O
using	O
sql	O
server	O
,	O
sqlite	O
,	O
HDF	O
,	O
ORM	O
(	O
SQLAlchemy	O
)	O
in	O
python	O
.	O

First	O
and	O
foremost	O
pymongo	O
is	O
a	O
document	O
based	O
DB	O
,	O
so	O
each	O
person	O
would	O
be	O
a	O
document	O
(	O
`	O
dict	O
`	O
of	O
attributes	O
)	O
.	O

Many	O
people	O
form	O
a	O
collection	O
and	O
you	O
can	O
have	O
many	O
collections	O
(	O
people	O
,	O
stock	O
market	O
,	O
income	O
)	O
.	O

pd.dateframe	O
->	O
pymongo	O
Note	O
:	O
I	O
use	O
the	O
`	O
chunksize	O
`	O
in	O
`	O
read_csv	B-API
`	O
to	O
keep	O
it	O
to	O
5	O
to	O
10k	O
records	O
(	O
pymongo	O
drops	O
the	O
socket	O
if	O
larger	O
)	O

@CODE	O

querying	O
:	O
gt	O
=	O
greater	O
than	O
...	O

@CODE	O

`	O
`	O
returns	O
an	O
iterator	O
so	O
I	O
commonly	O
use	O
`	O
ichunked	O
`	O
to	O
chop	O
into	O
smaller	O
iterators	O
.	O

How	O
about	O
a	O
join	O
since	O
I	O
normally	O
get	O
10	O
data	O
sources	O
to	O
paste	O
together	O
:	O

@CODE	O

then	O
(	O
in	O
my	O
case	O
sometimes	O
I	O
have	O
to	O
agg	O
on	O
`	O
aJoinDF	O
`	O
first	O
before	O
its	O
"	O
mergeable	O
"	O
.	O
)	O

@CODE	O

And	O
you	O
can	O
then	O
write	O
the	O
new	O
info	O
to	O
your	O
main	O
collection	O
via	O
the	O
update	O
method	O
below	O
.	O

(	O
logical	O
collection	O
vs	O
physical	O
datasources	O
)	O
.	O

@CODE	O

On	O
smaller	O
lookups	O
,	O
just	O
denormalize	O
.	O

For	O
example	O
,	O
you	O
have	O
code	O
in	O
the	O
document	O
and	O
you	O
just	O
add	O
the	O
field	O
code	O
text	O
and	O
do	O
a	O
`	O
dict	O
`	O
lookup	O
as	O
you	O
create	O
documents	O
.	O

Now	O
you	O
have	O
a	O
nice	O
dataset	O
based	O
around	O
a	O
person	O
,	O
you	O
can	O
unleash	O
your	O
logic	O
on	O
each	O
case	O
and	O
make	O
more	O
attributes	O
.	O

Finally	O
you	O
can	O
read	O
into	O
pandas	O
your	O
3	O
to	O
memory	O
max	O
key	O
indicators	O
and	O
do	O
pivots	O
/	O
agg	O
/	O
data	O
exploration	O
.	O

This	O
works	O
for	O
me	O
for	O
3	O
million	O
records	O
with	O
numbers	O
/	O
big	O
text	O
/	O
categories	O
/	O
codes	O
/	O
floats	O
/	O
...	O

You	O
can	O
also	O
use	O
the	O
two	O
methods	O
built	O
into	O
MongoDB	O
(	O
MapReduce	O
and	O
aggregate	O
framework	O
)	O
.	O

See	O
here	O
for	O
more	O
info	O
about	O
the	O
aggregate	O
framework	O
,	O
as	O
it	O
seems	O
to	O
be	O
easier	O
than	O
MapReduce	O
and	O
looks	O
handy	O
for	O
quick	O
aggregate	O
work	O
.	O

Notice	O
I	O
didn't	O
need	O
to	O
define	O
my	O
fields	O
or	O
relations	O
,	O
and	O
I	O
can	O
add	O
items	O
to	O
a	O
document	O
.	O

At	O
the	O
current	O
state	O
of	O
the	O
rapidly	O
changing	O
numpy	O
,	O
pandas	O
,	O
python	O
toolset	O
,	O
MongoDB	O
helps	O
me	O
just	O
get	O
to	O
work	O
:)	O

I	O
spotted	O
this	O
a	O
little	O
late	O
,	O
but	O
I	O
work	O
with	O
a	O
similar	O
problem	O
(	O
mortgage	O
prepayment	O
models	O
)	O
.	O

My	O
solution	O
has	O
been	O
to	O
skip	O
the	O
pandas	O
HDFStore	O
layer	O
and	O
use	O
straight	O
pytables	O
.	O

I	O
save	O
each	O
column	O
as	O
an	O
individual	O
HDF5	O
array	O
in	O
my	O
final	O
file	O
.	O

My	O
basic	O
workflow	O
is	O
to	O
<<<<<<< Updated upstream
first	O
=======
first	B-API
>>>>>>> Stashed changes
get	O
a	O
CSV	O
file	O
from	O
the	O
database	O
.	O

I	O
gzip	O
it	O
,	O
so	O
it's	O
not	O
as	O
huge	O
.	O

Then	O
I	O
convert	O
that	O
to	O
a	O
row-oriented	O
HDF5	O
file	O
,	O
by	O
iterating	O
over	O
it	O
in	O
python	O
,	O
converting	O
each	O
row	O
to	O
a	O
<<<<<<< Updated upstream
real	O
=======
real	B-API
>>>>>>> Stashed changes
data	O
type	O
,	O
and	O
writing	O
it	O
to	O
a	O
HDF5	O
file	O
.	O

That	O
takes	O
some	O
tens	O
of	O
minutes	O
,	O
but	O
it	O
doesn't	O
use	O
any	O
memory	O
,	O
since	O
it's	O
only	O
operating	O
row-by-row	O
.	O

Then	O
I	O
"	O
transpose	O
"	O
the	O
row-oriented	O
HDF5	O
file	O
into	O
a	O
column-oriented	O
HDF5	O
file	O
.	O

The	O
table	O
transpose	O
looks	O
like	O
:	O

@CODE	O

Reading	O
it	O
back	O
in	O
then	O
looks	O
like	O
:	O

@CODE	O

Now	O
,	O
I	O
generally	O
run	O
this	O
on	O
a	O
machine	O
with	O
a	O
ton	O
of	O
memory	O
,	O
so	O
I	O
may	O
not	O
be	O
careful	O
enough	O
with	O
my	O
memory	O
usage	O
.	O

For	O
example	O
,	O
by	O
default	O
the	O
load	O
operation	O
reads	O
the	O
whole	O
data	O
set	O
.	O

This	O
generally	O
works	O
for	O
me	O
,	O
but	O
it's	O
a	O
bit	O
clunky	O
,	O
and	O
I	O
can't	O
use	O
the	O
fancy	O
pytables	O
magic	O
.	O

Edit	O
:	O
The	O
real	O
advantage	O
of	O
this	O
approach	O
,	O
over	O
the	O
array-of-records	O
pytables	O
default	O
,	O
is	O
that	O
I	O
can	O
then	O
load	O
the	O
data	O
into	O
R	O
using	O
h5r	O
,	O
which	O
can't	O
handle	O
tables	O
.	O

Or	O
,	O
at	O
least	O
,	O
I've	O
been	O
unable	O
to	O
get	O
it	O
to	O
load	O
heterogeneous	O
tables	O
.	O

If	O
your	O
datasets	O
are	O
between	O
1	O
and	O
20GB	O
,	O
you	O
should	O
get	O
a	O
workstation	O
with	O
48GB	O
of	O
RAM	O
.	O

Then	O
Pandas	O
can	O
hold	O
the	O
entire	O
dataset	O
in	O
RAM	O
.	O

I	O
know	O
its	O
not	O
the	O
answer	O
you're	O
looking	O
for	O
here	O
,	O
but	O
doing	O
scientific	O
computing	O
on	O
a	O
notebook	O
with	O
4GB	O
of	O
RAM	O
isn't	O
reasonable	O
.	O

I	O
think	O
the	O
answers	O
above	O
are	O
missing	O
a	O
simple	O
approach	O
that	O
I've	O
found	O
very	O
useful	O
.	O

When	O
I	O
have	O
a	O
file	O
that	O
is	O
too	O
large	O
to	O
load	O
in	O
memory	O
,	O
I	O
break	O
up	O
the	O
file	O
into	O
multiple	O
smaller	O
files	O
(	O
either	O
by	O
row	O
or	O
cols	O
)	O

Example	O
:	O
In	O
case	O
of	O
30	O
days	O
worth	O
of	O
trading	O
data	O
of	O
~30GB	O
size	O
,	O
I	O
break	O
it	O
into	O
a	O
file	O
per	O
day	O
of	O
~1GB	O
size	O
.	O

I	O
subsequently	O
process	O
each	O
file	O
separately	O
and	O
aggregate	O
results	O
at	O
the	O
end	O

One	O
of	O
the	O
biggest	O
advantages	O
is	O
that	O
it	O
allows	O
parallel	O
processing	O
of	O
the	O
files	O
(	O
either	O
multiple	O
threads	O
or	O
processes	O
)	O

The	O
other	O
advantage	O
is	O
that	O
file	O
manipulation	O
(	O
like	O
adding	O
/	O
removing	O
dates	O
in	O
the	O
example	O
)	O
can	O
be	O
accomplished	O
by	O
regular	O
shell	O
commands	O
,	O
which	O
is	O
not	O
be	O
possible	O
in	O
more	O
advanced	O
/	O
complicated	O
file	O
formats	O

This	O
approach	O
doesn't	O
cover	O
all	O
scenarios	O
,	O
but	O
is	O
very	O
useful	O
in	O
a	O
lot	O
of	O
them	O

Consider	O
Ruffus	O
if	O
you	O
go	O
the	O
simple	O
path	O
of	O
creating	O
a	O
data	O
pipeline	O
which	O
is	O
broken	O
down	O
into	O
multiple	O
smaller	O
files	O
.	O

I	O
know	O
this	O
is	O
an	O
old	O
thread	O
but	O
I	O
think	O
the	O
Blaze	O
library	O
is	O
worth	O
checking	O
out	O
.	O

It's	O
built	O
for	O
these	O
types	O
of	O
situations	O
.	O

From	O
the	O
docs	O
:	O

Blaze	O
extends	O
the	O
usability	O
of	O
NumPy	O
and	O
Pandas	O
to	O
distributed	O
and	O
out-of-core	O
computing	O
.	O

Blaze	O
provides	O
an	O
interface	O
similar	O
to	O
that	O
of	O
the	O
NumPy	O
ND-Array	O
or	O
Pandas	O
DataFrame	B-API
but	O
maps	O
these	O
familiar	O
interfaces	O
onto	O
a	O
variety	O
of	O
other	O
computational	O
engines	O
like	O
Postgres	O
or	O
Spark	O
.	O

Edit	O
:	O
By	O
the	O
way	O
,	O
it's	O
supported	O
by	O
ContinuumIO	O
and	O
Travis	O
Oliphant	O
,	O
author	O
of	O
NumPy	O
.	O

One	O
more	O
variation	O

Many	O
of	O
the	O
operations	O
done	O
in	O
pandas	O
can	O
also	O
be	O
done	O
as	O
a	O
db	O
query	O
(	O
sql	O
,	O
mongo	O
)	O

Using	O
a	O
RDBMS	O
or	O
mongodb	O
allows	O
you	O
to	O
perform	O
some	O
of	O
the	O
aggregations	O
in	O
the	O
DB	O
Query	O
(	O
which	O
is	O
optimized	O
for	O
large	O
data	O
,	O
and	O
uses	O
cache	O
and	O
indexes	O
efficiently	O
)	O

Later	O
,	O
you	O
can	O
perform	O
post	O
processing	O
using	O
pandas	O
.	O

The	O
advantage	O
of	O
this	O
method	O
is	O
that	O
you	O
gain	O
the	O
DB	O
optimizations	O
for	O
working	O
with	O
large	O
data	O
,	O
while	O
still	O
defining	O
the	O
logic	O
in	O
a	O
high	O
level	O
declarative	O
syntax	O
-	O
and	O
not	O
having	O
to	O
deal	O
with	O
the	O
details	O
of	O
deciding	O
what	O
to	O
do	O
in	O
memory	O
and	O
what	O
to	O
do	O
out	O
of	O
core	O
.	O

And	O
although	O
the	O
query	O
language	O
and	O
pandas	O
are	O
different	O
,	O
it's	O
usually	O
not	O
complicated	O
to	O
translate	O
part	O
of	O
the	O
logic	O
from	O
one	O
to	O
another	O
.	O
