Large	O
,	O
persistent	O
DataFrame	B-API
in	O
pandas	O

I	O
am	O
exploring	O
switching	O
to	O
python	O
and	O
pandas	O
as	O
a	O
long-time	O
SAS	O
user	O
.	O

However	O
,	O
when	O
running	O
some	O
tests	O
today	O
,	O
I	O
was	O
surprised	O
that	O
python	O
ran	O
out	O
of	O
memory	O
when	O
trying	O
to	O
`	O
pandas.read_csv()	B-API
`	O
a	O
128mb	O
csv	O
file	O
.	O

It	O
had	O
about	O
200,000	O
rows	O
and	O
200	O
columns	O
of	O
mostly	O
numeric	O
data	O
.	O

With	O
SAS	O
,	O
I	O
can	O
import	O
a	O
csv	O
file	O
into	O
a	O
SAS	O
dataset	O
and	O
it	O
can	O
be	O
as	O
large	O
as	O
my	O
hard	O
drive	O
.	O

Is	O
there	O
something	O
analogous	O
in	O
pandas	O
?	O

I	O
regularly	O
work	O
with	O
large	O
files	O
and	O
do	O
not	O
have	O
access	O
to	O
a	O
distributed	O
computing	O
network	O
.	O

Thanks	O

In	O
principle	O
it	O
shouldn't	O
run	O
out	O
of	O
memory	O
,	O
but	O
there	O
are	O
currently	O
memory	O
problems	O
with	O
`	O
read_csv	B-API
`	O
on	O
large	O
files	O
caused	O
by	O
some	O
complex	O
Python	O
internal	O
issues	O
(	O
this	O
is	O
vague	O
but	O
it's	O
been	O
known	O
for	O
a	O
long	O
time	O
:	O
http://github.com/pydata/pandas/issues/407	O
)	O
.	O

At	O
the	O
moment	O
there	O
isn't	O
a	O
perfect	O
solution	O
(	O
here's	O
a	O
tedious	O
one	O
:	O
you	O
could	O
transcribe	O
the	O
file	O
row-by-row	O
into	O
a	O
pre-allocated	O
NumPy	O
array	O
or	O
memory-mapped	O
file	O
--	O
`	O
np.mmap	O
`)	O
,	O
but	O
it's	O
one	O
I'll	O
be	O
working	O
on	O
in	O
the	O
near	O
future	O
.	O

Another	O
solution	O
is	O
to	O
read	O
the	O
file	O
in	O
smaller	O
pieces	O
(	O
use	O
`	O
iterator=True	O
,	O
chunksize=1000	O
`)	O
then	O
concatenate	O
then	O
with	O
`	O
pd.concat	O
`	O
.	O

The	O
problem	O
comes	O
in	O
when	O
you	O
pull	O
the	O
entire	O
text	O
file	O
into	O
memory	O
in	O
one	O
big	O
slurp	O
.	O

Wes	O
in	O
of	O
course	O
right	O
!	O

I'm	O
just	O
chiming	O
in	O
to	O
provide	O
a	O
little	O
more	O
complete	O
example	O
code	O
.	O

I	O
had	O
the	O
same	O
issue	O
with	O
a	O
129	O
Mb	O
file	O
,	O
which	O
was	O
solved	O
by	O

@CODE	O

This	O
is	O
an	O
older	O
thread	O
,	O
but	O
I	O
just	O
wanted	O
to	O
dump	O
my	O
workaround	O
solution	O
here	O
.	O

I	O
initially	O
tried	O
the	O
`	O
chunksize	O
`	O
parameter	O
(	O
even	O
with	O
quite	O
small	O
values	O
like	O
10000	O
)	O
,	O
but	O
it	O
didn't	O
help	O
much	O
;	O
had	O
still	O
technical	O
issues	O
with	O
the	O
memory	O
size	O
(	O
my	O
CSV	O
was	O
~	O
7.5	O
Gb	O
)	O
.	O

Right	O
now	O
,	O
I	O
just	O
read	O
chunks	O
of	O
the	O
CSV	O
files	O
in	O
a	O
for-loop	O
approach	O
and	O
add	O
them	O
e.g.	O
,	O
to	O
an	O
SQLite	O
database	O
step	O
by	O
step	O
:	O

@CODE	O
