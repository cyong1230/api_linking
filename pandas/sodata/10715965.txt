add one row in a pandas.DataFrame
I understand that pandas is designed to load fully populated DataFrame but I need to create an empty DataFrame then add rows, one by one .
What is the best way to do this ? 
I successfully created an empty DataFrame with : @CODE
Then I can add a new row and fill a field with : @CODE
It works but seems very odd :-/ (it fails for adding string value) 
How can I add a new row to my DataFrame (with different columns type) ?
Note this is a very inefficient way to build a large DataFrame; new arrays have to be created (copying over the existing data) when you append a row.
@WesMcKinney: Thx, that's really good to know. Is it very fast to add *columns* to huge tables?
If it is too inefficient for you, you may preallocate an additional row and then update it.
You could use `pandas.concat()` or `DataFrame.append()`. For details and examples, see Merge, join, and concatenate .
Thanks ! It works. I will edit the question to include the full answer.
Hi, so what is the answer for the methods using append() or concat(). I have the same problem, but still trying to figuring it out.
why is this not the accepted answer? You have to scroll down all the way here to find the highest-rated answer?
@thias, that's one of those eternal questions of modern life, along with "what was Michael Jackson really like?"
You could create a list of dictionary. Where each dictionary in the list corresponds to a input data row. These rows are then added to the main list in a for loop. Once the list is complete, then create a data frame. This is a much faster approach. 
I has a similar problem where if I created a data frame for each row and append it to the main data frame it took 30 mins. On the other hand, if used below methodology, I was successful within seconds. @CODE
I've moved to doing this as well for any situation where I can't get all the data up front. The speed difference is astonishing.
The speed difference is indeed astonishing
Copying from pandas docs:                                                                          `It is worth noting however, that concat (and therefore append) makes a full copy of the data, and that constantly reusing this function can create a significant performance hit. If you need to use the operation over several datasets, use a list comprehension.` (http://pandas.pydata.org/pandas-docs/stable/merging.html#concatenating-objects)
For efficient appending see How to add an extra row to a pandas dataframe and Setting With Enlargement . 
Add rows through `loc` on non existing index data.
This should get voted, most concise answer for appending one row
Probably yes. But it would be better to include a summary of the answer in the reply, and use the links as reference.
Example at @Nasser's answer: @CODE
Consider adding the index to preallocate memory (see my answer)
Does not work on pandas 0.11.0
@MaximG: I strongly recommend an upgrade. Current Pandas version is 0.15.0.
`.loc` is referencing the index column, so if you're working with a pre-existing DataFrame with an index that isn't a continous sequence of integers starting with 0 (as in your example), `.loc` will overwrite existing rows, or insert rows, or create gaps in your index. A more robust (but not fool-proof) approach for appending an existing nonzero-length dataframe would be: `df.loc[df.index.max() + 1] = [randint(...` or prepopulating the index as @FooBar suggested.
@hobs: I completely agree with you. Thanks for your input. However, it's a different scenario from that proposed in the original question. If you know, a priori, the size of your data frame it's certainly faster to allocate memory.
If you know the number of entries ex ante, you should preallocate the space by also providing the index (taking the data example from a different answer): @CODE
Speed comparison @CODE
And - as from the comments - with a size of 6000, the speed difference becomes even larger: 
Increasing the size of the array (12) and the number of rows (500) makes
the speed difference more striking: 313ms vs 2.29s
Great answer. This should be the norm so that row space doesn't have to allocated incrementally.
Increasing the size of the array(12) and the number of rows(500) makes the speed difference more striking: 313ms vs 2.29s
`mycolumns = ['A', 'B']
df = pd.DataFrame(columns=mycolumns)
rows = [[1,2],[3,4],[5,6]]
for row in rows:
df.loc[len(df)] = row
`
