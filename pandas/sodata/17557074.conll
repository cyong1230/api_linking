Memory	O
error	O
when	O
using	O
pandas	O
read_csv	B-API

I	O
am	O
trying	O
to	O
do	O
something	O
fairly	O
simple	O
,	O
reading	O
a	O
large	O
csv	O
file	O
into	O
a	O
pandas	O
dataframe	B-API
.	O

@CODE	O

The	O
code	O
either	O
fails	O
with	O
a	O
`	O
MemoryError	O
`	O
,	O
or	O
just	O
never	O
finishes	O
.	O

Mem	O
usage	O
in	O
the	O
task	O
manager	O
stopped	O
at	O
506	O
Mb	O
and	O
after	O
5	O
minutes	O
of	O
no	O
change	O
and	O
no	O
CPU	O
activity	O
in	O
the	O
process	O
I	O
stopped	O
it	O
.	O

I	O
am	O
using	O
pandas	O
version	O
0.11.0	O
.	O

I	O
am	O
aware	O
that	O
there	O
used	O
to	O
be	O
a	O
memory	O
problem	O
with	O
the	O
file	O
parser	O
,	O
but	O
according	O
to	O
http://wesmckinney.com/blog/?p=543	O
this	O
should	O
have	O
been	O
fixed	O
.	O

The	O
file	O
I	O
am	O
trying	O
to	O
read	O
is	O
366	O
Mb	O
,	O
the	O
code	O
above	O
works	O
if	O
I	O
cut	O
the	O
file	O
down	O
to	O
something	O
short	O
(	O
25	O
Mb	O
)	O
.	O

It	O
has	O
also	O
happened	O
that	O
I	O
get	O
a	O
pop	O
up	O
telling	O
me	O
that	O
it	O
can't	O
write	O
to	O
address	O
0x1e0baf93	O
...	O

Stacktrace	O
:	O

@CODE	O

A	O
bit	O
of	O
background	O
-	O
I	O
am	O
trying	O
to	O
convince	O
people	O
that	O
Python	O
can	O
do	O
the	O
same	O
as	O
R	O
.	O

For	O
this	O
I	O
am	O
trying	O
to	O
replicate	O
an	O
R	O
script	O
that	O
does	O

@CODE	O

R	O
not	O
only	O
manages	O
to	O
read	O
the	O
above	O
file	O
just	O
fine	O
,	O
it	O
even	O
reads	O
several	O
of	O
these	O
files	O
in	O
a	O
for	O
loop	O
(	O
and	O
then	O
does	O
some	O
stuff	O
with	O
the	O
data	O
)	O
.	O

If	O
Python	O
does	O
have	O
a	O
problem	O
with	O
files	O
of	O
that	O
size	O
I	O
might	O
be	O
fighting	O
a	O
loosing	O
battle	O
...	O

Although	O
this	O
is	O
a	O
workaround	O
not	O
so	O
much	O
as	O
a	O
fix	O
,	O
I'd	O
try	O
converting	O
that	O
CSV	O
to	O
JSON	O
(	O
should	O
be	O
trivial	O
)	O
and	O
using	O
`	O
read_json	B-API
`	O
method	O
instead	O
-	O
I've	O
been	O
writing	O
and	O
reading	O
sizable	O
JSON	O
/	O
dataframes	B-API
(	O
100s	O
of	O
MB	O
)	O
in	O
Pandas	O
this	O
way	O
without	O
any	O
problem	O
at	O
all	O
.	O

There	O
is	O
no	O
error	O
for	O
Pandas	O
0.12.0	O
and	O
NumPy	O
1.8.0	O
.	O

I	O
have	O
managed	O
to	O
create	O
a	O
big	O
DataFrame	B-API
and	O
save	O
it	O
to	O
a	O
csv	O
file	O
and	O
then	O
successfully	O
read	O
it	O
.	O

Please	O
see	O
the	O
example	O
here	O
.	O

The	O
size	O
of	O
the	O
file	O
is	O
554	O
Mb	O
(	O
It	O
even	O
worked	O
for	O
1.1	O
Gb	O
file	O
,	O
took	O
longer	O
,	O
to	O
generate	O
1.1Gb	O
file	O
use	O
frequency	O
of	O
30	O
seconds	O
)	O
.	O

Though	O
I	O
have	O
4Gb	O
of	O
RAM	O
available	O
.	O

My	O
suggestion	O
is	O
try	O
updating	O
Pandas	O
.	O

Other	O
thing	O
that	O
could	O
be	O
useful	O
is	O
try	O
running	O
your	O
script	O
from	O
command	O
line	O
,	O
because	O
for	O
R	O
you	O
are	O
not	O
using	O
Visual	O
Studio	O
(	O
this	O
already	O
was	O
suggested	O
in	O
the	O
comments	O
to	O
your	O
question	O
)	O
,	O
hence	O
it	O
has	O
more	O
resources	O
available	O
.	O

I	O
encountered	O
this	O
issue	O
as	O
well	O
when	O
I	O
was	O
running	O
in	O
a	O
virtual	O
machine	O
,	O
or	O
somewere	O
else	O
where	O
the	O
memory	O
is	O
stricktly	O
limited	O
.	O

It	O
has	O
nothing	O
to	O
do	O
with	O
pandas	O
or	O
numpy	O
or	O
csv	O
,	O
but	O
will	O
always	O
happen	O
if	O
you	O
try	O
using	O
more	O
memory	O
as	O
you	O
are	O
alowed	O
to	O
use	O
,	O
not	O
even	O
only	O
in	O
python	O
.	O

The	O
only	O
chance	O
you	O
have	O
is	O
what	O
you	O
already	O
tried	O
,	O
try	O
to	O
chomp	O
down	O
the	O
big	O
thing	O
into	O
smaller	O
pieces	O
which	O
fit	O
into	O
memory	O
.	O

If	O
you	O
ever	O
asked	O
yourself	O
what	O
MapReduce	O
is	O
all	O
about	O
,	O
you	O
found	O
out	O
by	O
yourself	O
...	O

MapReduce	O
would	O
try	O
to	O
distribute	O
the	O
chunks	O
over	O
many	O
machines	O
,	O
you	O
would	O
try	O
to	O
process	O
the	O
chunke	O
on	O
one	O
machine	O
one	O
after	O
another	O
.	O

What	O
you	O
found	O
out	O
with	O
the	O
concatenation	O
of	O
the	O
chunk	O
files	O
might	O
be	O
an	O
issue	O
indeed	O
,	O
maybe	O
there	O
are	O
some	O
copy	O
needed	O
in	O
this	O
operation	O
...	O
but	O
in	O
the	O
end	O
this	O
maybe	O
saves	O
you	O
in	O
your	O
current	O
situation	O
but	O
if	O
your	O
csv	O
gets	O
a	O
little	O
bit	O
larger	O
you	O
might	O
run	O
against	O
that	O
wall	O
again	O
...	O

It	O
also	O
could	O
be	O
,	O
that	O
pandas	O
is	O
so	O
smart	O
,	O
that	O
it	O
actually	O
only	O
loads	O
the	O
individual	O
data	O
chunks	O
into	O
memory	O
if	O
you	O
do	O
something	O
with	O
it	O
,	O
like	O
concatenating	O
to	O
a	O
big	O
df	B-API
?	O

Several	O
things	O
you	O
can	O
try	O
:	O

Don't	O
load	O
all	O
the	O
data	O
at	O
once	O
,	O
but	O
split	O
in	O
in	O
pieces	O

As	O
far	O
as	O
I	O
know	O
,	O
hdf5	O
is	O
able	O
to	O
do	O
these	O
chunks	O
automatically	O
and	O
only	O
loads	O
the	O
part	O
your	O
program	O
currently	O
works	O
on	O

Look	O
if	O
the	O
types	O
are	O
ok	O
,	O
a	O
string	O
'	O
0.111111	O
'	O
needs	O
more	O
memory	O
than	O
a	O
float	O

What	O
do	O
you	O
need	O
actually	O
,	O
if	O
there	O
is	O
the	O
adress	O
as	O
a	O
string	O
,	O
you	O
might	O
not	O
need	O
it	O
for	O
numerical	O
analysis	O
...	O

A	O
database	O
can	O
help	O
acessing	O
and	O
loading	O
only	O
the	O
parts	O
you	O
actually	O
need	O
(	O
e.g.	O
only	O
the	O
1%	O
active	O
users	O
)	O

I	O
use	O
Pandas	O
on	O
my	O
Linux	O
box	O
and	O
faced	O
many	O
memory	O
leaks	O
that	O
only	O
got	O
resolved	O
after	O
upgrading	O
Pandas	O
to	O
the	O
latest	O
version	O
after	O
cloning	O
it	O
from	O
github	O
.	O

Memory	O
errors	O
happens	O
a	O
lot	O
with	O
python	O
when	O
using	O
the	O
32bit	O
version	O
in	O
Windows	O
.	O

This	O
is	O
because	O
32bit	O
processes	O
only	O
gets	O
2GB	O
of	O
memory	O
to	O
play	O
with	O
by	O
default	O
.	O

If	O
you	O
are	O
not	O
using	O
32bit	O
python	O
in	O
windows	O
but	O
are	O
looking	O
to	O
improve	O
on	O
your	O
memory	O
efficiency	O
while	O
reading	O
csv	O
files	O
,	O
there	O
is	O
a	O
trick	O
.	O

The	O
pandas.read_csv	B-API
function	O
takes	O
an	O
option	O
called	O
`	O
dtype	B-API
`	O
.	O

This	O
lets	O
pandas	O
know	O
what	O
types	O
exist	O
inside	O
your	O
csv	O
data	O
.	O

By	O
default	O
,	O
pandas	O
will	O
try	O
to	O
guess	O
what	O
dtypes	B-API
your	O
csv	O
file	O
has	O
.	O

This	O
is	O
a	O
very	O
heavy	O
operation	O
because	O
while	O
it	O
is	O
determining	O
the	O
dtype	B-API
,	O
it	O
has	O
to	O
keep	O
all	O
raw	O
data	O
as	O
objects	O
(	O
strings	O
)	O
in	O
memory	O
.	O

Let's	O
say	O
your	O
csv	O
looks	O
like	O
this	O
:	O

@CODE	O

This	O
example	O
is	O
of	O
course	O
no	O
problem	O
to	O
read	O
into	O
memory	O
,	O
but	O
it's	O
just	O
an	O
example	O
.	O

If	O
pandas	O
were	O
to	O
read	O
the	O
above	O
csv	O
file	O
without	O
any	O
dtype	B-API
option	O
,	O
the	O
age	O
would	O
be	O
stored	O
as	O
strings	O
in	O
memory	O
until	O
pandas	O
has	O
read	O
enough	O
lines	O
of	O
the	O
csv	O
file	O
to	O
make	O
a	O
qualified	O
guess	O
.	O

I	O
think	O
the	O
default	O
in	O
pandas	O
is	O
to	O
read	O
1,000,000	O
rows	O
before	O
guessing	O
the	O
dtype	B-API
.	O

By	O
specifying	O
`dtype={'age	O
'	O
:	O
int}	O
`	O
as	O
an	O
option	O
to	O
the	O
`	O
`	O
will	O
let	O
pandas	O
know	O
that	O
age	O
should	O
be	O
interpretted	O
as	O
a	O
number	O
.	O

This	O
saves	O
you	O
lots	O
of	O
memory	O
.	O

However	O
,	O
if	O
your	O
csv	O
file	O
would	O
be	O
corrupted	O
,	O
like	O
this	O
:	O

@CODE	O

Then	O
specifying	O
`dtype={'age	O
'	O
:	O
int}	O
`	O
will	O
break	O
the	O
`	O
`	O
command	O
,	O
because	O
it	O
cannot	O
cast	O
`"	O
40+	O
"`	O
to	O
int	O
.	O

So	O
sanitize	O
your	O
data	O
carefully	O
!	O

Here	O
you	O
can	O
see	O
how	O
the	O
memory	O
usage	O
of	O
a	O
pandas	O
dataframe	B-API
is	O
a	O
lot	O
higher	O
when	O
floats	O
are	O
kept	O
as	O
strings	O
:	O

@CODE	O
