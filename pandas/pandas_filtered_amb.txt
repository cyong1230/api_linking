Rolling median in python
I have some stock data based on daily close values . I need to be able to insert these values into a python list and get a median for the last 30 closes . Is there a python library that does this ?
It would be more efficient to do the median of the last 29 values . If 30 was an arbitrary choice , you can avoid having to calculate the mean of the middle two if you choose an odd-numbered window size .
isn't the median just the middle value in a sorted range ?
For an list with an even number of values the median is the mean of the two middle values .

@USER Pennington : So I was able convert my ~ 1,000 line program easily to using pandas . great call ... any idea how I might get my compound return series created ? I was was thinking of using a lambda function in the apply method of DataMatrix but I'm having some challenges ... Thanks
@USER Pennington : couponded is an array in this case and has no shift method ...
@USER Pennington : Done , sorry , new to Stack :)

a ) Interpolate / fill missing times in each TimeSeries ( I know this is possible in Pandas , I just don't know how to do it )
b ) strip the seconds out of python datetime objects ( Set seconds to 00 , without changing minutes ) . I'd lose a degree of accuracy , but not a huge amount
You have a number of options using pandas , but you have to make a decision about how it makes sense to align the data given that they don't occur at the same instants .
you can see these are off by 30 seconds . The ` reindex ` function enables you to align data while filling forward values ( getting the " as of " value ): #CODE
Strip seconds out of all your datetimes . The best way to do this is to use ` rename ` #CODE

I tested with apply , it seems that when there are many sub groups , it's very slow . the groups attribute of grouped is a dict , you can choice index directly from it : #CODE

append two data frame with pandas
I try to merge dataframes by rows doing : #CODE
Could you please post snapshots of both ` dataframe ` objects ( i.e. in the python interpreter , type in the name of the ` dataframe ` so the objects are enumerated on the screen ) . Also post what keywords you used when you performed the merge . As you can see #URL there are a few ways to combine dataframes ... make sure you're using the one that makes the most sense for what you're trying to accomplish
The ` append ` function has an optional argument ` ignore_index ` which you should use here to join the records together , since the index isn't meaningful for your application .

Just as a small addition , you can also do an apply if you have a complex function that you apply to a single column :
probably x is a confusing name for the column name and the row variable , though I agree apply is easiest way to do it :)
just to add , ` apply ` can also be applied to multiple columns :
Can apply take in a function defined elsewhere in code ? this is so that we can introduce a more complicated function

Is it possible create a matching panel that only has these columns and then somehow merge the two ?
Here's the 2nd panel I would like to append : #CODE
Sorry I missed this question some time ago . Could you have a look at the new concat function in pandas 0.7.0 and see if it meets your needs :
I recently spent a great deal of time on the join and concatenation methods .
Thanks for the post . I've moved on from this problem ( I hacked together a loop ) but looking at your concat documentation this does appear to solve the issue . I think the data structure is a candidate for redesign using a MultiIndex and I will revisit this in the future .

I realize the map function can only output lists . The output is a list of the header from the returned dataframe My output from print test1 looks like this : #CODE

How to apply slicing on pandas Series of strings
I'm playing with pandas and trying to apply string slicing on a Series of strings object .
I got it to work by using the map function instead , but I think I'm missing something about how it's supposed to work .
` apply ` first tries to apply the function to the whole series . Only if that fails it maps the given function to each element . ` [: 2 ]` is a valid function on a series , ` + ' qwerty '` apparently isn't , that's why you do get the implicit mapping on the latter . If you always want to do the mapping you can use ` s.map ` .
` apply `' s source code for reference : #CODE

To clarify : The ` any ( 1 )` approach wouldn't work if you had other values in the table that you didn't want to filter . Suppose there are many columns and you only want the ` any ` to apply to a subset of them ( you know the subset's labels ) .
There are at least a few approaches to shortening the syntax for this in Pandas , until it gets a full query API down the road ( perhaps I'll try to join the github project and do this is time permits and if no one else already has started ) .

Why is ` pandas ` so much faster than ` data.table ` ? Is it because of an inherent speed advantage python has over R , or is there some tradeoff I'm not aware of ? Is there a way to perform inner and outer joins in ` data.table ` without resorting to ` merge ( X , Y , all=FALSE )` and ` merge ( X , Y , all=TRUE )` ?
My hypothesis : because data.table is based on data.frame and data.frames are slow . And I think most of the pandas merge code is in Cython .
The comparison with ` data.table ` is actually a bit interesting because the whole point of R's ` data.table ` is that it contains pre-computed indexes for various columns to accelerate operations like data selection and merges . In this case ( database joins ) pandas ' DataFrame contains no pre-computed information that is being used for the merge , so to speak it's a " cold " merge . If I had stored the factorized versions of the join keys , the join would be significantly faster - as factorizing is the biggest bottleneck for this algorithm .
Of course , now that you've figured it all out in python , it should be easy to translate into R ;)
Does ` Rprof() ` reveal most of the time spent in the call ` sortedmatch ( levels ( i [[ lc ]]) , levels ( x [[ rc ]])` ? This isn't really the join itself ( the algorithm ) , but a preliminary step .
Recent efforts have gone into allowing character columns in keys , which should resolve that issue by integrating more closely with R's own global string hash table . Some benchmark results are already reported by ` test.data.table() ` but that code isn't hooked up yet to replace the levels to levels match .
Also , ` data.table ` has time series merge in mind . Two aspects to that : i ) multi column ordered keys such as ( id , datetime ) ii ) fast prevailing join ( ` roll=TRUE `) a.k.a. last observation carried forward .
` data.table ` has time series merge in mind . Two aspects to that : i )
join ( ` roll=TRUE `) a.k.a. last observation carried forward .
So the Pandas equi join of two character columns is probably still faster than data.table . Since it sounds like it hashes the combined two columns . data.table doesn't hash the key because it has prevailing ordered joins in mind . A " key " in data.table is literally just the sort order ( similar to a clustered index in SQL ; i.e. , that's how the data is ordered in RAM ) . On the list is to add secondary keys , for example .
If you supply a test case for a reasonably large , realistic data set , I'll be happy to run the benchmarks . You're more than welcome to , also . I actually have not yet optimized the code for the integer join key case ( put that on my todo list ! ) , but you can expect significantly better performance than the string case given the hash table study in the linked presentation .
Here's some Rprof results #URL It looks like 20-40 % of the time is spent in sortedmatch depending on the join type . Will have to look into integer columns another time -- I made a pandas GitHub issue to remind me to optimize that case ( #URL )
The graph depicted there shows how different tools and packages compare in terms of aggregation and join speed . It was really educational for me .
I hope someone does a join benchmark soon too !

See Reshaping and Pivot Tables
You can also use panels to help you do this pivot . Like this : - #CODE

Python Panda Pivot Table
I am trying to do a pivot table of frequency counts using Panda .
Just replace ` rows =[ ' Y ']` with ` rows =[ ' X2 ']` #CODE

Sage . It doesn't have the GUI tools of Enthought but otherwise contains a full scientific python stack .
Besides Python itself you can also choose there among a dozen of available Python IDEs ( e.g. spyder , Eric , PIDA , and others ) , Python-aware editors ( vim , emacs etc ) , alternative Python implementations ( pypy ) , compilers ( Cython , nuitka ) , etc . Debug build of Python ( python-dbg ) in tandem with gdb allow you right away debug your extensions while inspecting Python stack etc . And all of those Python-specific tools are available within the same software management framework as the rest of the system which carries thousands of generic and specialized software tools and resources .

I would also be interested in aligning my irregular timestamp intervals to one second resolution , I would still wish to plot multiple events for a given second , but maybe I could introduce a unique index , then align my prices to it ?
Thank you . I will join pystatsmodels -- if you are looking for stumbling noobs with use cases , I could be fertile territory .

I stumbled across pandas and it looks ideal for simple calculations that I'd like to do . I have a SAS background and was thinking it'd replace proc freq -- it looks like it'll scale to what I may want to do in the future . However , I just can't seem to get my head around a simple task ( I'm not sure if I'm supposed to look at ` pivot / crosstab / indexing ` - whether I should have a ` Panel ` or ` DataFrames ` etc ... ) . Could someone give me some pointers on how to do the following :
I can't work out whether I should be using ` pivot / crosstab / groupby / an index `
Then , using the ability to apply multiple aggregation functions following a groupby , you can say : #CODE

That's OK , although it has the problem that I don't even know the number of columns beforehand . I think I will continue converting the dataframe after loading with the apply method .

Reason I put table=True is that I want to * append * to existing tables for very large data sets . So it seems the combination of append and mixed-type is still on the todo-list ?

Pandas rolling median for duplicate time series data
I see that Pandas does not allow duplicate time series indexes yet ( #URL ) , but will be added soon . I am wondering if there is a good way to apply rolling window means to a dataset with duplicate times by a multi-index tag / column

join or merge with overwrite in pandas
I want to perform a join / merge / append operation on a dataframe with datetime index .

Try the ` truncate ` method : #CODE
Can you link to a source for this ? I'm on #URL and I haven't found the truncate function .
@USER : here's the link to the description of truncate in the current documentation ( v0.7.2 ): #URL
@USER no reason , just an API oversight . See my answer below -- if someone would contribute some docs about truncate that would be helpful .

I am trying to insert a pandas ( pandas.pydata.org ) DataFrame into a Postgresql DB ( 9.1 ) in the most efficient way ( using Python 2.7 ) .

it uses numpys " argmax " function to find the rowindex in which the maximum appears .
i tested the speed on a dataframe with 24735 rows , grouped into 16 groups ( btw : dataset from planethunter.org ) and got 12.5 ms ( argmax ) vs 17.5 ms ( sort ) as a result of %timeit . So both solutions are quite fast :-) and my dataset seems to be too small ;-)
If the number of " obj_id " s is very high you'll want to sort the entire dataframe and then drop duplicates to get the last element . #CODE
However mine uses the apply function of a dataframe instead of the aggregate .

Couldn't resist answering this , even though the question was asked , and answered , in 2012 , by Wes himself . Yes , just use truncate . #CODE

Cut the file showing only a few rows , say 13 rows --- three dead rows , then rerun the file . You may have the correct data there . The problem you are having is that the rows are 157968 . Pandas cant show all those rows . The output is a summary table . Or , its spyder doing the summary . I have seen similar summary tables .

I have been using the scikits.statsmodels OLS predict function to forecast fitted data but would now like to shift to using Pandas .

and apply agg() with it : #CODE

to pivot the values . I need a way to convert cdiff.DATE to a month rather than a date .
I tried all manner of ` strftime ` methods on cdiff.DATE with no success . It wants to apply the to strings , not series object .

I think you received a KeyError because `` df `` was indexed before joining , thus ' first ' was no longer a column to join " on " . To solve this , can you set the index after joining ?

I am trying do use a pandas multiindex to select a partial slice at the top level index ( date ) , and apply a list to the second level index ( stock symbol ) . I.e. below I want the data for AAPL and MSFT in the range #URL
which DOES work when passed to ix . Below is my desired output . #CODE

How to resample a dataframe with different functions applied to each column ?
You can also downsample using the ` asof ` method of ` pandas.DateRange ` objects . #CODE
@ Wes McKinney this should be ` resample ` in 0.8 , isn't it ?

Pandas : simple ' join ' not working ?
I like to think I'm not an idiot , but maybe I'm wrong . Can anyone explain to me why this isn't working ? I can achieve the desired results using ' merge ' . But I eventually need to join multiple ` pandas ` ` DataFrames ` so I need to get this method working . #CODE
Try using ` merge ` ( #URL ): #CODE
Interesting . So it looks like in order to get what I want I'll have to perform successive merges , since ` merge ` only take two DataFrames ?
I had the same problem and found this answer . It's correct . From the 0.16.2 docs : The related DataFrame.join method , uses merge internally for the index-on-index and index-on-column ( s ) joins , but joins on indexes by default rather than trying to join on common columns ( the default behavior for merge ) .

Best way to insert a new value

Now use this as an auxiliary index variable and unstack : #CODE

Is there a way to index ` series ` by the mapping of result / frequency defined by ` freq ` ?
Yes , use the ` map ` Series method : #CODE

Pandas : trouble understading how merge works
I'm doing something wrong with merge and I can't understand what it is . I've done the following to estimate a histogram of a series of integer values : #CODE
If I print ` hist ` and ` freq ` this is what I get : #CODE
They're both indexed by `" series "` but if I try to merge : #CODE
on : Columns ( names ) to join on . Must be found in both the left and
are False , the intersection of the columns in the DataFrames will be
inferred to be the join keys
Alternatively and more simply , ` DataFrame ` has ` join ` method which does exactly what you want : #CODE
Time to improve the merge docstring !

I would like to use pandas to create a HLOC chart of data for every one minute starting with time zero being 9:46 using the asof method .... I would also like to know how to stream data into a pandas dataframe as updates . Is this possible ? #CODE
How would do I get 0.8.0 . please point to link . I can get to github , how do i pull the dev version . On above how do I update the table new data without recreating the whole table . Is there a add to table method .... thinking ..... take new data , process ( ts.convert ) . append table .. numpy add to array . any help here .
You can append data ( yielding a new object ) with df.append ( new_data ) but that's not especially efficient

I am trying to draw a table ( rectangle ) as a plot with 96 individual cells ( 8 rows X 12 cols ) . Color each alternative cell with a specific color ( like a chess board : instead of black / white I will use some other color combination ) and insert value for each cell from a pandas data frame or python dictionary . Show the col and row labels on the side .
Your data presumably ( ? ) has the same number of values in each row . You need to define what you want to do . You can either truncate the data , or add an extra column .

This function was updated to the name ` idxmax ` in the Pandas API , though as of Pandas 0.16 , ` argmax ` still exists and performs the same function ( though appears to run more slowly than ` idxmax `) .
Previously ( as noted in the comments ) it appeared that ` argmax ` would exist as a separate function which provided the integer position within the index of the row location of the maximum element . For example , if you have string values as your index labels , like rows ' a ' through ' e ' , you might want to know that the max occurs in row 4 ( not row ' d ') . However , in pandas 0.16 , all of the listed methods above only provide the label from the ` Index ` for the row in question , and if you want the position integer of that label within the ` Index ` you have to get it manually ( which can be tricky now that duplicate row labels are allowed ) .
In general , I think the move to ` idxmax ` -like behavior for all three of the approaches ( ` argmax ` , which still exists , ` idxmax ` , and ` numpy.argmax `) is a bad thing , since it is very common to require the positional integer location of a maximum , perhaps even more common than desiring the label of that positional location within some index , especially in applications where duplicate row labels are common .
So here a naive use of ` idxmax ` is not sufficient , whereas the old form of ` argmax ` would correctly provide the positional location of the max row ( in this case , position 9 ) .
So you're left with hoping that your unit tests covered everything ( they didn't , or more likely no one wrote any tests ) -- otherwise ( most likely ) you're just left waiting to see if you happen to smack into this error at runtime , in which case you probably have to go drop many hours worth of work from the database you were outputting results to , bang your head against the wall in IPython trying to manually reproduce the problem , finally figuring out that it's because ` idxmax ` can only report the label of the max row , and then being disappointed that no standard function automatically gets the positions of the max row for you , writing a buggy implementation yourself , editing the code , and praying you don't run into the problem again .
Per #URL argmax is now idxmax . Just leaving the comment for others who stumble onto this question as I did .
Based on the second-to-last comment there , it looks like ` argmin ` and ` argmax ` will remain part of ` DataFrame ` and the difference is just whether you want the index or the label . ` idxmax ` will give you the label of the location where a max occurs . ` argmax ` will give you the index integer itself .
Note that you need to be careful trying to use the output of ` idxmax ` as a feeder into ` ix ` or ` loc ` as a means to sub-slice the data and / or to obtain the positional location of the max-row . Because you can have duplicates in the ` Index ` - see the update to my answer for an example .

Another way I did something similar was create a pivot table
In this case I want to convert this pivot table to 2d numpy array . Is there a way for me to index into each cell of this table . If so then I probably will be Ok with the table itself .

AttributeError : Cannot access callable attribute ' reset_index ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method
Aggregating functions are ones that reduce the dimension of the returned objects , for example : ` mean ` , ` sum ` , ` size ` , ` count ` , ` std ` , ` var ` , ` sem ` , ` describe ` , ` first ` , ` last ` , ` nth ` , ` min ` , ` max ` . This is what happens when you do for example ` DataFrame.sum() ` and get back a ` Series ` .

the best way to avoid this mess , imho , is to use use system python for that version ( 2.7 probably ) and , for other versions , use ` make alt-install ` when installing , which will give you executables like ` python3.1 ` and the like . if you really need to replace the version provided by the system , uninstall it .

let me try to answer this . basically i will pad or reindex with complete weekdays and sample every 5 days while drop missing data due to holiday or suspension
The final step is to " unstack " weekday from the
MultiIndex , creating columns for each weekday , and replace the weekday numbers with an abbreviation , to improve readability . #CODE
To create a line plot for each week , transpose the dataframe , so the columns are week numbers and rows are weekdays ( note this step can be avoided by unstacking week number , in place of weekday , in the previous step ) , and call ` plot ` . #CODE
I think the same concepts apply to an index of floats . You just need to write your own method to group samples into a period group and time step within the group . Hope that helps

That's a great and somewhat obscure answer . It would be nice to have a convenience function for this where you can pick the axes to interpolate over
Could also use DataFrame's interpolate method ? ` df2.interpolate() ` because ` df2.interpolate() == df2.apply ( pandas.Series.interpolate )` ( at least for me , ` pandas.__version__ == 0.14 `)

but it would work if the csv file would be somehow transpose . H
Obviously you may need to clean you data after import , e.g. you chould check for data types , remove empty fields ( or replace with None ) This version processes the entire dataset , but only returns one line , so you could use break at that point or perhaps append other interesting data . e.g. interesting =[ ' 1w ' , ' 3m ' , ' 9m '] ; if l [ 0 ] .strip() in interesting : ....

I'm trying to align my index values between multiple DataFrames or Series and I'm using
Series.interpolate but it doesn't seem to interpolate correctly . Or perhaps I am misunderstanding something . Here's a small example : #CODE
I don't think underlying mathematics apply that sum of interpolation equal to interpolation of sum . it only holds at special case
It might be a bug . Looking at the source , ` Series.interpolate ` doesn't look at the index values while doing interpolation . It assumes they are equally spaced and just uses ` len ( serie )` for indexes . Maybe this is the intention and it's not a bug . I'm not sure .
I modified the ` Series.interpolate ` method and came up with this ` interpolate ` function . This will do what you want . #CODE

After checking , it appears that DF is a tuple . The goal is to export the aligned data ( on common dates only ) using ` DF.to_csv ( path )` . It fails with the message that ' tuple ' object has no attribute ' to_csv ' . I don't understand why the join has created a tuple . Shouldn't this still be a dataframe that can be exported to a CSV ?
When I export the csv -- it gives back the * original * data set df1 ( & vice versa for if df1 and df2 are swapped in the align command ) . IE , it is not merging the data sets on common dates ( index ) . Is there a reason one shouldn't use align for this task ?
Using join works for what I needed . I'm still curious about the align ...
@USER Align , I would imagine , simply arranges the data .
` align ` returns aligned versions of the left and right DataFrames ( as a tuple ):

Not sure whats the way to append to current data frame in pandas or is there a way for pandas to suck a list of files into a data frame .
Once you have read the files and save it in two dataframes , you could merge the two dataframes or add additional columns to one of the two dataframes ( assuming common index ) . Pandas should be able to fill in missing rows .
Why not use read_csv , to build two ( or more ) dataframes , then use join to put them together ?
The pandas ` concat ` command is your friend here . Lets say you have all you files in a directory , targetdir . You can :

` dr = pd.date_range ( dt ( 2009 , 1 , 1 ) , dt ( 2010 , 12 , 31 ) , freq= ' H ') ;
dt = pd.DataFrame ( rand ( len ( dr ) , 2 ) , dr );
data = dt [ selector ]`
there is conflict of dt package and dt variable

Pandas : Sort pivot table
Just trying out Pandas for the first time , and I am trying to sort a pivot table first by an index , then by the values in a series .
What's the correct way to sort a pivot table by index then value ?

The design consideration of supporting convenience makes the learning curve much steep . I wish that there are better documentation for the beginning just presenting a consistent interface . For example , just focus on the ix interface .
using the ix

I'm getting a floating point error on a simple time series in pandas . I'm trying to do shift operations ... but this also happens with the window functions like ` rolling_mean ` .

On an aside : does truncate need to be existing indexes in the data ? I'm getting the following error ( on a different set of data to above ): #CODE

Note this is a very inefficient way to build a large DataFrame ; new arrays have to be created ( copying over the existing data ) when you append a row .
You could use ` pandas.concat() ` or ` DataFrame.append() ` . For details and examples , see Merge , join , and concatenate .
I has a similar problem where if I created a data frame for each row and append it to the main data frame it took 30 mins . On the other hand , if used below methodology , I was successful within seconds . #CODE
Copying from pandas docs : ` It is worth noting however , that concat ( and therefore append ) makes a full copy of the data , and that constantly reusing this function can create a significant performance hit . If you need to use the operation over several datasets , use a list comprehension . ` ( #URL )
Add rows through ` loc ` on non existing index data .
` .loc ` is referencing the index column , so if you're working with a pre-existing DataFrame with an index that isn't a continous sequence of integers starting with 0 ( as in your example ) , ` .loc ` will overwrite existing rows , or insert rows , or create gaps in your index . A more robust ( but not fool-proof ) approach for appending an existing nonzero-length dataframe would be : ` df.loc [ df.index.max() + 1 ] = [ randint ( ... ` or prepopulating the index as @USER suggested .

I have two aligned series in a DataFrame which have several unmatched " NaN " . I would like to print the intersection between them removing all " NaN's " , but without loose alignment . That is , I want to remove the rows from both series whem I find a " NaN " in one of them . It sounds simple , but I not doing any operation between the series to dropna's afterwards and cannot dropna's from the series separately . I couldn't figure out the right df function to do this - several are not documented .
Do you mean you want to drop rows where there are NaNs in either of the S or JEXP columns only ? ( I'm trying to help here ; please be clear , or I can't ! ) Try ` newdf = df1.dropna ( subset =[ ' S ' , ' JEXP '])`

Is there a more performant and / or more idiomatic way to do this ? I know about apply , but sometimes it's more convenient to use a for loop . Thanks in advance .
Thanks . Is apply more efficient than iterrows ?

Returning multiple values from pandas apply on a DataFrame
Now , supposing I have " a " and " b " as one group , and " c " and " d " at the other , I'm performing the t-test row-wise . This is fairly trivial with pandas , using ` apply ` with axis=1 . However , I can either return a DataFrame of the same shape if my function doesn't aggregate , or a Series if it aggregates .
Instead , ny expected output would be a DataFrame with two columns , one for the first result , and one for the second . Is this possible or I have to do two runs for the two calculations , then merge them together ?
Why are you using ` apply ` in the first place ? Your result is a new ` DataFrame ` with a shape different from the input ( both rows and columns ) , therefore it's a completely new obj . You could just have ` t_test_and_mean ` accept your input dataframe ( and the columns to group by ) and return a 1-row-2-columns dataframe , without using ` apply ` .

Some of the tables I'm displaying would be much easier to read with a little bit of formatting . I'd really like something like " zebra tables " where every other row is shaded . I read here about how this formatting can be implemented via css . Is there a really straight forward way to apply a css to an IPython Notebook and then have tables rendered using the style sheet ?
If you just stick that in one of your markdown cells , then it will apply to everything on the page .

b ) now do a left join on this on the data set A .
It can't be a constant O ( 1 ) time operation . Its complexity depends on the implementation , but by using normal python sets , it would probably be linear time in the number of element of B.index . ` pandas ` implementation does indeed build a ` set ` for each ` Index ` array and then does the * set difference * ( see #URL ) . It also does a ` sort ` of the output index , so finally the complexity is something like O ( m lg m ) with m = len ( B.index ) ...

I have two pandas DataFrames and I want to join them together such that I get the outer join with the duplicates removed . My problem is that ` .drop_duplicates() ` ignores the index when finding duplicates . If the index is different then it shouldn't be a duplicate . How do I remove duplicates if the row index and columns are duplicates ? The only thing I can think of is using ` df.to_dict() ` and then create a new DataFrame ( very inefficient ) .

And then join all the pieces together with ` pandas.concat ` or similar .
Is there a way to perform something similar by a clever use of ` crosstab ` or ` pivot_table ` or ` stack ` or something similar ?
Note that I have implemented new ` cut ` and ` qcut ` functions for discretizing continuous data :

How to optimally apply a function on all items of a dataframe using inputs from another dataframe ?
I am new in python and I am currenlt struggly to do simple things with pandas . I would like to apply the same function to each item of a given dataset but using a time-dependent parameter .
One way to do it is to use the ` map ` function , or ` numpy.vectorize ` ; it's also possible to do it with lambda functions . For example : #CODE

Again , ` read_table ` can be used , for example : ` pandas.read_table ( buf , sep= ' ' , index_col =[ 0 , 1 ] , header=None )` will create a table with multiple columns and a ` multiindex ` made of 2 levels : first level the year-month-day , second level the time . If you wish , you can then merge the multiindex in a normal index ( for example : ` df.index = [ ' %s %s ' % ( a , b ) for a , b in zip ( df.index.get_level_values ( 0 ) , df.index.get_level_values ( 1 ))]` . )
date_to = df.index [ len ( df.index ) -1 ] #last datetime entry in date frame
That's because the index is a list of strings , and it should be a list of datetime objects ( the snippet to " reconstruct " the index was an example ) . You can fix it with : ` df.index = map ( dateutil.parser.parse , df.index )` and then date-ranges are : ` pandas.DateRange ( df.index [ 0 ] , df.index [ -1 ])` .

You can use ` aggregate ` to define your aggregate function , which will just keep the first element of a column and drop the others . #CODE

Pandas : List of Column names in a pivot table
I got stuck trying to get the resulting names of a pivot table . The table printed using to_string() looks as below .

I'm having a bit of trouble altering a duplicated pandas DataFrame and not having the edits apply to both the duplicate and the original DataFrame .
Then I assign the ' d ' dataframe to variable ' e ' and apply some arbitrary math to column ' a ' using apply : #CODE
The problem arises in that the apply function apparently applies to both the duplicate DataFrame ' e ' and original DataFrame ' d ' , which I cannot for the life of me figure out : #CODE

can you specify the ' dropna ' value ? for example could you drop rows that are all zeros ?

[ Documentation ] ( #URL ) for read_csv now offers both ` na_values ` ( list or dict indexed by columns ) and ` keep_default_na ` ( bool ) . The ` keep_default_na ` value indicates whether pandas ' default NA values should be replaced or appended to . The OP's code doesn't work currently just because it's missing this flag . For this example , you could use ` pandas.read_csv ( ' test.csv ' , na_values =[ ' nan '] , keep_default_na=False )` .

Essentially my question would then boil down to : how to join several unaligned time series , where each series has a date column , and column for the series itself ( .CSV file exported from Excel )
the thing is you have to make sure that there is case like date exists but not rate . because dropna() will shift records and mismatch with index
Now , to join then together and align the data in a DataFrame , you can do : #CODE
This will form the union of the dates in ` ts1 ` and ` ts2 ` and align all of the data ( inserting NA values where appropriate ) .

So , apply this function to each of those 3 columns : #CODE

The ix [ , ] construct doesn't check if column exists .

I want to convert it ( all it's attributes ) to pandas dataframe . To do that i could loop through the file using beautiful soup and insert the values row by row or create lists to be inserted as columns . However I would like to know if there's a more pythonic way of accomplishing what I described . Thank you in advance .

It may be more efficient to break this up into a few operations as follows : ( 1 ) create a column of weights , ( 2 ) normalize the observations by their weights , ( 3 ) compute grouped sum of weighted observations and a grouped sum of weights , ( 4 ) normalize weighted sum of observations by the sum of weights .

I am not seeing ` concat ` as a function in the pandas namespace ; I'm not sure what I am missing .
I was running pandas ver 0.6.1 which doesn't have the concat function included . An upgrade to v 0.7.3 brings concat into the namespace . Works like a charm ! Thanks .

How to shift a column in Pandas DataFrame
I would like to shift a column in a Pandas DataFrame , but I havent been able to find a method to do it from the documentation without rewriting the whole DF . Does anyone know how to do it ? Help much appriciated .

You want to use the apply function and a lambda : #CODE

You'll first need to split the data by timestamp , then from there if you were to treat it as an array shift an element from the array to use as comparison if the quote type is equal then continue and shift a new element repeat until ! = then calculate spread . I don't know where your ` num_spread ` comes from .
that's exactly what i thought . i did groupby first but an array shift to compare is going to be extremely slow . i am trying to vectorize this . so far i have come up with assigning 1 to B and -1 to A . taking the difference in the array between itself and 1 lag will work except for the case where it is B A B A B A where it will yield 0 0 0 0 0 . I can fix this by looping but the whole point was to vectorize this to minimize computation time . num_spread comes from the amount of spreads that were calculated in that time period
I don't believe you can avoid iteration 100% with what you are trying to do . You can possibly duplicate the ` quote ` column twice shifting it by one each direction and apply it to your dataset to create a pivot table based on entries where ` quote ` is ! = to ` quote_next ` and ` quote_prev ` .

I'm using the excellent ` pandas ` package to deal with a large amount of varied meteorological diagnostic data and I'm quickly running out of dimensions as I stitch the data together . Looking at the documentation , it may be that using the ` MultiIndex ` may solve my problem , but I'm not sure how to apply it to my situation - the documentation shows examples of creating MultiIndexes with random data and DataFrames , but not Series with pre-existing timeseries data .
When I iterate over these composite objects , I have an ` iterseries ` routine for the frame and ` iterframes ` routine for the panel that reconstruct the appropriate metadata / data pairing as I drop one dimension ( i.e. the series from the frame with lead time varying across the columns will have all the metadata of its parent plus the ` Lead Time ` field restored with the value taken from the column label ) . This works great .
I've run out of dimensions ( up to 3-D with a Panel ) and I'm also not able to use things like ` dropna ` to remove empty columns once everything is aligned in the Panel ( this has led to several bugs when plotting summary statistics ) . Reading about using pandas with higher-dimensional data has led to reading about the ` MultiIndex ` and its use . I've tried the examples given in the documentation , but I'm still a little unclear how to apply it to my situation . Any direction would be useful . I'd like to be able to :
Once I have the frame given by this routine , I can easily apply the various operations suggested below - of particular utility is being able to use the ` names ` field when I
call ` concat ` - this eliminates the need to store the name of the column key internally

We added an ` append ` option to ` set_index ` . Try that .

How can I iterate and apply a function over a single level of a DataFrame with MultiIndex ?
Apply
However , I see that this doesn't drop the top level as you're looking for . Ideally you would be able to write something like :

This works for lists in general and I am familiar with it . How do I apply it to a pandas DataFrame ?

Indeed , pandas provides high level data manipulation tools built on top of NumPy . NumPy by itself is a fairly low-level tool , and will be very much similar to using MATLAB . pandas on the other hand provides rich time series functionality , data alignment , NA-friendly statistics , groupby , merge and join methods , and lots of other conveniences . It has become very popular in recent years in financial applications . I will have a chapter dedicated to financial data analysis using pandas in my upcoming book .
Would it be fair to say that numpy primarily provides efficient arrays , whereas pandas provides efficient dictionaries ? ( In both cases , limited to consistent data type rather than free form . ) To me ( I am just beginning to look into it now ) , this strikes me as the underlying difference : handling of label-paired data ( in 1d aka dicts and 2d aka tables ) . Data alignment , join , etc all become * possible * due to this , but for people who don't grok that underlying difference it's not even clear what those mean ( e.g. , what is " data alignment " of two numpy arrays ? ) .
Pandas offer a great way to manipulate tables , as you can make binning easy ( binning a dataframe in pandas in Python ) and calculate statistics . Other thing that is great in pandas is the Panel class that you can join series of layers with different properties and combine it using groupby function .

This isn't quite what's suggested in the documentation for pivot -- there , it shows results without the 1 and 0 in the upper-left-hand corner . And that's what I'm looking for , a DataFrame object that prints as #CODE
When the normal csv file ( with the preceding " 0 , ") reads into R , it doesn't properly set the column and row names from the frame object , resulting in painful manual manipulation to get it in the right format . Is there a way to get pivot to give me a DataFrame object without that additional information in the upper-left corner ?
to build on @USER , it would be really helpful if you could convert the pandas output to either a list of lists or a list of dicts and then show the result of the conversion e.g. with ` map ( list , B_p )` . ( since whatever form of the csv writer you are using needs to work on an iterable anyways ) .
Alas , that preceding comma also causes R to interpret the rownames as integers , placing the true row names as entries in the first column . My solution was to write script in R that saves the first column of the incoming Frame object , removes it , and assigned its values to the rownames . It's a bit clunky , but it works . I'm just confused because in the documentation ( help ( B_p.to_csv )) it doesn't show anything in the upper-left-hand corner when you pivot a table . I suspect this is some sort of left-over from the introduction of MultiIndexing , which I'll admit I don't understand thoroughly .

For making more general the answer ... first I will take the common index for synchronizing both dataframes , then I will join each of them to my pattern ( dates ) and I will sum the columns of the same name and finally join both dataframes ( deleting added columns in one of them ) ,

It seems like some combination of resample and / or fillna is going to get you what you're looking for ( realize this is coming a little late ! ) .

This is an approx . 3.2MB file if you dump it on disk . We now need to drop the ` DataRange ` type of your ` Index ` and make it a list of ` str ` to simulate how you would parse in your data : #CODE

I would like to roll through my data by date and on each date take a time slice in the past apply a function to every time series so I get a result such as this where X is the output of the function of timeslice . #CODE

Also is there some other way to do the following.Using Apply function seems to be very slow for large dataset .

pandas row specific apply
Similar to this R question , I'd like to apply a function to each item in a Series ( or each row in a DataFrame ) using Pandas , but want to use as an argument to this function the index or id of that row . As a trivial example , suppose one wants to create a list of tuples of the form [( index_i , value_i ) , ..., ( index_n , value_n )] . Using a simple Python for loop , I can do :
But there must be a more efficient way to do this ? Perhaps something more Panda-ish like Series.apply ? In reality , I'm not worried ( in this case ) about returning anything meaningful , but more for the efficiency of something like ' apply ' . Any ideas ?
If you use the apply method with a function what happens is that every item in the Series will be mapped with such a function . E.g. #CODE
A more complex usage of apply would be this one : #CODE
Following the OP's question for clarifications : Don't confuse Series ( 1D ) with DataFrames ( 2D ) #URL - as I don't really see how you can talk about rows . However you can include indices in your function by creating a new series ( apply wont give you any information about the current index ): #CODE

Have a look at the ` pandas.concat() ` function . When you read in your files , you can use ` concat ` to join the resulting DataFrames into one , then just use normal pandas averaging techniques to average them .

issue with pandas and semilog for boxplot
with an index of datetime . For some reason , when I use semilogy and boxplot with the video series , I get the error #CODE
but when I do it on the ' link ' series I can draw the boxplot correctly .
and I am able to draw the boxplot .
You could create a semi-log boxplot , for example , by : #CODE
Or , more generally , modify / transform to you heart's content , and then boxplot . #CODE

Pandas pivot warning about repeated entries on index
On Pandas documentation of the ` pivot ` method , we have : #CODE
But when I run the ` pivot ` method , it is saying : #CODE
Which doesn't makes sense , even in example there are repeated entries on the ` foo ` column . I'm using the ` name ` column as the index of the pivot , the first argument of the ` pivot ` method call .
Works fine for me ? Can you post the exact pivot method call you're using ? #CODE
If you have duplicates you may need to aggregate first . It would be nice to add an option to pivot to take either the first or last observed entry : #URL

I fixed the parser bug shown in the stack trace that you pasted . However , I'm wondering whether your date column is named " TRUE " or did you mean to just pass a boolean ? I haven't dug through pandas history but I know that in 0.8 we are supporting much more complex date parsing behavior as part of the time series API so here we're interpreting the string value as a column name .

Find all elements in dataframe column that startswith string
Try saving that to a temporary variable and using the temp variable inside ` startswith ` . I'm planning to add vectorized string functions to pandas soon , so that will be a big help in these cases , too .

` df = df.index.drop ( ' L ')` removes L completely from the DataFrame ( unlike ` df= df.reset_index() ` which has a drop argument ) .

Yes this was implicit in my answer . The bit about the copy was only for use of ` ix [ ]` if you * prefer * to use ` ix [ ]` for any reason .
` ix ` indexes rows , not columns . I thought the OP wanted columns .
` ix ` accepts slice arguments , so you can also get columns . For example , ` df.ix [ 0:2 , 0:2 ]` gets the upper left 2x2 sub-array just like it does for a NumPy matrix ( depending on your column names of course ) . You can even use the slice syntax on string names of the columns , like ` df.ix [ 0 , ' Col1 ' : ' Col5 ']` . That gets all columns that happen to be ordered between ` Col1 ` and ` Col5 ` in the ` df.columns ` array . It is incorrect to say that ` ix ` indexes rows . That is just its most basic use . It also supports much more indexing than that . So , ` ix ` is perfectly general for this question .
Thanks for the education . You're right . Never knew about that feature of ` ix ` .
As I noted in my comment above , ` .ix ` is * not * just for rows . It is for general purpose slicing , and can be used for multidimensional slicing . It is basically just an interface to NumPy's usual ` __getitem__ ` syntax . That said , you can easily convert a column-slicing problem into a row-slicing problem by just applying a transpose operation , ` df.T ` . Your example uses ` columns [ 1:3 ]` , which is a little misleading . The result of ` columns ` is a ` Series ` ; be careful not to just treat it like an array . Also , you should probably change it to be ` columns [ 2:3 ]` to match up with your " 3rd & 4th " comment .
The ` drop ` method is documented here .

I am using pandas in python . I have several Series indexed by dates that I would like to concat into a single DataFrame , but the Series are of different lengths because of missing dates etc . I would like the dates that do match up to match up , but where there is missing data for it to be interpolated or just use the previous date or something like that . What is the easiest way to do this ?

grouped pandas DataFrames : how do I apply scipy.stats.sem to them ?
I know that I can apply numpy methods by doing the following :
and so on . However , what if I want to compute the standard error of the mean ( sem ) ?
But you should check to make sure this is actually the SEM values you wanted , probably on some smaller example data .
That was it ! I checked it against my data and the axis=1 method correctly computes ` sem() ` . Oddly , the results of the method I used before to compute sem , ` std / sqrt ( # replicates )` , varies _slightly_ from ` sem() ` results , but we're only talking ~ 4e-11 difference max . I'm guessing it's rounding . Thanks mate .

Interesting to note that the first column , " i " , is an Integer col in PG . I'm not sure why Pandas thinks this is a " bool " type column . My real issue though is the " object " column which I think I need to be of some type of timestamp .
I'm a bit confused on what you are doing here . Doesn't the true / false part of QRY explain the bool nature of i for your example ? Maybe we need to see the DB schema .

I have a data table using pandas and column labels that I need to edit to replace the original column labels .
I have the edited column names stored it in a list , but I don't know how to replace the column names .

This will put them into dictionary form , including the earlier defined class and subject variables , and append them to an outputList . You'll end up with this : #CODE

The trick here is to use the ` axis=1 ` option in the ` apply ` to pass elements to the lambda function row by row , as opposed to column by column .

once sorted I replace the df.index with a numerical index #CODE

Append extras informations to Serie in Pandas
Is it possible to customize Serie ( in a simple way , and DataFrame by the way :p ) from pandas to append extras informations on the display and in the plots ? A great thing will be to have the possibility to append informations like " unit " , " origin " or anything relevant for the user that will not be lost during computations , like the " name " parameter .

Why map instead of apply ?
Using the dt option playing around with weekofyear , dayofweek etc . becomes a easier

It seems like it would be a good idea to have all the cur_df in one " big " DataFrame . Both for plotting and also for statistics later on . How would this be done ideally , considering they have not the same dates ? Is there a way to " merge " multiple DataFrames , but what is done at dates that occur only in one of the underlying DataFrames ?
making things continuous : you should just be able to use resample directly

Basically , I'm trying to pivot on location to end up with a dataframe like : #CODE
Unfortunately when I pivot , the index , which is equivalent to the original dates column , does not change and I get : #CODE
I'm current calling Pivot as such : #CODE
because I have a # of data columns I want to pivot ( don't want to list each one as an argument ) . I believe by default pivot pivots the rest of the columns in the dataframe .
I'm actually calling df.pivot without the third argument as in my actual data , i have a # of data columns and I want to pivot all of them . Would that be part of it ?
If you have multiple data columns , calling pivot without the values columns should give you a pivoted frame with a MultiIndex as the columns : #CODE
Yeah I'm seeing the information come out as a multiindex , but again , I get the same issue where pandas seems to recognize all the dates as unique and I get a bunch of Nans . Even if I set the pivot argument values to say column C , I still get the same # of rows as in my original table , just with Nans for all the repeated dates .
Just answered my own question . I was using an old Sybase module to import data and I think it used an old DateTimeType object from mxDatetime . In that module , a datetime of Jan 01 2011 would not necessarily equal another datetime of Jan 01 2011 ( e.g. each datetime was unique ) . Hence the dataframe pivot treated each column value as unique in the index .

I'm trying to drop the last row in a dataframe created by pandas in python and seem to be having trouble . #CODE
I tried the drop method like this : #CODE
I also tried to drop by index name and it still doesn't seem to be working .

EDIT : I just realized that most of the other functions ( min , max , median , etc . ) work fine but not the mean function that i desperately need :-( .
Also , you don't really need the lambda here , just feeding ` np.mean ` would work too , but I left the lambda in to illustrate how you would solve this when more general functions that you want to apply aren't working in their default ways . The ` .apply ` function is very powerful in Pandas .

You can replace the ` [ 0:4 ]` with ` [ df.index.values [ i ]: df.index.values [ j ]]` or ` [ df.index.values [ i ] for i in range ( N )]` or even with logical values such as ` [ df [ ' a '] 5 ]` to only get rows where the ' a ' column exceeds 5 , for example .

I think it uses ` patsy ` in the backend to translate the formula expression , and intercept is added automatically .

The reason your ` ADX ` call fails is because it expects an xts or matrix-like object with 3 columns : High , Low , Close . Your object contains 4 columns . Drop the date column before passing ` r_dataframe ` to ` ADX ` and everything should work . You can then add the datetime column back to the ` ADX ` output .
dalejung on GitHub has done quite a bit of work recently in creating a tighter pandas-xts interface with rpy2 , you might get in touch with him or join the PyData mailing list

Length : 3 , Freq : 3H , Timezone : None
Length : 3 , Freq : H , Timezone : None

Hi Wes , is there any update on this ? We run into issues that join columns are converted into either ints or floats , based on the existence of a NA value in the original list . ( Creating issues later on when trying to merge these dataframes )

I haven't done time benchmarking , but I am skeptical of the following immediately obvious way that comes to mind ( and variants that might use ` map ` or ` filter `) . The use cases of interest need to quickly get info on the types of all elements , so generators and the like probably won't be an efficiency boon here . #CODE
I went ahead and did a tiny benchmark in IPython . First is for ` vtype ` above , then for the ` apply ` route . I repeated it a dozen or so times , and this example run is pretty typical on my machine .

It seems to work for me on 0.8.1dev . Can you post a stack trace and / or what merged2 looks like ? Also are you sure you're using pandas 0.8 ? #CODE

I used pivot to reshape my data and now have a column multiindex . I want the resulting columns to be the X variables in a simple OLS regression . The Y's are another series with the same row index .
I can create a new dataframe , loop over both column indexes , and insert new columns into the new dataframe with the same name , but with names as strings instead of tuples . There must be a more elegant , single command , right ?

Thx @USER She . Not sure if this is the right forum but I would like to contribute - what would it take for me to be able say to create a pull request just for that one line of code ? I.e. how easy is it for me to cvs checkout the code , test my changes in iPython rather than with the prod version then creating a pull request ? Is there a tutorial for that somewhere ? I am familiar with subversion and I've done thousands of cvs commit in a proprietary system , just not familiar with the opensource tools .

Thanks ! I used this for a Kaggle competition ; we were given a dataset with music ratings from different users and we had to build a model that would predict how these users would rate new tracks from different artists . One of my features for the classifier was to look at the average rating given to a particular artist from that specific user . But if the user had never heard that artist before , that entry would show up as a missing value in Pandas . So in this case I would replace that missing value with the average rating given to that artist ( a bad first approximation , better to use the SVD )
Ah I see . I'm guessing you have something like users as the index and artist / track as a MultiIndex of columns ? It depends on the size of your DataFrame , but potentially you could repeat the mean rating so it's the same size as the ratings matrix and then use the NA mask to replace the missing values ?
close ; I did a read_csv on the training data , but I didn't choose an index . I built the features by using the pandas group operations , then applied the mean() on the group , and finally did a merge back onto the main dataframe . Some of the features use data from multiple columns , so I just grouped with those column labels , and then merged again ( with multiple indices ) . Thanks for cython-izing the merges :) For the missing data , I had to manually loop over the column and use get_value / set_value , it's not the most efficient way , but it works .

This supersedes the ` irow ` approach .
This will preserve your old indices ( 10 , 20 , etc . ) by moving them into a column called " index " in the df2 data frame . Then ` df2 [ ' A '] .ix [ 0 ]` will return ' a ' . If you want to remove the old 10-based indices , you can insert the flag ` drop=True ` into the parenthesis of the reset_index function .
I think it is ` df [ ' A '] .iget ( 0 )` because ` df [ ' A ']` is a ` Series ` , which has no ` irow ` .

Say I can read the file and concat all of them together into one DataFrame . Does the DataFrame have to reside in memory ? With SAS , I can work with datasets of any size as long as I have the hard-drive space . Is it the same with DataFrames ? I get the impression they are constrained by RAM and not hard-drive space . Sorry for the noob question and thanks for you help . I'm enjoying your book .
what if the values are strings or categorical - i am getting the error : incompatible categories in categorical concat

Pandas join / merge / concat two dataframes
should I be able to join it with y on index with a simple join command where y = x except colnames have +2 . #CODE
I expect the final to have 1941 non-values for both . I tried merge as well but I have the same issue .
If you are having issues with join , read Wes's answer below . I had one time stamp that was duplicated .
It sounds like maybe you want ` pandas.concat ` ? ` merge ` and ` join ` do , well , joins , which means they will give you something based around the Cartesian product of the two inputs , but it sounds like you just want to paste them together into one big table .
Edit : did you try concat with ` axis=1 ` ? It seems to do what you're asking for : #CODE
here is what concat looks like :
See my edited answer . Perhaps ` concat ([ x , y ] , axis=1 )` ?

Pandas : pivot a dataframe

concat pandas DataFrame along timeseries indexes
I have two largish ( snippets provided ) pandas DateFrames with unequal dates as indexes that I wish to concat into one : #CODE
Not sure what your ` concat ` line will do
Try to join on outer .

I often need to apply a function to the groups of a very large ` DataFrame ` ( of mixed data types ) and would like to take advantage of multiple cores .
I do something like that but using UWSGI , Flask and preforking : I load the pandas dataframe into a process , fork it x times ( making it a shared memory object ) and then call those processes from another python process where I concat the results . atm I use JSON as a communication process , but this is coming ( yet highly experimental still ): #URL

pandas concat ( ' outer ') not doing union ?
It looks pandas.concat is doing ' left outer ' join instead of just union the indexes . Seems a bug to me but maybe I'm missing something obvious . #CODE

How to resample a python pandas TimeSeries containing dytpe Decimal values ?
I'm having a pandas Series object filled with decimal numbers of dtype Decimal . I'd like to use the new pandas 0.8 function to resample the decimal time series like this : #CODE
When trying this i get an " GroupByError : No numeric types to aggregate " error . I assume the problem is that np.mean is used internaly to resample the values and np.mean expects floats instead of Decimals .
Thanks to the help of this forum i managed to solve a similar question using groupBy and the apply function but i would love to also use the cool resample function .
I found the answer by myself . It is possible to provide a function to the ' how ' argument of resample : #CODE

Currently i'm using string replace which i consider to be a significant perfomance penalty . The coding i'm using is this : #CODE

Unexpected result when upsampling hourly values using the pandas resample function
I try to upsample daily TimeSeries values using the pandas resample function . When i'm upsampling a single day ( 2012-01-01 ) i expect the result to be the mean of the day considered for upsampling . The result should look like this : #CODE
Is this a bug or a feature ? If it is a feature how can i set the resample arguments to achieve my goal ?

The method above works , but it is incredibly slow in the loop section . Is there a more efficient way for me to do this in pandas ? Given the " circular " dependency ( not really circular given the lags ) , I'm not sure how I could do either regular series math or use normal shift operations ( e.g as I do with ` Cash Return `) .

Pandas DataFrame Apply
A combination of boolean indexing and apply can do the trick .
Quite neat . However , I think that you can get away with ` .max ( axis=1 )` instead of ` apply ( ... )` .
` max() ` is ok too of course , i think i got biased towards ` apply ` by the way you asked the question :-)

Simplest way is probably ` list ( dt.T.itertuples() )` ( where ` dt ` is your dataframe ) . This generates a list of tuples .

The problem in your code is that you want to apply the operation on every row . The way you've written it though takes the whole ' bar ' and ' foo ' columns , converts them to strings and gives you back one big string . You can write it like : #CODE

Most operations in ` pandas ` can be accomplished with operator chaining ( ` groupby ` , ` aggregate ` , ` apply ` , etc ) , but the only way I've found to filter rows is via normal bracket indexing #CODE
If you would like to apply all of the common boolean masks as well as a general purpose mask you can chuck the following in a file and then simply assign them all as follows : #CODE
But I found that , if you wrap each condition in ` ( ... == True )` and join the criteria with a pipe , the criteria are combined in an OR condition , satisfied whenever either of them is true : #CODE

pandas : stacking DataFrames generated by apply

1 ) " join " the results back to the initial DataFrame
1 ) join this back to the original DataFrame ` df `

and than apply it by passing the function and the args to ` agg ` : #CODE

Excel considers 1900 a leap year , so be careful with exactly what you want to translate :

Minor bug : my_colors = [ cycle ([ ' b ' , ' r ' , ' g ' , ' y ' , ' k ']) .next() for i in range ( len ( df ))] will give ' b ' every time in python 2.7 . You should use list ( islice ( cycle ([ ' b ' , ' r ' , ' g ' , ' y ' , ' k ']) , None , len ( df ))) instead .
Yup . it = cycle ([ ' b ' , ' r ' , ' g ' , ' y ' , ' k ']) ; my_colors =[ next ( it ) for i in xrange ( len ( df ))] would cut it as well ...

I have a relatively simple python multiprocessing script that sets up a pool of workers that append output to a pandas dataframe by way of a custom manager . What I am finding is when I call close() / join() on the pool , not all the tasks submitted by apply_async are being completed .
The correct solution is to wait use the results objects to get the results and then append all of them in the main thread .

THANK YOU ! Where has ` unstack ` been hiding ???

Filtering and selecting from pivot tables made with python pandas
Is there a way to do this directly within the pivot table structure , or do I need to convert this back in to a panda data frame ?
Pivot table returns a DataFrame so you can simply filter by doing : #CODE
Thanks for your feedback . Is there a way to get a list of values in a pivot table column by specifying the header ? I can do this on the dataframe with ' df [ ' A '] .values ' but I'm struggling to obtain something similar from the pivot table
the result of the pivot table is a DataFrame . So you can simply do ` pivoted.bar.values `

How to keep index when using pandas merge
I would like to merge two data frames , and keep the index from the first frame as the index on the merged dataset . However , when I do the merge , the resulting DataFrame has integer index . How can I specify that I want to keep the index from the left data frame ? #CODE
For this particular case , those are equivalent . But for many merge operations , the resulting frame has not the same number of rows than of the original ` a ` frame . reset_index moves the index to a regular column and set_index from this column after merge also takes care when rows of a are duplicated / removed due to the merge operation .

I then attempt to extract the relevant subset of the large DataFrame by passing the list of TimeStamps to ` .ix [ ]` but it always seems to return an empty ` DataFrame ` . I can loop over the list of TimeStamps and get the relevant rows from the ` DataFrame ` but this is a lengthy process and I thought that ` ix [ ]` should accept a list of values according to the docs ?

Ok , I can create an empty dict , insert values and create a DataFrame .
( here using dictionary comprehensions , available in Python 2.7 ) . Just for completeness , though , you could -- inefficiently -- use ` join ` or ` concat ` to get a column-by-column approach to work : #CODE

As a follow up to this post , I would like to concatenate a number of columns based on their index but I am encountering some problems . In this example I get an Attribute error related to the map function . Help around this error would be appreciated as would code that does the equivalent concatenation of columns . #CODE
note that support for ` filter ( None , iterable )` ceased in Python 3 , need to do ` filter ( bool , iterable )` there

I'm trying to unstack a dataframe perform operations on it ( over time only ) and then stack it back together like this : #CODE
The problem is that multiindex is getting reversed after the operation , is there any easy way to make this work ? Perhaps I'm misusing the stack from the start ?
` stack ` and ` unstack ` add level ( s ) to the end of the MultiIndex , this is not controllable . You can change the order of the levels in a MultiIndex with ` reorder_levels() ` : ` stacked.reorder_levels ([ 2 , 1 , 0 ])` will give you the same MultiIndex levels order as in ` df `

As noted below , pandas now uses SQLAlchemy to both read from ( read_sql ) and insert into ( to_sql ) a database . The following should work #CODE

How do I really use the ` ix ` method of a pandas DataFrame ?
Having read the docs one the ` ix ` method of DataFrames , I'm a bit confused by the following behavior with my MultiIndexed DataFrame ( specifying select columns of the index ) . #CODE
It took me a long time to realize that what I had been trying to achieve with ` In [ 61 ]` was possible with ` In [ 60 ]` . Why does the ` ix ` method behave like this ? What I'm really trying to get at is the scenario at ` In [ 62 ]` .

We can join these strings with the regex ' or ' character ` | ` and pass the string to ` str.contains ` to filter the DataFrame : #CODE

You mention in your question that the red line is the mean - it is actually the median .
with a line at the median . The whiskers extend from the box to show

I have input data in a flattened file . I want to normalize this data , by splitting it into tables . Can I do that neatly with ` pandas ` - that is , by reading the flattened data into a ` DataFrame ` instance , and then applying some functions to obtain the resulting ` DataFrame ` instances ?

In case of fixed width file , no need to do anything special to strip white space , or handle missing fields . Below a small example of a fixed width file , three columns each of width 5 . There is trailing and leading white space + missing data . #CODE

Python pandas equivalent for replace
In R , there is a rather useful ` replace ` function .
` replace ( df$column , df$column == 1 , ' Type 1 ') ; `
Should I use a lambda with ` apply ` ? ( If so , how do I get a reference to the given column , as opposed to a whole row ) .
` pandas ` has a ` replace ` method too : #CODE
That is indeed true . iPython is a thing of beauty . In my defence , the replace function is not listed [ here ] ( #URL )

After this , I think I should insert a column in the dataframe that labels all my data with Monday through Friday -- for all the dates in the file ( there are 6 years of data ) . The reason for labeling M-F is so that I can sort each number associated to the day of the week in ascending order . And query on the day of the week .

Almost . So , in sum , I need to create another data frame which contains the rank / position of the row . And then , I need to join these .

Does there exist in pandas methods to perform a similar array calculation to obtain a datetime ( 64 ) date / time stamp with which I can replace the current sequential dataframe index ?

python pandas : apply a function with arguments to a series
I want to apply a function with arguments to a series in python pandas : #CODE
The documentation describes support for an apply method , but it doesn't accept any arguments . Is there a different method that accepts arguments ? Alternatively , am I missing a simple workaround ?
The documentation explains this clearly . The apply method accept a python function which should have a single parameter . If you want to pass more parameters you should use ` functools.partial ` as suggested by Joel Cornett in his comment .
For a DataFrame apply method accepts ` args ` argument , which is a tuple holding additional positional arguments or ** kwds for named ones . I created an issue to have this also for Series.apply() #URL

@USER : I notice that ` np.random.permutation ` would strip the column names from the DataFrame , because ` np.random.permutation ` . Is there a method in pandas that would shuffle the dataframe while retaining the column names ?

pandas merge timeseries , concat / append / ... ?
I start out with a timeseries and use a loop to produce new timeseries . I would like to merge the existing series with the new ones subsequently in every loop , while preserving their ( different ) indices . I tried concat , but somehow I cannot add another series after the first one ... #CODE
Hi eurmiro , thanks for your quick reply ! Unfortunately it is a bit more complicated than that ... I am doing something with the objects from the list ( regressions ) and use some of the output to create the timeseries ... so I really need to append the time series after every loop ...
I do something like this all the time but I use ` append ` like this : #CODE

I want to apply a groupby operation that computes cap-weighted average return across everything , per each date in the " yearmonth " column .
This still requires me to save out the groupby computation , rather than having the assignment directly on the LHS on the line where I perform the groupby operation . Apply might be a bit better than the loop in my hack at the bottom of the question , but they are basically the same idea .
Join can do this , but you will need to rename the added column . In this case A_r is new_col .
The join example at the bottom does work , but it's not presented clearly . If you feel like deleting the first part of the answer and making the latter part a little more clear , I will upvote in addition to accepting .
While I'm still exploring all of the incredibly smart ways that ` apply ` concatenates the pieces it's given , here's another way to add a new column in the parent after a groupby operation . #CODE
My understanding was that transform produces an object that looks like the one it was passed . So if you transform a DataFrame , you don't just get back a column , you get back a DataFrame . Whereas in my case , I want to append a new result to the original data frame . Or are you saying that I should write a separate function that takes a data frame , computes the new column , and appends the new column , and * then * transform with that function ?

Very weird bug here : I'm using pandas to merge several dataframes . As part of the merge , I have to call reset_index several times . But when I do , it crashes unexpectedly on the second or third use of reset_index .
Inspecting frame.py , it looks like pandas tries to insert a column ' index ' or ' level_0 ' . If either / both ( ?? ) of them are already taken , then it throws the error .
Fortunately , there's a " drop " option . AFAICT , this drops an existing index with the same name and replaces it with the new , reset index . This might get you in trouble if you have a column named " index , " but I think otherwise you're okay .

And now I want to replace the element of df_a by element of df_b which have the same ( index , column ) coordinate , and attach df_b's elements whose ( index , column ) coordinate beyond the scope of df_a . Just like add a patch ' df_b ' to ' df_a ' : #CODE

Is there an efficient way to apply this disaggregation map to get a new dataframe at a State level ?

Thank you for the tips . It is simple and elegant . And if I want to filter the Q1 , Q3 , Q4 together , which is " NOT endswith ( ' 0630 ')" , how to add the ' NOT ' to the command of " df [ df.index.map ( lambda x : x [ 1 ] .endswith ( " 0630 "))] " ?

I have tried with different methods , but couldn't sort this thing out the proper way . I think the first thing I should do should be to ' unstack ' the values in the csv , in order to have an aligned index first , and then create a DataFrame , but really don't how ...

There might be a slick vectorized way to do this , but I'd just apply the obvious per-entry function to the values and get on with my day : #CODE

If you would use the timestamps of the events as index of the series instead of the data , resample can do this . In the example below , the index of series s are the timestamps and data is the event_id , basically the index of your series . #CODE
resample ( this method can also be used on a DataFrame ) will give a new series with in this case 15min periods , the end time of a bucket ( period ) is used to refer to it ( you can control this with the label arg ) . #CODE
Definitely pay attention to the ` closed ` and ` label ` options to ` resample ` !

@USER . : I don't know enough about how pandas's internals work to know exactly why it works . However , it says [ here ] ( #URL ) that setting works with ix . The basic issue is that sometimes indexing into a DataFrame returns a copy of the result , and sometimes it returns a view on the original object . According to documentation on that page , this behavior depends on the underlying numpy behavior . I've found that accessing everything in one operation ( rather than ` [ one ] [ two ]`) is more likely to work for setting .
I didn't realize @USER B . was asking about ` ix ` in general . You can find documentation of that ( with the ` [ row , col ]` syntax ) in the pandas docs . What I don't know the internals of is why setting the elements works with this syntax and not some others .
The section after ' Assignment / setting values is possible when using ix : ' will explain exactly what you need ! Turns out ` df.ix ` can be used for cool slicing / dicing of a dataframe . And . It can also be used to set things . #CODE

KDB+ like asof join for timeseries data in pandas ?
kdb+ has an aj function that is usually used to join tables along time columns .
I see that pandas has an asof function but that is not defined on the DataFrame , only on the Series object . I guess one could loop through each of the Series and align them one by one , but I am wondering if there is a better way ?
this is also called * rolling join *
It could be easily ( well , for someone who is familiar with the code ) extended to be a " left join " mimicking KDB . I realize in this case that forward-filling the trade data is not appropriate ; just illustrating the function .

I was getting ` ValueError : Index contains duplicate entries , cannot reshape ` when doing ` unstack ` on a MultIndex but this solution works for that only I had to do ` df_unique = df.groupby ( level =[ 0 , 1 ]) .first() `
This is wrong . drop_duplicates acts on the values only ( at least in my version ) . You need to reset_index if you want to drop on index and values or just work with the index if you want to have a unique index . Maybe there is another way besides groupby to enforce unique index ?

Add parse_dates=True , otherwise your index will be plain strings and resample does not like that . df = pd.read_csv ( ' test_EURUSD / EURUSD-2012-07.csv ' , names =[ ' Symbol ' , ' Date_Time ' , ' Bid ' , ' Ask '] , index_col=1 , parse_dates=True )
Date = range ( len ( df2 ))
Volume = np.zeros ( len ( df2 ))

EDIT : I have just read the help on ` pandas.Series.diff() ` , but still I'd like to " replace " the subtraction used on diff by another function , say ` euclidean_distance() ' . Is there a way to do that ?

I would suggest you just write a function to do what you're saying probably using ` drop ` ( to delete columns ) and ` insert ` to insert columns at a position . There isn't an existing API function to do what you're describing .

I have a Pandas dataframe ' dt = myfunc() ' , and copy the screen output from IDLE as below : #CODE

The function to apply is like : #CODE

I would use transpose and the sort method ( which works on columns ): #CODE

Duplicate entries for index in pandas pivot function
My goal is to have data grouped by date , mat and strike ( I can drop the ' 3m ' and ' dataframe name ' columns since they're common to all data ) .
Can anyone help me with this issue , or propose an alternative approach to the pivot function ?
Maybe ` set_index ` is what you want ? ` pivot ` is a reshape operation : #CODE

Now for the strides . If you want to consider ` N ` consecutive points , you'll use #CODE

Basic problem : how do I map the function to the column , specifically where I would like to reference more than one other column or the whole row or whatever ?
Then you can use map : #CODE
I don't suppose you have nny ideas on the second part , viz referencing neighbouring rows in the dataframe from within the map / apply function ?
The exact code will vary for each of the columns you want to do , but it's likely you'll want to use the ` map ` and ` apply ` functions . In some cases you can just compute using the existing columns directly , since the columns are Pandas Series objects , which also work as Numpy arrays , which automatically work element-wise for usual mathematical operations . #CODE
If you need to use operations like max and min within a row , you can use ` apply ` with ` axis=1 ` to apply any function you like to each row . Here's an example that computes ` min ( A , B ) -C ` , which seems to be like your " lower wick " : #CODE
For the second part of your question , you can also use ` shift ` , for example : #CODE
For the second part , I would recommend introducing a column indicating the pattern for each row and writing a family of functions which deal with each pattern . Then groupby the pattern and apply the appropriate function to each group .

Note that as suggested in a comment by @USER , you should use ` np.eye ( len ( x ))` instead of ` np.diag ([ 1 ] *len ( x ))` . The ` np.eye ` function directly gives you a 2D array with 1 on the diagonal and 0 elsewhere .

Note that a simple ` apply ` will not work here , since it won't know how to make sense of the possibly differently-sized result arrays for each group .
can you not use ` map ` ?
What problems are you running into with ` apply ` ? It works for this toy example here and the group lengths are different : #CODE

Python Pandas : How to broadcast an operation using apply without writing a secondary function
It seems logical to use the ` apply ` function for this , but it doesn't work like expected . It does not even seem to be consistent with other uses of ` apply ` . See below . #CODE
Based on this , it appears that ` apply ` does nothing but perform the NumPy equivalent of whatever is called inside . That is , ` apply ` seems to execute the same thing as ` arr + " cat "` in the first example . And if NumPy happens to broadcast that , then it will work . If not , then it won't .
But this seems to break from what ` apply ` promises in the docs . Below is the quotation for what pandas.Series.apply should expect :
Is there some way of using ` apply ` that I am missing here ?
and I verified that this version does work with Pandas ` apply ` . But this is beside the point . It would be easier to write something that operated externally on top of a Series object than to have to constantly write wrappers that use list comprehensions to effectively loop over the contents of the Series . Isn't this specifically what ` apply ` is supposed to abstract away from the user ?
and use this in ` apply ` : #CODE
This works , but I consider it a workaround as well , since it doesn't address the fact that ` apply ` isn't working as promised . Can you verify that ` map ` will work in all the same situations where ` apply ` will work ? I also don't like the inconsistency in going from ` map ` for a Series to ` applymap ` for a DataFrame .
That contradicts the docs for ` apply ` , as well as its 0.8.1 behavior , in which it successfully performs the elementwise version of my example above , whereas version 0.7.3 seems to use the logic you describe . Since ` apply ` should work in 0.7.3 as it does in 0.8.1 ( according to the docs ) , that's why I think it's a workaround . ` map ` is fine , but ` apply ` should work .
I'm on github master and it does not work ; it probably worked in 0.8.1 by accident . ` apply ` is designed so that you can apply a ufunc and get back a Series with the index intact . Take a look at the source code , it tries to call func ( self ) and wraps that in a try / except block and then calls map_infer in the except . In your example , the function you gave * can * take a Series and return a Series but doesn't do element-wise operations so the code cannot know to trigger the element-wise case . To be explicit that you want the input function to be applied element-wise , you have to use ` Series.map ` .
Though I do agree with you the docstring for apply is very unclear about this aspect . We can improve the documentation for apply .
In fact , by saying that ` apply ` can take any function that expect a * single * argument , it's not just unclear , but plain misleading . I'm glad you confirmed by hunch about that try-except block . So to be clear , we should use ` apply ` whenever we have a vectorized / ufunc already , and ` map ` when we literally want to apply an elementwise operation to a series ?
Yup , that's exactly right on ` apply ` vs ` map ` .

Stepping the trace in the case of date column , shows that matplotlib tries to do x [ 0 ] on the dates to retrieve tz info , which throws a KeyError . This is not done on y column . Pandas has location based indexing tools , but they are not used by matplotlib internals .

Sorry for not answering your questions . I've found a solution . No i wasn't able to use .month nor .year . I decided to change date to first day of each month using a lamdba function that calls replace ( day=1 ) . Bad thing here is that i've to alter the database .

The inner syntax ` ( df ! =0 ) .any() ` doesn't work . A DataFrame object doesn't have the ` any ` function , at least not in 0.7.3 . You'd have to map that to the columns using ` map ` or ` apply ` or something .

@USER : To avoid the error , replace ` int ( x )` with the expression ` int ( text ) if x.isdigit() else x ` .

Now I want to merge the data frame with the series , such that the values from the series are broadcasted along the second level index . The resulting data frame should look like this : #CODE

How to groupby the first level index and apply function to the second index in Pandas
And I want to apply a function ` func ` ( exp : `' lambda x : x*10 '`) to ` second ` , somewhat like : #CODE
This way , the index column is not dropped and still accessible for your ` apply ` .

I have a dataframe with repeat values in column A . I want to drop duplicates , keeping the row with the highest value in column B .
Wes has added some nice functionality to drop duplicates : #URL . But AFAICT , it's designed for exact duplicates , so there's no mention of criteria for selecting which rows get kept .

PS : but if you really just want the last column , ` apply ` would suffice : #CODE

@USER It seems to work for me , with your example ( without the ` skiprows `) ... perhaps you need to ` myData.T ` ( transpose ) .

So to try and generalize my question , how can I get ` df1 * df2 ` using ` map ` to define the columns to multiply together ? #CODE
Assuming the index is already aligned , you probably just want to align the columns in both DataFrame in the right order and divide the ` .values ` of both DataFrames .
Suppose we want to multiply several columns with other serveral columns in the same dataframe and append these results into the original dataframe .

( I think it can be some problem with ` lambda ` When I want to apply my function to the column I have an error : ` TypeError : only length-1 arrays can be converted to Python scalars `)
On top of a dodgy converter , i think you apply the converter to the wrong column ( look at the exception you get ) .

Normalize data in pandas
Is there a way to do this if you want to normalize a subset ? Say that row ` A ` and ` B ` are part of a larger grouping factor that you want to normalize separately from ` C ` and ` D ` .
You can use ` apply ` for this , and it's a bit neater : #CODE

If you want that done on every row in the dataframe , you can use apply ( with axis=1 to select rows instead of columns ): #CODE

I tried with various attempts of ' unstack ' , ' groupby ' and ' pivot ' but with no success . I could only reach my objective byusing a lot of python vector manipulation , but this was a slow and inefficient procedure . Is there any specific , more efficient pandas procedure in order to get the same result ? I'm getting lost at this ...

At the moment for conversion I use as below , but need remove unwanted rows first to apply it to all df . #CODE

I would like to add a new column , ' e ' , to the existing data frame and do not change anything in the data frame . ( The series always got the same length as a dataframe . ) I tried different versions of ` join ` , ` append ` , ` merge ` , but I did not get it as what I want , only errors at the most .
Note my original ( very old ) suggestion was to use ` map ` ( which is much slower ): #CODE
@USER if you already have ` e ` as a Series then you don't need to use ` map ` , use ` df [ ' e '] =e ` ( @USER answer ) .
this will effectively be a left join on the df1.index . So if you want to have an outer join effect , my probably imperfect solution is to create a dataframe with index values covering the universe of your data , and then use the code above . For example , #CODE
This worked fine to insert the column at the end . I don't know if it is the most efficient , but I don't like warning messages . I think there is a better solution , but I can't find it , and I think it depends on some aspect of the index .

Then , since you extend the base class , you have to replace the methods with a suitable descriptor : #CODE

Resample hourly TimeSeries with certain starting hour
I want to resample a TimeSeries in daily ( exactly 24 hours ) frequence starting at a certain hour .
Some weeks ago you could pass `' 24H '` to the ` freq ` argument and it worked totally fine .
Resample has an ` base ` argument which covers this case : #CODE

is there an existing built-in way to apply two different aggregating functions to the same column , without having to call ` agg ` multiple times ?
N / M I didn't see the extra call to ` returns ` in there . So this is the Series version of aggregate ? I'm looking to do the DataFrame version of aggregate , and I want to apply several different aggregations to each column all at once .

I would just use the transpose of the array because that's much faster , but then I would have a ` MultiIndex ` on the columns , and I haven't yet found any documentation in Pandas showing how to use ` MultiIndex ` s as columns .

An alternative slightly more flexible way , might be to use ` apply ` ( or equivalently ` map ` ) to do this : #CODE
Thanks for the answer , but it is more complicated than that : sometimes the values are something else entirely ( like characters ) . I think it wight be simpler to completely drop this column , and then add a new one with the year , or completely replace the values by the year .

First , I think you have to either specify named parameters or use ` args ` to pass additional arguments to ` apply ` . Your second argument is probably being interpreted as an axis . But if you use #CODE
because ` apply ` doesn't act elementwise , it acts on entire Series objects . Other approaches include using ` applymap ` or boolean indexing , i.e. #CODE

Is there a grep like built-in function in Pandas to drop a row if it has some string or value ?
Below example will drop all rows where column A holds ' a ' character and ' B ' equals 20 . #CODE

@USER : to drop the unmatched condition .
Generally , I find myself using boolean indexing and the tilde operator when obtaining the inverse of a selection , rather than df.drop() , though the same concept applies to df.drop when boolean indexing is used to form the array of labels to drop . Hope that helps . #CODE

One way to do this is to use apply : #CODE
If you want to change the values in only one column you can still use ` apply ` : #CODE
Note : since ` my_fun2 ` returns a single value , this time ` apply ` return a Series , so we need to slightly change the way we apply apply .

median median
For things like sum , mean , median , max , min , first , last , std , you can call the method directly and not have to worry about the apply-to-DataFrame-but-failover-to-each-column mechanism in the GroupBy engine .

pandas has no attribute ' Timestamp ' , nor does datetime ... ( what is ` pd ` and what is ` dt `) ?
>>> import datetime as dt
Hello , thank you for your answer . I made a mistake when I wrote the question . I replace 2 by 1 in the isocalendar . the propriety week of TimeStamp is very strange . it gives 1 for the seven first day of 2009 ... I don't understand or I don't know exactly the meaning of timestamp.week

Looks like this is going to work . Thanks for your help ! In general , though , is there a prefered approach to Split-Apply-Combine where Apply returns a dataframe of arbitrary size ( but consistent for all chunks ) , and Combine just vstacks the returned DFs ?

Use ` join ` : #CODE
I do not want to join them )

Use df = df.append ( dfin , ignore_index=True ) . But even with this change i think this will not give what you need . Append extends a frame on axis=1 ( columns ) , but i believe you want to combine the data on axis=0 ( rows )

Drop non-numeric columns from a pandas DataFrame
I would like to drop all non-numeric columns in one fell swoop , without knowing their names or indices , since this could be doable reading their dtype . Is this possible with pandas or do I have to cook up something on my own ?

pyPandas : mess with join / append / concat two dataframes
I have two dataframes : one with 12 cols and the other with 9 , both of them have 624 rows . I would like to join them side by side resulting in a 21 cols dataframe with the same 624 number of rows . I want to preserve the rows order as is . Observe that both dataframes are aligned in descending order of the column ' Name ' and the column ' L1 ' . I have tried several things join them by axis=1 ignoring index or not . All that I have is a dataframe with rows doubled and a bunch of NANs . I also tried concat and append , but with no success .
One alternative is to merge on ' Name ' and ' L1 ' : #CODE
Another is to call DataFrame.reset_index first before you call merge : #CODE
Thanks for helping . I had realized that reset would work , however , why to reset index to concat dfs ignoring them ?
I don't think so . At least in ` concat ` , you have to declare axis .

Pandas DataFrame : apply function to all columns
Is there a more pythonic way to apply a function to all columns or the entire frame ( without a loop ) ?

Pandas transpose concat()
How to transpose a DataFrame returned by concat() ? #CODE

I have been attempting to use the merge , concat and join functions to no avail . I feel like I am missing something crucial as in an SQL database I can simply perform a union all query ( which is quite slow admittedly ) to solve this issue .
You can use the keys argument for concat , this will result in a MultiIndex and will allow you to uniquely select data : concat ( pieces , keys =[ ' left ' , ' middle ' , ' right '] . I would also set ( id1 , ..., id6 ) as index , this will make it less verbose to select data .

Thanks @USER . Using your guidance , I was also able to implement the daily average I mentioned : sr.groupby ( sr.index.month ) .transform ( lambda x : x.fillna ( method= ' backfill ') / len ( x ))
I don't think you need the groupby here if you use resample , there is a ` fill_method ` parameter in ` resample ` that just fills the time bin .
@USER She - I was able to use s.resample ( ' D ' , fill_method= ' backfill ') to fill in data , however , I couldn't figure out how to use resample to get data starting from 2012-01-01 as opposed to 2012-01-31 .

I can understand why this might happen , as the weekly dates don't exactly align with the monthly dates , and weeks can overlap months . However , I would like to implement some simple rules to handle this anyway . In particular , ( 1 ) set the last week ending in the month to the monthly value , ( 2 ) set the first week ending in the month to the monthly value , or ( 3 ) set all the weeks ending in the month to the monthly value . What might be an approach to accomplish that ? I can imagine wanting to extend this to bi-weekly data as well .
Thanks for creating the github issue . As for the resample example given , unfortunately it doesn't return quite what I was looking for . Switching from ` ffill ` to ` bfill ` gets it closer in terms of what cells are filled with what value , but it still puts an observation in April and has no observations in Janaury ( see my edit on the original post for an example of the date range I'm most interested in ) . Interestingly , upsampling and then downsampling back to monthly results in a Feb-Apr series instead of the original Jan-Mar .

I believe you can use the append #CODE

Python Pandas : pivot table with aggfunc = count unique distinct
How do I get a Pivot Table with counts of unique values of one DataFrame column for two other columns ?
NB . I am aware of ' Series ' ` values_counts() ` however I need a pivot table .
Note that using ` len ` assumes you don't have ` NA ` s in your DataFrame . You can do ` x.value_counts() .count() ` or ` len ( x.dropna() .unique() )` otherwise .
You can construct a pivot table for each distinct value of ` X ` . In this case , #CODE
will construct a pivot table for each value of ` X ` . You may want to index ` ptable ` using the ` xvalue ` . With this code , I get ( for ` X1 `) #CODE

You could also do an inner join on stations.id :
the merge complains that there's ' no item named start_station_id ' . This approach is one of the ones I'd been trying to get work and was bumping into this problem a lot . Not quite sure what's going on here ...
It was my bad : " on " is only to be used when the columns occur in both DataFrames ( so my code was referring to a join on both id and start_station_id which is wrong here ) . Here you have to use " left_on " and " right_on " . For the reindex : non-unique indices are rather new in pandas . It could be that this isn't supported . Try df.ix [ ... ] instead of df.reindex which doesn't throw this error .

Can you load 2 separate data frames and do join / groupby on the datetime ?
Using the same basic loop as above , just append the set of every forth row starting at 0 to 3 after you run your code above . #CODE
If it is just formatting and the time is recorded to a precision that all entries are unique , then stack and merge would do it . Let me know and I'll post some code .
Here is a solution based on numpy's repeat and array indexing to build de-stacked values , and pandas ' merge to output the concatenated result .
Then build a de-stacked vector of TDRs and merge it with the original data frame #CODE

I have two time series I need to join :
How do I join these time series together in pandas as to express adjusted prices in expression #CODE
i had figured out that i need to pivot the first point-in-time timeseries for tickers go into the columns and date into rows and for the second timeseries expand the interval into daily granularity and also pivot it ( through dataframe.pivot function . by combining the two dataframes one can write function i need .
You can simply join the dataFrame with your daily bar and use fillna ( method= " ffill ") to forward fill the previous value . in your example you have adjustment factors for a range . #CODE

I was hoping to get this to work but a pyTable table where does not provide a len

If there are no blanks some columns convert to ` TRUE / FALSE ` , others leave as ` Yes / No ` but dtype is bool . Any idea why ?
` fhs = fhs.drop ([ 1002 ])` to drop that row and data types are still good .
first column comes into df as Yes , No , Yes , Yes type bool xxxx below
3rd column comes into df as FALSE , FALSE , TRUE , TRUE type bool
first column comes into df as Yes , No , Yes , Yes type bool xxxx below
3rd column comes into df as FALSE , FALSE , TRUE , TRUE type bool

I have found a workaround which is listed at the end of this post , but its not at all ' panda-style ' and prone to errors . The apply or transform function on a group seems like the right way to go but after hours of trying i still do not succeed . I figured the correct way should be something like : #CODE
Having the apply / transform mechanism be able to output structured values and those broadcast into colums ( i.e. if a tuple is produced by the applied function , the components go in separate columns instead of the tuple becoming an atomic element in a single column ) would be a fantastic feature , even if it is only syntactic sugar . Probably with another method name , to make intent clear ( applyfork or something like that , or a keyword splitseq=True in apply ) .

Ah I think that could be the issue . How can I join the two columns with the same label together and sum up their numbers according to the respective row index ?

Welcome to Stack Overflow ! We encourage you to [ research your questions ] ( #URL ) . If you've [ tried something already ] ( #URL ) , please add it to the question - if not , research and attempt your question first , and then come back .

After implementing a custom frequency in pandas by subclassing ` DateOffset ` , is it possible to " register " an offset alias for that frequency so that the alias can be used in built-in pandas functions such as ` date_range ` and ` resample ` ?

I have a basic ( almost naive ) question for plotting on top of a pandas df . Given the df below I am trying to do a stack bar plot for ' stats_value ' and ' read1_length ' v / s ' lib_name ' . #CODE

Basically , I want to mimic R's melt / cast without getting into hierarchical indexing or stacked dataframes . I can't seem to to get exactly the above playing with stack / unstack , melt , or pivot / pivot_table -- Is there a good way to do this ?
There is a ` melt ` in ` pandas.core.reshape ` : #CODE
The columns end up being a MultiIndex , but if that's a deal breaker for you just concat the names and make it a regular Index .
How might you concat the names , that is where I was confused ( " Basically , I want to mimic R's melt / cast without getting into hierarchical indexing or stacked dataframes " -- so was already aware of this , what I am confused on is how to get this into a flat structure with concatenated column names .

Following the official docs you can use loc #CODE

I have multiple ( more than 2 ) dataframes I would like to merge . They all share the same value column : #CODE
I read that join can handle multiple dataframes , however I get : #CODE
I have tried passing ` rsuffix =[ " %i " % ( i ) for i in range ( len ( data ))]` to join and still get the same error . I can workaround this by building my ` data ` list in a way where the column names don't overlap , but maybe there is a better way ?
I am interested to see if the experts have a more algorithmic approach to merge a list of data frames .
OK , I am a little slow :) . I would try ` pandas.merge ` instead of ` join ` . This should get you through the night , @USER , but I'm interested if the pros have a cleaner solution .
Of course , this is very manual . The ` pandas.concat() ` solution is _much_ better -- I thought ` concat ` gave the duplicated column name error when ` axis=1 ` , but I have a lot to learn . :)

Forcing dates to conform to a given frequency in pandas

Calculate diff of a numpy array using custom function instead of subtraction
What I want to do is to calculate the geographic distances between rows ( with the special condition that the first element is always zero , at the starting point ) . This would give me either a list of distances with ` len ( distances ) == coord_array.shape [ 1 ]` , or maybe a third column in the same array .
It is important to say that I already have a function that returns a distance between two points ( two coordinate pairs ) , but I don't know how to apply it with a single array operation instead of looping through row pairs .

How to shift a pandas MultiIndex Series ?
In a regular time series you can shift it back or forward in time .
We can shift it with : #CODE

These values are median values I calculated from elsewhere , and I have also their variance and standard deviation ( and standard error , too ) . I would like to plot the results as a bar plot with the proper error bars , but specifying more than one error value to ` yerr ` yields an exception : #CODE

but not sure how to proceed from here , and how to join the new column back to the original data frame . It would be very helpful if anyone could give me any pointer .

Based on the values in two columns merge values in other columns
One way I am thinking of is to use nested loops : outer loop read the lines sequentially and the inner loop reads all lines from the begining and look for map . However this process seems to be computational intensive .

You can compress to the zlib format instead using ` zlib.compress ` or ` zlib.compressobj ` , and then strip the zlib header and trailer and add a gzip header and trailer , since both the zlib and gzip formats use the same compressed data format . This will give you data in the gzip format . The zlib header is fixed at two bytes and the trailer at four bytes , so those are easy to strip . Then you can prepend a basic gzip header of ten bytes : `" \x1f\x 8b \x0 8\ 0\0\0\0\0\0\xff "` ( C string format ) and append a four-byte CRC in little-endian order . The CRC can be computed using ` zlib.crc32 ` .

I thought that adding a column of row numbers ( ` df3 [ ' rownum '] = range ( df3.shape [ 0 ])`) would help me select out the bottom-most row for any value of the ` DatetimeIndex ` , but I am stuck on figuring out the ` group_by ` or ` pivot ` ( or ??? ) statements to make that work .
Unfortunately , I don't think Pandas allows one to drop dups off the indices . I would suggest the following : #CODE

If you DO want a copy , you can ( in general ) use the copy method or , ( in this case ) use truncate : #CODE

pandas : apply function to DataFrame that can return multiple rows
One possibility might be to allow ` DataFrame.applymap ` function return multiple rows ( akin ` apply ` method of ` GroupBy `) . However , I do not think it is possible in pandas now .

How I do find median using pandas on a dataset ?
I think that should work , maybe there's something in your data causing the error , like nan's , im just guessing . You could try applying your own median function to see if you can work around the cause of the error , something like : #CODE
Here is a different approach , you can add the median back to your original dataframe , the median for the metric column becomes : #CODE
Wether its useful to have the median of the group attached to each datapoint depends a bit what you want to do afterwards .

Resample to DatetimeIndex raises an error
I created a DatetimeIndex and I want to resample the data with that index . When I do that I get an exception : #CODE
Any idea how to resample by an index ?
AFAIK you cannot pass in a DatetimeIndex to resample . As a workaround , just resample by the freq alias ( ' 1Min ') and then reindex to your generated index ?
I started a github issue to maybe think about adding in additional parameters to resample . Feel free to leave more feedback there .
Wes replied saying he plans to extend ` resample ` like this eventually .

Thanks for reporting . Is there any work around I could apply before it is fixed ?

I've played around with groupby and transpose to no avail , any tips would be great appreciated .
I'm able to recreate the 2nd table by creating an index using df.map ( lamda x : x.startswith ( "") , then recreating a DataFrame with the series . I suppose I could loop through and append to the DataFrame , however I feel like there should be a much smarter method to doing this .
You can use the ` pivot ` function : #CODE

Could you upload an example of when it saves with the graph cut off ?

Merge Columns within a DataFrame that have the Same Name

How do I stack two DataFrames next to each other in Pandas ?
and I would like to stack them next two each other in a single DataFrame so I can access and compare columns ( e.g. High ) across stocks ( GOOG vs . AAPL ) ? What is the best way to do this in Pandas and access the subsequent columns ( e.g. GOOG's High column and AAPL's High column ) . Thanks !
Have a look at the ` join ` method of dataframes , use the ` lsuffix ` and ` rsuffix ` attributes to create new names for the joined columns . It works like this : #CODE

I notice Pandas can apply different function to different column by passing a dict . But I have a long column list and just want parameters to set or tip to simply tell Pandas to bypass some columns and apply ` my_func() ` to rest of columns ? ( Otherwise I have to build a long dict )
One simple ( and general ) approach is to create a view of the dataframe with the subset you are interested in ( or , stated for your case , a view with all columns except the ones you want to ignore ) , and then use APPLY for that view . #CODE
Apply your function to that view . ( Note this doesn't yet change anything in df . ) #CODE

Starting with row number 2 , or in this case , I guess it's 250 ( PS - is that the index ? ) , I want to calculate the difference between 2011-01-03 and 2011-01-04 , for every entry in this dataframe . I believe the appropriate way is to write a function that takes the current row , then figures out the previous row , and calculates the difference between them , the use the ` pandas ` ` apply ` function to update the dataframe with the value .
@USER She , what if I didnt want the difference but the actual value . i.e. instead of diff ( 1 ) is there something like value ( 1 ) or value ( 1:3 ) .mean() . The second made up example would get the mean of the next , second and third value . That would be useful

I created an issue here . To get the ultimate perf you'd want to drop down into C or Cython and build the raw byte string yourself using C string functions . Not very satisfying , I know . At some point we should build a better-performing to_csv for pandas , too :

How to apply condition on level of pandas.multiindex ?
I.e. , I would like to apply np.mean over all counts of the detectors of 1 channel at each time separately .

( after diving in Pandas doc , I think ` cut ` function can help me because it's a discretization problem ... but I'm don't understand how to use it )
to plot the results you can use the matplotlib function hist , but if you are working in pandas each Series has its own handle to the hist function , and you can give it the chosen binning : #CODE

In the specific case of applying a ` diff ` function that could be vectorized ( applied like an array operation instead of an iterative pairwise loop ) , is there a way to do that idiomatically in pandas ? Should I create a " coordinate " class which support the diff ( ` __sub__ `) operation so I could use ` dataframe.latlng.diff ` directly ?

Thank you @ root , that's very helpful ! As a follow-up question , how would you go about to apply the same function on groups ? ( See updated question ) . Thanks again !

But for a start I would just be happy to get the first result . I suspect that I need to use searchsort and asof , but I am not quite sure how to do that with . A MultiIndex .
You're looking for a near timestamp , where ` asof ` searches for the latest timestamp .
It is only applied to a time series , so you would have to apply ` reset_index ` to your ` DataFrame `

This can be accomplished quite simply with the DataFrame method ` apply ` . #CODE
Now that we have our ` DataFrame ` and ` Series ` we need a function to pass to ` apply ` . #CODE
` df.apply ` acts column-wise by default , but it can can also act row-wise by passing ` axis=1 ` as an argument to ` apply ` . #CODE
This could be done more concisely by defining the anonymous function inside ` apply ` #CODE

` x ` would be each subgroup of the groupby operation , which you can examine with ` grouped.groups ` . In case of a multicolumn groupby these subgroups refer to several columns , but this is irrelevant as ` len ` counts by the rows in pandas objects .

Most efficient way to shift MultiIndex time series
I would suggest you reshape the data and do a single shift versus the groupby approach : #CODE
You can verify it's been lagged by one period ( you want shift ( 1 ) instead of shift ( -1 )): #CODE
One difference is that since it drops the NaN entries generated by shift() on the stack() , it has fewer rows than the previous , but that's taken care of in the join() . ( And I did mean shift ( -1 ); it's a hazard rate calculation , so it's forward-looking . )

How do I resample / align a pandas timeseries to the closest calendar quarters ?
Also , is there any way I can get it to align to Dec / Mar ( which seem to be closer to the original dates ) with the timeseries functions ?
I know no easy solution to get to align to the closest and I find the current version quite logical . But with ` label= ' left '` you can achieve what you want with the current data , still it doesn't align to the closest , so overall you probably have to figure out something else ( like using apply to change the dates so they would conform as you wish ) . #CODE

After reading in don't you want the reverse ? ` lambda L : L.split ( ' , ')` - not join again ...
I rewrote the answer to remove the map function as it was more confusing than helpful . thank you for your answer

Using resample to align multiple timeseries in pandas
The goal is to align the data to calendar quarter markers so the 3 data sets can be compared . Just glancing at the below dates , Mar 2012 , Dec 2011 , and Sep 2011 seem like reasonable markers for alignment .
just edited the question and added desired final output . the goal ( if it doesn't go without saying ) is to get this programmatically , so adjusting different resample params for each series in some non-automatically detectable way is unfortunately not helpful

I think the easiest way to do this is to ` join ` on index . I've tweaked the original variables to DataFrames to enable this ( Note : they ought to be DataFrames rather than Series anyway ) : #CODE

I've got some radar data that's in a bit of an odd format , and I can't figure out how to correctly pivot it using the pandas library .
Note that I did not set ` loc ` as the index yet so it uses an autoincrement integer index . #CODE
However , if your data frame is already using ` loc ` as the index , we will need to append the ` time ` column into it so that we have a loc-time hierarchal index . This can be done using the new ` append ` option in the ` set_index ` method . Like this : - #CODE
Welcome . Glad it helps ! Notice that for your dataframe , you do not have the autoincrement index . So I edited my answer to suggest that you use the ` append=True ` option while adding ' time ' into your existing ' loc ' index .
` df_by_speed = panel [ " speed "]` gives you the result for the original question . Now to extract a specific Series by the ` loc ` , since ` loc ` is an index in ` df_by_speed ` , it is as simple as ` df_by_speed.ix [ ' A ']` where A is the location name .
You can use the pivot method here : #CODE

If I transpose the input to model.predict , I do get a result but with a shape of ( 426,213 ) , so I suppose its wrong as well ( I expect one vector of 213 numbers as label predictions ): #CODE

Since resample() requires a TimeSeries-indexed frame / series , setting the index during creation eliminates the need to set the index for each group individually . GroupBy objects also have an apply method , which is basically syntactic sugar around the " combine " step done with pd.concat() above . #CODE

I found a way using ` join ` : #CODE
This is not so direct but I found it very intuitive ( the use of map to create new columns from another column ) and can be applied to many other cases : #CODE
Thanks , the map method seems pretty powerful . Will certainly use it often .

I would like to apply a function to a dataframe and receive a single dictionary as a result . pandas.apply gives me a Series of dicts , and so currently I have to combine keys from each . I'll use an example to illustrate .
Thanks ! I had overlooked ` map ` completely , and my rewritten function is much cleaner now .

pandas pivot dataframe to 3d data
There seem to be a lot of possibilities to pivot flat table data into a 3d array but I'm somehow not finding one that works : Suppose I have some data with columns =[ ' name ' , ' type ' , ' date ' , ' value '] . When I try to pivot via #CODE
` pivot ` only supports using a single column to generate your columns . You probably want to use ` pivot_table ` to generate a pivot table using multiple columns e.g. #CODE
The hierarchical columns that are mentioned in the API reference and documentation for ` pivot ` relates to cases where you have multiple value fields rather than multiple categories .
However , if you want separate columns for different value fields for the same category ( e.g. ' type ') , then you should use ` pivot ` without specifying the value column and your category as the columns parameter .
... it is probably more intuitive / readable to stick to using stack / unstacks and groupby as in the other solution below . I basically never use pivot_table

Append two multi indexed data frames in pandas
` pandas.append() ` ( or ` concat() ` method ) can only append correctly if you have unique column names .

However , my goal is to be able to use a row-wise function in the ` DataFrame.apply() ` method ( so I can apply the desired functionality to other functions I build ) . I've tried : #CODE
Row-wise functionality should be possible with apply . For example , ` df.apply ( lambda x : sum ( x**2 ) , axis = 1 )`

If you have a key that is repeated for each row , then you can produce a cartesian product using merge ( like you would in SQL ) . #CODE
This won't win a code golf competition , and borrows from the previous answers - but clearly shows how the key is added , and how the join works . This creates 2 new data frames from lists , then adds the key to do the cartesian product on .
My use case was that I needed a list of all store IDs on for each week in my list . So , I created a list of all the weeks I wanted to have , then a list of all the store IDs I wanted to map them against .
The merge I chose left , but would be semantically the same as inner in this setup . You can see this in the documentation on merging , which states it does a Cartesian product if key combination appears more than once in both tables - which is what we set up . #CODE

` combine_first ` is not actually an ` append ` operation . See - #URL
while ` append ` is #URL
Append columns of other to end of this frame s columns and index ,

What would be a way to read this file and align the date / values ?

Just replace `' / Users / spencerlyon2 / Desktop / test.csv '` with the path to your file

How can I replace all the NaN values with Zero's in a column of a pandas dataframe
I have also looked at this article How do I replace NA values with zeros in R ? whilst looking at some other articles .
The " problem " is that the chaining breaks the fillna ability to update the original dataframe . I put " problem " in quotes because there are good reasons for the design decisions that led to not interpreting through these chains in certain situations . Also , this is a complex example ( though I really ran into it ) , but the same may apply to fewer levels of indexes depending on how you slice .
It's one line , reads reasonably well ( sort of ) and eliminates any unnecessary messing with intermediate variables or loops while allowing you to apply fillna to any multi-level slice you like !

Merge parameters for Pandas
I have a loop in Python which sequentially imports CSV files , assigns them to a temporary DataFrame object and then attempts to merge / concact them to a ' master ' DataFrame . The code is below : #CODE
The MLS_Stats DF is initially empty , which is the reasoning for the if loop , since I don't think you can merge a DF with an empty DF .
For each merge , I want build the DataFrame by including any new uniquely indexed rows and new columns , but exclude overlapping columns . The above code currently includes the overlapping columns with _x and _y suffixes .
You can filter duplicate rows with ` drop_duplicates ` , and select to join only columns that are not yet present . #CODE

You can use the append function to add another element to it . Only , make a series of the new element , before you append it : #CODE
I believe append returns a new Series ( rather than doing it in place ) so you want ` test = test.append ( pd.Series ( 200 , index =[ 101 ]))`

How to apply a function to two columns of Pandas dataframe
Now I want to apply the ` f ` to ` df `' s two columns `' col_1 ' , ' col_2 '` to element-wise calculate a new column `' col_3 '` , somewhat like : #CODE
can you apply f directly to columns : df [ ' col_3 '] = f ( df [ ' col_1 '] , df [ ' col_2 '])
Here's an example using ` apply ` on the dataframe , which I am calling with ` axis = 1 ` .
Depending on your use case , it is sometimes helpful to create a pandas ` group ` object , and then use ` apply ` on the group .
Yes , i tried to use apply , but can't find the valid syntax expression . And if each row of df is unique , still use groupby ?
i provide a detail sample in question . How to use Pandas ' apply ' function to create ' col_3 ' ?
Use apply on the whole dataframe , passing in rows with df.apply ( f , axis=1 ) . Then rewrite your function ` get_sublist ( x )` to index the col values like this ` start_idx = x [ 1 ] , end_idx = x [ 2 ]` .
Since you haven't provided the body of f I can't help in anymore detail - but this should provide the way out without fundamentally changing your code or using some other methods rather than apply

pandas's resample with fill_method : Need to know data from which row was copied ?
I am trying to use resample method to fill the gaps in timeseries data . But I also want to know which row was used to fill the missed data .
With resample , I will get this #CODE

align edge ( default ) | center
For vertical bars , align = edge aligns bars by their left edges in left , while align = center interprets these values as the x coordinates of the bar centers .
So adding try adding the keyword align = ' center ' to you first plot call and that might get aligned your x-axis .

As a work-around , I'm probably going to simply import from my target sql and do a join . I'm concerned because of the size of the datasets . #CODE

If I ` reset_index ` on both frames I can now merge / join them and slice however I want , but how can I do it using the ( multi ) indexes ?

Here's a good explanation and simple comparison between pandas and numpy record arrays - Normalize / Standardize a numpy recarray

I have a dictionary name date_dict keyed by datetime dates with values corresponding to integer counts of observations . I convert this to a sparse series / dataframe with censored observations that I would like to join or convert to a series / dataframe with continuous dates . The nasty list comprehension is my hack to get around the fact that pandas apparently won't automatically covert datetime date objects to an appropriate DateTime index . #CODE
I can create an empty dataframe with a continuous DateTime index , but this introduces an unneeded column and seems clunky . I feel as though I'm missing a more elegant solution involving a join . #CODE

Could you add an example of ' something complicated ' to the original post ? Assuming you had a ` DateTimeIndex ` with regular frequency you could always use ` df.resample ` to aggregate the data at another regular frequency ( like every two months ) and then use ` df.pct_change() ` to get the returns . Also there are various options for ` pct_change() ` [ see ` periods ` , ` freq `] that allow you to specify how many data points should be used to compute the returns ( ` periods ` defaults to 1 , which is why the solution gave the same answer as your function ) .
Because you have a relatively small data set , the easiest way is to resample on the parameters that you need to calculate the data on then use the ` pct_change() ` function again .

Well , the whitespace is in your data , so you can't read in the data without reading in the whitespace . However , after you've read it in , you could strip out the whitespace by doing , e.g. , ` df [ " Make "] = df [ " Make "] .map ( str.strip )` ( where ` df ` is your dataframe ) .
I don't have enough reputation to leave a comment , but the answer above suggesting using the map function along with strip won't work if you have NaN values , since strip only works on chars and NaN are floats .

To extract the Series ( and plot ) from the panel you can use ` ix ` with the following syntax : #CODE

Use ` ix ` : #CODE
( It's good practice to do in a single ix / loc / iloc since this version allows assignment . )
That said , I kinda disagree with the docs that ix is :
use loc for labels
use ix for both ( if you really have to )
It feels like there ought to be a way to do this in one pass ( using loc / without chaining ) , however assignment ( ` s [ ' b '] .ix [ 1:10 ]`) works so I guess it's ok .
Surprisingly ( for me at least ) , although comparable for small Series , this starts to become slower than using ` ix ` when the Series is longer than 250 . ( Tested using ` %timeit ` in ipython . )

Pandas rolling apply with missing data
I think a partial answer to this question is probably via using the keyword argument min_periods in the rolling apply function . Ex : pandas.rolling_apply ( x2 , 3 , foo , min_periods=1 ) helps .

The best way to do this in pandas is to use drop : #CODE
Finally , to drop by index instead of by name , try this to delete , e.g. the 1st , 2nd and 4th columns : #CODE
@USER I don't know of any performance improvement , but readability-wise , ` drop ` is a more SQL-like description of the operation in question . Couldn't ` del ` potentially be interpreted as setting all the values in that column to ` NaN ` ?
I think in version 0.16.2 drop by index doesn't work - do nothing . Can you repair your answer ?
Drop by index

How to drop rows of Pandas dataframe whose value of certain column is NaN
Don't ` drop ` . Just take rows where ` EPS ` is finite : #CODE
` notnull ` is also what Wes ( author of Pandas ) suggested in his comment on another answer .
Though of course that will drop rows with negative numbers , too . So if you want those it's probably smart to add this after , too . #CODE
You could use dataframe method notnull or inverse of isnull , or numpy.isnan : #CODE

You can use groupby and then apply to achieve what you want : #CODE

I think you want to ` resample ` your dataframe , but I'm not sure . Please give some example data and show what you want to achieve .

My actual goal is to use ` groupby ` , ` crosstab ` and / or ` resample ` to calculate values for each period based on sums / means / etc of individual entries within the period . In other words , I want to transform data from : #CODE
` freq= ' M '` is for month-end frequencies ( see here ) . But you can use ` .shift ` to shift it by any number of days ( or any frequency for that matter ): #CODE
Thanks , this may be the trick I need to create a solution based on the rrule hack . However , this doesn't help with resampling on a range , as resample will still use bins aligned to the beginning of the month AFAIK .
I don't have a simple workaround for you at the moment because ` resample ` requires passing a known frequency rule . I think it should be augmented to be able to take any date range to be used as arbitrary bin edges , also . Just a matter of time and hacking ...

I want to find all values in a Pandas dataframe that contain whitespace ( any arbitrary amount ) and replace those values with NaNs .
And finally , this code sets the target strings to None , which works with Pandas ' functions like fillna() , but it would be nice for completeness if I could actually insert a NaN directly instead of None .
What you really want is to be able to use [ ` replace `] ( #URL ) with a regex ... ( perhaps this should be requested as a feature ) .

I've already explored Panda's fillna , but it doesn't seem to meet my needs . I've also considered the np.where method , but I'm not sure how'd it work in this situation . I'm pretty new to Pandas , but maybe the map / apply function are what I need ? This can probably be accomplished a thousand different ways , but looking for something that won't crawl given the size of the data .
The paired dict has the city as the key and the borough as the value . Now the last step is to apply / map it back to the borough column ... how do I do that ?
Edit : If you've already created your dict as in your edited post , just use ` d [ ' Borough '] = d.City.map ( paired [ ' Borough '])` to map each city to the borough from your dict . ` map ` is a useful method to know about . It can map values either with a Pandas series , with a dict , or with a function that returns the mapped value given the key .
There are cases when when the same city may be paired with different boroughs , for instance the city ' New York ' is mapped to the Borough Manhattan in like 97% of occurrences , but how does map handle that situation ?

Now I've got the idea . Another way of doing this could be to join these data frames , it will remove the non-matching entries and I can drop count column afterwards .
a cute one-liner : data [ data.groupby ( ' tag ') .pid .transform ( len ) > 1 ]
@USER Also if there is repeated / unordered index it may be non-trivial to put them back in order . Ah ha , currently you can do : ` g.filter ( lambda x : len ( x ) > 1 , dropna=False ) .dropna() ` to keep the order .

Of course I can just manually replace the truncated words , but I'm curious to know what the cause is ?

I'm also puzzled why the ` apply ` version along ` axis=1 ` is so much slower . It should literally be just a shortening of the syntax , no ?

Currently I think you need to create a custom subclass . You'd need to override the ` apply ` and ` onOffset ` methods to take into account your holiday calendar .

That may be the case , Hayden . If that's true , I guess I have to figured out how to properly convert my dataframe into a timeseries that I can resample . So far I have not been successful at that either .
I am able to get the desired result by converting my dataframe to a timeseries using this command : " ts = pd.TimeSeries ( df [ 0 ])" , and then I can resample the timeseries . Not as elegant as doing it straight from the dataframe , but it works for now .
You can resample over an individual column ( since each of these is a timeseries ): #CODE

Update : A useful workaround is to just smash this with the DatetimeIndex constructor ( which is usually much faster than an apply ) , for example : #CODE
In 0.15 this will be vailable in the dt attribute ( along with other datetime methods ): #CODE
With more complicated selections like this one you can use ` apply ` : #CODE

problems with apply function in pandas after update
I tried to apply ' manually ' the function recursively to see if some of the dates passed as the x parameter in the lambda definition where wrong , but managed to get correct results any time . But the ` apply ` method just seem not to work anymore , and cannot understand why .

If you unstack STK_ID , you can create side by side plots per RPT_Date . #CODE

I think the OP's primary concern is with the division , not the shift . The answers provided so far both address only the latter .
My concern was with the division , but the reason I'm getting that result is because of pandas alignment . Shift resolves that issue .
` shift ` realigns the data and takes an optional number of periods .
Aah , shift is what i needed . I worked it out with .values , but this is the way I'd rather do it . Thanks !

Python pandas insert long integer
I'm trying to insert long integers in a Pandas Dataframe #CODE

If I used ` cmov_mean ` in ` scikits.timeseries ` , what should I use when I " resample " in pandas ?
When I " resample " my daily averages to monthly and then plot both , I notice a big time offset . There is a " convention " setting to " start " or " end " but I don't see a " mid " setting .

I have some base info in a pandas DataFrame . I need to join it with some reference tables that I have access via a pyodbc connection . Is there any way to get the sql result set into a pandas DataFrame without writing the result set out to a csv first ?

Merge of multiple data frames of different number of columns into one big data frame
Also what if instead two CSV files I had two data frames and wanted to do the same , for example if I loaded first csv to ` df1 ` and second one in ` df2 ` and then wanted to make a merge to ` df3 ` that would look like example above .
Why not try the ` concat ` function : #CODE
Note : the ` concat ` does have some additional options if you have slightly different requirements .

You need to use ` | ` instead of ` or ` . The ` and ` and ` or ` operators are special in Python and don't interact well with things like numpy and pandas that try to apply to them elementwise across a collection . So for these contexts , they've redefined the " bitwise " operators ` ` and ` | ` to mean " and " and " or " .

There seems to be an inconsistency between MultiIndex rows and MultiIndex columns . Using the transpose allows you to select rows as df [ " AA " : " AA "] which then return a MultiIndex DataFrame ( not losing information ) , however , df.xs ( " AA " , axis=1 ) returns a DataFrmae with a single level Index ( thus losing information ) . In addition to this , when I define a single level ( Index ) DataFrame with columns AA and BB then df [ df [ " AA "] > 0 ] will give me all the rows of columns AA and BB where the element in AA is greater than 0.0 . However , if I do the same in a MultiIndex column DataFrame , then I get a crash .

Awesome , thank you ! build dataframes for each user and concat , very clever !
Finally , if you don't like the way the frame looks you can use the transpose function of panel to change the appearance before calling to_frame() see documentation here

How to keep MultiIndex when using pandas merge
A similar question was asked in How to keep index when using pandas merge , but it will not work with MultiIndexes , i.e , #CODE
How can one make the merge while preserving the MultiIndex in the left dataframe ?

and " 10 " would indicate where the distance shifted to being " 5 " . ( It would have to be exactly there , if the shift was detected a step later , wouldn't matter . )

Actually , many of DataFrameGroupBy object methods such as ( apply , transform , aggregate , head , first , last ) return a DataFrame object . I used the method ` filter ` in [ one ] ( #URL ) of my blog posts .

yes , df1 + df2 will try and align the columns .

you should convert your time steps to a ` DatetiemIndex ` and than resample R2

What is the ` pandas ` way to do this ? I would try to copy the original table , drop columns that differ ( ` result ` and ` run `) , reindex that , combine both things again with the new index as multi-index and then run the mean method for that outer multi-index level . Is that the way to do it , and if yes , how do I do these index things properly in code ?

I have a pandas pivot_table that aggregates 2 data sets in 2 columns across several rows . I would like to add another column that is the difference between the aggregated values in the two existing columns by row . Is there a way to implement this directly in the pivot_table() call ? I know that the returned pivot is a dataframe so I can calculate it through other means , but just curious if there is a more efficient way .
Define the function you want to apply . #CODE
Then , apply it . #CODE
Thanks , I ended up doing something similar . Was wondering if the pandas pivot had any similar built in functionality .

Efficient way to apply multiple filters to pandas DataFrame or Series
I have a scenario where a user wants to apply several filters to a Pandas DataFrame or Series object . Essentially , I want to efficiently chain a bunch of filtering ( comparison operations ) together that are specified at run-time by the user .
I want to take a dictionary of the following form and apply each operation to a given Series object and return a ' filtered ' Series object . #CODE
Your right , boolean is more efficient since it doesn't make a copy of the data . However , my scenario is a bit more tricky than your example . The input I receive is a dictionary defining what filters to apply . My example could do something like ` df [( ge ( df [ ' col1 '] , 1 ) & le ( df [ ' col1 '] , 1 )]` . The issue for me really is the dictionary with the filters could contain lots of operators and chaining them together is cumbersome . Maybe I could add each intermediate boolean array to a big array and then just use ` map ` to apply the ` and ` operator to them ?

It looks like this is not a bug and the subtle difference is due to the usage of the ` reindex_like() ` method . The call to ` reindex_like() ` inserts some NaN data into the series so the ` dtype ` of that series changes from ` bool ` to ` object ` . #CODE

is it possible to do fuzzy match merge with python pandas ?
I have two DataFrames which I want to merge based on a column . However , due to alternate spellings , different number of spaces , absence / presence of diacritical marks , I would like to be able to merge as long as they are similar to one another .
Perhaps I should have been clearer ( will edit question now ) . I want to merge on similar values between two DataFrames
I would just do a separate step and use difflib getclosest_matches to create a new column in one of the 2 dataframes and the merge / join on the fuzzy matched column
Could you explain how to use ` difflib.get_closest_matches ` to create such a column and then merge on that ?
Similar to @USER suggestion , you can apply ` difflib ` ' s ` get_closest_matches ` to ` df2 `' s index and then apply a ` join ` : #CODE
If these were columns , in the same vein you could apply to the column then ` merge ` : #CODE
As a heads up , this basically works , except if no match is found , or if you have NaNs in either column . Instead of directly applying ` get_close_matches ` , I found it easier to apply the following function . The choice of NaN replacements will depend a lot on your dataset . #CODE

Then you can insert it again into your DataFrame . But that should work with DataFrames too , maybe you can suggest it .
I used this and DataFrame.apply to apply it to all major columns in the dataframe . After thinking about it a bit more , I think this is the intended design , and it perfectly accomplishes the goal .

map ( lambda x : ( x [ 3 ] , [ int ( x [ 1 ]) , int ( x [ 2 ])]) , [ line.split() ])

You can either load the file and then filter using ` df [ df [ ' field '] constant ]` , or if you have a very large file and you are worried about memory running out , then use an iterator and apply the filter as you concatenate chunks of your file e.g. : #CODE

I realize Dataframe takes a map of { ' series_name ' : Series ( data , index ) } . However , it automatically sorts that map even if the map is an OrderedDict() .
or just concat dataframes #CODE

I create a fresh dataframe with the appropriate index , drop the data to a dictionary , then populate the new dataframe based on the dictionary values ( skipping missing values ) . #CODE

Resample Searies / DataFrame with frequency anchored to specific time
Which I want to resample to say ' 5s ' . Normally I would do : #CODE

I was pleased to see that this method also works with the replace function .
i'd use the pandas replace function , very simple and powerful as you can use regex . Below i'm using the regex \D to remove any non-digit characters but obviously you could get quite creative with regex . #CODE

numpy diff on a pandas Series
Pandas implements ` diff ` like so : #CODE
So why doesn't ` np.diff ( s )` work ? Because np is taking ` np.asanyarray() ` of the series before finding the ` diff ` . Like so : #CODE

I am attempting to left merge two dataframes , but I am running into an issue . I get only NaN's in columns that are in the right dataframe .

Does Pandas Cache values on IX call ?
I noticed some strange behavior when using IX on large pandas dataframes .
Yes , ` ix ` caches results . ` b.ix ` returns a ` _NDFrameIndexer ` . Its ` __getitem__ ` method calls the DataFrame's ` get_value ` method , which calls the ` _get_item_cache ` method , which caches results .

Note : it is not equal to ` dt ` because it's become " offset-aware " : #CODE
Think of np.datetime64 the same way you would about np.int8 , np.int16 , etc and apply the same methods to convert beetween Python objects such as int , datetime and corresponding numpy objects .

You can use the ` resample ` method ( #URL ) if you have a time series ( if the time is used as the index ): #CODE
Actually , this data frame is already a concatenation of re-sampled data in OHLC format . The problem is that re-sampling does not give me missing 1 min intervals , just the time intervals from starting time to end time during each date . Feels to me like the trick is to select all unique dates present , create 24 hour time samples for each of these dates and merge the two sets .

This gives me the error for serie_5 ( the second concat ): #CODE
Another way is to use join : #CODE
just create a Python list and append your Series into it and then provide it to pandas.concat as @USER was writing above .
The use of join looks generic enough ! I changed it to " serie_5 = serie_4.join ( serie_3 , how = ' outer ')" in order to nclude 2012-01-12 in the example above . The reason I want to get a generic solution is that I want to combine several of different time series where there will be missing data and use Pandas functionality to handle the missing data . Thanks !

Suppose I have two DataFrames a b where a is larger than b and has all NaNs . b.index is a subset of a.index , however b has real values . I wish to merge the values from b into a . #CODE

In this case they're equivalent . Apply can also do aggregation and other things

Edit : I just read about modules shelve and pickle . It seems like they would achieve what I'm trying to do , basically save lists on disks . Because my lists are large , is there any way not to load the full list into memory but , rather , efficiently append values one at a time ?
` append ` is a wrapper for ` concat ` , so ` concat ` would be marginally more efficient , but as @USER says Pandas is probably not appropriate for updating a HDF5 file every second . If you absolutely need Pandas for some reason , could you collect a list of Series and update the file periodically instead ?
Bren is right about numpy / pandas working best when preallocated . If memory is no constraint just preallocate a huge zeros array and append at the end of the program removing any excess zeros . Which I suppose is a bit of what Matti is saying .
Another option is to use messaging to transmit from one process to another ( and then append in memory ) , this avoids the serialization issue .

` tz ` means time zone and ` Not Windows ` and ` Windows ` are categories extracted from the User Agent in the original data , so we can see that there are 3 Windows users and 0 Non-windows users in Africa / Cairo from the data collected .

My first 3 columns have separate values for ` YEAR ` , ` MONTH ` , and ` DAY ` . I need to merge these three columns and have the entire date in one column for all the rows .
You are looking for ` apply ` ( ` merge ` is like a database join . ): #CODE

merge two dataframe and create a new one with multiindex
I would like to merge the two dataframe in a new dataframe with a multi-index like :

I am trying to add a column of smaller ` len ` into a ` DataFrame ` where indexes of smaller item are a subset of a larger item . So if RIMM has data for every single day , but GOOG is missing some day . I want to add RIMM to the matrix with header GOOG #CODE
You are looking for an outer ` join ` , here is a simple example : #CODE

` len ( np.arange ( 12 ))` and ` len ( pd.stats.moments.rolling_mean ( np.arange ( 12 ) , 6 ))` both equal 12 as I would have expected - what result were you expecting ?
I have faced this with rolling statistics in pandas , too . I'd say for non-time-related measurements , such as an altitude vs . distance profile , a central-based moving window makes more sense , since it does not introduce lag or shift . Now for time-based measurements , I think taking only previous values makes more sense , since it would be conceptually wrong if " future " values influenced present ones .

If you use a pandas Series rather than a list , you can use its ` diff ` method : #CODE

One " built-in " way to accomplish it might be accomplished using ` shift ` twice , but I think this is going to be somewhat messier ...

I assume what you are trying to do is change the frequency of a Time Series that contains data , in which case you can use ` resample ` ( documentation ) . For example if you have the following time series : #CODE
Then you can change the frequency to seconds using resample , specifying how you want to aggregate the values ( mean , sum etc . ): #CODE
Update : if you're doing this to a DatetimeIndex / datetime64 column a better way is to use ` np.round ` directly rather than via an apply / map : #CODE
Hence you can apply this to the entire index : #CODE
@USER you're right of course ! I forgot about milli-seconds ... whoops ! I have corrected this and added how to apply this to the entire dt_index .

Think I figured out the second part : sp500 [ " regression "] = exp ( sm.OLS ( log ( sp500 [ " Adj Close "]) , sm.add_constant ( range ( len ( sp500.index )) , prepend=True )) .fit() .fittedvalues )
You're going to need to post more information - version numbers and how you built pandas - maybe a traceback would also help narrow things down . I am unable to replicate a segfault with pandas 0.9.1 on 64-bit linux in IPython with or without pylab . You might also want to report bugs on github issues rather than stack overflow . Easier to get developers ' attentions there .

As You can see both DataFrames have ` Date_Time ` as a common column . I want to Join these two DataFrames by matching ` Date_Time ` .
You are looking for a ` merge ` : #CODE
The keywords are the same as for ` join ` , but ` join ` uses only the index , see " Database-style DataFrame joining / merging " .
If you try and join on a column you get an error : #CODE
It is giving me an error when i run merge : - UnicodeDecodeError : ' ascii ' codec can't decode byte 0xc2 in position 268 : ordinal not in range ( 128 )

The ` for ` loops and ` append ` s will not be efficient and should be avoided . Try rewrting these using numpy functions and / or the DataFrame ` apply ` method ...

Also , would you agree then , using your suggestion , if we want to apply a function / algorithm restricted every unique date in the file one should just groupby the ' datetime ' object ?

As commented , in newer pandas , Series has a ` replace ` method to do this more elegantly : #CODE
@USER good spot ! ( been a while since I wrote this ! ) replace definitely best option , another is to use ` .apply ( { ' March ' : 0 , ' April ' : 1 , ' Dec ' : 3} .get )` :) In 0.15 we'll have Categorical Series / columns , so the best way will be to use that and then sort will just work .
@USER I've taken the liberty of replacing the second line with the ' replace ' method . I hope that is Ok .

some problem with ix and loc , the pandas documentation could be clearer
Welcome to Stack Overflow ! Please consider editing your post to add more explanation about what your code does and why it will solve the problem . An answer that mostly just contains code ( even if it's working ) usually wont help the OP to understand their problem . It's also recommended that you don't post an answer if it's just a guess . A good answer will have a plausible reason for why it could solve the OP's issue .

When you do ` len ( df [ ' column name '])` you are just getting one number , namely the number of rows in the DataFrame ( i.e. , the length of the column itself ) . If you want to apply ` len ` to each element in the column , use ` df [ ' column name '] .map ( len )` . So try #CODE
I came up with a way using a list comprehension : ` df [[( len ( x ) < 2 ) for x in df [ ' column name ']]]` but yours is much nicer . Thanks for your help !
To directly answer this question's title ( which I understand is not necessarily the OP's problem but could help other users coming across this question ) one way to do this is to use the drop method :

From various examples , I could manage to create the bar plots . Changed the stats name to a column instead of being an index , which later allows to choose only those rows where the statitis matches the argument . Also added the node value as a column . Appended all the nodes ' DataFrames into a bigger list . Finally doing pivot table and putting the pivots into a dataframe and plotting the dataframe in bar mode created the necessary graphs .
I think the best way is to merge all dataframes together , then you could use all nice Panda functions to slice and mix-and-match anyway you want .
The alldfs should be similar to your dict . I would merge them like this : #CODE
You could of course use stack / unstack to structure your DataFrame a bit different depending on the amount of data and the way you will be using it most .

Pandas join grouped and normal dataframe
I need to join levels with lines ( atom , ion , level ): at first on atom , ion , level_number_upper and then atom , ion , level_number_lower . Is there a way to precompute the join - memory is not an issue , but speed is .
To show what I want to join merge here a code snippet #CODE
and then I want to join / merge grouped data with lines on atomic_number and ion_number
Why not join / merge first then do the groupby ?
So the levels Dataframe is much shorter than the lines dataframe . It would cost a lot of performance to do the join / merge before the groupby .
Just to confirm , are you are wanting to merge / join a groupby object with a dataframe ?
Well no - I want to join / merge the result .

` transform ` is not that well documented , but it seems that the way it works is that what the transform function is passed is not the entire group as a dataframe , but a single column of a single group . I don't think it's really meant for what you're trying to do , and your solution with ` apply ` is fine .
So basically , you don't need to use transform here . ` apply ` is the appropriate function here , because ` apply ` really does operate on each group as a single DataFrame , while ` transform ` operates on each column of each group .

For the output you want read_csv will not be able to do this on the fly . Just read single and merge the dataframes

Try to convert the ' sales ' string to an ` int ` , if it is well formed then it goes on , if it is not it will raise a ` ValueError ` which we catch and replace with the place holder . #CODE
you might need to edit the line that splits the line ( as your example copied out as spaces , not tabs ) . Replace the print line , with what ever you want to do with the data . You probably need to relpace ' NaN ' with the pandas NaN as well .
If it's already in the DataFrame you could use ` apply ` to convert those strings which are numbers into integers ( using ` str.isdigit ` ): #CODE
@USER uncommented , and commented the ` to_dict() ` . Although , ` apply ` is the important bit of my answer ( weirdly no other answers seem to use it ) .

Although Chang's answer explains how to plot multiple times on the same figure , in this case you might be better off in this case using a ` groupby ` and ` unstack ` ing :

This will append the correct values but does not update the index properly and the graph is messed up .
If you want to plot the bars of all columns and the mean you can ` append ` the mean : #CODE

For example , say you want to pivot the data so there are separate columns for
a function of your creation ) can easily be applied to the columns of ` pivot ` . pd.rolling_mean() , like all rolling / moving functions in pandas , even accepts a ` center ` parameter for centered sliding windows . #CODE

Non standard interaction among two tables to avoid very large merge
Profiling shows the culprit is obviously ` B.ix [ row [ 0 ]] .irow ( np.searchsorted ( B.ts [ row [ 0 ]] , row [ 2 ])))` . However , standard solutions using merge / join would take too much RAM in the long run .
Replace function calls with loops
The last major improvement I can think of would be to replace df.apply() with a for loop to avoid calling any function 200M times ( or however large A is ) .
This is slightly faster than original , but there is till a lot of overhead ( for me 4ms for the merge and 2.5ms for the second line ) . Probably easier to maintain than the numpy solution I gave .
Many thanks ! I was under the impression that apply was preferable to loops . Looking at the source , I'm not that sure , as it is just regular python with more functionality than the one I need . Moreover , I assumed that Pandas indexes provided enough performance . A dict is just perfect -- I just have to find the way to put the data in the files as a dict instead than as df , since originally it was precisely a dict --

How can I retrieve specific columns from a pandas HDFStore ? I regularly work with very large data sets that are too big to manipulate in memory . I would like to read in a csv file iteratively , append each chunk into HDFStore object , and then work with subsets of the data . I have read in a simple csv file and loaded it into an HDFStore with the following code : #CODE
If you replace that line with :

apply on group replicating complete MultiIndex

I would like to fill gaps in a column in my DataFrame using a cubic spline . If I were to export to a list then I could use the numpy's ` interp1d ` function and apply this to the missing values .
I'm surprised you accepted the answer so fast ( no offense , hayden ;) because I thought you especially wanted to interpolate time series , but I guess you didn't mean exactly pandas.TimeSeries . I am interested in exactly these topics as well , currently . See #URL
OK thanks for your help ! and last thing I hope . I have multiple columns each containing NaN data . Sol the df.dropna() drops too many rows . How do you apply that to one column only ( i.e. ' data1 ')

Apply function on Pandas dataframe
I'm a newbie to pandas dataframe , and I wanted to apply a function to each column so that it computes for each element x , x / max of column .
Pandas DataFrame : apply function to all columns

The last two lines don't raise an error when I leave off the ` on ` kwarg , but return ` NaN ` results for the joined column . They also fail if I drop the index on both stacked DataFrames ( e.g. , do ` wstk.reset_index ( inplace=True )` before the join ) .

Trying to use the awfully useful pandas to deal with data as time series , I am now stumbling over the fact that there do not seem to exist libraries that can directly interpolate ( with a spline or similar ) over data that has DateTime as an x-axis ? I always seem to be forced to convert first to some floating point number , like seconds since 1980 or something like that .
) as x-argument , interestingly , the Spline class does create an interpolator , but it still breaks when trying to interpolate / extrapolate to a larger DateTimeIndex ( which is my final goal here ) . Here is how that looks : #CODE

To simulate different consumption rates , replace all real outbound timestamps

How to apply function to date indexed DataFrame
Then ` apply ` this to each state in the DataFrame : #CODE
Using ` join ` or ` merge ` works too : #CODE

Pandas interpolate changed in version 0.10 ?
Your old code used to work " by accident " . Calling the column's ` interpolate ` method as below is the correct way .

Dataframe merge creates duplicate records in pandas ( 0.7.3 )
When I merge two CSV files , of the format ( date , someValue ) , I see some duplicate records .

`' viridis '` ( will be default color map in 2.0 )
I would suggest the ` cubehelix ` color map . It is designed to have correct luminosity ordering in both color and gray-scale
So your requirements are " lots of colors " and " no two colors should map to the same grayscale value when printed " , right ? The second criteria should be met by any " sequential " colormaps ( which increase or decrease monotically in luminance ) . I think out of all the choices in matplotlib , you are left with ` cubehelix ` ( already mentioned ) , ` gnuplot ` , and ` gnuplot2 ` :
The white line is the luminance of each color , so you can see that each color will map to a different grayscale value when printed . The black line is hue , showing they cycle through a variety of colors .

Assuming Y is a column in your dataframe , one way is to use ` diff ` and cumsum : #CODE
diff returns timedelta objects now in pandas master .

But I do not see how to ' normalize ' fx and fy so that they have the same levels and ` fx.lables ` and ` fy.lables ` have the same coding . ` fy.labels = fx.lables ` clearly does not work . As the following demonstrates that it changes the meanings of the labels [ a c e ] becomes [ a b c ] . #CODE
Another related scenario is that I have an existing , known index , and want to factor the data into this index . For example , I know that every data point has to take one of the five values [ a , b , c , d , e ] and I already have an index ` Index ([ a , b , c , d , e ] , dtype =o bject )` and I want to factorize vector y =[ ' a ' , ' c ' , ' e '] into a Categoricial variable with ` Index ([ a , b , c , d , e ] , dtype =o bject )` as its levels . I am not sure how it can be done either and would like someone who knows to give some clues .
I get the general idea , but I think there is a bug . In [ 6 ] , it should be range ( len ( fx.levels )) .

You can ` unstack ` the groupby : #CODE
By the way , I think the lambda won't work . I used map instead .

The above works , but I would like to have the TimeSeries ( one per instrument ) in one DataFrame , i.e. a column per instrument , so that I can use DataFrame.plot() . The problem is that no two TimeSeries have the exactly the same index , i.e. I would need to merge all the TimeSeries ' indexes .
pd.concat() performs an ' outer ' join on the indexes by default and holes can be filled by padding forwards and / or backwards in time . #CODE

Python pandas resample added dates not present in the original data
I am using pandas to convert intraday data , stored in ` data_m ` , to daily data . For some reason ` resample ` added rows for days that were not present in the intraday data . For example , 1 / 8/ 2000 is not in the intraday data , yet the daily data contains a row for that date with NaN as the value . DatetimeIndex has more entries than the actual data . Am I doing anything wrong ? #CODE
Prior to 0.10.0 , pandas labeled resample bins with the right-most edge , which for daily resampling , is the next day . Starting with 0.10.0 , the default binning behavior for daily and higher frequencies changed to ` label= ' left ' , closed= ' left '` to minimize this confusion . See #URL for more information .
` resample ` converts to a regular time interval , so if there are no samples that day you get NaN .

The real issue is -- and now comes a threefold question : how can it be that just importing pandas broke matplotlib's ability to handle datetime objects , when just two lines earlier pandas was clearly not even involved in that same operation ? Does pandas upon import silently alter other modules in the top level namespace to force them to make use of pandas methods ? And is this acceptable behavour for a python module ? Because I need to be able to rely on it that importing , say , a random number module , won't silently change , say , the pickle module to apply a random salt to everything it writes ..

@USER So you want it exported to an array ? I think the issue is that behind the scenes pandas stores using indices of .level , and doesn't store this array ... I will take another look . Hopefully there is a better way than ` np.array ( map ( np.array , df.index.values ))` ( ! )
So I found that index.get_loc is similar to what I want . It translates from a key to an actual location - but it is not as useful as the .ix notation of a series . For now I think i will just do my_index = Series ( arange ( len ( df )) , index=myselectedindex )

I have a time series object ` grouped ` of the type ` pandas.core.groupby.SeriesGroupBy object at 0x03F1A9F0 ` . ` grouped.sum() ` gives the desired result but I cannot get rolling_sum to work with the ` groupby ` object . Is there any way to apply rolling functions to ` groupby ` objects ? For example : #CODE
I'm not sure of the mechanics , but this works . Note , the returned value is just an ndarray . I think you could apply any cumulative or " rolling " function in this manner and it should have the same result .

If you are creating a timeseries , you can use the ` tz ` argument of ` date_range ` : #CODE

( Note that I don't actually use ` date_range() ` so using its ` tz ` parameter is not an option . )
Update : In recent pandas , you can use the dt accessor to broadcast this : #CODE
Here's one way ( depending if tz is already set it might be a ` tz_convert ` rather than ` tz_localize ` ): #CODE

If you take the first and last duplicate value of each year and shift the data in-between by an hour , that should be the easiest way of correcting the issue . You'll obviously have to take into account that the first data points start in daylight savings .

I would like to know if the following problem can be solved using python pandas library . I tried using merge and join but I am not sure how to go about getting the desired result .
If the dicts contain both numerical and string values , then you could combine them using a join , followed by a groupy and aggregation . For example , #CODE
Thanks . The problem is , the data in csv is mixed . The rows have both integers and strings . In that case , I would like to know if there is a way to add only the numbers and append the ' D ' values as a list

What do you want col1 and col2 to look like after you pivot ? Your example output shows A1 and B1 for the final row yet neither of those values are associated with the 18 and 22 . I have a couple of options : #CODE

I am using ` A= A.groupby ([ ' sequence ']) .sum() ` and ` B= B.groupby ([ ' sequence ']) .sum() ` to sum the shares across each sequence . I then want to concatenate these sets again and sum the shares across sequences . However , I try using ` C = concat ([ A , B ])` and now find that I only have the column shares as an index and cannot group by sequence . ` C.group ([ ' sequence ']) .sum() ` gives me an error KeyError : u'no item named sequence ' .

Pass the ` axis ` option to the ` apply ` function : #CODE
Great . Does apply pass the columns including item1 , item2 when I use axis=0 ? What happens when there is a hierarchical indexing in the columns and the rows ?

` ix `' s main purpose is to allow numpy like indexing with support for row and column labels . So I'm not sure your use-case is the intended purpose . Here are a couple of ways I can think of , mostly trivial : #CODE
I don't think ` ix ` supports negative indexing at all . It seems to just ignore it altogether : #CODE

You can resample the data to business month . If you don't want the mean price ( which is the default in ` resample `) you can use a custom resample method using the keyword argument ` how ` : #CODE
@USER Don't know which method is preferred : normally I would use ` resample ` . Are there any advantages when using ` asfreq ` ? ( when using ` asfreq ` the keyword seems to be ` method ` ( not ` fill_method ` in 0.10 )
I added a comment to your question . ` resample ` should work , not sure about advantages of ` asfreq ` .

The ix notation allows you to slice columns . It can be confusing at first , but it reads in English as : " Take all of the rows and only the columns from 1 to the end " .
Now we have renamed the columns so they will align the way we want . Performing the division calculation is easy as Pandas will do all the alignment . No loops necessary ! #CODE
Since pandas in it's current form assumes time series data are arranged with time in the index , not the columns , transposing the DataFrame , at least temporarily , will enable the use of many built-in methods , such as ` shift ` / ` diff ` / ` pct_change ` / etc . #CODE

You can use the DataFrame ` apply ` method : #CODE

For each date , when there is an ' S ' in the signal column append the corresponding price at the time the ' S ' occurred . Below is the attempt . #CODE
That is , " give me the price at 1620 ; ( even when it doesn't give a " sell signal " , S ) so that I can diff . with the " extra B's " -- for the special case where B > S . This ignores the symmetric concern ( where S > B ) but for now I want to understand this logical issue .
Unrelated , but you ** really ** don't want to do all those ` * ` -imports . ` from numpy import * ` will replace the builtin ` any ` with ` numpy `' s any , which doesn't handle genexps , and so ` any (( False for i in range ( 3 ))) == True ` ; ` from os import * ` will replace ` open ` with ` os.open ` , and so most ` open ` calls will return ` TypeError : an integer is required ` ; and so on .

What would be the best way to synchronize the two datasets ? Do I have to write an algorithm which is then going to go back and say on User 2 , insert the following entry ( 6 , 35 ) . So eventually the new data looks like : #CODE

You may find it faster to extract the index as a column and use ` apply ` and ` bfill ` .

I'm a bit confused , first you append something to cash then you reassign it to an empty series . Are you sure this is the same as your code ? Is it possible to include an example with the first few lines of the DataFrame ( ` df.head() `) you have and the DataFrame you would like ?

Update : in 0.15 you will have access to a dt attribute for datetimelike methods : #CODE
Here's one ( slow ! ) workaround to do it using ` apply ` , not ideal but it works : #CODE
It seems like a bug ( that you can't do ` apply ( lambda x : x.month )`) , perhaps worth adding as an issue on github . As Wes would say : " welcome to hell " .

This happens when using apply as well #CODE

Two Columns of a pandas dataframe - Concat in Python

How to apply quantile to pandas groupby object ?

` map ` before ( or even after ) the ` zip ` ?
I ran a formula on the price col of the csv file . Indeed , they are all " numbers " . apply ( float ) for some reason was rejected w / ValueError : could not convert string to float : price .

Merge multi-indexed with single-indexed data frames in pandas
How can I merge the two data frames with only one of the multi-indexes , in this case the ' first ' index ? The desired output would be : #CODE
Note : you are almost doing a ` join ` here ( except the df1 is MultiIndex ) ... so there may be a neater way to describe this ...
you can * nearly * merge like this : ` df1.merge ( df2 , left_on =d f1.index.get_level_values ( ' first ') , right_on =d f2.index.get_level_values ( ' first '))`
According to the documentation , as of pandas 0.14 , you can simply join single-index and multiindex dataframes . It will match on the common index name . The ` how ` argument works as expected with `' inner '` and `' outer '` , though interestingly it seems to be reversed for `' left '` and `' right '` ( could this be a bug ? ) . #CODE

You can replace ` nan ` with ` None ` in your numpy array : #CODE
Unfortunately neither this , nor using ` replace ` , works with ` None ` see this ( closed ) issue .

These are the lines I tried to replace the True and False , and got a dataframe filled with all True values : #CODE
` applymap() ` can be used to apply a function to every element of a ` dataframe ` #CODE
Hmm . That doesn't seem to be working -- look at the output again . It seems to be because the type of the objects in the ` DataFrame ` is actually ` numpy.bool_ ` , not Python's ` bool ` .

Is there a way to replace the ' nan ' label with "" in the x-axis ?

Have you tested the ` calcvol ` function separately ? It's good practice to do that first before you apply . ( I don't suppose this is a simple as a forgotten axis argument : ` optionsData.apply ( calcvol , axis=1 )` ?

For the rows which have ' ' in front I want to cut that and move into column C before the ' = ' sign .
If I use normal slice it will cut values where there is no ' ' sign .
And ` startswith ` does not work on float values .
If you want to use ` startswith ` with a float , you can just first convert it to a str with str() . But I do not really see how the column D can be of type float ?
You could create a function which takes an entry in ` df.D ` columns and returns a Series . Then you can use Series ` apply ` with this function : #CODE

I know I could resample the prices and fill in the details ( ` ffill ` , right ? ) , but that doesn't seem like such a nice solution , because I have to assume the frequency I'm going to be indexing it at and it reduces readability with too many unnecessary data points .
Check ` asof ` #CODE

I'm trying to merge a series of dataframes in pandas . I have a list of dfs , ` dfs ` and a list of their corresponding labels ` labels ` and I want to merge all the dfs into 1 df in such that the common labels from a df get the suffix from its label in the ` labels ` list . i.e. : #CODE
I'm trying to make a series of merges that at each merge grows at most by number of columns N , where N is the number of columns in the " next " df in the list . The final DF should have as many columns as all the df columns added together , so it grow additively and not be combinatorial .
The behavior I'm looking for is : Join dfs on the column names that are specified ( e.g. specified by ` on= `) or that the dfs are indexed by . Unionize the non-common column names ( as in outer join ) . If a column appears in multiple dfs , optionally overwrite it . Looking more at the docs , it sounds like ` update ` might be the best way to do this . Though when I try ` join= ' outer '` it raises an exception signaling that it's not implemented .
Here is my attempt at an implementation of this , which does not handle suffixes but illustrates the kind of merge I'm looking for : #CODE
This assumes that the merging happens on the indices of each of the dfs . New columns are added in an outer-join style , but columns that are common ( and not part of the index ) are used in the join via the ` on= ` keyword .
The twist on this would be one where you arbitrarily tag a suffix to each df based on a set of labels for columns that are common , but that is less important . Is the above merge operation something that can be done more elegantly in pandas or that already exists as a builtin ?
Your merges are going to be made on overlapping column names -- is that what you want ? Could you look inside the function where the error occurs and let me know what the value of the ` group_sizes ` variable is before the error occurs ? The cardinality of the " join space " is the issue and may need to be worked around .
@USER : Yes , I want an outer join but I want it to use the indices of the left and right df . I now pass it `` left_index=True `` and `` right_index=True `` and that gets rid of the combinatorial explosion but still does not do what I intended . I just want to combine all the dfs ( they all share an index ) and for the overlapping column names , use the suffixes in the suffixes list .
@USER : It's possible in the sense that there's always a unique index for all the dfs in question . I can explain what's unclear if you let me know what - I basically want an outer merge that adds columns together based on a unique index
thank you , the `` reduce / combine_first `` solution is best and I agree that renaming the dfs before combining is the elegant solution . Can you explain though why this does not cause combinatorial issues while merge does ? From the documentation , this seems like an outer join on an index with merge , but they behave very differently ...
Good question -- it seems to be a limitation of the current merge implementation . Opened github issue #URL to follow up on Wes's question above .

Subset the ` dataframe ` to only those records with the desired Status . ` Groupby ` the ID and apply the lambda function ` diff() .sum() ` to each group . Use ` transform ` instead of ` apply ` because ` transform ` returns an indexed series which you can use to assign to a new column ' diff ' .
very smart , you changed diff ( 1 ) to diff ( -1 ) so that diff would be taken between i and i-1 , but then the signs were all negative , hence -diff ( -1 ) .
may want to consider the effect of calculating diff() on an ungrouped dataframe . i.e. , what if the row with shift == -560 was bad ?

As @USER comments this fails for NaN's and isn't particularly robust either , in practise using something similar to @USER ' s answer is probably recommended ( Note : we want a bool rather than raise if there's an issue ): #CODE
@USER you're right you want to be using assert_frame_equal afterwards ( I don't think pandas exports a similar ) , although beware of using it from quant's answer as that can raise ( rather than return bool ) .
@USER I think you want to do something similar to quants but return a bool , have included a recipe .

If you want to have multiple ` DataFrames ` on the same sheet how would you combine them ? Instead merge , join , or concatenate them into a single ` DataFrame ` beforehand as pandas gives you multiple ways of doing so . And then do your export .
I think you are going to be better doing a ` concat ` of these DataFrames before exporting ( via ` to_excel ` ) . That is : #CODE
Thanks , but the integration of data frames is messy ( they are merged together and not in sequence ) when exported on the sheet . Unfortunately I think I should have to study a lot merge , join , concatenate functions in order to have usable and correctly formatted data on the sheet . Anyway this command is useful too . Thanks . M

Awesome . I was hacking out the aweful ` grp2.agg ( lambda x : u " | " .join ( sorted ( set ( map ( str , x.tolist() )))))` . Thanks for showing me the ropes on using arrays for real ! Where is a good reference ? Thanks again .

[ Updated to adapt to modern ` pandas ` , which has ` isnull ` as a method of ` DataFrame ` s .. ]
You can use ` isnull ` and ` any ` to build a boolean Series and use that to index into your frame : #CODE
You could use the function ` isnull ` instead of the method : #CODE

I've also tried doing this with concat and I get the same results .
Are you trying to merge or concat these DataFrames somehow ? What are you expecting the output to be ?
You should be able to use ` concat ` and ` unstack ` . Here's an example : #CODE
@USER that's very strange ! Is this still true if you take just the head of the series ? If so , could you possibly append the output of ` s.head() .to_dict() ` for both Series to your question ? That would be really helpful .

What do I need to change to obtain the latter ? I suppose I could use ` pd.read_table ( " test.txt " , na_filter=False )` and subsequently replace ' NULL ' values with NaN and change the column dtype . Is there a more straightforward solution ?

One day I hope to replace my use of SAS with python and pandas , but I currently lack an out-of-core workflow for large datasets . I'm not talking about " big data " that requires a distributed network , but rather files too large to fit in memory but small enough to fit on a hard-drive .
I would then have to append these new columns into the database structure .
I am building consumer credit risk models . The kinds of data include phone , SSN and address characteristics ; property values ; derogatory information like criminal records , bankruptcies , etc ... The datasets I use every day have nearly 1,000 to 2,000 fields on average of mixed data types : continuous , nominal and ordinal variables of both numeric and character data . I rarely append rows , but I do perform many operations that create new columns .
Finally , I would like to append these new columns into the on-disk data structure . I would repeat step 2 , exploring the data with crosstabs and descriptive statistics trying to find interesting , intuitive relationships to model .
The modeling process requires that I analyze every column , look for interesting relationships with some outcome variable , and create new compound columns that describe those relationships . The columns that I explore are usually done in small sets . For example , I will focus on a set of say 20 columns just dealing with property values and observe how they relate to defaulting on a loan . Once those are explored and new columns are created , I then move on to another group of columns , say college education , and repeat the process . What I'm doing is creating candidate variables that explain the relationship between my data and some outcome . At the very end of this process , I apply some learning techniques that create an equation out of those compound columns .
e.g. I have tables on disk that I read via queries , create data and append back .
Thanks for the links . The second link makes me a bit worried that I can't append new columns to the tables in HDFStore ? Is that correct ? Also , I added an example of how I would use this setup .
The actual structure in the hdf is up to you . Pytables is row oriented , with fixed columns at creation time . You cannot append columns once a table is created . However , you can create a new table indexed the same as your existing table . ( see the select_as_multiple examples in the docs ) . This way you can create arbitrary sized objects while having pretty efficient queries . The way you use the data is key to how it should be organized on-disk . Send me an off-list e-mail with pseudo code of a more specific example .
Is there any reason I couldn't just transpose a dataframe , add it to an HDFStore , and then index it by the " column " names ? This would allow me to access only the " columns " ( in the form of rows ) that I need , transpose them back , and then append any new ones I create . Is there a disadvantage to having a very wide table in an HDFStore ?
I've recently spent time assessing how to use pandas in an " out of core " manner just like SAS . This is very helpful . I would be interested in hearing your thoughts on comparing the hdf5 approach you outline here , to dumping ( chunk by chunk ) the file ( a csv file for example ) in a SQLite3 table . I don't use pandas to_sql but rather dump it using the sqlite3 module so that I can define the beginning and end to a transaction . This speeds up the dumping into a physical file . I can then use read_sql to call it into pandas . I can also add columns ( though not drop or rename ) and append observations .
querying : gt = greater than ... #CODE
How about a join since I normally get 10 data sources to paste together : #CODE
Hi , I'm playing around with your example as well and I run into this error when trying to insert into a database : ` In [ 96 ]: test.insert (( a [ 1 ] .to_dict() for a in df.iterrows() )) ---------------
My basic workflow is to first get a CSV file from the database . I gzip it , so it's not as huge . Then I convert that to a row-oriented HDF5 file , by iterating over it in python , converting each row to a real data type , and writing it to a HDF5 file . That takes some tens of minutes , but it doesn't use any memory , since it's only operating row-by-row . Then I " transpose " the row-oriented HDF5 file into a column-oriented HDF5 file .
The table transpose looks like : #CODE
And although the query language and pandas are different , it's usually not complicated to translate part of the logic from one to another .

I think type change is fine as long as the timestamp they map to is preserved #CODE
I want to do a map with a customized function where I'm expecting pandas.lib.Timestamp type .

How do I turn a row into a map ?
that worked a few months ago , but doesn't seem to work in the latest Pandas . How can I turn a row into a map , or otherwise do simple concise custom printing of rows ?
This will get you a map : ` speeds.ix [ 3 ] .to_dict() `
This will convert to a map : ` speeds.ix [ 3 ] .to_dict() `

Trouble with pandas cut
I can then append this to my dataframe to have a new column . Each date is deciled and each data point then has their corresponding decile on that date . Thanks .
Stack Trace : #CODE
Can you paste the entire stack trace ? Or is that the only error message you receive ? To return 1 to 10 just modify the lambda function like so : ` lambda x : pd.qcut ( x , 10 ) .labels + 1 `
I pasted your stack trace into your original question . Hopefully someone with more knowledge can help you with that error . Are you able to upgrade to python 2.7 ?
It might . Can you paste the output of ` df.dtypes ` ? WHere ` df ` is the dataframe you want to decile . Also , you shouldn't have to drop the NaNs . I believe the label that is returned is a -1 .

How do I join two dataframes ( pandas ) with different indices ?
I want to insert the values from ` df2 ` into ` df1 ` , keeping empty rows where ` df1.index = ' POP '` .
I tried ` join ` , ` combine ` , ` combine_first ` and ` concat ` , but they all seem to take the rows that exist in both df's .
` df1.join ( df2 )` should default to a ` left ` join which only preserves the columns from df1 . Is that what you want ? In pandas 0.10 that is the default .
It sounds like you want an ' outer ' ` join ` : #CODE

My current solution is to define a temporary dataframe w , based on the fancy boolean indexing , set the corresponding values in ' y ' to 0 in w , and then merge w back to d using the index . There must be a more efficient ( and hopefully more direct ) way of doing this : #CODE

And I'm trying to efficiently join where the keys match and the date is between the valid_from and valid_to . What I've come up with is the following : #CODE
While this seems to do the job it doesn't feel like a particularly elegant solution . I was wondering if anybody had a better idea for a join such as this .
( Note : the ` value ` column of ` df2 ` is accessed as ` value_y ` after the merge because it conflicts with a column of the same name in ` df ` and the default merge-conflict suffixes are ` _x , _y ` for the left and right frames , respectively . )

Thanks @USER , I didn't know that qcut / cut had a labels-attribute ( isn't showing in IPython autocompletion unfortunately ) . So far I thought I had to call ` labels=False ` to the function-call to get the labels as well . But it's nicer like this .

You say that the best way is to plot each condition ( like ` subset_a ` , ` subset_b `) separately . What if you have many conditions , e.g. you want to split up the scatters into 4 types of points or even more , plotting each in different shape / color . How can you elegantly apply condition a , b , c , etc . and make sure you then plot " the rest " ( things not in any of these conditions ) as the last step ?
From what I can tell , matplotlib simply skips points with NA x / y coordinates or NA style settings ( e.g. , color / size ) . To find points skipped due to NA , try the ` isnull ` method : ` df [ df.col3.isnull() ]`

For this to work correctly ( at least in pandas 0.14 ) , I think you need to replace chunk = indexed_what [ indexer ] by chunk = indexed_what.iloc [ indexer ] . Otherwise the slice is treated as an index range but it is a positional range .

" AssertionError when using apply after GroupBy " . It's since been fixed .
Is there any workaround on 0.10 ? In some cases I can get ` apply ` working after ` groupby ` and in other cases not .

Pandas Merge ( pd.merge ) How to set the index and join
I tried the following merge : #CODE
Any recommendations on what is going wrong here ? I want it to merge based on both date and cusip / idc_id . Note : for this example the cusips are lined up , but in reality that may not be so .
Reset the indices and then merge on multiple ( column- ) keys : #CODE
You could append `' cuspin '` and `' idc_id '` as a indices to your DataFrames before you ` join ` ( here's how it would work on the first couple of rows ): #CODE

` strip ` only removes the specified characters at the beginning and end of the string . If you want to remove all ` \n ` , you need to use ` replace ` . #CODE
You could use ` regex ` parameter of ` replace ` method to achieve that : #CODE

Maybe a nice trick / slightly dirty way to get around the unicode issues is to convert unicode columns into string columns with the " xmlcharrefreplace " option ; later on you can translate this back into unicode if you want to .

Now groupby both columns and apply the lambda function : #CODE

Is there an efficient matrix-based operation combo that can be used to achieve this ? I tried looking at various join , merge etc . operators in the docs , but couldn't find anything offering similar logic .
` alternating = big [( big.index.to_pydatetime() - start ) .total_seconds() / 17 % 2 == 0 ]` , but I can't seem to find a way to map the total_seconds() call to all elements .

Renaming a pandas pivot table without losing axis labels
When I invoke rename on a pivot table , I lose the axis labels : #CODE
When you pivot , the values of x and y are the labels , and that is expected behaviour .

You can use the ` DataFrame ` ` drop ` function to remove columns . You have to pass the ` axis=1 ` option for it to work on columns and not rows . Note that it returns a copy so you have to assign the result to a new ` DataFrame ` : #CODE
This does indeed work well , but in this instance I only need to keep about 5-6 out of 40-50 series of data , and the series I want to drop may fluctuate based on changes in the input data file . Good to learn usage of the .drop function though - thanks !
I just had to do something similar to what you've done , and in my case , I've pre-computed the list of things I need to drop , and then passed in the list to the drop() function . Worked like a charm !
@USER Zelleke , what if i had about 50 columns i want to drop and 50 columns i want to keep . and the number of columns can change each instance i run it . is there a way to do some sort of df.drop ( #URL ) so kind of dropping chunks of cols at a time

maybe replace the num with enumerate in the for loop ?

Pandas : Read Timestamp from CSV in GMT then resample
I have a CSV with epoch GMT timestamp at irregular intervals paired with a value . I tried reading in from the CSV but all the times are shifted to my local timezone . How do I have it just read in as-is ( in GMT ) ? Then I would like the resample to one minute intervals , HOWEVER , I would like to be able to have it skip gaps which are larger than a user specified value . If this is not possible , is there way to resample to to one minute , but in the gaps , put in an arbitrary value like 0.0 ? #CODE

I reduced to a series and did a value_counts it worked like a charm . The only thing I could figure out that might be the issue is in the dataframe dtypes were all object ( vs int64 ) but once I drop to the series , it went to longs .

I actually think it won't always make sense to apply ` reshape ` to a Series ( do you ignore the index ? ) , and that you're correct in thinking it's just numpy's reshape :

How to drop extra copy of duplicate index of Pandas Series ?
So how to drop extra duplicate rows of series , keep the unique rows and only one copy of the duplicate rows in an efficient way ? ( better in one line )
One way would be using ` drop ` and ` index.get_duplicates ` : #CODE
Sorry , i should have made my intention more clearly . What I want is : keep the unique rows and only one copy of the duplicate rows . Not totally drop the duplicated ones . I modified the question accordingly .
You can groupby the index and apply a function that returns one value per index group . Here , I take the first value : #CODE
@USER sorry , " arbitrary " of length len ( s ) :) . I'm kind of guessing here , since I can't find it in the source ...
@USER Can you check ' s.groupby ( s.index ) .first() ' for MultiIndex Series ? Below is my snippet : import pandas as pd ; idx_tp = [( ' 600809 ' , ' 20061231 ') , ( ' 600809 ' , ' 20070331 ') , ( ' 600809 ' , ' 20070630 ') , ( ' 600809 ' , ' 20070331 ')] ; dt = [ ' demo ' , ' demo ' , ' demo ' , ' demo '] ; idx = pd.MultiIndex.from_tuples ( idx_tp , names = [ ' STK_ID ' , ' RPT_Date ']) ; s = pd.Series ( dt , index=idx ); # s.groupby ( s.index ) .first() will crash on my machine

In SQL , this is standard set logic , accomplished differently depending on the dialect , but a standard function . How do I elegantly apply this in Pandas ? I would love to input some code , but nothing I have is even remotely correct . It's a situation in which I don't know what I don't know ..... Pandas has set logic for intersection and union , but nothing for disjoint .

The ` Target ` is just a constant , so instead of trying to find the root for ` f ( x ) = 0 ` , you'd define ` g ( x ) = f ( x ) - Target ` and apply ` newton ` to ` g ` .

I'm trying to identify the rows with unicode and strip the $ sign and comma , converting to float . #CODE
EDIT : Thanks for the two responses , I can reproduce those with no problem . However when I use the apply function to my case I get an ' unhashable type ' error . #CODE
You are just printing these and not ` apply ` -ing them to the DataFrame , here's one way to do it :
If I understand you right , you're looking for the ` apply ` method : #CODE

Update : I now recommend installing the scientific python stack using Anaconda .
@USER it may be preferable to install pandas via the fedora package #URL I now recommend using conda ( much easier for the python data stack ) .

apply a function to a pandas Dataframe whose retuned value is based on other rows
I want to apply the same process to the whole quantity column . I don't know how to approach this problem with the pandas library other than looping through the Dataframe row by row .
Here , we groupby ` [ ' item ' , ' price ']` and apply the function above . The output is a series of relative weights for the unique combinations of item and price . #CODE

After building basic class with ` __str__ ` and plotData() methods I would like to apply some filters and build a new class where additional column is the filter . I would like to do that in ` __init__ ` but keep everything what already was done . In another words I don't want to re-write the whole ` __init__ ` only want to add new column to the basic dataframe .

However , when stored ( and retrieved ) dates are ` unicode ` rather than ` Timestamp ` . To convert back to what we started with we could ` apply ` ` Timestamp ` to the column and ` set_index ` : #CODE

I'm currently trying to build a fairly simple script that will compare two DataFrames from a CSV and perform an inner merge to remove duplicates . Now I noticed that one of my CSVs looks like this : #CODE

To limit memory usage , simply replace the dict cache with something like a LRU .

I try to apply exactly the same logic to my original problem with large dataframe inside a class . The code is : #CODE
I found in here that there could be a problem with type of the columns but Depth is type ` numpy.float64 ` Hper is type ` float ` Vper is type ` float ` so I understand how it can apply to my problem .

Replace the line #CODE
You can use ` iget ` to retrieve by position :

Drop row in Panda Series and clean up index
I have a Panda Series and based on a random number I want to pick a row ( 5 in the code example below ) and drop that row . When the row is dropped I want to create a new index for the remaining rows ( 0 to 8) . The code below : #CODE
My problem is that the row number 8 is dropped . I want to drop row " 5 NaN " and keep - 0.000052 with an index 0 to 8 . This is what I want it to look like : #CODE
Somewhat confusingly , ` reindex ` does not mean " create a new index " . To create a new index , just assign to the ` index ` attribute . So at your last step just do ` sample_mean_series.index = range ( len ( sample_mean_series ))` .

edit : I'm new to StackExchange and Python . Not sure where to drop sample data . Here's an image of the dataframe .

In some circles this operation is known as the " asof " join . Here is an implementation : #CODE
What about using ` Series.searchsorted() ` to return the index of ` y ` where you would insert ` x ` . You could then subtract one from that value and use it to index ` y ` . #CODE

If you want to combine ` join ` your MultiIndex into one Index ( assuming you have just string entries in your columns ) you could : #CODE
Note : we must ` strip ` the whitespace for when there is no second index . #CODE

pandas : merge rows on timestamp
I'd like to merge the rows based on the first column and have the output look like this : #CODE

Sorry @USER . I was not using it properly . I am still struggling to get a combination of groupby and stack to recast the dataframe .
Construct the index as desired and apply it to the dataframe
Now create the desired index and apply it . Here are several approaches for the index . #CODE

I suspect you are trying to set the date as the index too early . My suggestion would be to first ` set_index ` as date and company name , then you can ` unstack ` the company name and ` resample ` .
For posterity : in my actual , messier dataset , I needed to also use ` groupby ( levels =[ 0 , 1 ]) .last() ` to remove duplicate indices so I could ` unstack ( level=1 )` the dataframe , and then , to get the final result , I had to call [ ' return '] on the dataframe : e.g. with Andy's ` df4 ` ,, ` df4 [ ' return ']` got me the DataFrame I needed . Now I have a DataFrame with over 6k labeled TimeSeries . Sweet !

The first resample starts at 2000-01-03 and the second resample starts at 2000-01-04

Apply multiple functions to multiple groupby columns
The docs show how to apply multiple functions on a groupby object at a time using a dict with the output column names as the keys : #CODE
What I want to do is apply multiple functions to several columns ( but certain columns will be operated on multiple times ) . Also , some functions will depend on other columns in the groupby object ( like sumif functions ) . My current solution is to go column by column , and doing something like the code above , using lambdas for functions that depend on other rows . But this is taking a long time , ( I think it takes a long time to iterate through a groupby object ) . I'll have to change it so that I iterate through the whole groupby object in a single run , but I'm wondering if there's a built in way in pandas to do this somewhat cleanly .

Resample time series in pandas to a weekly interval
How do I resample a time series in pandas to a weekly frequency where the weeks start on an arbitrary day ? I see that there's an optional keyword base but it only works for intervals shorter than a day .
You can pass anchored offsets to ` resample ` , among other options they cover this case .

Pandas Drop Rows Outside of Time Range
I have been looking for solutions but none of them separate the Date from the Time , and all I want to do is drop the rows that are outside of a Time range .
Note : the same syntax ( using ` ix ` ) works for a DataFrame : #CODE
@USER -- My first thought was also that it worked the same , but I think DataFrame tries to apply it to the columns ( without the ix ) .
@USER I think it's good practice to use ix with Series as well , and that way the syntax is identical , so I updated the first part .
@USER thanks for mentioning that , I will append :)

How can you not get negative numbers ? You're doing ` 1-abs ( stuff )` . If abs ( stuff ) > 1 , the result will be negative . Anyway , setting start_exon that way will always give you the * first * exon . If you need the first exon after the start_codon , that's something a little different .

@USER : Then I think root's answer is the easiest . If you can't / don't want to replace your reading with ` pandas.read_csv ` , then probably my ` numpy.delete ` is easiest , but I think you're better off with his answer .
I just noticed that he already had ` pandas ` as a tag on his question , so yeah , this is definitely the way to go . ` read_csv ` is much simpler , harder to get wrong , and probably faster than what he has , and there's no good reason not to drop the column in ` pandas ` instead of post-deleting after ` to_records ` .

currently to create my time axis I'm using the following : ` x = mdates.num2date ( x , tz=None )` ` x = [ dt.replace ( tzinfo=None ) for dt in x ]`
Like Andy , I'm confused by what you're trying to do . Isn't the point of a histogram to show counts on the y-axis ? You may be trying to force the use of ` hist ` ... consider taking a step back to construct a bar plot .

Pandas dataframe resample at every nth row
I am planning to resample the dataframe so that if the dataset passes certain size , I will resample it so there are ultimately only the SIZE_LIMIT number of rows . This means I need to filter the dataframe so every n = actual_size / SIZE_LIMIT rows would aggregated to a single row in the new dataframe . The agregation can be either average value or just the nth row taken as is .

I need to apply some function for every columns and create new columns in this DataFrame with special name . #CODE
You can use ` join ` to do the combining : #CODE
where you could replace ` df*2 ` with ` df.apply ( your_function )` if you liked .
I would skip the ` apply ` method and just define the columns directly . #CODE
Not as elegant as DSM's solution . But for whatever reason I avoid ` apply ` unless I really need it .
BY avoiding the ` join ` this also has the nice benefit of not having to reassign the dataframe .

I am using python for some data analysis . I have two tables , the first ( lets call it ' A ') has 10 million rows and 10 columns and the second ( ' B ') has 73 million rows and 2 columns . They have 1 column with common ids and I want to intersect the two tables based on that column . In particular I want the inner join of the tables .
I could not load the table B on memory as a pandas dataframe to use the normal merge function on pandas . I tried by reading the file of table B on chunks , intersecting each chunk with A and the concatenating these intersections ( output from inner joins ) . This is OK on speed but every now and then this gives me problems and spits out a segmentation fault ... no so great . This error is difficult to reproduce but it happens on two different machines ( Mac OS Snow Leopard and UNIX , RedHat ) . I finally tried with the combination of Pandas and PyTables by writing table B to disk and then iterating over table A and selecting from table B the matching rows . This last options works but it is slow . Table B on pytables has been indexed already by default .
straightfoward disk based merge , with all tables on disk . The
See this answer for a comment on how doing a join operation will actually be an ' inner ' join .
For your merge_a_b operation I think you can use a standard pandas join
table ; instead of storing the merge results per se , store the row index ; later
I am avoiding to use join / concat / merge option of pandas since they all spit out a " segmentation fault : 11 " every now and then . I struggle to reproduce this error : a ) it is not on a particular row of either table and b ) it happens on different computer / OS . It does not happen all the time but still means I cannot use that route . SO far I am making use of indexes to fill in a columns on A with values from B . Which is probably like a inefficient " join " . Any other ideas are welcomed .

Newer versions of pandas escape the tags . To avoid it replace last line with : #CODE
That is an excellent idea , thanks ! I basically replace the stars in my example code above with ` ` . The significance is decided based on values in ` p-value ` column .

Pandas DatetimeIndex truncate error
` df ` was created by concatenating multiple dataframes together using the ` concat ` function .

add column with time rounded to millisec and groupby it , apply cumsum within each group
Apply a lambda function that indexes the current time from the ts series . The function returns the sum of all ts entries between ` x - ms and x ` . #CODE
You need a way of handling NaNs and depending on your application , you may need the prevailing value asof the lagged time or not ( ie difference between using kdb+ bin vs np.searchsorted ) .
ugh , the second asof ( s.asof ( lag )) is wrong . What you really need are the indices from the first asof . I will post a corrected version .

Join pandas series while keeping new indices
I have tried concat : #CODE
but I need the output that adds values of existing indices and keeps new indices if they appear , so a mix of concat and ' + ' . How can I do that ? My expected output is #CODE
Or you could align the two first , and then simply add them with ` + ` : #CODE

Why not just take the [ transpose ] ( #URL ) ??
You could replace : #CODE

But it seems inefficient to compute cumsum on each call . Is there a way to first compute the cumsums and then apply ' ohcl ' to the data ? #CODE
I wasn't able to get your resample suggestion to work . Did you have any luck ? Here's a way to aggregate the data at the business day level and compute the OHLC stats in one pass : #CODE
Here I create a dictionary of dictionaries . The outer key references the columns you want to apply the functions to . The inner key contains the names of your aggregation functions and the inner values are the functions you want to apply : #CODE

Assuming that Date is the index rather than a column then you can do an " outer " ` join ` : #CODE

How to drop a list of rows from Pandas dataframe ?
Then I want to drop rows with certain sequence numbers which indicated in a list , suppose here is ` [ 1 , 2 , 4 ] , ` then left : #CODE

This is because of using integer indices ( ` ix ` selects those by label over -3 rather than position , and this is by design : see integer indexing in pandas " gotchas " * ) .
* In newer versions of pandas prefer loc or iloc to remove the ambiguity of ix as position or label : #CODE
Note : Series has a similar ` iget ` method .

Using the ` map ` lets you put in any condition you want , in this case you can do this more simply ( as pointed out in the comments by DSM ) #CODE
Like the last non map solution from DSM , worked for me

Python pandas , how to truncate DatetimeIndex and fill missing data only in certain interval
for each date in the DataFrame truncate to only have data in the
the pandas truncate functions only allows me to truncate according to date , but I would like to truncate according to datetime.time here .
for each date in the DataFrame truncate to only have data in the range of 9:00 : 00AM - 11:30 : 00AM and 13:00 : 00 - 15:15 : 00
Generate a new dataframe with an index at 500 msec . Merge this dataframe with the original one using outer join . This gets you a dataframe with rows at regular intervals . Rows for missing observations will contain NaN values . Then fill missing NaN values with fillna .
@USER You can probably create a two-level index ` [ ' date ' , ' time ']` and then apply time filtering for the second level , but that is beyond my current level of pandas-fu now .

and Series have a ` .unique() ` method . So you can use ` map ` and a ` lambda ` : #CODE

Note , however , that while you can attach attributes to a DataFrame , operations performed on the DataFrame ( such as ` groupby ` , ` pivot ` , ` join ` or ` loc ` to name just a few ) may return a new DataFrame without the metadata attached . Pandas does not yet have a robust method of propagating metadata attached to DataFrames .

Andy , Thanks for the help . df is actually a series currently . When I do df [ ' tracking '] = pd.np.arange ( len ( df )) I get ' tracking not in this series ! '

To pick the last row using ` irow ` : #CODE

Okay , I have progress . I've delved into the code to realize that the repr function on Series eventually calls ' _format_datetime64 ' , which checks ' isnull ' and will print out ' NaT ' That explains the difference between these two . #CODE
The former seems to honor the NA , but it only does so when printing . I suppose there may be other pandas functions that call ' isnull ' and act based on the answer , which might seem to partially work for NA timestamps in this case . However , I know that the Series is incorrect due to the type of element zero . It is a Timestamp , but should be a NaTType . My next step is to dive into the constructor for Series to figure out when / how pandas uses the NaT value during construction . Presumably , it is missing a case when I specify dtype= ' M 8[ us ]' ... ( more to come ) .

Pandas append data frames , add a field , and then flood the field with a default value ?
I have several data frames that contain all of the same column names . I want to append them into a master data frame . I also want to create a column that denotes the original field and then flood it with the original data frames name . I have some code that works . #CODE

I think that's not the best shape for your DataFrame -- I think columns like " letter " , " number " , " acc " , " rt " or something ( giving them more meaningful names ) would be easier to pivot . Anyway , with your current arrangement : #CODE
Then we apply this to a slice of the ` _rt ` columns : #CODE
All the values which we're using are within 3 standard deviations , so this cut isn't very interesting , but we can apply it anyhow : #CODE

I'm new to pandas ( and python ) and have been slowly working my way trying to apply things learned to my own datasets .
Would you be okay with using a ` map ` on the columns ?

How to apply conditional logic to a Pandas DataFrame .
I could apply a loop and do re-construct the DataFrame ... but that would be ' un-pythonic '
You want to apply a function that conditionally returns a value based on the selected dataframe column . #CODE
` le ` tests whether elements are less than or equal 2.5 , similarly ` lt ` for less than , ` gt ` and ` ge ` .

And then use the ` .fillna ` method to replace the ` NaN ` s with 0 and then assign the columns : #CODE

python pandas : diff by user in a DataFrame containing multiple users
I'd like to get the diff of the timestamp within users . That is , for each event , I want to get the time elapsed since the previous event generated by the same user . #CODE
As mentioned in the comments , you can use ` groupby ` . I'd ` groupby ` and then ` diff ` . ` groupby ` will ( unsurprisingly ) group the rows : #CODE
And then we select the column we're interested in along these groups and ` diff ` it : #CODE
This almost works . Unfortunately , the Series returned from the grouped diff contains some strange values . Running df [ " diff "] .value_counts() yields " TypeError : unhashable type : ' numpy.ndarray '" . I can go back and remove these values , but it feels clunky .
Still not sure where the arrays come from , but here's the fix : df [ " diff "] [ df [ " diff "] .apply ( type ) == numpy.ndarray ] = numpy.NaN

I think the easiest way to accomplish this is to subtract one shifted column from the other . The shift function does exactly that , it shifts an array by a default of one index . #CODE

` keep_default_na ` : bool , default True

I thought it was from the calculation . But if I apply ( np.float64 ) it changes the numbers to what I need . Thanks .

@USER must be a bug , works with ` test.TRAINER_MANAGING.str.startswith ( ' Han ' , na=True )` and ` endswith ` , which are the only ones tested against in the [ commit ] ( #URL ) you link to ) .

@USER that sounds like could be [ a good feature request ] ( #URL ) , I guess you can use loc : ` df.loc [( ' Group1 ' , ' C ')]`

And ` apply ` it ( row-wise ): #CODE

I'm not sure how big your dataframes are , but you could unstack ResultTable and access it with a multiindex tuple

You can first add it as a normal column and then append it to the current index , so : #CODE

Pandas resample not adding up
I then resample the dataframe as below #CODE
I just realised that if I replace NaN's with 0 , I don't have the problem . #CODE
But I would like to resample without filling in NaN's with 0 . Is that possible ?

but now , I need to apply this ( multiparameters ) function along 0-axis .
Maybe I'm wrong and using ` shift ` function is not a good idea

I have a DataFrame with a column that has some bad data with various negative values . I would like to replace values 0 with the mean of the group that they are in .
Replace those negative values with NaN , and then calculate the mean ( ` b `) in each group : #CODE
Then use ` apply ` across each row , to replace each NaN with its groups mean : #CODE
Using @USER ' s example , you could use ` groupby ` / ` transform ` with ` replace ` : #CODE
@USER : ` mask ` is a ` Series ` , which is a subclass of numpy's ` ndarray ` class . The tilde is the [ invert operator ] ( #URL ) when applied to a numpy ndarray . Since ` mask ` is of dtype ` bool ` ( i.e. a boolean array ) the inversion is done bit-wise for each element in the array . ` not mask ` has a different meaning . This asks Python to reduce ` mask ` to its boolean value as a whole object and then take the negation . Numpy arrays refuse to be characterized as True or False . ` not mask ` raises a ` ValueError ` .

How to apply linregress in Pandas bygroup
I would like to apply a scipy.stats.linregress within Pandas ByGroup . I had looked through the documentation but all I could see was how to apply something to a single column like #CODE
But how do I apply a linregress which has TWO inputs X and Y ?
and if using a groupby you can similarly ` apply ` ( to each group ): #CODE
Thanks Andy , Yes it can accept it . The question is how to do it BYGROUP . For example I have datetime that I have GROUPED into Year and month . I want to do the linear regression for each of the groups then return the values from the lin regression . Also I have a DataFram so how can I apply that using two columns in the DF ? Thanks Jason
@USER This reminds me a lot of [ this answer ] ( #URL ) , drop the na first then do the calculation . :)

Resample Daily OHLCV to Weekly OHLCV
I would like to resample / convert Daily ( ohlcv ) to Weekly ( ohlcv ) . Is it possilbe to do this with pandas ?

You can set ` ignore_index=True ` when ` append ` -ing : #CODE

One improvement on this is to use ` irow ` , which grabs only the rows in the specified integer positions : #CODE
irow is cool . I hope it is an optimized lookup . I am starting to think my current requirement might not be supported in pandas . Although I suspect it is not too hard to modify to get this , I might be mistaken . The reason is because of the notation df [( df.age > 20 ) & ( df.age < 30 )] [ 9:10 ] will fetch the 10th element exactly . So I suspect a lot of infrastructure to do this call is already in place .

This is the closest I've gotten , by changing the dictionary value to a list of lists and using a transpose . What is the better way ? #CODE

I would like to insert a new series into ` data ` #CODE
Then use ` concat ` : #CODE

Python : KeyError ' shift '
I assume the last option to be the most appropriate but then I have an error with panda ' shift ' attribute .

but when i apply a function from scikit-learn i loose the informations about columns : #CODE
Is there a way to apply scikit or numpy function to DataFrames without loosing the information ?

Hmmm , runs for me on pandas 0.10.1 and statsmodels 0.4.3 . As a complete novice at Stack Overflow , could it be that I've omitted something important in my question ?

but can't figure out how to use this to replace my data values .
How best to replace these detection limit indicators with values that Pandas can handle ?
Yes , using na_values with pd.ExcelFile.parse worked to replace the < 0.1 values with NaN , but I don't really want NaN here , because below detection limit is very different than missing values ( Tufte argued that leaving out " no damage " values on the plot of o-ring damage played a significant role in the Challenger disaster ) . I would like to replace the `' < 0.1 '` values with ` 0.1 ` to be conservative . Unfortunately the Series with the `' < 0.1 '` values also had NaNs , so I had to do this to get it to work : ` ts [ ts.str.contains ( ' < 0.1 ') .replace ( NaN , False )]= 0.1 `

@USER : In the old days , Pandas Series were a subclass of NumPy ndarrays . ` ndarrays ` have a [ ` tolist ` method ] ( #URL ) . The Series ` tolist ` method [ * overrode * the ndarray method ] ( #URL ) to handle datetime64s differently . So the break in naming convention is a hang-over from NumPy's naming convention . You are right that nowadays it should be ` to_list ` to conform with all the other Series and DataFrame ` to_* ` methods .

Replace MultiIndex's contents with DataFrame columns
How can I insert the contents of the MultiIndex back into regular columns , such that I can select them like any other column ? #CODE
Note : the ` drop ` argument argument ( if set ` True `) will not add the index columns :
` drop ` : boolean , default False
Do not try to insert index into dataframe columns .

Strange . The align keyword behaves as the opposite of mpl and the log keyword is also weird . @USER , i would workaround it by using : ax.bar ( df.index.values , df.values )

How to Apply an equation to a Pandas dataframe ByGroup
I have been reading all day by cant find and exact solution . Now my question is how do apply this back to the original data frame . I would like a new column in the DF that applies the linear regression y=mx+c to each line in the original data using the column 3 as the input BUT to do that using the specific coefficients ( slope , intercept ) that are different for each YEAR and MONTH . Any ideas most welcomed :)
So I would like to apply that relationship back to all the Year=2010 and Month=1 to look like this . Then for the rest of the DF apply the same approach for each month of each year . #CODE
I don't think it's entirely clear ( to me ) what you are asking , perhaps it would help to provide an example DF and what you want it to be ? Perhaps you want an ` apply ` which refers to ` Corr_grouped ` ( ? )
Your first step is to merge the grouped object with DF . To do that first create a common grouping column .
Now you can merge it : #CODE

One way to do this is to ` resample ` ( hourly ) before you plot : #CODE

I'm working with timeseries data that represents vectors ( magnitud and direction ) . I want to resample my data and use the ` describe ` function as the ` how ` parameter .
The question is : how do I implement my own ` describe ` function so that it works with ` resample ` ?
Okat , I think I got it . Instead of resampling , you can ` groupby ` where the group is a unit of time . To this group you can apply a function of your choice , for example your directionAverage function .

How to drop columns by criteria ?
How do I drop all the columns of ` df ` that contain only ` None ` , empty strings , or white-space-only strings ?
would it be an option to loop through the columns and drop when the check is True ?
Apply a lambda testing for the conditions you want to drop : #CODE
Index df.columns with the boolean Series from above to get the columns you want to drop : #CODE
You can use ` applymap ` to apply your function to the elements of the ` DataFrame ` : #CODE
` applymap ` was the key to getting the ` re.match ` function to work for me . It failed when using just ` apply `

@USER Just to illustrate that this really has nothing to do with pandas , note that ` len ( set ( np.array ([ np.nan ] * 10 ))) = 10 ` .

Would converting the quantity to a string , i.e. , `` zip ( map ( str , i ) , map ( str , j ))`` , be an acceptable workaround ?

` numpy ` doesn't use a single globally-unique NaN constant . It has a constant , ` numpy.nan ` , which is predefined , but ` len ( set ( np.array ([ np.nan ] *10 ))) == 10 ` , and ` f = np.array ([ np.nan ] *2 ); print f [ 0 ] is f [ 1 ]` gives ` False ` .

Insert 0-values for missing dates within MultiIndex
I like your stack / unstack method . I am not sure if there is a better way to add rows . If you know all the categories , maybe you could make a DF with all the dates / categories and merge it with your data-containing DF . That would leave NAs that you could fill with zeros . I don't know if that would be faster though .....
the version that iterates through the group does not throw a memoryerror for my local dataset ( the stack / unstack version does )

I have minute based OHLCV data for the opening range / first hour ( 9:30 - 10:30 AM EST ) . I'm looking to resample this data so I can get one 60-minute value and then calculate the range .
By default resample starts at the beggining of the hour . I would like it to start from where the data starts .
You can use the ` base ` argument of ` resample ` : #CODE

I wish to merge df2 with df . Because the index times do not match up I wish to match the df2 time with the closest last time in df , which is the first row . One artificial way I had come up with to solve this was to add a fake microsecond value to the second time series so that it becomes unique . But this is slow for big dataframes . Is there a particular reason why this is not allowed ? It seems like a logical thing to do . Are there any better ways for me to overcome this limitation ?
I want to do pd.merge on on df and df2 on the index , but since the index is not exactly equal , merging does not work . What I ultimately want to do , is to merge df2 with the roughly closest index in df .

since you have 600k rows , create a mask array by ` df.sourceid == 10 ` maybe slow . You can create Series objects that map value to the index of the DataFrame : #CODE

You can use the module StringIo which you can use just like a file handle , so you can strip some data , dump the clean stuff into a virtual file handle , and your csv reader will be none the wiser

One of my favorite aspects of using the ` ggplot2 ` library in R is the ability to easily specify aesthetics . I can quickly make a scatterplot and apply color associated with a specific column and I would love to be able to do this with python / pandas / matplotlib . I'm wondering if there are there any convenience functions that people use to map colors to values using pandas dataframes and Matplotlib ? #CODE

would work ? The ` join ` produces : #CODE
And then drop the rows without a utime : #CODE

You can pass a dictionary of functions to the ` resample ` method : #CODE
Pass the dictionary to the ` how ` parameter of ` resample ` : #CODE

Resample intraday pandas DataFrame without add new days
A combined groupby / resample might work : #CODE

Inner join with MultiIndex fails if no overlap
Is there any good way around this ? I'd like to be able to tell in advance if the intersection is empty , so I can avoid the exception .
I'm not 100% on this , but doing an outer join and dropping the NAs is the same as an inner join . So in the case of no matching indicies , you just get an empty dataframe . If we modify your example to include one matching record , this appears to be the case : #CODE
And so after the outer join + ` dropna() ` , you can see how many rows ` d3 ` and go from there . Using your original example : #CODE

I think the answer given by Chang She doesn't translate to dataframes that have a unique index , like this one : #CODE
I'm assuming you already have the DataFrame . In which case you can just turn the columns into a MultiIndex and use stack then reset_index . Note that you'll then have to rename and reorder the columns and sort by sample to get exactly what you posted in the question : #CODE
Edit : use merge to join original ids back in #CODE
After the call to stack , " s1 s2 s1 s2 ... " is part of the row labels ( index ) . reset_index simply makes it a column instead . After this it's still a DataFrame so index / columns access is still the same .
If that's the case , use merge or set the name to be the index to begin with . I'm updating the solution with this

I have a ` dt ` : #CODE

Maybe use [ drop ] ( #URL ) ?. ` df.drop ( my_cols , axis=1 )` will produce a view of the DataFrame with the dropped columns . All you need is then to assign it to the new DF : ` df2 = df.drop ( my_cols , axis=1 )`
Use [ drop ] [ 1 ] as in this answer to another question : #URL [ 1 ]: #URL
You can either Drop the columns you do not need OR Select the ones you need #CODE

I wondering if I can apply the ` pandas.ols ` model to a data frame of multiple response variables against one independent variable at one time .

+1 : I didn't know about ` icol / irow ` until someone else used it on SO about a month ago !

You could use ` unstack ` before sorting : #CODE

One way to do this is to insert a dummy column with the sums in order to sort : #CODE
and maybe you would drop the dummy row : #CODE
@USER whoops typo :) . Yes , it does seem silly using a dummy ( tbh I could've been more clever with my apply [ 12 ] to do it in one , and it may well be more efficient , but I decided I wouldn't like to be the person reading it ... ) . Like I say , I think there is a clever way to do this kind of comlex sort : s
@USER you can append ' C ' to the list of columns to sort by , so it's : ` df.sort ([ ' sum_B_over_A ' , ' A ' , ' B ' , ' C '])` ... I should really add link to the [ sort ] ( #URL ) docs .

If you reset the index of your klmn1 dataframe to be that of the column L , then your dataframe will automatically align the indices with any series you subtract from it : #CODE

Predict number of rows in merge / join
In my code I use merge / join in numerous places . Recently I bumped on an join probably just making a Cartesian product ( probably only out of 5000 files to process ) . Since the code works on a 64 bit system / python , this join keeps running to fill all memory , blocking every process / user on this hardware node . Since no actual error occurs , it is very hard to debug as well .
Is there a easy way to test the validity of a join / merge , which i could use in an assert statement ?
In my opinion the only way to do it is to understand the different join types ([ this ] ( #URL ) gives the equivalent to SQL )
merge / join creates a new dataframe , that the memory problem . I think of a test to check the content of the join fields . If the join columns do not match ( compare two set ( index_col.values ) of join columns ) this might be used to decide to merge or just return an error ...

you can then merge this new dataframe with the original one by using the merge function : #CODE

Problems with Replace - pandas dataframe
I want to replace df.ix [ 0 ] [ ' date2 '] with df.ix [ 1 ] [ ' date2 '] for each symbol -- the symbol changes through the dataframe so I can't just apply this through the whole dataframe .
I was going to replace the NaN with the date .
2 ) Basic question : How to replace an NaN ( or even another number ) in a pandas DF ?
plus 1 for using .apply() where my solution used a for loop . I always forget about apply .

` Alpha == ' B '` , ` Bool == False `
` Alpha == ' B '` , ` Bool == False ` , column `' I '`
` Alpha == ' B '` , ` Bool == False ` , columns `' I '` and `' III '`
` Alpha == ' B '` , ` Bool == False ` , columns `' I '` , `' III '` , and all columns from `' V '` onwards

Pandas : Merge two Dataframes on a column , but keep only data for unlike Columns

Now I want to resample it as below : #CODE

... but using ` eval ` always leaves me feeling all yukky ' n ' stuff ... Please let me know if there's some other way .
1 E.g. , any approach I can think of involving the ` filter ` built-in is probably ineffiencient , since it would apply the criterion ( some lambda function ) by iterating , " in Python " , over the panda ( or numpy ) object ...
( with enough work we could lose the ` eval ` , this is simply proof of concept ) after which something like #CODE

Using append results in a horrible mess including NaNs and things for reasons I don't understand . I'm just trying to " rbind " two identical frames that look like this :
concat is more like rbind than append . See comment from @USER

As @USER points out , we can generalise to drop the last ` k ` columns with : #CODE

python pandas ( horizontal ) concat error with timeseries of the same name
Vertical concat is fine #CODE
btw , the result is correct if you concat two DF with same column names

How to apply " first " and " last " functions to columns while using group by in pandas ?
For example I can replace sets of values by their sum , or by their minimal or maximal value . I can do it in the following way : ` grouped.sum() ` or ` grouped.min() ` and so on .
In older version of pandas you could would use the irow method ( e.g. ` x.irow ( 0 )` , see previous edits .
Just in case it's useful to anyone , according to [ the docs ] ( #URL ) , ` irow ` is now deprecated ( ` x.iloc [ 0 ]` does the trick instead )
I'm confused with [ the docs ] ( #URL ); it states : ` Aggregating functions are ones that reduce the dimension of the returned objects , for example : mean , sum , size , count , std , var , sem , describe , first , last , nth , min , max . ` So what are they talking about ?
In some sense there's three types of mapping here : aggregation , apply and filter ( the above is kind of a filter , although it uses the agg verb ) . This is complicated thing is that you can use ** either ** agg or apply to get the ` .iloc [ 0 ]` job done , not sure why I used agg , apply is probably a better description . Since this post I fixed nth to work better so IMO that's the preferred solution here .

In the above illustration the result of the ` apply() ` function is a Pandas Series . And it lacks the groupby columns from the ` df.groupby ` . The essence of what I'm struggling with is how do I create a function which I apply to a groupby which returns both the result of the function AND the columns on which it was grouped ?
In the example you've appended , what's the purpose of the groupby ( it'll just find dupes ) , you can just do an apply to df itself and add that as a column : ` df [ ' func3 '] = df.apply ( lambda row : row [ ' col2 '] ** 2 , axis=1 )` . ?
I don't can't see an example where it makes sense to groupby all columns and apply , rather than just apply ( DataFrames apply can be very non-trivial and save to multiple columns ) . ( Also you don't need to create a dfout return variable , you can just return the calculation e.g. ` return df [ ' col3 '] **2 ` :) )
example updated ... and now it works ! Geesh . It appears that when the apply is on every row it does not return the keys , but if the apply results in aggregation it does return the keys
The best way to understand how your apply is going to work is to inspect each action group-wise : #CODE
for an arbitrary function which I am applying , the groupby seems to drop my grouping columns from the result and returns only a Series of answers . Clearly using the sum() method gets around that , but it's not helpful for custom functions which are not implemented as groupby methods . I added an example to my question to illustrate better .

I am interested in this question in the context of the ` groupby ` operation . If we apply this operation to a data frame , as a result we do not get another data frame . I wonder why not . Why not to have another data frame that has lists as values for some cells ?

Can also use iloc ( rather than ix ) , which plays nicer with int / float labeled data .

How to do a data frame join with pandas ?
I believe this is what ` Pandas.DataFrame.join ` is for , but I cannot formulate the code needed to join the data frames ` A ` and ` B ` correctly .
I think I would use ` merge ` here : #CODE

How can I add a title to a histogram created using the pandas hist function on Series in case the " by " argument is used ?
Another strange observation I made is that when I create a figure first , and then use the hist function , in the case of the by argument , the first figure is ignored and the hist functions creates a new one , which I cannot find the gca of matplotlib . For simple histograms again no problem .

For Example , I use ISO3 CountryCode's to merge from different data sources
Is the best solution to generate my own Dictionary() and then when it comes time to print or graph to then merge the country names in ? This is ok , except it would be nice for ALL of the dataset information to be carried within the dataframe object .
Agree , except that I use stack() and unstack() a lot to reshape the data ( i.e. to get balanced panels across years ) . I would prefer not to carry the redundant CountryName information around as a specified Index in these pivots but rather keep the Index to the minimum info for fast referencing . After all of the reshaping I could merge the country names back in as a separate column .
The index option has a format method that lets you apply a formatter in the form of a function : #CODE
This works quite well to replace the Index Codes prior to printing etc ...

a ) you specify the ` index_col ` rel . to the number of columns you really use -- so its three columns in this example , not four ( you drop ` dummy ` and start counting from then onwards )

Is there a way to concat , join or merge dataframes based on both the index and columns ? For example , suppose I have a list of dataframes and I want something like #CODE
where df.index should be the union of the indices in dfList ( ' outer ' join ) and df.columns should also be the union of the columns in dfList . I think all of the concat , join and merge methods just do a join on either the index or the column . I suppose a workaround is stack / unstack or reset_index ? I feel like I'm missing something simple .

The problem is that matplotlib doesn't handle multiline latex strings well . One way to fix it is to replace the newline characters in the latex string with spaces . That is , do something like this : #CODE

And then some dict replace nonsense : #CODE

How can I " unmelt " this dataframe back ? I want the ` Name ` column to be kept , but the values of ` variable ` field to be transformed into separate columns . The ` Name ` field is not unique , so I don't think it can be used as an index . My impression was that ` pivot ` is the right function to do this but it is not right : #CODE
I think this situation is ambiguous since the ` test ` dataframe doesn't have an index that identifies each unique row . If ` melt ` simply stacked the rows with ` value_vars ` SepalLength and SepalWidth , then you can manually create an index to pivot on ; and it looks like the result ends up the same as the original : #CODE
` len ( iris ) == 150 , len ( test ) == 300 ` . The original index on ` test ` has a unique value for each row in ` test ` , but not for each value in the * original * ` iris ` dataframe . My code ` range ( len ( test ) / 2 ) * 2 ` is two lists [ 0 .. 149 ] concatenated together , which can be seen in the output from ` test [ -10 :] ` ( the original and new indices don't match up ) .

Yes , I was just miss-reading the output . My off by 7 comment is on a line that is showing " -0700 " . Setting the timezone makes life easier . e.g. os.environ [ ' TZ '] = ' UTC '

One shortcut would be to replace ` df1 [ " label "] = len ( df1.index ) * [ " df1 "]` with ` df1 [ " label "] = " df1 "`
( rather than ` df1 [ " label "] = len ( df1.index ) * [ " df1 "]` . )

how to apply functions to grouped dataframes in Python pandas ?
I would like to apply a function per group that does something specific with a subset of the columns in ` grouped_iris ` . How could I apply a function that for each group ( each value of ` Name `) sums ` PetalLength ` and ` PetalWidth ` and puts it in a new column called ` SumLengthWidth ` ? I know that I can sum all the columns per group with ` agg ` like this : #CODE

There's a convenience function for not ` isnull ` , called ` notnull ` : #CODE
+1 for noticing the ` notnull ` . However , it seems that ` - np.isnan ( df )` is about 8 times faster : S
@USER interesting ! I suspect this is partially / mainly because ` notnull ` supports more ` dtypes ` than ` float ` ?

I'm wondering if someone can help me come up with a solution to a problem ; I basically have a pandas series of lists ( space delimited ) which I've created using pandas ' string operations on series - ` str.split ( ' ')` . I need to create another series of lists which is an intersection of each of those lists with another list .
and apply set intersection : #CODE

Beware , this solution makes the assumption that the data is zero-centered . A slightly more accurate answer : abs ( s - s.mean() ) > pd.rolling_std ( s , window=5 ) * 3

What I'm looking for is a way to " keep " specific columns in a dataframe and exclude the rest . The method you suggested is good for selecting first and last ** rows ** of a dataframe for any given columns , however what I'm after is a way to keep / drop columns using combined ranges / lists of columns in a slice .

The code above handles a business_code of length 6 but I'm finding that the business_codes vary in length and > 6 . I'm validating data state by state . Each state varies in their business_code lengths ( IL - 6 len , OH - 8 len ) . All codes must be padded evenly . So a code for IL that is 10 should produce 000010 , etc . I need to handle all exceptions . Using a command line parsing parameter ( argparse ) , and string.zfill .
` { ... } ` means replace the following with an argument from ` format ` ,

I don't think this behaviour is available in ` to_html ` , but one way is to just insert it in manually : #CODE

append the filtered to a list
concat at the end
Does this work if none of the rows in a chunk meet the filter criteria ? I was getting ' cannot append empty sequence ' exception .
i think you might have to do : if len ( filtered ): l.append ( filtered ); concat haandles Nones , prob bug if it doesn't handle empty sequences

Apply a function that returns the group row that has the index of the minimum ' q ' value .

I have written a function ( below ) that works with ` apply ` to perform this , but it is unacceptably slow . Instead , is there a way to use ` pandas.ols ` to directly perform this sort of cumulative regression ?
Here is the function I am able to use with ` apply ` on the identifier-grouped object : #CODE
pandas does offer cumsum ( cumulative sum ) and cumprod ( cumulative product ) that you could apply to a series . If you can break down you reduce your function into products and sums you could achieve what you are trying to do ...
Following on the advice in the comments , I created my own function that can be used with ` apply ` and which relies on ` cumsum ` to accumulate all the individual needed terms for expressing the coefficient from an OLS univariate regression vectorially . #CODE

Short and belated answer to a long question : it seem that you could use some help understanding long format data frames . Each value is unique , because there is only one ' runner ' with the given name per race . It can melt your brain at first , but is extremely powerful and essential for taking advantage of the capabilities of ggplot2 . Hadley Wickham explains this quite well in a few articles , for example : #URL ?

When I drop some variables from a DataFrame , the returned dataframe is as I expect except the index.name is removed . Why would this be ?
anyway to preserve the index.name during drop operations as the new
do you mean you want to drop the `' b '` row but keep the `' b '` label for the row with ` [ 5 , 6 , 7 ]` ?
No I would like to drop the ' b ' row but keep the entire Index in the new dataframe as the same name as before . ( i.e. a , c would be named ' first ')
Looking at the source code it doesn't look like drop saves the index name when it reindexes the axis .

Set the index for use in the ` join ` function that is coming next . #CODE
Join df and dfm . #CODE
Do an inner merge on ` df ` and ` dfm ` . The keys are the intersection of the common columns of ` df ` and ` dfm ` -- namely , GENE , DIRECTION and SOURCE . An " inner " merge keeps only those rows where both ` df ` and ` dfm ` share the same keys . So in the resultant merged DataFrame , ` df `' s GENE , DIRECTION and SOURCE must match ` dfm `' s GENE , DIRECTION and SOURCE . Thus , all rows with the wrong SOURCE are dropped : #CODE
AttributeError : ' module ' object has no attribute ' merge '` was I supposed to import something else for pandas to work properly ? Thanks !
Sorry ; I can't think of a plausible reason why ` pd.merge ` should raise an AttributeError . [ The merge function ] ( #URL ) has been a part of pandas at least since version 0.6 . If you are working from an interactive prompt or IDE , you may want to try saving the code to a file ( e.g. ` script.py `) and running ` python script.py ` from the command-line . That may eliminate some possible sources of errer .
AttributeError : ' module ' object has no attribute ' merge '` So should I try reinstalling my pandas and python programs ?
TypeError : merge() got multiple values for keyword argument ' how '` . So merge definitely works , but it does not seem to recognize that the value of ' inner ' has been supplied ? Which is odd because it should default to ` inner ` unless I'm mistaken . Sorry this is causing so many problems , and thanks for your help so far

Is it possible to apply user defined functions to series in pandas ?
If we have two series ` s1 ` and ` s2 ` we can apply arithmetic operations to them : ` s1 + s2 ` or ` s1*s2 ` . The arithmetic operation will be applied pairwise ( assuming that the two series have the same length ) as a result we get a new series . This feature makes a lot of things much more easier .
Now , I try to define my own operator and apply it to two series : #CODE
And I try to apply it to two series : ` f ( s1 , s2 )` . It does not work . It is expectable , to a certain extent , since the user-defined function doers not know how to treat series . So , my question is if there is an elegant way to do what I want to do ?

This seems to work , but to do what I want , I need to get the ABSOLUTE time difference , not the relative difference . However , just running ` abs ` or ` np.abs ` on it gives an error : #CODE
Am I approaching this the right way ? If so , how should I get ` abs ` to work , so that I can then select the minimum absolute time difference , and thus get the closest time . If not , what is the best way to do this with a Pandas time-series ?
Thanks - that helps . However , I don't think it'll do what I want all of the time . For example , if I have data at 10:25 , 10:32 and call ` asof ` with 10:30 , it'll return 10:25 , rather than 10:32 which is actually closer . That is , what I want is the closest time , not necessarily the closest time * before * the time I have given . Maybe I will need to do it using my manual method , but I can't see why ` np.abs ` won't work .
Yes please do make an issue on github . When you do , can you please provide clearer details , i.e. , can you cut your dataset down to a small size and put up a self-contained reproducible example ? That will help us debug the issue . Thanks

Apply function to each row of pandas dataframe to create two new columns
I want to create two new columns for this dataframe based on applying a function to each row of the dataframe . I don't want to have to call the function multiple times ( eg . by doing two separate ` apply ` calls ) as it is rather computationally intensive . I have tried doing this in two ways , and neither of them work :
Using ` apply ` :
Trying to apply this to the DataFrame gives an error : #CODE
I was then going to assign the values returned from ` apply ` to two new columns using the method shown in this question . However , I can't even get to this point ! This all works fine if I just return one value .
To make the first approach work , try returning a Series instead of a tuple ( apply is throwing an exception because it doesn't know how to glue the rows back together as the number of columns doesn't match the original frame ) . #CODE
The second approach should work if you replace : #CODE
The solution to the second approach works - thanks :-) . However , I can't get the first approach to work . Returning a series works , and I get a ' mini-df ' returned , but I can't seem to get the values returned from the ` apply ` function into the original dataframe . Using ` st [ ' a '] , st [ ' b '] = st.apply ( calculate , axis=1 )` doesn't work , and neither does wrapping the right-hand side in ` zip ( * )` . Any ideas about what I'm doing wrong here ?
Apply pandas function which returns multiple values ?

Assuming your ratios are positive , if you take ` c = 1.0 ` then ` log2 ( c + x )` will map [ 0 , inf ) --> [ 0 , inf ) .

The ` ix ` attribute is a special thing that lets you do various kinds of advanced indexing by label choosing a list of labels , using an inclusive range of labels instead of a half-exclusive range of indices , and various other things .
What you've requested in your code is actually ambiguous between " all labels up to an including 3 " and " all indices up to but not including 3 " , and labels always win with ` ix ` ( because if you don't want label slicing , you don't have to use ` ix ` in the first place ) . And that's why I said the problem is that you're not using a stop index at all .

The general idea now is to apply these calibration data to the measurements .

It looks like you may have some nulls in the column . You can drop them with ` df = df.dropna ( subset =[ ' item '])` . Then ` df [ ' item '] .value_counts() .max() ` should give you the max counts , and ` df [ ' item '] .value_counts() .idxmax() ` should give you the most frequent value .

Using ` irow ` is cumbersome , since it requires switching from ` ( )` notation to ` [ ]` notation ( ` irow ` returns a ` Series ` object )

And then you can use ` irow ` ( to get one location , or slice by integer location ): #CODE

To get the data in the shape you want you can use the unstack method : #CODE

For the example you give ( selecting ( ' B1 ' , 1 , 2 , ... )) you can use xs iso ix . In contrast to ix , xs can return a view on the data when using labels ( more details on ix returning view / copy see docs ) .
Now merge will work .

I wouldn't even think that they'd need a binary search here ... Under the assumption that both are sorted you can easily convince yourself that this can be done in O ( N ) time . ( Consider the merge stage of a merge-sort ) . I'd be interesting to see if a python implementation could beat this under those assumptions .

Pass ` numpy.argsort ` to the ` apply ` method instead of using it directly . This way , NaNs / NaTs persist . For your example : #CODE
@USER , The originals are datetime64 [ ns ] and the return index after the apply are objects . I ma using version 0.11.0.dev-3790f16 .
continued - except you could mask yourself ( eg isnull ( df )) then just ignore the true values - eg they have nans

I'm assuming the ` AttributeError ` is because pandas will not automatically try to convert / truncate the float values into an integer ?

Write a function to replace the end conditions based on window size ?
you could use the shift function , like so , #CODE

You can index by SubID and then use ` join ` to get what you want : #CODE
or , you could use ` merge ` to accomplish without indexing : #CODE

@USER , often times these loops are implemented directly in C , thus reducing iteration time -- take ` map ` for example . Vectorized methods are even faster .

Pandas will automatically insert row identifiers .

Next , we can join the created series into the original dataframe . You have to reset the index , dropping the shingle position level . The resulting series has the original index and an entry for each shingle . Merging this into the original dataframe produces : #CODE
Finally , you can do your groupby operation on Gender , unstack the returned series and fill the NaN's with zeroes : #CODE

To complement unutbu's answer , here's an approach using ` apply ` on the groupby object . #CODE

Apply different functions to different items in group object : Python pandas
I want to group a duplicate data at time ` 14:42 : 10 ` and apply different functions to ` exe_price ` and ` exe_vol ` ( e.g. , sum the ` exe_vol ` and compute volume weighted average of ` exe_price `) . I know that I can do #CODE
Is there a way to group and then apply different ( written by me ) functions to values in different column ?
Thank you for your quick response . I wonder since my ' grouped ' is now a panda DataFrameGroupBy object , I cannot really apply your fucntion directly can I ?
Apply your own function : #CODE

Can i use it replace all None objects in the dataframe without worrying about performance problem when storing dataframe into a hdf5 file ?

How to return max value of quantile cut range instead of quantile label

One of my goals is to precisely join different timeseries based on their millisecond counters .

I was surprised to see that there was no " rolling " function built into pandas for this , but I was hoping somebody could help with a function that I can then apply to the df [ ' Alpha '] column using pd.rolling_apply .

I have used ` resample ` to get a second series with monthly averages . #CODE

I have the following boxplot : #CODE
My question is : how can I change the whiskers / quantiles being plotted in the boxplot ? Assume I have a dataframe where I can compute the quantiles by rows or columns , as in : #CODE
can you explain the final format of dataf3 ? and what are those backquotes doing ? it'll be difficult to translate those ` ddply ` calls to pandas . I guess ` groupby ` should be used but I find this format very cryptic so it's hard to translate to python
Because " % " is an illegal characters in ggplot I had to enclose the names in backticks . If you drop the " % " sign , you can make the plot without ticks . I added it . Try to run it in R to see the final format of dataf3 . The rows are the species , the calculated quantiles and meanx are the columns , so a dataframe with 3 rows and 7 columns .

We can apply column operations and get boolean Series objects : #CODE

and then use ` loc ` to select those rows in the ` word ` and ` tag ` columns : #CODE
Alternative , you could use ` apply ` . ` apply `' s callable is passed a sub-DataFrame which gives you access to all the columns : #CODE
Using ` idxmax ` and ` loc ` is typically faster than ` apply ` , especially for large DataFrames . Using IPython's %timeit : #CODE

IMHO I think ` foo [ foo.b.str.match ( ' ( f . * )') .str .len() > 0 ]` is a pretty good enough solution ! More customizable and useful than startswith because it packs the versatility of regex in it .

I wanted to run this data through some basic feature selection algorithms , but most require numerical values only . I could map each of the unique classifications to a new field ( feature ) with a boolean mapping scheme , but then i'll generate an extremely large number of new features and I'm not sure if that will give a meaningful output . Admittedly the best solution might be to run the data through a decision tree , but wanted to see if there were any other strategies that others have come up with in the community for handling data sets of mostly nominal data that have been successfully used on real world applications .

I think I will need to resample my data ... but I don't know how !
I need to align data in order to have same ' x ' column for the 2 dataframes .
It seems that ` Series.interpolate() ` doesn't use index value to interpolate the NaN values . So I changed the answer to use ` numpy.interp() ` .

I'm running into a strange issue ( or intended ? ) where ` combine_first ` or ` update ` are causing values stored as ` bool ` to be upcasted into ` float64 ` s if the argument supplied is not supplying the boolean columns .
EDIT : I started tinkering around a little more . The plot thickens . If I insert one more command : ` test.update ([ ])` before running ` test.update ( b )` , boolean behavior works at the cost of numbers upcasted as ` objects ` . This also applies to DSM's simplified example .

Transpose Multidimensional Table
Due to the very large nature of the problem , I'm using the pandas as the main Database API as its very easy to apply function to column .
Then stack the DataFrame , and drop our dummy columns index . #CODE

pandas join / merge ' Reindexing only valid with uniquely valued Index '
I get a very robust error on a join operation . I attempted a merge ( left_index , right_index ) as well with the same result .
Length : 89 , Freq : None , Timezone : None
my code ( cut from larger code ) #CODE
I use merge and joins consecutive on a basic DataFrame . At one point I start to get errors when attempting the next merge / join . I could not reproduce this in a simple test , but I saved the dataframes before the problems start .
Once the error occurs I can not get any merge or join statement to be successful . At first I did not see that the error was linked to a repeated merge / join action . Any single merge / join of the latest set works now . As soon I need another merge / join I get the same error . Struggling for several days now ...
Hoi , I found duplicate column names in the dataframe ... Is this a bug , accidentely dupicated columns names , triggering an error when columns are reindexed ??? by a merge / join way after the duplicated columns enter the dataframe ???
I manually prevented the duplicated columns as a test and now the merge / join work !!! The error was related to the columns ... It would have been nice if the error message reflected this ... I am very supprized that duplicated column names are supported . Or is this a bug ?

I can convert a pandas string column to Categorical , but when I try to insert it as a new DataFrame column it seems to get converted right back to Series of str : #CODE
Guessing this is because Categorical doesn't map to any numpy dtype ; so do I have to convert it to some int type , and thus lose the factor labels -> levels association ?

2013-04-10 - 1.374779 Freq : D , Name : E
Thanks - works perfectly . The transpose function is not fully documented for panels but very help ful advice . Many thanks

If I understand correctly , it appears you are trying to create an integer index with unique values for each combination of columns A and B . Unless you require the integer index for a specific reason , you can simply create a MultiIndex using columns A and B with ` df.set_index ([ " A " , " B "]) .sort() ` which will allow you to do all of the same selection and slicing the integer index would using ` xs ` and ` ix ` . Example : ` df.ix [ " a "] .ix [ " y "]` or ` df.xs ( key= " x " , level=1 )` .

I understand this can be done by adding an extra column that contains values to allow grouping , for example you could join the above DataFrame to ` [ 1 , 1 , 1 , 2 , 2 , 2 , 3 , 3 , 3 ]` and groupby the added column . But it seems like it shouldn't be necessary to add an extra column for this operation .

Apply function to pandas groupby
This code throws an error , ' DataFrame object has no attribute ' size ' . How can I apply a function to calculate this in Pandas ?
` apply ` takes a function to apply to each value , not the series , and accepts kwargs .

the index outputed by ` numpy.unique ` is sorted by value , so the smallest value 10 is assigend to index 0 . If you want this result by using ` factorize ` , set ` sort ` argument to ` True ` : #CODE

I have a pandas DataFrame that includes a pipe-separated string in one of the fields . I've split this into a list inside an ` apply ` and added it to the DataFrame . The number and content of the values in the pipe-separated string vary . #CODE
The quickest way to get started with that is to ` stack ` your dataframe : #CODE

But A ) I'm not sure how to apply the conditional logic and B ) I have to apply the logic to each column iteratively rather than to the dataframe as a whole .
How can I apply conditional logic to the non-null values of a dataframe , preserving the nullity of the other fields ?

I have a dataset at a daily interval and want it to resample to what sometimes ( in scientific literature ) is named dekad's . I dont think there is a proper English term for it but its basically chopping a month in three ~ ten-day parts where the third is a remainder of anything between 8 and 11 days .

Interestingly enough , very often ` len ( unique() )` is a few times ( 3x-15x ) faster than ` nunique() ` .
You mean this ? ` .CLIENTCODE .apply ( lambda x : len ( x.unique() ))` , from [ here ] ( #URL )

Part of using ` pandas ` is using vectorized ops to avoid for loops . As Paul says , though , without an SSCCE it's a bit of a headache ; with one , you'd probably have an answer by now . : ^ ) You can always replace the source data with equivalent random data .
If you replace those screenshots with short _typed_ samples , we can copy / paste the data try out your code .

or ( ix will work here as well , this is just more explicit ) #CODE

Conform your data to this index , and " fill forward " to downsample the way you show in your desired result . #CODE

My question is : Is there a more efficient .map() which takes the Series and performs map on only unique values ? ( Given there are a LOT of repeated countrynames )
There isn't , but if you want to only apply to unique values , just do that yourself . Get ` mySeries.unique() ` , then use your function to pre-calculate the mapped alternatives for those unique values and create a dictionary with the resulting mappings . Then use pandas ` map ` with the dictionary . This should be about as fast as you can expect .
Memoizing ` guess_country ` will certainly speed it up , but over 30 million data points it may be perceptibly slower than creating a dictionary . Even if you memoize ` guess_country ` , ` map ` will still call it for every single point , which means you suffer the extra function call overhead every time . By precalculating unique values and only calling the function on those , you limit the function call overhead to only the number of unique values . Whether the difference matters will likely depend on how many distinct values you have .

You could always grab the hourly data_array and flatten it . You would generate a new DatetimeIndex with hourly freq . #CODE
I'm assuming that read_csv is parsing the ' Date ' column and making it the index . We change to frequency of ' D ' so that the ` new_ind ` lines up correctly if you have missing days . The missing days will be filled with ` np.nan ` which you can drop with ` s.dropna() ` .

The solution without Panel ` df = pd.concat ( map ( pd.DataFrame , d.itervalues() ) , keys =d .keys() ) .stack() .unstack ( 0 )` if ok ... I just need to transposate DataFrame with ` df = df.T ` and ' data1 ' ... ' data4 ' are " first level " index . So I can get a DataFrame which contains only data1 with ` df [ ' data1 ']`

As we want a zero-based index , we set the first cell to ` 0 ` , and we append that column to the index of ` df ` to create the ` MultiIndex ` : #CODE

In this case , how can we execute the groupby on values of A , then apply this computation to each individual group , and finally plot the D values for the two groups ?

You could have provided data I can cut / paste in my terminal . That would have made giving you a complete answer easier .

I want to collapse the DataFrame so that col2 will be unique . In col1 ( and all the other numerical columns ) , I want to put the median of all the values where col2 was equal .

Rolling apply question
For each group in the groupby object , we will want to apply a function : #CODE
We want to take the Times column , and for each time , apply a function . That's done with ` applymap ` : #CODE
Given a time ` t ` , we can select the ` Value ` s from ` subf ` whose times are in the half-open interval ` ( t-60 , t ]` using the ` ix ` method : #CODE

However , when I drop this column from the pd.read_csv chunks and append to my HDFStore , I still get the same Exception . When I try to append each column individually I get the following new exception : #CODE
My suggestions for this would be to either truncate the data in subsequent chunks ( with a warning ) or allow the user to specify maximum storage for the column and truncate anything that exceeds it . Maybe pandas can do this already , I haven't had time to truly dive deep into ` HDFStore ` yet .
I've tried to debug and inspected the items in the stack trace . This is what the table looks like at the exception : #CODE
Once I have the dict of max string-column lengths , I try to pass it to ` append ` via the ` min_itemsize ` argument : #CODE
trouble shoot by trying the store the columns one at a time until u get a failure ; the error message is the last field . pass data_columns=True to append ( in practice you won't do this as u prob don't need to query on ALL the columns )
The next logical extension of what I'm trying to do is determine the max length of object fields so I can pass a dict to min_itemsize of the append method . I added update 4 with a possible bug . Thanks .
this is not valid . you are telling it a particular column should have a minimum length , but all of the strings are in a single block , as u r not specifying data_columns ( it is a bug that I let u specify this - and error msg should be better ) , you only can have a min per column if they are separate fields , otherwise the min is per column in a ( same for each one ) block . just use the min of the min ( note that you are essentially looking thru your data twice in order to do this ) . better to just pick a reasonable number ( separate issue is then we could allow u to truncate beyond that )
#URL will provide an error for your code here ( because you are trying to min_itemsize with a column that is not queryable ) . Also , you can use lib.max_len_string_array ( s.values ) for a quick max in your apply ( faster and you don't need to test for object type )
so with a chunksize of 10,000 the append operations failed because chunks 1 and 2 had

If the index isn't unique ` concat ` won't be able to align the columns properly back together since there will be multiple values for some indices , if that makes sense .
make sense ... and unfortunately there are multiple dates for the same basket . When you're suggesting to use resample , is it within the " handler " or before for the dataframe ? I hope i've been asking something that make sense , as it not really clear what resample is actually doing ( need to get back home first ! )
Having checked on an example , it seems to work also for duplicate indices . ` concat ` apparently aligns data with the same index in the order they appear . I edited my answer to account for that .

Could you post the full traceback ? Also , could print out the row from each DataFrame where the error occurs . Finally , is ` len ( reobsmaf ) == len ( remodmaf )` ?

Edit : As a workaround , you can just drop the DatetimeIndex from the DataFrame and pass it the numpy array . It makes prediction a little trickier date-wise , but it's already pretty tricky to use dates for prediction when there is no frequency , so just having the starting and ending dates is essentially meaningless . #CODE

How to index into a pandas multindex with ix
Can I acheive this with ix ? e.g. something like : #CODE

resulting_i = i + argmin ( abs ( obs1 [ 1 , i ] - obs2 [ 1 , #URL )` ? Assuming that you are not performing another loop here , I don't believe KDTrees would speed things up too much .
I was wondering of anybody replied to you . Thanks for posting more details but unfortunately it's still unclear to me how your current algo . works .. I'm not a python developer so maybe somebody else will chime in . I would suggest though to present your algorithm in pseudo code instead . Or maybe ask on a more CS stack exchange ..
where ` indices ` will contain the column indices in ` obs2 ` corresponding to each observation in ` obs1 ` . Note that I had to transpose ` obs1 ` and ` obs2 ` .

I don't understand what is your question . Also , what is that ` apply ` method ?

and the " reshape " feature ( does not apply to 3-d case though ... ): #CODE

How to apply custom column order to boxplot ?
I can get a boxplot of a salary column in a pandas DataFrame ... #CODE
How can I apply my custom column order to the boxplot columns ? ( other than ugly kludging the column names with a prefix to force ordering )
A simple , brute-force way would be to add each boxplot one at a time . #CODE
not where I was headed . I typically just use ` apply ` with a hard-coded lookup table . see my edited response for a different approach , though .

The resample function start from the earliest date to last date . The output that i get is as follows : #CODE
whereas i want the algorithm to resample in the reverse order . I want the output something like this : #CODE

My idea was to use ` cut ` and ` value_counts ` to first compute the number of pairs having a distance inside bins , which works fine : #CODE

My goal is to read EURUSD data ( daily ) into a time series object where I can easily slice-and-dice , aggregate , and resample the information based on irregular-ish time frames . This is most likely a simple answer . I'm working out of Python for Data Analysis but can't seem to bridge the gap .

Pandas : use ` stack ` inplace
in pandas , I have a big ` DataFrame ` and I want to use the stack function without having to copy ( memory limit ): #CODE
this could be done if the Frame has a multi-index ( a simple index would yield a Series ) , but the stack operation by necessity almost always has to copy , see : #URL
FYI , there are various methods for getting around issues like this , here's one , slice into pieces perform your operations and concat , #URL

I need to read in a dataset that is too large to fit into memory . I would like to import the file using pd.read_csv and then immediately append the chunk into an HDFStore . However , the data type inference knows nothing about subsequent chunks .

`` .ix `` will return a view or a copy , * depending on numpy* , which means practially , the layout in memory is the issue . before the astype , this was a single dtype , so a single block of memory , so you get a view , after the astype , somethings are copied ( by numpy ) , so you can no longer get a view , so you get a copy , weird , but it doesn't provide ' guarantees ' . If you use the `` ix [ row , col ] = value `` syntax you can avoid this whole issue ( or in 0.11 , `` loc [ row , col ]`` and `` iloc [ row_number , col_number ]`` . HTH

Pandas Pivot tables row subtotals
I tried with a pivot table but i only can have subtotals in columns #CODE
I can achieve this on excel , with a pivot table .
Then apply the groupby function and add a column City : #CODE
We can append the original data to the summed df by using append : #CODE

I have the following code attempting to plot a timeseries . Note , I drop the second column because it's not relevant . And I drop the first and last rows . #CODE

how to perform an inner or outer join of DataFrames with Pandas on non-simplistic criterion
we would like to produce a SQL-style join of both dataframes using a non-simplistic criteria , let's say " df_b.c > df_a.a " . From what I can tell , while ` merge() ` is certainly part of the solution , I can't use it directly since it doesn't accept arbitrary expressions for " ON " criteria ( unless I'm missing something ? ) .
my current approach for inner join is to produce a cartesian product
for outer join , I'm not sure of the best way to go , so far
I've been playing with getting the inner join , then applying the negation
Edit . HYRY answered the specific question here but I needed something more generic and more within the Pandas API , as my join criterion could be anything , not just that one comparison . For outerjoin , first I'm adding an extra index to the " left " side that will maintain itself after I do the inner join : #CODE
then we do the cartesian and get the inner join : #CODE
How would you produce a " join " of df_1 and df_2 on " c > a " ? Would
How would you produce the " left outer join " of same ?
Inner join , because this only calculate the cartesian product of ` c ` ` a ` , memory useage is less than cartesian product of the whole DataFrame : #CODE
to calculate the left outer join , use ` numpy.setdiff1d() ` to find all the rows of ` df_a ` that not in the inner join : #CODE

I have finally decided to use apply which I understand is more flexible .
I finally decided to use apply .
apply is more flexible than agg and transform because you can define your own function .

method in Pandas . However the vertical x labels get cut off . when I try to change the bottom variable by saying
So the method is sending it's own value for bottom and I don't know how to replace that .

Accessing Row from Previous Day in Pandas Dataframe with Apply
You could do a ` shift ` first : #CODE

The ` rename ` function should convert the the dictionary to a mapper and apply it to each index . However , for the ` MultiIndex ` case , it only walk through each tuple but not each index .

I want to resample a DataFrame with a multi-index containing both a datetime column and some other key . The Dataframe looks like : #CODE
Because resampling is only allowed on datatime indexes , i thought unstacking the other index column would help . And indeed it does , but i cant stack it again afterwards . #CODE
Strangely enough , i can stack the other column level with both : #CODE

Trying to use replace method with pandas
I am trying to do a simple replace with pandas : #CODE
The ` replace ` method was added in version 0.9.0 ( see release notes ) .

if your set_value operation increases the size of the frame it returns a new object ; it is possible there is a bug lurking . can u give me a better idea of why u r thing to do , maybe I can give u r a better answer . if u r setting a lot of values like this it will be somewhat inefficient . oftentimes it's better to create a new frame and perform a join concat operation
... I am thinking doing some kind of merge of our answers would provide the best one . It is good to know that sorting is necessary , but it doesn t seem to solve the problem in this particular case ...
Should I apply the ` df.sortlevel ( level= ' Transition ')` after each ` df.set_value() ` call ?

These columns all contain an identical kind of data , and I'd like to stack them into a single series , ergo : #CODE
From here , I can't quite figure out how to reindex my series such that the indexes go from 0 to ` len ( s )` . I have tried the following , to no avail : #CODE

Drop rows of pandas dataframe that don't have finite values in certain variable ( s )
I cannot see what the built-in function is for the following simple but seemingly common / useful task : Drop rows which have no value for any of my key columns . #CODE

Then one could run ` eval ( In [ 10 ])` to get the updated value for ` E ( x )` . IOW , ` In ` can be thought of as an " addressable " form of one's interaction history , one that that allows one to get to a particular input expression without having to do a linear ( reverse ) traversal of the history with the up-arrow .

I have a rather big dataset around 5287657 with around 15 columns . I was trying to create a pivot table and it gives me a ` MemoryError ` when trying to create the DataFrame .
I think at the end of the video Wes suggests this was mainly because of the bespoke date parser being used in read_csv , I don't see how this would help in a pivot . How much memory in python using before the pivot ?
@USER :P ython was using around 30-35MB of memory after reading the entire data set to a dictionary . Once I start the assigning the columns and rows to the pivot table , then memory starts increasing and it fails once it reaches approximately 1.72GB of Memory .
Interesting ... could you share the code you are using to do the pivot ? ( Just so we're talking about the same thing . )
@USER : And I am using the following to create the pivot table LTable =p andas.tools.pivot.pivot_table ( df , values =[ ' Rat ' , ' B_AFLOW ' , ' B_VFLOW ' , ' B_PFLOW ' , ' C_AFLOW ' , ' C_VFLOW ' , ' C_PFLOW ' , ' Rat1 ' , ' Rat2 '] , rows =[ ' OverLine ' , ' Label '] , cols =[ ' Input_FileName '])
@USER but you are crashing before you pivot ? At the moment you are reading in files into a dictionary the converting ( these two steps are the memory inefficient bit ) , I'm suggesting creating a list of DataFrames first ( then concating to one df ) .

Map a pandas DataFrame index
What I really wanted was to do something like a boxplot : #CODE
Calling ` map ` will be magnitudes slower . #CODE

how to merge two dataframes of different index level ( by row )
How can I merge t1 and t2 by rows ? #CODE

Map your Series / Dataframe before writing to the csv file : #CODE

How about shift it first and then add them together ? #CODE

pandas drops index index on merge in Python ?
I am merging two dataframes using ` merge ( ..., how= ' left ')` since I want to retain only entries that match up with the " left " dataframe . The problem is that the merge operation seems to drop the index of my leftmost dataframe , shown here : #CODE
Before the merge , ` df1 ` was indexed by ` id ` . After the merge operation , there's just the default numeric index for the merged dataframe and the ` id ` field was dropped . How can I do this kind of merge operation but retain the index of the leftmost dataframe ?
edit : I could as a hack do : ` df1.reset_index() ` but merging and then set the index again , but I prefer not to if possible , it seems like merge shouldn't have to drop the index . thanks .
You could try indexing ` df1 ` and ` df2 ` by name instead of id , and then use ` join ` instead of ` merge ` like this : ` df1.join ( df2 )` which will preserve the index .
Why don't you set_index after the merge ?
I would but it drops the column that I want to be the index after the merge , namely ` id `
If you didn't call set_index before merge , it should still be around in df1
Preserve index after merge is a little bit strange while the method is " outer " . So I think it's better not to preserve it .
You've already pointed out doing a reset_index before the merge and a set_index afterwards , which works . The only way I know of to preserve indices across a merge is for the merge to involve an index on at least one of the data frames being merged . So here , you could do : #CODE
to merge df2's index , which we've taken from its ' name ' column , against the ' name ' column on df1 .

Is there a better way to align pandas data in this way ?
The data will not be aligned so a simple concat will not be enough . Although if you create the frames in such a way its a simple matter to loop through setting indices and combine_first . I was really hoping there was a simple pandas way to to do this .
just reindex the frames before you concat in that case
Perhaps concat every file / frame and create a pivot table from the final DataFrame ? #CODE

I'm trying to join two dataframes in pandas to have the following behavior : I want to join on a specified column , but have it so redundant columns are not added to the dataframe . This is analogous to ` combine_first ` except ` combine_first ` does not seem to take an index column optional argument . Example : #CODE
If you are trying to merge columns from ` df2 ` into ` df1 ` while excluding any redundant columns , the following should work . #CODE

The whole point in this example is to ` append ` a ` DataFrame ` with some missing values to a ` HDFStore ` . When I use the example code I end up with an ` atom type error ` . #CODE

the condition column is a bool ) #CODE
The diff column are now timedelta64 [ ns ] , so you want integers in minutes

although you may want to play around with the ' shape ' and matching ` strides ` to get the exact windowed shape you are after .

HDF5 keeps a B-tree in memory that is used to map chunk structures on

Group by k1 , select column k2 and apply a lambda function . The lambda gets frequency counts for each level of k2 within k1 and then we divide by the count of k1 : #CODE

PANDAS drop a range of rows from df
I want to drop m number of rows from the bottom of a data frame . It is integer indexed ( with holes ) . How can this be done ?
If you want to remove some middle rows , you can use ` drop ` : #CODE

I knew it would work if I assigned an entire array to ` df [ ' c ']` but is there no way to assign just particular elements and have it infer the rest are ` NaN ` ? It looks like I have to explicitly construct an array of size ` len ( df )` with nans and non-values ...

That would have been a better example to give . : ^ ) But in the general case , there's no guarantee that there's only one matching value , right ? So I suspect that the result is going to be fundamentally array-like . You can shorten the syntax -- using ` max ` , for example , or ` iget ` -- but other than that I'm not sure . Maybe Hayden will have something clever ..

And then you can pivot ` df1 ` . By default it uses the mean of the aggregated values , but we want the sum : #CODE
First using ` apply ` you could add a column with the signed shares ( positive for Buy negative for Sell ): #CODE
Use just those columns of interest to you and unstack them : #CODE

Consolidate irregular date values in Python ( Pandas )

I use this ( somewhat kludgy ) R function to merge data.frames and keep the order of one of them : #CODE
In pandas a merge resets the index , but you can easily work around this by resetting the index before doing the merge . Resetting the index will create a new column called " index " which you can then use to re-create your index after the merge . For example : #CODE

Therefore , if ` df ` does not have a unique index , you must make the index unique before proceeding as above . Depending on DataFrame , sometimes you can use ` stack ` or ` set_index ` to make the index unique . Or , you can simply reset the index ( so the rows become renumbered , starting at 0 ): #CODE

I've tried the pivot method , stack / unstack , etc . but these methods are not what I'm looking for . I'm really quite stuck at this point and any help is appreciated .
Because you have a MultiIndex in place already , ` stack ` and ` unstack ` are what you want to use to move rows to cols and vice versa . That being said , ` unstack ` should do exactly what you want to accomplish . If you have a DataFrame ` df ` then ` df2 = df.unstack ( ' minor ')` should do the trick . Or more simply , since by default ` stack ` / ` unstack ` use the innermost level , ` df2 = df.unstack() ` .
Aargh ; beat me to it . ` pivot ` should work too , after resetting the index , i.e. ` df.reset_index() .pivot ( " major " , " minor ")` ( with or without a ` , " price ")` or ` [ " price "]` , depending on how important whether the name is still floating around is . )
Ha nice , I was obviously not using unstack correctly , thanks .

pivot table subtotals
I get a pivot table with Grand Total row only ( " All ") .
Is there any efficient way to insert rows with subtotals into this table just as Excel pivot table with Field Setting > Layout Print > " Show item labels in tabular form " allows to do ?
what language do you want to perform the pivot in ?

I want the data arranged so that each column represents 1 month . Each row in the table represents a different time series . Does that make sense ? I would demonstrate with a table but I have no idea how to insert a table on this website .

E.g. , like ` running_sum =d ata1.groupby ([ ' Bool ' , ' Dir ']) .cumsum() ` - ( Doesn't work )
The reason you cannot simply use ` cumsum ` on ` data3 ` has to do with how your data is structured . Grouping by ` Bool ` and ` Dir ` and applying an aggregation function ( ` sum ` , ` mean ` , etc ) would produce a DataFrame of a smaller size than you started with , as whatever function you used would aggregate values based on your group keys . However ` cumsum ` is not an aggreagation function . It wil return a DataFrame that is the same size as the one it's called with . So unless your input DataFrame is in a format where the output can be the same size after calling ` cumsum ` , it will throw an error . That's why I called ` sum ` first , which returns a DataFrame in the correct input format .
Thanks for the solution and explanation . The way I looked at it was to examine the structure of data3.groupby ( level =[ 0 , 1 , 2 ]) .sum() which clarified why its necessary to append .groupby ( level =[ 0 , 1 ]) .cumsum() . I never thought data3.cumsum() would work .
As the other answer points out , you're trying to collapse identical dates into single rows , whereas the cumsum function will return a series of the same length as the original DataFrame . Stated differently , you actually want to group by [ Bool , Dir , Date ] , calculate a sum in each group , THEN return a cumsum on rows grouped by [ Bool , Dir ] . The other answer is a perfectly valid solution to your specific question , here's a one-liner variation : #CODE
Note the repeated dates , but this is doing a strict cumulative sum internal to the rows of each group identified by the Bool and Dir columns .

First resample the data frame into 1D intervals . This takes the mean of the values for all duplicate days . Use the ` fill_method ` option to fill in missing date values . Next , pass the resampled frame into ` pd.rolling_mean ` with a window of 3 and min_periods=1 : #CODE
I just had the same question but with irregularly spaced datapoints . Resample is not really an option here . So I created my own function . Maybe it will be useful for others too : #CODE
Hope my suffix conventions are readable : _s : string , _i : int , _b : bool , _ser : Series and _df : DataFrame . When more suffixes present type can be both . #CODE

Also can use ` df.iloc [ np.random.permutation ( np.arange ( len ( df )))]` if there's dupes and stuff ( and may be faster for mi ) .
Note that ` numpy.rollaxis ` brings the specified axis to the first dimension and then let's us iterate over arrays with the remaining dimensions , i.e. , if we want to shuffle along the first dimension ( columns ) , we need to roll the second dimension to the front , so that we apply the shuffling to views over the first dimension .

The ` bar ` method takes a parameter ` align ` . Set this parameter as ` align= ' center '` . ` align ` aligns the bars on the center of the x values we give it , instead of aligning on the left side of the bar ( which is the default ) .

I get a ValueError ( " Currently , you need to give a freq if dates are used . ") , which according to [ #URL should be allowed to be ' None . ' I include a frequency ( ' D ') : ` model= statsmodels.tsa.arima_model.ARMA ( c , freq= ' D ')` and it works , but when I try to do ` results= model.fit ( 24 )` it throws a TypeError ( "' int ' object has no attribute ' getitem ') . Any thoughts on this ?
The freq issue should be fixed in master , but is not yet released yet . What version of the code are you running ?

how does boxplot in pandas / python work ?
I found this link and I'm trying to understand how boxplot works .
I would expect boxplot to want 4 values for each box however it seems to construct a box for each 10 points and I was wondering what is going on in the background ..
Boxplot creates five boxes corresponding to each column of the ` DataFrame ` . Each box depicts the median ( red line ) , 25th percentile ( lower edge of the box ) , 75th percentile ( upper edge of the box ) , and the most extreme observations ( the whiskers ) .

the feature is very useful , now i have to maintain one set of book-keeping code to hold and align the meta data . I don't know too much about the pandas internal , but a primitive reserved field for each dataframe object , a dict for example , will help me a lot ..

You could also use ` ix ` if you set ` TYPE ` as the index first : #CODE

I must pass the columns to drop the duplicates , right ? But the duplicates I wanna drop are in one of the levels of an index , not as a column .
df.reset_index() would insert the index values as columns . But I agree the answer below seems ' more ' correct
which will drop the datetime field to a column before the groups are grouped by groupby , therefore it will remain in the dataframe in the result .
If you have a MultiIndex where level 0 has values like ' abc-8182 ' and level 1 has timeseries values , then the above code should drop that duplicate row and keep the first in your example . You're saying that's not happening ?

Still not sure what you're asking , but if you have a function for one value , you can then use Series / DataFrame ` apply ` or index's ` map ` , e.g. ` df.index.map ( lambda t : t.value )` ?

Pandas has proven very successful as a tool for working with time series data . For example to perform a 5 minutes mean you can use the ` resample ` function like this : #CODE
You can also use the standard time functions yearmon() or yearqtr() , or custom functions for both split and apply . This method is as syntactically sweet as that of pandas .

Then ` len ( v1 ) == len ( v2 )` and these can be plotted as CDFs of ` a , b ` on the same scale .
The way we use it in some goodness of fit tests is to stack the arrays , so they are defined on all points , points from both arrays .

can u give a small example of what kind of functions u r going to apply with the group ? and a small example frame would be helpful .

at which point I'm really running out of ideas ... is there some way stack and unstack might be able to help me ?
An alternative would be to unstack the State and City columns before resampling , but i doubt if thats more efficient .
An alternative using stack / unstack #CODE
Possible pandas bug - stack ( level =[ 2 , 1 ]) worked , but stack ( level =[ 1 , 2 ]) failed

For a series , you can do it with ` append ` , but you have to create a series from your value first : #CODE
For a DataFrame , you can also use ` append ` or ` concat ` , but it doesn't make sense to do this for a single cell only . DataFrames are tabular , so you can only add a whole row or a whole column . The documentation has plenty of examples and there are other questions about this .

It goes a bit against Pandas ' philosophy , which seems to see ` Series ` as a one-dimensional data structure . Therefore you have to create the ` Series ` by hand , tell them that they have data type `" object "` . This means don't apply any automatic data conversions .
We can now merge these two ` DataArray ` on their common ` x ` dimension into a ` DataSet ` : #CODE

Pandas dataframe merge issue
I am learning python and pandas via Wes McKinney's Python for Data Analysis . One of the examples in Chapter 2 is a merge of MovieLens data on movie_id that is not working . I think the issue is that in ratings the movie_id is an int64 and in movies it is an object . The merge returns an empty data frame .
Unfortunately , the type isn't changed and the merge still returns an empty set . I am running pandas 0.10.1
Strange , then . My ` movie_id ` Series in the movies dataframe is ` Name : movie_id , Length : 3883 , dtype : int64 ` , which seems right , and the merge behaves the way you'd expect .
See if your merge now works .

Pandas DataFrame concat vs append
I have a list of 4 pandas dataframe containing a day of tick data that I want to merge into a single data frame . I find can't seem to understand the behavior of concat on my timestamps . See details below : #CODE
Using append I get : #CODE
Using concat I get : #CODE
Notice how the index changes when using concat . Why is that and how would I go about using concat to reproduce the results obtained using append ? ( Since concat seems so much faster ; 24.6 ms per loop vs 3.02 s per loop )
So what are you doing is with append and concat is almost equivalent . The difference is the empty DataFrame . For some reason this causes a big slowdown , not sure exactly why , will have to look at some point . Below is a recreation of basically what you did .
I almost always use concat ( though in this case they are equivalent , except for the empty frame );

+1 , it would be really nice to see the performance diff ...

inner join A and B , then ffill on B #CODE

I'm not sure there's a vectorized hook , but you can use ` apply ` , anyhow : #CODE

According to this answer by Pandas ' author , ` set_value ` returns a reference to a new object . You need to use ` concat ` or ` append ` to add rows to a data table . See section " Merge , join , and concatenate " in the Pandas manual .

The general use case behind the question is to read multiple CSV log files from a target directory into a single Python Pandas DataFrame for quick turnaround statistical analysis charting . The idea for utilizing Pandas vs MySQL is to conduct this data import or append + stat analysis periodically throughout the day .
The ` append ` method on an instance of a DataFrame does not function the same as the ` append ` method on an instance of a list . ` Dataframe.append() ` does not occur in-place and instead returns a new object . #CODE
or you can use ` concat ` : #CODE

I'm an R user and I cannot figure out the pandas equivalent of match() . I need use this function to iterate over a bunch of files , grab a key piece of info , and merge it back into the current data structure on ' url ' . In R I'd do something like this : #CODE

Would I be able to essentially do he same thing for weekly open etc ? I'd have to resample the data forst into weekly ohlc .
Converting hourly ohlc to weekly ohlc might take a little bit of coaxing as you would have to sample open , high , low , close differently ( ' how ' parameter of resample )

Another approach could involve creating another column - ' Q ' with values [ ' Q1 ' .. ' Q4 '] . You could then pivot on this column to have columns Q1 .. Q4 . It would then be easy to add a previous year column ' PQ4 ' by Q4.shift . Lastly run a custom avg for each row

Join em up #CODE
In essence you are flattening the blink frame to a series that you then can apply to each of the trial
this looks great ! I just have one question though : where are the values in your very first line coming from ? What's the [ 0 , 10 ] and [ 5 , 15 ] ? Are those timepoints ? If so , doesn't that mean I can just replace them with corresponding the columns from my ` trials ` dataframe ?
First , sort the ` blink ` by ` tstart ` before you check each row in ` trials ` , merge the overlapped ones , it takes ` O ( n log n )` , take a look at this
Use the binary search to insert the start in the sorted ` tstarts `
Use the binary search to insert the stop in the sorted ` tstarts `

Did you try using ` ix ` instead of ` xs ` ( e.g. , ` pd_price.ix [ pd_prices_index , s_symbol ] = size `) ?

You could use join , but it really depends on what sort of result your looking for .

Current fix : create a new time vector : time = [ t0 + dt.timedelta ( hours=i ) for i in range ( len ( nts.data ))]

I think you can do this using ` .shift() ` , which can shift a series forward or backward ( defaulting to one forward . ) You want to keep rows if they're not the same as the next ones , so something like : #CODE

pandas time series join
The keys are not under my control and not a perfect equal join where one and only one match is found . My though is to do the join then remove negative durations because I know triggers must precede actions . Then I need to remove all but the lowest duration for each key . See sample code . I want all the action fields other_data in the result_df . ( Only for the joins of minimum duration )
I know that my result may include some artificially short durations but I suspect due to the nature of the join key reuse that this will happen seldom and do not pose a problem for the rest of the analysis . Previously I had been using arbitrarily selected max durations to toss out erroneous join data but tossing these outliers will be problematic because high outliers are what I am most interested in . #CODE
1 . Join like normal
3 . order the result DF by the join key and the duration [ descending order ] ( will look into that )
4 . create a new Boolean field making use of the shift ( 1 ) comparing the current record to see next has a lower duration but the same key . ( also need to play around with this idea )
5 . select out all the records where my new bool field indicates that it is the lowest for its key .
you also might find some of these recipes useful : #URL e.g. you may actually need to look several periods ( and not just 1 on your shift ) , but you idea looks about right

Is it possible to insert a row at an arbitrary position in a dataframe using pandas ?
What I would like to do is insert a row at a position specified by some index value and update the following indices accordingly . E.g. : #CODE
You could slice and use concat to get what you want . #CODE
This will produce the dataframe in your example output . As far as I'm aware , concat is the best method to achieve an insert type operation in pandas , but admittedly I'm by no means a pandas expert .

A series is a 1-dimensional object ; its transpose is ( vacuously
Wait : it indeed works for the example I've shown , but it doesn't exactly solve the whole question : whatever the orientation of t ( can be t or transpose t ) , it always outputs the same result ! I'd like to be able to force the orientation I want , not just have the outer dot product .
why don't you use dot vs outer instead of doing transpose ? If you really need t to be treated as a 2-d matrix , you should be doing what @USER suggested and do t = t [: , None ]

Group them and then apply our customized function

How to join these two pandas data frames ?
I am unable to join Data Frame 1 with Data Frame 2 , I suspect this is due to one of them having an int64 index and the other having a string index . How do I convert the string index to an int64 one as well ( If you agree with my diagnosis ) . If not , how to merge these two data frames .
This seems to have solved the problem and I am now able to join . If you have a more elegant solution , Id still love to hear .

You seem to be using HDFStore in a similar fashion to how I would like to use it . I have not had the time to create the wrapper code that handles a lot of the storing and retrieving . Would you mind sharing your ` txtfile2dtypes ` code ? Also , does your data have a lot of character data ? I run into issues when storing a csv file into an HDFStore with variable character data . The file-size blows up because I have to set ` min_itemsize ` to such a large value . I eagerly await the addition of a ` truncate ` option .
As a workaround , just split your queries up into multiple ones and concat the results . Should be much faster , and use a constant amount of memory

I have a data set where pictures were presented 3 times and measurements were taken for each presentation . Prospectively I would like to normalize the values for each picture ( based on the 3 repetitions - so 3 numbers ) and run an ANOVA on the categories : first presentation , second presentation , third presentation ( for all pictures ) . Before I get to that , however , I have to reorganize my data so that I can easily access data - based on the picture name AND the number of the repetition .
` groupby ` and ` pivot ` are good for performing aggregations over groups of data or when you have something specific you need to do to individual groups . ` stack ` and ` unstack ` help with reshaping your data , but move indexes to cols and vice versa .
You could try something like , ` meas_id = [ 1 , 2 , 3 ] * ( len ( df2.index ) / 3 )` and then do ` df3 = df2.reset_index() .set_index ([ " pic_name " , meas_id ])` . I think that should work as long as there are no missing measurements .

Basically , read it in , and then do the pivot yourself , keeping every other element and then fixing the column names .

I'll take a crack at your bonus question : you could map ` Timestamp ` onto your list . #CODE

Another tricky way is to transpose it first , divide it , and then transpose back : #CODE

Then you can ` apply ` this ( row-wise ): #CODE

Thanks @USER . I also worked out that ` df.ix [ np.random.random_integers ( 0 , len ( df ) , 10 )]` would also work .
Actually this will give you repeated indices ` np.random.random_integers ( 0 , len ( df ) , N )` where ` N ` is a large number .

apply changes to the column names of a dataframe

@USER it's ** very ** " forgiving " which is almost never what you want . Worse it's not consistent within a column , so without dayfirst it looks up US-style dates first then UK-style , so if you're all UK-style it'll translate before the 12th : day-mon-year as mon-day-year , and 13th onwards correctly . The annoying part is that IIRC this uses some very old python date lib which is very difficult to touch / fix . :(

Take the time difference ( using ` shift ` ) til the next value , and multiply ( value * seconds ): #CODE
Then do the resample to seconds ( sum the value*seconds ): #CODE
you can isnull ( df [ ' difference ']) will give True on NaT , so you could subtract then use mask I think

I found an elegant solution would be to use custom objects instead of the usual names of the Series object . These " MetaIndex " are basically a string + metadata and would replace the usual column labels in my_dataframe.columns The class definition looks something like that : #CODE
Thanks about version 0.11 ( eventhough I still have a hard time working with the development versions since as a windows user I still didn't figure out how to have a proper C-compiler to work with pip ) . Concerning the propagation issue , I know that's what is making you sceptic about the feature , but what I am looking for is a system that works according to common sense in simple situations ( initializing a DataFrame with a dict of series , or join an extra column ) , and drops the metadata that anyway would not be trustable in not-obvious-situations . I'm wondering if my suggestion works for this ?

Append new columns to HDFStore with pandas
I'm using Pandas , and making a HDFStore object . I calculate 500 columns of data , and write it to a table format HDFStore object . Then I close the file , delete the data from memory , do the next 500 columns ( labelled by an increasing integer ) , open up the store , and try to append the new columns . However , it doesn't like this . It gives me an error #CODE
HDFStore ( and HDF5 in general ) are row oriented . You will want to append on rows and make it your longest dimension . As Zelazny7 indicates you can add columns by creating another node , keeping in mind that you need to keep these synchronized yourself ( IOW they should have the same row indices ) , see : #URL

If you want to use the result of list comprehension as an index , you should use : ` df [[ x for x in df [ ' shift '] if df [ " MA10 "] > df [ " MA100 "]]]` , but I think this will raise some exception . Please post your sample data and desired result .
There are a few approaches . Using ` apply ` : #CODE

You can use the ` .shape ` property or just ` len ( DataFrame.index )` as there are notable performance differences : #CODE
EDIT : As noted @USER Allen in the comments ` len ( df.index )` and ` df [ 0 ] .count() ` are not interchangeable as ` count ` excludes ` NaN ` s ,
Also , remember that `` len ( df.index )`` and `` df [ 0 ] .count() `` are not interchangeable : `` count `` excludes NaNs , which is probably helps explain why it is slower .
There's one good reason why to use ` shape ` in interactive work , instead of len ( df ): Trying out different filtering , I often need to know how many items remain . With shape I can see that just by adding .shape after my filtering . With len() the editing of the command-line becomes much more cumbersome , going back and forth .
Use ` len ( df )` . This works as of pandas 0.11 or maybe even earlier .
Due to one additional function call it is a bit slower than calling ` len ( df.index )` directly , but this should not play any role in most use cases .

And select these rows with ` ix ` : #CODE

If I strip the tzinfo with ` .tz_convert ( None )` , the date gets converted to UTC : #CODE
Is there a TimeSeries method to apply ` .replace ( tzinfo=None )` to each date in the index ?

You can use ` apply ` to do this : #CODE
@USER are you saying the above worked on newer or older pandas ? There are a few edge cases in pandas ' apply which have been tweaked over last few releases so this could be one of them !

You can convert this column to integers by ` apply ` -ing ` int ` : #CODE

I am deleting the h5 file each time as I want clean results . Wondering if there is something as silly as it is failing because the df does not exist in the h5 ( and hence neither do any columns ) at the time of the first append ?
This issue is that you are specifiying a column in min_itemsize that is not a data_column . Simple workaround is to add ` data_columns=True ` to your append statement , but I have also updated the code to automatically create the data_columns if you pass a valid column name . I think this makes sense , you want to have a minimum column size , so let it happen .

#URL just pass a dict of your keys to pieces to concat

Pandas Pivot table subtotals per each cols
Thanks I guess I just have to iterate over each of the hierarchy and apply pivot_table to get my desire output .

I have a pandas data frame with a list of organism names and their antibiotic sensitivities . I wish to consolidate all organisms into one column , in the DataFrame below , based on the following rules .
I got lost when I tried to sequentially map AS201 -> AS101 , AS202 -> AS102 , AS203 -> AS103 etc . based on the condition .
Note : Usually it would be enough to use ` df [ orgi ] .str .contains ( ' aureus ')` ( without the ` == True ` , but since ` df [ orgi ]` might contain ` NaN ` values , we need to also map the ` NaN ` s to False , so we use ` df [ orgi ] .str .contains ( ' aureus ') == True ` .

Now that we have things set up we will be using the ` Panel ` method ` resample ` . We will supply two arguments , the first will be the new frequency we want the data to be at and the second is a keyword argument ` how ` that we wil specify as ` mean ` . This is the main part of my answer ! #CODE
Note that the `' 24H '` parameter we gave the ` resample ` method was simply an extension of the example posted by the OA . See this link for more information on what types of descriptions this argument can take .
Notice that you don't have to pass starting and ending times that align perfectly with the frequency you passed in . For more details on how to work with the ` Panel ` object or time series data in ` pandas ` check out the links given in those words .
I am not sure what you mean by accumulation , but the ` how ` parameter in ` resample ` can take function names or numpy array functions that returns an aggregated form of the input array . For example you could pass in , ` np.sum ` , ` np.max ` , ` np.min ` , ` np.prod ` , ect . See [ here ] ( #URL ) for more examples .

Which I would like to transform into a matrix where the new columns and indexes are unique values of two of the columns ( ` A ` and ` B `) and the cells are the join between these two unique values from a third column ( ` C `) .
I'm then trying to glue all of these Series together as rows in a new DataFrame , but can't get any of them to either ` append ` or ` concat ` into the new DataFrame ( following both the docs or similar questions on SO ) , e.g. #CODE

I am working on several taxi datasets . I have used pandas to concat all the dataset into a single dataframe .
now i want to extract all the notnull values from first few rows ( say from row 1 to row 6 ) .
As a heads up , ` irow ` will be deprecated in the next release of pandas . New methods , with clearer usage , replace it .
Thank you so much for the info . But apparently the command you mentioned is not what i want :( :( in a row , i need to extract all notnull values . => for several rows , without iterating , can i do it in a more compact way is the question . Thank you so much for replying :)

Some combination of the `` shift `` and `` isnan `` methods should do it . ( With `` dropna `` and `` diff `` , ultimately . )
@USER This is very elegant . Only trouble is that this provides the " wrong sign " for every other value in df [ " first "] . Is there a way to change diff so that we are always subtracting FROM column 2 ?
I see the issue now . Updated answer . I wrongly thought you were referring to the iget used along with transform . Thanks

When you shift dates the first one is a NaT ( like a nan , but for datetimes / timedeltas ) #CODE

Confusion regarding Merge in Pandas
I am trying to merge two pandas dataframes without index : #CODE

Concatenate each row's identifer by applying ` join ` row-wise . #CODE
Unstack it , and join it to the original DataFrame on ' foo ' . #CODE

` lambda s : tuple ( s )` is the same as just ` tuple ` , just like ` lambda x : len ( x )` is the same as ` len ` .
It's less complicated and faster than using ` apply ` or ` map ` . Something like ` np.dstack ` is twice as fast as ` zip ` , but wouldn't give you tuples .

Transpose csv structure using python pandas
Using Python Pandas how do I convert ( transpose ) the above structure to the following ? #CODE
My attempts at using pivot have been futile .

I've tried merge and join ( obviously incorrectly ) but I am getting a bunch of NaNs when I do that . It appears that I am getting NaNs on every alternate ID .
` merge ` and ` join ` could both get you the result DataFrame you want . Since one of your DataFrames is indexed ( by ID ) and the other has just a integer index , ` merge ` is the logical choice .
Merge : #CODE
Join : #CODE

The midpoint formula that I wish to apply is dependent on the bid / ask spread of the instrument . If the current spread is wider than the minimum tick increment , the midpoint will be the simple average of bid and ask prices at that moment . If the spread is equal to the minimum , the midpoint is weighted based on the bid and ask quantity .
I'm not sure this used to work . If I'm reading the code right , the intent is to apply this function to corresponding elements of the four ` DataFrames ` . But that's not what it did in olden times -- previously , I'm pretty sure the first branch would have been taken , because ` ( ask_price - bid_price ) > tick_increment ` was non-empty , and thus truthlike . So I suspect this code was buggy in the past . We can write a vectorized version of this which can work , at the cost of doing twice the work , but if there's a multi-DataFrame version of ` applymap ` I'm not sure I've used it .
@USER it definitely ran without errors , though I'm not certain it was getting the right result . I may try installing an older version to find out what was actually getting calculated . I think you are understanding what I am aiming to do - essentially apply the midpoint formula for each symbol , and at each timestamp , in the same way that I could get a DataFrame of bid / ask spreads using ` spread = ap - bp ` .

pandas dataframes have their own boxplot method ( i.e. ` mydataframe.boxplot() `) . Does that get you where you need to be ?
I would like to plot them alongside something else , the pandas boxplot function creates a new figure for each boxplot set . also , apparently it won't let me customize color or position .

You need this odd apply at the end because not yet full support for timedelta64 [ ns ] scalars ( e.g. like how we use Timestamps now for datetime64 [ ns ] , coming in 0.12 )

How would I replace the names in the source and target files with the indexes from the nodes file ?
Then we make the ` dict ` we want to replace things with : #CODE
Unfortunately ` .replace() ` doesn't work like you think it might on a DataFrame , because it applies to rows and columns , not elements . But we can ` stack ` and ` unstack ` to get around this : #CODE
PS : the same dict-based approach will work if you only want to apply the replace to certain columns , but you'd have to restrict the application . For example , if you wanted to go the other way , you probably wouldn't want the ` 2 ` in the weight column to become ` fou ` .
Oddly , neither worked . The replace method only replaced the first column .
I find that very surprising . Could you find a minimal example for which it doesn't work and post the ` .to_dict() ` ? One possibility is that there's whitespace in one column not present in another , and so the replace isn't working because there's not actually a match .

I believe I need to use hierarchical indexes on Pandas , however I could not figure out the exact syntax . Can stack / unstack be useful here ?

i.e if entry occurs 1st time i need to append 1 if it occurs 2nd time i need to append 2 and likewise i mean i need to count no of occurences of an email address in the file and if an email exists twice or more i want difference among dates and remember dates are not sorted so we have to sort them also against a particular email address and i am looking for a solution in python using numpy or pandas library or any other library that can handle this type of huge data without giving out of bound memory exception i have dual core processor with centos 6.3 and having ram of 4GB
Use the built-in sqlite3 database : you can insert the data , sort and group as necessary , and there's no problem using a file which is larger than available RAM .
Essentially we are taking a chunk from the table and combining with a chunk from every other part of the file . The combiner function does not reduce , but instead calculates your function ( the diff in days ) between all elements in that chunk , eliminating duplicates as you go , and taking the latest data after each loop . Kind of like a recursive reduce almost .
My first thought was also to put data in database but it wont do the trick i need to track 1st occurence of each email address also and count too and what about getting the diff of dates whats the soln for that ??
so the first time an email appears its email is then the reference date for subsequent appearances ? for the 2nd email it's easy , count is 1 and days is diff of days , what about 3rd email . does days get updated to be the diff between 3rd date and 1st date or is the number of days somehow involved ( maybe the 3rd days is max of current and 3rd date - reference date ? )

Merge columns of a dataframe into a list after merging by the first column
But , to answer your question , you can apply ` list ` . #CODE

and append each to a new df .
Depending on how complex my desired cut is , I often use a listcomp , like ` df [[ col for col in df.columns if some_complex_condition ( col )]]` . But depending on what you mean by taking a subset , you might actually be after ` groupby ` groups after transposing . It would definitely help to see the kind of output you want to get .

Could you add some sample data ? When you pass a pandas DataFrame or Series to ` AR ` , statsmodels [ assumes ] ( #URL ) it has a DatetimeIndex , which has a frequency attached . So try it without passing the freq argument . Also check for any ` nan `' s . If you have some try ` y = data.sentiment.dropna() ` .

I don't have time to tease it out , but I was thinking something like ` df.sort ( " char ") .groupby ( np.arange ( len ( df )) % 2 )` .

Pass a function to ` apply ` and specify ` axis=1 ` .

But the original values in the dataframe did not change . The replace method itself replaced and returned the new values correctly , but the inplace option seems not to affect the original dataframe when applying a conditional . This may be obvious to experienced Pandas users , but surely there must be some simple way of doing this instead of looping over every singel element ?

This still does not work . I think the na_values option does not apply to the columns that is being parsed as dates . The problem is really parse_dates does not work for columns with missing values .

But I get the idea , I can extract indices , shift them and create another series from which I'll take delta's .

I'm working through the " Python For Data Analysis " and I don't understand a particular functionality . Adding two pandas series objects will automatically align the indexed data but if one object does not contain that index it is returned as NaN . For example from book : #CODE

Apply pandas function to column to create multiple new columns ?
So I think I need to drop back to iterating with ` df.iterrows() ` , as per this ?
I don't think you can do multiple assignment the way you have it written : ` df.ix [: , 10:16 ]` . I think you'll have to ` merge ` your features into the dataset .
This ways better than user1827356 and Zelazny7 as it dosn't require an merge which is expensive

Pandas Data Frame join by grouping

drop a single tuple from a multi tuple column
Is there a way to transform this another way or is there a way to drop ' b_val ' from each of the column names ?
I think ` unstack ` is the correct way to do what you've done .
You could drop the first level from the the column names ( a MultiIndex ) using ` droplevel ` : #CODE
However , a cleaner option is just to ` unstack ` the column ( the series ): #CODE

Note the ` list() ` inside ` df = pd.DataFrame ( list ( cursor ))` evaluates as a list or generator , to keep the CPU cool . If u have a zillionty-one data items , and the next few lines would have reasonably partioned , level-of-detailed , and clipped them , the whole shmegegge is still safe to drop in . Nice .

This seems like it would be fairly straight forward but after nearly an entire day I have not found the solution . I've loaded my dataframe with read_csv and easily parsed , combined and indexed a date and a time column into one column but now I want to be able to just reshape and perform calculations based on hour and minute groupings similar to what you can do in excel pivot .
I know how to resample to hour or minute but it maintains the date portion associated with each hour / minute whereas I want to aggregate the data set ONLY to hour and minute similar to grouping in excel pivots and selecting " hour " and " minute " but not selecting anything else .

with the ultimate goal of doing a boxplot on the dataframe .
strange , I thought this fits very well with the ` pivot ` example in [ the doc ] ( #URL ) , but I keep on getting ` AssertionError ( ' Length of index , columns , and values must be the same ')` .

However an apply within an apply still isn't going to be particularly efficient ...

How to apply a function to several columns of a GroupBy object ?
If you dont specify a function per column , all columns will be passed to the function ( for both apply and agg ) . So : #CODE

if i could just strip out the minute part that would be good enough .

Take a loot at transpose #CODE
Duh . Swapping axes = transpose . Thanks !

This can also be done using apply , no need to sort . #CODE

For example in the case of John on the 2012-01-01 : Dist = 60 ( Diff John-Mark ) + 120 ( Diff John-Kevin ) = 180
Thanks for you update , I originally thought you wanted the diff per buyer , not from the first buyer , but that's just a minor tweak .
Hi Jeff , thanks a lot . This is exactly what I was looking for . However , you were right I am also trying to calculate the diff per buyer ( which would be for Kevin : 120 ( John-Kevin ) + 60 ( Mark-John ) = 180 ) and finally aggregated those daily sums to a monthly granularity ( which would be for John 180 ( 2012-01-01 ) + 60 ( 2012-01-02 ) = 240 ) . How would you do this ?

As a workaround , in earlier pandas you can use apply : #CODE
@USER tranform expects one result to all the things in the group , whereas apply expects a value for each row in the group . Although both act of the groups ( sub DataFrames ) so it is a little confusing .
That makes sense , but doesn't seem to be very clearly documented . For example [ here ] ( #URL ) it starts by describing transform as a form of apply , and later makes them sound almost equivalent : " ... For these , use the apply function , which can be substituted for both aggregate and transform in many standard use cases . However , apply can handle some exceptional use cases , for example ... "

NW , I reckon that link gives the answer - just replace the aggregate function with count .

Is there an efficient way to merge two sorted dataframes in pandas , maintaing sortedness ?
If I have two dataframes ( or series ) that are already sorted on compatible keys , I'd like to be able to cheaply merge them together and maintain sortedness . I can't see a way to do that other than via concat() and explicit sort() #CODE
I like the heapq.merge() suggestion in the commments there , maybe I can use that but it doesn't seem like a native numpy thing ? I want to take advantage of sortedness since with very large series merging when we know it's sorted should be linear in total length of the arrays , whereas any sort will be non-linear . ( Ironically when I started using pandas I assumed the " merge " operation did just this , instead of being a form of join . )

What's the equivalent of cut / qcut for pandas date fields ?
Pandas cut and qcut functions are great for ' bucketing ' continuous data for use in pivot tables and so forth , but I can't see an easy way to get datetime axes in the mix . Frustrating since pandas is so great at all the time-related stuff !
To bin by groups of price or quantity , I can use cut / qcut to bucket them : #CODE
But I can't see any easy way of doing the same thing with my ' recd ' or ' ship ' date fields . For example , generate a similar table of counts broken down by ( say ) monthly buckets of recd and ship . It seems like resample() has all of the machinery to bucket into periods , but I can't figure out how to apply it here . The buckets ( or levels ) in the ' date cut ' would be equivalent to a pandas.PeriodIndex , and then I want to label each value of df [ ' recd '] with the period it falls into ?
Just need to set the index of the field you'd like to resample by , here's some examples #CODE
This doesn't seem like a general solution , e.g. if I want to group on two different dates , or a date and a non-date ( via cut or category variable ) . I'll update the question with the structure of output I'm looking for .
How about using ` Series ` and putting the parts of the ` DataFrame ` that you're interested into that , then calling ` cut ` on the series object ? #CODE

I have a Pandas Series where each element of the series is a one row Pandas DataFrame which I would like to append together into one big DataFrame . For example : #CODE
concat them : #CODE

now this file was created by compiling many files , so the index for it is really odd . So all I wanted to do was reindex it with ` range ( len ( data ))` , but I get this error : #CODE

drop duplicates in Python Pandas DataFrame not removing duplicates
So drop duplicates is not removing anything . I tested to see if the nodes where actually the same and I get : #CODE
Say two values are " nearly " equal ( say x1 and x2 ) , is there any way to replace them in a way that they are both equal ???? What I want is to replace x2 with x1 if they are " nearly " equal .
One workaround would be to round the data to however many decimal places are applicable with something like ` df.apply ( np.round , args =[ 4 ])` , then drop the duplicates . If you want to keep the original data but remove rows that are duplicate up to rounding , you can use something like #CODE
The second line uses the ` apply ` method on groupby to replace the dataframe of near-duplicate rows , ` g ` , with a new dataframe ` g.apply ( lambda row : g.irow ( 0 ) , axis=1 )` . That uses the ` apply ` method on dataframes to replace each row with the first row of the group .
Hopefully someone who knows pandas better than I do will drop by and show how to do this better .

This isn't a single process solution and will not map to mclapply . But if you run processing functions on a single dataset that doesn't change , this helps - #URL
This functionality is available from the map method in multiprocessing.Pool()
See this answer for more details : Is there a simple process-based parallel map for python ?

Update : Dan Allan's ` resample ` suggestion worked , but now the the xticks are unreadable . Should I be extracting them separately ?
I think this task is more easily accomplished using ` resample ` , not ` group ` . How about #CODE
The resampling operation creates a MultiIndex . Calling unstack ( level=1 ) solved it . Thanks again !

I would like to replace each NAN ' x ' with the previous non-NAN ' x ' from a row with the same ' id ' value : #CODE

Using your example data . Note that due to copy paste the tabs becaming white space ( so using sep= ' \s+ ' , iso ' \t ') and i have set the first row of the data as the column names ( not using header=None ) . Concatenating one column to a string can be done using join . #CODE

You can create a column in your ` DataFrame ` based on your Days Late column by using the ` map ` or ` apply ` functions as follows . Let's first create some sample data . #CODE
Can you tell me how I can pivot these results . I think the groupby produces a series that can not be pivoted .
The ` groupby ` method generates a ` Series ` with a ` MultiIndex ` . You can use ` unstack ` to pivot the lowest level index into columns , as shown in the edited answer above .
and use the ` unstack ` method to turn the ` status ` index into columns : #CODE

Pandas DataFrame from WB WDI data : combine year columns into " year " variable and merge rows
This seems like some complex pivot or opposite-of - " melt " , but I cannot figure it out .
Group your ` DataFrame ` by ` country ` and ` countrycode ` and then apply your own function : #CODE
There are sveral stack / unstack examples in the docs #URL ( same page also demos pivot and melt ) . An extra answer is just fine .
I'm suggesting that @USER may put this in his ( accepted ) answer , as it uses the actual names from the WDI data , and makes it more cut and paste for someone else using them . Sorry -- I'm sure this isn't the right way to communicate this ...

Pandas : How to use apply function to multiple columns
I have some problems with the Pandas apply function , when using multiple columns with the following dataframe #CODE
When I try to apply this function with : #CODE
If you just want to compute ( column a ) % ( column b ) , you don't need ` apply ` , just do it directly : #CODE
@USER following [ 53-54 ] allow you to apply more complex functions .

The reason they are different here is that the Series is providing alignment , e.g. it maps the values to an index ( which in this case is 0 , 1 , 2 ) . The numpy array is not mapped and thus cannot align . This is essentially a degenerate case because in practice this is quite inefficient , better to line up the small arrays in a bigger 2-d array and fill in the NaNs

Pandas error : ' DataFrame ' object has no attribute ' loc '
` loc ` was introduced in 0.11 , so you'll need to upgrade your pandas to follow the 10minute introduction .
In fact , at this moment , it's the first new feature advertised on the front page : " New precision indexing fields loc , iloc , at , and iat , to reduce occasional ambiguity in the catch-all hitherto ix method . "
I am finding it odd that ` loc ` isn't working on mine because I have pandas 0.11 , but here is something that will work for what you want , just use ` ix ` #CODE
` loc ` works for me with 0.11.0 .

This is obviously using the data generated below , but you can easily apply to your example .
The ` loc ` then takes that boolean array and select the applicable columns #CODE
that's in 0.11 . you can use ix in place of iloc loc

I am working with a hourly time series ( Date , Time ( hr ) , P ) and trying to calculate the proportion of daily total ' Amount ' for each hour . I know I can us Pandas ' resample ( ' D ' , how= ' sum ') to calculate the daily sum of P ( DailyP ) but in the same step , I would like to use the daily P to calculate proportion of daily P in each hour ( so , P / DailyP ) to end up with an hourly time series ( i.e. , same frequency as original ) . I am not sure if this can even be called ' resampling ' in Pandas term .

Following the scikit-learn tutorial here , if we have a ` Pandas.DataFrame ` that has a column named ` colors ` , how can we create a loop to loop through all of the DataFrame's columns ( or a list containing the required columns ) so that all categorial variables ( eg . variable ` colors ` that can have values ` blue ` , ` red ` , ` purple `) will be replaced by ` len ( colors )` number of dummy variable columns ` colors #blue ` , ` colors #red ` , ` colors #purple ` ?

If I understand you , you don't actually want ` shift ` , you simply want to make a new column next to the existing ` DATE ` which is 180 days after . In that case , you can use ` timedelta ` : #CODE

How do strip I convert Unicode to strings ? #CODE

In the special case where you have a columnar MultiIndex , but a simple index , you can transpose the DataFrame and use ` index_label ` and ` index_col ` as follows : #CODE

If the dataframe has more than the two series , and you only want to plot those two , you'll need to replace ` df ` with ` df [[ ' korisnika ' , ' osiguranika ']]` .

You can use ` ix ` like you were , but applying a different slice ... #CODE
I think a more explicit way of doing this is to use drop .
` drop ` can even be calculated in-place ( without extra assignment ) . Faster and simpler !

428 base , mult = _gfc ( freq )
--> 429 return tslib.dt64arr_to_periodarr ( data.view ( ' i8 ') , base , tz )

Here's a way to create that pivot table with ' size ' as the aggregating function : #CODE

You probably need ` apply ` , so something like : #CODE

Wrapping it in a Series in the apply returns a DataFrame : #CODE

I think I'd drop the first level of the multi-index ( the one with PARTS ) * : #CODE
Thanks for this , it worked . But I found that i could not do the drop level df1.columns = df1.columns.droplevel ( 0 ) as you specified . I had to do this instead df2.columns = df1.columns.droplevel ( 0 )

You could also ` apply ` ` np.prod ` , which is what I'd originally done , but usually when available the direct methods are faster . #CODE

Problem : the append traps this exception
Ok running on the patched version . How would I force it to use a supported dtype index . All my fields are uint64 and strings . I tried a df = df.reset_index ( drop = True ) but I still seem to have NotImplementedError : indexing 64-bit unsigned integer columns is not supported yet , sorry

I don't know much about plotting , but ISTM you can use ` groupby ` the way you want [ NB : this assumes your index consists of integers , not strings -- replace ` 0 ` by `' 0 '` if I'm wrong ]: #CODE

I've seen a few solutions which map / do list comprehension to ' manually ' put the dataframe together . Is that the only way ? I was hoping pandas had some basic function to magically do this kind of thing ... apply ? join ?
Try to use merge to achieve your goal . For example : #CODE

And then append a new columns for ` index ` : #CODE
It just generate a new object but not replace the original one .

multi-column factorize in pandas
The pandas ` factorize ` function assigns each unique value in a series to a sequential , 0-based index , and calculates which index each series entry belongs to .
` Factorize ` only works on single columns . Is there a multi-column equivalent function in pandas ?
You can use ` drop_duplicates ` to drop those duplicated rows #CODE
To achieve your goal , you can join your original df to the drop_duplicated one : #CODE
I'm not looking to drop them , but to assign a unique index to each pair of distinct values ( i.e. I eventually want to add a new column to the data frame , with values [ 0 , 1 , 2 , 2 , 1 , 0 ]) .

I have tried generating a BDay date range and then changing the freq to hourly but this doesn't work . #CODE
To restrict to Business days within business hours ( apply these in either order ) .

How to insert pandas dataframe via mysqldb into database ?
I can connect to my local mysql database from python , and I can create , select from , and insert individual rows .
My question is : can I directly instruct mysqldb to take an entire dataframe and insert it into an existing table , or do I need to iterate over the rows ?
Thanks , this is how I've been doing this so far . I'm looking for a way to directly insert into mysql without the csv detour .
` if_exists : { ' fail ' , ' replace ' , ' append ' } ` , default `' fail '`
` replace ` : If table exists , drop it , recreate it , and insert data .
` append ` : If table exists , insert data . Create if does not exist .

Issue with Pandas boxplot within a subplot
I'm having an issue drawing a Pandas boxplot within a subplot .
Based on the two ways I'm trying , creating the boxplot either removes all the subplots that I've already created , or plots the boxplot after the subplot grid . But I can't seem to draw it within the subplot grid . #CODE
Any ideas how I can get my boxplot image to appear in the bottom right grid in the subplots ( the one that's empty in the first set of images ) ?
This appears to be a bug , or at least undesirable behavior , in the pandas plotting setup . What is going on is that if you supply a ` by ` argument to ` boxplot ` , pandas issues its own ` subplots ` call , erasing any existing subplots . It apparently does this so that , if you want to plot more than one value , it will create subplots for each value ( e.g. , one boxplot for Y1 by day , another for Y2 by day , etc . ) .

You are looking for pandas pivot . First do : #CODE
Thank you ! I don't know how I forgot about pivot tables
Pivot wasn't quite right , but this worked : #CODE

My first idea is to resample DataFrame

about concat in pandas : using row data to create new columns
I want to do a custom concat i.e using the rows in a group by object

You could resample ` df2 ` to 5 min and fill it .

Hi nordev , thanks for telling me how to limit the data range ; I've been trying all sorts of variations on your code , and looking for instructions in the numpy manual , but couldn't make it work . In the end , I simply cut those rows from my data .

As far as I know there is no such feature available in ` pandas ` , as the plotting capabilities of ` pandas ` are very limited compared to ` matplotlib ` . Also , if you want to have the ticklabels / tickmarks of the x-axis connected to the " middle axis " ( also while panning / zooming ) , then it's easiest to insert an extra spine ; take a look at [ ` mpl_toolkits.axisartist `] ( #URL ) for some examples of this .

Cool , the struct module looks very useful . I would just append the ` entry_frame ` dictionaries to a list and then create a DataFrame from a list of dicts after the whole file is read .

Extra Bin with Pandas Resample
and when I go to resample it : #CODE
I end up with an extra empty bin I wouldn't expect to see -- 2001-06-01 . I wouldn't expect that bin to be there , as my 28 days are evenly divisible into the 7 day resample I'm performing . I've tried messing around with the closed kwarg , but I can't escape that extra bin . Why is that extra bin showing up when I've got nothing to put into it and how can I avoid generating it ?

Is there a more efficient way to map in a dynamic number of abc -type columns ?
@USER I'm not sure what you mean , so ApplyMap applies the function to every cell ( being every intersection of row and column ) so basically across the entire dataframe . Whereas .map just does it for a single row or a single column ?

merge 2 dataframes in Pandas : join on some columns , sum up others
I want to merge two dataframes on specific columns ( key1 , key2 ) and sum up the values for another column ( value ) . #CODE

Then you can ` stack ` ( first by `' Marker '` then by `' mrk '`) : #CODE

However , if you really want to drop the second level of the index , you could do this : #CODE

for the stupid hack solution , you can do search / replace in the csv and rename ` NA ` to something like ` NA_safe ` .

You didn't need column names not to have spaces , but you did need to join them by something you could separate . The space between " Mon " and " id " was hard to distinguish from the spaces between the columns . Sometimes I've found it easier to do something like ` pd.read_table ( " gistfile1.txt " , sep=r " \s+ " , skiprows=1 , header=None )` and fix the columns after the fact .

Is it possible to append to an empty data frame that doesn't contain any indices or columns ?
but the ` append ` doesn't happen in-place , so you'll have to store the output if you want it : #CODE
actually that append doesn't happen in place is the most important info here ;)

I am generating some delimited files from hive queries into multiple HDFS directories . As the next step , I would like to read the files into a single pandas dataframe in order to apply standard non-distributed algorithms .

plot column 1 where column 2 is notnull , but with different style .

In 11.0 all three methods work , the way suggested in the docs is simply to use ` df [ mask ]` . However , this is not done on position , but purely using labels , so in my opinion ` loc ` best describes what's actually going on .

I have a data frame and I want to create a new column whose values are defined by values located in other columns ( in the same row ) . It is very simple if I use simple operations ( ` + ` , ` - ` , ` * ` and even ` abs `) . For example : #CODE
Unfortunately it did not work . I think the reason why it did no work is because ` my_func ` contains ` asin ` , ` atan ` and other functions that cannot be applied to series . For example if I try ` abs ( df [ ' col1 '])` I get no complains but if I try ` asin ( df [ ' col1 '])` I got error message : #CODE
Is there a trick that will let me use ` asin ` ( or my own function ` my_func `) in the same way as ` abs ` or ` + ` are used ?

This is perhaps a little surprising , since ix works with the string ... ` df1.ix [ ' 2012-01-01 01:00 : 00 ']` .

I am sampling at a freq of 880MHz , I want to do some calculations on the samples , and make use of the data in the 880 row of the .csv file .
I did this by using the freq colon as indexing , and then just use the sampling freq to get the data , but the tricky part is , if I sample with 900MHz I get an error . I would like it to take the nearest data below and above , in this case 880 and 910 , from these to rows I would use the data to make an linearized estimate of what the data at 900MHz would look like .
Put an empty row in the middle and interpolate ( edit : the default interpolation would just take the average of the two , so we need to set ` method= ' values '`) : #CODE
This was exactly what I was looking for thank you .. A side comment , I don't know how the interpolate function work , but the way you are using it , it assumes that the 900MHz is in the middle of the two other frequencies , so the result is not correct .

A somewhat cheeky way could be to use ` loc ` : #CODE

Have you heard of the heapq module and merge sorting ? Guido implemented this approach for sorting a large collection of files . Check it out here #URL

From the outside , it doesn't look like non-unique indices are taken advantage of in any way . For example , the following ` ix ` query is slow enough that it seems to be scanning the entire dataframe #CODE
( I realize the two ` ix ` queries don't return the same thing -- it's just an example that calls to ` ix ` on a non-unique index appear much slower )
When index is unique , pandas use a hashtable to map key to value O ( 1 ) . When index is non-unique and sorted , pandas use binary search O ( logN ) , when index is random ordered pandas need to check all the keys in the index O ( N ) .

Ah ha , it does , I didn't realise you could do that with ` replace ` .
@USER So actually what I want is the exact inverse of ` tz_localize ` which is what the ` replace ( tzinfo=None )` does for datetimes , but it is indeed not a very obvious way .
For reference , here is the ` replace ` method of ` Timestamp ` ( see tslib.pyx ): #CODE
Setting the ` tz ` attribute of the index explicitly seems to work : #CODE

You could just apply the Series constructor to that column : #CODE
Hmmm , a simple is to just apply something like ` make_series = lambda x : pd.Series ( x ) if x == nan else x ` , there's probably a more efficient way though .

Can you explain how this works ? When I look at the individual Series that are created , e.g. Series ( 1 , index=s [ 1 ]) , The series index are the letters in the list ( as I'd expect ) . But s.apply() yields a DataFrame with the series indexes as columns . How did the pivot happen ?

It is generally not a good idea to put a LOT of nodes in an ` .h5 ` file . You probably want to append and create a smaller number of nodes .
You can just iterate thru your ` .csv ` and ` store / append ` them one by one . Something like : #CODE
Thanks so much Jeff . By the way , What imports / packages do I need to use ` HDFStore() ` , append tables , and use ` read / write_hdf ` in Pandas ?

in general you want to have many more rows than columns ; hdf5 is row based . try storing the transpose of your frame

Starting in 0.11.1 ( coming out this week ) , replace has a new option to replace with a regex , so this becomes possible #CODE

Merge on single level of MultiIndex
Is there any way to merge on a single level of a MultiIndex without resetting the index ?
I have a " static " table of time-invariant values , indexed by an ObjectID , and I have a " dynamic " table of time-varying fields , indexed by ObjectID+Date . I'd like to join these tables together .
What if you created an additional column with the level of the MultiIndex you want to join on and then merged / joined on that on that ? Might not be totally efficient , but at least you maintain the index .
Yes , that would work . It'd cost some memory , and it wouldn't help speed . At that point , though , I think I might as well drop the index entirely , if it's not going to help speed up merges .
I get around this by reindexing the dataframe merging to have the full multiindex so that a left join is possible . #CODE
Do the join on the subindex by reindexing the newFactor dataframe to contain the index of the left data frame #CODE
Yes , since pandas 0.14.0 , it is now possible to merge a singly-indexed DataFrame with a level of a multi-indexed DataFrame using ` .join ` . #CODE
The docs also mention that ` .join ` can not be used to merge two multiindexed DataFrames on a single level and from the GitHub tracker discussion for the previous issue , it seems like this might not of priority to implement :
so I merged in the single join , see #6363 ; along with some docs on
how to do a multi-multi join . THat's fairly complicated to actually

I'm trying to create a horizontal stacked bar chart using ` matplotlib ` but I can't see how to make the bars actually stack rather than all start on the y-axis .
There was another good answer , here in Stack Overflow .

Thanks to @USER for the additional idea of using transpose . On my machine it yields a significant improvement : #CODE
FYI , sometimes you can transpose and access faster ( depended on your memory is aligned ) , in this case ( about 10% boost ): `` wpt = wp.transpose ( 1 , 0 , 2 ) . %timeit wpt.ix [: 499 ] : 78.7 us per loop , %timeit wp.ix [: , : 499 ] : 87.6 us per loop ``

possible duplicate of [ How to insert pandas dataframe via mysqldb into database ? ] ( #URL ) on seconds thoughts , it's perhaps the opposite , nevertheless related .
If you are appending to an existing table , use the keyword argument ` if_exists= ' append '` .

Loop over lists and concat in pd.dataframe

pandas dataframe.drop ( col , axis=1 ) does not drop column from column.levels in multiindex dataframe

There might be more than one index map to your value , it make more sense to return a list : #CODE

Pandas stack unstack pivot hierarchical index - reshape dataframe
Have a look at #URL more specifically at the apply and transform sections

getting specific median from data
actually the * mode * is the most common value in a data set . the median is the value that half that values are less than and half are greater than . integral from -infinity to m where m is the median is 1 / 2
Your function return the median of the first 3 values ( or less if there are less ) #CODE
thanks to @USER Allen for resample , rather than groupby date
You can take the median start-of-day location in one line . #CODE
Trying that out now . I find `` lambda d : d.date() `` is very slow compared to resample ( ' D ') . Maybe it's worth adding `` how= ' range '`` .
File " / opt / local / Library / Frameworks / Python.framework / Versions / 2.7 / lib / python2.7 / site-packages / pandas / core / generic.py " , line 234 , in resample
File " / opt / local / Library / Frameworks / Python.framework / Versions / 2.7 / lib / python2.7 / site-packages / pandas / tseries / resample.py " , line 100 , in resample
@USER , you can only use resample when the index is time-based . @USER means `` df.set_index ( ' time ') .resample ( ' D ' , how=f )`` .

it could generate a `` nan `` in B ( for the first n elements actually , if not are abs > 1 ) , in this case I think `` nan `` is ok as its undefined by the OP

You could melt the DataFrame : #CODE

Essentially you need to stack all the results yourself , whether in a list , numpy array or pandas DataFrame depends on what's more convenient for you .
But it might be better to align with pandas , depending on what structure you have across models .

A roundabout way is to ` reset_index ` of the transpose : #CODE

Instead of resampling the whole group , take the messages column only , and then resample , #CODE

Now I have two data frames ( ` df ` and ` df2 `) and I would like to merge them using ` col1 ` and ` col2 ` . The problem that I have now is that ` df2 ` does no have these columns . After ` groupby ` operation ` col1 ` and ` col2 ` are " shifted " to index . So , to resolve this problem , I try to create these columns : #CODE
You can use ` left_index ` ( or ` right_index `) arguments of ` merge ` :
Use the index from the left DataFrame as the join key ( s ) .
and use ` right_on ` to determine which columns it should merge the index with .

Is there a way I can pivot the data using Pandas so that rows are daily dates ( e.g. 1-Jan , 2-Jan etc . ) and the corresponding 1 column to each date is the daily sum of hits ( sum of the hits for that day e.g. sum of hits for 1-Jan ) divided by the monthly sum of hits ( e.g. for the whole of Jan ) for that month ( i.e. the month normalised daily hit percentage for each day )

pandas resample doesn't work with numpy 1.7
Something funny is going on when you parse the date columns . I would need to look at your file ( post a link , or a part of in your question ) . Your parsing looks fine.In any event , ` pd.to_datetime ` will take what you posted and turn it into a ` DatetimeIndex ` , which is what you need for resample .

Please show the full stack trace

After you've read in the DataFrame ( without restricting dtype ) you can then convert it ( using technique from this post ) with ` apply ` : #CODE
You shouldn't edit the question , but rather ask a new one :) . It's essentially the same trick in both cases ( just define a function which does it to a single string and then apply it the column ) .

Then append the new data , which automatically extends the columns : #CODE
Replace the missing values with 0 : #CODE

I would like to append it to an existing DataFrame and then run some analysis on it .
a lot of this depends on exactly what you are going to then do with the data , i.e. do you need a frame of the ENTIRE series for example . Another option is to append to a `` HDFStore `` ; then select what you need ( you have synchronize the read / write a bit though )
If you care about the data at all , then yes , use logging to build a very robust system that guarantees that everything is saved . Then separately have an analytics app that reads the logs in chunks every once in a while and appends the chunks to a DataFrame . If you don't care about the data and can always start over , then just aggregate the incoming data into chunks and append 100 or 1000 rows at a time . If you absolutely need to append the rows asap , at least append 1000 empty rows to allocate memory and then manually insert the data as if you just had numpy arrays .
Append your results to that list .
List append is O ( 1 ) , DataFrame append is O ( ` len ( df )`) .
Thanks . But with this method , you can't use pandas on the * latest * data of the last few seconds . Let's say I insert data today at 15:23 : 00 , 15:23 : 01 , 15:23 : 02 , 15:23 : 04 . Then if I want to access ` df [ ' 2015-12-15 15:23 : 01 ' , ' 2015-12-15 15:23 : 03 ']` , I can't . Then it means I have to write my own code to filter rows by time in ` cache ` . this means we have to reinvent the wheel !

Append data in pandas
I have four sensors that are sampling in four threads . I would like each sensor to append the current value to a specific column in a panda data frame .
I was thinking you were trying to merge 4 columns into one . I'm not sure you really need to use pandas to merge between threads , why not use something like SQLite or just use a simple numpy array ? You could chunk your calls ( i.e. , generate 5+ values at a time ) , keep track of where you are in terms of observations , and then send update / insert commands on the fly to fill the columns . Seems like that would be much more efficient . For the numpy array , you could pass the column index instead .
If you have some column that is unique to each observation ( let's call it " ObservationID ") , then it's really simple to do this . You can use either ` pandas.concat ` or ` pandas.merge ` . I'll go with ` concat ` because I find it slightly simpler . #CODE
Finally , you could also do this with ` pd.merge ` , but I don't really see why you would need to ( unless you had columns with the same name on both parts , then you either need to use merge or pick one of the sensors to win ) . To do that , you need a join condition ( so that's why we're using the unindexed sensors for this ): #CODE

Show only the n'th ticklabel in a pandas boxplot
I have a pandas boxplot with FICO score on the x-axis and interest rate on the y-axis .
How do I access the plot , subplot , axes objects from pandas boxplot .
Hi nordev , thanks for the pointer . However to use MultipleLocator I need to get access to the underlying object . While matplotlib is certainly more flexible , the learning curve is much higher and from my exploration - creating a boxplot was not as simple and transparent as in pandas .

This provides a correlation between 30 days of GE prices and the same 30 days of IBM prices , rolling it through the entire column of dates . What I would really like to do is hold a constant 30 day window for one of the stocks ( example .. days 1001-1030 ) and correlate it to every possible 30 day window in the other stock ( 1-30 , 2-31 , 3-32 , etc . ) .

If you are sure that everything you want to drop is either the literal empty string ( `''` , ` None ` , ` np.NaN ` , or ` 0 `) and that you don't want to keep ` 0 ` , then you can just fill the ` NaN ` and convert to boolean and check whether the sum is 0 . You can tweak depending on how you want to drop . #CODE
` fillna() ` lets you " fill " ` NA ` values with some other value . Here , we fill with False ( because ` bool ( float ( ' nan '))` evaluates to True ) , but you can fill with any value or with a variety of different methods . ` astype ` converts the array from one type to another . So putting ` astype ( bool )` means that it converts the entire array to either ` True ` or ` False ` ( which are equivalent to ` 1 ` and ` 0 ` respectively ) and then you can just sum to find the number of ` True ` values in a row or column .
Thanks @USER , that works brilliantly . Would you mind explaining what the df.fillna is doing ? I understand ( I think ) the ` df.fillna ( False )` but what is the ` astype ( bool )` doing ? How does it know what to treat as an NA ? Thanks

I hope to compute weekly statistics for each trader with the average order volumes . To do so I am currently unstacking the trader column and resample the data using : #CODE

pivot " stacked " data with multiple indexes ?
And i want to pivot it to look like this : #CODE
You can set your multilevel index and then ` unstack ` the level within that index back to columns : #CODE

Since the ` pd.Period ` can parse quarterly periods , you could use it as the custom ` date_parser ` . Then , to convert the date to the last day of the quarter , you could use ` map ` and the ` end_time ` attribute : #CODE

Is there any way to change some column names in pandas dataframe using ` lambda ` , but not all ? For example , suppose that this data frame has columns whose name are ` osx ` , ` centos ` , ` ubunto ` , ` windows ` . In this data frame , I want to replace all column names with that column name appended by ` x ` , so in this case , I can rename the column name by : #CODE

I would like to resample this DataFrame on a daily basis and compute the following complex statistics .
What went wrong with concat propsosed in the link above ? e.g. pd.concat ([ df1 , df2 ] , keys =[ ' AVG-DIFF ' , ' AVG-VAR '] , axis=1 ) works fine .
Hi Wouter , thanks for your help . The problem with the concat method is that it does not work for me when I try to unstack the Trader column . I updated my original description to be more precise .
* Trader * is a column level , if you want to pivot this you need to use stack and not unstack ( which if for pivoting a level on the index ) .
Thanks , that solves the first issue . But how do I shift the trend column by 1 period ?
The * trend * columns of df3 is df2 . So you can do df2.shift ( freq= ' D ') .stack ( ' Trader ') . I do not know if it is possible to shift on a DateTime level of a MultiIndex ( operating directly on df4 ) . Not really a problem here , since data on a single level DatatimeIndex is available in df2 .
To access a MultiIndex column a tuple is needed . So in function f : df [ ' Diff '] = abs ( df [( ' acctual ' , ' Quantity ')] - df [( ' trend ' , ' Quantity ')]) . Also you need to return the adapted frame ( add return df at the end of the function ) .
You do not need groupby / apply to compute the diff column . df4 [ ' Diff '] = abs ( df4 [( ' acctual ' , ' Quantity ')] - df4 [( ' trend ' , ' Quantity ')]) . From this aggregated data can be computed with groupby e.g df4.groupby ([ df4 [ ' Start '] .map ( lambda x : x.year ) , df4 [ ' Start '] .map ( lambda x : x.week ) , df4 [ ' Product ']]) .sum() . Not that the latter does not have a Trader column . Are you sure that the groupby you are doing is what you want ? Some funky mapping Trader <-> week .

a Python ` dict ` is unordered . The keys are not stored in the order you declare or append to it . The ` dict ` you give to the ` DataFrame ` as argument has an arbitrary order the DataFrame takes for granted .

I'm not super-happy with the ` lookup ` line -- you could replace it with your original ` max ` call , of course -- but I think the ` idxmax ` approach isn't a bad one .
In [ 9 ]: %timeit df [ ' critic '] = abs ( df.lookup ( df.index , df.critic_vector ))

Is there a way to do this ? I think the easiest way is to apply a function of dividing by index1 values by 3 , but not sure how you apply a function to an index . Perhaps though pandas has it's own methods for redefining index values to have groupings like this which are still unique when you consider both indexes ?

I tried to use ix to perform the same operation but it takes even more time .
transpose so that your dims are like : items ( low ) x major ( dates ) x minor ( ids ) , will give better perf
`` .ix `` in this case might not * always * be returing a view , it actually depends on how numpy is aligning the memory . If you try this with a transposed panel ( e.g. your major is your items dim ) , then I think you will * always * get a view , but since you are iterating over another dim , I am not 100% sure . This could explain the diff .

As @USER mentions there are a few ways to do this , but I recommend using loc / iloc to be more explicit ( and raise errors early if your trying something ambiguous ): #CODE
The latter two choices remove ambiguity in the case of integer column names ( precisely why loc / iloc were created ) . For example : #CODE

Can you only download the bits you want and then append to the existing data frame ? This sounds like more of an issue with your data source and not so much a Pandas question . Maybe you could clarify .

I want to append a calculated row that performs some math based on a given items index value , e.g. adding a row that sums the values of all items with an index value 2 , with the new row having an index label of ' Red ' . Ultimately , I am trying to add three rows that group the index values into categories :
My current solution involves transposing the DataFrame , applying a map function for each calculated column and then re-transposing , but I would imagine pandas has a more efficient way of doing this , likely using ` .append() ` .
The role of " transpose , " which you say you used in your unshown solution , might be played more naturally by the ` orient ` keyword argument , which is available when you construct a DataFrame from a dictionary . #CODE

I generate the initial list by calling a previously developed function . I tried using ` eval ` on the list , which seems to solve an issue that comes about when using ` raw_input ` and iterating over items within it - kinda trying to work out some of the issues I've been having when transitioning to ` IPython ` and ` Python 2.7 ` ( originally used ` Python 3.3 `) .
Which , actually , now that I think about it , may be what is causing the problem- although ` eval ` still would not fix it .

Here s the SELECT and MERGE : #CODE
Due to your initial merge operations and multiple coaches per team there is indeed duplication of the playerID / Name , this has nothing to do with hierarchical indexing .

How to resample a TimeSeries in pandas with a fill_value ?
Is it possible to resample a ` TimeSeries ` using a ` fill_value ` for missing data like I can with ` reindex ( fill_value=0 )` ? I don't want my integers cast into floats . #CODE

transpose it #CODE

your data is 2-d , how do you want to make it 1-d ? e.g. take a single column for example , or apply a function across all the columns in a reduction operation , or concatenate the data
I just saw this answer . What if the dataframe only has one column ? ` unstack ` would return a series with a two-level index , and ` pd.Series ( df )` does not seem to work ( it's really odd what it does , since it splits the column title into characters and populates the Series with copies of this splitting )

Thanks @USER . Do you know if append returns a copy / view / reference of the original dataframe ?

same number of elements as you are trying to replace ) , and this MUST be sorted for this to work #CODE

Second question is once I am done with column wise comparison I want to be able to count for each row how many ' true ' are there , I am currently doing ` astype ( int32 )` to turn bool into int then do ` sum ` , does this sound reasonable ?

Hmm ... why the downvote , gang ? This seems like a well stated use case that may apply to others .
Then use pandas read_csv to load this revised file , assigning to the index the first two columns . ( They will be a ` MultiIndex ` . ) Then use ` unstack ` to pivot the index of Qs into columns .

pandas merge with dataframes of different frequency ( hourly and daily )
I'm trying to merge dataframes that are either daily or hourly . It seems on the first iteration through my loop , I can merge the first daily with hourly values . But the second time around I get this error : #CODE

I am writing a function that takes 3 pandas Series , one of which is dates , and I need to be able to turn it into a dataframe where I can resample by them . The issue , is that when I simply do the following : #CODE

However , this doesn't carry over directly to pandas Series . I seems that map / apply / lambda seems the way to go . I've arrived at this piece of code , but getting an invalid syntax error . #CODE
You can just use an apply with the same method suggested there : #CODE
Or perhaps you could use one of the Series string methods e.g. ` replace ` : #CODE
Hmm not too sure , think you may be better off just with the apply , but something using match could be possible : ` s.str.findall ( r ' ( ? <= \ ( ) [ ^ ( ] * ( ? =\ ))')`
The issue came up with apply in fact . Findall works ! Last thing - how do I get rid of the square brackets around the results other than stripping them after the fact ?

Join them by ` bookId ` , and then group them by ` readerId ` and ` otherReaderId ` #CODE
You , you can join the dataframe to itself , and then some groupby tricks might achieve your goal .

I have some problems while reading file even from txt or HDF5 formats to dataframes in pandas because a txt file of about 200 mb of strings stored as txt and read with read_table causes a memory consumption of about 600 Mb . If I append the dataframe to HDFStore the file is about 200 mb too . Here is how I read the file #CODE

I know that I could use the ` truncate ` method to extract the periods and create new timeseries to work with , but I am wondering whether I can define a custom ' period ' ( e.g. 1st of May through 1st of Sept ) . Then I would like to calculate correlation coefficients between two different timeseries for only during that period . Is there a way to do this ?

to get a series of bool . And I can do moving median or moving average to get an idea about the size of the data holes .
From that , I can make histograms of hole lengths , and of the ` and ` or ` or ` of isnull of multiple series ( that might be substitutes for eachother ) , and other nice things .
` Series ([ len ( list ( g )) for k , g in groupby ( a.isnull() ) if k ])` is probably slightly more efficient .

Is there an option not drop the the indices with ' nan ' in them ? I think silently dropping these rows from the pivot will at some point cause someone serious pain . #CODE
Workaround to fill the index with a dummy , pivot , and replace #CODE

this is correct ; what are you using in the rhs that gets the assertion error ? it must be align able or equal length to the index in the case of a list / numpy array

just to complete perigee's answer , it cost me quite some time to find a way to append the data . #CODE
This code uses the pandas read_csv method to get the new quote from yahoo , and it checks if the new quote is an update from the current date or a new date in order to update the last record in history or append a new record .

Is there any possibility to convert the resulting ' gr ' DataFrame into a DataFrame with a PeriodIndex so that I can use the resample function to calculate weekly and monthly averages ?
Important : I need to resample the ' gr ' Dataframe not the original ' df '

pandas - apply function to current row against all other rows
I would like to compute another matrix from this input one that will compare each row against all other rows . Let's assume for example the computation was a function to find the length of the intersection set , I'd like an output DataFrame with the ` len ( intersection ( Alice , Bob ))` , ` len ( intersection ( Alice , Carol ))` , ` len ( intersection ( Alice , Dave ))` in the first row , with each row following that format against the others . Using this example input , the output matrix would be 4x3 : #CODE
Interesting . I replaced my list comprehension with a slightly nicer nested apply . But this is even more compact . I wonder if `` np.equal `` can be worked into it ....

The first two columns contain codes for weather stations and sometimes they have leading zeros . When pandas imports them without specifying a dtype they turn into integers . It's not really that big of a deal because I can loop through the dataframe index and replace them with something like `" %06d " % i ` since they are always six digits , but you know ... that's the lazy mans way .

I'd replace the negative 1 but my reps not yet high enough .

I've used the map function on a dataframe column of postcodes to create a new Series of tuples which I can then manipulate into a new dataframe . #CODE
What I'd like to be able to do is iterate through a series of pages and then add that content to a dataframe . I know that Pandas can convert nested dicts into dataframes , but really struggling to make it work . I've tried to use the answers at How to make a nested dictionary and dynamically append data but I'm getting lost .
When subsequently doing an apply / map , you'll usually want the function to return a Series ...

When I do ' g = df.groupby ( ' author_id ')' g is then just ' ' and I cant seem to be able to apply the function ...
Yes , it returns a groupby object . You apply the function using ` g.apply ( some_function )` , whether you can apply it depends on the function ...
The output of the unique function is a numpy array , which doesn't provide the apply method . You can create a ` Series ` by that array and then apply your function : #CODE

If they have different names you can ` drop_duplicates ` on the transpose : #CODE

Apologies if this is obviously documented somewhere , but I'm having trouble discovering it . I have two TimeSeries with some overlapping dates / indices and I'd like to merge them . I assume I'll have to specify which of the two series to take the values from for the overlapping dates . For illustration I have : #CODE

where ` id ` is an id for each point consisting of an ` a ` and ` b ` value , how can I bin ` a ` and ` b ` into a specified set of bins ( so that I can then take the median / average value of ` a ` and ` b ` in each bin ) ? ` df ` might have ` NaN ` values for ` a ` or ` b ` ( or both ) for any given row in ` df ` . thanks .

Append String Split to Pandas DataFrame
I am trying to append some of the words from a string into the Pandas Dataframe but after few trials it seems that I may not be succesful in it .
if you can't do that for some odd reason , you need to merge single dataframes using pandas concat :

You should use ` isnull ` ( rather than ` == None `) to check for ` NaN ` : #CODE

If your date / time column were in the datetime format ( see dateutil.parser for automatic parsing options ) , you can use pandas resample as below : #CODE

Resample a time series with the index of another time series
I have 2 data frames with identical columns but different datetime indices . I want to resample one of them to use the index of the other and forward fill data from the one on any dates in the index of the other in which there wasn't data for . #CODE
' a ' represents the data frame whose index I'd like to use as the resampling reference . ' b ' represents the data frame I'd like to resample and forward fill data . ' c ' represents what I'd like the results to look like .
To resample by a reference index , use ` reindex ` . #CODE

Error when trying to apply log method to pandas data frame column in Python

This is very similar to what pandas is doing under the hood anyways [ groupby , then do some aggregation , then merge back in ] , so you aren't really losing out on much .
Pandas doesn't complain , because now you have an array of Python objects . [ but this is really just cheating around the typecheck ] . And if you want to convert back to array , just apply ` np.array ` to it . #CODE

Yes , that's what I was looking for . I need to develop a better feel for stack and unstack .
You're very close . You just need to use the index to generate an explicit list from index to put the ` n ` in the middle . Then , with the list of dataframes , you can just use ` concat ` straight up . #CODE

Apply iterates through every item in the dataframe .

You can combine the columns of two DataFrames using ` concat ` : #CODE

is Pandas concat an in-place function ?
I guess this question needs some insight into the implementation of concat .
if I do ' pd.concat ( list_of_pieces )' , does concat allocate another 30G ( or maybe 10G 15G ) in the heap and do some operations , or it run the concatation ' in-place ' without allocating new memory ?

Then stack it and add the Date and the Hour levels of the MultiIndex : #CODE

I think that `` read_csv `` should a ) raise on an invalid passed dtype , and b ) just translate `` str `` dtype to `` object `` , open an issue ?

snap @USER . I tried to simplify it and left out the quotes on the attributes . I've edited it . Thanks .
Then you're looking for a ` pivot ` : #CODE
then do the pivot . :)

How does one append large amounts of data to a Pandas HDFStore and get a natural unique index ?
Also : when using large amounts of small files , it would be better to first get the current number of rows and up nrows each time with the length of the current dataframe at the end of each append action ( much better for performance ) . If this is a vague comment , let me know and i will work it out somewhere
ahh ... yes if you split over multiple files that would be true . when you append LOTS of rows , for sure turn off indexing ( `` index=False ``) , and ptrepack occasionally ( then index at the very end )

pandas resample documentation
So I completely understand how to use resample , but the documentation does not do a good job explaining the options .
So most options in the ` resample ` function are pretty straight forward except for these two :

That's because you are using join incorrectly .

I have multiple data-frames with stock prices that I want to align into a single data-frame that contains only the close prices for all stocks .
Right now , I have 8 data-frames I need to align this way . Is there any way to loop thru a list of data-frames and align them like above - instead of manually tying the data-frame names ( something like df [ 0 ] to df [ 7 ] figuratively speaking ) ?
I think I'm getting closer . I now loaded the dataframes into a dict where the key is the ticker of the stock and the value is the dataframe with OHLC quotes for the stock . Now I have a dict with 8 key / value pairs . The remaining question is : How do I join the dataframes into one by iterating thru my dict ?

Then just apply the aggregation function and boom you're done : #CODE

Commenting on the first code-formatted section : for the my_columns can I just replace that with my list.txt file rather than type out the individual column names ?

This code fragment looks fine , works in 0.11 with pretend data . Perhaps you could append a small example which exhibits this behaviour , and also post the traceback . ( Also , you can convert this to use the new ` loc ` , but results should be equivalent . )

I am trying to create a function that updates a pandas DataFrame stored that I have stored in a PyTable with new data from a pandas DataFrame . I want to check if some data is missing in the PyTable for specific DatetimeIndexes ( value is NaN or a new Timestamp is available ) , replace this with new values from a given pandas DataFrame and append this to the Pytable . Basically , just update a Pytable . I can get the combined DataFrame using the combine_first method in Pandas .

I'd like to consolidate the table to just show one row for each Id which has the most recent date .
which for each group grabs the row with largest ( latest ) date ( the argmax part ) .

Then use pandas ' groupby and / or resample functionality to do whatever you want to do .
I wonder whether ` to_datetime ` is faster that ` apply ( pd.Timestamp )` ? Certainly that'll be the only choice come 0.11.1 :)
`` pd.to_datetime `` should be faster if the cython doesn't raise ( in which case it essentially falls back on `` apply ( Timestamp )`` .

Then from the DataFrame , just ` stack ` and find the zeros : #CODE

Filtering based on the " rows " data after creating a pivot table in python pandas
I have a set of data that I'm getting from a SQL database and reading into a pandas dataframe . The resulting df is about 250M rows and growing everyday . Therefore , I'd like to pivot the table to give me a much much smaller table to work with ( few thousand rows ) .
Filtering and selecting from pivot tables made with python pandas

If you're dates aren't evenly spaced , you can ` resample ` ( by day ) first : #CODE
Thanks Andy . If I have datetime instead of dates and I do not want to resample as I want to keep my original index , is there a way ?
My datetime index is actually up to the second in accuracy . May be I can resample , then pad with limit and then extract only the values I want according to my original index ? Regarding the extraction what is the fastest way to do it ?
For now Andy , I think I will resample to the second and then use limit . How can I easily extract the values corresponding to my original index from my resampled dataframe ?
@USER Maybe have a column of originals=True and * don't * ffill that column after the resample .
1 resample the dataframe to the second
Resample for every second could be a rather a large overhead though . : s

But I don't want to resample the data .

transpose a subset columns in dataframe ( not groupby , need to create new columns )
I've a table which has two columns , the first one is the indice of the site , and the second is the number of states per hour during 24 hours . Thus for each site , I've 24 ( lines ) x2 ( columns ) data . How can I transpose the second column ( 24 lines data per site ) into the line which contains 24+1 columns with site indice .
What is ` sitesData ` ( why can't you just transpose that ) ? I'm struggling to understand exactly what you're asking here , for one thing , the code you've written only creates only one DataFrame ( newLine ) , presumably that's not what you want . Could you give an example with some fake data ?
Since the sitesData contains two columns , first one is the site indice , and the second is the siteState . sitesData contains n site information , thus has n*24 lines data . For each site , it has 24 lines in the siteData . I cannot simply transpose the sitesData .
This may also be possible using ` unstack ` ...

Apply ` histogram ` to each group . #CODE
Following your linked example in R , the horizontal axis should be dates , and the vertical axis should be the range of bins . The histogram values are a " heat map . " #CODE

storer format has its uses but since u cannot append data it is unsuitable for really large data sets . in addition u cannot query . you pay a small penalty for writing a table . what are u trying to accomplish ?
`` Table `` is an excellent format for this , with read times very comparable to `` Storer `` . However , more important is the ability create the data in a straightforward manner , but possibily writing in several append operations ( so you do not have to keep all of the data in memory at once ) , m2c

Pandas Dataframe Append or Set_Value
However after trying ` set_values ` and ` concat ` ( which is even more difficult ) . I am not able to get a suitable resolution . The other problem is that it has to stop before reading " END " ...
For me , ` concat ` is a last resort . It can get unavoidably hairy . And , for what it's worth , I don't think I've ever used ` set_value ` -- it is rarely necessarily to get so granular .

This will be considerably faster than using map ( especially python's builtin map ) .

Then you can grab out the difference using a ` shift ` : #CODE
The shift isn't inplace , you can set it to your column with ` df [ ' delta '] = ... ` , yes the ip will still be there .

Difference is that the axes returned by matplotlib ( via pandas ) is captured , and passed again with ` ax=ax ` . Its also more conform the preferred OO-style for using matplotlib .

I want to replace negative values in a pandas DataFrame column with zero .
Here is the canonical way of doing it , while not necessarily more concise , is more flexible ( in that you can apply this to arbitrary columns ) #CODE
Would .ix be slightly better than .loc since .ix is the more general form ? Or are there arguments in favor of loc over ix ? - #URL
You could use the clip method : #CODE
True , but the ` clip ` method is different ( has default parameters for ` lower ` and ` upper `) , and it has additional methods such as ` clip_lower ` and ` clip_upper ` .

You're looking for the transpose ( ` T ` ) method : #CODE

I want to resample it for 20 minutes and get the hit count for certain 20 minute time slot . ( eg . between ( 06.00.00-06.20.00 or 06.40.00-07.00.00 etc . ) . I can print the hit count for the whole data file as follows . #CODE

It did align the column names with the values , but lost the row labels . #CODE
that you want to align the input ( for example you then don't have to to specify all of the elements ) #CODE
I see . So the ` loc ` attribute of the data frame defines a special ` __setitem__ ` that does the magic I suppose .

But but ..., if you use dtype =o bject ( e.g. in x and df via OP's construction , which I agree is not best way ) then 2 , 3 and 10s are all ints ... it's almost always not worth worrying about anyway . This seems just like the transpose of OP's effort : s

stack columns in dataframe , and also stack index column
I have a problem stacking columns from a dataframe in pandas and in addition stack a column and making it an indexcolumn that not contains unique values . #CODE
I would like to stack the dummy variables ( aware , aware_2 , aware_3 ) , and also separately stack the resp variable and assosiate the stacked dummies with that variables . So it should be a new dataset with 2 stacked columns

... define your diff function ... #CODE
By the way , if you're in IPython Notebook , you may like to use a colored diff function

I am on window 7 , python 2.7.2 , pandas 0.11.0 , django 1.4 , wsgi and apache 2.2 . I have a pandas script that works fine if I run it directly with python and also works in ipython with %run . However , when I run pandas in my view i get " LookupError : unknown encoding : cp0 " . This only happens when using ols in pandas within the view . I'm also a little confused why py3compat.py is entering the picture as i'm using python 2.7 . Also , I have seen some posts about wrapping a printed variable in a str() , but I'm not sure how that would apply here . The whole traceback is : #CODE

How to replace values with None in Pandas data frame in Python ?
Is there any method to replace values with ` None ` in Pandas in Python ?
You can use ` df.replace ( ' pre ' , ' post ')` and can replace a value with another , but this can't be done if you want to replace with ` None ` value , which if you try , you get a strange result .
@USER replace is actually a very feature-rich ( read complicated ) function , the [( dev ) docstring ] ( #URL ) is really good though .

I kind of think that the first one is incorrect behaviour here . It's like the popular saying : " Writing aggregation functions is hard ... let's go write apply functions . "
If you pass a ` dict ` or ` list ` to apply , you will have item-by-item agg , IOW , you will get a ` Series `

I know i should do df.groupby ( ' channel ') and then apply function to each group .
In this case the apply value already returns the exact same df with only different cost values . So you can do : ` df = df.groupby ( ' channel ') .apply ( myfunc )` . But if you insist on only modifying the cost column this would also work : ` df [ ' cost '] = df.groupby ( ' channel ') .apply ( myfunc ) [ ' cost ']` . But i wouldnt use the latter since a change in the index might cause misalignment , even though it would work in this case .

There's nothing wrong with your code . My guess is that you have missing values in your data . Try a ` dropna ` or use ` missing= ' drop '` to Logit . You might also check that the right hand side is full rank ` np.linalg.matrix_rank ( data [ train_cols ] .values )`

( Some operating systems provide record-oriented files that have more complex internal structure than the common flat file . The above does not apply to them . )

Pandas : Filtering pivot table rows where count is fewer than specified value
I have a pandas pivot table that looks a little like this : #CODE

Is there anyway to use the mapping function or something better to replace values in an entire dataframe ?
I would like to replace the strings in the ' tesst ' and ' set ' column with a number

The way I've attempted this is not working out so well . I've tried to create two lists , and for each entry in the first list , search to find a match of the first two entries in the second list . If there's a match , then my script would delete the row in the second list and append the pair of dates . With each row representing the complete number of correspondences between a questioner and an answerer , I would then write a script to convert the row into the edge / attribute data . I've been using this as some sort of guide .

@USER give an index to the Series when you apply ; they will become column names
I'm having a little trouble with the amount of memory that this method consumes and I'm wondering if you could give me a little advice . I have a DataFrame that contains about 8000 rows , each with a string containing 9216 space delimited 8-b it integers . This is roughly 75MB , but when I apply the last solution verbatim , Python eats 2GB of my memory . Can you point me in the direction of some source that would tell me why this is , and what I can do to get around it ? Thanks .

Sometimes the oddities of a feature mean something , i.e. contains information . And I might need to circle back with the client about oddities I find . Or if I'm going to replace values I have to make sure I'm not steamrolling over something recklessly .
I see that pandas makes it easy peezy to replace values but in the beginning I just want to look .

Removing duplicates in a groupby using apply

If you're doing it on all columns , you can just do ` df.ix [ df.type == 7 ] = 0 ` . Or of course if you have a list of the columns whose values you want to replace , you can pass that list in the second slot : #CODE

You can append using ` to_csv ` by passing a file which is open in append mode : #CODE
Use ` header=None ` , so as not to append the column names .

With regards to Pandas : df.merge() method , is their a convenient way to obtain the merge summary statistics ( such as number of matched , number of not matched etc . ) . I know these stats depend on the how= ' inner ' flag , but it would be handy to know how much is being ' discarded ' when using an inner join etc . I could simply use : #CODE
But thought this might be implemented already . Have I missed it ( i.e. something like report=True for merge which would return new_dataframe and a report series or dataframe )

Finding median with pandas transform
I needed to find the median for a pandas dataframe and used a piece of code from this previous SO answer : How I do find median using pandas on a dataset ? .
It seemed to work well , so I'm happy about that , but I had a question : how is it that transform method took the argument ' median ' without any prior specification ? I've been reading the documentation for transform but didn't find any mention of using it to find a median .
Basically , the fact that .transform ( ' median ') worked seems like magic to me , and while I have no problem with magic and fancy myself a young Tony Wonder , I'm curious about how it works .
When you pass the argument `' median '` to ` tranform ` pandas converts this behind the scenes via ` getattr ` to the appropriate method then behaves like you passed it a function .

The problem you are facing is that your excel has a character that cannot be decoded to unicode . It was probably working before but maybe you edited this xls file somehow in Excel / Libre . You just need to find this character and either get rid of it or replace it with the one that is acceptable .

I want to merge the two together so that the ' NAME ' columns match up , basically just add the two Sdev_I1 / Sdev_I2 to the end of the first sample . I've tried ... #CODE
Seems you didn't strip the comma . And I guess there're some spaces in your ' NAME ' column . It's better print your dataframe or provide your csv files .
Seems you didn't strip the comma symbol in the second csv , you might try to use converters to convert them : #CODE
You can also use replace : ` df2.replace ( r ' ( . * ) , ' , r ' \1 ' , regex=True )`

Following advice in earlier posts , rather than append each new row to the dataframe I construct a dataframe with the historical file and append 5000 " blank " records to it with correct timestamps . I then write each new row over a blank row , filling any rows if timestamps are missing and updating pointers .
It is true that keeping the data all the same type will speed this up . However , I would say the main conclusion is not " if efficiency is important , keep all your columns the same type " . The conclusion is if efficiency is important , do not try to append to your arrays / DataFrames .
This is just how numpy works . The slowest part of working with numpy arrays is creating them in the first place . They have a fixed size , and when you " append " to one , you really are just creating an entirely new one with the new size , which is slow . If you absolutely must append to them , you can try stuff like messing with types to ease the pain somewhat . But ultimately you just have to accept that any time you try to append to a DataFrame ( or a numpy array in general ) , you will likely suffer a substantial performance hit .
Thanks for that BrenBarn . I can see that @USER is right and my original question has become too long . In fact I only append every 5000 rows and the loop being timed is actually writing data into a row appended when the main file was read to create the dataframe .
@USER : The same logic applies , though . If you write data to a DataFrame with multiple types , it actually has to do two writes to two separate numpy arrays . I wouldn't expect this slowdown to be as much as the append , but it still does involve two separate numpy operations instead of one .

replace string / value in entire dataframe
I have a very large dataset were I want to replace strings with numbers . I would like to operate on the dataset without typing a mapping function for each key ( column ) in the dataset . ( similar to the fillna method , but replace specific string with assosiated value ) .
Use replace #CODE

I have floating point numbers . :( Most of those counts will be 1 . This may still useful for cumulative distributions though , thanks . Can I resample somehow like I can do with TimeSeries ?
Since ` hist ` and ` value_counts ` don't use the Series ' index , you may as well treat the Series like an ordinary array and use ` np.histogram ` directly . Then build a Series from the result . #CODE

I am downloading price data from bloomberg and want to build a DataFrame in the fastest and least memory intensive way . Let's say I submit a data request to bloomberg through python for the price data for all current S P 500 stocks from 1-1-2000 to 1-1-2013 . Data is returned by ticker and then date and value , one at a time . My current method is to create a list for the dates to be stored in and another list for the prices to be stored in , and to append a date and price to each list as they are read from the Bloomberg data request response . Then when all the dates and prices are read for the particular ticker , I create a DataFrame for the ticker using #CODE
I'm not 100% sure which your after , but you can ` concat ` a list of DataFrames : #CODE
or do an outer merge / join : #CODE
See the merge , join , concatenate section of the docs .

Timestamps have a ` replace ` method ( just like datetimes ): #CODE
It works fine but I wonder if it will be more efficient to use read_csv with differents parameters . Because read_csv set date to today and we replace date ... maybe we could do this using only one step ?

It's usually makes sense to read first , and then perform transformations to your requirements ( as these are usually efficient and readable in pandas ) . In your example , you can ` pivot ` the result : #CODE

Does there exist a built in function in pandas that will Combine the two dataframes , using Col A as keys and updating value in Col B if it exists , otherwise append . Such that the output of this function on X and Y is #CODE
I've looked into merge and update and append but they don't seem to act the way I want , update updates by index instead of Col A value , merge doesn't overwrite , ect . Thanks !
One way to do this is to ` concat ` then drop the duplicates : #CODE

And then you can append the label column to the index :
Is there a way to create timezone aware Series and then insert them into a dataframe / make them an index ? #CODE
Struggling a lot with this issue , MultiIndex loses tz in many other conditions too .

@USER I've cobbled something together ... I think I prefer using an outer concat tbh .
If you know each of these are the same length then you could create the DataFrame directly from the array and then append each column : #CODE
However , if they are variable length then this will lose some data ( any arrays which are longer than ` prcpSeries `) . An alternative here is to create each as a DataFrame and then perform an outer join ( using ` concat ` ): #CODE

Problem is that this is a pd.Series ( with Multi-Index ) . Currently I iterate over it to build a DataFrame again . I am sure that I am missing a method . Basically I want do drop 1 column form a DataFrame and then " rebuild it " so that one column is summed and the rest of the fields ( which are the same ) stay in place .
replacing this in code just drop those whole rows ... Think it is probably to do with the NaN == issue in pandas ?

Now you can apply the respective ` fillna ` s , one cheeky way : #CODE

I think you're going to be much better off getting this into one DataFrame , so consider using a MultiIndex . Here's a toy example , which I think will translate well to your code : #CODE
@USER No it should align it ( and put NaNs where data isn't present ) .

I'm trying to resample daily data to weekly data using pandas .
You can only resample by numeric columns : #CODE
you could do say median of the dates in the re sample interval if u really wanted
Thanks Andy . All the values in my dataframe are numeric . The resampling is working in my code . But I need the date as a column in my resampled df . I don't get that when I resample using " 7D " . The date column is dropped

this method is not very forgiving if there are missing data . If there are any missing data in same1 , same2 , etc it pads totally unrelated values . Workaround is to do a fillna loop over the columns to replace missing strings with '' and missing numbers with zero solves the problem .

yep .... kind of an `` ix `` artifact ... I wouldn't recommend users do it that way in fact ( as inherently the accessor is so overloaded )

now pivot that table so that the week_days are as columns ( could also change the needdays to string formats of days but leaving that for you . #CODE

I've noticied the same behavior implemented in Excel when building a pivot table with multiple columns .
You could do this once this pivot table has been created : #CODE

And now I use ` split ` and ` replace ` to split by `' , '` and remove all `' : '` #CODE
@USER oh I see , I'd missed the : bit . Do the regex replace * first* : ` s2.str.replace ( ' :\ d+ ' , '') .str .split ( " \s* , \s* ")` .

I've tried with to replace tzinfo but failed to find a proper solution . I'm thinking about something like the following ( pseudo code ): #CODE

I'm using pandas to do an ` outer ` merge on a set of about ~ 1000-2000 CSV files . Each CSV file has an identifier column ` id ` which is shared between all the CSV files , but each file has a unique set of columns of 3-5 columns . There are roughly 20,000 unique ` id ` rows in each file . All I want to do is merge these together , bringing all the new columns together and using the ` id ` column as the merge index .
I do it using a simple ` merge ` call : #CODE
The problem is that with nearly 2000 CSV files , I get a ` MemoryError ` in the ` merge ` operation thrown by pandas . I'm not sure if this is a limitation due to a problem in the merge operation ?
The final dataframe would have 20,000 rows and roughly ( 2000 x 3 ) = 6000 columns . This is large , but not large enough to consume all the memory on the computer I am using which has over 20 GB of RAM . Is this size too much for pandas manipulation ? Should I be using something like sqlite instead ? Is there something I can change in the ` merge ` operation to make it work on this scale ?
I think you'll get better performance using a ` concat ` ( which acts like an outer join ): #CODE
This means you are doing only one merge operation rather than one for each file .
As of memory , you should be able to use a gen expression instead of list comprehension ... ( not sure about the inner workings of the ` concat ` though )
@USER Good spot btw ! ( tbh I didn't know concat would accept a generator ! )
@USER : brilliant solution , works , thank you ! could you explain why it works better though ? is it that each individual merge takes up memory that is not garbage collected in time and then you run out of memory ?
@USER well , the main thing is that you are doing lots more merge operations , and also it only builds * one * DataFrame ( again this is an expensive operation ) . I wonder if there a leakage bug , I don't see why it should run out of memory ... ( I see why it would be much slower though ) .

If you enable logging in python , then when an exception is received you can use the method ` logging.exception ` to log when an exception has been caught - this method will print out a nicely formatted stack trace that shows you exactly in the code where the exception originated . See the python document on logging for more information . #CODE

If you want to join a list of DataFrames like this you could use concat : #CODE
I'm note sure how you want to join the separate blocks together , but this leaves them as a list . You can concat from there however you desire .
A separate question from how to work with the multiple blocks . Assuming you've got the list of ` Dataframe ` s , which I've called ` res ` , you can use pandas ' concat to join them together into a single DataFrame with a MultiIndex ( also see the link Andy posted ) . #CODE

You get one string per line and the other cells are ` NaN ` , then the math to apply is to ask for the ` max ` value : #CODE

I knew there was something , I just couldn't find it ! FWIW the ` isin ` method is about 10 times faster than using ` apply ` on my particular dataset .

Insert , puts a series into an existing frame #CODE
Didn't realize you can specify an axis for concat , thanks !

then apply this function across each group : #CODE
In order to get ` 1 , 2 ,... ` you couls use ` np.arange ( 1 , len ( x ) + 1 )` .
Awesome , thanks @USER . Knew it was going to be a groupby , but couldn't figure out how to apply it properly .

Use merge #CODE

here cols is df [ ' C '] and values is df [ ' D '] . We group those two things by cols and then apply the aggregating function , which in this case is np.size . Each row looks like #CODE

How can I apply the testfunction on x which returns the two c [ 1 ] values ?
Do you mean to say , you need to apply testfunction on df1 and df2 individually and store the result somewhere ?
Like ` y = map ( testfunction , x )` ? This will apply testfunction to every item of the iterable you pass and return a list of the results .
Thank you very much Brian for your answer ! I just spent 2.5 hours on this problem . I thought that I need to apply a " for loop " ...
Use the built-in map function #CODE
This will apply the function ` testfunction ` to every item in ` x ` and return a list of the results .

melt is a reverse unstack #CODE

You can use the ` normalize ` argument of ` bdate_range ` ( which defaults to True ): #CODE
hmmm ... I did not know about normalize

I am trying to join ( vertically ) some of the tuples , better to say I am inserting these tuples in the dataframe . But unable to do so till now . Problem arises since I am trying to add them horizentally and not vertically . #CODE
Is it possible or do I have to iterate over each word in tuple to use insert
alternatively if you have many of these rows that you are going to append , just

Length : 372 , Freq : None , Timezone : None
Length : 371 , Freq : M , Timezone : None
IIUC you can ` filter ` : ` grouped2 = data.groupby ( lambda x : x.year ) .filter ( lambda x : len ( x ) > 11 ) .reset_index ( drop=True )`

Elaborating on answer using melt , both helped me understand pandas a bit more , but there was more unknowns for me in " melt " answer : #CODE
I figured the key is to use ` melt ` , and a bit of acrobatics afterwards . So here's your DataFrame : #CODE
I stepped through the large melt method and added it to the above question . Thanks for the help .

If you make ` rows ` a boolean array of length ` len ( df )` , then you can get the ` True ` rows with ` df [ rows ]` and get the ` False ` rows with ` df [ ~rows ]` : #CODE
Or rather , ` rows = np.random.binomial ( 1 , percentile*100 , size=len ( df_source )) .astype ( ' bool ')`

your can only save a Panel4D as a table ( append works ) . put an example of what your are trying to do

If I was attempting to debug it I would do the simple thing . Cut the file in half - what happens ? If ok , go up 50% , if not down 50% , until able to identify the point where its happening . You might even want to start with 20 lines and just make sure it is size related .

Is there any equivalent function like " apply " for rows . Since apply seems to work on " columns " only .
@USER ` apply ` takes an ` axis ` kwarg to toggle between rows and columns

To get the unobserved values filled , we'll use the ` unstack ` and ` stack ` methods . Unstacking will create the ` NaN ` s we're interested in , and then we'll stack them up to work with . #CODE
I had to slip that ` fillna ( 0 )` in the middle there so that the ` NaN ` s weren't dropped . ` stack ` does have a ` dropna ` argument . I would think that setting that to false would keep the all ` NaN ` rows around . A bug maybe ?

The result you currently have is a ` Series ` with a ` MultiIndex ` , so all the usual rules will apply . If your result is called ` res ` , then ` res.ix [ False ]` gives you just the Falses , indexed now by only ` user_id ` . Likewise for ` res.ix [ True ]` . See the docs .

Stack / Unstack dont seem to work .

Note that you get the exact same value , 0.939085 , for the ' identity ' entry in the second case too , for ' CY135502 . ' I suspect there's something wonky about your input data's alignment before using unstack .
Maybe you are looking for ` unstack ` : #CODE
Hmm ... I'm running into a bit of trouble here . The matrix includes 1.00 identities ( perfect matches ) across the diagonal . However , when I apply the " unpack " function , all of those turn into a value 0.939085 , for which I'm not sure how it's happening . In less than a few minutes , I will update the original question with the data set I'm working with .
Well now , somehow I managed to fix the problem by not setting the index to ' Accession ' while importing the data . Instead , I called on ` .set_index() ` function only when I really needed the index to be ' Accession ' . Hence , when I wanted to stack the data ( as opposed to unstack it ) , what I ended up doing was doing : #CODE

@ DD MM YYYY HH MN SS Hs Hrms Hma x Tz Ts Tc THmax EP S T0 2 Tp Hrms EPS

Well I don't know anything about robot_detection module , but a DataFrame with 50000 lines doesn't sound that much , really . And looking at your for-loop it seems like the function ` is_robot ` is not applied to the whole DataFrame but to each line , which I assume to contain one entry of the log . Thus I suggested to use ` apply ` instead .
I see , I guess then you can split the DataFrame , apply ` value_counts() ` to each split , and then sum all the splits to get the final result .

I'm trying to join two datasets in Pandas . What I want to do is put the results of ` df2.groupby ( ' BuildingID ')` into a new Series in ` df1 ` . The reason being that the building ID is the level I'll be working with , while the ItemID is a collection of items within the building .
You are using groupby without doing anything group specific . Why not just join df1 and df2 to create a df with a MultiIndex of ' BuildingID ' and ' ItemID ' ? The groupby object also contains a all groups , you probably dont want to store it like that .
That works fine with the simple data set here ( and is essentially what I want to do ) but when I try it on the full data set it tells me ` Exception : Reindexing only valid with uniquely valued Index objects ` at the ` merge ` step .
I'm also on v0.11 . I'm trying to cut down the real dataset now to see what's the simplest version that produces the error .

Thanks Very Much ! In addition to this I want to apply some function to each group ? How to access groups one by one ?
I'm splitting the dataframe since it is too large . I want to take the first group and apply the function , then the second group and apply function etc . so how do I access each group ?

I would unstack the statistics from ` describe() ` , then you can simply use ` sort() ` , so : #CODE

so len of the frame is the answer , unless I misunderstand what you are asking #CODE

I have looked at ` pivot ` and ` groupby ` but didn't get the exact form .
HINT : possibly this is solvable by ` pivot ` but I haven't been able to get the form
Probably not the most elegant way possible , but using unstack : #CODE
I cooked up my own pivot based solution to the same problem before finding Dougal's answer , thought I would post it for posterity since I find it more readable : #CODE

my problem is that the first row of the Excel file contains valid data and not the column names . so using " df.columns = [ ' W ' , ' X ' , ' Y ' , ' Z ']" I would lose data ... so I need to append the col names on top of existing data then change the col names ....

subset before you apply #CODE
2nd part , row-wise apply , returning ' sum ' of elements in A and B columns . You can
pretty much what what you want in apply . #CODE
This is cool - how would I do a row-wise apply taking two values from different columns but in the same row into the apply function ? Say two columns were first / name and family / name strings and I wanted to take value1 and value2 and do stuff to them .

Join the index #CODE
Here is another way ( and allows various ways of joining if you specify ` join ` kw ) #CODE

neat apply tricks #CODE

When the increment is anything other than .125 , I find that the new index values do not " find " the old rows that have matching values . ie there is a precision problem that is not being overcome . This is true even if I force the index to be a float before I try to interpolate . What is going on and / or what is the right way to do this ?
Yes , I guess it's datelike , but I really need no special / clever features except for one decimal of years . The final output ? Is this : #URL ie I have data for several years , and I want to interpolate column values to a ( finer ) grid in order to animate smoothly .
Thanks . No , I wasn't expecting reindex to do the interpolate . As I said , ths was the first step / setup for interpolate . " 1 / 8 which can be exactly done in binary " is a main insight I was missing . But I still don't see that I got what I asked for . Especially when the example fails even if the index is a float .

I am a new python / pandas user . I am trying to get a time delta ( in secs ) between dynamic ranges ( based on diff in values ) of a time series data frame . My sample dataframe is : #CODE
I am trying to get the time delta between price diff of 4 . Once the diff in price is reached , that price point become the ' starting point ' for the next calculation and so on .

I have the following problem . In my pandas data frame , I had couple of records ( specifically , four of them ) that were ( unintentionally ) duplicated , and I dropped them with ` drop_duplicates ( take_last = True )` . Now , in one of the columns I have strings that I've been trying to map on integer values using ` unique_vals , int_representation = np.unique ( df.x , return_inverse = True )` but I found that for some reason the number of unique strings in my original column , and the number of unique integer values in ` int_representation ` is different , which doesn't make any sense .
So , I am going through the original data frame now , trying to understand the reason for that , and what I found is that all of a sudden I am getting an error when accessing the data frame's index where one of the dropped duplicates was located . It's really strange coz , say , ` df.xs ( 10 )` works , ` df.xs ( 11 )` doesn't , and ` df.xs ( 12 )` works again . And this happens exactly four times , for indices corresponding to records that had been removed . I have also checked that when I don't drop , the problem disappears .
Just wanted to map those strings to integers ( I'm calculating some stats on a text , and I need words in a number representation ) , and was hinted here that what I call keys in my code is an easy way of doing that , which is actually true .

@USER Take the leap and try ` eval ` ! It does alignment for you . Currently the ` dtype ` must at least be numeric , the caveat * there * is that mixed numeric ` dtype ` frames will be slower , especially large ones since a new array must be allocated to hold the promoted , concatenated array , but they should still be faster than using direct ` numexpr ` and plain ol ' Python .
@USER . I will try eval() after I figure out how to import the eval function . Thanks .

Thanks for the idea . I tried , it but it returns True for everything . I will analyse the map section of the statement , as this is the part most confusing to understand about this specific issue , and Pandas in general . Thanks again . here is what I got as results :
Name : dtu_topic_split , dtype : bool
To select rows with one item you're interested in , you can ` apply ` a ` lambda ` function . For example : #CODE

here's the reason for this : since you are effectively masking certain values on a column and replace them ( with your updates ) , some values could become `` nan `

How to speed up Pandas multilevel dataframe shift by group ?
I am trying to shift the Pandas dataframe column data by group of first index . Here is the demo code : #CODE
the problem is that the ` shift ` operation is not cython optimized , so it involves callback to python . Compare this with : #CODE
How about shift the total DataFrame object and then set the first row of every group to NaN ? #CODE
similar question and added answer with that works for shift in either direction and magnitude : pandas : setting last N rows of multi-index to Nan for speeding up groupby with shift

thanks for mentioning ` grouper ` , as it is not documented ! I finally found a solution to change rows in original dataframe while iterating over its grouped object , using ` grouped.grouper.indices ` . I had to use it because I have duplicate DateTime indices in the dataframe . Also the transformation is too complicated to fit a ` grouped ` then ` apply ` paradigm , it involves clustering and filling in multiple dataframes at once while going through each group .

In the master branch of pandas on GitHub the [ ` hist `] ( #URL ) method has a ` figsize ` parameter for this exact feature . This is not present in earlier versions , as far as I know .

resample at the same frequency ( the additional day that we added makes this pad
It would be better if I didn't insert all these minute-rows .

I am using the JIRA Python module which is an extension of the REST API to automate the process of deleting and creating issues in JIRA . I am trying to create issues in JIRA using a ' for ' loop in my python script that uses imported data that I have collected from another database . I need to format the fields when creating the issue so the data I have can align properly with the appropriate fields in JIRA . My Python code is below for creating the issues and storing the data to put into JIRA which is being stored in custom variables :

Probably your ` Id ` column is actually composed of strings , not ints . What does ` map ( type , df.Id )` return ?

You could try pympler , which worked for me the last time I checked . If you are just interested in the total memory increase and not for a specific class , you could you an OS specific call to get the total memory used . Eg , on unix based OS , you could do something like the following before and after loading the object to get the diff . #CODE

I am attempting to parse .dta files and enter each row into a separate table . The .dta files are composed of a lot of different variables , and I want to insert each variable into a separate " variable table " . I am using the new .dta reader from pandas , which is named statareader . I do not have a lot of experience with python , and was hoping for a little help with my syntax . Also I am using python 2.7.5 #CODE
You are generating invalid SQL code . An ` INSERT ` statement does not accept ` ( )` parenthesis around the table name . To quote a table name ( which makes it case sensitive , so be careful ) put double quotes around it : #CODE
The above example also uses proper SQL parameters for the row data ; you generally want to let the database prepare a generic statement and reuse the prepared statement for each insert . By using SQL parameters you not only ensure that ` row [ a ]` is properly escaped , but also let the database prepare the generic statement . I used the default paramstyle format for pg8000 .

You could use ` apply ` , e.g. : #CODE

I cannot directly use the unstack command because it requires a multiindex and I only have a single-level index . I tried putting in a dummy index that all had the same value , but I got an error " ReshapeError : Index contains duplicate entries , cannot reshape " .
Not sure how generalizable this is . I call this the groupby via concat pattern .
Essentially an apply , but with control over how exactly its combined . #CODE
You can use ` groupby ` , ` apply ` , ` reset_index ` to create a multiindex Series , and then call ` unstack ` : #CODE

dan , I have edited the question a little bit . Instead of field being lang , now I want to look at the field which has the type bool .

Please help ! I don't know how to create a function that will iter over each column and do this . I have been thinking about using the merge function but I'm not sure how to do this .

Consider separating the operations : first create DataFrame ( s ) from xml , maybe then merge / concat / reshape , then do computations . At the moment it's not clear ( at least to me ) which part your question is about ... and mixing together xml parsing and computation makes for confusion !

I have a DataFrame with an index called ` city_id ` of cities in the format ` [ city ] , [ state ]` ( e.g. , ` new york , ny ` containing integer counts in the columns . The problem is that I have multiple rows for the same city , and I want to collapse the rows sharing a ` city_id ` by adding their column values . I looked at ` groupby() ` but it wasn't immediately obvious how to apply it to this problem .

So its not necessary to groupby month first , unless you have other reasons to do so . If so , you can also apply the last groupby on the monthly df as well .

You don't seem to have the right terminology , in pandas you are looking for either [ merge ] ( #URL ) or [ join ] ( #URL ) .
thanks . merge does work .
Use ` merge ` function : #CODE

in general you DO want to build these ( rather than replace ) , much more efficient , but in case you actually want to do what you are doing ( and there are valid use cases ) , e.g. say you just want to replace a part of the frame ...
prob better off using concat then : #URL building then modifying is not very efficient
How to join all rows of sng1 to nm . nm.loc [: , sngl.columns ] = sngl.loc [: ] .values doesn't work .

values gives u back the underlying ndarray ; it allows you to map the values with a new index ( the events ); passing in the series to the Series constructor and a new index will reindex using the passed index , not what u want in this case

I am unable to do this in using pandas merge function and I'm not sure where to even start .
If it fits in memory , you can ` merge ` the two dataframes with an ` outer ` method base only on ` chrom ` column , then filter your result by doing the range inclusion math : #CODE
This is how merge works : chromstart and chromend exist in both dataframe , but we merge on name only . It will add suffixes to the duplicated columns in the resulting dataframe . Have a look at merge documentation

Use the replace method on dataframes : #CODE

Usually when I want to put content of a file in a data frame I make a loop over the lines of the file , split the lines into the fields and use this values to specify a dictionary . After reading one line I append the dictionary to a list ( so , the number of dictionaries in the list is equal to the number of lines in the file ) . Then I use the list of dictionaries to construct a data frame .

Ok it was simple . the len ( dataframe ) was the easy answer .
print len ( df3 )
Ah . thanks tshauck , I guess that was actually what i was trying to do . The result to that test actually printed the count for each field i.e.Field1 = 10 and next line Field2 = 10 . I guess you could also apply the count to one particular Field ?

@USER : This solution only works if the index is properly sorted * before * you drop one of the levels . Otherwise , the new index will be sorted differently to the old one and the result will be incorrect ! See [ this ] ( #URL ) for more details .
` unstack ` the problematic index level ( s ) away
` stack ` the problematic index level ( s ) back .

can you post the full stack trace ?
Assuming ` sequence.apply ` applies the lambda to each element in the sequence , ` sequence.apply ( lambda x : x 0 )` yields a sequence of Boolean values , and ` sequence.apply ( lambda x : x 0 ) .apply ( lambda x : sum ( x ))` attempts to sum each Boolean value , resulting in a `' bool ' object is not iterable ` -kinda error . You get a similar error from : #CODE

what is the quickest / simplest way to drop nan and inf / -inf values from a pandas DataFrame without resetting ` mode.use_inf_as_null ` ? I'd like to be able to use the ` subset ` and ` how ` arguments of ` dropna ` , except with ` inf ` values considered missing , like : #CODE
The simplest way would be to first ` replace ` infs to NaN : #CODE

Quick way to replace all occurences of a value in a pandas data frame with NA

How to apply to_datetime or with sort_index so that the dates are sorted ?
I don't know how to apply to_datetime , because the output of the dates are not sorted . #CODE

At the moment you can do this with an apply or the delta attribute : #CODE

I've got data in .csv files , which I'm trying to read using the pandas read_csv function . Date and time are in two separate columns , but I want to merge in them into one column , " Datetime " , containing datetime objects . The csv looks like this : #CODE

You can do this with ` melt ` : #CODE

Rather than do a ` pivot ` * , just ` set_index ` directly ( this works for both examples ): #CODE
* You're not really doing a pivot here anyway , it just happens to work in the above case .

I have a variable storing a string timestamp ( in Unix time ) that I'd like to append to an existing Python Pandas Data Frame as a column . That is , I'd like the column to contain 143 repeats of the single timestamp , because this is the observation count of the data frame . Thanks .

A simple way would be to copy the df and reverse the index yourself , and then append both together . #CODE
` copy() ` doesnt prevent that . Using ` copy ` seems to only apply on the data , not the index . Which is a bit strange , reading your spreadsheet twice might be the best option , altough not terribly efficient .

See documentation for ` hist ` and the documentation for ` np.histogram ` .

Thinking about it , perhaps it makes more sense to do the pivot first : #CODE

I have vehicle information that I want to evaluate over several different time periods and I'm modifying different columns in the DataFrame as I move through the information . I'm working with the current and previous time periods so I need to concat the two and work on them together .
@USER It's not really clear what you expect that to do , but using groupby ensures each groups is a DataFrame and then , for example , you can apply a function to that .
@USER if you really wanted to do it manually you could use : ` g = df1.groupy ( ' time ') ; [ g.get_group ( x ) for x in g.groups ]` , but I recommend apply .

Thanks falsetrue ! this has been SOO helpful I can apply this to everything now ! :) I can't tell you how long this has taken me !

I have been doing Pandas , in which I apply a ` .groupby ( ' Subtype ')` on the dataframe , but after I do that , I'm not sure how to proceed further . Any help would be appreciated !

I think method suggested by @USER is better only because he is using build-in functionality of pandas library . He suggests define mode as " a " . " A " stands for APPEND
You can append to a csv by opening the file in append mode : #CODE
If you read that and then append , for example , ` df + 6 ` : #CODE
You can specify a python write mode in the pandas to_csv() function . For append it is ' a ' .

syntax error when attempting to insert data into postgresql
I am attempting to insert parsed dta data into a postgresql database with each row being a separate variable table , and it was working until I added in the second row " recodeid_fk " . The error I now get when attempting to run this code is : pg8000.errors.ProgrammingError : ( ' ERROR ' , ' 42601 ' , ' syntax error at or near " imp "') .
Eventually , I want to be able to parse multiple files at the same time and insert the data into the database , but if anyone could help me understand whats going on now it would be fantastic . I am using Python 2.7.5 , the statareader is from pandas 0.12 development records , and I have very little experience in Python . #CODE
** NEVER interpolate values directly into SQL like this** , you leave yourself critically vulnerable to [ SQL injection ] ( bobby-tables.com) . Also , in future please mention your PostgreSQL version in questions . Thanks !
The syntax error probably comes from strings ` {} ` that need quotes around them . ` execute() ` can take care of this for you automtically . Replace #CODE
The table name is completed the same way as before , but the the values will be filled in by ` execute ` , which inserts quotes if they are needed . Maybe ` execute ` could fill in the table name too , and we could drop ` format ` entirely , but that would be an unusual usage , and I'm guessing ` execute ` might ( wrongly ) put quotes in the middle of the name .
What do you intend with ` z ` here ? As it is , you loop z from `' 1 '` to `' 9 '` before proceeding to the next for loop . Should the loops be nested ? That is , did you mean to insert the contents ` dr ` into nine different tables called ` tblv001 ` through ` tblv009 ` ?

read the column with no converter , then just replace on all at once , something like `` df [ ' column '] .replace ( ' . ' , '') .replace ( ' , ' , ' . ') .astype ( float )`` should be much faster as its done as array ops
The real case is a bit longer than the provided example and includes some stripping ( spaces ) , columns del , string concat , renaming / replace .... The decimal marks are replaced for three columns : USD Sales , Qty , USD_EUR exchange rate . Based on them EUR sales and EUR unit prices are calculated . In the initial file we also have a ' - ' , for some other reason , before the exchange rate to be fixed ( " - " , "") . The result is : #CODE
The 30'000 x 150 DataFrame is processed in less than 15 seconds :-) :-) including all the other things I did not detailed here too much ( stripping , del , concat , .. ) . All that read / write / read / write had been deleted from the code skipping the ' converters ' during the read_csv .

You can just do it using ` cut ` on the groupby ( you can specify the bins if you want ) , or groupby however you want , using the data above ( that's why I am reading via ` StringIO `) #CODE
yeh .... `` cut `` is pretty interesting here , can specify your own bins ( which is prob what the op wants to do ); easier than actually specifyng the groupby mapping , though could do that too
After you've constructed the timeSeries you can use the resample function : #CODE

Also , a lot of Pandas methods have a ` na ` argument , which let you decide which value you are going to use to replace not-available values
NaN is used as a placeholder for missing data consistently in pandas , consistency is good . I usually read / translate NaN as " missing " . Also see the ' working with missing data ' section in the docs .
After years of production use [ NaN ] has proven , at least in my opinion , to be the best decision given the state of affairs in NumPy and Python in general . The special value NaN ( Not-A-Number ) is used everywhere as the NA value , and there are API functions ` isnull ` and ` notnull ` which can be used across the dtypes to detect NA values .

` make_pivot_table ` just returns a pivot table using pandas ` pivot_table ` function .

I would do this using a ` shift ` and a ` cumsum ` ( here's a simple example , with numbers instead of times - but they would work exactly the same ): #CODE
Then you can use this in a groupby ` apply ` : #CODE
Thanks Andy ! After a long timeI got an answer :) , But Why i get this error ? AttributeError : ' Timestamp ' object has no attribute ' shift '
@USER it looks like you've tried to apply shift to a Timestamp rather than a column / Series ( not sure how you did that though ) .

Basically the current excel workbook runs some vba on opening which refreshes a pivot table and does some other stuff .
Then I wish to import the results of the pivot table refresh into a dataframe in python for further analysis . #CODE

[ Using Python3 ] I'm using pandas to read a csv file , group the dataframe , apply a function to the grouped data and add these results back to the original dataframe .
Basically I'm trying to group by ` cc ` and calculate the percentile rank for each value in ` total_value ` within that group . Secondly I want to apply a flow statement to these results . I need these results to be added back to the original / parent DataFrame . Such that it would look something like this : #CODE

so pandas doesn't know how to join the new sales figures back onto the old dataframe .
Another option is to use an apply : #CODE
Another option is promote the ' day ' level to a column and then use an apply .

Python 2.7 and Pandas merge 2 csv files with Forex Data
I want to merge the data based on the date time column which is the first column in both files . as you can see the second file does not have exact same records as the first file , so missing some data from second file , but thats ok . I want to bring the Close column from second file to the first file based on matching date tile field between them
And it should merge them using the same columns .
Matt , will the above merge both csv files based on common matching date time in the row ? and also will it just add the close or column #4 from the second csv file to the merged file ?
If I remember correctly it will just merge the common columns and add keep all the unique columns as they are .
If I understand your question correctly , Andy had the right idea in his comment . You don't want to merge to two , you want to copy the column of one into the other . #CODE
A few ways . Either avoid them from the beginning by not passing ` names =[ ]` to ` read_csv ` or replace them before writing with ` df.columns = range ( len ( df.columns ))` . See the docs [ here ] ( #URL ) for reading / writing options .

All I keep getting is index contains multiple values which it indeed does , but why wouldn't it understand that there are similar and map them to the same line as I did in my desired output ?
Isn't that " Transpose " . Correct me if I am wrong .
Can you clarify exactly what you'd like ? Does ` pivot ` give the correct output or not ? If not , what is incorrect about it ? Do you just want the values for 2012-11-13 ?
@USER so i clearly say what the ` pivot ` is giving me index contains multiple values . I also listed what I would like as an ouput .
Is ` df.pivot ( index= ' sample_date ' , columns= ' metric__name ' , values= ' sample ') .ix [ ' 2012-11-13 ']` what you want ? Your desired output doesn't make sense any other way . I don't know what you mean buy " understand that [ they ] are similar and map them to the same line " .
If list of functions passed , the resulting pivot table will have hierarchical columns
Regarding the difference between the two , I think ` pivot ` just does reshaping ( and throws an error if there is a problem ) , whereas ` pivot_table ` offers more advanced functionality , aka " spreadsheet-style pivot tables " .
can you explain the difference between ` pivot ` and ` pivot_table ` ?
I think it is purely that [ pivot_table ] ( #URL ) allows more advanced functionality , " spreadsheet-style pivot tables " , wheras [ pivot ] ( #URL ) just does * reshaping * ( and throws an error if there is a problem ) .

The file I am trying to read is 366 Mb , the code above works if I cut the file down to something short ( 25 Mb ) .
Thanks a lot for the comments . The code blows up with the same problem if I run it from the Python shell . I can get it to work if I cut down the lines ( currently there are 900'000 lines , 71 columns ) . Turns out that even having only " empty " lines , i.e. " ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, " will blow up Python , although the file size for this is fairly small ( 63 Mb ) . Is there a limit as to how big a dataframe can get ? Maybe the number of lines x columns is a problem ?
Actually discovered the limit was my problem . Now it is working . fromtxt is too slow for me . Now I am encountering different out of memory errors where a table ( after merge ) is getting incrementally bigger to about 15M rows . Might have to consider memmap and / or hdf5

Also , what if I want to replace those values with a correctly shaped DataFrame ?
Use ` loc ` in an assignment expression ( the ` = ` means it's not relevant whether it is a view or a copy ! ): #CODE
This is an assignment expression ( see ' advanced indexing with ix ' section of the docs ) and doesn't return anything ( although there are assignment expressions which do return things , e.g. ` .at ` and ` .iat `) .
Bottom line : use ` ix ` , ` loc ` , ` iloc ` to set ( as above ) , and don't modify copies .
Just to expand on @USER answer . This does not a view * or * a copy , it is an assignment expression , so it doesn't return anything ( FYI occasionally there * are * assignement expressions that DO return things , e.g. `` .at / .iat ``) . However , `` df.loc [ rows , columns ]`` * can * return a view ( more generally its a copy ) . Confusing , but done for efficiency . Bottom line , is to use `` ix / loc / iloc `` to set , and don't modify copies .

Note : ` indexer_between_time ` by default both ` include_start ` and ` include_end ` are True , it also offers a ` tz ` argument to compare the time to a different timezone .

Note you can apply strftime directly from a Timestamp object e.g. ` rng.map ( lambda t : t.strftime ( ' %Y-%m-%d '))` .

It seems to me ARMA does not support the date format generated by pandas ? If I remove freq option in date_range , then this command will again not work for large series since the year will go well beyond pandas limit .
The problem is that this freq doesn't give back an offset from pandas for some reason , and we need an offset to be able to use the dates for anything . It looks like a pandas bug / not implemented to me . #CODE

I get ValueError : Can only append to Tables . What I'm doing wrong ?
When you tried to do this with ` put ` it kept overwriting the store ( with the latest chunk ) , then you get the error when you append ( because you can't append to a storer / non-table ) .
` put ` writes a single , non-appendable fixed format ( called a ` storer `) , which is fast to write , but you cannot append , nor query ( only get it in its entirety ) .
` append ` creates a ` table ` format , which is what you want here ( and what a ` frame_table ` is ) .
Then append each DataFrame : #CODE

2 ) I tried filtering using the apply function : #CODE
Hey Tom , thanks a lot for the example . I have extended it to what I need : Try ` new_ids = pd.Series ([ " a1 , c3 "])` and ` df = pd.DataFrame ( { ' vals ' : [ 1 , 2 , 3 , 4 ] , ' ids ' : [ ' a ' , ' b ' , ' c ' , ' f '] , ' stuff ' :[ " insert " , " whatever " , " you " , " fancy "] } )` . I would like to filter the rows by appending the string in columns " ids " and " vals " to match what is in new_ids ... Preferably without creating an additional column first , as I am doing currently .
I see what you mean by append now . Sounds like Andy's comment got it working for you .
Yes it did . However , if anyone has a better way of doing this ( without creating an extra column in which I append the strings ) then please do let me know please .

How to apply condition on level of pandas.multiindex ?

` concat ` , ` merge ` , ` join `
So far I've only been able to combine the values , ( ie . add the elements together , append the series to each other , or merge based on values ) . Because the dataset is large , I'm looking to avoid loops . Although thats the only way I can think to do it so far . I feel like this should be pretty easy to accomplish with the power of pandas .

you want an additional level which concat forms when passed a dict

As a side comment , if what you're trying to pickle is just a dict that maps strings to strings , you can just use a ` dbm ` instead of a ` dict ` , because that just is a map from strings to strings that's automatically persistent . If you're mapping strings to lists or something , you can use ` shelve ` ( which just auto-pickles all of the values to persist them in the ` dbm ` file ) . This doesn't solve your problem , since you have to pick a location to store the .dbm file in , just as you do for the .pkl file ; it just ( potentially ) makes other parts of your code easier / faster / more robust .

The ` dict ( {} .items() + function ( i , j ) .items() )` is supposed to merge both dict in one as ` dict() .update() ` does not return the merged dict .

Pandas : How to merge Time Series with complementary Dates ?
I tried pd.concat in all variations ( or creating a series C with the orginial index and merging on it ) but it always just appends and does not merge as expected .
Why not just ` sort_index ` after doing the append / concat ? #CODE

You should check out ` scipy.sparse ` ( link ) . You can apply operations on those sparse matrices just like how you use a normal matrix .
If this is the case , list your ' items ' in rows and create ` A ` using ` scipy.sparse ` . Then replace the first line as indicated .

I want to merge A and B while keeping order , indexing and duplicates in A intact . At the same time , I only want to get values from B that are not in A so the resulting DataFrame should look like this : #CODE
Any pointers would be much appreciated . I tried doing a concat of the two columns and a groupby but that doesn't preserve column A values since duplicates are discarded .
And then you can append ( ` concat ` ) these with A , sort ( if you use ` order ` it returns the result rather than doing the operation in place ) and ` reset_index ` : #CODE

You can also resample if you are spanning multiple dates .

Can some one show me how to do this ? I have seen some of the slider examples but I am not able to translate that to pandas plots or what axes or arguments to pass to the Slider object .

Is there an easy way to find the disjoint set of records ( what would be left on each of the two original dataframes that is not included in the resulting inner join ) between two pandas dataframes based on a MultiIndex ?
I attempted to do this by finding the symmetric difference between the set of muliIndex keys of the two dataframes , but this has proved difficult . I have been struggling to get this to work . My other option , which seems like it might be a bit easer is to add a dummy column of integers that can act as a different single index that is preserved even after I do the multiIndex merge so I that I can use the python set operators on this de facto single key .
[ Note that this is related to but slightly different than this question because this merge is not based on a MultiIndex object , but on the values in columns of the dataframe : How do I do a SQL style disjoint or set difference on two Pandas DataFrame objects ? ]

` loc ` could be swapped out for ` ix ` here .

Plan B would be to keep a shorter MultiIndex DataFrame that just contains empty DataFrames appended every time I append the main one . I can retrieve that and get the index much more cheaply than the main one . Inelegant though .

Thank you so much @USER ! I spent more than an hour implementing what you did with merge ... Had no idea about it . That's exactly what I needed .

switch your slashes ... change ` \ ` to ` / ` or double up on them ( replace ` \ ` with ` \\ `) ... your \r is being interpreted as a carriage return

Using a dictionary to replace column values on given index numbers on a pandas dataframe

Here's one way ( though it feels this should work in one go with an apply , I can't get it ) . #CODE
You could also do a one liner using merge as follows : #CODE

I have a merge data frame ( mdf ) which the 2 data frames are retrieved from SQL . I wish to create a new col within mdf which will be the subtraction of existing 2 columns .
I'm not sure what you mean by a " merge data frame , " but here's a sketch of what you might be after . Please elaborate a little your question so it will be more useful to others . #CODE

You can also try to replace those symbols and separator : #CODE

Not implemented error when i tried to resample a data frame with datetime index
When i try to resample , i got a not " implemented error " .

pandas normalize multiindex dataframe
How do I normalize a multiindex dataframe ?
I know how to normalize a basic dataframe : #CODE
but I'm not able to apply this on each " name " group of my dataframe
Are you sure the result you gave is the correct one ? I'm assuming you want to normalize value1 and value2 separately . If that's not correct , let me know . #CODE
Your desired result suggests that you actually want to normalize the values in each name group with reference to the elements in ` value1 ` and ` value2 ` . For something like that , you can apply a function to each group individually , and reassemble the result . #CODE

But when I try to drop it into a dataframe :

loc will not attempt to use a number ( eg 1 ) as a positional argument at all ( and will raise instead ); see main pandas docs / selecting data

I read about the .stack method but couldn't figure out how to apply it for this case .

@USER : I meant if it was ever called because if you have a csv file with first column 1 , 2 , 3 ,... and you call ` set_index ` on that it will be indistinguishable from the default index , but it's still important for me to know if it was indexed or not because I am doing a merge operation that relies on an indexed dataframe
what do you mean by " indexed " and how does merge operation rely on it ?
@USER : I am using ` concat ` to concat columns of two dataframes together and I want to avoid concatenating together two dataframes that are not indexed that's all . indexed I mean dataframes where the user explicitly assigned a unique column identifying each row to the dataframe with ` set_index ` . maybe this is impossible to tell ?
Can't you just call set_index ( column ) again before doing the merge ? If you're worried that the index isn't there anymore because it's been moved to the index you can do a try : except KeyError :
@USER : good point but if you're writing a generic merge function you don't know what column the user wanted to merge
Unless of course someone called df.set_index ( np.arange ( 0 , len ( df ))) :) . I'm a bit confused by the question , if he's really asking of "` set_index ` was ever called on the dataframe "

I'm not aware of a method on DataFrame to do this . Do you expect things like count and the quantiles to change ? Or just the mean and standard deviation ? Can you apply the weighting first and then call describe on the resulting series ?
Thanks TomAuspurger ... that was my suspicion , but I was hoping to avoid that extra coding ... I'd expect it to apply to all of the metrics .
Assume you have 70% of the population with revenue 0 , and 30% with revenue 1 . You'd want median revenue weighted by population to be 0 . If you multiply revenue by weight and apply describe ... you probably get a median of 0.15 ( vector 0 , 0.3 ) which is irrelevant .
First , with stata's weight option ` [ w=wt ]` ( I've truncated the output , 50% here indicates the median ): #CODE

stack it first and then use value_counts : #CODE

Hello SO ! Currently have got two large HDFStore containing each one node , both the nodes doesn't fit in memory . The nodes don't contain NaN values . Now I would like to merge these two nodes using this . First tested for a small store where all the data fits in one chunk and this was working OK . But now for the case where it has to merge chunk by chunk and it's giving me the following error : ` TypeError : Cannot serialize the column [ date ] , because its data contents are [ empty ] object dtype ` .
Check the len before appending #CODE
I added an issue about this , I think there is a subtle error here unrelated to HDFStore that is raising an error ( rather than just letting it proceed , which I think is correct ) , an empty frame will normally succeed on the append , except in the case you had ( and I illustrate ): #URL
Clear example as you've given on github . Very impressive how you manage to discover the underlying problems when errors are raised . Will have time this evening to check on my datasets . One little thing that I notice from the results you provide : how come can the node ' mergev46 ' have a larger shape than ' var6 ' by doing an ' inner ' merge ?

You can use the ` pivot ` method for this .

I have a pandas series of booleans and was wondering what the best way is to apply " or " or " and " to the whole series . I am thinking something along the lines of a Haskell #CODE
will apply a function to each element in the series so doesn't seem to do what I need .

You've got everything you need . You'll be happy to discover ` replace ` : #CODE
Ah , I only see it now I posted my answer . Is there a difference with ` map ` in this case ?
It seems that something else ( not in the dift ) is just left with ` replace ` , but converted to ` NaN ` with ` map `
I think `` map `` is a better choice here , actually , because if a value isn't in `` d `` then the value is invalid and should be replaced with `` NaN `` .
` replace ` seems to apply to DataFrame not to a Serie
You can just use ` map ` : #CODE

Transpose so that we can use the ` diff ` method ( which doesn't take an axis argument ) .

Otherwise , using the resample function . #CODE

As you say , looping ( iterrows ) is a last resort . Try this , which uses ` apply ` with ` axis=1 ` instead of iterating through rows . #CODE
Thanks a lot , Dan , this works ( although I had to be a bit careful with argmax on an all-false array ) .

I am more familiar with R but I wanted to see if there was a way to do this in pandas . I want to create a count of unique values from one of my dataframe columns and then add a new column with those counts to my original data frame . I've tried a couple different things . I created a pandas series and then calculated counts with the value_counts method . I tried to merge these values back to my original dataframe , but I the keys that I want to merge on are in the Index ( ix / loc ) . Any suggestions or solutions would be appreciated #CODE
Note that ` transform ( ' count ')` ignores NaNs . If you want to count NaNs , use ` transform ( len )` .
Thanks a lot . Very helpful . I've been trying to apply that to a larger DataFrame and keep on getting this error

If the criteria I apply return a single row , I'd expect to be able to set the value of a certain column in that row in an easy way , but my first attempt doesn't work : #CODE

1 ) Using a loc_dict = { 301 : 1 , 302 : 7 , 303 : 3 } to replace loc_var and a ind_dict = { 1 : 4 , 2 : 8 , 3 : 10 , 4 : 15 } to replace ind_var
This can be done by ` stack / unstack ` and ` groupby ` very easily : #CODE

corrected string cast to bool with a comparison xxx == ' True ' #CODE
Is this line : ` checked = bool ( index.model() .data ( index , QtCore.Qt.DisplayRole ))` really gives a boolean ? I would have written ` checked = index.model() .data ( index , QtCore.Qt.DisplayRole ) .toBool() ` because the result of data() is a QVariant .
Here it is ( I had to create a class called ` Dataframe ` to simulate the panda Dataframe structure . Please replace all the ` if has_panda ` statements by yours ): #CODE

Ok , I did a little more research and this pretty much covers what i wanted to do . using matplotlib colormaps did the trick . here is a link to the options of map colours .

Thanks , the _list of_ dicts is the key . The files are hundreds of Mbs gzip compressed and several Gbs uncompresed , so will read line by line and append to the corresponding DataFrame .

A nested apply will do it #CODE

Ah I think the problem is with my df's index , it is not a sequence like ` range ( len ( df ))` . ` iloc ` however seems to work with both a " normal " index and my index . I'm not very experienced with pandas but this behavior suggests that using ` iloc ` would be more stable ?

Pandas , numpy , scipy , and the rest of the scientific computing stack are going to look rather noisy as they are building packages . I would go ahead and test some code out , but you're probably good to go .

Further update - I figured out how to conform to the pattern I wanted . Pretty simple really - just made a copy of the dataframe and then did my workings on that , returning just the series with the result in it .

however , pandas seem to truncate the number to its integer part : #CODE

suppose I have a dataframe with index as monthy timestep , I know I can use ` dataframe.groupby ( lambda x : x.year )` to group monthly data into yearly and apply other operations . Is there some way I could quick group them , let's say by decade ?

Before solving classification or regression tasks I want to add new transformed columns i.e. to clean up source data or to normalize them .

where I have `` df [ ' B ']`` you can put a scalar ( e.g. ' Close ') , though you should really do this in another coulumn ( e.g. columns you are selectin from , `` df [ ' A ']`` does not have to be the same as the mask `` df [ ' A '] > df [ ' B ']`` , otherwise you will get a mixed float / string column , generally not useful ( and not efficient for anything ) . You can also have another column where I have `` df [ ' B ']`` as the replacement value ( and pandas will align it to the selector column ) . FYI this is exactly equivalent to : `` df.loc [ df [ ' A '] > df [ ' B '] , ' A '] = df [ ' B ']``
The basic problem is that ` if / else ` doesn't play nicely with arrays , because ` if ( something )` always coerces the ` something ` into a single ` bool ` . It's not equivalent to " for every element in the array something , if the condition holds " or anything like that .

But if you want to do this in pandas , you can ` unstack ` and ` order ` the DataFrame : #CODE

Length : 91910 , Freq : None , Timezone : None

I have tried several variants on : ( I assume I'll need to apply the limits to each plot .. but since I can't get one working ... From the Matplotlib doc it seems that I need to set ylim , but can't figure the syntax to do so . #CODE

Take each column individually . Group it by value , and use the ` filter ` method on groups to replace any group with less than 3 values with ` NaN ` . Then replace those ` NaNs ` with ` X ` .
And , to be sure , you can merge the original and the recoded frames just as you expressed it in your question : #CODE

You are using an indexing short-cut which doesn't apply , see here : #URL

First of all , get rid of that ` for i in range ( len ( filepaths ))` ! The pythonic way is ` for i , filepath in enumerate ( filepaths )` . ` enumerate ` gives a tuple so you can say ` ExcelFile ( filepath )` instead of ` ExcelFile ( filepaths [ i ])` .

EDIT : the ` AttributeError : ' numpy.int64 ' object has no attribute ' replace '` error you get is due to the fact that you use integer column labels ( this is a bug ) . Try setting the columns labels to something else , eg : #CODE
I tried this after removing the table creation code ... sql.write_frame ( df , con =d b , name= ' TEST ' , if_exists= ' replace ' , flavor= ' mysql ') , It says : AttributeError : ' numpy.int64 ' object has no attribute ' replace '
Note , there seems to be a bug with the ' replace ' option ( see eg #URL ) . Does it run without ` if_exists= ' replace '` ?
Without replace it generates this error .. ValueError : Table ' NEW ' already exists . With replace gives the above error . I followed what you said , now the problem is with this replace thing . :(
I think the best option at the moment is to delete the table yourself in MySQL if you want to overwrite it ( as ' replace ' is not working at the moment : #URL ) . Note that you don't have to create the table beforehand , ` write_frame ` will do it for you .

I am trying to merge a couple of dataframes that I create from csv files , and it is failing on one of the files with the following error : #CODE
By Googling the problem I got the suspicion that it might have to do with the uniqueness of the columns when trying to merge . I did see this problem in an instance where the tuple provided as suffixes to the merge function consisted of two identical entries . However , I have remedied that and am now getting the same error but for a different file . Now I have absolutely no idea what might be causing this . The indices on the data frames are just integers [ 0 , 1 , 2 , 3 , 4 ,... ] so look fairly harmless .
Hence my question : Can I debug the merge function in pandas , and if so , how ? I can't seem to find the source code ( and I suspect it might not be written in Python anyway ) .
That's weird cos you can join / merge non-uniquely indexed on 0.11 ( which pandas version are you using ? ) Can you give a small example where this doesn't work , e.g. does it fail with ` df = df.head() ` ( or with just a couple of non unique index )

It looks like you want to first do a ` ffill ` then do a ` shift ` : #CODE
To do this over each group you have to groupby category first and then apply this function : #CODE

Apply the function ( you could group by uuid , site if you want as well ) #CODE
This forces all the elements to be there ( note the ` NaN ` becuase no way to ` ffill ` on the first element ) . You could drop these if you want .

Actually you should pay attention to these . These basically are saying you are storing a data type that PyTables is going to `` pickle `` ! Try storing as a table ( either use `` append `` or `` store.put ( ' df ' , df , table=True )`` which stores in the `` Table `` format ; better handling of things like `` nan `` certain dtypes ( that the `` Storer `` format will give you a PerfWarning . See #URL
is `` my_dictionary `` a pandas object ? ( if it is , then first do a `` store.remove ( ' my_dictionary ')`` if its not a pandas object then you shouldu use the attribute method above . Tables try to `` append `` ( while `` put `` always overwrites )

This will create a PeriodIndex of monthly freq . Note that iterating over the index yields the tuples ( as integers ) #CODE

Also , generally good practise to specify that you are indexing by the label ( with the loc ): #CODE
This is a surprising error in 0.8.1 ( but seems to be fixed in later versions ) , perhaps a workaround ( if the above doesn't work ) is to set the fancy index first ( ` df_A_gt_half = dfrm.A 0.5 `) and then do the assignment using that ... and are forced to use ` ix ` rather than ` loc ` .

The initial way I suggested ( which is clearly not optimal ) , once you've read it in as a DataFrame you can remove these rows using ` notnull ` ( you want to keep only those rows which are all ` notnull ` ): #CODE

using ` drop ` method ? #URL

The same code works if I drop the chunksize bit : #CODE

When you apply your own function , there is not automatic exclusions of non-numeric columns . This is slower , though ( that the applicatino of ` .sum() ` to the groupby #CODE
Thanks Jeff . How could I apply different functions on several columns in one go as well , e.g. sum on column " B " and set on column " C " ?
each group get's passed a `` DataFrame `` ( called x ) , so x [ ' A '] is a `` Series `` , just like regular indexing ( but its just the rows in that group ) . the `` x [ ' A '] .sum() `` thus reduces to a scalar value , as do the other terms . Net you are returning a `` Series `` with values for the `` index =[ ' A ' , ' B ' , ' C ']`` . These are stacked ( row-wise ) to form the result frame at the very end of the apply . You can do bare strings for the keys when you use `` dict `` , equiv is `` { ' A ' : x [ ' A '] .sum() } ``
great ! just to point out though , you should still do the aggregation on A and B via a direct `` .sum() `` rather than apply because these are cythonized . The apply going to be slower , so do only where you really need it .
Great answer ! A slightly more direct way to apply different functions on several columns is to use the ` agg ` function , so ` df.groupby ( ' A ') .agg ( dict ( A = ' sum ' , B = ' sum ' , C = lambda x : ' {%s} ' % ' , ' .join ( x )))`
Thanks a lot , I have been looking into the ` agg ` function . Any ideas how that would compare performance-wise to apply ?
they are the same ; apply is a bit more flexible in how it looks at the output ( just slightly )
You can use the ` apply ` method to apply an arbitrary function to the grouped data . So if you want a set , apply ` set ` . If you want a list , apply ` list ` . #CODE
If you want something else , just write a function that does what you want and then ` apply ` that .

Whenever you are done calculating your result , you need to do ` output.set ( ... )` , and replace ` ... ` with the result you want to have displayed . I don't know what variable contains the result you want to show , though , but presumably you have some variable somewhere that has the data you want to show .

For each row of table1 , I want to evaluate a distance for that particular individual idx1 with all rows in table2 . Then I select the argmin idx2 , and save somewhere that idx1 is matched with idx2 .
I've done the group combination using groupby method of a pandas.DataFrame() and then use numpy . Works great , and the quadratic algorithm became linear when n >> len ( group_combination ) . Thank you .

The ` normed=True ` returns a histogram for which ` np.sum ( pdf * np.diff ( bins ))` equals 1 . If you want the sum of the histogram to be 1 you can use Numpy's histogram() and normalize the results yourself . #CODE

If you want to avoid matplotlit altogether , join the two series first and call plot on the resulting DatFrame : #CODE

As an example of an application , I would like to drop rows in my dataframe where that condition is met .
I tried the suggestion from @USER to drop entries ( rows ) from my dataframe , but the following command : #CODE
Thanks . The odd thing is that if I use that index object to drop rows from my dataframe I get : ` / opt / python / virtualenvs / work / lib / python2.7 / site-packages / pandas / core / config . #URL DeprecationWarning : height has been deprecated . ` . This is with the latest stable version of Pandas ( 0.12.0 from two days ago ) . I updated the OP .

Applying a map elementwise to every element on a Series ( Pandas )
Why am I unable to apply my function elementwise on my ` Series ` object ?
Also , why do DataFrames and Series have different method names for elementwise operations ( i.e. ` map ` for Series vs ` applymap ` for DataFrames )

Thanks ! What happens when I append another dataframe to a store that has a full index on that column ? Will it automatically continue to be ? Or is it required to do the store.create_table_index on the final file ? Is it then happening on disk and therefore does not create memory issues when my store has grown to 80 Gigs ?
I think when u append it will index to the new scheme . the only reason you actually need to sortby is to force it to reindex ( you can actually do this via a pytables function call as well , which is what sortby is doing )
I think for your application , using `` ptrepack `` is correct , e.g. write in whatever order you have the data coming in , then `` sortby `` to create a CSI ( as you are query by a different primary field ) . However , you can also set the index to be CSI when you create it , so it will create the index as you append ( but I suspect that might be slower as it potentially may have to move data more than once )

Now use the ` resample ` method to get to the frequency you're looking for : I'm going to demonstart quarterly , but you can substitue the appropriate code . We'll use ` BQS ` for * B*usiness * Q*uarterly * S*tart of quarter . To aggregate , we take the sum . #CODE
Ok so now we want the ` Open ` for today minus the ` Close ` for yesterday for the gross change . Or ` ( today / yesterday ) - 1 ` for the percentage change . To do this , use the ` shift ` method , which shifts all the data down one row . #CODE
You could write a function and ` apply ` it to each DataFrame in the panel you get from Yahoo .

I have calibration factors that need to be applied after specific dates and for certain ranges of instrument readings ie . a higher reading will require a different calibration factor . I am way I am trying to apply a lookup table that is based on the time and also the raw instrument reading through the use of a nested python dictionary .
is there a way to iterate through my dataframe to apply these calibration factors with the nested dict as a form of a lookup table ?

** Surely ** it's a bug that a shift ( by a month ) is actually going to the next month and not incrementing by a month ...
I agree with Andy ; that can't be the intended behavior of ` shift ` . A cleaner way to shift times to the end of the month is this : #CODE

Till ` pandas ` is not officially implemented in ` plt.fill_between ` function , you can still apply ` pd.Series ` or ` pd.DataFrame ` as ` pd.Series() .values ` and ` pd.DataFrame() .values ` to make ` fill_between ` plots .

Use N of connections in N threads . Then join theads and procces results . #CODE

there is more overhead with calling `` value_count `` ( which has to reconstruct the series ) on each group ( rather than `` unique `` which just return an ndarray ) . This can actually be non-trivial . If you don't need the indexes inside the function then you can often avoid this penalty ( by not instantiating the series , which value_counts does , and then gets discarded because all you need is the len of it )

Thanks for this . I tried ` g.filter ( lambda x : len ( x ) > 1 )` but it didn't work for me , I got an exception . Now I'm trying ` g.filter ( g.size() > = threshold )` . It didn't fail immediately , which is a good sign , but it is slow ... I started it ~10min ago and it's still running ... I've got ~90000 groups ( without the filtering ) .
g.filter ( lambda x : len ( x ) > = threshold ) File " / var / tmp / SKL / lib / python2.7 / site-packages / pandas / core / groupby.py " , line 2094 , in filter

but in both cases it's not working and I'm getting an error saying " cannot concatenate ' str ' and ' int ' objects " . Concat for two ` str ` columns is working perfectly fine .

Instead of length ` len ` , I think you want to consider the number of unique values of Name in each group . Use ` nunique() ` , and check out this neat recipe for filtering groups . #CODE
A general remark : Sometimes , of course , you do want to know the length of the group , but I find that ` size ` is a safer choice than ` len ` , which has been troublesome for me in some cases .
`` apply `` would return a shorter Series , with one entry per group . Instead , we want a Series of the same length as the original one , with each group's entire contents mapped to `` True `` or `` False `` as a block . Then we can use that boolean Series to mask the original Series . See the [ documentation ] ( #URL ) for more .
You could first drop the duplicates : #CODE

And then use ` apply ` : #CODE

I think loc was introduce in 0.10 ( recommend updating to latest stable ) , difference is loc is label based vs ineger location based ix tries to infer your meaning

pandas select from Dataframe using startswith
And got AttributeError : ' float ' object has no attribute ' startswith '
Section 4 : List comprehensions and map method of Series can also be used to produce more complex criteria :
Could you give a small example which demonstrates this , I'm surprised that the list comprehension wouldn't raise in the same way as the map ...
and the boolean indexing will work just fine ( I prefer to use ` loc ` , but it works just the same without ): #CODE
It looks least one of your elements in the Series / column is a float , which doesn't have a startswith method hence the AttributeError , the list comprehension should raise the same error ...
Sorry the data is 27 cols long , kind of unwieldy even to post a clip . I tried this > table [ ' SUBDIVISION '] .str .startswith ( ' INVERNESS ' , na= ' False ') with another ' . ' and the comparison came out bad ( all selected false ) I still don't get why my original syntax failed since I thought I was following the documentation .

Alternatively you could use ` apply ` ( but this will usually be slower ): #CODE
Since you are using the " trailing row " you are going to need to use ` shift ` : #CODE
thanks shift is what i was looking for . now i can find examples in the Pandas book

You can use the ` shift ` Series method ( note the DatetimeIndex method shifts by freq ): #CODE

I've isolated that column , and tried varies ways to drop the empty values . first , when im writing : #CODE
Now im trying to drop those entries . Iv tried : #CODE
You should use ` isnull ` and ` notnull ` to test for NaN ( these are more robust using pandas dtypes than numpy ) , see " values considered missing " in the docs .
The ` dropna ` DataFrame method has a subset argument ( to drop rows which have NaNs in specific columns ): #CODE

How to apply a function to two columns of Pandas dataframe
Pandas : How to use apply function to multiple columns
And apply the function like this : #CODE
Any Ideas on how to get around the axis parameter error ? Or a more elegant way to calculate the pct change ? The kicker with my problem is that I needs be able to apply this function across several different column pairs , so hard coding the column names like the answer in 2nd question is undesirable . Thanks !
The confusion stems from two different ( but equally named ) ` apply ` functions , one on Series / DataFrame and one on groupby . #CODE
The DataFrame apply method takes an axis argument : #CODE
The groupby apply doesn't , and the kwarg is passed to the function : #CODE
Thanks for your answer Andy . If I stick with the groupby apply and remove the axis param , I get a key error ` KeyError : u'no item named 0 '` for accessing the elements as ` row [ 0 ]` ect . Is there a way to use the groupby apply and still use a notation that keeps it easy to apply to several differently named column pairs ?
@USER updated , you can use the groupby with axis=1 , you can apply pct_change to entire dataframe . Or perhaps you want to do this one each group using an apply ( ` lambda x : x.pct_change() `) .
@USER you need to tweak delta a bit , see my last example ( assuming that's from the apply )
@USER not sure I have a good reference , but a good trick is to set up a break point in the function you're going to apply and then see how you can access things you want

Pandas merge and join not working
On some pairs ` merge ( df1 , df2 )` is working correctly but ` df1.join ( df2 )` is not . For example , thesea are the subsets from one of the pairs #CODE
Now , to make things really strange , on some other pair of dataframes ` merge ( df1 , df2 )` is not woking but ` df1.join ( df2 )` is working . #CODE
I would recommend doing a concat here ( which works better if they are indexed , but doing have the repeated studentid column ) . The benefit being that this generalises to multiple DataFrames / Series . #CODE

You could define a function to subtract the quarterly totals from the annual number , and then apply the function to each row , storing the result in a new column . #CODE

ReshapeError while trying to pivot pandas dataframe
Using pandas 0.11 on python 2.7.3 I am trying to pivot a simple dataframe with the following values : #CODE
The pivot results successfully in following : #CODE
What do you expect your pivot table to look like with the duplicate entries ? I'm not sure it would make sense to have multiple elements for ( 1234 , bar ) in the pivot table . Your data looks like it's naturally indexed by ( questionID , studentID , dateRecorded ) .
Otherwise you can come up with some rule to determine which of the multiple entries to take for the pivot table , and avoid the Hierarchical index .

Assuming these are just strings you could simply add them together ( with a space ) , allowing you to apply ` to_datetime ` : #CODE

Python Pandas merge only certain columns
Is it possible to only merge some columns ? I have df1 with columns x , y , x and df2 with columns x , a , b , c , d , e , f , etc .
I want to merge the two dfs on x , but I only want to merge columns df2.a , df2.b - not the entire dataframe .
I could merge then delete the unwanted columns , but it seems like there is a better method .
You could merge the sub-DataFrame ( with just those columns ): #CODE

Drop rows with multiple keys in pandas
Then just drop rows where " InOtherDF " is ` True ` . You might have to adjust it slightly to ignore the index when giving back the row-tuples .
I think this is a cleaner way using ` merge ` #CODE
It feel like there ought to be a neat way to do this using an inner merge ... #CODE
Feels like ought to be a neat merge way

A purely pandas way might be to apply the Series constructor to put this into one DataFrame and stack into a Series ( so you can use value_counts ) ... if you didn't care about the index / timestamp you could use collections ( which may be faster ): #CODE
` Apply ` Series constructor to get a DataFrame and ` stack ` it into a Series : #CODE
Now when we stack , we see the names of the levels : #CODE

Pandas : boxplot of one column based on another column
I would like to boxplot the age of each Group ( A , B , C ) . Note that I have some ` NaN ` values in the dataframe . How can I do this in Pandas ?
Misread 1st time so gave answer for histograms ... keeking that below . for boxplot the code is : #CODE
Thanks .. Do you know how to get rid of `" Boxplot grouped X "` in the figure title ?
Don't have the example data open anymore ... if in ipython with pylab imports : simply title ( " Boxplot grouped X ") should do . Otherwise " import matplotlib.pylab as plt " and plt.title ( " ..... ") should do the trick
Thanks Joop , unfortunately the title command just changes the part that says ` Age ` in the boxplot ( in your post ) , that's why I asked .
" Boxplot grouped by Group " is in space used by suptitle . but it merely over-writes stuff there so it stays messy .

Is there any concern with calling transpose ? For example . are there any cases in which dtypes gets messed up ?

The result of ` df.groupby ( ... )` is not a DataFrame . To get a DataFrame back , you have to apply a function to each group , transform each element of a group , or filter the groups .

I came out with the following solution . As preparation steps , I ll group by speaker name and set the file name as index by the set_index method . I will then iterate over the groupbyObj and apply the calculation function , which will return the selected speaker and the files to be marked as used .
1 . How can I approach a specific group by a groupby object ? bcz I thought maybe instead of setting the files as indexed , grouping by a file , and the using that groupby obj to apply a changing function to all of its occurrences . But I didn t find a way to approach a specific group and passing the group name as parameter and calling apply on all the groups and then acting only on one of them seemed not " right " to me .

I have also tried to read file line by line , create a series from ` row.split() ` and append the series to a dataframe , but it appears to crash due to memory .

I want to linearly interpolate the 30 minute data to the exact time values of the sparse data . I don't want the nearest values . For example , I want the values of df1 at 16:05 , interpolated from the values of df1 at 16:00 and 16:30 .
Subselect and join #CODE

Could also replace the 0s with ` NaNs ` ahead of time and call ` prod ` on that .
@USER or use the DataFrame prod method :)
Just to throw out an alternative ( you can use the ` prod ` DataFrame method ): #CODE

You can simply replace all of these occurences with NaN : #CODE
and perform ` get_dummies ` ( which I think ought to ignore the NaN , but there is a bug , hence the ` notnull ` hack ): #CODE

I am trying to append 3 columns from one dataFrame to the end of another , similar to the following : #CODE
Although I can't post my dataFrames , I seemed to have narrowed it down to the data type or consistency . If I write both df's to a file using to_csv , then read them back into new ones with read_csv , the concat process works fine .
If I try to take the originals , and convert them to numeric types by .astype ( str ) .convert_numeric , the types match the read versions , however the concat still fails .
All this seems to do is reverse the concat order ( ie :

I would like to point out this , which is ` True ` because ` bool ( np.nan )` is ` True `
and also ` bool ( float ( ' nan ')) is True ` , agree it's odd ... personally think it should be Falsey . Funny this came up again .

does the string in eval support functions , like , df [ ' c '] = df.eval ( ' a.diff() + b ') ?
Even python offer eval and exec , so the security is depend on how to use it . dataframe.eval only support arithmatic is too limited .

Quite easily with the ` shift ` method . #CODE

` apply ` on a series returns a DataFrame if the function wich is applied returns a Series for each element of the series , where the different elements of the returned Series become the values of the different columns of one row . So in this case first the ` list ` function converts the string in BINDATA to a list , which is then converted to a Series ( see also the answer of @USER , which does actually the same but is written a little bit different )

IIUC , you're looking to melt : #CODE
I'm not sure I follow exactly what aggregation you want , but you should be able to apply ` groupby ` however you like . For example : #CODE

You could use an aggregate with a ` join ` : #CODE

Pandas Join Two Series
I have two Series that I need to join in one DataFrame .
When I use concat I get a DataFrame that has one index ( good ) but two columns that have the same values ( bad ) . #CODE

Difference between asfreq and resample
Can some please explain the difference between the asfreq and resample methods in pandas ? When should one use what ?
` resample ` is more general than ` asfreq ` . For example , using ` resample ` I can pass an arbitrary function to perform binning over a ` Series ` or ` DataFrame ` object in bins of arbitrary size . ` asfreq ` is a concise way of changing the frequency of a ` DatetimeIndex ` object . It also provides padding functionality .
An example of ` resample ` that I use in my daily work is computing the number of spikes of a neuron in 1 second bins by resampling a large boolean array where ` True ` means " spike " and ` False ` means " no spike " . I can do that as easy as ` large_bool.resample ( ' S ' , how= ' sum ')` . Kind of neat !

Pandas will automatically align these passed in series and create the joint index
I think ` concat ` is a nice way to do this . If they are present it uses the name attributes of the Series as the columns ( otherwise it simply numbers them ): #CODE
@USER hmmmm , I think it depends what you want the answer to be ... something with merge : ` pd.merge ( s1.reset_index() , s2.reset_index() , how= ' outer ')` ?
" It is worth noting however , that concat ( and therefore append ) makes a full copy of the data , and that constantly reusing this function can create a signifcant performance hit . If you need to use the operation over several datasets , use a list comprehension . "
@USER what " constantly reusing this function " means is that you should prefer doing the concat ** once ** ` pd.concat ([ list_of_dataframes ])` vs concating many times ` new_df = pd.DataFrame() ; for df in list_of_dsf : new_df = pd.concat ([ new_df , df ])` or similar .

Using a Python dict to replace / clean data in a Pandas DataFrame
And I need to map / replace the names using a Dict , ( which I have in a Excel sheet )
When I read the translate table into Pandas I get a DF that looks like #CODE
I want to look up values in the 1st col of the xlate table match them to table2 [ ' SUBDIVISION '] and if found replace contents of SUBDIVISION with the values in xlate column 2 if not leave them alone ( bonus .. actually if col 2 is NAn I'd like to leave it alone as well ) for instance above finding INVERNESS POINT will be replaced by INVERNESS
And use this to ` replace ` the column : #CODE
Ahh I had tried to reassign the translate dict to a new variable which it did not like .. so it seems to have replaced but not " inplace " on the DF ?

Index uses a Hashtable to map labels to locations . You can check this by ` Series.index._engine.mapping ` . This mapping is created when necessary . If the index ` is_monotonic ` , you can use ` asof() ` : #CODE

How can I apply formatting to the secondary Y-axis ( the one that displays on the right ) ?

Finding the intersection between two series in Pandas
I have two series s1 and s2 in pandas / python and want to compute the intersection i.e. where all of the values of the series are common .
How would I use the concat function to do this ? I have been trying to work it out but have been unable to ( I don't want to compute the intersection on the indices of s1 and S2 , but on the values ) .
then use the set intersection method .
Just noticed pandas in the tag . Can translate back to that . #CODE
Have added the list ( .... ) to translate the set before going to pd.Series Pandas does not accept a set as direct input for a Series .
also , you can use ` & ` operator for set intersection .
Actually , you can't just apply Series to a set ( which is annoying ) ` TypeError : Set value is unordered ` , seems unnecessary restriction / not very duck .

You can count the NaN values , drop them , and append the same amount again at the end . So something like : #CODE
@USER exactly right ; apply DOES call things twice ( on purpose ) to see if there are modifies in place ( in which case slow path is taken ); otherwise a faster path can be taken .
Thanks for the clarification ! The ` +1 ` in the function will mess up the shift in the subsequent rows though ! Still a nice solution ...
not sure about your i = 0 case , but if you care you can do a conditional , in the `` shift `` if needed
Note that ` np.roll ` is the important thing here . It takes an array , an integer number of places to shift and an ` axis ` argument so you can shift an ` ndarray ` along any of its axes .

You can use ` concat ` : #CODE
Note : if these were DataFrame ( rather than Series ) you could use ` merge ` : #CODE
Thanks Andy , that last method in comments works ... I cant seem to get the merge to work though ... sorted now :)

The problem is that ` drop ` takes an array-like argument ` labels ` , and you are only passing it a timestamp . You should be able to use a list comprehension instead of your loop too : #CODE

Using a plain ` dict ` instead of ` defaultdict ` , and changing ` d [ parts [ 0 ]] .append ( parts [ 1 ])` to ` d [ parts [ 0 ]] = d.get ( parts [ 0 ] , [ ]) + [ parts [ 1 ]]` , cut the time by 10% . I don't know whether it's eliminating all those calls to a Python ` __missing__ ` function , not mutating the lists in-place , or something else that deserves the credit .
Running in PyPy 1.9.0 instead of CPython 2.7.2 cut the time by 52% ; PyPy 2.0b by 55% .
If you can't use PyPy , CPython 3.3.0 cut the time by 9% .
So , that's one third of your time spent in ` string.split ` , 10% spent in ` append ` , and the rest spend it code that ` cProfile ` couldn't see , which here includes both iterating the file and the ` defaultdict ` method calls .
The profiling stack for the pandas version is quite long ( lot of c functions , it's highly optimized , and written in cython ) and I think it would be useless to post it . I posted just the timing informations . You can check for yourself , just do : ` import pandas as pd ; df = pd.read_table ( ' largefile.txt ' , header=None , index_col=0 )` And that is all you need to load the data into a DataFrame object with nonunique indexes . ( which is the case here )
However , the point about the has size is important . I could ' cheat ' by reducing the hash size down to a very small number which for a multivalued dictionary would increase insert speed at the expense of lookup . Therefore to really test either of these implementations you really need to also test the lookup speed .
Ok , see edits above . I do really think that the lookup speed is probably rather important too .. you can gear the insert speed and the expense of the lookup ( although I've intentionally not done this in my timings ) .
map ( hash , ( 0 , 1 , 2 , 3 )) [ 0 , 1 , 2 , 3 ]
map ( hash , ( " namea " , " nameb " , " namec " , " named ")) [ -1658398457 , -1658398460 , -1658398459 , -1658398462 ]

You could always look at the process & it's memory usage for a single file . If you're running linux , try ` top ` and then ` Shift + M ` to sort my memory usage .

And then you can transform it , in this case reshape it with ` pivot ` to create columns for the different metrics : #CODE

However , the x-axis ticklabels are orientated vertically and hence I can't read the text . Also my legend outside is cut off .
Also , how do I change the plot size so that I can see my legend . They are cut off .

How to merge two DataFrame columns and apply pandas.to_datetime to it ?
What would be a more pythonic way to merge two columns , and apply a function into the result ?

this is inefficient since it references the index ` ix ` of ` df ` for each row - is there a better way ?

@USER can just transpose do transpose it

When attempting to overwrite a sqlite table with the contents of a Pandas ` dataframe ` , Pandas ` DROP ` s the table , but doesn't recreate it before attempting the ` INSERT ` .

Just apply the ` min ` function along the axis=1 . #CODE

Then use ` groupby ` and ` apply ` : #CODE

I'm trying to apply simple functions to groups in pandas . I have this dataframe which I can group by ` type ` : #CODE
I want to apply a function like ` np.log2 ` only to the groups before taking the mean of each group . This does not work since ` apply ` is element wise and ` type ` ( as well as potentially other columns in ` df ` in a real scenario ) is not numeric : #CODE
is there a way to apply ` np.log2 ` only to the groups prior to taking the mean ? I thought ` transform ` would be the answer but the problem is that it returns a dataframe that does not have the original ` type ` columns : #CODE
The first proposal gives ` ( ' Not implemented for this type ' , u'occurred at index type ')` . The second one works but it drops the ` type ` , so you can't group afterwards . ` _get_numeric_data() ` can't be used with groups I believe . So can't think of how to use the second one to apply ` np.log2 ` to numeric data only and then group or group first and then apply only to groups

I think you'll be able to ` apply ` a different function here ( rather than sum ) to achieve the desired result .
@USER then sum or more complicated analysis e.g. via apply :)

Pandas drop function : Unalignable boolean Series
I would like to iterate over df0 [ " MAPINFO "] and drop rows that don't match condition and append the means to another df . My code is as followed : #CODE
My solution would be to make bool index which implements inclusion criteria then just use it : #CODE
Thanks for helping . Your suggestion works just fine for the those dfs . I'm trying to modify your code to apply lambda function for each MAPINFO at once . Acctually , those df are huge.Best .

I'll open a github issue since passing ` align ` to the plot method doesn't do really do this ...
What do you think ` align ` should do as an argument to ` plot ` ?
@USER Not ` matplotlib.plot ` . There's a ` Series.plot ` method that when you pass ` kind= ' bar '` you essentially replicate ` matplotlib.bar ` and when ` kind= ' line '` ( the default ) you get similar behavior to ` matplotlib.plot ` . The current behavior of ` align ` with ` s.plot ( kind= ' bar ' , align= ' center ')` is a bit off . It seems to do the opposite of what ` matplotlib.bar ` does .
One of the issues I am having is that all of my NaNs are at the end of the 2013 values . matplotlib wants to drop these from the graph by default which causes issues when trying to layer these plots on top of each other . Your graphs only have NaNs in between the beginning and the end . How would you handle graphing one Series that contains all 12 months of data , and another that only contains the first 7 months ?

Apply function to a MultiIndex dataframe with pandas / python
I have the following ` DataFrame ` that I wish to apply some date range calculations to . I want to select rows in the date frame where the the date difference between samples for unique persons ( from sample_date ) is less than 8 weeks and keep the row with the oldest date ( i.e. the first sample ) .
I normally use R for the procedure and generate a list of dataframes based on the name / dob combination and sort each dataframe by sample_date . I then would use a list apply function to determine if the difference in date between the fist and last index within each dataframe to return the oldest if it was less than 8 weeks from the most recent date . It takes forever .

pandas groupby apply on multiple columns
I am trying to apply the same function to multiple columns of a groupby object , such as : #CODE
What is the correct way to apply the function to multiple columns at once ?

` melt ` is pretty useful once you get used to it . Usually , as here , you do some renaming / reordering before and after .
This one is very similar to the ` melt ` solution provided by DSM : #CODE
Applying ` stack ` will pivot the column axis on a sublevel of the row axis already indexed by ` item ` . As you want ` user ` first , let's do the operation on the transposed DataFrame by using ` .T ` : #CODE

Let me add that you can indicate on which column to drop duplicates , which would be the email column : ` df.drop_duplicates ([ ' Email '])`

I would suggest replacing the work_hours lambda and map with ` between_time ` which is direct and to the point : #URL

this bug is resolved in master , but concat is the right way to do this
This would work well in case the columns you want to merge on are index columns , which you can achieve using ` pandas.set_index() ` , possibly preceded by ` .reset_index() ` .
I've tried that and it doesn't seem to be working . What I end up with is a stack of the three dataframes with blanks for all columns which aren't present in the other dataframe . What I'm looking for is for those blanks to be filled in from the other dataframe where there is a match in the column I'm merging on .
The ` axis=1 ` option allows you to concat horizontally using a common index for all DataFrames you want to merge . The default ` axis = 0 ` will lead to a vertical stack .

data.replace ( ' , ' , '' , regex = True ) to replace , with nothing and easy to tranform into float type with the gentleman method !
Glad you like the ` replace ` method !

Is there a ` drop_duplicates() ` functionality to drop every row involved in the duplication ?
yes there is drop_duplicates method ... CHeck documentation #URL Depending on how data is structured you should be able to do set operations too . can definately replace items in frame with items from another frame . Not sure offhand if it is possible to check for uniqueness across all the columns
if there are non duplicates ... which dataframe has the accurate ones . Ie do you merely want to find the unique items or do you need to merge them with some additional logic ?
Apply by the columns of the object you want to map ( df2 ); find the rows that are not in the set ( ` isin ` is like a set operator ) #CODE

I have a classical database which I have loaded as a dataframe , and I often have to do operations such as for each row , if value in column labeled ' A ' is greater than x then replace this value by column'C ' minus column ' D '
` apply ` should generally be much faster than a for loop .
This example you provided is : ` ( df [ ' A '] == 999 ) & ( df [ ' B '] == 999 )` , But if you have a branches with else statement also you should use ` apply ` along the asix .
I added an example to the answer that covers that case ( using ` apply `) .

Interpolate only one value of a TimeSerie using Python / Pandas
I just need to interpolate ONE value , not the whole TimeSerie
I just want to interpolate data using a linear function between the previous index ( ` 2013-08-11 14:23 : 49 `) and the next index ( ` 2013-08-11 14:19 : 14 `)
Take a subset of the Series including only the entry above and below your target . Then use ` interpolate ` . #CODE
Thank-you Dan Allan . I'm afraid I don't have the reputation to comment but Dan Allan's interpolate function raises an exception if asked to interpolate a time already defined in the ts index . E.g. #CODE

I am attempting to use python insert the parsed data into an sql database based on the columns and rows . The columns are each a different variable , and as such there is a variable table for each variable . As in : tbl 1 will include data points 1A , 2A , 3A , and so on . The code I am trying to use is : #CODE

In cases where you have multiple branching statements it's best to create a function that accepts a row and then apply it along the ` axis=1 ` . This is usually much faster then iteration through rows . #CODE

I now want to merge ( 1 ) into ( 2 ) in such a way as to match the first 2 columns of ( 1 ) with the corresponding columns of ( 2 ) , pulling in the other column in ( 2 ) appropriately [ in the actually data set of ( 1 ) , ' id ' , ' revision ' and ' colour ' are not necessarily consecutive columns , and there are other columns ] .
Nice :) how would I have done this using a merge or something ?

Append rows to a pandas DataFrame without making a new copy
Note Unlike list.append method , which appends to the original list and returns nothing , append here does not modify df1 and returns its copy with df2 appended .
How can I append to an existing DataFrame without making a copy ? Or in the terms of the note how can I modify df1 in place by appending df2 and return nothing ?
Why not use concat ? #CODE
Upcoming pandas 0.13 version will allow to add rows through ` loc ` on non existing index data .

but I don't know how to translate this line ` d = diff ([ 0 c ( n )]); `
Note that either of these requires that you're using ` pandas ` at least at ` 9da899b ` or newer . If you aren't then you can cast the ` bool ` ` dtype ` to an ` int64 ` or ` float64 ` ` dtype ` : #CODE
#ts [ np.random.rand ( len ( ts )) > 0.5 ] = np.nan
c = valid.astype ( int ) .cumsum() # because of Pandas bug with bool and cumsum for pd < 0.12

Anyway , as the ` hstack ` docs say , it's " Equivalent to ` np.concatenate ( tup , axis=1 )` . The ` concatenate ` function is more general than ` hstack ` , in that you can stack on a dynamically-chosen axis ; I think ` hstack ` is more readable when you're specifically thinking in terms of " columnwise " instead of " axis 1 " , but really , that's a minor quibble either way .

Hi , it seems this algo will fail if we have ` l = [ 1 , 0 , 1 , 0 ]` , I think ` if len ( current_drop ) > len ( current_rise ): ` need to be changed to ` if len ( current_drop ) > = len ( current_rise ) and len ( current_drop ) > 1 and current_drop [ 0 ] <> current_drop [ -1 ]: ` same for the other ` if `

You can use concat #CODE

I have tried using ` pd.cut ` or ` np.digitize ` with different combinations of ` map ` , ` apply ` , but without success .

just FYI , about to merge into pandas master this functionaility : #URL ( you can do this on 0.12 and before by : `` s.apply ( lambda x : x / np.timedelta64 ( 1 , ' D '))``
This is a good candidate for .apply . You can do this in the same line where you compute column values by putting .apply ( lambda x : x / np.timedelta64 ( 1 , ' D ')) at the end to apply the conversion at the column level . e.g. s3 =( s1-s2 ) .apply ( lambda x : x / np.timedelta64 ( 1 , ' D ')) .

I am trying to use scipy.opt to give me the weights back that conform to individually being bound to ( 0 , 1 ) , then also have the grouped class weights to be bound by the limits I showed above .
for purposes of illustration , suppose I were to do this with constraints I would have in psuedo code : c_ = ( { ' type ' : ' eq ' , ' fun ' : lambda W : sum ( W ) - 1} , { ' type ' : ' eq ' , ' fun ' : lambda W : .08 <= sum ( equity weights ) <= .51 } , { ' type ' : ' eq ' , ' fun ' : lambda W : .05 <= sum ( intl_equity weights ) <= .21 } , etc .....
To map each of the level names to an interval do this : #CODE
Map each of your level names to an interval and add that to your original dataset as a column named ` range ` . Does that make sense ? I'll add something to my answer to illustrate this .

that's why `` resample `` is prob the right solution , unless she wants dups

What I want is a shift to the closest after the next second . So in this case , the series would look like #CODE

Im using groupby method to group data in a dataframe and then use the outcome to modify the dataframe ( for example , changing a bool value in one of its columns ) I've tried the modification in two ways :
I understand now the source for most of my problems using groupby . To my taste , the groupby mechanism is logically too ambiguous and the design encourages the user to use it in the wrong why . The way I saw this , the whole idea behinds data analysis with pandas is to group and apply . I thought grouping is the most expensive task , so I imagined the proper use would be to group only once , and then do what ever you want with the groups . as long as the group members dont change , you shouldn't regroup a dataframe . This idea is also implied from the design , as you can save a groupby object , which for me implies that the author of pandas wanted as to create a groupby object only once .
If Im right , the linkage between a groupby element to a dataframe is quite weak , and the results of several dataframe modifications and groupby operations cannot be fully anticipated . So what is the solution ? running groupby for each and every apply operation ? that seems redundant ..
You just need to return the frame in your function . Apply takes the output of the function and creates a new frame ( of the applied data ); if you return ` None ` in your function then it uses the original ( and if you don't return a value , then you are implicity returning ` None `) #CODE
Sorry , didn't get it completely .. what do you mean by " if you return NONE n your function then it uses the original " you mean that the underlying dataframe is determined according to my apply function returning or not returning a value ? Im still confused about what going on under the hood here . I thought that a groupby object simply holds the index of all gruop and their members . but this is clearly not so . So what , groupby object holds a copy of the original dataframe ? Please elaborate more and be more clear ..
also , this implies regrouping the dataframe for each apply operation , which can be expensive and redundent ( see my edits .. )
Maybe you can explain in a simple example what your problem is . Optimizing is often the last step . Make sure you have correctness first . Profile , THEN optimize if needed . Groupby is a cheap operation ; the apply can be more expensive . But unless you have LOTS of groups this shouldn't matter .
why don't you group by both columns then ? If your process is iterative then just groupby ( and apply after each iteration )

Do an outer join : #CODE
If you have different column names that you want to merge on , in this case let's say that `' ID '` in ` Donors ` should be `' fundraiser ID '` you can do #CODE
would the merge be able to work if the column names were actually different ? In the donor dataframe it should have been " fundraiser ID " instead of just ID .

Surprised no one mentioned the ` drop ` method in pandas : #CODE

Problem description : I have two dataframes ( ' Train ' and ' Test ') with nearly identical columns ( ' Test ' has two variables that don't appear in Train , and Train has one variable that doesn't appear in Test ; however , to produce " Test " , I needed to do some processing in R , because I couldn't figure out how to do the equivalent of PLYR's full join in Pandas . The end result was that all spaces and apostrophes were replaced with periods in the R dataframe names in Test .
I would suggest using ` rename ` with a function , e.g. to replace apostrophes and spaces with ` . ` : #CODE
is a full-join different from an outer join ` ( df1.join ( df2 , how= ' outer ')` ?

which should work on any ` pandas ` version that has the ` map ` method on ` Series ` objects and any numpy version that is supported by ` pandas ` .

You can use the ` resample ` ( DataFrame or Series ) method : #CODE

I was writing up a version using ` groupby ` and ` apply ` , but this works too . : ^ )
@USER it does ... I wonder if there is an easier way to do it , I'm sure I've used / seen a neater solution than arange ( len ( x )) .
`` df.groupby ( ' col3 ') [ ' col1 '] .apply ( lambda x : Series ( np.arange ( len ( x )) , x.index ))`` works , but yours is cleaner

Another solution would be to use ` pandas.melt ` to avoid unnecessary creation of a ` MultiIndex ` , though this isn't that expensive if your frame is small and with my solution you still have to create a temporary for the " molten " data . The guts of ` melt ` suggest that both ` id_vars ` and ` value ` are copied since ` id_vars ` creation uses ` tile ` and ` value ` creation uses ` df.values.ravel ( ' F ')` which I believe makes a copy if your data are not in Fortran order .

The values in the action columns represent the number of times the user took that action on that day . I would like to translate this into a wide ` DataFrame ` but be able to extend the time frame arbitrarily ( say , to 365 days ) .
You can use your MultiIndexed DataFrame , create a new index with ` itertools.product ` combining all the users from your DataFrame and all the days you want , and then just replace the index filling the missing values with 0 . #CODE

I'm trying to copy a pivot table in excel in python using the pandas ` pivot_table ` function , I'm having trouble with the output . Basically , one of my rows is a long string and when outputting to the console I just get summary information .
The former ouputs a pivot table as expected but this is the output for the latter : #CODE
Have you tried saving the pivot table to csv ?
@USER can you add the actual statements that work and don't work with the " simple example " that you show above ? Is the simple example the DataFrame that you are trying to pivot on ? Or ... what ? Thanks

You might create a ` DataFrame ` for what you want to match , and then ` merge ` it : #CODE

You are overwriting your frame with each iteration of the loop . As Phillip Cloud suggested , you can make a list of frames that you append with each loop . I simplified your code differently , but I think this gives you what you want . #CODE

Freq : D , Length : 1826 , dtype : float64 ----- I have data as shown and I tried using ts.groupby ([ lambda x : x.month , lambda x : x.month ]) .mean() for monthly Climatology . It worked but the averaging is not centered at middle of of the month rather it is centered at end of the month . Does this mean that average is taken by 15 days on either side ?
Freq : D , Length : 1826 , dtype : float64 ----- I have data as shown and I tried using
Freq : D , Length : 1826 , dtype : float64 ----- I have data as shown and I tried using ts.groupby ([ lambda x : x.month , lambda x : x.month ]) .mean() for monthly Climatology . It worked but the averaging is not centered at middle of of the month rather it is centered at end of the month . Does this mean that average is taken by 15 days on either side ?

To cut to the chase I've been having some problems even doing the most basic inserts of dummy data into an HDF5 file . I was following the supplied code in another post but when I get to writing in the storer format the code execution hangs . I've not tried the table format yet , I'd like to get this working first . I'm running the following file .
using ipdb in ipython I've got a call stack to the hanging line as pasted below . This line is calling cPickle , which I assume is some sort of compiled library . I can't step into this line ( using ' s ') any further so am out of ideas as to what the problem is . #CODE
In stepping through the code I've noticed that ` BlockManagerStorer.write() ` method , which is about half way up the call stack above , is looping through 2 sets of data blocks ( lines 2002 to 2006 ) . The first loop runs fine and it is the second loop that hangs . Further the ` GenericStorer.write_array() ` method that is then called in the next stack down has ` value.dtype.type == ' numpy.float64 '` in the first pass but ` value.dtype.type == ' numpy.object '` in the second pass leading to a different branch on line 1785 of io / pytables.py being taken . EDIT : The first pass is writing a ~800 Meg file so it appears to be most of the expected output file .

Perhaps you want interpolate instead of resample . Here's one way : #CODE
That said , it seems like resample could be improved . At first glance , the behavior you've demonstrated is mysterious and , I agree , unhelpful . Worth discussing .

as an aside , if you do have lots of data , `` table `` format prob better for you , as you can `` append `` , e.g. do chunked reads and writes ( and queries ); `` storer `` cannot

- read all files in one directory and concat them into one data frame with one numerated index .

And now resample #CODE
Thanks for your answer ! But I get Errors : If I drop the " parse_dates =[[ ' date ' , ' time ']]" in the " pd.read.csv ... " line , the Error : AttributeError : ' str ' object has no attribute ' date ' occurs . If I don't drop it , the Error : AttributeError : ' SeriesGroupBy ' object has no attribute ' filter ' occurs .

I realize this usage pattern circumvents most of the performance benefits of the numpy / Pandas stack .
Another aspect of this question : can all such functions be converted to a numpy-efficient representation ? I've much to learn about the numpy / scipy / Pandas stack , but it seems that for truly arbitrary logic , you may sometimes need to just use a slow pure Python architecture like the one above . Is that the case ?
You should apply your function along the axis=1 . Function will receive a row as an argument , and anything it returns will be collected into a new series object #CODE
As for the second part of the question : row wise operations , even optimised ones , using pandas ` apply ` , are not the fastest solution there is . They are certainly a lot faster than a python for loop , but not the fastest . You can test that by timing operations and you'll see the difference .
Some operation could be converted to column oriented ones ( one in my example could be easily converted to just ` df [ ' a '] + df [ ' b ']`) , but others cannot . Especially if you have a lot of branching , special cases or other logic that should be perform on your row . In that case , if the ` apply ` is too slow for you , I would suggest " Cython-izing " your code . Cython plays really nicely with the NumPy C api and will give you the maximal speed you can achieve .
@USER I saw that you rarely use apply along ` axis=1 ` . Is there any specific performance reason ? Shouldn't that be the fastest way to itterate over the array row wise ?

How can I drop or disable the indices in a pandas Data Frame ?
I am learning the pandas from the book " python for data analysis " and I already know I can use the dataframe.drop to drop one column or one row . But I did not find anything about disabling the all the indices in place .

This will scale with ` n * m ` where ` n == df1.shape [ 1 ]` and ` m == df2.shape [ 1 ]` so you should transpose the axes of either ( you said above that you have the option to do this ) such that the smallest axis is the columns axis .
Well , I thought about something similar to your solution , however I didn't pursue it further because I want to apply it to millions of rows , so I'm afraid this won't scale well .
Wonder if there should be a native way to do apply across multiple dataframes ...
A little bit cleaner version would be ` map ( correlate , i , j )` . You also avoid a bit of the overhead with using ` lambda ` , variadic functions , and ` zip ` .

You can't ( at least easily ) . When you call ` concat ` , ultimately ` np.concatenate ` gets called .
You easily generate two arrays with different strides sharing the same memory like so : #CODE
Is this behaviour a bug ? ` merge ` has a ` copy ` parameter , so when I do ` df_concat = df.merge ( df1 , left_index=True , right_index=True , copy=False )` If the indexes of both DataFrames are the same ( which in this case is true , the Series objects don't have to be changed , why is pandas copying the data when I explicitly instructed it not to with ` copy=False ` ? Even if I first create an index and then create the ` df ` , ` df ` and ` df_concat ` and ` df.index is ` df_concat.index ` returns ` True ` , still the data is copied .
If that's the case then I'm just asking what does the ` copy ` parameter of the ` merge ` function does ? :)

No Errors with Transpose Function and No Change In Result
I am having difficulty making use of a transpose functions made available through pandas and scipy :

I have the same question for concat on a ` Series ` .
so my updated question is how to perform the rename / case insensitive concat with a Series ?
You can replace ` str.lower ` with whatever function you want to use to rename your ` index ` .

and I want to apply a function which uses the index of the row : #CODE
I don't believe ` apply ` has access to the index ; it treats each row as a numpy object , not a Series , as you can see : #CODE
To get around this limitation , promote the indexes to columns , apply your function , and recreate a Series with the original index . #CODE
In my case ( a dataframe , with axis=1 ) , x.name() returns the value of the index when I apply a function lambda x : x ...
so when calling ` apply ` on ` DataFrame ` its index will be accessible through ` name ` of each series ? I see this also is true for ` DateTimeIndex ` but it is a little weird to use something similar to ` x.name == Time ( 2015-06-27 20:08 : 32.097333 + 00:00 )`
You may find it faster to use ` where ` rather than ` apply ` here : #CODE
I recommend testing for speed ( as efficiency against apply will depend on the function ) . Although , I find that ` apply ` s are more readable ...

Replace DataFrame column index with MultiIndex
I have a pandas DataFrame with a " flat " column index , [ ' Sample 1 ' , ' Sample 2 ' ... ] , and I've constructed a MultiIndex that has two levels , the first of which having the same elements as my DataFrame's column index ( ' Sample 1 ' ... ) . I want to replace the column index of my DataFrame with this MultiIndex , with each of the elements of the MultiIndex replacing the column index element that has the matching name . So , the column headed with ' Sample 1 ' should now be headed with ( ' Sample 1 ' , ' group x ') , column with heading ' Sample 2 ' should now be ( ' Sample 2 ' , ' group x ') , and so forth .
Seems like this should be a simple join or matching concatenation , but can't find the method ( and / or keywords ) to do it .

I'm trying to iteratively read / append multiple csv files - that amount to about 8GB in total - into a HDF5 store based on this solution and this solution for creating a unique index . Why I started to do this is because I read that doing so would result in a file that would be fast accessible and relatively small in size , and thus to be able to read into memory . However as it turns out I get a h5 file that is 18GB large . My ( Windows ) laptop has 8GB of RAM .
the ` min_itemsize ` argument specifies the minimum size of this string column ( for ' F ' , ' G ') ; this is only to guarantee that your strings don't exceed this limit ; but it makes ALL rows for that column that width . If you can lower this it will cut your storage size
Its not clear what your filtering is actually going to do ; Can you explain a bit more ? You need to thoroughly understand how HDF storage works ( e.g. you can append rows , but not columns ; likely you need to create a results table where you append transformed / filtered forws ) . You also need to understand how the indexing works , you need a way to access these rows ( which a global unique will do , but depending on the structure of your data might not be necessary )
Excellent point , did not think about that ! There is filter I apply that filters out about 90% of the data . ( However the rest of the calculations have to be done over the concatenated DataFrame . ) Thanks a lot !

pandas HDFStore.append data_columns after append operation
I am reading a csv file to a dataframe chunk by chunk and appending it to an HDFStore as the entire data cannot fit in memory . In my append operations , I am passing a list of data columns via the data_columns parameter so that I can perform out-of-core filtering via HDFStore.select passing conditions to the where parameter .
The reason is that the time to perform chunk by chunk appends to HDFStore tables are increase exponentially on each write . I am guessing that each time an append is made to HDFStore with new data , the index in data columns is recalculated . It would be much more efficient if the data is placed entirely in the store and data columns are specified after creation so that the index is calculated just once .

I am adding data to a pandas ` Series ` via the ` Series #append ` method . Unfortunately , when ` nan ` is added to a ` bool ` Series , it is automatically converted to a ` float ` Series . Is there any way to avoid this conversion , or at least coerce it to ` object ` dtype , so as to preserve the distinction between ` bool ` s and ` float ` s ? #CODE
@USER : what do you mean by " separate Series " ? Do you mean that for every Series ` foo ` , I should add a bool Series ` foo_missing_values ` , which is True for every missing value in ` foo ` ?
As @USER said , the best way is going to be to append a ` Series ` with ` object ` ` dtype `

I have changed the list of lists to a column stack according to suggestions below . It is working fine now . So tsdata , which was a list of lists is changed to #CODE

Pandas : interpolate missing rows and plot multiple series in dataframe
You may not actually need interpolation . The above executes even if some data is missing , and the plot lines visually interpolate the data linearly . But if you want actual interpolation -- say for additional analysis -- see this answer .

I guess I could write a function using ` try ` and then use pandas ` apply ` or ` map ` , but that seems like an inelegant solution . This must be a fairly common problem , right ?
In fact , you can apply this to the entire DataFrame : #CODE

If you really wanted to do this you could use a groupby apply : #CODE
Pivot table gets you a more useful DataFrame : #CODE

Please can someone suggest how to replace ` 1 ` by ` i ` when using a ` for ` loop .

and apply the function to each group , and then ` unstack ` from a Series to a DataFrame : #CODE
@USER Not sure I do fully either , I think I should probably put it up as an issue on github to discuss / fix , suspect it is untested behaviour , but it seemed that you can only apply aggfunc to columns which were going to be in your pivot_table ( IIRC the rest are not passed to the function so couldn't be used ) , also I was seeing SNDArrays which was confusing ... I need to investigate a further .

You can then drop the ` NaN ` s and the titles , which I think is described in the other answer : #CODE

Using a TimeGrouper in this way is equivalent of resample ( and that's what resample actually does )
and then pick out whatever ' frequency ' you want . Eg here is a 5s resample with a 30s interval . #CODE
Thanks ! what if you don't want to apply a rolling mean but an arbitrary function in rolling overlapped windows ?
see here and the cookbook . #URL you also might be able to just tshift ( ' 15s ') then resample at 30s freq . alternatively there is a loffset argument u can pass to TimeGrouper

while trying the simple ( non date-specific ) numpy function ` ptp ` gives me an integer answer : #CODE
Assuming you have indexed by datetime can use groupby apply : #CODE

Thanks , that does the trick . The `` .filter `` method seems to take the same time regardless of the number of records which meet the selection criteria unlike the `` .xs `` concat method . I suppose this is obvious , but I thought I would mention it .

I wanted to try ` concat , join , merge , append ` but those doesn't seem to be appropriate .
It's definitely worth reading the docs and really understanding what each one does relative to the other . You'll gain a bit of intuition for when you should use each . Personally , I end up using ` concat ` more than any of the others .
kind of feels like there could be a way to insert via iloc in one go , but I can't think of it .
Thank you @USER Cloud . I tried both your answers . For this example all work perfectly but when I introduced this to my files only ` df.join ` was ok . Using ` concat ` got ' ValueError : Shape of passed values is ( 18 , 52760 ) , indices imply ( 18 , 52727 )' . It's strange because df1 len is 52716 and df2 52555 .
I've checked df1 index column and they were some duplicates . After removing , both methods works . Anyway ` join ` and ` concat ` doesn't work properly in case there are duplicated index entries ...
If you have duplicate indexes in ` df ` and then you're resetting the index of ` df2 ` to match that of ` df ` , then you'll necessarily have duplicates in the ` join ` ed index . So the extra index entries are expected . Did you have something else in mind ?

Note that you can't just use ` n ` , because you may have a group where ` len ( group ) 2 ` , therefore
` len ( x [: n ]) ! = n `
It's also present in apply though ( with both our answers ) . ?

For ` object ` data I can map two columns into a third , ( ` object `) column of tuples #CODE
( see also Pandas : How to use apply function to multiple columns ) .
@USER Thanks . It is not for further analysis - just I want to calculate distribution of pairs ( e.g. to calculate mutual information ) . Another option for me is to use ` collections.Counter ` and ` map ( lambda x , y : ( x , y ) , df [ 0 ] , df [ 1 ])` as in this use case I don't need index anymore ; I was curious if I can manage within ` pandas ` .

This means you can invert np.percentile using map , and then apply a shift and a subtract to get the " percentage if array in interval " you're after .

Python pandas timeseries resample giving unexpected results
The data here is for a bank account with a running balance . I want to resample the data to only use the end of day balance , so the last value given for a day . There can be multiple data points for a day , representing multiple transactions . #CODE
pass ` sort=False ` to a groupby ( you can't do this with resample though ) #CODE
To be able to resample your data Pandas first have to sort it . So if you load your data and sort it by index you get the following thing : #CODE
a sort on duplicates is arbitrary ( e.g. no guarantees from merge or quick sort ) , IIRC
there is a `` sort `` keyword to `` groupby `` to handle this case ! ( but not too resample )

Dataframe Merge in Pandas
For some reason , I cannot get this merge to work correctly .
When trying to join the above to a table based on the ` rsparid ` columns to this Dataframe ... #CODE
Note , the " datalines " column can have thousands more rows than the rspars , thus the left join . I must be doing something wrong ?
I think you're misunderstanding ` merge ` . If there are no * values * in ` datalines.rsparid ` that are equal to the values in ` rspars.rsparid ` then you'll get ` NaN ` s because there's nothing to join on .

It feels like there ought to be a neat way using stack ( essentially separating into away / home , enabling a groupby apply ) .
Create a ` defualtdict ` ( with default value 0 ) where you will keep the current scores of the teams , and apply along the ` axis=1 ` a function that updates this dictionary and returns a tuple of results . Then just concatenate your DataFrame and the resulting DataFrame from the ` apply ` function along ` axis=1 ` . #CODE
stack / groupby / apply is * significantly * faster than this method ( once there's more than 40 or so rows ) !
@USER Hah true for the Counter :) Actually I agree with you that grouping should be the preferred way of thinking when you use pandas , but I didn't knew the knowledge level of the OP so I tried to follow his intuition of doing it row-wise . But since we are comparing the methods , your is faster , but it takes at least twice as much memory since a stack cannot be a view and I don't know the implementation details of groupby so I cannot tell how much that costs ( assuming that I did the same thing of appending columns and not concat ) .
I'm not sure about the memory ( I would've thought it would be much less than the defaultdicts ... ) , not sure if stack creates a copy ( concat doesn't so it might not wither ) . Would be ** shocked ** if memory was larger for stack approach tbh , but happy to be proved wrong !
You're not including the memory of the defaultdicts ( ! ) which are very inefficient ( when compared to numpy ! ) both in terms of speed and space , they are the thing which slows you down . So it * may * be twice the arrays ( I'm not sure either ) , but that doesn't mean twice the memory used overall . groupby / concat / cumsum all are very cheap , written in cython with numpy so fast and low memory .
@USER Just measured it . If the original data frame takes N memory , defaultdict version takes 6xN more memory , and the stack version takes 32xN more memory .
@USER It's not * so * weird , the stack is ( pretty much ) indep of number of teams , the defaultdict is completely dep . If there were just one team , defaultdict has to store one number ( hehe in which case both are answers get confused ) , stack stores same as if there were lots . Also just noticed , I dislike the global variable / side effect in your function ( but I'm not sure there is a neat way to pass the defaultdict in the arguments ) ...
@USER how is it independent when in one case it takes 32x more memory and in the other it takes only 4x more memory ? And the global can be passed as an argument to apply , but that is not the an issue :)
First we have to ` stack ` in such a way we can do this independently ( of away / home ): #CODE
Finally , we just have to insert these into the frame with the correctly named columns : #CODE

@USER sorry you do this * before * the stack , I was going to add a line at the end saying then you stack , whoops ! ( Was just fixing the bug in pandas which required the index to be dropped :) )
@USER Ah thought so ... the numbers don't match up with your original input ( ? ) , I think you can do this easier with value_counts , will append to answer .

Or , another way is by selecting part of you data to only view that . In the code below I slice the first 10 rows and the first ten columns of the table using ` ix ` : #CODE

First replace all the string values with ` None ` , to mark them as missing values and then convert it to float . #CODE
You could use ` pd.to_numeric ` method and apply it for the dataframe with arg ' coerce ' . #CODE

The data looks like the above stream , after I did some find and replace in VIM ( I know i can just script this in python ) . How do I best get this weird-format into Pandas ? I ideally want datetime , the String aggregatedimension value , and the quantity . But there is a lot of None , in this parse-needed data .
You can define minimal classes for ` Aggregate ` , ` DateAggregateDimensionValue ` , and ` StringAggregateDimensionValue ` , then ` eval ` each line in turn : #CODE
Use these minimal classes to ` eval ` the input strings : #CODE
Of course , this comes with all the usual caveats about using ` eval ` , such as beware of any possible injection of malicious code . But I suspect you are already in control of this input file yourself , so if you inject your own malicious code , you have only yourself to blame .
` dump() ` will show all the named results values that are available to you - note how the quantity attribute can be accessed directly using ` data.qty ` . This was setup for you with the definition of the results name " qty " in `" quantity= " + intvalue ( " qty ")` . ` timestamp ` , ` s1 ` , and ` s2 ` can be accessed similarly . ( There is still a little ` eval ` ing in this , cleaning that up is left as an exercise for the reader . )

How about this . First check the NaNs are in the same place ( using isnull ): #CODE
Then check the values which aren't NaN are equal ( using notnull ): #CODE

I would duplicate twice your original dataframe , then in the first copy ( df1 ) delete first two rows , in the second ( df2 ) delete first row . Then merge columns from these three dataframe in this order :
If your final game plan involves doing on of these ... just use these . If it's something else , consider writing it as a generic rolling apply .
Note : you can also set the freq to be a DateOffset ( e.g days , minutes , hours , etc . ) , which would be more difficult to do with a reshape , and this gives you lots of flexibility .

Pandas stack columns in dataframe an make a histogram
I would like to plot a histogram of the data . However I want to stack the columns and do the histogram . Seems like a simple task , however I can not do it !!
What I want to do is stack the columns ( VSPD1-VSPD7 ) , and make them the index column . I tried : #CODE
I have included the dataframe above . What I want to do is stack the columns ( VSPD1-VSPD7 ) , and make them the index column . I tried : cnames=list ( df_vspd.columns )

You can simply use an ` apply ` to do this : #CODE

It doesn't feel like a good solution , however , because I have to bust out ` itertools ` , build another MultiIndex by hand and then reindex ( and my actual code is even messier , since the column lists aren't so simple to fetch ) . I am pretty sure there has to be some ` ix ` or ` xs ` way of doing this , but everything I tried resulted in errors .
You can use either , ` loc ` or ` ix ` I'll show an example with ` loc ` : #CODE
Actually I think this is the optimal way of filtering out a list of labels in an arbitrary level of MultiIndex without creating all the tuples . I would just use ` loc ` for clarity .

Getting pandas new indexing " loc " and " iloc " to work with PyInstaller
If I replace ` iloc ` with the older ` ix ` , everything works as it should so I'm suspecting that PyInstaller can't find / use the latest version of pandas . How can I get PyInstaller to work with ` iloc ` as well ?

I have experimented with multi-indexes , and suspect the answer lies in that direction with some creative shift() . However , I'm afraid I've mangled the problem through a haphazard application of various pivot / melt / groupby experiments . Perhaps you can help me figure out how to turn this : #CODE
Firstly to get the change from previous year , do a shift on each of the groups : #CODE
However , I think you may be better off keeping it as a pivot : #CODE

Case 1 : I need to transform the DataFrame to the independent and dependent variables of a regression . The transformation involves generating a 2D array and a 1D array group by group and then stack together the rows of the arrays from each group . It would be great if only I can write : #CODE
I am trying to transform the data points in a DataFrame to a two dimensional array group by group and then transform the same data to a different one dimensional array . For example , the two dimensional array can be the independent variables of a regression while the one dimensional array can be the dependent variable . I could have written two functions , one for each array , and apply them separately , but that would be very slow . So traverse the data once and generate both arrays would be preferred .
Now as for why packing the result into a dataframe is awkward , not to mention it is slow . The two dim array is the result of np.dot() . So when I need to construct the DataFrame as the return value , I have one two dim array and one one dim array at hand . I can then either use np.column_stack to stack them together or write a loop to slice the two dim array one column at a time to a dict . Neither is not ideal .
You don't need to use apply here , and unless you are using a cythonized function which can operate on a frame / series , it doesn't make any difference in perf .

But when I try to use the append command , #CODE
I want to get an array / tuple of shape / len 6
Your problem can be solved , I believe , if you drop the MultiIndex business . Imagine ''' df ''' only has the ( non-unique ) ' Patient ' as index . ' Slices ' would become a simple column . #CODE
As long as your new dataframe ( df2 ) defines the same index you can now join on that index quite simply : #CODE
IMHO ` MultiIndex ` should be avoided unless there's no way around it . You can easily move your ` MultiIndex ` to columns using some combination of ` stack ` / ` unstack ` / ` reset_index ` / ` melt ` . Queries will be faster and * much * easier if your index levels are columns in your ` DataFrame ` .

I'm working on replacing an Excel financial model into Python Pandas . By financial model I mean forecasting a cash flow , profit loss statement and balance sheet over time for a business venture as opposed to pricing swaps / options or working with stock price data that are also referred to as financial models . It's quite possible that the same concepts and issues apply to the latter types I just don't know them that well so can't comment .
So far I like a lot of what I see . The models I work with in Excel have a common time series across the top of the page , defining the time period we're interested in forecasting . Calculations then run down the page as a series of rows . Each row is therefore a ` TimeSeries ` object , or a collection of rows becomes a ` DataFrame ` . Obviously you need to transpose to read between these two constructs but this is a trivial transformation .
Some of the corkscrews I have to build , however , have 100's of intermediate operations . In these cases what's my best way forward ? Is it to accept the slow performance of Python ? Should I migrate to Cython ? I've not really looked into it ( so could be way off base ) but the issue with the latter approach is that if I'm moving 100's of lines into C why am I bothering with Python in the first place , it doesn't feel like a simple lift and shift ?

The problem with ` resample ` is that when you first resampled ` a ` , the last entry become april 2053 . So when you do your second resampling the end date will be 2053-04-01 . So it did the correct resampling but the first one moved the end date from november to april .

If not , could you suggest how to modify Pandas in order to provide irregular duration ` Period ` objects ? ( this comment suggests that it might be possible " using custom DateOffset classes with appropriately crafted onOffset , rollforward , rollback , and apply methods ")
It looks like a central assumption in ` PeriodIndex ` is that every Period has the same ` freq ` .

If I give you the complete code then you don't learn anything . Stack Overflow isn't really a site for " give me an implementation of this " .
Since i tried tu find the prediction values of my variables ... For that , i'm developping this code and i tried to apply what i found in this link #URL

NOTE : You don't necessarily have to ` concat ` the ` list ` of ` DataFrame ` s , but often that's a useful next step so that you don't have to keep operating on a ` list ` of ` DataFrame ` s .

If I replace the strings ' L ' and ' S ' with integers like -1 and 1 it works OK , so that's a work around . But the stranger issue is what happens if I look at mixing the output of ` np.where ` with a DataFrame that has fewer rows . #CODE
I thought it might be due to Pandas somehow sharing metadata about Index objects and perhaps being allowed to truncate when inserting data if the data comes from an object that has the same index . #CODE
it's not misleading at all ; in fact it's the primary reason for having an index in general ; all operations align . if you don't wnt it then use numpy directly which doesn't align
then just use the `` .values `` after the `` np.where `` and use at your own risk . you are mixing concepts here . The purpose of pandas is to align by the index . There could be a non-alignment index I suppose , but then what's the point ? If you don't care at all , then use straight numpy . Then when you do care about an index , put it in a frame .
I think you also might want to separate your work into separate frames that DO have meaning . Then merge / combine at a later step .
Sometimes that's a good idea , but it's not a restriction that the data container should impose on the programmer . Let the programmer decide when they want to enforce that . In my case , the DataFrames do have a relationship , and the " proper " way to do this is to first calculate the column that I want in the second DataFrame ( the argument of ` np.where `) , then perform a ` merge ` or ` join ` based on other ( non-index ) columns . There are practical reasons not to set those columns as the index , and since I'm just relying on this as a data structure , it's no fun that it makes this assumption .
But given that ` np.where ` would be faster than a merge , it would be nice if I could trust that was just inserting values positionally , not by alignment , unless the index objects are actually the same object . But hey , that's just my opinion .
true .... not pushing you to upgrade , but in ( 0.9 or 0.10 ) there is an alignable where , FYI . Yep sometimes its best to just keep the ' index ' information as a column . Your problem is a bit ' unique ' in that sometimes you don't want to align . I will think about this .

I think it's because formula and data are not keyword arguments . Try to drop them from the call to ols .
drop the formula and data keywords did work . I am having trouble understanding why

Boxplot with pandas groupby
What I want to do is to be able to create a boxplot with this data grouped according to different catagories in a specific line of the multiinex . For example if I were to group by ' SPECIES ' I would have the groups , ' aq ' , ' gr ' , ' mix ' , ' sed ' and a box for each group at a specific time in the timeseries .
but it gives me a boxplot ( flat line ) for each point in the group rather than for the grouped set . Is there an easy way to do this ? I don't have any problems grouping as I can aggregate the groups any which way I want , but I'm not sure what I'm doing wrong with this boxplot .
Basically groupby output needed to be transposed so that the boxplot showed the right grouping :

` matplotlib ` also offers the function ` boxplot ` to do vertical box plots .
matplotlib's ` boxplot ( ..., vert=False )` makes horizontal box plots .

First , ` ix ` will first try integers as labels then as indices , so it is immediate that 59 and 62 are the same . Second , if the index is not ` 0 : n - 1 ` then 1107177 is a label , not a integer index thus the difference between 60 and 61 . As far as the float casting goes , it looks like you might be using an older version of pandas . This doesn't happen in git master .
Here are the docs on ` ix ` .
Notice that the sorted row indices are integers and they don't map to their locations .

if you use a flat file , a dict structure is easy to map to a CSV file

reshape data frame in pandas with pivot table
With pivot table you can get a matrix showing which ` baz ` corresponds to which ` qux ` : #CODE

In that case high-low is not ok . You need to decide how to compute time / bin and than translate to pandas .

I get the warnings below when building : ( for readability I use " PYINSTALLERDIR " to replace the full path , which is " C :\ Users\name\Downloads\ pyinstaller-pyinstaller-v2.0-544-g337ae69 \ pyinstaller-pyinstaller-337ae69 \ " . #CODE

I'm trying to hand off data from Pandas ( imported from a CSV ) to a MySQL database via Python's MySQLdb library . I'm having trouble when literal backslashes come into play . I escape the single backslashes from the raw input , so Python knows they're a literal backslash , not an escape on the subsequent character . But when I try to execute an INSERT query , MySQLdb says there's a syntax error . But here's the confusing frustrating part : If I copy / paste the exact string into MySQL , it executes without problem .
The values of these rows converted to a string to append to the end of an INSERT statement ( note that all MySQL columns will be varchar , so all values are wrapped in single-quotes ) #CODE
First , the bug : I incorrectly used ` str.replace() ` to replace the single literal backslash with an escaped double : #CODE

Why does pandas groupby cut give different form of output with single record input ?
a complete hack is to check for ` len ( s ) == 1 ` and pass in ` pd.concat ([ s , s ])` and then divide the result by 2 . Ick .

You can do this with a combination of ` groupby ` , ` replace ` and a ` dict ` #CODE
The ` apply ` method applies a function to each of the aforementioned subsets and concatenates the result ( if needed ) .
One thing to note is my use of ` g.name ` . This doesn't normally exist as an attribute on ` DataFrame ` s ( you can of course define it yourself if you want to ) , but is there so you can perform computations that may require the group name . In this case that's the " current " fruit you're looking at when you apply your function .

how to resample without skipping nan values in pandas
When I apply the below code pandas is considering NaN as Zero and returing the sum of remaining days . #CODE
Huh it's super complicated :D First , it's partially produced by a hack to fix another bugs ( ` _adjust_bin_edges ` metod ) . And second , some unit-tests rely on the thing that it will produce an extra bin :-/ So if I want to keep it backward compatible it can be avoided just in some really really special cases . I can give you a diff that makes the unit-tests pass and works in this case and also in the case of the bug report but without duplicated indexes . But the diff is super , extra ugly and I would never commit it :)

@USER It sounds like you're talking about lazy evaluation , which is a much broader topic than ` eval ` / ` query ` . I'm looking at the ` filter ` method and wondering if something like that for ` DataFrame ` * content * ( as opposed to the axes ) might be useful . I wouldn't be opposed to ` refine ` ...

You are looking for apply . Your example would look like this : #CODE

Pandas resample dataframe
Then ` resample ` by second ( this is the mean value by default but can be changed to sum etc - e.g. add ` how= ' sum '` as parameter ): #CODE

Apply function to values and index of series
I like the idea and is along the lines of what I was thinking about . The trouble is I can't make it work , if I apply your exact code ( plus my def of dates ) I get a Type Error : ` TypeError : ( " unsupported operand type ( s ) for /: ' buffer ' and ' int '" , u'occurred at index 2012-04-01 00:00 : 00 ')`
And you / are working on a ` DataFrame ` object inside of ` apply ` indeed .
Actually inside ` apply ` we seem to be working with a ` Series ` object , the name of which is the index of the ` DataFrame ` we're iterating through . This is why your name suggestion works for accessing the value of the index . It seems ` apply ` collapses a " dimension " of the initiating object so this is why working on a ` TimeSeries ` directly only left me with a value object and no way to access the value of the index . Not a perfect solution then but def better than where I was . Thanks again for all the help .
That doesn't work ( although it looks like it should ! ) . When I apply your solution I get ` s ` where ` s.index.month % 3 == 1 ` and ` s / 4 ` where ` s.index.month % 3 ! = 1 ` ; I wanted ` s / 4 ` and ` 0 ` respectively
ok I think is easy to enable though , let me think about it . you can also do as a chained loc ( same idea )

Can you edit your question by adding the output of : ` print() ( type ( labels ))` , ` print ( len ( labels ))` , ` print ( list ( labels ) [ 0 ])` . The truncated traceback you give does not give us any clue how ` SGDClassifier ` can be involved in this as we only get error accessing a pandas datastructure that is not used internally by ` SGDClassifier ` .

Just use the ` apply ` function along the ` axis=1 ` and pass the ` pattern ` parameter as an additional argument to the function . #CODE
I knew you'll gonna ` melt ` my mind again :)
Once I started to understand ` melt ` , I used it a lot more ! It's a pretty big hammer :)
Like regexp ;) I agree , I just always give the simplest , predictable in speed solution to the OP , and live the exotic ones to you :) ` apply ` should always be ` O ( N )` if I'm not wrong . I don't even know how to calculate how ` melt ` with ` groupby ` behaves ? ` O ( ? )`
One option would be ` DataFrame.idxmax ` , but that gives the first ( i.e. leftmost ) occurrence . However , suppose we could replace the ` True ` values with their column index , we could just use normal ` max ` . Since ` True ` is ` 1 ` and ` False ` is ` 0 ` , we can do this by multiplying with the integer range ` [ 0 , 1 , 2 , ... ]` broadcast vertically : #CODE
Oops ! We need to make ` 0 ` come out as ` NaN ` and the rest of the columns to shift over . We can do this using ` np.r_ ` , which concatenates arrays : #CODE

I'm using Pandas to compare the outputs of two files loaded into two data frames ( uat , prod ):

You can pass a dictionary of replacement values into the Series replace method : #CODE

Is it possible to reshape the pivot table in a data frame suitable for analysis in R ?, something of the type : #CODE
Ah ha ! I knew you'd eventually warm up to ` melt ` . Pun ( sort of ) intended :/
@USER :D I use it . It's just that I hesitate to use it on SO since I don't want to melt the OP's brain :P But in this case it was really obvious ... It should have been named unpivot :-/
Incidentally , we had a discussion about that over at pandas ' github ([ issue #4150 ] ( #URL )) . I agree with you , but ` melt ` is taken from R ( specifically , the ` reshape2 ` package by Hadley Wickham ) and it has been in ` pandas ` for awhile I believe , so we decided to stick with it . ` pivot ` in R is called ` dcast ` ( ` data.frame ` ` cast `) .
@USER I think we should * add * unpivot as a melt alias ( or vice versa ) . Then everyone's a winner . ... It seems strange to use melt but not ( d ) cast terminology from R .
One way is to use ` stack ` : #CODE
Just throwing this out there , but if this is faster than ` melt ` it might be worth implementing ` melt ` in terms of this .

I have researched this simple problem extensively but can't find an answer . I am trying to merge two files using pandas ' ` pd.merge ` based on a common column named " JN " . I believe it is treating my ' joined ' ( ` os.path.join `) filename as a string instead of a dataframe / csv file . After I call the ` pd.merge ` function the error says " string indices must be integers , not str " . #CODE
You cannot ` merge ` two strings . I think you're confused about what ` os.path.join ` returns . It returns a string . You have to actually read in the ` DataFrame ` s from the files named ` JJ ` and ` WW ` , then perform the ` merge ` .

Just apply the Timestamp ` time ` method to items in the date column : #CODE
Using the method Andy created on Index is faster than apply #CODE

And then apply the function along the ` axis=1 ` . #CODE

Pandas : join with outer product
How to join / multiply the DataFrames ` areas ` and ` demand ` together in a decent way ?
Now ` apply ` needs to return a ` Series ` , not a ` DataFrame ` . One way to turn a ` DataFrame ` into a ` Series ` is to use ` stack ` . Look at what happens if we
` stack ` this DataFrame . The columns become a new level of the index : #CODE
Now we want the first level of the index to become columns . This can be done with ` unstack ` : #CODE
` del ` + ` pivot ` turns out to be faster than ` pivot_table ` in this case . Maybe the reason ` pivot ` exists is because it is faster than ` pivot_table ` for those cases where it is applicable ( such as when you don't need aggregation ) .
Wow , awesome walkthrough ! ` apply ` is now among my top 5 functions to always remember . Concerning the ` pivot_table ` solution : At which point am I supposed to enter the line ? No matter when in my attempt above , I always get ` no item named Edge ` .

I am doing some analytics using Solr and specifically using the faceting and pivot functionality for a large set of log files . I have a large log file that I have indexed in Solr along the lines of . #CODE

Does ` H9 ` change with each iteration of the loop , so that you get ` len ( subsl )` different plots ?

use `` loc `` don't chain !!!!
A different approach using ` map ` : #CODE
You also don't need to ` zip ` since you're just unpacking it in the argument list of the ` lambda ` : ` map ( lambda x , y , z : value_map.get (( x , y ) , z ) , df.l , df.n , df.v )` .

I would like to divide my data table called ( my_data2 ) in two samples called ( learning sample and test sample ) . How to apply the logistic regression on the first part of my table ( the first sample ) , then apply predict on the second part ? Thank you .

One solution could be to use get dummies ( which should be more efficient that apply ): #CODE
You could use an apply with a couple of locs : #CODE

But how to merge the result ?
The resample via ohlc ( here I've resampled by hour ): #CODE
You can apply to_json to any DataFrame : #CODE
@USER yup , just do the same but replace PRICE with VOLUME :)
@USER , do you mean i need to first resample on PRICE then resample on VOLUME ?
@USER , could you help to check how could i merge the VOLUME and OHLC DataFrames ?
Don ' forget to upvote helpful answers . You can just concat them , but I'm not sure this will make sense in this case ( since one is a resample ) , since it's quite different to this you should ask as a new question . :)

What is the most idiomatic way to normalize each row of a pandas DataFrame ? Normalizing the columns is easy , so one ( very ugly ! ) option is
I would suggest to use Scikit preprocessing libraries and transpose your dataframe as required : #CODE

Use tshift . You may need to resample first to fill in any missing values . I don't have time to test this , but try this . #CODE
And now drop all the filled-in times we added , taking only values from the original Time index . #CODE
When I run this on df [ " A "] instead of df , I get all NaNs . I checked the df and it doesn't have any entries that are NaN . I also ran it without the resample operation in there and the same thing happened . When I ran it on df [ " Time "] after converting it to a column of datetime objects , it just added 5 seconds to each time . I think part of the trouble for me is that I don't really understand the resample operation ( and can't find any tutorials or explanations online ) so I don't know how to tweak the function to help me .

@USER It does work with ` Series ` . Can you post a ` Series ` object that reproduces the stack overflow ?

you can what I do in the apply , its operating on the numpy arrays directly . I don't think readsav can do this conversion .

I tried to make this algorithm : random draw between 0 and 1 ( tir ) .si tir ' ' pred then Xestime2= 1 else Xestime2=0 . I wish apply this algorithm in df [ ' X3 '] but I had 0 in all the values ?? of X3 columns . Which explains thats i have an error in my code .
You have to rewrite the entire file , you can't just append to each line . e.g. df.to_csv ( ' foo.csv ')
I understand your code but when I try to apply it to my code it does not work !!
I tried to apply what you made but i have an error !! My coding

I'm hoping there's something I've overlooked in the pandas library / documentation that allows one to know the progress of a split-apply-combine . A simple implementation would maybe look at the total number of data frame subsets upon which the ` apply ` function is working and report progress as the completed fraction of those subsets .
have u done a %prun ( profile ) on the code ? sometimes you can do operations on the whole frame before you apply to eliminate bottlenecks
Obvious warning being this will slow down your function ! You could even have it update with the progress #URL e.g. count / len as percentage .
I did the above in my answer , also cheeky percentage update . Actually I couldn't get yours working ... I think with the wraps bit . If your using it for the apply it's not so important anyway .
Note : the apply progress percentage updates inline . If your function stdouts then this won't work . #CODE
@USER DataFrame's apply is row-wise so I don't * think * dropna can be written in terms of apply ( needed to use this answer ) . Also , dropna is written in cython ( not pure python ) so doing something like this will be much slower .
To directly answer the original question , replace : #CODE

I came across what looks like the perfect solution on another SO answer ( #URL ) but when trying to apply to this series , I'm getting an exception : #CODE
I really want to be able to apply the changes within the dataframe to apply and reapply groupby conditions and perform the plots efficiently - and would love to learn more about how the .apply() method works .
As far as the ` apply ` method goes , it does slightly different things for different objects . For example , ` DataFrame.apply() ` will apply the passed in callable across the columns by default , but you can pass ` axis=1 ` to apply it along the rows .
You can reproduce it with dupes in the findall ( see my answer ) , I think Jeff's apply solution should work fine with dupes in the DatetimeIndex .
Thanks @USER - your answer did help remove my confusion around the duplicate index , as I know that a pandas index will take duplicates in the index , so I now understand it's the index within the .apply() that was kicking out the error . Your explanation of how apply works as well is really useful , so even though it's not a solution to the original question it's really appreciated .

First convert your ` q_date ` column into a ` datetime64 [ ns ]` Series , then ` map ` over the column with a custom format string #CODE

Using pandas , is it possible to compute a single cross-tabulation ( or pivot table ) containing values calculated from two different functions ? #CODE
BTW , is it possible to make " len " and " mean " columns of their own when reshaping with melt ? I tried : pd.melt ( table.reset_index() , id_vars =[ ' c1 ' , ' c2 ']) , but I get a column named " NaN " with " len " and " mean " as elements . Thanks !
What do you exactly mean with " columns of their own " ? Do you want one column with " mean " and one column with " len " besided the columns " c1 " , " c2 " , " c3 " , " c4 " , as you would get with ` df.groupby ([ " c1 " , " c2 " , " c3 " , " c4 "]) .aggregate ([ len , np.mean ]) .reset_index() ` ? In that case you can do it directly instead of via crosstab and melt .

There are many ways to drop columns :

stack columns in pandas DataFrame CSV
I have a dataframe that is the result of a pivot table that has columns : #CODE

Python Pandas - Merge DataFrame , preserve index
What I'm trying to do is basically to merge two ` DataFrame ` objects in Pandas while preserving the index ( ` DateTimeIndex `) . Neither the ` merge ` or ` ordered_merge ` support this .
Any sort of ` merge ` operation - so far as I can tell , simply doesn't support preserving indexes on many-to-many data
Finding indexes unique to ` aapl_new ` and ` append ` ing those to ` aapl_old ` - no idea how to find which rows in ` aapl_new ` don't appear in ` aapl_old `
Can you explain what you mean by " merge " ? Do you simply want to concatenate the DataFrames ?
By " merge " I do mean to [ merge ] ( #URL ) like in the documentation . The problem was when I would do a merge operation , pandas would destroy the index , meaning that I lost the DateTime the data came from . Does this clarify ?
First , to merge the two dataframes you can simply concatenate the two using ` concat ([ aapl_old , aapl_new ])` ( this will preserve the index ) . But to remove the duplicates you can think of two strategies :
1 ) You can only concat those from ` aapl_new ` that are unique ( do not appear in aapl_old ) . To select those , you can do : #CODE
And this you can then concat with aapl_old : #CODE
2 ) You could also just concat the dataframes , and then remove the duplicate rows : #CODE

Pandas truncate DataFrame after a column condition is met

If you try to return multiple values from the function that is passed to ` apply ` , and the DataFrame you call the ` apply ` on has the same number of item along the axis ( in this case columns ) as the number of values you returned , Pandas will create a DataFrame from the return values with the same labels as the original DataFrame . You can see this if you just do : #CODE

It's easy to pivot to get this without the empty line of ` b 0 0 0 0 ` ; is that important ?
To fix the index , you could just add the X as this index , you could first apply set_index : #CODE
If you wanted to do this pivot ( you can also use pivot_table ): #CODE
Yeah sorry about not being clear - the pivot tables where all I was looking for ... forgot about those . However after testing out ` get_dummies ` this works out better for what I need to work with . Thank you !

Pandas : Replace a set of rows by their sum
Now suppose I want to take rows A and B , and replace them both by a single row that is their sum ; and , moreover , that I want to assign a given index ( say ' sum ') to that replacement row ( note the order of indices doesn't matter ) .
Another option is to use concat : #CODE
and we want to concat with all the remaining rows : #CODE

If you want to preserve some of the original timestamps ( but you have to choose which if you are binning them ) , you can specify a function to apply on the individual columns . For example take the first : #CODE
If you just want to resample on 5 mins and add the counts regardless of the keys , you can simply do : #CODE
I've read that the time series resample can work with repeated time stamps - I'd really just like to aggregate so that the counts ( data1 ) are added when creating a 5 min time bin . In this example there were just 2 time stamps ( same value ) . A larger data set would have 1000's or more time stamps for keys , and a cound for each . Would like to sum the counts for unique keys when binned
This would allow for example , to create 2 min , 10 min bins using resample - and look at the data over different bin sizes ( time scales )

I want to retain the possibility of selecting the previous content of the matrix by a single column name after the merge .

I'm trying to do a periodic storing of financial data to a database for later querying . I am using Pandas for almost all of the data coding . I want to append a dataframe I have created into an HDF database . I read the csv into a dataframe and index it by timestamp . and the DataFrame looks like : #CODE

Alternatively , pass ` as_index=False ` to ` groupby ` and use ` loc ` in the ` lambda ` , which is a bit more meaningful IMHO since you're indexing by name rather than integer location : #CODE
` as_index ` and ` apply ` will only work on pandas git master . If you're not using master , then you'll get the following : #CODE

Not sure what the problem is , I am fairly new to python and I cannot currently find any solutions to this problem on stack overflow .

How to apply a complex formula using Pandas in Python ?
I need to apply a specially designed moving average filter on a traffic dataset ( NGSim ) . This process is very tedious in Excel not only because dataset is very large but also because the formula has to look into columns for getting some values and sum them up . I was wondering if there are any examples like this or any other complicated formulas used in Pandas , Python . Kindly provide any example resources .

In some cases you'll want to concat this to the DataFrame in place of the dict row : #CODE
Also needed to merge the statuses in , adding a suffix so name collisions got decent names .

The ` AttributeError : ' Series ' object has no attribute ' _data '` is caused because ` weather [ selector2 ] [ ' Dry_bulb_temperature ']` is a Series while concat expects a DataFrame which can not be concat-ed with a Dataframe , i.e. concat needs two similar types ( previous comment is wrong as pointed out by @USER below ) .
` concat ` expects objects the same * type* , not only ` DataFrame ` s . You can ` concat ` two ` Series ` , for example .
I'm not sure what is happening in your concat . It might be that you have a field name confused . I see both ' Dry_Bulb_Temperature ' and ' Dry_bulb_temperature ' in different parts of your question .
Assuming the two dataframes have the same index , I would concat the whole thing , then do your filter : #CODE
This concats correctly : ` df = pd.concat ([ room1 , weather ] , axis=1 )` . The problem is that ` weather [ ' Dry_bulb_temperature ]` is a Series object , while concat expects a dataframe ...
It looks like you want to do a join ( which can merge a DataFrame and a Series on their index ): #CODE
See more in the merging , join and concating section of the docs .
You can create the Series / column using loc , avoiding chaining : #CODE

You could use stack here and then use a boolean mask ( for those values over 50 ): #CODE

how do I insert a column at a specific column index in pandas ?
Can I insert a column at a specific column index in pandas ? #CODE
using idx = 0 will insert at the beginning #CODE

This errors . I'm pretty sure the ' df [: 15 ]' in the map is the problem as I don't know how to pass a slice of the current future records to the function
so that the values align ( with the start point , rather than the end point ) #CODE
Hi low diff is just #CODE
Thanks . I get an error when I try to do this : " ValueError : Freq was not given and was not set in the index " . Does the index matter ? I need to do this for my entire dataset rather than a subset i.e infinite periods . I've updated my example with the code I use to read the CSV
Still getting the same error when I try the read_csv above . ` df [ ' max_bidhigh '] = pd.rolling_max ( df [ ' bidhigh '] .tshift ( -14 ) , 15 )` gives an error of ` ValueError : Freq was not given and was not set in the index ` - I can't see how to set ' Freq ' in the documentation .
using ` shift ` rather than ` tshift ` seems to do what I need . ` df [ ' max_bidhigh '] = pd.rolling_max ( df [ ' bidhigh '] , 15 ) .shift ( -14 )` and
yep ... should have pointed u there in the first place . normally the rolling functions are ' backward ' looking in that the index is related the the future value ( and not the first value ) , that is why you need to shift

I'm not familiar with the concept of a design matrix . My plan is to create a script to take input conditions that will then assign a space-efficient physical layout of all needed samples in a 96 or 384 well plate ( 12x8 or 24x16 wells , respectively ) . Then do the experiment and append the DataFrame with a new column of results for each row . That will be used to output the data in concentration-response curves for each protein*drug pair ... all in an automated fashion .

I want to look at the data by hour , to see how tickets are flowing through the Help Desk by shift - so an intermediate step could be something like this : #CODE
Join it to the original dataset and set the index
Now , iterate through each of the rows and concat : #CODE

or using ` map ` : #CODE

pandas DataFrame : replace nan values with average of columns
How can I replace the ` nan ` s with averages of columns where they are ?
This question is very similar to this one : numpy array : replace nan values with average of columns but , unfortunately , the solution given there doesn't work for a pandas DataFrame .
Apply per-column the mean of that columns and fill #CODE

You'll also have to do a little hack here to allow us to use the eq DataFrame method , unfortunately this does a sort ( or you could keep track of the original unique index ) . #CODE
Select the columns you are interested in ( alternatively you could just drop the Service_Date column ): #CODE
Then you can use the eq DataFrame method : #CODE

pandas stack dataframe imported from excel
Ok , i'll try . The code you sent works fine for the dataset above . But when I tried to add more variables it was not able to drop thesuffixes anymore . I cant figure out why ! I made a new dataframe that explains my problem but it is to big to fit in the comments

how to replace values in a data frame that is already constructed ( pandas ) ?
Then , after reading the data frame ` df ` , I could tell pandas to insert the available elements in ` val ` in the appropriate rows of ` df_empty ` ( ` 0.277162 ` in row corresponding to ` a x ` , ` 0.324269 ` in row corresponding ` a y ` and ` 0.435033 ` in row corresponding to ` b y `) . This would give me the ` df_desired ` .
It seems that your data is really indexed by the ` c1 ` and ` c2 ` values . If so , you should make your data structure a Series with a MultiIndex and then use that to insert the values you need . #CODE
@USER : You need to provide more information about what you mean by " the values that the user provides " . How do * you * know which row you want to replace ? If the user provides an item in row ` x ` , you can use ` df.ix [ x , ' val ']` to insert a value at that position in the table .
@USER Are you looking to * append * rather than replace ?
@USER Hayden : How would you do it with append ? If , when " df " is read , it happens to contain " a x G2 0.277162 " , then this should replace " a x G2 0 " in the empty data frame I'd created . Would append have this effect ?
You could do a left merge between df_empty and df , then fillna with 0 : #CODE

As @USER Cloud points out , ` loc ` is better for this job . And if you specify the key as a tuple , you can multi-index set as well . #CODE

I have a dataframe of data that has a year column ( ' Year ' and a dollar value column . I want to group by the year , then for each row , determine if the row is above the group's median by 20% or below the group's median by 20% .
A numpy ` ndarray ` is not a valid argument to ` bool ` unless its ` size ` is 0 or 1 . This means that you cannot evaluate its " truthiness " in an if statement unless it has 0 or 1 elements . This is why you're getting the error you reported .
actually one question : the answer above is based on one condition ( is the row-value greater than the median ? ) . how would the answer be modified to check if its above 1.2 *median OR below .8 *median ? would it just be bools = x.dollar > = x.dollar.median() * 1.2 OR x.dollar <= x.dollar.media() * .8 ? Thanks
hmmm , apologies for being a novice at this , but I can't figure it out . I can get the ' or ' logic to work using numpy comparison functions , but ultimately the logic I need is not ' or ' ... I'm trying to apply a case-statement-like function element wise that works on groups : if element above 1.2 * group_median , then assign ' h ' , if it's below .8 *group_median , then assign ' l ' , the third logical possibility ( it's within .2 of the group median ) is ignored or assigned ' n ' . Any thoughts ?

thanks . I realized that I was using wrong bracket ` ix = numpy.random.choice ( 10 , size=5 , replace=False , p=None )` and ` df = train1.loc ( ix )` :)

I have a rather basic question for pandas , but I've tried merge and join to no success
Many DataFrame methods rely on the indexes to automatically align data . In your case , it seems convenient to set the index of these DataFrames to the product code . So you'd have this : #CODE
Now , ` join ` requires no extra parameters ; it gives you exactly what ( I think ) you want . #CODE

This does not work for me because it modifies the original DataFrame . A potential solution is to use join , but that does not allow me to name it nor does it allow it be filled with a scalar values : #CODE

How to offset / shift a Pandas Dataframe into another year ?
I would like to offset / shift the data into the previous year so that ` 2014-01-01 00:15 : 00 1269.6 ` will be converted into ` 2013-01-01 00:15 : 00 1269.6 ` .
to shift the dataframe 15 mins into the past but would not like to offset / shift by the number of 15min intervals as this might cause errors in leap years and with clock changes .
This works but I agree with you , it would be great if one could shift / offset it by a year . ` df.index = df.index-pd.offsets.YearBegin ( 1 )` or ` df.index = df.index-pd.offsets.YearEnd ( 1 )` deliver similar results like ` df = df.shift ( -1 , ' A ') `

Pandas Pivot Table Display in ReportLab

replace ` ( f1 , f2 , f3 , val )` with whatever you want to use to print the table . Or you can print it from the list of tuples .

I have a pandas.DataFrame that won't pivot the way I expect . While ` pivot_table ` properly arranges everything , the fact that it uses aggregate functions to get there is off-putting . In addition , ` pivot_table ` seems to return an unnecessarily complex object rather than a flat data frame .
Is there a way to get ` pivot ` to give essentially the same output as the ` pivot_table ` command did ( but hopefully flatter and neater ) ? Failing that , how do I flatten the output of ` pivot_table ` ? What I want as output is something more like this : #CODE
If you don't want the aggregation of ` pivot_table ` , you indeed need the ` pivot ` function . However , ` pivot ` does not work with providing multiple index-columns ( actually I don't know why ) . But , there is a similar function to pivot , ` unstack ` , which works the same but based on the ( multi ) index instead of columns .
and then unstack on the last level ( default ) , so on ' vehicle ' ( which become the column labels ): #CODE
Yeah , that's why you need to first drop the level , and only then reset the index . Because otherwise you end up with the needed names at different levels .

Pandas stack / groupby to make a new dataframe

I've been experimenting with Panels and pivot tables , but they don't seem to be quite right , and they often overflow the memory .
So , I've tried this with my table , and it quickly blows up with a MemoryError on the actual pivot . I'm only doing a subset of the table ( loaded into memory ) which is only 1.5M triples with values ( full table is 18M triples ) . I'm actually replicating an algorithm that someone else has used in another environment to do this , so I'm wondering if there's an efficient shortcut built-in or I just have to find a way to do it iteratively .
I may have found a way to do this pivot table with much fewer rows in the original table . I'll post another issue / question if that doesn't work .

You can do this manually using loc and mean : #CODE
By default this looks at 3 periods back , so we have to shift it two up : #CODE
and transpose to get of in the correct form : #CODE
what about the resample ( q ) function ...
You can join these columns to the original dataframe with #CODE
Using resample . #CODE
This is ultimately the most flexible as you can now resample by different frequencies . #CODE

good eval test case

Getting percentage of another Column in Pandas Pivot Table
To expand a little on what's going on here , during the apply the function is called on each group ( in this case there are two , one for B= ' c ' and one for B= ' d ') , here's the c group : #CODE
Apply then outputs this together with the B= ' d ' group to get the desired result .

You could groupby person and then apply a shift to the values : #CODE
shift() is shorthand for shift ( 1 ) , to go back two periods use shift ( 2 ) :)

Is there a way to replace these values with boolean values ?

` location ` : an experimental condition , can be `" same "` , `" diff "` or ` None `
Full stack trace #CODE

You can see the previous vehicle class using shift : #CODE

In order to replace the value of END_DATE for those rows that have a S or P in STATUS

Or even simpler , try : ` print len ( ' A\B\C ')` . Compare to ` print len ( ' C :\ b\a ')`

You can apply ` value_counts ` to the Series groupby : #CODE

this will be quite inefficient . Better to construct the frame column by column then transpose at the end .
If you can do it with list ( memory wise ) it'll be efficient . Another option , if memory was an issue , is to do it chunkbychunk ( i.e. read it in several pieces and concat the result ) .
for i in xrange ( len ( df )):

It seems cool , and an approach I had not concieved of . They are all time stamps , I'm just slow on the uptake ,, need to think about it , I'm sure you are right I just need to catch up ! And apply the code ,
You could reset_index . Also , perhaps use a df.index.map , rather than apply . ?

As you can see from the example , I don't think the ` replace ` or ` where ` methods would work as the values of replacement are index location dependent and not input value dependent . Because I want to do this more than once I've put it in a function and I do have a solution that works as shown below : #CODE
A faster way to copy a column's non-NaN values to another column is to use loc and boolean mask : #CODE
` where ` seems to be faster than ` loc ` : ` df.outts.where ( - pd.notnull ( df.orts ) , df.orts , inplace=True )`
@USER and even faster with isnull rather than -notnull , awesome !

And if you wan't a list of the prices per day then just do a groupby per day and return the list of all the prices from every group using the ` apply ` on the grouped object : #CODE
@USER ` max ` is doing completely different thing - it returns the maximal value . You need the ` argmax ` , which returns the position of the maximal value in the index . What version of pandas are you using ? Also try ` ix ` instead of ` iloc ` .

You can do it with map : #CODE

To compare today's values to yesterday's last value , take the last column and use shift to look " up " one row . #CODE

Pandas Combining 2 Data Frames ( join on a common column )
I would like to join these two DataFrames to make them into a single dataframe using the DataFrame.join() command in pandas .
I am very new to pandas and have no clue what I am doing wrong as far as executing the join statement is concerned .
Or , you could get around this by simply deleting the offending columns before you join . If , for example , the stars in ` restaurant_ids_dataframe ` are redundant to the stars in ` restaurant_review_frame ` , you could ` del restaurant_ids_dataframe [ ' stars ']` .
it also says ' business_id ' column overlaps , isn't it supposed to overlap since that's the column I'm creating the join on ? How do I address that issue ?
Hey @USER I tried the join method but all I get is 4503 entries in the restaurant_ids_dataframe and zero entries in the columns belonging to the restaurant_review_frame . Could you please let me know why this is happening ? I have performed a left join as you suggested using your above statement but it doesn't seem to give me any items from the restaurant_review_frame for some reason . What I am looking for is to create a dataframe with all the columns from both the dataframes , joined on business_id . I also deleted the extra columns other than business_id .
You can use merge to combine two dataframes into one : #CODE
where on specifies field name that exists in both dataframes to join on , and how
defines whether its inner / outer / left / right join , with outer using ' union of keys from both frames ( SQL : full outer join ) . ' Since you have ' star ' column in both dataframes , this by default will create two columns star_x and star_y in the combined dataframe . As @USER mentioned for the join method , you can modify the suffixes for merge by passing it as a kwarg . Default is ` suffixes =( ' _x ' , ' _y ')` . if you wanted to do something like ` star_restaurant_id ` and ` star_restaurant_review ` , you can do : #CODE
Your advice solved my problem . The only change I had to make was I did an inner merge instead of outer . i.e. how= ' inner ' instead of outer . Thanks for your help .
how=inner|outer|left|right , how to merge , intersection of keys left and right|union ( ALL ) keys left and right|left keys only|right keys only|
In case anyone needs to try and merge two dataframes together on the index ( instead of another column ) , this also works !
P.S. I had to use merge because append would fill NaNs in unnecessarily .

Trying to append this to a new datastore . The datastore does not exist so I use the following to create and append the data ; #CODE

II . Second , I would like to ' melt ' the data into some new dataframe that counts the number of events for a given PictureID in a given period . I'll use examples with just two periods . #CODE
So that I can then stack ( ? ) this new data frame into something that provides period counts for all unique PictureIDs : #CODE

After this you can merge this with your DataFrame on index .

Note : A solution which only calculated the mean once might be preferable ... one option is to stack , groupby , mean , and unstack , but atm this is a little fiddly .

In R , it is easy to aggregate values and apply a function ( in this case , ` sum `) #CODE
You can use groupby , which can apply a function to the index values ( in this case looking at the first element ): #CODE

I want to group by the columns : date , textA and textB - but want to apply " sum " to numberA , but " min " to numberB . #CODE
... but I cannot see how to then apply two different aggregate functions , to two different columns ?

Merge a list of DataFrame's on a column ?
The problem is that , when you did the first merge , you changed the names of the columns ( adding suffixes ) and there won't be a name collision on the second merge , so the suffixes in the second merge will never be used . The solution is to rename the columns manually after the merge . #CODE
@USER Oh I think I understand now , you want to chain the merge operations . I updated the answer .
I have created a Gist containing a function to join a " list " of dataframes . The list is actually a dictionary that contains whose keys are the suffixes that are used in case of a column name collision :
Join a list ( dict ) of pandas dataframes

You can apply a ` join ` operation between your original dataframe and the resulting aggregated data : #CODE

Now that we have the grouping , we can apply the aggregator : #CODE

Pandas : Drop all records of duplicate indices
I have a dataset with potentially duplicate records of the identifier ` appkey ` . The duplicated records should ideally not exist and therefore I take them to be data collection mistakes . I need to drop all instances of an ` appkey ` which occurs more than once .

I know I can brute force this approach and Iterate over the rows . Or I can transpose and convert to a dictionary and Parse that Way . I can also create Sparse Series objects and then create sets but then have to reference the column names separately .

If you want to select the 0 column in this way you should use loc : #CODE

To fix your solution you can append every iteration to a list : #CODE
I'm not familiar with sqlalchemy / django queries . I'm guessing they are a way to map a db onto objects . One reason we chose to use strings is because ` numexpr ` takes strings . The parser is ` ast.parse ` with a few bells and whistles ( so , e.g , syntax errors are propagated ) . We also wanted to allow for future engines in the backend , e.g. , ` numba ` . Not sure I follow why SQL injection is an issue in this case . I suppose a more general sort of " injection " attack is possible , but the parser takes care to disallow anything except expressions ( no functions ) . Would love help in trying to break things :)

If you want to override the format as desired by the OP , you can replace ( " monkey patch ") the ` IntArrayFormatter ` to print integer values with thousands separated by comma as follows : #CODE

The dataframe returns unsorted after I reindex . My ultimate goal is to have a sorted dataframe with index numbers from 1 --> len ( dfm ) so if there's a better way to do that , I wouldn't mind ,
Remember , if you want ` len ( df )` to be in the index you have to add 1 to the endpoint since Python doesn't include endpoints when constructing ranges .

And you can do an apply over the major axis : #CODE
Hmm , it doesn't look like this lets me store it the way I want , but it does let me access the data in a way which may let me do what I need . I clearly had not looked close enough at Panels . I will have to look into how apply more carefully , I don't understand why you would have the function return 1 . Thanks though !
Hmm , well trying it out doesn't quite seem to do what I want . The return value needs to be of length of the panel , where if I want to operate on the whole array to compute a single object it won't let me return that . Apply doesn't let me reduce the panel . For example as a test I wanted to apply ` np.reshape ( x , ( 2 , 2 ))` on a panel of 4 dfs and it fails . I guess I need to do all my work within the apply , and flatten it back out . This has me on the right track now I think so thanks again .

Use ` loc ` ( and avoid chaining ): #CODE

Therefore , what you want to do is add ` NaN `' s for the weekend dates . ` pandas ` offers a few different ways to do this . There's an overview here , but basically you want to look at the ` asfreq ` and ` resample ` methods .

` fromtimestamp ` localizes the time if no tz is provided , you can use ` utcfromtimestamp ` to get what pandas does ( which does not localize here ) #CODE
If you want to localize the tz , you can do this #CODE
and what tz are these epoch seconds coming from ?
The TIMESTAMP is queried from database and returned as JSON data . No tz is provided .

@USER What is " H7 ? " What do you get when you type " type ( H7 )" I think the problem is that it's generating a new axes . If you replace " do some plot " with a plt.plot ( data ) command , it will plot to the current axes . ( plt.subplot sets the current axes to the subplot returned ) .
This will work even if ` axs.size len ( subsl )` since ` StopIteration ` is raised when the shortest iterable runs out . Note that ` axs.flat ` is an iterator over the row-order flattened ` axs ` array .
More generally , for ` axs.size - len ( subsl )` extra plots at the end of the grid do : #CODE
The array ` axs ` has ` axs.size ` elements . The index of the last element of the flattened version of ` axs ` is ` axs.size - 1 ` . ` subsl ` has ` len ( subsl )` elements and the same reasoning applies about the index of the last element . But , we need to move back from the last element of ` axs ` to the last plotted element so we need to step by -1 .

I am new to python and I am working on pandas . I have a GW2test.csv file containing date , time and other columns with data collected every 30 min . I need to resample the data for daily averages .

I wanted to merge these files so that i have something like this #CODE
If there are two Bact5 rows in file1 , three Bact5 rows in file2 , how many Bact5 rows do you want in the output ? If it's six , then you can use join method by @USER Hayden .
Then you can simply ` join ` them : #CODE
@USER when you do a join with 2x2 duplicates you get 4 in the joined DataFrame . It's unclear how pandas should join in this case , so you need to be more explicit to it ( and tell it what do you want ) .
On the similar note , is there a way to merge values based on index . For example , instead of listing Bact5 in two rows , can we merge its value corresponding to file2 in one row separated by a delimeter ?

Creating a weighted score column from Pandas pivot table
I have the following pivot table in pandas : #CODE

You can also adapt this to append rows to a DataFrame .
Agreed , it does materialize the data as a dict . However , you don't have to materialize _all_ of it at once ; just consume part of the generator , then append the data to a DataFrame in chunks . Just use itertools.islice to get the chunks from the generator / row_iterator .

If you replace your zero values with ` NaN ` , ` cut() ` and ` qcut() ` behave as expected ; those rows will have a bin value ( from ` Categorical.labels `) of ` -1 ` : #CODE

Pandas dataframe merge
I want to consolidate this into a dataframe like this : #CODE
How can I merge them based on all.index ?
Do a ` groupby ` on the index ( I presume from the data you posted that the values ` 102P ... ` are in the index ) . And count the values . That will return a DataFrame with zeros and ones . Just replace them with appropriate values . #CODE
Ok , I will give outer join a try , thanks .
@USER Updated the answer , added example . I'm sorry for misleading you , I was not clear enough when I said merge :( I was not thinking about the ` merge ` function but a merge as a general term ...

First creating two diff data frames #CODE

create a new key that is the combination of the fields you want to pivot . #CODE

I am sure the place to start is df.groupby ( ' time ') but then I can't seem to figure out the right way to use concat ( or other function ) to build the split data frame that I want . There is probably some simple function I am overlooking in the API .

So the issue was that argmax found the 0th item if they were all False so I just special cased that ( I also used values so I'd get the position rather than the label ) .

python pandas : why map is faster ?
can anyone here explain a bit why the map approach is faster ? Is this a python feature or this is a pandas feature ?
I bet you ` from operator import methodcaller\\df2 [ ' a '] .map ( methodcaller ( " startswith " , " t "))` will be significantly faster yet .
@USER ; it's not using the built-in ` map ` ( which would be slower in this case ) .
@USER : I see ; I was just wondering where the second parameter to ` map ` was :)
Arguments about why a certain way of doing things in Python " should be " faster can't be taken too seriously , because you're often measuring implementation details which may behave differently in certain situations . As a result , when people guess what should be faster , they're often ( usually ? ) wrong . For example , I find that ` map ` can actually be slower . Using this setup code : #CODE
And now it seems like the ` map ` is winning when used as an index , although the difference is marginal . But not so fast : what if we manually turn the listcomp into an ` array ` or a ` Series ` ? #CODE
This is probably also the section of the docs where Wes states that [ startswith is slower than slicing ] ( #URL ) !

Firstly I tried filling on a per-row basis . Trying to shift back one row to get the previous value if the current ' Date ' is empty : #CODE
So then I tried applying with axis=0 ( per column basis ) and modifying the function so it only applies this to the ' Date ' column ( I can't see how to apply this to just one column ) #CODE
That doesn't work because the empty ' Date ' values are not ' NaN ' they are empty strings . Also , ideally , I want to do this using ` apply ` as there are other things I want to do within that function which I removed to simplify this question .
then show what you actually want ; this is the efficient way of doing it , apply is essentially a loop in python space

Are you trying to shift ends by one ( month ) ?
My initial suggestion was to do the shift after you've reindexed ( since you're about to do that anyway ): #CODE
the shift index looks like a better fix , still would like to know if there is a simple date add function , which is how I'd do it in sql , that could apply ?
I see that , and I like it .. I'd still like to know if there is a simple DateAdd type function that I could use that might also apply for use elsewhere if needed ?

Each list in this column can be passed to ` set.update ` function to get unique values . Use ` apply ` to do so : #CODE

Is it possible to merge it like following instead : #CODE
And then concat these into a DataFrame : #CODE
thank you , this helped me to merge my tables correctly . My data contains strings and I dont intend to do any more calculation on it . I might parse it further if the need be . thanks again .

My question is , how can I merge the columns ` Year ` , ` Month ` , ` Day ` into column ` NewDate `
If it's already in your DataFrame , you could use an apply : #CODE

I would like to add additional calculated columns to each DataFrame that is inside the Panel , preferably without a for-loop . I'm attempting to use the apply function to the panel and name the new columns based on the original column name appended with a ' p ' ( for easier indexing later ) . Below is the code I am currently using . #CODE
The code above currently duplicates each DataFrame with the calculated columns instead of appending them to each DataFrame . The apply function I'm using operates on a Series object , which in this case would be a passed column . My question is how can I use the apply function on a Panel such that it calculates new columns and appends them to each DataFrame ?
If you want to add a new column via ` apply ` simply assign the output of the apply operation to the column you desire : #CODE
Or , in this case , simply `` apply ( newCalculation )`` .
Neither of these worked for me . The first doesn't work because it actually creates a new DataFrame inside the panel and it doesn't append the column to each DataFrame . '
There are some day by day calculations we need to perform , which is why it will be cleaner to not append all csv's into the same DataFrame .
Try using a double transpose : #CODE

After removing groups without any ratings , I want to be able to fill in the NaN ratings with the mean rating for each group . However , I need to have at least one rating to be able to do that . So I want something that will drop the ` lob ` group , but keep every record of both the ` mol ` and ` thg ` group .

I am using the pandas library for creating pivot tables in csv files .
This is a portion of my original list.csv file i'm trying to convert to a pivot table . #CODE
right now my only problem is the fact that the values field cant take more than one value and wondering if anyone has any idea on how to use stack or reshape functions .
It looks like you can stack / reshape into the correct form . Please copy and paste the actual text rather than an image ... this doesn't seem to match the original arguments : S
I think you're looking to do some kind of stack / unstack afterwards ( most likely ` .stack ( 0 )`)
Yeah that was the problem ! I got the pivot table right , but the rows were not in the format i wanted them to be . Stack was the solution . Thanks Andy :) !

This link explains that pandas has align functions and reindex functions to accomplish the above , but how to accomplish addition with automatic re-dimensioning and alignment , favoring the maximum dimensions ?
` pandas ` overrides arithmetic operators to do exactly what you want . Currently , there's no option to control the index joining . They do an outer join by default : #CODE
both the matrices have different label ordering , will it work , does it automatically align the labels following the #URL right , inner join for overlaps and conflicts ?
I figured out the answer without resorting to pandas but still interested to know how the same can be accomplished in pandas , The solution is to use the outer Join : #CODE

Do a inner join on ` member ` and ` time ` columns : #CODE

But I feel pandas has this functionality somewhere in ` resample ` , ` reindex ` etc . but I can't quite get it .
You can concatenate the two time series and sort by index . Since the values in the second series are ` NaN ` you can ` interpolate ` and the just select out the values that represent the points from the second series : #CODE
You need to use method = ' values ' for the key arguments in interpolate to get the same answer as in numpy pd.concat ([ data , ts ]) .sort_index() .interpolate ( method = ' values ') [ ts.index ]
Assume you would like to evaluate a time series ts on a different datetime_index . This index and the index of ts may overlap . I recommend to use the following groupby trick . This essentially gets rid of dubious double stamps . I then forward interpolate but feel free to apply more fancy methods #CODE

And several more , because I want to find out how many data points are there per user on average / median / etc ..

Merge Two DataFrames With Hierarchical Columns
I would like to merge two DataFrames while creating a multilevel column naming scheme denoting which dataframe the rows came from . For example : #CODE

are you looking to transpose a matrix ?

My data look like this . The " real " Actor and Recipient data are always 5-digit numbers , and the " Behavior " data are always letter codes . My problem is that I also use this format for special lines , denoted by markers like " date " or " s " in the Actor column . These markers indicate that the " Behavior " column holds this special type of data , and not actual Behavior data . So , I want to replace the markers in the Actor column with NaN values , and grab the special data from the behavior column to put in another column ( in this example , the empty Activity column ) . #CODE

I have a pandas dataframe representing several different timeseries of data from regions , subjects and using different measures . The pandas pivot-table allows me to pivot easily to a particular subset of the data and plot it . However , I can't for the life of me figure out how to then add error bars to the plot . Since the act of pivoting takes the mean or the values for the specified parts of the table , I wrote a little labmda function to make a second table which is perfectly aligned with the first containing the standard error . However I can't make the plot update by adding error bars with these values . I believe I could work around by extracting data from the table into vectors , but this defeats the usefulness of the data frame .
which is the correct value of the standard errors . but i can't find a way to get the plot to update with the errorbars . as far as i can tell , i could extract the vectors and then plot them using plt.errorbar , but i feel like there should be an easy way to tell the pandas dataframe that these are associated errors and i want them on the plot . any help is greatly appreciated . ( please excuse the length of this post ! i wanted to explain throughly and i'm a total noob on this forum . also , stack overflow would not allow me to tag this with errorbars , error bars , or bars )

Pandas Dataframe , Apply Function , Return Index
Then I can apply the function to my dataframe , grouped by I D: #CODE

Now , ` loc ` throws an error because there is no index ` None ` : #CODE

I'm a newbie to Pandas and I'm trying to apply it to a script that I have already written .

Don't use eval , use ` ast.literal_eval ` ! ( which should work , since I think it's the repr which has been saved ) .

How to use crosstab / pivot with multi dimensions
I tried using pivot tables to have more than one values in the ' values ' field for the pivot_table function but it doesnt work , so im trying to see if i can do it with crosstabs .
In python , after converting it to a pivot table , the dtype is float64() and using #CODE
And i want to group them by date , after they are transformed into pivot table

Apply ` tolist ` on each of the group's column B : #CODE
I found one solution but it's not appending unique values . Please suggest me a way to append unique values df =d ataframe [[ ' A ' , ' B ']]
for i in range ( len ( df [ ' A '])) :

@USER I really needed the answer to this question but couldn't find it for ages on Google . Can we edit the question to get rid of the sparse stuff which isn't really relevant and change " override " in the title to " replace " or " update " ?

possible duplicate of [ How to remove relative shift in matplotlib axis ] ( #URL )
This is essentially the same problem as How to remove relative shift in matplotlib axis

Looking for a fast way to get a row in a pandas dataframe into a ordered dict with out using list . List are fine but with large data sets will take to long . I am using fiona GIS reader and the rows are ordereddicts with the schema giving the data type . I use pandas to join data . I many cases the rows will have different types so I was thinking turning into a numpy array with type string might do the trick .
Unfortunately you can't just do an apply ( since it fits it back to a DataFrame ): #CODE

What I would like to do is combine these into one data frame and then interpolate the missing values such that I have a complete dataframe .

Compare 2 columns of 2 different pandas dataframes , if the same insert 1 into the other in Python
So what I want to do is search df2 for all instances of a match from both columns of df1 ( noting the different data formats ) and insert the data from df2 into df1 . Like this ( df1 ) #CODE
You can use a plain ol ' merge to do this . But first , you should do a little cleanup of you DataFrames , to make sure your datetime columns are actually datetimes rather than strings ( Note : it may be better to do this when reading as csv or whatever ): #CODE
Now a simple merge will give you what you're after : #CODE

@USER what happens if you remove the space from ` final track ` so it becomes ` final_track ` ? It is also possible to override the column names with your own ` names= full_list_col_names ` and also pass ` usecols=cols_of_interest ` but this seems overkill to me . Could you try renaming the cols to replace the spaces with underscores

How to use pandas to group pivot table results by week ?
Below is a snippet of my pivot table output in .csv format after using pandas pivot_table function : #CODE
The tool you need is ` resample ` , which implicitly uses groupby over a time period / frequency and applies a function like mean or sum .
Resample the columns ( ` axis=1 `) weekly ( `' w '`) , summing by week . ( ` how= ' sum '` or ` how= np.sum ` are both valid options here . ) #CODE
you can pass `` axis=1 `` to resample , to avoid the double transposing
Thanks ! however , what if i want data that is week beginning instead of week ending ? For example , the date column remains as it is but all the data below the 2012-11-11 will shift to the left to be under 2012-11-04 ?

I have a dataset created with pytables that I am trying to import into a pandas dataframe . I can't apply a ` where ` filter to the ` read_hdf ` step . I'm on pandas ' 0.12.0 '

` melt ` it first in R , and then load it ...?

missing first row after resample

I have a pandas dataframe and would like to drop all columns save the index and a column named ' bob '

Pandas dataframe : drop columns whose name contains a specific string
I want to drop all the columns whose name contains the word " Test " . The numbers of such columns is not static but depends on a previous function .
And the op did not specify that a number had to follow ' Test ' : _I want to drop all the columns whose name contains the word " Test " _ .

I'm having trouble with Pandas ' groupby functionality . I've read the documentation , but I can't see to figure out how to apply aggregate functions to multiple columns and have custom names for those columns .
This will drop the outermost level from the hierarchical column index : #CODE

but since they have matching dimensions you need to transpose one of the DataFrames #CODE

Pandas Merge Error : MemoryError
I'm trying to two relatively small datasets together , but the merge raises a ` MemoryError ` . I have two datasets of aggregates of country trade data , that I'm trying to merge on the keys year and country , so the data needs to be particularity placed . This unfortunately makes the use of ` concat ` and its performance benefits impossible as seen in the answer to this question : MemoryError on large merges with pandas in Python .
The attempted merge : #CODE
You seem to have duplicates in your ` df ` what happens when you drop the duplicates and then merge ? ` df.drop_duplicates ( inplace=True )`
They are not actually duplicates . df actually contains 93 columns , and each observation is unique to the year and trading partner . I only wanted to put a small subset of the data on SO to avoid confusion . Thanks for the idea tough ! Also , the merge doesnt seem to be form lacking memory . When I do the merge it I dont utilize over 50% of my memory .
No worries , another thing to check which caught me out is if you have any NaN ( null ) values in any columns you are merging with , up to you what to do but I would drop these also if you have any

To understand why your approach didn't work , remember that the function ` round ` needs two arguments , the number of decimal places and the data to be rounded . In general , to apply functions that take two arguments , you can " curry " the function like so : #CODE
For a modestly sized ` DataFrame ` , ` applymap ` will be horrendously slow , since it is applying a Python function element by element in Python ( i.e. , there's no Cython speeding this up ) . It's faster to use ` apply ` with ` functools.partial ` : #CODE
You could even make a function that returns a partial function that you can apply : #CODE
Why not use ` numpy.round ` and pass the ` DataFrame ` as an argument ? ` 100 * np.round ( df , 2 )` seems to solve the problem for me . If some columns have type inappropriate for ` round ` , just exclude them before passing to the ` round ` function . This should avoid overhead from ` apply ` -like things .

` map ` over the elements : #CODE

Merge identical index in pandas dataseries
As you can see there is multiples of the same dates . I would like to merge these and then sum the values so it turns out to be : #CODE
Assuming your index are proper dates , you can resample to a daily interval . This intoduces NaN values at missing dates which need to be dropped afterwards : #CODE

I am busy writing code to translate this , but my guess is that it is such a simple use that there must be method to do this . Cant seem to find it in documentation . Any pointers to the method that would simplify this ?

I'm referring to the key / value nature of your question . Where you have `` : `` , the question addresses `` = `` , but of course it's the same problem . Your data could be read in as a " CSV " file with one item per line ( so , no commas ) . Assuming you have multiple entries like this and you want to put them into a table format , you should look at `` stack `` and `` unstack `` in the documentation .

I tried a pivot table but ended up with an ' Index contains duplicate entries error ' .

You can resample passing day as frequency , without specifying a ` fill_method ` parameter missing values will be ` NaN ` filled as you desired #CODE
This constructs a second time series and then we just append and call ` asfreq ( ' D ')` as before .

is it safe to assume ` pd == pandas ` and ` DT == datetime ` ?

This link is useful , although I cannot figure out how to apply it to my situation .

However that's all I want , count and mean . I want to drop std , min , max , etc ... So far I've only read how to modify column size .

I use Pandas in IPython Notebook rather than IPython as a terminal shell , I don't see any options in ` set_option ` that supports the colouring , it maybe something that could be done as a plugin to apply some css or output formatting . This is the only way I think you could achieve this

Pandas Merge - How to avoid duplicating columns
I am attempting a merge between two data frames . Each data frame has two index levels ( date , cusip ) . In the columns , some columns match between the two ( currency , adj date ) for example .
What is the best way to merge these by index , but to not take two copies of currency and adj date .
Why not just select the columns you want to merge on like this : ` dfNew = merge ( df , df2 [[ ' data_col_2 ']] , left_index=True , right_index=True , how= ' outer ')` this avoids the duplicate columns and clash
@USER in that case there is nothing you can do other than define some method that drops all the columns that end in ` _y ` and rename the ` _x ` columns if you do not know upfront , after performing the merge , at least to the best of my knowledge
@USER actually thinking about it you can just take the complement of the columns that don't overlap e.g. ` cols_to_use = df2.columns-df.columns ` and then perform a merge using this list of columns that are in df2 and not in df ` dfNew = merge ( df , df2 [ cols_to_use.tolist() ] , left_index=True , right_index=True , how= ' outer ')` if htis works I will post as answer
You can work out the columns that are only in one dataframe and use this to select a subset of columns in the merge #CODE
then perform the merge using this ( note this is an index object but it has a handy ` tolist() ` method ) #CODE
This will avoid any columns clashing in the merge

Note that to use ` loc ` you must be working with Pandas 0.11 or newer . For older versions , you may be able to use ` .ix ` to prevent the chained assignment .
@USER suggests that this makes it " incorrect " whereas my view is that this is the obvious and expected behavior . In the special case when you have a mixed data type column and there is some type promotion / demotion going on that would prevent Pandas from writing a value into the column , then you might have a correctness issue with this . But given that ` loc ` is not available until Pandas 0.11 , I think it's still fair to point out how to do it with chained assignment , rather than pretending like ` loc ` is the only thing that could possibly ever be the correct choice .
Thank you for the insightful comment . In general , this is one of my biggest gripes with Pandas . Using alternate functions with assignment syntax might be suboptimal design . At least with ` [ ]` , there's no confusion . That operation is purely for getting and setting . I dislike it that an entirely additional function , ` loc ` ( or even ` ix ` honestly ) , subsumes that functionality . It hides the fact that it's a function entirely predicated on a side-effect , whereas in most of the rest of Python , ` __getitem__ ` and ` __setitem__ ` are the standards for absorbing that side-effect impurity .
The problem with the way Pandas chooses to address this is that it mixes up the ` [ ]` notation ( which has come to mean * to access * ) with function call notation ` ( )` . The obfuscation saved by not needing to reason step-wise about each iterative get or set is replaced by obfuscation that ` loc ` is not itself a data object but rather a function that pretends not to behave like a callable and acts like it is gettable or settable .
Lastly , either way does not matter to me . If there's more definitive examples for mixed datatype columns , I'm happy to endorse ` loc ` as the preferred method . But , for example , I work in a production environment where we are still using Pandas 0.8 . And we will be using it for at least another year because of the rules about how we can modify our production system . Therefore , regardless of what is the official , newest-release-endorsed , ' correct ' method , it's still useful to point out other ways to solve something . Using ` loc ` is not even possible for me except on our research system .

looking around I saw the pivot table function - ( df is the dataframe holding the original data ) #CODE

Would it be possible to map something like the following lambda function to mimic a SQL coalesce function ? #CODE
What do you expect the ` y ` argument to be ? The whole point of ` map ` is that it passes each value , one by one , to your function .
Sounds like you're looking for the ` DataFrame.apply() ` method . The ` apply ` method is a very general way to apply a function across either the columns or rows of a ` DataFrame ` : #CODE
By default it applies a function to the columns , but by passing ` axis=1 ` you can apply a function to each row : #CODE

Join with MultiIndex DataFrames Creates Empty Fields [ pandas ]
I'm trying to join two DataFrames in pandas on two fields , ' date_key ' and ' user_uuid ' , but when I do I only get an empty set , despite there being overlap when I search the tables for matches .
I made sure to strip out the index of definedRIDs so that it looks like this example from the docs .
But when I exported the dataframes to csv files and searched them by hand , I had user_uuid and date_key combos in both files that matched . Any ideas on why I'm having this mismatch on the join ?
if not (( len ( self.left_on ) == self.right.index.nlevels )):
you're correct , once the keys were the same type , the merge option with no index works ! The join appears to require an index , though .
The correct solution was in fact to join the frame with no index ( on the left ) to the frame with the multindex on the right : #CODE

You can apply the swap_axes method after construction : #CODE

I think calling ` map ` is the fastest method as this will vectorize the operation without performing an expensive loop iteration . Personally I would create the DataFrame with the correct data in the first place rather than using integer values which then need to be converted into strings , several thousand rows is not a lot so it should be quick to perform the mapping prior to grouping
` map ` * does * iterate . You don't see it , but if you dig in you'll eventually get to a ` list ` comprehension or a ` for ` loop .
@USER wasn't aware of that I knew that ` apply ` did iterate but thought ` map ` didn't

Oh maybe pandas objects don't support broadcasting ? I've never used pandas . You could replace ` r*a [: , None ]` with ` np.outer ( a , r )` . That is , don't reshape ` a ` , leave it regular 1d , and then use the outer function instead of regular multiplication .

Is there a " pandas " way to identify which rows from a DataFrame are not present in a merge result ?
I have two large data chunks with 2 common columns tying them together . One " should " be a subset of the other , but when I perform a merge , the final product is smaller than the subset , so I want to look at the rows in the original ( as well as the ones in the subset ) to see why it's failing to merge properly . It's probably some error in an upstream script , but it's possible that my newbie pandas use is in error .
I've edited to show an example of how the data relate to one another . I have GT , score and rsid in chip_raw that are tied uniquely to a CHROM and POS ( these two columns together identify uniqueness ) . Then , I have vcf_gt and gq which come from a different file . I'd like to see what is in chip_raw that isn't in merged , and the same for seq_data / merged .
In some cases , simply inspecting the result of an ` outer ` merge for ` NaN ` will do it . #CODE
Using ` right ` or ` left ` can also help identify which side of the merge is creating the problem .
Great suggestion . I hadn't thought about using an outer join for this , but that is perfect .

pandas.core.groupby.GroupByError : len ( index ) ! = len ( labels )

Conform the results to 20ms , rolling 20 periods with no minimum #CODE

With the data formatted like this , it's trivial to calculate simple frequency distributions . For instance , if I want to determine the number of times each user tagged something ( in descending freq . order ) , it's as simple as : #CODE
The average daily annotations per user is just the daily total annotations divided by the number of users that day . As a result of the ` groupby ` operation , both of these Series are indexed by date , so they will align automatically . #CODE
To average annotations per month across users , it is most convenient to use ` resample ` , a nice feature for grouping by different time frequencies . #CODE
@USER just pass a function to the resample then OP can do exactly what he wants

ahh ... didn't see the N , yes for sure , its automatically text that way . Best to put thin in a DataFrame , replace that value with a number and you will be good to go .

Another option is to concat rather than merge : #CODE

Wouldn't it be best to strip out the whitespace in this example ?

There error is raised because when you use numpy's where it returns the integer locations of the rows , rather than the labels , this means you can't use drop ( as this uses labels ) .

Iterating simultaneously / List comprehension issue ( in UDF to obtain Merge Report in Pandas )
Stata users will know that when merging data a ` _merge ` variable is produced that indicates by ` _merge ` being 1 that the ` merge ` was successful for that observation , by 2 that the observation is in the master dataset only , or by 3 that the observation is in the using dataset only . I am trying to re-create this in pandas by writing my own function . I have the following workings : #CODE
The arguments are DataFrame1 , DataFrame2 , List of ' Keys ' ( i.e. columns to merge on ) and an optional argument HOW which is passed to the pd.merge argument how = HOW . Eventually the arguments will be extended to all of those within the pd.merge function .

How would I go about doing this ? I have attempted creating the data frame and using ` resample ( ' h ' , how= ' count ')` to split into hourly counts , and using ` groupby ` , but can't quite put the timestamps into buckets and create the lists of values for each stream per bucket .

You can just use ` apply ` and assign direct to the column like so ` df [ ' start_time '] = df [ ' start_time '] .apply ( lambda x : dt.datetime.fromtimestamp ( x ))` , this is better than a list comprehension
Use ` apply ` #CODE
Note that this is in GMT already . ` datetime.fromtimestamp ` does a conversion to local tz ) . If you want that . #CODE
To convert to the tz of Asia / Kolkata . You have to represent this as object . This is will work . #CODE

Pandas performance issue of dataframe column " rename " and " drop "
Why a simple dataframe column `" rename "` and `" drop "` operation costs that much percentage of time ( 8.9 % + 10.7 % ) ? Anyway , the `" merge "` operation only costs 43.7 % , and " rename " / " drop " looks not like a calculation-intensive operation . How to improve it ?

plt.legend ( loc = ' best ')

And then I append one result to another . What I'd like to do is use a single line of code to search for any data that includes " nt " OR " nv " OR " nf . " I've played around with some ways that I thought should work , including just sticking a pipe between terms , but all of these result in errors . I've checked the documentation , but I don't see this as an option . I get errors like this : #CODE

You can use loc : #CODE

Python using lambda to apply pd.DataFrame instead for nested loop is it possible ?
I'm trying to avoid nested loop in python here by using lambda apply to create a new column
Your apply doesn't work as by default it works column wise , plus you misunderstand what the lambda parameters actually represent so your lambda func does not map to the columns as you expected . If you wanted it to work row wise you need to do something like this ` df [ ' c '] = df.apply ( lambda row : row.A + row.B , axis=1 )` but @USER ' s answer will achieve what you want and is simpler
As @USER points out in the comment , the argument to the function in ` apply ` is a series , by default on axis ` 0 ` which are rows ( axis ` 1 ` means columns ): #CODE

Why not reshape ( pivot ) after you've read in the DataFrame ? #CODE

Now , using concat / append and slicing , you can re-add the last row under a new date with : #CODE

Notice the overlap between IDs 0 and 1 and 1 and 2 at the edges ( I dont want that , messes up my calculations ) . One possible way to get around this is to using groupby on IDs and then loop through that groupby and then apply a rolling_sum

Replace any string in columns with 1
I'm working with pandas . My goal is to convert several columns within a dataframe from containing either NaN or string data , into more or less a dummy variable ( 0's for NaN ; 1's for any string ) . I'd like to do this without using a complete list of strings and replacing them each , because there are typos and this would lead to errors . I've been able to replace all the NaN data with 0's using the fillna function , which works like a dream !
I am hoping for something similar that will replace all string data with 1's , but leave the 0's in place . I've searched stackoverflow and elsewhere , to little avail .
The data look roughly like this , where I only want this to apply to columns starting with T_ : #CODE
EDIT : Since you to replace all strings with the number ` 1 ` ( as per your comments below ) do : #CODE
Yet another way is to use ` filter ` and join the results together after replacement : #CODE
Right , so I understand how to use the replace() option , but as I said above I'm hoping to find some solution that will replace * any string* ... This is because the above data is only a small chunk , I actually have upwards of about 50 strings to replace with 1's . I suppose I could type them all in , but the list of strings may change as I edit my selection criteria ( for munging the data ) . Basically I am looking for the equivalent to " search & replace all strings with 1's "
This gets the job done , except that I need to apply it to only 6 of the many columns in my dataframe . I should have made it more clear above that there are additional columns containing strings that I don't need to replace . How would I do that ?
Also , since I really want to understand this , would you suggest I check out the regex documentation and the replace documentation ? Anything else ?
Replace docs are fairly complete .... I wrote the regex functionality of ` replace ` , so feel free to ask me qusetions
You * could * do this with ` DataFrame.filter() ` and then join the replaced columns with the not-replaced but that seems like it might be a bit inefficient and is also more complicated to understand ... want me to put it up ?
Darn . I was * really * hoping that I could outdo you with ` replace ` :( Guess not :)
Update : I've combined this solution with using a dict to apply the fillna method as suggested here . I wouldn't have known how to use zip to create a handy dict , though , so the answer is rather split between the two of you . Either way , learned a ton from this so thanks .

How can I access the ` name ` field of the resulting median ( in this case ` hello , foo `) ? This fails : #CODE
If you perform an operation on a single column the return will be a series with multiindex and you can simply apply ` pd.DataFrame ` to it and then reset_index .

Also , you might instead want to look at using ` apply ` , which lets you return an entire DataFrame . This way you could , instead of collapsing the items into a list , actually return a new grouped table with one row for each unique value in the source column .
Where the argument x is the Column int he DF ? Thanks this is helpful . It seems I don't need to actually iterate over the index within each group . How would one do that if it were necessary ? Apply seems like it could be useful as well , and that seems to work in prety much the same way . I'll have a look now . thanks a lot .

Do it with ` loc ` : #CODE

Pandas will truncate the output if the result is likely to exceed the default settings , you can change this using ` set_option ( ' display.max_rows ' , 1000 )` or whatevery value , you can lookup the docstring and if you in IPython and perform a tab completion it will show you the default setting and the current settings .

I have a multi-index series / dataframe with ID and timestamp as key . This data structure has daily data for various IDs . Can I use the resample function to look at end of the month snapshot of this data structure ? #CODE
Can I use the resample function call to help me out ? I know I can create end of the month date list and loop through those dates and find those values . But I want to use the full functionality of pandas .
Why do you need to resample ? Just set the index to ` ts ` and then slice , like so : #CODE

I would like to resample this serie in order to get a serie like that #CODE

You might want to take a look at improving your algorithm so it consumes less memory - take a look at this page , it contains some good info on memory allocation in Python . This question on Stack Overflow also has some good tips for profiling your memory-usage !

I am trying to apply Logistic Regression in Python using statsmodel.api.Logit .
To ignore rows with missing data and proceed with the rest , use ` missing= ' drop '` like so : #CODE

Note that you'll need ` pandas ` version 0.11 or newer to make use of ` loc ` for overwrite assignment operations .
-1 " Another way to do it is to use what is called chained assignment . " No . Emphatically , no . It's * only * useful to know that chained assignment isn't reliable . It's not that it's a reliable , non-optimal solution , [ the situation is much worse ] ( #URL ) . You've even acknowledged this [ elsewhere on Stack Overflow ] ( #URL ) . Please try to avoid giving the illusion that chained assignment is a viable option . The first two methods you gave were enough , and are the preferred way to do this .
You can use ` map ` , it can map vales from a dictonairy or even a custom function .
And map : #CODE

You need to pass a ` list ` , ` dict ` , ` tuple ` , or generator of ` DataFrame ` or ` Series ` objects to ` concat ` . What you're doing now will throw a ` TypeError ` because ` lastData ` is being passed as the ` axis ` argument ( and there are numerous lines like ` axis == 0 ` , which will trigger a ` TypeError `) . Do it like this : #CODE

How to resample a time series producing its geometric mean ?
I'm new to Python and I just came cross a tricky question when using ` pandas ` to resample some data .
When I want to resample my time series data , it is very straightforward to apply the arithmetic mean function .

` df.date = df.date.apply ( lambda d : datetime.strptime ( d , " %Y-%m-%d "))` here which doesn't work since I'm working with integers , not strings . I think I need to use ` datetime.date.fromtimestamp ` but I'm not quite sure how to apply this to the whole of df.date . Thanks .

to associate values with an index . Once you've associated the cumulative counts ( values ) , e.g. [ 8 , 11 , 13 ] , with the ` cat ` numbers ( index ) , e.g. [ 1 , 2 , 3 ] , you are basically home free . The rest is just standard applications of unstack , fillna , div and groupby .
Inside the ` lambda ` function , ` x ` is a DataFrame . ` x [ ' count '] .cumsum() ` is a Series . That Series has its own index which I'd like to replace by ` x [ ' cat ']` . If we use ` pd.Series ( x [ ' count '] .cumsum() , index=x [ ' cat '])` , then a ` ValueError : cannot reindex ... ` exception is raised because the original Series has duplicate entries in its index -- so it is unclear how to map from the old index to the new index . Every Series has a ` values ` attribute which returns the underlying data as a NumPy array . All we care about is the data , not the original index , so I used ` .values ` to pick off the data .

It seems like a fairly simple equation . If you are already going to allow the maximal st . dev ., why not just see what the profit is given your maximal shift allowed , and back-calculate the weight ?

at the point in the script where I wanted to stop and look around . Once dropped into the IPython shell , I could investigate variable dimensions and experiment with manipulating things . When I found the combo I wanted , I would copy the commands , drop out of the interactive interpreter and modify the script . The reason this worked for me is it did not involve modifying the structure of the program simply to get debugging information .

This statement ( and similar others ) used to insert new data returns a ` NaturalNameWarning ` warning : ` store.append ( ' equity / bloomberg / 4615238QCN_Equity ' , df )` . It is not respecting the Natural Naming requirement which generates a warning . This might be linked to the encountered issue .

@USER , I tried your suggestion from iPython with concrete values in my conditions . They worked fine . But , when I embed the same in a program and call with parameters , I get an error - the final lines are : ` code ` File " / usr / local / lib / python2.7 / dist-packages / pandas / core / series.py " , line 225 , in wrapper if len ( self ) ! = len ( other ): Type Error : len() of unsized object

I think you could use shift function to shift the column in grouped data by one row . Then , if you subtract original and shifted , you'll get the raw number of hours spent on the activity . #CODE
I think I have finally solved the problem . I suspect that yemu's answer is good , but I prefer this as it was an exercise in finally learning how to apply my own functions : #CODE

sorting pandas dataframes according to cut in python ?
Now how can you sort df according to the labels generated by ` cut ` ( the ` df [ " bins "]` column ) ?
Could you not just sort column ' a ' first ? ` df.sort ( columns =[ ' a '] , inplace=True ) df [ " bins "] = pd.cut ( df.a , np.linspace ( -2 , 2 , 6 ))` will then order it prior to applying the cut for the new ' bins ' column
Or you could just sort ' a ' after applying ` cut ` would achieve the same thing

The " float_format " option has not worked for me . Do I need to convert every float to a string and replace the " . " with " , " ?

Pandas interpolate data with units
( obviously there is not point to interpolate depth over depth , but it's a test before it gets more complicated ) . #CODE
But when I try to interpolate the missing values doing : #CODE
But if I try now to interpolate the missing values by dropping the units , it works : #CODE
Interpolate #CODE

Then pivot : #CODE

The basis for that pandas operation is called ` stack ` : #CODE
Working and tuning your data from the stacked dataframe above is straightforward . You can follow by resetting the index , split it into year month day columns , and apply the math on the non NaN data that are in a single column now .

Python : Pandas merge 2 dataframes using a common columm
How can I merge 2 dataframes df1 and df2 using a common column ' ADD ' into df3 ?
You have to do an outer merge ( join ): #CODE

So you need to apply an aggregation operation ( e.g. ` sum ` , or use ` apply `) to your grouped frame , which will then create a new frame , which you can ` to_excel ` .
This makes sense . I suppose I should be using code that just sorts if I want to organize by groups for the output , but not apply any aggregation operations . Thanks for the clear explanation - the fact that some output did make it to the excel file made me think it ought to work the way I supposed , but obviously not .

See this answer's edit history for a less direct way using groupby , mean and unstack .
Perfect , works like a charm . However i'm fuzzy on exactly what the unstack operation does . The Pandas documentation on that function doesn't make a lot of sense to me .
@USER It's basically a pivot to turn the test level a set of columns , which made me realise what you are doing is just a pivot ! ahem !
That makes more sense now . I understand pivot tables from working with them in Excel years back . The stack and unstack operations make a little more sense to me now in this context .

You can do this with the lesser-known ` align ` method and a little ` unstack ` magic : #CODE
Nice analysis . I deleted my answer in shame when I saw your superior construction , so much cleaner than all my zipping . Nice to see align used , also . I'll restore it for `` combine_first `` ....

This will align the frames and fill the nan's with 0
yep .... that is shorter than `` align / + `` ... sure

Python Pandas : Resolving " List Object has no Attribute ' Loc '"
Then I'm trying to do a simple replace based on IDs :
` AttributeError : ' list ' object has no attribute ' loc '`
@USER answer is correct . Loc assignment works fine if the right-hand-side list matches the number of replacing elements #CODE
If you want to keep the entire dataframe and only want to replace specific values , there are methods such replace : Python pandas equivalent for replace . Also another ( performance wise great ) method would be creating a separate DataFrame with the from / to values as column and using pd.merge to combine it into the existing DataFrame . And using your index to set values is also possible : #CODE
But for a larger set of replaces you would want to use one of the two other methods or use " apply " with a lambda function ( for value transformations ) . Last but not least : you can use .fillna ( ' bla ') to rapidly fill up NA values .

` df [ ' a '] .map ( df.groupby ( ' a ') .apply ( lambda x : len ( x )))`

Then finally , strip out the times that are outside your 9:30 AM to 4:00 PM time range : #CODE

My point of view would be that if you express this as " Find the single row where Condition 1 is True . Now find the single row where Condition 2 is true . Now get all the rows between them " then that is an incorrect way to approach it unless you can document the guarantees about the data . A better approach is " Along dimension 1 , what condition should be true ? Along dimension 2 , what condition should be true ? Now take the intersection of the sets of rows where those are true . "

I'm trying to do a pivot of a table containing strings as results . #CODE
I know I can map the strings to numerical values and then reverse the operation , but maybe there is a more elegant solution ?
I think the best compromise is to replace on / off with True / False , which will enable pandas to " understand " the data better and act in an intelligent , expected way . #CODE
You essentially conceded this in your question . My answer is , I don't think there's a better way , and you should replace ' on ' / ' off ' anyway for whatever comes next .
As Andy Hayden points out in the comments , you'll get better performance if you replace on / off with 1 / 0 .
I ran across this while trying to resolve the same problem , and then a colleague walked into my cube and made this suggestion . Voila . It worked . Thought I would share it here . Note that I changed the pivot to follow pandas 0.14.1 syntax ( thank you " future warning ") #CODE
So basically , you add lambda identity function as the aggfunc to the pivot magically works as you would expect . There may be some performance difference for really big data , however , if you transform to numerically coded " results " first , you essentially still have to touch each cell in the table anyways .

The ` hist ` method of ` Series ` ( the type of ` values `) can be called with a ` bins ` keyword argument . I'll add that to the answer .

I think you're looking for the ` closed= ' right '` and ` label= ' right '` arguments of ` resample ` : #CODE

@USER even without showing us you can still call ` read_csv ` and use param ` parse_dates=my_col_list ` , the list can be either the column names or the index value , this would be easier than parsing after loading . Also you can get the seconds of these values by accessing the attribute ` .second ` rather than what ` getSec ` is doing , in fact you seem to be calculating time difference in minutes so why not just subtract the values and then access the ` .minute ` attribute . Another idea would be to use [ shift ] ( #URL )

Python : append / merge dicts to pandas in a pythonic way
I'm new to python , I'm trying to append result from my iteration loop that give out different number of keys each time in the loop .. something like #CODE
You can either append each dict to your DataFrame as you loop : #CODE

How to create pivot with totals ( margins ) in Pandas ?
I have success when I try to make pivot without margins ( totals ): #CODE
but if I want create pivot with margins : #CODE

What have you tried ? Read the docs on groupby and resample .

If you really want , you could strip the extra tabs at the end .

Updating a tri-match pandas script to update instead of append columns
I think you can achieve that by changing the merge to ` how= ' right '` .

Your question is a little unclear , but you seem to be trying to apply a function to each row of the DataFrame . Try #CODE

` timeit ` of the ( presumably ) desired single ` bool ` indicating whether the two ` DataFrame ` s are equal : #CODE
return bool ( asarray ( a1 == a2 ) .all() )`

I would consider using a [ Pandas DataFrame ] ( #URL ) . You could have an index for the ID ( dict key in your example ) . Then perform an outer join operation and apply a function that does the comparison to the result . Of course there will be ways to do it with ` dict ` , but it seems like you really need a bit of relational logic on the IDs followed by application of an arbitrary piece of code ( the comparison operations ) . That's what Pandas is good at .
One of the distance metrics ( jaro , perhaps ) in the ` jellyfish ` library would probably apply . Here is an [ example answer ] ( #URL ) that prints out words of a certain distance , but you could just as easily print out the distance itself ...

` reindex ` is an amazing function . It can ( 1 ) reorder existing data to match a new set of labels , ( 2 ) insert new rows where no label previously existed , ( 3 ) fill data for missing labels , ( including by forward / backward filling ) ( 4 ) select rows by label !

Is there a way to normalize all the possible column names to " Address " in pandas so I use the script on different files ?

Is there a way I can prevent pandas for ruining the data in the first place when I ix it ?

Then this give the intersection you need to group your data : #CODE

Iv'e tried to use the groupby mechanism , but with no success . using the simple apply mechanism is ok , but seems a little cumbersome ( I'll need to keep a dictionary containing a counter of appearances for each ID )
Can you explain what is happing here ? because i was trying to use groupby and apply and what i got back was a series with the ID as index and the modified ID's as lists for each index . what is going on here under the hood ? what is the translation into natural language of the code above ?
` apply ` and ` transform ` do similar things . ` apply ` is a complicated beast because it behaves differently depending on the type of object the function returns . I have not attempted to memorize [ the rules which govern this behavior ] ( #URL ) , I simply try a few plausible variations until I find the one that works . In this case , since I knew transform is intended for changing a Series to another Series * of equal length* , I tried transform .
To better understand what my solution is doing , I suggest first looking at ` df.groupby ( ' ID ') [ ' ID '] .transform ( lambda x : x )` then ` df.groupby ( ' ID ') [ ' ID '] .transform ( lambda x : x.rank ( ' first '))` and ` map ( ' {}_{} ' .format , [ 1 , 2 , 3 ] , ' abc ')` . If you understand those pieces , then I suspect you'll understand my solution , at least on the level I understand it .

How to pivot this data frame with pandas
I'm trying to pivot this DataFrame ' a ' : #CODE
by calling the pivot method #CODE

@USER : If you're having trouble with that , it might be a good idea to ask a new question along the lines of " Extracting month and day from datetime index " , with some easily reproducible data . Stack Overflow works best with self-contained questions that solve a single problem .

` target_period = source_period * float ( len ( df.index )) / 100 `

Build a MultiIndex from X and Y , and use unstack . #CODE

is there a way to append a new row to this newly created dataframe . Currently I have to create a dictionary , populate it , then append the dictionary to the dataframe at the end -however is there a more direct way ? thank you in advance .
Upcoming pandas 0.13 version will allow to add rows through ` loc ` on non existing index data .
enlargement only allowed thru `` loc `` ( `` iloc `` could add not-at-the-end so its a bit ambiguous )
I've found that this will drop any columns from the added Series that are not already in the DataFrame . In this case , the documentation is misleading in that it states that " Setting With Enlargement " is like an " append " operation , even though " append " will add any new columns to the DataFrame .

Possibly useful , but this doesn't say how to apply the transformation to the date index ...
You can also work with ( and resample ) as periods #CODE

Python Pandas business day range bdate_range doesn't take 1min freq ?
I am trying to use bdate_range with ' 1min ' freq to get minute by minute data on all business days . #CODE
I think the freq = ' 1min ' totally overwrites freq = ' B ' from function name bdate_range

Transpose columns in python / pandas
I've tried using the stack / unstack and the transpose function , but I just can't figure it out . Does anyone have any pointers ?
You want to use the ` Period ` as an index . ` set_index ` will do this for you . THen you can transpose your resulting table : #CODE

replace values in pandas using regex that matches all values except the provided one
I want to use regex with pandas to replace values in a column to mark correct answer for the question .
and it doesn't replace values in pandas .
Can you provide some more context ? Which version of Python are you running ? And what is the ` df ` object ? I'm not seeing where ` incorrect_dict ` is being passed to any replace / substitute call .
I can't check your answer right now , but the reason I chose to replace values is that I wanted a universal approach and other variables I have are in text format .

On ` groupby ` object , the ` agg ` function can take a list to apply several aggregation methods at once . This should give you the result you need : #CODE

Python Pandas : Groupby and Apply multi-column operation

I could like to test a few conditions for each row and if all conditions are passed I would like to append the row to list . For example : #CODE

It doesn't usually make sense to perform ` value_counts ` on a DataFrame , though I suppose you could apply it to every entry by flattening the underlying values array : #CODE

I know about the apply function but it is too easy in my case ..

I would like to map 1 , 2 and 3 to 1 and 4 , 40 , and 50 to 2 . Is there an easy way to do it in a data frame .
In the above example I have used only two groups . But I would like to keep it flexible . For example for three groups I would like to map 1 and 2 to 1 , 3 and 4 to 2 , 40 and 50 to 3 .
` heapq.nlargest ( len ( df [ ' ELEV ']) / 2 , df [ ' ELEV '])` but got this error :

In the dataframe above I would like to apply the qcut function to B while partitioning on A to return C .

To apply your custom function to each row , use ` apply ` with the keyword argument ` axis=1 ` . #CODE

So , if I try to insert into a new data frame I get all sorts of errors ( i.e. dimension mismatch , etc ) . It seems to me that when I perform an operation on a data frame the output should be a data frame , not a Series . Does anyone have a recommendation on how to use , e.g. df.mean() , and have a data frame returned ?
I want to selectively average slices of my original data frame , and insert these averaged values into a separate data frame . #CODE
So long as the result can be [ broadcasted ] ( #URL ) you can do this ` a [ ' mean '] = a.mean() ` , also do you really want to concat the results as new rows and not as a new column ?
@USER you can just pass the ` mean() ` into a ` DataFrame ` constructor for ` concat ` like so ` b = pd.concat ( b , DataFrame ( a ( a.A > 5 ) .mean() ))` , although this doesn't work I think you wanted ` a.loc [ a.A > 5 ]` ?

Since I can't easily construct a sample version with your exact column labels , there might be slight additional tinkering with getting rid of any index columns that are included in the ` diff ` and moved with the transposition . But this worked to make a simple heat-map-ish plot for me on a random data example .

The number of periods defined in your index is ` len ( x )` , which is the same as your ` duration ` variable . Now , I can see that in the Gist you have shared , ` duration ` is 100 . This means that your index shall contain time starting from midnight of 1st Jan 2013 to the next 100 seconds .
You may wish to alter duration or maybe keep ` freq ` as a day to get more than 1 day worth data .
The following is the result of ` head ` on the CSV produced if I alter the ` freq . ` to day ( D ) . #CODE

And use loc to look at the subDataFrame where msk is True , and only the specified columns : #CODE

I have a long dataframe with daily dates starting from 1999 . I apply a filter to the original_dataframe to create a new_dataframe_1 and another filter to create new_dataframe_2 .
If you want the columns from both DataFrames joined together , do an inner join : #CODE
do a ` reindex ` on the intersection of the indices : #CODE
Sorry . I explained poorly . I don't want to join the two dataframes . I just want to get a new dataframe with all the common dates . Both new dataframes are pulled from the original dataframe . The rows represent times . I dont want the third dataframe to be twice as wide . I just want the ' common ' dates of the two dataframes .

Generating Pivot Tables in Python - Pandas ? Numpy ? Xlrd ? from csv
I have been searching for hours , literally the entire day on how to generate a pivot table in Python . I am very new to python so please bear with me .
What I want is to take a csv file , extract the first column and generate a pivot table using the count or frequency of the numbers in that column , and sort descending #CODE

I'm ultimately trying to get a pivot table of mean rating for usage by brand . Or something like this : #CODE
My approach was to merge the datasets like this : #CODE
And then attempt to create a pivot table using rating as the value , own as the rows and brand as the columns . But I kept running to key issues . I have also attempted unstacking either the measure or brand levels , but I can't seem to use row index names as pivot keys .

Replace row in DataFrame dependant on a value
I would like to replace an entire row in a data , subject to one column in that row satisfying the condition .
Use ` loc ` to filter out the part of the DataFrame you want to zero , and then assign the value to it . Below , it selects all lines where ` c ` column value is `' M '` and it takes all columns from ` b ` to ` c ` , and set the value of this selection to ` 0 ` : #CODE

stack / unstack / pivot dataframe on python / pandas

Pandas Pivot table , Dealing with Multiple Values in column
I want to get a pivot table that combines all of the prices per UNSPSC code .
I also tried to pivot by #CODE
How can I get a pivot table with that shows the sum of the prices to each unique UNSPSC ?

You can apply value_counts to the SeriesGroupby ( for the column ): #CODE
I actually hoped this to return the following DataFrame , but you need to unstack it : #CODE

How to Normalize Names
splitCompaniesSet = map ( lambda cmpnyName :
set ( map ( lambda name : name.split ( " ") , cmpnyName ) ) , dataFrame [ ' Company '] )
Basically create a list of sets , each set has the company name split . Then , starting with the first element , find the set intersection of every other element with that one . For every non-empty intersection , change the name to whatever the simplest match was
among all the non-empty resulting sets , i.e. take one more set intersection with all the nonempty sets and set the result to be the company name for all those non-empty matches .
Interesting , I will give it a go . I have been looking up normalization on the unicodedata module . I was not sure if that would apply to this situation however . I also discovered from zope.component import getUtility
Hey this is awesome . Thanks for exposing me to the fuzzywuzzy article . I will take some time to check it out and see how I can apply it .

So if I wanted to return a vector with the sum of all the values in Bool per letter where the number of entries per letter exceeds three , how would I do it ? With the data provided , this would be [ 2 , 1 ] ( 2 for B and 1 for C )
To sum the values of ` Bool ` by ` Letter ` where the number of members in each group is greater than 2 : #CODE

I get the error : ` KeyError : u'no item named name of first item I try to insert '`

I want to convert datetime to object because pandas have a bug ( see #URL ) in pivot totals calculation if column ( which used as header ) type is datetime .
the solution in that issue looks fine , just transpose the rows / columns . What you are trying to do has nothing to do with `` datetime64 [ ns ]`` as a dtype and will not help .
yes , but only if pivot row column isn't datatime too .

I'm pretty sure the quotation marks cause it to interpret all the commas within as escaped . So , you need to strip all of them off . That's relatively simple to do , but because of the unicode issues I'm going to be crazy and suggest you read it in , strip the quotes and then write it to a file to use with ` read_csv ` ( because it'll simplify the encoding issues ) .
Here's how to write to a file and strip the quotes , write to a new file , then read in with read_csv : #CODE

Outer join in Python for thousands of large tables
So , I have some 4,000 CSV files and I need to outer join all of them . Each file has two columns ( a string and a float ) and between 10,000 - 1,000,000 rows and I want to join by the first column ( i.e. , the string variable ) .
So now I'm looking for alternatives - are there other potential solutions that I'm missing ? What other outer join implementations exist for Python ? Is there a paper / site somewhere that discusses and compares the time complexity of each implementation ? Would it be more efficient if I simply had Python call , say , sqlite3 , and then have sqlite3 do the join ? Is the string key the issue ? If I could use a numerical key instead , should it be any faster ?
Don't merge iteratively . You are merging a smallish frame ( call this the ' mergee ') with a larger frame ( call this the ' merger ') . Then repeating this , causing the ' merger ' to get bigger and have more rows .
merge 1 and 2 to form 1_2
Then repeat , so then you merge 1_2 and 3_4 to form 1_2_3_4
This doesn't change the amount of work you are doing , but it makes each merge much simpler , which lowers the memory barrier and spent time ( as it has to go thru the cartesian product of the keys ) . It may make sense to randomize the merge order .

The normal matplotlib boxplot command in Python returns a dictionary with keys for the boxes , median , whiskers , fliers , and caps . This makes styling really easy . #CODE
The Pandas library has an " optimized " boxplot function for its grouped ( hierarchically indexed ) dataframes . Instead of returning several dictionaries for each group , however , it returns an matplotlib.axes.AxesSubplot object . This makes styling very difficult . #CODE
Each bar consists of 8 items . e.g , The 5th item is the median . The 7th and 8th items are probably the fliers , which we don't have any here .
Knowing these , to modify some part of the bar is easy . If we want to set the median to have ` linewidth ` of 2 : #CODE
You could also specify the ` return_type ` as ` dict ` . This will return the boxplot properties directly in a dictionary , which is indexed by each column that was plotted in the boxplot .

when I call the function , both the hist and the scatter plot should appear

Pandas : Drop consecutive duplicates
What's the most efficient way to drop only consecutive duplicates in pandas ?
Use shift : #CODE
Another method is to use ` diff ` : #CODE
Thanks to Bjarke Ebert for pointing out a subtle error , I should actually use ` shift ( 1 )` or just ` shift() ` as the default is a period of 1 , this returns the first consecutive value : #CODE
@USER no worries , glad to help , I noted your comment and you are correct , the resulting row values was what the OP wanted but the index values were the wrong rows as you correctly potined out , interestingly using ` diff ` was slower for a 50k series , probably due to the value comparison
Also , ` df.col ! = df.col.shift() ` is much more general . Using ` diff ` only works for integers whereas ` shift ` works for floats , strings , etc .

How to sort a boxplot by the median values in pandas
I've got a dataframe ` outcome2 ` that I generate a grouped boxplot with in the following manner : #CODE
What I'd like to do is sort the plot by the median for each state , instead of alphabetically . Not sure how to go about doing so .
What do you mean by " inside of alphabetically " ? Do you only want to look at the first letter of each state's name ? You can sort by the median or alphabetically , but you can't do both .
To sort by the median , just compute the median , then sort it and use the resulting ` Index ` to slice the ` DataFrame ` : #CODE

You can use ` loc ` too : #CODE

The 5 in the lambda above comes from the correct width . You'd need to select out all the columns that need leading zeros and apply the function ( with the correct width ) to each .

Is there a ` reset_index ` equivalent for the column headings ? In other words , if the column names are an ` MultiIndex ` , how would I drop one of the levels ?
drop a column , or move into the index ( as a row ) ? I don't think there's not really a clean way to move to index aside from wrapping in .T ...
The idea being to move a given level of the column names's MultiIndex to the DataFrame as a new row ( or optionally just to drop it altogether ) .
Maybe a simpler question is how do I simply drop a level of hierarchy in the column names ?

The result obtained , I need to apply the following conditions #CODE
You can make your own aggregate functions to apply to grouped data #URL . So for your case you could try something like : #CODE

Have you tried ` replace ` ? #CODE

please replace it with a list comprehension .

For my assignment I'm supposed to plot the tracks of 20 hurricanes on a map using matplotlib . However when I run my code I get the error : " AssertionError : Grouper and axis must be the same length " . I'm a newb to python and matplotlib , so could someone please point me in the right direction of what I need to change . Thanks in advance
@USER Ok thanks . Does the column names ' lat ' and / or ' lon ' have to be specified in the csv file ? What I'm trying to do is put the longitude and latitude columns from the file into two seperate list so that I could plot them on the map .
Does the column names ' lat ' and / or ' lon ' have to be specified in the csv file ? What I'm trying to do is put the longitude and latitude columns from the file into two seperate list so that I could plot them on the map .

I think the problem is actually that the boolean array from the shift operation is one short of the the other conditional . Try adding a false to the first conditional at index zero you should then be able to combine the two conditionals .
yes .... the issue is the Series vector ( e.g. `` df.D > 1 ``) * looks * like it should work , but its ambiguous how it should broadcast , e.g. should that Series named D apply to all of the other columns ( in which case what should it do ? ) , or should it effectively have no name which means it SHOULD broadcast . You problem could also be solved by using `` df > 1.0 `` because I think that is what you intend ( e.g. that it DOES broadcast )

All I simply want todo is cut this data into weeks and write it out to CSV files , I can already do this for conventional weeks ( Mon-Sun ) but I now want to cut into weeks that are defined as Sun-Fri .

Problem ! Though this works , it ends up being really memory inefficient . I'm working with a 13 million row dataframe , and attempting to run the apply ( sequence_id ) bit ends maxing out the 20gb of ram I have available . I've worked with bigger dataframes , so it must be something to do with this particular operation . Any thoughts on how we could optimize it ?

I'm trying to plot the path of 15 different storms on a map in 15 different colors . The color of the path should depend on the name of the storm . For example if the storm's name is AUDREY , the color of the storm's path should be red on the map . Could some please help / point me in the right direction ?
Thank you so much !!! I finally got rid of that error . However the lines do not show up on my map . Is there anyway to fix this ?

Merge DataFrames in Pandas using the mean
I have a set of DataFrames with numeric values and partly overlapping indices . I would like to merge them an take the mean if an index occurs in more than one DataFrame . #CODE
Now I would like to merge the DataFrames and take the mean for each index ( if applicable , i.e. if it occurs more than once ) .
One small thing : If I had more columns in the DataFrames , how would I define that I want to merge and average ' col ' and do another / no operation on the others ?
@USER : You could apply any of the above methods to the Series ` df1 [ ' col ']` and ` df2 [ ' col ']` . For example , @USER ' s answer would look like this : ` pd.concat (( df1 [ ' col '] , df2 [ ' col ']) , axis=1 ) .mean ( axis=1 )` .

you can use apply function : #CODE

I'm having a similar problem at the moment , I think you shouldn't use reindex but something like asfreq or resample .

You should use loc to do this without chaining , which will garauntee that assignment works : #CODE
Assignment in loc is overridden so that the implementation detail of whether it's actually a view or a copy doesn't matter , it does matter if you chain so avoid / be very careful .

very close ! I had to convert dates to ' 12 10 ' instead of ' 12 Oct ' because pandas crosstab alphabetizes and the 3-letter months messes that up . So if I can isolate the two-digit month , I can apply calendar.month_abbr [ ## ] to get that 3-letter month . Thanks !
I approached this problem from a slightly different angle and created a function that can be used as a general method of ordering columns in a crosstab in pandas . It may also work for a pivot table but I didn't test that nor did I look at the details . I suppose it can also be used to order row labels too but I didn't try for that .

Sorry if this question is simple I'm a newb to using Python and Basemap . Anyway I'm trying to plot the path of 20 hurricanes on a map ( graph ) . The map itself and the legend show up perfectly but the paths of the hurricanes do not . Also I'm not getting any traceback messages but I think I have an idea of where my problem may be . Could someone please tell me where I went wrong .
#Plot the points on the map

I have a bit of GIS experience and I'm now trying to learn pandas . Any help would be appreciated . The goal here is to join one point to one person .
I've tried both merge and join but that doesn't give me the right output , way too many values . I've tried massaging the merge output with drop_duplicates and unique methods , but no luck so far . I've read through the merge documentation and I have this feeling there's a simple method to do this ... but so far I haven't found it .
As an aside , the background on my task is to generate the appropriate number of random points w / in each census block and join them back to the travel survey data so that it can be visualized in a dot visualizer .
If the indices of both dataframes match then what happens when you do this ` merge ( set1 , set2 , left_idnex=True , right_index=True )` ?
I think in your case as the two dataframes have the same number of rows , all you want is to add ` PLSAM ` to the other dataframe so the easiest thing to do is just ` set1 [ ' PLSAM '] = set2 [ ' PLSAM ']` , this will add the column from set2 to set1 , the merge and joins will not work in your case because you have no unique values even though the column values are identical , if this answers your question I will post as answer
The signature of merge from the pandas docs : #CODE
If that doesn't work , the most likely issue is that the indices aren't matching . My suggestion would be to set the indices of each data frame to be the DTRACT column , and then continue with your merge .
Hi Kyle , Set 1 and set 2 share the exact same indicies . I derived set 2 from set 1 . I tried the merge method as you specified initially . I have 141,947 values in set 1 and 2 . I would like to end up with a merge that gives me 141,947 values . Not 9,480,254 of them .
The indices might be in the same range , but do they correspond to the same data points ? I know you said set 2 was derived from set 1 , but looking at the example data , for each of the indices provided , the DTRACT columns don't match up , which would cause issues similar to what you're experiencing when you try to merge . Perhaps you could specify how set 2 was derived ?
Forget about ` merge ` . Maybe it's because I use a lot of databases , but I prefer to the ` join ` method of the dataframe , and I greatly prefer to have indices defined for each dataframe . Like this : #CODE
Hi Paul , thanks so much for your response . I've edited my data above to show what it looks like . I tried the join method and setting the indexes as you specified , but I still 25 values out instead of 5 . In your example , the dtracts are unique . In mine there are not . Perhaps this is an important difference .
@USER you're right . since the indices aren't unique , ` join ` won't work . glad you got it all worked out though .
I think this is far simpler than you think , the reason the merge and join do not work in your case is that although you have a common column , the values are not unique , this would not be a problem if the indices of both dataframes were the same but in your case it seems they are not .

I'm trying to use ` where ` on my Pandas DataFrame in replace all cells that don't meet my criteria with ` NaN ` . Howevever , I'd like to do it in such a way that will always preserve the shape of my original DataFrame , and not remove any rows from the resulting DataFrame .

This is close enough for me to accept , but it looks like you have to specify a particular column for this to work in cases where there are multiple columns , i.e. ` df.groupby ( " item_id ") .filter ( lambda x : len ( x.arbitraryColumn ) > 1 )` . Otherwise you end up with an error like " ValueError : The truth value of an array with more than one element is ambiguous . Use a.any() or a.all() "
Okay , now I'm really puzzled . I copied and pasted the above right from the window , so it works for me . What version of ` pandas ` are you running ? Maybe this is something that's been fixed recently ; I'm running trunk , more or less ( 0.12.0-751-gbea5051 ) . [ One reason I'm so puzzled is because the number of columns should have nothing to do with anything , the ` len ` here is giving the number of rows in the group . ]

Why should it need either change ? The workflow would be just fine if to_datetime treated time only strings like ' 03:00 : 00.xxx ' to be time from start of epoch . Its counter intuitive for it to append today's date to it
`` to_datetime `` DOES NOT append today's date , nor does it assume anything . I gave you an example which does exactly what you want . A time-only string is supported as a timedelta , as I stated above .
My bad , I used the term append loosely . However to_datetime does assume a date that I did not provide in case of string input . My point is simply that the following MUST be true -

Why they're being converted like that : I'm not sure . Might be a bug , but it should be simple enough to ` apply ` something keep it all straight .

I can't see any explicit recursion in your code . Could you please post the complete stack trace ?

Drop Duplicate in market data
So it would be drop the row if both the bid and the ask are the same as the row before it .
You can use ` .shift() ` to shift a column , and use ` any ` to check for differences . For example , given a frame like #CODE
Step-by-step , we shift down 1 : #CODE

are you looking for a join ? really unclear what your question is . take out the twitter api stuff and boil down to a sample input / desired output
I think what you want to do is do an outer merge , assuming you have the username in both ` start_df ` and ` friends_df ` then you perform the merge like so df = friends_df.merge ( start_df , left_on =[ ' username '] right_on =[ ' followee '] , how= ' outer ') . Without seeing the columns of ` start_df ` and ` friends_df ` or ` followers_df ` I cannot advise further , please post sample data from all dfs .

But with this method , I'd have to iterate through all the item_ids , then merge everything together , which seems woefully over-complicated .
So how can I apply this to the full dataframe , ffill-ing the observations ( but also the item_id index ) such that each item_id has properly filled rows for all the dates in baseDateRange ?
Essentially for each group you want to reindex and ffill . The apply gets passed a data frame that has the item_id and date still in the index , so reset , then set and reindex with filling .
answer to your first part is yes that is reasonable , you can drop on the reset_index to not have a dup . for the second , I think the apply is a bit confused by the index because of the way you are aggregating ( so it's dropping the name of the index ) . You can do another reset_index inside the apply , then at the very end ( after the apply , `` .reset_index ( drop=True ) .set_index ([ ' date ' , ' item_id '])`` , so reset the mi

How can I add a header to a DF without replacing the current one ? In other words I just want to shift the current header down and just add it to the dataframe as another record .
Thank you . This is really cool and good to know but I meant how to replace the header ' A ' and ' B ' from the first df above but also just add the values ' A ' and ' B ' as another row , in other words move values ' A ' and ' B ' down to index 0 as the new first record in df .

It you want to apply these to all your pandas tables you can use css . A ( not recommended ) way is put the following into a markdown cell of the active notebook . #CODE

I need the apply function that returns several value from several complex calculations . I can return those values in a tuple , and thus the outcome of the groupby-apply action would be a Series with group name as indexes and the tuple as values . I would like it to return a DataFrame instead , So I could keep all the pandas functionality and flexibility .
In general , The outcome of a groupby-apply operation would be a Series In the case apply returning 1 value . In the case of apply returning 2 or more values , I would like the outcome to be a dataframe . so my question is how to do that . See the original Q for more details and examples
Based on the edited question , maybe this is what you are looking for . Returning a series in the apply call results being collated into a dataframe ( guessing that is what you are looking for )
That's definitely answer the example I gave , but not my problem . which means that I gave a bad example .. Imagine that I need the apply function to retrieve few complex calculations . A function that is not a built-in function . In the case of apply returning 1 value , the outcome is a series . In the case of apply returning 2 or more values , I would like the outcome to be a dataframe . so my question is how to do that .

for this and your other questions , pls look at the docs : [ here ] ( #URL ) , you need to use `` ix / loc `` or `` xs ``
you cannot add a column that doesn't have all the levels of a multi-index . best to keep it in another frame then join / index as needed . Maybe give a complete toy example and we can provide more help . Its not clear what you are trying to do .

I have a dataframe where some entries in ` column_1 ` have ` NaN ` values . I want to replace these by the corresponding values in ` column_2 ` . Both columns hold ` float64 ` values .
you are modifying a copy , see [ here ] ( #URL ) , use `` mydf.loc [ ix , ' columns_1 '] = value ``
This is an example of chained indexing . For getting values , this is generally ok ; however for setting values , it may or may not work as you may be trying to set values on a copy . It is always better to set via the indexers ` ix / loc ` for multi-dimensional setting .
In this example , use ` mydf.loc [ ix , ' columns_1 '] = 45 `

On 12+ dev of Pandas .. so I'll try apply ( int ) I still don't get why the Dtype={ does not work on read_csv ?
as @USER pointed out , a quick way is to drop this value . run df [ df [ ' Credit_exp '] ! = ' \\N '] [ ' Credit_exp '] .astype ( int ) . if no error occurs , you're good to go . otherwise , try numpy.unique ( df [ ' Credit_exp ']) and sense check string values . and of course drop them

The second command causes the following error which I do not understand . Any thoughts on what might be going on here ? I replaced map with apply and that didn't help matters . #CODE
Perfect Jeff , thank you ! Once I got rid of the rows with missing data , I could directly apply this to the new problem . ;-)

How can I drop duplicate data in a single column , group-wise in pandas ?
unstack doesn't work , because there are duplicate entries ...

What you could do is map your strings to ints / floats and map your column B to their dict lookup values into a new column C and then create the sparse dataframe like so : #CODE

Thanks a lot ! Is there a simple way to calculate actual average or median time instead of strictly granular ? E.g. , average between 00:04 : 45 and 00:05 : 25 would be 00:05 : 05

Transpose : #CODE

The ` reset_index() ` is just to shove the current index into a column called ` index ` . Then ` pivot ` will take your data frame , collect all of the values ` N ` for each ` Letter ` and make them a column . The resulting data frame as 400 rows ( fills missing values with ` NaN `) and three columns ( ` A , B , C `) . ` hist() ` will then produce one histogram per column and you get format the plots as needed .
I'm on a roll , just found an even simpler way to do it using the by keyword in the hist method : #CODE

I'm exploring Pandas - trying to learn and apply it . Currently I have a csv file populated with a financial timeseries data of following structure :
Now I want to convert the csv data into a pandas DataFrame object , so that date and time fields merge and become the DateTimeIndex of the DataFrame like this : #CODE
now apply the lambda function , doing what the parser should have done : #CODE
Could you see what happens when you drop the column names from the parameters to ` read_csv `

datetime.date creating many problems with set_index , groupby , and apply in Pandas 0.8.1
My goal : group a DataFrame by a categorical column " D " , and then for each group , sort by a date column " dt " , set the index to " dt " , perform a rolling OLS regression , and return the DataFrame ` beta ` of regression coefficients indexed by date .
Now here is what happens when I try to work with these using ` groupby ` and ` apply ` : #CODE
If I save the ` groupby ` object and attempt to apply ` foo ` myself , then in the straightforward way , this also fails : #CODE
I can simplify the problem just to the ` set_index ` call within the ` apply ` function . But this is getting really weird . Here's an example with a simpler test DataFrame , just with ` set_index ` . #CODE
By adding a call to reset the index inside of the function to be applied with ` apply ` , it gets rid of the problem : #CODE

it is a bug : in pandas code len ( df1 ) is going to return 3 that is the length of the df1 index , but then the code iterates on df1 3 times through the for loop inside code . However the iterator will return the columns names ' a ' and ' b ' and this length is 2 , so the third iteration raises a StopIteration
@USER I'll join you on the crusade for Panels . I've had the need to start using them , and for a certain kind of problem they're ideal .

How to merge 2 columns in 1 within same dataframe ( Python , Pandas ) ?
After pd.read_csv ( ... ) he's using ' dt ' ( datetime ) column as index of dataframe . #CODE
How do i modify that line of code in order to do the same ? I.e. merge two columns within one data frame , and then delete it . Or is there a way to do that during read_csv itself ?
You should be able to concat two columns using apply() and then use to_datetime() .

Id like to provide the row name , the column name , and a number of rows to shift back . Then I'd
Just to clarify , you want a function that takes a dataframe , a location ( row and column ) and a number right ? You'll want to use pandas shift method . #CODE
`` df.ix [ df.index.get_loc ( ' C ') -2 , ' b ']`` will be much faster ( as it doesn't shift anything , just an index lookup )

or ` left outer join ` with ` where ` clause : #CODE
Last one could be implemented in pandas with ` merge ` : #CODE
@USER see another solution with pandas merge .

I'm using pandas 0.12.0 , matplotlib 1.3.0 and python 2.7.5 . Any suggestion that leads to a way of combining / stitching multiple ' hist ( by =) -plots ' in the same subplot grid is welcome .

Starting in 0.13 ( releasing very soon ) , you can do something like this . This is using generators to evaluate a dynamic formula . In-line assignment via eval will be an additional feature in 0.13 , see here #CODE
Create a generator that evaluates a formula using ` eval ` ; assigns the result , then yields the result . #CODE
So its user determined ordering for the evaluation ( and not on-demand ) . In theory ` numba ` is going to support this , so pandas possibly support this as a backend for ` eval ` ( which currently uses numexpr for immediate evaluation ) .
It is nice to have the ' formula ' and eval feature in coming update cersion . And I want to know more about how to use df [ ' lazy_eval_col_x '] syntax to triger the on-demand calculation .

How would you create a column ( s ) in the below pandas DataFrame where the new columns are the expanding mean / median of ' val ' for each ' Mod_ID_x ' . Imagine this as if were time series data and ' ID ' 1-2 was on Day 1 and ' ID ' 3-4 was on Day 2 .

add title to collection of pandas hist plots
I've tried using the title keyword in the hist command ( i.e. title= ' My collection of histogram plots ') , but that didn't work .

Inspired by this question and @USER ' s answer , I wrote a parallel-version of map at github . The function is suitable for infinitely parallelizable processing of a read-only large data structure in a single machine with multiple cores . The basic idea is similar to @USER sugggested , using a temporary global variable to hold the big data structure ( e.g. , a data frame ) , and pass its " name " rather than the variable itself to workers . But all of this are encapsulated in a map function so that it is almost a drop-in replacement of the standard map function , with the help of pathos package . The example usage is as follows , #CODE

I would have thought you could map a seasons dictionary to your data and then group based on that .

Finding common rows ( intersection ) in two Pandas dataframes
I can think of many ways to approach this , but they all strike me as clunky . For example , we could find all the unique ` user_id ` s in each dataframe , create a set of each , find their intersection , filter the two dataframes with the resulting set and concatenate the two filtered dataframes .
Maybe that's the best approach , but I know Pandas has really clever and efficient SQL-like functionality . Is there a simpler way to do this ? I've looked at ` merge ` but I don't think that's what I need .
Ah . I had thought about that , but it doesn't give me what I want . I'm looking to have the two rows as two separate rows in the output dataframe . This solution instead doubles the number of columns and uses prefixes . I don't think there's a way to use ` merge ` to have create the two separate rows .
+1 for merge , but looks like OP wants a bit different output . I've created what looks like he need but I'm not sure it most elegant pandas solution
or join and then unpivot ( possible in SQL server ) #CODE

Thanks , that seems to work well . I have some back up questions if you don't mind . 1 ) What is being passed to the function f when it is called with apply ? Is it each groupe of data sequentially ? I assume it must be . 2 ) How can the function be called with multiple columns so people2 = Grouped.apply ( f ( ' a ' , ' b ' , ' c ')) ? Clearly the fucntion would have to be changed , but in your example the function is not very abstract . I would want to write def f ( df , col1 , col2 , col3 ) - so that it could be used beyond the columns referenced inside the function .
+1 , the main part of answer I think is to use apply instead of transform
Would it be right to say then that calling transform passes only the named column , or each column in the DF to the function individually and it is not possible to pass more than one column , whereas apply passes the whole data frame and then column values can be used within the function ? I think that was where I was getting it wrong ...
ok , I think apply as in @USER answer is more appropriate here ?

Try to use ` transform ` and ` shift ` combined together : #CODE

I am using some code to merge two csvs and sort these by two columns . Ouputting a new csv .
and replace the last line with : #CODE
You're welcome ! If you really want to thank me , look up the functions I'm using there and figure out how each of those things is implemented , play around with them and apply it in your own code :)

did you try to use ` axis=0 ` ? This should be the case since you want to apply the function for each row ...
I just tried it and it didn't work . According to #URL axis=1 is to apply to each row .
I also experienced this error . It turned out that the pandas Time Series data type was causing the problem . When I applied the function with the time expressed in epoch ( or anything ) success , but with the time converted to pandas Time Series , there was this error . So my suggestion would be to convert to Time Series after you apply the function , which obviously is contingent that you don't need your time variable in the function being applied .
* apply function not tested with pandas Time Spans .

You could ` apply ` the conversion on the appropriate column : #CODE

You can circumvent this to_datetime issue by resetting the index , manipulating the series via map / lambda / strptime , and then finally setting the index again . #CODE

This is a bug , because the index is not ordered monotonically , see here . but no reason to use ` TimeGrouper ` , this is somewhat internal ATM , use ` resample ` . #CODE
Hi Jeff , thank you for your reply and this is a great idea , but it somehow does not really solve the complete problem . The standard grouping method of " resample " is " how= ' mean '" . Is it defined anywhere how I can change that to something more useful ? I have to apply a custom function . The point is that I also have multiple entries per day and your resample-solution is only correct with the " mean " method when there is only one entry per day .

a lot depends on how you are storing ( e.g. appending all at once is usually best ) , do you need to append , and compression . my2c ; that chunksize is pretty small , default is 50k rows .

and I'm trying to drop a column ' red_id'from a dataframe because i want class average for pass / fail in for loop as shown above .

Use of pandas.shift() to align datasets based on scipy.signal.correlate
with a positive shift of ` 410 ` . I think I am just not understanding how ` pd.shift() ` works , but I was hoping that I could use ` pd.shift() ` to align my data sets . As far as I understand , the return from ` correlate() ` tells me how far off my data sets are , so I should be able to use shift to overlap them .
` panda.shift() ` is not the correct method to shift curve along x-axis . You should adjust X values of the points : #CODE

How to use argmin with groupby in pandas
I could , of course , come up with some horrible hacky thing in standard python where I iterate over all values of cat , then select the subset of my data corresponding to that value , perform the argmin operation then figure out where in the original dataframe that row was . But there's got to be a more elegant way to do this .
` argmin() ` is not an agg function , you can use apply to get the closest index of every group : #CODE

Pandas : Merge or join dataframes based on column data ?
Despite reading the documentation , I am not clear on whether I can use join() or merge() for the desired outcome . If I try a join to the existing dataframe like the simple one I've used previously , I end up with the new columns but they are full of NaN values . I've also tried various combinations using Name1 and Name2 as either columns or as indices , with either join or merge ( not as random as it sounds , but I'm clearly not interpreting the documentation correctly ! ) . Your help would be very much appreciated , as I am presently very much lost .
I'm not sure if this is the best way , but you could use ` reset_index ` to temporarily make your original DataFrame indexed by ` Name2 ` only . Then you could perform the ` join ` as usual . Then use ` set_index ` to again make ` Name1 ` part of the MultiIndex : #CODE

I'm trying to append a level to a Pandas Series . Say a create a simple Series : #CODE

Are you trying to apply a function to each row by taking arguments from different columns ? This has already been [ answered here ] ( #URL ) .

EDIT : I originally start with a dataframe that hase one column . I add 4 columns in 4 difference apply steps , and then when I try to add another column I get this error .
what are you actually trying to do ? using apply with a function that returns a list will try to coerce this to a Series , thus it needs the same length as the original lenght , OR a scalar ( including None ) .
Output in your question is not the one you get from apply . Your output in first case is DataFrame with 4 columns , as @USER said , it's coersed list into rows .
@USER I think that the output is the output from apply because apply will run each row through func=random , and that func will print out [ 1 , 2 , 3 , 4 ] . I am not sure what you are pointing out .
I had this issue , and my solution was just to join my list into a string ... then split it after the apply .
and then use apply : #CODE
This doesn't work on my example because there is one line of code that is different . It is missing df [ ' E '] = 1 . I add the column ' E ' and then I do apply . I think that that is throwing it all off . The problem that I am working on starts with a dataframe with one column and then I keep doing apply to the dataframe to add columns . I add 4 columns and then when I try to add a fifth column , I get that error .

EDIT : To clarify , I'm not just doing this for subtracting the mean , it was just a simple example . A more realistic example would be linearly filtering the array along axis 0 . I'd like to use the scipy.signal filtfilt function to filter my array . This is quite easy if I can just pass it a tpts x feats matrix , but right now it seems that the only way to do it is column-wise using " apply "
apply also works on entire dataframes . If you want to subtract 5 ( or an avg number ) from every item in the data frame , you can do that as well by excluding the axis argument .
So , it's not that I want to apply a function to every item in the dataframe , it's that I want to pass the entire contents of a dataframe to a function . For example , the hilbert transform in scipy takes a timepoints x features array , and computes the transform along the first axis . It is faster to pass a 10,000 by 100 matrix to this function than it is to pass 100 separate 10,000 length columns to this function , which is what would happen if I used " apply " . I'm trying to get around this .

You should transpose the dataframe first before setting the index : #CODE

Trying to drop NaN indexed row in dataframe
I want to drop the row with the NaN index so that I only have valid site_id values . #CODE
I've found that the easiest way is to reset the index , drop the NaNs , and then reset the index again . #CODE
Doesn't work in pandas 0.17.0 . ` Index ` has no ` labels ` attribute , The suggested drop leads to ` AssertionError : axis must be a MultiIndex ` .
Indices don't have ` isnull ` , so we convert to a ` pandas.Series ` . ` isnull ` generates a boolean mask , e.g. ` [ False , False , True ]` , where True denotes the value at the corresponding position is null ( ` numpy.nan ` or ` None `) . We then select the rows whose index corresponds to a true value in the mask by using ` df [ boolean_mask ]` .

I've also tried as many variations of merge , concat , and join as I can think of . They all have errors , so for example : #CODE
Your use of ` join ` is throwing an error because ` site_id ` isn't exactly a column in this case - it's the DataFrame's index . You should be able to just use : #CODE
By default , ` join ` assumes you want to join the two frames on their indexes . By using ` on= ` , it started looking for the ` site_id ` in the DataFrame's columns .
Alternatively , you could use ` merge ` : #CODE

Merging two dataframe using ID and asof
I have two data frame that I wish to join together where the left dataframe have information index by ( date , ID ) and the right dataframe has information index by ( period , ID ) , where period is year-month .
I end up doing a group by ID for the left frame , iterating thru the groups , select the same group on the right frame , and then doing and asof operation for the index of the group from the left dataframe , like so : #CODE
But this function is quite slow . Does anyone have a way to make this merge function faster and more efficient ?

Am I stuck with using a for loop to apply the boolean ' and ' operation on all columns ( i.e. from column A to column Z ) ? To reiterate my question in another manner is there an efficient way or built-in pandas function to ' AND ' a pandas Series on all the columns of a pandas dataframe ?

I have read a lot of Stack Overflow , but can't find much on this topic . It could be I'm not using the right keywords , so if you have suggestions , that could also help . Thanks !
plus the selected rows usage x 2 , which will happen when you concat the rows
after the concat the usage will go down to selected rows usage

I keep trying different things like ` merge ` , ` concat ` , ` dstack ` , etc ...

Did you run #3 ? I'm getting ` RRuntimeError : Error in layout_base ( data , cols , drop = drop ) : At least one layer must contain all variables used for facetting `

The first error says that ` list ` object doesn't have attribute ` ix ` ( and it doesn't have , in fact ): #CODE
Lists and NumPy arrays do not have an ` ix ` method : #CODE
But Pandas Series and DataFrames do have an ` ix ` method : #CODE

Actually , it's strange that there's no possibility to pass ` how= ' cross '` to merge or join methods . May be devs decided that it'll be not used very often .

I'm trying to select out only users that have visited on multiple dates . I'm currently doing it with ` groupby ([ ' UserID ' , ' Date '])` and a for loop , where I drop users with only one result , but I feel like there is a much better way to do this .
` Series.nunique() ` is slower than ` len ( Series.unique() )` , can we consider this as a bug ?
The you can drop users with only one count .

Have you ever used the percentile numpy function when using the pandas function resample ??

Resample data with pandas
When i resample : #CODE
As you see my initial date is 2009-11-17 14:14 : 00 but resample day start at 2009-01-12 . Can anyone explain that happens ?
what you mean low to high ? my dataset is just 30min interval records and with resample i just want to make more readable my data ( 14:14 --> 14:00 , 14:44 --> 14:30 )

Hi @USER , I am getting ` OverflowError : Python int too large to convert to C long ` when I apply your solution . Any suggestion about the reason ?

There is also the ` drop_duplicates() ` method for any data frame ( docs here ) . You can pass specific columns to drop from as an argument . #CODE

This is as expected . The slicing on labels ( start : end ) , only works if the labels exist . To get what I think you are after reindex for the entire period , select , then dropna . That said , the loc behavior of raising is correct , while the [ ] indexing should work ( maybe a bug ) . #CODE

My question , then , is how I can accomplish this while reducing my memory overhead . I think the problem is trying to perform the reindexing with the groupby / apply method , but I don't kow what the alternative is . It seems like there should be way I could do something similar iteratively that would require less memory , but I'm not sure how to go about it .
I would solve this by creating a ` DataFrame ` that just contained the full set of dates needed . Then group the original ` DataFrame ` by ID and join on the date- ` DataFrame ` using ` outer ` , to pull in any missing dates ( and consequently , ` x ` and ` y ` will be ` NaN ` for the rows where the dates had to be pulled in with the join ) .

I had to modify the DIFF lines to add ( and change case on the " or " but in spite of that not working EG : quarter_score_diff = region [( region.Quradate == most_recent_date ) or ( region.Quradate == last_quarter )] .diff() getting ::
Basically all you need is for a diff from prior period #CODE
and for a diff from a year ago ( 4 quarters ) #CODE

Or you can use apply : #CODE

As of version 17.0 , you can format with the ` dt ` accessor : #CODE

I have a Pandas DataFrame that includes rows that I want to drop based on values in a column " population " : #CODE
here , I want to drop the two rows that have ` sibling ` as the value . So , I believe the following should do the trick : #CODE
It does drop 2 rows , as you can see in the resulting value counts , but they were not the rows with the specified value . #CODE
To use ` drop ` you should do : #CODE
You can do a similar thing with a condition over multiple columns : ` data = data [( data [[ ' col1 ' , ' col2 ' , ' col3 ']] ! = 0 ) .all ( axis=1 )]` - to drop all rows with zeros in at least one of those columns .

For all the people who end on this page , please upgrade Pandas to the latest version . I had this exact problem with a stalled computer during load ( 300 MB Stata file but only 8 GB system ram ) , and upgrading from v0.14 to v0.16.2 solved the issue in a snap .

I can fix the data so it's easily plotted with a simple unstack call , i.e. ` df.resample ( ' D ' , how= ' count ') .unstack() ` , but why does calling resample with ` how= ' count '` have a different behavior than with ` how= ' sum '` ?
It does appear the ` resample ` and ` count ` leads to some odd behavior in terms of how the resulting dataframe is structured ( Well , at least up to 0.13.1 ) . See here for a slightly different but related context : Count and Resampling with a mutli-ndex
You can fix the issue by specifying that ` count ` applies to the ` volume ` column with a dict in the ` resample ` call : #CODE

Let me build up the expression ` dfg = df [[ ' bin ' , ' col7 ' , ' col11 ']] .groupby ( ' bin ') .agg ( { ' col11 ' : [ np.mean ] , ' col7 ' : [ count_ones , len ] } )`
I now want to apply some aggregate functions to the records in each of my bin groups ( An aggregate funcitn is something like sum , mean or count ) .
Now I want to apply three aggregate functions to the records in each of my bins : the mean of ' col11 ' , the number of records in each bin , and the number of records in each bin that have ' col7 ' equal to one . The mean is easy ; numpy already has a function to calculate the mean . If I was just doing the mean of ' col11 ' I would write : ` dfg = df [[ ' bin ' , ' col7 ' , ' col11 ']] .groupby ( ' bin ') .agg ( { ' col11 ' : [ np.mean ] } )` . The number of records is also easy ; python's ` len ` function ( It's not really a function but a property of lists etc . ) will give us the number of items in list . So I now have ` dfg = df [[ ' bin ' , ' col7 ' , ' col11 ']] .groupby ( ' bin ') .agg ( { ' col11 ' : [ np.mean ] , ' col7 ' : [ len ] } )` . Now I can't think of an existing function that counts the number of ones in a numpy array ( it has to work on a numpy array ) . I can define my own functions that work on a numpy array , hence my function ` count_ones ` .
Now I'll deconstruct the ` count_ones ` function . the varibale ` x ` passed to the function is always going to be a 1d numpy array . In our specific case it will be all the ' col7 ' values that fall in bin #1 , all the ' col7 ' values that fall in bin #2 etc .. The code ` x == 1 ` will create a boolean ( TRUE / FALSE ) array the same size as x . The entries in the boolean array will be True if the corresponding values in x are equal to 1 and false otherwise . Because python treats True as 1 if I sum the values of my boolean array I'll get a count of the values that == 1 . Now that I have my ` count_ones ` function I apply it to ' col7 ' by : ` dfg = df [[ ' bin ' , ' col7 ' , ' col11 ']] .groupby ( ' bin ') .agg ( { ' col11 ' : [ np.mean ] , ' col7 ' : [ count_ones , len ] } )`

Can't find a way to unstack a dataframe

So upon getting one dF with just current date and one with just last quarter's date the diff of the two dataframes would look like
I find this helpful to see what is actually passed to the apply , which in this case is a frame #CODE
For each region , show me the column-wise diff from 2 periods ago , among the scores #CODE
Diff from 2 qtrs ago , just return me the last value #CODE
What I'm trying to create is R1 diff from prior quarter followed by R2 diff from prior quarter . eg in your example solution , just the last line in each region . I don't get how the lambda knows to go one back for the diff , I also need one to go back 3 for the diff to get the diff from a year ago ... so would that be ( lambda x : x [ -3 ] .diff() ) ?? so this gives me the last row only totRegscore.groupby ( level= ' region ') .apply ( lambda x : x.diff() [ -1 :])
I added some helpful logic , and show how to use diff .

Can you enable the debugger to get a stack trace ? Just pass ` debug = True ` as an argument to ` app.run() ` .

The above works just fine , but I can't understand why I have to wrap the function in a lambda . Based upon the syntax used with transform and apply it seems to me that the following should work just fine : #CODE
But then of course it could be better just to cut out the middle man function all together and simply use an array with list comprhension ....
Passing arguments to ` apply ` just happens to work , because ` apply ` passes on all arguments to the target function .

This is pivot . In [ pandas ] ( #URL ) , in [ numpy ] ( #URL )
I'm still having trouble figuring this one out ( just picked pandas up today ) . Pivot requires an index , but I cannot tell it to keep the current ones ( I assume that whichever function did it would ask me what to put on the blanks- 0 vs . NA for example )
Thanks @USER for test case , you can pivot with : #CODE
@USER great , if I just had a bit more practice with pandas :) tried pivot , but didn't want to add index as column and decided that this is not possible to do with pivot

Comparison and join of two different data frames with strings ?
I would like to compare and then join two data frames with strings based on a common sequence within the strings .

and apply this custom converter #CODE
and apply this conversion function : #CODE
May be some conversion happens when you load data , try check what dt [ ' Time '] output is . I updated my answer with correct conversion function , don't have enough time to dig in previous code to find out where error is ( probably some issue with how pandas stores timedelta ) .

I am trying to normalize my timeperiods for each index in x . But am having trouble for some reason .
` np.shape ( x ) = ( 814061 , 1 )` & ` len ( x.ix [8 498857 ] [ ' received ']) = 2 ` & ` len ( x.ix [8 536896 ] [ ' received ']) = 3 ` ** The index is only 100 unique numbers of different length timestamps ** If that helps .
` x.ix [8 536896 ] [ ' received '] = 2013-09-09 16:27 : 54 , 2013-09-09 17:56 : 15 , 2013-09-09 15:01 : 35 ` and I want to normalize to [ 0 , 1 ] start and stop for all indices .

Then I want to resample and I'm getting strange string concatenations since the numbers are still string #CODE

I apply some functions and generate a new column values to a existing column of Pandas dataframe . However ` df [ ' col1 '] = new_list ` does not work to assign new list to the column . Is it the wrong way and what is the accurate way to apply such operation ?

I don't know why the index method has inconsistent behavior while doing column-wise apply function .
And I want to apply lambda to the second columns , it it saying the Series object can not be apply ? #CODE
When you index with `' B '` you get a series . When you index with ` 1:2 ` or with ` [ ' B ']` , you get a DataFrame with one column . When you use ` apply ` on a series , your function is called on each element . When you use ` apply ` on a DataFrame , your function is called on each column .
and then you can use apply ( you don't have to use ` lambda ` , BTW ): #CODE
If you want to apply ` upper ` to DataFrame , you can use pandas.applymap() : #CODE

Each row is a student . For each school , I want to get the percent of students passing for each school by subject . So I'd like a pivot table with a rows as schools , columns as subjects and values as fraction passing .

You can also use map function instead of generator : #CODE

I have tried the following , but nothing happens ( or the does not stop within an hour ) . What I intend to do is to split the dataframe ( data ) into smaller dataframes and append these to a list ( datalist ): #CODE
Here's a groupby way ( and you could do an arbitrary apply rather than sum ) #CODE
I would sort the dataframe by column `' name '` , set the index to be this and if required not drop the column .

Align two time series data sets on an hourly basis ( Python , Pandas )
I have two data sets that I am trying to compare . One is measured meteorological values which are measured approximately every 15 minutes , but not at a consistent time each hour ( i.e. 12:03 , 1:05 , 2:01 etc . ) . The other data set is modelled data for the location on the hour exactly . I would like to extract the value from the measured data which is occurs closest to the hour mark to join with the modelled data .
I currently have both sets as a DataFrame format and have created an hourly time series to use as an index . Does anyone know of an easy way to align these without looping through all the data ?
Although , I just copied the data from your screen , re-created df from your data and tried to resample using ' ohlc ' . Still returns the same error . I don't need to call any additional libraries do I ? Or call pandas implicitly ( ie . import pandas * ) ?

how to replace elements in a column using pandas
I'd like to replace the elements of ` value ` with ` zero ` but only for the words that match those in ` b ` .
Or you can use ` ix ` selector : #CODE

Pandas Append Behavior
Append is similar to one of python ` list ` , you will get both dataframes " stacked " together . Whether a ` ValueError ` is raised in case of indexes with duplicates is controlled by ` verify_integrity ` param to ` append ` , defaulting to ` False ` . #CODE
For the reference , here is extensive guide on different merge and join means of dataframes
I guess I'm asking a bit more in depth than this . What I really need is a veryify_integrity=True except it skips those that are duplicates but it also checks every column to make sure it is a duplicate not only in index but also content and if it isn't then it will join the df as well
Not sure how else to describe it but what I want is : Merge two dataframes even if they have the same index , but if they do have the same index I want to not merge when all the resulting columns also match . So merge all but those rows that are exact duplicates
I edited out the last one . I'd lose something like 20k rows by merging using combine_first . So the way two comments above is the only way it would work for me . Append wouldn't work as it keeps all the and I'm dealing with like 10m rows that are the same and 2k updated rows and 200 rows earlier than the end of the first df also being different . I also edited with my old way of doing it

Here is a way to create a json format , then literally eval it to create an actual dict #CODE
` map ( list , df.values ) == ast.literal_eval ( df.to_json ( orient= ' values '))` evals to ` True `
If you're sure your leafs won't overlap , replace last line #CODE

Difference between map , applymap and apply methods in Pandas
Can you tell me when to use these vectorization methods with basic examples ? I see that ` map ` is a ` Series ` method whereas the rest are ` DataFrame ` methods . I got confused about ` apply ` and ` applymap ` methods though . Why do we have two methods for applying a function to a DataFrame ? Again , simple examples which illustrate the usage would be great !
Another frequent operation is applying a function on 1D arrays to each column or row . DataFrame s apply method does exactly this :
so using apply is not necessary .
The reason for the name applymap is that Series has a map method for applying an element-wise function :
Summing up , ` apply ` works on a row / column basis of a DataFrame , ` applymap ` works element-wise on a DataFrame , and ` map ` works element-wise on a Series .
strictly speaking , applymap internally is implemented via apply with a little wrap-up over passed function parameter ( rougly speaking replacing ` func ` to ` lambda x : [ func ( y ) for y in x ]` , and applying column-wise )
Thanks for the explanation . Since ` map ` and ` applymap ` both work element-wise , I would expect a single method ( either ` map ` or ` applymap `) which would work both for a Series and a DataFrame . Probably there are other design considerations , and Wes McKinney decided to come up with two different methods .
@USER mentioned that apply works on row / columns , while applymap works element-wise . But it seems you can still use apply for element-wise computation .... #CODE
Good catch with this . The reason this works in your example is because np.sqrt is a ufunc , i.e. if you give it an array , it will broadcast the sqrt function onto each element of the array . So when apply pushes np.sqrt on each columns , np.sqrt works itself on each of the elements of the columns , so you are essentially getting the same result as applymap .
Adding to the other answers , in a ` Series ` there are also map and apply .
Apply can make a DataFrame out of a series ; however , map will just put a series in every cell of another series , which is probably not what you want . #CODE
Also if I had a function with side effects , such as " connect to a web server " , I'd probably use ` apply ` just for the sake of clarity . #CODE
` Map ` can use not only a function , but also a dictionary or another series . Let's say you want to manipulate permutations .
You can compute it using ` map ` . Not sure if self-application is documented , but it works in ` 0.15.1 ` . #CODE

I assume I want to drop the index , and create date , and category as a multiindex then do a groupby sum of the metrics . How do I do this in pandas datadrame ?

I'm using pandas to get hourly data from a dataset with fifteen minute sampling intervals . My problem using the ` resample ( ' H ' , how= ' ohlc ')` method is that it provides values within that hour and I want the value closest to the hour . For instance , I would like to take a value sampled at 2:55 instead of one from 3:10 , but can't figure out how to find the value that is closest if it occurs prior to the timestamp being evaluated against .
I suppose you could create another column that is the Hour and subtract the time in question and get the absolute ( unsigned ) value that you could then do the min function on .. It is not code but I think the logic is right ( or at least close .. after you find the mins ,, then you can select them and then do your resample ..

are you trying to implement inner join ?
Transpose a matrix - ` zip ( *matrix )`
Using ` pandas ` , you can view what you want as a corrupted pivot table . You can get most of the way doing something like #CODE
Then we make a pivot table , using the identity function as the aggregation function because we're only reshaping . This produces a ` DataFrame ` with a ` MultiIndex ` for the column index : #CODE

Using pandas to merge data and output Json
I know I can do it without Pandas , but I'm wondering if I can do it in Pandas . for example can you do a join like ` iq between iq_start and iq_end ` in pandas ?

Pandas DataFrame : Concat and to_excel output
That looks like a bug to me in the treatment of repeated columns . I can reproduce in 0.12.0-559-ga11e143 , and the problem goes away if I renumber the columns with ` q.columns = range ( len ( q.columns ))` . If I make the columns ` [ 0 , 0 , 2 , .., 8] ` , then you see a partial problem .

You have a couple options with pandas . For simple statistics , you can use the resample method on a DataFrame / Series with a datetime index . #CODE

Alternatively you can use 2 dataframes , indexed the same , and combine / merge when needed .

A list of freq aliases is here : #URL

Create a series with ` [ 0 , 1 , ... ]` as the values and then call ` resample ` : #CODE

append columns of a data frame to a different data frame in pandas
I'd like to append the columns ` val1 ` and ` val2 ` of ` df2 ` to the data frame ` df1 ` , taking into account the elements in ` c1 ` . The updated ` df1 ` would then look like : #CODE
Replace ` df2 ` with ` df2 [[ ' val1 ' , ' val2 ' , ' c1 ']]` .
@USER :) : You need to include the ` c1 ` column in ` df2 ` -- otherwise ` pd.merge ` will not know on what shared column to merge . See my ( edited ) comment above .

Thanks , do you think a histogram is possible ? X axis as field and y as 10 , 20 ....? How can I do a hist ?

how to append every row of pandas dataframe to every row of another dataframe
I know I can use iteration to append columns of df2 to each row of df1 , but I am wondering whether there are some more efficient way to do this in pandas , like its concat functions .
If I understand you , you need cartesian product . You can emulate this with merge in pandas : #CODE
@USER I've opened an issue here #URL I think it's easy to add parameter value how= ' cross ' to merge and join . Don't know if devs think it's appropriate , if they're , I think I could create PR :)

In case it's not possible to have a shape like that , the following options for obtaining a good a x b shape would be acceptable for me : fill missing elements with NaN ; truncate the longer rows to the size of the shorter rows ; fill the shorter rows with repeats of their mean value .

I use len function . It's much faster than empty() . len ( df.index ) is even faster . #CODE

You can do vectorized comparisons on the column index directly without using the ` map ` / ` lambda ` combination .

We can replace those ` Versions ` to a common name with : #CODE

You can use the ` diff ` method : #CODE

What you are looking for is the ` melt ` function #CODE

I have a very long spreadsheet ( CSV ) wherein overly long URLs have been moved from the " Website " column to the Notes column a few columns over . Unfortunately , the convention for marking such cells has not been consistent ( some say " see over , " others might say " See over-long url , " but I believe all include " see over " in varying cases ) . I'm using a pretty complex pandas script to create a corrupted pivot table , but I'm still flailing on figuring out all of panda's shortcuts . How would I create a condition to move the notes column cell for a given row into the website column cell on the condition that the latter contains " see over " ? In this case , I also want to blank out the notes column cell ( in other cases , I want to keep that cell intact ) .
Have a look at the [ ` replace `] ( #URL ) method to standardize the naming . You'll get more specific examples if you provide a short sample problem and expected output .
Yep , that's what would happen . ` df4 [ ' Notes '] .str .lower() .str .contains ( ' www ')` is a boolean array . It might have all Trues , all Falses , or a mix . The ` if ` tries to call ` bool ` on that series , basically , but there's no canonical way to decide whether an array of multiple bools should be considered True or not . If you want to do something if every Note contains ` www ` , then you want ` if df4 ( etc ) .all() : ` ; if you want to do something if any of them , then ` .any() ` . That's what the error message is saying , just maybe a little too concisely .

A good way to think about this is that columns and index are the same type of object ( ` Index ` or ` MultiIndex `) , and you can interchange the two via transpose .
Note : this attribute is just a list , and you could do the renaming as a list comprehension / map . #CODE
In Pandas version 0.13 and greater the index level names are immutable ( type ` FrozenList `) and can no longer be set directly . You must first use ` Index.rename() ` to apply the new index level names to the Index and then use ` DataFrame.reindex() ` to apply the new index to the DataFrame . Examples :

Good improvement . I think `` apply ( ... )`` could be achieved more simply by `` replace ( dir_dict )`` . I haven't tested that , but I think that's how replace works .
@USER thanks , have almost no time to practice pandas / python , SO is my only place for now . Changed the code . Also I'll check if replace could be modified to replace missing values to nan

Now I want to normalize ` x ` by row --- that is , divide each row by its sum . As described in this question , this can be achieved with ` x = x.div ( x.sum ( axis=1 ) , axis=0 )` . However , this creates a new DataFrame . If my DataFrame is large , a lot of memory can be consumed in creating this new DataFrame , even though I immediately assign it to the original name .

You can apply a function that tests row-wise your ` DataFrame ` for the presence of strings , e.g. , say that ` df ` is your ` DataFrame ` #CODE
You can take the transpose , call ``` convert_objects `` , which works columns-wise , and then compare the data types to get a boolean key like this : #CODE

Pandas merge hierarchical dataframes and maintain hierarchy

and then you apply it row by row .

I was able to do this in the DataFrame using a lambda function with map ( lambda x : x.lower() ) . I tried to use a lambda function with pd.series.apply() but that didn't work . Also when I try to isolate the column in series with something like series [ ' A '] should it return the index ( although I guess this makes sense ) because I get a float error even though the values that I want to apply the lower method to are strings . Any help would be appreciated .

Tbh I think the for loop could be the best / most efficient way from a dict , although join feels " cleaner " .
What you need is the ` join ` function : #CODE
Indices from the original dataframe will be used as if you had performed an in-place left join . Data from the original dataframe in columns with a matching name in the additional dataframe will be overwritten .

You can also do ` groupby ( ..., as_index=False )` , though buggy with apply in 0.12 , fixed in 0.13 .

You should use loc and do this without chaining : #CODE
See the docs on returning a view vs a copy , if you chain the assignment is made to the copy ( and thrown away ) but if you do it in one loc then pandas cleverly realises you want to assign to the original .

In R's ddply function , you can compute any new columns group-wise , and append the result to the original dataframe , such as : #CODE
In Python / pandas , I have computed it first , and then merge , such as : #CODE
To add multiple columns , you could use ` groupby / apply ` . Make sure the function you apply returns a DataFrame with the same index as its input . For example , #CODE

Why not use dictionary to map ** i ** to ** dir** ?

Indexing DataFrame by Argmax of Column over Another Column
Formerly what I am looking for is to index by the argmax of a column over another column .

( The ` sort=False ` is there just to preserve the original horse order ; if you don't care about that , you can drop it . )

EDIT : I put some print statements in my function and also passed len as the grouping function to see how things changed . I also removed the NA values . No luck . In fact , can someone please explain the following : #CODE

Maybe this is not what agg was intended for . Maybe I should be using apply ...

Use the string methods to truncate to the first 4 ( assuming this won't cause any false matches ) .
I'd look into the ` str ` methods on DataFrames . Loop through the each of the unique locations , check for matches ( the first 3 letters from the other frame ) and replace when there's a match . May be worth asking a separate questions about that specifically .
@USER may be you can just cut column names to 4 letters in both DataFrames
You could use fuzzywuzzy to do the matching of town names , and append as a column to df2 : #CODE
Now you can merge ( on the overlapping columns ): #CODE
Note : to force the dtype to object ( and have mixed dtypes , ints and floats , rather than all floats ) you can use an apply . I would recommend against this if you're doing any analysis ! #CODE
Two notes : First , given how easy it is to use cdf1.Local.value_counts() or len ( cdf1.Local.value_counts() ) etc . I'd strongly recommend adding some check summing to make sure that when shifting from Location to the first few letters of Location , you aren't accidentally eliminating a location . Secondly , I sincerely hope there is a typo on line 4 of your desired output .

Thanks that seems to have fixed that problem . But now I have another error which says " ValueError : Currently , you need to give freq if dates are used . " I suppose that " freq " is another argument that I need to include in AR() . But what does " freq " mean ?

Currently there is a ` median ` method on the Pandas's ` GroupBy ` objects .
Median would be the calcuation of percentile with ` q=50 ` .

I am trying to merge to Dictionaries on a common key ( b ) . I am getting an output I would not expect :

Resample a pandas dataframe by an arbitrary factor
Pandas resampling is really convenient if your indices use datetime indexing , but I haven't found an easy implementation to resample by an arbitrary factor . E.g. , just treat each index as an arbitrary index , and resample the dataframe so that its resulting length is 4X shorter ( and being more intelligent about it than just taking every 4th datapoint ) .
This would be useful for anyone that's working with data that operates on a much shorter timescale than datetimes . For example , in my case I want to resample an audio vector from 44KHz to 11KHz . Right now I have to use scipy's " decimate " function , and then re-convert it back to a dataframe ( using dataframe.apply wasn't working because it changes the length of the dataframe ) .
You can use ` DatetimeIndex ` to resample high frequency data ( up to nanosecond precision , caveat : I believe this is only available in the upcoming 0.13 release ) . I've successfully used pandas to resample electrophysiological data in the 24KHz range . Here's an example : #CODE

I'm still learning pandas , so some parts of this code may be not so elegant , but general idea is - get all notes columns , find all urls in there , combine it with ` URL ` column and then concat remaining notes into ` Notes1 ` column : #CODE

You can apply this per column , but much easier just to check the dtype . in any event pandas operations exclude non-numeric when needed . what are you trying to do ?

` ix ` index access and ` mean ` function handle this for you . Fetch the two tuples from ` df.ix ` and apply the mean function to it : non existing keys are returned as nan values , and mean ignores nan values by default : #CODE

If I may suggestion a some improvements unrelated to your question : 1 ) use ` for i in range ( len ( df ))` . That way , your loop will terminate even if you forget to increment your ` i ` . But as my answer states , you don't even need loops , so the point is moot .

As annoying as those stray decimal places are , they are an unavoidable artifact of storing base 10 numbers in binary . Usually I ignore them , or truncate them as Tom suggested . Is there a reason that you need to store and retrieve the exact number of decimal places ? Please explain more about what you want to do with the data -- the big picture .

This is now pretty competetive with this PR : #URL ( going to merge for 0.13 shortly ) #CODE

This answer solves this toy example and will be enough for me to rewrite my actual function , but it does not address how to apply a previously defined function without rewriting it to reference columns .
You can go with @USER example , if it's possible for you to rewrite your function . But if you don't want to rewrite your function , you can wrap it into anonymous function inside apply , like this : #CODE

I would like to convert that into an hourly time series and interpolate missing values ( 15:00 ) so that I end up with : #CODE
How do I convert / map the dataframe data to a time series in Pandas ?

I need to replace the values in the dataframe with the corresponding values from the map , but keep the original value if the it doesn't map to anything .

Difficulty using Diff on pandas date time series
If I then take the diff of this series : #CODE
instead of ` diff ( timeSeries )` , use ` timeSeries.diff() ` #CODE

In my smaller example for bheklilr if you look at the testing.txt the first line ( one of the 3 things it matches against testing.csv ) it's got ` Mdata ` where as in my csv it's got ` Mdata ( D )` , I want to strip the ` ( D )` parentheses data from the csv MATCH2 column and then match , it doesn't even need to be a regex to know what to strip as there are only 2-3 variations of parentheses data that I need to temporarily strip .

I often find I can map a step in a process to a sequence of vectrorized methods on a Series or DataFrame .

Diff on pandas dataframe with more than one column

would calling pd.to_datetime from apply allow for easier parralelization after import or would you still have to manually split up the data frame into N / M parts ( N = num rows , M = num logical procs ) and execute afterward ? I'm really hoping for some of the straightforward parallelization cases pandas gets n_jobs type support that some scikit-learn functions have ( like gridsearch ) .

Looping a merge for a amount of csv files using pandas
As of right now I'm able to get the files but I'm unable to determine a way to develop a way to iterate a data frame over each read csv and then merge all of these data frames together and push out a csv file .
ID's are different in two files , so do you want to join on ID or just add new columns without looking at ID column at all ?

How to align the bar and line in matplotlib two y-axes chart ?
How to shift the line to align with the bar at the same x-tickers ?
to align them , and the plot now looks ugly :

Pandas Handling Missing Values when going from Data Frame to Pivot Table

I disagree -- I think you are just unfamiliar with the right way to stack data in a DataFrame . Generally , you'd want to use the info in * both * your column 1 and column 2 as indexes , so you can quickly search for data by either . The things in column 2 don't belong as column names , but even if they do , this is a completely separate question than the title of your post . I suppose editing the title could help ( though I still think it's not a meaningfully different question ) .
You can pivot your DataFrame after creating : #CODE
This way it is implicit that you're seeking to reshape the averages , or the standard deviations . Whereas , just using ` pivot ` , it's purely based on column convention as to what semantic entity it is that you are reshaping .
Generally , the main use case for doing what you're trying to do with ` pivot ` is when you're formatting some table so that it prints nicely to the screen , or is exported nicely to HTML , LaTeX , or .csv , or something . Like formatting a table that will go into a presentation or article submission . Otherwise , as far as efficiently manipulating data , you want things to be multi-indexes when you can ( like keys to a database table ) or at least as repeated columns so you can do efficient indexing and joining , etc . But you don't want to blow them out into their own columns .

Python Pandas fuzzy merge / match with duplicates
I tried following this thread : is it possible to do fuzzy match merge with python pandas ? but keep getting index out of range errors ( guessing it doesn't like the duplicated names in fundraisers ) :( So any ideas how I can match / merge these datasets ?

I have tried with loc and map , but I don't manage to find a solution .
Use apply : #CODE

How about not calling add_area_column if the DataFrane is emtpy ? ( e.g. Take the ` if ` out of the ` add_area_column ` and put it where you would call ` apply `)
these edge cases for apply are pretty tricky ... to fix your issue , don't use apply : df [ ' width '] * df [ ' height ']

Insert values into pandas datafrmae based on MUltiIndex
I started by adding a date_order column this ` data [ ' date_order '] = 1 ` and then I tought I could increment over with an iterator using a lambda function and the map function .

Use ternary operator in apply function in pandas dataframe , without grouping columns
How can I use ternary operator in the lambda function within ` apply ` function of ` pandas ` dataframe ?
you can also replace ` ( x == 4 ) .astype ( float )` with ` ( x == 4 ) *1 ` which , for large arrays has been a bit faster in my tests but I'm not sure how generalizable it is .

` shift ` is much better than going down to ` values ` .
Be careful ; depending how your prices data is sorted by date ( asc , desc ) , the shift example would be wrong . ` prices / prices.shift ( 1 ) - 1 ` is for ascending order .

Python pandas time series resample function extends time index
I'm playing around with some financial time series data in pandas , and I'm trying to take a resample of some timestamp data . This is the starting data : #CODE
And when I try a 40-minute resample , the time range is expanded : #CODE

You can do this with ` to_string ` . There is a ` formatters ` argument where you can provide a dict of columns names to formatters . Then you can use some regexp to replace the default column separators with your delimiter of choice .

select from hdf5 apply function ( e.g. mean )
In theory yes , see here . In practice , not at the moment . You would have to drop down to pytables by using the ` store._handle ` to get at the data that is needed . You would also have to handle ` nan ` , for example .

I want to combine ` MEETING DATE ` and ` MEETING TIME ` into one column . datetime.combine seems to do what I want , however , I need to apply this function column-wise somehow . How can I achieve this ?
perhaps you could ` apply ` the function ( or anyfunction you want ) to MEETING DATE and MEETING TIME #URL
You can use apply method , and apply combine like this : #CODE

Get a bool series by compare the difference with 2 hours .
cumsum() the bool series to get a series that can split the dateframe .
call groupby and apply to get the begin and end datetime for every group .

One way is to do a merge : #CODE
* Except for NaN vs . False , but I think NaN is what you really want here . As @USER points out , in python ` False == 0 ` so you may get into trouble with False as the representative for missing vs being found with id 0 . ( If you still want to do it then replace the NaN with 0 using ` .fillna ( 0 )`) .

This was a bug and has been fixed in master ( 0.13 ) , a temporary workaround is to use ix ( ! ): #CODE
awesome , thanks , I searched tracker , but haven't found it . and forgot ix method too :)

join three pandas data frames into one ?
If you are trying to join Series objects rather than Dataframes , then you want #CODE
now I have the following error : AttributeError : ' Series ' object has no attribute ' join '

I'm trying yo interpolate the value ` value ` of column ` result_column ` using ` index_column ` .
Then i append a new row with the index ` value ` #CODE
I am not sure what you are trying to interpolate . I am pointing out that trying to astype the index is not going to work ( as its not designed to be of dtype float ) . Pls read the interpolation docs , they are very complete .

Notebook might be too heavy , can you try on master that shoudl be slightly faster and / or share the notebook ? You can also try to strip the json from outputs , it will decrease notebook size .

Apply Function Along DataFrame Index
What is the best way to apply a function over the index of a Pandas ` DataFrame ` ?

Append string to the start of each value in a said column of a pandas dataframe ( elegantly )
I would like to append a string to the start of each value in a said column of a pandas dataframe ( elegantly ) .

Insert a link inside a pandas table
I'd like to insert a link ( to a web page ) inside a pandas table , so when it is displayed in ipython notebook , I could press the link .

It's going to depend on the size of you DataFrames which is faster , but another option is to use join to smash your multi line " JSON " ( Note : it's not valid json ) , into valid json and use read_json : #CODE
Note : of that time the join is surprisingly fast .

@USER ` TypeError : ' bool ' object is not callable `

ah yep ... that does it . is there a way to make pandas strip elements in CSVs ? that would seem like a fairly common task ( one I just expected to be built in ) .

construction_error ( len ( arrays ) , arrays [ 0 ] .shape [ 1 :] , axes )
tuple ( map ( int , [ len ( ax ) for ax in axes ]))))

I want to create a third dataframe that would be the equivalent of a minus in SQL : dropping all the observations in the intersection of the two dataframes . This would leave me with DF3 : #CODE

Conditional merge for CSV files using python ( pandas )
I am trying to merge ` =2 ` files with the same schema .
- I need to merge multiple CSV files with same schema but with duplicate rows .
- Where the row level merge should have the logic to pick a specific value of a row based on its value . Like phone picked up from file1 and address pickedup from file2 .
One way to smash them together is to use merge ( on store_id and number , if these are the index then this would be a join rather than a merge ): #CODE
Thanks , just tried it out . I get this error on merge : ` ValueError : ' array is too big . '` . Also , how will this work for more than 2 files ? I need to merge res with df3 ?
@USER that's strange ... To get it with more than two frames use join ( passing list of frames and the on value )
How about use ` concat ` , ` groupby ` , ` agg ` , then you can write a agg function to choose the right value : #CODE

And then check for where extra1 or extra2 isnull , keep those rows , and then drop the extra rows . #CODE
If you want to drop the bad lines , you might be able to use ` error_bad_lines=False ` ( and ` warn_bad_lines = False ` if you want it to be quiet about it ): #CODE

Note : I would probably clean up the indentation to make this more readable ( you can also replace df.type with type in most cases : #CODE

Context : I have data stored with one value coded for all ages ( age = 99 ) . However , the application I am developing for needs the value explicitly stated for every id-age pair ( id =1 , age = 25 , 50 , and 75 ) . There are simple solutions to this : iterate over id's and append a bunch of dataframes , but I'm looking for something elegant . I'd like to do a #URL merge from my original dataframe to a template containing all the ages , but I would still have to loop over id's to create the template .
Don't know , may be there's more elegant approach , but you can do something like cross join ( or cartesian product ): #CODE
Thanks ! This is exactly what I was looking for , and I guess I even said the words many to one in my question , but I didn't understand that you could merge like that

And after that you can use pandas.DataFrame.apply function , with axis=1 ( means apply function to each row ): #CODE
Thanks but its not working , Its throwing a traceback and which says : AttributeError : DictReader instance has no attribute ' apply ' , is it because i am reading this as a dictonary ?

@USER oh , it will in 0.13 - there was a bug in apply maybe .

Thanks . I also found this useful for speeding up counting a specific value in a series . e.g. ` df.word.value_counts() [ ' myword ']` is about twice as fast as ` len ( df [ df.word == ' myword '])` .

This will be significantly more efficient than an apply or using lists ...

with a dataframe df in Pandas , I am trying to plot histograms on the same page filtering by 3 different variables ; the intended outcome is histograms of the values for each of the three types . The following code works for me as PLOTs , but when I change to HISTs , they all stack on top of each other . Any suggestions ? #CODE
Is this a bug in Pandas or are plot and hist not interchangable ?
Plot and hist aren't interchangabe : hist is its own DataFrame and Series methods . You might be able to force the plots to be independent by passing in an ax keyword . So #CODE

For any rows with a NaN in a Pandas DataFrame , shift by 1
What I'm trying to figure out is for the columns that have a NaN in the last row ( this being row i , I would like to shift those columns by 1 . )
There might be a way to do this with less duplication , but the following should work , anyway . First , find out which columns we need to shift , and then replace those columns by the shifted versions . #CODE
Shift it : #CODE

I haven't used any alternative file systems so no help there . blosc does use multi processors . I am not sure if index does though I think not . it seems that in your case a local disk is so fast that it's not worth the time to create a table , unless you are doing a lot of queries ( and don't append )

I am trying to merge time-course data from different participants . I am iteratively extracting a dataframe per participant and concatenating them at the end of the loop . Before I concatenate , I would like to add the ID of my participants to an additional index .
It might be better to just add the ID as a column , then add to index when you concat ?
Maybe you can use ` keys ` argument of ` concat ` : #CODE
If you want to append other dataframes later : #CODE
Though the keys can be useful at times , I don't think this actually answers the question . This way another concat cannot be made afterwards , which the question seemed to imply would be necessary . So , e.g. I don't see how you could easily append ` df4 = pd.DataFrame ( np.random.rand ( 3 , 2 ))` with a new key `" D "` after having done the above . [ This answer ] ( #URL ) seems more suited to this purpose .

To break it down , for each row we are calculating the days in each month by creating a Series of 1s for every day between the start and end , then summing them up for each month using resample : #CODE

If you have an arbitrary number of places to split at , then you can use a boolean Series / shift / cumsum / groupby pattern , but if you can get away without it , so much the better .

If you use ` apply ` on the groupby , the function you pass is called on each group , passed as a DataFrame . So you can do : #CODE
However , this will raise an error if the group doesn't have at least two rows . If you want to exclude groups with fewer than two rows , that could be trickier . I'm not aware of a way to exclude the result of ` apply ` only for certain groups . You could try filtering the group list first by removing small groups , or return a one-row ` nan ` -filled DataFrame and do ` dropna ` on the result .

I doubt wether it's more efficient to do this in pandas . There is no built-in queue-behaviour as far as i know ( but you could write your own wrapper around a pandas data-frame using the concat method and / or using index-slices )

What will a NULL in the data " mean " semantically for your code ? Could you possible replace the NULLs with a minimum date value or similar ?
The NULL values should translate to a NaT value in Pandas . This is what I get with my workaround seen under the " UPDATE : " heading

python pandas - replace number with string
You could also try to use a " map " function . #CODE

pandas group by n seconds and apply arbitrary rolling function
The accelerometer data is not uniformly sampled , and I want to group data by every 10 or 20 or 30 seconds and apply a custom function to the data group .
If the data was uniformly sampled , it would have been easy to apply a rolling function .
However , since it is not , I want to apply groupby using timestamp interval .
However , I cannot figure out how to group by an arbitary number of seconds and then apply a function to it .
Or have a look at the resampling-functions here . Maybe you could apply a custom resampling-function instead of using the groupby-method . #CODE

python : shift column in pandas dataframe up by one
shift column gdp up : #CODE

In this particular case , since the returned result is a Series , you can make use of a trick using ` unstack ` : #CODE
I might have done something like ` agg_df.reset_index() .groupby ([ " id "]) .first() [ 0 ] .tolist() ` , but the ` unstack ` trick is nice .

Thanks Thomas but no - I was looking for an OCaml pure solution , for the very reason that I have a complex set of algorithms already written in R , and I find it to be much too slow , especially at data manipulation . Thus I wish to cut R out of the loop completely .

This is pivot , if I get your idea right , with pandas it will be as follows .
Pivot it : #CODE
pivot is based on this answer #CODE
if i have text file can i just read in the data vy np.genfromtxt() ? Then perform the pivot ?
@USER It's not that you asked poorly , it's that I wasn't familiar with a pivot table until alko used the word and I was able to look it up :P

yes , this isn't working ` df.ix [ ' bar ' , ' two ']` . Or wasn't actually , apparently your code would not work if [ ' Trial '] was already set as an indey when it was run . Strangely enough , after I run you code , df [ ' Trial '] no longer works :( which is a pity because I wanted to do this in order to better apply the same function to multiple trials ( I want to downsample all of the measurements in every trial to two - just two ) .

If you want to replace the index with simple sequential numbers , use ` df.reset_index() ` . I strongly suggest reading a little bit of the pandas documentation , like 10 minutes to Pandas to get a sense for why the index is there is how it is used .

Resample pandas dataframe only knowing result measurement count
I want to resample my data so that every trial has just two measurements
I know pandas has a resample function , but I have no idea how to apply it to my second-level index while keeping the data in discrete categories based on the first-level index :(
How does ( 0 , 1 ) become 8 and not 12 but ( 2 , 1 ) becomes 6 ( taking NaN as zero and not as missing data ) ? What's your resample rule ? The mean of the first half of the data and the mean of the last , allowing overlapping , and setting NaN to 0 ?

and filter needed rows with ` ix ` : #CODE
@USER you can use sort=False param , and ix / idxmax instead of max . see updated code

if it's relevant that you have Timestamp columns , e.g. you're resampling or something , then be explicit and apply ` pd.to_datetime ` to them for good measure** . #CODE
Cut the intro , and just show the relevant DataFrames ( or small versions of them ) in the step which is causing you trouble .

I could be misreading the code , but why do you need to ` concat ` at each iteration of the loop ? Can ` data_all ` just be a list , and you ` concat ` at the end of the for loop ?
Also keep in mind that pandas ' ` concat ` works with iterators . Something like ` yield group ` may be more efficient than appending to a list each time . I haven't profiled it though !

many thanks , I actually browsed around for this a lot , but " make multiindex to column " and similar queries always got me threads which wanted to pivot their dataframes ...

How to find out about what N the resample function in pandas did its job ?
I use the python module ` pandas ` and its function ` resample ` to calculate means of a dataset . I wonder how I can get to know about what N the resampling for each day / each month takes places .

I have a dataframe with 2 columns Address and ID . I want to merge IDs with the same addresses in a dictionary #CODE

Now just apply usual pandas transformations and delete unneseccary columns : #CODE

define aggfunc for each values column in pandas pivot table
Was trying to generate a pivot table with multiple " values " columns . I know I can use aggfunc to aggregate values the way I want to , but what if I don't want to sum or avg both columns but instead I want sum of one column while mean of the other one . So is it possible to do so using pandas ?
Now this will get a pivot table with sum :
You can concat two DataFrames : #CODE
update Actually , in your case you can pivot by hand : #CODE
You can apply a specific function to a specific column by passing in a dict . #CODE

I'm trying to replace the location " Lobur " with " LON " as shown below : #CODE
If you want to chain the string methods , you have to insert another ` .str ` : #CODE
you can use apply : #CODE

append the output to the empty dataframe
brilliant ! I'm still looking at what the combination of apply , lambda , pd.Series and stack does , but it works exactly as intended . thanks !

This will give you a multiindex with the timestamp and a date . If you don't want the index to be permanent , drop the inplace= argument .
You can use the ` normalize ` DatetimeIndex method ( which takes it to midnight that day ): #CODE
In 0.15 you'll have access to the dt attribute , so can write this as : #CODE

Pandas groupby apply function that combines some groups but not others
I'm using pandas ` groupby ` on my DataFrame ` df ` which has columns ` type ` , ` subtype ` , and 11 others . I'm then calling an ` apply ` with my ` combine_function ` ( needs a better name ) on the groups like : #CODE
Have you tried just using an apply ?

Resample Pandas Dataframe with " bin size " / " frequency "
9I have a multi-indexed dataframe which I would like to resample to reduce the frequency of datapoints by a factor of 3 ( meaning that every 3 rows become one ) .
I tried to turn my time column into a pandas datetime index like so , and then use resample : #CODE
But the first line of that gives me actual dates ( 1970-something ) which is quite unhelpful for the second line . Browsing arund stack overflow I found some similar quiestios which all had solutions NOT based on panda's resample - and also , sadly , not viable for my use case .
The trick here is to separate your question , you really have two problems : small ints for to 1970 something ( because that is the start of epoch time ) , and something about resample ( it not working is no suprise given the first issue ) .
So we can use helper function like this and apply it to each group to get desired results . #CODE

You could move ' col1 ' and ' col2 ' to the index and then unstack them . This assumes you only have one ( col1 , col2 ) combination per timestamp . For your original ` df ` : #CODE
Using ` pivot ` is probably more appropriate then the unstacking as shown above : #CODE

When apply a function to a list , it occurs " TypeError : ' Int64Index ' object is not callable "
I have tried on some simple list like : ` x =[ 0 , 1 , 2 ] , titleNot0 ( x )` . It works . But if I apply the function to the groupby , it returns " TypeError " . Please help me to fix it . Thank you !
maybe it's because when you apply this to a list ` ls [ x ]` returns an integer , when you apply this to a DataFrame , ` ls [ x ]` returns a Series .

Is there a way to drop those entries that do not belong to the respective months and provide a new time-index for each day ? I need to keep the zeros within a month .
First I would replace the 0s with NaN ( to symbolise missing data ): #CODE
Use this with a groupby apply : #CODE

apply a function to a groupby function
I want to count how many consistent increase , and the difference between the first element and the last element , on a groupby . But I can't apply the function on the groupby . After groupby , is it a list ? And also what's the difference between " apply " and " agg " ? Sorry , I just touched the python for a few days . #CODE
The ` apply ` method calls ` foo ` once for every group . It can return a Series or a DataFrame with the resulting chunks glued together . It is possible to use ` apply ` when ` foo ` returns an object such as a numerical value or string , but in such cases I think using ` agg ` is preferred . A typical use case for using ` apply ` is when you want to , say , square every value in a group and thus need to return a new group of the same shape .
The ` transform ` method is also useful in this situation -- when you want to transform every value in the group and thus need to return something of the same shape -- but the result can be different than that with ` apply ` since a different object may be passed to ` foo ` ( for example , each column of a grouped dataframe would be passed to ` foo ` when using ` transform ` , while the entire group would be passed to ` foo ` when using ` apply ` . The easiest way to understand this is to experiment with a simple dataframe and the generic ` foo ` . )
The ` agg ` method calls ` foo ` once for every group , but unlike ` apply ` it should return a single number per group . The group is aggregated into a value . A typical use case for using ` agg ` is when you want to count the number of items in the group .
you could jus use lambda in apply like that :
if you subtract values of a particular cells , there's no difference between agg and apply , they both create a one value for each group #CODE

you can also load the CSV with separator ' ^ ' , to load the entire string to a column , then use split to break the string into required delimiters . After that , you do a concat to merge with the original dataframe ( if needed ) . #CODE

Final result where we fill the nans to 0 , then astype to bool to make True / False . Then concatate

Concat Pandas Time Series gives AttributeError : ' TimeSeries ' object has no attribute ' _data
When concat'ing two pandas Time Series object ` resample `' ed ` prices ` and ` amounts ` together , Python throws an error .
ohlcPrice is a frame ( this is what ohlc on a series does ) , so u r trying to concat a frame and a series ; try DataFrame ( ohlcVolume ) in the concat

ah , snap ! I was beaten to it ...

When I run this code on this platform ( RHEL 6.4 ) i get the following stack trace . #CODE
Thanks to alko , who asked for a smaller dataset , I found an issue with the dataset that the newer Fedora19 stack was ignoring .
For other folks , be aware of silent dataset repair that seems to take place on the newer stack on Fedora19
The stack I have on RHEL6.4 below works fine , so this is solved

Insert a Pandas Dataframe into mongodb using PyMongo
What is the quickest way to insert a pandas DataFrame into mongodb using ` PyMongo ` ?
` db.myCollection.insert ( records )` should be replaced by ` db.myCollection.insert_many ( records )` see warning ` // anaconda / bin / #URL DeprecationWarning : insert is deprecated . Use insert_one or insert_many instead .

Just set the index before writing to csv . ` df.index = np.arange ( 1 , len ( df ))`
You can shift this index by ` 1 ` with #CODE

You know to merge them and to update the names so that I don't have to remember which one is ` Id ` and which is ` id ` , a few lines earlier I had called #CODE

Thanks . It certainly looks like 0.12.0 on both 2.7 and 3.0 . So in future releases it will be immutable ? I was using the second answer found here : #URL It looks like this way of creating a cartesian join will not work in future ?

Thanks ! That's exactly what I needed . I couldn't for the life of me get that pivot to work before .

It's a bit kludgy since I have to insert the name of the dataframe and the aggregation column into the function , which I would prefer to avoid .

The problem is that pd.Index does not call the eq method on the WrapStr instance but instead just checks if the two instances are the same . #CODE

or , very similarly , with `` filter `` like so : `` df.groupby ( level=0 ) .filter ( lambda x : len ( x ) > 1 ) [ ' type ']`` . Will be faster than transforming and masking .

How can a pandas merge preserve order ?
I have two DataFrames in pandas , trying to merge them . But pandas keeps changing the order . I've tried setting indexes , resetting them , no matter what I do , I can't get the returned output to have the rows in the same order . Is there a trick ?
Note we start out with the loans order ' a , b , c ' but after the merge , it's " a , c , b " . #CODE
For one thing , you need to pass ` sort=False ` or it will sort on the join key , which you don't want . But that isn't sufficient to solve the problem ; the unsorted order is still going to group together all rows that came from the same source row . One easy workaround is to do ` x.merge ( x.merge ( y , how= ' left ' , on= ' state ' , sort=False ))` , which will merge each row in ` x ` with the corresponding for in the merge , which restores the original order of ` x ` . But hopefully there's a better solution that's escaping my brain at the moment .
First , you're asking it to sort based on the join keys . As the docs explain , that's the default when you don't pass a ` sort ` argument .
Second , if you don't sort based on the join keys , the rows will end up grouped together , such that two rows that merged from the same source row end up next to each other , which means you're still going to get ` a ` , ` c ` , ` b ` .
The fastest way I've found to merge and restore order - if you are merging " left " - is to include the original order as a column in the left dataframe before merging , then use that to restore the order after merging : #CODE
x [ " Order "] = np.arange ( len ( x ))

Pandas Resample Strange Zero Tolerance Behavior
I'm attempting to resample a time series in Pandas and I am getting some odd behavior : #CODE
I would have expected the last value in the resampled series to be either zero or omitted . Is this expected behavior for the resample function ? If helpful I can post the full time series , but it is a bit long ...

What are some alternatives for overcoming a merge fail in Pandas that is to big to fit in memory ?
I am trying to merge a couple of DataFrames using pandas merge ( by left index ) and it fails because the machine is running out of RAM .
Hey @USER , this worked great . I just have one comment for future readers which I couldn't add to your referenced solution - the merge function would probably need to change , meaning a left join between two ' full ' dataframes would turn into an inner join , since not all indices would be available for each ' small ' merge . Hope I managed to explain myself clearly and that this would help . Thanks again .

Join two pandas dataframes with only end dates
I've looked at ` Series.asof() ` and the ` ordered_merge() ` functions but I can't see how to both join on the ` symbol == from_symbol ` clause , and use the ` end_date ` to find the first entry . The ` end_date ` is inclusive for the join .
Don't know if there's more elegant way to do this , but at the moment I see 2 ways of doing it ( I'm mostly use SQL , so these approaches are taken from this background , since ` join ` is actually taken from relational databases , I'll add SQL syntax also ):
Join , then take first row .
If there're could be duplicates , you can create DataFrame ` mapping2 ` like above and join on it .
Apply
SQL ( actually , SQL Server ) way would be to use ` outer apply ` : #CODE
Could you also reindex the mapping df and fill backward . So you would then have all of the dates prior to the end date for which the mapping applied and simply merge on ' symbol ' and ' date ' ?
The second version doesn't work well on my actual data set , which has some data which doesn't match ( and so throws errors on the lookup ) , so I can't easily test the performance . The first is running at around 50 seconds for 250,000 rows in the price table and 1,000 rows in the symbol map .

I think there's no way around doing groupby twice . Then join . #CODE

@USER I don't completely get your question . For testing purposes you can add ` print line ` in a loop to check for skipped lines , or replace ` df = ... ` for ` print source.read() ` to check what's left . Don't forget to replace things back when running your batch .

use ` drop ` method : #CODE
@USER it's only rep nothing major plus I know this is an answer that is correct , others are free to think different but it doesn't make me think I should delete or edit this answer . In fact dropping a column would delete the column if you assigned the result of drop to itself e.g. ` df = df.drop ( col , axis=1 )` .

The itemsize is created on the first append ( and cannot be changed later ) . If ` min_itemsize `
is not specified it will be the max length of strings in that append . #CODE

As you manually drop table ( the same behaviour can be achived with ` recreate=True ` param ) , it seems that reason lies in creation statement , which for your case is generated as #CODE
and emptifying it not with ` drop ` statement ( this is a bad practice from dba point of view ) , but with ` delete from pattern ` statement .

How would you modify this for the case where x has multiple columns ? Also , why do I get the error " File " / usr / local / lib / python2.7 / dist-packages / pandas / core / series.py " , line 3176 , in interpolate
TypeError : array cannot be safely cast to required type " on the interpolate line ?

Compute the usual rolling mean with a forward ( or backward ) window and then use the ` shift ` method to re-center it as you wish . #CODE

I would like to apply it to the " col1 " column of a dataframe similar to : #CODE
If the keys in ` di ` refer to ` df [ ' col1 ']` values , then @USER and @USER show how to achieve this with ` replace ` : #CODE
[ `` replace ``] ( #URL ) is equally good , and maybe a better word for what is happening here .
@USER : Stay away from ` apply ` ( especially with ` lambda `) if you can help it . It is likely to be the slowest solution available .
nope , none of the ` replace ` variants work and they all give me a column filled with ALL the values in the dict ( even if the keys to call some of them are not in that column ) .

pandas dataframe interpolate
So I see that there is a way to interpolate for a pandas series object #URL . Is there an equivalent method for dataframes ? I'd like to interpolate every column .
Does this apply it to the columns or rows ? I tried df =d f.apply ( lambda col : col.interpolate ( ' linear ') , axis=1 ) , yet it's still not interpolating all the columns .
Normally different columns in a pandas DataFrame contain different type of information , so an interpolation method may not apply or you may need different methods depending on the data .
Supose a DataFrame called data , with columns ' TimeStamp ' , ' Lat ' , ' Lon ' , ' Value ' . You can interpolate NaN values of each column by doing : #CODE
More information about the interpolate function :

In any event , what I recommend ( until openpxyl comes a tad further ) is to map the formulas to a new xlsxwriter.Workbook object . I've had success using that module to create new xlsx workbooks ( with formatting and formulas ) , and without knowing how well the formats will translate from the openpyxl object to the xlsxwriter one , I believe it will be a viable solution for preserving at least the formulas .

How to resample timedeltas ?
I would now like to load this into Pandas to resample and plot the measurements . I've done this before , but those times my timestamps have been since epoch or in datetime ( YYY-MM-DD HH : #URL ) format . If I'm loading my first column as integers I'm unable to do #CODE
. It also does not seem possible if I'd convert my first column to ` timedelta ( seconds= ... )` . My question is , is it possible to resample this data without subverting to epoch conversion ?
Is there a way to resample like this if the time column contains datetimes rather than ints ? I tried using this with datetimes and got ` TypeError : incompatible type for a datetime / timedelta operation [ __floordiv__ ]`

I'm sure this should be easy and something to do with resample . The trouble is I don't want to do anything with the dat , just select the data frame ( to correlate it afterwards )
You don't need to resample your data unless you want to aggregate into a daily value ( e.g. , sum , max , median )

You can use a combination of groupby and apply : #CODE
I drop ' Location ' at in the last line because groupby inserts the grouped levels into the first positions in the index . Sorting and then dropping them preserves the sorted order .

Here is an example based on your data using the hist method of a pandas.Series

pandas dataframe shift dates
I have a dataframe that is indexed by dates . I'd like to shift just the dates , one business day forward ( Monday-Friday ) , without changing the size or anything else . Is there a simple way to do this ?
You can shift with ' B ' ( I think this requires numpy > = 1.7 ): #CODE
Thanks for the quick response . I know I didn't ask this in my original question , but how would you shift just 1 column along with the dates ( leaving all other columns as is ) ?
I think you'll want to shift the column / series , and then set that to a column in the DataFrame . That is , ` df [ col ] = df [ col ] .shift ( 1 , freq= ' B ')` .

Python Pandas : Using Aggregate vs Apply to define new columns
But switching aggregate for apply seems to work .
Why does apply work and not aggregte ?
Good question . Actually , if you define some test function like ` def test ( x ): print x ; return x.sum() ` and call ` aggregate ` in both cases , you'll see that in first case ` x ` is a DataFrame and in second case ` x ` is a Series ( and when you call ` apply ` , it's always DataFrame ) . I don't have time to dig into the code at the moment , and I'm sure some pandas developers will show up and explain this behaviour :)
I have struggled to work out what is going on exactly with these groupby operations . As Roman points out , the first argument passed to agg is a series , therefore if you want to agg based on values in multiple columns you have to call the second column in the function based upon the index values of the series that is passed automatically . apply always passes as data frame as he points out . If you want to see some really strange behaviour check out transform , it seems to pass series and dataframes as the first argument to the function . Quite confusing IMO
@USER Hayden it sounds like the best approach is just to switch to apply or attempt to use cythonized functions when aggregate() fails ? Also , I imagine the groupby code being ' hairy ' doesn't mean you think its unreliable ? Seems to consistently match results I get in SQL . Thanks .

` toAdd ` and ` toRemove ` will then be used to insert and remove documents from the mongodb database . In ` toAdd ` we will have a list of DataFrames for inserting into the database . In ` toRemove ` we will have a list of the ` _id ` of the documents that should be removed from the database .

Maybe you could replace all double whitespaces with a tab and use the tab as a delimiter . Alternatively , if you only need the first seven columns , simply remove the remainder before giving it to the dataframe .
Not yet , trying your approach i've trouble to understand how replace the separator , without looping trough the file line by line ( my real data can hundred thousands lines ) . Once i got this resolved i can try to use " linecache.getline " in order to generate the list of " names " : header = linecache.getline ( r , 24 )
Hm , I can't see other problems in your data besides the lat / long at the moment . You could read the header line and replace lat / long with your approach above , `' LatD ' , ' LatM ' , ' LatS ' , ' LonD ' , ' LonM ' , ' LonS '` , read the file from just below the header , using whitespace as a delimiter and apply the previously read and amended header line as an index to the new dataframe .
pass ` header=0 ` to be able to replace existing names . The header can be

Another question on grouping items in pandas . Currently I am grouping them using the merge function with the stack function in the following code : #CODE
Where are you getting concat from ?
The problem with join is that you end up with duplicate rows because of the non-unique index .
Yep - I had to add numbers to the type index to make it work , which isn't very elegant . On the other hand , it allows you to control which records within a type align in a row .
What you tried was almost correct , you only needed a ` axis=1 ` in ` concat ` ( and no stack ) . But the problem with your dataframe is that you have a non unique index , so ` concat ` cannot know how to concatenate the two dataframes along that axis ( you have eg multiple ' SS ') .
And then you can just concat the two with ` axis=1 ` and the ` keys ` you provided : #CODE
You can always drop the ` count ` again with ` merged.index = merged.index.droplevel ( 1 )` .

python dataframe pandas drop column using int
I understand that to drop a column you use df.drop ( ' column name ' , axis=1 ) . Is there a way to drop a column using a numerical index instead of the column name ?
Drop multiple columns like this : #CODE

+1 because I like to encourage addressing of numpy and pandas objects with martix type syntax . ( df.shape vs len ( df.columns )) . Truth be told , if you look at the pandas descriptor for shape , it calls len ( df.columns ) but numpy arrays and matricies have them as an attribute . most efficient vectorized operations can be done with regular python syntas as opposed to vectorized operations and is almost always wrong ( numba / jit operations excepted from that criticizm )
Wouldn't ` len ( df )` give you the rows ?
In this particular case : if you learn that the number of rows can be calculated with len ( df.index ) , next time you need the number of columns it comes natural to do len ( df.columns ) .

The problem is that I am calculating the distance among 83.803 boolean vectors , and the result is quite big , which makes difficult to go through all the cells of a 83.803 vs 83.803 matrix and insert its values . Keeping it in mind I would like to know if there isn ` t any " more intelligent " way to do it ! A started working with pandas a couple of weeks ago and realized that it's philosophy is to avoid iterating through cells and deal with it as a whole , and thats why a imagine that there is a simple way to load condensed matrixes directly .

merge multiple data frames ( more than 2 ) in Python
I would like to merge as runing below codes , however the output is not a data frame but the attributes of object , I am still fresh with Python . Somebody shade me a light ? Thanks lot . #CODE

Pandas clean column and apply optional multiplier
I can find some of this by using ` \d{3}k ` however I'm unsure how to replace the ' k ' part with the three ' 000 ' without affecting the first ` \d{3} ` . And how can I manage to get rid of the trailing characters ? At the same time , before or after ....
You can use the string methods on pandas objects to strip the ` s `' s and replace the ` k `' s and ` K `' s with ` 000 ` . #CODE

Depending on the size of the file , one solution can be to pre-process the file in memory to replace all the occurrences of ` || ` with ` |NaN| ` using python ` io.StringIO ` #CODE

Interpolate on the fly to get previous valid entry from pandas DataFrame

You could just ` append ` if 2 isn't in the column : #CODE

So , you can apply a function to your data frame to do this ... #CODE

You need to store as a ` table ` ( ` put ` stores in ` Fixed ` format , unless ` append ` is specified ) . #CODE

Insert or replace like operation for Pandas HDFStore ( table )
I'm trying to figure out how to perform an " Insert or Replace " like operation on a pandas hdf store ( table format ) . Is there an efficient way of doing this ? I'm only considering matching on the index .
@USER Thanks ! In my use case I have a bunch of raw data files ( sessions ) that I ETL into a larger store . Pandas + Pytables does great , except when I have to go back to a previous session and reprocess it for some reason . In such a case I can not append but must first destroy the old entries . I found that tricky with Pandas . Also , I ran into some grief when I tried concurrent reads with an HDF5 store which I think is not an issue with sqlite
concurrent reading is OK ; but make sure that you are NOT writing at the same time . I do what you are describing all the time . I will simply rewrite the files ( just read them in chunks , replace as needed , write as chunks ) . Your data flow is much more efficient to create essentially ' read-only ' data for the most part .

Pandas Pivot table nearest neighbor
I have a CSV which is sorted by a few indexes . There is one index in particular I am interested in , and I want to keep the table the same . All I want to do is add extra columns which are a function of the table . So , lets say " v " is the column of interest . I want to take the " z " column , and add more " z " columns from other places in the table where " c " = " c+1 " and " c-1 " and " d+1 " , " d-1 " , and just join those on the end . In the end I want the same number of rows , but with the " Z " column expanded to columns that are " Z.C-1.D " , " Z.C.D " , " Z.C + 1.D " , " Z.C.D-1 " , " Z.C.D +1 " . If that makes any sense . I'm having difficulties . I've tried the pivot_table method , and that brought me somewhere , while also adding confusion .

In particular these would be 2d numpy arrays but they could be any non-numeric type . Now I want to pivot my dataframe . Is there a way to pass an aggregating function of my choice which works on these objects ? I don't seem to be able to do it and I get the error :
You can group first and then pivot : #CODE

As far as it goes , it looks like ` std() ` is calling ` aggregation() ` on the ` groupby ` result , and a subtle bug ( see here - Python Pandas : Using Aggregate vs Apply to define new columns ) . To avoid this , you can use ` apply() ` : #CODE

Why does function behavior used within pandas apply change ?
Now , using ` apply ` and ` to_integer ` with ` df1 ` : #CODE
But if I apply it to this ` df2 ` : #CODE

make dict to apply same function to all columns #CODE

Clearly the matrix will be triangular . Is there a way to pivot the matrix into a molten data table of the form : #CODE

My Dataframe looks like this and i have to insert a few rows
For dates where the accesstype 3 is missing i have to insert rows
Hi i tried it out in Python 2.6.6 the replace command does not work there , but in python 2.7 it works fine .. any help please feel free

@USER yeah , mean ` apply ` with ` count ` but you've already added that .

Resample daily pandas timeseries with start at time other than midnight
I can do a nasty work around by ` shift ` ing the data first , but it's unintuitive and even coming back to the same code after just a week I have problems wrapping my head around the shift direction !
I've just spotted an answered question which didn't come up on Google or Stack Overflow previously :
Resample hourly TimeSeries with certain starting hour

I cannot figure out how to do it if the columns on which I want to join are not the index . What's the easiest way ? Thanks !
I was not sure from the question if you only wanted to merge if the key was in the left hand dataframe . If that is the case then the following will do that ( the above will in effect do a many to many merge ) #CODE
right , but there is a [ bug ] ( #URL ) in panads that will break left join
for your information , in pandas left join breaks when the right frame has non unique values on the joining column . see this bug .

Partial merge in Pandas
E.g. , column to join on = ' number ' . In case both df1 and df2 has non-NaN value on overlapped column ( like ' price ') , df2 will be preferable . Otherwise , one with non-NaN should be written .
You can use ` numpy.where() ` after merge : #CODE
Summary : ` merge ` followed by a little DataFrame manipulation and then ` update ` .
Next merge the two data frames on `' number '` #CODE
If a more complicated set of rules for choosing values were desired then replace ` mdf.update ( pdf )` with ` mdf.combine ( pdf , function_of_two_variables_returning_preferred_value )` .

Granted that the behavior is inconsistent , but I think it's easy to imagine cases where this is convenient . Anyway , to get a DataFrame every time , just pass a list to ` loc ` . There are other ways , but in my opinion this is the cleanest . #CODE
We would like to be able to insert and remove objects from these

Thanks @USER . Can you shed any light on why the join is reported as being ambiguous ? It's a rather opaque error given the circumstances ! ( p.s. maybe it's finally time to do my first fork of the source code ! )
I don't think it's ambiguous more that it just needs to be implemented ( their might be some ambiguity if the levels could match in multiple ways ; prob have to assume that the order of the levels is the join order ) . this is pretty analogous to doing a straight frame-frame join I think so could be straightforward .
another way to approach this is to reset the index on both frames , then join them , set the index and multiply
Move the non-matching index level ( s ) to columns using ` unstack `
Put the non-matching index level ( s ) back using ` stack `

Using pandas to drop all observations in a group after a condition has been met
I have a pandas data frame that is basically an unbalanced panel . All I want to do is , for each group , drop all observations after a condition has been met . So given this dataset : #CODE
` .values ` get the raw ndarray values , I use this because there are some bugs in my pandas version ( 0.12 ) that can't cumsum bool Series . This is fixed in 0.13 , so you can use ` .cumsum() ` method of Series object .

The Mean Abs Deviation column is to be performed on the Rev / Unit column in the first table . The tricky thing is taking into account the respective weights behind the Rev / Unit calculation .
Is there any way to insert a function into the agg method ? See edited OP for more details

There's ` fill_method ` parameter in the ` resample() ` function , but I don't know if it's possible to use it to replace ` NaN ` during resampling . But looks like you can use ` how ` method to take care of it , like : #CODE
I think I would use a resample ( note if there are dupes it takes the mean by default ): #CODE

Pivot table using Python Pandas , unit price an sum
I would like to generate a pivot table using py-pandas from this kind of data #CODE

Simple way to append a pandas series with same index
Is there a simple way to append a pandas series to another and update its index ? Currently i have two series #CODE
and i can append ` b ` to ` a ` by #CODE
you can also use ` concat ` with ` ignore_index=True ` : ( see docs ) #CODE

It should give me a dataframe where all the quoted guys are strings . Most likely pandas will make everything else np.float64 , but I could deal with it afterwards . I want to wait with using ` dtype ` , because I have many columns , and I don't want to map types for all of them . I would like to try to make it only `" quote "` -based , if possible .

Apply SequenceMatcher to DataFrame
You have to apply a function , not a float which expression ` SequenceMatcher ( None , str ( m.ITEM_NAME_x ) , str ( m.ITEM_NAME_y )) .ratio() ` is .
Thanks ! Is there any way that I could add the SequenceMatcher results as a new column in the existing dataframe ? Otherwise I would have to merge the results back in .

One solution is to concat shifted Series together to make a DataFrame : #CODE

join two CSVs by IDs
Is is possible to join these two CSVs to make a CSV in similar format using pandas ?

Update : I realised what you were actually asking , and I think this ought to be an option in sortlevels , but for now I think you have to reset_index , groupby and apply : #CODE

If you're worried about duplicates ( join nth place ) you can take the head : #CODE

you should be able to use pivot to accomplish what do you want

I am working with a large dataset in CSV format . I am trying to process the data column-by-column , then append the data to a frame in an HDF file . All of this is done using Pandas . My motivation is that , while the entire dataset is much bigger than my physical memory , the column size is managable . At a later stage I will be performing feature-wise logistic regression by loading the columns back into memory one by one and operating on them .
But after that , I get a ValueError when trying to append a new column to the frame : #CODE
Stack trace and error message : #CODE
PyTables is row-oriented , so you can only append rows . Read the csv chunk-by-chunk then append the entire frame as you go , something like this : #CODE

but it tells me there is no attribute ' first ' when I apply the same thing to the index . #CODE
You could use ` groupby / apply ` : #CODE

It makes sense to put the data into a multi-indexed dataframe ; however , I would like to selectively extract the data ( e.g. pull out data from station " A " , then truncate within specified date range , then calculate stats ) . However , I am having difficulty figuring out how to trunctate a multi-indexed dataframe and begin to believe it is not possible . Posted below is an example dataframe to give an idea of what I am dealing with . #CODE
Is it possible to truncate via a tertiary index ?
I have considered converting the data to a pivot table , but I do not believe that is the best route to go . In this case multi-indexing makes the most sense IF I can sort out the date issue .
you could use ` ix ` as in : #CODE

What is pythonic way to do dt [ , y : =myfun ( x ) , by=list ( a , b , c )] in R ?
In R in ` data.table ` this is just 1 line : ` dt [ , y : =myfun ( x ) , by=list ( a , b , c )]` .
The exact usage depends on how you wrote your function ` myfun ` . Where the column used is static ( e.g. always ` x `) I write ` myfun ` to take the full ` DataFrame ` and subset inside the function . However if your function is written to accept a vector ( or a pandas ` Series `) , you can also select the column and ` apply ` your function to it : #CODE

I am following some examples from the book Python for Data Analysis , and came across an interesting problem . I was wondering if there is a way to " reverse " of a colormap , for example from a blue extreme to a pink extreme to the opposite ( pink to blue ) for the ' cool ' color map .
In the example below , I chart baby names using the ' cool ' color map , that shows girls in blue color and boy in pink color . Is there a way to ' reverse ' the mapping ?

To add a symbol , transpose , add and transpose back #CODE
these are all inherently slow operations because you are doing lots of little indexing operations . instead it is much more efficient to either construct a panel directly from a multi level dict or concat frames . if you have lots of symbols then creating as new ( rather than modifying ) will be much faster

Erreur dans eval ( expr , envir , enclos ) : les valeurs de y doivent tre 0 = y = 1 #CODE
I can translate in english if it is not clear . But , when i do the same thing in R , everything is working find : #CODE

Unfortunately , it doesn't look like boxplot returns the axes : #CODE

I have two data sets which I read in with pandas Dataframe . Let's call them set 1 and set 2 . set 1 and set 2 contain text documents . Some of the text documents in set 1 occur in set 2 , and I am looking for a way to find those duplicates . I first thought about using sets which would return a list of all the elements in the intersection of the data sets . #CODE

I have data that looks like the following but I also have control of how it is formatted . Basically , I want to use Python with Numpy or Pandas to interpolate the dataset to achieve second by second interpolated data so that it is a much higher resolution .
So I want to linearly interpolate and produce new values between each of the real values I currently have while keeping the original values as well .

the way you are making the dataframe , the other column is index , pandas apply the functions to columns not the index

What is the most efficient way of doing this ? something like using df.str.split ( ' FLOATVALUE ' [ -1 ]) and then merge the data again by ID ? I know Regex can be used but it would be slow .
Re : your first comment , yes . Re : your second comment , it seems like you could hang onto the original ID column -- why discard it if you're just going to append the numbers back ? If that doesn't answer your question , open a separate question showing your desired output .

You might need to take the transpose first , like ` df.values.T ` . In DataFrames , the columns are axis 0 .

Hmm . I agree this is unexpected behavior and may be a bug . As an aside that may help you in the meantime , with datetime-indexed data , [ resample ] ( #URL ) is usually a better choice than reindex . See in particular the keyword arguments `` label `` and `` close `` which may be related to your issue .

There's always a slicker way to do things than the way I reach for , but I'd make a flat frame first and then pivot . Something like #CODE

you will simply look it up in the pivot table : #CODE
to make life easier , we may re-index the pivot table : #CODE

Your operation doesn't make sense as a DataFrame . The index labels in your expected result don't match up with the labels in the original ` data ` . You'll want to take the Series method and ` apply ` it to each column in ` data ` .
Apply it to your data set : #CODE

I thought apply treated each group as a sub-dataframe , which i can then manipulate and then return . I believe my understanding of the structure is flawed , and I've had trouble finding anything to help correct myself .
After running this function , I was hoping to be able to reaccess each subgroup and perform further analysis on it . But I'm curious about the resulting format . After I perform my groupby function , I can use the describe() function , and it will return a table subindexed by each grouped name , with the statistics . After my apply function , I want to look at the same type of table , but it congests it down to one , with the rows being describe parameters , without the level of group indexing
I think there's some alignment magic that happens at the end ( rather than just a concat ) , often I find groupby apply a dark art .
@USER : I still don't really understand what you're trying to do , but if you want to " perform further analysis " on each group , why don't you just do * that * analysis in the groupby function ? That is , make a function that actually does the analysis you want done , and apply that with ` groupby ( ... ) .apply ( ... )` , so it just returns the results of your analysis .

Also it seems your stack trace is referring to another part of your script and may be pointing to another bug .

If you think about pivot tables in ` Excel ` , you can add additional columns and change from sum to mean to min or max . Is it possible to get the multiple values in a ` pivot ` in ` Pandas ` ?
Here is a pivot example : #CODE
Is there a way I could take ` np.sum ` to the ` pivot ` example here ?
Both simultaneously . I could append the first pivot to add the second pivot data , but was wondering if it can be done in one swipe .

In Windows , this solution requires however that you align / streamline the installed software versions ( 32-bit or 64-bit ) of Excel , the ODBC driver and the software package from which you open the ODBC connection . As an example , an installed Excel 32-bit version will require a 32-bit ODBC driver and normally a 32-bit installation of Python . Note : this latter point remains to be confirmed for the Python case ( I'm a beginner to Python ) , but I can definitely confirm this point for ODBC connections launched from SAS , SPSS or Stata .

This was a very subtle bug in how freq reconversion on a queried DataFrame was reconstructed , resolved here . Docs are built daily at 5pm . Should be correct then .

to take to account that some periods span over one hour , i think one solution is to change the frequency to minute , backward fill all the nan values , and then resample hourly with mean calculation : #CODE

@USER Oh , I see , I can append the occurrences in chunks too , to the giant db .

Setting linewidth of Pandas boxplot fliers
Just want to chime in an say that I've submitted PR to matplotlib to allow full boxplot customization #URL

Basically I have two files . One has a standard date format with year , month , day , hour . The other has year , julian day , hour . I am trying to align them using pandas DataFrame function and don't know what to do about the missing month data . Is Pandas able to convert this natively ?

in 0.12 , you have to concat the result of selecting multiple criteria ( keeping in mind that you may generate duplicates ) #CODE

and merge over it #CODE

Replace column values in pandas multiindexed dataframe
I think I should be able to replace value in column via : #CODE

@USER its answered my question about how to add the data I wanted to my DataFrame without copying it and possibly messing up the order . I'm still not sure why the ` groupby ` / ` apply ` is failing when I do it one way and not the other , but it seemed easier to go after the more general question .

I need to join the files based on the CO_Num column . The problem I am having is with the Operator column . In plat1.csv that column is empty , I need to use the COMPAC.csv file to fill it in . I tried using the code below , but it did not work . The Operator field was still empty in plat2.csv . #CODE
Basically , I need a way to join / merge these two csv files where the Operator column in plat1.csv is overwritten by what is in the COMPAC.csv file , and if there is no CO_Num , that line is simply empty in the Operator column ( all other columns remain untouched ) .
You shouldn't include the empty Operator column of c in the merge ( including it means merging on both CO_Num and Operator which means there are no shared keys ): #CODE
@USER the operator column is still empty ?? That's confusing . Can you append the head of the csvs ( or some example frames which show this ) to your question ... I don't think it should be empty unless there are no CO_Num matches .

You can't unpack a 1d structured array , since all that ` unpack=True ` does is to transpose your array so that columns vary along the first axis , and the transpose of a 1d array is itself . Thus , you get the same result with ` unpack ` : #CODE

` pandas.fillna() ` is mean to replace ` NaN ` values with something else , not insert ` NaN ` into null data slots . See this example for details : #CODE

How do we apply a function to an entire group in pandas and python ?
How can we apply a function to an entire group in pandas dataframe in python ? This is the code that I have so far : #CODE
Could you be more specific about how ` magic_apply ` will differ from ` apply ` ? Maybe give an example of ` myfunc ` ?
As @USER points out the " magic apply " is simply called ... apply . It's a groupby method : #CODE

Well , the first thing that jumps out at me is that the second ` lambda ` is unnecessary and slows things down . You could just replace it with ` math.log ` , but there's almost certainly a better option with vectorized operations . Unfortunately , I'm not familiar enough with Pandas to vectorize this properly .
You can use numpy ( vectorized ) logarithm with panda's diff : #CODE

I'm creating two same date ranges using Pandas and Matplotlib . After conversion of numpy.float64 to Pandas timestamp I have 1 minute diff - why ? #CODE

or drop the column after importing , I prefer the former method ( why import data you are not interested in ? ) . #CODE
If the columns are varying length then you can just the header to get the columns and then read the csv again properly and drop the last column : #CODE
I would like to drop the last column of the cvs , but the cvs's are of varying column lenghts . If I understood correctly your method would entail first checking the column lenght of the cvs . I would like the program to do this for me .
Droping by label maybe has the same problem as the label of the column will be different depending on the column lenght . Is it possible to drop based on location ? As a workaround I'm currently using this ` a=len ( df.columns ) -1 df =d f.iloc [: , : a ]`

Without a clear definition of what you want , i assume your heatmap is a simple 2D histogram . So why dont you resample / pivot your DF to this and plot it with ` plt.imshow ( df_all.values )` ?
I do not know what you mean by heat map for a time series , but for a dataframe you may do as below : #CODE
@USER if you want the heat map to be also based on daily intervals , then you need to first use pandas ` resample ` method , otherwise just modify ` set_xticks ` and ` set_xticklabels ` calls

The way aggregation works is that you give a key and a value , where the key is a pre existing column name and the value is a function to map on the column .

Note that when using EMA one has to be careful : since it includes a memory going back to the beginning of the data , the result depends on where you start ! For this reason , typically people will add some data at the beginning , say 100 time steps , and then cut off the first 100 RSI values .

Beautiful .. I thought something like ix index quarter existed just could not apply it !....

I don't see much documentation for .index .set_levels . In the example above , setting levels is simple since we have only two levels . Can a dictionary be passed to replace only values in one index without touching ( or having to specify values for ) the other axes ?

It sounds like it's a custom extension type . We should still be handling this properly , though . There have been some other similar issues reported with exceptions being shown in debugger when they shouldn't ( i.e. when they are caught up the stack , and the setting is to not report caught exceptions ) . Let me try this and see whether I can repro .

Does each json item represent a row in the database ? Depending on the number of items , it might be best to insert rows as you parse the json using sql commands .

That's an unusual definition of outlier . So ` pd.Series ([ 10 , -1e6 , 12 , 13 , 14 ])` should return ` [ 12 , 13 , 14 ]` ? Are you missing an ` abs ` ?
You can get the ` last_valid ` using shift and numpy's where : #CODE
@USER think I was just missing an abs in the second part , this should be correct now in your example !

Thanks . But how do i apply that formatting to a dataframe ? like --- print dfTotalv3.format ( {0 } { 1 } { 2 : ,. 2f } { 3 : ,. 2f } { 4 : ,. 2f } ) or how could i just format one column by referencing its field name and the format i want

see here : #URL you need to set using : `` df.loc [ row , column ] = value `` , and not chained assignment . In addition , you are better off using a vectorized method or apply if you cannot vectorize .

Pandas - Replace Integers with Floats

close but .reset_index ( level=0 ) gives an error ValueError ( ' cannot insert %s , already exists ' % item )
You could replace the zeros with NaN's in you want to .
Its a bit obscure in one line . ` df.unstack() .dropna() ` basically flattens your DataFrame to a series and drops al NaN's . The ` get_dummies ` gives a table of all the occurrences of the letters , but for each level in the unstack DataFrame . The grouping and sum then combine the index to the original shape .
I noticed , you could have left it , no harm in having some options . I still agree with your earlier comment that its less pretty ( and readable ) than the ` pivot ` solution from Roman . The concept of pivot is also better known , then ... ` get_dummies ` . :)

you can use ` ix ` to get the index in a different order and plot it using ` df.plot ` : #CODE

It's worth mentioning that you can often be explicit with loc / iloc : #CODE
To select with a position and a label use ix : #CODE
Note labels take precedence with ix .
It's worth emphasising that assignment is garaunteed to work with one loc / iloc / ix , but may fail when chaining : #CODE

When I insert this with support 3 #CODE
Your return is in the wrong place and for n in len ( pattern ) is wrong too ....

The above code works if I replace the np.NaN with a number , such as " 2.3 " .
I've tried to follow the stack trace , but after a while it gets complicated ( I'm not familiar with Pandas , Numpy and Matplotlib source code ) .

Yes , it will work . But it's almost the same DT <-> TS converting . What's about converting DF -> TS in R ?

When one look at your data suggests it would be a simple ' group by ' in SQL , map reduce is probably going to unnecessary

Pandas DataFrame Matplotlib BoxPlot Boxes
How to make a boxplot where each row in my dataframe object is a box in the plot ?
But when I plot this data frame as a boxplot , I only get one box with the Open , High , Low , and Close values of the entire Volume column .
How do I do this using the Pandas DataFrame and Matplotlib boxplot() ? I just want a basic boxplot plot where each row from the DataFrame is a OHLC box in the plot . Nothing fancy at this point . Thanks !
I simply want to graph a boxplot where each box is a row of my DataFrame . My first piece of code is graphing a boxplot where each box is a column of my DataFrame . I want each row to be a box . Understand ? I would think this should be quite easy , im just not able to figure out how . Do I need to transpose or unstack something ?
A couple of points : 1 ) boxplots and candlestick graphs , despite their similar appearance , are conceptually quite different ; 2 ) you actually are trying to make a candlestick graph ; 3 ) even if you transposed your data with ` SP.T ` , the ` boxplot ` method will not produce what you want it to and ; 4 ) the real change you face is figuring out how to take your dataframe and turn it into a format that ` matplotlib.finance.candlestick ` can use .

It's a simple linear algebra , you multiply matrix with its transpose ( your example contains strings , don't forget to convert them to integer ): #CODE
I should probably look at numpy more . You just took the dot product of the matrix with its transpose . I think I can do it in one step in pandas

Use apply : #CODE
you can also use ` map ` : #CODE

Is there a way to estimate how much memory a merge operation will require ?
Given two dataframes A and B , and X amount of shared columns to merge on ( B is merged to A ) , is there a way to pre-calculate / estimate in real-time how much memory the operation will require ?

Another option would be to resample . #CODE
Using ` resample ` ( as @USER has shown ) is another possibility .
Use ` asfreq ` if you want to guarantee that the values in your downsample are values found in the original data set . Use ` resample ` if you wish to aggregate groups of rows from the original data set ( for example , by taking a mean ) . ` reindex ` might introduce NaN values if the original data set does not have a value on the date specified by the reindex -- though ( as @USER .nouri points out ) you could use ` method =p ad ` to propagate last observations here as well .

If your are not certain you data are always nicely 1-minutly ( so the 5 rows are not always equal to 5 mins ) , you can set the ` freq ` keyword in rolling_max to ` 1min ` .

Is there a more memory efficient way to do this in HDFStore ? Should I set the index to the " sec_id " ? ( I can also use the chunksize option and concat myself , but that seems to be a bit of a hack . )
Jeff , I updated sec_id and dt in the dataframe . Sorry , I had to update " sec_id " and " dt " to " id " and " date " . This code sample I have is direct from the code .
0.12 is fine ; FYI the format keyword doesn't do anything with append ( and it's for 0.13 anyhow ); append always is a table

How could I align the data between two dates ( EX : 2013-12-10 18:00 : 00 and 2013-12-11 00:00 : 00 at 20 minute intervals such that it appeared as below : #CODE
There are more columns in the data that are not shown above , and using this code causes the non-numeric columns to drop off . Also , how would I pad the head and tail ends with the specified timeframe ?
@USER .nouri I am using Pandas version 0.11.0 . The Error I am getting is described in the following stack trace #URL
@USER pandas is under heavy development , if it is possible for you update it to current stable version . i will look into the stack trace once i get the chance

pandas merge columns to a single time series
it's a simple matrix operation , not sure how to map it to pandas ...

Append column to pandas dataframe
I've tried using the ` append ` method , but I get a cross join ( i.e. cartesian product ) .
Did you try the ` join ` method ?
It seems in general you're just looking for a join : #CODE
@USER Join and concat use a lot of the same code under the hood , so the " right " way probably only matters when you consider edge cases . For instance here if both DataFrames had a ' data ' column the join would * fail* , whereas a concat would give you two columns named ' data ' .

I have recently been made aware of the dangers of chained assignment , and I am trying to use the proper method of indexing in pandas using loc [ rowindex , colindex ] . I am working with mixed data types ( mix within the same series of np.float64 and list and string ) - this is unavoidable . I have an integer index
@USER Could you try for a smaller example ? Personally I think it's annoying to diagnose atm as there's too much noise ( e.g. drop the columns you're not using , some of the rows that aren't raising , make the line with the ands have fewer conditions ) Ideally smaller than a 5 by 5 :)

Now I apply the cut : ` up3 = up2 [: cut_loc-1 ]` , which should just shorten the ` DataFrame ` . However , when I go to plot it ` up3.plot ( x= ' Field ' , y= ' Moment ' , color= ' red ' , label= ' Up '` I get the error `' numpy.ndarray ' object has no attribute ' find '`

Resample while keeping last date from input file ( and not last day computed by ` resample `)
Please note the date 2013-02-20 . This is the true date from my input data , and not a date created by ` resample ` .
Perhaps not the most fancy way , but you can always ` groupby ` your time frequency and apply a custom function returning what you want .
Then groupby the month frequency and apply the function : #CODE

` data.column_name ` would translate to ` data [ " column_name " ]` , which is not what you intend

per Jeff's comment , this becomes an issue when resampling data . If I have a time series that contains NaN values and want to resample to percentiles ( per this post ) #CODE

Download the the table , using a utility into a csv file , read the csv file chunk by chunk using pandas and append to HDF5 table using ` pandas.HDFStore ` . I created a dtype definition and provided the maximum string sizes .
however , the created DataFrame always infers the dtype rather than enforce the dtype I have provided ( unlike read_csv which adheres to the dtype I provide ) . Hence , when I append this DataFrame to an already existing ` HDFDatastore ` , there is a type mismatch for e.g. a float64 will maybe interpreted as int64 in one chunk .

maybe : DataFrame ([( i [ 1 ] , i [ 0 ]) for i in enumerate ( set ( x.index ))]) and then merge ?
Thanks , @USER . Looking at [ the source code ] ( #URL ) , I see ` Categorical ` calls ` factorize ` .

@USER you could make the same argument for allowing ` bool ( arr )` on numpy ndarrays . The goal is to get people to use the right thing and not be surprised when they try to do something and it fails because * this time * they have an NaN value in one column that they didn't have earlier .
I * would * make that argument for allowing ` bool ( arr )` to have some implementation and not raise an error . If you're using ` ndarray ` * you * should know what ` bool ` does on that input , rather than the library preventing it or warning because some people might not . I would say that Python's principle of least astonishment is not satisfied by the ` numpy ` choice to make that produce an error . It would be less astonishing if it returned ` True ` . That is , ` bool ([ 1 , 2 , 3 ])` returns ` True ` , but ` bool ( np.asarray ([ 1 , 2 , 3 ]))` raises an error . That's worse , IMO , but it's such a longstanding convention now .

To resample , you can make it an index , and use resample #CODE
@USER there is builtin resample method , see example and link to docs in my answer .

Now I can ` concat ` the last 3 dataframes on a new minute passing and then calculate what I need . But that means I'm needlessly recalculating every dataframe ` n-1 ` times .
m_i = len ( X_i )

Complicated pandas merge operation
Where df2 has the same fruits as df1 , but not the same years ( I'm almost completely sure that df2's years are a subset of df1 , although it would be nice to find a method , that allows for years in df2 that aren't included in df1 ) . Df3 is a table with characters for all the fruits contained in df2 and df1 . I would like to merge the three tables together , so each row in the new combined DataFrame has year , fruit , price , qty , weight ( possibly NaN ) and colour . I am not sure if such a data structure would be best contained in a Panel or a DataFrame - inputs on this are also very welcome . Thanks !
At first all JOIN may be performed in SQL - and it will be faster .
Hm , I can't get your first ( or second ) suggestion to work . Isn't the syntax for join , df.join ( .. ) ?
If you were confident that there was only one weight of fruit ( i.e. independent of the year ) you could just drop the year column from df2 : #CODE

Do you know that the series will always be in the same order ? If so , I'd create a MultiIndex , and the unstack from that . Just read in the ` Series ` like you've done . I'll work with this data frame : #CODE
We need to repeat the index so that the first observation for each Series is at index 0 . Now set that as the index and ` unstack ` . #CODE
The ` unstack ( 0 )` say to move the outer index labels to the columns .
Ah , you were one minute faster :-) . Yes , this is the other option : ` unstack ` with a multi-index , or ` pivot ` with Series
You can also do it with ` unstack ` as @USER explained : #CODE

For example , let's say I'm looking to find and categorize transients using some moving window process ( e.g. wavelet analysis ) or apply a FIR filter . How do I handle the boundaries , either at the end or beginning of a file or at chunk boundaries ? I would like the data to appear as one continuous data set .
Keep a map of the entire data set in memory so that I can address the data set as I would a regular pandas Series object , e.g. data [ #URL
Based upon the helpful hints I built a iterator that steps over files and returns chunks of arbitrary size --- a moving window that hopefully handles file boundaries with grace . I've added the option of padding the front and back of each of the windows with data ( overlapping windows ) . I can then apply a succession of filters to the overlapping windows and then remove the overlaps at the end . This , I hope , gives me continuity .
This can very easily get quite complicated . For instance , if you apply an operation that does a reduction that you can fit in memory , Results can simpley be a pandas.Series ( or Frame ) . Hoever ,
You might want to resample the data to lower frequencies , and work on these , loading the data in a particular slice as needed for more detailed work .
Thanks for your insight . I looked at your ENH module . I built a iterator that steps over the files but that allows for data padding on either end . By overlapping the data chunks , I can apply a filter and then cast aside the padding at the end , thus preserving continuity .

You should be using resample rather than reindexing with a date_range : #CODE
@USER I guess that depends on what you are doing later , usually pandas is pretty clever with alignment ... have you tried the [ panel resample method ] ( #URL ) ?

I'll preface by saying I'm a programming n00b by stack standards . I have experience with data analysis and scripting -- this is what I do professionally at a financial firm -- but I have no idea what I'm doing on the back end .

Also have a look at the arrow library to replace the ` datetime ` part . It's fantastic for these sorts of things .

So the below code seems a lot closer in that it gives me a funky df if I pass the in the list and Transpose the df . Any idea on how I can get this reshaped properly ? #CODE
I think you're going to be able to wrap this with a concat , something like : pd.concat ([ series_chunk ( chunk ) for chunk in lines_per_n ( f , 5 )]) , where series_chunk is the function returning each row as a Series ( the bit in the try / except block ) .
Thank you again . Just trying to wrangle the final output I'm getting now ( appended ) . I was expecting df.T to work but it didn't do transpose columns and rows .
More bugs from me , you need to use concat with axis=1 , updated my answer , sorry !
@USER can I trouble you for one final question before I put this to rest . This is probably incredibly simple but my iteration skills are a little weak . How would concat each dataframe it creates in the loop as one object ?
One way is to concat them together , tweaked ...
I'm not sure how to correctly implement your suggestion . Are you recommending to use either append or concat ? I get how the first concat we have will concat the chunks but it seems like now I have to wrap the whole thing in another concat .
Check out the IO part of the docs for several examples , arguments you can pass to this function , as well as ways to normalize less structured json .
If you have several json files you should concat the DataFrames together ( similar to in this answer ): #CODE
As mentioned in the comments you may be able to do this more directly by concat several Series together ... It's also going to be a little easier to follow : #CODE
@USER this is how to create a DataFrame from the output you gave ( although not strictly a " csv " ! ) . I suspect it's possible for you to concat some objects together more directly , but difficult without a [ short example ] ( #URL )
Thank you . I've added all of the code and sample data . Would love to be able to concat the object outputs into a dataframe somehow . Any help would be appreciated .

You can do this directly with an apply instead of last ( and get the -1th row of each group ): #CODE

But most likely you can do a transform or apply ( depending on what something is ): #CODE
Thanks - the refactor makes sense . Couldn't figure out the transform apply approach when I looked into it at first . I was hoping there was some nice syntactic sugar .
@USER you need something to be a function for transform / apply , worth checking out the docs : #URL
@USER completely depends on the something whether or not you can do apply / transform ! :)

1 ) Cut out the non essential data . ie the first 1675 lines , roughly I want to remove and the last 3-10 lines , varies day to day , I also want to remove . I can remove the first lines , kind of . The big problem with this idea I'm having right now is knowing for sure where the 1675 pointer location is . Using something like #CODE

doesn't solve the problem because there will be " borders " . I.e. after that sort , the last value for one ticker will be above the first value for the next ticker . And computing differences then would take a difference between two tickers . I don't want this . I want the earliest date for each ticker to wind up with an ` NaN ` in its diff column .
But if I understand the mechanics of groupby , my rows will now be sorted first by ` ticker ` and then by ` date ` . Is that correct ? If so , would I need to do a merge to append the differences column ( currently in ` result [ ' current ']` to the original dataframe ` df ` ?
and then ` join ` the two dataframes at the end .
This is certainly a concise solution . I have proposed one alternative above in an edit to my original post . Yours is much cleaner . That said , what if it was important to preserve the original order of the rows ( as defined by their ` ( ticker , date )` tuples ) ? Would you just use your solution on a copy of the original dataframe and then merge ( keying on ` ticker ` and ` date `) ?
Great point . But the data holes are not particularly common among tickers . I.e. say ticker 1 is supported on only odd days and ticker 2 is supported on only even days . If you pivot as you suggested , and then compute diffs down the columns , you'll wind up with ` nan `' s everywhere . Is there a way to combine my general approach ( ` set_index ` , ` groupby ` , ` transform ` , ` reset_index `) with the reindexing you mentioned to do the " realigning " at the end of the process ? I think there is a lot of value in the ` groupby ` construct to enforce that calcs do not " cross groups " .
I'll give this the check mark because it's taught me quite a bit . But it still seems odd to me that there isn't a more natural way to do this with groupby . For example , I get weird behavior if I take the original dataframe and try : ` df.sort ([ ' date ']) .groupby ([ ' ticker ']) .transform ( lambda x : x.diff() )` I would have hoped pandas would be able to figure out that it should ignore text columns and then apply the diff function to the numerical columns . In general , is there a way to use a different function per column in ` transform ` ( like you can with ` agg `) ?
You can use ` pivot ` to convert the dataframe into date-ticker table , here is an example :
convert the dataframe by ` pivot ` format : #CODE
This will accomplish everything I want . And what I really like is that it can be generalized to cases where you want to apply a function more intricate than ` diff ` . In particular , you could do things like ` lambda x : pd.rolling_mean ( x , 20 , 20 )` to make a column of rolling means where you don't need to worry about each ticker's data being corrupted by that of any other ticker ( ` groupby ` takes care of that for you ... ) .

Then how do I apply clustering to this to determine a cut-off threshold ?

I think methods with apply are going to need some annoying sorting at the end ... : s

` len ( test_data [ 0 ] [ ' SomeGeneID '])
len ( test_data [ 0 ] [ ' DifferentgeneID '])
To be preceise , your resulting dataframe in general can't be a proper csv , as different rows may have different lenghts . In case groups have length not greater than ` l ` , you can try something like ` pd.DataFrame ( map ( list , grouped [ person ] .values ))` to make it a DataFrame with ` l ` columns , with ` None `' s for missing values ( shorter rows ) .

File " C :\ Anaconda\lib\ site-packages \pandas\core\ generic.py " , line 348 , in drop
File " C :\ Anaconda\lib\ site-packages \pandas\core\ index.py " , line 1215 , in drop
For something like this , you could use ` shift ` and a loop . There are some index tricks you can pull but it's unlikely this is a bottleneck , so we might as well be simple . #CODE

Yes , but then you are using pandas ` hist ` function , and not matplotlibs . And this handles eg NaNs as expected . See my update .
These NaN's are not handled well by the ` hist ` function of matplotlib . For example : #CODE
Another option is to use pandas ` hist ` method on your series and providing the ` axes [ 0 ]` to the ` ax ` keyword : #CODE

There is a parameter for DataFrame.to_csv called na_rep . If you have ` None ` values , it will replace them with whatever you pass into this field . #CODE
Of course you could always write your own function to output a csv , or simply replace the "" in the output file using : #CODE

I'm seeing this * only * after running map / reduce workers ( shepherd / mincemeat ) . That is , I'm getting data out of redis after ` shepherd.run_server() ` finished running ( in the main process ) . Using returned results ( as keys ) , I'm getting dataframes out of redis as strings ( put there by .to_json() in reduce workers ) and then saving them to HDF5 - no subprocess , no m / r , just a plain Python function . What's truly weird is that if I do that in IPython " manually " the above exception does not happen .

I've got a dataframe , and I'm trying to append a column of sequential differences to it . I have found a method that I like a lot ( and generalizes well for my use case ) . But I noticed one weird thing along the way . Can you help me make sense of it ?
So far , so good . If I replace the middle line above with the more concise code shown here , everything still works : #CODE
What's going on here ? When you call the ` .diff ` method on a Pandas object , is it not just calling ` np.diff ` ? I know there's a ` diff ` method on the ` DataFrame ` class , but I couldn't figure out how to pass that to ` transform ` without the ` lambda ` function syntax I used to make ` data1 ` work . Am I missing something ? Why is the ` diffs ` column in ` data3 ` screwy ? How can I have call the Pandas ` diff ` method within ` transform ` without needing to write a ` lambda ` to do it ?
Pandas has a lot of methods where they don't just call the numpy function , mainly because they handle different dtypes , handle nans , and in this case , handle ' special ' diffs . e.g. you can pass a time frequency to a datelike-index where it calculates how many n to actually diff .
Too kind . @USER , when I first read your answer , I totally missed that you'd put ` Series ` into your first line . I kept trying ` pd.DataFrame.diff ` and having it blow chunks . Works perfectly with ` pd.Series.diff ` . Can you recommend a place I could look to see which methods ( ` diff ` , ` sum ` , whatever ) properly belong to ` Series ` vs ` DataFrame ` vs some other structure ? One challenge I've had with Pandas so far is trying to peel back the " magic " that makes it work in 90% of cases to figure out why it's not working in one of my 10% examples .
@USER that is outrageous !! lmao ( I also do the print trick , or append to a list , it works great ... although sometimes changes depending on the return type ! )
Say my dataframe had two values columns : value_1 and value_2 . I can do : ` diffs_df = data3.groupby ([ ' ticker ']) [[ ' value_1 ' , ' value_2 ']] .transform ( lambda x : x.diff() )` and that works fine . But ` diffs_df = data3.groupby ([ ' ticker ']) [[ ' value_1 ' , ' value_2 ']] .transform ( pd.DataFrame.diff )` blows chunks . So is it correct to say that ` transform ` always operates on a series ( even if it means it has to work on multiple series in succession ) , while ` apply ` works on multiple series all at once as a DataFrame ?
What determines the type of object passed to the function ... is it the # of columns , or is it ` transform ` vs ` apply ` , or is it ` [ ' colname ']` vs ` [[ ' colname ']]` ?
I know I am late to this party , but is there any to know that this is running a ` diff ` in order ? The code you have Jeff does not pre-sort value based on ` date ` . Is one supposed to do so ? Are there any guarantees that ` groupby ` preserves the order between rows ?
Got it . Your example makes that quite clear . In my original post , I showed two methods that worked ( 1 : involved ` lambda ` and 2 : involved just using the ` diff ` method directly on a Pandas ` Series `) . Which of these do you think is more appropriate / best practice ? In general , do you try to use as few ` lambda `' s as possible , or do you find they make it easier to see the structure of what's going on ?

What Pandas data type is passed to transform or apply in a groupby
Apply
to inspect the calling stack : #CODE
@USER , you can print the calling stack , and analyze what's going on .
Is there any way to force transform to pass the multi-column dataframe ( i.e. not the individual series from the columns ) to the function ? I basically want the same behavior of ` apply ( my_func , axis =1 )` but forcing it to return a result with the same index ( i.e. what transform is supposed to do , but rather than working column by column , I want to be able to access multiple columns at the same time ) .

From here , we can use a little hack to directly append these columns in one step with the appropriate column names . This will only work for regular data ( the json object needs to have 3 values or at least missing values need to be handled in our CustomParser ) #CODE
On the Left Hand Side , we get the new column names from the keys of the element of the stats column . Each element in the stats column is a dictionary . So we are doing a bulk assign . On the Right Hand Side , we break up the ' stats ' column using apply to make a data frame out of each key / value pair .

grouping by column and then doing a boxplot by the index in pandas
I have a large dataframe which I would like to group by some column and examine graphically the distribution per group using a boxplot . I found that ` df.boxplot() ` will do it for each column of the dataframe and put it in one plot , just as I need .
The problem is that after a groupby operation , my data is all in one column with the group labels in the index , so i can't call boxplot on the result .
Now I want to group by column b and boxplot the distribution of both groups in one boxplot . How can I do that ?
You can use the ` by ` argument of ` boxplot ` . Is that what you are looking for ? #CODE

We want to see your code :) , people here like fixing code that is gets close and shows effort . If you can't provide code you should read the docs on sorting lists . You can provide a function to replace default behavior , just like many other languages allow you to do ( through sub classing or other means ) .

After several weeks of refining this I have the following code thanks to awesome folks on SO which produces dataframes as I need but I'm not sure how to concat the dataframes in the program into one for the final dataframe object variable . I just assign the concat statement to a variable then I end up getting the last dataframe only . #CODE
output from above program which I need to concat as one dataframe : #CODE
see here : #URL just append the df's to a list , them to a list , then concat at the end , e.g. `` result = pd.concat ([ list_of_frames ])``
Note : it could be that using axis=1 to the final concat means you can avoid T-ing earlier .

I just found this method ... which is neater ... but is there a way perhaps using ` resample ` and if using this method could I convert a minutely OHLC timeseries into a 15 minute timeseries OHLC ? ( N.B. there can be some missing minutes so splitting by every 15 rows wont work ... ) #CODE
no , you would have to specifically do that ; easier IMHO to just ohlc on say last , them do a separate sum on volume and concat anyhow

Does such a function exist ? If I build my own function , how can I apply it to the DataFrame columns ?

I cannot append a series to dataframe .

Pandas apply to data frame groupby
If I groupby ( g object below ) and then apply following function to first 1000 rows of df , it works . But if I apply it to entire df , I get this exception : #CODE
First reset the index , then group and apply . You can recover your original index by then setting the index at the end . The reset index is turned into a column called ' index ' ( which set_index then drops ) .

pandas for python is neat . I'm trying to replace a list-of-dictionaries with a pandas-dataframe . However , I'm wondering of there's a way to change values row-by-row in a for-loop just as easy ?
If you really want row-by-row ops , you could use ` iterrows ` and ` loc ` : #CODE
And there's always ` apply ` : #CODE

Merge / Join / Append two Pandas DataFrames with MultiIndex columns by both index and cols
I have tried several ways , including with Merge and Join , but can't get it to work .
In v0.13.1 , this does not support any other join method other than left ... If it had " outer " , this would be the exact thing I'm looking for !

As memory is the main issue of my problems I'll give an outline of a part of my program . I have dropped the predictions for the time being , which reduced the complexity of my program significantly , instead I insert a standard sample for every non duplicate in my test set . #CODE
We may need to distinguish between flattening in the sense of removing everything except the second column ( which can be done once , out-of-core , and the results stored in a new file ) and * reading * the whole second column ( which if you need to get into swap , may be unavoidably slow ) . That section of the code does both . What is ` sum ( map ( len , new_text ))` , i.e. the total size of the strings you need to keep in memory ?
The list ( np.array ... bit is not going to be performant , use a list comprehension . Also ` for item in xrange ( len ( test ))` should be ` for item in test ` .
If the order of your rows is not important you can use sets to find elements that are in train set but not in test set ( intersection trainset testset ) and add them first to your " result " and after that use set difference ( testset-trainset ) to add elements that are in your test set but not in the train set . This will allow to save on checking if sample is in trainset .

for your last question , freq I can't correctly define freq , so don't know what to suggest .

` p - median ( diff ( .index ( x )))`
Sure median will be 24 hours , as most of days exist in list : #CODE
If you wish to automate the process of inferring the median frequency in days , then you could do this : #CODE

First question is how do I strip off the header with Python 2.7 . With 3.3 the above code works fine but it doesn't strip off the header in 2.7 . Have had numerous problems going between the two versions of Python ... it's maddening . I have 2.7 terminal installed and finally went out last night and changed over 2.7 shell as well . Had been using / getting fed up with the discrepancies between 2.7 terminal and 3.3 shell I have been working with .
Once I download the data from the data source , one of many different sources I plan to use throughout the course of the program I'm starting to work on , I want to be able to strip off the unnecessary data and then do mathematical work with the remaining data and put the results into new columns right alongside the original data . Aka , July 15 , 2001 math calculations will be right beside the original data for July 15 , 2001 . To accomplish this I need to find out where the first row of stripped data is located so I can set up the for-loop ( firstrow , lastrow ) . Until I can find out where both beginning of the stripped data is located and where the end of the stripped data is located I can't do anything else . How do I retrieve the number that tells me where the first / last row is stored at ? I've been told previous to use head / tail or iloc . When I try using df.iloc all I get is the error DataFrame has no attribute ' iloc ' . When I try to use df.head ( 0 ) or df.tail ( -1 ) I get the first / last row showing up as the first number of the head or tail line . How do I get that number though so I can actually use the number to set the first / last row of data from the computer program . If I try : #CODE
It still gives me the same thing . How do I strip off the row number from the entire head / tail line .

I think if you want to plot each year over top of each other , the xaxis must have the same date range . To support leap year , you can shift all date to year 2000 , here is my try : #CODE

But when i try to resample with lambda like
Why isnt this working ? Is there another way to do it ? Id like to resample the data by quarter or week and get the difference from the start of the period . End / Start -1 .

My aim is to " unstack " the results such as I could have one row per ` name ` as long as ` unit ` and ` ref ` are the same .
so far , I tried to join the same table based on ' name ' -- for now , no check on ` unit ` neither ` ref ` -- : #CODE
Pandas is probably better for this kind of thing . There might be some more exotic SQL functions that do a transformation like this , but I'm not sure . Below is your example doing the most simplistic approach , which is just to JOIN a view of each one together . The way this comes to mind is that since you are producing rows that have " virtual " columns derived from data , that suggests a composite row is being created from more fundamental rows . So the approach below breaks out a set of rows corresponding to each of the four conditions , v1 / W , v1 / L , v2 / W , v2 / L . Now if in practice , there's any number of " vN " , that's where the transformational ability of Pandas is probably more appropriate . #CODE
This is the more " exotic " SQL for your question . I'm using SQL Server 2008+ to get the PIVOT command . I'm not sure if it's exactly covering the case for your columns though . #CODE

But here is the stack trace I'm getting #CODE
I'm using python 2.7.3 , and with pandas the stack trace above is from stable release 0.12.0 . I've tried this with development version 0.13.0rc1-119-g2485e09 and got the same results ( different line numbers ) .

Please edit your question and include some sample code as well as the complete stack trace .

Instead of using concat , why not update the DataFrame in place ? ` i % 10 ` will determine which 1000 row slot you write to each update . #CODE

I have a legacy system and am running version 0.9.0 , can't upgrade till after project , so I will have to use a string replace or delete method many thanks .

Map a row index back to the original indexed dataframe
My question is : how do I map these 7 rows to the original dataframe ? #CODE
Thanks , I have corrected my post . I am not going to use this , but , out of curiosity , how do I map between slice and dataframe , in both directions ? Is it tricky to do , then skip this question .

Then , I try to append the cumulative number calculation under the label " new " as suggested in the post #CODE
The problem is , as the Error message says , that the index of the calculated column you want to insert is incompatible with the index of ` df ` .
For this reason , you cannot insert it into the frame . However , this is a bug in 0.12 , as this does work in 0.13 ( for which the answer in the linked question was tested ) and the keyword ` as_index=False ` should ensure the column ` L1 ` is not added to the index .
In pandas 0.13 ( in development ) this is fixed ( #URL ) . It is for this reason the ` as_index=False ` is used in the groupby call , so the column ` L1 ` ( fow which you group ) is not added to the index ( creating a MultiIndex ) , so the original index is retained and the result can be appended to the original frame . But it seems the ` as_index ` keyword is ignored in 0.12 when using ` apply ` .
@USER Ah , yes , indeed this does also work . But with ` apply ` it doesn't . Do you know what is the difference in this case between both ?

Python pandas resample takes too much memory
I have a time series of firm fundamental data like market value , closing price indexed by date and cusip with weekly frequency . I would like to resample it to get data with monthly frequency but the problem is that my code takes too much memory and too much time for very little amount of data .
Using ` resample ( ' M ' , how= ' last '` will be a little bit faster . But can you show an example dataset ( reproducible piece of code ) ?
Caching the functions help somewhat if I am making repeated calls but it still takes too much time and I think it is because I unstack and stack ' Cusip ' . Is there a way I could avoid doing that ?

I have a dataframe with 6 columns . The first 5 uniquely identify an observation . The 6th is the value of that observation . I would like to pivot the data so that , of the 5 identifying columns , 3 become a hierarchical row index while the other 2 become a hierarchical column index .
Are there any other ways to do this kind of thing ? I had initially tried making all the columns except for ` value ` into an index and then using ` unstack ([ ' observer ' , ' obstype '])` . But this gave me an unnecessary extra level in my column hierarchy : an unnamed level whose only entry was ` value ` ( i.e. the name of the column whose data I actually wanted in the guts of my table ) .
If I go with the " make a huge index , then unstack " method how do I kill the extra level in the column hierarchy ?
And in the case of ` unstack ` to get rid of the extra level , you can use ` droplevel ` : #CODE

I have a pandas Panel with a non-unique major_axis and I am trying to sum the non unique rows using groupby , but I get an error saying that the major_axis is not iterable . I have searched stack overflow and the message board , but it seems like the Panel is not as widely used as the dataframe .
The answer of @USER is indeed the solution to your question , although I think you misunderstand the groupby . You still need to apply a function or aggregation on the ` groupby() ` call , in your case to sum all items in a group ` data.groupby ( .. ) .sum() ` .

@USER That's also fastest solution so far ( just ) , though loc seems * slow * here ... Seems like this could be good feature request .
The ix operator is the power indexing operator . Remember that the first argument is rows , and then columns ( as opposed to data [ .. ] [ .. ] which is the other way around ) . The colon acts as a wildcard , so it returns all the rows in axis=0 .

Hadoop ( Pydoop ) ( map / reduce )

I have a dataframe which I want to split into 5 chunks ( more generally n chunks ) , so that I can apply a groupby on the chunks .

I have a ` DataFrame ` that has a number of string columns and a datetime column . I want to resample the datetime columns appropriately using pandas ` df.resample() ` . For instance , my data looks like : #CODE
Then I try and resample the ` DataFrame ` to daily using ` df.resample ( " 1D " , how= " sum ")` . This doesn't work : #CODE
The only work around we have found is to instead we truncate the dates using numpy datatypes . E.g. #CODE
@USER The effect I'm looking for is to have timestamps rounded / truncated into arbitrary groupings . So being able to truncate them to one day , two days , etc . I would prefer to use the pandas format for this versus the numpy datatype mentioned . My example is unfortunate , but I couldn't share my dataset - in my raw dataset , sessions and users have lots of duplicate ( web logs , one use with many sessions , many timestamps within a session ) .
Set a timestamp index , sort the timestamp index ( the way I am doing it here , you don't need to sort , while a resample does need it sorted ) . and perform the resample in whatever frequency you want ( 1D in this example ); this is tantamount to a resample but it just ' groups ' and doesn't do the calculation ( yet ) .
then in the apply , do your calculation , which in this case is another groupby . #CODE
This should not be a memory problem . You almost never want to unstack twice in a row on a large set as its memory conducive .

Using a print statement to see the " Lines to be appended " section below , I then assign the variables to dict pointers and the append the dict to what started as a blank dataframe .
The first line / append seems to change the type for that first entry on the last column . #CODE
Return from line print prior to append to df : #CODE
pls make an example that is easily reproducible , e.g. can be cut and pasted .
This is a bug , noted here : #URL Merge / join / concat does not handle timedelta properly ATM .
Got around this by encapsulating the dict ' parms ' in a spawned dataframe per loop and append the single row df to the major dataframe .

For filling the NaNs , you can apply this on all columns in one line as follows : #CODE
For the resampling , normally you can do the following to resample with different functions on multiple columns at once : #CODE

I kinda hate you but more myself because I've spent the past couple of hours not realizing DataFrame had resample too . :)
Glad I could help ! Most functions on Serieses you can also apply on a DataFrame , and if you can't , you can always apply a Series functions on all columns at once like this : ` df.apply ( lambda x : x.seriesmethod() )`

I am given daily rainfall data ( e.g. s , below ) which I can convert to a pd.Series and resample into monthly periods ( sum ; e.g. sm , below ) . But I then want to calculate the difference between each monthly value and the mean for the month . I have added a synthetic example : #CODE
when you resample resample with both sum and mean ..

It seems a little annoying to have to turn the ` Series ` object ( or in the answer above , ` dict `) back into a DataFrame and then append it , but it does work for my purpose .

I could append that data frame by getting the quarter for each entry ( or Month , or Year ) , then use ` Pandas ` ` groupby ` function , but that seems like it's extra work when I should be using ` TimeSeries ` .
Something like this ? I'm first doing a groupby , and then applying a resample on each group . #CODE
Ah , I didn't realize you could use ` resample ` directly on the groupby ! Still so much features hidden and undocumented .. ( OK , I know you can see it in tab completion , but that's not so visible ) .
And indeed , much cleaner to do the ` set_index ` before ! ( I was thinking this would give problems for the resample with it then being a multi-index , but of course , the groups itself don't have the grouping value as an index yet )
@USER atm resample just passes to g.apply ( pd.DataFrame.resample ) , but perhaps in the future the groupby method could be written specifically and be more efficient ... :)

However I wish to plot create a facet-wrapped histogram , and have each facet share the same xlim . The command I use seems to apply the xlim only to the last of the facets . ( Also you can see that the labels are applied to the last of the facets only ) . Is there a way to specificy a global xlim ? ( And global labels ) ?

Freq : T

Use resample to get each days maximum : #CODE
@USER I think you last step ( the resample by hour ) is not what is he asks . And by the way , if not all hours available , this will generate errors ( ' Cannot convert NA to integer ')

Ok . I'm actually a bit surprised this worked , but here's a starting point . Can anyone suggest a method where I don't need to do the ` zip ` ping to condense all the inputs for ` np.interp ` down to one column before using ` map ` ? ( See edits below . This is exactly what ` DataFrame.apply ` does ... ) I.e. is there a Pandas function that acts like ` map ` does for Series , but takes in an entire row of a DataFrame as its input ( but doesn't involve ` groupby `) ?
Now consider 3 different ways to tack on a column which will interpolate between the given columns to an x-coordinate of 5 : #CODE

Pivot a pandas dataframe and get the non-axis columns as a series
If I pivot the table using D.pivot_table ( rows= " Category " , cols= " Period " , aggfunc= " sum ") I get a multi-indexed data frame looking like this #CODE
I've looked at stack , unstack , various combination or rows , cols , and values arguments to pivot table and of the melt function in pandas.core.reshape , but none of them seem to do quite what I want .
Maybe you're looking for a groupby object instead of pivot ? #CODE

Instead of doing what you're doing with labeldict , you could make that information into a DataFrame and then join it with your original one : #CODE
You can use merge instead : #CODE

@USER apply answer from question then " 1 " the found rectangle and recurse .

The pandas object also performs ix addressing directly . #CODE
@USER , correct .... that's one of the oddities of ix in any event

@USER presumably there isn't an ` iloc ` either ( if it's [ pre 0.11 ] ( #URL )) , in which case you need to use ( the more ambiguous ) ` ix ` .

I have some data that I'm taking from ' long ' to ' wide ' . I have no problem using ` unstack ` to make the data wide , but then I end up with what looks like an index which I can't get rid of . Here's a dummy example : #CODE
The purpose of column names perhaps becomes clearer when you look at what happens when you unstack twice : #CODE

finds the median of these delays ;
presumes the first time period starts at half the median value before the first timestamp ( putting the measurement in the middle of the time period );
adds data to this array according to the correct ( timestamp - starttime ) / median element .
The median time between datapoints is 20 seconds , half that is 10 seconds . To make sure we put the lines well between the timestamps , we make the start time 10 seconds before the first datapoint . If we just make the start time the first timestamp , it's a lot more likely that we'll get 2 timestamps in one interval .
So , 1388435242 - 10 = 1388435232 . The timestep is the median , 20 seconds . The numvals here is 3 . #CODE
median delay : ` df [ ' time '] .diff() .median() `
presumes the first time period starts at half the median value before the first timestamp ( putting the measurement in the middle of the time period ) ; I don't know what you mean here
Calc median #CODE
first , why not ` df [ ' diff '] = df [ ' time2 '] .diff() ` ? second , why preserve old data , i.e. introduce those time1 , time2 , and not simply overwrite ` df [ ' time ']` ?
@USER yep ... `` diff `` is better hear ; I was trying to ' illustrate ' so didn't overwrite the columns

As of .13 , you should be able to use ` reindex ` and ` interpolate ` to do this ( as long as you have ` scipy `) . #CODE

pd.scatter_matrix ( frame , alpha= 0.5 , figsize=None , ax=None , grid=False , diagonal= ' hist ' , marker= ' . ' , density_kwds={} , hist_kwds={} , ** kwds )

Your csv hasn't column names which you later use for pivot , spcify them expicetly . See example in update to my answer .
Solution in your case is to specify columns in code that loads data using ` names ` paramether . Try replace #CODE

In statsmodels " freq " controls the units for the time series not the " data window " . How can I change the " data window " .

I cannot figure out how to get this to work . The closest I got was using merge , but that left me with different suffixes .
I want to keep the trials separate , and just merge the Times

If you do not want to add a column to your original DataFrame , you could create an independent ` Series ` and apply the ` groupby ` method to the ` Series ` instead : #CODE

Apply function to sets of columns in pandas , ' looping ' over entire data frame column-wise
I have tried using ` df.groupby ` and ` df.filter ` to loop-over the columns but I cannot really get it to work , because I am not at all sure how I apply effectively the same function to chunks of the data-frame , all in one go ( as apparently one is to avoid looping over rows ) . I have tried doing #CODE
Your calculation is more NumPy-ish than Panda-ish , by which I mean the calculation can be expressed somewhat succinctly if you regard your DataFrame as merely a big array , whereas the solution ( at least the one I came up with ) is more complicated when you try to wrangle the DataFrame with melt , groupby , etc .
Note : I am a bit frustrated that I needed to thrown in the two transposes . I just couldn't get ` groupby ` and ` apply ` to play nicely with ` axis=1 ` . If someone could show me how to do that , I'd be very grateful . The trick here was knowing that when you call ` groupby ( lambda x : f ( x ))` that ` x ` is the value of the index for each row . So ` groupby ( lambda x : x [ 0 ])` groups by the first letter of the row index . After doing the transposition , this was ` A ` or ` B ` .

I am trying to replace the letter T or any other letter for that matter with the next highest integer for that table . The first table contains no errors , the second table contains 1 T and the third contains 2 x t's .

python pandas How to remove outliers from a dataframe and replace with an average value of preceding records
I have a dataframe 16k records and multiple groups of countries and other fields . I have produced an initial output of the a data that looks like the snipit below . Now i need to do some data cleansing , manipulating , remove skews or outliers and replace it with a value based on certain rules .
i.e. on the below how could i identify the skewed points ( any value greater than 1 ) and replace them with the average of the next two records or previous record if there no later records . ( in that group )
So in the dataframe below I would like to replace Bill%4 for IT week1 of 1.21 with the average of week2 and week3 for IT so it is 0.81 .
Thanks @USER yeah i kind of got your logic to work in my dataset . I am having a couple of problems though . One is I only wanted to get the mean of the next rows that relate to the same group . i.e. i need to apply a groupby to your example in my case it would be by ' Country '

However , when I do ` apply ` , I'm getting dataframes only : #CODE
` filter ` docstring says I should be getting Dataframes . Why ? And how can I fix that ? ( I want to simply drop some groups from grouped-by df ) .
Internally , ` apply ` and ` filter ` try different ways of looping through the data : a " slow path " that is sure to work for any function , and a " fast path " that only works for some functions . These paths can operate on whole chucks of the data ( as a DataFrame ) or one row at a time ( as Series ) .
Which groups do you want to drop , and what criterion are you trying to use ?

thanks for the encouragement . I'm thinking about this sort of thing in the context of adding more useful marginals in pivot table type reports . Basically sequentially reducing the levels in an axis and providing a subtotal for each level as you go .
The trick is to use the transposed sum . So we can insert another column ( i.e. row ) with the name of the additional level , which we name exactly like the one we summed over . This column can be converted to a level in the index with ` set_index ` . Then we combine ` df ` with the transposed sum . If the summed level is not the last one you might need some level reordering .

define a function to diff on that column ( and just return the rest of the group ) #CODE
the diff is now a timedelta64 [ ns ] , see here for how to convert / round to a specific frequency ( e.g. days ) .
Thanks @USER . The problem is , after the ` apply ( f )` , I cannot do something like ` nth ( 1 )` to return the 2nd value of each group . Any ideas ? In other words , the following fails : ` dd.groupby ( ' user_id ') .apply ( f ) .nth ( 1 ) .dropna ( how= ' all ')`
in `` f `` you could do `` x.head ( 2 ) .tail ( 1 )`` to do that ( you could also do `` x.iloc [ 1 ]`` , but if you have a groupsize < 2 that will fail . alternatively , you can do a second group / apply ( `` nth `` is a groupby operation ) , e.g. `` dd.gropuby ( ' user_id ') .apply ( f ) .gropuby ( ' user_id ') .nth ( 1 )``
also , the function you suggest computes the diff between each subsequent time . I need the diff computed from the ** first ** action to each subsequent action , so that the first time a user_id appears , that is time zero and each new action has its age computed relative to that first action .
easy , just do `` x [ ' diff '] = x [ ' date '] -x [ ' date '] .iloc [ 0 ]``

Note that you have to treat corner case , ` bins [ 0 ]` , separately , as it is not included by cut in leftmost bin .

Hence how do I get my function to apply to the data frame in a row-wise fashion ?

If you want to stick with ` pandas ` , based on answer from @USER you can replace the last line with , ` >>> u [ 1 :] = q >>> u= u.cumprod() `

To get it in the same format , I can ` unstack ` the ` dataframe ` : #CODE
The approach of @USER with ` groupby ` and ` unstack ` is certainly fine , but for completeness , you can also do this directly with ` pivot_table ` ( see doc ) [ version 0.13 and below ]: #CODE

your code does nothing because you are not appending to your ` dfs ` list , don't you want to replace the line ` data = pd.read_csv ( filename )` with ` dfs.append ( pd.read_csv ( filename )` . You would then need to loop over the list and ` concat ` , I don't think ` concat ` will work on a list of ` df ` s .
also you are mixing an alias for the module with the module name in your last line , shouldn't it be ` big_frame = pd.concat ( dfs , ignore_index=True )` ?, anyway once you have a list of dataframes you will need to iterate over the list and concat to ` big_frame `
you need to loop over ` dfs ` now , so something like ` for df in dfs : big_frame.concat ( df , ignore_index=True )` should work , you could also try ` append ` instead of ` concat ` also .
Can you tell more exactly what is not working ? Because ` concat ` should handle a list of DataFrames just fine like you did . I think this is a very good approach .
I stand corrected on the use of ` concat ` after reading the [ docs ] ( #URL ) , it should work like @USER says , can you edit your question and post the error message
Thanks for the snippet . It helped me , but I should point out that this code won't run as is . ` directory ` is not defined anywhere ( did you mean ` path ` ? ) . Also , apparently ` join ` stands for ` os.path.join ` ( I might be wrong ) . In any case , I replaced ` join ( directory , file )` with simply ` files ` and I got what I needed .

Your code in the other question was just fine , just replace ` read_csv ` with ` read_excel ` .
( 5 ) Loop over list of files to append to empty dataframe : #CODE
This is certainly OK , but I think the approach in the almost identical question #URL to append to a list and then ` pd.concat ( the_list )` is cleaner .

Instead of setting up a meshgrid , just unstack . That positions ` b ` as the columns and leaves ` a ` as the rows , with each value in its correct place and ` NaN ` in an where the a / b combination does not exist . You can fill those with 0 .

' module ' object has no attribute ' cut '
I am trying to use the pandas " cut " method , but am getting an ` object has no attribute ` error . I've imported pandas and it works for other commands . Is this a known issue or am I missing something ? I restarted Python thinking this might just be a random bug but it doesn't seem to be ...
Your version is quite old . I'm guessing it doesn't have the method ` cut ` . Try updating pandas .
The ` cut ` function was introduced in pandas v0.8.0 . Update to a newer version and your code should work !

how to do right join where key is null in python pandas
Is it possible to do a right join where key is null in python pandas .
That is , can I join DateFrames to produce only values from the right that do not match the left ?
You could make a dummy column of ones in each dataframe , use pd.merge() to right join the dataframes , and then use a mask to filter out the nulls . #CODE

I am looking for a vectorize way to do this but the closest thing I found on stack shows the problem can't be resolved because 2 values update depend on the i term see : #URL

Re-shaping pandas data frame using shape or pivot_table ( stack each row )
What I want is to stack it so that it takes the following form , where the columns identifiers have been changed ( to X and Y ) so that they are the same for all re-stacked values : #CODE
I am pretty sure you can do it with pd.stack() or pd.pivot_table() but I have read the documentation , but cannot figure out how to do it . But instead of appending all columns to the end of the next , I just want to append a pairs ( or triplets of values actually ) of values from each row .
So I want to stack each individual row selectively ( e.g. by pairs of values or triplets ) , and then stack that row-stack , for the entire data frame , basically . Preferably done on the entire data frame at once ( if possible ) .

But that's not really useful . So on top of finding the ` inf ` s , we should stack the column index into the rows ( unpivot , if you will ) then drop all the ` NaN ` values . This will give us a nice summary of the rows / columns with ` inf ` s . #CODE

can you post some of the data in a copy / pastable form ? my general advice is to create a hierarchical index on the columns and then simply use the ` stack ` method of the dataframe .
Then if you ` stack ` the dataframe , you get this : #CODE

I do step 1 once , then repeat step 2-3 many ( ~100 ) times . In the future I may need to pre-process ` emission ` ( apply ` cumsum ` or other functions ) before computing ` counts ` .
I save ` emission ` as an Extensible array ( ` EArray `) , because I generate the data in chunks and I need to append each new chunk ( I know the final size though ) . Saving ` counts ` is more problematic . If a save it like a pytable array it's difficult to perform queries like " counts > = 2 " . Therefore I saved counts as multiple tables ( one per particle ) [ UGLY ] and I query with ` .get_where_list ( ' counts = 2 ')` . I'm not sure this is space-efficient , and
Timestamps can be accumulated in RAM . However , we don't know the arrays size before starting and a final ` hstack() ` call is needed to " merge " the different chunks stored in a list . This doubles the memory requirements so the RAM may be insufficient .

Python Pandas interpolate with new x-axis
I want to interpolate the data so that I have a new series to cover ` X =[ 23:392 : 1 ]` . I looked up the document but didn't find where I could input the new x-axis . Did I miss something ? How can I do interpolation with the new x-axis ?
This can be done with ` pandas `' s ` reindex ` and ` interpolate ` : #CODE
The idea is the create the index you want ( ` s.index + idx `) , which is sorted automatically , reindex an that ( which makes a bunch of ` NaN ` s at the new points , and the interpolate to fill the ` NaN ` s , using the ` values ` method , which interpolates at the index points .
Let me know if thou think just passing an array to ` interpolate ` would be easier . We decided to keep the API ( and implementation ) simple and use ` reindex ` ing instead .

Now I can loop through the entire thing and ` split ` , but can I do it with map ?
Something like - ` data [ ' i '] = map ( split_and_get_first_part , data [ ' interest '])` ?
Is " one.a " a string , and you just want to split on the ' . ' character ? Is there a specific reason to use map instead of a list comprehension , and did you try writing ` split_and_get_first_part ` already ?
You could use the ` map ` method : #CODE
You are genius . I don't know why I didn't think about using the ` map ` method !
But it still has to iterate over the entire list so , while its more concise than a ` for ` loop , it's not necessarily more efficient . It's possible that ` map ` is a more efficient way of looping than ` for ` in this case , but you'd have to do some tests to see if that is the case . It's worth noting that in Python 3 , ` map ` returns an iterator rather than a list , so , depending on what you're doing with it , it may well be more efficient in Py3 .

@USER : That's odd . I just tested this on ` pd.concat ([ data ] *1000 )` and found ` str.contains ` to be 2x slower than ` apply ( lambda x : ' Fruit ' in x )` . Perhaps my version ` 0.12.0-933-g281dc4e ` is too old ?
The difference is that these are actually slightly different operations . contains is more like this : `` reg = re.compile ( ' Fruit ') ; data [ data [ ' Type '] .apply ( lambda x : bool ( reg.search ( x )))`` ( also contains handles `` nan ``

Merge Duplicates based on column ?
Instead of the ` map ` , you could use ` df [ ' fruit '] .str .lower() ` here .

4 1386865557684 1 M 0 30.526506 180 ' , which is what I want . Also , it looks like I did successfully drop the non ' M ' events , which was the goal . Thank you !

You can do a monthly ` resample ` on each group .
And then set it as the index , groupby on ` [ ' Code ' , ' ID ']` and then apply a ` resample ` on each group : #CODE
But you can also work further with your results , ' unstack ' ( bring index levels to columns ) and then plot : #CODE

in a pandas dataframe how can I apply a sort of excel left ( ' state ' , 2 ) to only take the first two letters . Ideally I want to learn how to use left , right and mid in a dataframe too . So need an equivalent and not a " trick " for this specific example . #CODE
For last two that would be ` df [ ' state '] .str [ -2 :] ` . Don't know what exactly you want for middle , but you can apply arbitrary function to a column with ` apply ` method : #CODE
I have another example where i am try to apply the first two digits of an 8 digit number . then i get the error . ' invalid index to scalar variable ' how can i apply the above to take the last 2 numbers in ' year ' ?

I'm making a sum between an empty array and a scalar . Notice that if I construct the array with ` np.array ` or with ` pd.Series ` it works fine ( the result is an empty array ) . The same is true if I replace ` np.min ([ 0.5 ])` ( which is a ` np.float64 `) with a float : ` 0.5 ` .

I would like to make it so that I have a table with the values of column 0 on the x axis and the values of column 1 on the y axis with the corresponding value of the pair at the intersection of the rows and columns such that it looks like :
Generally , you'll get better answers here if you share a bit of what you have already tried before asking for help . As it stands , it sounds like you want us to do your work for you . To get you started , you're looking for the ` set_index ` method and the ` unstack ` method .
You want a [ pivot table ] ( #URL ) . ` pd.pivot_table ( rows=0 , cols=1 , values=2 )`

Python Pandas : Join on unique column values and concatenate
How can I join these dataframes based on ` id ` and then concatenate their columns together ? The desired output is as below : #CODE
There may be a better way with ` concat ` , but this should work : #CODE

Basically , there's no need to apply the set of functions to the two groups separately and append the results together . That's essentially what group by is doing : split , apply ( separately ) and combine .

I've also tried faking a value column of all 1's and pivoting , but almost immediately get a memory error , likely because pivot produces a dense DataFrame .
what for are you building this pivot ? may be it's an XY problem , and you don't need it at all ?
Don't know what is vis . For basket analysis , it is a clustering method , and I am pretty sure all cluster methods can deal with your initial data without need of pivot .
I don't think you will a memory issue with this , as the final result won't be that big ( and so the unstack won't blow up ) #CODE
just do it iteratively then , e.g. do a section , repeat , do a section , then join the resulting frames .

I want to append pandas data frames to the ends of CSV files . The tricky part is when I append rows , some of the times the columns may be different . I want code like this #CODE
Your best bet is to join the DataFrames into one with all the columns represented . Otherwise you're not just adding rows " to the end " of the CSV file , you have to go back and change the header .

I have tried to look at Pandas ` apply ` and ` groupby ` methods , but can not come up with something that generates the desired overlapping groups .
Then do ` value_counts ` for every group and combine the results into a dataframe . ` cumsum() ` the counts and drop even rows will get the right counts .

thanks but how do I apply that to a dataframe .

I have a very long time series over 10 years with half-hourly measurements as Csv file . Every now and then the measurement device break down . I want to interpolate this gaps either with the monthly average or a moving average ( which neglect missing values ) . I guess I need a for-loop to do this but I have no Idea how to do this exactly . Could anybody help me ?
So i get the daily sum of my evaporation data . I can resample the monthly daily average as well but I don't know how to tell Python it need to use for each gap the meanvalue for this specific month .
I'd propose to interpolate the missing values from the two surrounding values ; that should be closer to the real missing value than a monthly average .
Pandas has a nice ' interpolate ' function on both series and dataframes : ( #URL ) . I'm going to suggest , especially if you have ' several days ' of missing data , that you just leave the values as NaNs ( #URL ) . Pandas has really nice support for plots with NA values and seeing a plot that has the correct measurement values and then a ' gap ' is easily to interpret . Also that approach gives additional information , lets say that you're looking at the plots and you see that the weekends have more gaps than other days , that might indicate the measure device is less stable on the weekend ( or whatever ) .
Yes , thank you . I know this interpolation function of pandas , but it uses only some values before and after the gap . this is too inaccurate . the data represant evaporation in a forest . If for some days the device broke down and it interpolate just the values before and after the gap then I don't get the course over the day and it's in general to inaccurate .
I agree that for your use case interpolation might not be what you want that's why I suggested just leaving the data as NA values . Pandas has great support for it and functions like ' mean , min , max , std ' will all work well ( do the right thing ) with NA values . If you replace the missing values with ' monthly average ' than all of those stats will be incorrect .
the delimiter is " ; " . I need to interpolate it cause I want to sum up all values per year to create a water balance . This values are evaporation data . If i build a cumulative sum for a year with so many missing values , the sum will be much to less below the real evaporation . So I need to interpolate .

You could replace ` df [ " class "] == 1 ` by another condition .

Automatic PostgreSQL CREATE TABLE and INSERT from CSV or Pandas DataFrame

when apply this concept to my larger working file I seem to lose the order of my columns . so month comes before product and month10 comes before month2 etc . why would this happen or rather how can i avoid losing my shape ?

@USER - That's what the ` replace=True ` in ` np.random.choice ` specifies . ( I mistakenly assumed that you wanted sampling with replacement . ) You can just specify ` replace=False ` , but you'll get an error with city / states that have less than 3 listings . If you'd like to only repeat when there's less than three , then specify ` replace=len ( x ) < n ` . If you want to just return all of the available listings when there are fewer than 3 , then you can just add an extra line ( ` if len ( x ) <= n : return x `) .

Pandas , concat Series to DF as rows
either in the Doc's or other questions . Since you can append two DataFrames by row
While not especially efficient ( since a new object must be created ) , you can append a
single row to a DataFrame by passing a Series or dict to append , which returns a new DataFrame as above . End Quote .
and attempting to append " S1 " produces an error . My question is WHY will appending " S1 not work ? The assumption behind the question is that a DataFrame must code or contain axes information for two axes , where a Series must contain only information for one axes . #CODE
You were close , just transposed the result from ` concat ` #CODE
Tom , Using your answer , if I now want to concat another Series to the result I get : AttributeError : ' Series ' object has no attribute ' _data '
Can you edit your question to show the third Series you want to concat ? If you have it at the same time as ` s1 ` and ` s2 ` , you can do ` pd.concat ([ s1 , s2 , s3 ] , axis=1 ) .T
Please see the edited version of the problem , the concat operations do not happen at the same time .
It's still not clear what your 3rd Series is . And again , you don't need to make an empty DataFrame . Once you have the DataFrame from my answer , you can add additional rows / columns with ` .loc [ 2 , :] ` or ` append ` or ` join ` , take a look at the [ docs ] ( #URL ) .
is to just convert the data to an array of lists and append the array ( s ) to the DataFrame .

So I want to replace #CODE
" Google or learn " is a generic attitude that could apply to all forums , especially Q& A-style forums . However , I'm obviously here for a reason . " Vote Down requires 125 reputation . "
Perfect for what I wanted . I just needed to understand how to apply the string operations to the dataframes / series , but I guess that's as simple as it is for strings . For row [ 0 ] - row [ 0 ] .capwords() should solve several problems at once , I expect !

When I tried to insert the data through : #CODE

I have tried ` apply ( lambda x : set ( x ))` but it only works on individual lists as opposed to the entire column .

What are you doing with the index that you need to format it this way ? There's the ` index.format ( ' %Y-%m-%d ')` method , which returns a list of strings . Are you going from higher frequency data to lower frequency ? In that case you want to look at ` resample ` .

So if the DataFrames are in the right order , you can simply use ` concat ` like so : #CODE

this will raise starting in 0.13 , you need to `` apply `` to your groupby to get back a dataframe , see : #URL

My attempt : I tried to replicate ` df3.col1 ` ` len ( df.columns.values )` times to get a dataframe that is of the same dimension as ` df ` : #CODE
But this creates a dataframe of dimensions 3 * 5 , whereas I am after 5*3 . I know I can take the transpose with ` df3.T() ` to get what I need but I think this is not that the fastest way .
Another way is create list of columns and join them : #CODE

Happy to help . I wonder if this is worth proposing as an enhancement to Pandas . There are all sorts of things I wish I could do with Pandas DataFrames in this spirit . Color only certain columns . Normalize before coloring . Etc . The open question for me is " what's the right general approach " ? I.e. Is it something based on ` pcolor ` or is it something that should involve the DataFrame HTML representation ? What do you guys think ?

I would like to use the ` pandas.rolling_apply ` function to apply my own custom function on a rolling window basis .

I have run into , what I think , is a fairly simple problem yet again . I would like to apply the following function to a pandas data frame . #CODE

You will need to [ index ] ( #URL ) properly , is this a pre-existing dataframe or are you populating and creating from scratch , you can index using integer or label based , if you were doing this from scratch then you would need to keep track of the row number as you append each entry so something like ` results.iloc [ row ] [ ' siteClean '] =host ` or similar . Still it will be much simpler and easier to understand if you populated the list first , if you have a lot of urls though then it could be slow
In general , it's better to avoid looping over your frame's rows , if you can avoid it . If I understand your problem correctly , you want to look at a single column from your frame , and apply a function on each element of that column . Then you want to put the result of all those function calls into a column of the original frame . Maybe a new column , maybe in place of the old column . This sounds like a job for ` pd.Series.map ` . #CODE
Then you can use ` map ` to generate a new ` Series ` whose entries are those of the input transformed by the specified function . You can stick that new output series wherever you like . It can be a new column ( in your old ` DataFrame ` or elsewhere ) or it can replace the old column . Note that ` map ` only works on a ` Series ` , so be sure to select down to one column before using it : #CODE
then you would replace the old column with the new one :

First change the ` bool ` column to actually booleans ( also be careful with your names . DataFrame has a ` bool ` method ): #CODE
Finding the minimum dates is pretty easy . Use the ` bool ` column to index into ` df ` : #CODE
There are probably a bunch of ways to set the values , but one is to set ` Group ` as the index and ` join ` the ` dates ` . #CODE
Thanks . Worked except for joining bit . I had to convert the series to a dataframe and then perform a left merge : ` pd.merge ( df , dates , how= ' left ' , on= ' Group ')`

One way I could think of was to put the treated cell in a one-cell DataFrame and then doing a left merge on it , but this is much too inefficient for such big data .
You can then refer to the row , or specific entry , using loc : #CODE

I am reading a text file which was copied from a CSV file . When I read the file in python , I get a ton of unnecessary repeating lines as seen below . How can i strip away those three unwanted lines , including \cf0 and \cell\row at the beginning and end of each text ?

If I replace the ` plt ` command with the pandas command , so I have : #CODE
Added ` bottom= 0.1 ` to ` hist ` call fixes the problem , I guess there is some kind of divide by zero thing , or something .
I'd recommend using the ` log=True ` parameter in the pyplot hist function : #CODE

Oh , I really like that solution . I was doing something similar , where I added a diff column , set the values to 0 if it was below my cut , and then ` cumsum ` ed that . I like making a grouping series as a one shot a lot better .

If you are using UNIX type OS , please check if you care just about merging files how to merge two files consistently line by line

i modified my answer so it no longer uses indexes . gm still getting array of floats so i have to map them to ints to be used with iloc

And concat : #CODE
You might consider passing the families as the ` key ` parameter to ` concat ` if you want a hierarchical index for the columns with ( family , species ) pairs .

However if i insert a new column and repeat the same procedure the new attribute is stored but old attributes are lost #CODE

You can use the tilde ` ~ ` to flip the bool values : #CODE

And merge back ( adding 1 to start from 1 instead of 0 ): #CODE

It looks like you want a ` rolling_max ` and ` rolling_min ` combined with a ` shift ` since you don't want to include the current value in the calculation : #CODE

This works perfectly . But it feels very slow . Is there a particularly slick algorithm in pandas or another toolkit to do this fast ? I took a shot at writing something bespoke : it keeps track of all sorts of intermediate data ( locations of observed maxima , locations of previously found drawdowns ) to cut down on lots of redundant calculations . It does save some time , but not a whole lot , and not nearly as much as should be possible .

In a python pandas DataFrame , how do you shift row indexes up to fill empty rows ?
So I want to preserve the data , but shift the indexes together . ( specifically , I have this data , and I want to plot it with matplotlib , but I don't want there to be giant spaces in the data , so I want the data points to be pushed together . Is this the right way of doing this ? )
The ` drop=True ` says not to try to insert the original index into the DataFrame .

Seems there is no elegant way . This is the workaround I just figured out . Basically create a repeating list just bigger than original dataframe , and then left join them . #CODE

You could use the map function . Something like lengthDF [ ' size '] = lengthDF.index.map ( lambda x : ' small ' if x < 10 else ' large ') . Obviously , instead of the lambda , you can have a more complex named function that returns any number of values ( e.g. , ' small ' , ' medium ' , ' large ') based on what each index value is . If performance is a concern , you may want to run some tests : I'm not sure the map function is vectorized , so it may be faster to do a couple of passes as in the answer above rather than use map to do it in one line .

Related #URL I guess the difference is how to separate out into multiple sql jobs and concat e.g. WHERE id is within a certain range ( but I suspect this will rarely be worth the effort in itself ) ...

if len ( x.split ... )

And now , instead of ` value_counts() ` , use ` apply ( f )` . Here is an example : #CODE

in _save_chunk ( self , start_i , end_i ) 1094 ix =
-> 1096 lib.write_csv_rows ( self.data , ix , self.nlevels , self.cols , self.writer ) 1097 1098 # from collections import

So , say I want to perform this calculation on every ratio in the row , and then take the median of the three resulting values : #CODE
Furthermore , you normally don't need to ' iterate over the df ' as you do here . To apply a function to all groups , you can do that directly on the groupby result , eg ` df.groupby() .apply ( .. )` or ` df.groupby() .aggregate ( .. )` .
Can you give a more specific example of what kind of function you want to apply to the ratios ?
To calculate the median of the three ratio's for each sequence ( each row ) , you can do : #CODE
The ` axis=1 ` means that you do not want to take the median of one column ( over the rows ) , but for each row ( over the columns )
Another examle , to calculate the median of all Ratio1's for each ID , you can do : #CODE
Here you group by ` ID ` , select column ` Ratio1 ` and calculate the median value for each group .
` data [ ' ID ']` will give you the ` ID ` column , so you cannot use it as a key . You want one specific value of that column . To apply a function on each row of a dataframe , you can use ` apply ` : #CODE
2 . I basically want a median of the three ratios for each sequence , and some other statistical calculations ( std , cv , etc . ) . In the later stages I want the medians for all Ratio1 of ID1 , medians of Ratio2 , Ratio3 , and then another median of the medians from the ratios .
Oh , very cool , Joris ! What if I wanted to perform some other , non-standard calculation on each ratio , and THEN calculate the median of the resulting three values ? What if some of this calculation required a value from a separate dictionary , where the ID to corresponding three ratios must be input as key ?
Maybe just calculating and assigning it to a new column , and then taking the median . But , if you have further questions , try to give a really specific case ( so not ' some function ' , it all depends on what kind of function ) , and some code with what you already tried or with which you can reproduce the problem . And going through the docs Jeff linked to can be very helpfull , just to learn the concepts .

You can apply an ` expanding_mean ` ( see docs ) to each group : #CODE

I would like to replace every line that does not have " GE " in the " from " or " to " column , by two lines , one having " GE " in the " from " column and one having " GE " in the " to " column .
In the example above , I would replace the third line by the following two lines :
GE VD 1500
VS GE 1500
I tried using " apply " but I can't figure out how to return a correct data frame . For example #CODE
Apply that function to the rows : #CODE
And concat together #CODE

Set the index to the exchtime , resample at 1second interval and take the max

concat part of string from groupby pandas python
How do i concat part of a date with a file name after doing groupby .
The problem is that you are concatenating a pandas Series ` Fx [ ' File ']` with the string representation of a pandas Series ` str ( Fx [ ' Date '])` , what you need to do is apply the ` str ` cast function to the elements of ` Fx [ ' Date ']` like this : #CODE

Pandas boxplot x-axis setting
I want to create a boxplot of data collected from four different sites over the past twenty years ( i.e. each site will have 20y of data ) . This will produce 80 boxes on the figure . To make the figure legible , I want each box offset , and have different color boxes for each site . This will yield a repeated series of boxes ( e.g. boxes for site1 , site2 , site3 , site3 , site1 , site2 , site3 ,... ) . Creating a boxplot is not a problem ; offsetting the boxes does seem to be an issue . e.g. #CODE

You can see which dates failed to parse by checking ` isnull ` : #CODE

What is wrong here , and why won't all the values map to the correct ID ?
Drop NAs if you haven't already , then explicitly convert whole columns to the appropriate dtype as needed .
I have also run into strange problems sometimes if my index wasn't continuous ( if you drop NAs , you may have to reindex ) , or if my x-axis values weren't pre-sorted .
Or drop the letters , as you suggested , and convert objects to numeric instead :

First , groupby code and colour and then apply a customized function to format id and amount : #CODE
What functionality do you what ? A dictionary map the code , colour pair to a dataframe contains id and amount ?

The simplest solution is to parse your ` Dataframe ` and replace your price of ` - 1.00 ` by ` np.nan ` with something like : #CODE

Apply read_csv instead of read_clipboard to handle your actual data : #CODE

You can use ix method : #CODE
hey i can call specific column ( label ) s via that call also using loc . I am looking specifically for using the colon /: command , i guess what i understand as ' slice ' . My example was poor because DF was so small , but this isn't really what I am looking for .

In the last loop are you doing anything else with data2 , or are you just reading it ? Have you tried appending that to a list and ` concat ` enating the results ?
You now have a time-indexed dataframe ! If you really want to have all of the data in one dataframe ( from all of the file types ) , you can just adjust the ` glob ` to include all of the files using something like ` glob.glob ( ' * .xls ')` . I would warn from personal experience that it may be easier for you to read in each type of data separately and then merge them after you have done some error checking / munging etc .

I'm trying to drop rows from a groupby object based on a condition : #CODE
With a dataframe , if I wanted to drop anything with a value less than 10 , I could just call ` df [ df [ ' value '] =10 ]` . I can't seem to find a way to do something similar with a groupby object to get . #CODE
I guess you are assuming that you can select stuff within the groupby object . As far as I know , you can't . But you can do that in the resulting object , after you apply your aggregation function .

I'd like to show the scatter plots with data points for one group of data , let's say , in green and the other group in red in the very same scatter matrix . The same should apply for the density plots on the diagonal .

Pandas time series merge : result longer than components
Length : 2863 , Freq : None , Timezone : None
Length : 22992 , Freq : None , Timezone : None
Maybe I am missing something , but how can an outer merge / align ever yield more data points than the two constituents together ? For sure 22992 + 2863 28345 !
Thanks for the quick reply ! I have now managed to find the source of the above behavior by isolating the parts of the dataset that produce the " excess data " upon doing the outer merge . As it turns out ( and is actually documented for Pandas - my bad here ) , the outer merge produces a Cartesian product for data points with the same index value ( in my case the same ms timestamp ) . That means that for , say , 2 datapoints in ts1 and 4 in ts2 with the same time index , the result is 8 merged datapoints . This contracts with merge.xts in R which I have used a lot in the past where the result would be 6 .

Now , I can resample this into OHLC data using ` df.resample ( freq , how={ ' price ' : ' ohlc ' } )` , which is fine , but I'd also like to include the volume .
When I try ` df.resample ( freq , how={ ' price ' : ' ohlc ' , ' volume ' : ' sum ' } )` , I get :
The problem isn't with the resampling , it's from trying to concat a MultiIndex ( from the price OHLC ) , with a regular index ( for the Volume sum ) . #CODE
I guess you could manually create a MultiIndex for ` ( volume , sum )` and then concat : #CODE
But it might be better if resample could handle this automatically .

@USER does that mean there is no way of putting an alphanumeric category like " missing " or " no data " in a column which is datetime64 ? Is the best way to fix this to convert it string , replace the null values , then convert it to datetime again ?

You can ` unstack ` the result : #CODE

Merge of multiple data frames of different number of columns into one big data frame
You need to decide in what axis you want to append your files . Pandas will always try to do the right thing by :
Notice that ` read_csv ` is transposed with ` axis=1 ` , so it will be concatenated on the column axis , preserving its names . If you need , you can transpose the resulting DataFrame back with ` d.T ` .
That's a nice ipython trick ! ... Can't you use axis=1 to concat to avoid .T ?
@USER , YES ! I must have been either using pandas 0.12 or doing something else wrong when I tried axis=1 with ` concat ` and failed . Not it works . :)

How to apply hierarchy or multi-index to panda columns

You never mentioned you had single unpaired Series ( es ) instead of a DataFrame . You need to provide an example that's faithful to your context so that our solutions apply . Can you re-state your problem ?
Groupby ` Group ` first and then apply your customized function : #CODE
@USER Not sure I understand your point . Do you mean that when you use an apply on a groupby mapping , you can treat the object received by apply as a portion of the original df ?
@USER , yes . Series.apply() will apply a function to every element of the Series . Every object is a portion of the entire Series . Groupby will apply a function to every portion of the DataFrame that matches a value in the grouping Series ( es ) . Together all groups make the whole DataFrame .

I need to group some data in a pandas dataframe but the standard grouping method does not quite work how I need it to . It must group so that each change in " loc " and / or each change in " name " is treated as a separate group .
The default grouping function ( correctly ) groups all the loc and name values so we are only left with 3 groups ( john / abc is 1 group ) . Does anybody know how the grouping can be forced to group how i require it to ?
This is not really a job for ` groupby ` because the order of the rows matters . Instead , compare consecutive rows by using ` shift ` . #CODE

To find the unique values for only one Data Frame I've used ` pd.unique ( df1.values.ravel() )` . While looping through the Data Frames , using a list to append the unique values throws a memory error ( The range mentioned in df1 , df2 , df3 is just an example . The actual range could be millions ) . How do I handle this ?

Having B column flattened , you can merge it back : #CODE

Looks worrying that you get data from January 2013 although your time series starts in September 2013 . Is there an issue with your rawdata ? Maybe something irregular , that you did not observe so far , because the file is so big ? The docs for ` resample ` are a bit thin , unfortunately ( #URL ) ...

` df ! = 0 ` creates a boolean DataFrame where ` df ` is nonzero : #CODE
` ( df ! = 0 ) .any ( axis=0 )` returns a boolean Series indicating which columns have nonzero entries : #CODE

I currently have a series of 18 DataFrames ( each representing a different year ) consisting of 3 Columns and varying amounts of rows representing the normalize mutual information scores for amino acid residue positions like :

But when it comes to analyze daily datas if i change resample to " W " in day_min ( 4th line ) , and to " D " in 6th line it gives this error : KeyError : Timestamp ( ' 2013-01-01 00:00 : 00 ' , tz=None )
You can use TimeGrouper , by week ( and see whether the value is equal to the week's min ) and then resample by day : #CODE
If you wanted this as columns as days of each week , you could do the groupby within the apply : #CODE

You could use a groupby apply for this : #CODE

Pivot data and maintain original sort order
I'd like to pivot my data which results from a django queryset while maintaining the original ( non-alphabetical ) sort order on the index column . The pivoted data will then be used in a google visualization line chart .
I've hacked together my own code to do the job but it's a bit ugly and I was wondering if it could be done using a pandas DataFrame pivot .
Using pandas pivot produces the following results . The pivot worked but the rows are in the wrong order . #CODE
I've investigated using google visualization pivot but that only seems to work on a query not on an existing DataTable .

now concat #CODE

Also using python 2.7.5 and pandas 0.12.0 . Also worth mentioning I would like to apply this to datasets of up to 1 million rows . Forgot to mention this !

I want to append a ` reason ` column that gives a standard text + the column name of the minimum value of that row . In other words , the desired output is : #CODE
It seems like the transpose approach is twice as fast .
@USER : Using ` apply ` to do this with a ` lambda ` expression is a worse solution than using the builtin ` idxmin ` directly on the transposed data . For one , ` idxmin ` automatically skips NaN . For two , ` idxmin ` is already optimized to function as an array operation , whereas your ` lambda ` incurs the cost of a function call across the rows , needlessly . For three , relying on the Pandas API preserves the modularity and readability of the code . Reading that ` lambda ` is needless extra work for anyone using your code . For four , ` idxmin ` is already tested and documented , whereas the ` lambda ` isn't .

python pandas : apply a function with arguments to a series . Update
I would like to apply a function with argument to a pandas series : I have found two different solution of SO :
python pandas : apply a function with arguments to a series
Passing multiple arguments to apply ( Python )

I have a Date Time series from which I am trying to select certain elements , along with their date time index in order to append them to a new series .
Anyway to get the index too ? ( I don't just want to print it , I want to append some to a new series . For example , add all the numbers divisible by two to a Series , along with their index ) .
Don't completely get what you mean with " I want to append some to a new series " , but you can access index with ` index ` property : #CODE

It would of course be great if the timezones were not converted all to UTC . However I realize this problem may still persist for a while . But if anyone has a hint for a workaround I'd be glad to hear ( I tried passing a tz directly to the DateFormatter . That works , but the Locators don't seem to like it much ) .

Intersection of date ranges , calculation over all elements with an intersecting date range
if you're using pandas and assuming the date are in datetime format , you can group by on ' code ' and then apply a min , max function to that .

It sounds like you want to do a groupby , difficult to answer without more info ( but perhaps that should be a separate question ) . What I meant was you shouldn't create an empty DataFrame ** or ** concat with an iterrows ( just concat , or join / merge ) ... ATM this is an [ XY problem ] ( #URL ) .
I want to ` concat ` a row , because I'm validating it before putting it into the new ` DataFrame ` . For this reason I also can't reassign ` empty ` on each ` for ` loop .

Now once you have your line of text insert it into a string and then split using commas . That will give you a string array . Then make an int array by converting the strings to text . This should not be a problem as long as all data in the column are integers . If not , test for non-integer values and convert them to strings that are valid intergers . E.G. if array [ 0 ] == " no data " array [ 0 ] = " 0 " , or array [ 0 ] = null . Then create column 3 by adding the integer values for the first and second columns together .

` merge() ` can't do this kind of join , but you can use ` searchsorted() ` :
do a regular merge with outer join ( how= ' outer ') ;
drop all the rows you don't need from the outer join .

@USER thanks ! So ` apply ` gets called once for the column or once per element ? ` x.astype ( float )` is the whole column or just one element ? and ` x.sum() ` is the group by group sum ? How do these rules work ? :)

Where ` Foo ` is the index , and ` Loc 1 ` to ` Loc 7 ` are the columns . But converting to Numpy matrices or recarrays doesn't seem to work for generating input for ` nx.Graph() ` . Is there a standard strategy for achieving this ? I'm not averse the reformatting the data in Pandas --> dumping to CSV --> importing to NetworkX , but it seems as if I should be able to generate the edges from the index and the nodes from the values .

FYI You can create those tab numbers like this : ` list ( map ( str , range ( 1 , 2 8))) ` .

If you want it as ` int ` you can map the list using ` map ( int , list1 )`

I think you can bring the concat out of all the indentation . My feeling is that this should be broken into separate functions for readability .
Note : This does more concats , but I think it's worth it for readability , you could do just one concat ...

I just can't make it , no matter what I try ( I tried ` apply ` with ` axis=1 ` and have it return a tuple , a list , a Series object .. neither worked ) .
I saw that I can create a DataFrame and set the dtype to ' object ' and then I can put tuples in a cell . How do I do it with ` apply ` ?

Drop some Pandas dataframe rows using group based condition
I've got some data on sales , say , and want to look at how different post codes compare : do some deliver more profitable business than others ? So I'm grouping by postcode , and can easily get various stats out on a per postcode basis . However , there are a few very high value jobs which distort the stats , so what I'd like to do is ignore the outliers . For various reasons , what I'd like to do is define the outliers by group : so , for example , drop the rows in the dataframe that are in the top xth percentile of their group , or the top n in their group .
I could iterate through the groups , I suppose , and for each group find out which rows to drop , and then go back to the original dataframe and drop them , but that seems terribly clumsy . Is there a better way ?
That's interesting , and addresses the specific example I gave very nicely . Is there a more general way of doing it , for example if I wanted to drop ( or keep ) only those rows that had a value above the group mean or mode ? or that were within the top xth percentile ?
@USER you could do a groupy apply to return just those rows ...

One way to get the desired result is to use an apply e.g. via the following function : #CODE

You could use set to create a list of the index locations that conform to your rule , and then use that list to slice the data frame . For example : #CODE

I would like to create a DataFrame df3 with only the data from columns [ ' c '] renamed respectively ' df1 ' and ' df2 ' and with the correct date index . My problem is that I cannot get how to merge the index properly . #CODE
Well , I'm not sure that merge would be the way to go . Personally I would build a new data frame by creating an index of the dates and then constructing the columns using list comprehensions . Possibly not the most pythonic way , but it seems to work for me ! #CODE
You can use concat : #CODE
Hi , I have found the issue : I had a duplicate row index in my data that was making the concat function raise an exception .

How to index into a pandas multindex with ix

It's worth mentioning that you can use ` loc ` ( which I think is slightly cleaner ): #CODE

I found this code in Stack ( show feature names after feature selection ) but don't totally understand it and have not been able to get it to work .

Assuming df1 is your first dataframe and df2 is your second one , you can merge on the country and perform a left join , you need to rename the country column on df2 first though : #CODE

Note : perhaps it makes more sense to concat rather than append : #CODE

You can also try this : merging two tables with millions of rows in python , where you use a merge function that is simply ` drop_duplicates() ` .

Basically you just have the function that does ` row / row.sum() ` , and you use ` apply ` with ` axis=1 ` to apply it by row .
Another option is to use div rather than apply : #CODE
If you're looking for a percentage of the total , you can divide by the len of the df instead of the row sum : #CODE

that's the correct behavior , but I'd like to retain the ` cat ` column information in the resulting df . can that be done ? do I have to ` merge / join ` that info in later ? I tried : #CODE

I've looked at some threads on Stack and the pandas FAQ to no avail , even when using these under the display namespace ( or without ) , as I've attempted here .

This should truncate everything before the old date . You can also add a ` after ` argument to whittle it down further if you desire .

I am given a ` new index ` - ` new_id ` couple . If the ` new_index ` ( combination ) already exists in the ` multi_df ` , I want to append the ` new_id ` to the existing index . If the ` new_index ` does not exist , I want to create it and add the id value . For instance : #CODE

Here is my debug code , when you do indexing , Index object will create ` _tuples ` and ` engine map ` , I think the memory is used by this two cache object . If I add the lines marked by ` **** ` , then the memory increase is very small , about 6M on my PC : #CODE

numpy diff doesn't deal properly with timedelata ( well it works but it's pretty raw ); you should have at least numpy 1.7

The problem with the first approach is that I have no way of accessing my categorical data ( i.e. the ` subject ` , ` stimuli ` , and ` resp ` columns , amongst others I've left out here ) , while the problem with the second is that I end up with a ` DataFrame ` thousands of columns wide ( and wider again for each transformation I apply : velocity at each step , angle at each step , etc ) , and no useful way of accessing specific time serieses ( i.e. what I've been currently calling as ` data.rx.mean() .plot() ` .

Python pandas concat intersect
I am struggling hard to get an intersection of two DataFrames in pandas . #CODE
All i want is to find an intersection of 2 DataFrames by index so only lines 8 through 10 appear in the result .
as it adds the DataFrames contents together as it should do . Why does concat over axis=0 doesnt work ? How to concatenate DataFrames by index ?
You want to try a merge . ` concat ` is better for gluing multiple frames together , merge will handle detection of overlapping rows and all that : #CODE

Resample and align #CODE

Apply numpy functions to pandas DataFrame
I have a DataFrame where each element is a numpy array and I would like to apply to them numpy functions .
now let's try to apply ` np.dot ` along ` axis=1 `

Why doesn't my apply function return the length of the string ?
Just for the sake of trying something , does the same error happen if you replace your use of ` apply ` with ` map ` since you're looking to spray the operation onto a single ` Series ` object ?
What if you try ` csv [ ' text '] .apply ( lambda x : len ( str ( x )))` ?
and you can also use ` map ` instead of ` apply ` since you're operating along the values of a ` Series ` .

Is the issue that result of [ 11 ] is set not a list ( just apply set to it ) . Atm this feels like the [ XY Problem ] ( #URL ) ...

The only change I had to make is to replace datetime with datetime.datetime #CODE

Now , I ` apply ` it to the dataframe : #CODE
You should use bool rather than ' Y ' and ' N ' ... !
The thing you have to realize about apply is you need to write functions that operate on scalar values and return the result that you want . With that in mind : #CODE
Just want to clarify that when using ` apply ` on a series , you should write function that accept scalar values . When using ` apply ` on a DataFrame , however , the functions should accept either full columns ( when ` axis=0 ` -- the default ) or full rows ( when ` axis=1 `) .
OK - I think I just figured it out - to use functions on a dataframe , you have to use ( should use ) apply . So , I can chain together functions by using apply inside of the main function . Is that right ( does that make sense ) ?
It's worth noting that you can do this ( without using apply , so more efficiently ) using ` str.contains ` : #CODE

FYI , your method of using `` loc `` to convert doesn't work because pandas doesn't check whether the entire column is converted ( as you could have done something like `` df.loc [ 1:5 , ' one '] = df.loc [: , ' one '] .astype ( float )`` , so for efficiency we don't force this type of conversion . you could have done `` df [ ' one '] = df [ ' one '] .astype ( float )`` which would have worked ( but @USER soln is in general better for float conversion in any event ; `` astype ( float )`` would fail if their was a non-convertible entry ( e.g. a ' foo ') in your data ) .
I originally used df [ ' one '] but it did not work so I went and tried " loc " . I did retry and everything worked . Thanks .

Step-by-step , first we stack : #CODE

Me and some co-workers came across this same problem . What we ended up doing was doing a parallel process split on the file into smaller 1 million row file chunks . Then depending on how you are sorting , ou can come up with some sort of directory scheme to " sort " the files into . If it's transaction data , you could use AWK or pandas to parse out each 1 million row chunk into a relative year_quarter directory / file , and then you can sort on these aggregated files . If you need the data in one file , then at the end you can just stack them back together in order . Good luck !

yeah , should be closed brackets around ix : ` stateDF.ix [ targetState , " Full Name "]` ...

If you reversed the keys and values in your dict then you can just use ` map ` : #CODE
then call map : #CODE
This assumes that your keys are present in the map , if not you will get a ` KeyError ` raised
now call map to perform the lookup and assign back to state : #CODE
I also have some states which are not present in the dictionary . If I apply map() , the corresponding values in the new series are missing . Can I somehow specify that I want to apply identity function for the values not present in the dictionary , i.e. leave them as is ?
@USER .Escondido so for above example because New York was not in the dict you want it to stay as New York is that correct ? You could create a function that did a map lookup , if the key was not there then do nothing , or you could filter which values in the dataframe to assign to , only those which are present in the dict and convert those .

This reports the ` dtypes ` as ` float64 ` as expected , any idea why ` append ` with a dict is not working as expected ?
This is fixed in master / 0.13.1 ( when creating the frame to append it was not converting the object dtypes , which is what a dict starts out as ) . see #URL

Make your data into a TimeSeries object and then call resample : #CODE

You should also just do this to strip the whitespace : #CODE

So now I have a dictionary of dataframes , indexed by by StartTime and each one contains all of my data for a run , plus some extra to ensure I have it all for every run . But to compare two runs together I need to have a common X value to stack them on top of each other . Now every run is different and the point I want to consider " the same " varies depending on what i'm looking at . For the example below I have used the largest value in that dataset to " pivot " on . #CODE
My final aim is to have a dataset and methodology that means I can investigate the similarity and differences between different runs using different " pivot points " amd produce a graph of each one which I can then interrogate ( or at least tell which data set is which to interrogate the data directly ) but couldn ; t get past various errors with creating it .

Is it possible to specify more than one column for the merge in pandas ( python ) ?
I want to merge two tables , A and B . Table A has values in one column , in B these are spread amongst two columns : #CODE
are columns in B mutually exclusive ? if so , you can create a third column and use that to merge
Yes , they are ! You mean , merge B1 and B2 first ?
When you specify columns to join on , make sure to refer to them as strings . If you need to refer to two columns in your join , use a list of strings . #CODE

I have a problem with this too . I have multiple dataframes which I want to merge based on a string representation of several " integer " columns . However , when one of those integer columns has a np.nan , the string casting produces a " .0 " , which throws off the merge . Just makes things slightly more complicated , would be nice if there was simple work-around .

Can you explain what you mean by " The values aren't missing , but the column doesn't specify a value for each row on purpose " . Is this like a pivot table , where the values below the value are empty but implictly the same as the one above ? You may have to use a .ffill() on your FK columns to fill it before converting types .
If you can have a NaN in the FK column , then could you replace the NaN's with some other random number , let's say if all current FK's are positive integers , then use a negative int like -999 to distinguish empty values . Another option is to only include the rows that have a value : " df [ df [ ' FK_COL '] .notnull() ]" . Then apply the filling on NaN values , and save this dataframe to a new variable for loading into the database .

using apply you can make new value and assign it to new column
for R folks : there is no ifelse in direct form ( but there are ways to nicely replace it ) . #CODE
The functions ` prix ` and ` msg ` are fairly simple . The only tricky portion is the list comprehension ` [ m.append ( msg ( m )) for m in records ]` which iterates through all of the records , and modifies each to append your new field , created via a call to ` msg ` .

In case of two dimensional data , the simple map function will not work directly . To cope with this situation the numpy shape , reshape and ravel functions could be used like : #CODE
Great it works ! Just a quick comment , the original map function converts the 2d data to a 1d data ( sympy.latex works well with lists ) , hence it is required to take care of each cell individually -> ravel reshapes the 2d array into 1d and the subsequent reshape builds up the 2d version again .

Lastly when using cut is there a way to specify infinity so the last bucket is all values greater than say 60 ?

Where core_cols is a list of about 10 fields that I'm coping over and new_ids are the ids from the small DataFrame . This code works fine but it is the slowest part of my code my a magnitude of three . I just wanted to know if they was a faster way to merge the data of the two DataFrame together .
I tried merging the data each time with the merge function but process took way to long that is way I have gone to creating a larger DataFrame that I update to improve the speed .
And concat everything together when you want ( they are kept in a list in the mean time , or see my comments below , these sub-frames could be moved to external storage when created , then read back before this concatenating step ) . #CODE
then at the end read them all in and concat ( in memory ) , and write out a gigantic new file . The concat step could be done all at once in memory , or if truly a large task , then can be done iteratively .
I am able to use multi-processes to perform my computations AND write each individual Panel to a file separate as they are all completely independent . The only dependent part is the concat .
Just to provide additional feed back the first solution was the fastest option for my program . The 2nd option of concat the DataFrame and then reindexing increased the run time by about half .

It takes about 10 seconds to run ` df.col1.map ( lambda x : len ( x )) .max() ` when timing it with IPython's ` %timit ` .
You might be able to save some time by simply using ` map ( len )` -- the ` lambda ` does nothing but waste time here . Maybe 25% or so , I'd guess .
I came to the same conclusion when DSM commented on ` map ( len )` . Shaves off about 40% compared to ` len ( lambda x : len ( x ))` method .

Join flat index with hierarchical indexed DataFrame
I want to join them to receive : #CODE
In words , merge a single-level indexed dataframe into a hierarchical index dataframe . The missing values from the second level should be filled with copies .
This doesn't look that general to me , and I believe their is a way to do a merge directly , though maybe not w / o modifying the 2nd frame ( to make a merge key , e.g. this is kind of like cross merging ) . #CODE

E.g. you could use groupby apply with ` def f ( x ): return ( 1 . * x [ ' weight '] * x [ ' jobs ']) .sum() / x [ ' jobs '] .sum() ` but it will probably be less efficient than the above .

I haven't been able to find a method do this yet . I've looked at unpack / pivot / deaggregate ( couldn't find the right solution with any of these terms ... )

You could do this as a one line apply ( the first column being negative , the second positive ): #CODE

Using the standard dictionary check to see if a key exists , ` if ' df_coord ' in store.keys() : ` , returns false unless the ` / ` is included . Is there another simple way to evaluate for the existence of a key without having to join strings ?

then use apply to get your " Type " #CODE
original Apply : #CODE
revised Apply : #CODE
This will be faster than the apply soln ( and the looping soln )
Apply #CODE

I have a Pandas dataframe as below . How can I merge the ` round ` and ` square ` values under ` shape ` Series as ` other ` ? ( In R terminology , I want to merge the ` round ` and ` square ` levels of the ` shape ` factor into a new level labelled ` other ` . ) #CODE

This is a use case for the new ` query ` / ` eval ` functionality in pandas 0.13 #CODE
Your code just does one of the steps . In the full operation , I split the dataframe into three , add three new columns using different equations , and append back into the final data frame . In that case , databcc would be a temporary intermediate variable .
This should be extremely memory efficient and fast , because we update by reference ( and no the entire join is not materialised ) . ` on = " crystal "` performs a join on that column and finds matching row indices in ` data ` corresponding to each row in ` key ` , and on those matching rows , we simultaneously update / create the necessary columns .

You could create a dict that flips the values and call map , this would return a series and you can create a new dataframe and leave the original intact : #CODE
An easier way to describe your function is as x -> 1 - x , this will be more efficient that apply / map . #CODE

Append sum to a DataFrame
In Pandas , how do I append a sum of all columns of a dataframe to the bottow row ?

The reset_index ( drop=True ) is to fix up the index after the concat and dedupe . Without it you will have an index of [ 0 , 1 , 0 ] instead of [ 0 , 1 , 2 ] . This could cause problems for further operations on this dataframe down the road if it isn't reset right away .
Can also use ignore_index=True in the concat to avoid dupe indexes .

I have tried with both the ( pandas ) pd.ols and the ( statsmodels ) sm.ols to get a regression scatter plot with the regression line , I can get the scatter plot but I can't seem to get the parameters to get the regression line to plot . It is probably obvious that I am doing some cut and paste coding here :-( ( using this as a guide : #URL

But ` headers ` does not cut it . Which results in #CODE

Is there a simple way to either transpose the Series , or otherwise get it written as a row ?

I am looping around some other data then adding categories ( variable=temp_category ) one by one to my dataframe . If the ` category ` already exists , I want to get the ` description ` from the dataframe , append some text to it and then update the record .

I have a list that I loaded from a txt file and ran some code to match data . But I get ` TypeError : Unhashable list ` I looked at several answers on Stack and can't find out where I passed a list into the loop . I am guessing it has something to do with df because it works when I am not using data that was loaded in . #CODE
I think this behaviour is deprecated then changing , it'll return bool if there is no groups ( and the things in brackets are considered groups ... : s ) .
To explain the behavior of the depreciated ( in 0.13 ) match : It now returns bool unless there are groups in the pattern ( here the parenthesis are groups , hence C is returned in one row ) ... : s

transpose the data frame ( takes ages )

If I resample this DataField by any frequency , the timezone is kept : #CODE
their are a couple of outstanding bugs w.r.t to resample and extra binning : #URL if you would like to investigate and try to pinpoint ( or better yet fix ) would be appreciated ! you can comment on that issue directly

Thanks for the answer ! I changed the last line in your code like this : ` counts = df.groupby ([ df.day.to_period ( ' D ') , ' col_name ']) .agg ( len )` , as I need to group data by day and not by exact time from the index . It worked and gave me multiindexed Series object . However I still cannot plot this data as I cannot ` unstack ` them . I asked separate question on this issue here #URL
But as ` unstack ` doesn't work , maybe there's another way to plot this data ? Thanks

I am looking to ' smooth ' regularly-sampled 30-sec time series data using the pandas ` rolling_window ` function , with a window type other than ` boxcar ` - ideally ` hamming ` . However , so far all windows which I have tried to apply , over varying window lengths from 2 to 100 , appear to offset the smoothed data to lower values , e.g. :
To verify correctness apply the rolling window on a step function . If there were an offset , it would show up

Take the intersection of their indices . #CODE
Then take the intersection of these as defined before .
Later you may wish to do something like the following to get the ` df ` at intersection index . #CODE

However their IS a way to do this . Here is the sketch . Use ` select_as_coordinates ` to actually execute your query ; this returns an ` Int64Index ` of the row number ( the coordinates ) . Then apply an iterator to that where you select based on those rows .

This is the function you'll apply to each of the lists in ` groups ` . Just like before we hand of the pair to ` SequenceMatcher ` to get the ratio . Only now we need to keep the name around . So in that function ` x ` is a tuple like ` ( ' maria ' , ' mary ')` . We need to know the name in the best match and the ratio of the best match , so I threw them in a dict with ` {name : ratio} ` . The other thing here is that ` max ` takes a second argument . This time it's just saying the thing to maximize is ` x [ 1 ]` , the ratio .

You will need to convert to floats ( if necessary , it won't work for ints ) for this to work see the docs on ` replace ` and in particular the method used here is ` ffill ` which means forward fill : #CODE

Edit : another solution which is faster is to use ` value_counts ` ( and normalize ): #CODE
I had thought this was more concisely written as a ` resample ` , if you use a DatetimeIndex :
len ( Series.unique() ) might be even faster .
Interestingly , len ( Series.unique() ) is usually much faster than Series.nunique() .

And concat these Series together : #CODE

Pandas unstack doesn't work
But when tried ` unstack ` the series to get rid of the MultiIndex and plot my data , and got the error : #CODE

You don't need to if you simply pass a `` min_itemsize=40 `` ( or whatever number is ' big enough ') , this will apply to all object columns , alternatively , you can use : `` df.dtypes `` to see which are object ( the values are the dtype )

Can read_table do this or do I have to write a preprocessor to time align the data before giving it to pd.read_table ?
What exactly do you mean by time align the data ? For example , what would the row at time ` 2010 / 056 / 12:25 : 32.112 ` look like in your final df ? Would it be at 12:25 : 32.1 or 12:25 : 32.2 ? Would it contain 6 or 36 columns ?

Why do pyplot methods apply instantly and subplot axes methods do not ?
I'm editing my graphs step by step . Doing so , ` plt ` functions from ` matplotlib.pyplot ` apply instantly to my graphical output of pylab . That's great .

Apply function to a specific number of rows in a DataFrame
I have a weather data and I would need to apply a function to a specific number of rows . For example , to calculate mean values of every 10 or 15 rows . The number of rows is important because there are quite many missing values in dates and I don't want to rely on it .
I tried ` groupby ` but there I can only specify hours or minutes . Anyway I would like to apply any function independent from ` DateTime index ` .
I think slicing ` DF ` would be an option ` df [: 9 ]` but I don't know how to apply this to all rows ?
Also what is the function you want to apply ? Does ` df.resample ( ' 10min ' , how= )` work ?

4 : I apply the transaction level criteria to the datafarme
produces 2 differently sized dataframes across the two Sequences . I tried to join the final step of the two sequences to see which rows exist in Sequence 1 and not Sequence 2 , but a left merge between the two does not show any differences .
I'm pretty sure defining the criteria after adjusting the dataframe with the merge is the right way to go , but I want to understand why the results are different if nothing is changing in the underlying , transaction level dataframe other than the addition of a new column .
I think I may have figured it out . The docs mention that whenever joining columns on columns , the indexes will be ignored . after merging via a left join , the post-merge dataframe does not match the pandas-generated index of the left dataframe , despite being the same size .
since defining the boolean index before the join has the original index , applying that criteria to the post-merged dataframe results in mismatche errors .
My mistake was thinking the index of the left dataframe is maintained after a left join .

I want to apply the expanding mean , such that ` ptsA ` and ` ptsB ` for each player get counted in ( and are not left ) to the net result . Final output should make it more clear : #CODE
and obviously , I need to do the same , and ` groupby ( ' plB ')` and then Im drawing a blank how to join these two results correctly .

Merge pandas DataFrames based on irregular time intervals
I'm wondering how I can speed up a merge of two dataframes . One of the dataframes has time stamped data points ( ` value ` col ) . #CODE
I'd like to merge these two dataframes more efficiently than the ` for ` loop below : #CODE

Replace values in Pandas data frame within a loop
I am trying to loop through a pandas data frame and replace values in certain columns if they meet certain conditions . I realize there are more straightforward ways to do this in general , but in my specific example I need a loop because the result for one row can depend on the prior row . Below is a reproducible example of what is going wrong . When I try to replace text it does not replace it . #CODE

I was planning on approaching this problem by creating multiple df's like @USER said , and then just merge them on either name or unique ID , much like you would do with a db . I think you should be able to use the pandas.merge ( df1 , df2 , on= ' identifier ') to link them together .

` a.strip() ` results in ` AttributeError : ' Series ' object has no attribute ' strip '`

Just don't use ` join ` : #CODE

Then do a join on the original data : #CODE

Python pandas groupby object apply method duplicates first group
I am confused about this behavior of apply method of groupby in pandas ( 0.12.0-4 ) , it appears to apply the function TWICE to the first row of a data frame . For example : #CODE
Then I try to do something similar using apply on the groupby object and I get the first row output twice : #CODE
Edit : @USER provides the answer below . I am dense and did not understand it immediately , so here is a simple example to show that despite the double printout of the first group in the example above , the apply method operates only once on the first group and does not mutate the original data frame : #CODE
This is checking whether you are mutating the data in the apply . If you are then it has to take a slower path than otherwise . It doesn't change the results .
@USER : Could the result of the first call be saved so it is not called again ? This might help if the function called by apply takes a long time ... ( along with being more intuitive , since this question comes up a lot . )
The ` apply ` function needs to know the shape of the returned data to intelligently figure out how it will be combined . To do this it calls the function ( ` checkit ` in your case ) twice to achieve this .
Depending on your actual use case , you can replace the call to ` apply ` with ` aggregate ` , ` transform ` or ` filter ` , as described in detail here . These functions require the return value to be a particular shape , and so don't call the function twice .

is there a reason you're making two axes when ` len ( area_tabs ) = 1 ` ?

@USER they do different things . Sometimes you want to get by label ( loc ) , sometimes by position ( iloc ) , sometimes both ( ix ) . Sometimes ix is ambiguous . iloc is faster .
@USER Not sure I would say that ... e.g. ` df = pd.DataFrame ([[ 1 , 2 ] , [ 3 , 4 ]] , [ 1 , 0 ] , [ 1 , 0 ])` try ` df.loc [ 0 , 0 ] , df.iloc [ 0 , 0 ] , df.ix [ 0 , 0 ]` , ` ix ` is ambiguous for this integer indexed DataFrame - which is why loc and iloc exist . They also act differently when you have a non-unique index ( where loc / ix are type unstable ) .
This will return the cell located at the intersection of the third row with the fourth column .

At this point I would expect ` df [ ' b ']` to contain the letters ` A ` through ` J ` , but instead it's all ` NaN ` . However , if I replace the last line with #CODE

Alternatively you could create a map for the criteria to do boolean selection on month and year . #CODE

How to resample a time series Pandas dataframe ?
I am trying to resample 1 minute based data to day . I have tried the following code on IPython #CODE
use `` data.set_index ([ ' DATE ' , ' TIME '] , inplace=True `` ; the returned copy now is just getting discarded so the resample will fail
instead . Right now you're asking Python to pass the ` mean ` object to the ` resample ` method as the ` how ` argument , but you don't have one defined .

What can I do if I have several DataFrames and I want to apply the same set of operations to each with operations that do not support ` inplace=True ` ? Is there a way to change the original DataFrame in a for-loop ? For example , instead of ` df_train [ df_train > 1 ] = 1 ` and ` df_test [ df_test > 1 ] = 1 ` iterating over the two frames and changing the content of the DataFrames in the for-loop .

The gaps column holds the values I am trying to unstack .
I want to unstack the data so that with each neighborhood I can see the count for each gap .
Is it possible to unstack the gap values while preserving the groups of zone , city and date ?
When I try to use the function as unstack ( ' gaps ') , I get a key error that says ' Level gaps not found '

The zeroes for HH mm ss were added just to conform exactly to the examples .
dt = pd.datetime.strptime ( date_string , ' %d %m %Y ')
return dt

FYI I used ' and ' here but ' & ' is ok too ( query / eval have a somewhat relaxed parsing )

Second , let's try to replace with a scalar of a different data type : #CODE
Unless you transpose df.T [[ 4 , 5 , 6 ]] = df.T [[ 4 , 5 , 6 ]] , but this seems like cheating ...
by doing something like ` df.loc [: , ' date ']` looks similar . But what you are actually saying is not replace this column with what is on the right hand side , but rather , overwrite using the row mask ( it happens to be null in this case ) . The dtype conversion is not done here because you could be potentially doing a very expensive operation .
no , you don't set new rows that way , use append . You need to study the indexing section : #URL there are a lot of very deliberate choices ( and not all mimic numpy exactly )
Also , in case I wasn't clear , the intent of my df.loc [ 7:9 ] = val example was not to append rows , but to change the content of existing rows ( with index = 7 to 9 ) .
I'm sorry , I don't think I'm being very clear . What I'm trying to do is modify the values ( but not the dtypes ) of a 2x2 block of a DF that includes at least one column that is of dtype == datetime . Using the above example , I'm trying to do : df.loc [ 4:5 , [ ' date ' , ' val ']] = df.loc [ 4:5 , [ ' date ' , ' val ']] . The goal is to take the 2x2 piece of df with index ( 4 , 5 ) and columns ( ' date ' , ' val ') and replace it with a same-shaped , same-typed 2x2 block . I get the same error from : df [ 7:9 ] = df [ 7:9 ] . But if datetime is first converted to int ( df [ ' date '] = v [ 0 ]) , we don't get the error running the above code . Thx !

I think a nicer way to do this , assuming you were planning on apply it to an entire column , is to use one of the vectorised string methods : ` str.split ` : #CODE

ewma or shift of MultiIndex
If I have a Pandas ` DataFrame ` with a ` MultiIndex ` where the first level is symbol , the second level is a date , what is the easiest way to perform an ` ewma ` or a ` shift ` operation on the data .

Python Pandas Not Getting Consistent Result using ix
Selecting with a scalar value ( like a 2 ) , will drop the dimension of the result ( e.g. you will get a Series ) . To return a frame , pass a list ( ` [ 2 ]` is a single element list )

I'd then drop the ` previous ` column as it's no longer needed . Ending up with : #CODE

works perfectly ! Do you mind me asking why ix doesn't work - seems it did in smaller samples , but not in those with 5k rows

Join on dataframe without a key
Am not sure why this join seems to be working , yet alas no rows can be seen - I cannot slice the DF .
Btw I do not join on the key , as I only want to really join on index .
It sounds like the join is working , but you are seeing a summarized view .

I want to merge them into one dataframe looking like #CODE
This looks like pivot table , which in Pandas is called ` unstack ` for some bizarre reason .
? The method to produce a pivot table isn't called ` unstack ` , it's called ` pivot_table ` .

I simply want to join such that the final DF will look like : #CODE
As you can see the column names of both original DF's are the same , but not in the same order . Also there is no join in a column .
You can use append for that #CODE

You can group and apply an user-defined function : #CODE

In other words , how to translate this pseudocode :
Are you trying to apply two different types of equations based on the value in serialNumber ?
After the merge between the object_list and percentages , you could " query " the dataframe based on the value in serialNumber and apply the correct formula ; #CODE
The apply function is similar to pythons builtin " map " . You can ' apply ' the same function over the rows or columns ( where axis=1 is for row-wise [ top to bottom ] where the indexes will be the column names , and axis=0 is column-wise [ left to right ] where the row indexes are the indexes )

Create a summary Pandas DataFrame using concat / append via a for loop
If it has been created append the next DF ( last two rows ) for each additional tab .
and concat : #CODE

I have tried a number of things but it seems to cut out rows . Like : #CODE

Apply function to pandas Series with argument ( which varies for every element )
I have a pandas Series and a function that I want to apply to each element of the Series . The function have an additional argument too . So far so good : for example
python pandas : apply a function with arguments to a series . Update
I had to face this problem in my code and I have found a straightforward solution but it is quite specific and ( even worse ) do not use the apply method .
I want to multiply elements in a [ ' x '] by elements in t . Here the function is quite simple and len ( t ) matches with len ( a [ ' x '] .index ) so I could just do : #CODE
Well I imagine a sort of broadcast of t in order to match the length of a [ ' x '] . Let's say len ( t )= 2 and len ( a [ ' x ']) =4 then t [ 0 ] would operate with a [ ' x '] [ 0 ] and a [ ' x '] [ 2 ] while t [ 1 ] on a [ ' x '] [ 1 ] and a [ ' x '] [ 3 ]
That's an unusual broadcast rule , and not one that will be widely desired . So the Pandas API doesn't directly handle it for you . Your best bet would be to write a function that maps the ` t ` vector into a correctly-sized column in the data frame , using whatever mapping convention you'd like , and after that is created , * then * you can just use a simple ` apply ` or ` map ` or basic array function to operate on them . But you shouldn't want Pandas to support arbitrary ways of broadcasting elements . That interface would be so wide open it would necessitate that the data structure was meaningless .

Sorry if I am doing something stupid , but I am very puzzled by this issue : I pass a DataFrame to a function , and inside that function I add a column and drop it . Nothing strange until here , but after the function has finished the DataFrame of the global namescope is showing the added dropped column . If I declare the DF as global , this is not happening ...

My objective has been to create a DataFrame for each stock with the Date and Time combined into a TimeSeries " DateTime " index . I then intend to resample this to Daily and Monthly DataFrames .
I cant see how to post the entire stack trace here . This is the last part

I would like to perform something similar to an SQL groupby operation or R's aggregate in Pandas . I have a bunch of rows with irregular timestamps , I would like to create temporal bins and count the number of rows falling into each bin . I can't quite see how to use resample to do this
If you're indexing on timestamps , then you can use resample . #CODE

can you show `` data.info()`` before this ? you should have `` float64 `` dtypes already . secondarily , you don't need the apply , you can do something like : `` data [ data [ ' currency '] ! = ' A ' , ' amount '] =d ata [ ' qty '] *data [ ' rate ']``

Apply function then Filter DataFrame

You can [ groupby an index level ] ( #URL ): ` site.groupby ( level= ' DK ' , axis=1 )` , and then iterate through that like a normal groupby object . It may be cleaner to use and ` apply ` after grouping , instead of iterating over the groups .

Now I drop these rows from the initial DF for the training data : #CODE

Calculations ( mainly ) , I'm now initialising like this : ( for example ) hourly_pred = pd.DataFrame ( { ' T ' : np.zeros ( len ( fpred )) , ' W ' : np.zeros ( len ( fpred )) } ,

Pandas - resample and standard deviation
I run resample ( " 10min " , how= " median ") .dropna() and I get : #CODE
Updated answer : Added how to resample by std .

The default as of v.0.13 is ' truncate ' . #CODE
To change the default value directly , line 229 in config_init.py should be changed from ` cf.register_option ( ' large_repr ' , ' truncate ' , pc_larg ... ` , to ` cf.register_option ( ' large_repr ' , ' info ' , pc_larg ... `

Obviously you have to replace the strings ' xlabel ' and ' ylabel ' with what you want them to be .

Normally one can use ` axvline() ` although I encounter two problems . Even if I call ` plt.axvline ( x= 0.5 , color= ' r ')` just to produce an arbitrary line , I do not see it on top of the pandas plot . I am using IPython with ` %pylab inline ` by the way . And secondly , I do not now how to translate the dates into x position that are being used in ` cum_edits.plot() ` since the translation is invisible to me . Should I go about producing these vertical lines ?

I would strip them in python prior to using pandas

I am struggling with indexing and bools but i can't solve this . I strongly suspect that i need to use a lambda function , but i don't know how to apply it . So please have mercy it's too long that i'm trying on this . Hope i've been clear enough .

@USER : ` np.timedelta64 ( 1 , ' H ')` would work , but ` astype ( ' timedelta64 [ h ]')` -- I needed a lowercase ` h ` -- seems to truncate , so I wouldn't get 1.5 out .

I simply want to append values on axis=1 for the result of : #CODE
Yes , I tried to replace zeros with a list ( although I'd rather not do that because I need those zeros to stay zeros in my original df , and creating a new df may not be the best option ? ): #CODE
Why not just replace the zeros with [ 0 ] first ?
You would need to use ` applymap ` instead of ` apply ` to do it that way . But more generally , working with lists inside DataFrames can be somewhat awkward , and working with columns where some values are lists and some are numbers is also likely to be awkward .
You can apply your values to your matrice by looping through the list with values that you
want to apply and append them to the matrice in the loop . #CODE
your array with NAN , after normalization , row / row.sum , is because you've first assigned values to your matrice and then call the normalization on the matrice . You need to do the other way around , you need to normalize the matrice and call the matrice again #CODE
It depends , you can convert your dataframe to an array , it might be easier to append values and normalize them that way , see thread #URL

No , sorry . No idea what you are trying to do nor how to translate that to valid code using Pandas .

is it possible to map to an item in a dictionary list , ie :

how to apply a function which takes the caller as its arugment

Following is the matrix which shows the intersection of pages accessed by common . #CODE

is it feasible to strip a redundant index returned by groupby in pandas ?
And note that I cannot strip it afterward , since the actual computation is much more convoluted such as : #CODE
When calling apply , add group keys to index to identify pieces ` .
You don't get what I'm asking here . I tried ` group_keys ` and ` as_index ` , but those are related to the grouping variable itself ( i.e. removing ` Species ` or not here ) . What I'd like to strip away here is the redundant numerical index , not the grouping variable .

Trying to merge dataFrame

How can I insert a row into a dataframe , while preserving numerical order of row indexes ?
Now I want to insert an odd-indexed ttl , so I use ` DataFrame.loc ` : #CODE
The problem is that the numerical order of my indexes ( which double as timestamps ) is not preserved . How can I actually insert the " kowabunga " row such that it resides between index 2 and 4 ?

Reindexing error makes no sense does not seem to apply as my old index is unique .
Reindexing only valid with uniquely valued Index objects does not seem to apply

( I forgot that I need to drop a leading 1 from 11-digit numbers , too )
About your second question : You may want to look at the replace function . So the above code can be changed to : #CODE

Any Ideas ? Even via cut and paste ie output of a dataframe display ?
The dataframe data as below , was simply cut and pasted from the HTML rpr in IPython Notebook #CODE

It should return a ` bool ` instead of the error
Ok I added the Error Traceback if it helps . The correct thing to return is a ` bool ` value

I want to unstack one column in my Pandas DataFrame . The DataFrame is indexed by the ' Date ' and I want to unstack the ' Country ' column so each Country is its own column . The current pandas DF looks like this : #CODE
When I use df.pivot I get the following error " ReshapeError : Index contains duplicate entries , cannot reshape " This is true since I'm looking at a Dates that are reported at the same time by each country . What I would like is to unstack the ' Country Column so only one Date would show for each month .
If you can drop ` Product ` , ` Unit ` , and ` Flow ` then it should be as easy as #CODE
Thanks Douglas , I've updated my answer assuming you can also drop ` Product ` . If that's not the case let me know .

Probably better to split your data using regexp and then apply some date parsing using strptime IMO , I can't think of an easier method

Pandas align multiindex dataframe with other with regular index

In my case above I do not know where the value is in ` df.Matrix ` so ` str.contains ( ' Good ')` checks for a ` bool value ` before tagging it with whatever is in sch [ i ] and the part that check for the bool value causes the error . Your code does not have an error because you assigned locations and inputs values . It is not the same because if I give you a file with mixed data and tell you to extract ' Good ' your program would not work
What is a good way to avoid this ? Make a duplicate dataframe and modify that , modify an column of the same length and append it at the end , or just go with the suppressing of the error ?
best way is to create the Series , then just assign it directly , e.g. `` df [ ' Value '] = s `` , rather than creating it empty and overwriting values . Just create the Series as you need it ; pandas will align it ( filling the remaining values with nan )

Followup question : How can I implement the pandas shift with dataframes that are different lengths ? Imagine I have two time series that overlap but one extends before and the other extends after the other . So there are non-matching values in each index , and the indexes are ( almost certainly ) different lengths . pandas will not allow shift() with this error : #CODE
The pandas answer is to use ` shift ` : ` first.shift() > second ` . ` shift ` shifts the labels .

In other words , I would like to replace the id's of df2 with the corresponding values in df1 . I have tried doing this by performing a for loop but it is very slow and I am hopping that there is a function in pandas that allow me to do this operation very efficiently .
then join the two columns #CODE

I am trying to merge some dataframes based on specific column . Each dataframe is consisted of two columns .
I can do the merge for two of them . But When i get the result and try to do the next merge I have problem with the suffixes :
The names of the columns of the dataframe were the same , so it was not possible to merge them .

I'm dealing with some large CSV files . Basically I have two for the year 2009 and 2010 . I read these both is seperatly using pandas , and then append the 2010 file to the end of the 2009 dataframe .
I'm sure it is just a copy / paste error , but you have an erroneous bracket in your append statement

Use ` apply ` for row-wise methods : #CODE

concurrent reading works just fine in multi-threading / processes . the change in 3.1 was more to align PyTables with how the underlying HDF5 protocol works . That said , it is possible it was buggy in < 3.0.0

pandas concat is not possible got error why
I like to add a total row on the top of my pivot , but when I trie to concat I got an error . #CODE
Thats the pivot Table #CODE
The total of it I like to join #CODE
Does ` table ` have a ` MultiIndex ` for its columns ? Try ` table = table [ ' Weight ']` and then do the concat .

I'm running a small script with PANDAS and I get a rather impressive interpreter error . I've included the full stack trace at the end of this question since it's so damn long .

Was there a section of the docs that you found particularly confusing ? Or was it just tough to translate what the docs were saying to your particular problem ? If you have any improvement be sure to share them on [ Github ] ( #URL )

Well , it does select the number of rows I want but I can't apply ` size() ` method on this new object . #CODE

@USER I'm not sure that is possible , once you assign ` NaN ` you get a dtype conversion to ` float ` s as ` bool ` s cannot represent ` NaN ` in any meaningful way .
@USER no worries , it seems there is an implicit dtype conversion after assigning ` NaN ` , it converts to float64 probably because ` NaN ` has no meanings for ` bool ` s . You would need to take this into account
If you don't care about the bool / float issue , I propose : #CODE

How can I merge one dataframe into another , inserting rows that do not exist ?
My goal is to merge the second dataframe into the first , such that each item in the ` msg ` column of dataframe2 gets placed into the ` event ` column of dataframe1 at the index indicated by column ` t ` .
Did you look at the ` join ` method ?
@USER , Yikes , looks like I made a small mistake in my example ( it's fixed now ) . There is no ` t ` column in the first dataframe , so I would need to merge onto the index , it seems . I'll give it a whirl . For clarity , the ` t ` column in the second dataframe corresponds to the index values in the first .
@USER your first df seems to have year as it's index it isn't clear how t relates to the first df's index , you need either indexes that mean the same thing or a common column in both df's in order to merge them in any meaningful way
My attempt with ` samples.loc ` illustrates what I'm trying to do . The only problem is that it seems unwilling to insert rows that do not already exist .
@USER in which case what you do is ` df1.set_index ([ ' t '] , inplace=True )` and then merge : ` df =d f.merge ( df1 , left_index=True , right_index=True , how= ' left ')`
@USER just rename the column before merging ` df1.rename ( columns={ ' msg ' : ' event ' } )` , or you can just do it after merge by dropping the column from df : ` df.drop ( ' event ' , axis=1 )` then rename msg to event : ` df.rename ( { ' msg ' : ' event ' } )`
What you want to do is merge the other df to your first one on either an index ( which should represent the same thing ) or columns which should represent the same thing .
Now merge : #CODE

pandas join DataFrame force suffix ?
How can I force a suffix on a merge or join . I understand it's possible to provide one if there is a collision but in my case I'm merging df1 with df2 which doesn't cause any collision but then merging again on df2 which uses the suffixes but I would prefer for each merge to have a suffix because it gets confusing if I do different combinations as you could imagine .
Why not just concat or append df1 and df2 together but rename the clashing columns in df2 so you can id where the original data came from ?
I'm doing this now . I guess in the end , it would be nice to save the step and provide a suffix to the join / merge functions . I'll mark this as the correct answer . Thanks .
this would be a nice addition to merge in pandas .

You can force the dtype to be int rather than bool : #CODE

I have a pandas dataframe that looks like the one below , and I want to drop several labels .
However , I have many labels that I want to drop , so I am looking for a command that works like : #CODE
How can I drop , or the reverse select , several rows with different labels in one command ? I am using pandas only for a few weeks now but can't find an answer to this problem . #CODE

just do it directly : `` df [ ' new_column '] = df.loc [ sample_ind , ' c '] + df.loc [ sample_ind , ' d ']`` ; this will be very fast . You only need to use `` loc `` on the lhs if you are overwriting part of an existing column . If you are doign that , then even better is to just construct the rhs as you need it an assign .

How can I strip the whitespace from Pandas DataFrame headers ?
My question , then , is how can I strip out the unwanted white space from the column headings ?

Conditional Replace Pandas
I have a dataframe and I want to replace the values in a particular column that exceed a value with zero . I had thought this was a way of achieving this : #CODE

I don't want to redefine each and every method of a class to pass through . This is just one example . I'd like to apply this elsewhere .

` TypeError : cannot properly create the storer for : [ _TABLE_MAP ] [ group -> / test_sparse ( Group ) '' , value -> , table -> True , append -> True , kwargs -> { ' encoding ' : None} ]`

unstack is not working .

Is is possible to write my own column names when writing a pivot table to a csv file ? #CODE

For each dataframe column you wish to convert to a list , you can transpose the values , and then convert it to a list as follows .
Then obtain the values using ` .values ` and transpose using ` .T ` , and convert to a list using ` .tolist() ` #CODE

There is a property ` dt ` specifically introduced to handle this problem . The query becomes : #CODE

@USER : Thanks . I just noticed that the only apparent differences between my `` index `` and ones in `` working `` examples is the `` freq `` and `` Timezone `` . My index was created with `` df.index = df [ ' timestamp '] .apply ( dateutil.parser.parse )`` on strings like `` Sat Jan 14 11:01 : 38 GMT 2012 `` . I guess that's not working . Is a fix obvious ?

`` df == DataFrame ( np.tile ( rowmax , len ( df )) .reshape ( df.shape ) .T , index =d f.index , columns =d f.columns )`` will get your boolean frame ( kind of like a broadcasted comparison operator ); faster , but prob not more clear than the `` apply ``

1 . It looks like ` stack ` returns a copy , not a view , which is memory prohibitive in this case . Is this correct ?
2 . I want to group the DataFrame by rows , and then get the different histograms for each grouping . If we ignore the memory issues with ` stack ` and use it for now , how does one do the grouping correctly ? #CODE
This means we don't easily know how to build our grouping . It would be much better to just operate on the first level , but then I'm stuck on how to then apply the grouping I actually want . #CODE
I think you are doing a row / column-wise operation so can use ` apply ` : #CODE
Also , you were pretty close to getting this correct , but you'd need to stack and unstack : #CODE
Thanks , Andy . This gets close , but the grouping still isn't quite right . See the example I'm about to put into the original question . Also , does stack return a copy instead of a view ?
The issue is the nans make the number of rows variable , so we can't readily know ahead of time how to construct an appropriate grouper for ` d.stack() ` . It would be better to just operate without the ` stack ` , as you've done ; I just don't see a way to get the appropriate grouping in there .
@USER you groupby the level , stack is a copy . I'm confused what the final result is that you actually want ....
Is there any disadvantage to this approach , or any particular reason you want to use stack and groupby ?

@USER Thank you , I understand the difference and see that both selections are not the same ; this can be confirmed using the ` is ` operator for comparison . But is there a simple rule ? The documentation looks rather complicated in this point . How far does the following statement apply to one but not the other select ? " Whenever an array of labels or a boolean vector are involved in the indexing operation , the result will be a copy . " In both versions ` df [ " col "] == 3 ` is a boolean vector for selection , but in the former version the condition is used first ; in the latter the Series is selected first .

The exporter insists that all data used respond to ` type ( x )` with either ` int ` , ` float ` , ` bool ` or a few others . It doesn't know what to do with a ` numpy.int64 ` .

You can achieve an optimal chunksize in a ` Table ` format by passing ` expectedrows= ` ( and only doing a single append ) . However , ` ptrepacking ` will STILL have a benefit here .

Why does pandas apply calculate twice
I'm using the apply method on a panda's DataFrame object . When my DataFrame has a single column , it appears that the applied function is being called twice . The questions are why ? And , can I stop that behavior ?
Also , calling it four times when you apply on the column is normal . When you get one columnm you get a Series , not a DataFrame . ` apply ` on a Series applies the function to each element . Since your column has four elements in it , the function is called four times .

I've been playing around with ` set_index ` and ` stack ` / ` unstack ` methods but with no success ...
stack and unstack to exactly reproduce result you posted : #CODE
By the way , there are probably many ways to arrive at your desired DataFrame . A technique that is often useful for finding a solution is to think backwards . You start with the desired DataFrame and ask yourself what one operation might result in the desired DataFrame . In this case , the operation that came to mind was ` pd.pivot ` . ( An alternative solution could probably be based on using ` unstack ` -- though , on second thought , it may not be very different since under the hood ` pivot ` uses ` unstack ` ... ) . Then the question becomes , what DataFrame , ` something ` , is needed so that #CODE
By looking at other examples of ` pivot ` in action , it became clear than ` something ` had to equal #CODE
You can use a combination of ` DataFrame.groupby ` , ` DataFrame.reset_index ` and ` DataFrame.T ` ( transpose ) #CODE
Or you can use ` concat ` : #CODE

doing a nested a nested apply / grouping like this is not the answer . This is almost pure python code , you are not leveraging any of pandas strengths . You prob want to groupby at the top level ( or construct a multi-index ) , select the values that you want to include , then use a cythonized function to apply it . You only want to do 1 level of groupby and apply ( except in some very very rare cases ) . You are ultimately are doing some vectorized operations , but you are doing them backwards at the lowest level .
You can do the apply and groupby by one multilevel groupby , here is the code : #CODE
FYI this is a completely parallelizable problem once u split the groups and you could say do them in separate processes , eg something like concat ( dict ([ ( g , group_process2 ( grp )) for g , grp in d.groupby ( grps ) ]))

` freq ` is a string .
I would like to define the same kind of DatetimeIndex using a timedelta as freq #CODE
A more robust way to get this is to use a DateOffset ( which you can pass as freq to ` date_range ` ) .

Pandas : how to apply function to only part of a dataframe and append result back to dataframe ?
I could then figure out how to append these lists back to the original list ( though I have no idea how to go from a dataframe to a list again to do this ) . Is there more of a direct way to only apply the function to the numeric variables ? Also , how would you change the pandas dataframe back to its original list form anyway ?
To ' append ' the new one to the original dataframe , if needed , can be simply : ` df [[ ' c3 ' , ' c4 ']] =d f [[ ' week1 ' , ' week2 ']] .apply ( lambda x : ( x - x.mean() ) / x.std() )` . The new columns will be named `' c3 '` and `' c4 '`

pandas pivot table using index data of dataframe
I want to create a pivot table from a pandas dataframe
So pivot uses the index as values : #CODE

You can use ` apply ` and use a lambda to subtract the list values column-wise : #CODE
I was trying to remember what the correct / better method was and couldn't remember this one so i posted ` apply ` as an answer .
Ed , in this case , ' sub ' was what I was looking for , but I'm certainly keeping the ' apply lambda ' method in my back pocket -- the next problem in queue isn't a straight subtraction . Thanks !

There are different ways . You want to combine the level names of the column multiindex , in reverse order , and join them with a ` _ ` , so it feels like you're going to have to specify at least three things . You could remove the dependence on the names , anyway , with something like #CODE

How can I avoid repeated indices in pandas DataFrame after concat ?

But you still accumulating data in ` total ` , while ` series ` being garbage collected . Maybe optimize algorithm ? It looks to me you just want to join files of the same format , if it so there is no need to use pandas for this .

I am guessing this would work with a pivot table but since I was using a dataframe everything got mixed up .
For the fifth column I want the consolidate the number of unique values for each Address . For example in NY the value should be 2 since there are two unique values and a duplicate of ' 12 Silver ' #CODE
While with enough stacking tricks you might be able to do this all in one go , I don't think it'd be worth it . You have a pivot operation and a bunch of groupby operations . So do them separately -- which is easy -- and then combine the results .
Then we can pivot : #CODE

You can resample by month using sum as the aggregation function : #CODE
When you set this as the column , it'll align the index for you : #CODE
Now you definitely want to fill in the NaN with 0 . Given your desired solution it looks like you want to shift the results up by one too ... #CODE
Thanks for the suggestion . I had this idea as well ; the problem I ran into was unconventional start dates . Your example works smoothly because both ` pd.date_range ` and ` .resample ` result in a ` DatetimeIndex ` starting on the same day . However , my time periods were 28 day blocks ( Tuesday to Monday ) . I couldn't find a way to get ` .resample ` to work properly with this index , it always started near ` vaca_days [ 0 ]` even when this didn't align with my index . No help from ` .shift() ` or ` loffset ` . My eventual workaround was to add an extra blank day to ` vaca_days [ 0 ]` at the proper start date .

is all the data text or are there any numbers ? for example , if someone types 3.14 as a string , do you really want to strip the period ?
Translate is often considered the cleanest and fastest way to remove punctuation ( source ) #CODE
It is returning me with an error that DataFrame does not have a ' translate ; attribute . Sorry , also need to mention data is huge , thats why trying to implement on Pandas .
Use ` replace ` with correct regex would be easier : #CODE
@USER just read the docs , ` strip ` removes characters at beginning and end , hence the error , you should use ` replace `
@USER Hi thanks for your answer , I assume that will work . But I am looking forward to store values that I continually need to replace in a list and run that command every time . The list will continue to grow as a filter . This method has to be efficient and also allowed to be updated easily .

In any case concat produces a different output than subsequent append calls . This could be a test case to add ...
You could submit this to #URL so that it's in the bug tracker . That said , you'll probably get better performance out of ` concat ` anyway .
my pd.version.version is ' 0.13.0-408-g464c1f9 ' , did you check also if it still keeps this the second time , when you append res2 ? Because only at this moment I see the problem ( see my example )

To drop the MultiIndex , you could use #CODE

Eventually I want to melt down this data frame to look like the following . #CODE

I want to apply filters based on the following pattern from ' CAT1 ' ' CAT2 ' ;

You could just define a function and pass this to ` apply ` and set ` axis=1 ` would work , not sure I can think of an operation that would give you what you want
Then apply it to your dataframe passing in the ` axis=1 ` option : #CODE

Trying other user-defined functions produces similar errors . In all these cases , it's pretty clearly trying to apply peak_to_peak() or np.mean() ( or whatever ) to the ( subsets of the ) ' key2 ' column from df , whereas for the built-in methods and predefined functions , it ( correctly ) ignores the ' key2 ' column subsets .

umm , wouldn't len ( yourFrame ) give you the number of rows ?

In python when using pandas , the similar rows can be dropped using ` drop_duplicates ` . Is there any way to separate the dataframe into two dataframes and not actually " drop " rows ?

You have the right idea with the indexing but swapping values will be easier with the ` replace ` method of the dataframe . eg . #CODE

I'm trying to merge some CSV files that I got as a result of some huge text files . the format is like this : #CODE

replace rows in a pandas data frame
I can even start with a 0 data frame ` data =p d.DataFrame ( np.zeros ( shape =( 10 , 2 )) , column =[ " a " , " b "])` and then replace one line each time .
Is there a reason you have to do it this way ? I would recommend building lists with ` append ` and then converting to a dataframe when you've generated all the data , if possible . It will be a lot quicker and you can always iterate through subsets of the dataframe afterwards in your analysis if you need to operate on slices .
Use ` concat ` to add a row #URL
As EdChum says , it doesn't really matter if you're just noting stuff in an interactive session . Our comments really assumed that you were trying to build a dataframe in a loop . I would probably ` append ` in your situation but that's just habit . So long as the data type works for what you're doing I wouldn't worry too much .

see here : #URL This is bascially an unsafe operation , modifying using chained indexing ( e.g. the `` df [ col ] .loc [ row ] = value `` ; you * can * instead do : `` df.loc [ row , col ] = value `` to avoid this issue ( even better is to use `` update / replace `` in any event .
Thanks , I changed to using replace and it works fine .

I want to do a calculate the percentage of bidders where the equation would be ( E+A ) /( E+A+N ) . Is the best way to do a pivot table then implement the equation ? #CODE
Is this the best way to calculate percentage or is there a better way than pivot tables ?

For future readers of this question , could you update to be clearer about what you actually * want * as an output ? " get the index of each element from ' CAT1 '" is ambiguous . Do you want the * first * index of each distinct entry in ` CAT1 ` or do you want to assign each distinct entry a number and replace the text with this number ?

Also check out #URL which covers ` apply ` , ` map ` as well as ` applymap ` .
Thanks . Just a follow-up . In the df1 [ ' Flag '] = df1.applymap ( f ) .sum ( axis=1 ) .astype ( bool ) statement , when we sum it up by columns , then shouldnt first row would have value of 2 ( 1+1+0 ) ?. Then bool of that should be True rite ?. Then why is it False ?
yes , sorry - I copied the result from the ` all ` function but wrote in the ` sum ` function ( bool ( sum ) gives True for all of the rows which isn't a good example ) . Fixed now .

The output of the above examples would be sent to a " map " or " reduce " type function .

That is very strange . I cant figure out why it is doing that . If you roll back to version 0.12 it handles the Series replace correctly . Maybe raise an issue on Github ?

Python plotting points on a city map
And I want to plot them on a map of San Francisco with the ' Type ' column being in red for the data in ' G ' and blue for ' C ' . How would I a simple plot like this ? How would I also include streets on the map ? Can an simple example be provided ?
There are tons of ways to plot data in a map .
The geopy lib includes geocoding libraries to convert addresses to gps coordinates but you still need to take a look at the license ( for example google asks you to buy a license or only use the service with their maps ) . An easy way to obtain a map background is using a free tile service here are some useful urls :

Try using isin ( thanks to DSM for suggesting ` loc ` over ` ix ` here ): #CODE
`` loc `` will be faster than `` ix `` as it doesn't have to check whether things ' look ' like integers but are actually locations ; `` in1d `` is not well suited to `` datetime64 [ ns ]`` or `` timedelta64 [ ns ]`` types FYI
@USER : Thanks for the suggestion . I've replaced ` ix ` with ` loc ` , above .

The initial error , it appears is that I included NaN values , I cut the frame to exclude NaN in both columns . and It ran for a bit but got some sort of Theano error , Which I am now admiring ....

A pivot table should do the trick . #CODE

Replace words in list by words in dataframe , Python
i have a list of sentences and i need to replace each word by a word from a data frame column . it is a one to one correspondence . here's my code : #CODE
so now i need to replace words in z with these words ( correct ones ) in image i.e dataframe .. how do i do that ?
yes i did that .. now the problem is with this error - " replace takes atleast 3 arguments ( 2 given )" . check it out , i've edited my code ..
You're using str.replace wrong . You should be invoking it on a string , and specifying a substring to replace and a new string to replace it with . It works like this : #CODE

and converted it to a pivot table like this : #CODE
Using `' records '` with ` to_dict ` makes it give you a list of dicts , one dict per row , which is what you want . You need to use ` reset_index() ` to get the index information in as columns ( because ' records ' throws away the index ) . Conceptually , the dicts you say you want don't distinguish between what's in the index of your pivot table and what's in the columns ( you just want all index and column labels as keys in the dict ) , so you need to ` reset_index ` to remove the index / column distinction .
@USER : I edited my answer to explain why index has to be reset . I don't think there's any need to create the pivot table differently ; just reset the index . You just happen to want information in columns when the pivot table is giving it to you in the index .

processLine just creates an object and append this object to list . What is supposed to be in row variable in your comment ?

I'd like to add distribution to boxplot when using it with pandas dataframe like this : #CODE
and now I would like to insert " kde " . It's easy for single plot , for ex . : #CODE

I would like to maintain the data types from the original data frame as I need to apply other operations to the total row , something like : #CODE
Append a totals row with #CODE

Thanks for being supportive . its a big struggle as a beginner . Many doubts aren't available on the internet and there aren't many people here in my city who know python . If u look , i have a dataframe in which one column contains addresses . i need to iterate over each word of each address and replace it with the right one ..
The method ` replace ` is good for this . Instead of putting the incorrect / correct mapping into two columns of a DataFrame , use a Series . #CODE
will it work with sentences ? my data contains sentences . so i will need to iterate over each word of sentence to replace it .. your approach is awesome . Can it be extended to incorporate some more complexities ? i'll upload the image of my data , if u want .
That's a little too complex to discuss without something concrete to work with . I suggest you make some short toy examples and open a new question . Or see if [ this old answer of mine ] ( #URL ) gets you close enough . You'll have to split your sentences into columns of words and then apply .

That article used ` id ` s to achieve different formatting for each table but you could just apply the desired CSS styles directly to the appropriate HTML tags . For instance , your image is from their " Box 3 " example , which used the ids ` box-table-a ` and ` box-table-b ` and the corresponding formatting is
Replace the standalone ` #box - table-X ` with ` table `

Some of these are documented [ here ] ( #URL ) , but some are missing . However , it looks like many of the missing ones are fairly clear because they just apply a mathematical function of the same name to each group ( e.g. , ` cummin `) .
these ultimately just call the same named DataFrame method ( or an optimized for groupby version ) ( asside from the specific methods `` transform / apply / agg / groups ``)

You'll need to mess with the ` b ` column to get things flipped . I'd say multiply by -1 , apply the sub and div , then multiply by -1 again .
If I find a more elegant way ( for example , using the column index : ( 0 or 1 ) mod 2 - 1 to select the sign in the apply operation so it can be done with just one apply command , I'll let you know .

Background : I read in a bunch of data from a CSV file and load it into a pandas dataframe . Some of the data is complex ( I convert it from strings when loading ) . Some of the values were equipment errors , distinguished by by being too big . I want to replace all the values whose magnitude is above a certain threshold with np.nan . This is easy with a numpy array ( provided you use a " complex nan " as shown ) , but has been challenging in pandas . I've documented the steps I've tried below - the last attempt almost gets there , but any row where a replacement occurs is converted to real .
@USER I think this should probably be a separate question , or a comment on the github issue ? You can replace with nan + jnan explicitly with : ` df3.where ( df3 <= 5000 , nan+jnan )` , the default is nan .

Pandas uses ` datetime64 ` , ` timedelta64 ` , ` int64 ` , ` float64 ` , ` bool ` and ` object ` ( this includes strings and any other custom objects ) . You can use other types such as ` float32 ` , and pandas will try to maintain it , but some operations will implicitly cast to e.g. , ` float64 ` .

This is a follow-up on my reply to pandas pivot dataframe to 3d data , where later I needed to re-index ( get a different sorting of the Panel labels and of the elements accordingly , as well as filling in with NaNs of the missing labels ) .

Pandas - Append to a DataFrame Column
I have a DataFrame with non-unique column names . What I would like to do is append either a series or a single column dataframe to a given column within the existing dataframe on the index based on that DataFrame column index location ( as opposed to the column name , which would overwrite values in columns with the same name ) . Basically I am looking for something that works like ` pd.df.columns [ 1 ] .append ` .
So , why can't you append a row , as above and then fill it in with data as it comes ?

Alternative , but slower ways are to use the normalize method ( to bring it to the start of the day ) , or to use the date attribute : #CODE
* In 0.15 there will be a dt attribute so that you can access these as : #CODE

Running a groupby on a pivot table with Pandas
I have a pivot table that looks like this : #CODE
This is the pivot tables index : #CODE

( Using dataframes because it's critical the indexes align . )
In this case it looks like you can do a concat : #CODE
If you need to include source , the index information , within the DataFrames in SourceData , then I would do this before doing the concat .

You could also find the location of the first offender you could use argmin : #CODE
Already some great answers to this question , however here is a nice snippet that I use regularly to drop rows based on non-numeric values on some columns : #CODE
The way this works is we first ` drop ` the ` data_columns ` from the ` df ` altogether , and using a ` join ` we reimplant the ` data_columns ` as parsed by ` pd.to_numeric ` ( with option `' coerce '` , such that all non-numeric entries are converted to ` NaN `) . The result is saved to ` numdf ` .

Using the apply function , we can compare entire columns to the empty string and then aggregate down with the ` .all() ` method . #CODE
Now we can drop these columns #CODE

Pandas left outer join multiple dataframes on multiple columns
I am new to using DataFrame and I would like to know how to perform a SQL equivalent of left outer join on multiple columns on a series of tables
I have tried using merge and join but can't figure out how to do it on multiple tables and when there are multiple joints involved . Could someone help me on this please ?
Merge them in two steps , ` df1 ` and ` df2 ` first , and then the result of that to ` df3 ` . #CODE
I dropped year from df3 since you don't need it for the last join . #CODE
Thanks a lot for your answer . As I have quite a few dataframes , I was looking for a quick way of joining the tables all together in one step - similar to using ' join ' as you can join more than two tables in one step . However , I can't figure out how to do it on multiple join conditions . If there's no quicker way of doing this , I'll just stick to what you suggested :)

dynamic function to insert or create table using pydobc to sql server
Ideally i want to send all variables without specifying anything except whether i want to replace or append a table . Is that possible ?
Is it as simple as using ' create table ' instead of ' insert into ' , and somehow dynamically deriving the data type info e.g. ( varchar , int ( 20 )) .
Can you be more specific ? You can query to see if the table exists , and if not ` create ` it . Or you can use ` create or replace ` to always re-create it .

Can't figure out how to apply the solution to my code ,, when I try to just run the code as provided I get module not callable and when I try to place the code in the " with " clause it blows up .. ( so I have not translated that correctly . ) I tried to just use the Family=clause inside the " with " gives KeyError : 0

Generally speaking , ` for i in xrange ( len ( thing )): ... thing [ i ]` should be replaced with ` for item in thing : ... item ` . See my answer for an example .

Then unstack and plot : #CODE
Like it . I have two questions now . How can I control the x-axis so 0 is not included ? I would like to drop the dead space between 0-1 . Also , why use the sum call in the df.groupby ( lambda x : ( x.year , x.month )) .sum() I know it not changing the values so it seems unneeded but I think it might have to do with dropping the date index ? Thanks , Douglas

then you can select the rows using ` loc ` instead of ` iloc ` : #CODE
Note that ` loc ` can also accept boolean arrays : #CODE

Now if I try to replace those nan's with zerosby using #CODE

for icat in range ( len ( label )):

Conditional merge in pandas
I'm new to Pandas and trying to convert some of my SAS code . I have two datasets , the first one ( header_mf ) contains mutual fund information indexed by crsp_fundno and caldt ( fund id and date ) . In the second data ( ret_mf ) set I have fund returns ( mret column ) with the same index . I'm trying to merge each entry in the first dataset with the returns from the previous 12 months . In SAS , I could do something like this : #CODE
Is there an efficient way to do a conditional merge like this in Pandas ?
Sorry , if this reply comes to late to help . I don't think you want a conditional merge ( at least if I understand the situation correctly ) . I think you can get your desired result by just merging header_mf and ret_mf on ` [ ' fundno ' , ' caldt ']` and then creating the columns of past returns using the ` shift ` operator in pandas .
Obviously , the header file may have a lot of variables in it ( besides my made up ` foo ` variable ) . But , if this basically captures the nature of your data then I think you can just merge on ` [ ' fundno ' , ' caldt ']` and then use ` shift ` : #CODE

From what I've seen , ` pandas ` index objects support the same membership methods as python ` set() ` objects , such as intersection , union , difference , and symmetric difference . Try using the intersection operator ( ` `) on the two DatetimeIndex objects . #CODE

It seems that the histogram bars don't correctly align with the grids ( see first subplot ) . That is the same problem I face in my own plots .
Look for the ` align ` option in matplotlib hist . You can align left , right , or center . By default your bins will not be centered which is why you see left aligned bins . This is spelled out in the matplotlib hist docs : #URL

The ` converters ` parameter tells ` read_csv ` to apply the given

How to do a left join on a non unique column / index in Deedle
I am trying to do a left join between two data frames in Deedle . Examples of the two data frames are below : #CODE
Interestingly in Python / Pandas I can do the following which works perfectly . How can I reproduce this result in Deedle ? I am thinking that I might have to flatten the second data frame to remove the duplicates then join and then unpivot / unstack it ? #CODE
Here is one way to do what you wanted - but not very nice . I think using pivoting would be another option ( there is a nice pivot table function in the latest source code - not yet on NuGet ) .
Now we can join the two series ( because their work order codes are the keys ) . However , then you get one or two data frames for each joined order code and there is quite a lot of work needed to outer join the rows of the two frames : #CODE

Appreciate the quick answer @USER . However , I seem to get conversion error when trying to apply the .loc .
Moreover , as this is just an example , do you think there is a way to apply the " *= -1 " to all columns , as ' 1Y ' : ' 1Y4M ' ?
File " C :\ anaconda\lib\ site-packages \pandas\core\ frame.py " , line 4416 , in apply

What is the difference between pandas agg and apply function ?
Using ` apply ` : #CODE
` apply ` applies the function to each group ( your ` Species `) . Your function returns 1 , so you end up with 3 groups .

So for all 4.000 locations you apply the distance function to all 11.000 towers ? That seems rather wasteful , as most towers are * not * near . You could already greatly reduce the work by binning all the towers on certain lat / long combinations , such that you only have to iterate over a small subset of all the towers to find your best match .
This whole function can be made into a ` map ` operation . You're taking one structure and mapping it to another structure with a 1-1 correspondence between elements , which makes this a ` map ` .

I have a DataFrame from which I want to normalize some arbitrary columns using another arbitrary column : #CODE
I want to normalize all the columns ( attrib ) belonging to an arbitrary joint ( eg . ' head ') using another arbitrary joint ( eg . ' torso ') . For instance something like . #CODE
Moreover , I was wondering if there are any better ways to perform the same normalization than the one I am trying . Perhaps using groupby and then and applying the normalize function to the selected DataFrame ?
In your first code block , you need to replace ` multiind_first ` with ` mi_level_one ` and ` multiind_second ` with ` mi_level_two ` .

Since indexes won't match it must join on ` First ` and ` Last ` columns . Suggestions please ?
Thanks a million , it perfectly solves the problem in question . I have an extra issue though , can you think of a way to merge only specific columns ? For example if my ` df2 ` would have another column , say ` Mean ` , when merging , all columns ( ` Score ` and ` Mean `) would get appended to the original dataframe ` df ` . What if we wanted only column ` Score ` to get merged ?

I'd like to pivot the data so that I end up with : #CODE
Drop rows that are all zero : #CODE

Median across multiple axes
I'd like to calculate the median of all identically named columns across both index and columns , ideally getting a series : #CODE
Getting the median across one of these dimensions is trivial , but I can't seem to get it across both ( and doing one and then the other is not the same thing ) .
I've tried melt and index shenanigans , but I always seem to end up with the median long just one axis .
Splendid ! I thought melt was the key , but I neglected the [ " value "] .

Thanks for the ideas . I played around with the indexing a little bit , and this seems to be the easiest / fastest . I didn't like having to strip the index of its name , and transposing the values etc . seemed cumbersome . I realized something interesting ( and probably worth easily fixing ): #CODE

Best way to Shift along a Multi Index Time series
Theres Other columns , but for examples sake they are irrelevant . What I am trying to do , and I might just be unclear on the shifting documents , is to shift all of the data down or up by the first level index . So shift all of the data from the next date index up to the previous date index . So if there was no other dates in between the example , I would like a data frame that looks like : #CODE
Does this make any sense ? The idea is to shift all of the data up a period to compute change in prices .
Try using the [ ` shift `] ( #URL ) method as Wes suggests [ here ] ( #URL )
pandas.core.reshape.ReshapeError : Index contains duplicate entries , cannot reshape . this is raised on the unstack part of the function
Why don't you just shift the date and merge it ? d = pd.Series ( df.date.unique() ); df = pd.DataFrame ( { ' date ' : d , ' next ' : d.shift ( 1 ) } ); pd.merge ( , df , on= ' date ' , how= ' left ')
Wouldn't that just give me a shifted dates set , wouldn't it give me the original df with a shifted date column / index ? I needed to shift all of the information . Maybe however , I can have a shifted index as you show , copy the dataframe , reindex by the shifted one and then join it and drop the new index ? I'll try it and see .

Does this mean that it recognizes each string as a word ? so when I apply the filter it will filter against each of the words in my List2 ? #CODE

This was my first Stack Overflow question , so unclear on the etiquette . If your answer is almost there , do I edit it , do you , or doesn't it matter ? And how did you get your output table to line up ? When I click " edit " to grab the markdown and copy it to a text editor it's not formatting properly .

And turn off ` Insert selected variant by typing dot , space , etc . `

I can unstack it now and get the years at the column level . #CODE

Pandas drops timestamp columns in resample
I have a DataFrame which has two Timestamp columns , one of which gets used as an index for resampling , the other remains as a column . If I resample the DataFrame , the one remaining as a column gets dropped . I would like it to be resampled as if it were a numeric column ( which it is really ): #CODE
Now resample to daily : #CODE
Then resample them ; you need to view them as integers to compute on them ; Timestamp will handle this float input ( and essentially round to the nearest nanosecond ) #CODE
you could do it via a groupby with TimeGrouper as well . I suppose you could have a signature like : `` df.resamplee ( freq .... how={ columns : func ..... } , how_default = default_function )`` to have per-column overrides and a default . Pls file an issue on github for this . ( and a PR would be even better ! )

Pandas , large file with varying number columns , in memory append
Normally as new data comes I would append to the existing table : #CODE
In these cases , I would like to get a behavior similar to the normal DataFrame append function
HDFStore stores row-oriented , so this is currently not possible . You could need to read it , append , and write it out . Possibly you could use : #URL
You could need to read it in , append , and write it out . Possibly you could use : #URL

` map_dtype() ` function , as you can see I have to manually map data types with there string representation in BigQuery
I'm not sure that the methods in ` pandas.io.gbq ` will help you much here . If I understand correctly , you have a CSV file and you are attempting to map the dtypes to the BigQuery types . As far as I found when working on ` pandas.io.gbq ` , there's not a really convenient way to do this .
In this case , I feel like a dictionary is your best bet for the lookups , and you can replace the ` for ` loop with list comprehension : #CODE

There's a bug in the first implementation . ` sq.sort_index() .cumsum() *1 . / len ( sq )` . That should be ` len ( ser )` -- the length of the original list . It works fine in this example because all the values in ` ser ` are unique . If ` ser ` were to have duplicates though , you would be dividing by an incorrect length .

I have been trying to get a proper documentation for the freq arguments associated with pandas . For example to resample a dataframe we can do something like #CODE
which will resample this weekly . I was wondering what are the other options and how can I define custom frequency / rules .
btw yup i do it by groupby for now , but am trying to understand what I can do by resample only

The types of the columns on that dataframe come out as object , object , int , float , bool , object . They're all as I would expect except the first two columns , which I want to be datetime and timedelta .
( I don't want to have to tell pandas which columns are datetimes and timedeltas or tell it the formats , I want it to try and detect them automatically like it does for into , float and bool columns . )

R parameter DROP equivalent in Pandas

How do I get a list of what was specifically created in a particular call ? This would be necessary , for instance , if there were preexisting objects in the same axes from an earlier plot , and I wanted to differentiate , ie to treat just the new ones . Here is my code which plots twice to the same axis . I would like to end up having the stacked bars clustered next to each other , so I thought I could just make them a little narrower and shift them right and left . But for that I would need handles of what was made in the second call . #CODE

How can I sort a boxplot in pandas by the median values ?
I want to draw a boxplot of column ` Z ` in dataframe ` df ` by the categories ` X ` and ` Y ` . How can I sort the boxplot by the median , in descending order ? #CODE
You can use the answer in How to sort a boxplot by the median values in pandas but first you need to group your data and create a new data frame : #CODE

What code are you using to calculate the dot product , that raises the ValueError ? You probably need to transpose ` data ` ( ` data.T `) so that the dot product makes sense . Also keep in mind that pandas aligns the labels those need to match .

How to replace all words in a series with a few specified words in Pandas , Python ?
I want to essentially find and replace using python .
However , I want to say if a cell contains something , then replace with what I want .
that can contain `' optiplex 9010 for classes and research '` which I just want to replace with `' optiplex 9010 '` . Or `' macbook air 11 with configurations ... etc . '` and I want simply `' macbook air 11 '`
I can either manually go into excel and filter by what contains ' optiplex 9010 ' and then replace everything with a simple description , doing the same for macbooks , etc .
You would probably be better off building a dict to hold your reduced / normalised key values with your desired replacement strings , then do a parse of your current string values by reducing / normalising them and then perform a lookup on the dict and replace the current value with your dict value . This is no different to what partial string matches perform in search engines . It is not something that is basic . You may need to use a library like nltk or similar to perform the initial match
However , you might be better off , especially if you have already have a complete list of topics , to normalize the names ( e.g. using fuzzywuzzy like in this question / answer ): #CODE
How would I represent everything before the ' macbook air 11 ' for example if the description was ' one computer-macbook air 11 ' and I wanted to replace this with ' macbook air 11 '
Then use a regex sub to replace the string value of all digits with ` 5 ` and lower case the string : #CODE
Of course if you want to just replace all , you can use a regex or string replace : #CODE
Interesting , How would I get it to replace everything in the cell ... essentially deleting the content and then putting back what I want . I see that all of the 1's are replaced by 5 but what I would want would be to have everything that has a 1 replace it with only a five so 10 would become 5 and 11 would be come 5 only not 55 , etc

I have a numpy array which contains time series data . I want to bin that array into equal partitions of a given length ( it is fine to drop the last partition if it is not the same size ) and then calculate the mean of each of those bins .
More generally , we'd need to do something like this to drop the last bin when it's not an even multiple : #CODE

I usually read everything as string , then apply a helper function that includes try / except to convert each individual string . Or you can validate your strings with regex , and substitute all the values that aren't a date with '' .

My preferred way is Jeff's using loc ( it's generally good practice to avoid working on copies , especially if you might later do assignment ) .
Thank you so much for the detailed analysis , In your last example , df.B.values [ df.A.values == 23 ] , is there any way I can replace ' B ' by a variable instead ?

Merge dataframes on index

I have a dataframe with a Product as a first column , and then 12 month of sales ( one column per month ) . I'd like to ' pivot ' the dataframe to end up with a single date index .
I think pandas ` melt ` provides the functionality you are looking for

Can Pandas find all the lines that join any pair of ** dots ** and don't intersect any of the given ** lines ** without iteration ?
Can Pandas find all the lines that join any pair of dots and don't intersect any of the given lines without iteration ?

Replace the ` #NULL ! ` with ` NaN ` s and then ` dropna ` . #CODE

I have the following problem , I have a Panda data frame and I want to process each row ny using the apply method . Each row should be processed by using a function ( static method ) within the same class .. #CODE

You can group by `' Currency '` and apply ` diff ` but first you need to convert the data to ` float ` , try this : #CODE
Hi again @USER . Your code-update is very much appreciated . However , when applying " diff " to my groupby-function , it seems that Python mess up the original order of the currencys . What's probably happening is that when I apply groupby currencies , Python also SORTS the data accordingly -> so e.g. if my original data is stored in order " EUR , CHF , DKK " the diff-command makes the data " CHF , DKK , EUR " , i.e. when putting back the currency-labels , they obviously will be mis-labeled . Is there a way to maybe tell Python , * not * to order by currency , but leave the order as is ? Thanks .

If the value of column B is 1 , insert ' Y ' else insert ' N ' .
Avoid chained indexing by using ` loc ` . There are some subtleties with returning a view versus a copy in pandas that are related to ` numpy ` #CODE

So just enter % ( name ) s in the query , replace ' name ' with whatever name you want . And add an dictionary for params .

Replace NaN's in one column with string , based on value in another column
Simply , where column B = ' t3 ' , I want to replace the NaN value in column A with a new string .
Use ` loc ` see #URL #CODE

It could be an groupby's apply will do it , like I say in other question , depends what you're doing !

I'm trying to merge / join two dataframes , each with three keys ( Age , Gender and Signed_In ) . Both dataframes have the same parent and were created by groupby , but have unique value columns .
It seems like the merge / join should be painless given the unique combined keys are shared across both dataframes . Thinking there must be some simple error with my attempt at ' merge ' and ' join ' but can't for the life of me resolve it . #CODE
we can't test this without having ` nytimes.csv ` . my guess is that since `' Age ' , ' Gender ' , ' Signed_In '` are indices , you don't need the ` on ` kwarg in the call to ` join `

I'm assuming there is a much more pythonic expression someone might like to share . THe Stack Overflow public loves knowing something isn't homework and so I'm going to tell you straight this is NOT homework . I'm looking for something along the lines of : #CODE
Note : most of the time you don't need to do this , apply , aggregate and transform are your friends !
thanks for the input really appreciate it . i want to write something funny to let you know that i'm extremely grateful but , alas , my sense of humor is a little dark and i don't want to offend the people of stack overflow .

thanx .. you made my day . you could surely join in on the Python chat group some day and help

Have you thought about wrapping a csv reader so that it splits the date column and append day , month and year and create a pandas frame with that iterator .
good post to include new dt example !

and then ` shift ` s the result by ` -offset ` , where #CODE

You need to use the merge operation in Pandas to get a better performance . I'm not able to test the below code since I don't have the data but at minimum it should help you to get the idea : #CODE
sorry I just fixed that It's the result of the first merge

I think it may be simpler to use an apply here : #CODE
@USER this seems to work with apply : s ( Thanks for editing , makes it much easier , if I hadn't already +1d , I would again ! ) :)

When you do an apply each column is realigned with the other results , since every value between 1 and 5 is seen it's aligned with ` range ( 1 , 6 )` : #CODE
When you do the apply , it concats the result of doing this for each column : #CODE

Map each element in a Series to a Series and combine the result into a DataFrame
and a mapping that map pokemon name to the series consisting of its color and weight .

I would then like to apply some vba formatting to the results - but i'm not sure which dll or addon or something I would need to call excel vba using python to format headings as bold and add color etc .
The following example shows how to use the xlsxwriter python library to create a workbook , and insert worksheets , then insert data from pandas dataframes , ( and even charts based on the dataframes into excel ) .

I need to check if the ` dtype ` of this dataframe is ` bool ` . I tried with : #CODE
The other thing to note that ` isinstance ( df , bool )` will not work as it is a pandas dataframe or more accurately : #CODE
That's what I did . I need to check if ` df.a.dtypes == bool ` . How can I do this ?
Okay . I had to do this : ` if df1.v.dtype == ' bool ' : ` to check if it is a boolean DataFrame . Thanks !
@USER ` dytpes ` is in fact a ` numpy.dtype ` see : #URL you could also do ` df.a.dtypes.name == ' bool '` would yield ` True `

I think I'd rather not use join or merge , as these return copies . I'm afraid that adding many ( i.e. , > 1e+5 ) columns to df , each time ( re ) assigning to df , consumes too much memory . Am I correct ?

Both axes objects are available with ` ax.left_ax ` and ` ax.right_ax ` , so you can grab the line objects from them . Matplotlib's ` .get_lines() ` return a list so you can merge them by simple addition . #CODE

Doesn't ` apply ` call my lambda function , once for each column ?
Plain ` x.unique() ` will be an array of the unique elements . Thus ` x.unique() / len ( x )` will be multiplying all of those elements , element-wise , by ` 1.0 / len ( x )` , and giving you back variably-lenghted arrays depending on the amount of unique entries per group .
Note that ` len ( x )` produces an integer , whereas ` x.unique() ` produces an array and the elements of that array will have whatever types they happened to have as entries in ` x ` . So it won't necessarily be the case that ` x.unique() / len ( x )` is well-defined at all . And even if it is technically defined , it might give you unexpected output .
For example , if one of the entries of ` x ` is the integer ` 4 ` and if ` len ( x )` is the integer ` 5 ` , then the entry in the output of the expression ` x.unique() / len ( x )` corresponding to the unique value ` 4 ` will actually be ` 0 ` ( ! ) due to the specifics of integer division in Python .
Thus , even after correcting the mistake of computing the unique array instead of the length of the unique array , you must still be careful : ` len ( x.unique() ) / len ( x )` will also result in an integer being divided by another integer , and most of the time the numerator will be less than the denominator , yielding ` 0 ` .
Minor : you can use ` x.nunique() ` instead of ` len ( x.unique() )` .

how to append / insert an item at the beginning of a series ?
how can i append an item at the very beginning of this series ? the native pandas.Series.append function only appends at the end .
to do that kind of breaks the intent of pandas.Series ... you will likely see that it takes much much longer to insert at the begining of a series than at the end . are you sure that ` Pandas.Series ` is the right data structure for your problem ?
See the Merge , Join , and Concatenate documentation .
Similarly , you can use append with a list or tuple of series ( so long as you're using pandas version .13 or greater ) #CODE

Replace string by numerical value
How about reading the values in as ` NaN ` like ` df = pandas.read_csv ( " file.csv " , names =[ ' A ' , ' B ' , ' C ' , ' D ' , ' E ' , ' F ' , ' G ' , ' H ' , ' I ' , ' J ' , ' K '] , header=None , na_values =[ ' SUPP '])` this will replace ' SUPP ' with ` NaN ` which you should be able to replace
But you can replace ` NaN ` with 3.0 so it should achieve what you want no ?

Originally I thought using ` nonzero ` would make things simpler , but the best I could come up with was #CODE

The problem I'm having is that when I load this into a DataFrame and run resample with a 15min frequency , summing the values in-between , the DateTimeIndex labels are coerced to intra-hour 15 minute intervals ( i.e. 0 , 15 , 30 , 45 ) but what I want is to retain the original time series DateTimeIndex ( i.e. starting from ` datetime.datetime ( 2014 , 2 , 24 , 1 , 7 , tzinfo= UTC )`) . I've tried using the resample ` loffset ` config argument which does affect the preferred behavior on the DateTimeIndex but the summed values aren't changed accordingly . #CODE

Use ` where ` instead of ` apply ` and add days with ` np.timedelta64 ` #CODE

To find the median values , you could use #CODE
but it is less clear which row should be associated with the median , since if there is an even number of rows in a group , the average of the middle rows is used to compute the median .
The median is thus not associated with one row , but rather two .
to find both the median value and an " associated " row .
Thanks . Building on this and trying to make this solution as general as possible . How would I find something more generic , such as the ` median ` ( and its index ) as opposed to the maximum value ?
The median is more tricky . I've taken a shot at it above . In general , I think you would need to find a formula for * the index * of the row you are looking for , and then use ` df.loc ` to pick off that row .

and I want to pivot it like this : #CODE

If you load the spreadsheet into libreoffice , for example , you can see that the column headings are correctly parsed and appear in row 15 with drop down menus to let you select the items you want .
What I meant was you do see rows 1-14 but row 15 has clearly been identified as the column headings by libreoffice . In my version there is a drop down menu in every field for row 15 . Do you not get the same thing ? This is what I see #URL .

Map categories into integer indexes
Merge df1 and df2 ( Note change in index for df2 )
df2 = DataFrame ( data , columns =[ ' f11 ' , ' f12 ' , ' f13 ' , ' f21 ' , ' f22 ' , ' f23 '] , index= np.arange ( len ( data ))

Is it correct to assume that ` df.merge ` can be used as a universal method to solve all merge / join problems that can be solved with the other methods ?
`` join `` is a convenience method that uses `` merge `` ; `` append `` is a convenience method that uses `` concat `` . `` concat `` is for gluing together structures that doesn't need merging logic , while `` merge `` uses fuller logic ( and as a result is ' more complicated ')
Thanks @USER - would it be fair to say that ` merge ` uses a fuller logic than ` join ` ? What about the new [ query language ] ( #URL ) ? Does it support an even richer logic / algebra ?
No the query language is mainly for selection . join is implemented using merge , just take a look at the code , see here : #URL

Just put your code in a function an use ` apply ` : #CODE

join or merge values calculated on grouped pandas dataframe
If I just group the object and apply the interval function , it looks like this : #CODE
Is there a standard way to join or merge values calculated from a grouped object , or should I be using ` transform ` differently ?

Pandas has a ` merge ` function . It sounds like something like this would work ... #CODE

I want to match on ` Id ` , ` Sex ` and ` Group ` and either update ` Time ! ` with ` Time ` value ( from the ` update ` df ) if match , or insert if a new record .
I think I would do this with a merge , and then update the columns with a where . First remove the Time column from up : #CODE

Apply custom function to the temporary column
* keep_default_na * : bool , default True , If na_values are specified and keep_default_na is False the default NaN values are overridden , otherwise they re appended to .
Thanks for the thoughts . That seems to drop the rows with NaN as the index ( good ) although the row seems to still exist ( just empty ); I'm not sure if that will be a problem in the future . The main problem still exists , though . As seen above , some rows have text in the index , I need to get rid of those ! I could just iterate through each row , try converting to a timestamp and delete if it fails , but that seems unnecessary .

But I cannot work out how to use the group by functions ( transform , apply , etc ) to achieve the same result . How can I do this in a concise way using pandas ?
( Please note : In the full application this is not a sine wave but is a measured physical response to a signal sent out at the start of each period . So I am not looking for a robust way to align the signals or to detect frequencies . )
@USER it's the for loop which is slow , as is apply ( to a lesser extent ) . Although it's possible this could be made faster !

If ` NaN ` indices are not supported by Pandas , we can drop the empty entries in the dictionaries above .
use ` concat ` : #CODE

How to efficiently join a list of values to a list of intervals ?

Weighted average using pivot tables in pandas
I have written some code to compute a weighted average using pivot tables in pandas . However , I am not sure how to add the actual column which performs the weighted averaging ( Add a new column where each row contains value of ' cumulative ' / ' COUNT ') .

Numpy : Drop rows with all nan or 0 values
I'd like to drop all values from a table if the rows = ` nan ` or ` 0 ` .

I am reading data from image files and I want to append this data into a single HDF file . Here is my code : #CODE
So , I am hoping someone can explain why this isn't working and how I can successfully append all the data to one file . I am willing to use a different method ( perhaps pyTables ) if necessary .
This will work in 0.11 . Once you create a group ( e.g the label where you are storing data , the ' df ' here ) . If you store a ` fixed ` format it will overwrite ( and if you try to append will give you the above error msg ); if you write a ` table ` format you can append . Note that in 0.11 , ` to_hdf ` does not correctly pass keywords thru to the underlying function so you can use it ONLY to write a ` fixed ` format . #CODE
default mode is to append ( to the file ); w creates a new file

In the excel file the value looks like 1 / 31 / 2014 5:47 : 00 AM In case this error message helps u out C :\ Miniconda\lib\ site-packages \pandas\tseries\ index.pyc in __new__ ( cls , data , freq , start , end , periods , copy , name , tz , verify_integrity , normalize , closed , ** kwds )

Here's a workaround , by using append . #CODE

Does pandas have ` groupby after join ` similar functional ? So I can right join the df with my list of tuple ( on the multi index ) and then ` groupby ` ?

Apply then calls the function on each group and assimilates the results
The result of apply is a list of index values that represent rows with B == 1 if more than one row in the group else the default row for given A
The index values are then used to access the corresponding rows by ix operator

you have to use the `` * `` , you pass a list-of-dataframes to concat
I think the complaint is more that whenever any operation is done append , concat , ... it does not do it in place but returns a df instead . This makes it hard for in-place operations like done with list comprehension . I agree it's a pain ; however , I am currently unaware of any in-place operation .

How to merge pandas dataframes bit by bit in python
I'm wondering how i can do the following merge ?
I have a larger dataframe ( newauthdf ) , which consists of Nodes and Years ( from 1990-2000 ) . I'd like to calculate eigenvector centrality for each of these nodes conditioned upon the year.So the node ' Bill Clinton " will have a different centrality in 1990 versus 2000 . My implementation is below , but I keep getting ' cannot create Block manager with duplicate items set ' . From reading stackoverflow , it seems like this error is created because of redundant columns . I don't want the merge to create new columns for each year-centrality merge , but rather add to the existing column . Is the best way to deal with this to just create the complete eigenvector dataframe at once AND THEN merge with my newauthdf ? #CODE
instead of appending one-at-a-time , append the created dataframes to a list , then at the end `` df = pd.concat ( list_of_dataframes )`` will give you the result

#URL - unsupported at this time . unless a lot of interest in it ( these are done in python space anyhow , so no real advantage to using eval here ) .

that I can join on ` df1_keys ` and ` df2_keys ` .
Although these aren't supported directly , they can be achieved by tweaking with the indexes before attempting the join ...
and set union with the ` | ` and intersection with ` ` : #CODE
So if you have some DataFrames with these indexes , you can reindex before you join : #CODE
So now you can build up whatever join you want : #CODE
How would you extend this to doing the algebra on columns and not on indices ? ( i.e. I mean doing the set difference on a particular column , as in the diagram in the OP ) . I thought I could simply do ` df.set_index ( columns_for_the_join )` before I join , but some of the entries in the columns are ` NaN ` , and when I convert this column to indices , it fails , since I can't later index my Dataframe with ` ind_diff = ind - ind2 ` if ` ind ` ( and therefore ` ind_diff `) have NaN values .
@USER columns are also an ` Index ` , so you can do the same set operations with ` df.columns ` . Not sure I understand what you're asking , you should be able to join on indexes with NaNs ... Perhaps ask this as a new question ?

Can I merge them into : #CODE
where the column is the userId , index is the title . isbn is dropped after the merge .
Once you ` merge ` these frames , you can use ` pivot_table ` : #CODE
The data is sparse but I do need them . I am gonna try to apply on a small set of data first . Thanks a lot !

` loc , iloc , ix ` : row-first
A natural question of mine : why don't pandas developers unify the row / column propensity of DataFrame operations ? For example , that ` [ ]` and ` loc / iloc / ix ` are two most common ways of indexing dataframes but one slices columns and the others slice rows seems a bit odd .
loc / iloc / ix are multi axis indexers able to index all axes at the same time ; [ ] only handles columns and is a dict like accessor ; these are pretty distinct and useful in their own right . the most common ops are probably [ ] accessing ; making it harder to do this would just make code more verbose

Ok , the ix trick is doing it , but there must be a deeper reason for this problem . I thought I can treat a pandas Series very much like an ordered dict - or am I completely wrong here ...

Why pandas xs doesn't drop levels even if drop_level = True
It doesn't drop for ` dfi.xs (( -1 , -1 , -1 ) , drop_level=True )` ( one 1 match )
then why ` dfi.xs (( 2 , 1 ) , drop_level=True )` can drop ?
And not drop for ` dfi.xs (( -1 , -1 , -1 ) , drop_level=True )` ( one 1 match )

When does Pandas xs drop dimensions and how can I force it to / not to ?
Sometimes I see ` xs ` would return a Series from a DataFrame if the return is only one row , sometimes not . How to enforce it happen / not happen ? ( may be related to Why pandas xs doesnt drop levels even if drop_level = True ) #CODE
the issue you filed was completely different than the issue of drop level not working with a fully specified indexer -that might be a bug - what you filed is not

I have a Pandas dataframe where I am trying to replace the values in each group by the mean of the group . On my machine , the line ` df [ " signal "] .groupby ( g ) .transform ( np.mean )` takes about 10 seconds to run with ` N ` and ` N_TRANSITIONS ` set to the numbers below .
@USER that's not the same though is it ? The result of ` df [ " signal "] .groupby ( g ) .mean() ` has length ` len ( g )` rather than ` len ( df )` .

Is there a vectorized way to apply that formatting command in either context ?
This is the solution I knew was out there somewhere . Still need to get my head around all the possibilities with ` replace ` . More generally , can ` str.replace ` handle named groups ?

Ok , then how would I apply that in my case ?

The way to get the previous is using the shift method : #CODE
You can just apply this to each case / group : #CODE
your last can just be : `` df.groupby ( ' case ') [ ' change '] .diff() `` ( though I don't think `` diff `` is cythonized so speed should be the same

One way is to ` replace ` the lower zeros with NaNs : #CODE

It seems using ` at ` ( or ` iat `) is about 10 times faster than ` loc ` ( or ` iloc `) .

it works now with using stock.iloc [ -1 ] [ ' Close '] , what is the diff between [ -1 :] and [ -1 ] , i thought [ -1 :] is syntactic rightful than [ -1 ] ... i used stock.ix [ -1 ] [ ' Close '] with 10.1 , but after the upgrade pandas 13.1 could not tolerate [ -1 ] with .ix
`` [ -1 :] `` returns a slice , while `` [ -1 ]`` is a scalar . You have to be very careful with `` ix `` , see here : #URL this has been true since beginning of pandas , but docs are newer

This is the key it tells loc which rows to set #CODE

how to apply a function to multiple columns in a pandas dataframe at one time
Question : 1 - what if I have a dataframe with 50 columns , and want to apply that formatting to multiple columns , etc column 1 , 3 , 5 , 7 , 9 ,
Is there also any way to programatically create that string ( which would change depending on the number of columns you had ) and apply the format_number function ? I.e. the above would work fine if I knew exactly how many columns were in the sheet every time , but If I didn't know the number of columns , and wanted to apply the same function to every column , is there a better way of doing it ?
@USER : If you just want to apply it to all the columns , just do ` df.applymap ( format_number )` .
You could use ` apply ` like this : #CODE
@USER ignoring my code example , if you perform ` apply ` to a dataframe then the dataframe itself is modified by any changes in your function so you would not need to assign to the column , you may still need to depending on what your function is doing . The point being that you just need to call ` df.apply ` and not need to say do ` df [[ ' col1 ' , ' col2 ' , ' col3 ']] =d f.apply ( lambda row : format_number ( row ) , axis=1 ))` , in my code the assignment is done by the ` format_number ` function so I guess the assignment is implicit rather than explicit like BrenBarn's answer

If you have the ISIN , then you also have the the symbol , see the wikipedia page for information about how to translate ISIN to symbol :
Write a little bit of python code to translate the ISIN back to the symbol , then use the usual DataReader query .

Alternatively you could simply use the apply function on all rows of df . #CODE

and hence I can apply ` timedelta64 ` conversions . For microseconds #CODE

A third alternative is to first get the unique members of ` xlab ` ( using e.g. ` set `) and then map each xlab to a position using the unique set for the mapping ; e.g. #CODE
@USER - I've added in two alternative schemes that both give uniform scaling ; one gives every point a new x / y position , while using e.g. ` set ` allows for the same label to map to the same x / y position .
You might want to demonstrate labeling the x and y ticks with their respective strings . E.g. ` ax.set ( xticks=range ( len ( xuniques )) , xticklabels=xuniques , ... )` Either way , nice answer !

pandas replace multiple values one column
In a column risklevels I want to replace Small with 1 , Medium with 5 and High with 15 .
You could define a dict and call ` map ` #CODE
yes that is correct ( but I realize that the issue is that the OP replace format is wrong )
Your replace format is off #CODE
I wasn't sure what was wrong with the ` replace ` format line so I suggested using ` map ` instead . +1 for spotting OP error

You can go by using the power of apply function : #CODE

You could also replace the and / or idiom with if-else construct : #CODE

have a mixed-type frame . Instead use loc for this . See the docs on why this ia bad idea and may not work ( which it doesn't here ) , #URL #CODE

You can use ` apply ` and test your column like so : #CODE

what would be the most efficient way to use groupby and in parallel apply a filter in pandas ?
You can write more complicated functions ( these are applied to each group ) , provided they return a plain ol ' bool : #CODE
just chain em ( but you do need a second groupby ): `` DataFrame ([[ 1 , 2 ] , [ 1 , 3 ] , [ 2 , 5 ] , [ 2 , 8] , [ 5 , 6 ]] , columns =[ ' A ' , ' B ']) .groupby ( ' A ') .filter ( lambda x : len ( x ) > 1 ) .groupby ( ' A ') .sum() ``

I have financial trade data ( timestamped with the trade time , so there are duplicate times and the datetimes are irregularly spaced ) . Basically I have just a datetime column and a price column in a pandas dataframe , and I've calculated returns , but I want to linearly interpolate the data so that I can get an estimate of prices every second , minute , day , etc ...
What I started to do was write code to interpolate my data linearly so that I could get the price every 10 seconds , minute , 10 minutes , hour , day , etc . However , with business days , weekends , holidays , and all the time where trading can't happen , I want to make python think that the only time which exists is during a business day , so that my real world times still match up with the correct date times , but not such that I need a price stamp for all the times when trading is closed . #CODE

API reference for read_csv tells that it returns either a DataFrame or a TextParser . The problem is that concat function will work fine if X_chunks is DataFrame , but its type is TextParser here .
You can't use concat in the way you're trying to , the reason is that it demands the data be fully materialized , which makes sense , you can't concatenate something that isn't there yet .

Python -- Pandas : How to apply aggfunc to data in currency format ?
I have a table above . Want to apply groupby function to the data and apply sum ( over revenue_total ) . Pandas gives an NA value since revenue_total is an object data type . Any help #CODE

Note : For large frames it may be worth making these columns an index ( to perform the join as discussed in the other question ) .
One way to merge on two or more columns is to use a dummy column : #CODE
The booleans were present in df2 , the True has an NaN in one of the merging columns . Following your spec , we should drop those which are False : #CODE
Fantastic ! Since your solution depends on the use of ` Series ` , what would happen if we are doing the difference ( i.e. join ) on ** multiple columns** ?
` AttributeError : ' DataFrame ' object has no attribute ' isnull '`
keep in mind this is also destructive / has side-effects : changes index of L and R and replaces NaNs with NULL_VALUE on those cols . Also diff may have different index

But I've encountered cases where there are multiple records in the `' map '` row with the same value ( the max ) . I noticed that the Index object ( containing the corresponding column identifiers ) has both a ` .min() ` and a ` .max() ` method , so I thought this might be a good way to extract a single value . Any of the following throws a repeating error in Ipython that finally exits with a " recursion limit reached " catch : #CODE
Do you mean : ` best_param = df.index [ ix ]` ?
@USER No , I don't . The columns each correspond to a test of a different parameter value ( and are named with the parameter value ) , and the rows are different metrics . I'm trying to extract that parameter that gave the highest metric . So I need ` df.columns [ ix ]`

Have you looked at the online docs for [ ` dropna `] ( #URL ) and also [ this ] ( #URL ) ? ` dropna ` by default will drop rows where any values are ` NaN ` , what do you want to achieve ? dropping entire series ( columns ) that have any ` NaN ` ? If so do ` dataframe [ dataframe <= 0 ] .dropna ( axis=1 )`

Apply Different Resampling Method to the Same Column ( pandas )
I have a time series and I want to apply different functions to the same column .

Handling Duplicate Columns in Pandas DataFrame constructor from SQLAlchemy Join
I know that ` read_csv ` has ` mangle_dup_cols ` but how can I do the same from a sql join in sqlalchemy after issuing : #CODE

Pandas append filtered row to another DataFrame
I have 2 pandas data frames ` df ` and ` df_min ` . I apply some filters to ` df ` , which results in a single row of data , and I'd like to append that row to ` df_min ` . I tried using a loop to traverse ` df ` , and tried using ` loc ` to append the row to ` df_min ` . I keep getting a ` Incompatible indexer with DataFrame ` ValueError for the line where I use ` loc ` . I guess I am not using ` loc ` correctly . What would be the best way to accomplish what I am trying to do ? #CODE
EDIT : I also tried the following instead of ` loc ` , but got the same error : #CODE

` combine_first ` is just matching the index , not the values ( like a merge would ) , and updating the NaNs in these rows . If you had additional columns in ` question_struct ` or NaNs in ` df ` then these would be combined into the new DataFrame , since you don't it doesn't extract anything .
So whats the best solution here ? A merge ?
@USER it depends what you want to happen with discrepancies / differences in the data . If there both the same , use merge , if you want them to update in combine_first you have to index them with the unqiue index .

Note : replace works for the None but not the ` [ ]` ... this solution seems to be a little sensitive ( hence the use of negation ` ~ `) ... #CODE

Opposite of melt in python pandas
I cannot figure out how to do " reverse melt " using Pandas in python .
[ Docstring of melt ] ( #URL ): " Unpivots " a DataFrame ... :)

i try to convert the Date into a string but whatever i try it doesn't working . I tried to loop over the row and convert it with str() . I have tried to change the dtype of the object with ` dt [ ' Date '] .apply ( str )` and I have tried a special dtype object and use that : #CODE
for i in range ( len ( tesla ) , 5 ):

Pandas how to apply multiple functions to dataframe
Is there a way to apply a list of functions to each column in a DataFrame like the DataFrameGroupBy.agg function does ? I found an ugly way to do it like this : #CODE

This psuedo code demonstrates how to reduce a set of points to a single point per grid partition while tallying the number of points in the grid partition . This can be useful if you have a set of points where some areas are sparse and others are dense , but want an even distribution of displayed points ( such as on a map ) .
For each point that is found in the partition , the function increments a tally count . Finally , when the reduction / tallying is completed per grid partition , one can then visualize the tallied point ( e.g. , show marker on map at the single point with a tally indicator ): #CODE

Assuming these were datetime columns ( if they're not apply ` to_datetime `) you can just subtract them : #CODE

Now you can now set this as the index and resample : #CODE

But when I apply it to the DataFrame , I get an error . #CODE

Once the string is a date time object ( dt ) , I believe the code is : ` return dt.year , return dt.month , return dt.day ` .

Thanks to @USER for pointing out the sweet apply syntax to avoid the lambda .
Thanks , Andy . Apply actually passes the values correctly , so you don't need the lambda . I edited the answer to use the simpler syntax , and avoid the lambda .

I am running TF-IDF on a single column . I want to use this TF-IDF and another scaled integer column to train my Logistic Regression classifier . Unfortunately I am running into problems doing this as I am just beginning in machine-learning . The code currently has a bug , but the bigger issue is that I am simply unsure how to join the two features together and what is best practice here . Would anyone be able to help me get this running please ? #CODE

pandas DataFrame pivot table sum function not correct
After applying the pivot function to my df , I am returned with data that dont make sense :
What are the data types ? If you have mixed types that could cause the problem . For example , you can't sum a mix of strings and floats in pandas but Excel would silently drop the string value and sum the floats .

That returns a numpy array , you could just use the mask to perform the dataframe selection like ` df [ df > 2 ]` this returns a dataframe with ` NaN ` for values that do not satisfy the boolean criteria and the values that do as a dataframe . Up to you what you then do with ` NaN ` values , either set to ` 0 ` or drop them using ` dropna `

How do I join columns from diferent Pandas DataFrames ?

hello Jeff , was playing with min periods , when you set min_periods=1 it seems to perform like expanding_mean() after a NaN till it hits the length of the window . bar [ 0 , 7 ] on your example is the mean of 7 ( len 1 ) and bar [ 0 , 8 ] is ( 7+ 8) / 2 . think this is why you have values 0:3 and I do not

Then apply a lookup from ` df2 ` to each element of the lists in ` vals ` : #CODE
Finally concat : #CODE

you can use ` shift ` and ` | ` operator ; for example for + / - 2 days you can do #CODE
Thanks Behzad . could you flesh this out a bit more ? I keep getting this error : TypeError : unsupported operand type ( s ) for | : ' bool ' and ' float'for ----> 3 idx |= data2.buy.shift ( 1 ) | data2.buy.shift ( 2 )

In my practice , the strongest , easiest-to-see difference is that a Panel needs to be homogeneous in every dimension . If you look at a Panel as a stack of Dataframes , you cannot create it by stacking Dataframes of different sizes or with different indexes / columns . You can indeed handle more non-homogeneous type of data with multiindex .

Note : that's a really bad way to iterate over the rows , either use iterrows or apply . Using range like that creates a huge python list ( in python 2 ) , xrange is slightly better .

I believe that you are operating on copies of the dataframe . I think you should use ` apply ` : #CODE

I also tried something like this ( suggested on another stack overflow question ) #CODE

When I viewed the ` df ` the rows and columns had been transposed , so that the index made 232 columns and there were 5 rows . How can I set the index vertically , or transpose the dataframe ?
Paul's response is the most preferred way to perform this operation . But as you suggest , you could alternatively transpose the DataFrame after reading it in : #CODE

` join ` the items with space : #CODE

Note that this assumes that all the values are either 0 or 1 , as the " binary values " in the question makes it seem . We could use ` ( df [ listvals ] ! = 0 ) .sum ( axis=1 )` instead if we only know that it's 0 or some nonzero value so that ` [ 0 , 3 , 0 ]` doesn't fool us .

This means that ` FrioAcum ` is not iterable , it is viewing it as a single value , it needs to be a list or other iterable object in order for the shapes to align , what exactly is ` FrioAcum ` ?

This looks like a job for pivot : #CODE
If you don't want a MultiIndex column , you can drop the ` col3 ` using : #CODE

add ` ~ ` to the beginning and drop ` regex=False ` ; i guess that is added in ` 13.1 ` ;

Now you unstack the second level ( type ) to make it a column and fill in the blanks : #CODE

I want to be able to assign a matrix of values to a matrix of locations using loc or , in this simplifed case , I just want to modify them by some simple operation like multiplication .

@USER I added the complete stack trace . Do you mean that I should convert the .xlsx to a .csv ?

How do we resample above series with 0.1 interval ? look like the .resample func only work on datetime interval ?
We need ` new_idx ` to be a union of the original index and the values we want to interpolate , so that the original index isn't dropped .
I think using cut is more stable , was hoping you / someone knew a better way ! Floats are fiddly / sensitive as we've just found , they're sometimes useful though ... just handle with care .
One option is to use cut to bin this data ( much less elegant than a resample but here goes ): #CODE
Say if you wanted to resample with how= ' sum ' : #CODE
There's a lot of NaNs now , you can now , as Tom suggests , interpolate : #CODE

You can just resample by ' 2H ' : #CODE

I want to generate a single column of them with a unique index so that I can merge it with the columns from the other dataframes at a later point , which would looks somewhat like this , but with an unique index : #CODE
some kind of join ? Not clear what you're after .
Do you want to stack AAPL / GOOG / MSFT on top of each other in one column , add the ' Returns ' & ' WinLose ' columns , and then add another column ' unique_id ' ( or something similar ) ?
Sorry for my lacking explanation . If you look at the data frame ' frame ' , that is what I want . But I want to stack e.g. the skew metric for all the stocks on top of each other but with a unique index so that in frame it can be joined with the other metrics calculated from the same period .

have you looked at resample , or this section of docs : #URL
resample data . #CODE

How to pass more than one parameter to map function in panda
At present I have to write different functions with hardcoded values for different types of field . MAP function only takes 1 argument . Is there any other function other than MAP
I don't think so map operates on each element in a series , if you want to pass do something with multiple arguments on a row-wise basis then you could use ` apply ` and set ` axis=1 ` like so ` mn.apply ( lambda row : getTag ( row ) , axis=1 )` in ` getTag ` you can select the columns like so : ` row [ ' fld1 ']` and ` row [ ' fld2 ']` . This should achieve what you want
Instead of using map , you could use pd.cut ( Thanks to DSM and Jeff for pointing this out ): #CODE
And to answer the general question : Yes , there is a way to pass extra arguments -- use apply instead of map ( Thanks to Andy Hayden for pointing this out ): #CODE
Still , I don't recommend using ` apply ` for this particular problem since ` pd.cut ` is be faster , easier to use , and avoids the non-deterministic order of dict keys problem . But knowing that ` apply ` can take additional positional arguments may help you in the future .

Resample pandas times series that contains elapsed time values
I think the trick is to extract start and end times and resample , I think there is a cookbook example for what's on and off at each period , this is a ( fiddly ) extension of those examples ...
One way to get the split times is to resample from Start and End , merge , and use diff : #CODE
To replace the NaTs with 0 it looks like you have to do some fiddling in 0.13.1 ( this may already be fixed up in master , otherwise is a bug ): #CODE

How to apply a long set of conditions on a pandas dataframe efficiently - stock backtesting
I'm attempting to apply a long set of conditions and operations onto a pandas dataframe ( see the dataframe below with VTI , upper , lower , etc ) . I attempted to use apply , but I was having a lot of trouble doing so . ) . My current solution ( which works perfectly ) relies on a for loop iterating through the dataframe . But my sense is that this is an inefficient way to complete my simulation . I'd appreciate help on the design of my code . #CODE
Get row data simply obtains key data from the dataframe and computes certain information based on globals ( like how much capital I have already , how many stocks I have already ) and spits out a list . I append all these lists into a dataframe that I call the portfolio .
Generally speaking , you need to assess the way in which the desired result for a given row depends on the data that is " higher up " in your set . If a given row's output can be created based on * input * data in higher rows , you can save yourself some time and effort using something like ` apply ` . But if your desired output for a given row depends on the * output * from earlier rows , then your problem is inherently ordered and as a result , you're unlikely to be able to do much better than a ` for ` loop even if you wind up with cleaner code .

yep ; you need to isolate setting / reading in a locked object ; note that ' continuously expanding with new rows / cols ' is in general a bad idea . you should simply create new frames with the new data , then `` concat ` them together to create new one . You will be copying data in both cases .

Pandas merge columns , but not the ' key ' column
Then I need to drop the ` id ` column since it's essentially a duplicate of the imp_type column . Why does merge pull in the join key between the 2 dataframes by default ? I would think there should at least be a param to set to False if you don't want to pull in the join key . Is there something like this already or something I'm doing wrong ?
I know , but it's just frustrating since it shouldn't have been implemented that way from the beginning , and to have to do it after every merge adds up and feels hacky .

How do I group by Time by strip away microsecond , for example ,
I guess I forgot resample function , but how about a generic function ( or lambda ) . Also , resample methods are limited , right ?
You need to make the datetime the index so that you can resample : #CODE
How do I apply a count function ? like how= ??? ) Hmmm , I figured out , it's np.size

Replace rarely occurring values in a pandas dataframe
You can [ set up a progress meter for apply ] ( #URL ) , but this obviously slows down whatever it is you're doing . Generally a bad idea to return different types of data in an apply ( here a string or a Series ) , it's unclear what you want the apply to return ...
@USER Your comment suggests to me that I may not understand ` apply ` properly . My understanding was that my function would return the string ' RARE_VALUE ' if the condition were met but keep the existing string / null if it weren't . Is this incorrect ?
Ah wait , I see what you're saying , I mistook this for a DataFrame apply . No you're correct , but boolean masking at each step is ** slow** !!
@USER similarly for the astype bool ...

" the consumer which samples closest to the event time ' wins '" . Are we talking Price-Is-Right rules or smallest ` abs ( event_time - consumer_time )` ?

That works great ! Thanks @USER I didn't know about ` loc `

Pandas : peculiar performance drop for inplace rename after dropna

that's near what i am guessing ... but even yahoo just put the parameters ( index as u guess ) , the length of data chunk to download is vastly diff , i am talking a about 1:200 dataframe ( there are 6~7 nos of column , e.g. open , high ... adj close , etc . ) time 1200+ nos of stocks !!! shouldn't there be any performance diff ? and , if your guess is right , anything i could do to enhance the time to download ( performance ) ?

@USER Because your data don't need to be aggregated in any way . They're all unique values . For example , ` groupby ` would be appropriate for determining the sum of all GDPs , or the median population , etc . All you're looking for is values already available in the dataframe .

Or , if these are numpy arrays you need to apply tolist to each item first : #CODE

Ok , here's one way to attack it , using features from the ` matplotlib ` ` hist ` function itself : #CODE

I have a DataFrame with multi-index [ ' timestamp ' , ' symbol '] that contains timeseries data . I merging this data with other samples and my apply function that uses asof is similar to : #CODE
yes , use merge and vectorized operations instead . You are doing an O ( n2 ) operations in python space here searching each the entire rowset each time .
@USER I added an example , I hope it's clear . Basically I'm finding the appropriate timestamp and assigning that col first to the destination table , then doing a merge on that . Perhaps there is a better way ?
not sure what you are going for here . looks like you might simply want to resample on both intervals to common times then join , seems easier .
@USER These aren't intervals they are event timestamps and since they don't align , I need to use indexof to find the " closest " match .

pandas groupby apply function that take N-column frame and returns object
Is there a ' transform ' method of something like that to apply a function to groups ( all columns at once ) and return an object ? Anything I try seems to return one object per column in the group .
and suppose I do a groupby on Date and apply some function to the groups labeled by ( Term , Month , s ) . The result should be something like #CODE
You could apply the function and then aggregate each group manually . For example , assuming the aggregation is a mean and the function is the sum of the column , you could : #CODE
Then we can apply the fit() on all the columns at once for each group of rows : #CODE
I should have mentioned that I can use the ' apply ' or ' agg ' function but it returns the same ( redundant ) object for each column . So it appears that my function is receiving access to the full group each time ( which is what I want ) but that it is being called once for each column in the group ( or at least that's how the result is populated ) .
Hmmm , I think you are correct for the usage of ` agg ` , but the ` apply ` method should normally receive all the columns at once for each group . I edited my answer above to illustrate that .

You can convert all elements of id to ` str ` using ` apply ` #CODE

Join two Dataframes
As you can see the date format is slightly different . How do I join the two data frames using the date index . So basically use #CODE
Or , if you wish to keep all the keys in both indexes , use an outer join : #CODE

You've got a few good answers down below . One thing to note : your preferred solution probably depends on what you want to do in case the indices of ` data ` and ` other_data ` are not identical . Do you want the new dataframe to have as its index the union , or the intersection or some other combination of the indices of the building blocks ? ` concat ` should get you the union . Various options for ` join ` and ` merge ` let you explore some of the other possibilities .
Note that you need to use ` axis=1 ` to stack things together side-by side and ` axis=0 ` ( which is the default ) to combine them one-over-the-other .
Seems like you want to join the dataframes ( works similar to SQL ): #CODE
The ` on ` kwarg takes a list of columns or ` None ` . If ` None ` , it'll join on the indices of the two dataframes . You just need to make sure that you're using a dataframe for the left size -- hence the double brackets to force df [[ ' foo ']] to a dataframe ( df [ ' foo '] returns a series )

You could drop the NaN and then strip the offset and then do the conversion : #CODE

Pandas : drop a level from a multi-level column index ?
How can I drop the " a " level of that index , so I end up with : #CODE

Reorder columns in pandas pivot table

You really want to return a bool to indicate rather than a string . Also avoiding apply where possible .

As you prefer you can transpose the result if you prefer : #CODE

I already know you can pull specific keys from dictionary objects in pandas if you already know the exact value for the key , but what if you wanted to pull the median key's values of a dictionary without knowing the value ( or in this case , author name ) ?
So I would want to get the median author name . There are five authors , so the 3rd author is the one I want ( george ) , and just print all the data associated with him . Then eventually I'd also want to print the number of books he's published ( which is two ) . Do I have to convert the dictionary object back to a csv file or something ? Tips or helpful tutorials anyone knows on pandas dictionary objects would be great , thanks !
If you were taking about an integer / float column then you can just use the median method : #CODE

I love seeing the same basic idea of what I pieced together with no coding knowledge - but written correctly . This is the kind of thing you just couldn't get in a class ! The best part of this is that I can adapt this code much more easily , and I could replace the " 4 " in sets with a variable that splits whatever I need .

Any idea how I can replace ' Consultation ' with blank and convert the column data type to int64 or float ?
The data is a bit distorted that is why when I initially read with pd.read_csv , the column is mixed with string and int . It is my wish to remove the string part to apply aggregation functions on that columns .

I don't understand the question . Transpose will make years into index .
Transpose using ` df.T ` .

My basic question is can I efficiently apply this structure to HDF ? Specifically :

which of course as to do with the length of ` x ` . How can I " reshape " x to merge both ? I looked online on how to modify the length but no success .
So prob easiest to simply drop them and reindex to the new times you want . If you WANT to preserver Time / spread duplicates then this becomes a much more complicated problem . You will have to either use a multi-index and loop on the duplicates or better yet just resample the data down ( e.g. say mean or something ) .
`` resample ( .. )`` is your friend
You can't actually resample this at the nanosecond level it will blow up your memory , but see above for an easy way .

interestingly this is slower than the map method in the other answer ` 10000 loops , best of 3 : 96.4 s per loop ` versus ` 10000 loops , best of 3 : 125 s per loop `

apply hierarchy or multi-index to panda columns

TypeError when trying to join Pandas dataframe by index
I'm trying to join a column from one ` pandas dataframe ` to another using its date as the index . However my code produces a ` TypeError ` . Please could somebody explain why this error's being produced and what I can do to fix it ?
I think you want to do this ` cln_df22 = cln_df2.append ( df3 [[ ' MeHg ( ng / L )']])` , what you attempted was to access an item from the dataframe of the append attribute which doesn't exist

well it say `` df.ix [ ' bar ' , ' one ']`` , note that it uses `` ix `` ( its very similer to `` .loc ``) . but this is NOT the same as `` [ ]`` . In general with a multi-index you should use a full tuple indexer , or `` .xs `` . The interpretation of a list of indexers * can * be ambiguous
That example , has a COLUMN multi-index ( it IS a bit confusing as it then takes the transpose to make it a multi-index on the INDEX ) .

I am trying to join two numpy arrays . In one I have a set of columns / features after running TF-IDF on a single column of text . In the other I have one column / feature which is an integer . So I read in a column of train and test data , run TF-IDF on this , and then I want to add another integer column because I think this will help my classifier learn more accurately how it should behave .
What is causing my problem here ? How can I fix this ? As far as I can see I should be able to join these columns ? What have I misunderstood ?
Take a sequence of 1-D arrays and stack them as columns to make a
As ` X ` is a sparse array , instead of ` numpy.hstack ` , use ` scipy.sparse.hstack ` to join the arrays . In my opinion the error message is kind of misleading here .
So , use ` scipy.sparse.hstack ` when you have a sparse array to stack .

Filter out pandas pivot table rows
I have a pivot table like so : #CODE
' Imps Type ' not ' Imp Type ' right ? If the cut off is 3000000 : #CODE
Meaning keeping the rows where at least one of the value are above the threshold , or drop the rows where all the values are below the threshold ? See edit .

Honestly I'm lost as to how to do it . Do I need to append data from the original reviews back onto my sorted dataframe ? Do I need to make a function to apply onto the groupby function ? Tips or suggestions would be very helpful !
It isn't entirely clear what you want to do . But the size() function ( not groupby ) is what removes most of the columns . The columns ( like date ) aren't specific to the reviewer so it's not clear what it would mean to append them to the review counts . But you could do reviews.groupby ( ' critic ') .date .max() and similar functions to summarize data from other columns .
As DanB says , groupby() just splits your DataFrame into groups . Then , you apply some number of functions to each group and pandas will stitch the results together as best it can -- indexed by the original group identifiers . Other than that , as far as I understand , there's no " memory " for what the original group looked like .
Instead , you have to specify what you want to output to contain . There are a few ways to do this -- I'd look into ' agg ' and ' apply ' . ' Agg ' is for functions that return a single value for the whole group , whereas apply is much more flexible .
Suppose you want to return a dataframe of the first and last review by each reviewer . We can use ' apply ' , which works with any function that outputs a pandas object . So we'll write a function that takes each group and a dataframe of just the first and last row :

one way to do that is to ` tag ` each sibling as ` 1 ` ( first ) or ` 2 ` ( second ) and then pivot ; for example starting with #CODE

I am trying to run hstack to join a column of integer values to a list of columns created by a TF-IDF ( so I can eventually use all of these columns / features in a classifier ) .
Finally , I want to join them all together , and this is where our error occurs and the program cannot run , and also I am unsure whether I am using the StandardScaler appropriately here : #CODE
Another possibility is to use Pandas's ` concat ` . #URL . I assume Pandas has paid more attention to these issues than the ` sparse ` coders .

Pandas resample
I am trying to resample the data in pandas , to a regular 100 milliS or a S frequency . I expected this to work
x = ts.asfreq ( ' 100L ' , method = ' ffill ') which seems to work . Am I doing something wrong ? Is resample not meant to be used here ?
But that is not what the ffill method does . When you pass how = ' ffill ' to the resample method , it uses dataframe.ffill which fills in null values . It's a synonym for #CODE
If you want to use resample and return the first observation within each group , use the ' first ' method : #CODE
So , are you really looking for a sampled series or do you just want to drop superfluous rows ? You could maybe use ` drop_duplicates ` or a ` groupby ` .
@USER Downsampling is a form of resampling so this should be the function for you ! Resample also takes an argument ` fill_method ` , which can be set to ` ffill ` which I think is what you were trying to get to before . You should take a look at the [ resample documentation ] ( #URL ) which will give you more flexibility to customize .

Pandas has a lot of built-in functionality to apply functions in a vectorized way over Series and DataFrames . When that fails , you can use ` map ` or ` apply ` . Here ` map ` will applies a function element-wise on a Series .
For more on map and apply , see this answer .

Then , regarding your more general question you could use ` map ` ( as in this answer ) . In your case :

I want to check if there is minute without any data , so I did resample #CODE
This is better than having multiple DataFrames , because you can apply fast numpy / pandas operations to the entire DataFrame whereas , if you had a list of DataFrames you would be forced to use a Python loop to operate on the sub-DataFrames individually ( assuming you want to perform the same operation on each sub-DataFrame ) . Doing so is generally always slower .

Is it possible to use cut on a collection of datetimes ?
Effectively you turn ` datetime ` or ` date ` into a numerical distance measure , then cut / qcut it .

Say I have a dataframe ` my_df ` with a column `' brand '` , I would like to drop any rows where brand is either ` toyota ` or ` bmw ` .

I can then ` join ` that to my original dataframe , and ` fillna ` with the ` method= ' ffill '` #CODE

Note : ( as pointed out by OP ) by default NaNs will propagate ( and hence cause an indexing error if you want to use the result as a boolean mask ) , we use this flag to say that NaN should map to False . #CODE

However , DataFrame , unlike Series , has no ` map ` method . The other obvious alternative , ` s.ix [ d ]` , gives me the error " Cannot index with multidimensional key " , so this is apparently not supported either .
I know I can do it by converting the DataFrame to a list of lists , or by using a row-wise ` apply ` to grab each item one by one , but isn't there any way to do it without that amount of overhead ? How can I do the equivalent of ` Series.map ` on multiple columns at once ?
You could create a MultiIndex from the DataFrame and the ix / loc using that : #CODE
My timing results are somewhat different from yours . I get 900us for the MultiIndex solution and only 90us for your ` apply ` . But that ` apply ` doesn't actually do the indexing ; timing ` d.apply ( lambda r : s.ix [ r ] , axis=1 )` gives a much slower result of about 5.5ms . However , I thought of another way : ` d.apply ( tuple , axis=1 ) .map ( s )` . This seems to be even faster at about 580us . Even so , doesn't it seem like there should be a built-in way to do this without creating a new data structure ? The values I want to index with are already sitting right there .

If you create a frame of a single dtype it is a view . If you create it then add a column , it is not . This is the reason for the difference in assignment behavior . @USER answer gives a higher level reason while you should not rely on this behavior and just use loc always speciying all indexing dimensions .

Widening Pandas Data Frame , Similar to Pivot or Stack / Unstack

I am reading rows from the origin dataframe and trying to append it to target dataframe .
then i construct a dict to append to target dataframe . #CODE
As an intial comment , iterating over an object and then modifying it inside the loop is inherently dangerous and likely to yield unexpected / undefined results . What happens if you just append to a list and then append to the dataframe outside of the for loop ?

Series ( and DataFrames ) have a hist method for drawing histograms : #CODE

If it is a DatetimeIndex the apply won't work .
If you're doing timeseries work , I recommend using a DatetimeIndex . In this example , you can use a TimeGrouper to group by month ( which groups by year-month , like in a resample ): #CODE

I want to merge rows in csv files by matching the id with a given dictionary .
Literally , I should match : B.value , if ( B.Value == l.item() ) , I should insert the item from dict in the same row where we found the value in column B .
If you l were a DataFrame you could do a merge : #CODE
Merge on column A and the index of ` l_df ` : #CODE
What If I need to match with A column , how can I merge it ?

you can try append or concat . you are doing an assignment .
I'm worried that append and concat will be to slow . I may try reindex the smaller dataframe and work for there . It seems that if I reindex to match the st dataframe it works fine .

Merge two identical CSV from the same directory - Python
I have two data frames with the same structure in a CSV . I want to read both CSV and merge them to create one bigger data frame . In the directory there are only the two data frames .
concat takes a list or dict of series : #URL , so what you can do is make a list of the dataframes and concat them all together to make your big df : #CODE
Alternatively you can append : #CODE
The problem is that the example of my question is just a simplifcation . I really want to merge 30 csv in one csv .

This behaviour isn't guaranteed , it's much better to do assignment using loc or assigning the column directly : #CODE
I should add that you can do this by assigning to the original frame ( using loc / ix ) , however you can ( and should ) usually avoid this by vectorising your solutions rather than iterating over each row : #CODE

I am trying to append ` df1 [ ' B ']` based on ` df2 [ ' B ']` so that my desired output would be : #CODE
I believe you'll want a simple merge : #CODE

` listdir ` only returns the filename , not the complete path . To get the complete path you will need to join ` targetdir ` and ` file ` ( bad variable name as it masks the ` file ` type ) . Also , you will have to capture the result of ` .append ` as it returns a new object rather than appending in place . #CODE
I recommend using concat rather than append , as this way you don't make many intermediary frames : #CODE

One way is to merge twice . First with just the percentile column so you can backwards fill : #CODE
and then merge with this result : #CODE

the date_format argument does not apply to timedelta dtypes . Easist to simply convert them to strings first , e.g. `` df [ timedelta_field '] = df [ ' timedelta_fields '] .apply ( str )``
Time diff is an integer given in nanoseconds , not a date . I would recommend either pickling or hdf5 if you need to round-trip .

@USER OK I think I can see the issue with my code , ` df.ix [(( df.mi.value_counts() > 2 ) .index )] .dropna() ` seems to give you what you want . The line ` df.mi.value_counts() > 2 ` produces a series of boolean values , the index is the value of mi , we therefore want to use this index which will produce some NaN values as some counts are not > 2 , we therefore should drop these . Give it a try and let me know , I will have a think about how else this could be done as it is a bit clunky

Pandas ; tricky pivot table
I have a pandas dataframe that I need to reshape / pivot . How to do it just seems beyond me at the moment . The dataframe looks like this : #CODE
It looks like this requires more than one pivot or a combination of pivot and groupby , but I'm having no luck ...
+1 . As someone w / o any R experience , it's nice to see ` melt ` in action .
Great ! I hadnt come across the melt method yet , cheers .
As an alternative to ` melt ` , you can set a MultiIndex and chain the ` stack ` and ` unstack ` commands : #CODE

you can melt the categories : #CODE

You can do an apply on the LgRnk column : #CODE

I found this presentation , which is going about SQLALchemy and GeoAlchemy2 . And where it is mentioned that it support PostGIS Raster as well . It seems to be very interesting ! But using the documentation I don't see how I can apply this to Raster data
Alternatively , take a look here #URL to see if any of the functions return what you want for your numpy array and replace the query in get_raster accordingly . ( Possibly this #URL )

Using apply , as in ` df.x.apply ( tuple , axis=1 )` will work , but then I'd somehow need to iterate over the first level of the index . ` df.x = df.x.apply ( tuple , axis=1 )` sort-of works , but the index is still unchanged ( i.e. still has two levels ) .

Map string values in a Pandas Dataframe with integers
In Pandas ` DataFrame ` how to map strings in one column with integers . I have around 500 strings in the ` DataFrame ` and need to replace them with integers starting with ' 1 ' .
No idea of how to map these Requests with 1-500 . Similar question . python pandas replacing strings in datarame with numbers
So you want to parse the url request to search for the string before the .axd or .aspx and then convert to an int ? What will you do with this int , will this form an index or does it map to another column ?
@USER : There is no association with the url and the integer we are mapping . I just need to map them in order to make it more easy for other processing .
So what you could do is construct a temporary dataframe and merge this back to your existing dataframe : #CODE
Now merge this back to your original dataframe #CODE

Ignore the fact that there is a redundant column , I can drop that easily . The problem is that the first row ( for timestamp 2014-03-03 00:00 : 00 ) is missing and I'm not sure why . Is this a linefeed setting perhaps I need ?
Yeah I just tried that and saw success too . Unfortunately , I cannot upgrade my stack just yet to 0.13 , I have to find a 0.12 solution .

Thanks ! This works perfectly . I'm just curious , how does eval ( x [: -1 ] +D [ x [ -1 ]]) work ?
Yeah , sorry for that extra ` 0 ` . ` eval ` basically runs the ` string ` provided to it as ` python ` code .

food_pd = pd.DataFrame ([ item for sublist in map ( lambda a : a.keys() , a.values() , food.itervalues() ) for item in sublist ] , columns =[ " Food "]) .
use ` ( a.keys() , a.values() )` ( with brackets ) inside ` map ` . This returns a tuple of key value pairs , e.g. ` ( ' Apple ' , 50 )` .

FYI : ` site2.index.values ` produces this ( I've cut out the middle part for brevity ): #CODE

Or align one of them vertically first : #CODE

pandas : how to use both name and dataframe of groupby object in an apply
Now I also have a function ` func ` that takes ` id , group ` as input . I would like to apply ` func ` to each ` id , group ` in the groupby object . Currently I use a loop : #CODE
Is there any better ( faster ) way of doing this using an apply or similar ?
You can use ` apply ` on the groupby object to apply a function to each group . Since the function will need to accept the group as its argument , you can use the following : #CODE

I would use the data for analysis , such as finding matches with other datasets , and visualisation , such as visualising profiles individually , or some statistic of each profile on a map .

On this note - it surprises me that ` loc [ ' label '] ` retrieves ** all ** entries on subsequent levels , but ` iloc [ k ]` only returns ** one** .
doesn't work if N > len ( levels ) or K > len ( levels [ N ]) , works if you know these in advance ...

Using pandas built in functions you would have to apply ` notnull() ` over all series and then call a numpy function to the DataFrame anyway .
Edit : Apparently pandas has a ` notnull ` function for DataFrames in 0.13 , you can replace all ` ~ np.isnan ( df )` with ` df.notnull() ` if you wish .

See How to insert pandas dataframe via mysqldb into database ? .
Thanks so much for the answer and the link to " how to insert pandas to mysqldb " . I wasn't able to get it to work so I am using pymysql package . Do you know if pandas work with pymysql ?

And can you apply it to selected columns , instead of the whole dataframe ? - because If I have a text column like someones name , I can foresee it throwing an error ?

Panel truncate error : tuple object has no attribute ' year '

Reverse what you are doing and iterate over the smaller number of pubs . This will be order of magnitudes faster . Ix / loc is very fast when setting big ranges / slices . Using it for a small number of changed many times is inefficient . #CODE

Given an ` opi ` and a ` new_value ` , I want to replace the first occurrence of Nan in the row with the ` new_value ` .
Thanks for the response Jeff . I was really looking to only replace the first occurrence of a NaN in a ** specific ** row . I did see the ` df.fillna() ` in the documentation but it performs the replacement on ** all ** rows not one specific row .

Calculate median of the columns pepCN1 , pepCN2 and pepCN3 , respectively , for each gene
Calculate the median of the results of ( 1 )
The above data for gene ` CD38 ` would give two median values ( ` R1 ` and
another value for ` R3 ` based on the median of the two values in
In both of these cases , the statistics would then be calculated in the correct way . The fact that the statistics are calculated after the first round of " reducing " the data ( i.e. calculating the first median ( s ) ) is intended : the three data columns represent technical replicates , and should be handled individually before " merging " them together in a final statistical value .
The script will here reduce the two ` pepCN1 ` values to a single median , disregarding the fact that there are no values ( i.e. no data from replicates 2 and 3 ) with which to calculate statistics with from the other data columns . The script will function and give the correct ` CN ` value ( the median ) , but the statistics of standard deviation and coefficient of variation will be left out ( i.e. show as ` NaN `) .
In cases like this , I want the script to somehow see that reducing the data column to one value ( the first median ) is not the way to go . Essentially , I want it to skip calculating the first median ( here : ` R1 `) and just calculate statistics on the two ` pepCN1 ` rows . Is there a way to do this ? Thanks in advance !
prob easiest to do something like : `` df.dropna ( subset =[ ' pepCN2 ' , ' pepCN3 '] , how= ' all ' , axis=1 )`` apriori to drop them from your main calculation and then use the return value of the drop to actually do what you need .
don't start with an empty frame and try to append ; instead , append the results of the computation to a list , then do a `` pd.concat ( list_of_frames )`` at the end ( will be much faster too )
no , I just mean result part , e.g. when you are appending a frame to another frame ( and starting with an empty one ) . instead of starting with an empty one , append the resultant frame to a list , then when you are ready for the final frame , just do `` pd.concat ( list_of_frames )`` ; I think I fixed the issue you are seeing in upcoming 0.14 ( but not out yet ) .
no you cannot just group , you have to apply / transform .

The quickest way I know how to wrangle this thing into a long form dataframe is using ` stack ` and then ` reset_index ` : #CODE
Maybe my real question is " why isn't ` melt ` a DataFrame method ? This works pretty well : ` pd.melt ( wide_df.reset_index() , " subject ")` , but it feels like it would be easier to read as chained method calls that can be read in linear order .
not sure why their isn't a `` melt `` on DataFrame , could / should be .

Otherwise you could just filter the NaN ( I'm assuming that is what you want to do , difficult to tell without sample data and code ) and apply the map : ` df [ ' flag '] = filtered.notnull() .map ( ' N ')`

I want to pass a dictionary of items into Dataframe , but these items can have variable length arrays . What is the most elegant way to append np.nan to the arrays so that they match the sizes ?

@USER yes you can but it depends on how you want to filter the rows , you can do this by position indexing ` iloc ` and label ` loc ` , you can check the docs : #URL
" If you wanted to add frequency back to the original dataframe use transform to return an aligned index : " Does .transform ( ' count ') replace the original index , or did it never have an index until this ?

One way is to use an apply : #CODE
That's simple ! One criticism : doing format ( *x ) assumes there aren't other columns . One can replace it with format ( x [ ' Quantification '] , x [ ' Calibration ']) .

Pandas : merge miscellaneous keys into the " others " row
I am mainly interested in row " a " , " b " and " c " . I want to merge everything else into an " others " row like this #CODE
Finally , append this column to the second df : #CODE

and just cannot figure out how to drop only a single ' sublevel ' , e.g. ` df.col1.a `
Drop is a very flexible method , and there are quite a few ways to use it : #CODE
Drop a single column using a tuple : #CODE
In 0.14 , you'll also be able to pass regex of what to drop ...
There's also a way to drop the entire level of a index / column : #CODE

Thanks for the response so is this what you basically did : store the table make a dict , loop through the table for yes values and store it in the dictionary and use the rows as keys . Then generate the patterns using join .

Use a pivot table : #CODE
Then replace all the Nan's with zeros : #CODE
Then you might want to drop the multiindex column : #CODE

I need to combine different functions into one and use the apply function ( of those individual functions ) within the main function itself . My case is something more complex so i'll use a basic example for this .
So I'd get different columns with the sum , diff , product , quotient etc .
The above function would give me all values in a single column separated by commas . is there a way to apply each one in such a way that they come under different columns ??

Join dataframe with different indices
I want to join the data from df2 into df1 and for the missing indices I want to have NAN .
In a second step I want to replace the NaN with the last available data like this : #CODE
First merge #CODE
or specify the columns from both left and right hand side to merge with : #CODE

I want to create a line chart that shows the number of concurrent events happening throughout the day . Renaming time to ` start_time ` and adding a new column that computes the ` end_time ` is easy enough ( assuming that's the next step ) -- what I'm not quite sure I understand is how , afterwards , I can resample this data so I can chart concurrents .
A way to get the open events is to combine the start and end times , resample and cumsum : #CODE

Also possible to value_count with bins aka cut ( or qcut ) . Usually helps to describe overall problem - why you want to do something as well as what it is you're doing
count = len ( my_variable [ my_variable <- 1.01 ] )
This works just fine in > = 0.13 . Prior to 0.13 Float Indicies were not anything special . They have logic now to avoid the rounding / truncation of indexers to integers . in-other-works the values are looked up as is , not coerced at all ( for Float64Index ) . In fact this is the point of this type of index , to make a uniform indexing model with ` [ ] , ix , loc ` returning the same exact results .

You need to specify the agg function as len : #CODE

The reason why I use the ` correlation =[ ]` it is because I wish to populate the ` correlation ` with multiple correlation tables . That is why I use the ` append ` since this is in a loop .
concat the list into one larger frame , and use ` to_csv ` : #CODE
iterate over each DataFrame and use ` to_csv ` with append ( ` mode= ' a '`) . Something like : #CODE
I didn't explain myself correctly in my question . See the changes in my question . I know that correlation is a DataFrame but not if I append each dataframe in a loop .
@USER in that case you can either : concat to make one big DataFrame and use to_csv , or mode= ' a ' ( append ) whilst using to_csv .

Can also use df.filter ( regex= ' DUMMY ') rather than loc :)

Pandas will always try to align data based on the index . When you assign one DataFrame to another , it tries to match the data up based on the index , not the sequential row number .

Python Pandas - replace values with NAN in multiple columns based on mutliple dates ?
The tuple is a hierarchical index . In the tuple value 1 is a category , value 2 is an ID and value 3 is an event date . I want to use this event date as the maximum date -1 in the column and replace values after that date with ` NaN `
The dataframe could potentially contain 100000 columns . I understand how to replace the value is one column I think using a Boolean mask . I do not understand how to efficiently do this over multiple columns .
The ` apply ` returns a dataframe with True / False values ( the ` ` expression is evaluated for each column where ` x.name [ 2 ]` selects the third level of that column name ) , and the where replaces the False values with NaN .

Apply to each element in a Pandas dataframe
Is there anyway to apply this simple function to each element in the data frame ?

So I insert the results into a Dataframe and would like to get 3 columns ( Experiment , first draw , second draw ) How do I efficiently split it so I can plot the results on to the results Dataframe as a number ( ie ) #CODE

Below is my code . Is there a faster way to achieve this ? In reality ` len ( multi_df )` and ` len ( look_up_list )` are quite large so I need to optimise this line : ` [ multi_df.ix [ idx ] **2 for idx in look_up_list if idx in multi_df.index ]` .
I use 0.12.0 ( added to the question as well ) . ( I am trying to apply your solution and compare times . )
I'm not sure it is a bug , but it is definitely not right for this purpose , and I do think it is better to stick with the indexers , ` loc ` , ` iloc ` or ` ix ` when possible .
just try to select it via loc

Relabeling pivot tables
Lets say I have 2 pivot tables and I merge them , to make the case simple I will use binary digits . The code below works : #CODE
found the answer to the second problem ` pd.merge ( df , df2 , how= ' inner ')` It is the method ' how ' in merge .

python pandas strange error when concat the return values of ' apply '
The apply function is trying to return a whole dataframe in your case . You can't really do that . You can use apply to map a column or a row of a frame to a row / column or scalar . Printing is fine , but that's no surprise ...
What you are trying to do is basically a join . You want to join the tables with the names and the surnames of the employees with the rest of their details . What I would do , if I were you
is load the whole HR database in a dataframe and join it with the new_employees dataframe .

In pandas , you can use ` apply ` to do similar thing #CODE

Generally with multi-dtyped frames it depends on the construction of when it would work ( e.g. if you create it all at once , I think it will always work ) . Since you are creating it after ( via join ) it is dependent on the underlying numpy view creation mechanisms .
don't ever ever ever assign like that , use ` loc ` #CODE

Why does my Pandas join shift rows of the joined data ?
In Pandas , when I ` join ` , the joined data is misaligned with respect to the original DataFrame : #CODE
The final statement shifts the joined column " up " two rows , loosing the first two values of ILIPred2 and making the last 2 ' NaN ' . I expected the joined column to align with all of the others .
There's just no way to help you out without having some data to reproduce the problem . That said , ` join ` doesn't blindly put two dataframes next to each other . It behaves like an SQL join and aligns the data on the indices or specified columns . So it sounds like you need to clean up your data .
Also why are you calling ` shift ( 2 )` ?
They do , but I don't see what the problem is . You call ` shift ( 2 )` and I see two rows with ` NaN ` values . You need to be more explicit about what you're doing and what the problem is ( i.e. , what you expect ) .
@USER : I expected the dataframe added in the final line to align , but it is shifted " up " two rows .
And ` pd.concat ([ flu_test , pd.DataFrame ( exp ( flu_trend_2.predict ( flu_test )) , columns =[ " ILIPred2X "])] , axis=1 )` works as ( I ) expected ` join ` to work .
@USER I think you called ` pd.DataFrame ( exp ( flu_trend_2.predict ( flu_test )) , columns =[ ' ILIPred2 ']) .index ` having already done the ` join ` . Creating that seperate DF from scratch before the join gives me an index from 0 - 49 . Either way , you should probably be setting the ` Week ` column as the index and joining on that ( to ensure proper alignment )

Thanks everyone for the responses . I'm using pandas 0.13 . I tried ' sqlalchemy ' and it seems that I can do ` engine = create_engine ( ' oracle+ #URL )` . However , I have no luck after that as I tried writing to the database using to_sql() and it still doesn't work : ` df.to_sql ( ' TRORE.HW_TEST_PY ' , engine , flavor = ' oracle , if_exists = ' replace ')` and not ` engine.has_table ( ' MY_TABLE ' , ' MY_SCHEMA ')` either . So is it the case that you can create an engine in 0.13 but if you want to do anything with it you need 0.14 ?
Ok following what was suggested , I finally was able to use to_sql and it connected to Oracle and created a table succesfully . Thanks for the help ! I did find the run time a bit long as I was only inserting 5 rows and less than 10 columns . It probably took almost a minute . As I was only trying to insert into a specific schema , even though I specified it in the name ( eg . my_schema.table ) , it still created the table in the main schema of the database . All my string fields are showing ' Exluded ' after the table is created but the number fields are fine .

Count size of rolling intersection in pandas
To calculate the intersection between consecutive sets , make a shifted version ( I replace the NaN to an empty set for the next step ): #CODE
Combine both series and calculate the length of the intersection : #CODE
Another way to do the last step ( for sets ` ` means ` intersection `) : #CODE
The reason the rolling apply does not work is because 1 ) you provided it a GroupBy object and not a series , and 2 ) it only works with numerical values .
@USER Yes , thinking of the combine was the hard part :-) . For set difference , ` s - s2 ` does work , but intersection as ` s & s2 ` seems not to work for pandas serieses . I was looking for a way to apply a function on the elements of two serieses , but didn't find an obvious one .

I think you're looking for cut : #CODE
And you either just map over the labels : #CODE
Hi Guys , sorry from looking at the docs I have no idea what the CUT function does ?

There hopefully will be some support for parallel ` apply ` in 0.14 , see here : #URL

` random.choice ( range ( 0 , len ( x )))` is better written as ` np.random.randint ( 0 , len ( x ))`

This is wrong because the replace function only takes numbers from 0 to 59 as args for minutes .
also , there's no reason to use apply in this case .

Select and merge many statements like the following #CODE
Perform some sort of join
Caution : As shown above , there seems to be a speed benefit , particularly as ` N ` gets large . However , if ` N ` is too large , then forming ` idx = np.zeros ( N , dtype= ' bool ')` may not be feasible .

Apply upper and lower bounds to Pandas Dataframe
You could iterate over each column / bounds-list and apply the same filter . #CODE

@USER , this ` Namespace ` approach causes a lot of memory consumption for me , too . I've tried this with a DF with millions of rows and 6 columns ( taking up 2 GB of RAM ) , and the workers end up with about that much usage , too . What's more , access to the data that is fast ( < 1ms ) when profiled in non-multiprocessing becomes really slow for the worker in multiprocessing context . Even after the mem usage swells in worker , a single ` ns.df.loc [ ix ]` call can take several seconds . @USER and @USER , do you have any ideas about this ?

I apply a custom function on the DataFrame column ( convert_time ) #CODE
I encounterd the same question and I used a same way like you to solve it . ( apply a function to remove the unnecessary data )

It's very easy to interpolate NaN cells in a Pandas DataFrame : #CODE

consider using the series method hist rather than ... whichever one ( ? ) you are using . I suspect using values will work i.e. ` df [ df.TYPE == ' SU4 '] .GVW .values `
@USER ha ! It was more of a guess rather than an answer . marillion : IMO it's weird / unpythonic that hist cares about this and doesn't just iterate over it .
I assume the reason this does not work is because the matplotlib ` hist ` method tries to access the first ` 0 ` -index element of the input . But because the Series uses its integer index as label and not location , this gives a key error for a sliced Series ( as the first element will not have index ` 0 ` anymore )
And indeed , as @USER says , you can also use the pandas ` hist ` method : #CODE
I think I will be using the pandas ` hist ` method . Just feels more consistent . I still have troubles in mixing up matplotlib and pandas plotting options though . Thanks for all the help and comments !

you can also initialize the data frame with the list of dicts without calling append , ie df = pd.DataFrame ([ dict1 , dict2 ,... ]) but I think there is no performance difference .

` map ` might be better for this simple case
Only better in terms of simpler syntax : ` df.rain_column.map ( d )` , and perhaps faster performance-wise , it depends on data size and type for a dataframe with 100 rows then ` apply ` is marginally faster ( apply 228 us vs map 287us ) , for one with 10000 rows then map is 26 times faster ( map is 512 us vs apply 13 ms )
Alright , this makes a lot of sense , since apply is more general purpose than map .
There's not need to use a ` map ` anymore . Since version 0.15 , Pandas allows a categorical data type for its columns .

How can I use apply with pandas rolling_corr()

Also how do we do the same using some applymap or apply / map method of df ?.
In your ` for-loop ` , if you replace ` df [ ' Prediction '] =1 ` with ` passenger [ ' Prediction '] =1 ` , then you * would * be adding a new " column " to the ` passenger ` row but this would not affect ` df ` since ` passenger ` is a * copy * of a row of ` df ` , not a * view * into the underlying data in ` df ` . So again , this is just not the right way to accomplish your goal .
Ok . I understand this is not how Pandas works . I was trying to relate it's functionality with other data analysis tools . But in R , I could do a similar thing using apply method and then using the same if-else statement . It creates the right flag . Isn't pandas an add on of R dataframe with more better features ?. Or do you think we also have a similar way to do this in pandas using some applymap or apply method and if-else construct ?. This is the R code which creates the flag properly :
c$Flag -> apply ( c , 1 , function ( X ) {
TypeError : cannot compare a dtyped [ float64 ] array with a scalar of type [ bool ]
df [ ' Pclass '] == 1 & df.Age < 18 )) where it is taking df [ ' Pclass '] == 1 as Series ( array ) while df.Age < 18 is bool . But then how is it working for you ? I see you got the answer above for the same condition .

I'm new to Pandas and would like some insight from the pros . I need to perform various statistical analyses ( multiple regression , correlation etc ) on > 30 time series of financial securities ' daily Open , High , Low , Close prices . Each series has 500-1500 days of data . As each analysis looks at multiple securities , I'm wondering if it's preferable from an ease of use and efficiency perspective to store each time series in a separate df , each with date as the index , or to merge them all into a single df with a single date index , which would effectively be a 3d df . If the latter , any recommendations on how to structure it ?
Unless you are going to correlate everything with everything , my suggestion is to put this into separate dataframes and put them all in a dictionary , ie { " Timeseries1 " :d f1 , " Timeseries 2 " :d f2 ... } . Then , when you want to correlate some timeseries together , you can merge them and put suffixes in the columns of every different df to differentiate between them .

That doesn't work for me : I get ` np.sum ( test_mask ) + np.sum ( train_mask )` that's not the same as ` len ( quality )` .

Resample pandas dataframe and count instances
And I want to resample to daily frequency , it is quite easy : #CODE
However , I need to track how many instances are going into each day . Is there a good pythonic way of using resample to both perform the specified " how " operation AND track number of data points going into each mean value , e.g. yielding #CODE
This will return a DataFrame with a MultiIndex column : one level for ' A ' and another level for ' mean ' and ' count ' . To get a simple DataFrame like the desired output in your question , you can drop the extra level like ` df1.columns = df1.columns.droplevel ( 0 )` or , better , you can do your resampling on ` df [ ' A ']` instead of ` df ` .

Align the series ( essentially combine their index ) #CODE
Interpolate and eliminate the original values and show me the differential #CODE

Why not just use apply with these functions .
@USER obviously you have to elaborate . Anyway , like I commented usually it's better to use groupby methods e.g apply .

You could index into ` columns ` and use ` drop ` , e.g. ` df2 = df1.drop ( df1.columns [[ 2 , 3 ]] , axis=1 )` , but I wonder if there's a cleaner way .. hmm .
yep , awesome , ` df1.iloc [: , ~ic ( len ( df1.columns )) .isin ( np.r_ [ 0:3 , 4 ])]` works like a charm .

hhm , i keep getting series object has no attribute stack ?
Thanks DSM ! @USER that would be a sign that the apply did not create a dataframe . Are you sure there are lists in your series ?

Now comes the third part - Let's apply same reasoning . ` reindex ` is not defined in ` MyDataFrame ` . Where should we look next ? Class hierarchy , that means ` pandas.DataFrame ` . Now ` reindex ` is indeed defined by this class and it returns a pandas.DataFrame object ! . ( See this : #URL ) So , no wonder ` y ` is a ` pandas DataFrame ` .

I want do something like ` dat [ ' p + str ( type )'] = replace ` to get : #CODE

Apply method of DataFrame vs List Comprehension
I can do it with list comprehension but I would like to understand if I can do that with the apply method of DataFrame . Here is a toy model : #CODE
Probably it is just because of my shallow knowledge of pandas , but when I use the apply method I imagine the serie as a list or so end hence I do not have any idea on how to " put " the index attribute .

however , both getting the count of distinct values using ` len ( set ( index.get_level_values ( ... )))` and building the boolean vector afterwards by iterating over every row feels more like I'm fighting the framework to achieve something that seems like a simple task in a multiindex setup . Is there a better solution ?

There are a few problems here . ` drop ` by default operates on rows , not columns . Also , by default it returns a new object and doesn't modify the original . Are you sure the problem isn't due to it trying to print out the remaining columns , some of which also contain undisplayable Unicode ?
There are a few problems here . ` drop ` by default operates on rows , not columns . Also , by default it returns a new object and doesn't modify the original . The problem may be due to it trying to print out the remaining columns , some of which also contain undisplayable Unicode .

groupby after concat , column missing in the group mean
concat two dataframe , then groupby ' type ' and calculate the mean , columns of second df , i.e. d1~d10 , showing in the concat'ed dataframe but not in the grouped mean . i might have missed some pt ... pls indicate ... here the codes . #CODE

Pivot on log event category and group by day
I want to pivot on the log categories and group by day . So I want to get to something that looks like : #CODE

My data : I have a 3D spatial dataset , with data at uneven XYZ positions . The precise position of the data points is vital , so I can't resample to an even grid , which would be much easier to deal with . Each XYZ datapoint has an associated set of detail , including character , integer and float and boolean classes . I basically have a fairly disordered ' cloud ' of data .

Pandas : peculiar performance drop for inplace rename after dropna
The ` ( df1-df2 ) .dropna() ` call creates a slice of the dataframe . When you apply a new operation , this triggers a ` SettingWithCopy ` check because it could be a copy ( but often is not ) .

I get the following error : AttributeError : ( "' str ' object has no attribute ' loc '" , u'occurred at index 2010-12-31 00:00 : 00 ') . I have chacked the df type is correct : . I am using Python 2.7 ( Anaconda )

Perhaps creating a duplicate dataframe and doing an intersection of ` id1 ` and ` id2 ` will do ? Or should I consider a totally different idea ?

To assess my need I have done the following after stack . #CODE

These are wind speeds : once I have similar data from a number of different met stations I want to be able to form a DataFrame with the bins as the index and the columns as the freq . distrs .

I tried to see if this could be done when doing the pivot table but I don't think it can be .

I am following the docs but getting an error when using the ` if_exists : replace ` parameter . DB table is already created ( did so with django's syncdb command ) . The docs on ` pandas.to_sql() ` says : #CODE

The problem seems to be loading the dataframe into the hdfstore . I drastically reduced the size of my file , but kept one of my very wide columns ( 1259 characters ) . Whereas the size of the csv file is 878.6kb , the size of the hdfstore is 53 megs . Is pytables unable to handle very wide columns ? Is there a threshold above which I should truncate ?
The wide object columns are definitely the problem . My solution has been to truncate the object columns while reading them in . If I truncate to a width of 20 characters , the h5 file is only about twice as large as a csv file . However , if I truncate to 100 characters , the h5 file is about 6 times larger .
I include my code below as an answer , but if anyone has any idea how to reduce this size disparity without having to truncate so much text , I'd be grateful . #CODE

slices aren't conditional , you'll have to apply a filter .

When I try to replicate the example here , my violin plots ( with my data ) don't show the median and median , along with the 25th and 75th percentile , but the original example does .
Thanks - But didn't fix it . I wonder if the problem is the ` NaNs ` ? ( even though the box plot can figure out the median , and percentiles )

Python Pandas Pivot tables to pie chart
What is the best way to change all the non zero values in the following pivot table into a pie chart using percentages of the total ( sum of all the data in the pivot table ) ? #CODE
So basically I want to create a pie chart from the pivot table where values are present and use labels such as ' Unix A ' with a value of 3 / sum=% of everything in the table .
First stack your data ( so the different columns become an index level , see stack docs ) , and then select only the positive ones : #CODE

I'm trying to replace a row in a dataframe with the row of another dataframe only if they share a common column .
The result of the inner merge between both dataframes returns the correct rows , but I'm having trouble inserting them at the correct index in the first dataframe
You could use ` apply ` , there is probably a better way than this : #CODE
One way is to merge index values as well - pd.merge ( df1.reset_index() , df2.reset_index() , ... ) . Then run apply twice to return indices of df1 and df2 as appropriate . You could then select appropriate rows from the two and concatenate pd.concat ([ df1.ix [ ix1 ] , df2.ix [ ix2 ]] , ignore_index=True ) . This might make it faster

I am trying to finish up a homework assignment and to do so I need to use categorical variables in statsmodels ( due to a refusal to conform to using stata like everyone else ) . I have spent some time reading through documentation for both Patsy and Statsmodels and I can't quite figure out why this snippet of code isn't working . I have tried breaking them down and creating it with the patsy commands , but come up with the same error .

I am lost in a sea of ix , xs , MultiIndex , get_level_values and other Pandas .
Are either of those the best solution ? I felt that I should be able to do something with xs , or ix , but the former seems to only let you filter by a specific value , and the latter only indexes on the position in the series ?

That was an issue related to boolean operations on bool arrays with a single element . Thanks for pointing out the issue though , since I don't think the index should be preferred to the columns in query . FWIW , there will be an ambiguity if you name them the same , but you can get around it by using ` index ` to refer to the index . Just don't name any columns ` index ` ! :P

Just drop them : #CODE
this will drop all rows where there are at least two ` NaN `
then you could then drop where name is ` NaN ` : #CODE
@USER you mean what happens without threshold param or in some dynamic situation ? Without setting threshold then it will drop any rows containing NaN which would remove the Graham row which is not what you want , you would need to define the criteria for dropping rows if you were to use ` dropna `

I have a dataframe to start with , with that dataframe I want to apply some function . I want to repeat this many times and build / stack the reults from the operations in a new larger dataframe . I was thinking of doing this with a for loop . Here is a simplified example that I can not get to work : #CODE
You could just append : ` large_df= large_df.append ( df_new )`
It will be fastest to build all of the results first and concatenate once in the end . If you append one result at a time , the memory for the results has to be re-allocated each time .

Clearly the ` apply ` method isn't a disaster . But just seems weird that I couldn't figure out the syntax for doing this directly across all the columns with ` mul ` . Is there a more direct way to handle this ? If not , is there an intuitive reason the ` mul ` syntax shouldn't be enhanced to work this way ?
Jeff , thanks for the reply . And sorry for highlighting something that's already been flagged as an issue . For what it's worth , I think my ` apply ` column-wise solution , above , is easier to read than the ` pd.concat ... ` method in your ` [ 129 ]` . What do you think ?
using `` apply `` will be much slower .
I was also thinking about approaching this using ` Series.map ` ( i.e. treat the ` ccy ` level of the index as a ` Series ` and use ` map ` to convert it to a series of the foreign-exchange rates and then multiply the original dataframe by that series ( which would be like-indexed ) . But it's a bit annoying since a level of an index isn't a series ( without a little bit of effort ) .

also , What if I had two more columns ' C ' ' D ' but need to drop duplicates only from ' A ' ' B ' ?
cool ! What if I had two more columns ' C ' & ' D ' but need to drop duplicates only from ' A ' & ' B ' ?

Both options works very nice :) Do you know hot to apply your method to use secondary ` y_axis ` [ example ] ( #URL ) ?

You can either ** pad ** the data so that its a multiple of 60 , or you can use ** np.array_split ** to group data into chunks without raising the error . Here is an example : ` meanTime = [ np.mean ( i ) for i in np.array_split ( time , len ( time ) / 60 )]`

When I replace a row of a df , it causes an existing column of dtype=int to become float . I would like to keep it as int .
see this question , here : #URL This is currently a bug , best to use `` append `` to avoid the dtype change
Thanks Jeff . I had seen that link and did try append , but I'm replacing a row , not adding one , so I didn't think the enlargement problem applied . I did try append though to see if it would let me overwrite , but it oddly seems to give me duplicate index entries , e.g. I get two rows with date 2014-01-01 . I couldn't figure out how to get append to overwrite the existing row , which is what I'm trying to do .
Doing this kind of assignment is almost not the right way to go about this . You are much better off of creating the new data , selecting the data you want out of the existing frame , then `` concat `` together .
+1 . Your last method could also be written as ` df.loc [ testdate ] = map ( adddata.get , df.columns )` .
@USER in python 3 ` map ` returns a map object as opposed to a list , which causes ` TypeError : object of type ' map ' has no len() `

It is common to want to append the results of predictions to the dataset used to make the predictions , but the statsmodels ` predict ` function returns ( non-indexed ) results of a potentially different length than the dataset on which predictions are based .
@USER .nouri : So in ` 0.6.0.dev ` ` len ( train ) == len ( preds )` , regardless of missing values in ` train ` ? What is returned in ` press ` where there are missing values in ` train ` ?
Predict shouldn't drop any rows . Can you post a minimal working example where this happens ? Preserving the pandas index is on my radar and should be fixed in master soon .

pandas applying regex to replace values
I want to strip it down to just the numeric values .
I know I can loop through and apply regex #CODE
to each field then join the resulting list back together but is there a not loopy way ?
whats the best way to apply it to the column in the dataframe ? so I have df [ ' pricing '] do I just loop row by row ?

It is like a dataframe , but requires ` apply ` to generate a new structure ( either reduced or an actual DataFrame ) .
Doing something like : ` df.groupby ( ... ) .sum() ` is syntactic sugar for using ` apply ` . Functions which are naturally applicable to using this kind of sugar are enabled ; otherwise they will raise an error .
In particular you are accessing a ` group.index ` which can be but is not guaranteed to be a ` DatetimeIndex ` ( when time grouping ) . The ` freq ` attributes of a datetimeindex are inferred when required ( via ` inferred_freq `) .
You code is very confusing , you are ` grouping ` , then ` resampling ` ; ` resample ` does this for you , so you don't need the former step at all .
` resample ` is de-facto equivalent of a groupby-apply ( but has special handling for the time-domain ) .

Their are ways to apply some operations to multiple slabs of a n-dim ( esp . via new ` apply ` in 0.13.1 , see here .

As you can see , the ` AND ` operator drops every row in which at least one value equals ` -1 ` . On the other hand , the ` OR ` operator requires both values to be equal to ` -1 ` to drop them . I would expect exactly the opposite result . Could anyone explain this behavior , please ?
values to be equal to -1 to drop them .
That's right . Remember that you're writing the condition in terms of what you want to keep , not in terms of what you want to drop . For ` df1 ` : #CODE

I basically want to replace values only in column C . For example , replace ` [ 2 , 3 , -2 , -3 ]` with ` [ 1 , 1 , -1 , -1 ]` .

@USER , thanks ! ` df.filter ` works beautifully . Just one more thing , if I want to drop all ` date* ` columns , how should I do it ? I tried ` like ! = date ` but it does not work . If so , @USER ' s solution may be more flexible as I can use ` df.drop `
You can use regex expression inside ` filter ` , so you can use a regex expression that says to * not * match a certain string , eg ` df.filter ( regex= " ^ ( ?! date ) . *$ ")` . In the upcoming pandas , this functionality will also be provided in ` drop ` , so this will be easier .
Eg to drop all dates , you can provide a regex expression that says to not match a certain string : ` df.filter ( regex= " ^ ( ?! date ) . *$ ")`
In the upcoming pandas ( 0.14 ) , this functionality will also be provided in ` drop ` method , so this will be easier .

Using partial with groupby and apply in Pandas
I am having trouble using partial with groupby and apply in Pandas . Perhaps I am not using this right ? #CODE
There is no need to use ` functools.partial ` here , as you can provide arguments to the function inside the ` apply ` call .
If your function has as first argument the group ( so switch the order of the arguments ) , then the other arguments in ` apply ` are passed to the function and in this way you can specify the ` columnName ` in the apply : #CODE
The reason it does not work with partial , is that ` functools.wraps ` does not seem to work together with ` functools.partial ` ( ` wraps ` is used inside the apply ) .

Fetching top n records in pandas pivot , based on multiple criteria and plotting them with matplotlib
Usecase : Extending the pivot functionality of Pandas . Fetch top n records plot them against its own " Click % " ( s ) vs . no of records of that name #CODE
f [ ' len '] [ ' click '] / sum ( f [ ' len '] [ ' click ']
except that the top n logic should be embedded ( head ( n ) does not work with n depends on my data-set - I guess I need to use " apply " ? - and post this the Object , which is a "" object needs to be identified by matplotlib with its own labels ( top n " name " here )
Store the actual occurances ( len of names ) , and find the fraction of a " name " in population
( 1 ) is > f [ ' len '] [ ' click '] / sum ( f [ ' len '] [ ' click ']) and
Have you tried ( f.astype ( ' float ') / f.sum() ) .sort ([ ' len ' , ' sum '] , ascending=False ) [: 3 ]
@USER After I run " =d f1.pivot_table ( rows= ' name ' , aggfunc =[ len , np.sum ]) and then your command this gives " ValueError : Cannot sort by duplicate column len " .After I remove " len " this gives , " % str ( by ))

Append Level to Column Index in python pandas
I have several Dataframes with the same columns that I'd like to merge on their indices only . #CODE
I could also do a merge on two dataframes and use the suffixes parameter #CODE
I can then daisy chain my merges but the suffixes parameter only applies to columns that share a name . Once I've suffixed the first merge , the names will no longer be in common with the third dataframe .
I figured the solution would be to append a level to the column index of each dataframe with the relevant information necessary to distinguish those columns . Then I could run a pd.concat() and get something that looks like this : #CODE
Ultimately , I want a result to look a lot like the last concat statement . Is there a better way to get there ? Is there a better way to append a level to the column index ?
I you want a MultiIndex , you can do this directly in the ` concat ` function to get the same results , like : #CODE

I am trying to pivot this data frame so that I can identify the minimum and maximum trade date for a given contract , but this is not working intuitively : #CODE

What does work , however is nesting append and concatenate #CODE
Not great , but at least it doesn't care which things are 0d : ` np.concatenate ( map ( np.atleast_1d , [ tmp , id , freqs ]))`

Better way to compare all items in a dataframe and replace similar items with fuzzy matching python
I'm wondering if there's a better way to compare all items in a dataframe column to each other and replace those items if they have a high fuzzy set matching score . I ended up using combinations , but my feeling is that this is memory intensive and inefficient . My code is below .
possible duplicate of [ Python / Pandas : How to Normalize Names ] ( #URL )
Sorry , I may have mislabeled the title of my question . it's more centered on how I might be able to efficiently run through all items in a list ( comparing each item to each other ) and replace them .
So clustering ...? If you have 5 words which are close to each other how do you pick which is the true name ( i.e. the thing to replace them with ? )
Still not following how the cluster name is chosen . Also , you don't need to apply list in definition of cn1 since you're iterating through it .
basically , i choose the name that has the shortest length and i replace only if the fuzzy partial ratio is 100

I'm trying to create a new row for each group in a dataframe by copying the last row and then modifying some values . My approach is as follows , the concat step appears to be the bottleneck ( I tried append too ) . Any suggestions ? #CODE
Why you need to append a new row after a data frame ?
df.groupby ( ' group ') .last() should give you the last row of each group . You should be able to modify and concat once for all groups
Thanks to user1827356 , I sped it up by a factor of 100 by taking the operation out of the apply . For some reason first was dropping by Group column , so I used idxmax instead . #CODE

` str.contains ` can take regex . so you can use `' | ' .join ( words )` as the pattern ; to be safe map to ` re.escape ` as well : #CODE
`' | ' .join ( map ( re.escape , words ))` would be the search pattern : #CODE

Merge many tsv files by common key using Python pandas
I would like to perform an outer join on all of the files using the field " CHR :P OS : REF : ALT " to form one giant matrix . Example for two files : #CODE
I got the output above using the following code , but I am having trouble looping over the hundreds of * tsv files in the directory ( path / to / testN.vmat ) . how can I modify this into something that will merge all the individual * tsv files from a directory into a single tsv file ? #CODE
If you make the ' CHROM :P OS : REF : ALT ' the index you join multiple frames : #CODE
In someways it's more honest to think of this as a concat rather than a join : #CODE
interesting , what do you mean by " more honest to think of this as a concat " ? I always thought of concatenating as just stacking the two matrices on top of each other
@USER good question , I say because each " frame " is really just a column in the final result , rather than a more complex merge ( where several columns are coming from each frame ) ... subjective thing to say though !

One thought I had would be to " unstack " such that every row represented a single time period , and then iterate that way . Calling unstack on the df above gives this : #CODE
You could do this with a concat ( and itertools permuations ): #CODE
And you can just concat this result to the original frame ...

Then using the built-in apply function in pandas . But then , I realize none of my dates are in fact strings , so that does not really solve the problem . #CODE

Freq : H

So I wouldn't expect there to be any significant deterioration in speed . And you could always drop back to numpy operations on the numpy array ` pan.values ` if need be , though , hopefully , that would be unnecessary .

And now , we use the same ordering technique to order the rows of the pivot table ( instead of the rows created by groupby ) . #CODE

You can use ` get ` ( though it only works on the primary selection axis , e.g. in a frame its the columns ) , so you need to use the transpose to access . #CODE
no , you need to do this on the entire frame , selecting out individual elements is general VERY slow . Try doing what I suggegted AFTER a groupby ( e.g. in the apply function itself , which only has a time-index and only has the elements for which group you need )

For a dataframe below , I want to select only the UPCs that has total EQ above 1,000 . #CODE

not recommended to create an empty frame at all ( nor can you create it with specified types ) . Create the data you need , e.g. series or whatever , and just `` concat `` together or use the above method if you * really * need separate dtypes .

The columns would definitely be in an arbitrary order ( as dictionaries do not have orders ): is that a problem ? The other issue is that you don't want to append it to a list , you want to just set it as ` testdict [ list_instr_objects [ i ] .name ] = list_instr_objects [ i ] .data ` ( and it doesn't have to be a defaultdict , just a dictionary )

Second , use the ` shift ` and ` where ` commands to create the percentile3 column : #CODE
You could also use ` shift ` to accomplish this within a ` groupby ` / ` apply ` : #CODE
this is pretty awesome , never thought about shift . I did try to use groupby and apply with a vlookup and couldn't quite crack it . this works perfectly , but just for my own educational use , if there were some years skipped and a vlookup was needed , would you know what to do ?
I think the easiest would be to use the ` resample ` command to add the skipped years ( your percentile variable would be missing in those years ) , and then you could just use ` shift ` and ` where ` in exactly the same way .

Why is this printing the print statement in the function 4 times ? The way I would have thought it works is to group ` df ` into 3 dataframes ( for each machine ) and apply ` func ` on each of those grouped dataframes . But this is not what I observe ...

So , in this case ` .append ` should append only the first row from ` df2 ` to ` df1 ` . So , only if there is a new row in ` df2 ` that is not present in ` df1 ` ( based on name and ` col3 `) that column will be added / updated , else it wont be .
This almost seems like something that ` concat ` should do .
* Why * should it only append that row ? How , specifically , do you decide which rows you want ?
Sorry , I forgot to insert a row in df2 . Hope this is much clear . ` col3 ` is a category variable , so is there is new ` name ` in the category , you would append .
To put it another way : you want to concatenate the two frames and then drop duplicates ? Or if there's a new row involving Deb from Oklahoma -- say ` deb 1000 2000 OK ` -- do you want the original ` deb 500 625 OK ` row to be removed ?
Concat both dataframes , then drop duplicates
Using an outer join / merge , then drop duplicates
Concat then Drop
This method is possibly more memory intensive than an outer join because at one point you are holding ` df1 ` , ` df2 ` and the result of the concatination of both ` [ df1 , df2 ]` ( ` df3 `) in memory .
Outer join then Drop
Doing an ` outer ` join will make sure you get all entries from both dataframes , but ` df3 ` will be smaller than in the case where we use ` concat ` .

Use the ` to_frame() ` method . To get your desired output , you'll need to transpose the axes and reset the index afterwards . #CODE

Drop rows with all zeros in pandas data frame
In this example , we would like to drop the first 4 rows from the data frame .
One-liner . No transpose needed : #CODE
Aw , I like my transpose , it golfs better :)
Replace the zeros with nan and then drop the rows with all entries as nan .
After that replace nan with zeros . #CODE

instead , use the indexers ` loc / ix ` to reliably set values .

I am trying to append a date to a timeseries data .
I want to append to date series with ` 20140216 ` date . Currently I am using append command to update series . Code : #CODE
Can somebody suggest me any good ways to append date to timeseries data ?
Appending is usually best avoided ( it's usually preferable / more efficient to use concat or join ) .
@USER suggestion is for DataFrames or Series , pd.concat or join / merges : #URL It really depends what you're doing .

You can use join .
To join ` df2 ` and ` df1 ` , we need to set the column to join on , and transpose ` df1 ` so that we have stock name as index : #CODE
You can use loc directly to select some columns from your DataFrame ( to use @USER ' s example ): #CODE

You should post specific examples of what your expected output from your example series is . If you want to do any of these row wise , you may have to transpose the DataFrame first .
For example if three values in a row are all higher than the previous , and the fourth is lower , then I want to add the first three . Does that make sense . I want the capability that excels cell.offset property or array element provides . I need to be able to apply calculations on multiple values from a dataframe column , rather than a single value .

pandas do a " true " concat
It seems to me that pandas is returning a data frame with two series instead of one single serie when I do the concat . That gives me some troubles .. #CODE
What exactly are you up to ? Is ` df1.combine_first ( df2 )` what you want ? Pandas is doing a true ` concat ` here in the sense that it concatenates ` df1 ` and ` df2 ` without dropping any rows ...

Python Pandas- how to unstack a pivot table with two values with each value becoming a new column ?
When I unstack the pivot and reset the index two new columns ' level_0 ' and 0 are created . Level_0 contains the column names C and D and 0 contains the values . #CODE
Is it possible to unstack the frame so each value ( C , D ) appears in a separate column or do I have to split and concatenate the frame to achieve this ? Thanks .
You want to ` stack ` ( and not unstack ): #CODE
Although the ` unstack ` you used did a ' stack ' operation because you had no MultiIndex in the index axis ( only in the column axis ) .
Thanks Joris . Groupby works in this example , but in the real application I have non-unique dates so I am using pivot to put the dimensions in the horizontal axis and to group the dates so each date in the index is unique so they can be resampled . Stack I think will do the trick .
You can always give the ` level ` keyword to stack to specify which level of the columns should go to the index ( default it is the last one ) . BTW , I don't know the exact application , but you can also group on dates / datetimes in groupby to create unique dates .

Debug Pandas Dataframe Apply
I want to apply my_func ( a custom created function ) to each row of a dataframe . #CODE
You need to set ` axis=1 ` in ` apply ` : #CODE
If you're only passing in the row , you can just do ` df.apply ( my_func , axis=1 )` . Atlernatively , you can use the ` args ` kwarg or a ` lambda ` to pass in more arguments . ` apply ( my_funx , axis=1 , args =( par1 , par2 ))` or ` apply ( lambda row : my_func ( row , par1 , par2 ) , axis=1 )`

they are equivalent in this case ( and generally they are as well ; more validation on `` loc `` is the difference )

I now want to align the last event dates on the final row of the dataframe and set the index to 0 with each preceding row index -1 , -2 , -3 and so on . The periods no longer being absolute but relative to the event date .

Using Pandas , how do I drop the last row of each group ?
How can I drop the last row of each group instead ? The result would be : #CODE

In R , I have always been able to merge dataframes very easily as follows : #CODE
However , I have been unable to reconstruct this in pandas with merge ( how= " left , right , inner , outer ") .. For example : #CODE
The intersection should contain one row with the ` gene : HES4 ` . Is there some sort of string matching I need to turn on for this ? :
As far as I know the columns are labelled so that they should merge fine , I only want to merge by the ` Gene ` column and keep all test rows : #CODE
Hey Andy , would ` join ` be more appropriate in this instance ?

However , it is really slow . I looked around and found ` apply ` can do the work too : #CODE
It's even faster . With my larger dataset ( about 90k rows ) , the ` transform ` method takes about 44 secs on my computer , ` apply ` takes ~2 secs and the ` for loop ` takes only ~1 secs . I need to work on much larger dataset so even the time difference between the ` apply ` and ` for loop ` makes a difference to me . However , the ` for loop ` looks ugly and may not be easily applied if I need to create other group-based variables .
I think there should be a better API to append a level to a MultiIndex ... don't think there is ( yet ) .

it doesn't work . On the other hand , ` pd.concat ` does accept multiple dataframes , but it doesn't take the argument ` on ` to specify the columns on which to join .
Why was ` merge ` constrained to only work with two dataframes ? Isn't joining multiple tables a common operation when working with relational data ?
You can eg do ` df1.merge ( df2 , on= ' some_column ') .merge ( df3 , on= ' some_column ')` . Also in SQL you have to do multiple joins to join multiple tables .

I could modify the JSON document , so that " records " reads the strings in properly . But is there a way to modify " records " directly , to somehow strip the single-element arrays into the elements themselves ?

One efficient hack is to replace the levels ( of the MultiIndex ) inplace , sort , then put them back : #CODE
@USER If you do [ 13 ] on level=1 , this won't work ( it works precisely because it's a different metric on each level ) . If you have to do two on level=0 then your going to have to do a little more work ( something like : ` df.groupby ( level=0 , as_index=False ) .agg ( { ' D ' : ' min ' , ' E ' : ' median ' } ) .sort ([ ' D ' , ' E ']) .index `)
Thanks ! Also I think in your comment with ` agg ` you shouldn't use single quotes around the function names ` min ` and ` median ` . Am I wrong ?
@USER actually using quotes is a trick to use the pandas version of min / median rather than defined elsewhere . You can do both ( I think pandas does some trickery to use it's version of min rather than python's built-in ! ) but better to use the quotes :)

Above " apply_by_multiprocessing " can execute Pandas Dataframe apply in parallel . But when I make it to Celery task , It raised AssertionError : ' Worker ' object has no attribute ' _config ' . #CODE

periodindex not well supported at lower than day freq - just use datetime index - this usage might be a bug

Pandas - The difference between join and merge
I want to merge them , so I try something like this : #CODE
But I'm trying to use the join method , which I've been lead to believe is pretty similar . #CODE
I always use ` join ` on indices : #CODE
First , ` join ` expects a single , common column . However , that doesn't seem to help here . The follow should , IMO , work . Unfortunately , it does not : #CODE
Specifying an outer join is even more confusing : #CODE
My thoughts are that ` merge ` is for columns and ` join ` is for indices .

I was using ` df.to_sql ( con=con_mysql , name= ' testdata ' , if_exists= ' replace ' , flavor= ' mysql ')` to export a data frame into mysql . However , I discovered that the columns with long string content ( such as url ) is truncated to 63 digits . I received the following warning from ipython notebook when I exported :

@USER , I figured out what the problem is . Basically STATA has a limit for strings up to str244 . So when I save csv from STATA , some characters were cut in half , the encoding of which cannot be recognized by python . I don't have a solution for it yet . I did some quite tedious editing in STATA which I certainly do not want to replicate in python .
@USER , maybe this is too late already , but Stata prints out the script version of any manual editing you do ( something like ` replace var2 = 28 in 4 `) . If you have the log of your manual edit saved somewhere , it shouldn't be difficult to automatically convert ` replace var2 = 28 in 4 ` and the like into pandas code - just parse this string for the arguments to your custom pandas replacement function .
@USER : it depends on how many different command you used . If your script is just ` replace ` done hundreds of times , then it shouldn't be so difficult to do , since you just need to parse a couple of arguments ( I had to do something similar a while ago ) . Maybe post it as a different question and see if people respond ?

Pandas boxplot order not correctly sorted
That the boxplot is sorted alphabetically by ' Character ' , but that the ' Numbers ' are not always sorted . So for instance the order from left to right could be
Do I need to sort the dataframe before doing boxplot or can I do something when calling the boxplot _ _ ?
You can apply ` strip ` to each element in a column this way : #CODE
Like the idea , but I'm missing the ` inplace=True ` like the ` replace ` method has ( #URL ) - otherwise it will just return a Series and not a DataFrame
The problem is that the Series don't do boxplot - as I recall it .

You can use ` shift ` to nudge your data up or down a row . So ` df.shift ` will have an ` NaN ` in the first row and then otherwise have you data nudged down one row .
will get you a column of ` True ` and ` False ` . If you really prefer ` 1 `' s and ` 0 `' s replace the last line with : #CODE

But a groupby operation doesn't actually return a DataFrame sorted by group . The ` .head() ` method is a little misleading here -- it's just a convenience feature to let you re-examine the object ( in this case , ` df `) that you grouped . The result of ` groupby ` is separate kind of object , a ` GroupBy ` object . You must ` apply ` , ` transform ` , or ` filter ` to get back to a DataFrame or Series .

I'm very new to projections and struggling to convert a map dataset from OSGB36 ( Eastings and Northings ) to WGS84 ( lat / longs ) .
I've used geopandas to import a shapefile . Now I want to export it as GeoJSON so I can then use Folium to map it on leaflet.js .

mean = sum ([ float ( row [ 1 ]) for row in data ]) / len ( data )
I was running into the same kind of issue and found that the resample method can be used to do that just using the parameter 3M ( for 3 months ) .
If you have a dataframe with index as pandas datetime object then all you need to do is ask to resample on 3 months basis . #CODE

Why am I getting an empty row in my dataframe after using pandas apply ?
I'm fairly new to Python and Pandas and trying to figure out how to do a simple split-join-apply . The problem I am having is that I am getting an blank row at the top of all the dataframes I'm getting back from Pandas ' apply function and I'm not sure why . Can anyone explain ?
The groupby / apply operation returns is a new DataFrame , with a named index . The name corresponds to the column name by which the original DataFrame was grouped .
Ah , okay , so the reason it is there is that the groupby / apply operation _replaces_ the usual indexing with indexing by the grouping value ? Is that correct ?
Yes , the result of the groupby / apply is a new DataFrame , with a named index , and the name corresponds to the column name by which the original DataFrame was grouped .

As @USER Hayden points out that ` normalize ` sets the times to 0 so that you can compare directly to a Timestamp with 0 for the time . #CODE
Fantastic , Thank you @USER ! I was reading about normalize but couldnt see how to use it in this instance . I've not read anything about the indexer_between_time method before . I'll do some research . Thanks again !

This way reduce the timings by half on my side . By problem is I have values for hours initialy in 24 columns and date in first one . So I have to be more efficient as I can because after , I have to stach columns and do an apply to do a relativedelta on the datetime ( which is take actually 2 min more ) ...
I have done one for this typical problem { too long dataframe apply row functions} . I am very suprising if I am the first person to deal with this problem .

I think the cleanest way is to check all columns against the first column using eq : #CODE

too long dataframe apply row functions
Read a csv file with time value in columns and get a dataframe with 1 columns values and datetime index most efficiently as possible . I do a read_csv , then a stack and the following function but it's more time and memory consumming .
The command you're looking for is ` stack ` . More information in the documentation : #URL
Basically , you're looking at using stack and then converting ` Hxx ` to ` #URL which should be straightforward .

The first level of the MultiIndex is a string corresponding to a userID . Now , most of the userIDs are 13 characters long , but some of them are 15 characters long . When I append a record containing the long userID , pytables raises an error because it is expecting a 13 characters field . #CODE
Append #CODE

What I would like to do is getting time differences . For each column , I would like to replace the date and time by the time since the last post happened . For example , if my first post happened at 8 pm , and my second post at 8 45 , I want to get ' 45 minutes ' in my first column . Ideally , my output is like this ( the difference is calculated in seconds ) #CODE

How to join two dataframes on datetime index autofill non matched rows with nan
As you don't have a common column you need to specify to use the indices of both dataframes and that you want to perform an ' outer ' merge : #CODE

Pandas merge not working
However , when I try to merge the two using : #CODE
Can you show part of the dataframe whith rows that should merge ? Because in the ` .head() ` you show now there aren't any .
Do you have common values for your columns to be merged ? If not then the default type of merge is ' inner ' and so this could explain why you have no rows , if you want to combine all data from both then do ` result = pandas.merge ( step1_merge , transp_merged , on =[ u'type_str ' , ' GRID '] , how= ' outer ')`
I am having a similar problem . There are several points that do not join / merge . However , when searched for in excel they are found .

Thanks for keeping this . TBH I think this is better , I know which one I'd prefer to read ( stack / unstack are indecipherable )
I prefer @USER ' s way ( more robust ) ! Which levels to stack / unstack is not very readable ...

I've made it work with ` records.groupby ( ' product_name ') .filter ( lambda x : len ( x [ ' url ']) == 1 )` . Note that simply using ` len ( x )` doesn't work . With a dataframe with more than two columns ( which is probably most of the real-life dataframes ) , one has to specify a column for x : any column , except the one to group by with . Also , this code initially didn't work for me because my index on the dataframe was not unique . I'm not sure why this should interfere with the function of filtering , but it did . After reindexing the dataframe , I finally got it to work .

Pandas Left Outer Join results in table larger than left table
From what I understand about a left outer join , the resulting table should never have more rows than the left table ... Please let me know if this is wrong ...
Therefore I merge them as such : #CODE
To avoid this behaviour drop the duplicates in df2 : #CODE
@USER yup , drop the duplicates , edited answer to reflect that .

Rather than do a apply here , I would probably check each column for whether it's numeric with a simple list comprehension and separate these paths and then concat them back . This will be more efficient for larger frames . #CODE
Now you can concat these and potentially reindex : #CODE

I have tried " where " , " isin " , " join " , " merge " , and I am not able to replicate this in Pandas .
I basically have two columns ( x y ) with values one through 10 . I then want to do a self join with a specific criteria shown below . #CODE
I can't think any simple method that can do the join less than O ( N**2 ) . Are you sure the SQL don't check every pair values ? Maybe sort the ` x ` column , and for every value ` v ` in ` x ` , binary search ` v+4 ` , but it's a little complicated .

You can use the diff groupby method directly : #CODE
which does the same thing as using shift and subtract : #CODE
Ah , I was talking about calling a method ( ` fillna `) on an object wrapped by a curly bracket , which I am usually not a fan of . I prefer using ` apply ` as suggested by Karl .
@USER true , it's not very pythonic ... you can always do the fillna on the result on the next line . shame shift doesn't have a default value arg . : S
@USER remembered the diff method does this , much nicer !
Or you could use shift within a ` groupby ` / ` apply ` : #CODE
Thanks . I like the ` apply ` solution !

Merge existing dataframe into fixed size new dataframe
Then I want merge these kinds of table into new dataframe
How could I merge them in that way ?

using zip() on the shortest amount of data , I get the following error ''' % ( len ( value ) , len ( cur_axis )))

Is there a way to combine ( append ) h5 files with the same table format more efficiently ? ( SQL Union like functionality ) .I tried this SO but couldn't get it to append tables .
if not , is splitting on rows a reasonable thing to do when most of the queries are select from where for all the columns ? i am thinking about writing a map / combine function that will look in all the parts of a table for select from where queries . Pandas select_as_multiple() function does this for splitting based on columns .
This brings me to the second part of my questions , what if i don't merge the files and use the pre-merge files to be processed in map / combine way , could that be a reasonable way to approach this ? how should i think about implementing this ?
I do a very similar , split-process-combine method , using multiple processes to create intermediate files , then use a single process to merge the resulting files . Here are some tips to get better performance :
Turn off indexing while you are writing the files by passing ` index=False ` , see here for the docs . I believe that ` PyTables ` incrementally updates the index , which in this case is completely unecessary ( as you are going to merge them afterwards ) . Index only the final file . This should speed up the writing quite a bit .
I merge the files in the indexed order , basically by reading a subset of the pre-merge files into memory ( a constant number to use only a constant amount of memory ) , then append them one-by-one to the final file . ( not 100% sure this makes a difference but seems to work well ) .

Pandas Groupby apply function to count values greater than zero
Pandas Groupby apply function to count values greater than zero

you could complete the dict by replace missing values to some invalid value like -1
As you have probably discovered , ` DataFrame ( mydict )` is valid code . You could simply take the transpose ( ` .T `) to get your desired result .

Merge specific column into dataframe

If I replace values of two rows using ` .loc ` indexer in the following way : #CODE
However , if I replace values of all rows in the following way : #CODE

Getting a error numpy.ndarray ' object has no attribute ' append ' in python
' numpy.ndarray ' object has no attribute ' append ' when i run the code of groupby in python The command is

Rather than 2 you can insert first the Series ( in this example it happens to be the same ): #CODE
Brilliant , this might sound really stupid but I never knew about the lt function ( or the corresponding gt , le , ge , etc ) . Thank you so much for the help

Here's the full stack : #CODE

You might want consider using apply : #CODE
create a column ` tsdiff ` that has the diffs between consecutive times ( using ` shift `)
Arbitrariy assign things 20s to group 0 , else to group 1 . This could also be more arbitrary . if the diff from previous is 0 BUT the total diff ( from first ) is > 50 make in group 2 . #CODE
Groupem ( can also use an apply here ) #CODE

@USER the resample worth w / o the sort , not the Grouper .... hmm
Sorry , but in fact it still not working : I used pivot table instead of this method but presently I still need to resolve the above issue.Because there is now an additional column with dates all at 1970-01-01 .

How do I convert a pandas pivot table to a dataframe
I want to use a pivot table to summarise a dataset and then be able to access the information in the pivot table as if it were a DataFrame .
Now I want to summarise using a pivot table , simply counting the number of patients in each hospital : #CODE
First up , this isn't a pivot table job , it's a ` groupby ` job .
Pivot tables are for re-shaping your data when you haven't set an index ( see this doc article ) , ` stack ` and ` unstack ` are for reshaping when you have set an index , and ` groupby ` is for aggregating ( which is what this is ) and split-apply-combine operations .
To select certain rows in a multi-index , I usually use ` ix ` as follows : #CODE
Thanks very much for the reply . That's very helpful . But I don't quite understand the difference between a pivot table and group by . When should each be used ?

If you exceed this , the query will work , but it will drop that variable and do a reindex

replace ` timestamp ` with ` date ` .

unstack multiindex dataframe to flat data frame in pandas

the " more general " solution does not solve some problems : 1 . minor tick labels may be duplicates , but they are also blanked . 2 . it returns a map which causes a " TypeError : object of type ' map ' has no len() " . Solved it by changing to " return list ( map ( ' \n ' .join , np.fliplr ( Blank_ar )))" 3 . It is impossible to rotate labels with that solution . I will stick to the 2 level index solution , it is very nice . thank you

Average value for each user from pivot table ( dataframe )
You probably want to use the resample method
you can convert your timestamp column to datetime type before using the pivot table . See edit

Thanks @USER . another quick question is how do i resample between two given timestamps to arrive at daily rainfall.For example I would like to calculate daily rainfall by summing all not NaN values between 7:30 given day to 7:30 next day.NOTE there could be more than 24 values in the timeseries as the frequency is not hourly .

time and memory lambda apply
Apply is only useful when you cannot vectorize .

Merge DataFrames with NaN

Then I tried to concat the two dataset first : #CODE

Thanks . So I am not wrong that it the data should be the same . Is there anything that can confuse the use of loc ? Works on my computer as well btw .

see the docs #URL You need to specify how you want the resample done eg sum or mean .. you can specify by field ... The default is mean ,, not exactly sure why you are getting sum instead .

How to get all fields for only a specic user_id from a pivot dataframe indexed by two fields ' timestamps ' and ' user_id ' ?

Do you mean after the fact , or do you want / need to truncate them before they're even stored in the frame ?
Before even stored would be great . Would an apply be the fastest way for after the fact ?
Writing an apply right now . Is there a way to get the width of a column of type object ?

Then resample : #CODE

I am using pandas to join several huge csv files using HDFStore . I'm merging all the other tables to a base table , ` base ` . Right now I create a new table in the HDFStore for the output of each merge , which I call ` temp ` . Then I delete the old base table . Finally , I copy ` temp ` to ` base ` and start the process over again on the next table I need to join .
Luke , I'm curious why you wouldn't just append additional csv's directly to the base table , rather than have the intermediate ( slow ) step of creating a new table ?

Does the 2nd line merge A1 into B , or do I need to say ` B = pd.merge ( B , A1 , on= ' key1 ')`
@USER See the docs on [ merge ] ( #URL ) . ` pd.merge() ` takes a ` suffix ` argument tuple that will be appended to the column names if they are the same , like ` val1_x ` and ` val1_y ` . You may , of course , choose to rename the columns from ` A ` before merging . Also , setting ` how= ' left '` in merge will cause an SQL-style left-join ; i.e. , the values in ` B ` will persist whether they exist in ` A ` or not . My advice is to read the docs and play around with the above two commands until you find what you want .

I have data in long format and am trying to reshape to wide , but there doesn't seem to be a straightforward way to do this using melt / stack / unstack : #CODE
hmm could be a pivot then
A simple pivot might be sufficient for your needs but this is what I did to reproduce your desired output : #CODE
Yeah , that's basically what I ended up doing , although you also have to separate out the columns that don't change , like height , drop duplicates and then concat those later .

In Pandas , I can use ` df.dropna() ` to drop any ` NaN ` entries . Is there anything similar in Pandas to drop non-finite ( e.g. ` Inf `) entries ?
You can use ` .dropna() ` after a ` DF [ DF == np.inf ]= np.nan ` , ( unless you still want to keep the ` NAN ` s and only drop the ` inf ` s )

Just apply ` to_datetime ` : #CODE
you can pass the format directly into the to_datetime , no need for apply

How to get sum of same field for ALL user_id from a pivot dataframe indexed by two fields ' timestamps ' and ' user_id ' ?

i think the cut function is what I am looking for
You can simply ` map ` the list , like so : #CODE
or if you're using pandas ( as your question suggests ) use ` astype ` method or ` map ` : #CODE

Cannot interpolate on Pandas Dataframes
I am trying to interpolate missing values on my Pandas Dataframe : #CODE
If you still want to use interpolate without upgrading pandas use can do this : #CODE

is_retweet 506 non-null bool
is_truncated 506 non-null bool
dtypes : bool ( 2 ) , float64 ( 2 ) , int64 ( 11 ) , object ( 16 )`
I have fixed that problem now thanks , having a problem now with trying to use samples of the JSON data , rather than all of it . I have ` z = pa.read_json ( ' C :\ Users\Admin\JSON files\ file1.JSON ')` as before , but I wish to use sample of 100 / 200 of the retweet_count so I can find mean / max / etc of diff size samples . I've tried ` keys = z.retweet_count.keys()

Dataframe exportation to excel ( shift of rows )

I want to cut the first down to only the matches , so #CODE
You can just ( inner ) merge : #CODE

@USER I created an issue about this [ here ] ( #URL ) . I'd not realised this wouldn't work in some cases , interesting . Could you resample first ( to get consistent freq ) and then rolling_ in reverse ?
I think that should work , but only if you resample in the frequency that you want to aggregate on . If you have sparse data , this could dramatically increase your set size . For instance , if you have a few days worth of hourly observations collected in different years , you'd have to add in thousands of null values . It would be nice if there was a more native support that truly behaved the same way that rolling_* does , just in the other direction .

I look for drop / add the hour which correspond to a daylight . I process as follow but I obtain only shift on datetime since also want a shift for values .
You can drop the timezone and subtract the first index . But what is the meaning of ` value ` ?
If the daily peak of your power consumption is at 19h in summer time it's the same in winter time . There is no shift ! I don't understand your method ? : s

A temporary solution is to change the above described function in nanops.py , and replace x=float ( x ) to x= x.astype ( np.complex ) , so the part of the code now is : #CODE
Presumably you want to real part of the complex numbers ( numpy.real ) , unfortunately this doesn't play super nicely with pandas Series / DataFrame , so you need to apply back the indexes : #CODE

It sounds like you could be after a resample : #CODE

Pandas resample with " seasonal " frequency
Using pandas , I am trying to resample daily data into seasons and depending on the start of the daily index , I seem to get different results . Basically if the start is in Q2 or Q4 , resample works as expected , but not if index starts in Q1 or Q3 . Note that the same does not happen with the end date as resample seems to behave correctly there .

However , since I presently have some issue with this version ( no possibility to really calculate the mean using resample with DataFrame ) I would like to know how I can quickly upgrade my version to 0.14 .

a more usual way to do this would be to resample : #CODE
With regards to accessing the rows , you should use loc , though I like to access with a Timestamp : #CODE

Pandas resample with dataframe returns just two fields instead of four
You can do a resample of a groupby , provided you have a DatetimeIndex : #CODE
@USER yeah , that could definitely break the resample ( which uses mean under the hood ) , you'll notice my blank values are NaN , I strongly urge you to use those ( float dtype rather than object ) . see : #URL
@USER oh , I see what you mean ... what do you expect the output to be with a resample of strings ( e.g. what is the mean of a list of strings ) ? pandas can't resample non-numerics with a numeric agg like mean , so you have to pass it a different agg functions ( which can handle strings ) .

If i want to convert column B to int values and drop values that can't be converted I have to do : #CODE

I want to cut the first down to only the matches , so #CODE
if you want to remove matches , just drop ` index-1 ` from ` df1 ` and ` index-2 ` from ` df2 ` . #CODE
Doesn't seem to be doing any actual comparing . I have it printing after the merge and it's just df1 . Also , for dropping the index , is that ` df1.drop ( df1 [ ' index-1 '])` ?
Would the merge be something like ` pd.merge ( df1 , df2 , on =[ ' Qty ' , ' Price '] , how= ' inner ' , suffixes =( ' -1 ' , ' -2 '))` ?

I have messed around with groupby , and calculated actual epoch values at the start and end of 5 minute time blocks for each of the observation periods , in a separate dataframe . But I can't see how to bring these to bear in a function that applies per observation period in the original dataframe above , where there are multiple values for each observational period . I suspect the answer lies in a better comprehension of groupby and the apply method , but I'm having trouble getting this off the ground . ( Also , maybe I am not using the right search terms , but I'm not finding much on this posted to the forum already . I'm only able to find info on working with timeseries ) . Two options I've considered but can't figure out how to program :
This ' normalized ' time field we can map to the 5-minute interval : #CODE
[ Edit ] Update - I think you can use the apply function to subtract the right min time #CODE
Okay , this is making some progress . But each of my observation periods starts at a different time ( the dates & times are spread out over 3 months ) . So , I can't just do step 1 across all my ' epoch ' data - I need to apply it separately for each ' observation ' group . Similarly , I'd need to do step 2 by ' observation ' group . So I think the plan of action will be to use the sort of approach you've provided , but apply it via groupby ?
I've updated the answer . Based on the dataframe you created you can get the right min value and apply to the epoch column

A few caveats apply :

Sorry it still works fine for me , you can drop the last row by just doing ` df1_clean = df1_clean [: -1 ]`
No that still remains but your df1_clean prior to the pivot does not have the erroneous date you mentioned

You can use shift to align the previous row with current row and then you can do your operation . #CODE

In ` Pandas ` we can drop duplicates by using ` dataframe.drop_duplicates() ` which keeps the first row of the duplicate data by default . If ` keep_last = True ` , the last row is kept .
How can we keep any random row and drop the duplicate rows using pandas ` drop_duplicate ` ?

If you want to drop duplicates based on specific columns , you can use the ` cols ` argument in ` drop_duplicates ` : #CODE

Edit : I tried assigning using both loc and iloc but it doesn't seem to work :
loc : #CODE

I like to group a DataFrame according to their date and get the mean of each group then merge them into a single DataFrame . #CODE
sub is somekind of format of DataFrame , but how can I merge then together to a single DataFrame df2 ? #CODE

I have two dataframes that i'm trying to merge on key ' Time ' . The first dataframe ( events ) has a set of events with time as below ( indexed over Date ): #CODE
I'm trying to merge both these table to get the count next to events table using the below pandas merge function however it throwing a unhashable instance error - can someone advise ? #CODE
I suspect the problem is with you left_on , right_on . If this was a 1:1 merge ( let's say ) , then the merge would only ever happen if the combination of ' Time ' and ' Event ' in the Events dataframe , exactly matches the combination of ' Time ' and ' count ' in the other data frame . From what I see above , this is never going to be the case . It looks to me like you are just wanting to merge on ' Time ' in both sets . If this is not right , try putting the lef_on syntax in tuples . Lists are not hashable as they are mutable .
I'm prettys sure you don't want to merge on those variables , you just want to merge on Time . Is that not right ?
Tried to just merge on Time but getting the same error :

or provide eg a function to replace all periods with underscores : #CODE

this will append your csv name to your destination path correctly

You'll usually see the SettingWithCopy warning if you use consecutive [ ] in your code , and the are best combined into one [ ] e.g. using loc : #CODE
Aside : as mentioned in comments resample offers ` how= ' ohlc '` , so you may be best of doing this , padding , filling and then joining with the resampled volumes .

Given that the time periods in the data are non-uniform and contain overlap , there are a few approaches possible . If you're alright with linearly averaging entries and exits , you can take each time period and calculate how many entries and exits occur per hour on average , then , given an hour , you could iterate through all data points , find how much a data point overlaps with that hour ( i.e. 15 minutes or the whole hour ) , and apply the data point's average entries / exits per hour modified by the percentage of overlap to an accumulator .

data is DataFrame and has been obtained by quite complex algorithm , including merging parts of different Series by means of . ix [ ... ]

I am doing a transformation on a variable from a pandas dataframe and then I would like to replace the column with my new values . The problem seems to be that after the transformation , the length of the array is not the same as the length of my dataframe's index . I don't think that is true though . #CODE
When I check the length , these lengths seem to disagree . The len ( array ) says it is 2 but when I call the stats.boxcox it says it is 50000 . What is going on here ? #CODE
Did you check ? Print out ` len ( df )` and ` len ( stats.boxcox ( df.variable ))` .

I have 2 data frames created by pivot tables #CODE

Struggling with pandas ' rolling and shifting concept . There are many good suggestions including in this forum but I failed miserably to apply these to my scenario .
Now I use traditional looping over the time series but ugh , it took like 8 hours to iterate over 150,000 rows which is about 3 days of data for all tickers . Got 2 months data to process it probably won't finish after I come back from a sabbatical , not mentioning risk of electricity cut off after which I'd have to start over again this time no sabbatical while waiting .
Thanks to Jeff's tips , after swapping and sorting ix level I am able to get the 12sma right with rolling_mean() and with a effort managed to insert the first 12ema value copied from 12sma at the same timestamp : #CODE
So , I tried to fill 12ema column from Feb 5 , 15:45 and onward . I tried apply() with a function but shift gave an error : #CODE

If you manage to get the data in ine single Dataframe it should be possible using ' apply ' .
and then you can group by the ' i ' column and apply an arbitrary function to the subgroup . #CODE

from which I want to retrieve all the users ( userXY ) from one of the experiments ( exp0Z ) and append them into a single big DataFrame . I have tried ` store.get ( ' exp03 ')` obtaining the following error : #CODE
I can retrieve a single user by calling ` store.get ( ' exp03 / user01 ')` , so I guess it is possible to iterate the ` store.keys() ` and append manually the retrieved dataframes , but I wonder if it is possible to do so in a single call to ` store.get() ` or other similar method .

I am working on a large dataset and there are a few duplicates in my index . I'd like to ( perhaps visually ) check what these duplicated rows are like and then decide which one to drop . Is there a way that I can select the slice of the dataframe that have duplicated indices ( or duplicates in any columns ) ?
or ` apply ` : #CODE

You're looking for ` resample ` followed by ` fillna ` with ` method= ' bfill '` : #CODE
@USER Cloud : This don't work ! Your answer makes a problem for the last day of the year for the hour 00:00 have the good result but not after ( hours 01:00 , and next ) . This is due by the resample method which do 01 / 01 / 2015 00:00 : 00 , 01 / 02 / 2015 00:00 : 00 , ... Has someone other method ?
You may have to actually read the documentation and play around with some of the arguments to ` resample ` . I'm pretty sure you can change some combination of ` closed ` , ` label ` , and ` loffset ` to achieve your desired result .

Pandas DataFrame Insert Computed Rows
In essence I want to insert new rows in between existing rows based on a calculation that uses values from the 2 rows straddling the new row .
In my example you can see we insert a row which is the mid point value of the row both before and after .
You could use a merge like this : #CODE

This solution will change the order of you columns , which I think is fine in most cases . You can replace ` dict ` with ` OrderedDict ` if you want to preserve the column orders . #CODE

Then , I append a row of missing values . #CODE
Finally , I can insert values into this DataFrame one cell at a time . ( Why I have to do this one cell at a time is a long story . ) #CODE
This approach works perfectly fine , with the exception that the append statement inserts an additional column to my DataFrame . At the end of the process the output I see when I type ` df ` looks like this ( with 100 rows of data ) . #CODE
The append is trying to append a column to your dataframe . The column it is trying to append is not named and has two None / Nan elements in it which pandas will name ( by default ) as column named 0 .
In order to do this successfully , the column names coming into the append for the data frame must be consistent with the current data frame column names or else new columns will be created ( by default ) #CODE

Fastest Way to Drop Duplicated Index in a Pandas DataFrame
If I want to drop duplicated index in a dataframe the following doesn't work for obvious reasons : #CODE
If I want to drop an index I have to do : #CODE
This would drop all duplicates though ?

@USER Does this work with multiple levels ? In my real scenario , ` mul ` also has a multi-index and I always get ` Join on level between two MultiIndex objects us ambiguous `
I've updated the question to reflect the fact that it's really about MultiIndexed Series on both sides of the join . Didn't realise that was important .
Simply merge #CODE

when I try to perform the shift for ` departure hour ` I get " cannot shift with no offset " .
The difference between what I have and what you are showing is the index . When you concat ,
do ` concat ( list_of_frames , ignore_index=True )` in this case because I'll bet that the read in dfs's have an index starting with 0 , so you want a unique consecutive index .
Set the index and shift #CODE
" I'll bet that the read in dfs's have an index starting with 0 , so you want a unique consecutive index . " - Can you explain what that means the implications are ? Also , the set index and shift part confuses me a bit : I actually want the ` departure time ` column to be the previous row's index timestamp , so I'm doing a ` .shift ( 1 )` to get that but it blows up when I set timestamp as an index as part of the parse . I'll check to see if your approach fixes that . Thanks .

I think you can ` unstack ` it first , generate the ` T_3 ` and ` stack ` it back again . #CODE
Thank you ! I need to pass by an unstack . Is that I fear for memory reason .
For my PC , even the number of observation is unchanged , I have a lot of memory trouble when I use stack / unstack and concat functions .

when I try add new column diff with to find the difference between two date using #CODE
I get the diff column in days if more than 24 hours . #CODE

I could just loop through the first level of the MultiIndex and concat each column , but I feel that I should be able to do this without a loop . Maybe with a clever groupby ?
Since ` indicator ` is really just a boolean indexer , drop its ` NaN ` s and convert it to bool dtype #CODE
Interesting use of melt / merge to get it in a form where you can pivot . Thanks .

Pandas dataframe transpose , to_csv
In the code below , in line 4 I can transpose the dataframe , but in line 5 , when I use to_csv , the new CSV file is created , it remains the original version and not the transposed one .

I have a MultiIndex pandas DataFrame in which I want to apply a function to one of its columns and assign the result to that same column . #CODE
I managed to apply the function slicing the dataframe with ` .loc ` as the warning recommended : #CODE

pandas pytables append : performance and increase in file size
I have more than 500 ` PyTables ` stores that contain about 300Mb of data each . I would like to merge these files into a big store , using pandas ` append ` as in the code below . #CODE
The append operation is very slow ( it is taking up to 10 minutes to append a single store to ` merged_store `) , and strangely the file size of ` merged_store ` seems to be increasing by 1Gb for each appended store .
You might try tweaking the ` chunksize ` parameter to ` append ` ( this is the writing chunksize ) to a larger number as well .
Thank you , removing the indexing is speeding up the process substantially . However , I still get very large file sizes : for every 300Mb table that I append to the merged store , I get an increase in size of 1Gb which eventually is going to fill in my disk . This should not be due to compression , as I did not apply compression to the 300Mb files .
are the tables EXACTLY the same ? When you concat them , the strings will all be of the resulting table string-sizes ( which could be bigger depending on what the minimum is )
I have parsed the data so that the size of long strings in every table is restricted to fixed values ( to avoid errors in the ` append ` operation ) . The tables are not exactly the same , and their size also varies slightly , but I have kept the same ` min_itemsize ` parameters for the merged table , and so I would not expect strings to change size ...
Because all the tables were generated with the same ` min_itemsize ` parameters ( and contained strings of fixed maximum length ) , I assumed that the fields of the merged table would automatically be created with the right size without explicitly passing the ` min_itemsize ` parameter to the append function . I have now done this and everything seems to be ok . I will follow your suggestion about the compression , thank you again for your help !
another suggestion . if your data ( say tags , a big string field ) is somewhat sparse , in that it is contained on certain rows but not others . It * might * pay to keep that in a separate table ( with an index key ) , then effectively join it when selecting it . Alternatively , it might pay to store these in another table as a set of sub-tags ( presupposing that say these are a bunch of sub-fields ) , and indexing in to the main table . All that said , disk is cheap :) relatively speaking to your time .

How can I either get Pandas to understand merged cells , or quickly and easily remove the NaN and group by the appropriate value ? ( One approach would be to reset the index , step through to find the values and replace NaNs with values , pass in the list of days , then set the index to the column . But it seems like there should be a simpler approach . )

Pivot DataFrame
I want the columns to be grouped by month in a pivot table . When I pivot the data a column for each day is being created . #CODE
any reason not to use a pivot table within Excel ?

with ` colors ` being an list of size ` len ( df )` containing colors

( I'm using mysql ( mariadb if that makes a diff .. )

Insert data into Pandas DataFrame without index or column overhead ( so not concat or append )
How can I just append this structure to the top of the DataFrame without building a temporary structure that has the same column / index naming .
I'm not sure there is a way . However , even if there were , the real overhead is that you cannot update the existing DataFrame in-place , so a new DataFrame must be created to hold the result . The overhead of creating your small DataFrame to insert is probably small compared ot the overhead of creating the new large DataFrame to hold the result .
It's not a performance issue -- rather a readability issue . It's for a demo where pre-populating certain rows prior to aggregating and plotting makes a lot of sense ( and is common ) for the problem domain . The data is never large enough that inserting rows and generating a new object would matter , but it makes the code vastly harder to read if you have to lace in all these calls to helper functions that try to understand the hierarchical indices and make the insertable data into a little DataFrame with matching indices / columns so that ` concat ` can work .
@USER is exactly right ; much more efficient to append even small frames ( or big ones too ) to a list then concat all in 1 go . inserting is simply not efficient row-wise in a column based structure . that said what IS efficient is appending to a HDFStore which IS row based
Sure , but I thought it was worth it to ask before doing that . Especially since inside of that function , ugly parsing will need to happen of the indices . Whereas , I envision something at the ` pandas ` level ripping out the ` data ` attribute , doing a plain ` numpy.vstack ` of the data , calling the DataFrame constructor , and re-applying whatever index choices there were in the first place . Very much what ` concat ` must already do , except not forcing the user to pre-arrange for the indices to all make sense with each other other than positionally .

unstack to columns via panels or set_index not working in pandas
Ive tried a bunch of approaches , such as panel , or sort , set_index , unstack , reindex . What is the workflow for this in pandas ?
If I try to set_index , unstack ... -> I get a levels type erro .
If I set_index of item then unstack , ` AttributeError : ' Int64Index ' object has no attribute ' levels '` .
What are you trying to do ? ( What is your desired output ? ) It's tought to follow what your question is here . You can't unstack a single level index ...
@USER , I set_index then unstack . However , I got pivot ( time , item , value ) to work ; although pivot is just a set_index and unstack under-the-hood . As I am new to pandas , I am now discovering that the ' long-and-tidy ' format can be handled by Seaborn ; so maybe I dont even need to do this step for the plots I need to generate ...

But I'm looking for generalization for arbitrary number of columns . I tried to apply these methods as reduction function , but couldn't make it work .

Multi-index columns are not yet supported , but it is also not clear what should be the output in sql I think ? So it will be up to the user to first drop a level or flatten the multi-index .

Pandas : join ' on ' failing
... which I want to join using ` ID ` , with a result that looks like this : #CODE
You can just do a merge to achieve what you want : #CODE
The problem with ` join ` is that by default it performs a left index join , because your dataframes do not have a common index values that match then your comment column ends up being empty
Following on from the comments , if you want to retain all values in ` df1 ` and add just the comments that are not empty and have ID's that exist in ` df1 ` then you can perform a ` left ` merge : #CODE
This will drop any rows with empty comments , use the ID column to merge both ` df1 ` and ` df2 ` to but perform a ` left ` merge so retains all values on left hand side but will merge comments that match ` ID ` column , the default is ` inner ` which retains ` ID ` s that are in both left and right dfs .
Further information on ` merge ` and further examples .
So you only want values where ` Comment ` is not ` None ` ? You can drop them after or during the merge : ` df1.merge ( df2.dropna ( subset =[ ' Comment ']) , on= ' ID ')` If this is what you want then I will update my answer
@USER . merge by default will perform an ` inner ` merge , this means that it will give a result where the ID is in both left and right dataframes , if you want to retain all values on the left then perform a ' left ' merge , if you want a combination of both sides then perform an ' outer ' merge . So which is it you want ?
THAT did it ! ' Left ' merge is the way to go , thanks a lot !

Third question : I think that the code is quite pythonic , but I am not proud of that because of the last list comprehension which is running over the series of the dataframe : using the method apply would look better to my eyes ( but I'm not sure how to do it ) . Nontheless is there any real reason ( apart from elegance ) I should work to do the changes ?
... which is exactly what happens here . map would be better ? ... But its use is deprecated ...

I'm not sure there is a better method than using apply in this case

perhaps ` reindex ` creates a new dataframe , ` ix ` returns a view
@USER you are , of course , absolutely right . what do ` loc ` and ` iloc ` do ?
The reason for the seeming redundancy is that , while using ` ix ` is syntacticly limiting ( you can only pass a single argument to ` __getitem__ `) , ` reindex ` is a method , which supports taking various optional parameters . ( docs )
Thanks - What happens if I I want to update ` df2 ` with the output of these commands ? I am getting different results when using ` reindex ` with ` inplace=True ` vs using ` ix ` ( I updated the OP )

I'm working with a 2.6 gig data file on a machine with 40 gigs of memory . I would think that I'd have enough memory to work with the data in pandas and yet I run out of memory when I do something as simple as trying to drop a column in place .
in-place operations don't necessarily save you memory , as they are usually implemented as a copy , modify , and replace internally , FYI . You should show a sample frame ( you can put in random data ) , as well as operations you are doing .
yes going to suggest that ; use a hash ( or really just a string to number map ); would be vastly more efficient in memory space

Unfortunately it isn't currently possible to apply Excel formatting when writing data with Pandas ` to_excel() ` .
You can apply column formatting when using XlsxWriter as the Excel writer engine . See Working with Python Pandas and XlsxWriter .

Shift ragged rows of Pandas DataFrame to clean data with partial string search
I abandoned trying to shift everything to match up correctly . Instead , I'm trying to create a new column that combines entries from columns 5 and 6 for those values ending in KT . And I'm creating a second new column for those values starting in T .
the .iloc value was an attempt to merge the results together . There has to be a slicker way to get this formatted . Any thoughts ?
The following can be done in one pass ( using loc as iloc doesn't allow boolean masking ): #CODE
To get the end result , you could either concat these as a Series ( to avoid aligning the columns ) , or change the name of the columns to be more descriptive before concating : #CODE

Join df to itself , sort pairs and remove duplicates ( so that [ X ,
This works extremely well unless len ( ip_list ) > 5000 . Just creating
Any advice would be appreciated . Have I simply gone too far into " Big Data " where traditional methods like the above aren't going to cut it ? I didn't think having 500,000 transactions qualified as " Big Data " but I guess storing a 130,000 x 30,000 matrix or creating a list with 30,000,000 elements is pretty large ?
I think your problem is that a matrix representation is not going to cut it :

I am looking for a way to search through the whole record and replace " .0 " with nothing ( "") . So that any value in the column with say , 456.0 will change to 456 .
Use a function and apply to whole column : #CODE

I guess the problem is with the type . ' cat1 ' ' cat2 ' are of the type `' bool '`

pandas apply filter for boolean type

I feel like talking the transpose twice is redundant , and that I am missing a simpler solution . Is there a better way ?

I think there may be a bug in the tz handling here , it's certainly possible that this should be converted by default ( I was surprised that it wasn't , I suspect it's because it's just a list ) . #CODE
This way you can use resample : #CODE
@USER yes , you can groupby ` dti.to_period ( ' M ')` . Tbh , most of the time you want datetime to be the index that way you can resample / groupby using TimeGrouper ( ' M ') . Edit : The above is for * month * to do hour use ' H ' .
@USER it may be better to ask a new question ( site helps that way ) , feel free to drop the url here if you do ! Saying that , you can do ` s.groupby ([ dti.day , dti.hour ]) .size() ` if that's what you're after ?

I noticed that you say " append " in your description , but I don't see it used in your code . I do see list.extend() used , though , so maybe that's what you mean . If you're expecting to append something but are using extend ( which basically concatenates iterables to the list whose extend method you are calling ) , that could be your issue . But maybe it's just a misunderstanding on my part .
Sorry , that should be append . I changed it to extend for a test . Editing the post now .

but your problem is not that really , its the mixed dtypes in a column . read in by chunks then either append to a list and concat , or append as you go to a `` table `` store . mixed types in a column are really bad

Then I pivot and plot like so : #CODE

I'd like to replace values in a ` Pandas ` ` DataFrame ` larger than an arbitrary number ( 100 in this case ) with ` NaN ` ( as values this large are indicative of a failed experiment ) . Previously I've used this to replace unwanted values : #CODE
Edit : As far as I can tell , everything behaved as it should . As a follow up is my method of replacing values non-standard ? Is there a better way to replace values ?
As suggested in the error message , you should use loc to do this : #CODE
I was getting this warning while trying to reset the contents of an entire DataFrame but couldn't resolve it using ` loc ` or ` iloc ` : #CODE
I used the same to replace a single column , for df.loc I got this warning too .

You probably want to be use pandas Timestamp for your index instead of datetime to use ' freq ' . See example below #CODE
Now you can do timeseries / frame operations , like resample or TimeGrouper .

There is no builtin way to do this . You must create a Series from your objects and ` cumsum ` that . This can be done fairly easily with ` map ` . For instance : #CODE

On IPython notebook when you display the dataframe it outputs it left-aligned which would appear correct , however when you output the first column , you can see that it doesn't align , if you remove the western strings , so keep just chinese characters , then it aligns correctly , this looks like a problem with either mixing character sets ( although I would expect it to treat everything as unicode ) or a character width issue with mixed character sets

` pd.tools.plotting.scatter_matrix ` returns an array of the axes it draws ; The lower left boundary axes corresponds to indices ` [: , 0 ]` and ` [ -1 , :] ` . One can loop over these elements and apply any sort of modifications . For example : #CODE

Ive got this same problem ` \r\n ` in ` to_html ` output so thanks for the post . However I get ` AttributeError : ' file ' object has no attribute ' replace '` on ` contents = contents.replace ( ' \n\n ' , ' \n ')`

I get an error with this code , " TypeError : object of type ' map ' has no len() "
Sorry , didn't notice you are using ` python 3.x ` , change it to ` list ( map ( ...... ))` should do it , in ` python 3 ` ` map() ` returns a ` map object ` rather than a ` list ` .

You can apply a multiindex to a dataframe with #CODE
You can access row in a dataframe with a multiindex with the ` ix ` method as follows : #CODE
Another options is to use loc here ( rather than ix ) , but much of a much-ness ! :)
As far as I do not want to drop original columns

` lambda x : len ( x )` can be simplified to ` len ` ..

Once you tested ` flatten ` on one OrderedDict , it is straight-forward to apply it to a list of OrderedDict

How to resample large dataframe with different functions , using a key ?
I have a large time-series set of data with over 200 recorded values ( columns ) . Some values need to be averaged and some need to be summed , and I have a list that determines which is which . I need help figuring out how to feed that list into the how= function of resample .

There might be a better way , but I first append a dummy column and calculate the ` freq ` based on the column , like : #CODE
Instead of adding a dummy column and summing it , you could simply choose one and use ` len ` ( in recent ` pandas ` , anyway ): ` df.groupby ([ " Year " , " Region "]) [ " Year "] .transform ( len )`

How to customize axes in 3D hist python / matplotlib
Can you please clarify how do you adjust axes . Let's = data= pandas.DataFrame ( { ' A ' : np.random.rand ( 100 ) *1000 , ' B ' : np.random.rand ( 100 ) *10 , ' freq ' : np.random.rand ( 100 ) *2220 } ) . should I change ax.set_xticks or w_yaxis.set_ticklabels , it's confusing a little bit
` { ' A ' : [ 1 , 2 ] , ' B ' : [ 2003 , 2008 ] , ' freq ' : [ 2 , 3 ] } `

I'm new to pandas . I've built a dataframe where all the values are lists that look like [ Year , Datapoint ] ( e.g. [ 2013 , 37722.322 ] or [ 1998 , 32323.232 ) . How do I get rid of the year value and just replace the list in each cell in the dataframe with just the float datapoint ?
Yeah , thanks a lot @USER Zhu . Quick follow-up : I have a couple hundred columns , so I can't call a column the way you called the column Val . How would I apply the method you suggested on across every column ?

Inconsistent behavior of apply with operator.itemgetter v.s. applymap operator.itemgetter
` apply ` gives wrong result #CODE
apply is being passed an entire row which is a series of 2 elements which are lists ; the last list is returned and coerced to a series . embedded lists as elements are not a good idea in general .

apply custom function / method to the groups ( sort within group on col ' A ' , filter elements ) . #CODE
to return index values and select the rows using ` loc ` .
If you wish to select many rows per group , you could use ` groupby / apply ` with a function that returns sub-DataFrames for
each group . ` apply ` will then try to merge these sub-DataFrames for you .
Another way is to use ` groupby / apply ` to return a Series of index values . Again ` apply ` will try to join the Series into one Series . You could then use ` df.loc ` to select rows by index value : #CODE

1 pivot table , major axis being a serial number
I would like to divide each column of my pivot table by the value in the Serie using index to match the lines . I've tried plenty of combinations ... without being successful so far :/ #CODE
And you haven't told the division that you want to align on the ` A ` part of the index . You can pass that information using ` level ` : #CODE

If it makes it easier , I can change the line : ` df [ ' c '] = [ df_c1.T , df_c2.T ]` to no longer include the transpose : ` df [ ' c '] = [ df_c1 , df_c2 ]` , but the source data has to be JSON in the format shown .
My current solution ( more or less ) is to iterate each element in the original column `' c '` , and then do a join with its parent row while slicing the columns I want to keep . I append this data frame to a list and then do a final ` pd.concat ` on the list of all the dataframes .
Also , the reason is because the entire data frame is read in externally but that one column happens to be all JSON . I apply pd.io.json.read_json() to the column but that leaves me with a column of DataFrames that I need to expand out .

@USER : that must mean that you have columns which don't match the pattern that you showed me . Loop over them and see which ones are different , e.g. ` print ([ p for p in parts if len ( p ) ! = 4 ])` .

Overlaying actual data on a boxplot from a pandas dataframe
I am using Seaborn to make boxplots from pandas dataframes . ` Seaborn ` boxplots seem to essentially read the dataframes the same way as the ` pandas ` ` boxplot ` functionality ( so I hope the solution is the same for both -- but I can just use the ` dataframe.boxplot ` function as well ) . My dataframe has 12 columns and the following code generates a single plot with one boxplot for each column ( just like the ` dataframe.boxplot() ` function would ) . #CODE
Can anyone suggest a simple way of overlaying all the values ( by columns ) while making a boxplot from dataframes ?
A general solution for the boxplot for the entire dataframe , which should work for both ` seaborn ` and ` pandas ` as their are all ` matplotlib ` based under the hood , I will use ` pandas ` plot as the example , assuming ` import matplotlib.pyplot as plt ` already in place . As you have already have the ` ax ` , it would make better sense to just use ` ax.text ( ... )` instead of ` plt.text ( ... )` . #CODE
To overlay stuff on ` boxplot ` , we need to first guess where each boxes are plotted at among ` xaxis ` . They appears to be at ` 1 , 2 , 3 , 4 ,.... ` . Therefore , for the values in the first column , we want them to be plot at x=1 ; the 2nd column at x=2 and so on .
I am sorry , the phrase " actual values " is confusing . By that I meant a sort of scatter plot overlayed on the boxplot - i.e. with dots in place of the numeric values . Thanks for looking at my question !
@USER I want to do something very similar to this however I am using dataframe.boxplot ( by= ' column1 ') this grouping doesn't seem to work with your example . do you have any suggestions to this . essentually my box plot is xaxis df [ column1 ] , yaxis df [ column2 ] and I would like to plot the scatter plot on top of it . but with the xticks set to texts i am confused . I was able to find the xticks and xtick_labels which the boxplot set .

Another approach is to use ` stack ` : #CODE
which works because ` stack ` here returns the flattened version : #CODE

Hi Emil , Thanks for your comment . I was thinking the it does aggregation . But here I just want a three month sample starting from the first date and a six month sample data without aggregation . Could timegrouper achieve it ? I tried using date_ranges and the ix function . It kind of worked .
If you only want to extract a 3 / 6 month chunk of data from your data frame why not truncate ?

But when I apply the conditions to obtain the final dataframe : #CODE
Does the len match the second condition if you do this : ` a [ ~ (( a.cond1 == ' 120.A ') & ( a.cond2 == 2012 ))]` ?

Insert row on top of pandas dataframe with index 1 day before

I am trying to query data from a postgresql db and insert it into an sqlite db .
What version of pandas are you using ? Because in version 0.13 ( or below ) there is a bug in the ` if_exists= ' replace '` implementation , which is fixed in 0.13.1 ( latest stable release at the moment )

I have multiple file in a folder , and want to read them , transpose them , and then rewrite them into a new folder . I think I have everything going , but can't figure out how to rewrite everything .
I'm using Ubuntu , so this might not apply that accurately , but here's how I'll do it . #CODE
Hi Nanashi , thanks for your help . I have tried doing as you suggested , and I don't get an error , but also don't get any file . Perhaps it is because I'm using windows , as it looks like it works perfectly for you ! Printing out filename does show all of the files that i want to transpose though .
Kindly check if your backslashes are escaping anything . Also , is it alright if you edit your post and append your updated code ? :)

However , also note that Pandas has string operators builtin . Using them will be far faster than using ` apply ` to call a custom Python function for each item in the Series . #CODE
If you want to strip all brackets or double quotes from both ends of each string , you could instead use #CODE
^ [ " means replace [ or " at the beginning , right ? What does [ ] after that correspond to ? Thanks .

i created the following dataframe by using pandas melt and groupby with value and variable . I used the following :
You can drop the `' None '` row like this : #CODE
No need to go via ` apply ` ; ` 100 * df2 / df2.sum() ` should work .

I have created a python pandas data frame object and now I am trying to write it out to csv . The following command will work in Windows , but when I run the exact same code on the exact same data in Unix , the column headers do not align with the columns after writing out to csv . The df looks fine within Python when I am running it from he command line ( ie , df [ " somecolumn "] shows me what I would expect ) . Any ideas ? #CODE
EDIT : Here is a look at my inputs which I would lie to merge by the first column " VAR " : #CODE
what if you open it from an Excel-like program ? Do the columns align then ?

My pleasure . Thank you as well for the ` resample ` comment as well ! Looking forward to seeing your full code ! And if you haven't started using IPython's notebooks , I would also recommend it .

Creating a dict with keys as unique values incrementing the integer counter ie . Fopr every unique key I store a list of corresponding integers ( I later convert this list toa pandas Series object append it to the orignal dataframe , and also drop ( not inplace but re-assigning ) the orignal string user column .
I have tried the dictionary approach , where I load the " keys " from user and map it back to integer counter - the problem though is that I have to be ABLE to read the whole data frame first . Also the hash ( string ) will still be a 79 bytes , compared to 12 bytes for int . On trying the islice ( recursive read ) , I needed to create " set " and memory ( again a dict ) - to map it back to the integer counters . Was looking at something more pythonic .

1 ) read in csv data , convert to DataFrame , coerce data type , write out using ` HDFStore ` ( depending on your needs could be ' fixed ' or ' table ' format ) . Do this is a separate process , then exit the process . When the dataset is large , I read it in a logical format ( e.g. say a range of dates ) , then output a ' table ' format HDF5 file . Then can append to this .

Pandas : Merge data with different timing
Together I would like to join them , such that #CODE
You just do an outer merge on the left and right hand side indices , are the column names different ?
Provided example . Column names are the same , outer merge sounds promising .
Ah I understand your comment now I think - the outer merge works , but the data is in the wrong order ... dfProd.sort fixed this . Thanks !
@USER If you do an outer merge and where you have duplicate dates in both dfs will result in duplicate rows as you can see in my answer , are you wanting to merge the duplicated rows as you could have many duplicated rows is my question
Just perform a merge the fact the periods are different and don't overlap suits you in fact : #CODE
You can rename , fill , drop the erroneous ' GDP_y ' column
@USER the default merge type is inner so the values have to match in both dfs , is that what you want ?

constantly appending is not efficient at all ; append the generated frame for each group to a list ; concat at the end
For looping over a groupby object , you can try ` apply ` . For example , #CODE
Here , ` example_function ` is called for each group in your groupby object . You could write ` example_function ` to append to a data structure yourself , or if it has a return value , pandas will try to concatenate the return values into one dataframe .

I'm trying to group by a column , find the minimum date value in that group and insert it in a new column for all values in that group .

So dumb ! Thanks so much guys ! I'm happy it was that easy and not something much harder . @USER , I didn't need to transpose anything

Pandas : Timing difference between Function and Apply to Series
Why is the apply ( lambda ) method ~ 3.5 x slower . In more complex dataframes , I have noticed a larger difference ( ~10 x ) .
So in this case it looks like most of the performance difference is related apply converting each column to a ` Series ` and passing each series separately to rolling_mean . Having it use ` Raw=True ` has it just pass ndarrays .

Shift entire column on a pandas dataframe
I want to shift to the right an entire column , fill with NAN the first column and drop the last column : #CODE
shift shift the data by rows . Here's my trick #CODE

Use ` loc ` to find where column B values are not null : #CODE

You're doing a chained assignment . You should use loc . check out the indexing docs . #URL
Thank you for your quick reply . I did a bit more investigating , reading up on your suggested links . In order to set the value of only ONE element ( i.e. for a specific column in a specific row ) , first pick out the relevant series from the data frame ( df [ ' A ']) , and then use loc on that series -> df [ ' A '] .loc [ ' [ 21-23 )' , ' M ' , ' [ 10000-20000 )'] .

I would use the ` shift ` method to look backward in the dataframe : #CODE

Aftermath : I ended up being a coward and preprocessing the data before importing into pandas . But I'm still curious if there's a general solution , using something like apply or map .

@USER . Yes I am sure . ` sweep ` is based on ` apply ` . if you read the documentation for apply , the definition is amply clear IMHO . Before you ask , yes apply is mentioned on the docs page for sweep ( look under see also ) .
Pandas has an apply method too , apply being what R's sweep uses under the hood . ( Note that the MARGIN argument is " equivalent " to the axis argument in many pandas functions , except that it takes values 0 and 1 rather than 1 and 2 ) . #CODE
You can use an apply with a function which is called against each row : #CODE
Note : that axis=0 would apply against each column , this is the default as data is stored column-wise and so column-wise operations are more efficient .
You can do the same in numpy ( ie ` data.values ` here ) , either multiplying directly , this will be faster as it doesn't worry about data-alignment , or using vectorize rather than apply .
Great answer . I am playing around with this approach now and had a question . Can you please comment on the use of ` lambda ` in ` apply ` ? Any reason to prefer it over a function declared using ` def ` ? Thanks much .

Are you trying to append them to an ` HDFStore ` iteratively ? Appending to a Panel will always be in memory , so it's not totally clear what you mean . For example , just iteratively appending to a Panel will of course result in an in-memory Panel with your frames .
their are example about appending to a HDFStore panel , but you need to use append and you have to append complete panels
So I can't store a panel and then append dataframes to that panel whilst it is still in hdf5 then ?

for instance , the error you're seeing has nothing to do with plotting , so cut that stuff out .
See this section in the docs : #URL for an explanation of why you get the warning . Previsouly when eg adding a dataframe with a datetimeindex and a series with a datetimeindex , it would align on the index ( instead of the columns as other dataframes ) . This is not deprecated and you should use a dedicated method .

I've tried to resample this data by day using : #CODE
The ` append ` method does ' append ' the rows to the other dataframe , and does not merge with it based on the index labels . For that you can use ` concat `
` .append ` will append the rows ( and columns of ` df2 ` that are not in ` df1 ` will be added , which is the case here ): #CODE
Apart from that , the ` resample ` should normally work .
Thanks for you suggestion , I tried using ` pd.concat() ` ( I had to pass 0 as the axis argument as 1 resulted in ` V #URL reindex from a duplicate axis ` instead of ` .append() ` but the resulting ` DataFrame ` is the same as with ` .append() ` , a non-uniform non-monotonic time series . I think the index is the problem but I'm not sure where to start looking , I thought that some time stamps might contain hour information while other not which is why I wanted to resample or groupby day . Do you have any suggestions / methods I could look at ? Thanks again .
The error you get is because you have duplicate indices in one of your dataframes . And as the error says , ` concat ` cannot handle this . So you should remove these duplicates indices ( and then the resample should be a good approach ) , or otherwise you could maybe use merge : ` df1.merge ( df2 , left_index=True , right_index=True )` . But what error do you get if you first do ` .resample ( ' D ')` on both dataframes and then do ` concat ([ .. ] , axis=1 )` ? Because this should normally work ( with the resample , you shouldn't have the problem of the duplicate values ) .
BTW , you have to use ` axis=1 ` , as ` axis=0 ` will concatenate the two dataframes along the first axis , so concatenate the row indices , which is indeed the same as ` append ` ( you want to concatenate the columns indices ( axis=1 ) , while matching the row indices )
To answer your question , doing ` .resample ` on both ` DataFrames ` ahead of the ` concat ` doesn't make a difference , I still get the ` ValueError `
Yes , that last one . So that df1 or df2 ( or both ) have each two or more index values that are exactly the same ( in your case probably the same days ) . And because of that , ` concat ` does not know how to match the indices between df1 and df2 ( as there are more matches possible ) .
Note that resample is not inplace . So doing ` df.resample ( ' D ')` is not enough , you have to reassign it : ` df = df.resample ( ' D ')` ( or give it another name of course , as you like )
Did you do the resample correctly ? Because this really should work .

Obviously I could write plain Python that , given the period I re-sampled to in Pandas , could give me the Series I need , but I'd like to know if there is a trick within Pandas that helps me with this , or something I could do in Numpy , as I want to apply this to largish datasets ( hundreds of users , thousands of days , multiple login / logouts a day per user ) .
Then we can perform an outer join on the two tables , and fill the NA values the join created with 0s : #CODE

@USER : Does the central ` ( ? P [ a-zA-Z ] + ) - ( ? P [ a-zA-Z ] + )` part works ( except on the pokemon case ) ? I edited to replace the word boundaries , maybe pandas doesn't like those .

Pandas error with basemap / proj for map plotting
The code is supposed to create a plot map of Haiti but I got an error as below : #CODE

This will give you ` len ( df.index.levels [ 0 ]) * len ( df.index.levels [ 1 ])` plots .

Then merge using correct answer below .
So my column 0 on either data set is the key i want to merge on , and i want to keep all data from both result sets .
But on the join I get the following errors : #CODE
What happens if you do ` underlying.drop_duplicates ( cols= ' dt1 ' , inplace=True )` and ` options.drop_duplicates ( cols= ' dt2 ' , inplace=True )` does the merge work ? Also do you have any ` NaN ` values ?
@USER drop_duplicates causes the same error as the merge ` AssertionError : Cannot create BlockManager._ref_locs because block [ IntBlock : ` .
I had a look at your data ` svxySynthetic.csv ` has unique values for ` dt1 ` but ` optionsArg ` has duplicated values for ` dt2 ` as you have one entry for ` call ` and another for ` put ` , and in fact out of 372032 rows , you only have 2411 unique ` dt2 ` values so how do you want to merge these ?
OK I see one of the problems , you have duplicate column names : change your line to this : ` options = pd.read_csv ( " txt2.txt " , names =[ ' dt2 ' , ' ticker ' , ' maturity ' , ' strike ' , ' cP ' , ' px ' , ' strike_1 ' , ' yield ' , ' rF ' , ' T ' , ' rlzd10 '])` and then decide how you want to merge the datasets
what if i join underlying to the option ? i want the duplicate date lines actually , and just want the underlying price per date of the option . not the option price per day of the underlying . does that make sense ?
If you swap the order and specify left join then it should work : ` merged =o ptions.merge ( underlying , left_on'dt2 ' , right_on'dt1 ' , how= ' left ')`
You should still be able to merge on the columns : #CODE
This will perform an inner merge so only the intersection of both datasets , i.e. where the values in column ` 0 ` exist in both , if you want all values , then specifcy ` outer ` : #CODE

Ah , now I see . When I move my mask into loc , it works .

I saw something called diff on the dataframe / series but that does it slightly differently as in first element will become Nan .
Use shift . #CODE
You could use ` diff ` and pass ` -1 ` as the ` periods ` argument : #CODE

How to apply a function to a mixed type Pandas DataFrame in place ?
This is how I apply a function to Pandas dataframe , it works in place and modifies the original data frame . #CODE
But if I try the same on this data frame ( it has ints and floats instead of just ints ) , then it fails to apply inplace and always returns a dataframe . But I have a huge dataframe , so I want to do it inplace . #CODE
So no way to apply a function in-place to a DataFrame in pandas ?
But of course it'll return a frame , apply returns a frame , this seems to work inplace for me in 0.13.1 ( even with floats ) . Generally you'll want to vectorize rather than use apply , obviously here , as mentioned above , you'd use ` x [ ' b '] +=1 ` . Also , I think using iterrows is preferable to using apply like this .
I realise this does answer the question ( well ) , but using apply with side-effects seems hacky / wrong / I really dislike it !

Python boxplot out of columns of different lengths
I convert the dataframe to numpy using df.values and then pass that to boxplot .
When I try to make a boxplot out of this pandas dataframe , the number of values picked from each column is restricted to the least number of values in a column ( in this case , column F ) . Is there any way I can boxplot all values from each column ?
NOTE : I use df.dropna to drop the rows in each column with missing values . However , this is resizing the dataframe to the lowest common denominator of column length , and messing up the plotting . #CODE
Added code . Ok , I see what is happening . I use df.dropna to drop the rows in each column with missing values . However , this is resizing the dataframe to the lowest common denominator of column length , and messing up the plotting . Any solutions ?
How about don't drop the NaN ? I think ` boxplot ` will handle NaN values itself .

If you want to iterate across your database and apply a function to each row , you might also want to consider the apply function #CODE

Anything with a single underscore as the first character of a name is generally " private " which in pandas code base really means " subject to change " . So , you shouldn't be using ` _ix ` for anything . Use ` loc ` , ` iloc ` , ` [ ]` syntax , or ` ix ` to perform assignment and to select subsets of your data . This error happens because ` _ix ` is not instantiated until you call ` ix ` ( and its value is ` None ` until that happens ) , but this implementation detail is completely irrelevant to you as a user of pandas . Use the public APIs and you usually won't get these kinds of errors .

Number of non-missing values in array ? Len ( x ) excluding missing values ?

If a column contains only strings , we can apply ` len ` on it like what you did should work fine : #CODE
However , a ` dtype ` of object does not means it only contains strings . For example , the column ` strange ` contains objects with mixed types -- and some ` str ` and a ` float ` . Applying the function ` len ` will raise an error similar to what you have seen : #CODE
Continuing the above example , let us convert ` strange ` to strings and check if ` apply ` works : #CODE

When you call ` resample ` on your 1 column dataframe , the output is going to be a 1 column dataframe with a different index -- with each date as its own index entry . So when you try and assign it to a column in your original dataframe , I don't know what you expect to happen .
If you do , a different solution is to merge the two dataframes on the date , after making sure that both have a column ( not the index ) with the date .
You can't resample at a lower frequency and then assign the resampled ` DataFrame ` or ` Series ` back into the one you resampled from , because the indices don't match : #CODE

Depending on how you're printing the object , either the ` __str__ ` or ` __repr__ ` method will get called to get a description of the object . If you want to get back ' df2 ' , you could put them into a dictionary to map the name back to the object .

Self Join in Pandas : Merge all rows with the equivalent multi-index
I'm trying to do a self join on the index Date , Gran , Country , Region producing rows in the form of
Can you show an example of the data you have and what the expected output would be ? And have you looked at the ` merge ` and ` join ` functions ( #URL ) ?

The reason you get a NaN value , is because the index does not match ( in ` rows [ ' naics '] = naics.indnaics ` ` rows ` has index 0 , while ` naics.indnaics ` has index 89 ) , and assigning the value will try to align the indices .

I want to normalize the weekly sales given in A by dividing each row of A by corresponding CPI value given in B .
The DataFrames ' merge method should be faster even though it copies data . You can set the flag ` copy=False ` to minimize unnecessary copying .

EDIT : Another closely related question : how do I create an empty Dataframe with dimensions mxn ? I have got the answer below how to create an empty dataframe with dimensions 1xn , which is by setting the index . But how do I create an empty nxm dataframe filled with zeroes ? The reason I am asking , is because I suspect ( ? ) it is faster to create a zero filled dataframe , and then replace each element as needed . The alternative is to create an empty dataframe with dimensions 1xn and then add columns as needed - which I am told is slow . So it might be faster to create an empty dataframe with nxm dimensions and then replace elements as needed ( by copying a list to each column ) . Say a column has 100 rows , and I create a sublist with 25 rows , so I just copy this list to the correct subcolumn , and repeat . This is faster than adding a new column ?
in general creating an empty frame , then filling it column by column is not very efficient ; use a dict / list instead , or create sub-frames and concat them

Then I tried to apply the same solution to ` pandas ` : #CODE

Actually , why not do it with a map like :
map ( lambda x : x.value_counts() , mdf.columns )

Pandas ' merge returns a column with _x appended to the name
The keys I want to merge with are in column A . B is also ( most likely ) the same in both dataframes . Though this is a big data set I am working on cleaning so I do not have a extremely good overview of everything yet .
The suffixes are added for any clashes in column names that are not involved in the merge operation , see online docs .
So in your case if you think that they are same you could just do the merge on both columns : #CODE
What this will do though is return only the values where ` A ` and ` B ` exist in both dataframes as the default merge type is an ` inner ` merge .
So what you could do is compare this merged df size with your first one and see if they are the same and if so you could do a merge on both columns or just drop / rename the ` _x ` / ` _y ` suffix ` B ` columns .
I would spend time though determining if these values are indeed the same and exist in both dataframes , in which case you may wish to perform an ` outer ` merge : #CODE
Then what you could do is then drop duplicate rows ( and possibly any ` NaN ` rows ) and that should give you a clean merged dataframe . #CODE

Then based on each item in the row I do some complex operations on some other set of dataframea nd files and do a regression . THe output of that regression , i would like to insert as a column into this original dataframe . Tried a few things , but its not working .

How do you shift Pandas DataFrame with a multiindex ?
With the following DataFrame , how can I shift the " beyer " column based on the index without having Pandas assign the shifted value to a different index value ? #CODE
Use ` groupby-shift ` to apply the shift to each group individually : ( Thanks to Jeff for pointing out this simplification . ) #CODE

So far I can modify the ` _metadata ` attribute to supposedly allow preservation of metadata , but get an ` AttributeError ` after the join . #CODE
I think something like this will work ( and if not , pls file a bug report as this , while supported is a bit bleading edge , iow it IS possible that the join methods don't call this all the time . That is a bit untested ) .

yep ; you can install 64-bit python ( and all packages ) , or use `` conda `` to do so . 32-bit has a 4GB addressable limit , but python requires contiguous memory , so that's too big to stack reliably . in my experience 32-bit has issues with anything > 1GB ; 64-bit scales no problem however .

Not sure how your multiple observations are organized in ` json ` . But it is clear that what is causing problem is you are having a nested structure for the `" profilePicture "` field . Therefore each observation is expressed as a nested dictionary . You need to convert each observation to a ` dataframe ` and ` concat ` them into the final ` dataframe ` as in this solution . #CODE

Pandas : Count Unique Values after Resample
And here's what I would like to get to : Resample the datetime index to the day ( which I can do ) , and also count the unique users for each day .
You don't need to do a resample to get the desired output in your question . I think you can get by with just a ` groupby ` on date : #CODE
And then if you want to you could resample to fill in the time series gaps after you count the unique users : #CODE

This can be achieved with the ` loc ` indexer : #CODE
In this example , I made use of the built-in function ` isnull ` that can check all the elements of a ` pandas.Series ` to see if they are null ( ` NaN ` or ` None `) . It returns a ` pandas.Series ` of Boolean values . Those locations which evaluate to ` True ` will be considered part of the index for evaluation .
So by passing this as the first dimension of the index for ` loc ` , we can modify the values in only those rows . The second dimension identifies the column to modify . Putting the value of ` 0 ` on the right-hand-side will automatically broadcast that scalar into a compatible array shape for assigning it into the column ( some ` K ` -by- ` 1 ` column vector , where ` K ` will be the number of null entries ) .
I understand the first half of your answer ( before loc ) . So I should add this before the for loop , right ?
The for-loop is unnecessary ( and in fact , inefficient ) . In general , in the Python scientific stack , you want to avoid for-loops in favor of operations that can be directly applied to arrays and handle their array-ness in the expected way .

datetime won't operate on a pandas Series ( column of a dataframe ) . You can use ` to_datetime ` or you could use ` datetime ` within ` apply ` . Something like the following should work : #CODE
Or use apply : #CODE
to place the day there as well . should be way to do that in code , but will be quick workaround . Can always drop the Day column afterward if you dont want it .
Yeah , I did the ` to_datetime ` answer a while ago and the came back and added the ` apply ` answer at about the same time as you .
not a speedfreak myself , but i tested yours and my apply method . Youe is about 50% faster as mine has the overhead of creating a new dataframe before doing the apply . All in all your is probably better . Also handles the 1 for day better than mine .

It turned out that ` .size() ` had way less rows than the original object ( also if I used ` reset_index() ` , and however I tried to stack the sizes back into the original object , there were a lot of rows left with ` NaN ` . The following , however , works #CODE

Drop columns from pandas dataframe , regardless of whether ALL column names are present
you would need to cast the ` drop_list ` to a set otherwise you get a ` TypeError : descriptor ' intersection ' requires a ' set ' object but received a ' list '` error . So ` df.drop ( set.intersection ( set ( drop_list ) , df.columns.tolist() ) , axis=1 )` would work

Because questions describing your requirements and asking someone to write the code for you or explain how to write the code are considered off-topic for Stack Overflow , but none of the standard close reasons apply . Some people seem to think that " too broad " , " unclear what you're asking " , or " lacks sufficient information to diagnose the problem " are always sufficient to cover these kinds of questions , but this case illustrates why they often don't get the right message across .
I think a direct way is to put the GDP values in the ` border ` ` DataFrame ` . Then , all what is needed is just to ` sum ` the ` groupby ` object and then do a ` merge ` : #CODE
You can directly merge the two using pandas ` merge ` function .
The trick here is that you actually want to merge the country column in your ` datadf ` with the neighbor column in your ` borderdf ` .
Finally , merge back with the data to get the country's own GDP .

Faster way to get row data ( based on a condition ) from one dataframe and merge onto another b pandas python

Use ` factorize ` : #CODE

I want to replace ` None ` entries in a specific column in Pandas with an empty list .
and by the way , this gives me an error : ` ValueError : Must have equal len keys and value when setting with an iterable `
You can do an assignment where the length of the True values in the mask are equal to the length of the list / tuple / ndarray on the rhs ( e.g. the value you are setting ) . Pandas allows this , as well as a length that is exactly equal to the lhs , and a scalar . Anything else is expressly disallowed because its ambiguous ( e.g. do you mean to align it or not ? )
you make another frame and index them the same . Just like you would in a database like schema . Join when needed .

Python Pandas Merge result does not contain values from right dataframe
I try to merge two dataframes : #CODE

Python : Replace " {} " in DataFrame
I have dataframe ( in a matrix form ) where each cell is either ` 0 ` or ` {} ` . I want to replace the ` {} ` for ` 1 ` .
Both are not working . There are no errors , just the ` {} ` does not get replace . Is there an issue with ` {} ` ? I got this matrix by applying ` pd.DataFrame ( some dictionary )` .
` replace ` will not work here as it is for strings .
` replace ` will not work as it is a built-in that operates on strings . The only problem with your second method is that you were using the values of each column instead of the indices .

Python : Replace a cell value in Dataframe with if statement

Pandas : how to use convert_objects to replace strings with NaN values
This is related to a previous question I've asked , here : Replace any string in columns with 1
However , since that question has been answered long ago , I've started a new question here . I am essentially trying to use convert_objects to replace string values with 1's in the following dataframe ( abbreviated here ): #CODE
Where there may also be a step prior to this , with the 1s as NaN , and fillna ( 1 ) is used to insert 1s where strings have been .
I've already searched posts on stackoverflow , and looked at the documentation for convert_objects , but it is unfortunately pretty sparse . I wouldn't have known to even attempt to apply it this way if not for the previous post ( linked above ) .

Boxplot stratified by column in python pandas
I would like to draw a boxplot for the following pandas dataframe : #CODE
It uses all the unique MAT values , which in the full p1 dataframe number around 15,000 . This results in an incomprehensible boxplot .
Is there any way I can stratify the MAT values , so that I get a different boxplot of N0_YLDF for the first quartile of MAT values and so on ....
This is great , thank you so much again ! Is there any way you can replace the x-axis labels by the actual MAT quantile value ?
thanks for the further edits . I am trying to change the color of the boxes in the boxplot using pyplot.setp ( ax [ ' boxes '] , color= ' blue ') . however I get the error '' AxesSubplot ' object is unsubscriptable ' . Any idea on how to avoid this error ? thanks !
Ah , I found this reply of yours ( @ CT Zhu ) for the boxplot styling . that works : #URL
Pandas has the ` cut ` and ` qcut ` functions to make stratifying variables like this easy : #CODE

... yea thanks . I suppose it's been a little while since I embarrassed myself on stack overflow ;-) Thanks for setting me straight

I want to drop all values for a similar value in col.1 after 1 is encountered in col.2 . The result should be like : #CODE

Use loc and idxmax : #CODE
I think you should use ` ix ` ( or plain ` [ ]`) instead of ` iloc ` , since ` idxmax ` will return the actual index , so ` iloc ` will fail if the indexes aren't consecutive integers starting from 0 .
@USER so ` df.ix [ df [ ' B '] .idxmax() ] [ ' A ']` is better ? I've updated my answer , I thought in this case there would be no difference between ` iloc ` and ` ix ` , I wasn't aware of the index limitation
`` ix `` would give the wrong answer IF it had an non-natural integer index ( e.g. 2 , 4 , 6 , 8) . You should actually use `` .loc `` ( and dont't chain index ! ) , e.g. do `` df.loc [ df [ ' B '] .idxmax() , ' A ']``
@USER : Is there any official guideline these days on when you should use ` loc ` vs ` ix ` ?
well , use `` ix `` when you simultaneously need both location and position indexing on different axes , e.g. `` df.ix [ ' A ' , -1 ]`` , otherwise I always use `` .loc ``

Example output , where I want to group by follow , std_epoch , and Focal -- and drop duplicate values from column ' 0 ' , group-wise ( in this example , that's row 2 in the input ) . #CODE
I'm trying to drop NaN and duplicates , group-wise . ' 0 ' is the name of a column . I've added input and output , above .
@USER I just realized that while I want to use groupby and drop_duplicates() , I don't need to do so at the same time . I can just drop duplicates first across the whole dataframe , then use groupby . Still not sure why this worked in the previous version but creates an error in pandas 0.13.1 - but for all practical purposes I guess I've answered my own question thanks to your comment .
Solution : basically , to rethink the problem . As noted in my comment , I don't need to use groupby to drop duplicates , I'd just put them in the same line in my previous code . I'm still not clear on why this creates an error where it didn't before ( perhaps it should always have thrown an error , though it did produce the outcome I was looking for ! ) . However , I'm simply doing this in two lines of code now .
To drop duplicates : #CODE
In order to apply a method on a DataFrame that is grouped ; your need to use a loop as follows : #CODE

Now I need to shift the timeseries data ahead , say I want to shift the entire data by 60 secs , or a minute . So the target output is #CODE
Second question : Can you clarify your desired behavior a bit ? In your example data , if the Timestamp of 1 matches to the value of Timestamp 64 , where do you want Timestamp 4 to map to ? Should it also map to Timestamp 64 ? In other words , for Timestamp ` t ` , should it always match ` s ` where ` s ` is the smallest timestamp such that ` s > t+60 ` ? Are duplicate matches okay ?

-- append the series to the dataframe

thanks so much - i didn't realize you could groupby with two columns - that was my problem . the one question i have is why is there a 1 in diff ( 1 ) ?

I have checked and the ` apply lambda ` part gives the expected results if I ` debug print ` it .
The problem seems to be assigning the apply lambda construct back to the DataFrame .
Note : it could be there are some NaNs causing this float upcasting , in which case you may have to reconsider your approach ( since you won't be able to convert to int ! ) , one option might be to do the string formatting and then apply ` to_datetime ` : #CODE

merge items into a list based on index
Your code will not append you have to do ` df =d f.append ( df )` secondly are you sure you want to do this ? do you want to do ` df = pd.concat ([ df , df ] , axis=1 )` to add the dataframe as extra columns ?

I am not sure that I understand your question , but in the last part you say that you want to make sure that your data is not correlated . You apply Principal Component Analysis ( PCA ) to any dataset , the resulting principal components are not correlated by definition .

The function FilterMirroredDuplicates does the same as the select statement above it . Working on this , I did find that Filter 2 above doesn't generate the appropriate set of indices to drop . Either the statement or function above should solve your problem .
Try replacing ` ix ` with ` loc ` in either example . See [ Indexing and Selecting Data ] ( #URL ) for the difference between ` loc ` , ` iloc ` , and ` ix ` , as the last can be interpreted as either an index label , or positional .
I get different numbers that with ` ix ` , but still wrong ones =/

Merge columns together if the other values are blank
I tend to routinely get data files which have a lot of similar columns , but for each row only one of those columns actually has any data . Though sometimes it only looks that way . Ideally what I want to do is have a function that I can input a list of columns to check , and for any rows that contain just 1 value have a row that combines those columns together and change that column to NaN so I can easily remove the excess columns at the end . If multiple columns have data than don't merge / change for that row .
I don't fully understand how to get my apply function to actually apply to the row to change it .

Then , you can retrieve a subset of indices , by passing a list of tuples to the ix property . #CODE

Do you want to group by the year and month , or just somehow merge every two rows regardless of the Idx values ? What should the merged Idx value be if the year and months differ ?
I want to just merge every two rows . * however , could you also show an implementation that does consider row value , for learning's sake ? the year and month values shouldn't differ - they are same for every two rows . if they do differ , you can default it to the later year and month value
You would get that error if ` np.arange ( len ( df )) // 2 ` did not have the same length as ` df ` itself . Here's a wild guess : Make sure you have ` np.arange ( len ( df )) // 2 ` and not ` np.arange ( len ( df ) // 2 )` for example . ( Note the parentheses . )

One approach would be to use a custom dict to create a ' rank ' column , we then use to sort with and then drop the column after sorting : #CODE

In that case merge them and then depending on the complexity of your function either use a lambda or define your function and just apply it row-wise so ` merged = df.merge ( df1 , on= ' Date ')` then ` merged.apply ( myfunc , axis=1 )` or ` merged.apply ( lambda row : myfunc ( row ) , axis=1 )` I'd need to see your function first though before deciding the best approach , also it's getting late here in blighty so I may not answer
In fact what I would do is merge and then perform boolean masking on the merged df : ` merged [ merged [[ ' Value1 ' , ' Value2 ']] .max ( axis=1 ) > my_val ]` this will return the highest values for each row that are higher than your threshold value . When performing the merge you may get duplicated columns where Value1 from both dfs don't match , by default they will have suffix ` _x ` and ` _y ` , you can rename or not care seeing as you just want the highest value
@USER Thanks for the response . I gave it a try and got a huge string of errors . Let me see if I understand it correctly : I want to merge the two dataframes prior to using any function , correct ? I would do this using ` merged= ShortDF.merge ( LongDF , on= ' Date ')` . Am I understanding that properly ?
Yes that is correct , then apply a function row wise or if you're just looking for values larger than some threshold then do boolean masking using max() that will be very quick . Sorry love to help further but it's getting late here .
OK it looks like your date is actually your index , in which case you need to merge using the index , I think you can drop the on= ' date ' param but check the docs . Have to sleep now , good luck

I'm trying to merge two dataframes ( call them df1 and df2 ) of different lengths which are both indexed by their dates . The longer of the dfs ( df1 ) has all the dates listed in the shorter of the two ( df2 ) . I've tried to combine them using the following command : ` merged = df2.merge ( df1 , on= ' Date ')` , however I get the following errors which I don't understand when I try to do so . #CODE
Use ` join ` instead of ` merge ` ; it merges on indexes by default .
Looking at your error messages it looks like you have overlapping column names ; add something like the ` rsuffix= ' _y '` option to join .
I think it is most naturally to use ` join ` because it merges on indexes by default . So something like the following : #CODE

Left or right merge gives me a data frame that contains rows that are present in one of the data frames . But I need a data frame that contains rows that are present in one data frame AND NOT present in another one .
If it is just one merge key then you could do it with ` isin ` and ` ~ `
@USER ., I have more than one merge keys .
The problem team in 2112 has no value for foo in either table . So , the left join here will falsely return that row , which matches in both DataFrames , as not being present in the right DataFrame .
What I do is to add a unique column to the inner DataFrame and set a value for all rows . Then when you join , you can check to see if that column is NaN for the inner table to find unique records in the outer table . #CODE

I suggest that you can transpose your dataframe first and then do some simple compares : #CODE

Is it possible to pivot table without specifying column and row names in Pandas ?
Is it possible to pivot dataframe table without specifying column and row names in Pandas ?
Do you just want to transpose it ? ` df.T ` ?

I think interpolate needs regular spaced time series . Looks like you need to resample before .

Note : This is not the same as this question ; and I have read the documentation , but am not enlightened by it . I've also read through the " Related " questions on this topic , but I'm still missing the simple rule Pandas is using , and how I'd apply it to for example modify the values ( or a subset of values ) in a dataframe that satisfy a particular query .
pandas relies on numpy to determine whether a view is generated . In a single dtype case ( which could be a 1-d for a series , a 2-d for a frame , etc ) . numpy * may * generate a view ; it depends on what you are slicing ; sometimes you can get a view and sometimes you can't . pandas doesn't rely on this fact at all as its not always obvious whether a view is generated . but this doesn't matter as loc doesn't rely on this when setting . However , when chain indexing this is very important ( and thus why chain indexing is bad )

I first used the ` pandas ` ` shift ` method to add shifted lon / lat columns ( inspired by this SO question ) , so I could perform the calculations over a single row .
Then I used the pandas ` apply ` method ( as was suggested here ) to implement the ` pyproj.Geod.inv ` calculation , looping through slices of the ` pandas ` ` DataFrame ` for each individual in the population . #CODE

However , I can't seem to figure out how to convert a column into a datetime object so I can pivot out that columns and perform operations against it .

Use an ` INNER JOIN ` in your original query to return rows from ` table1 ` with matching IDs in ` table2 ` for the status codes you need . While you're at it , put the status codes in a variable and parameterize the SQL statement execution . Code would look something like this : #CODE
I'm afraid that is the only way to include ` df1 [ ' ID ']` in the query , since pyodbc will not know what to do with the Series . You also have to use ` ID in ( ... )` because the Series is the equivalent to an array . As I mentioned you can split it into a number of different queries if there are too many IDs , or if the IDs represent a significant proportion of the total rows in the table , then query the entire table and join the resulting DataFrames . Which is the best solution ( single query with many parameters vs multiple queries vs whole table ) depends on your data , so you will need to experiment .

Maybe the shift function could be useful for you , check it out : #URL

Now use ` apply ` and ` cut ` to create a new dataframe that replaces the percentile with the decile bin it is in ( apply is iterating over each column ): #CODE
Use apply once again to get a frequency count : #CODE

I'm trying to replace the values in one column of a dataframe . The column ( ' female ') only contains the values ' female ' and ' male ' .
Thanks . Exactly what I was looking for . If I were to map ' female ' to 1 and anything else to ' 0 ' . How would that work ?
You can edit a subset of a dataframe by using loc : #CODE

Join Pandas Dataframes according to array
I am attempting to join several dataframes together . The list of names of these dataframes is stored in another dataframe called ` companies ` , which is displayed below . #CODE
You can define an empty ` DataFrame ` and append all other dataframes to it . See the example below : #CODE
so for your example just turn the values into a list and then concat : #CODE
Thanks for the help again ed . There seems to be an issue with the way I built the list , however . companies [ ' Symbols '] is a list of strings that are the names of dataframes , as opposed to dataframes themselves . I did this because the list is written by a whileloop using append : ` ... row =p andas.DataFrame ([ dict ( Symbols=stock , Date= stockhistory.index [ 0 ])])` ` ... companies= companies.append ( row , ignore_index=True )` I've found an awkward workaround but I'm interested : how might I write this list so it works with your technique ?

How can I replace the ` is-earlier-than ` with something that's actually meaningful to Python ?

You should use the apply function which applies a function on either each column ( default ) or each row efficiently : #CODE
and then call apply like this : #CODE

@USER Thanks ! Yes , you are right ! hist may work . Follow you suggest , I have tried
`` hist ( series , bins=30 , range =[ 0,150 ]) .figure ``

and would like to use it to create a corresponding Pandas dataframe with a subset of the keys . My current approach is to take each ` dict ` from the list one at a time and append it to the dataframe using #CODE

And then I merge the two ..

Now I want a ` isnull ` column indicating if the row has any ` nan ` : #CODE

The only difference is that this gives NaN instead of an empty list in the case where there is no match . Unfortunately you cannot use ` fillna ` to replace the NaN with an empty list due to this issue . However , you can drop it with ` dropna ` , and in many cases you won't really need the empty item for the cases with no match anyway .

I am implementing pivot table modules just like MS excel pivot table .
So I am implementing pivot table manualy with groupby , melt modules of pandas .

I'm a novice at this sort of thing but have definitely been using both as of late . Truth be told , they seem very quite comparable but there is far more documentation , Stack Overflow questions , etc pertaining to Pandas so I would give it a slight edge . Do not let that fact discourage you however because Julia has some amazing functionality that I'm only beginning to understand . With large datasets , say over a couple gigs , both packages are pretty slow but again Pandas seems to have a slight edge ( by no means would I consider my benchmarking to be definitive ) . Without a more nuanced understanding of what you are trying to achieve , it's difficult for me to envision a circumstance where you would even want to call a Pandas function while working with a Julia DataFrame or vice versa . Unless you are doing something pretty cerebral or working with really large datasets , I can't see going too wrong with either . When you say " output the data " what do you mean ? Couldn't you write the Pandas data object to a file and then open / manipulate that file in a Julia DataFrame ( as you mention ) ? Again , unless you have a really good machine reading gigs of data into either pandas or a Julia DataFrame is tedious and can be prohibitively slow .

You requirement is a little unclear , if all your values can ever be small , mediu , or high and you want to drop rows that are any of these values then this will result in now rows so could you explain clearer what you require

linear fit by group in apply takes too long using pandas

but this fails with the error ` MergeError : No common columns to perform merge on ` .

Reference values in the previous row with map or apply
Normally , we can use either ` map ` or ` apply ` , but it seems that neither of them allows the access to values in the previous row .
What would you exactly want to calculate ? Can you give a concrete example ? It could maybe be done with the ` rolling_* ` functions ( #URL ) . If it is just based on the previous row , you can also calculate it and then shift it .
If you just want to do a calculation based on the previous row , you can calculate and then shift : #CODE

Given a ` list ` of ` namedtuples ` , does anyone know how to create a pandas ` DataFrame ` from selected columns of which some contain dictionaries that I want to treat as columns ? If you simply call ` pandas.DataFrame() ` the dictionaries will not be " expanded " , so you have to create another ` DataFrame ` from them , and append the columns you want to the original DataFrame . This strikes me as inelegant and inefficient ; can it be done in one line ? #CODE

Thanks for your suggestions ! I used a combination of the two responses to change the original text into a list , saved matches to a list , then joined the list and saved it to a new variable using the format @USER stated . Now to format , scale , and apply to the actual data set . It's significantly less verbose as well .

concat it with my df DataFrame
So the end keyword in ` date_range ` apparently sets the quarter preceeding ` df.index [ len ( df ) -1 ]` , is there a way to tell ` date-range ` to use the quarter following ?
` pd.date_range ( start =d f.index [ 0 ] , end =d f.index [ len ( df ) -1 ] + pd.datetools.QuarterEnd ( 1 ) , freq= ' Q ')` is what I came up with , would like to know if there is something simpler as according to #URL end and start keywords are strictly inclusive

Not to nitpick but there's actually only a single syntax modification , and that's the use of the ` @ ` sign to support local variables . The other differences are semantic ( e.g. , you can use ` or ` like ` | ` for bool numpy arrays , chained comparisons ) . These actually just parse to their native Python equivalents , because they are ultimately passed to ` numexpr ` which only accepts valid Python code .
maybe add that you * can * quote the individual names ( only with eval ) , e.g. `` pd.eval ( ' df [ df [ " annual rate "] > 0 ]')`` works ( but a bit klunky )

I have a strange problem in Pandas . I want to replace any entry that has ` np.Inf ` with the value ` np.NaN ` . However , when I do : #CODE
You can use ` df.replace ` to replace your ` np.inf ` values . #CODE

Append to dataframe from another with calculations
I am taking the second dataframe and doing some calculations with it to append to the first dataframe . However it does not appear that what I am appending to the first data frame is actually happening .
Could you use concat instead ? ` m= m.concat ([ a0 , a1 , a2 , a3 , a4 , a5 , a6 , a7 , a8 , a9 ] , ignore_index=True )`
I get an error trying to use this ... AttributeError : ' DataFrame ' object has no attribute ' concat '
[ ` append `] ( #URL ) does * not * operate in place .

Say I have a dataframe , and I want to replace any entries higher than 1 with 1 . The dataframe has several dtypes ( strings , numbers and dates ) so : #CODE
You can't call the ` replace ` method on the result of ` DataFrame.update() ` because it returns ` None ` and ` None ` has only dunder methods .
you're right , but how does that apply here ?

But when I calculate the median absolute deviation ( mad ) , I get an extra column ' a ' , which is all zeros : #CODE
maybe not important , but just for clarification , pandas mad() is not median absolute deviation , but mean absolute deviation . from the [ docs ] ( #URL ) .
This is a bug , slated to be fixed for 0.14 ( releasing soon ) , see here . The bug is that non-cythonized routines are calling ` apply ` rather than `` agg ` effectively .

I then use ` to_sql ` to append the data up to mySQL #CODE

I have tried ` stack ` and ` unstack ` but haven't been able to make it work .
I'm not sure how common " melt " is as the name for this kind of operation , but that's what it's called in R's ` reshape2 ` package , which probably inspired the name here .

As expected this apply works over the groupby object : #CODE
However , when specifying options for apply it throws up an error : #CODE
Pandas : Timing difference between Function and Apply to Series
There are different ` apply ` methods for ` DataFrame ` s and ` GroupBy ` objects . Only ` DataFrame.apply ` has a ` raw ` argument : #CODE

Pandas DataFrame merge suming column
I'm trying to merge two ` DataFrames ` summing columns value .

Pandas 0.13.1 pivot_table ( pivot ) error - not in 0.12.0 - should ( and how ) I go back to 0.12.0 ?
It could be that I am misunderstanding pivot vs . pivot_table and / or how it works in 0.13.1 .

Say I have a dataframe and I do a boxplot : #CODE
You can use the ` ax ` keyword argument as below in the ` boxplot ` method #CODE
Thanks . I also noticed that Pandas ` boxplot ` uses its own title . But even if I do ` ax.set_title ( ' My title ')` it doesn't fully change it . It's as if there were two titles . Do you know happen to know how to change it ?

Say I have a dataframe and I do a boxplot : #CODE
Thanks . I am having trouble getting this to work . The auto-rotation works great , but the dates are still formatted as ` 2014-02-24 00:00 : 00 ` .. Hmm . And I made sure to call ` boxplot ` with the axes returned by ` add_subplot ` and everything

and therefore , of course , the merge will not work . I have tried other various ways ( including returning numpy arrays ) , but all of them are not neat . What should I do ? I am aware that I could split the function up to run the code twice , once for each column - but that's not really efficient . To be clear , my expected results ( for the variable result ) is #CODE
My mistake you can pass that param on a groupby apply , could you post some sample data so I can see what your df looks like prior to the groupby
@USER So if you reset_index and then do groupby and apply does it work ?
@USER , OK I see the problem , you could using your original code just drop the duplicate index values : ` df.groupby ( level=0 ) .apply ( someFunc ) .reset_index ( level=0 , drop=True )`

python pandas unstack MultiIndex directly to sparse object
Is there a way to directly take a Series object with a three- or two-level MultiIndex and unstack it directly to a sparse DataFrame or sparse Panel ? i.e. without first creating a non-sparse ( dense ) object ?
no , unfortunately unstack currently enumerates the cartesian space of all the indices ; need someone to enhance this ( whom has an interest in sparse ) !

I can't seem to figure out how to add a % of total column for each date_submitted group to the below pandas pivot table : #CODE
The most natural way is to do it as you create the pivot table . Here I assume that date_submitted is a column ( not in the index ) using ` reset_index ` . And make sure that your values are in a column ( here I call that ' value_col ') . Then #CODE
If you want to add this as a column , you can indeed ` concat ` both serieses to one dataframe as suggested by @USER : #CODE

the ` level= ' state '` kwarg in ` div ` tells pandas to broadcast / join the dataframes base on the values in the ` state ` level of the index .

you can do `` s.to_frame() `` , but you know this is going to be horribly slow . you are copying the entire frame EACH time you append , not to mention concat copies as well . What are you trying to achieve ?

You can ` cut ` and then call ` describe ` : #CODE
I think this would be a nice addition to the docs , as I didn't find it ( or I didn't search good enough ) . Eg here #URL where ` cut ` is explained , ` describe ` is not mentioned .

Suppose I want to calculate how many days ago each observation occurred , and return that as a simple integer . I know I can just use ` apply ` twice , but is there a vectorized / cleaner way to do it ? #CODE

I think you want to use the ` apply ` method of the ** DataFrame** , using axis = 1 , e.g. df.apply ( lambda row : print row , axis=1 ) . This method will generate a series , which you could add to your ** DataFrame** . I'll write this up into an answer for you .
How would you handle duplicate dates ? Drop them ? Add the values ? Get the means ? Or would you prefer to get single rows as dataframes ? Series has a .to_frame() method , so that can be done . Also , do you want to interpolate missing dates or ignore them ?
You can use the ` apply ` method of the DataFrame , using ` axis = 1 ` to work on each row of the DataFrame to build a Series with the same Index .
Have a look at the resample method docstring . It has its own options to fill up the missing data . And if you decide to make your multiple dates into a single one , the method you're looking at is groupby ( if you want to combine values across rows ) and drop_duplicates ( if you want to ignore them ) . There is no need to reinvent the wheel .

pandas groupby add column from apply operation
Awesome , a ` transform() ` was exactly what I needed ! But do you mind explaining what's the difference between ` transform ` and ` apply ` ?
` apply ` we will work just fine too . If you replace ` transform ` with ` apply ` , you should get the same output . ` apply ` is the more general method ; ` transform ` is appropriate when you want to return something like indexed .

17 func = stack [ -1 ]

Pandas stack chart with multiple dataframes
I would like to build a stack graph for Size with these intervals ( 0-5 , 6-10 , 11-15 , 16-25 ) . The x-axis would be Time ( 1 day ) .

I'm working on a project within I will do a lot of basic computations on many small 6x6 matrices ( multiply , inverse , transpose ... ) . Theses 6x6 matrices are represented in 3D space by an array of shape : NxNxNx6x6 ( N is about 500 ) .

Join 2 dataframes with a startdate and enddate column
I would like to know how I can join data from a definition table with a start and end date column in to my event table with a date column where the date column is between the startdate and enddate columns .
I tried creating a function that would filter on table B and then using apply like below . #CODE
For comparison , here's the timeit of ` apply ` on the same frames : #CODE

To overcome this , you can use ` loc ` ( assuming ` result ` is the result of the resampling ): #CODE

Is it possible to compare all the rows that have the same item and keep only the item that has the lowest diff ?
This groups by ` item ` , ` as_index=False ` means that you want grouped output looking more like the original , ` [ " diff "]` selects the ` diff ` column , and ` min() ` says that we want the minimum value .

I'm using ` groupby ` on a pandas dataframe to drop all rows that don't have the minimum of a specific column . Something like this : #CODE
However , if I have more than those two columns , the other columns get dropped . Can I keep those columns using groupby , or am I going to have to find a different way to drop the rows ?
Method #1 : use ` idxmin() ` to get the indices of the elements of minimum ` diff ` , and then select those : #CODE
Method #2 : sort by ` diff ` , and then take the first element in each ` item ` group : #CODE

I know how to create a new column with ` apply ` or ` np.where ` based on the values of another column , but a way of selectively changing the values of an existing column is escaping me ; I suspect ` df.ix ` is involved ? Am I close ?
@USER For indexing with boolean vectors this is perfectly fine , if you want to add in other forms of indexing you would want ` loc ` . For instance : ` df.loc [ df.name.str.contains ( ' e$ ') , ' flag '] = ' Blue '` . You are right to be concerned about views vs copies . Reversing the order of access ( for me ) gives an error : ` df [ df.name.str.contains ( ' e$ ')] [ ' flag '] = ' Blue '`

Well since you added a another part I will flesh out my answer . To do part 1 I would use a list comprehension to loop the different groupby levels and get the size of all the groups . Then ` concat ` together the resulting dataframes from each groupby : #CODE

Pandas drop unique row in order to use groupby and qcut
How do I drop unique ? It is interfering with groupby and qcut . #CODE
So you want to drop values that are unique ?

When I apply a numeric function to the group , such as max() or mean() , I get a DataFrame with type ` object ` returned #CODE
When I select only numeric columns first and then apply a numeric function to the group , such as max() or mean() , I get a DataFrame with a numeric type #CODE

I need to modify a column of my pandas dataframe , depending on the boolean value of another column . suppose I have one column of values , one column of true / false , and I want to sum 1 to those values with corresponding bool true . I tried with iterrows , but this make a copy of the dataframe and doesn't modify it .
` df [ ' val '] = df.apply ( lambda row : row [ ' val '] + [ 0 , 1 ] [ row [ ' bool ']] , axis=1 )`
Don't use ` bool ` as a column name , plus your values , if they are supposed to be booleans , are invalid , they should be ` True ` , ` False ` not ` true ` , ` false `
Use ` loc ` to create a boolean mask and update the values using this mask , example : #CODE

I am using the pandas DataFrame and have tried using different functions with no success ( append , concat , etc ... ) . Any help would be most appreciated !

Weighted boxplot in Pandas
I want to draw a weighted boxplot , where the weights for each box are given by ColA_weights and ColB_weights respectively , I simply do #CODE

More generally , if you want to transform the groups of a ` GroupBy ` object with any arbitrary function , use the methods apply , transform , or filter . See the docs linked by Jeff above to understand the distinctions between these three .
i've had no success using transform , apply , or aggregate to accomplish my goal . :-/

Bool vector if value in list pandas

So I was playing with the ` concat ` method : #CODE

No I would like to replace the NaN's in the ` x1 ` column of ` data [ 1 ]` with the data that is in the x1data dataframe . My first idea ( given that I'm coming from R ) was to simply make sure that I select an object from x1data that has the same dimension as the x1 column in my panel and assign it to the panel : #CODE

Not that ` interpolate ` isn't implemented on Panels iirc , which was the OP's original need . You'll probably need to iterate over the items of the panel .
@USER or maybe a nested ` apply ` 0_o
Based on your comments you can achieve what you want if you reshape ` data ` , interpolate using the ` DataFrame.interpolate() ` method and then return the array to its original value . It works for pandas 0.13.1 . #CODE

So ` sampleA ` didn't have an observation for column 3 = ` 300 ` , so we added that row with zeros in columns 4 and 5 . But the tricky part happens with ` sampleD ` , which had only ` count ` of ` 1 ` , so it didn't pass criteria and thus its value for ` psi5 ` is NaN , and either could be skipped since I'll probably do a pivot table from this and fill emtpy with na , or add a row with ` NaN ` s .
I'd probably use ` stack ` and ` unstack ` to do this in a vectorized way . The NaNs for sampleD are a bitt tricky because I need to use the Nan caused by unstacking to fill in the stop column . But you can either get rid of sampleD at the beginning are add the NaN to sampleD at the end ( that's what I will do ):
2 ) Unstack on all the indices except stop and fill the missing values created by unstack with zero : #CODE

Though some locations has 4 non nan values , the whole process is stopped , saying the ' cubic ' method requires at least 4 non nan values . How can I make it conditional to apply the ' cubic ' method to change values for those locations which can run ' cubic ' method ?
This will drop the rows of all ` NaN ` s .

But it is extremely slow . Would it be quicker to do this in list comprehension , or with an apply function ? What is best practice for this kind of operation ?

You could first replace NaNs with empty strings , for the whole dataframe or the column ( s ) you desire . #CODE

For the aggregation methods , the list is fairly short so I can easily make a list of ` if ` statements . However , by definition the ` apply ` ` lambda ` functions are bespoke for each definition . Here's an example which takes a couple of columns to derive a percentage : #CODE
I'm aware of the ` eval ` method but this doesn't sound like good practice ( as I'd one day like to open this up for others to use and ` eval ` is open to abuse ) . Similar is the ` jsonpickle ` library but this is also open to abuse in principle . The alternative would be fixed list of functions , but I don't see that this type of arbitrary function can be made into a fixed list .
While it's true that ` eval ` can be a security hole , it's possible to restrict what is available to it by modifying the globals : #CODE
Thanks Matthew . I'll go with the above approach . I've done some further reading and there are other ways to exploit ` eval ` even with the whitelist but I think I'm prematurely optimising if I choose not to use it . It's for internal use right now and only I will be submitting new definitions .

While this works , iterating over columns feels like it goes against the spirit of python and ` numpy ` . Is there a more natural way I'm not seeing ? I would normally use ` apply ` , but I don't know how to use ` apply ` and still get the unique population value for each row .

how to zscore normalize pandas column with nans ?
I have a pandas dataframe with a column of real values that I want to zscore normalize : #CODE
What's the correct way to apply ` zscore ` ( or an equivalent function not from scipy ) to a column of a pandas dataframe and have it ignore the ` nan ` values ? I'd like it to be same dimension as original column with ` np.nan ` for values that can't be normalized

Iterate and apply function over level ( s ) of MultiIndex dropping the iteration level
Can't you just apply a lambda that strips out the first index level ?
What about ` reset_index ` with these options ? ` reset_index ( level=0 , drop=True )` ; that should drop the first index and not insert it back into the data and preserve level=1 in the index .

add / merge items to pandas data frame
Instead you want use the index labels to select / drop .

This question appears purely related to Python . I think you will find more constructive feedback on Stack Overflow than here .

Conditional merge / get duplicate rows
What I want to do is a merge of the sort #CODE
However , there is sometimes more than one right row for a left row . However , there is always only one that also satisfies ` ( right.amin = left.a ) ( right.amax = left.a )` . So , the nicest thing would be to somehow put this into the merge . I did not manage to , so my second approach will be to first to a ` how=right ` merge , grab all the rows and then go group-wise :
As it seems to be less performant , I prefer to find the duplicates using ` result.set_index ( ' id ') .index .get_duplicates() ` rather then ` result.groupby ( ' id ') .filter ( lambda x : len ( x ) 1 )` . However , this leaves me with a list of duplicate ( and float ) indices : #CODE
I am doing ` how= ' right '` - this will give me all the left rows ( yes , given the database , no left row will drop in the process ) , but in addition , all the right rows that match ( whenever there is more than one per left row , all of them ) . Did this answer your question ?

Is there a native Pandas way to do this ? Note that number of rows per ` a ` grouping in the actual data is variable , so I cannot just transpose or reshape ` pdf.values ` . If there isn't a native way , what's the best method for iteratively constructing the arrays from hundreds of thousands of rows and hundreds of columns ?

In a nutshell , for performance reasons , deletions don't actually delete data . A rename is simply a delete followed by an append . You can simply ` ptrepack ` your file to reclaim the space ( or create a new one ) . HDF5 is NOT a regular database . It is performant for certain types of operations , namely appending , and querying .
Agree that A and B are all in memory . I am reading a file ( .csv ) into memory performing the rename and only then saving it to HDF5 . As soon as I apply the rename method to the frame , it doubles the output size . If I omit the rename method the file size is half . Since all operations are performed in memory with the writing to HDF only happening at the end , I can't seem to understand why the rename method would seem to cause the frame to double in size and thus create a double-sized HDF file .
are you starting with an empty file ? ( e.g. `` mode= ' w '``) , each time ?, tables by definition append . So if you run your script twice it will append twice .

Datatypes are column based . Doing a transpose ` df.T ` in a mixed-type frame , will necessarily convert to a type that can hold both types , meaning that a string and float will yield a ` object ` dtype .
So the best way to take advantage of pandas is to load the data in a way which allows whole columns to have NumPy dtypes other than ` object ` , and never transpose ( unless the entire DataFrame is of homogeneous dtype ) .

The DataFrame plotting methods return a matplotlib ` AxesSubplot ` or list of ` AxesSubplots ` . ( See the docs for plot , or boxplot , for instance . )

Is there a way to do this without the for loop or using where ( ) or apply ( ) functions .
` apply ` should work well for you : #CODE
It just depends on how you want to treat NaNs . If you return ` NaN ` when ` row [ ' C ']` is ` NaN ` , then you won't even need this case , since ` x * NaN ` is ` NaN ` . If you want to return 0 , you can do a ` fillna ( 0 )` after ` apply ` ing ` fund ` . Also , for various reasons ` np.nan == np.nan ` is * always * False , so your way wouldn't quite work . Pandas gives the ` pd.isnull ` function to check for NaNs .

It would be faster to generate the ordering column using map as this uses cython so ` df [ ' Tm_Rank '] = df [ ' Tm '] .map ( sorterIndex )` , then order using this and then drop

Thanks . I actually have two indices ( one is ` datetime ` , the other one is a string ) . Is ` to_datetime() ` an ` Index ` method ?? If so , how can I apply it to only one of the index levels ?

Apply resampling to each group in a groupby object
And I would like to apply this function to every dataframe in a groupby object with something like the following : #CODE
How do I create functions like the above and have them properly apply to a groupby object ?
i've added some example as to what the problem is . the sample data generated isn't particularly good , but anything larger that would be appropriate to resample would be awkward to put there :-/ . if anyone has a good way of making smallish data that is appropriate for resampling , it would be wonderful if they could supply it .
There is no point grouping on ' a ' in your example . They are just random numbers ; none of the values repeat so each value in ' a ' would be a group and the resample want do anything .
df = pd.DataFrame ( { ' a ' :p d.Series ( randn ( len ( rng )) , index=rng ) , ' b ' :p d.Series ( randn ( len ( rng )) , index=rng ) } ); ;
am i just using pandas incredibly badly ? i am used to being able to groupby and apply a transformation without difficulty . am i supposed to do things differently for this application for some reason ?

use ` loc `

I want to use tools like transpose / pivot for this instead of copying the whole thing bit by bit to another DF and create it as I wish .
I think I would do it something like this ( using ` unstack `) : #CODE

Pandas unstack but only create multi index for certain columns
The goal here is to transpose this data , but such that some of the columns are single index and some are multi index . The row index should remain [ ' Date ' , ' ID '] . The single index columns should be ' line_no ' , ' floor ' , ' buyer ' and the multi index columns should be the hourly measures for each of the quality measures .
unstack uses the specified level , and leaves the remaining index alone as unstack creates * columns *
Ok thank you , unstack could do with an option over which existing columns you wish to unstack . Perhaps it makes no difference , but it seems nonsensical to me to have ( line_no ., alter ) , ( line_no ., ' good ) etc . as line_no . is constant . What's your beef with inplace ? Just out of interest - I'm always keen to write the best possible . R
you can give multiple levels to unstack , FYI

I would like to merge the two dataframes together to achieve a heirarchical index on the columns where the top level is the name of the dataframe it originated from , followed by the current column names . That is , #CODE
You shouldn't use map on a frame , FYI ( which doesn't exist anyhow ) , you could use ` applymap ` , but MUCH more efficient to use a vectorized operation #CODE
Concat together and use the keys argument #CODE

The method you are looking for is unstack #CODE
You need to group columns with command grouby and also to play with command stack and unstack .

New to Pandas and having trouble with something that seems simple . I've tried various merge and concat commands but haven't hit on the proper one yet .

But now I want to align and sum these vectors . All of the solutions I've found are rather slow . I am using Pandas 0.12.0 on Python 2.7 .
So far so good . But now I want to join them so that I can sum them : #CODE
But every second counts ! Is there a way to cut it down even further ? Is there a smarter way to add these vectors by dimension ?
This is equivalent to a ` concat ( ms ) .groupby ( level =[ 0 , 1 ]) .sum() ` . ( the ` sort ` at the end is just for illustration and not necessary ) . though you prob want to ` sortlevel() ` to sort the index if you are doing any types of indexing after . #CODE

Set no title for pandas boxplot ( groupby )
When drawing a pandas boxplot , grouped by another column , pandas automatically adds a title to the plot , saying ' Boxplot grouped by .... ' . Is there a way to remove that ? I tried using #CODE
as per Pandas : boxplot of one column based on another column

How to do a conditional join in python Pandas ?
Well , I can think of a few ways . ( 1 ) essentially blow up the dataframe by merging on ` company ` and then filter on the 30 day windows after the merge . This should be fast but could use lots of memory . ( 2 ) Move the merging and filtering on the 30 day window into a groupby . This results in a merge for each group so it would be slower but it should use less memory
Now do a merge and then select based on if ` date ` falls within ` beg_date ` and ` end_date ` : #CODE
This merge essentially inserts your window end dates into the dataframe and then backfilling the end dates ( by group ) will give you a structure to easily create you summation windows : #CODE
Another alternative is to resample your first dataframe to daily data and then compute rolling_sums with a 30 day window ; and select the dates at the end that you are interested in . This could be quite memory intensive too .

I have an array here that describes some data in a pandas ` Panel ` . I would like to drop the ` NaN ` s ( which are rows along the major axis ) and leave the data intact but it seems that calling ` .dropna ( axis=1 , how= ' any ')` will discard one row from the item that has 10 good rows and calling ` .dropna ( axis=1 , how= ' all ')` will leave one row of ` NaN ` s on the item that has 9 good rows . How can I dispose of the ` NaN ` s without loosing data ?

First of all I group overwrite the scores by grouping and later stack the
Now , you can apply interpolation methods on the ` NaN ` values as described in the docs .

Drop duplicates while preserving NaNs in pandas
When using the ` drop_duplicates() ` method i reduce duplicates but also merge all ` NaNs ` into one entry . How can i drop duplicates while preserving rows with an empty entry ( like ` np.nan , None or ''`) ? #CODE

You can add a column containing the ' Course number ' ( per user ) and then pivot it . #CODE

Dataframes in Pandas have a boxplot method , but is there any way to create dot-boxplots in Pandas , or otherwise with seaborn ?
By a dot-boxplot , I mean a boxplot that shows the actual data points ( or a relevant sample of them ) inside the plot , e.g. like the example below ( obtained in R ) .

the automatic adjustment of the ` yaxis ` can lead to a large amount of unused space in the plot . I wonder if this is because the dataframe has points that exceed the boxplot whiskers ( but for some reason the outliers aren't displayed ) . If that is the case , what would be a good way to automatically adjust ` ylim ` so that there isn't so much empty space in the plot ?
There are several types of Boxplots as described on Wikipedia . The ` pandas ` boxplot calls to ` matplotlib `' s boxplot . If you take a look at the documentation for this the argument ` whis `" Defines the length of the whiskers as a function of the inner quartile range . So it won't cover the entire range by design .
And then use the ` boxplot ` method on the dataframe , I see something similar #CODE
Alternatively , you can use the seaborn ` boxplot ` function , which does the same thing but with some nice aesthetics : #CODE

My question is how do I append the series to the dataframe ? I've looked through the documentation and none of the examples seem to fit what I'm trying to do .
No need to initialize an empty DataFrame ( you weren't even doing that , you'd need ` pd.DataFrame() ` with the parens ) . Instead make a list of Series and concat those together with ` df = pd.concat ( series , axis=1 )`
2 ) Then use ` pd.DataFrame ` to create an instance directly from the dict without loop over each col and concat .

I have a Pandas DataFrame in which the index is ( notice the Freq : H ) - #CODE
Notice that the Freq is now None and also that there are less rows and a later start date-time . The first row is now the first row of the original DataFrame that contains at least one non-NA column value .
One quick-and-dirty work-around is to just add a dummy column containing all 0s . Then , upon reload , the Freq of the DatetimeIndex is preserved . Obviously , that has unnecessary storage overhead .
Their is an option introduced in 0.13.1 ( might have been 0.13.0 ) , where you can set ` dropna=False ` on a ` put / append ` to avoid dropping an all-NaN row . This is done for efficiency , as most of the time in say storing a Panel , you have lots of all-NaN rows , but no reason to store them .
Otherwise the frequency information will be preserved . Note that if you are ` appending ` the frequency information will NOT be preserved if you append multiple times .

But your third and fourth ` for-loops ` perplex me : ` for i in range ( len ( pattern2-2 )): ` . Python names can not contain hyphens . So what does ` pattern2-2 ` mean ? If ` pattern2-2 ` is just another array of strings ( albeit with an invalid variable name !? ) then your third and fourth loops can be handled the same as shown above .

In other words , assuming that I want ` diff = A-B ` , symbolically I want : #CODE
How can I do this for all rows ? The result for ` df.loc [ -1 , diff ]` should be ` NaN ` .
Just use ` shift ` : #CODE
The default is to shift by 1 , see the online docs
To apply to a groupby then you can do this : #CODE
Thanks . What if want to apply the result within a group , respecting the boundaries of the group ? Sorry for the last minute twist , but it turns out that I need to make sure this happens .
No difference , still use shift see related : #URL
I think I need to do : ` diff = grouped [ ' A '] .shift ( 1 ) - grouped [ ' B '] .shift ( 0 )`
Just use ` df.B ` ungrouped ; you're not doing anything to it so it doesn't need to be grouped : ` diff = grouped [ ' A '] .shift ( 1 ) - df.B ` .

I figured out if I force it using ix function #CODE
It does not cut off any and gets me what I want . Also , if I use ` str ( data.ix [: ' url '])` ,
Problem is I cannot specify the index position inside the ix function as I plan to iterate by row using apply function . Any suggestion ? #CODE
Join it to your original DataFrame , and you're done . #CODE

Seems a little cleaner ( at least to me ) to use ` isin ` since you have a nice list of the banned users ( you can then map the True / False to Yes / '' : #CODE
If I add another list ` isin ` would not work either . Like if I add ` Adminlist = [ ' Bill Gates ']` and ` df [ ' Banned '] = df.Users.isin ( Adminlist ) .map ( {True : ' Admin ' } )` I guess I would have to use a merge function then .

I was writing an answer with resample while Jeff made that comment . See my answer . I think you want ' count ' instead of ' sum ' .
could replace the longer ` dates= ... ` line , but it seems less robust .

pandas merge from mysql datatype mismatch issue
and then tried the merge , to no avail .

this is my first stack overflow question so please pardon any ignorance about the forum on my part .
Thanks Jeff , it sounds like this is a question for Enthought . I've been able to work around the problem for now by using ` apply ` , but am still concerned .

Try " Apply completion on . " or try to play with " AutoCompletion delay " .

@USER ; You mean to the stack exchange answer ?

Using multiprocessing map with a pandas dataframe ?
I am using ( python's ) panda's map function to process a big CSV file ( ~50 gigabytes ) , like this : #CODE
Is there a way I can use parallelization on this ? Perhaps using multiprocessing's map function ?

I need a method to get the selected features , ( and preferably something to drop the unselected ones , for when I apply the models and selected features on new " test " data ) .

For each row , you convert the full string to lowercase , and replace the ' partial string to lower ' with the original partial string with two underscores added on both sides .

where ` item [ 4 ] == ' Iris-virginica '` filters what you want , and ` map ( float , item [: 3 ])` is for ` str ` to ` float ` , then ` np.mean ( ..., axis=0 )` is to get ` mean ` of the filtered data .

Hi Gank . The " field " was supposed to show you can apply the " .values " method to various fields of the dataframe such as columns or a selected column . " .index " is an example of replacing " field " with an actual field that is available :) I guess that could be clearer ...

Basically I would like to go over all the dates in a loop and perform some data analysis at each pass ... I want to keep all the data , so resample won't do ...

In other words I want to specify a set of coordinates and get back a vector of values at those coordinates . This is not work I am not fussy about the precise for of the index ` ix ` , any of list of pairs , pair of lists , 2-d array , pandas.DataFrame is fine . I am interested on doing this both on numpy arrays and Pandas DataFrames .

Well , you can avoid the apply and do it vectorized ( I think that makes it a bit nicer ): #CODE

To change the values directly , use loc : #CODE

Actually it's not a complete duplicate ... the question is asking specifically for Pandas . The response below shows how to assign an axis to a plot function call from ` pandas.DataFrame.plot ` which makes it possible to apply the ` matplotlib.pyplot ` refinements .

Python Pandas : Pivot groupby object
How do I pivot the results to get the formats mentioned above ?
Use ` unstack ` . #CODE

Figured it out . Selecting out the Series in the Dataframe effectively allows me to assign to it and the original dataframe . this allows me to use the slicing syntac to apply logic influencing the results : #CODE

No but how will I do pandas operations , say I want to shift the series by say 10 seconds ? That becomes impossible ? Related question is how to store milliseconds since epoch in pandas ?

Finally , merge the bin halfway points back into the original dataframe . #CODE
Then you can drop ` quartile boundaries ` if you want .

I want to append the Pandas dataframe to an existing table in a sqlite database called ' NewTable ' . NewTable has three fields ( ID , Name , Age ) and ID is the primary key . My database connection : #CODE
The dataframe I want to append : #CODE
As mentioned above , ID is the primary key in NewTable . The key ' L1 ' is already in NewTable , but key ' L11 ' is not . I try to append the dataframe to NewTable . #CODE
The error is likely to the fact that key ' L1 ' is already in NewTable . Neither of the entries in the dataframe are appended to NewTable . But , I can append dataframes with new keys to NewTable without problem .
Is there a simple way ( e.g. , without looping ) to append Pandas dataframes to a sqlite table such that new keys in the dataframe are appended , but keys that already exist in the table are not ?
You can use SQL functionality ` insert or replace ` #CODE

it seems like all you want is a join
@USER That's true . But in my situation , I use merge function before loop through all rows
Here is an example of how to do it . It should be fast on large datasets , since it does not use any loop or specific functions . It uses pandas loc function . #CODE
I do not use .loc function . I merge 2 tables by school_id to get school_name in school_detail . After I change my code , It take only 1 / 4 time before .

stack it first and then use value_counts : #CODE

If you want to translate this to a wide format #CODE

Python 2.7 Pandas : How to replace a for-loop ?
For the 5th row I may find 2 such observations . I then want to do summary stats on the observations and append those summary stats to a list .
Then I go to the 6st row and go back 5 rows and find all the obvs where the 1th column is higher than the 2nd column . I get all obvs , do summary stats on the obvs and append the results to the new dataframe .
So for each row in the dataframe , I want to go back 5 days , get the obvs , get the stats , and append the stats to a dataframe .

Is it possible to use some merge or update function to do this ?
Use ` map ` : #CODE
@USER I added another example where we do no boolean masking and just set the sex column , this is nearly 5 times faster , so I think if you can either ensure there is consistent text strings or add additional keys to the map will optimise this method if you are worried about mixed case

Return multiple columns from apply pandas
Is there anyway I can make this faster ? For example , can I instead of returning one column at a time from apply and running it 3 times , can I return all three columns in one pass to insert back into the original dataframe ?

I am making a script to process csv that I can reuse . Right now I am using this code to normalize my columns in the csv files so they can all have similar columns . #CODE
It will match and replace for example `' genUS '` , `' GENUS '` , `' Genus_666 '` #CODE

Well , ` melt ` will pretty much get it in the form you want and then you can set the index as desired : #CODE
Now use melt to stack ( note , I reset the index and use that column as an ` id_var ` because it looks like you want the [ 0 , 1 , 2 , 3 ] index including in the stacking ): #CODE
If you don't need the [ 0 , 1 , 2 , 3 ] as part of the stack , the melt command is a bit cleaner : #CODE

The result of a resample places the time as the index of the dataframe . When this happens , calling a mpl function on the df does not pass the index . You can see thus if you type tick_bars.values . you won't see the time .
Edited answer to make it conform to candlestick syntax . The root cause is the fact that you need to push the time out of the dataframe index ... those are not passed to functions like candlestick .

I've tried a few methods - eg for each data frame replace the columns with a multi-index like ` .from_product ([ ' ABC ' , columns ])` and then concatenate along ` axis=1 ` , without success .
Add a symbol column to your dataframes and set the index to include the symbol column , concat and then unstack that level :
That works - the key concept is to add a column and then pivot / unstack etc . I was stuck thinking about adding rows or column headings .
You can do it with ` concat ` ( the ` keys ` argument will create the hierarchical columns index ): #CODE
Really ` concat ` wants lists so the following is equivalent : #CODE

You could try using DataFrame's ` apply ` . Write a function that includes an exception handler and apply it to the DataFrame . #CODE

How to append data to a panel which is stored in HDFStore file
I have a Panel stored in a file , and I want to append more data to that panel
appending in memory works fine , but when trying to append data to the file I get this error : #CODE
So appending is allowed on any of the indices , hence you can append a new panel that has changed major and / or minor axes . However the items axis is fixed the first time that a table is appended .
You can specify different axes to append if you would like , eg . ` axes =[ ' items ' , ' major_axis ']` or simply transpose the panel to get it in the form that you need . This is a parameter that must be specified on the first append .

You can loop through the ( alpha , epsilon ) indices using ` for i , j in np.ndindex (( len ( alphaRange ) , len ( epsilonRange )))` , and then do the calculation cell-by-cell , but this is likely to be slow way to perform the calculation and should only be done if finding the value at ` ( 0 , 0 )` is required * before * finding the value at ` ( 0 , 1 )` , etc .

you should avoid `` apply `` if you can vectorize , e.g `` df [ ' diff '] .where ( df [ ' diff '] .abs() > = 0.3 )`` will be much faster
I need to compare every row with rows before . We are starting with the open price at the first minute . We call that the first pivot . We then check at every row if either the high or the low is higher / lower than first pivot with 0.3 . If it is then we have a higher high or a lower low . We advance further to see where that swing ends with a leg that goes the other way by at least 0.3 ... and so on . It is similar to peak_valley_pivots() found here : [ link ] ( #URL ) but I want it to be fast without numba using pandas only .
@USER . Yes ... you are right , but the next minute / row 2014-05-09 09:35 : 00-04 : 00 we encounter a new higher high 187.71 and we forget about 187.67 and compute the diff from 187.71 backwards to the first pivot . Thus the swing ends when we have a down move from last higher high that goes at least 0.3 the other way . Same thing for down trends
so i haven't tested this , but something like this will get you what you want . what happens if both the ` low pivot - diff ` and ` high pivot + diff ` in the same minute ? #CODE
Would not work ... because the trick is , as @USER -Kozela already identified , that you cannot mark a point as pivot until you find the next potential pivot ( ie if you are in an upward trend , you can't say it's done until you find a low lower by 0.3 ) . His solution works but it's very slow because it's pure python looping ( ~5min for 1.5 mil lines )
It's a bit tricky since you cannot mark a point as pivot until you find the next potential pivot ( ie if you are in an upward trend , you can't say it's done until you find a low sufficiently low ) .
It looks like Pawel's solution is the only one to work ... after all . I made a few minor adjustments : 1.replaced " irow ( 0 )" with " iloc [ 0 ]" as it will be deprecated ; 2 . used iterrows() and made it a little faster ( 1.5 mil lines in 2m51s vs +5min ); 3 . replaced " row.low < pivot - diff " with " (( pivot-row.low ) / diff ) > 1 " and " row.high > pivot + diff " with " (( row.high-pivot ) / diff ) > 1 " because I was getting some swings below the threshold ... can't explain why !?

I want to use MultiIndex to stack the data later , and I tried this way : #CODE

Now the dates for one csv file are different than the other , but when loaded with read_csv , the dates are well defined . I've tried the join command , but it doesn't seem to preserve the dates . #CODE
What you want is called outer join and is controlled with ` how ` argument , see [ in docs ] ( #URL )
By default , pandas DataFrame method ' join ' combines two dataframes using ' inner ' merging . You want to use ' outer ' merging . Your join line should read : #CODE

You can use ` drop ` to retain your original data . If this is not feasible because of memory constraints , you should prefer unutbu's answer . #CODE

Is there a nice way of doing this ? I've been going about it with what I think is a very roundabout way , i.e. making a new DataFrame for every ` Localization ` and working on from there , but there's a lot of lines and the problem of having to merge all the resulting DataFrames in the end . I'm hoping there's a smarter way of doing it , at least !
The basic idea is to group data based on `' Localization '` and to apply a function on group . #CODE
your groupby can be collapsed down to : `` df.groupby ( ' Localization ') [ ' Size '] .transform ( lambda x : x / len ( x ))``

you should post your code as a separate question and see if folks here can help you vectorize . IMHO not much reason to actually use numba as pandas can do a lot more with vectorizing ( no loops ) . pandas uses cython under the hood so most operations are optimized . get your code correct , then optimize . To answer your question , you * can * use `` df.values `` to get the underlying numpy array and process if you want , but you will then be responsible to translate back to a DataFrame ( if you want ) .
ok , that all looks vectorizable ( in general only a recurrent relation is NOT vectorizable directly , though sometimes they are possible , e.g. via shift / diff ) , but I understand your conundrum . You cannot really mix numba with pandas ; try using df.values .

I want to merge old_close and new_close to form a new DataFrame that includes all the data in both the DataFrames but excludes all duplicate values on both indices .

I would like to group a ` DataFrame ` then apply ` myfunc ` along columns of each individual frame ( in each group ) and then paste together the results . There are hacky ways to do it , but I wonder it seems like there is some simple kwarg I'm missing .
then just iterate ( and maybe concat , you'll have to handle that yourself ) , e.g. `` concat ([ myfunc ( grp ) for g , grp in df.groupby ( ... ) ])``

ok , revised the answer ; you can do almost anything inside the apply FYI

Get the data , calculate the percent change with ` pct_change ` ( or the log difference with np.log ( df.diff() )` . Your ` x ` in this case will just be the index . There may be some problems with using dates as the ` x ` variable , so you might just want to make a dummy column with ` ( range ( len ( df ))`

` shift ` is the way to shift a column forward or backward , allowing you compare values in different rows . The parenthesized expression with ` ` return the results you want but as booleans ( True and False ) rather than 1 and 0 , so I used ` .astype ( int )` to turn them into 1 and 0 . Depending on what you need to do this may not be necessary since booleans also are integers so will work in integer calculations .

` T ` is not defined in your example , and lines 4 + 5 also throw errors ( ` dt [ ' vc ']` , Indexing ) .
sorry , T is just the t-statistic .. ( its just a toy example , the real problem is getting a tuple out / in to lambda / map
As far as the unpacking goes , maybe you could let the function take an argument for whether to add or subtract , and then apply it twice .

And w / r / t speed , with len ( df ) = 10k and len ( ids_to_check ) = 20k , the try / except is about 2x slower . This is surprising to me since the other method has to traverse the index twice . Any intuitive explanation for this behavior ?

Well , I would probably convert it to a dataframe with the ` to_frame ` method and then transpose it ( example , using a slighty different series ): #CODE

This matches your output . You can assign an array to a column , as long as the length of the array / list is the same as the frame ( otherwise how would you align it ? ) #CODE
Your grid is too short , its only 3 elements , your frame is 4 . How are you supposed to align it to the frame ?

I don't see what data you want to merge . The first merge works but does not contain any merged data ...
You have a few issues going on . First your merge statements are not constructed correctly . You shouldn't be using both a ` left_on ` and ` left_index ` or ` right_on ` and ` right_index ` at the same time . You should use only one left option and one right option .
The reason you get an error in your second statement is because the index levels do not match . In your left merge , the left index is a single level , and you while you specify both ` right_index=True ` and ` right_on= ' event1 '` , the ` right_on ` attribute is taking precedence . Since both are single level integers , there is no problem . I should point out that the merge , if constructed correctly , ( ` pd.merge ( left , right , left_index=True , right_on= ' event1 ' , how= ' left ')`) does not produce an empty DataFrame ... See code below .
In your right merge , you specify using the right index with ` right_index=True ` and ` left_on ` takes precedence over ` left_index=True ` . The issue here is that the right index is 2 levels , where as your ' key1 ` field is a single level string . #CODE

How can I merge the column ' Names ' so that I have all the names once and keep track of the corresponding data in the other columns ?
why are you trying to merge a dataframe with itself ? "` pd.merge ( dfs [ name ] , dfs [ name ] ... `"
in general , all you need to do is read in each sheet , set its index ( using ` set_index `) to be ' Names ' , and then merge from there with some sort of rsuffix based on the sheet name .

@USER what you are doing doesn't make sense , unless you want to use `` where `` ( which query / eval don't support ) . `` where `` will give you a mask for each element , BUT , the way you setup the problem , `` values `` is a column and hence a row mask is enough ( and is broadcastable ) .

Can one save a pandas DataFrame to binary in " append mode " ?
Can one save a pandas DataFrame to binary in " append " mode , analogous to using mode= ' a ' in the to_csv() DataFrame method ? It would be nice to have : #CODE

I am new to Python and have worked my way through a few books on it . Everything is great , except visualizations . I really dislike matplotlib and Bokeh requires too heavy of a stack .

In pandas I can set the date to be an index and use the shift method : #CODE
You could perform a groupby / apply ( shift ) operation : #CODE

I have a data frame in which I want to identify all pairs of rows whose time value ` t ` differs by a fixed amount , say ` diff ` . #CODE
For example , if ` diff = 22.2423 ` , then we would have a match between rows 4 and 7 .
The obvious way to find all such matches is to iterate over each row and apply a filter to the data frame : #CODE
Further , I want to look and check to see if any differences of a multiple of ` diff ` exist . So , for instance , rows 4 and 9 differ by ` 2 * diff ` in my example . So my code takes a long time .
If I can do this , then I can simply compare ` df.t ` , ` df.t - diff ` , ` df.t - 2 * diff ` , etc .
If you want to check many multiples , it might be best to take the modulo of ` df ` with respect to ` diff ` and compare the result to zero , within your tolerance .

In Pandas , is there an easy way to apply a function only on columns of a specific type ?
What is an easy way to apply a function only on columns of a certain type ?
And now you can use that list with an apply or whatever .

I'm trying to drop rows of a dataframe based on whether they are duplicates , and always keep the more recent of the rows . This would be simple using ` df.drop_duplicates() ` , however I also need to apply a ` timedelta ` . The row is to be considered a duplicate if the ` EndDate ` column is less than 182 days earlier than that of another row with the same ID .
This table shows the rows that I need to drop in the ` Duplicate ` column . #CODE

Drop all duplicate rows in Python Pandas
The ` pandas ` ` drop_duplicates ` function is great for " uniquifying " a dataframe . However , one of the keyword arguments to pass is ` take_last=True ` or ` take_last=False ` , while I would like to drop all rows which are duplicates across a subset of columns . Is this possible ? #CODE
As an example , I would like to drop rows which match on columns ` A ` and ` C ` so this should drop rows 0 and 1 .
Actually , drop rows 0 and 1 only requires ( any observations containing matched A and C is kept . ): #CODE
If that was what I wanted , I'd just use ` df.drop_duplicates ([ ' A ' , ' C '])` as the default keeps one observation take the first or last as I mentioned in the question - although I've just realised I had the keyword wrong as I was writing from memory . What I want is to drop all rows which are identical on the columns of interest ( A and C in the example data ) .

How can I " join " together all three CSV documents to create a single CSV with each row having all the attributes for each unique value of the person's string name ?
The ` join() ` function in pandas specifies that I need a multiindex , but I'm confused about what a hierarchical indexing scheme has to do with making a join based on a single index .
You don't need a multiindex . It states in the join docs that of you don't have a multiindex when passing multiple columns to join on then it will handle that .
You need to chain them together like in the answer given . Merge df1 and df2 then merge the result with df3
One does not need a multiindex to perform join operations .
One just need to set correctly the index column on which to perform the join operations ( which command ` df.set_index ( ' Name ')` for example )
The ` join ` operation is by default performed on index .
That way , your code should work with whatever number of dataframes you want to merge .

Pandas Pivot MultiIndex efficiently
There are a number of pivot operations I need to do to work with the data . For example , instead of having 0 , 1 , 2 , 3 ... ( the relative position of an order in a queue ) , they have 102.297 , 102.296 , ... i.e. the price of the order as an index . He're an example of such an operation : #CODE
This can be achieved by a combination of ` stack / unstack / reset_index ` , but it appears to be really inefficient . I haven't looked at the code , but I'm guessing a copy of the table is made on each ` stack ` / ` unstack ` , causing my 8GB system to run out of memory and start hitting the page file . I don't think I can use ` pivot ` in this case either , because the required columns are in a multi-index
@USER Not sure if you mean the result or the input data . I've added an example input csv to the post . In terms of result , I've also added one example . But it can be a variety of different transforms - the specific result is not important , but I'm rather interested in how to efficiently do pivot operations on multi-index dataframes . Sorry if it wasn't clear .
Unstack essentially creates an enumeration of index x columns so it can create a huge memory space when you have a lot of columns and rows .
Groupby the ' level ' on the columns and apply f ; don't use apply directly , but just concat the results as rows ( this is the ' unstacking ' part ) .

return `` None `` as concat will ignore it

Most of the column names overlap ( i.e. are the same ) and the join just does fine . However , there is one columns in each DataFrame that I would like to preserve as ' belonging ' to the original DataFrame so that I have them both available for future use . I gave each a different name . For example , ` DF1 ` has a column entitled opselapsed_time and ` DF2 ` has a column entitled constructionelapsed_time .
Do I need to just stick with using a merge / join type solution instead of combine_first ?
Joins along the date index and keeps the different columns . To the extent the column names are the same , it will join ' inner ' or ' outer ' as specified .

This is partial string indexing , see here : #URL ; in this case its the same as if you specified ` df.loc [ Timestamp ( ' 2014-05-30 ')]` because its an exact match ( e.g. you have daily freq ) #CODE

Pandas will always align the data , that's why this doesn't work naturally . You are deliberately NOT aligning .

( 3 ) Or you could do it all in a function that was called by the ` groupby / apply ` : #CODE

SettingWithCopyWarning , even when using loc ( ? )

I found this description of how to resample a multi-index :
When using ` count ` , state isn't a nuisance column ( it can count strings ) so the ` resample ` is going to apply count to it ( although the output is not what I would expect ) . You could do something like ( tell it only to apply ` count ` to ` value_a `) , #CODE
Or more generally , you can apply different kinds of ` how ` to different columns : #CODE
Prior to 0.14 it was difficult to groupby / resample with a time based grouper and another grouper . ` pd.Grouper ` allows a very flexible specification to do this .

pandas apply function that returns multiple values to rows in pandas dataframe
I would like to apply a transformation to each row that also returns a vector #CODE
I end up with a Pandas series whose elements are tuples . This is beacause apply will take the result of myfunc without unpacking it . How can I change myfunc so that I obtain a new df with 3 columns ?

Now I ordered scm.AreaMax , in order to get the best bin . To do this , use the " cut module " and add a new column called bins containing the generated intervals . The following code is an example of what is described above : #CODE

I've got two DataFrames I want to concat . Both have the same column and they are of different dtypes . One is float , the other is a string . I want to concat these columns while keeping the granularity of the float column . See below for an example : #CODE
Basically how can I concat these while keeping 124.028125 as the value for Row1 ?
I knew this before and just forget . I did df.ix [ 0 ] and saw there was still no precision , so I assumed that it had somehow cut off the precision . Dumb , sorry !

pandas , apply string operation to column should be string type , but has missing values ( np.nan )
When I try to truncate each string in that column based on certain condition : #CODE

Merge a lot of DataFrames together , without loop and not using concat
I have > 1000 DataFrames , each have > 20K rows and several columns , need to be merge by a certain common column , the idea can be illustrated by this : #CODE
Does it even make sense to merge it , then ? What's wrong with a panel ? #CODE
This is an interesting idea indeed . But it works best if there is only one VALUE column , while I have multiple per dataframe ( see edit ) , and the number of columns are not the same across all dataframes . Furthermore , it is actually 2X slower than the loop version , while the loop version is already 2X slower than the ` concat ` version .
I was suggesting using the Panel directly , since that seems to be what you're doing already . If it matters which dataframe the data appears to be in , just store that information as an extra column and concat without bothering with the index .

How to normalize data efficently while INSERTing into SQL table ( Postgres )
In order to normalize I would have to check for the distinct values of event_type in the raw log and insert them into the event_types table .
Firstly , on the issue of normalization . Yes , you should always normalize and to the so-called 3rd Normal Form ( 3NF ) . This basically implies that any kind of real-world data ( such as your event_type ) is stored once and once only . ( There are cases where you could relax this a little and go to 2NF - usually only when the real-world data requires very little storage , such as an ISO country code , a M / F ( male / female ) choice , etc - but in most other cases 3NF will be better . )
You can then invoke this function as a simple ` SELECT ` statement , which will return the ` id ` of the newly insert log message : #CODE
a ) Normalization doesn't have anything to do with how much storage a value might require . b ) Normalization doesn't offer different guidelines for different data types ; normalization through BCNF is based on functional dependencies , not on data types . c ) Normalization doesn't mean " replace text with ID numbers " . d ) Your " event_type " table allows duplicate data .

For circular import reasons ` matplotlib ` can't know about ` pandas ` . Shared libraries provide you with tinker-toys to build bigger tools , not have _every_ conceivable tool . That way lies madness ( for the maintainers ) . I suspect your computation is < 10 LoC in pandas via ` GroupBy ` .
Sorry , you want ` cut ` not ` GroupBy ` #URL

groupby the ' c ' column , and consider all the columns that you want passed to the ` apply ` EXCEPT for c ( this is what ` df.columns - [ ' c ']` does , as normally the grouping column IS passed to the apply .
updated for the unstack ; good thinking ! I think need a way exclude the grouped column ( now you can do `` as_index=False `` but that actually drops them , maybe `` as_index= ' exclude '`` might work ) ....

Hos to use the merge function to merge the common values in two DataFrames ?
I have two DataFrames , I want to merge on the column " Id "
I wish to keep all values from both the df s but merge the duplicate values into 1 .

I have a two column dataframe df , each row are distinct , one element in one column can map to one or more than one elements in another column . I want to filter OUT those elements . So in the final dataframe , one element in one column only map to a unique element in another column .
The above grabs the rows that may be allowed based on looking at column ' B ' alone . And then merging the intersection leaves you with rows that only meet both conditions : #CODE

Try using apply with a custom function over axis=1 : #CODE
@USER Actually , it does . Just specify the columns when you apply the function , like this : ` df [[ ' col1 ' , ' col2 ']] .apply ( ... )`
This should be pretty fast ; I think even the use of map here will be a Cythonized call . If a boleen vector is sufficient for the newcol , you could just simplify it to the following : #CODE
This should still be pretty fast because it uses ` pandas `' vectorized string methods for each of the columns ( the apply is across the columns , not an iteration over the rows ) .

Jeff gave a great answer there , but it fails if the group size is less or equal to parameter I pass in head ( parameter ) when concatenating my rows as in Jeffs answer : In [ 31 ]: groups = concat .....

` ast.literal_eval() ` interprets the string ** exactly as would a float literal be interpreted in Python code** . It doesn't truncate * anything* . ` str() ` on a dictionary on the other hand , triggers ` repr() ` calls on float values , which most certainly * does * simplify float values .

is there something analogous for this for read_excel to alter all unicode column names and strip random whitespace ? something as simple as this : str ( col ) .strip() raises errors

Sorry in advance if this is a little long winded but if I cut it down too much the problem is lost . I am trying to make a module on top of pandas and matplotlib which will give me the ability to make profile plots and profile matrices analogous to scatter_matrix . I am pretty sure my problem comes down to what object I need to return from Profile() so that I can handle Axes manipulation in Profile_Matrix() . Then the question is what to return form Profile_Matrix() so I can edit subplots .
And this could have been cut down much more with out losing the problem . strip out anything involving pandas , use synthetic data .
You do not need those lines . Please edit your question to strip out all of the pandas calls and pass around ` np.random.rand ( 50 )` as your data . There is too much cruft is this code to see what is going on clearly .

I want to replace python None with pandas NaN . I tried : #CODE

The alternative is to use [ Unidecode ] ( #URL ) to replace non-ASCII codepoints with reasonable ASCII equivalents and surrogates . U+2014 is EM-DASH , which can reasonably be substituted with a doubled plain ` - ` dash , for example . Ditto for U+2019 , which is a fancy single quote , `'` could replace it . That library does this .

So after playing a bit with this I came up with the following , but still can not concatinate / append . #CODE
It is important to note that whatever solution you find to append to your dataframe , you will not be able to avoid a full copy of your dataframe unless you have preallocated memory by defining an empty frame of sufficient size which you fill up sequentially . Everybody , please correct me if I am wrong about this and how appending would be possible without preallocation in that case .

I am calling a function from within a ' for each loop ' which attempts to insert values into a Pandas DataFrame based on a specified column start and end location . The function is this : #CODE
My issue is that despite the same starting conditions when I call this function it seems to generate a list of inconsistent length . e.g. with values of srowb = 1 and erowb = 18 it will generate a list ( tmp_brollb ) which has either len ( tmp_brollb ) = 17 or len ( tmp_brollb ) = 18

` isnull ` and ` notnull ` work with ` NaT ` so you can handle them much the same way you handle ` NaNs ` : #CODE
just use ` isnull ` to select : #CODE

To drop duplicates you can do one of the following : #CODE

Well it looks like within groups , there is one observation per month and you want the percent change from one month to the next . You can do that with a ` groupby / apply ` by grouping on ' product_desc ' and then using the built in ` pct_change() ` method : #CODE
Or you could use shift as you suggest with a small modification : #CODE
Thanks for your help on this . The pct_change function was exactly what I needed . However , when I apply the sort they don't sort the same way . Did you convert the activity_month into dates with strptime first ? My results are sorted by product_desc but have the activity_month out of order .

Detailed Error stack is as shown below #CODE

Thx . I should try that . However , I've experienced that shift does not yield very good performance . Another thing that comes to mind - can this method handle stepvise calculations . say 2014Q1 and 2014Q2 are 0 , and you need to calculate 2014Q1 before the next one . And on last note , I am still interested the fastest of setting data , regardless of alternate methods ...
well , shift is a vectorized operation , so it should outperform other methods ; if you have a specific performance issue pls post that . If you want a * recurrence * relation that you might need to do something else . But often these CAN be expresses as reduction type operations .

I could modify both the DataFrame and the original Panel by selecting by label in the call to ` loc ` . The documentation says that ` loc ` is strictly limited to labels , so I am not sure the code you have posted can work at all : I got a ` KeyError : ' the label [ 0 ] is not in the [ items ]'` when pasting the example into an IPython console ( ? ) .
@USER as the documentation states you should * only * assign values using the multi-axis indexers , `` ix / iloc / loc `` ; slicing and selecting * may * work ( and only in a single-dtyped case ) , but in general is not robust and should be avoided .

I'm trying to programatically construct a pandas ` TimeGrouper ` . A quick look at the code shows that the ` freq ` parameter of the ` TimeGrouper `' s ` __init__ ` method gets converted into a ` DateOffset ` by the ` to_offset() ` function . Furthermore , ` to_offset() ` checks whether its parameter is an instance of ` DateOffset ` and if true , returns it .
passing ' 10T ' is enough ( this is what ` to_offset ` does , converts it to an offset object )` . Furthermore , you rarely need to explicity construct a ` TimeGrouper ` ( and in 0.14.0 , releasing shorty ) , you don't need to at all . You generally just ` resample `` #CODE
their is another way , specify a tuple ( see above ); I guess you could put forth an enhancement to add a { name : value } to be accepted as well ( as the freq parameter )

You can do this , but since the right-hand-side ( the assignee ) is not labeled , it will just assign the first len ( assignee ) values #CODE

you should probably have to add a " mode= ' a '" argument in the " to_csv() " command , to say you want to write to the file in append mode ( otherwise the mode defaults to ' w ' , which indeed overwrites the previous content ) #CODE

Now I find that the class of ` a ` and the class of ` b ` are different ; ` b ` is a ` pandas.core.series.Series ` object and therefore you can not apply the method ` append2 ` to it .
You could do something like this . You don't need to sub-class at all , rather just monkey-patch . And this would be more efficient that appending twice ( as an append copies ) . #CODE

but still can't quite get the output to the formats I need . I'm not quite sure how to apply the df.groupby syntax or the df.apply syntax to what I'm working with .
Here is a neat apply trick . I'll create a function and print out what is incoming ( and maybe even debug in their ) . Then easy to see what's happening . #CODE
Perfect ! This is exactly what I was looking for . I guess my confusion stems from the fact that the Series.value_counts doesn't seem to fit into the arguments required by the df.apply method . How does it know which axis to apply the value_counts to ?

I have N number of data frames ( e.g. df1 , df2 ) which consist of 1s and 0s and I am trying to find a way to use Pandas to multiply them all together based on a 3-dimensional index in order to find the intersection of them ( i.e. result ) .
I think that you can just put all of your frames in a list and reduce . They will align each time ; including the fill_value=1 will propogate the values when multiplied vs a NaN ( which is what I think you want ) . #CODE

You can reformat the values by passing a reformatting function into the ` apply ` method as follows : #CODE

@USER always worth checking different approaches usually you cannot beat using ` loc ` and numpy functions

Apply an operation to the value of a series ( 2 and 3 ) #CODE

I've added the version information and also the dtypes for the conflicting append .
So , you need to conform your dtypes in EVERY chunk to be the same . You might have mixed data in that particular column . One way to do this is to specify ` dtype = { column_that_is_bad : ' object ' } ` . Another is to use ` convert_objects ( convert_numeric=True )` ON THAT column to coerce all non-numeric values to ` nan ` ( this will also change the dtype of the column to ` float64 `) .
you need to conform the dtypes before you write them to HDF , look at df.dtypes of each chunk before you are writing them . some data is change them ( e.g. you may have missing values in one but not in another ) . You need to coerce to the proper format .

I have a series with a datetime index , and what I'd like is to interpolate this data using some other , arbitrary datetime index . Essentially what I want is how to make the following code snippet more or less work : #CODE
` reindex ` doesn't do what I want because it can't interpolate , and also my series might not have the same indices anyway . ` resample ` isn't what I want because I want to use an arbitrary , already defined index which isn't necessarily periodic . I've also tried combining indices using ` Index.join ` in the hopes that I could then do ` reindex ` and then ` interpolate ` , but that didn't work as I expected . Any pointers ?
Thank you , this got me most of the way there . After ` interpolate ` I could then use ` reindex ` to get back down to just the desired index .

I would like to merge two dataframes on time , but instead of merging exactly I would like to get the max ( time1 ) = time2
I think that pandas index.searchsorted is the best way to do this but I can't quite figure out how to get the merge to work

Pandas resample with all day range
I use code below to resample data : #CODE
But I want the whole day range from 00:00 : 00 to 23:00 : 00 , how can I change the resample date range ?
It is DateTime or I can not resample it .
Well , you ccan ` reindex ` after the resample using a hourly period index for the whole day . So something like the following : #CODE

Can you post the data ? I don't understand why you do ` crosstab ` and drop all the columns from ` Forma ` , maybe you want ` Series.value_counts ` . matplotlib can't do real 3D plot , try ` visvis ` .

You need to pass ` mode= ' r '` explicity to force an open in read-only mode . Default is to open in ` mode= ' a '` ( append mode ) .

no , no automatic way to do that ( though you could write a function to take the max abs value of an int column and figure it out )

Next , for the duration , you can use ` diff ` for each group : #CODE

Python Pandas join dataframes on index
I am trying to join to dataframe on the same column " Date " , the code is as follow : #CODE
It complains dataframe df_train_csv has no column named " Date " . I'd like to set " Date " in both dataframe as index and I am wondering what is the best way to join dataframe with date as the index ?
This generated dataframe contains all weeks in the range so I need to merge those two dataframe into one .
Well it doesn't work because you set the ` index_col ` to date when you read the csv , you can either not set the index_col to ' Date ' OR set the ' Date ' col in df_train_fly as the index column also and pass ` left_index=True , right_index=True ` to the join
Actually you cannot pass left_index=True etc .. unless you do a ` merge ` so if you want to use join then drop the ` index_col= ' Date '` param in ` read_csv `
I think in your case if you did this it should work : ` merged = df_train_csv.join ( df_train_fly , how = ' right ' , lsuffix= ' _x ')` so drop the ` on =[ ' Date ']` param as this is for specifying a column to join on , if you leave this out the default is ` None ` which will use the index to join on
I used the other option that you mentioned : not setting the ' on ' param and let them join on index automatically , which is working ! BTW , to set the index for a dataframe , like this df_train_fly.set_index ( ' Date ') does not really change the index and I have to assign the value back to df : df_train_fly = df_train_fly.set_index ( ' Date ') to make it work .
@USER I am using the latest pandas 0.13.1 and Python 2.7.3 . I have posted the full data and I am still wondering how it works if we need to join on more than one field except index ? Because when omitted , the index is used as join condition , but when other columns are specified , I think index is not used .
If you want to use the index and columns then you cannot use join for that , you have to use merge , it would be easier to not set ' date ' as an index and then merge on the multiple columns so like ` merged = df_train_csv.merge ( df_train_fly , on =[ ' Date ' , ' other_col '] , how = ' right ' , lsuffix= ' _x ')`
So the above join will not work as the error reported so in order to fix this : #CODE
the above will use the index of both df's to join on
You can also achieve the same result by performing a merge instead : #CODE
Hi EdCum , thanks for your help ! I removed index_col= ' Date ' and use merged = df_train_fly.join ( df_train_csv , on = [ ' Date '] , how = ' left ' , lsuffix= ' _x ') . But it gives me very strange results ( all columns are NaN and date from df_train_csv is NaT ) and I guess the date in df_train_csv is in a different format from that of df_train_fly , because when I use inner join , an empty set will be returned . What is wrong with the timestamp ?

Then apply : #CODE
@USER - getting a little off topic ... but ... re chain indexing - I used df.iloc so I didn't need to be too careful with indices . In order to not Chain index , I would need ix - so would you do this while iterating row by row : df.ix [ df.index [ i ] , ' some_col '] =some_value
no , simply use ix it will figure out if u pass it integers ( and don't iterate - always better ways )

` In [ 9 ]` , where I create the top level of the MultiIndex can be made in a bunch of different ways . If you have more cols you can do something like ` np.arange ( len ( df.columns )) .repeat ( 2 )`

pandas groupby and join lists
I have a dataframe df , with two columns , I want to groupby one column and join the lists belongs to same group , example : #CODE
what does the function in df.groupby ( ... ) .apply ( lambda x : ... ) apply to ? what is the form of x ? list ?
` object ` dtype is a catch-all dtype that basically means not int , float , bool , datetime , or timedelta . So it is storing them as a list . ` convert_objects ` tries to convert a column to one of those dtypes .

I am new to python and data analysis . I have a dataframe consisting of 3 columns - site ( string object ) , date ( datetime ) and value ( integer ) . I want to plot a graph using the x axis as a concatenation of site and date and y axis as value . However I am not able to merge these 2 columns properly . It gives an error saying expecting a string or buffer but datetime.date found .

groupby , apply , and set not behaving as expected ... is this a bug ?
Now my attempt to concat letters produces a funny result : #CODE
[ @USER ] ( #URL ) , why does ` dat.groupby ([ ' names ']) [[ ' letters ']]` pass all the columns through to apply ( letters , numbers , names ) ? Why not just the ' letters ' column as a dataframe instead of a series ? Is it ignoring ` [[ ' letters ']]` ( syntatic sugar that's not allowed ) ?

You don't need the final apply , see here : #URL you can simply `` astype ( ' timedelta64 [ D ]')`` or divide by `` np.timedelta64 ( 1 , ' D ')`` ( they are sligthly different in how they round .
I think I'd use ` diff ` here : #CODE
Based your code ( your ` groupby / apply `) , it looks like ( despite your example ... but maybe I misunderstand what you want and then what Andy did would be the best idea ) that you're working with a ' date ' column that is a ` datetime64 ` dtype and not an ` integer ` dtype in your actual data . Also it looks like you want compute the change in days as measured from the first observation of a given ` group / stage ` . I think this is a better set of example data ( if I understand your goal correctly ): #CODE
Given that you should get some speed-up from just modifying your apply ( as Jeff suggests in his comment ) by dividing through by the ` timedelta64 ` in a vectorized way after the apply ( or you could do it in the apply ): #CODE
But you can also avoid the ` groupby / apply ` given your data is in group , stage , date order . The first date for every ` [ ' group ' , ' stage ']` grouping happens when either the group changes or the stage changes . So I think you can do something like the following : #CODE
Apply method : #CODE
So I think avoiding the apply could give some significant speed-ups
Yeah @USER , I thought about ` transform ` but at least for 0.13.1 I usually find transform no faster than a generic ` apply ` so I didn't include it . But I will update the answer with that as an alternative .

I'm very new to Python and was hoping someone might be able to give me some tips . I'm also new to posting on stack over flow , so any tips on how to display a table in it would definitely be appreciated as well .

I will need to reshape my results back to the row x col dimensions of the original image so I can't just drop the all zero rows .

You should use ` loc ` and select the column of interest ' C ' in the square brackets at the end

how do merge two dataframe in pandas
I want to merge two dataframe based first column .
How to merge df1 and df2 having first column as index
See #URL or google ` pandas merge `
From the documentation on how to merge , it looks straightforward : #CODE

I take a multi-indexed pandas series named ' dat ' and try to append it to an empty series named ' s ' . This is how ' dat ' looks like : #CODE
Now , when I append it to the empty Series ' s ' it returns this : #CODE

You can get the dataframe you want to merge with : #CODE
To get the result you posted ` merge ` #CODE

IIUC -- unfortunately your code doesn't run for me with your data and you didn't give example output , so I can't be sure -- you're looking for ` merge ` . Adding a new student , Fred Smith , to table 3 : #CODE
or maybe an outer merge , to make it easier to spot missing / misaligned data : #CODE

When I create a pivot table on a dataframe I have , passing ` aggfunc= ' mean '` works as expected , ` aggfunc= ' count '` works as expected , however ` aggfunc =[ ' mean ' , ' count ']` results in : ` AttributeError : ' str ' object has no attribute ' __name__ `
How do I create a pivot table with multiple functions ?
The dataframe is 6k rows long- I'm looking for the counts ~600 codes ( rows in the pivot table ) , over the course of 15 months ( cols ) . As of now , I'm sending a mean pivot table and a count pivot table to csv's , then combining in excel .
also , one last suggestion , try ` len ` ?
That's it ! ` aggfunc =[ np.mean , len ]`

Of course you don't show your data so this may or may not apply and I apologize if not , but for a large dataframe I'm dealing with I see a speedup of , well , 24 million times !

Thanks . Is this technically a ` transform ` or ` apply ` operation ? I never quite understood the difference .
` apply ` is the most general category of operation on a group , so lots of things fall under its umbrella . What distinguishes transform operations is that they produce something indexed like the input , and that happens here , so I guess you could think of it as a transform .

I am new to Python and am uncertain why I am seeing memory usage spike so dramatically when I use Numpy ` hstack ` to join together two ` pandas ` data frames . The performance with ` pandas.concat ` was even worse - if it would finish at all - so I am using NumPy .
As shown in this thread it is not possible to append an array in place and this would not be efficient since there is no guarantee to keep the extended array continguous in memory .

Call unstack twice : #CODE
Calling ` df.T ` would work for that case rather than calling ` unstack ` twice and I think you cannot use ` unstack ` in the case of a multi-index
Transpose the dataframe : #CODE

Well , one approach is the following : ( 1 ) do a ` groupby / apply ` with ' id ' as grouping variable . ( 2 ) Within the apply , ` resample ` the group to a daily time series . ( 3 ) Then just using ` rolling_sum ` ( and shift so you don't include the current rows ' x ' value ) to compute the sum of your 70 day lookback periods . ( 4 ) Reduce the group back to only the original observations : #CODE
You are going to need your data sorted by ` [ ' id ' , ' dates ']` . Now we can do the ` groupby / apply ` : #CODE
It just becomes the next parameter in the ` apply ` . See me edit for details .

No need for the ` lambda ` on the first one : ` apply ( ' { : 0 > 15} ' .format )` should work too .

as the apply function creates only one column with tuples in it .
You can put the two values in a Series , and then it will be returned as a dataframe from the apply ( where each series is a row in that dataframe ) . With a dummy example : #CODE

I would drop to numpy to do this a little faster : #CODE

Python Pandas Replace Special Character
What is returned from the code ? Does it cause an error or just fail to replace the character without spitting out an error ?
If all you're concerned about is the ` ` , you should decode in UTF-8 , and then just replace the one character .
` Another update ` does not work . The replace value has to be like ` u ' \xc9 '`

Hmmm , seems confusing . But why is it treating the bool as str ? if row [ 4 ] .bool() == False :
AttributeError : ' str ' object has no attribute ' bool '

Their are some issues currently ( even in master ) dealing with ` NaT ` in ` PeriodIndex ` , so your approach won't work like that . But seems that you simply want to resample ; so do this . You can of course specify a function for ` how ` if you want . #CODE

Pandas DataFrame map string path in to integer path
I want to map all these strings in path to integers . #CODE
@USER Cunningham : There is no logic , need to map strings to unique integer value . Reason is I can not keep string values for future needs .

The CSV file contains a columns of tags , written in English , such as " math " and " literature " . I want to map those tags to integers like " math " : 1 , " literature " : 2 . How can I do this with Pandas ?
You can feed a ` dict ` with the strings as keys ( ' math ' , etc ) and the integers as values into the ` map ` method . For example : #CODE
You could also use ` factorize ` to accomplish much the same thing but you wouldn't control the mapping from string to integers ( although in this example it ends up being the same ): #CODE

Use ` to_datetime ` to convert to a string to a datetime , you can pass a formatting string but in this case it seems to handle it fine , then if you wanted a date then call ` apply ` and use a lambda to call ` .date() ` on each datetime entry : #CODE

Merge two DataFrames by column of sets
I would like to merge ` df2 ` to ` df1 ` by ` name_tokens ` to get ` ranking ` . Merging condition is : #CODE
How should I customize my merge method to accomplish this ? ( I have millions of records in ` df1 ` , and it can be many to one merge to ` df2 ` , so a fast implementation is desirable ) .
Okay , this data is a little messy for a dataframe , then . Because it seems like you could have a column ` type ` which will be " university " , " college " , or whatever . And a column ` name ` . In a traditional db , the other fields ( that we don't have names for ) might go in a dataframe of ` ( school_id int , tag string )` . Once you've separated out ` name ` and ` type ` getting the rank is a very simple join . I don't know if you have more complicated ideas in mind .

agggenfreq =d f2000 [[ ' freq ' , ' name ' , ' sex ']] .groupby ([ ' name ' , ' sex ']) .sum() [ ' freq ']

It is . Thanks . In R , we could write something such as grepl ( " date " , colnames ( df )) , which creates a logical index by which we subset . Maybe that same logic does not apply here , although I have seen people do that for selecting rows .
In this case , I might use ` endswith ` : #CODE

Edit : I initially was calling the intersection the union , but have since changed this .
Well , one way to do this is using ` isin ` ( but you can also do it with the ` merge ` command ... I show examples for both ) . For example : #CODE
You can also do this with ` merge ` . Create a unique column in the subset dataframe . When you merge , the unique rows from the larger dataframe will have ` NaN ` for the column you created : #CODE
So select the rows where test == NaN and drop the test column : #CODE
Edit : @USER notes that the merge method performs much better for large dataframes .
I just wanted to add some further documentation for other people reading this ... The latter method mentioned above is MUCH faster than the former for big dataframes . I tried both and didn't get a solution after 10 minutes of sorting through dataframe with 80,000 rows and 5 columns , but it only takes a few seconds for the latter merge method . Thanks again !
Thanks @USER , I edited the answer to indicate the better performance of the ` merge ` method .

Can you possibly minimize the sample ? Perhaps replacing the ` map ` s with a single , looped one , would work . What version of Python ?
@USER I tried to do that , but now I cannot map the functions to the result of split in the code above . The error message says that a float is not iterable ( presumably the elements of the diagnoses Series ? ) . When I check the elements of diagnoses , it seems to be a Series of lists though

That's ok , they're slightly hidden . The doc could do with these examples . ` chunksize ` is a bit of a pain , you have to deal with unevenly-sized chunks . Also preallocate your arrays / dataframes with the fixed size you know you'll need , don't dynamically do concat / append whenever you can avoid it .

Then append / concatenate / update the contents of ' Desc ' in the first row with the contents of ' Desc ' in the second row if the rows are duplicates .
Append a third , fourth , etc . ' Desc ' as necessary .
what will happen to the record data ? drop it ?

pandas dataframe concat with ' NA '
OR if you want concat , when you define ` df_a ` , use columns of df as columns #CODE

python pandas HDFStore : how to append dataframe containing complex numbers

Yet when I would map a lambda function to this Series of lists , I get a ` TypeError : ' float ' object not iterable ` .
To avoid this error , replace the NaNs in ` treatments [ ' DIAGNOS ']` : #CODE

The way you store it should be correct . It's just harder to access data . Instead of ` im =d f_train.Im_as_np [ 0 ]` use ` ix ` to access data : #CODE

Probably the simplest approach is to use the ` resample ` command . First , when you read in your data make sure you parse the dates and set the date column as your index ( ignore the ` StringIO ` part and the header=True ... I am reading in your sample data from a multi-line string ): #CODE
And if you want a month counter , you can add it after your ` resample ` : #CODE

Using ' apply ' in Pandas ( externally defined function )
What am I doing wrong ? I think I'm not really understanding how apply ( and its cousins , aggregate and agg ) works . If someone could explain , I'd be ever so grateful !
You could group by year , isolate the prop column , apply ` argmax ` , and use ` loc ` to select desired rows : #CODE
Note that the use of ` argmax ` and ` loc ` rely on ` df ` having a unique index . If the DataFrame does not have a unique index you'd need make the index unique first : #CODE
Note that ` argmax ` is an ` O ( n )` operation , while sorting is ` O ( n log n )` . Even for small DataFrames , the speed advantage is noticeable : #CODE
Yes , you are right -- there was a mistake . ` argmax ` returns the * label * of the row , not the integer index . So I should have used ` df.loc ` ** not ** ` df.iloc ` .
Note that in 0.14.0 ( releasing shortyly ) , you can bypass most of this with `` nlargest() / nsmallest() `` , rather than explicity sorting ( or using argmax ) , see here ( its down a little bit ): #URL

AttributeError : Cannot access callable attribute ' info ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method
No worries , the given solution still applies , you just have to call it inside of an apply of your groupby : ` df.groupby ( bla ) .apply ( lambda df_grouped : df_grouped.groupby ( level =[ 0 , 1 , 2 ]) .apply ( fancy_func ))`

Why do you drop the indices ? They contain useful information ( that is , what you have grouped over to get your data - most likely you will need this when you want to do calculations based on the groupings :) ) ! #CODE
Thanks FooBar , I really appreciate your help . But do you have an idea how to do that filtering with df_1 . In my real world problem , I need to use the unstack df as there are more complicated comparisons necessary

I'm trying to create a boxplot using Plotly and I get an error when attempting to use a Pandas DataFrame that's been grouped . Some initial digging produced this chunk of code to convert Pandas to Plotly interface : #CODE
Are there alternatives to this method of converting DataFrame's to a Plotly-compatible series ? The above code is for line graphs , but I'd like to iterate over my dimensions to produce a boxplot for each group in my DataFrame . Here is the error message I'm getting :
right , that makes sense . My data are school performance scores grouped by county ( where each county has several schools with corresponding grades ) . I'd like to plot a boxplot for each county ( representing the min / med / max , etc . of the scores in that county ) .
Instead of putting code in the comments , please edit your post . Can you provide a link to the example you mean ? The only plotly boxplot example I find by googling doesn't make any mention of a Series .

Does a pandas dataframe , which is effectively a grid , have the ability to add an item into a specific row and column intersection ?
Yes , use ` loc ` , ` iloc ` , or ` ix ` . Assumming your row index really does start at one ( typically starts with zero in pandas ) you can , for example , do ( for a dataframe called df ): ` df.loc [ 2 , ' B '] = ' x '` . The [ 10 minutes to pandas doc ] ( #URL ) covers this

how to apply Functions on numpy arrays using pandas groupby function

You can use the ` concat ` function : #CODE
I would generally not repeat and / or append , unless your problem really makes it necessary - it is highly inefficiently and typically comes from not understanding the proper way to attack a problem .
I have a data frame which lacks one line for each identifier . I wanna insert this line in it , so what Im gonna do is to repeat this line for N times , and append it to the original data frame , then resort it .

Simply open the output file in append mode : #CODE

Reshaping two-column data using pandas pivot
When you have this column , you can use ` pivot ` : #CODE
@USER I get this error when trying to apply . why might this be ?
values = np.random.normal ( 10 , 2 , len ( date ))
df [ ' set '] = ( df [ ' date '] .diff() < 0 ) .cumsum() #counts each jump back to the first date ( -ve diff = True = 1 )
What version of pandas are you using ? With 0.13.1 the above code works without error . And do you get this with the last step ( the pivot ) ?

in python pandas , how to merge two frames side-by-side ?
is there a way to conveniently merge two data frames side by side ?
or maybe there is some special parameter in append #CODE
possible duplicate of [ Pandas join / merge / concat two dataframes ] ( #URL )
You can use the ` concat ` function for this ( ` axis=1 ` is to concatenate as colomns ): #CODE

You can use ` isnull ` : #CODE

Now I want to read data from database and want to insert specific column values from the database to this dataframe . Since there are 5 columns its easier to do it individually . But i have to extend the number of columns of the dataframe to 70 . For that I am using for loop .

If you constructed an index that is a hash of the full row , you could simply do an ( inner ) join of the two data frames . Of course , building such an index might be too expensive .
Not that elegant , but I think this should work : concat ` all_combinations ` and ` valid ` , and then drop all duplicates : #CODE

I want to apply a weighted sum to a DataFrame . In the past I have used #CODE
I want to apply a weighted average to the sum where the most recent is multiplied by 0.6 , 2nd by 0.2 , 3rd and 4th by 0.1 .
I think you can do it will a ` rolling_apply ` within a function called by a normal ` groupby / apply ` . So something like the following : #CODE

Python : Drop Rows With NaNs When Memory Constrained
I have 3 numpy arrays [ A , B , C ] they all have same number of rows but different number of columns . I need to drop the rows from all arrays if any of the arrays have a nan or inf on that row . I need to use as little memory as possible .
for example , if the first row of A has a nan or inf , I need to drop the first row of A , B , C

I want to insert the name and student count from second into first but based on date column . It doesn't look that difficult , but I am not able to figure out this one .
You can eg use the ` merge ` function ( see the docs on merging dataframes : #URL ) . Assuming your dataframes are called ` df1 ` and ` df2 ` : #CODE
Above it uses the common columns to merge on ( in this case ` date ` and ` sitename `) , but you can also specify the columns with the ` on ` keyword ( see docs ) .

Pandas Merging / Join Dataframes
Well , I'm going to go out on a ledge and show you how to merge with suffixes : #CODE

Sorry , ( now nuked ) comment was wrong , the correct kwarg is ` align ` . ` ax.bar ( locs , vals , align= ' center ')` will put the bars centered on the locations in locs .
If your index consists of strings , I think the code you provided would also not work ? But in that case I think using ` np.arange ( len ( df )) + 0.5 ` instead of ` df.index + 0.5 ` will work .
@USER yep np.arange ( len ( df )) + 0.5 works - terrific ! Thanks so much !
@USER I found that legend for the " net " ( orange line ) is not printed out . If I do this : plt.legend ( loc = ' lower left ') The new legend still consists of price and cost only .. The orange line doesnt exist

@USER Thanks . I'm new to Python Is this documented somewhere ? I could not find it in the Pandas documentation . I did find something about loglog plots in the matplotlib documentation , but I don't know how to apply this to a Pandas dataframe .

It is not updating +1 count as I wanted . I believe it is some kind of type issue , but not sure how to force the type . I am using DataFrame for my data as I want to use group function to split the data and apply the above function . Any suggestions ?
Probably subindexing is fine in this case but you could have used ` loc ` also would work : #CODE

Cut by time in pandas

Merge rows of a dataframe in pandas based on a column
How to merge the rows based on date and sum up the count for the same date . Like in sql #CODE

You could also use ` isinf ` without using it to replace values , e.g. ` x [ ~ np.isinf ( x ) .any ( axis=1 )]` .

I think you can simply using ` groupby ` and ` cut ` to group the data into time intervals . In this example , the original dataframe has 10 days , and I group the days in to 3 intervals ( that is 80 hours each ) . Then you can do whatever you want , take the average , for example : #CODE

but that does not seem to work . I've also tried using the ix and groupby functions , but I can't seem to get either to work . Is there a function that works similarly to the subset() function in R ?

However this results in ` ValueError : Could not construct Timestamp from argument type ' bool ' `
Use indexing instead of apply , it's much faster : #CODE
Explanation in the docs : #URL another example on SO : #URL Probably in this case it won't be a problem ( but eg if you switch ` [ " new_column "]` and ` [ is_null ]` order , it could be a problem ) , but because it sometimes causes problems , and it is not always easy to tell when / when not it will cause problems , it is better to try to use the ` loc ` idiom to prevent this .

Since you've got to do the ` apply ` anyway , I think it is cleaner to move the ` json.loads ` and column creation all into the apply ( don't use the ` converters ` for ` read_csv `) : ` df [ ' field3 '] .apply ( lambda x : pd.Series ( json.loads ( x )))`

your initial assumption is correct : ` df [ df.precip 0 ]` does work . it's your testing that's flawed . when you resample , you're ending up with a bunch of ` np.nan ` values when there is no hourly data for the day . so when you plot it , it looks disjointed .

I want to know how to replace sub-string with pandas , like ' SUBSTR ' in SAS .
By searching this subject , I've known basic syntax of replace() . But I couldn't apply to my specific problem .

One solution is to turn the Series into a DataFrame , join to the grouper DataFrame , then groupby on the columns of the grouper then reselect out the columns of the grouped . I.e. #CODE

From this dataframe , I want to construct a dataframe that details all the money awarded in a single year , for making a boxplot : #CODE
Is there any reason you don't just use the pandas boxplot method ? ` df.boxplot ( by= ' Year ')`
I'd do this in a two-step process : first add a column corresponding to the index in each year using ` cumcount ` , and then ` pivot ` so that the new column is the index , the years become the columns , and the money column becomes the values : #CODE
Awesome response ! Thank you for introducing the dataframe pivot operation --- I guess I should start to RTFM , but SO turns out to be easier and faster in 95% of all cases .

I have an non-empty Excel-file where I want to insert a dataframe .
Everything else in the file is deleted . I want to keep everything . On the specified place , I have left empty rows and columns for that dataframe ... How can I only " append " it to that place , in a way like copy and paste , without deleting everything else ?

If so what would be the best way of reading such evaluation texts ? Would it be best to go through the csv file and replace all " without a comma before or after with ' ? How could this be implemented ?

You can use ` loc ` : #CODE
Although unwieldy using ` loc ` will scale better with larger dataframes as the apply here is called for every row whilst using boolean indexing will be vectorised .
I don't think using loc works , since boolean masking always produces a copy of the DataFrame , rather than a view .
That is only true if the column already exists I think as I get no warning unless the column exists , I did some timings and using ` loc ` method is 1.66ms for a data frame size of 3000 rows versus the apply method which takes 60.2 ms
Notice how there's no z column using the loc method ? ;)
I've added a way you can use ` loc ` now , the key is to do the first assignment to the new column which will produce ` NaN ` s and then you can do the boolean masking again and select the column in ` loc `
@USER you don't need to makes the rows on the rhs side , e.g. you can just do `` df [ ' y ']`` ( as it will align the lhs and rhs ) . Also no need to create z before hand ( and the loc will create it ) . ( this is in 0.14rc1 ; might have been buggy before that )
foo.loc [ foo [ ' Country '] == ' Canada ' , ' z '] = foo.loc [ foo [ ' Country '] == ' Canada ' , ' y ']` with and without using ` loc ` on the left hand side
@USER yes , as always you are right I will update my answer , thanks for the explanation , I thought you needed to mask on the right hand side too , didn't think it would be clever enough to align correctly

Here is an imaginary example , where I want to join df2 to df1 : #CODE
A better method is to build up smaller frames , then do a larger merge . You also might want to do something like this

There appears to be a quirk with the pandas merge function . It considers ` NaN ` values to be equal , and will merge ` NaN ` s with other ` NaN ` s : #CODE
Is there a way to ignore missing values during a merge without first slicing them out ?
You could exclude values from ` bar ` ( and indeed ` foo ` if you wanted ) where ` id ` is null during the merge . Not sure it's what you're after , though , as they are sliced out .
( I've assumed from your left join that you're interested in retaining all of ` foo ` , but only want to merge the parts of ` bar ` that match and are not null . ) #CODE

How do I apply a function to a pandas dataframe ?
I have tried to apply a function to a pandas dataframe like this #CODE
So the first problem in your code is that you are calling apply and setting param ` axis=1 ` this applies your function row-wise which is fine .

What I would like is only one row for each unique combination of the fields used for the index . Right now I get multiple rows for say `' asset subs end dt ' = 10 / 30 / 2008 ` and `' reseller csn ' = 55008 ` if the dummy variable comes up 3 times . I would rather have one row for the combination of index field values with a 3 in the dummy variable column . #CODE
when you do this ` groupby ` , everything with the same index is grouped together . assuming your data in ` EXPERTISE ` is ` notnull ` , you will get a new ` DataFrame ` returned with unique index values and the ` count ` per each index . try it out for yourself , play around with the results , and see how it can be combined with your existing ` DataFrame ` to get the final result you want .

cf.register_option ( ' large_repr ' , ' truncate ' , pc_large_repr_doc ,
validator=is_one_of_factory ([ ' truncate ' , ' info ']))
validator=is_one_of_factory ([ ' truncate ' , ' info ']))

Currently , I am using ` df_sub =d f [ df.ID.isin ( ID_list )]` to do it . But it takes a lot time . ` ID ` s contained in ` ID_list ` doesn't have any pattern , so it's not within certain range . ( And I need to apply the same operation to many similar dataframes . I was wondering if there is any faster way to do this . Will it help a lot if make ` ID ` as the index ?
show df.info() , and the len of ID_list . You might be swapping memory if your frame is really large .
It depends on the size of your data but for large datasets DataFrame.join seems to be the way to go . This requires your DataFrame index to be your ' ID ' and the Series or DataFrame you're joining against to have an index that is your ' ID_list ' . The Series must also have a ` name ` to be used with ` join ` , which gets pulled in as a new field called ` name ` . You also need to specify an inner join to get something like ` isin ` because ` join ` defaults to a left join . query ` in ` syntax seems to have the same speed characteristics as ` isin ` for large datasets .
If you're working with small datasets , you get different behaviors and it actually becomes faster to use a list comprehension or apply against a dictionary than using ` isin ` .
Thank you all for help . I've try out these different methods and get back with results . But can how can I apply Cython to pandas ? It's hard to declare type .
@USER , the link I provided gives a pretty good intro into using Cython with pandas : #URL I would probably try to use join or hash tables before you get into that though . You may not get much gain anyway if you're just looking to do an isin type of operation . It's better if you have a somewhat unique algorithm you need to optimize .

Summarizing the comments , for a dataframe of this size , using ` apply ` will not differ much in performance compared to using vectorized functions ( working on the full column ) , but when your real dataframe becomes larger , it will .
Indeed , I get 201us ( np ) vs 208us ( math ) , so almost the same for this dataframe , but for a larger one ( this one 100 times repeated ) , numpy is clearly faster than using apply .
Also for the concatenation , for this dataframe , using apply is not slower ( even a bit faster 500 vs 700 us ) , but for larger dataframes ( 7000 rows ) it is again clearly slower ( 200 vs 80 ms ) .
Regarding to the performance , I just notice if I use the vectorized functions , I may cause a memoryError ( I have 3G ram ) but apply does not have such a problem . So I think the vectorized functions are reading everything in memory right ? The original file is about 12M in size , above is just a sample section of the file

When using ` apply ` the id generation is performed per row resulting in minimal overhead in memory allocation .
yeah , this way works , but in this thread , #URL it is said the vectorized function is faster than using apply call , and from my experiments it seems true . The vectorized functions tend to use more memory than apply call , but the confusion is that I still have lots of memory left when the memory error occurs
I can reproduce the memory problem . Still investigating further but no luck on faster approaches . Also timed the apply solution and it takes about 5 minutes with 500k rows
Sorry , I can also reproduce it on 0.13.1 , but the issue does not occur in 0.12 or in 0.14 ( released yesterday ) , so it seems a bug in 0.13 . So , maybe try to upgrade your pandas version , as the vectorized way is much faster as the apply ( 5s vs > 1min on my machine ) , * and * using less peak memory ( 200Mb vs 980Mb , with ` %memit `) on 0.14 .
So , maybe try to upgrade your pandas version , as the vectorized way is much faster as the apply ( 5s vs > 1min on my machine ) , and using less peak memory ( 200Mb vs 980Mb , with %memit ) on 0.14

Can I apply a function that uses ' shift ' on a grouped data frame , and return a simple data frame from pandas ?
I hope the subject line is relatively clear . I'm using python pandas , and I'm working with daily pricing data on equities . I have one large csv file with data on 4000+ symbols , with approximately 100 days ' data . So there are many repeated date and symbol values , but symbol date combinations are unique . I'm trying to get percentage change on each ticker date combination , for multiple lag ( shift ) dates . On a dataset of one symbol , this would be as simple as #CODE

I need to create group every 5 minutes from starting from ` 16:00 : 00 ` . That is all the rows with in the range ` 16:00 : 00 ` to ` 16:05 : 00 ` , its value of the new column ` period ` is 1 . ( the number of rows within each group is irregular , so i can't simply cut the group )
You can use the ` TimeGrouper ` function in a ` groupy / apply ` . With a ` TimeGrouper ` you don't need to create your period column . I know you're not trying to compute the mean but I will use it as an example : #CODE
Or an example with an explicit ` apply ` : #CODE
It works because the groupby here with as_index=False actually returns the period column you want as the part of the multiindex and I just grab that part of the multiindex and assign to a new column in the orginal dataframe . You could do anything in the apply , I just want the index : #CODE
Thanks ! I did it in a silly way by looping over groups then add the column to groups , and last concat the groups ...
Yes , you can do it in an apply . Just do a groupby in a function that the first groupby / apply calls
Depending on what your doing if I understand the question right can be done a lot more easily just using the resample method #CODE

What's the approved way around this ? Do I have to copy the ID's into new columns , replace the values in those columns with the dict values , and then work from there ? I'm guessing there's a better way ....
If I understand you correctly then you can simply call ` map ` : #CODE
I'm not sure you can , this is probably the most efficient way as merging will copy the source dataframe I think and map on a series is highly optimised as it is written using cython . If you are happy with this answer then you can upvote and accept it .
@USER you could use a function and apply it to the dataframe but I'm not sure you would save much time , it would depend on how many unique ids there were , once you have created the dicts then using ` map ` is really fast
Thanks , Ed . The map solution did word , and it's fast , so I'm happy .

The above answer is pretty much right on , but wanted to mention another workaround that I've used in this situation . Basically , just treat the query string like you would any other string where you'd want to insert a variable . #CODE

Python - pandas - Append Series into Blank DataFrame
I can create a DataFrame with just h and then append g to it : #CODE
But now suppose that I have an empty DataFrame and I try to append h to it : #CODE

It's a reasonable way to do it . You could change the apply a little to only return a ` ranks ` Series . That would allow you to just assign a new ` ranks ` column to the original dataframe as the result of the ` groupby / apply ` . But your way works just fine .
@USER . I tried to only return rank series ` dec ` , the groupby will append the whole series to each row-column ( cell )

replace the values using the round function , and format the string representation of the percentage numbers : #CODE

I am struggling to set xlim for each histogram and create 1 column of graphs so the x-axis ticks are aligned . Being new pandas , I am unsure of how to apply answer applies : Overlaying multiple histograms using pandas . #CODE
I have tried the following , but the xlim edit on the subplot does not seem to translate the bin widths across the new x-axis values . As a result , the graph now has odd bin widths and still has more than one column of graphs : #CODE
The layout option is very helpful . However , the bin setting seems to only apply to the range of data not the entire interval that we display using xlim . For example , say that I'd like to bucket the counts over [ -1 , 1 ] with a total of 10 buckets ; then the values from 0 to 0.2 should be in a single bucket , but that's not the case with bins=10 . Any idea why not ?
Yeah , you can use both , sort of like the ` breaks ` argument in ` R ` ` hist ` .

add how you created the dataframe . but some hints : play around with the functions ` groupby ` , ` unstack ` , and maybe ` sum `

jeff , i getting the output i want . I just want to see if another way exist with pandas , like apply or shift . I tried but i couldn't figure out

I was going to use Pandas to do the diff .
But diff also performs on the timestamp column and I can't get it to ignore a column while
Why did you set ` drop=False ` ? That puts the timestamps in the index ( where they will not be touched by ` diff `) but also leaves a copy of the timestamps as a proper column , to be process by ` diff ` .

Can Pandas be made not to return NaT and if not how could I check against them as I will have to replace them in the list .

I am new to stackoverflow and pandas for python . I found part of my answer in the post Looking to merge two Excel files by ID into one Excel file using Python 2.7
However , I also want to merge or combine columns from the two excel files with the same name . I thought the following post would have my answer but I guess it's not titled correctly : Merging Pandas DataFrames with the same column name
Are the values for ' test ' column same in both excel files ? Are the number of rows and IDS the same from both excel files ? If the former then you can just drop one of the columns and rename the remaining column , if the latter then you can perform a merge without passing ` how= ' outer '` as this will default to inner and will merge on ids that are present in both
Then just to finish off drop all the extra columns : #CODE
Thanks for the answer . This would work if I was working on simple data such as in my example , but as I said in the last paragraph there are ** 200+** columns with the same name ( i.e. test1 , test2 , ... test200 ) in df1 and df2 that I want to merge into one file . I wouldn't know the names of these columns ( the true names of the " test " columns is unknown ) before hand to be able to conditionally select column values and drop extra columns .

Also , how do you join this back to original dataframe ?
We can resample this to days ; it'll be a much longer timeseries , of course , but memory is cheap and I'm lazy : #CODE
How do I merge the birth rate back to the original table ? Indexes aren't compatible ... Turns out size isn't such an issue .

And then I stack the dataframe : #CODE

In this case it would be the last three rows since C can either map to 13 or 14 .
I tested the g.transform ( lambda x : len ( x.unique() )) , works good but is slow especially when there are a lot of groups . The code below works much faster so I put it here .

however , this doesn't insert newdate into the newRow of the DataFrame as the index of the appended record . How this can be done ?

In the timestamp sequence , weekends and some other weekdays are not present . I want to resample my time index to have the aggregate sum of volume per minute . So I do the following :
You could also start with a constructed business day freq series ( and / or add custom business day if you want holidays , new in 0.14.0 , see here

Use ` loc ` for label indexing : #CODE

Once you understood what happened here , I'm sure you can apply this to find the maximum of ' t1 ' .

Udate : If you don't want to depend on the column order , you can also specify the values to use to fill for each row ( like ` .fillna ( value =d f [ ' D ']`) . The only problem is that this only works for Series ( when it is a dataframe , it tries to map the different values to fill to the different columns , not the rows ) . So with an apply to do it column by column , it works : #CODE

You're really asking about contigiuous groups ( rather than the standard groupby , which ignores whether items in the same group are neighbouring ) so I think you need to use diff : #CODE
Note : the NaN is converted to True ! also the ` astype ( bool )` isn't strictly necessary if you can guarantee that each group is non-empty and increasing by each time .

Pandas replace non-zero values
I know I can replace all nan values with ` df.fillna ( 0 )` and replace a single value with ` df.replace ( ' - ' , 1 )` , but how can I replace all non-zero values with a single value ?

look up the ` resample ` function ( #URL )
Just ` resample ` . Note that the ` NaT ` are currently a bug ( in 0.14.0 ) , so you need to drop them first . #CODE
you would need to resample these columns separately then `` concat `` them as they could have different ranges , something like : `` concat ([ df.set_index ( col ) .resample ( .... ) for col in columns_that_i_want ] , axis=1 )``

I've tried the various possible syntaxes , ` .div ` and ` / ` and referencing through [ ] as well as ` .name ` , it's all the same . Dimensions fit , but it seems to append all the columns to be divided to each other , creating the second number , which is of course larger by a factor than the column that it then tries to divide by . What am I doing wrong ?

How to inset a new row at given position in pandas DataFrame and shift indices below +1
How to insert a new row with A=9 and B=9 at position with index=2 and shift indices below +1 ? The result is #CODE
There may be more efficient ways , but here is one approach - taking slices of the head / tail around the point you want to insert values , and stacking it all together with pd.concat . #CODE

Pandas time series resample
and I'd like to replace the datetime column with a different datetime column that has additional values in it , and obtain something like this #CODE

I am looking for the correct logic to combine two columns with related data from an .xlsx file using pandas in python . It is similar to the post : Merge 2 columns in pandas into one columns that have data in python , except that I also want to transform the data as I combine the columns so it's not really a true merge of the two columns . I want to be able to say " if column wbc_na has the value " checked " in row x , place " Not available " in row x under column wbc " . Once combined , I want to drop the column " wbc_na " since " wbc " now contains all the information I need . For example : #CODE
You can use ` loc ` to find where column ' wbc_na ' is ' checked ' and for those rows assign column ' wbc ' value : #CODE
Thank you , this answer seems to give the result I was looking for . So the loc method is basically returning the row # correct ?
` loc ` performs label based selection , the [ online docs ] ( #URL ) are worth looking at to understand the semantic differences between ` loc ` , ` ix ` and ` iloc ` , by the way you should have enough reputation to upvote now ;)

I was wondering if the CPU tempreature is getting too high , and it's a protective mechanism . The parallel in my program is essentially cutting huge data into small pieces and have independent operations for each piece . Finally join them . And BTW , ` multiprocessing ` doesn't work in python .

does the trick . ` s.map ( len )` applies ` len() ` to each element and returns a series of all the lengths , then you can just use ` sum ` on that series .

I have at hundreds / thousands of arithmetic operations w pandas dataframes . I can't switch to eval in every point and in any case , this was an example -- at other times , they are parts of different frames , etc .

But is there a better way to do this as I need to duplicate holiday rows by 5 times , and I have to append 5 times if using above way .

Pandas replace function & " Returning a view versus a copy "
I'm trying to use the replace function on my data frame , and I know that the problem is that I'm modifying a copy and should use ` .loc ` or ` .ix ` to return a view instead of a copy , but I'm a total beginner and struggled a bit with the pandas documentation on this .

` Cannot access callable attribute ' reset_index ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method `
Thanks a lot ! My fault was , that I did not realize , that I have to apply some function to the groupby dataframe , like ` .size() ` , to work with it ... #CODE

You could drop the ` NaN ` rows if they are surplus but I would think you would want to keep the rows as you have a different IP address .
Do want to overwrite the csv or append the new entries ?

If you want it back in the origal form , you have to ` stack ` it back ( the ` reorder_levels ` is needed because of ` stack ` inserts the ' stations ' after the existing row index , while we want it as the first level of the row index ): #CODE
Do you also have duplicate datetime values within the values for one station ? ( this could indeed trigger this warning , as you cannot use unstack with duplicate entries ) If so , is this needed ? ( you can always check with ` data.index.get_duplicates()
You can feed that label ( the tuple in this case ( multi-index )) to drop : ` data.drop (( ' Armamar3 ' , pd.Timestamp ( ' 2010-05-09 11:30 : 00 ' , tz=None )))

pandas groupby X , Y and select last week of X1 and X2 ( which have diff frequency )
Then you can select the rows you want in an apply call on the grouped object : #CODE
If you can't upgrade or don't solve the issue you have with 0.14 , you can try to use ` ix ` instead of ` iloc `

There are probably a few ways to do this , but one approach would be to merge the two dataframes together on the filename / m column , then populate the column ' n ' from the right dataframe if a match was found . The n_x , n_y in the code refer to the left / right dataframes in the merge . #CODE
If you have one large dataframe and only a few update values I would use apply like this : #CODE
If you want to do this for the whole table , I suggest a method I believe is superior to the previously mentioned ones : since your identifier is ` filename ` , set ` filename ` as your index , and then use ` update() ` as you wanted to . Both ` merge ` and the ` apply() ` approach contain unnecessary overhead : #CODE

OK , first problem is you have embedded spaces causing the function to incorrectly apply :
So you can call ` replace ` instead of calling ` apply ` : #CODE
you can also use ` factorize ` method also .
you can use `` series.replace ( dict )`` I believe to do the substitution , then `` convert_objects ( convert_numeric=True )`` to change to float ( forcibly ); you can also `` factorize `` to make categoricals ( e.g. map the strings to numbers )
@USER so is ` replace ` faster than calling ` map ` or ` apply ` and passing a dict now ? Wasn't aware of ` factorize ` also , when was this introduced ?
replace should be much faster ; `` factorize `` has been their quite a while ( but not advertised :))

Is this file ending common enough that there's a built in option to ignore it in pandas ? If not , any suggestions for not reading the EOF character into the dataframe or is the best way to manually test for it and drop the row after reading ?
My tentative solution is to apply this function after reading : #CODE
if this is consistent , you could modify your script to check for NAN on the last line and then remove it : ` df [ : len ( df ) -1 ]` In the API docs i only see the ability to change the lineterminator which won't fix your problem .

if you use an index its okay . ` df [ ' new '] = pd.Series ([ 0 for x in range ( len ( df.index ))] , index =d f.index )` .
also , a list comprehension is entirely unnecessary here . just do ` [ 0 ] * len ( df.index )`
The reason this puts ` NaN ` into a column is because ` df.index ` and the ` Index ` of your right-hand-side object are different . @USER shows the proper way to assign a new column of zeros . In general , ` pandas ` tries to do as much alignment of indices as possible . One downside is that when indices are not aligned you get ` NaN ` wherever they aren't aligned . Play around with the ` reindex ` and ` align ` methods to gain some intuition for alignment works with objects that have partially , totally , and not-aligned-all aligned indices . For example here's how ` DataFrame.align() ` works with partially aligned indices : #CODE

Can you include the entire stack trace ? ( if you could find a small repoducible example that'd be great )

Any ideas on how to get this file to load ? Unfortunately I can't just strip out accents , as I have to interface with software that requires the proper name , and I have a ton of files to format ( not just the one ) . Thanks !

But if your ` DataFrame ` always looks like the one shown here , which is like a upper diagonal matrix without ` 0 ` s inside , you can just take a short cut : #CODE

Thanks for your answer . I edited my question , any way to accomplish the way I asked now ? Also , I'd rather drop the columns that I no longer need than select the ones I do need , since my real data has 400+ columns .
@USER : Not sure what you mean by " the column headers are not always clear in what they contain " . Can you not just * replace * the column headers with row 0 ? It's generally awkward to have individual rows with special meaning ; you want all rows to have the same format .
Now you can use an apply with zip : #CODE

The first quartile can be calculated by taking the median of values in the dataframe that fall below the overall median , so we can see what data.quantile ( 0.25 ) should have yielded . e.g. #CODE
I thought this may not be a " NaN " issue , rather it might be quantile failing to handle even-numbered data sets ( i.e. where the median must be calculated as the mean of the two central numbers ) . However , after testing with dataframes with both even and odd-numbers of rows I saw that quantile handled these situations properly . The problem seems to arise only when NaN values are present in the dataframe .
I have applied dropna ( how= ' all ') , which makes my example misleading . However , I am not willing to apply dropna ( how= ' any ') , since I do not want to lose valid data just because a NaN sits in the next column over . Your suggestion is good . That being said , there still seems to be a fundamental problem with quantile ( or so I think ! ) .
The same as the median if ` q=50 ` , the same as the minimum if ` q=0 `

the order within group apply function
order is preserved within a group and to the subframe that is passed to apply or a reduction function . you should show what you are doing and why this matters .
@USER it matters for apply functions like x - x.shift ( 1 ) . If order is not preserved I may get wrong answer .
If you are using apply not only is the order not guaranteed , but as you've found it can trigger the function for the same group a couple of times ( to decide which " path " to take / what type of result to return ) . So if your function has side-effects don't do this !

I am returning data from a database query and want to create a new column in the resulting dataframe . I need to shift the results of one column forward one month to create a new column .
If the index is not meaningfull , you can just do ` df.reset_index ( drop=True )` and you will get a new integer range ( 0 .. len ( df )) index . Otherwise maybe show how you made up the dataframe .
So I was able to fix it by adding ignore_index=True into the dataframe append statement . Instead of df.append ( loop_df ) I changed it to df.append ( loop_df , ignore_index=True )
And then to get rid of the duplicate values ( if you don't need to keep the original index ) , you can eg do ` df.reset_index ( drop=True )` , or you can use ` ignore_index=True ` in ` append ` or ` concat ` .

you are chain indexing , see here : #URL ; use ix / loc

Elegant and efficient way to repeatedly merge dataframes into single column of a dataframe
I have a set of dataframes containing key , value pairs ( as columns ) , where each has a subset of the keys . I'd like to merge them all together into a single value column of an initial dataframe with keys from the same key space ( with additional columns to be preserved ) .
As pointed out by @USER , this is not a Pandas bug , but the intended behavior . When assigning a Pandas object ( e.g. a DataFrame ) to a ' slice ' of a DataFrame ( e.g. returned by .loc as above ) , Pandas attempts to ' align ' the indices of the two . That is , it assigns the rows on the RHS to rows on the LHS by their Pandas index .
The reason the answer of @USER -zhu works is that accessing the .values attribute of the RHS returns a Numpy array , which has no notion of a Panda-style index . When Pandas receives such an object on the RHS , it does the assignment without trying to align indices ( - which works since my example pre-arranged for the two to be in the same order via sorting )
Base on what you just described , should it be a classic case of ` merge ` ? For your real case data : #CODE
the above results in 83 rows , but I need the full 3121970 rows of df as I want to do this repeatedly with thousands of ' common ' dataframes , rather than just two ( df2 & df3 ) as in the toy example . While with only 2 dataframes I could just use merge ( ..., how= ' left ') , doing that repeatedly doesn't combine the value column , it just adds additional columns value_x , value_y , ... A way to combine these via selecting the non-NaN value would work ( - explicit iteration is too slow )
Very helpful update . Is it feasible to ` concat ` all the ** common_i ** dataframes into one first ? Or in real application the ** common_i ** dataframes will come in real time and the new data need to be added into the dataframe every time a new ** common_i ** comes in ?
Indeed , since ` common ` doesn't contain keys not in ` df ` , it also works as ` = common.ix [: , ' value '] .values ` or even using ` loc ` instead of ` ix ` . It may indeed be a Pandas bug ( I'll file a report ) , but your solution works ! Thank you @USER -zhu ! In my actual use-case , the common dataframes are from a dataset much too large to fit into memory and have been split , but the dataset from which the df is drawn has been split to fit into memory and doesn't need to be re-combined . I'm essentially trying to do an on-disk left join , but without needing to re-combine the left table from its partitions .

and I would like to merge all of the columns containing similar names in to the same column .

Typically anything that's important will be a documented option . In this case , the only option available in kwargs is ' append , ' which is a deprecated alias to ' if_exists . '

When I write Pandas DataFrame to my SQLite database using to_sql method it changes the ` .schema ` of my table even if I use ` if_exists= ' append '` . For example after execution #CODE

Apply styles while exporting to ' xlsx ' in pandas with XlsxWriter
When using the pure XlsxWriter I can apply formats to cells what also works nice .

I'm hoping there's a nice optimized method from pandas like resample that can do this in a couple lines .
look up ` resample `
This reads the file that had the data originally posted - then creates a multi-index off of the stock_id and the end_date . The get_val function below takes the entire frame , a ticker e.g. ' AAPL ' , and a date and uses index.searchsorted which behaves like map :: upper_bound in C++ - i.e. finds the index where the date would be inserted if you wanted to insert - i.e. find the enddate closest to , but after the date in question - this will have the value that we want and we return that with get_val .
Then I grab a cross section from a Series with this Multiindex based on the stock_id of ' AAPL ' . Then we form an empty list which will be used to flatten the list of tuples of dates from the multiindex with the key of ' AAPL ' . These dates become the index and values of a series . Then I map this series to get_val to grab the stock price that is desired .
Understood . It is to do that from here , all we need to do is sort , resample and fill the series . Let me change the answer .
Based on this new dataframe , I can group it by ` ticker ` and ` row ` , and apply a daily ` resample ` on each of these groups and ` fillna ` ( with method ' pad ' to forward fill ) #CODE
The last command was to drop the now superfluous ` row ` index level .
Yes , an easy solution is to add ` how= ' first '` in the ` resample ` call ( the default is ` mean ` , which does not work with non-numeric values ) . The result will be the same in your case . But apart from that , it is not really recommended to have such a mixed dtype ( also other operations you could want to do will have this problem )

append data in a loop python base condition

then for sure you want to use `` table `` format , as you can append and even store it in chunks ; `` fixed `` is somewhat faster but you cannot append , nor query at all . read docs : #URL ( and cookbook is a link )

Passing the function ` pd.Series.mean ` directly to ` resample ` works - for some data , but trips up , for example if a sampling bucket has no values ( e.g. on minutes ` T ` above ) . I expect that's why it's better to pass `' mean '` and let Pandas do the right thing . Only `' mean '` appears not to select a suitable function in this case .

checked the type of dt :
` type dt class ' pandas.core.frame.DataFrame ' `
@USER : Happy001 asked to see the dtype of ` realtime ` . You showed the * type * of what you get when you select a column from a groupby object ( ` SeriesGroupBy `) , and the * type * of an unrelated Series after you apply ` pd.to_datetime ` to its elements , which by construction is ` Series ` . So far , nothing you've shown is incompatible with the error message ` pandas ` gave , which says that you're trying to subtract a ` timedelta ` from a ` unicode ` string . Instead , look at ` df [ ' realtime '] .dtype ` , and ` df [ " realtime "] .apply ( type )` .

python drop non-integer rows , convert to int
Is there a simple way to drop rows containing a non-integer cell value , then / and convert strings to integers , then sort ascending ? I have dataset ( single column of what's supposed to be just record numbers ) that has strings that I want to remove . This code seems to work , but then sorting seems to sort as if " float " is " string . " For example , the record numbers are sorted like so : #CODE
Is there a way to do this without converting to float first ? I'd like to drop the numbers that don't fit into int64
Is there a way to just drop non-integers from the original input df1 rather than making the new series ?

` freq not specified and cannot be inferred from first element ` Do you know how do I specify frequency ? I tried to add ` freq= ' Q '` , but doesn't work .

Replace word w.r.t word in another column using Levenshtein distance
Column ` C ` is my expected output . I need to compare each word in strings of column B with the word in A and replace it if Levenshtein distance is 1 .
You have asked two questions in one which is never a good idea on Stack Overflow . I'm just going to reply to your first question , if you want someone to look at your second problem then I suggest you write a new question specifically for it .

I can see that I could pre-process the file to strip out the commas - I'd like to avoid that if possible but would welcome suggestions if this is the only way .

I think the easiest way to plot this data with all the lines on the same graph is to pivot it such that each " template " value is a column : #CODE

I think the more pandonic ways are to either use ` resample ` ( when it provides the functionality you need ) or use a ` TimeGrouper ` : ` df.groupby ( pd.TimeGrouper ( freq= ' M '))`

python pandas drop value from column ' B ' if that value appears in column ' A '
This code appears to drop duplicates from ' A ' , but leaves ' B ' untouched : #CODE
But I still can't see how to remove duplicates in ' B ' ( or return unique values in ' B ') . The two columns are actually coming from separate CSV files . Should I not join them into a single DataFrame ? Is there are way to compare and drop duplicates if I don't ?
If you can cut it down to a minimum failing test case , we can probably figure out what it is . ( Equally likely , you'll figure out the problem while doing so . )

How do I join table and field names in a pandas function ?

You can do it by using concat to make a new dataframe and plotting that , though I think you'll have to rename one of the columns . #CODE

to_sql is using insert sql method and this is so slow than copy from sql method .
You are correct ` to_sql ` is using ` INSERT INTO ` via sqlalchemy ( code where this happens is here ) , and so naturally you cannot use ` COPY FROM ` using ` to_sql ` .

How to apply OLS from statsmodels to groupby
So how can I go through my dataframe and apply sm.OLS() for each product_desc ?

@USER , ` clip ` , or ` .clip_xxx ` methods are actually slow in this case , only direct calculation is able to be slightly faster than the boolean indexing version .
Note : If you are concerned about speed then it would make sense to transpose the DataFrame , since appending columns is much cheaper than appending rows .
You can append a row using loc : #CODE

Merge multiple dataframes with non-unique indices
I don't believe that does what I want . It seems to concatenate the individual series sequentially , while I want a merge . Please see my edited post for a better example .
This is very nearly a concat : #CODE
However , concat cannot deal with duplicate indices ( it's ambigious how they should merge , and in your case you don't want to merge them in the " ordinary " way - as combinations ) ...
Note : I can't seem to append a column to the index without being a DataFrame .. #CODE
Now we can go ahead and concat these : #CODE
If you want to drop the cumcount level : #CODE
Note2 : This is pandas 0.14 , in 0.13 you have to pass a numpy array to ` _cumcount_array ` e.g. ` np.arange ( len ( s0 ))`) , pre-0.13 you're out of luck - there's no cumcount .
This works well . I don't think I could have come up with this myself :-) . It's much more slower than a Cython program to do the same though . I think the kind of merge I want is specific enough that I will have to do it in Cython if I want speed for big time series . I learned quite a bit about Pandas from your post . Thanks !
@USER -- ok , I guess I made it optional to pass an output array in 0.14 , so pass something like : ` np.arange ( len ( s0 ))` ...

Consider resample by ' M ' rather than grouping by attributes of the DatetimeIndex : #CODE
Note : you have to drop the NaN if you don't want the months in between .

If it feasible for you to replace ` { ` with `" { ` , and ` } ` with ` } "` , it can be read correctly by : ` pd.read_csv ( ' data / training.dat ' , quotechar= '"' , skipinitialspace=True )`

What is problematic for me is how to translate this statement into pandas in a way that is fast and idiomatic and etc .
I've already tried for one-day retention by simply creating a lagged column and merging the original with the lagged dataframe . This certainly works . However , for seven-day retention I would need to create 7 dataframes and merge them together . That's not reasonable , as far as I'm concerned . ( Especially because I'd also like to know 30-day numbers . )
( I should also point out that my research led me to #URL , which indicates a merge behavior that does not work on my install ( pandas 0.14.0 ) which fails with error message ` TypeError : Argument ' values ' has incorrect type ( expected numpy.ndarray , got Series )` . So there appears to be some sort of advanced merge / join behavior which I clearly don't know how to activate . )
If I understand you correctly , I think you can do it with a ` groupby / apply ` . It's a bit tricky . So I think you have data like the following : #CODE

bypass read_csv and apply to_datetime after :

I think the problem is ` apply ` expects to return the same number of rows as the input .
You could also do it with a ` groupby / apply ` since it is more flexible . So something like the following : #CODE
Or could iterate over the rows concat

pandas vectorized string replace by index

What " merge " operation do you want performed ? Sum ?

You can use the pandas groupby-apply combo . Group the dataframe by " Item " and apply a function that calculates the process time . Something like : #CODE

You can do it for each column seperate and then concat the results : #CODE
If you don't want the multi-index column , then remove the ` keys= .. ` from the concat function call .

@USER still I think BrenBarn is correct . If you are just updating existing data then the performance hit may not be an issue , it sounds like all you'd be doing would some stats on the updated values , note that groupby itself does nothing only when you apply a function does it do something . If you know which group is to be updated then you can call ` get_group ( ' updated_item )` can call apply on just that group see : #URL

I'd like to append phstab on each iteration so so the output is just one long dataframe instead of four instances . I tried inserting the following statement in the loop but it didn't work #CODE

How do I to translate this json format into correct format that can be used pandas read_json()

How to reindex to merge two dataframes ?
I am trying to merge two dataframes that both have a ' product_desc ' column . I am using Pandas 0.13 and Python 2.7 . #CODE
I exported both dataframes to flat files and there are no duplicate values for the indexes or other columns in either . What do I need to do so these two dataframes will merge ?
@USER Yes , you are correct on get_duplicates . There were no duplicates when I had the correct function . However , I still get the above error message when I try to merge .
It might help to have a full stack trace of the error in your question . I have no idea what goes wrong - but for a start you could try to subset your data and see if the merge works .
The merge function takes four arguments : dataframe 1 , dataframe 2 , left_on = " dataframe 1 column " , right_on = " dataframe 2 column which matching values in dataframe 1 "

Now I want to read them and merge into one big DataFrame . I tried it by using #CODE
When I read the 3 working files and merge them ; and read D11 independetly , the line #CODE

I saw some examples here at Stack Overflow showing how to write a simple model for that purpose : #CODE

When considering calculations involving subsequent and preceding rows you should consider using ` shift ` , this is what is designed for
Ah OK , in that case you want to look at [ ` resample `] ( #URL ) , something like ` data.CH.resample ( ' 5min ' , how= ' diff ')` not sure if it should be ` diff ` or ` sub `

Calculate during merge using pandas

Python Pandas : Get row by median value
I'm trying to get the row of the median value for a column .
I'm using data.median() to get the median value for ' column ' . #CODE
How can get the row or index of the median value ?

You could use the ` apply ` method : #CODE
Thanks !! I found my stupid mistake while using apply() . I did apply ( wordnet.synsets() )

For example , say that I know what slices I want to apply on each level name , e.g. as a dictionary : #CODE

When I try to pass the ` if_exists= ' replace '` parameter to ` to_sql ` I get a programming error telling me the table already exists : #CODE
From the docs it sounds like this option should drop the table and recreate it , which is not the observed behavior . Works fine if the table does not exist already . Any ideas if this is a bug or I'm doing something wrong ?
Strange , can you show ` pd.__version__ ` ( just to be sure it's not picking another version of pandas , as this was a bug ( with replace ) in 0.13 and older , but that should be fixed now ) . Also , can you show ` pd.io.sql.has_table ( ' foobar ' , engine )` and the full error traceback ?

How to merge two columns together in Pandas
dataframes have ` join ` and ` merge ` methods . either of those will work .
you could use an ` apply ` statement to select the values from the correct columns .
Use the ' data ' variable . That is right after you read_csv . Also i hope stack overflow has been useful to you -- be sure to accept the answers to your previous questions ( including this one ) if it solved your problem .

Of corse we can get around this problem using some data wangling techniques like ` transpose ` and ` slicing ` . I am wondering there should be a quick way in API ?

I now would like to combine the ' Day ' and ' Hour ' columns into one ' Date ' index column . I did a lot of searching and so far I have only seen solutions that are based on pd.read_csv and pd.read_table . However , as this is a series ( not a dataframe / csv / excel ) , these solutions do not seem to apply .
You can add them ( if you first multiply the hours by the number of nanoseconds ) , but you have to drop down to numpy to do the calculation* : #CODE
* Apparently Index overrides the ` + ` operator to make it append ...

So I am trying to merge the following columns of data which are currently indexed as daily entries ( but only have points once per week ) . I have separated the columns into year variables but am having trouble getting them into a combined dataframe and disregard the date index so that I can build out min / max columns by week over the years . I am not sure how to get merge / join function to do this .
#Create year variables , append to new dataframe with new index
Finally , do a pivot table to transform and get row-wise ` max / min ` : #CODE
I think because the pivot table is indexed by week , it cant be called by column . Thanks again for all your help , are you a friend of Sankalp ?

Now we can use ` value_counts ` and then ` concat ` to get the current pieces : #CODE

I am having trouble finding a way to keep only the lines containing the first occurrence of a ' value ' . I want to drop duplicate ' values ' , keeping the row with the lowest ' Date ' .The end result should be : #CODE

use `` apply `` ONLY as a last resort ( e.g. you can't do vectorized things ) . even if you have a very complicated function to do , you can often do vectorized calculations on most of it , saving the last for `` apply `` , which is essentially a loop .
Using apply took 172ms versus 39ms using Jeff's method , I can also confirm that it made negligle difference whether the apply was called inside or outside the function but it does modify the df so you didn't need to return the df as it was being modified inside the function
@USER : thanks for your clarifications once again , very glad you helped me with it . As a new user like me to python / pandas , the problem mostly is , that I can only search / google for solutions as the libraries contain so many classes and functions that I don't know what to look for ( in this case the ` DatetimeIndex ` class ) . And then sometimes different solutions ( in this case using ` apply `) come up on google / stackoverflow and yet again I can NOT verify that there is no better solution as I dont have the insight into the library . But I keep learning heavily each day , thanks :)

You could ` merge ` DataFrames - it is similar to ` join ` in SQL #CODE

On dates where data only exist in one column , I'd like this value to be placed in my new column . On dates where there are entries for both columns , I'd like to have the mean value . ( I'd like to join using the index , which is a datetime value )
Edit2 : I written some code which should merge the data from both of my column , but I get a ` KeyError ` when I try to set the new values using my index generated from rows where my first df has values but my second df doesn't . Here's the code : #CODE
Look at this answer to merge [ in case you need to add suffixes in cases of similarly named columns ]: " #URL , Now read up on here : " #URL To figure out how to select certain indexes . What you will want to do after the merge , find rows where a value is missing using variations of df [( df [ ' colA '] .isnull() == True ) & ( df [ ' colB '] .isnull() == False )] , and set the value if missing . Then take the mean across colA and colB

Pandas : apply a function to a multiindexed series
Now I want to apply any function to each series indexed by numbers.hash only , e.g. summing the values in each time series that is made up of local_time and the value . I guess I can get the number.hash indices and iterate over them , but there must be a more efficient and clean way to do it .
Or groupby and apply an arbitrary function #CODE

Rather than telling us about the problem , show it to us : edit your question to give an example we can copy and paste . First , find the minimal failing case . Does it still fail for ` a [ 1:400 ]` ? For ` a [ 1:100 ]` ? ` a [ 1:5 ]` ? Note that if you have data in the list you can't show us , then replace the strings by " A " and floats by 1.0 , etc ., until you come up with an example you * can * post .

I have a time series excel file with a tri-level column MultiIndex that I would like to successfully parse if possible . There are some results on how to do this for an index on stack overflow but not the columns and the ` parse ` function has a ` header ` that does not seem to take a list of rows .
My best idea so far is to use ` transpose ` , but the it fills ` Unnamed : # ` everywhere and doesn't seem to work . In Pandas 0.13 ` read_csv ` seems to have a ` header ` parameter that can take a list , but this doesn't seem to work with ` parse ` .

@USER hmmmmmm good point , I was thinking the same about the loc ambiguity ( if they are labelled with 0 or 1 ... maybe best not to use at all ?
Sorry , my mistake . As long as ` msk ` is of dtype ` bool ` , ` df [ msk ]` , ` df.iloc [ msk ]` and ` df.loc [ msk ]` always return the same result .
@USER , in your example , if I change 0.8 to 0.2 I get ` len ( train )` equal to 59 and ` len ( test )` equal to 41 .

However , now I would like to go back through and iterate through the keys for each Accession code and run a function I've defined as find_match_position ( reference_sequence , 13mer ) which finds the 13mer in in a reference sequence and returns its position . I would then like to append the position as a value for the 13mer which will be the key .

Join all the columns into a new one : #CODE
and then drop the other columns you don't need ( see also here : Delete column from pandas DataFrame ) , or just use the series
Ok I tried this and it mostly works . I just need to replace the yes / no in the smoking column with smoking / nonsmoking . Thanks .

In fact you should be able to simplify it further : ` x.loc [ rows , ' price1 '] = x [ ' price '] / x [ ' amount ']` it should align corretly without the mask on the rhs
Okay thanks , yeah I'm using 0.14 currently and it's not displaying a warning at all . I'm still surprised on how it works on a big dataset when converting to float64 before dividing . I know now that using ` loc ` is better to use anyhow but still ... as a new user without the insight into all functionalities , this might be very very confusing ( as it was for me ) that it works for some number and for some not . So either don't offer the possibility of chain assignment at all or , when offering it , fix it so that it works correctly as expected I'd say .

I've written the following code to find gaps in columns , generate a list of indices of dates where these gaps appear and use this list to find and replace missing data . However I get a ` KeyError : Not in index ` on line 3 , which I don't understand because the keys I'm using to index came from the ` DataFrame ` itself . Could somebody explain why this is happening and what I can do to fix it ? Here's the code : #CODE
So you should use ` loc ` : #CODE
note that it is not necessary to use the same index for the rhs as it will align correctly

pandas - using the ' melt ' function to reshape a table
I think melt can take me halfway there but I'm having trouble getting the final output . Any ideas ?
looks good to me . Also you can put site , returnperiod and peril into the index and then unstack peril ....

Removing duplicate columns from a pandas dataframe : behavior of transpose + drop_duplicates
The 2nd form does work ( ` df.T.drop_duplicates ( inplace=True )`) , but it is operating on a copy ( the transpose itself doesn't copy , but the ` drop_duplicates ` DOES ); so it is modifying a copy that you then don't have a reference .

But I can't work out how to merge them together into the format at the top .

Note : you have to use TimeGrouper to groupby months ( just like you would in the resample ) .

gives " AttributeError : rint " if I insert values and then apply np.round() . If I copy df.describe() , change some values and then do np.round() it works fine . Both are DataFrames so I don't see why the behaviour could be different .

Rather than drop the index , I think you can / should just use the ` .values ` .

I use astype ( ' S3 ') for three digits numerical index , astype ( str ) truncate them to single character .

use ` map ( int , df.DateVar.str.split ( ' / ') [ 0 ])` to convert each element to integer ?

Thanks . For some weird reason that didn't work . I keep checking that I have dupes by using df.duplicated() .value_counts() and it does show as many rows as True but then when I apply ` df.sort ( df.columns.tolist() )` as you suggest , it still is not sorting all of the duplicated rows .

You need parentheses after your initial mean / median calls ( as below ) - otherwise you are assigning the function to DataFrame , not the value it returns #CODE

try ` for z in y : print len ( z )` to check if all y's have expected length ?

Trying to merge two timeseries gives me the error that the Series have no columns . And merging them still doesn't solve the main problem that I had . ( Selecting the right indices )
Merging should work . You need inner join , right ?

We can use ` shift ` and ` where ` to determine what to assign the values , importantly we have to use the bit comparators ` ` and ` | ` when comparing series . ` Shift ` will return a Series or DataFrame shifted by 1 row ( default ) or the passed value .

and a specific value using loc : #CODE
( To mix labels , loc , and position , iloc , you have to use ix ) #CODE
Thank you very much . Yeah so I was right with my assumption to avoid chaining . Thx for clarifying that these are actually so called MultiIndexes ; I thought this name is only applied for MultiIndexing in rows ... But I see the analogy , especially because one can easily transpose a DataFrame ( and thus still keep the MultiIndex ) .

why would you loop doing this type of indexing ? You are not using the power of pandas here , just use `` .isin `` , and or / join , see this for benchmarks : #URL
I have data set ( A ) and I have to find for each entry in A all rows in B with the same data in the first column . Then I perform some calculation using this data from B and insert the result back to A . I think I have to search for A [ i , 1 ] in B [ 1 ] , and ` .isin ` is not very fast in searching single items ( 2 seconds in the above case ) .

When plotting a Dataframe you can choose the axes object using ` ax= ... ` . Also in order to prevent the two plots from overlapping I have modified where they align with the ` position ` keyword argument , this defaults to ` 0.5 ` but that would mean the two bar plots overlapping . #CODE

Basically , ` df.shift ( 0 ) .loc [ start ]` gives a 4x4 , and we only want the the diagonal elements . Then we shift the 4x4 up and down to get the other two values we want for each column . Put all into an ` array ` and get the ` .mean() ` .
Select out the pairs of column / index that we want ( the unstack creates a multi-index with these pairs ) #CODE

In the script below , Why are tz and tz2 are different ? #CODE
In this case , tz displays as : #CODE
So in answer to your question , ` tz2 ` is correct as it localizes to a time zone that is correct for its date , while ` tz ` is ' correct ' for the current date .
Are you saying that the timezone of ` t.tz_localize ( pytz.UTC ) .tz_convert ( tz ) .tz ` depends on the value of ` t ` ? I can see that as reasonable , but I've tried a variety of ` t ` values , including today's date . None gives the ** LMT ** value . I agree this has become a question about pytz behavior rather than pandas behavior .
Once PEP 431 is finished , ` datetime.tzinfo ` methods will take ` is_dst ` parameters where appropriate and ` pytz ` will be able to implement time zones that do the right thing without having the user jump though ` localize ` and ` normalize ` hoops .

The example is simplified- I'm trying to show what I'm doing with the full code block below- which has a multiindex . When I use .join with the full code set , I get ` ValueError : cannot join with no level specified and no overlapping names `
@USER - I wasn't the one that flagged the answer . The answer itself was automatically flagged by Stack Overflow and showed up in the review pool because it was a code only answer . I was just giving you a heads up that adding additional information to the post would prevent automatic flagging .
It's certainly strange . The full data set is a little over 1700 rows / 18columns after the transpose . Caused me to smack my head against my head against the desk for half a day ...

No , I have tried that and received this : AttributeError : Cannot access attribute ' values ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method
You need to apply some kind of aggregation to the GroupBy object to return a DataFrame . Once you have that , you can use ` .values ` to extract the numpy arrary .
Input : data = binned_data.sum() .reset_index() .values and then I got the Output : ValueError : cannot insert Weight , already exists . What do you think is causing this ?

You can get the colors from seaborn like this : ` colors = sns.color_palette() ` . Ffisegydd's answer would then work great . You could also get the color to plot using the modulus / remainder operater ( % ): ` mycolor = colors [ icolumn % len ( colors ]` . I use often use this approach myself . So you could do :
mycolor = colors [ icol % len ( colors ]

In DF1 are a few hundred thousand records with columns lat1 and lon1 and there are 50,000 records in DF2 with columns lat2 , lon2 and zip . I want to apply a function f ( lat1 , lon1 , lat2 , lon2 ) which calculates the distance between two points ( defined using lat1 , lon1 , lat2 , lon2 ) . I ultimately want to add zip from DF2 into Df1 corresponding to the record in D2 corresponding to the smallest distance between that row in Df1 and all rows in D2 .
For all 110k+ records in ` df1 ` do you want to apply your distance function for every record in ` df2 ` ?
It's also worth noting that this is going to run in polynomial time , which is going to take a while with the number of rows you have . I chose to use map and list comprehensions because they will be faster than a standard ` for each `
I don't see a way for him to accomplish this below polynomial time . Python can certainly handle this many calculations and since there seems to be no limit on time I think it should be ok . However I took this into account and used map , and nested comprehensions which are going to be faster than a for loop . #URL

If I remove the " regex = True " from the replace arguments , I get #CODE

pandas apply function to multiple columns and multiple rows
I have a dataframe with consecutive pixel coordinates in rows and columns ' xpos ' , ' ypos ' , and I want to calculate the angle in degrees of each path between consecutive pixels . Currently I have the solution presented below , which works fine and for teh size of my file is speedy enough , but iterating through all the rows seems not to be the pandas way to do it . I know how to apply a function to different columns , and how to apply functions to different rows of columns , but can't figure out how to combine both .
and then I insert temp list into df
I compared the time of three solutions for my df ( the size of the df is about 6k rows ) , the iteration is almost 9 times slower than apply , and about 1500 times slower then doing it without apply :
execution time of the solution with iteration , including insert of a new column back to df : 1 , 51s
execution time of the solution without iteration , with apply : 0.17s
execution time of accepted answer by EdChum using diff() , without iteration and without apply : 0.001s
Suggestion : do not use iteration or apply and always try to use vectorized calculation ;) it is not only faster , but also more readable .
Also if possible avoid using ` apply ` , as this operates row-wise , if you can find a vectorised method that can work on the entire series or dataframe then always prefer this .
seeing as you are just doing a subtraction from the previous row there is built in method for this ` diff ` this results in even faster code : #CODE
` abs ` should be ` np.abs ` in first case ?

The problem I'm facing is : I only have integers describing the calendar week ( KW in the plot ) , but I somehow have to merge back the date on it to get the ticks labeled by year as well . Furthermore I can't only plot the grouped calendar week because I need a correct order of the items ( kw 47 , kw 48 ( year 2013 ) have to be on the left side of kw 1 ( because this is 2014 )) .
#URL that grouped bars need to be columns instead of rows . So I thought about how to transform the data and found the method ` pivot ` which turns out to be a great function . ` reset_index ` is needed to transform the multiindex into columns . At the end I fill ` NaN ` s by zero : #CODE
whereas I have the problem with the axis as it is now sorted ( from 1-52 ) , which is actually wrong , because calendar week 52 belongs to year 2013 in this case ... Any ideas on how to merge back the real datetime for the calendar-weeks and use them as x-axis ticks ?
Then change the layout ( reshaping ) as mentioned in the question already by using ` pivot ` . The date will be the index . Use ` reset_index() ` to make the current ` date ` -index a column and instead get a integer-range as an index ( which is then in the correct order to be plotted ( lowest-year / calendar week is index 0 and highest year / calendar week is the highest integer ) .
I think resample ( ' W ') is a better way to do this - by default it groups by weeks ending on Sunday ( ' W ' is the same as ' W-SUN ') but you can specify whatever you want .

Since ` argmax ` is the index of the maximum row , you will need to look them up on the original dataframe : #CODE
NOTE : I selected the ' size ' column because all the functions apply to that column . If you wanted to do a different set of functions for different columns , you can use ` agg ` with a dictionary with a list of functions e.g. ` agg ( { ' size ' : [ np.sum , np.average ] } )` . This results in ` MultiIndex ` columns , which means that when getting the IDs for the maximum size in each group you need to do : #CODE
Thank you for your clear answer . I noticed that the code works for just the ` size ` column version but does not work when I do : ` grouped = df.groupby ([ ' code ' , ' colour ']) .agg ( { ' size ' :[ np.sum , np.average , np.size , np.argmax ] } ) .reset_index() ; grouped [ ' max_row_id '] = df.ix [ grouped [ ' argmax ']] .reset_index ( grouped.index ) .id ` . Am I missing something obvious ?
using a dictionary as the argument for ` agg ` results in ` MultiIndex ` columns , so it needs to be ` grouped [ ' max_row_id '] = df.ix [ grouped [ ' size '] [ ' argmax ']] .reset_index ( grouped.index ) .id ` . I'll update my answer to make this clearer

I can resample weekly , starting on Sundays and closing on Saturdays : #CODE
After trying the various options of ` resample ` , I might have an explanation . The way ` resample ` chooses the first entry of the new resampled index seems to depend on the ` closed ` option :
when ` closed=left ` , ` resample ` looks for the latest possible start
when ` closed=right ` , ` resample ` looks for the earliest possible start
The next example illustrates the behaviour of ` closed=right ` , which is the one that I didn't understand in my initial post ( ` closed=right ` by default in ` resample `) . The earliest " right-side " Saturday of a 2 weeks interval closed on the right happens on 2014 / 06 / 07 , as shown by the following : #CODE

I would extract the day from the minute data set and merge it with the day index of the other dataframe . Something like ... #CODE

I've tried using the ` apply ` function across the column , but to no avail . So , I took a very naive ( but not very concise ) approach to create these columns : #CODE

No , calling ` .median ` on the ` .groupby ` object returns the median for each group ( in my case there are 12 groups ) . The output looks at the same as my example except they'd 12 values for each column in the ` .groupby ` object .

You were right , using the ` .transform ` : ` values_to_subtract = test [ ' A '] .median() - grouped [ ' A '] .transform ( ' median ')`

If you need these to be bool columns , then use ` astype ( bool )` on each column .
As an aside you can nearly this with a resample ( except for the last missing rows and Changed column ): #CODE

Instead , I get an error telling me that equiv is not a callable function . Fair enough , it's a dictionary , but even if I wrap it in a function I still get frustration . So I tried to use a map function that seems to work with other operations , but it also is defeated by use of a dictionary : #CODE

If I understand you correctly you can just call ` map ` : #CODE
Rather than fill as an empty column , you can simply populate this with an apply : #CODE

Pandas concat producing NaN
I think you want ` concat ( df , axis=1 )` .

I love stack overflow - a serious question : I want a line and bar on the same chart , and I cannot see what my code is doing wrong ( the bar chart doesn't get added ) . And for that I get a down vote ? Also , a title change so that the question no longer reflects my information need . The code above works fine for multiple line charts and multiple bar charts - my problem is one of each .
The most difficult thing is how to align x-axis ticks . Here we define the align function , which will align ` ax2.get_xlim() [ 0 ]` with ` x1 ` in ` ax1 ` and ` ax2.get_xlim() [ 1 ]` with ` x2 ` in ` ax1 ` : #CODE

Pandas ; calculate mean and append mean to original frame
Append the mean of every country as a third column
Perform a ` groupby ` by ' Country ' and use ` transform ` to apply a function to that group which will return an index aligned to the original df #CODE

I'm aware of df.where function but apparently it's not possible to apply for columns , it works just for all DataFrame .

How to drop rows based on year on a based dataframe in a function using * args

or in more generic way using regex replace .

@USER there's a subtle thing here that = can create a copy if used along with loc / iloc / ix .

the columns ' names map similarly : #CODE

You can shift ` A ` column first :

Idk . Imagine if you needed to clean or pre-process the .csv data from truefx . With 700 million rows per month , even a modest fraction in need of preprocessing makes it bad , even if you chunk the data and read only all of one chunk at a time . Of course , when you hit this size , you probably need to shift to a proper database , but still , there are people out there who try to just ` read_csv ` this kind of stuff . I think I'd rather use a file generator in Python at that point . But I agree with you that in many cases , " over computing " to load it all is not that harmful .

Should be pretty direct with pandas ` stack ` and some string parsing .
@USER I've never used stack before but actually it looks really useful . Might take a little bit of messing around with the index afterwords though .

You can use the ` value_counts ` function for this ( docs ) , applying this after a groupby ( which is similar to the ` resample ( ' D ')` you did , but resample is expecting an aggregated output so we have to use the more general groupby in this case ) . With a small example : #CODE
To get this in the desired format , you can just unstack this ( move the second level row indices to the columns ): #CODE

Why not use the built in string method , rather than apply . ` df [ 0 ] .str .count ( ' : ')`
@USER using that function is actually slower than using ` apply ` ( see my answer ) .

@USER What about something like ` def f() : return len ( requests.get ( ' #URL ) .text )` ? Or ` def f() : print " Hello World " ; return 5 ` ? Or ` def f() : return hashlib.md5 ( open ( ' hello.txt ') .read() )` ? The majority of Python functions are not operations in linear algebra , and so the majority do not have an easy ` numpy ` implementation .

I like the ` concat ` solution . I neglected to mention that I have other columns in my particular dataframe with a single level ( say call ` df [ ' C '] = ' a '` in my example above ) . When I try doing ` pd.concat ([ df , df [ ' Val '] / 2 ] , axis=1 , keys =[ ' Val ' , ' C ' , ' Half '])` I get ` AssertionError : Cannot concat indices that do not have the same number of levels ` . Is this a fundamental limitation of concat ?
I think this option is preferable to the concat option because you don't have to risk incorrectly re-labeling the ' Val ' column . Please correct me if you disagree !

cumsum per group in column ordered by second column append to original dataframe
Will give a hierarchical index solution , but I could not find a way to take the resulting cumulative sum column and attach it to the original dataframe without multistep merge commands . The reset_index command just converts the table back to its original state .
Looking for a better method to achieve desired output as shown . I have messed with lambda , apply , aggregrate commands can't quite get anything to work . #CODE

I have a bunch of csv files , I am reading them using pandas from python . I want to use a combination of map & lambda functions to do this
I have a bunch of csv files , I am reading them using pandas from python . I want to use a combination of map lambda functions to do this .
the map function does not append to NN .
Have you tried using ` concat ` and a generator expression instead : #CODE

` memory_map ` does not seem to use the numpy memory map as far as I can tell from the source code It seems to be an option for how to parse the incoming stream of data , not something that matters for how the dataframe you receive works .

pandas resample dealing with missing data
I am using pandas to deal with monthly data that have some missing value . I would like to be able to use the resample method to compute annual statistics but for years with no missing data .
Here is what I obtain if I resample : #CODE
and then drop all rows where ` count ! = len `

This works because the ` transform ` operation preserves the index , so it will still align with the original dataframe correctly .
I found another approach that uses apply() instead of transform() , but you need to join the result table with the input DataFrame and I just haven't figured out yet how to do it . Would appreciate help to finish the table joining part or any better alternatives .
You can join this back to df with ` df.merge ( group_features , left_on= ' Key ' , right_index=True )`

You could add a column full of ` True ` and then pivot : #CODE

Pandas wants to align the rhs side ( after all you are subtracing DIFFERENT indexes ) ,

Merge with Overlapping columns
If I have two data frames that contain data during the same period ( although no necessarily for the exact same dates ) and I want to merge them together . The data can be thought to look like this : #CODE
The merge is to be performed on the data and ID columns as follows : #CODE
Formulated another way , is there a method to use merge such that when performing and outer merge , in the resulting data frame where an inner merge would ' fail ' , to keep the value that is not null ?
This of course assumes that the name would have to be the same in df1 and df2 where an inner merge would succeed . I know this to be the case .
I know that I could easily do this after the merge : #CODE
I don't think there's anything built-in to merge to have the behavior you're describing . But , as you note , it shouldn't be that big of deal to handle after the fact .
Cool ! Never seen partition or endswith . I was iterating previously , but I like those two little extras ...

Deducting the median from each column
I'd like to deduct the median from each column so that the median of each becomes 0 . #CODE
How do I do this in a pythandic way ? I'm guessing it is possible without iterating over the values , computing the median and then deducting . I'd like to do it tersely , approximately like so : #CODE
` median ` of ` numpy ` computes median of overall data .
Does this really subtract the median of each column ? Seems it would do the overall median .
So the next line gets the median along the columns #CODE
In numpy 1.9 ` median ` has a ` keepdims ` keyword argument that will spare you the ` None ` indexing ( which isn't needed in this particular case anyway ) .

If you're interested in the history of these zones , I suggest reading through the europe file in the TZ data . The comments alone are quite interesting .
Also , there are some time zones that are just aliases and are completely interchangeable . In the TZ data , they're called " links " . For example , you can see here that ` Europe / Vatican ` and ` Europe / San_Marino ` are both linked to ` Europe / Rome ` , and are therefore equivalent .

I realize there are different settings one can apply that will change the look and feel of either individually ( matplotlib savefig() plots different from show() ) , however I haven't been able to find any easy to follow documentation that shows how to set the default fonts while using matplotlib backend : MacOSX . Can someone show how to make the font that appears in the shown figure also appear in the saved figure ?

Edit 1 : Could it be fast to set the index of the chunk equal to the id column and then only keep rows that successfully merge with keep_ids ?

and use ` concat ` to take the year out .

Now you can assign all the entries in these rows to NaN using loc : #CODE

Insert a row to pandas dataframe
Is there any direct way how to add / insert series to dataframe ?
Generally , it's easiest to append dataframes , not series . In your case , since you want the new row to be " on top " ( with starting id ) , and there is no function ` pd.prepend() ` , I first create the new dataframe and then append your old one .
But now you can easily insert the row as follows . Since the space was preallocated , this is more efficient . #CODE
That's nice workarround solution , I was trying to insert series into dataframe . It's good enough for me at the moment .
Just assign row to a particular index , using ` loc ` : #CODE
If you don't want to set with enlargement , but insert inside the dataframe , have a look at #URL

@USER is it possible ? something like ` for i in range ( len ( folder )): df_%i = pd.DataFrame() % i ` so I can then save my CSVs or output in these dataframes

You cannot write to the ` year ` attribute of a datetime , so the easiest thing to do is use ` replace ` : #CODE

A bit of a non-scalable solution would be to ` drop ` 2014 and then call ` max ` and ` min ` - #CODE

Pandas dataframe with multiindex column - merge levels
How can I flatten / merge the column index levels as : " Level1|Level2 " , e.g. ` size|sum ` , ` scaled_size|sum ` . etc ? If this is not possible , is there a way to ` groupby() ` as I did above without creating multi-index columns ?
Thank you , but it is not what I am asking for . I edited the question to make it clearer . I wnat to get rid of the two levels in the column names . If there are two levels , n i want to merge them into one , like ` size|sum `

Then , convert to a numpy array , transpose , and pass to the DataFrame constructor along with the columns . #CODE

This is just an exemple , in reality my files are big , there are 100 metric and millions of lines and I need to do it efficiently . I tried with groupby , but I still need to combine the units with the Metric and to transpose the value .
Then , I made a little function to rename the columns . The pivot table will have a MultiIndex as the columns , this collapses it down to combined the name / units . #CODE
I think I would use a groupby and merge here : #CODE

Set cell value to NaN under certain conditions using lamda map in Pandas
I've tried to make an iterative approach but it's far too slow / may not be working at all . Using lambda and map is very fast in Pandas but I can't figure it out .

I could be wrong bu it seems that you can't append with ` to_csv `
To append to a csv use append mode ( see this or this question ) .

Any dataframes with identical multiindexed columns append and sort
The key here is that I don't know how the dataframes will be ordered in the list I basically need something that knows when to concat ( obj , axis=1 ) or concat ( obj , axis=0 ) and can do this to combine my list of dataframes . Maybe there is something already in pandas that can do this ?

You want to resample it and have OHLC , with give frequency ( let's say M or 2M ) buckets ending 18JUN .
So for M freq , last bucket would be 19MAY-18JUN , previous one 19APR-18MAY , and so on ... #CODE
BUT so far we can't apply the right DatetimeIndex to ts.resample()

This was a lot easier than I thought . It's simply a case of calling ` unstack ` on the ` groupby ` object then plotting it . #CODE

well as I explained above , dtypes are consolidated into single ndarrays , so they must be concatenated ( which is where all of the time is spent ) . You can simply keep them as a dict of Series to not copy if you want . But then you must manually align them and operations become tricky . Better to do this once then write them as HDF5 files ; they come already blocked by dtypes ( on reading ) .

Use a combination of ` shift ` and ` all ` : #CODE

I would convert these to pandas Periods instead of Timestamps and then diff these #CODE

I think to use ` apply ` in this case will be difficult as it is conditional ( based on the surrounding cells ) . I think you many want to generate separate ` DataFrame ` s for ` Around_A , B .... ` . Once you get those , you can use ` dropna() ` to get rid of the rows containing ` nan ` s , which will make the dataset much smaller and may avoid the memory issue altogether .

Pandas Interpolate Returning NaNs
they were just fed in with vanilla pd.csv_read . Only things I did was drop an extra header row with ` df = df [ 1 :] .reset_index ( drop=True )` and

It could be that you're looking for some kind of merge : #CODE

I'm not 100% clear on your question , but I think you may want the pandas ` cut ` function , which bins data into groups . The code below cuts the data into 3 groups based on market_cap , groups , and calculates the average debt ratio . #CODE
To have different numbers of groups , just pass a different parameter to cut . #CODE

Basically I want to create a new column " Ratio " that divides Price / Buy or Price / Sell , depending on which abs ( buy ) or abs ( sell ) is greater . I am not really sure how to do this ... would I use an apply function ?
The condition ` ( abs ( df [ " Buy "]) abs ( df [ " Sell "]))` gives a 0 / 1 valued column depending on whether buy or sell is greater . You multiply that column by Price / Buy . If Sell price is high , the multiplication will be zero .
Here is the solution using apply - First define a function operating in rows of the DataFrame . #CODE
Finally , set the ` Ratio ` column appropriately using apply .
Thanks this works ! But is there a way to do it with an apply function or something as well ?
I've edited the reply to include solution using apply

If set the threshold to 1 and take the abs value like so : ` df.groupby ( level =[ ' categories ' , ' features ']) .filter ( lambda x : max ( abs ( x.xs ( ' subfeature1 ' , level= ' subfeatures ') - x.xs ( ' subfeature2 ' , level= ' subfeatures '))) > 1 )` . I'm not getting ` cat3 ` like I expect ? The code makes sense .

i am looking to apply multiply masks on each column of a pandas dataset ( respectively to it's properties ) in python .
how can i apply the concat_mask on df , so that i select rows , in which all Boolean criteria are matched ( are True ) ?
. Can You insert that into your answer ? Then i will marked it as answered :)
Thanks for Your answer . In the proper code i actually iterate throw all columns and apply various of diffenrent conditions to mask each column . This is all what the code ment to express .

not yet #URL ( but you can do it in the apply ; this is for using an aggregate function ) .
Since we're using views here , this should be more efficient / faster than the apply ...

Concat , Groupby and sum() panda
Next , I want to append , Concatenate the dataframe to the the one prior in a cumulative concat function .. It also appears to do this correctly #CODE

For my issues with upgrading , would there be a good workaround ? I prefer it to be in a column without a tz , as I'm comparing it to another value that is tz=None .
You have to append nanos ( since numpy datetime arrays * don't understand pandas offsets ): #CODE

And then if you really want to drop the zero values , you can keep only the nonzero ones using ` fast_df = fast_df [ fast_df [ ' value '] ! = 0 ]` .

Pandas rolling apply with variable window length

` int ` doesn't have a representation of NaN . The normal way to deal with it would be to drop all the NaN's first : #CODE
The indices will still align when you do operations on both , I'm not sure what you mean ? Perhaps you shoud use ` np.trunc ( y.Year )` , and leave the datatype as float , if you really want to be able to see missing data .

However , when I try concatenating it with the dates ( the last bit of code ) , I get a very random error ( KeyError : 203591L ) . Weirdly , it works fine if I replace the ` v = ` with just ` print ` and remove the line with the append statement .
Have updated to pandas 0.14.0 and it works now - thank you ! Just need to figure out how to append the new column to the dataframe and set it as the index , but I'm sure that's the easy bit . Thank you again .

You could try and use ` read_json ` and ` concat ` which may be more efficient . At the moment there's too much going on in this question though , tough to answer .

` query ` can't handle everything , but it can handle stuff this simple . If you need something more sophisticated , you could use ` exec ` or ` eval ` to construct functions on the fly ; there are obvious security hazards there , but you'd have the same issues using ` sympy ` ( it uses ` eval ` too . )

How do I reshape or pivot a DataFrame in Pandas
So column names are now the values that were in CF and the intersection of the row and columns in the new table are the values that were in the value column in the original table .
Is there a way to drop the " CF " text in the output . It is confusing to someone reading that output that is not familiar with DataFrames . CF looks like a column with values 0:3 but the real values for CF are 1.0 , 1.5 , 2.0 .

now you need to append the result as a ; delimeted string to the csv file , with some kind of for loop #CODE
Here's a potential approach using pandas . If the input file doesn't exist , it writes a dummy file ( modify this to suit ) , then opens the excel file , and reads back into a dataframe once the user has closed . Replace ` excel_path ` with a path to your Excel install ( I'm using LibreOffice to here ) . #CODE

@USER I have updated the question with complete stack trace

Figured it out ! A heat map is what I wanted #CODE

There are 6 location tree columns total which I have labeled UK , Region , Sub-Region , County , City , Borough / Neighborhood . What I need to do is merge the location tree columns with the job listings data frame . The normalized location column in the job listings data frame contains a mix of different geography types ( e.g. some listings just say UK , some are just the region , and some are very specific London neighborhoods ) .
and so on for the remaining location categories . My thinking was to merge on the most abstract location first and then move to more specific location categories to merge the LocationNormalized values that are more specific .

I'm trying to reduce meterological data using ` pandas ` 0.13.1 . I have a large dataframe of floats . Thanks to this answer I have grouped the data into half-hour intervals most efficiently . I am using ` groupby ` + ` apply ` instead of ` resample ` because of the need to examine multiple columns . #CODE
I want to use ` math.atan2 ` on the ' Ux / Uy ' columns and am having trouble successfully ` apply ` ing any function . I get tracebacks about attribute ` ndim ` : #CODE
My original question was : what kind of value should be returned from my ` apply ` ed function so that a groupby-apply operation results in a 1-column DataFrame or Series with a length equal to number of groups and group names ( e.g. Timestamps ) used as index values ?
this will be much more efficient to not use apply at all , rather compute the mean aggregates first , then use np.atan2 . I'll put up an example tomorrow
Just looking at your exception , looks like you're trying to apply function to each row but didn't specify axis=1 e.g. df.apply ( f , axis=1 ) #apply function to each row

I would like to drop the row where the userid is Nan . If I wanted to drop another row I could do #CODE
and other attempts all return errors of differing varieties . Is there a way to drop the row without having to reindex ?
I already tried something like this . isnull is not defined for MultiIndex so the command would be more like df.loc [ ~ pd.isnull ( data.index.levels [ 0 ])] but this does not work since the returned array ignores the NaN value .
Easier to reset and drop from the frame , then set the index . #CODE

Pandas uses the index " line up " operations in that the operation will apply only to the common indices . So if you want to subtract one row from all in a DataFrame then you need to convert that to a numpy array first as shown in the answer .
Pandas is great for aligning by index . So when you want Pandas to ignore the index , you need to drop the index . You can do that by converting the DataFrame ` a.loc [ ' 2005 ']` to a 1-dimensional NumPy array : #CODE

lol , snap ! Though this is a neater way of writing out the header !
To write out you could write the header first and then append : #CODE

then apply the filter like so :

It seems the most time consuming part is ` nonzero ` of ` numpy.ndarray ` . Since almost all of my operations are based on ` pandas ` , I wonder which function in ` pandas ` rely on this method heavily ?
@USER Yes , it's indeed the case . I am looping over the index of dataframe . Then use extracted index to slice another dataframe . Because these are irregular operations , I can't use merge / join . Let me see if I can come up with a shorter code to illustrate my purpose . In the meantime , do you know which ` pandas ` function rely heavily on ` nonzero ` method in ` numpy ` ? Thanks !
practially all indexing functions use `` nonzero `` , which btw is one of the most heavility optimized functions . You should not be looping .

well you can simply access the subfeatures field in the apply then .

Then I would like to translate the remaining variables in a numpy ndarray .
You mean like use [ ` drop_duplicates() `] ( #URL ) , you could calc std deviation using ` df.std ` and then drop those columns , it sounds like ` drop_duplicates ` would do what you want

Pivot operations still confuse the hell out of me , so if someone can give me an intuition as to why ` level=1 ` is the correct argument , I'd much appreciate it . As best I can tell , ` level ` means something akin to set this hierarchical index level as the table heading .

It doesn't affect you as its not visible to you . This tests whether you are mutating the input in the apply or not . Just use `` apply `` or iterate over the groups .

Python Pandas merge samed name columns in a dataframe
What I would like to do is merge those same name columns into 1 column ( if there are multiple values keeping those values separate ) and my ideal output would be this #CODE

Now from here I have the issue of not being able to pull the cross section on the hypothetical column that would include all the blank values that did not have the label ' BIT ' , ' GAS ' or ' OIL ' . In an excel pivot table , I can do this by checking the ( blank ) box when selecting the columns to be included in a pivot table . I want to do the same thing here to get a frequency count of all those that are blank .

Merge after groupby

I have a dataframe ( df ) and trying to append data to a specific row
The goal is to compare the Fruit at Rank 1 to each rank and then append the value .
I'm using difflib.SequenceMatcher to make the comparison . Right now i'm able to append to df but i end up appending the same value to each row . I'm struggling with the loop and append .

Pandas : How to stack time series into a dataframe with time columns ?
What is the best way to stack it into a dataframe such as : #CODE
But here is some code that would accomplish it . First , a time column is added , and the index is set to just the date part of the DateTimeIndex . The ` pivot ` command reshapes the data , setting the times as columns . #CODE

I split the ` df ` in several pieces ( in a data-safe way ) , store them in ` hdf5 ` files , load them from 4 clients , manipulate the data in parallel , re-save in `' hdf5 '` and finally merge the result in the final ` df ` . This solution either works , or generates extreme swapping from the clients .
Thus in a non-trivial computation it is essential that you use the built in functions . Using ( apply / aggregate ) is nice for a generalized function evaluation , but pandas cannot make too many assumptions about what is going on in the user function , and these are evaluated in python space . #CODE
Consider for instance : ` df.groupby ([ ' Day_id ' , ' Frame ']) .apply ( lambda x : x [ x.Avg_U > 0 ] .Speed .mean() )` or ` df.groupby ([ ' Day_id ' , ' Frame ']) .transform ( lambda x : len ( x ))` , I couldn't find yet a smarter vectorized form .

`` .asfreq `` converts the freq on a regularly sampled series , you prob want to `` .resample `` , what are you trying to do ?
use resample ; asfreq doesn't make much sense on an index with out a frequency

retaining order of columns after pivot
I do a pivot : #CODE
, then replace missing values nan with zeros : #CODE
where columns were sorted lexicographically during the pivot .
In the pivot , add the " values " argument to avoid hierarchical columns ( see #URL ) , which prevent reindex to work properly : #CODE
then , as before , replace nan's with zeros #CODE

I want to do something special for groups where val == 1 appears but not val == 0 . E.g. replace the 1 in the group by 99 only if the val == 0 is in that group .
I don't think this does what I want . I wish to replace the val 1 by 99 only if the val 0 appears in that group . I modified the question to try to make this clearer .

Transpose pandas dataframe
Do you want to process all 5 ? : for x in range ( len ( data )): df= ... append each to a list , or whatever you wanna do with it

I ended up having to make them all int . Like cols = map ( int , cols ) to make it work .

` df.to_sql ( ' Tweets ' , conn , flavor= ' sqlite ' , if_exists= ' append ' , index=False )`

when u assign the rhs is automatically aligned to the lhs , that is , the matching labels are picked out . this is a feature ! in this soln u are just assigning values ( a numpy array ) so their is nothing to align ( which in this case is what u want )

Search and replace missing lines in the data structure by pandas
The location of these missing lines ( the missing of values at time points ) and replace them with the lines with nan .

Why is it not rigorous to average the flow rate like that ? Flow rate is linear with time , as is the mean , sounds fine to me . Second , if you resample , then the time steps are regular , and so can be factored outside of the sum to get cumulative flow - you can just need to multiply the whole column by the time step .
I understand what you talk about . Precisely , is the resample function with the " mean " parameter do a weighted average ( to adjust to the irregularity of the time step ) ? Otherwise , how to do ?

Take a look at ` shift ` , which as the name implies shifts values up / down the index .

And then graphed into a line graph depicting counts . I've looked around and there doesn't seem to be any clear cut way . Any help is greatly appreciated .

but when I do this , the ` len ` column gets automatically cast as ` np.datetime64 ` , which I assume is happening because that's the dtype of the original ` date ` column : #CODE
I could go back and reconvert this ` len ` column to nanoseconds since GMT epoch , but that is pretty ugly and I feel like there must be a better way . Any ideas ?
use `' size '` ; this is currently an API bug ( in that the ` len ` should just be translated directly to ` size `) , see here #CODE

How to resample a Pandas multi-index data frame via methods depending on the column name
I can resample ` g ` successfully using one method across all columns , e.g. by ` g.resample ( ' 6H ' , how= np.sum )` . How can i resample ` g ` with different methods for each column , e.g. by summing the ' A ' columns and averaging the ' B ' columns ?
If you start with f , you can use a groupby with a TimeGrouper to do the resample " manually " : #CODE
To get from g to f you can reshape with stack : #CODE
Another approach , which might be " simpler " , in that you actually do the resample , is to make the dict from the columns : #CODE

pandas : groupby and unstack to create feature vector for classification
You're looking for ` pivot ` : #CODE
Note : Originally this answer suggested ` pivot_table ` ( which uses an aggfunc on repeated values , by default mean , and that's not what you want here - as @USER points out ) , it offers some other additional features over pivot but is a little slower : #CODE
I have this feeling that in older versions of pandas , pivot was sensitive to NaN so you had to use pivot_table ...
@USER great point , perhaps ` pivot ` is the correct function to use here ( in my experience it's a little more sensitive though ) ... I was surprised it allows the NaN !
@USER I have this feeling that pivot __used__ to not play well with missing data , but maybe I'm making that up . Thanks , updated the answer ( definitely pivot is correct here )

Ideally , I would like to replace the last two lines with something similar to this : #CODE
if you combine this with another apply , you'll get info for the total columns #CODE

Python pandas join on with overwrite
I realize this question is similar to join or merge with overwrite in pandas , but the accepted answer does not work for me since I want to use the ` on= ' keys '` from ` df.join() ` .
Basically I want to replace the ' values ' in the ` df ` with the values in the Series , by ` keys ` so every ` keys ` block gets the same new value . Currently , I do it as follows : #CODE
To my knowledge , pandas doesn't have the ability to join with " force overwrite " or " overwrite with warning " .

` pandas ` ` timeseries ` is prefect for this application . You can merge series of different sample frequency and ` pandas ` will align them perfectly . Then you can downsample the data and preform regression , i.e. , with ` statsmodels ` . An mock-up example : #CODE

I've come up with this , using itertools , to find mid-day timestamps and group them by date , and now I'm coming up short trying to apply imap to find the means . #CODE
Since not sure what your end output should look like , just create a time-based grouper manually ( this is essentially a resample ) , but doesn't do anything with the final results ( its just a list of the aggregated values ) #CODE
You can get reasonable fancy here and say return a pandas object ( and potentially ` concat ` them ) .

I am trying to normalize a large ( about 900 MB ) json file into a pandas DataFrame using the json_normalize() function . This works for all the other files I am processing ( which range from about 150-500 MB in size ) , although they each take a few minutes to finish . However with this file it seems to just run forever and never finishes . I have tried reading the json file directly into a DataFrame using read_json() before normalizing as well as simply loading the json from the file with json.loads() . Both methods have the same result . The code I am using is : #CODE

I'm pretty sure there's nothing built in to ` pandas ` , but if you have the full stack installed you can use ` scipy ` : #CODE

Here is the solution I came up with but I really wanted to avoid using list and append and take advantage of a generator instead but not yet comfortable enough working with generators . #CODE

` rolling_apply ` the ` prod ` .

I'd like to consolidate all 3 data frames , so they all contain rows with indices that are present in all 3 of them . How is this achievable ? #CODE
Take a look at concat , which can be used for a variety of combination operations . Here you want to have the ` join ` type set to inner ( because the want the intersection ) , and ` axis ` set to 1 ( combining columns ) . #CODE
Interestingly ` merge ` is faster for larger dataframes than concat , I compared concat with merge on a 40000 row dataframe and saw this ` 100 loops , best of 3 : 6.3 ms per loop `
` 100 loops , best of 3 : 4.87 ms per loop ` when comparing concat with merge
Concat works great . Is there any way the 3 dataframes could be kept separate ? Sorry , I messed up the example .
Use ` merge ` and pass param ` left_index=True ` and ` right_index=True ` , the default type of merge is inner , so only values that exist on both left and right will be merged . #CODE
So we merge all of them on index values that are present in all of them and then use the index to filter the original dataframes .
Merge seems like an interesting alternative to concat . When do you use one or the other ? It seems ` merge ` is more versatile . I updated my post . The way I framed it wasn't what I was actually looking for ...

in either case , it is plotting your data against ` np.arange ( len ( data ))` so you can use ` set_xlim ` .
@USER , in version ` 1.3.1 ` , ` ax.margins ( x= 0.1 , y= 0.1 )` raise a ` ValueError : more than two arguments were supplied ` . Shouldn't the source code be ` if len ( args ) == 2 : ` and ` elif len ( args ) == 3 : ` in lines 1835 and 1837 respectively ?

Hoping that this is an allowable SO question but I am hoping to get some advice on how to convert the below code which processes lines in a file to produce a dataframe into one that uses generators and yields because this implementation using list and append is far too slow .
Here is the solution I came up with but I was really hoping to avoid using very slow lists and append operation . I was hoping for cool generator and yield solution instead but not comfortable enough yet working with generators .

You can explore the possibility of using ` .str ` method , either you can extract the numbers using ` regex ` , or take a slice ` .str .slice ` , or like in this example , replace ` days ` with a empty string : #CODE
Or ` split ` instead of ` replace ` , e.g. ` df [ " Days_To_Maturity "] .str .split() .str [ 1 ] .astype ( int )` .

Anyone know how I can make it replace those parts correctly in pandas ?
One thing is you forgot to use ` str ` again for the second replace . You need ` dcol.str.replace ( " ( AMI )" , "") .str .replace ( " N / A " , " Non ")` . Not sure if that is the whole problem though .
This does not appear to be adequately documented ; the docs mention that ` split ` and ` replace ` " take regular expressions , too " , but doesn't make it clear that they always interpret their argument as a regular expression .
I found a work around too , by doing ` .str .replace ( u " ( " , "") .str .replace ( u ")" , "")` and then adding a second replace to remove / change the text part .

Why do you need to check if the ` % ` exists ? ` strip ` will still work if there is no ` % ` , it just won't do anything if there's nothing to strip .

Use resample function #CODE

ggplot expects the data to be in ' long ' format , so you need to do a little reshaping , with ` melt ` . It also currently does not support plotting the index , so that needs to made into a column . #CODE

Great thanks ! I really need to learn all the possibilities with map
followed by the the ' map ' function

There might be , but AFAIK , there is no way to use bracket indexing ` df1 [ ... ]` or any of the indexers like ` .loc [ ]` , ` .at [ ]` , or ` .ix [ ]` to accomplish this easily . Each of these returns " rectangular " DataFrames , whereas you want to select a * sequence * of individual values using row and column label coordinates . There is a way to use ` df.apply ` -- there almost always is -- but I think ` apply ` should be avoided when possible because it is often a relatively slow alternative . ( Under the hood it uses a Python loop which calls a Python function for each row or each column ... a recipe for slowness . )

and when I try to apply pct_change : #CODE

I can transpose it , and add a new column with values easily : #CODE
However , if I add a date index to original dataframe , then transpose it , I can't add any new columns to the transpose . #CODE

Very ugly , but I was able to replace the functionality I needed by using ` df1.ix [ df2.index , ' UTCdatetime '] = df2.ix [ df2.index , ' UTCdatetime ']` after first combining the indices into df1.index .

Use a combination of ` idxmax() ` and ` loc ` to filter the dataframe : #CODE

For each row in this dataframe I would like to count the occurrences of each of C1 , C2 , C3 and append this information as columns to this dataframe . For instance , the first row has 1 C1 , 0 C2 and 0 C3 . The final data frame should look like this #CODE
So , I have created a Series with C1 , C2 and C3 as the values - one way top count this is to loop over the rows and columns of the DataFrame and then over this Series and increment the counter if it matches . But is there an ` apply ` approach that can achieve this in a compact fashion ?
You could apply ` value_counts ` : #CODE

Merging ( inner join ) these two dataframes should work : #CODE

when i use a merge , I get something like this .
You can just apply ` dropna ` to ` a12 ` before ` merge ` : #CODE

PS I also had an issue with combine_first and tz ( pandas tzinfo lost by combine_first ) - doesn't appear to be related but just in case .
you can try to use the `` table `` format for this . Generally pandas is not well tested for mixed timezones in a column at all , or missing values with datetimes that have timezones . Welcome more test coverage for these . That said for `` datetime64 [ ns ]`` works really well with missing values . Usually If I needed to do something like this , I would keep a separate column of ' tz ' info .

To do this , the starting datetime must be naive . Localize the timezone after the series is generated by passing the ` tz ` keyword argument to ` date_range() ` : #CODE

Pandas : apply tupleize_cols to dataframe without to_csv() ?

I have two DataFrames with the same kind of index ( userid ) but neither is a subset of the other . I want to remove all rows from the smaller that do not appear in the larger . I was under the impression that this was the intended use of the loc function but it actually adds rows . #CODE
This is not a bug , see my comments here : #URL `` loc `` acts like a `` reindex `` when presented with a slice / index-like .

pandas apply np.histogram to reshape dataframe
` np.histogram ` is neither a reducer ( returns a single value ) , nor a transformer ( returns the same number as the input ) . So ` apply ` doesn't know how to map the return values .
Here is another way ( and conceptually how to think about apply ) #CODE
Perfect , thanks ! I knew I was missing something about the internals of ` apply `

pandas multiindex shift on filtered values
` diff ( 1 )` produces this error : #CODE
while ` shift ( 1 )` produces this error : #CODE
You can also do ` diff ` but I think this line is unnecessary ` g [ ' d0 '] = g [ g [ ' alert_v '] == 1 ] [ ' timeindex ']
return g [ ' td '] = g [ ' d1 '] - g [ ' d0 ']` as ` shift ` already calculates the difference so you should just return this
For multindex group , select rows , diff , and insert new column paradigm : this is how I got it to work with clean output .
shift throws key error , so just sticking with diff() #CODE

The code above obviously does not work . It is not clear to me how to correctly pass the fixed ` y ` to the function while having ` apply ` iterating through the ` x ` columns ( ` x1 ` , ` x2 ` , ... ) . I suspect there might be a very clever one-line solution to do this . Any idea ?
The function you pass to ` apply ` must take a ` pandas.DataFrame ` as a first argument , you can pass additional keyword or positional arguments to ` apply ` that get passed to the applied function . So , your example would work with a small modification . Change ` ols_res ` to #CODE
Then , you can use ` groupby ` and ` apply ` like this #CODE

Use ` groupby ` and we can pass a dict of functions to apply to each column , for ` WL ` column we apply ` count ` from ` pandas.Series ` , the ` all ` applies a test on all values and returns ` True ` if all values in the series are ` True ` and ` False ` otherwise . #CODE
In order to assign these values back to the original dataframe you can use ` transform ` , unfortunately I couldn't figure out how to apply different functions to different columns as transform won't accept ` agg ` function or a user defined function .

Pandas groupby apply how to speed up
and if ` objs ` is a list that contains above results you may join them with : #CODE

This should align on the levels ( though their is some ambiguity how to do it , e.g. on which level ) . #URL

Python pandas boxplot from dataframe
I have the following dataframe , which has a multiindex . I thought I could groupby run then create a boxplot to show the variance between the different runs , but that doesn't seem to work . #CODE
how do I create a boxplot from it ?

This clearly is a bug , ` aggregate ` will try to convert the result to the same ` dtype ` the original ` DataFrame ` has . Here the ` sum ` will return ` 1 ` and ` bool ( 1 )` is ` True ` . If both of the values in ` ok ` are ` False ` s , the result will be ` False ` ( ` bool ( 0 )`) . Further examples : #CODE

Then , we ` melt ` the dataframe to merge the rows and columns together ( i.e. ' Jan ' column and ' 1997 ' row become a single ' Jan 1997 ' row with the correct percentage value ) . #CODE
Thanks . melt is what I was looking for .

rolling apply for a binary ( or n-ary ) function in pandas
To do this , I have the following function that I would like to use a rolling apply with - all this does is calculate covariance assuming zero mean if not centered and calculate the usual covariance when it is centered . #CODE

It is possible that I need to replace ` c ` with ` c.getvalue() ` in the read_csv line , but when I do that , the interpreter tries to print the contents of ` c ` in the terminal ! Surely there is a way to work around this .

There is a ` map ` function for pandas for doing this , so you would just do something like : #CODE
This will assign a new column ' col_id ' to your dataframe and map the string values to their int counterparts .

Thanks ! Seems like this is what I need . Unfortunately , something's not working with the loop . I get ` KeyError : ' swap '` pointing to the ` if ` statement . I thought it was a datatype issue ( i.e. float vs bool ) , but tweaking that didn't resolve the error .
Turns out the way the ` swap ` series was made resulting in a wrong series in ` pandas 0.14.0 ` , changed that everything works . Again see ` [ in ] 114 ` . ` swap =p d.Series ( { ' swap ' : False} , index =d ates )` results in a series of ` nan ` s in ` float dtype ` , not the intended ` bool ` type ` False ` .

merge the describe method output in pandas
I'm using pandas to do data munging and I can't seem to figure out what seems like a basic merge . Essentially , I have multiple describe methods on a dataset . The output of the describe method is shown below . I used simple numbers for this example . The name of the column is the same ( Metric4 ) . #CODE
I want to merge these together is order to get an output that looks like the following : #CODE
This is a rather simple case of ` merge ` , in which you can supply the suffixes with the additional ` suffixes =( '' , ' 2 ')` argument : #CODE

I want to find out the mean of the differences of creationdates of each date . For this I am doing a groupby and call diff and then mean on the grouped data #CODE

pls show your input and what is the expected output , in a copy-pastable form . What you are doing is very inefficient . A groupby should try to use vectorized functions when possible . Then join them up at the end .

You can just call ` apply ` and access the ` time ` function on the datetime object create the column initially like this without the need for post processing : #CODE

it does not apply operator by elements , but returns a 2*n DataFrame of NaNs : #CODE
this uses indexing by column name , and doesn't use logical operators on columns , rather than that it traverses rows with apply function : #CODE

if i understand your problem correctly , you want ` merge ` these 2 dataframe on ` index ( date ) , Product , Hub ` and obtain ` Period ` from ` data_1 `
To join on index + some columns , you can do this : #CODE

Generate your pdf of matplotlib figures as before and then insert pages containing the dataframe table afterwards . I view this as a difficult option .

Really ? So Hash [ 1 - ( 1 / 3*3 )] ! = Hash [ 0 ] was my point , but even without arithmetic , there will be a huge range values for the keys that will give potentially unfortunate results . I'd avoid this at all costs personally . if precision is to decimal place , I'd multiply it by 10 and truncate maybe .

How to resample data in a single dataframe within 3 distinct groups
I've got a dataframe and want to resample certain columns ( as hourly sums and means from 10-minutely data ) WITHIN the 3 different ' users ' that exist in the dataset .
A normal resample would use code like : #CODE
However , in the original CSV , there is a column containing URLs - in the dataset of 100,000 rows , there are 3 different URLs ( effectively IDs ) . I want to have each resampled individually rather than having a ' lump ' resample from all ( e.g. 9.00 AM on 2014-01-01 would have data for all 3 users , but each should have it's own hourly sums and means ) .
You can ` resample ` a ` groupby ` object , groupby-ed by URLs , in this minimal example : #CODE

Thanks !! I figured out the ` openpyxl ` dependency problem and now it's working fine :) The output of ` print df.head ( 5 )` now prints a table but the table is cut two times with a \ is that just to make it easier to visualize longer rows on a small screen ?

thanks for the reference although I haven't been able to apply it to my specific problem .
There's also evidence that statsmodels supports timeseries from pandas . You may be able to apply this to linear models as well :
Now , if I delete this ' date_delta ' setting , then it compiles but the problem is that len ( result.params ) = # of unique dates , instead of the number of parameters for one linear regression performance . Comments welcome .

Such a simple solution . Great thank you ! One more question though . I have data of four weeks and sum precipitation currently on hourly base . This gives me lots of hours with 0 precipitation . The Problem is when I try to plot such a large dataframe as a bar chart pandas / matplotlib tries to also plot the 0 values and the plot is useless . have you an idea how to solve this ? should I replace all 0 values with np.nan ? Thanks

I could also ask : How do I merge a Python object ( ` meta : { ... } `) into a serialised JSON string ( ` df.to_json() `) ?

It worked fine with the test data ( 200 lines ) but gives me the following error when I apply it to the real data ( 20 million lines ): #CODE

I am trying to create a stacked histogram with data from 2 or more uneven pandas dataframes ? So far I can get them to graph on top of each other but not stack . #CODE
How do I get them to stack ?

I ended up just not using pandas to append to the db . But now that I have some time I can have another crack at it .

use a lamda apply to pass groups to the function
Anyhow , to get the differences of the TimeSeries , I found using ` shift() ` to get the time differences threw a ` StopIteration ` error , using ` diff ( 1 )` threw no errors .

If you are sure they have the same length , I'm assuming they are both ` pd.Series ` indexed by integers . You can use ` timestamps ` as index for ` trades ` instead , and do a ` resample ` . I use random data to show methodology : #CODE

Could you update your question with your new comment as it clarifies things more , also explain how you want to merge the rows in the situation where you have fewer rows in A or B

This will work by calling ` apply ` and passing param ` axis=1 ` to apply it row-wise : #CODE

You simply ` groupby ` your ` time ` column and then apply the ` mean ` method to each element . See documentation here . #CODE

Surely , this can be done more compactly with Pandas ( pivot ? ) ? Do you know a neat way to bin the two features ( for example 10 bins on the interval 0 ... 1 ) and then plot a class density heatmap where color indicates the ratio of 1's to total rows within this 2D-bin ?
Yep , it can be done in a very concise way using the build in ` cut ` function :
Thanks for the ` cut ` and heatmap suggestion ! Though , it seems ` crosstab ` isn't doing the right thing ? For each bin I want to plot the ratio between 1's and the total points * within this bin* . That's why I divide ` sum / count ` . That's different from ` crosstab ` ? Is there a concise solution with ` pivot ` ?

I would like to be able to just access the values in a row in a pandas data frame without all the additional metadata . For example , when I get the 0th row of the color column , I would like to just get " green . " However , when I use ` loc [ 0 ]` I get additional metadata : #CODE

Judging by the line that reads ` 1093 / 1092 0.142 0.000 9.162 0.008 indexing . #URL ( _setitem_with_indexer )` , I strongly suspect my nested loop assignment with ` loc ` to be the culprit . The whole function takes about 9.3 seconds to execute and has to be performed 144 times in total ( i.e. ~22 minutes ) .
Why not -- after labeling the interval's trial number ( and instead of creating all new column's with NaNs ) -- you assign the appropriate values via some sort of left join instead ? ( i.e. using pandas ** merge ** function -- is there some reason why that's not possible ? )

Pandas Merge on Name and Closest Date
I am trying to merge two dataframes on both name and the closest date ( WRT the left hand dataframe ) . In my research I found one similar question here but it doesn't account for the name as well . From the above question it doesn't seem like there is a way to do this with merge but I can't see another way to do the two argument join that doesn't use the pandas merge function .
Is there a way to do this with merge ? And if not what would be the appropriate way to do this ?
I will post a copy of what I have tried but this was trying it with an exact merge on date which will not work . The most important line is the last one where I make the data3 dataframe . #CODE
You won't be able to merge using a partial match , you'd have to merge what you can and then perform a lookup for the other rows , I've done this before where there were inexact matches . You have to write some function and then apply it row-wise to your merged dataframe
I was kinda implying that you'd need to try to write the function yourself , basically the elements are 1 . does a date exist , if not then do a searchsorted call on the data to find where if I was to insert the date , what the index value would be , then use this index value plus the previous value to see which is closest and return it .
It did take a long time though . I sure still would have liked to have a way to do it in merge . :)
Merge the two DataFrames on the new ` closest ` date column #CODE

Does ` df [ ' date '] =p d.to_datetime ( df_cal [ ' date '])` work ? also you can just drop the time portion by doing ` df [ ' date '] = df [ ' date '] .apply ( lambda x : x.date() )`

you don't need to strip the time when you are resampling , FYI , that's the point of resampling by say ' D ' it ignores the times .

and I'm trying to apply a transformation in order to get a dataframe that looks like the following #CODE
Is there a simply way to do this ? I have tried using melt , groupby , and pivot_table but with no luck . This seems like such a simple task so perhaps I am overthinking it .
One way would be to assign to an empty dataframe the ' ID ' and ' Wt.1 ' columns to the empty dataframe as the target ' ID ' and ' Wt ' columns , this has a minor advantage in that you don't get a messy append at the end where you have ` NaN ` values and both ' Wt ' and ' Wt.1 ' columns . #CODE

what happens if you just do ` pd.read_csv ( ' data / training.csv ' , sep= ' \t ' , index_col=0 )` does this read the values correctly ?, also what happens if you drop the ` index_col ` param , it looks like it is unnecessary seeing as you are assigning a new one later , by default it will assume your csv has no index column so if that is your intention then remove the param
` pd.DataFrame ( data2.values ` is the key here . ` data2 ` is a ` DataFrame ` and has its own index . Now you want to wrap it in a new ` DataFrame ` with new timeseries index , ` pandas ` will try to match and align the original index with the new one , but there are no matches .

How to use a for loop to append the data frame generated from a function in python
My question is how can I apply this function to several classifiers and append their result as a long data frame like #CODE
I also tried map function in array like #CODE
Thanks ! But I am still a little bit confused while ` i ` is in ` hhh ` why it does not do the concat or append as your code does ? Thanks again ! :) @USER

` d.index = d.index.apply ( lambda x : x.time() )` won't work . I finally managed to do it by doing a reset of the index , apply , and set again :
You can simply use ` slice ` , and if the column is not ` str ` you need to map to ` str ` as well : #CODE
If the column is ` Timestamp ` you may also , map to ` pd.Timestamp.time ` : #CODE

Insert a pandas DataFrame plot into a matplotlib subplot
So far I have been able to input what I have wanted into the subplots , but I want to be able to input a pandas DataFrame plot into ax3 and I can't seem to do it . I have already written the pandas program and was just going to insert it into the larger script so it was shown in the subplot .

What I'd like to do is normalize the values to the sum ( along the major_axis ) at the first timestep , i.e. , I'd like to do #CODE
yes , it works , but is not what I need ;) I want to normalize each * item * individually , so simply taking the ` max() ` won't do .

A simple method would be ` df.loc [: (( df [ ' A '] == 0 ) & ( df [ ' C '] == 0 )) .idxmax() ]` but this doesn't apply for every id
@USER you can just groupby id then apply idxmax ( filter before the groupby )

you are using in append mode . So if you run your script eight times , data will be replicated each time .

Ok ! Sorry I'm pretty new to python and stack overflow

Optimizing Pandas groupby / apply
I suspect you prob want to merge the final transform back into the original frame #CODE
I've been trying to apply your code to my dataset but keep running into MemoryErrors . My local machine only runs python 32 so i switched to an Amazon m3.medium ( 1 VCPU 3.75GB Mem 64-bit python ) but even this instance Kills the job . How can you achieve these kinds of runtimes ?

how to unstack ( or pivot ? ) in pandas
I thought I could unstack df2 to get something that resembles my final dataframe but I get all sorts of errors . I have also tried to pivot this dataframe but can't quite get what I want .
There is still a bit of cleanup to do if you want to convert the index level " variable " into a column called " HOUR " and strip out the text " HOUR " from the values , but I think that is the basic format you want .
The ` stack ` method turns column names into index values , and
the ` unstack ` method turns index values into column names .
So by shifting the values into the index , we can use ` stack ` and ` unstack ` to perform the swap . #CODE
thank you for the detailed explanation . It has helped me better understand stack / unstack . Thank you .

Thanks for your answer . You see , my issue is that the NaNs are only introduced by pandas in the first place . In my original data I don't have any NaNs . They appear when I'm pivoting my data . Now , I completely understand why pandas introduces NaNs to align the data . However , do you know of an easy way to make pandas skip them when plotting ? The fix you describe would work , of course , but then I could skip pandas all-together and directly plot the results of my individual simulations . Maybe ` pivot_table ` is the wrong function for what I want to achieve . Can you comment on that ?
I think the interpolation is actually the right thing to do if you need to have the data in common time grid . You essentially have points in time without the data . Then you'll just have to guess the data , and interpolation is one way of doing that . I am not a pandas expert , but your pivot does not look bad . If the measurement results had different names for different runs ( e.g. ` f1_1 ` , ` f1_2 `) , you could use ` concat ([ df1 , df2 , df3 ]) .sort ( ' t ')` , but still you'd have the ` NaN ` s there .

I would like to insert 3 rows after each grouping in column `' a '` . Specifically , I want to have some auto-incrementation in column `' b '` and put ` None ` objects everywhere else : Something like : #CODE
What you want to do is not really an insert operation , as the data structure behind the ` DataFrame ` does not allow simple inserting . So , in essence , you will have to build a new ` DataFrame ` from the pieces of your old ` DataFrame ` .
Append the slice from the existing table to the new ` DataFrame `
Append the new data to the new ` DataFrame `
( Or you can concatenate instead of append , if you find it easier . ) One thing to think of is what you do with your indices . If you do not use them , you may ignore them (= create new as needed ) by using the ` ignore_index=True ` keyword argument on ` concat ` or ` append ` .
Just ` concat ` the inserts that you want to insert in ( and they will be appended in the rear , or ` df.append ( the_insert )` , which does the same thing ) and ` reset_index ` the resultant to get things in the right order : #CODE

Effectively , I will try to join m1 and m2 together into a single dataframe using merged / join .
thanks for the prompt reply ! I thought about this but actually my m1 has 70,000 + records and my m2 has 4000+ records I am just afraid that if I merge them together the computer will run out of memory . I will try your codes later on when I finish something in my python in case it will crash .

I have a pandas dataframe with mixed type columns , and I'd like to apply sklearn's min_max_scaler to some of the columns . Ideally , I'd like to do these transformations in place , but haven't figured out a way to do that yet . I've written the following code that works : #CODE
I know that I can do it just in pandas , but I may want to eventually apply a different sklearn method that isn't as easy to write myself . I'm more interested in figuring out why applying to a series doesn't work as I expected than I am in coming up with a strictly simpler solution . My next step will be to run a RandomForestRegressor , and I want to make sure I understand how Pandas and sklearn work together .

how about ` len ( ts ) == 0 ` ? edit : ` ts.empty ` should work
` len ( ts )` worked for me ` ts.empty ` didn't
I use len function . It's much faster than empty() . len ( df.index ) is even faster . #CODE

The only remotely useful column of data is the second one , ` tottime ` , and that just looks like " self time " . What you need to know is not self time , but inclusive time , and not as an absolute time , but as a percent , and not just of functions , but of the sites where they are called . Also , the number of samples does not need to be large . If your program takes 15 seconds when it should take less than one second , then the odds are 14:1 that a * single stack sample * will show you why it's taking that time .
Using apply is always the last operation to try . Vectorized methods are much faster . #CODE

This works for me ! Brilliant use of cut ! Thanks champ

@USER That works for the entire dataframe easily . How would you apply this to a single column ? ` df [( ' date ' , '' , '')] .swaplevel ( 0 , 2 )` did not work .
@USER The dataframe is generated from a pivot table that has the index reset . I need to format the table in a specific way before outputting it to a csv , where it will go on to take a life of its own . I don't get to choose the level of where the ' date ' text gets placed .

However what comes out is the below vardataframe.head() result , which does not properly change the index of the table from Symbol back to numeric . And this hurts me in a line or two when I try to do a merge command . #CODE
As you see the problems with the above are now there are two Symbol columns and the index hasn't been set correctly . What I'd like to do is get rid of the second Symbol column and make newindex the new index . Anyone know what I'm doing wrong here ? ( Perhaps a misunderstanding of the ix command ) . Much appreciated !
gives you a column ` Symbol ` of all ` NaN ` because ` Symbol ` is not a column of ` vardataframe ` , but only exists in its ` index ` . Querying a non-exist column with ` ix ` gives all ` NaN ` . As @USER mentioned , you can do ` vardataframe.reset_index ( inplace=True )` ( or ` vardataframe= vardataframe.reset_index() ` to put ` Symbol ` back as a column .

How can I apply this process to all the columns I want at once and produce a dataframe of it all ? Sorry if this is a repeat ; the pandas questions I've found that seem to be about related topics are all over my head .

So do you want to have the same values for rows with the same data ( year , month , day ) ? Maybe you could ` merge() ` ( like ` join ` in SQL ) two dataframes if you had year , month , day in grouped data .
` merge() ` can do ` left join ` like SQL and should duplicate data from group in rows in dataframe .

However , the above does not work for a ` boxplot ` ( the tick labels for the x axis are rendered empty )
Why ? How can I use ` boxplot ` in a way that allows me to use ` matplotlib ` date locators and formatters ?

I have 3,000 .dat files that I am reading and concatenating into one pandas dataframe . They have the same format ( 4 columns , no header ) except that some of them have a description at the beginning of the file while others don't . In order to concatenate those files , I need to get rid of those first rows before I concatenate them . The ` skiprows ` option of the ` pandas.read_csv() ` doesn't apply here , because the number of rows to skip is very inconsistent from one file to another ( btw , I use ` pandas.read_csv() ` and not ` pandas.read_table() ` because the files are separated by a coma ) .

How do I convert a groupby dataframe I created in order to drop duplicates by a group back into a normal dataframe ? #CODE
Instead of creating a groupby to drop duplicates , have you considered : #CODE

Some first solution is to create another lists and just put the values into the lists . At the end I can normalize the length of lists ( for example with some values like ` NaN `) and create from these lists DF .

You can use pandas ` shift ` and ` loc ` to filter out consecutive duplicates . #CODE
+1 , I think you have to use ` shift ( 1 )` in place of ` shitf ( -1 )` to get OP more expected result .

Merge the two dataframes #CODE

Pandas drop duplicates if reverse is present between two columns
I'm using Pandas and I want to drop rows which are present twice but simply reversed like the following ... from this ... #CODE
now concat the 2 dataframes : #CODE

I thought I could apply a list of strings , but it does not work either , because it is ` not in the ColumnDataSource ` . If I zoom in deeper , the numbers are getting even less meaningfull . Then it might say ` 03 ` , but 03 of what ? At which minute , which hour ? Is there a solution for this ?

python pandas : groupby apply function looks at prior rows
so I've updated my code below , maybe I'm not understanding how apply works , but I thought this would execute twice ( once for each group ) . Then , my function would loop over each row within those executions . I'm still puzzled as to why it's going 3 times ... I thought " executed " would print 5 times . Thoughts on this ?
So this contains all the information you need . At the moment , I'm struggling to merge this information back to the original dataframe . ` df [ ' differences '] = test ` gives a huge mess .

Panda's DataFrame double transpose changes numeric types to object

I have two dataframes that I would like to concatenate column-wise ( axis=1 ) with an inner join . One of the dataframes has some duplicate indices , but the rows are not duplicates , and I don't want to lose the data from those : #CODE
The default ` concat ` behavior is to fill missing values with NaNs : #CODE
I want to keep the duplicate indices from df1 and fill them with duplicated values from df2 , but in pandas 0.13.1 an inner join on the columns produces an error . In more recent versions of pandas concat does what I want : #CODE
What's the best way to achieve the result I want ? Is there a groupby solution ? Or maybe I shouldn't be using ` concat ` at all ?
You can perform a merge and set the params to use the index from the lhs and rhs : #CODE
Concat should've worked , it worked for me : #CODE
Ok , merge works in both 0.13.1 and 0.14.0 , so thanks for that solution . Concat gives me the ' cannot reindex from a duplicate index ' error in 0.13.1 , and in 0.14.0 I get : ' ValueError : Shape of passed values is ( 2 , 5 ) , indices imply ( 2 , 3 )' . I wonder why ...? I'm running python 3.3.5 .
same here . At least merge works
I have version 0.16.2 and with multi-index none of the solutions does not work for me ; merge gives a dataframe as output but with many columns of the left array as NaN - and they should not be .

its not specific to apply , but more general in groupby , nor is ever point mentioned in the doc-string .

Or something like ` df.columns = [ ' blah{} ' .format ( i ) for i in range ( 1 , len ( df.columns ) +1 )]` , or `" blah " + pd.Series ( range ( 1 , 6 )) .astype ( str )` , etc .

I tried to apply this method to each subset of data with a nested-loop script : #CODE
Grouping data frames and applying a function is essentially done in one statement , using the ` apply ` -functionality of pandas : #CODE

pandas : apply a function to the many columns of a large DataFrame to return multiple rows
Taking the idea from From this answer : pandas : apply function to DataFrame that can return multiple rows

This is by far the craziest question I have asked on SO but I am going to give it a shot in the hope of getting some advice about whether or not I am leveraging the right tools and methods for processing large amounts of data efficiently . I'm not necessarily looking for help on optimizing my code unless there is something I am completely overlooking but essentially would just like to know if I should be going with a different framework all together instead of Python . I'm new enough to Python to not be completely sure if it is possible to process large amount of data and store into DB much more efficiently . Essentially the below implementation reads text files in a directory with each text file containing 50K lines of json objects that need to be parsed and read and then converted into a csv before loading into a database . I hate using list containers and I am hoping that there is something else I can research implementing in Python for doing this in a much much better way . My initial thoughts are that I should be researching using generators but not totally sure . The crazy concat part at the end is important because it converts a comma separated list into it's own rows .
The ' key36 / 62 / 65 ' each occur twice in that weird concat construct . Was that intentional ?
If you have " oddly-shaped " json , then you can either ` json_normalize ` when reading , or parse the columns which contain multiple columns after reading in the DataFrame ( e.g. using a Series string method or apply ) .

Note : ` astype ( ' int64 ')` would also work , or just leaving it as bool ...

After you sort the df , all your loc / ix methods should work . #CODE

I would like to map each file to its unique dataframe . Something like : #CODE
An even better solution is to drop the range : #CODE

just use an ` apply ` with a function that creates a dictionary based on the ` str.count ` of the substrings

Well , I am creating somewhat of a randomizer for an experiment . In order to counterbalance appropriately , I want to be able to randomize the rows and the columns independently from each other , but the data inside the table isn't all ints , but rather , lists of strings , dictionaries , and such . That said , I am trying to find out if there is a way to basically do what was done in the link I posted ( randomize column-wise ) and apply that to rows . I was able to make this work , but only if the dataframe contains numbers only , though I want to extend the possibility to strings and such .

My question : how can I join the results to display as desired ? I tried merge , join , and append , but they do not allow for empty data frames ( which happens ) . Also , I am sure there is a simple way to bin the data by the dates . Thanks .
Alternatively , you may use ` groupby ` together with ` cut ` : #CODE

I would like to interpolate the missing values . How do I do this ?
Tried using : fillna ( method= ' ffill ' , inplace=True ) which seems to work , however I need to interpolate :
the page you linked suggested interpolate ( method= ' linear ') but this does not seem to work ?
Tried using : apply ( pd.Series.interpolate ( method= ' linear ')) however I get the following error :

The first command fills the ` None ` values with ` np.Inf ` to save them from being dropped in the following ` dropna ` on line 3 . Then ` pd.concat ` is used to stack the columns together on top of each other . After dropping the spurious ` None ` values from columns 2 and 3 , the ` np.Inf ` placeholder values can be assigned their desired value .
Thanks . I didn't think of the concat trick !
I wouldn't be surprised to be able to do the concat even without the list comprehension , but I could not find a way .
Another one using ` loc ` and loops : #CODE

Python pandas , How could I read excel file without column label and then insert column label ?
I have lists which I want to insert it as column labels .

Python Pandas concatenate / merge DataFrames using index or column identifiers
I would like to use the ' pandas.concat ' method to merge two DataFrames , but I don't fully understand all ' pandas.concat ' arguments . I've got two DataFrames , which have the same identifying variables in the columns , but differ in one single column . #CODE
Question : How can I achieve the same result ( shown above ) using ' concat ' ?
No I have not . The dataset is large ( rows 3847440 , cols 16 ) , and I don't know how to test for the break . How can I test for a break ?? I'm asking for the concat method since I'm hoping that there is an argument ( s ) that will rely on the indices of the two dataframes to concat / merge them .
fair enough :) I would just like to have certainty that the concatenation doesn't break the dataframe . If you know how to achieve the same result using the concat method please be so kind and enlighten me .
and you need to keep order using values from columns ` AAseq Biorep Techrep Treatment ` then use ` merge ` #CODE
THANK YOU VERY MUCH !! The ' merge ' function is EXACTLY what I was looking for / missing . Awesome . Your help is much appreciated !
` merge ` works similar to ` join ` in SQL .

Do I want to join / merge , concat , or append these two Pandas DataFrames ?
How would I paste these two tables together using join / merge / concat / append / add in such a way that the population age 0-14 , and 15-64 columns are side-by-side ?

Since it is hard to predict what expressions return a copy or a view , " chained assignments " are to be avoided . Instead , assign to an indexer like ` loc ` or ` ix ` : #CODE

I've noticed that for a ` DataFrame ` with a ` PeriodIndex ` , the month reverts to its native ` Int64 ` type upon a ` reset_index() ` , losing its ` freq ` attribute in the process . Is there any way to keep it as a ` Series ` of ` Period ` s ?

I am working on a wind analysis for a new development . I would be able to predict the air flow pattern in the development for each hour of the year as a function of the wind speed and direction for that particular hour . Of course it would take too much time to run 8760 wind CFD simulations . My approach is to run only 16 simulations ( 8 wind directions and 2 wind speed ) and interpolate the flow distribution from these results .
I ended up using a different approach . I first append the dataframe with the unknown velocities to the first dataframe with all the simulation data . I then cut this new dataframe by binning the wind directions . At this point I sort the table by wd , X , Y , Z . This leaves nice gaps for each grid node that I can simply fill by interpolation using the dataframe interpolation function . It works with a small sample data test . Will try in these days with the full set of data .

Use ` merge() ` - it works like ` JOIN ` in SQL - and you have first part done . #CODE
The basic idea is to merge the two tables so you have the times together in one table . Then I filtered on the recs that are the largest ( closest to the time on your dfB ) . Let me know if you have questions about this .

Ive been hacking away with update , ix , filtering , where , etc ... I am guessing there is an obvious solution I am not seeing here .

My idea was to apply this function ( or similar ) column wise . Have played with .apply() but because its a double ( or triple ) function call i.e. f1 . ( ) .f2 ( x , y ) or f1 . ( ) .f2 ( x , y ) .f3 ( x , y ) it gives me an error . Any ideas would be greatly appreciated and I think this would be a very useful bit of code to have out there !

how to apply preprocessing methods on several columns at one time in sklearn
My question is I have so many columns in my pandas data frame and I am trying to apply the sklearn preprocessing using dataframe mapper from sklearn-pandas library such as #CODE

I've got a fairly large data set of about 2 million records , each of which has a start time and an end time . I'd like to insert a field into each record that counts how many records there are in the table where :
active [ i ] = len ( a [( a [ ' Start '] <= a.loc [ i , ' Start ']) & ( a [ ' End '] > a.loc [ i , ' Start '])])` . Do you think this is slower , faster , or comparable in speed ?

Merge two DataFrames with some equal columns
I need merge both DataFrames in something like this ( ovewriting values on df1 that are empty on df2 , adding extra columns and rows that not exists on df1 ): #CODE
I tried using ` merge ` getting something like : #CODE
But it's not what I need . I don't know if I'm on the right path and should merge the suffixed columns or if there is a better way to do this .
Added ovewriting values on df1 that are empty on df2 , adding extra columns on df2 that should be present on df1 after " merge " and rows that should be appended on df1
pd.merge has a couple of multipurpose params . The on key actually is actually only used to join the two dataframes when the left_index and right_index keys are set to False - the default value . Otherwise it will just join the identically named columns that are found from the on value . In this case the two columns ' text ' and ' noteId ' . ( I made it a more general by using df1.columns.tolist() as the param - this means any identically named columns in df2 will overwrite the data from df1 instead of marking it text_y )
Hi I looked at the update solution which should've worked but update only supports left merges . I dug a little deeper into the capabilities of merge and found the solution .

Pandas : How to unstack a series
Assuming that your two first columns form a MultiIndex , you can simply use unstack : #CODE

Is it possible to edit index using something like replace ?
alternatively , you may just ` map ` the index to ` int ` : #CODE

Pandas : Merge hierarchical data
I am looking for a way to merge data that has a complex hierarchy into a pandas ` DataFrame ` . This hierarchy comes about by different inter-dependencies within the data . E.g. there are parameters which define how the data was produced , then there are time-dependent observables , spatially dependent observables , and observables that depend on both time and space .
The ` index =[ 0 ]` in ` df_parms ` seems necessary because otherwise Pandas complains about scalar values only . In reality I would probably replace it by a time-stamp of when this particular simulation was run . That would at least convey some useful information .
Merge Observables
With the data-frames available , I join all the observables into one big ` DataFrame ` . #CODE
This is the bit that I like the least in my current approach . The ` merge ` function only works on pairs of data-frames , and it requires them to have at least one common column . So , I have to be careful about the order of joining my data-frames , and if I were to add an orthogonal observable then I could not merge it with the other data because they would not share a common column . Is there one function available that can achieve the same result with just one single call on a list of data-frames ? I tried ` concat ` but it wouldn't merge common columns . So , I ended up with lots of duplicate ` time ` , and ` site ` columns .
Merge All Data
Finally , I merge my data with the parameters . #CODE
Thanks to Jeff's answer I was able to push all my data into one data-frame with a generic merge . The basic idea is , that all my observables already have a few common columns . Namely , the parameters .
And then I can merge all of them together by reduction . #CODE
I think you should do something like this , putting ` df_parms ` as your index . This way you can easily concat more frames with different parms . #CODE
It seems messy , but based on your data might not be an easier way . you can prob get away with `` df_position.join ( df_weight )`` though ( its the same as your merge , but ' cleaner ')
I was able to merge them all together with generic merge operatinos by using your answer and adding the parameters as common columns to all data-frames . You can see the details in my edited question .

Split a data frame using groupby and merge the subsets into columns
So the index is non-unique but is unique if I group by the column ` name ` . I would like to split the data frame into subsections by name and then assemble ( by means of an outer join ) the score columns into one big new data frame and change the column names of the scores to the respective group key . What I have at the moment is : #CODE
So ` unstack ` seems the method of choice .
The function you look for is unstack . In order for ` pandas ` to know , what to unstack for , we will first create a ` MultiIndex ` where we add the column as last index . ` unstack() ` will then unstack ( by default ) based on the last index layer , so we get exactly what you want : #CODE
It works but ` unstack ` provides a faster way . Thank you for your answer @USER .
Even though ` pivot_table ` is not as fast as ` unstack ` , for my real data frame with 10 M rows this method brought down the processing time from 28 min to a respectable 17 s , thank you :)

Also , if you insert ' df [ ' Yield '] = Px ( .05 )' , it creates the new column with the Px() for that Rate . So i think it is something to do with the YieldCalc formula .

Typically everything that you do within ` groupby ` should be group independent . So , within any ` groupby.apply() ` , you will only get the group itself , not the context . An alternative is to compute the ` index ` value for the whole sample ( following , ` index `) out of the indices for the groups ( here , ` selected `) . Note that the dataset is sorted by groups , which you need to do if you want to apply the following .
Thanks @USER for the quick reply . Unfortunately it does not work for me : I get ` ValueError : level > 0 only valid with MultiIndex ` when trying to apply your line ` c = test.groupby ( level=1 ) .count() ` . I think it must be due to the fact that my original data is not indexed as yours ( 0 , 1 , 2 , 0 , 1 , 2 ) but ( 0 , 1 , 2 , 3 , 4 ,... ) . Did you already group text by ' name ' before applying your solution here ?

We use ` loc ` here to perform label indexing , see the docs : #URL
its not the date that I'm changing conditionally , but rather whether or not I'm going to append another variable to my FileName series .

I am merging several CSV files and sometimes the customer contains letters and pandas imports as a string . When I try to merge the two dataframes I get an error because I'm trying to merge two different types . I need everything stored as strings .
you can drop the `` skip_footer `` option and drop from the frame after ( though this might cause dtype issues itself ) , you could also pre-filter to get rid of the footer . as an aside , you could help address those missing issues ! ( the dtype missing from the python engine is pretty easy actually )

that's trivial . replace ` df ` with ` df [ df.columns [: 2 ]]`

I don't know of any pure numpy solution for what you want but you can use list comprehension and zip to transpose . #CODE
Use concat : #CODE
If you want to append as rows then remove the ` axis=1 ` param .

Some of my columns contain NaN values which I do not want to include into the z-score calculations so I intend to use a solution offered to this question : how to zscore normalize pandas column with nans ? #CODE

pandas groupby filter , drop some group
I need to filter these groups and drop all groups which have no time greater than 5 .

I would first use concat to create a larger DataFrame : #CODE
I think the easiest way to append a new index is to add a new column and then ` set_index ` : #CODE

@USER I agree , but in another function I need to take a data frame and compute various statistics and append those to the output , but I can't do that without changing the original it seems . This illustrates the kind of things I need to do .

Replace column values based on another dataframe python pandas - better way ?
N #URL simplicity's sake , i'm using a toy example , because copy / pasting dataframes is difficult in stack overflow ( please let me know if there's an easy way to do this ) .
Is there a way to merge the values from one dataframe onto another without getting the _X , _Y columns ? I'd like the values on one column to replace all zero values of another column . #CODE
I want to replace zeros in df1 with the values in df2 .
I subset based on the names that exist in df2 , and then replace those values with the correct value . However , I'd like a less hacky way to do this . #CODE

This seems like it would be especially useful for stack overflow , but I often find myself needing to share small dataframes and would love it if this were possible .

Sounds like you are on the right track . Creating two groups is a good approach . Since your precip_avg starts as " the average precip level for all Jan 1st 2pm across all years " this average will not change when you apply other filters . The average would be the same if you looked at the past 50 years or the past 5 years . This may be desirable or it may not be .

Assuming I understand you , and you're looking for the indices where either the element is True or the next element is True , you can take advantage of ` shift ` and use #CODE
Using ` shift ( distance )` you compare two rows : #CODE

If I only want the Year month and day how do I drop the trivial hour ?
The quickest way is to use DatetimeIndex's normalize ( you first need to make the column a DatetimeIndex ): #CODE
From 0.15 they'll be a dt attribute , so you can access this ( and other methods ) with : #CODE

I've tried turning the series into a dataframe with a dummy column in order to try merge or lookup but for various reasons I can't get that to work . There is missing data so key errors are possible with some potential methods .
Previously answered questions don't seem to apply . Someone good with lambda functions or the .asfreq method might be able to come up with something . #CODE

( There are lots of other ways to convert from ` bool ` to ` int ` ( ` +0 ` , ` *1 ` , etc . ) but this is more explicit . )

replace x = cond ( x > 100 y 50 , x , y )
Using StackOverflow and the documentation I have only been able to find how to apply a function dependent on a single variable to more than one column ( using the axis option ) . Please help .
This will be much faster than performing an apply operation as it is vectorised .
@USER you're welcome , you can accept this as answer , there will be a tick mark underneath the voting buttons . Using apply and iterating should always be the last choice , if possible find a method that operates on the whole dataframe

A defaultdict sounds scary , but it's just a dictionary . In this case , each value is a list . If we ` append ` to a nonexistent value , then it gets spontaneously created . Convenient !
This means that all you have to do to group by year is leverage the apply function and re-work the syntax

Length : 51225 , Freq : None , Timezone : None
To drop a range : #CODE
To drop a row : #CODE
It is not that drop doesn't work for dates . It seems instead that it does not automatically converts strings to dates as slice indexing does .
2 . waht should I do to cut some range ? thnx
yes it works fine but I need to drop ' 2014-07-16 14:24 ' thnx
to drop a range in a middle of the df . thnx
Please edit your question , so it asks specifically about dropping a row . The way it is written right now it suggests that you want to drop a range , and that's why I answered it that way . Is my edited answer what you were looking for ?
pls , look at the index . pls , look at the qstn . the request should drop everything before 2014-07-16 14:24 , but it doesn't . you gave the workaround but the problem is still there : df.drop() doesn't work with datetime index .

Dynamically creating variables , while doing map / apply on a dataframe in pandas to get key names for the values in Series object returned
It looks like THIS loop is the killer here.Also , intutively , looping on a dataframe is a BAD practice . How can I rewrite this , perhaps using Map / Apply ? #CODE
I also know that ` df2.apply ( funct1 , axis=1 )` contains part of mycustom " names " ( ie feature values ) , how would I then build these names using map / apply ?
Ie . I will have the values , but how would I create the " key " `' P_ ' +feature_name+ ' _ ' +feature_value+ ' _C '` , since feature value post apply is returned as a series object .
I have only unique values in the df2 ( for a feature_name ) , will pivot help ? See also , day_custom & is_tablet_phone.txt files . If pivot will help , please also explain the intuition on WHY I need to pivot in this case ( with no duplicate values ) ?
I updated my post as you asked . Also I think there are some problems with the code . word = feature_name + " _ " + feature_value - Feature_value is a tuple object , so we cant concat them . Also newtable = df2.pivot_table ([ ' click ' , ' impression '] , ' feature_name ' , \
Gregor - +1 for making the time & effort . I have to admit that itertuples() struck me , omly when I saw the tuples in your code . I am not accepting , since I am still looking at a map / apply way ( possibly ) to solve this .
itertuples() is what worked for me ( worked at lightspeed ) - though It is still not using the map / apply approach that I so much wanted to see . Itertuples on a pandas dataframe returns the whole row , so I no longer have to do ` df2 [ df2 [ feature_name ]= =feature_value ] [ ' click ']` - be aware that this matching by value is not only expensive , but also undesired , since it may return a series , if there were duplicate rows . itertuples solves that problem were elegantly , though I need to then access the individual objects / columns by integer indexes , which means less re-usable code . I could abstract this , but It wont be like accessing by column names , the status-quo . #CODE

Replace date with NaT in Pandas dataframe
To answer your second question most pandas functions handle NaN's appropriately , you can always just drop them : #CODE
Thank you for the solution . The thing is that I didn't want to drop these NaT rows as I wanted to subtract this date column from another date column . It does the job and gives NaT values in the resulting columns wherever there was a NaT in the original column .

Then , pass that into the ` groupby ` object with apply . #CODE

convert groupby to dataframe , join the groups again

you can iterrows() to iterate through rows , and use date_range() and concat method .

Find intersection between rows
If I want to find the difference between two consecutive rows in a pandas DataFrame , I can simply call the ` diff ` function .
I have rows that contain ` set ` s of characters . What I want to do now is compute the intersection of each set in rowise pairs . in other words , I'd like to use ` diff ` , but supply my own function instead . Is there a way to accomplish this in pandas ?
I've tried using ` shift ` , but it throws an error #CODE

If you are starting from excel and want to go to the next evolution then I would recommend MS access . It will be a lot easier then learning Panda for python . You should just replace the CountIf() with :
the final line uses the apply method , with the paramater key set to 1 , which applies the method -first parameter - row wise along the DataFrame and Returns a Series which is appended to the DataFrame .

How can I accomplish this concisely and efficiently ? I've tried using combinations of ` groupby ` and ` apply ` , but I'm new to PANDAS and keep throwing Exceptions .

The result of your ` .... groupby ( ' id ') .date .transform ( lambda ii : ii == ii.max() )` is a Series , but the index of that series is range ( len ( df )) . When you try to do ` newdf [ ' new_col '] = new series ` , it tries to match the indices . Casting to list makes pandas ignore the indices , or reseting the index first puts the indices on the same level .

Drop entire row subject to value in column
Second step ( drop row ):
Please post the entire stack trace ** in your question** . Please also format your code as code .
Thank you for you quick response ! Unfortunately I have to delete the rows from the original DataFrame . Using your suggestion perfectly works for returning True / False . But as soon as I try using the if-command I get the Value-Error mentioned above . Do you have an idea how to solve this problem and how to further drop the respective row ?
Maybe the mentioned approach is too complicated for solving my problem . I have a DataFrame and would like to only keep a specific percentile ( e.g. 25-percentile ) and drop all other rows . The percentile refers to values in one column . Do you have anonter idea how to approch this problem ?
Thanks , I found a solution using the quantile-command . I created an additional column with Nan if value in column1 is greater than the quantile and one if otherwise . I then drop the NaN rows and only the first quantile is left . This is probably not a very efficient method , nevertheless it is working :) .
Why do you need to drop the rows ? If you're only looking for the values of the first quartile , the quantile method gives that to you already .

I'm trying to write a function that takes a continuous time series and returns a data structure which describes any missing gaps in the data ( e.g. a DF with columns ' start ' and ' end ') . It seems like a fairly common issue for time series , but despite messing around with groupby , diff , and the like -- and exploring SO -- I haven't been able to come up with much better than the below .

You can do this with merge and groupby . See below , I modified some of your data to get matching values in the columns , to show the results better . #CODE
And if you want to keep rows from sample that have zero for a total around you could do the merge like this : #CODE

If distance between compared rows is constant ( for example 15 rows ) then you could create new " shifted " column using ` shift ( -15 )` and then you have both values in one row .
Thank you furas . The distance is not constant . At 930 , i want shift ( -1 ) , 931 : shift ( -2 ) etc . so that i'm always comparing with the row at 1600 on the previous day .
I have a hack for this purpose . I resample the array to daily data and join it to my original frame . Then I fill the closing price forward .
This is pretty much what I did . I was hoping there would be an easier way to do this . If not , I will accept this answer . Also , after you join , to divide the first half columns with the second half of columns , you need to break the dataframe into half and rename the columns again . All of these seem a little unwieldy .

use for loop to concat dataframe to a larger dataframe
my question is for every step of for loop , a new dataframe will be generated . I want to concat the data frames together to have a larger one but somehow my function will only return the last step of the result rather than the merged result #CODE
but you didn't concat rate with anything else .
oh , you need to do rate = pd.DataFrame ( { " classfier " : classifier_i , " model " : index_i , " recall " : recall_rate , " precision " :p recision_rate , " accuracy " : accuracy_rate } ) to make it a DataFrame before you concat it .

next you just apply a lambda function that finds the union between columns . Had trouble finding a quick method for the union but this works #CODE

Both indices having timezone , one with freq and one without : #CODE
this is a bug . need some code to figure out whether to recreate the tz ( e.g. it has to be on all pieces and they have to be the same ) .
I think this works as long as both are > 2 in length ( it is because an index of len ( 2 ) cannot have an inferred_freq , only a set freq ) .
Have my own version of concat in my code , where I'm exactly doing that . Though could be useful to show the bug , or find the reason ... Thanks for quick reply !

Pandas apply and lambda function efficiency

where ` i.index ` could be different from ` range ( len ( i ))` if you wanted .

I am doing some data preparation with Python using Pandas and I am working with a dataset that has about 80 variables with missing values and I want to capture any patterns of missingness to cut down on the amount of missing value indicators I have but I am having trouble finding any good strategies for doing this . here is an example of what I am getting at : #CODE

why i failed to join two dataframes when using python pandas ?
what i need is to join d1 and d2 using the first colum , the result should be , which kind of DataFrame operation is needed ? #CODE
That won't work it looks what you want to do is to add just the last column which you could achieve using ` concat ` : #CODE
or you should ` merge ` : #CODE
If we analyse what went wrong with the join , you have a clash of columns which it cannot resolve unless you specify a suffix : #CODE
We can now drop the superfluous column ` 0_y ` and rename the added column : #CODE

You can use ` join ` operation on the ` frequency ` object you created , or do it in one stage : #CODE

why the dataframe generated using merge operation is not of 3x3 dimension rather than 3x5 ?
i follow the instruction #URL , but is confused when the merge colums is not of the same index . for example , column 1 in d3 corresponding to column 1 in d4 . #CODE
why the following merge could create an exactly 3x3 DataFrame , while the formmer can create a 3x5 DataFrame ? #CODE
I don't see your problem here : Your expected result is just a subset of the columns panda's ` merge ` provides . Merge syntax is ` x.merge ( y )` .
` 0 ` is the join column and is same as you would expect . Your ` 1 ` column is the ` 1 ` column of the ` d3 (= x )` data frame , so ` 1_x ` , Your ` 2 ` column is the ` 0 ` column of the ` d4 (= y )` data frame , so ` 0_y ` . Pandas can't know that you would like to call it ` 2 ` , but it sure knows where this column comes from and names the column accordingly .
Good question . From the merge command in your first example we know that 0 , 0_x and 1_y will have the same values . From your second example we know that 0 , 0_x and 0_y will have the same values . But this time pandas seems to notice that and does not report 0_x and 0_y . Let's see if someone knows a good explanation for that .
In your first merge you are merging lhs on column ' 0 ' and rhs on column ' 1 ' but you have no identical values so it has to create two columns with suffixes . The remaining columns have no matches either so you create additional columns .
In the second example you merge on column ' 0 ' , whereby you do have identical values so it doesn't need to create an additional column , however you still have a clash of column names for ' 1 ' and values so it has to create the additional columns with suffixes .
I think your confusion stems from the expectation that because you've specified the columns to merge on that it will then use these columns like an index and match the other columns against these rows , it will not . It will only do this if you set these columns as the index : #CODE

I have a large dataframe with 11 columns where I want to replace NaN values with zeros , if every value in another group of columns is NaN , and otherwise to convert the number that is not null to an integer . I am doing this in the following way , but with only 8000 observations it is taking a long long time to complete ( although it does so correctly ) . I reckon this took nearly 20 minutes : #CODE
( Obviously you can drop the copy if you're okay with an in-place change . ) #CODE

i don't think a dict of dataframes is a good idea . Try structure all dataframes in one -- stack vertically and use key as index a level of multiindex .

The ` apply ` operation of pandas is quite expressive , I could first ` group ` , and then do the Cartesian product on each group using ` apply ` , and then aggregate the result using ` sum ` . The problem with this approach , however , is that ` apply ` is not lazy , it will compute all the intermediate results before the aggregation , and the intermediate results ( Cartesian production on each group ) is very large .
Intermediate result after the ` apply ` ( can be large ) #CODE
Could you give a small self-contained example with fake data to show what you want ? ( E.g. I don't see why you can't move a summation into the apply to avoid expanding more than one group at a time , but maybe that doesn't work in a real case for some reason . )

The desired output will be another column of just the STATE abbreviation , so I understand that I am trying to find an equivalent of a ` df.split ( ' , ')` and to append the second [ 1 ] index from that split for a Series or dataframe . ( If I am mistaken , please correct me ) .

IIUC , you could use ` merge ` : #CODE

is there a way to insert ` s ` into ` df ` without creating a reindexed copy of ` df ` first ? Currently i am using #CODE
Use ` concat ` : #CODE

How to drop the index column of DataFrame ?

Hey Paul , I think you are correct ; however , attempting to output a row of data directly to CSV results in each output overwriting the last . the row " b.ix [ j ]" can be written to CSV with ` b.ix [ j ] .to_csv ( path )` , but I do not know how to append data to a CSV file
to append the row to the existing file .

Why are vectorized operations like apply so much quicker ? I imagine there must be some row by row iteration going on there too .
`` apply `` is NOT vectorized . `` iterrows `` is even worse as it boxes everything ( that ' the perf diff with `` apply ``) . You should only use `` iterrows `` in very very few situations . IMHO never . Show what you are actually doing with `` iterrows `` .
3 ) Apply involves can usually be done by an iterator in cython space ( this is done internally in pandas ) ( this is a ) case .
This is dependent on what is going on inside the apply expression . e.g. ` df.apply ( lambda x : np.sum ( x ))` will be executed pretty swiftly ( of course ` df.sum ( 1 )` is even better ) . However something like : ` df.apply ( lambda x : x [ ' b '] + 1 )` will be executed in python space , and consequently is slower .
6 ) updating an empty frame a-single-row-at-a-time . I have seen this method used WAY too much . It is by far the slowest . It is probably common place ( and reasonably fast for some python structures ) , but a DataFrame does a fair number of checks on indexing , so this will always be very slow to update a row at a time . Much better to create new structures and ` concat ` .

The examples I've seen using ' map ' or ' apply ' generally show one datatable which seems intuitive enough . However , I am working across two tables and they are large ( T1 is 2.5million rows , T2 is 96000 rows ) .
I think you misunderstand merging -- it takes the intersection of the column you merge on . Merging your example dataframes gives a dataframe with four rows -- not 8 .
Unfortunatley not . I get this message : " ValueError : array is too big . " I'm pretty sure that I will get 5 billion rows having looked into the data ( I agree it is not creating a cartesian product ) . I plan on trying itertools with the groupby feature . I might make two grouped objects , one for each table , to start . Then iterate to find the " matching " groups . I will then merge and apply on each as you have done , aggregating to a new table . If you know how to do that I'd be grateful to see it on this ( tiny ) example . If I succeed , I'll post it myself :)
To me , looks like the easiest thing is to merge on ` letter ` and then ` groupby ` . #CODE

Resample in a rolling window using pandas
I cannot apply a rolling window because this would first be daily and secondly I need to specify the number of values ( a rolling window does not aggregate by time frame , some posts addressed this issue but they are not relevant to my problem as the rolling would still be for each new day ) .
I cannot apply resampling , because then the sample would be every 5 months , e .. g I would only have values for May 2012 , Oct 2012 , March 2013 ... Finally , as the function is not linear I cannot reconstruct it by first doing a monthly sample and then applying a 5 period rolling window on it .
How can I do this in pandas ? One approach could be to combine several ( 5 in this example ) resampled ( 5 months ) time series , each with one month of offset and then align all these series into one ... but I do not know how to implement this .
To clarify : I am looking for 5 calendar months ( data is not necessarily evenly spaced ) , including the current month , so for May 2012 I go from Jan 2012 to May 2012 ( the length of the windows is 5 months , regardless if I have only one day per month or 20 ) . User @USER is correct , in addition I only care of a monthly result , so I need to apply the same for June 2012 , July 2012 , etc .
If pandas has imported you date and time data , you should be able to get select data from given months using the syntax ` dft [ datetime ( 2013 , 1 , 1 ): datetime ( 2013 , 6 )]` . Just program a loop or equivalent to cycle the start and end month values and apply your function to the values in the resulting dataframes . ( Sorry , I don't have a date stamped data set handy to test this myself right now )
Then - use the ` melt ` function to convert the data from wide to long , so each rolling period will have one entry . #CODE

Say I have a Series of data based on a symmetric ( distance ) matrix , what is the most efficient way to drop duplicates from the following series ? #CODE

I would consider yours a dupe of [ this ] ( #URL ) question , but the accepted answer there is not what I would use . Still , the [ higher voted answers ] ( #URL ) there apply to your situation .

` df [ ' E '] = abs ( df [ ' D '] * 0.5 )` , in the first line of the 4th code block . I think the problem is that the map function from seaborn is passing the whole ` df [ ' E ']` list to matplotlib's errorbar function , not just the part that applies to that subplot .

I am using PANDAS with a SQLAlchemy to write to MYSQL DB using ` DataFrame.to_sql ` . I like to turn on the flag for `' append ' -- df.to_sql ( con=con , name= ' tablename ' , if_exists= ' append ')` Since the program does several small writes to the tables during the day , I don't want the entire table overwritten with replace . Periodically , I get the duplicate entry error : #CODE
call the ` .to_sql() ` on a temporary table then use a query to update the main table with the temp table . Then you can drop the temp table . So for example : #CODE

Call ` apply ` on the dataframe ( note the double square brackets ` df [[ ' A ']]` rather than ` df [ ' A ']`) and call the string method ` isdigit() ` , we then set param ` axis=1 ` to apply the lambda function row-wise . What happens here is that the index is used to create a boolean mask . #CODE
This will be considerably faster that using ` apply ` on a larger frame as this is all implemented in cython . #CODE

Translate your signal into a " quantity held at ` t `"

Just use ` diff ` e.g. ` df [ ' timestamp '] .diff ( 5 ) > 123ms ` or similar

Just make a list of the columns from the dataframes you want and use concat : ` df_list = [ test_1 [ ' Position '] , test_1 [ ' Force '] , test_2 [ ' Position '] , test_2 [ ' Position ']] pd.concat ( df_list , axis=1 , ignore_index=True ) .to_csv ( " filepath ")` if the order is as regular as you say you could easily write a simple for loop to build this list

Then i tried to append the DataFrame like this : #CODE

My final goal is to find percentiles of this subset , however np.percentile cannot handle NaNs . I have tried using the dataframe quantile method but there are a couple of missing data points which cause it to drop the whole column . It seems like it would be simple to use a conditional statement to select values without returning NaNs , but I can't seem to find anything that will return a smaller subset without the NaNs . Any help or suggestions would be much appreciated . Thanks !

I have a Dataset with true and false values for as classifiers . I have a DataFrame representation of this Dataset . However this dataset has about a ratio of 30 : 70 of true and false values for the classifier . I would ideally want 50 : 50 for both classifiers . What is the best way to show how many True and False Values I have of both classifications and then drop some rows of data from the classification with the highest number ?
I would ideally want to drop 4 of the above dataset which has trustworthy value of 0 . This is just a very small subset of my Dataset .
To drop last 4 rows with 0 #CODE
Thank You . The grouping worked however when I drop data from tail I don't have 4 data points dropped . For instance in a sample dataset Ive got 93 false and 323 true . When i drop 1 , 2 , 3 or any number from tail i end up with 29 false and 187 true . I am not sure whats going on here ?

Okay , so if you're splitting off B yourself , why not just add an ` id ` column first ? something like ` A [ ' id '] = range ( len ( A ))` .
Do a left merge of A and B , which by default merges on common columns #CODE
Filter C to just those observations not found in B and drop the marker . #CODE

I'm using df.columns.values to make a list of column names which I then iterate over and make charts , etc ... but when I set this up I overlooked the non-numeric columns in the df . Now , I'd much rather not simply drop those columns from the df ( or a copy of it ) . Instead , I would like to find a slick way to eliminate them from the list of column names .
Is there any slick way to do this ? I suppose I could make a copy of the df , and drop those non-numeric columns before doing columns.values , but that strikes me as clunky .
and if they conform to the standard whereby the non-numeric columns of data are all ` object ` types ( as they are in my dataframes ) , then you can do the following to get a list of the numeric columns : #CODE

Pandas Series concat raises error
Looks like the version of the pandas is indeed the culprit . When I ran it on version ' 0.12.0 ' , it actually did the join correctly .

Insert pandas chart into an Excel file using XlsxWriter
Now , how do I insert that graph into an Excel file using XlsxWriter ?
If on the other hand you want the matplotlib style charts generated by Pandas then export them as images and insert them into a worksheet using the XlsxWriter ` insert_image() ` method .

So I am using pandas to create a dataframe from a CSV file and I have a column which is of dtype datetime . This works as expected with smaller datasets . If the dataset is large the operations i perform on this column change it to an object instead of datatime . Is there any way to preserve the dtypes ? I tried using iloc or ix with the dataframe but that didnt work . Below is some of my code and where the problem lies . #CODE

This returns a series , with true / false values for each stock , but I can't figure out anyway to usefully merge this back with the original dataframe and count the number of Trues for each date . Do you guys have any suggestions ? Thanks so much
Do you want ` transform ` so that it returns an object with its index aligned to the original dataframe ? like ` b = stock_data_df.groupby ( ' stock id ') .transform ( apply ( lambda x : x [ ' price '] > = pd.rolling_max ( x [ ' price '] , 20 )))`
Not sure if you so the edit , but I tried ` b = stock_data_df.groupby ( ' stock id ') .transform ( apply ( lambda x : x [ ' price '] > = pd.rolling_max ( x [ ' price '] , 20 )))` and then I get ` TypeError : ( ) takes exactly 1 argument ( 0 given )`

So I have am using Pandas to create a data frame with some columns which have types bool , int64 and date time . For smaller datasets the dtypes remain but for larger datasets pandas converts all of these to objects . Would anyone know why its doing this and how i can explicitly set type if then ?

In a pandas hist ( ) plot with sub-histograms , how to insert titles for the x and y axes , and overall title ?
How do I add titles for respectively the x and y axes on each of the sub-histograms , or alternatively , overall ? Also , how to insert an overall title for the plot ?
` hist ` should return ` axes ` objects which can be used to set things like titles and labels . What version of pandas are you using ?

edit - you can also use stack for same result
The following is basically what the pivot method does in case someone needs to do something similiar but more complicated like even more hierarchical columns . #CODE
no Problem only reason there are orange and oranges is because they are different in his example . Always can do some data cleaning with df.replace I guess . You could probably use unstack to solve this as well

I'm building an application that passes data from a CSV to a MS SQL database . This database is being used as a repository for all my enterprise's records of this type ( phone calls ) . When I run the application , it reads the CSV and converts it to a Pandas dataframe , which I then use SQLAlchemy and pyodbc to append the records to my table in SQL .
df.to_sql ( ' Calls ' , engine , if_exists= ' append ')`

edit : I shoudl add that I already know what the 95%CI values are . This is just a plotting question ( how to apply the axvline to each of these subplots ) . Thx .

I used unstack , which is essentially pivot ... #CODE

Easier to reindex and / or concat / append a new dataframe though . Generally setting ( with this kind of enlargement ) , only makes sense if you are doing it with a small number of values . As this makes a copy when you do this .

Ok , so why is the 4th method the fastest . It constructs a boolean indexing array then uses nonzero , so pretty fast . The first three methods use searchsorted ( twice ) after already determining that the index is unique and monotonic , to figure out the endpoints , so you have multiple things going on .

Note that when setting properties of an axis using its methods , most of the ` plt ` attributes become ` set_X ` . For example , instead of ` plt.ylabel ( ' my_y ')` you do ` ax1.set_ylabel ( ' my_y ')` . You can still use the ` plt ` methods , but they will apply to whatever the current plot is . The variables ` ax1 ` and ` ax2 ` give you a little more freedom about when you do things .
can you verify that an array you create on the fly works with ` hist ` ? Instead of ` df2 [ ' C ']` try using ` np.arange ( 50 )` then you can narrow down where the problem is .

How to Join two separate text data files and align data over varying time intervals and avg
I have two text data files from two different instruments . One is an elevator that raises and lowers at 7 and 8 minute intervals . I need to match up ( align ) the data from one instrument to the data at the upper and lower position times ( duration either 7 or 8 minutes ) . Below is data from instrument ( Picarro ) and elevator ( AEM ):
I want to be able to merge these two separate files and output into a new list . From this new list , then i want to perform statistical analysis , mean , std dev , etc on the data . But first I must align the data among those time frames . The interval pattern for the AEM appears to be 7 , 8 , 8 , 7 minutes , and then repeats so need to create some loop i assume for this but far beyond my Python skills . I would like to create intervals along this pattern to corroborate the data .
How can you align dates with 6 days between them ? Do you want to sort the values based on the dates ?

Downsize series containing boolean data with resample
I have a large time series dataframe containing numerical and boolean data in separate columns . I am trying to downsample the data from 1-min interval to 15-min interval . The boolean columns are system statuses and I am struggling with how to downsample them and still retain any faults . Currently , my resample uses ` last ` thus would overlook any system faults that occur on any row but the last .
What I would like it to do : if ' fault ' occurs on any row during the 15-min series , then the resulting timestamp after the resample would read ' fault ' else would read ' ok ' .
I know the solution reside in ` how= ''` of resample , but because I am new to numpy and pandas I can't figure out what to use .
Use ` resample ` to convert to 5 minute frequency . Use ` how= np.any ` to logically ` or ` the values in the time bin . #CODE
and give it as the ` how ` argument of ` resample ` . Here's an example .
Use the function to resample ` ts ` . #CODE

What is the best way to merge these two dataframe such that it appends additional columns to position with the region it falls in if it falls in any region . Giving in this case roughly the following output : #CODE
Unfortunately , my data files are too large to hold df in memory . I'm trying to find an alternative solution where I load position and region into dataframes with a multi-index with chromosome as the outer index and then do the merge independently for each chromosome . I'm working out this code now but if anyone sees a better way of doing this , please let me know . Thanks .

Why , in the .replace() example above , is the ' ba ' and ' BA ' not selected by the regular expression fed as the first argument in the replace() method and replaced by ' XX-XX ' ? It seems to me to be saying ^ any character followed by . an a , or dog , replace , starting with that any character , with ' XX-XX ' , regardless of case .
So how would I find ALL instances of that condition and replace ALL of them with XX-XX ? What changes in the regex ? @USER

I'm assuming is what you're actually asking for is how you can override the format for all integers : replace ( " monkey patch ") the ` IntArrayFormatter ` to print integer values with thousands separated by comma as follows : #CODE

I have a set of tab delimited files that I have to go through read them , use them as pandas dataframe , do a whole bunch of operations on them and then merge them back to one excel file , the code is too long so I am going to go through the problematic part of it

If you've saved the JSONP response in the file ` jsonv3.json ` , you could strip off the function call wrapper and process the content with something like this : #CODE

I have a DataFrame that is resampled into a smaller DataFrame , which retained a datetimeindex . I transposed the dataframe and now wish to remove the dateindex and replace it with strings ( labels ) , then export it to .csv to be used in a format that can be read by javascript ( doing all data manipulation in python ) .
I tried to follow this ( Insert a row to pandas dataframe ) workaround , but couldn't make it work .
hmm , still no success . I was able to change the index to ' label ' then transposed , but then have the format in your answer , except that ' value ' is a date ; ' 2000-12-32 ' , which I can't seem to change . If I use df_year.columns = [ ' value '] after transpose , it throws away the ' label ' and puts ' value ' on the second column ..
Again , you need to do these steps after you transpose the data - in your edited example it's before . You also have a typo , ` .column ` instead of ` .columns `

You have the right idea with groupby . It has the ability to split up your data by the day then give you access to those groups . The trick here is using the apply method on the Series df [ ' date_time '] . Apply on a series applies the input method element wise and returns a new Series . You can use this to split up by days and then again to split up by hours .

The output of a MERGE operation on two pandas data frames does not yield the expected result : #CODE
Update : this post suggests that MERGE is the way to go for the purposes of joining two data frames
The output that you posted is definitely not correct , you should have 6 rows . Reason it " isn't working " is cv005_29357 ! = cv006_29357 and cv006_17022 ! = cv009_17022 . An outer merge should recognize these as unequal and not join these rows together
the problem was at the file level : the entries in the ` filename ` column of the ` dfscores ` file being read had a ` trailing whitespace ` which caused the JOIN to fail . Admitted , this is not a glorious moment for me but neverthelesss these things happen and i think it is worth posting the answer as it may happen to other less experienced coders .

FYI , unless you are doing this with a very small number times this is not very efficient AT ALL . It copies the frame each time . MUCH better to concat frames .

Sorry you want to replace negative values with ` nan ` ?

There is some munging to do here as in some cases I have to copy the columns over as object type . Wierd . The other option is to just drop the columns that are the problem .

@USER I would rather do ` df = df [ df [ ' A '] ! = df [ ' A '] .shift ( -1 )]` - first ` [ ' A ']` then ` shift ( -1 )` to shift only one column not all ` df ` .

I'm trying to replace the ` NaN ` values on column ` Age ` for people that were alone with the mean age of those who were alone , and the same way with those who weren't alone . The purpose is just exercise pandas DataFrame , replacing ` NaN ` values based on a rule .

But in the code below , it seems there are just too many lines . That is , the rank() function in pandas is super convenient . Seems to me there should be some parameter somewhere that says to the data frame , " Hey , apply this function you already know about , but instead of doing to the original column itself , as you do it , make it a new column at the end of the data frame "
Please explain exactly what you want to achieve , in general you want to avoid any form of iteration and using apply if the calculation can be vectorised . It looks like you are just adding a new column , in which case just do df [ ' new_col '] = some_calc_on_df . If you want to append another dataframe which has the same index then use append or concat , no need to merge unless the order is different and you want to join on some id column

You can just use concat and set the axis to 1 ... #CODE
it automatically gives NaN padding ( outer join ) if you set parameter join to inner , it would give no padding .

either the pivot is a multilevel dataframe of the are a single NaN in that answer . check your intermediate value of pivoted . also suspect that the " pivoted.to_sparse() " returns a copy . try : " pivoted =p ivoted.to_sparse() " so capture the to sparse in the original variable

I would like to replace df1 score values with df2 scores whenever both id and event match in the two dataframes . Both df1 and df2 are of unequal sizes and unsorted . I need to store the new scores in df1 based on df2 . The order of df1 is important to me but df2 can be sorted . Please suggest .

That is a pivot operation , you can either use ` .pivot_table ` : #CODE
Wow ! thanks a lot ! what is the difference between ` pivot ` and ` pivot_table ` ?
ok I will see API links . Even if it's longer I like your groupby solution because it helped me to understand groupby , aggregate and unstack methods .

You are getting this error because the function you are passing to apply doesn't return anything . If all you care about is the printed output , you could just return the df back , like this . #CODE
Then the apply will run through without error . #CODE

` unstack ` can be used to pivot a level of a MultiIndex into columns . The ` fillna ` replaces missing values with 0 , as shown in your desired output . #CODE

A combined ( joined by date ) view of the data can be achieved using ` merge ` which can join dataframes on indicies and / or columns . Because date is an index in both frames , specify ` left_index ` and ` right_index ` #CODE
THANKS . I had just figured out the merge ( from your earlier suggestion ) and was then going to try and figure out what to do next . This will save me hours of frustration .

My problem is with the DATE and EPOCH columns . I want to merge them into a single datetime column .
Then there are ` pandas.to_datetime ` and ` pandas.to_timedelta ` , which run much faster than the methods described above . But I cannot merge the results together without resorting to string functions , which are again mainly row-by-row .

Pandas - GroupBy and then Merge on original table
I'm trying to write a function to aggregate and perform various stats calcuations on a dataframe in Pandas and then merge it to the original dataframe however , I'm running to issues . This is code equivalent in SQL : #CODE
And then join on the original table : #CODE
This works fine until I want to join on the original table :
For some reason , the grouped dataframe can't join back to the original table . I've looked at ways of trying to convert the groupby columns to actual columns but that doesn't seem to work .
By default , ` groupby ` output has the grouping columns as indicies , not columns , which is why the merge is failing .
Then , your merge should work as expected . #CODE

How can we drop that speed by one or two orders of magnitude ?
This Dataframe has 200,000 rows and 11 columns , all strings of various lengths . Making the strings a fixed length barely budges the access speed . Making the values floats ( which isn't feasible for this use case ) would drop the speed by about half .
This is approximately the whole use case , using the suggestion of BrenBarn to use ` iloc ` instead of ` ix ` . Note that we access only two rows at a time . The large number of calculations comes from comparing every row to every other row ( 200,000 ^2 / 2 ) . #CODE

Everytime pd.isnull ( row [ ' Control ']) ( This should be the only if condition ) is true then merge this row with the previous row ( whose ' control ' value is not null ) .
For ' Action ' , simply merge them without any punctuations added ( e.g. FDA / OGROP / ORA / CE-FO / CHI-DO / CHI-CB / )
How come the drop() doesn't work ? I am trying drop the current row ( if its [ ' Control_# '] is null ) so the next row ( whose [ ' Control_# '] is null ) can be added to the previous row ( whose [ ' Control_# '] is NOT null ) iteratively ..
Have you looked at the shift method ? #URL
I think you need to group the rows together and then join up the column values . The tricky part is finding a way to group together the rows in the way you want . Here is my solution ...
This method is applied along the Control series to sort indexes into groups , which is then used to split up the dataframe to allow you to merge rows #CODE
That is really the only tricky part after that you can just merge the rows by applying a function to each group that gives you your desired output
edit above regex to clean join string joins #CODE
Thanks ! For the most part I understand but I don't really get the way you used ` joiner.join() ` . If each is notnull then each = each + joiner ( I only see you joined one each with a joiner ); but how did you manage to get ` If next_each is null : then returns each ( joiner not added ); else if next_each is not null : then returns each = each + joiner + next_each ` work ?? Thanks for your patience man , really appreciated ! And btw ` joiner.join() ` doesn't seem to work when type ( Recd_Date / Due_Date ) becomes pandas.tslib.Timestamp ( ' NaN's becomes ' NaT's ) , any idea why ? Thanks !
u'Subject ' : u'DLECTRONIC DATA DUMP - PROD RECALLS , ALERTS & ' } ,
I meant I don't quite understand ` s = joiner.join ([ str ( each ) for each in col if pd.notnull ( each )])` , as for what i understand you joined ' each ' with ' joiner ' ( e.g. ` M EGAN ` joined with ` , `) , but for joining ` M EGAN ` ` , ` with next_each aka ` BERNSTEIN LIEBHARD & ` , it's the ` pd.Series ([ joinFunc ( g , col ) for col in g.columns ] , index= g.columns )` that does the job right ? And how did you make sure if next_each is null ( e.g. there's nothing below ` M EGAN `) then ` M EGAN ` does not join by joiner ` , ` ? thanks !

For example , if the value in float_col is greater than 5 , I want to multiply the value in in_col ( in the same row ) by 2 . I'm guessing I'm supposed to use one of the ` map ` ` apply ` or ` applymap ` functions , but I'm not sure which , or how .
There might be more elegant ways to do this , but once you understand how to use things like ` loc ` to get at a particular subset of your dataset , you can do it like this : #CODE
Thank you ! I wasn't familiar with the loc function . I was sure there had to be an easy way to get just the subset I wanted , but didn't know how .

maybe show an example of `` resample ( ' 2W ' , how= ' mean ')`` which i think is relevant here

You can also use ` sets ` to check for ` subsets ` also and ` map ` the elements to ` ints ` : #CODE

We then use ` loc ` to perform label indexing to find where the column ' cat ' has the value that matches ' Home Applicances ' or just to do plain boolean indexing ( second example ) .

I wasn't clear enough , the whole file is dictionary of words , which words are keys and values are list of tuples ( Date and freq ) . actually I'm not sure what I want is a DataFrame with each word as raw and each tuple as columns . but the problem is that the number of tuples are not equal for all the words . so I'm trying to figure out which DataFrame is more appropriate .
` df.loc [ i ]` will append new rows . If you know the total number of entries from the start , you could change ` index=range ( 0 )` to the correct value . Next steps would probably be #CODE

Apply regex replace to python pandas data frame
For some reason , the function runs properly on strings , but when running it on the data frame with apply it returns empty strings , and not the first three octets .
You can use the ` replace ` method of the ` str ` attribute ( to access string manipulation functions , see docs ): #CODE

drop_duplicates drops keeps first row in frame unless take_last parameter is set to True . You can always sort your dataframe by a then drop duplicates , but might not be the fastest option
Turns out that the errors raised on version 0.12 * should * be raised on 0.14.1 . The bug here is that ` Grouby.filter ` should apply to the entire subframe , not rows within the subframe .

I'd groupby the ' bug_id ' column , then take the first entry and append this back to your dataframe : #CODE

does ` df.groupby ( cut ( df.a , bins ))` work ?
I am calculating a series by multiplying two columns of a dataframe . I apply groupby on that series . Get ` ValueError : Buffer has wrong number of dimensions ( expected 1 , got 2 )`

` df [ df.major == 0 ]` to get all the values where the major is zero . If it's a ( labeled ) index or a normal column does not matter . You can also stack these to do

python pandas apply function group by group
My question is : can I avoid to iterate group by group to apply my_stat_function , and is there exist something faster , maybe applying the function apply ? I would really like something more " pandas-ish ' and faster .
You can apply functions to groups : ` df.groupby ( ' user_id ') .apply ( my_stat_function )` or similar , have you tried this ?
Thank you for your help , my problem is that I don't know how to define my_stat_function in order to apply it like this , because I need the full data of each group , It is not a row by row execution . Do you see what I mean ?
You could groupby user and apply the function , you'd have to rewrite your function though

Drop using several MultiIndex levels
I can use ` drop ` to get rid of several rows using the first ` n ` MultiIndex levels like this : #CODE
I can also ` drop ` from a single level : #CODE
So how can I drop ` [(8 , 3 ) , ( 9 , 4 )]` ( i.e. the 3rd and 4th rows ) from my dataframe ?

generally speaking you don't want to iterate through a dataframe . look into the ` apply ` method : #URL Also , the point of stackoverflow is to be a resource for future readers who might have a similar question . There are currently over 7100 pandas questions . Your title , as it current reads , will not at all help future readers understand what the topic of this question is .
You can then apply some boolean logic to find the count which you're interested in : #CODE
Note , the ` ~ ` means NOT true . So , in this case , Is NOT isnull .

Do you want to do a shift : ` wealth = wealth.shift() * ( 1 + returns )` ? shift returns your df shifted by 1 so you should be able to do ` wealth = wealth.shift() * ( 1 + returns )`
The ` shift ` is necessary since you do not get the return on the first day .

Pandas join on 2 columns
I'm having some trouble getting these two dfs to join the way I would like . The first df has a hierarchical index that I created using ` df1 = df3.groupby ([ " STATE_PROV_CODE " , " COUNTY "]) .size() ` to get the count for each county . #CODE
I believe the way you have set up the groupby results and the second dataframe , this merge call will work : #CODE

Python : Pandas : Speeding up an Apply Function
I am trying to do a pandas apply function on a 33 MB dataframe ( in CSV form ) and it is going incredibly slow . And I am trying out figure out why . I was doing an apply on a much bigger dataframe ( 16 GB ) and it finished in about 6 hours . This function is operating on a much , much smaller dataframe and I let it run for 1.5 hours and still nothing .
PS- also if someone knows how to add a progress bar on an apply function that would be a great added bonus :) Thanks again ! #CODE
Then , rather than using apply , you could merge your existing data against the stock df , something like this : #CODE
If you want to fill missing values , rather than having custom logic in apply , it's much faster use ` fillna ` #CODE

Will ` concat ` with ` inner ` option handle the case for you , if your dataframes are already matched ? #CODE

You need to combine the functions that apply to the same column , like this : #CODE

? I want to create a new bool column " med " that has True / False based on ? 60735 in the columns MED1 ... MED5

But since we have lots of rows and relatively few columns , we could loop over the columns , and in the typical case probably cut down substantially on the number of rows to be looked at . For example , something like #CODE

how to resample state change data for line chart ?

Pandas DataFrame Replace every value by 1 except 0
I need to replace every other value to ' 1 ' except ' 0 ' . So my expected output . #CODE
Just use something like ` df [ df ! = 0 ]` to get at the nonzero parts of your dataframe : #CODE

Multiindex in pandas pivot table
I am working on a pivot table that looks like this : #CODE
Can you add some code to reproduce this dataframe ? ( eg a sample of the original frame and the pivot call )
There might be something more clever built in but one way is to work with MultiIndex as a list of tuples , and map out the new column names as you described . #CODE
Then apply to the columns and assign : #CODE
`' _ '` above was used a marker for the columns no longer desired , so the last step would be to drop those . #CODE

Classic case for ` izip_longest ` , operating on column 9 as Series . You just need another transpose in the end , since you want to put them on rows . #CODE
They both transpose lists , so you get a list with the first value of each list , then a list with the second value of each list , and so on . ` izip ` would truncate all lists to the length of the shortest , izip_longest adds ` fillvalue ` s to match the length of the longest .

If names aren't important , you could do this , which will reset the row index , and replace the MultiIndex columns with a tuple with each level . #CODE

I would just map each category to a y-value using a dictionary . #CODE

I get a strange error on the second line : ` ValueError : cannot insert title , already exists `
performance ( standard optimized operations compared to enlarging a dataframe iteratively with ` concat ` , wasting memory

But read the whole page instead ; the Pandas ` groupby ` feature is more flexible than R ` dcast ` ( it's designed to also accomplish everything SQL aggregation , Excel pivots , etc . can do ) , but that means your ideas may not always map one-to-one between the two .

Why ? I thought I could pass any function to ` apply ` on a group .
@USER I agree about ` transform ` , although I thought you can mimic the behavior of ` transform ` with apply ( which is more generic )

Pandas : Merge array is too big , large , how to merge in parts ?
When trying to merge two dataframes using pandas I receive this message : " ValueError : array is too big . " I estimate the merged table will have about 5 billion rows , which is probably too much for my computer with 8GB of RAM ( is this limited just by my RAM or is it built into the pandas system ? ) .
How can I break this problem up so that I can execute this merge method on smaller parts and build up the output table , without hitting my RAM limitations ?
You can break up the first table using groupby ( for instance , on ' scenario ') . It could make sense to first make a new variable which gives you groups of exactly the size you want . Then iterate through these groups doing the following on each : execute a new merge , filter and then append the smaller data into your final output table .
I'm attempting to be memory efficient by deleting temp each iteration . Is this value adding or not ? I want to avoid creating objects that remain in memory . Is append() the right choice ? I'd love to hear any other solutions to this ' too big to merge ' problem , thanks .

Unfortunately , if I try to resample , I get an error #CODE
What exactly is the question ? Are you ask for a process to interpolate , or a process to aggregate , or both ? Also , can you provide a snippet of what you have tried ?
Firstly , prepare a function to map the day to week #CODE
Thirdly , initialize your second dataframe with only columns of ' Group ' and ' Week ' , leaving the ' value ' out . Assume now your initialized new dataframe is ` result ` , you can now do a join #CODE
Last , write a function to fill the ` Nan ` up in the ' value ' column with the nearby element . The ` Nan ` is what you need to interpolate . Since I am not sure how you want the interpolation to work , I will leave it to you .
Turns out the key is to resample a groupby object like so : #CODE

I have a pandas DataFrame with a 2-level MultiIndex . Both levels of the MultiIndex are identical date ranges , spaced daily . I want to resample the DataFrame on a weekly basis , for both levels of the MultiIndex , but I'm having trouble . Please see below .
If I resample df directly , I get the following TypeError : #CODE
Fair enough , I unstack and resample on the first level , which gives half my answer : #CODE
Now , if I attempt to resample the second axis ( also index by date , in theory ) , I get the same error : #CODE
Thanks Jeff , that's exactly what I was looking for ! About your " should work " comment , are you referring to my first or second resample attempt ?
Oh ok , thanks ! I'm wondering why the reset_index() is required ? If I drop that , I get " The grouper name cohort_date is not found " .

I'll reframe my question . I need to keep all the Nan values that are not proceeded by non-Nan values . And replace the rest of the Nan by 0 . I want to do this on per name basis .
traverse untill a non-Nan value is obtained . Replace all Nan after this non-nan
else replace all Nan by 0 for this column .
trying to think ... if you can story the 1st Nan , can replace them after doing df.fillna ( value=0 , axis=1 ) . trying to construct algorith to store and replace the 1st Na in each column should not be difficult and avoids expensive looping

And you're getting this error because I force ` lxml ` to be strict . In the past ` lxml ` has dropped data on certain pieces of malformed HTML , which IMHO is just not cool . The other libs , OTOH do not do this and consequently do not drop data .

I'm generating a number of dataframes with the same shape , and I want to compare them to oneanother . I want to be able to get the mean and median accross the dataframes . #CODE
If you want to compute stats across each set of rows with the same index in the two datasets , you can use ` .groupby() ` to group the data by row index , then apply the mean , median etc . : #CODE
This method will work even when your dataframes have unequal numbers of rows - if a particular row index is missing in one of the two dataframes , the mean / median will be computed on the single existing row .
You can simply assign a label to each frame , call it ` group ` and then ` concat ` and ` groupby ` to do what you want : #CODE
much faster than my general function , you can also use apply on this for more general functions
Here is a solution first unstack both dataframes so they are series with multiindexes ( cluster , colnames ) ... then you can use Series addition and division , which automattically do the operation on the indexes , finally unstack them ... here it is in code ... #CODE
That'll work fine for the mean , but it's harder to generalize to other summary stats such as the median , which the OP was also asking for .
Didn't keep that in mind when I made the answer but I do think that this solution would absolutely be able to do that you can combine each of the series I made into a dataframe and run the median solution on the frame rowwise yeilding you the seiries which you can unstack

I'd like to point out that ` cut ` is an ordered factor in the ` diamonds ` dataset as is appropriate . If you didn't make it unordered , ` glm ` would use polynomial contrasts . You should reconsider what you are doing .

When I try to store this dataframe to hdfstore , I get an error saying " can't set attribute ' freq ' in node " , #CODE
Jeff's answer is correct . I was able to use format= ' table ' and not get the error " can't set attribute ' freq ' in node "

Is this something I would use ` groupby ` for and then apply a function to it ? I tried doing a ` groupby ` for the cust_id and date columns , but I was given an object so I'm not sure if it is formatted properly .

Pandas dataframe merge and element-wide multiplication
You can just get a list of the columns you want to multiply the scores by and then an apply function ... #CODE

Both of you have the desired answer . I really appreciated it . In fact I have a more generalised question . I have a function which takes a couple of column values as input and outputs an value and I wanna apply that function to each row in a dataframe . Is that possible not to use for loop to achieve that ? thanks in advance .

I suspect it has something to do with the eval / numexpr method of querying in the HDF and Query methods .

Resolve Pandas data frame merge conflicts with a function ?
Let's say I have two dataframes , which I would like to merge , but there is a conflict because rows and columns overlap . Instead of duplicating the rows , I would like to pass a function to resolve the conflict . Can this be done ? #CODE
So as you can see , the two data frames overlap in columns C and D , and in rows 2014-01-03 and 2014-01-04 . So now when I merge them I get repeated rows because of this conflict : #CODE
Instead of using concat use merge : #CODE
The ` how= ' outer '` does a union of keys from both frames ( SQL : full outer join )

For example , replace each link with a key , storing the keys in a dict : #CODE

This roundabout method of converting my ` array ` to a nested ` list ` and then converting it back to an array via ` apply ` is bothersome . Is there a more straightforward way that I'm just not aware of ?
I don't think that's a supported use case for DataFrames ; while you can cram nonscalar data into a cell , there's not much you can do with it after that . You'll have a column dtype of object , which is slow to begin with , and you can't really do any fast aggregation ops , so you'll have to fall back to relatively slow apply ops . Depending on preference you might be more interested in using a MultiIndex or a Panel instead of this approach .

You can perform a ` groupby ` on ' Product ID ' , then apply ` idxmax ` on ' Sales ' column .
@USER when I was using ` transform ` it was trying to align the results to the original df and so repeated the index values as we had multiple ' Store ' rows for each ' Product ID ' this was why I had ` drop_duplicates ` but I realised this was unnecessary as an aggregate function with ` idxmax ` would work fine

So , I thought I would take the test B scores for the score_range , create a new dataframe , insert the data and plot the histograms of the columns with the following : #CODE

Is there a multiple column map function for dataframes ?
And a function , which I want to apply to each row , storing the result into a new column . #CODE
You can use ` apply ` with the ` axis=1 ` argument to apply by row .
The more general apply will be slower . It's better to find a way to vectorize the operations . When you have more data a general apply will not scale very well especially the row by row version since each row is converted to a series of uniform type which if you have mixed types will be very annoying to use and inefficient .
Actually most of those are implemented in Cython which speeds up loops considerably . By vectorization I simply meant applying operations on whole sequences rather than single elements at a time , which is unrelated to the use of BLAS . What I'm saying is that spending a bit of time trying to avoid apply will probably yield reusable and more performant code .
Try the simplest of operations : string concatenation , with two Series of length 1,000,000 . Do this by adding them together directly ` a + b ` then try putting them in a ` DataFrame ` and calling ` df.apply ( lambda x : x.a + x.b , axis=1 )` . The latter takes an unbearably long time ( about 22 seconds ) where as the former takes about 60 milliseconds on my machine . Trust me , the string methods in pandas are orders of magnitude faster than calling ` apply ` .
What you are looking for is ` apply ( func , axis=1 )` This will apply a function row wise through your dataframe .

I've tried various combinations of pivot , un / stack and set_index to achive the target structure but failed . The closest i get to is by using : #CODE

Define two columns with one map in Pandas DataFrame
I have a function which returns a list of length 2 . I would like to apply this function to one column in my dataframe and assign the result to two columns .
That's too bad , since performance is much better when you can apply functions to whole Series rather than to individual values one-at-a-time .

2 ) group it up and apply a function to index your values . #CODE
3 ) reset indexes : use unstack to pivot your value indexes into columns and reset index to reset your date as a column ... #CODE

How to apply a function ( numpy ufunc ) with two array arguments to one pandas Series ?
As for ` apply ` , you're looking at the wrong documentation . You're looking at ` Dataframe.apply ` , but you have a series , so you should be looking at ` Series.apply ` . ` Series.apply ` doesn't take a ` raw ` argument .
You might think that removing the ` raw ` argument would fix your attempt , but ` Series.apply ` has a peculiar behavior where if ` f ` is a ufunc and no keyword arguments to ` f ` are supplied , it completely ignores ` args ` . I think this is actually a bug . The workaround is to not use ` apply ` for this ; the broadcasting rules make ` apply ` redundant for your situation .
Beautiful . I still don't know how apply should work if I need it again , but this solves the problem at hand . Thanks .
Actually , note that this is not really an answer for the question as stated . I would give you credit , but this would mislead others finding the question for an answer on ` apply . ` Sorry
@USER szl : Well , this * is * how you apply a NumPy ufunc in the way you want . I've expanded the answer explaining why your attempt failed ; does that answer your question ?
Thanks , I accepted the answer . Maybe you could be even clearer about no use of apply could broadcast ( if that's the right word ) the second argument if the ufunc expects an array .

Perhaps @USER can merge these comments into his answer ?

Sorry I meant ` df = df.reset_index() ` this will reset index ( i.e. drop current one ) and create a new one starting from ` 0 `
Also just to make sure that you know range ( 1 , N ) won't go through every element in the array as it will start with the second one in the format I gave you - I was copying what you had in your question . should be ` for i in range ( len ( arr )): ` or ` for each in arr : ` or to get index val pairs ` for i , each in enumerate ( arr ): `

I would like to merge based on one key column and take the max or non-missing data for each equivalent row . #CODE

You need to save the result of your join : #CODE

I have a dataframe with double index . Each index is represents an edge . I would like to pivot ( ? ) it into matrix . First index should become columns and second index should remain index .
you could use the ` unstack ` method : #CODE

thanks @USER I didn't know ` .view ( ' < i8 ')` and ` .view ( ' < M 8[ ns ]')` . I was using ` delta = dt - UNIX_EPOCH

How to merge two dataframe in pandas to replace nan
I have 2 dataframes , A and B , I want to replace only NaN of A with B values . #CODE
It kind of works , but only if the two dataframes have the same index ( see @USER ' s comment to Foobar's answer ) . Notice that if instead you want to replace A with only non-NaN values in B ( that is , replacing values in A with existing values in B ) , A.update ( b ) is perfect .

By default ` patch ` replaces things with really generic mock objects . As you can see , calling the mock just returns another mock . It has a ` len ` of 0 even if the replaced object wouldn't have a ` len ` . Its attributes are also generic mocks .

It is straightforward to normalize the data so they both start with a nominal temperature value of 20 C at time = 0 seconds , but what I really want is to synchronize the data so that the temperature ramps begin at the same time .

My question is , say I want the following attributes ( measures ) calculated ( all per user id ): average price , total price , max / min price , median price , average duration , total duration , max / min duration , median duration , and number of times logged in ( so number of instances of id ) and all on a per month and per year basis . I know that I could calculate each of these things but what is the best way to store them for use in a visualization ?
For context , I may want to visualize the group of users who paid on average more than 8$ and were in the app a total of more than 3 hours ( to this point a simple new table can be created with the info ) but if I want it in terms of what shows they watched and whether they were on wifi ( other attributes in the original data set ) and I want to see it broken down monthly , it seems like having my new table of calculations won't cut it .
Would it then be best to create a yearly table and a table for each month for a total of 13 tables each of which contain the user id's over that time period with all the original information and then append a column for each calculation ( if the calc is an avg then I enter the same value for each instance of an id ) ?

2 ) apply it to your Series after converting to dataframe #CODE

first merge your indexes into one index using the following #CODE
Then just group by your new indexes and merge the values together ... #CODE

My current code moves the column by index via " df.columns.tolist() " but Id like to shift it by Name .
We can use ` ix ` to reorder by passing a list : #CODE

The loop is hidden within the ` map ` commands that act on ` lines ` . I think there is no solution without loops at all , since most built-in functions use them and many string-parsing methods like ` re.finditer ` yield iterators only .

X is the input array that is passed to the function when calling the resample method on a dataframe object like so ` df.resample ( ' M ' , how=my_func )` for a monthly resampling interval .
Your question is kind of missing a few things that would help . What is X- are you trying to pass a dataframe to this function ? The pandas resample function expects a specific string to be passed to the how parameter . Like ' mean ' or ' max ' . You're function appears to only be returning a value- res ?
typically the functions passed to the ' how ' argument do return a single value , which is what I'm doing here . I haven't tried it yet but I think the resample method can return python objects too .
Interesting approach , however this doesn't actually do what the function is doing . The function is counting the occurences of values above a threshold that are in a sequence and takes the average for the array that is passed to it which corresponds to each month . I'm trying to think of how to modify this but I keep coming back to having to pass the function to resample ( besides essentially rewriting the resample function )

I have poor understanding of pivot tables overall , but you have student B twice and identical assessor D for student B and values 100 and 2 . I am guessing you need to aggregate it in some why , otherwise the output does not look like pivot table .

Which returns me a list of unique values , I then tried to merge it to the original database ( with df.merge ) but had no luck .
Then , merge , left on the original column , and right on the index . #CODE

Pandas merge with logic
I would like to merge two dataframes , but can't exactly figure out how to do so without iterating . Basically , I want to merge a row from df2 to df1 if df1.date > = df2.start_date and df1.date = df2.end_date . See example below : #CODE
I thought about creating a date series vector to merge with df2 , so that I can combine on date , but it seems very manual and does not leverage the power / speed of pandas . I also thought about trying to expand df2 into single days , but couldn't find any way to do so without a manual / iteration type solution .
The naive iterative approach is ` O ( n*m )` , where ` n = len ( df1 )` and ` m = len ( df2 )` , since for each date in ` df1 ` you would have to check its inclusion in up to ` m ` intervals .
Wow this is fast . I thought my naive implementation was relatively fast , I sorted both dataframes and held the index of df1 and df2 so that I was at O ( m+n ) . I was getting ~ 1.5 seconds for len ( df1 )= 720 and len ( df2 )= 25 , your implementation is clocking in at .002 seconds . Maybe the inclusion step was the bottleneck ? Either way , thank you so much . Is there a good place to find optimizations like this one or do you just know this from experience ?

to individually query each column and find the information I'm looking for but this is tedious even if I figure out how to use the abs function to combine the two queries into one.How can I apply this filtration to the whole dataframe ?
Not sure if this is close enough ` df.apply ( lambda x : x.index ) [ abs ( df ) > 1 ]`
You don't have to apply the filtration to columns , you can also do #CODE

I use this function with pandas to apply it to each month of a historical record : #CODE

see docs here : #URL this is a multi-multi merge and soln is at the bottom . pretty much `` pd.merge ( df1.reset_index() , df2.reset_index() , on=levels_to_join ) .set_index ( levels_to_set )``
Thanks -- I am starting to think that I can do this with ` pd.merge() ` .. ( i.e. as a join )

and when I apply the command pd.to_datetime() to these columns I get fields resulting that look like : #CODE

I would like to create a new column ` time_hour ` . I can create it by writing a short function as so and using ` apply() ` to apply it iteratively : #CODE
You can apply a lambda , e.g .

how to apply different functions to each group of pandas groupby ?
I want to group the dataframe by the column ' type ' and apply different function to each group , say , ` min ` for group with type A , ` max ` for group with type B and ` mean ` for group with type C .
I think you might be misunderstanding the intent behind ` groupby ` . No worries , it happens to the best of us too . The intent behind ` groupby ` is such that you can apply the same operations to subgroups of your data , as grouped by the ` groupby ` operation .

My system has 16gb of RAM and is running Debian ( Mint ) . After creating the dataframe I was using ~600mb of RAM . As soon as the apply method began to execute , that value started to soar . It steadily climbed up to around 7gb ( ! ) before finishing the command and settling back down to 5.4gb ( while the shell was still active ) . The problem is , my work requires doing more than the ' do_nothing ' method and as such while executing the real program , I cap my 16gb of RAM and start swapping , making the program unusable . Is this intended ? I can't see why Pandas should need 7gb of RAM to effectively ' do_nothing ' , even if it has to store the grouped object .
That example is somewhat pathological . The groupby creates a separate group for each distinct value . Since you generated the values as random floats , it's likely that they are all distinct , which means there are 3 million groups . Each group passed to your ` do_nothing ` is a DataFrame , so you are creating 3 million DataFrames ( which ` apply ` then has to aggregate into a single result ) . Even if each has only one row , this is a lot of overhead . It might be more illuminating to create an exmaple whose " groupiness " ( i.e. , number of distinct groups ) is more in line with your actual data .

So whenever there is a ratio of 2 neighboring numbers close to 10 , 100 , 1000 , this factor is assumed wrong and taken out of the series . Too bad the ` diff ` function does not have a ` fill_value ` otherwise this could be a one-liner . You also want to do the zero index correctly ( maybe using ` .ix ` or so ) .

Replace characters in large gzip file
I am trying to replace some characters in my hive output so that Pandas can read it properly as a DataFrame .
I thought about the chunks idea but was concerned that the edges may cut my characters in half so that they were not able to be recognized by replace() . Is there a way to do it in chunks of lines ?
Your can read a chunk of 10Mb , split where it has and end-of-line character , replace in every element of the split except the last one , and fuse the last one with the first element of the next chunk split . ( To me , a bit complicated for few outcome , better do it one line after another . )

using pandas I tried to make a horizontal bar plot , but the y-axes label is cut off .

Python Pandas : drop a column from a multi-level column index ?
How can I drop column " c " by name ? to look like this : #CODE

I need to expand this as matrix . How to do that ? My first thought was iterate through the rows and apply numpy.hstack for joining , store it and numpy.vstack the stored rows , but it doesn't work as intended .

I have a multi-indexed pandas dataframe and want to create a bar-chart where I group by one index and stack over the other .

You could convert the dataframe to be a series with ` stack ` :
This works because ` stack ` will convert your data from 5x3 to 15x1 , thus treating it as if it were all in the same column and ensuring that the correct degrees of freedom is used for the standard deviation . #CODE
Refer to the documentation on ` stack ` here and here

I don't think you can replace the datetime ` NaT ` as you've found , what is the problem with having ` NaN ` / ` NaT `' s ?
To replace missing values with a string #CODE
I had the same issue : This does it all in place using pandas apply function . Should be the fastest method . #CODE

I have recently read ' Python For Data Analysis ' and have been trying to bring it over to real world examples . I did have to replace some information in my Dataframe / images to generics ( e.g. app1 , app2 ) . Otherwise the data and results are all real .
You might try creating a pivot table from the grouped dataframe . Pandas generally wants different columns for each line to just automatically do what I think you're looking to do . #URL
Then , create a pivot table : #CODE
Of note , if you have thousands of different times , and want to view the number of errors for the entire day , you will need to do a little more processing . The resample function is really good for this .

Pandas - merge two DataFrames with Identical Column Names
I want to merge them into one DataFrame while keeping the same Column Names . So the result would look like this : #CODE
Which gave me a DataFrame containing twice as many columns . How can I merge the values from each DataFrame into one ?

Then , I can resample another column using the index .

Basically the way to do this is determine the number of cols , set the minimum number of non-nan values and drop the rows that don't meet this criteria : #CODE
` axis=1 ` tells it to drop rows rather than columns
I had to use len ( df.columns ) instead of len ( df ) . Worked like a charm .

When I create a pivot table , I get the following warning :
Is this something I should be concerned about ? What I did was just create a pivot table : #CODE
In addition , when I try to use the values in the pivot table to fill in NA values , I get the following warning :
As Ed said , pivot_table wants to be more explicit about the row pivot in the future since it sets the row as the index for the pivot result . So change your code to : ` fare_means = df.pivot_table ( ' Fare ' , index= ' Pclass ' , aggfunc= ' mean ')` to be future proof =)

Thanks ari . @USER - good point , this will join any repeated data so your answer handles that case correctly while this doesn't
An example of how to produce such a column could be ( assuming that the DT key could be used as a sentinel value ) ` df [ ' helper '] = ( np.where ( df.d == ' DT ' , df.index , np.nan ))`

In the case where you are using a DataFrame , you can use DataFrame.where to use another frame's values to replace the values when null .

count in pivot table of pandas dataframe
How do I calcuate the count in the aggregate column of pivot table ? #CODE
Using ` len ` or `' count '` as argument for ` aggfunc ` does work : #CODE
Yes len or ' count ' as argument for aggfunc does work . But len with ' quote ' and count without quotes do not work . Why ? may be len is built-in . But what about count ?
` len ` is a built in function , ` count ` is not an existing function . For the strings , some strings are available as aliases for cythonized functions , and the name `' count '` is used for this to get the value counts .

One approach - filter your dataframe to just the negative samples before you do the groupby , then combine back with your larger frame using ` merge ` #CODE

The problem of course being that -1 is not an index of ` a ` , so I can't use ` loc ` . As the warning indicates , I have not changed column `' a '` of the last row , I've only altered a discarded local copy .

A possible solution for now would be to create the database yourself , and then append the dataframe to it .

Use ` loc ` and a boolean mask to filter the dataframe and select the ' id ' column , then call ` unique() ` to remove duplicates : #CODE

I got it : for x in range ( len ( slices ))

I need to do a " custom " resample that fits futures hours data where :
After doing the resample , the output for the above df should be : #CODE

My goal is to create a python script which will take two files entered at the command line and drop columns from the first file if column headers are present within the second file , and write the output to a new file .
I also have a smaller file containing up to a 1000 sample IDs , which correspond to the column headers in the large file . These relate to columns that I wish to drop from the large file .
Thank you . I couldn't figure out how to do that . Are there more memory efficient ways of achieving the column drop ? I am hitting memory issues on my poor laptop .
To get your code to work all you need to do is drop the empty string from your dropcols list . Something like this : #CODE
Of if you want to handle the case where your dropcols list works even if you specify a column not in the larger dataframe , you could do something like this - taking the intersection of your dropcols and the columns in the dataframe . #CODE
+1 for the intersection method ! great way to preserve usability in cases where ID's in smaller file are not guaranteed to be in the column header of the large file .
A more memory efficient way to do this . The key is to apply ` usecols ` in ` pd.read_csv ` . #CODE
Thank you very much for that , I will give this a try . I had resorted to splitting my files into managable chunks in order to process them , else used awk to drop columns .

Looking at your code , it seems you could use pandas built in moving average / sliding windows functionality , combined with a group by and apply .
Next , I want to join this dataframe with the the larger dataset , based on the entry being between the two ` Equal_Span ` columns . There may be a way in pandas , but cartesian-type joins seem much easier to express in SQL , so first , I'm shipping all the data to an in-memory sqlite database . If you run into memory problems I'd make this a file based db . #CODE

Length : 26761 , Freq : None , Timezone : None `

If you want to append to the table , below is a little bit more hacky workaround . First redefine ` has_table ` and ` get_table ` : #CODE

Conditionally replace missing data pandas
Would there be an easy way to replace the two 0s only in Atime with the times ( 2.0 , 2.4 ) listed in Btime ?
Use ` loc ` and boolean indexing to select the values and assign back : #CODE

How do you merge Pandas DataFrame rows into a string in one column ?

2 , We can put all the items in the list into a big ` DataFrame ` by ` concat ` them row-wise , since we will do step #1 for each item , we can use ` map ` to do it .
4 , We probably want to rotate the ` DataFrame ` 90 degrees ( transpose ) and ` reset_index ` if we want the index to be the default ` int ` sequence .
Thank you so much . This worked . Can you explain this portion to me . " pd.concat ( map ( pd.DataFrame.from_dict , Data ) , axis=1 [ ' fields '] .T " I had went the route of iterating through the results 1 by 1 , and creating a tuple for each timestamp / path , appending that to a list , and then reading that list of tuples with from_record . Your way was much faster .

shift columns of a dataframe without looping ?
consider this toy example . i need to shift each column down by one * ( its position in the array ) . so a kind of diagonal shift : #CODE
this makes me think that there must be an easier , perhaps vectorized way ... perhaps using some kind of " apply " , however i'm not sure how to do that when each column needs to be shifted down as a function of its position in the array .
Apply still uses loops by the way
Unless you have really a lot of data ( dozens of gigabytes ) , shifting it does not take hours . It seems that the time is spent in rebuilding the indices . Especially with hierarchical indexing it is possible that the complex indices are rebuilt after each shift . If your tables are large , this may take a lot of time .
One possible approach ( at least to isolate the problem ) is to just extract the data into a ` np.array ` ( take the ` .values `) , shift it , and recreate the DataFrame . In ` numpy ` shifting the data is rather trivial by , e.g. : #CODE

What I would do is to create 3 new dataframes ( if it is always 1 , 2 and 3 days you want to shift ) . Each one would be exactly df1 , just with the index shifted by 1 , 2 and 3 days . #CODE
Then just merge : #CODE
This merge means a left join ( you'll keep all values from df2 , regardless if there's a match or not in the df_1 table ) , you're merging on the index ( using index as key )

I want to replace all non-NaN values with a ' 1 ' , if there is a ' 1 ' anywhere in that row . Just like this : #CODE

Pandas cut with user-defined bins
I am working in Python and using the cut functionality in Pandas . I would like to have the bins in my pd.cut to be based on user-defined comma separated integers , with predefined top and bottom bounds . In other words , I would like to predefine ` bins ` as ` [ 0 , 100000 ]` and then append user defined integers to this set of numbers . If the user entered ` 5 , 100 , 5000 ` then ` bins ` would become ` [ 0 , 5 , 100 , 5000 , 100000 ]` and would then work with ` pd.cut ( df [ ' Quantity '] , bins ` .

If you want to append rows to D within your function , you could declare it as global : #CODE

Create initial tables / drop existing indexes

@USER dunno works fine without any warnings on my environment , You could try [ winpython ] ( #URL ) you'd need tp update pandas though from [ here ] ( #URL ) it's very simple just open the winpython control panel and drop the exe into it to upgrade

Just apply the function directly - I guess this will take more CPU as it's calculating all the maxes , then just getting the ones you want , but doesn't create a new variable . #CODE

First , when you resample , add a ` .groupby ( level=0 )` so the original timestamp is preserved . ( based on this answer ) #CODE
Then apply a groupby on the first level of the MultiIndex to apply the operation you want . #CODE

How can I aggregate ( sum ) over an index which I intend to map to new values ? Basically I have a ` groupby ` result by two variables where I want to groupby one variable into larger classes . The following code does this operation on ` s ` by mapping the first by-variable but seems too complicating : #CODE

That isn't fully vectorized , though , because of the ` apply ` . Something like ` np.isfinite ( df ) .sum ( axis=1 ) -1 ` should bypass all Python loops .

I am trying to merge 2 dataframes . They look like this : #CODE

My approach to the problem is this . First , ` groupby ` the id and iterate so we are working with one user id worth of data at time . Next , ` resample ` the irregular data up to daily values so it is normalized .

Don't append DataFrames like that at all ( nor start with an empty one ) , each append is a copy . This will result in a single copy , and constant appending performance . Concat docs are here
perfect ! I was trying to figure out how to use concat instead , didn't think to do it that way ( obviously ) . Thanks !
I agree ( I just added this recipe to the cook book ) , but would someone like to do a PR for the doc-string and concat section ?

I am trying to understand how to apply function within the ' groupby ' or each groups of the groups in a dataframe . #CODE

So far I've tried hist , but it errors out : #CODE #CODE

I'm having some trouble figuring out what I'm doing wrong here , trying to append columns to an existing pd.DataFrame object . Specifically , my original dataframe has n-many columns , and I want to use apply to append an additional 2n-many columns to it . The problem seems to be that doing this via apply() doesn't work , in that if I try to append more than n-many columns , it falls over . This doesn't make sense to me , and I was hoping somebody could either shed some light on to why I'm seeing this behaviour , or suggest a better approach .
In general you want apply to return either :
why are you showing using apply anyhow ? this should just be column assignment , no ?
@USER I don't get why the concat example doesn't work ( it ought to return a Series ) ?
@USER , I guess it is because ` x [ 0 ]` is going to be the cell value ( therefore one of ` numpy.int ` , ` numpy.float ` , ` object ` , etc ) , and ` concat ` fails ? But ` df.apply ( lambda x : pd.concat (( x*5 , x*6 )) , axis=1 )` will work giving almost the same result as ` df.apply ( lambda x : pd.Series ( np.hstack (( x*5 , x*6 ))) , axis=1 )` , except the column index are ` 0 1 0 1 ` instead of ` 0 1 2 3 ` . So I guess my initial feeling is not right , ` Series ` is not absolutely required .

Here are solutions using apply . #CODE
Can't think of anything great for unique . This uses apply , but may be faster , depending on the shape of the data . #CODE

How to normalize text in a list that is itself in a column of Pandas dataframe ?

thanks ! ( moved extra info above at stack exchange's nagging )

apply function on dataframe involving two rows
I want to apply a function to calculate distance based on the longitude and latitude . Basically I need a way to express the function can handle two adjacent rows in dataframe
I know there are ways to apply function along axis 1 and 0 , but they seem only apply to single row or column . How can I express something involving several rows or columns .
If you just want the difference then just call ` diff ` to do the calculation
Apply won't be able to this , but you can do something simple like the following : #CODE

I would like to group the series by hours or days and apply a function group-wise which calculate the ratio #CODE

See ` shift ` to understand the params

I think there are many use cases in which neither align nor merge or join help . The point is to not use DB related semantics for aligning ( which for timeseries are not so relevant in my opinion ) . For me aligning means map series A into B and have a way to deal with missing values ( typically sample and hold method ) , align and join cause a not wanted effects like several timestamps repeated as a result of joining . I still do not have a perfect solution , but it seems np.searchsorted can help ( it is much faster than using several calls to join / align to do what I need ) . I could not find a pandas way to do this up to now .
How can I map A into B so that B so that the result has all timestamps of A and B but no repetitions ( except those which are already in A and B ) ?
There's also the [ ` join `] ( #URL ) method but I didn't test if this is faster . Most of the pandas code is already cythonized so I don't think you'll be able to improve the time needed much .
@USER : it seems join is the fastest , %timeit df1.join ( df2 , how= ' outer ') # 100 loops , best of 3 : 10.1 ms per loop
If you need to synchronize then , use ` align ` , docs are here . Otherwise merge is a good option . #CODE
Align and join is not what I need , I need to map timeseries A into B , no relational DB logic is required , I do not want to create new timestamps ( repeated ) . New timestamps are created by having repeated timestamps in both A and B .

Python Pandas resample , odd behaviour
I have 2 datasets ( cex2.txt and cex3 ) wich I would like to resample in pandas . With one dataset I get the expected output , with the other not .
When I loaded cex2 , there's a row whose date is 1969-12-31 18:00 : 00.003500 . Looks like an error since nearly every other row is from the last couple of days .. Thus , the resample will try to fill in between the junk date , to current- and you'll see errors . You have a junk line somewhere in your data .

I can achieve this if I know the exact number of data points in that range ( i.e. range * sampling freq ) , but the point of setting the Time to one of the indexes was to get around having to do that .
` ix ` is not encouraged . ` loc ` and ` iloc ` ( for pure positional indexing ) are prefered .

Why does the second block of code not work ? Doesn't DataFrame.apply() default to inplace ? There is no inplace parameter to the apply function . If it doesn't work in place , doesn't this make pandas a terrible memory handler ? Do all pandas data frame operations copy everything in situations like this ? Wouldn't it be better to just do it inplace ? Even if it doesn't default to inplace , shouldn't it provide an inplace parameter the way replace() does ?
No , apply does not work inplace* .
In general apply is slow ( since you are basically iterating through each row in python ) , and the " game " is to rewrite that function in terms of pandas / numpy native functions and indexing . If you want to delve into more details about the internals , check out the BlockManager in core / internals.py , this is the object which holds the underlying numpy arrays . But to be honest I think your most useful tool is ` %timeit ` and looking at the source code for specific functions ( ` ?? ` in ipython ) .
* apply is not usually going to make sense inplace ( and IMO this behaviour would rarely be desired ) .

It would probably be better to interpolate January data than ignore it with the min_periods=11 . All you have to do is replace ` data ` with ` data.interpolate() ` . You could also back or forward fill with fillna() .
Note that this is mainly for statistical reasons rather than for pandas-related reasons . If January data is systematically different from other months ( e.g. climate data ) , then tossing it out will bias your results much more than interpolating . And it is really easy to interpolate or backfill .

I would like to resample it to a regular time series with 15 min times steps where the values are linearly interpolated . Basically I would like to get : #CODE
However using the resample method ( df.resample ( ' 15Min ')) from Pandas I get : #CODE
I have tried the resample method with different ' how ' and ' fill_method ' parameters but never got exactly the results I wanted . Am I using the wrong method ?
It takes a bit of work , but try this out . Basic idea is find the closest two timestamps to each resample point and interpolate . ` np.searchsorted ` is used to find dates closest to the resample point . #CODE

I am using a Compound screen made up of 4 by 4 monitors with small ( but nonzero ) bevel -- the gap between the screens .

To use mean values for numeric columns and the most frequent value for non-numeric columns you could do something like this . You could further distinguish between integers and floats . I guess it might make sense to use the median for integer columns instead . #CODE

python lists : merge and replace
I have two string lists ` A ` and ` B ` , where ` B ` is almost a subset of ` A ` except that ` B ` has a postfix ` * ` for some of the elements . My goal is to replace elements in ` A ` with those corresponding elements in ` B ` that have the postfix . An example : #CODE
I'd like to get a list as ` [ ' abcd ' , ' ddse ' , ' 123d ' , ' aaaaa* ']` . The way I can think of is to split and transform ` A ` in to a two-column dataframe and merge with ` B ` , then put the ` * ` column back to column in ` B ` . But that does not look very elegant since I need to do a lot of this operation . Is there a better way of doing it ?
So you really want to check , for every element ` a ` of ` A ` , whether there is some element ` a* ` of ` B ` , and if there is some ` a* ` , to replace ` a ` in ` A ` with ` a* ` . #CODE
Thanks ! This works nicely . One more question , what if ` A ` and ` B ` are lists of lists ? That is , the ` A ` , ` B ` in my example are actually one element of the two lists ` A_L ` and ` B_L ` that I want to merge . I want to do a element-wise merge on ` A_L ` and ` B_L ` . How to nest your code into this problem ?
I'm not sure what you mean , but you just need to replace ` B ` and ` A ` with the respective list . Therefore , when I understand you right , just replace ` B ` with ` B_L [ 0 ]` and ` A ` with ` A_L [ 0 ]`
Yes that's what I mean . So to do the element-wise merge , I need to loop through the index for ` A_L ` and ` B_L ` ? Can I nest this loop inside the list comprehension as well ?
Thanks ! I just tried list comprehension it also works : ` [[( x + ' * ') if (( x + ' * ') in B_L [ i ]) else x for x in A_L [ i ]] for i in range ( 0 , len ( A_L ))]` . I think it is faster though I haven't test it yet .

Just apply it to every column : #CODE

Pandas data frame : resample with linear interpolation
Thanks for the answer . However , dt = df1.datetime - df2.datetime is then not defined .
what do you mean by that ? it's defined at ` dt = df1.datetime - df2.datetime ` , or do you mean something else ?

Did you melt your data.frame before plotting ? The sample data you posted above does not seem to match up to what's in ` entriesPerHourPerRain ` given the names of the ` aes ` mappings . Nor does it make much sense that you are using literal string values in your ` aes() ` calls . You typically use either symbols or you use strings with ` aes_string ` . Are you sure this is representative of the data and code you are actually running ?
Yes sorry I forgot to add the melt :

you just open like normal `` store = pd.HDFStore ( filename , mode= ' r ')`` ( mode is append by default , but if you aren't modifying doesn't matter ) . `` to_hdf / read_hdf `` auto open / close .

There's nothing built-in , so you'll need to calculate it with apply . For example , for an easy ' how many 7 day periods have passed ' measure . #CODE

( just realized I'm late with an answer but I'll leave this an an alternate answer if you want finer control over the output format ) Just replace the TextToWrite line with :

Shift and subtract to get the delta time between events
JohnB - Yes I know how to group-by based on the location . That it actually included as part of the question . But when you group by you end up with a group-by object which you can iterate over . for a specific group is there a way to apply a shift to it . Or do I need to iterate over it . Also once you have the groups there seems to be options to process the data but not add another column to the group . This is kind of where I'm stuck .

Warning : I had previously suggested ` df_test.ix [ i , ' Btime ']` . But this is not guaranteed to give you the ` ith ` value since ` ix ` tries to index by label before trying to index by position . So if the DataFrame has an integer index which is not in sorted order starting at 0 , then using ` ix [ i ]` will return the row labeled ` i ` rather than the ` ith ` row . For example , #CODE

Your problem is strange , I can't explain but the following worked for me : ` import datetime as dt mydata [ ' new '] = mydata [ ' event_date '] - dt.datetime ( 2006 , 1 , 1 )` could you confirm

Consider the following example in which we setup a sample dataset , create a MultiIndex , unstack the dataframe , and then execute a linear interpolation where we fill row-by-row : #CODE
Replace the following line : #CODE

Pandas dataframe float index and transpose error
I agree it will be faster but I could not find any method in the docs to transpose the data before reading it in . I would be be happy to see alternative answers as I'm sure there are better methods but as far as I can tell there is nothing actually wrong with what I have done ?
I figured out how to make this work entirely in pandas . Don't indicate an index nor a header row . Transpose the dataframe and drop the index . Then , create a list out of the first row of data which will be your string titles for the columns you really wanted . Assign the column names to this list and then reassign the dataframe to a sliced dataframe eliminating the first row of string names ( ' vfd3021 ' in my case ) .

Use the pandas method ` isnull ` to test : #CODE

I get the value-error if the date is 000000 . How do I continue and insert rest of the data ?

how to merge a groupby output ( series ) back to the dataframe
how do i put the z-scores back in the portfolios as a new column ( join creates the columns with the array , not like a continuous values )
Use transform ( instead of apply ) , as chrisb suggests , and assign the result to ` portfolios [ columname ]` : #CODE

Thanks @USER , thanks for code , I have small problem now , two months of cvs1 has 50,000 data Value and two months of csv2 has 4,000 data Value and so on , it's really hard to fit all the graph for two months at same size , [ link ] ( #URL ) from the graph , only two months of data is shown but all the signals doesn't fit . Python is plotting based on the data points but I want to plot based on days ( for ex : 2 months / 60 days ) . Is there any logic to apply , or methods , any suggestions are highly appreciable , since I am in this for long time . Thank you very much .
Its probably the easiest to put all you're data in a single ` Dataframe ` , then you would only have to execute commands like the ` rolling_mean ` once . Since your example is not easy to replicate it might depend on how your data is actually structured . But you could loop over the ` csv's ` and collect all individual Dataframes in a list or something , perhaps store some ` id ` as well , and then use ` pd.concat ` to merge all Dataframes into a single one . The rolling_mean and any other command including the plotting is very straightforward from then on .

Pandas making cumulative Z scores map onto dataframe
I would like to map a Z score onto the Dataframe for a particular stat for each team . With the Z score tracking how a team is doing compared to the rest of the teams within its league .
This function does what I want and returns a dictionary but I don't know how to map the dictionary on to the dataframe , or if it is the best way of going about it . #CODE
You can use ` .map ` to map a dictionary to values contained within a pandas dataframe . #CODE

and then apply the above technique . ( I sometimes toss in a ` reset_index ( drop=True )` , but that's up to you . )

Triple transpose might not be as elegant as your answer .
Another method without using transpose is to create a boolean mask on whether the first row has values larger than 0.5 and then drop the NaN's with a threshold and then finally make a list of the df columns to filter the original df . This is pretty obfuscated though ;) #CODE

I am trying to run the ` scipy.stats.entropy ` function on two arrays . It is being run on each row of a Pandas DataFrame via the apply function : #CODE
Turns out if any column in the original dataframe is an object , the series created via apply is of object type . Thanks again . #URL

Then define my group operation and apply it : #CODE

Python Pandas multi-index values not moving over in pivot table

You can pass multiple columns , as long as exclude the ' groupby ' column : ` sns.boxplot ( df [[ ' a ' , ' a2 ']] , groupby =d f.b )` . However , it will merge all values from all columns and then calculate the statistics per group , so you will loose the distinction between columns . Keeping the distinction and still plotting all at once will require some restructering of your original dataframe .

It isn't really any better than the answer you linked , but I think the way to achieve this in seaborn is using the ` FacetGrid ` feature , as the groupby parameter is only defined for Series passed to the boxplot function .
ok , I didn't understand you wanted the boxplot side by side .
As the other answers note , the ` boxplot ` function is limited to plotting a single " layer " of boxplots , and the ` groupby ` parameter only has an effect when the input is a Series and you have a second variable you want to use to bin the observations into each box ..
However , you can accomplish what I think you're hoping for with the ` factorplot ` function , using ` kind= " box "` . But , you'll first have to " melt " the sample dataframe into what is called long-form or " tidy " format where each column is a variable and each row is an observation : #CODE

Then , using the approach from this answer , pivot into columns , and drop the sentinel . This won't be ultra performant because of the apply , but probably reasonable on data of your size . #CODE

I've tried to lead / lag the variables on Data2 to create 180 new variables and merge it to Data1 , but that was very inefficient and lead to memory issues ( used over 100GB of RAM ) since Data2 is somewhat large .
Yeah , this is pretty inefficient . You're shifting a whole column 90 times per event ! I would try a more SQL type of approach . Select the 90 dates before and after each event for each stock . Merge those and things are already pre-shifted for you . That should be much more efficient .

I can do this with several steps ( e.g. merge , than fillna ( 0 ) , finally sum ) but was curious to know if there is a fast way to do this as it is a case which I am encountering frequency .
@USER : I am still struggling in finding a similar approach for join or merge , e.g. I join ( left ) on the index df1 and df2 , the result has values of df2 with no corresponding timestamps ( these values are NaN currently ) which have been extrapolated using a fill_method , e.g. taking the last known value of df2 . Does this exist in one command ? ( this is another recurring use case )

Note you can also use ` df.head ( 10 )` and ` df.tail ( len ( df ) - 10 )` to get the front and back according to your needs . You can also use various indexing approaches : you can just provide the first dimensions index if you want , such as ` df [: 10 ]` instead of ` df [: 10 , :] ` ( though I like to code explicitly about the dimensions you are taking ) . You can can also use ` df.loc ` and ` df.ix ` to index in similar ways .

You used a pandas tag ... So if these are pandas dataframes , just do a standard merge with the last column of each dataset as the key . Just make sure keys are unique ( and same dtype ! ) or else create one that is . Merging on floats here is not ideal , but may be OK , just check that you get a one to one merge at the end .

IIUC , what you're looking for is that the operative convention is that of numpy bool arrays , not Python bools : #CODE

You can either preprocess the data as levi suggests , or you can transpose the data frame after creating it . #CODE

Not sure why ` factorplot ` would be unable to do this -- couldn't the empty intersection of ` x1 , x2 ` just not be plotted ?

How do I apply filters and functions to previous rows in pandas ?
The proper use of apply is a bit unclear to me .

Faster alternative to grouby / shift
note that the groupby and shift is implemented in python ATM - needs cythonization to be fast - open issue is here : #URL
@USER So shift outside of groupby is cythonized but inside of groupby is not ?
ATM this is the best way to do this . a pull request to cythonized the groupby would be appreciated . the actual shift code is just creating and using an indexer so it is quite fast itself ( and would not benefit from cython ) . groupby needs to avoid processing in python space so essentially would create thrn indexer ( eg what you are using in the loc , but for the general non sorted case ) . nothing to hold out .

I have a 1-dimension numerical dataset ( but my question also applies for a n-dimension numerical dataset ) which I want to cluster , and I already know the values of the cluster centers . So I only want to map each data point to its associed cluster center ( the one which is the closest of the datapoint ) .

Further , you should not repeatedly using ` get_group ` , instead use the cythonized functions , ` apply ` , or iteration , see docs here

Say I have a large dataframe and that I want to apply one operation to every element in a column .

I was looking for an option like example 2 , except it won't apply the formatting to future dataframes . Thanks !

You can apply any operator or across a column . To mutate it in-place , just multiply the column by -1 : #CODE
You don't need the apply , just do ` - frame.abs() `

Map List of Tuples to New Column
I would like to map ` tp ` do ` df ` such that the the second element in each tuple lands in a new column that corresponds with the row matching the first element in each tuple .
I've tried using ` lambdas ` , ` pandas.applymap ` , ` pandas.map ` , etc but cannot seem to crack this one . So for those that will point out I have not actually asked a question , how would I map ` tp ` do ` df ` such that the the second element in each tuple lands in a new column that corresponds with the row matching the first element in each tuple ?
I suspect the problem you were having with ` map ` is that it takes a function or a dict . There are many places where a list of 2-tuples can be used in place of a dict , but this isn't one of them ; ` map ` will try to treat that list as a function .
You need to turn your list of tuples into a dict which is ridiculously easy to do in python , then call ` map ` on it : #CODE
The docs for ` map ` show that that it takes as a function arg a dict , series or function .
The online docs show how to apply an operation element wise , as does the excellent book

Stack / Unstack Multi-index pivot table in Python Pandas
How do I stack / unstack the " Peat-Forming " to have " PBL_AWI " and " Description " underneath ?
@USER Maybe , I can try playing around on the Excel side to get what I want . Do you happen to know off the cuff how to get sub-totals for the Non-Peatlands and Peatlands ? Was thinking of spilting the tables ( b = p_table.loc [ ' Non-Peatlands ']) Add an Total row and then append both tables back together . Hopefully having an overall total as well . I realize this is outside scope of my question , just I just post new one ?
For the totals you . calculate and append to dataframes peat and non_peatlands . You might have to play around with the MultiIndex to get it to merge . eg ` peat.index = pd.MultiIndex ( ' set the correct index probably should use from tuples ')` tuple of the Total peatlands looks like this ` ( " Total Peatlands " , "")` to get the cells to merge properly .

python pandas to drop lines and substitute values in specific columns of a csv file
automatically drop lines that do not have values at specific columns , for example columns 1 and 2
To drop the rows with NA , you can do : #CODE
To replace a certain value , you can do : #CODE
also , the example above would not work , because I want to be able to not drop if NaN is present in some other column
updated the answer , you can use ` subset ` to specify the columns . You can always first replace ' somestring ' with NaN , or just manually select the rows with ` df [ df [ ' col '] ! = ' somestring ']`

And then ` unstack ` to get the result : #CODE

How to resample one of the series with the other's indices , using interpolation ?
First you can align both serieses ( so they both have the same indices . It is also possible to only reindex one of both to the index of the other with ` reindex `) : #CODE
Then you can interpolate the missing values ( eg with linear interpolation based on the time index ): #CODE

Thanks , I was thinking linespace would take care that , [ figure ] ( #URL ) , if I insert zero will lose the shape and also if I add random numbers also lose the shape . If not linespace please suggest me what would be the best way to achieve . Thanks a lot
I don't know what that figure really shows but you could just append a new df : ` df2 = df2.append ( df2 , ignore_index=True )` or a new df : ` df2.append ( pd.DataFrame ( np.random.rand ( 50 , 2 ) , columns = list ( ' ab ')) , ignore_index=True )`

Is it an option to use ` pandas.tslib.repr_timedelta64 ` but then cut off the minute / second part ?
You mean , split the string on ' : ' and then append ' h ' ?
I appreciate that your solution leaves the ` Series ` with its index intact . I did not come up with a good way to either display ` %d d , %d h ` or ` %d d ` or ` %d h ` without very convoluted code . So I rather stuck to a list that I insert into the existing ` DataFrame ` and thus get back the index .

Given a pandas DataFrame or numpy array of floats , if the value is equal or smaller than 0.5 I need to calculate the reciprocal value and multiply with -1 and replace the old value with the newly calculated one .
The typical trick is to write a general mathematical operation to apply to the whole column , but then use indicators to select rows for which we actually apply it : #CODE

You can simply get the list of weeks where there is a change , then compute their differences , and finally join those differences back onto your original DataFrame . #CODE
I put no change to Nan because the duration seems to be undefined when no change is made . But zero will work too . With the above code , the NaN is put in automatically by merge . In any case ,

the code below gets you the index you want but I've had to do some work to strip the extra `'` and ` ( )` parts from your file . #CODE
Thanks , I solved my problem with the strip function

This statement produces a ` ValueError : Cannot shift with no offset ` . What am I missing ?
If you have a DatetimeIndex , the ` shift ` shifts your data with a period of time . If your index has no frequency , you have to provide that to the shift method with the ` freq ` keyword ( eg ` freq= ' s '` to shift the data one second )
If you just want the difference between two consecutive values in the index , you can use the ` diff ` method ( of a Series , a bit easier than shift and substract ): #CODE

Merge misaligned pandas dataframes
I would like to merge A and B , but my current solution results in a misalignment .
The current solution to merge the dataframes is as follows : #CODE
It doesn't make sense , can you show what the csv files look like ? Because you shouldn't have this shift in individual rows , unless you screw up big time with your commas ( or other separator )
Your function to merge the different frames is perfectly fine ! The misalignment is a problem in the read_csv or in your csv file itself

Also I want to keep " month " column ( as a regular column , not index ) but I can't figure out where to put it . Is there any easy way ( without any merge , join ) ?

Append new data to a dataframe
The problem is I want to dynamically create a dataframe to which I can append one row at a time . Also I do not know the number of rows that there would be . Therefore , I cannot specify the index when defining the dataframe .
The ' data ' is being populated properly but I am not able to append . At the end , df comes to be empty .

pandas apply with inputs from multiple rows
I need to do an apply on a dataframe using inputs from multiple rows . As a simple example , I can do the following if all the inputs are from a single row : #CODE
However , if I need ' a ' from the current row , and ' b ' from the previous row , is there a way to do that with apply ? I could add a new ' bshift ' column and then just use df [[ ' a ' , ' bshift ']] but it seems there must be a more direct way .
Edit : Some info on what I'm doing etc . I have a pandas store containing tables of OHLC bars ( one table per security ) . When doing backtesting , currently I pull the full date range I need for a security into memory , and then resample it into a frequency that makes sense for the test at hand . Then I do some vectorized operations for things like trade entry signals etc . Finally I loop over the data from start to finish doing the actual backtest , e.g. checking for trade entry exit , drawdown etc - this looping part is the part I'm trying to speed up .
Point taken about apply not being magically faster . Come to think of it I suppose doing multiple applies could well end up being slower than a single large for loop . The first answer to #URL made me want to try it .
Added a little background . Considering your point about apply and speed , I think what I'll do is try to vectorize as much as I can , and see what I'm left with , and then come back with more questions as I have them . If I tried to post all my code as it is it'd just be overload . Appreciate your advice .
This should directly answer your question and let you use apply , although I'm not sure it's ultimately any better than a two-line solution . It does avoid creating extra variables at least .
You could also look into reshaping the data with stack() and hierarchical indexing . That would be a way to get all your variables into the same row but I think it will likely be more complicated than the concat method or just creating intermediate variables via shift() .
Thanks . I timed your first method vs adding a shifted column and then deleting it after I finished the apply , and interestingly they were virtually identical ( and not very fast ) for any df size I tried , so I guess the time is all spent in the apply itself . Indeed the 2nd method ( vectorized ) is much faster and I'll try to do as much of that as possible but unfortunately I don't think it's possible to vectorize all of it .
@USER It's not just the apply , it's also somewhat expensive to concatenate / merge or create a new variable . In my example data , about half the time cost is due to concat and half due to apply . Just replace the apply ( mean ) with mean() and you'll see it's still slower .

I can strip out the rightmost ' .csv ' part like this : #CODE

Apply function to pandas dataframe that returns multiple rows
I would like to apply a function to a pandas DataFrame that splits some of the rows into two . So for example , I may have this as input : #CODE

How can I add a column ( or replace the index 0-9 ) , by a timestamp with the now time ? The np array will not always have size 10

pandas - DataFrame expansion with outer join
I want to generate a pandas DataFrame representing a map ` witter tag subtoken - poster ` where tag subtoken means anything in the set ` {hashtagA } U { i | i in split ( ' _ ' , hashtagA ) } ` from a table matching ` poster - tweet `
Then I would simply apply the extract function with a regex : #CODE

Use ` notnull ` and specifically ` idxmax ` to get the index values of the non ` NaN ` values #CODE

That is , the first two lines together constitute the headers . Is there any way to apply ` read_csv() ` to this without any major hassle ?

` diff ` is just the difference , if you want the difference quotient instead you can do ` df.POP.diff() / pd.Series ( df.index , index =d f.index ) .diff() ` . ` diff ` on an index means set-difference , so you have to convert it to a series first .

Pandas map by index value without dummy column
I would like to map the values in the Probability column to ' s ' for ' success ' for the first 10 values , and ' f ' for ' fail ' for the rest . To do this , I create a dummy column called Index , apply a transformation , and then drop the dummy column . #CODE

Using slightly different numbers ( for no particular reason ) , you can stack to for a Series and then use boolean indexing : #CODE

It is still object and not time series even after using to_datetime method . Does it mean that I have non-valid dates ? How will I know how many invalid dates are there ? Is there any way to drop those records ?

python re-sample at a uniform semiannual period ( equivaent of ' BQ ' in pandas resample )
is there an ' BQ ' equivalent semiannual resample in python ? i didnt find it here
i've a set of records , some of them follow jun-dec , some jan-jul , some feb-auh etc . how do i resample all of them to jun-dec ( concurrent for jun-dec , and following jun / dec for other records ?
Should give the right answer ( and can drop the first entry ) . #CODE

I found the unstack ( level ) method to work perfectly , which has the added benefit of not needing a priori knowledge about how many Codes there are . #CODE

I suppose in this particular case the difference is that you will never create a ' dummy ' 2010 row with tsset . Rather , stata just does a normal shift / lag but sets lags to missing as appropriate .

Stack columns from multiple CSV's into a master file
I am trying to take many CSV's and stack the column data into one file using the headers to
align the data . The headers are not always in the right order and some are missing but there are matches between the files .
You'll need to create a list of all the csvs using os.listdir or glob , loop through the list and read each csv into a dataframe and then use the concat function to merge them all into one long dataframe . pandas.concat() is header aware and will automatically align the headers for you . You can then output the dataframe to a csv . #CODE
Instead of creating a list , you could create a blank dataframe - pd.DataFrame() , then concat the frame within the loop . Not sure it will be hugely faster , but will narrow down the lines of code .

This fails TypeError : Join on level between two MultiIndex objects is ambiguous
Even if you could help me just merge these data frames . I get this error :
I came from R so I'm used to the easy cbind or merge .

Also , perhaps would be better to put all the data-frames in a list and then ` concat ` them at the end ; something like : #CODE

If you want to use ` apply ` it can be done , but you will loose the column names : #CODE
See edit , if you want to use ` apply ` , it can be done but you will loose the column names . Basically the ` apply ` function should return a ` Series ` of length of 3 , compare the two new edits .

Thanks ! Probably the most readable way to do it , although I generally try to avoid apply / lambda if possible for speed reasons if there is a good alternative ( though speed is not a concern here ) .

I am trying to write a pandas df to a csv . However , I need to loop a lot to get my output in chunks . I want to stack the output vertically in a csv . So after each iteration , I want to write to a particular set of rows ( 1:10 , then 11:20 , etc ... ) and clear memory so as not to make a giant object . Is it possible to use df.to_csv to do this ?
` to_csv ` accepts a mode argument to append : #CODE

I'd apply ` value_counts ` columnwise rather than doing ` groupby ` : #CODE

How to flush away the default suptitle of boxplot with subplots made by pandas package for python

I have a DataFrame where I would like to keep the rows when a particular variable has a NaN value and drop the nonmissing values .
In the above DataFrame , I would like to drop all observations where opinion is not missing ( so , I would like to drop the rows where ticker is aapl , goog , and wmt ) .

Note : Here we're reading 2 1 3 to mean [ " B " , " A " , " C "] , however you could replace the index of the result as desired ( i.e. replace it with one of these ): #CODE

Transpose columns with repeated data
I have the data stored in columns that I need to change to rows . The transpose method is not working as expected . #CODE
Use melt function for that .
you can use sort() after you melt to get the exact order

Maybe use stack ? #CODE

You can use ` map ` from a dict of your second df ( in my case it is called df1 . yours is DF2 ) , and then multiply the result of this by the amount : #CODE
So breaking this down , ` map ` will match the value against the value in the dict key , in this case we are matching ` Currency ` against the ` NAME ` key , the value in the dict is the ` OPEN ` values , the result of this would be : #CODE

Merge multiple data frames with different dimensions using Pandas
What I want to do is to merge them based on ` head ` column . And whenever the value of a ` head ` does not exist in one data frame we would assign it with NA .

Pandas DataFrame column to cell in pivot table
And I'm looking to get a pivot table out of this with A in the columns and B in the rows , then C and D as sets , as below : #CODE
` groupby ` and then ` unstack ` : #CODE

Pandas join memoryError
I'm trying to join 2 dataframes , essentially trying to replicate a vlookup in python .
I suspect this depends on how many rows share a sku number ( as this can explode - i.e. if 2 rows join with two rows then you get 4 rows etc . )

There is numpy's histogram function , and matplotlib's histogram plotting function ' hist ' .
This presentation doesn't rescale , it horizontally translates the individual histograms so that they don't overlap and then labels the X-axis with the column names ( at median values ) rather than represent scale . #CODE

Thanks a lot Jeff , I updated the OP with an example . I basically want to apply the solution that you described to ** each item ** in the series ( i.e. get the offset with respect to ** each timestamp** ' s hour in ` ms `)

If you just want this code to work , just replace the return with this , which uses the numpy divide function , which will broadcast on shape and not try to match indicies : #CODE
@USER , it worked for me , but didn't when I tried with a different shape . I think ` np.divide ( diff , pd.DataFrame ( time_intvall ))` should always work .

See the docs on apply . Pandas will call the function twice on the first group ( to determine between a fast / slow code path ) , so the side effects of the function ( IO ) will happen twice for the first group .
Yeah , or you could even still use apply on the main df , but then change item_grouper to iterate . I didn't step through all the code , but if possible , your life might be easier if you avoid any IO in the groupby and use standard pandas IO ( i.e. ` df.to_csv ( sep= ' \t ')` to write your tab separated file .

Transpose the dataframe before you use ` df.to_dict ` . #CODE

So I have a dataset consisting of several million rows of trading data that I am trying to apply a filter to as in the following code . The function ` trade_quant_filter ` looks for outliers and then adds the index of all the outliers to a list to deal with later . #CODE

@USER : If you do not like ` tabular ` , you may use any LaTeX package . The output of ` to_latex ` is IIRC for the ` booktab ` package . However , beware that the ` matplotlib ` LaTeX system does not necessarily like line breaks , so you may need to replace ` \n ` by spaces in the string given by ` to_latex ` . IMHO well-designed LaTeX tables look very good . Usually the trick is to reduce the number of lines ( horisontal lines are often unnecessary ) , but this is a matter of taste .

Desired pivot / groupby / crosstab Output : #CODE

I would personally do this the way @USER suggested instead . Unless you actually need the query string for some reason ( maybe to print it out ? ) , building a string to eval it as an expression , instead of building an expression , is usually the wrong answer .

/ e : I just realized that I missed an important condition in my question : I only want to drop the observations , if one or several SPECIFIC columns have duplicate values . Other columns can be different however . In the example above , lets say i dont care if there is a difference between color within a group , but only want to check if the age has a different value . ( I edited the example to reflect this ) .My actual case is more general and contains more columns , so i want e.g. to check a few columns and ignore others when dropping observations .

Have you tried multiplying them by some multiple of 10 ? Then you could truncate them and evaluate as an integer .

I'm really struggling with the Pandas ` rolling_apply function ` . I'm trying to apply a filter to some time series data like below and make a new series for outliers . I want the value to return ` True ` when the value is an outlier . #CODE

I first cut it in chunks and store them in a ` HDFStore ` named ` input.h5 ` then I use ` groupby ` on ` user_id ` following Jeff's way .
` group_user= store.select ( ' clean_input ' , where =[ ' user_id == %s ' %user ])` is too heavy in time complexity since I have really a lot of groups , and I am sure there is a lot of redundant sorting in the routine of ` store.select ` if I apply it 10 millions times .
But you have not introduced the variable sub_group_chunk , and If i try chunk.groupby ( sub_group_hash ) I get an error of the form : long object have no attribute _____get.item____ because It tries to apply the function to the index .

I figured out this also works : ` pd.read_sql ( " SELECT CONCAT ( y , ' : ' , w , ' : 1 ') AS dt , d FROM t " , db , parse_dates = { " dt " : " %Y : %U : %w " } )` . This way the parsing happens on the Python side . It still does CONCAT in SQL , so I'm not sure if it's better or not than what you suggest .

This seems like it should be straightforward but is stumping me . Really love being able to iterate through the groups of a groupby operation and I am getting the result I want from the groupby but I am unable to merge the final result into one dataframe . So essentially I have the below code which can capture the dataframes in a list but I don't know how to loop through a list of dataframes to make one dataframe : #CODE
And I just want to merge them all on the col1 key . In other words the following works great but how do I do it without hardcoding the individual elements in the list and loop through all of them in dfs ? #CODE

@USER LOL , thanks for that very spot on link . I was aware this sort of feature is something users often ask for and developers often say " use what is already there " so I figured I had to write a version for my own use . FWIW , I think it happens frequently b / c it remains a pretty stark contrast between how convenient this sort of thing is in something like stata compared to pandas ( pandas is also pretty young ) . I'll try to join the github world soon when I have time .

Now let's make something that has the right shape as what we want ( we shouldn't assume that we'll necessarily see every month , after all ; our test example only has 5 months with nonzero values ): #CODE

Accessing columns after applying the stack function on a pandas dataframe
How can I access the columns after the stack function is applied on a dataframe ?

I trying to create a function that replace df [ ' S '] using df [ ' S '] [ index-1 ] value .

@USER I'm iterating because I'm using it to insert values in a DB coming from an XLS . Dunno if it's the right thing to do . When I say I lose index association I mean that by passing the ` pandas.Series ` to the built-in ` filter ` function I get back a python ` list ` , which doesn't come with the original ` pandas.Series ` index .

I'd be sure that this is actually what you want to do , storing both data and summary statistics in the same frame is a little odd . That said , you can use ` concat ` to stack dataframes . #CODE

Confirmed that both solutions work . I'll accept the one that uses ` ix ` since it conforms more narrowly to the question as asked . But is either of the two preferred ?
` xs ` may be preferable here . The indexers ( ` ix ` , ` iloc ` , ` loc `) can do a lot more , but because of the syntax ambiguity between tuple keys and MultiIndex selection , it's likely less error prone .

In my experience , this approach seems slower than using an approach like ` apply ` or ` map ` , but as always , it's up to you to decide how to make the performance / ease of coding tradeoff .

you can use ` nameframe = eval ( " output_ " +str ( num ))` in the loop , but I agree with @USER you should be storing these as a list upon creation , or if you want to keep names use a dictionary .
Your second line can be simplified to ` df_length = sum ( len ( x ) for x in dfs )`

You could apply ` f ` first , and pass the return value to ` groupby ` : #CODE

Pandas SettingWithCopyWarning When Using loc
I'm not 100% sure if this is a bug but running your code I see that ` df1 ` is modified but ` df ` is not which is not what you are intending . You've assigned ` df1 ` to be a reference to a slice of your ` df ` but now performing the ` loc ` assignment has only modified ` df1 ` , it smells like a bug to me but I am not 100% sure if this is intended or not

Transpose your data , and add a True / False column for category : #CODE

vlookup in Pandas using join
I want to perform a merge , or join opertation using Pandas ( or whichever Python operator is best ) to produce the below data frame . #CODE
Perform a ` left ` merge , this will use ` sku ` column as the column to join on : #CODE
Another method is to use ` map ` , if you set ` sku ` as the index on your second df , so in effect it becomes a Series then the code simplifies to this : #CODE
Thanks . Having exact problem here as well . Could someone comment on what " join " method is for ? A method called ' join ' doing similar but not exactly database join is confusing .
@USER you can think of it as a DB style join , it joins on index by default or on a list of passed in columns , however it is not as flexible as merge which allows the columns on the left and right hand side to have different names , also if there is a clash you have to specify the left and right suffix there is a brief example here : #URL
@USER If " join " is DB style join , then df_Example1.join ( df_Example2 , on = ' sku ' , lsuffix= ' _ProdHier ') should work . But as OP saw , it doesn't
@USER the problem is the index mismatch , although you are joining on the specified column it will try to align on index so none of the join types will work in this case so a merge is a better option here , join should be thought of as simple join's where the indices are aligned so it's not quite a full blown DB style join in that sense
@USER Thanks . I'll stick to merge :)

pandas partial join on multiindex
Essentially I have two dataframes with multi-indices and I want to divide ` dfa.d ` by ` dfb.m ` , joining on ` ( " a " , " b ")` . I can't naively do ` dfa.d / dfb.m ` or ` join ` because it says that ` merging with more than one level overlap on a multi-index is not implemented ` .
if you look for my error message on [ the source code ] ( #URL ) it seems to make sense ( " join on 1 but no more than 1 ") ; I wonder why though .
get the non-matching index level ( s ) out the way by ` unstack ` ing them into columns
` stack ` the columns back to where they were .
` dfb ` has to be a ` Series ` , otherwise there's additional complication about which columns of ` dfb ` to use for the multiplication . You could replace ` dfb.squeeze() ` with ` dfb [ ' m ']` .

Python pandas to mimic excel pivot table
Note that pandas has function ` pivot_table ` for spreadsheet like pivot tables as documented at #URL Though I ( as a non Excel person ) find @USER ' s solution easier to comprehend .

In Python with Pandas how to shift through an Index of unknown frequency using a dictonary
I have been racking my brain for many days on how to solve this . Please let me know if my approach is totally un-Pythonic or there is a way to do this in Pandas . I have been trying to shift or tshift through a DataFrame index by one ' shift ' from a subset of slices in a dictionary ' lix ' . #CODE
The ' index ' above has no pattern or frequency , its simply ever increasing set of dates . The slices of ' so ' DataFrame.index are all in a dictionary . I would like to take lix [ ' A '] slice and shift it by one forward , but per so.index . Then I would take lix [ ' B '] and shift it by one forward etc . For example , I can get the end of the slice and use pd.datetools.BDay ( 1 ) to offset the end by one Business day , but that's not what is in so.index , so how to add one shift such that the result lands on a valid date from so.index ? #CODE
EDIT : slicing of the original so.index is just for illustration , I can't predict how so.index is sliced into a dictionary . re shift : pd.Series.shift ( so [ ' IBM '] [ lix [ ' A ']]) will return a DataFrame with the data shifted by one forward , but the idea is to get the shifted Index . Thanks
Have you tried [ ` shift `] ( #URL )

Sorry , just noticed that with your last column , you'd have to replace ` list ` with a custom function that converts the string representation of the list into an actual python list , the ` list() ` call won't do it so you'll just get `" No_Data " for all rows .

Resample function for Pandas Panel with nominal data
Do you just want to rename the labels in Minor_axis ( a -> Zone 1 , b -> Zone 1 , c -> Zone 2 , etc ) ? What has this to do with the resample ?

pandas apply function to corresponding cells of multiple frames
I'm trying to apply a function to corresponding cells of two identically sized dataFrames to create a new frame .
Did you test if looping is actually too slow ? ( it will depend on the timing of the applied function if the looping will be determinant for speed ) And can the function you want to apply be vectorized ?
If the rows already line up ( ? ) , wouldn't it make sense to merge them first ? The code would be more readable and it looks like you are creating a third dataframe anyway . From the data description it's not clear to me why you would have 2 dataframes rather than 1 in the first place .
Even if the rows don't exactly line up , you can merge the two datasets based on a certain column that should match to compare them ( eg ` pd.merge `)

It looks like there was a commit ( #URL ) in which ` _return_false ` was moved outside a class into the module scope . Do you see that on both CentOS and your PC ? It may be that the v0.14.1 for different distros was cut off slightly different git versions depending on how you installed pandas .
That commit was from 2012 , 2014 was only a couple of months ago , so even if two 0.14.1 versions were cut from different commits ( i.e. one not from the tag ) they'll both definitely still have that . +1 on detective work anyway .

_ " What fast methods could I use to load / save the data from disk ? " _ I don't know if this can apply to your use case , but have you investigated [ PyTables ] ( #URL ) ? Its blazing fast at loading data and interfaces nicely with ` numpy ` . Don't know about ` Panda ` though .
Then the only way to make the parsing faster is to write an application-specific parser in C ( or other compiled language ) . Generic parsing of CSV files is not straightforward , but if the exact structure of the file is known there may be shortcuts . In any case parsing text files is slow , so if you ever can translate it into something more palatable ( HDF5 , NumPy array ) , loading will be only limited by the I / O performance .

You could replace ` df [ df [ ' three '] == i ]` with a ` groupby ` on column three . And perhaps replace ` [ ' two '] [ j + 1 ] - [ ' two '] [ j ]` with ` df [ ' two '] .shift ( -1 ) - df [ ' two ']` .
I misunderstood apparently , can you fix your example so that it works and complies with your text . The loop in total now runs six times ( outer twice and inner 3x ) , and you append once outside the loop . How is that going to give a list with length 4 ?
If all you want to do is to get the difference between the rows of column two you use the shift method . #CODE

You're looking for a groupby with an apply . #CODE
Hi zerovector , I am getting an error when I try to apply that idea : def func ( x ):
return pd.Series ( { ' Obs ' : len ( x ) , ' Sum ' : sum ( x ) , ' Zeros ' : len ( x [ x.price == 0 ]) } ) -- the error says : TypeError : unsupported operand type ( s ) for + : ' int ' and ' str '
this is a simple question that is escaping me -- for the last part : len ( x [ x.price == 0 ]) } ) , the column is actually called price . -- however , when i type in len ( x [ x.price . == 0 ]) } ) , python does not like the syntax -- do you know of a simple way to get tell python not to read the period ? Sorry I know this is off topic !
Thank you zerovector -- your help put me on the right path to solve the problem ! An apply function was what I was looking for !

` KeyError : ' MultiIndex Slicing requires the index to be fully lexsorted tuple len ( 2 ) , lexsort depth ( 0 )'`
Thank you @USER . Yes sortlevel was the problem ! Quick question though : How would you do to drop the 1.5 level ( desired output without the ' Pump ' level ) ?

Python Pandas Multiprocessing Apply
I am wondering if there is a way to do a pandas dataframe apply function in parallel . I have looked around and haven't found anything . At least in theory I think it should be fairly simple to implement but haven't seen anything . This is practically the textbook definition of parallel after all .. Has anyone else tried this or know of a way ? If no one has any ideas I think I might just try writing it myself .
That is a good idea . I was planning on doing something much more dynamic but much more complicated . I think your way is much better and simpler though . I will give it a try with the apply function and report back .
Instead of populating res , which waits for all of the workers to finish all available tasks , is there a way to get the output from worker 1 and append it into a dataframe , than say the output from worker 3 ( order does not matter ) and append that and so on ?
I edited my question above to show what I have tried . It doesn't seem to work with imap or map ..

You can use ` stack ` here , which will produce a Series with the row and column information in the index , and then call ` nlargest ` on that : #CODE

Python Pandas Append List of Dataframes
Is there an elegant way to append all of them ? A one liner would be even better .
Did you look at [ ` concat `] ( #URL ) ?
Following parallel processing example works in IPython by using concat method : #CODE

You can add subtotals to the pivot tables directly but it requires a bit of munging .
You have to make sure that one of fields to be subtotaled starts in the rows , and the other starts in the columns . You then stack the resultant pivot table .
Pandas Pivot tables row subtotals

I have a dataframe that is created from a pivot table , and looks similar to this : #CODE
If you want them to add to the original dataframe , as in your example above , you can add a level in the columns and concat : #CODE

( Note that there is no .fit function for OLS in Pandas ) Could somebody shed some light on how I might get future predictions from my OLS model in either pandas or statsmodel-I realize I must not be using .predict properly and I've read the multiple other problems people have had but they do not seem to apply to my case .

The word " coordinates " reminds me of ` pivot ` , since
pivot is the tool to use .
But ` df ` does not have a third column of values . The values are in ` df1 ` . In fact ` df1 ` looks like the result of a ` pivot ` operation . So instead of pivoting ` df ` , we want to unpivot ` df1 ` .
So I tried melting ` df1 ` . Comparison with other uses of ` pd.melt ` led me to conclude ` df1 ` needed the index as a column . That's the reason for defining ` df2 ` . So we melt ` df2 ` .

I have this code reading a text file with headers . ANd append another file with the same headers to it . As the main file is very huge , I only want to read in part of it and get the column headers .
I will get this error if the only line there is the header . And I do not have an idea of how many rows the file has . What I would like to achieve is to read in the file and get the column header of the file . Because I want to append another file to it , I am trying to ensure that the columns are correct . #CODE
Examine the stack trace : #CODE

I have used ` merge ` in this fashion before , so i was very surprised when the ` filtered ` dataframe returned as empty . I have manually checked that there are common genes in the two different files , so that should be no problem . I'm wondering if the reason it's not working is because I'm doing something weird in the ` gen_genes ` function , i.e. working with strings .

Improve code to replace values above the median by the median value itself in a pandas DataFrame
I'd like to replace all the values above the median value of a column by the median value of the column itself .
You can use numpy where and apply to do it for all columns in a DataFrame : #CODE

Among the simpler built-in smoothing functions , ` pd.rolling_max() ` provides a reasonably good estimate . It however overestimates a little . I have also experimented with writing my own smoothing function , which carries forwards values when there is a > 20% drop . This provides a reasonably good estimate too , but the threshold is set arbitrarily . #CODE
Outliers in our case are low measurements caused by measurement logging failure . There are a number of techniques to detect outliers , the popular of which are named in NIST's Engineering Statistics Handbook . Given the clear trend in my data , I opted for a variation on " Median Absolute Deviation " : to compare each point in the measurement series with the rolling median , generate differences , and select a cutoff point appropriately . #CODE

I start by chunking the CSV file , and apply a ` groupby ` ( on the two last figures of the ` user_id `) on the chunks files so I can store a total of 100 files containing groups of users , and storing them in a HDF5 store .

From what app is what I meant as you are going to copy into the clip board the data from MS SQL , a text editor , python console etc .. from what app did you do this , also can you post data and steps to reproduce

@USER - Thanks . I just used the .values attribute and it worked like a charm . I'm obviously new so don't think I'm formatting everything properly within the stack overflow site .

I've tried to replace all of the " inf " with NaN and then use the Pandas function to remove them but that didn't seem to work ( may have done it wrong ) . I've read the .filter function may be what I'm looking for but still not sure .

i want to convert these strings to unique enumerated integers columnwise . i see that pandas.factorize() is the way to go , but it only works on one column . how do i factorize the dataframe in one go with one command .
Factorize returns a tuple of ( values , labels ) . You'll just want the values in the DataFrame . #CODE
Then concat that to the numeric data .

I have a dataframe that is created from a pivot table , and looks similar to this : #CODE

Er .. have you looked at [ ` concat `] ( #URL ) ?

then something like map , zip , join combination should get what you want ( ? ): #CODE

efficiently join two labels of a dataframe
I want to join ( as in sum up ) two labels , while I relace the new label .
We construct a dataframe to append to your existing one . We set the new index to ' c and d ' , then sum those rows where the labels are in ' c ' and ' d ' , then finally drop those .

The difference lies in the fact that ` grouped.median() ` calls an optimized ( cythonized ) ` median ` aggregation function , while ` grouped.quantile() ` calls a generic wrapper to apply the function on the groups .
So ` grouped.quantile() ` does a general apply and not an aggregation . The reason for this is that ` quantile ` can also return a DataFrame ( and thus is not always a pure aggregation ) , if you calculate multiple quantiles at once , eg with ` grouped.quantile ([ 0.1 , 0.5 , 0.9 ])` : #CODE

So that I keep only rows for which the time index is in the intersection of the time indices of the old series . How can that be Done with Pandas.Series ? and with R zoo ?
In pandas , there is an ` align ` operation , and would work like this : #CODE
Thanks , works perfectly . Can you advise for a tutorial for the other join possibilities ( outer .. ) ?
does not guarantee that the resulting 3 time series have the same time indices . Do I have to do all the possible alignments ? In R , the function merge seems to accept any number of series .
@USER you could nest or chain . I prefer chaining the ` join ` method : ` a.join ( b , [ options ]) .join ( c , [ different options ]) .join ( ... )`
You could also use ` concat ` , like ` df = pd.concat ([ a , b , c ] , join= ' inner ' , axis=1 )` , and then split the DataFrame back into Series
In R with zoo it would be : ` merge ( a , b , all = FALSE , retclass = NULL )` . Note that ( 1 ) ` all=FALSE ` returns the intersection only and ( 2 ) ` retclass=NULL ` returns the merged series by writing the outputs back to the arguments .
The ` merge.zoo ` default is ` all = TRUE ` and ` retclass = NULL ` * is * documented . Look at the help file for ` merge.zoo ` , not the one for ` merge ` .

@USER I get ` Type Error : ' bool ' object is not callable ` when I do that

In terms of how you make a new column based on other columns - if you absolutely need to iterate , then you can assign using ` loc ` , as you did in one example . But you should always look for a vectorized solution , then look at ` apply ` , and only then think about iterating . See this answer for some more background .

I'm not sure I would post this as an answer as it's a little too f*cked up even for me : ` df.loc [ df.Location_Id.isin ( df.groupby ( ' Location_Id ') [ ' Item_Id '] .unique() .apply ( lambda x : len ( x ) > 1 ) .replace ( False , NaN ) .dropna() .index ) , ' average_item_price '] = df [ ' averageprice '] / df [ ' averageprice '] .shift() `
To Ed : df.loc [ df.Location_Id.isin ( df.groupby ( ' Location_Id ') [ ' Item_Id '] .unique() .apply ( ?? lambda x : len ( x ) > 1 ) .replace ( False , NaN ) .dropna() .index ) , ' average_item_price '] = df [ ' averageprice '] / df [ ' averageprice '] .shift()
This give yous a boolean mask for groupby of each store for each date where there are exactly two unique ` Item_Id ` present . From this you can now apply the function that concatenates your prices : #CODE
I think this could be join on the two subFrames ( but perhaps there is a cleaner pivoty way ): #CODE

The ` update ` method unlike a normal ` dict.update ` adds to the values , it does not replace the values

If you return a Series of the ( split ) location , you can merge ( ` join ` to merge on index ) the resulting DF directly with your value column . #CODE
That is nicely done ! If I'm not mistaken , it only works if ` df ` has index that is ` range ( len ( df ))` , right ?
@USER Not quite . ` join ` is shorthand for merging on index with both frames , so the indices need only be consistent ( which it will be here as the apply and col selection don't affect it ) . I'll edit the answer .

b3 = b2.reset_index ( drop = True )

Anyway , you can either map / broadcast the Pandas ` Timestamp.strftime ` function over it , or use that ` to_pydatetime ` function that you found and then map the Python ` datetime.strftime ` function over the resulting array .
That's easy . Given a ` YYYYMMDD ` string , to convert that to an integer , you just call ` int ` on it or , if it's in a Pandas / Numpy array , you map or broadcast ` int ` over it , or , even more simply , just cast its dtype .

Here's a bit of a hacky way to do it . First , align ` df2 ` with ` df1 ` , which creates a frame indexed with the union of ` df1 ` / ` df2 ` , filled with df2's values . Then assign back df1's values . #CODE

You could use lambda function and ` apply ` .

In this case , I made the new index level by taking the first character of the original columns , but of course you can apply another function here if wanted .

You can create a dict and call ` map ` : #CODE
As user @USER has pointed out using ` map ` or ` apply ` should be a last resort if a vectorised solution can be applied .
What you wrote is incorrect , you are calling apply on the df but the column as a label does not exist , see below : #CODE
in general I wouldn't show a map / apply based soln if the vectorized one works ( it's confusing and much slower )

Multi column join in pandas from groupby
I have a large dataframe and a small dataframe that I would like to join together . The small dataframe holds the maximum weekly value corresponding to the range of values in the larger dataframe by group . I want to join the maximum value per week per group to the large frame .
This is what I want the final frame to look like after the join : #CODE
I might not have understood you question clearly . It appears that after joining you are left with the smaller frame . You can avoid that by using ` how= ' left '` or ` how= ' right '` as a join parameter . By default , join uses ` how= ' inner '` , which leaves you with only the intersection of the merged values .

If you groupby with as_index=False you can merge with tf : #CODE

You can pass density parameter to hist , like this #CODE
How can I make it recognize NUMPY hist when I have both pandas and Numpy loaded ?
The ' density ' option works in numpy's histogram function but not on pandas's hist function .
My data frame , I looked through other stack questions and noticed other people got this , too . No good explanations about what " normed " is doing .

I believe the functionality you're looking for is in the hist method of a Series object which wraps the hist() function in matplotlib

So after the history lesson : it is very likely that you will not be able to have your cake an eat it . ` rolling_apply ` is very convenient , but it is almost always going to sacrifice performance against a specific algorithm . In my experience , one of the more enjoyable parts of using the Python scientific stack is coming up with efficient ways of doing computations , using the fast primitives provided in creative ways . Your own solution calling ` rolling_max ` twice is a good example of this . So relax and enjoy the ride , knowing that you will always have ` rolling_apply ` to fall back on if you , or the good people of SO , cannot come with a smarter solution .

If you want instead a rolling maximum , you can use ( appropriately enough ) ` rolling_max ` . You can either resample yourself or get ` rolling_max ` to do it , something like #CODE
I'm not sure I follow . Are you trying ( 1 ) to shift the definition of a week by a few days , or ( 2 ) do you want a rolling window , so that each day has a different week we're taking the maximum over ?
A moving window , each day with a different week , using that day as a median .

Replace WhiteSpace with a 0 in Pandas ( Python 3 )
simple question here -- how do I replace all of the whitespaces in a column with a zero ?

I've managed to do this with apply , like this : #CODE
Depending on the size of frame and number of matches it may be more efficient to use join operations :
Then , join them on `' PersonID '` : #CODE
and , align with the original series : #CODE
The first join is actually a Cartesian product , right ? I'll try to time this on real data . Thanks .

Is there a more efficient way to compare every column in every row in one DF to every column in every row of another DF ? This feels sloppy to me , but my loop / apply attempts have been much slower . #CODE
I forgot to mention , my actual DF's have millions of rows and dozens of columns to compare . With that size , the apply attempts were taking hours .
The ` -1 ` tells NumPy to replace ` -1 ` with whatever positive integer is needed for the reshape to make sense . Since ` arr ` has 1000*100*4 values , the ` -1 ` is replaced with ` 1000*100 ` . Using ` -1 ` is nicer than writing ` 1000*100 ` however since it allows the code to work even if we change the number of rows in ` df1 ` and ` df2 ` .

` ts = pd.Series ( df , index = pd.bdate_range ( start = ' 2013 / 01 / 01 00:00 : 00 ' , end = ' 2013 / 01 / 31 23:59 : 59 ' , freq = ' S '))`

I would go for apply : #CODE

I need to shift a column of uneven timestamps by a column of uneven timedeltas . I tried to add the two columns but get an TypeError .

I can use merge but there must be a cleaner way , without having to drop column Z .
You can grab the columns you want in the merge syntax #CODE

The way I did it was to create a second data frame representing the groups , merge the two data frames , group by the row group names , and sum , #CODE
or replace ` df.groupby ( ' x ' , axis = 0 ) .sum() ` with ` df.set_index ( ' x ')` : #CODE

You can use strptime and a format to get the date string to insert into the yahoo web.get : #CODE

I know that with dummy variables I can get the value of the columns and transform as the name of the column , but is there a way to merge them ( combination ) easily , as R does ?
or the ` pivot_table ` ( with ` len ` as the aggregating function , but here you have to ` fillna ` the NaNs with zeros manually ): #CODE
Most functions in pandas will return a multi-level ( hierarchical ) index , in this case for the columns . If you want to ' melt ' this into one level like in R you can do : #CODE
thats almost that . Is there a way to merge the convert_me and age_col ? How can I create a single table , without the levels .

Is there an easy way to obtain an output like this without manually create all the pivot parts ? #CODE
so the selection can be expressed as a left join : #CODE
Thanks , this is a great sample . but in this way i must manually create the pivot parts , are there no way to use pivot or pivot_table directly ?

If I remove the min_periods=0 from df [ ' freq_average '] = pd.rolling_mean ( df [ ' freq '] , 5 , min_periods=0 , center=True ) I get a result similar to what you are referring to . Only for a window of 5 , the first 5 results and the last five results turn up as NaN . With that said , I am going to tinker with your answer and see if I can adapt it to handle an N sized window . I'll post back with findings !

` from the matplotlib docs ` Return value is a datetime instance in timezone tz ( default to rcparams TZ value ) .

Based on Andy's explanation of ` read_sql ` vs . ` read_csv ` difference in implementation and performance , the next thing I tried was to append the records into a CSV and then read them all into a DataFrame : #CODE
@USER definitely get / allocate more ram , 2gb is not very much imo - it's almost certainly swapping here ! You could use pytables / HDF5 to do the concat on disk ... see #URL but that may not be enough .

Resample time series excluding nan data
I have a daily data time series in which there are many NaN values . I want to resample to monthly data taking account only months with less than 10 day NaN values .
I've tried using the resample function , by this way :
Note : count by using ` isnull ` and ` sum ` . #CODE
Then do the resample ( this is what ` groupby ( g ) .mean() ` does ) #CODE

Your SQL is the equivalent of an inner join , so how about this ? #CODE

Now I want to replace one of dataframe ` NaN ` s , I used ` ffill() ` , it worked but I need to limit filling ` NaN ` , I don't need what I marked red .

You can then use apply to concatenate : #CODE
` drop_duplicates() ' is the right function . Coming from sql I immediately thought to GROUP BY . If the fierst column is intger type you suggest to cast to string and the to use apply ?

Question How do I add the score of the permutated words and stack it

` pd.expanding_apply ` applies the rollingsum function backwards to each row , calling it ` len ( dataset )` times . ` np.linspace ` generates a dataset of size ` len ( dataset )` and calculates how many times each row is multiplied by ` exp ( - 0.05 )` for the current row .

You can groupby the Label column , apply the list constructor . Here is an minimal example . #CODE

I originally wrote this in a SQL query which accomplished the task quite simply with a join : #CODE
First merge ` df ` and ` df2 ` on the ` bin ` column , and then select the rows where ` cut_min = perc cut_max ` : #CODE
Follow-up question : What if I wanted to do this like a left outer join ? For example , if I had an additional row with perc=1 , I would like it to report null for the result instead of dropping the row for not fulfilling the criteria . Thanks again !
Thanks ! I ended up using the first alternative method ( altering df2 to cover all possible values of bin and perc so that I could use an inner merge without losing rows ) .

if you want to convert it to strings , you can apply ` strfitme ` ( ` df [ ' timestamp '] .apply ( lambda x : x.strftime ( ' %Y-%m-%d '))`) . Or if it is to write it as strings to csv , use the ` date_format ` keyword in ` to_csv `

Pandas : Read Timestamp from CSV in GMT then resample
To convert a Timestamp / datetime64 column use tz_convert ( if the are tz naive , i.e. don't have a timezone yet , you'll need to tz_localize first ): #CODE

How to replace efficiently values on a pandas DataFrame ?
I just compared ` update ` against ` map ` and was a little surprised that ` map ` outpferformed ` update ` , for 90k dataframe ` update ` takes 19.4ms vs 8.61ms for ` map ` .
The reason I say this is that Jeff has always commented to me that ` map ` and ` apply ` are last resort methods so I thought ` update ` would perform better
@USER : I think ` map ` with a function argument can be slow . ` map ` with a series argument should be pretty fast . But I'm not up on pandas internals so I don't know for sure .
I think I was told as a comment that map is a cython optimised for loop , not sure if that changes if the datatype is a series . Anyway this was the comment from [ Jeff ] ( #URL )

Looping through df dictionary in order to merge df's in Pandas
I would like to merge them ' inner ' by their indexes but with iteration using a for loop . It would have to be the equivalent of doing #CODE
( Caveat : Using ` pd.concat ` is fragile here -- I'm assuming the DataFrames do not have NaN values , but may have different indexes . The ` dropna ` is then used to produce an inner join . )
the df have different indexes . The concat option is a nice alternative . Thanks
Or to drop a level , ` df.columns = df.columns.droplevel ( 0 )` .
ah yes , is " keys " in concat !
concat has you covered : #CODE

How's about using union * to join the two MultiIndex together : #CODE
* Initial answer was to use append , but I think union is more readable .

I have a large dataframe . I want to groupby three columns in the dataframe , and then apply a function to each group . However , I'm also interested in some groups and keys that are NOT in the dataframe . How do I add those to the groupby object , so I may use ` groupby.apply() ` uniformly on all groups ?
how about a merge and then groupby ? can you give an example ?

Note : In 0.15.0 you'll have access to these as Series dt accessor : #CODE
@USER that's bizarre , if I have a DatetimeIndex I just tested that I can ` tz_localize ` . Your error is odd / unhelpful ! ... Do you already have tz info ? I'm thinking perhaps there's something buggy here .
BTW , what do you mean by having ` tz info ` ? Also , I don't think it matters but I am on Python 3 ( latest stable as well ) . I also have the latest numpy , etc .

You don't need to create an actual CSV file ; the ` csv ` module works just fine with a ` StringIO ` , as you're already doing , and I'm pretty sure Pandas does also . Either way , the goal is just to get some iterable of rows and insert each one by looping over the rows and calling ` execute ` .
ok thanks given my limited knowledge of this what is the way through code to do this ? also how does this work with PYODBC to insert the values ?
For the ' write to sql server ' part , you can use the convenient ` to_sql ` method of pandas ( so no need to iterate over the rows and do the insert manually ) . See the docs on interacting with SQL databases with pandas : #URL

The text contains unwanted unicode characters which I normally strip out using #CODE
Can you not just call ` dropna ` or you want to replace the ` NaN ` with some value ?

I'm expecting to get a separate dataset with , say , D E and F . This will again have RequestI D; with ~80% overlap in requestIDs . I might need to perform a JOIN-type operation between the two tables so that I can correlate and analyze data from both datasets . I understand there's no actual JOIN support in Pytables but there's some workaround method . But I haven't found much information about it's efficiency or speed ! Has anyone tried it ? What sort of performance can I expect ?
sounds like you should store the data with pytables , and use pandas to do the join and file I / O .

Pandas : fastest way to check if words in Series A endswith one word of Series B
You can pass ` endswith ` a tuple here ( so you might as well use that instead of a Series ): #CODE

You can then map a dictionary onto those values : #CODE

Well , I want to grab all of the data frames that end in ' cd ' , for example , and append ( union all ) them into a final data frame . I don't want to have to explicitly call all 50 of them . I want to loop through a list of the data frames to accomplish this task .
I think a much better way is to create your own dictionary rather than resort to globals ! Just create your own and append to some list or dictionary of lists ? ( depending on the classification ): #CODE

When accessing one value of the column above , this gives back a ` Timedelta ` , and on this you don't need to use the ` dt ` accessor , but you can access directly the components : #CODE
The reason that ` df [ ' column_with_times '] .apply ( lambda x : x.days )` does not work is that apply is given the ` timedelta64 ` values ( and not the ` Timedelta ` pandas type ) , and these don't have such attributes .

Yes , exactly BUT my len ( data1 ) and len ( data2 ) is not the same .

As of Pandas 0.14 , ` rolling_apply ` only passes NumPy arrays to the function . A possible workaround is to pass ` np.arange ( len ( A ))` as the first argument to ` rolling_apply ` , so that the ` tau ` function receives the index of the rows you wish to use . Then within the ` tau ` function , #CODE
The number of combinations grows like ` n**2 ` , so ` tau ` gets called on the order of ` n**2 * m ` times where ` m = len ( A )` . So this could take a while , especially if you have lots of columns . [ Using ` itertools `] ( #URL ) is really quite fun ; learning it is not hard and well worth the time .

Thanks for the answer , and sorry about not including the cvs file , I cut the code that I used to make mine out of the post and forgot about it . I'll remember it for next time ! Anyway , for whatever reason your code doesn't work for me , it keeps spitting back errors . I'm guessing this is because I'm using python 3 . On the brightside , I figured it out myself after toying around with the parameters of the plot function I found adding x= ' Names and y= ' Births ' fixes my problem . Thanks anyway for the effort , I really appreciate it .

Is 64 still magic if you drop one of the columns ? My hypothesis is that you could be running into some memory boundary and [ thrashing ] ( #URL ( computer_science )) or similar

So that's how map works . Looks promising I'll have to test later on my desktop .

the above gave me error msg like comparsion by diff types .

Regarding using hierarchy , I'm aware that the structure should be highly dependent on how I'll be using the data . However , if we assume that the I define a table per date ( e.g. ' df / date_20140714 ' , ' df / date_20140715 ' , ... ) . Again I may be mistaken here , but using my example of querying date / time range ; I'll probably incur a performance penalty as I'll need to read each table and have to merge them if I want a consolidated output right ?
Recombing data is relatively cheap ( via ` concat `) , so don't be afraid to sub-query ( though doing this too much can drag perf as well ) .

Can one use comparisons to merge two pandas data-frames ?
Since the values you're joining on are no longer unique , you may not have the merge working as you expect . Perhaps look into .join , or .concat if you're looking to simply add the two tables together
Possible duplicate of #URL There is a proposed issue regarding Conditional Join for Pandas DataFrame ( #URL )
Wondering if a non-SQL solution would be easier ( ie : parse + merge in python ) .

I don't understand what operation you want to perform on the rows , but the ` shift ` method would suit you . ( There is both ` Series.shift ` and ` DataFrame.shift ` . ) #CODE

This is a great solution . I like the map with dictionary approach . Thanks Data Swede and Ajean !

I would assume to_excel() would only try to apply the parameter to float-formatted columns ( or even specific cells ) rather than to every piece of data , so I'm not sure what I'm missing . If need be I'll post a cleaned version of the specific table that reproduces the error , but I thought perhaps someone would recognize what I'm facing .

I think you are looking for a rolling apply ( rolling mean in this case ) ? See the docs : #URL . But then applied for each weekday seperately , this can be achieved by combining ` rolling_mean ` with grouping on the weekday with ` groupby ` .

Python pandas : replace select values in groupby object

It would help if you gave some idea of what the actual Series and DataFrames look like . From the error in your attempt to ` pd.concat ` I suspect that the index for one or more of your Series / DataFrame doesn't match the others . The index matters ! Also , ` append ` will naturally append * rows * not columns , fyi .
The index matters because pandas is database-savvy ... ` concat ` will only put things together that have the same index ( i.e. if one of your Series has row indices of [ 0 , 1 , 2 ] and one of them has indices of [ 2 , 3 , 4 ] , it will give you a DataFrame with indices [ 0 , 1 , 2 , 3 , 4 ] , with NaNs in the places where there is no overlap )

Append Py Pandas Dataframes into a single .csv , iterating by separate list
I'm attempting to use pandas to join content from three separate flat files into a single .csv . One of the output fields , ' StoreID ' , is based on a separate list of ID values ( ' Stores.txt ') . In essence , I need to publish a merged dataframe as a series of csv rows , while at the same time appending results for each store ID in a third column . I'm having trouble with iteration syntax . Any help would be most appreciated ! #CODE
it sounds like what you are trying to do is duplicate the combined dfs of itemID and storeLocation with each StoreID , correct ? it'd be better to concat them but set the ' StoreID ' prior to the concatenation , at the moment you are overwriting each merge with the last set operation so all your rows will have the same ' storeID ' . I'd just create a temp df or the intial merge , set the ' storeID ' column and then either merge or concat to your item_merged ' df .
My approach would be to merge outside the loop , make a temp df that is a copy , set the storeId for this df and concat to your merged df and then write out to a csv : #CODE
Thanks , Ed . In reality , I'm looking at n storeID values ( in the 100s ) . The intent is to loop & concat all itemID to StoreLocation pairs for every 1 storeID

I have read in a csv into a pandas dataframe and it has five columns . Certain rows have duplicate values only in the second column , i want to remove these rows from the dataframe but neither drop nor drop_duplicates is working .
drop and duplicates create new datafraames . So you want something like : ` df = df.drop_duplicates ( ' b ')`
By default drop and in fact most pandas operations return a copy , for some and in fact these functions then can be passed the param ` in_place=true ` to perform the operation on the original df and not return a copy
As mentioned in the comments , ` drop ` and ` drop_duplicates ` creates a new DataFrame , unless provided with an inplace argument . All these options would work : #CODE

EDIT : I will leave my thoughts about NaT's up above in the event that they are a factor , but upon further review it seems that my problem actually lies in the fact that these columns are timedelta64s . Does anyone know of any workarounds to obtain mean / median on timedeltas ?

Cleaner pandas apply with function that cannot use pandas.Series and non-unique index
you can tell ` groupby ` to drop the keys using the keyword parameter ` group_keys=False ` : #CODE

Replace all occurrences of a string in a pandas dataframe ( Python )
It is possible to replace all occurrences of a string ( here a newline ) by manually writing all column names : #CODE
You can use ` replace ` and pass the strings to find / replace as dictionary keys / items : #CODE

The use case : I want to apply a function to each row via a parallel map in IPython . It doesn't matter which rows go to which back-end engine , as the function calculates a result based on one row at a time . ( Conceptually at least ; in reality it's vectorized . )
This was what I had in mind ! Well technically " df.groupby ( np.arange ( len ( df )) // ( len ( df ) / 10 ))" to get a fixed number of groups ( 1 per core ) instead of fixed size . For some reason it hadn't occurred to me that the grouping key need not actually be related to the index at all ...

I understand that I can merge them together and get a score for each word : #CODE
i ) Automatically assign a score of zero to words that do not have a score . So , " play " is in df2 but is removed after the merge but I would like to keep it in the result after the merge . I expect df2 to contain many , many words that do not have a score so it is not possible to simply add these words to df1 and assign them as zero . So , I would want the merge to give this instead : #CODE
For ( i ) you just need to specify ` right ` join , and fill null values : #CODE

Use [ shift ] ( #URL )

More generally , I understand that the approach with ` FacetGrid ` is to make the grid and then ` map ` a plotting function to it . If I wanted to roll my own plotting function , what are the conventions it needs to follow ? In particular , how can I write my own plotting function ( to pass to ` map ` for ` FacetGrid `) that accepts multiple columns worth of data from my dataset ?
There may be cases where your function draws a plot that looks correct without taking ` x ` , ` y ` , positional inputs . I think that's basically what's going on here with the way you're using ` plt.plot ` . It can be easier then to just call , e.g. , ` g.set_axis_labels ( " Date " , " Stat ")` after you use ` map ` , which will rename your axes properly . You may also want to do ` g.set ( xticklabels =d ates )` to get more meaningful ticks .

I need to cast mydf to DataFrame , concat , then cast back #CODE
I don't think concat will ever return a MyDF though , so best to hope for will be ` MyDF ( pd.concat ( mdfs ))` .

I don't think this is the appropriate place to ask as this seems to be about the quandl API and not pandas or python ... But out of curiosity about quandl I took a glance at their page and wonder if you just need to replace GBPUSD with QUANDL / GBPUSD .

If you want to duplicate that data in every row of the original DataFrame for the corresponding subject , use ` map ` to grab the pattern for each subject I D: #CODE

Now what I want to do is replace the 0 values with NaN values when all row values are zero . Critically I want to maintain whatever other values are in the row in the cases where all row values are not zero .
But , if all you're trying to do is replace rows of all zeros for specific set of columns with ` NA ` , you could do this instead #CODE
To answer your original question , ` np.where ` follows the same broadcasting rules as other array operations so you would replace ` ??? ` with ` DF [ col ]` , changing your example to : #CODE

You're using enumerate - i.e. your ` indx ` variable would go from 0 to the number of rows in sanitizedData . However , the ` removeConsecutives ` series is not indexed by consecutive numbers . Perhaps it used to - but not after you used ` drop ` .
Also the code i am using is minimal , i actually am doing something to the data before summing it but i ruled out what i was doing to it as a cause of the problem . So i cant really use the sum function , nor can i use drop duplicates as i need the duplicates to be consecutive .

Pandas - Create a new column with apply for float indexed dataframe
No worries . As a general rule avoid using ` apply ` if there is a vectorised operation , for basic operations such as add / subtract / div and multiply there are built in operator support for these that are many orders of magnitude faster .

Pandas : how do I merge minute data rows in a timeseries
You can use the ` stack ` method for this : #CODE
This creates a Series with a multi-index ( index with two levels ) . To merge both levels into one index , you can do ( for each index entry , I just concatenate both levels with a space in between ): #CODE

Oh - perhaps this is the best solution . One can " shift " the data by a certain amount : #CODE
One way would be to use ` shift ` to move the relevant column down ` n ` rows and then concatenate the entries ( they are strings so we can use ` + `) : #CODE
thanks - i like shift . and yes , i agree it is better not to have lists . ( Though I'm realizing this only now )

Returning multiple columns with pandas apply and user-defined functions
And I want to use ` df [ ' x '] .apply ( lambda x : fn ( x ))` to return both ` y ` and ` z ` in separate columns . Is there a good way to do this by still using ` fn ( x )` ? In reality , my function will be much more complicated - so I only want to run it once within the apply and assign ` output [ 0 ]` , ` output [ 1 ]` , etc to individual columns .
How about this method ? ( n.b. , I edited this answer in light of the comment below ) so the apply step could take a single function with shared calculations and return the required series for the merge step . #CODE

Thank you . Given that the json format I have is nested , it was not parsed correctly by read_json() , giving me a TypeError . However , I used your form with json_normalize() : ` df = concat (( json_normalize ( d ) for d in data ) , axis=0 )`

From the docs , the dict has to map from labels to group names , so this will work if you put `' A '` into the index : #CODE

` apply ( Series )` gives me a DataFrame with two columns . To join them into one while keeping the original index , I use ` unstack ` . ` reset_index ` removes the first level of the index , which basically holds the index of the value in the original list which was in C . Then I join it back into the df .

Python bcolz how to merge two ctables
The bcolz come with iterators , so in doing something that can't be simply expressed for evaluate , you'll need to iterate the two inputs . Were you wanting the full range of behaviours of merge , or something more specific ?
FYI toolz.join is only streaming in the right argument . I would probably pull off numpy chunks from the ctable as recarrays , merge them in memory , and then append them onto a target table .

Python summarize row into column ( Pandas pivot table )
Here's the nicest way to do it , using ` unstack ` . #CODE

there is precisely one entry for that loc ,

This is the end of the stack trace : #CODE

` iloc ` is for integers only , whereas ` ix ` will work for both integers and labels , and is available in older versions of Pandas . #CODE

Anyway , you can do this using apply : #CODE
The function in the apply works by first seeing if the ID is in the dictionary ( and tuples won't be ) and if it is , go for that , and if it isn't find the first one that is ...

I need to slice several intervals out of one dataframe indexed with Freq : 120T . The start date of each of the desired intervals is given by a second dataframe indexed with Freq : None . The idea is that I need to take each of those start dates and include a time and # of periods to build each interval . The time attribute and the periods are the same for all intervals .
@USER Hayden Notice how df3 has 2 equal length intervals of df1 data , each of them start at the dates in the df2.index . Is like df1 has the granular data ( higher freq ) and df2 gives you the date .
You're looking for pivot ... at least once you've extracted the rows you're interested in .
Those that are on the same date , use ` normalize ` and ` isin ` : #CODE
Once it's in that form pivot away ( if there is likely to be missing data you may have to use ` pivot_table ` which is a bit more flexible ) ! #CODE
You can map the columns of the pivoted result : #CODE
@USER it's actually not intraday , you can do between time 18:00 and 02:00 . Not quite sure I follow where you're going with this . I * think * you mean you want to pick the next ( ? ) time after date and time dt ( for several dates with the same start time ) and then pick the subsequent N records ( inclusive ) ? You can do that with .asof if that's what you mean .
We can use this in a concat for each day in the index : #CODE
Note : I couldn't get this to work cleanly with asof itself ... which may be more efficient .
It works , thanks for your effort . It was a messy question . Now I just need to take the df and pivot it .

If I unstack once the result is : #CODE

Python pandas - particular merge / replacement
This takes a couple steps , left ` merge ` on the columns that match , this will create ' x ' and ' y ' where there are clashes : #CODE

So , I want to replace nan's in Y column that correspond to observations in X with " * NY " part , to numbers in Y that correspond to observations in X that have the same numeric part but without " * NY "
This was a bit more annoying to code , basically we can apply a custom function that performs the lookup for you : #CODE

OK but generally using ` apply ` should be a last resort , there are vectorised functions in pandas and numpy that will perform math operations on the whole df , please check the numbers I can't guarantee anything
Compared to the apply function which took 4.3s so nearly 250 times quicker , something to note in the future

There is a ` pandas.pivot_table ` function and if you define ` datadate ` and ` id ` as indices , you can do ` unstack ` the dataframe .
I think the OP wants it the other way ? pv to df . I think you can do that with melt , but it seems messy !

Inconsistent behavior of ` dataframe.groupby ( allcolumns ) .agg ( len )`
Now , here's the result of grouping on all the columns of ` df0 ` and aggregating with ` len ` as aggregator function : #CODE
Based on this result , I expected that the analogous operation for ` df1 ` , namely ` df1.groupby ([ ' X ']) .agg ( len )` , would give this : #CODE
What's the simplest way to get the output I expected ( as shown above ) from ` df1.groupby ([ ' X ']) .agg ( len )` ?
The simplest way to get what you're looking for in #2 may be a pivot table . Reset the index , then use df1.pivot_table ( index= ' X ' , values= ' index ' , aggfunc=len )

Pandas merge 2 databases based on 2 keys
I am trying to merge 2 pandas df's :
Sorry , I made a mistake . The year data have a value for every year . So for some years both a Calories and GDP data point are available . However it does not merge these as one row .
Even after modifying df1 and df2 to have matching keys , the merge still works as expected for me . It sounds like values which look equal are not actually equal . For example , if ` df1 [ ' Year ']` are integers , but ` df2 [ ' Year ']` are strings , Pandas won't join them as equal . You might want to check ` df1.dtypes ` and ` df2.dtypes ` and see that the dtypes match for the ` Year ` and ` ISO3 ` columns . For string dtypes you may wish to strip whitespace as well : ` df [ ' ISO3 '] = df [ ' ISO3 '] .str .strip() ` .

Update : I needed to transpose the numpy array so that ` rate_df ` was correctly oriented .
@USER ah , you need to transpose the array ( my rate_df was different from yours ) . Fixed in the edit !
Thanks Andy , that's awesome ! Btw , I fixed the data midstream . You probably were working with my original data and that's why the transpose was needed . My bad .

However , if we call ` df.drop ( name , axis=1 )` , we actually drop a column , not a row : #CODE
Use ` axis=0 ` to apply a method down each column , or to the row labels ( the index ) .
Use ` axis=1 ` to apply a method across each row , or to the column labels .

Hi Ed , thanks for the feedback :) So a few things I was curious about : 1 . ) The 3 output is " Man The Man " -- how does this method actually search for the words ? 2 . ) Second , I must say I'm confused where exactly I should insert my data frame variables into this code ? Does this method iterate through a list or do I manually have to put each word in ? Sorry for all the questions and thank you !

Why do you try to replace the NaNs with None ? NaN values will be ignored by the mean by default . So just do your grouping on the original dataframe ` df1.groupby ([ ' My_name ']) .agg ([ np.mean ])` . Is that what you want ? ( and otherwise , show your expected result fo the grouby )
First , I don't think you need to replace the ` NaN ` values with ` None ` , as ` NaN ` is the default indicator for missing values and will be ignored by ` mean ` by default in pandas ( ` mean ` has a ` skipna ` parameter that is True by default ) .

You're pretty close , basically you just want to ( 1 ) use diff instead of shift and ( 2 ) combine with groupby to automatically get NaNs . Btw , it's helpful if you can provide simple code to create the data . Anyway , something like this should work : #CODE

Filter Column in Pandas Pivot - Python
My data frame and pivot table look like this : #CODE

Also is there a way to insert grouped records with the count info into a MySQL database ?

Why is this an issue , there is no way to represent ` NaN ` for ints hence the conversion to floats . You'd have to substitute the ` NaN ` values with something that can be represented as an int like ` 0 ` , or convert to string and replace the ` nan ` string with an empty value and then export

You can call the ` str ` method and apply a slice , this will be much quicker than the other method as this is vectorised ( thanks @USER ): #CODE
Just out of interest how would I go about apply this to the index column ? As I can't seem to get that to work - I can always just reset_index() the column and do it then ..
@USER not easily , the problem is that although ` pd.Series ( df.index ) .str [: 1 ]` itself works , how do you merge or add this back to the df ? This line produces a series with index values beginning with 0 but your index in your main df is ` AAB , BAB .. ` etc . So the easiest thing is to reset index , extract the column and set the index again

In my line of work it is not uncommon to have a continuous vector that needs to be ' discretized ' . What I want to do is replace the values of a continuous variable that has been discretized by ` cut ` with the mean of another variable over those cut ranges .
Furthermore , the mean ( or whatever other function I want to use to generate a value ) must be based on the original data and applied to new data . Imagine the situation where I calculate mean bad rates for a continuous variable on a training data set , build a model and then have to apply that same transformation logic to new data .
yes , thank you for the suggestions . I should clarify ( and will do so in an edit ) that I need to be able to apply whatever transformation I devise to new data . So I have to save the lookup information somehow so I can use it later .

Thanks for your comments . The documentation I linked says that the ` data ` argument may be a " numpy ndarray ( structured or homogeneous ) , dict , or DataFrame " , and follows this line with one that says " *** Dict can contain *** Series , arrays , constants , or list-like objects " ( my emphasis ) . I interpret this to mean that * when the ` data ` argument is a ` dict ` * , its values can be Series , arrays , etc . IOW , the " list-like objects " bit is not referring to the ` data ` argument itself . I am specifically looking for a ` data ` argument that is a Python list , not a dict , so this clause does not apply .
For example , this is why ` OrderedDict ` was created . Why would people expect regular ` dict ` objects to keep track of the order the data was added ? That's too specific for a generic map data structure , but it's fine to have it as a special extra case , and was implemented by subclassing ` dict ` and adding the extra behavior . The " real " solution to your question would be to make an analogous ` OrderedDataFrame ` . Then , by assumption , since the order matters , it can faithfully map backwards to an ordered list of lists used to construct it . And this would be done with code like ` make_df ` under the hood

I'm looking for a method to perform an ANOVA and HSD tests from a dataframe in Python . I tried to read some examples on forums and tutorials but i didn't achieve to apply it to my work .
but i can't achieve to apply them to my example
For pairwise comparison for only some effects , we would need the pairwise comparison after estimating the multiway ANOVA with OLS . This is currently not available in statsmodels . The critical values and p-values of Tukey-HSD would not apply in that case .
What would be possible in this case is to estimate the full model with OLS , define all desired pairwise contrasts , use the ` t_test ` to get the raw p-values for the comparisons , and then apply one of the multiple p-value corrections that are available .

Panda merge return empty dataframe
both dataframes have a value 1 to 16 on the avgSpeedBinID field , however , when i try to merge the data frames together #CODE
i just cast the str one into an int and the merge worked . #CODE

One way you could avoid alignment on column names would be to drop down to the underlying array via ` .values ` : #CODE

Pandas dataframe left merge without reindexing
Wondering if there's a more intuitive way to merge dataframes #CODE
Left merge does not work because it reindexes #CODE
looks a little weird to me . It says let's join two tables on column A and also the index of the right table with nothing on the left table . I wonder why this works .

You could pass this as a freq , but this may / will be inaccurate for times which don't evenly divide : #CODE

In 0.15.0 there'll be a dt accessor : #CODE

Panda's boxplot but not showing the box
Note , ` showbox ` and ` whiskerprops ` are the ` kwds ` of boxplot , which are in turn passed to ` matplotlib.boxplot ` .

This can be done by lagging the columns in pandas with ` shift ` , or trying to do a complicated self-join that is likely very bug prone and creates the problem of perpetuating the particular date convention to every place downstream that uses data from that code .
another option is to resample and take the first entry : #CODE

How can I do this in Pandas ? I first thought I could do this with a ` join ` or ` concat ` operation , but I don't think that would work .

Say you want to start your intervals at ' 00:00 : 00 ' and you want them to be 3 periods long . Also notice how the df2.index is in Freq : None , vs the df1.index is in F #URL

I'm more interested in following convention - making my code more maintainable by the average programmer . For example , you'd always expect a dataframe to have the DatetimeIndex as the index not the columns - doing the transpose forces people to rethink their mental model . I'd prefer to follow the crowd unless I've a good reason to do otherwise . Since I found the bug and that the unittest factory creates a Panel with DatetimeIndex on axis=1 , I thought I'd ask if there's a convention .

@USER not really . Autocomplete works fine with variables , as ( I assume ) Anaconda keeps track of its type and autocompletes the classes / methods that are relevant . The problem with autocompleting chained methods is that you'd have to eval each method , determine the output , find its type , and then autocomplete the appropriate items . With large DFs , numpy arrays , etc . this could take quite a bit of time , especially with complex transformations .

Making no assumption about the indices and if they are aligned or not , you can align the rows first by using ` .reindex_axis ` and then use numpy broadcasting rules over ` .values ` , and finally re-construct the frame : #CODE

Applying a specific function to replace value of column based on criteria from another column in dataframe
The reason it overwrites is because the indexing on the left hand side is defaulting to the entire dataframe , if you apply the mask to the left hand also using ` loc ` then it only affects those rows where the condition is met : #CODE
The use of ` loc ` in the above is because say I used the same boolean mask semantics this may or may not work and will raised an error in the latest pandas versions : #CODE

I don't have " key values " - columns that I used to group the count . I need to insert this values in MySQL using PHP .

Thanks Jeff ! So align is the special sauce ... That is very helpful to know for visualizing what is happening behind the scenes .

You are looking for shared x- and y-axes which can be enabled by passing ` sharex=True ` and ` sharey=True ` as keyword arguments to ` hist ` .
as a side note , it appears there's currently ( Sept 2014 ) a bug in Pandas plotting where hist breaks with by if the number of plots does not fully fill a row . For example : ` pd.Series ( randn ( 1000 )) .hist ( by=randint ( 0 , 5 , 1000 ) , sharex=True , sharey=True )`

hdfstore error on append with pandas
Any ideas on why this would happen ? It's a rather large project , so I'm not sure what code I can offer , but this happens on the first append . Any help would be very much appreciated .
Not sure what to ptdump since I'm doing an append , but above lay the others !

Transpose it , sort it , transpose it back ?

Pandas concat ValueError : Buffer dtype mismatch , expected ' Python object ' but got ' long long '
when i try to concat the train dataframe with the label series based on pandas example it
full stack trace #CODE
I thought if you named the columns and didn't specify join then it would just create a data like [ trainDataColumns , trainLabelColumn ] i tried ignore_index=True but that didn't change anything
Nope , ` concat ` merges on the index . If you just want to merge them , you can reset the index of both ( ` reset_index `) and concatenate then .
Why don't you use append instead ? See the dataframe.append() method documentation here . Note that pandas.read_table() outputs a dataframe , so it looks like your trainLabel object is actually a dataframe too . The command will look something like this :

Right now I construct the list of tuples , before passing it with ix . Reading #URL , I get the feeling there is a more efficient way to do this than passing a list of tuples , but I can't wrap my head around how to approach it . How would one build the multi-index slicers in this case ?

What is ` len ( df1 )` , ` len ( df2 )` and what is the dtype of the ` time_0 ` column ?

python : join group size to member rows in dataframe
( Python 2.7 ) I wish to create a column in a python dataframe with the size of the group to which member rows belong ( indexed by row ID number ) . Groups are based on rows with identical values in two columns , date and amount . I've attempted to use groubpy and size - which is suggested for similar problems - but I can't get the resulting size values back to the source dataframe due to indexing problems . Should I use a dictionary to read all unique value pairings instead , and what would that look like ? Or should I learn how to merge the groupby object to the original dataframe with a join operation . Note : this is large dataset .

How to apply Pandas Groupby with multiple conditions for split and apply multiple calculations ?
Then we would do the same for the other 2 pairs . And organize the 18 calculations on a datframe . I know how to do this manually , but I'm hoping for some help doing it with groupby or perhaps pivot table .
This does exaclty what I want . Thanks . My only question is , what about the task makes it not doable with groupby or pivot ?
If ` cond1 ` were the negation of ` cond2 ` , then groupby could be used to a limited extent : ` A.groupby ( A [ ' A '] > 1 ) [ ' D '] .agg ([ ' mean ' , ' std ' , ' count '])` but here the count is not quite right since it does not further subselect only those rows where ` D ` is positive . Even if we could surmount that challenge , then we'd still have to arrange for the pairing of the columns . The pairing of the columns is not something that can be naturally expressed using groupby or pivot . So all-in-all , it seemed likely to me that we'd do better using a plain old loop .

You then map plotting functions onto that grid , passing any required arguments to the mapped plotting functions . #CODE

If the boolean series is not aligned with the dataframe you want to index it with , you can first explicitely align it with ` align ` : #CODE
Note : the align didn't work with a Series , therefore the ` to_frame ` in the align call , and therefore the ` [ 0 ]` above to get back the series .

The solution in this case is to append the data to a Python list and create the DataFrame in one fell swoop at the end :

934 if text or len ( elem ):
I could possibly live without floats and use strings , but curiously the things in my Dataframe appear to BE strings , since when I try to apply the round() function on any value extracted from there , it will protest that the input is not a float ...
You should replace it with this : #CODE

I'm converting a SQL query in python into a pandas dataframe . Then I'm using pandas sql to make a left outer join between two pandas dataframe .
Turns out my problem was with the text in the dataframes that I was trying to merge . It sufficed to specify the proper encoding when calling read_csv : for example ` encoding= ' latin-1 '`

Pandas : reindex and interpolate non-contiguous data
I want the dataset to insert and fill the missing rows with NaNs
It's a little puzzling your code , ` siteRef.name = ' SITE_REF '` and ther other one does not do what you think , to rename a column call ` rename ` , what've you've done is add an attribute to that dataframe called ` name ` . Also the result of your concat is also strange , it's showing you having double the number of rows for ` 0 ` compared to the rest , I think you need to do some more debugging to see where you are going wrong , look at your code one operation at a time and print the shape and head at each stage until you are confident there are no errors

transpose multiple columns Pandas dataframe
As someone who fancies himself as pretty handy with pandas , the ` pivot_table ` and ` melt ` functions are confusing to me . I prefer to stick with a well-defined and unique index and use the ` stack ` and ` unstack ` methods of the dataframe itself .
@USER ` stack ` -> ` melt ` ; ` unstack ` -> ` pivot ` , but they don't aggregate the data in any way and only operate on the row and column labels ( indices )

I could join the limits and this df and filter with a boolean condition . Any suggestions on either problem ? ( splitting a fraction of rows or adding a level-aware running index )

The Freq of df1 doesn't need to always be ' 240T '
df2 freq attribute is always going to be ' None '
With some help here I tried this approach , but only worked well when both df's freq was ' None ' . #CODE
Notice that this code uses a ` for-loop ` with ` len ( df2.index )` iterations #CODE
So this alternative method trades a ` for-loop ` with ` len ( df2.index )` iterations
The problem is the assumption of freq = ' D ' in df2 . As I pointed out in the considerations , the dates will not be continuous , so df2 freq would be = ' None ' , unless there's a way to change that .
I don't think the code depends on the ` freq ` . I just used that to set up the example .

Then , use the ` pivot ` function to reshape the data . #CODE

I would like to apply dummy-coding contrasting on it so that I get : #CODE

You can use ` isin ` to filter for valid rows , and then use ` replace ` to replace the values : #CODE
If the resulting values are all integers , you change the above ` replace ` line to enforce the correct ` dtype ` : #CODE

You can just use ` concat ` and pass param ` axis=1 ` , to append the arrays as columns : #CODE

The only thing I can think of is to use Python to export to csv files , and then write an Excel macro to merge all the CSVs into a single spreadsheet .
I think that the difference in size stems from excel and the different file formats and not from the writing method . In addition - you don't need an excel macro to merge csvs into a spreadsheet , writing a script with xlwt is fairly straight forward .
@USER , thanks - very useful . However , I understand PyExcelerate doesn't support Pandas , and most likely won't , at least not in the near future ( #URL ) . Maybe I could use it to merge the CSVs into an Excel , but at this point I'm inclined to do that via a VBA macro invoked from Python , also because formatting the sheets will presumably be easier that way .
Also , I believe I duplicated your error message when attempting to format the resulting spreadsheet in excel ( office 2010 version ) . It's weird , but some of the drop down tool bar format options work and some don't . But it looks like they all work fine if I go to " format cells " via a right click .

If you set the ' CreditCardName ' as the index of the second df then you can just call ` map ` : #CODE

But I need to time where those peaks occur as well . I know I could iterate over the output and find where in the original dataset those values occur , but that seems like a rather brute-force way to do it . I also could write a different function to apply to the grouped object that returns both the max and the time where that max occurs ( at least in theory - haven't tried to do this , but I assume it's pretty straightforward ) .
As an alternative you could index the group by using the ` argmin() ` function . I tried to do this with transform but it was just returning the entire dataframe . I'm not sure why that should be , it does however work with ` apply ` : #CODE
In my opinion the ` transform ` and ` apply ` functions are very opaque and the docs are not a great help . They are however extremely useful once you get to grips with them .

Map an anymous function to calculate the month on to the series and compare it to 11 for nov .

This is a simple merge and combine_first . Much faster than direct indexing . You can also specify ` left_on ` and ` right_on ` rather than renaming if you wish . #CODE
to keep original index , i have add right_index=True as merge parameters .

How to properly pivot or reshape a timeseries dataframe in Pandas ?
My approach was to create a multi-index ( Date - Time ) and then do a pivot table or some sort of reshape to achieve the desired df output . #CODE
@USER In this example it can not be 4 . The parameter length , in this example has to be 5 . The parameter length is fit to the data I have so the hour part for the index matches for all columns when you reshape / pivot the df .
Is it intentional that the first and last hour , ` 18:00 ` repeats ? Or in your actual data are the times unique ? ( The repeated hour makes ` unstack ` and ` pivot ` calls raise ` ValueError : Index contains duplicate entries , cannot reshape ` .
Now read in the DataFrame , and pivot on the correct columns : #CODE
is incorrect , as you have 18:00 : 00 twice for the same date , and in your initial data , they apply to different dates .
Then set your index using a multindex and unstack ... #CODE
it is probably better to do datetime conversion doing ` df [ ' t '] = pd.to_datetime ( df [ ' t '])` rather than call ` apply `

You need to extract prefixes and then do a left join . I am calling the first frame ` left ` and the second frame ` right ` : #CODE
@USER if you have master branch of pandas left join on index with multiple matches is fixed through [ this pull request ] ( #URL ) . if you do not have master branch specify ` how= ' outer '` in the last line , and then drop rows which are null in first column , or use ` pd.merge ` .

There are a number of ` MemoryError ` issues here on Stack , but the common theme I have seen is to make sure one uses a 64-bit machine and has plenty of RAM ( before getting into chunking and whatnot ) . Any thoughts on what is happening here ?

How does pandas merge really sort its result when using its default , sort=False ?
I'm a bit confused how the default sort works when I run a merge / join with 2 pandas dataframes . I would expect the order of my result set from #CODE

To do this , I think I need to use merge , join , concatenate or boolean indexing or grouping in Pandas .
Can you elaborate a bit more on how you merge the coordinates ? Is it based on the source name or the type ? Where is the ( 1879,328 9 ) pair coming from ? How do you choose which entry will get the ( 313 , 3331 ) or the ( 3649 , 5024 ) ? What version of pandas and python are you using ? Also , are you using tabs or spaces in your myTable file ?
It is based on start and end . Please look at the 4th and 5th column . I merge them if they are continuing
Also , @USER , what have you tried ? Did you get stuck using merge and join ? Any errors ?
we need to write a apply function first and then use group by . This is my latest guess

Pandas join on field with different names ?
According to this documentation I can only make a join between field having the same name .
Do you know if it's possible to join two DataFrames on a field having different names ?
I think what you want is possible using ` merge ` . Pass in the keyword arguments for ` left_on ` and ` right_on ` to tell Pandas which column ( s ) from each DataFrame to use as keys : #CODE

The pandas function ` append ` can be slow for larger dataframes . Instead , I would recommend storing the ` newframes ` in a python list and than using the concat function which appends all the frames only once . #CODE
There maybe actually something a little cleaner . The solution above is just a general solution for quickly merging a bunch of dataframes . Your particular problem might be more cleanly ( and quickly ? ) solved with a cross-product solution ( #URL ) , but I'd have to think more about how something like this would apply .

Python Pandas If value in column B = equals [ X , Y , Z ] replace column A with " T "
If column B equals [ X , Y or Z ] replace column A with value " T "
Remember to use ` ix ` or ` loc ` to avoid setting values on a copied slice .
Use ` isin ` and ` loc ` to set the value : #CODE
@USER using loc on this toy example took 915 us , using where took 615us , so in this case where is 30% faster
@USER for a 50,000 row df loc is slightly faster 9.27ms vs 11.2 ms

How can I create multiple columns dynamically in Pandas ? Do I always have to use append for something like this ? I remember reading ( e.g. in @USER ' s comment to this question ) that in newer versions the dynamic creation of columns was supported . Am I wrong ?
Multi-assignment could work , but I don't think its a great soln because its so error prone ( e.g. say some of your columns already exists , what should you do ? ) . And the rhs is very problematic as you normally want to align , so its not obvious that you need to broadcast .

1 ) Readability , which can partially be achieved with eval , but I don't believe eval can be used over multiple lines ?

Say I want to merge two dataframes , df1 ( consistent of columns ' a ' , ' b ' , ' c ' , ' z ') and df2 consisting of columns ( ' a ' , ' b ' , ' d ' , ' y ') , together . Columns ' a ' and ' b ' for both dataframes contain the same corresponding information . Normally I would do the following to produce a new dataframe where ' c ' and ' d ' are assigned to corresponding ' a ' and ' b ' : #CODE
However , is there a less verbose way I could pass in this information through the merge function without having to put the column titles in a list format ? My object here is to produce a new dataframe , df3 , that has columns ' a ' , ' b ' , ' c ' , and ' d '
You could also just merge the whole thing and drop columns ' y ' and ' z ' . Same pros and cons as unutbu's answer .
By default ` pd.merge ` will merge based on all columns shared in common . #CODE

What you are doing currently gives rise to what is referred to as chain indexing which may or may not work . In your case it is not working , it is preferable to use the newer forms of indexing such as ` loc ` , ` iloc ` and ` ix ` .
I think it is better to use loc : ` x.loc [ x [ ' Low '] == 0.0 , ' Low '] = np.mean ( avg_low )` I was told that you should use ` loc ` unless you cannot by Jeff

I have used ` pandas.groupby ` to group a pandas DataFrame on two columns and calculate average and median times . My resulting dataset looks similar to this : #CODE
I have gotten a bit closer by using ` to_datetime ( df [ ' Average Time '] , unit= ' d ')` . My times are now formatted like ` 1970-01-01 00:02 : 57.638400 ` in the DataFrame . However , when using ` to_excel ` to export to Excel they are formatted as ` 1970-01-01 00:02 : 58 ` in the Excel output . At this point , I only need to drop the date portion and add millisecond precision to achieve my goal . Any thoughts ?

Pandas - replace all NaN values in DataFrame with empty python dict objects
I'd like to replace the NaN with an empty dict , to get this result : #CODE
The " silly solution " won't even work , since it'll then try to use * that * dict to figure out which values to use within each column's Series . At which point you need to write out the index of every value . So no , it's not workable , unfortunately . Just use loc as described below .
This works using ` loc ` : #CODE
EDIT : The solution above works well for smaller frames , but can be a problem for larger frames . Using ` replace ` can solve that . #CODE
@USER Agreed , it's a bit silly . For smaller frames it works , but for larger frames , this can be an issue . However , we can use ` replace ` ! See edited answer .

I think split is a little more clear than regex but you can ` apply ` any function you choose to a series . #CODE

How to do resample of intraday timeseries data with dateOffset in Pandas / Numpy ?
This is what happens with a regular resample : #CODE
+1 for that resample trick , very nice !

chunk thru the table , see here and concat at the end - this will use constant memory

One thought - since the output is symmetrical , by iterating over every pair you are calculating each pair twice . Also , you can skip the comparison between an element and itself . So to at least cut down on the number of calculations , you could do something like this - using itertools to only calculate the distance for pairs , and then using pandas to fill in the rest . #CODE

Imagine a data frame with multiple variables measured every 30 min . Every time series inside this data frame has gaps at possibly different positions . These gaps are to be replaced by some kind of running mean , lets say + / - 2 days . For example , if at day 4 07:30 I have missing data , I want to replace a ` NaN ` entry with the average of the measurements at 07:30 at day 2 , 3 , 5 and 6 . Note that it is also possible that , for example , day 5 , 07:30 is also ` NaN ` -- in this case , this is should be excluded from the average that is to replace the missing measurement at day 4 ( should be possible with ` np.nanmean ` ? )
John -- thank you . I am aware of that function ( and Pandas also has a built-in interpolate method ); sadly , this is not really what I am looking for / what I described is an established method when it comes to the data I have .
Sorry , I'm not sure about that . I misread and thought the data was more regular . Perhaps resample() and then apply the above method ? If your data is consistently spaced except for some missing rows , then resampling ought to work fine and be easy to do . I'm sure you could do something with groupby but that could be a lot slower . Maybe someone else will have a better idea though .
Dear JohnE -- Thank you . Good idea using resample , that should indeed work . It's not exactly what I had in mind , but it should do the job . I will implement it in a few hours and mark your answer as accepted as soon as I get it to work . Sorry if I was a bit unclear in my question . I may open another question on shifting datetime indices by a certain amount of time instead of rows .

Missing data , insert rows in Pandas and fill with NAN
Here we set the index to column ` A ` but don't drop it and then reindex the df using the ` arange ` function .

Further you say that you don't need alignment . Pandas gives you this by default ( and no really easy way to turn it off , though its just a simple check if they are already aligned ) . While in numba you need to ' manually ' align them .

Where the " 0 " column is no longer the index for rows . Then we can apply ` df1 = df1 [ df1 [ 3 ] .isin ( df2 [ 0 ])]` . NOTE : application of ` df1 = df1 [ df1 [ 3 ] == df2 [ 0 ]]` will raise the error message ` Series lengths must match to compare `

Your result needs to be ` shift ` ed by 2 since you want the mean to include all previous ones , and not the current one ( make sure this is what you actually want , this seems a bit funny ) . You can replace the ` shift ( 2 )` with ` len ( res.index.levels [ 1 ])` to make it a bit more general in case you have more than 2 sides .
That is , the shift does not work since the differents side do not turn up in any particular order .

Using Python's pandas library , I imported a csv and set multiple columns as my index . Unexpectedly , the indexed columns are no longer present when I display the dataframe and I can't use the index columns as filter option . Google tells me that when I set my index , I should set ' drop ' to False . This has me wondering if I am mistaken in thinking of pandas indexes as being similar to SQL indexes .
First , what is the point of creating an index ? Does it speed up lookups or does it add some semantic information useful for things like stack / unstack / pivot / groupby ? Does it reduce memory usage ?
Why are date , exchange , and symbol moved out of dataframe ( and moved to index series ? ) ? Doesn't it make sense to leave them in the dataframe where I can filter on them as df [ df.symbol == ' MSFT '] ? I guess ' drop ' solves this , but the fact that this isn't default means I'm misunderstanding something here .
Indices are used to select and align rows of a data frame . The way I think of them is as labels for the dataframe rows , with the rows containing the data values . They enable you to use a dataframe to store higher dimensional data in a convenient way . I found them a little tricky starting out , but when you are aligning data sets they really shine .

pandas dataframe time series drop duplicates

I guess the lesson is to not generally assume numpy will translate a dataframe or series the way you think it will . Just use .values when there is any doubt ...

If the index that you want to index by is the outermost one you can use ` loc ` #CODE
If it is just to compare the locatoins in each series you should place all your series in a list , and then concat them . You can get all the values in one go by using ` loc `

Pandas to form clusters based on diff column

I want to drop all values after index ` 5 ` because it has no values , but not index ` 2 ` , ` 3 ` . I will not know whether each column will have data or not .
@USER : I changed the answer so as not to use ` argmax ` and ` df.loc ` . The new answer is more robust since it will properly handle DataFrames with repeated values in the index ( as well as being a bit faster ) .

2 ways , define a func and call apply #CODE
another way which I think is better and will be much faster is we could add the ' class ' column to your existing df and use ` loc ` and then just take a view of the 2 columns of interest : #CODE
this evaluates whether the price is greater than percentil_75 , if so then class 4 otherwise it evaluates another conditiona and so on , may be worth timing this compared to loc but it is a lot less readable
@USER you can upvote too ;) , the thing to take from this is to avoid loops and using apply unless it is not possible , what you want to do is to find if you vectorise your operation , that is perform your operation on the entire dataframe or series rather than a row at a time .
It's more an improvement in readability and maintainability than in performance :) . Didn't test it though , ` loc ` is pretty fast . @USER , do you have any test data available ?

drop the sep arg in your ` to_csv ` unless you think you need it

apply is just a loop and should be avoided where possible , your solution is fine , there are many ways of doing what you want . It depends on the size of the data , your sample code could be simplified : ` df.loc [ df [ ' C '] == ' a ' , ' new '] = df [ ' A ']` and likewise for the other condition

You create a lookup function and call ` apply ` on your dataframe row-wise , this isn't very efficient for large dfs though #CODE
There is a built in ` lookup ` function that can handle this type of situation ( looks up by row / column ) . I don't know how optimized it is , but may be faster than the apply solution . #CODE
On the toy dataset apply takes 470us , lookup takes 531us
Hmm for some reason timeit gets a memory error when I try this on even a modest sized df of say 4000 rows , for 400 rows I get 8.17ms using apply and 3.05ms using lookup , so I expect lookup to scale better

A simple inner merge would work : #CODE

You can create a function that determines if the value column ends in `' _regen '` and then apply it your values : #CODE
Yes but how to you actually pass the arguments within the apply function . You can't simply have a.loc [ a [ ' value '] .apply ( has_substring ( s , " regen ")) , ' key '] += ' _regen ' . Within the apply function arguments are passed with the " args= " parameter , something like this a.loc [ a [ ' value '] .apply ( func=has_substring args= " regen " , ' key '] += ' _regen ' How ever I can't get this to work .
What you have will work , args just needs to be an array within the apply function : ` a.loc [ a [ ' value '] .apply ( has_substring , args =[ ' regen ']) , ' key '] += ' _regen '`

There is a built in method ` rolling_sum ` , I also call ` shift ` at the end to align it the way you want : #CODE

Minor : ` notnull ` is also a method of DataFrames .

Thanks for the info about the file iterator . That makes sense . I will make a change to pass the ' filepath ' instead of the open file . However , renaming the columns as you suggest at the end will replace the column names , meaning I lose the first row of data .

Is there a more pythonic way to insert a row into a data frame ? I feel like this has to be a functionality of pandas but can not find it . Especially , is there a way to ' reset ' the indices ?
Use the ` loc ` attribute to assign data . Syntax is ` df.loc [ row_index , col_index ]` . An example : #CODE
I like it . Still hoping there's a built-in way to insert a row in a desired location .
I understand you as to what .loc does . I don't think I'm being clear on what i mean by ' insert ' row . I don't mean append a row . I mean insert a row at a certain location . So , if i wanted to insert row ' e ' between rows ' b ' and ' c ' , I would first df.loc [ ' e ' , ' E '] then df.reindex ( ' a b e c d ' .split() )
The parameter ` drop=True ` forces it to drop the old index . Leaving it at ` False ` keeps the old index as a new first column .

One way would be to use ` str ` methods to get the numbers , make a new dataframe from that , and then join ( or concatentate ) the results . For example , #CODE

You can use ` apply ` to check whether each row satisfy the condition , and use the resulting boolean Series to do the slicing : #CODE
Thank you ! ` apply ` does the trick .

As you may see , ` delta ` is much bigger than than the original table . It looks like the calculation performs a full outer join on the two columns using the indices , which is somewhat unexpected . How and when this behaviour occurs and is there a way to prevent it ?
It looks like the calculation performs a full outer join on the two columns using the indices , but when I tried to reproduce this behaviour using toy problems I never got similar result .

I would like to reformat this to a datatime object so that I can ` resample ` the data into half hourly bins .
Just for interest , here is a timedelta resample in action : #CODE

Pandas - resample a DataFrame by half-hourly frequency
I'd like to resample this into half hourly sets . I can see that one can do this by hour : #CODE

I don't understand your second issue : doing groupby gives biased results because the outcome is repeated / extended to pad . But if this is an issue , you can store it in a separate table and do SQL join .
I don't understand your second issue : doing groupby gives biased results because the outcome is repeated / extended to pad . ( Did you mean you only store the outcome once , at the end of the timeseries ? ) Anyway if this is an issue , you can store the outcome in a separate table and do SQL join . Or , you could backfill the outcome column everywhere except the last row with NAs after padding it .

how about concat ? #CODE
@USER sorry are you using my new modified method now or not ? you should be able to create a dataframe from the concatenated series and merge this back to your original dataframe

I have tried something like pivot table or unstack in pandas , but it is not working in this case . The problem is that each company might or might now have a representative for one region , in other words , there may be missing values in ` region_id ` for each company .
Oops ! I did not realize there are duplicates in com_id / region_id combination . I drop the duplicates in index and now it works !

The desired element dataframe ( i can just drop the n1 , n2 ... columns once i have the data ) #CODE
my original element_node_relate dataframe is dropping data . i.e. the number of entries decreases each time by quite a bit . I'm not looking to drop entries where the " n " field is blank .
Rather than looping and merging , I think a better approach would be to reshape df1 to ' long ' format , using ` melt ` . #CODE
Then , from there , you can join against your other frame and summarize however you want . #CODE

Append CSVs without column names
I have a list of CSVs , but they don't have column names . With my code in Python 2.7 it appends but the first row in each CSV is recognized as the column names . How can I append the CSVs without column names . For example : #CODE

My only solution to this has been to write an Excel macro to strip out characters not usually liked by databases when importing - and then import the file using python .

can you elaborate pls ? I am able to read in the file for specific columns also . But resample is an issue .

I have found the solution someself . If somebody wish to know , it could be done with the function ` pivot ` #CODE

How to merge / join / concat dataframes on index in pandas
I have a main dataframe df1 which is ' empty ' and some other dataframes of different length but with same columns as df1 . I would like to merge / join / concat df2 and df3 on certain index positions of df1 : #CODE
However , concat ([ df1 , df2 , df3 ] , axis=0 , keys .... ) puts the dfs consecutively together ...
You won't be able to achieve what you want using ` concat ` like this or merge for that matter without reindexing . By default for ` concat ` the original index values will be used and will just stack the dfs after each other . If you tried to merge using the indices then they will clash and create additonal columns e.g. ' X_x , ' X_y ' etc ..

I need the try statement to filter some NaNs ... 1st problem : I couldnt figure out how to use ' apply ' or something instead of the for loop . Does anyone know a more efficient way ? Second problem : #CODE
does not behave as I'd expected . I need to add frame_b's data to my returnframe where both frames ' indices ( timestamps ) are equal . But ' join ' appears to add some rows somewhere inbetween ( maybe due to duplicate indices ?? ) . It's important for my other functions that the length of returnframe is not changed by the latter addition of data .

I use ` np.vectorize ` to apply this function on a ` DataFrame ` - ` dataFrame ` - that has about 22 million rows . #CODE
plz show the related code where you apply ` getData ` over the data-frame
you should show what your actual problem is . it looks like you need to do a simple merge .

Warning : In the current implementation apply calls func twice on the

#URL Technically this is not implemented :) . Its actually quite complicated because you need to align the blocks ( dtypes ) of the 2 frames ; certainly not tested . Let's open an issue . Maybe in the short-term raise NotImplementedError . ( Pls do a pull-request if you would )
@USER I agree that this should raise for now . I wasn't sure if I was using this method wrong but an obvious workaround would be to iterate through each column and apply fillna() with axis=0 . Would it be so hard to incorporate this for when the method is applied to dataframes ?

Ask you to insert the name of the Excel file to be read
` test.exe ` does not run even when I insert ` from pandas import read_excel ` in ` test.py ` but I do not ask for any ` input ` file
Apologies but I can't reply to comments yet so I edited my post . It may be that you have different versions of components installed to me . I googled that error and there didn't appear to be any obvious solution . I have python v2.7.5 ( via activestate ) , pandas 0.14.1 ( via scipy stack 14.8.27 )

How to transpose ( stack ) arbitrary columns in Dataframe ?
Now I want to transpose ( stack ) columns ' 2010 ' , ' 2011 ' and ' 2012 ' into rows to be able to get : #CODE
By using ` df.stack() ` pandas " stacks " all columns into rows , while I want to stack just those pointed . So my qestion is how to transpose arbitrary columns to rows in pandas Dataframe ?

Alternatively , you could probably ` melt ` into a flat format , pick out the largest two values in each Date group , and then turn it again .

Pandas : join a row of data on specific index number
I want to join df2 on df so it looks like this : #CODE
It already does do a join with a specific index , it just so happens to be that your index in ` df2 ` is 0 and so when it joins it places the `' a ' , ' b ' , ' c '` in index 0 . #CODE
You have to make sure that the row that you want to join has the same index as where you want it to eventually be , yes . At the moment ( without re-indexing ) it's saying * " Oh well in ` df2 ` I was in index 0 so I'll put myself in index 0 in ` df `" *

Use ` np.vstack ` to stack a list / array / whatever of rows : #CODE

we can use boolean masking to select the values of interest , then ` concat ` them passing ` axis=1 ` , you can then just rename by directly assigning to the ` columns ` attribute : #CODE
Of you can ` join ` and set the desired suffixes ( thanks to @USER ): #CODE
Another way is to ` merge ` ( which is what ` join ` uses underneath ): #CODE
Or ` join ` instead of ` concat ` : ` df [ df > =0 ] .join ( df [ df < 0 ] , lsuffix= " A " , rsuffix= " B ")` .

If I replace all the slashes with ' over ' then I get the following : #CODE

Replace ' , ' to ' . ' , convert to a timedelta and add in the date #CODE

How can I add summary rows to a pandas DataFrame calculated on multiple columns by agg functions like mean , median , etc
After doing some graphing with the data in this long format , I pivot it to a wide summary table format with columns for each ID . #CODE
However , I can't find a concise way to calculate and add some summary rows to the wide format data with mean , median , and maybe some custom aggregation function applied to each of the ID-based columns . This is what I want to end up with : #CODE
I tried things like calling ` mean ` or ` median ` on the summary table , but I end up with a Series rather than a row I can concatenate to the summary table . The summary rows I want are sort of like pivot_table margins , but the aggregation function is not ` sum ` . #CODE

Join two DataFrames on one key column / ERROR : ' columns overlap but no suffix specified '
I think you want to do a merge rather than a join : #CODE
It's ignoring the on kwarg and just doing the join .
thanks . For some reason I thought ' merge ' requires two dataframes to have the same length .

You could use nested ` concat ` operations , the inner one will concatenate your last row 3 times and we then concatenate this with your orig df : #CODE

the name is a bit irrelevant , you can just append the resultant df to a list or dict , the name is just a reference to an object , why does this matter . Conceptually what you are asking doesn't make sense to me you'll need to explain better and edit this into your question
let data1 [ data1.delta1 > 0 ] .column , data1 [ data1.delta1 == 0 ] .column to be appended to a list or dict outside the loop and then find intersection later ? such as set [ 0 ] & set [ 1 ] ?
Now concat them : #CODE

What's the easiest way to append to the same data frame the result of dividing Row1 by Row2 ? i.e. the desired outcome is : #CODE
in addition , if you want to append multiple rows , use the ` pd.DataFrame.append ` function .
DAMMIT , forgot the ` loc ` . well , it's fixed now . the main point was to show how to access rows by number .

Or what do you mean by listing only the fields that are relevant ? Are you saying it would be better to divide up the files each to its own field ? ( And then have to do some sort of join between files ? )

Normalize DataFrame by group

Append Two Dataframes Together ( Pandas , Python3 )
I am trying to append / join ( ? ) two different dataframes together that don't share any overlapping data .
I am trying to append these together using #CODE
EDIT : in regards to Edchum's answers , I have tried merge and join but each create somewhat strange tables . Instead of what I am looking for ( as listed above ) it will return something like this : #CODE
OK , what you have to do is reindex or reset the index so they align
Use ` concat ` and pass param ` axis=1 ` : #CODE
` join ` also works : #CODE
As does ` merge ` : #CODE
In the case where the indices do not align where for example your first df has index ` [ 0 , 1 , 2 , 3 ]` and your second df has index ` [ 0 , 2 ]` this will mean that the above operations will naturally align against the first df's index resulting in a ` NaN ` row for index row ` 1 ` . To fix this you can reindex the second df either by calling ` reset_index() ` or assign directly like so : ` df2.index =[ 0 , 1 ]` .

( rows and columns ) . Arithmetic operations align on both row and column labels .

My aim is to achieve ` Sum ( Tag ) / Count ( NonZero ( Tags ))` for each user_id
Is it possible to achieve ` Sum ( Tag ) / Count ( NonZero ( Tags ))` in one command ?
To count nonzero values , just do ` ( column ! =0 ) .sum() ` , where ` column ` is the data you want to do it for . ` column ! = 0 ` returns a boolean array , and True is 1 and False is 0 , so summing this gives you the number of elements that match the condition .
@USER : You could get ` inf ` if there were no nonzero tags ( so that the count of nonzero tags was zero ) . Your original formulation is not well-defined for that case , so you'll need to think about what you want the result to be .

or even I can do list comprehension ... Anyway I want to ask , does somebody know about probably some specific method in Pandas , which could replace ALL strings in some column with the corresponding (= different ) integer ( or float ) values ..? Please , propose only pythonic way .

However , it is still not working as checking the len ( forward [ ' OPR_DATE ']) doesn't show me a reduced column at all .
len ( forward [ ' OPR_DATE ']) = 116560
len ( cash [ ' opr_date ']) = 7781
len ( forward ) = 98012
len ( cash ) = 7781
You can do an inner join operation . Since you only want data that exists in both dataframes .

Hi CT -- so a bit of a problem / update that I am struggling with here . This solution works .. sorta . I tried to apply this solution to my larger data frame and at first glance it worked wonders , but now taking a closer look there are several instances of " looped phrases " that slipped through the cracks . Any idea why this could be ?

You could sort the values in columns ` A ` and ` B ` so that for each row the value in ` A ` is less than or equal to the value in ` B ` . Once the values are ordered , then you could apply ` groupby-transform-max ` as usual : #CODE
It's not the most performant , but there's also ` df [ " Value "] .groupby ( map ( frozenset , df [[ " A " , " B "]] .values ) , sort=False ) .transform ( max )` .

Cleanest way to perform pandas join involving an index and also columns
Suppose I want to merge ( concat ? ) two pandas tables by joining on both an unnamed index and a column ( here " identifier ") . Is there a clean way to do this ? #CODE
a bad cut and paste !

I first tried using apply but it's not possible to return multiple Series as far as I know . iterrows seems to be the trick . But the code below gives me an empty dataframe ... #CODE
You can then stack this DataFrame to obtain : #CODE

where repositoryid and date are the levels 0 and 1 of a multiindex , respectively , and userid and watchers are the data columns . So , for every repositoryid , I basically have a time series of events of users who start watching a repository . For each repositoryid , I also know a specific creationdate from somewhere else . Now I want to drop all rows where date > creationdate+timewindow , where timewindow is some constant .
So , my question is , 1a ) how do I drop rows quickly from a multi-indexed data frame conditional on the level 1 index , or equivalently 1b ) how do I insert single-indexed data frames into multi-indexed data frames , and 2 ) is this even the best=fastest way to solve my problem , or should I try a completely different approach ?
( I also tried to not use indices , but that was too slow . I played around with join , merge , groupby , etc , unfortunately I did not manage to get them to solve my issue . I spent the last 5 days studying the excellent book " Python for Data Analysis " and trying to find solutions online to this problem , again without success . I hope that maybe an advanced pandas user has an elegant solution to this seemingly simple issue ? Thanks a lot in advance ! )

no need for apply ; you can directly subtract a Timestamp from a column to yield a timedelta64 dtype

Is loc or xs faster in pandas ?

Desired output - A new DataFrame of grouped / aggregated coordinates in an array so that I can apply a fuction to each array : #CODE
Distance calculation I wish to apply ... #CODE
note that to apply your distance function you have to do : #CODE

Then you could join the two dataframes : #CODE

XML is a tree-like structure , while a Pandas DataFrame is a 2D table-like structure . So there is no automatic way to convert between the two . You have to understand the XML structure and know how you want to map its data onto a 2D table .

I want to compute the mean of each row and subtract it from each value in that specific row to normalize the data , and make a new dataframe of that dataset .
@USER .Mr .W . I agree it can be pretty confusing ( I regularly get stuck and have to play around with axis numbers when dealing with NumPy / Pandas ) . Basically , ` axis=1 ` just means " across the DataFrame " . That can mean entries in a row ( as in ` df.mean ( axis=1 )`) , or , since the names of the columns also go across the DataFrame , ` axis=1 ` is used when column names need to be referred to . I think there are a couple of questions about this topic on Stack Overflow , one of which I answered [ here ] ( #URL ) .

I know the issue is with ` df.index.time ` since if I replace this by ` range ( 0 , len ( df.Volume ))` , I get the plot .
I also tried to replace ` df.index.time ` with a new variable ` time ` where #CODE

To count the number of rows in a DataFrame , you can just use the built-in function ` len ` .
it's probably better to replace the ` ~ pd.isnull ` wit ` pd.notnull ` for readability

One method could be to apply a lambda to the column and use the boolean index returned this to index against : #CODE

FYI - no need to use apply here ( tz_localize / convert are methods on index and series )
Thanks @USER . I tried that in the past without luck . I opened this question separately : [ Unable to apply methods on timestamps in Pandas using Series built-ins ] ( #URL )

Unable to apply methods on timestamps using Series built-ins

but I don't know how to append ` age ` to ` new_df ` , hope anyone could give me some advice .

I pivot this dataframe . #CODE

As pointed out in the comment below by @USER , there may be a bug in pandas when using ` pivot ` to do this with a DataFrame that has duplicate entries in the index .
This is also problematic because in this case , using ` set_index ` followed by ` unstack ` will be very tedious , since one of the would-be index levels is already the index , and the other is not . We want to " append " ` name ` into the index , without needing to first pop the unnamed , existing index out of there , which can be done but leads to annoying , unreadable syntax .
Original answer : Use the facilities provided in the ` pandas.DataFrame ` ` pivot ` function , to pivot on the column that you want to serve as the categories . #CODE
If you are having trouble with errors related to the index , try adding the index as part of the pivot : #CODE
Yeah , this looks like a bug then . I believe somewhere when it is looking up the indices , it exhausts them . Meaning , once it consumes an index value , the same value becomes unavailable for consumption . In some of the documentation and associated other SO questions , it looks like some of the pandas devs try to deflect this by saying that ` pivot ` is a " reshape " operation , as opposed to something that alters the index . But that's not a satisfactory explanation : reshaping is exactly what one is trying to do when going from duplicate index entries to the widened-column version .

Python : weighted median algorithm with pandas
I want to calculate the weighted median of the column ` impwealth ` using the frequency weights in ` indweight ` . My pseudo code looks like this : #CODE
This method seems clunky , and I'm not sure it's correct . I didn't find a built in way to do this in pandas reference . What is the best way to go about finding weighted median ?
` df [ ' indweight '] .sum() * ( .5 )` should calculate the number of observations that fall under the 50th percentile in the data , since ` indweight ` is a frequency weight . So it makes sense that the mean and median of ` indweight ` exceed its sum .
Have you tried the wqantiles package ? I had never used it before , but it has a weighted median function that seems to give at least a reasonable answer ( you'll probably want to double check that it's using the approach you expect ) . #CODE
the weighted median function is very similar to the accepted answer if you look at the code but doesn't interpolate at the end .

In short : I used a pivot table to transpose my matrix and then I was able to plot the single cols automaticly , at example 2 here .

With some help from stackoverflow and the pandas documentation ( thanks ! ) I figured out how to pivot the data : #CODE
The ` pivot ` is making a DataFrame whose columns have a MultiIndex . Since the top level , ` value ` , is the same for all columns , you could simply drop it : #CODE
Or , better yet , specify the ` values ` column when calling ` pivot ` : #CODE
That works ! I found that to keep the desired ` tstamp ` orientation I could use ` dfp.to_json ( orient= ' index ')` . I wonder if there isn't a way to use to_json directly instead of pivot ?
The pivot changes column values into column labels . None of the ` orient ` options ( split , records , index , columns , values ) does that , so no I think you need to call pivot .

What I would like to do is merge these two DF together and align them as so : #CODE
How do I align the data to fit with my desired output ?
Merge on both ` Words ` and ` Type ` : #CODE

In norm.ppf ( probability , mean , standard deviation ) so 10 is mean and 5 is std . My code will apply the norm.ppf for every element of lts .
You're not using Pandas for any purpose other than Series being a container , so it will be faster to cut Pandas out of this calculation .

Using Apply Map to Remove Unwanted Phrases from DF ( Pandas , Python 3 )
What do you mean by " remove cells " ? I think you mean remove rows right ? Then this will be an ` apply ` since you'll consider things rowwise . #CODE
So you can drop those rows that match your criteria with #CODE

Append rows from a Pandas DataFrame to a new DataFrame
What I am trying to do is collecting whole rows from these locations and append them to a new dataframe , the index value being ` i ` in the script given above . How can I get the entire row ( since I know the index through ` argmax() `) and append it to a new dataframe ? There is also the NaN issue , meaning if there is no data in the said interval , then the script should add NaNs for all columns in that row . What would be an easy way to do this ?

You can vectorize ' Words1 ' into a series and then apply a regex : #CODE

use ` loc ` : ` df.loc [ df [ ' hello '] .str .contains ( pattern )= =False , ' col '] = newVal `
` loc ` uses label based indexing see the docs : #URL
You can replace your wacky ` elif ` construction with a simple ` else ` in this case . ( Or , in fact , with nothing at all . The ` break ` takes care of the else for you . ) Go ahead and do it , it'll stop future readers of this question being distracted by it .

We can use ` shift ` to compare if the difference between rows is larger than 1 and then construct a list of tuple pairs of the required indices : #CODE
I use numpy's searchsorted to find the index value ( integer based ) where we can insert our value and then subtract 1 from this to get the previous row's index label value #CODE

Resample a pandas timeseries by " 1st Monday of month " etc
Let's assume you want the 2nd Tuesday of each month . You can resample to a particular day of the week , using a built-in pandas offset #CODE

Then just join all your entries in the fruit column with a comma , eliminate extra spaces , and split again to have a list with all your fruits . Finally count them : #CODE

Python Pandas DataFrame how to Pivot
I believe you want to pivot this using ` pd.pivot_table ` . See the examples on pivot tables to understand better how this works .

The second part of my code is function I am trying to apply #CODE
So problem solved , second problem springs up -- the changes being made don't " stack " within the for loop . So using the code above , the return output is " is a / good guy " . I was hoping to remove all word pairs that appeared in my excel file so that my final return output was JUST ' Good Guy '

Python Pandas replace returns strange results
if you are just wanting to replace the existing values or add them y performing a lookup then you can do ` df [ ' new '] = df.map ( from_to )` , however you need to post more code , input data , existing code and desired output
Thanks both , nested translation ? Could you elaborate with an example ? I use replace instead of map because there are more than on columns I need to convert . Here is an code example . this is a real life case that operator sometimes enter wrong code for same part . df = pds.DataFrame ([[ ' J1801457 ' , ' m2409-2 '] , [ ' 1457 ' , ' m2409-1 ']] , columns = [ ' c1 ' , ' c2 '])

Inconsistent behavior of ix selection with duplicate indices
I do not understand this ( seemingly ? ) inconsistent behavior . Is it on purpose ? I would expect objects that are returned from the same operation to have the same structure . Further , I need to build my code around this issue , so my question is , how do I detect what kind of object is returned by an ix selection ? Currently I am checking for the returned object's len . I wonder if there is a more elegant way , or if one can force the selection , instead of returning just the number 54 , to return the similar form #CODE
( Note that it's usually a better idea to use ` loc ` ( or ` iloc `) rather than ` ix ` because it's less magical , although that isn't what's causing your issue here . )

Replace Number that falls Between Two Values ( Pandas , Python3 )
I want to replace all the scores with either ' very low ' , ' low ' , ' medium ' , or ' high ' based on whether they fall between quartile ranges .
Finally you are performing chain indexing which may or may not work and will raise a warning , to set your column value use ` loc ` like this : #CODE
it's being caused by the first replace line and the inclusion of the values ' Very Low ' in the final df ... just not sure how to get around this

Take a look at this stack overflow example .

How do I subtract ( and replace with result ) A B and C in row one with the average in row 1 ?

How to Stack Data Frames on top of one another ( Pandas , Python3 )
Right now , I have them concatenated together so that it becomes 6 columns in one DF . That's all well and good but I was wondering , is there a pandas function to stack them vertically into TWO columns and change the headers ?

After loading the dataframe , I insert a Year column in position 0

You can pass ` plt.scatter ` a ` c ` argument which will allow you to select the colors . The code below defines a ` colors ` dictionary to map your diamond colors to the plotting colors . #CODE
This code assumes the same DataFrame as above and then groups it based on ` color ` . It then iterates over these groups , plotting for each one . To select a color I've created a ` colors ` dictionary which can map the diamond color ( for instance ` D `) to a real color ( for instance ` red `) .
@USER Ok I see :) I've edited it again and added a very simple example which uses a dictionary to map the colors , similarly to the ` groupby ` example .

I want to merge the following pandas Datadrame : first_df and second_df #CODE
To merge it I'm using : #CODE
the default merge type is ' inner ' this means where in your case the index values are present in both dfs , if you wanted the union of all index values then you could pass param ` how= ' outer '` , or ` left ` or ` right ` see the docs : #URL

It certainly helped , thank you . I had to change ` pivot ` to ` pivot_table ` with the entire dataset but you certainly pointed out the right direction .
This code creates a pivot_table called ` pivoted ` where each of the columns are now ` type ` and the data is the index . We then simply resample it using ` pivoted.resample ( ' W ')` .

Pandas Groupby Apply Function to Level
Assuming you have a series ` s ` , with a MultiIndex andthe two levels you have shown , you can groupby the first level and apply the `' first '` / `' last '` aggregations to get the values you want . #CODE

What have you tried here ? You could apply your function row-wise which would look something like ` df [ ' ls '] = df.apply ( lambda row : checker ( x.Review ) , axis=1 )` but ideally you want to vectorise your function so that it can be done on the whole column , at the moment your function looks incomplete so it's hard to suggest what improvements can be made

The easiest way would be to specific a single column of the groupby ( doesn't matter which one ) , and use ` transform ` instead of ` apply ` , like this . #CODE
The reason this didn't work while your first did is that your function returns a single value , rather than an array of values , so ` transform ` broadcasts back to the original frame's shape , while ` apply ` is more flexible and generally passes back whatever shape your function returns .

If you just want the column headings , you can apply the mask to itself . #CODE

Drop the 2nd row : #CODE

Originally , I used append api to create a single table ' impression ' , however that was taking 80sec per dataframe and given that I have almost 200 of files to be processed , the ' append ' appeared to be too slow .
Also , why is append so much slower than put ? Can it be sped up ?

But what if I want to get the percentage of the vote each candidate got ? Would I have to apply some sort of function on each data object ? Ideally I would like the final data object to look like : #CODE

You can sort by the date and then drop the duplicates , keeping the last ones like this : #CODE
But it may be preferable to set the date as the index , if we can drop the old indexes , and then just sort by the index : #CODE

I tried to aggregate by gene but I got an error , possibly due to the NaN . If possible I would like to keep the output as a pandas data frame since I will have to merge this to another df in the future #CODE
got it to work by changing ` if ser.empty : ` into ` if len ( ser )= =0 : ` Thanks !

Drop columns that aren't common between two dataframes ?
You don't necessarily need to drop the columns , just select the columns of interest : #CODE

pandas dataframe replace full word
The above replace results in : #CODE
There are several unwanted replacements . I could loop through the list and do a word replace , but I have a very large dataset and looping may not be efficient . I'm also aware of using ' \b ' for whole word replacement . But I'm not sure how I can do whole word replacement using ' DataFrame.replace ' method . Any help will be useful .
I think for more complex expressions i would write a function and than use pd.DataFrame.apply() . I use it very often and its performance is still ok even at DataFrames that are in len > = 10^6

Reshape / merge columns in Pandas dataframe after using pivot_table
The following pivot function is working but I need to reformat results . Essentially removing the ' financialyear ' label , and moving ' business ' up to the same line as the financial years . #CODE

Perform a ' left ' ` merge ` in your case on column ' B ' : #CODE
Another method would be to set ' B ' on your second df as the index and then call ` map ` : #CODE

We can perform a groupby on ' A ' and then apply a function ( lambda in this case ) where we join the desired delimiter ` ; ` with a list comprehension of the B values .
In Python you can join things by using ` some_delimiter.join ( things_you_want_to_join )` , e.g. `' , ' .join ( " abc ") == ' a , b , c '` . We can apply that to the ` B ` column after grouping on ` A ` : #CODE

And finally we ` join ` : #CODE

I don't want to cast to int and then mod / truncate ( because , in the case of zip_code and theoretically tax_id too , ' 07443 ' will get converted to 7443 which isn't good ) . I just want to clip the ' .0 ' and have to_excel() treat the whole column as strings ( unicodes , more specifically ) .

I wouldn't worry about it , it seems ok but I think you have to show what you've tried at the very least plus any code and approaches , it would help also to show what you've tried that works on simple data i.e. not in a pandas dataframe and are asking how to apply that to pandas would probably help . I guess at this point this looks like an open exercise without demonstrating your efforts
I don't think regex is the best classifier for sentences . Perhaps you could try using fuzzywuzzy or similar to get a ratio of how similar the phrases are then join of that . Not going to be efficient or particularly accurate though .

since , per the docs , " [ a ] dditional keyword arguments [ to ` apply `] will be passed as keywords to the function " .
hmmm this gets me sort of the way here but not quite -- how can I apply that if I need to actually CALL the dataframe in the arguement ? Edited OP above

I am trying to resample a pandas data frame with a timestamp index to an hourly occurrence . I am interested in obtaining the most frequent value for a column with string values . However the built in functions of time series resampling do not include mode as one of the default methods to resample ( as it does ' mean ' and ' count ') .

How can I join this data and append corresponding low frequency data columns to the higher frequency data if it falls on that day ?
One way would be to create a custom apply function and check each datum's YMD and look up the corresponding low frequency data , but that seems pretty inefficient .
Perhaps you are looking for resample ?
Note : you can do this super cleanly with a merge ( assuming no overlapping columns ): #CODE
Now reindex on the start of the day ( with normalize , in 0.15 you'll be able to use ` .dt .normalize() `) : #CODE
Is there a way to get this to work if my lower frequency data wasn't days ? It seems that normalize doesn't take any parameters , for example , if my low frequency data was on a monthly basis , or bi-weekly .

That is basically matrix multiplication of ` X '` and ` X ` where ` X '` is transpose of ` X ` : #CODE

It does what you describe , go row-wise ( so apply over ` axis=1 `) along ` df ` and use the entries as index for selecting in ` P ` .

Python Pandas : replace given character if found in column label
I would simply like to replace " " with " and " where found , either at the column labels or index labels .
What you tried won't work as ` Index ` has no such attribute , however what we can do is convert the column into a Series and then use ` str ` and ` replace ` to do what you want , you should be able to do the analagous operation on the index too : #CODE

I just wonder if it is a good practice to physically rename and replace / select raw data and create new files from it . I do not want to start doing something which later down the road I will realize was a huge mistake .

You should switch to using pandas built in group by and then merge it with your original frame . Try using this code ... #CODE

interpolate values between sample years with Pandas
If you're using a newer version of Pandas , your DataFrame object should have an interpolate method that can be used to fill in the gaps .

Parallelize apply after pandas groupby
I have used rosetta.parallel.pandas_easy to parallelize apply after group by , for example : #CODE
I have a hack I use for getting parallelization in Pandas . I break my dataframe into chunks , put each chunk into the element of a list , and then use ipython's parallel bits to do a parallel apply on the list of dataframes . Then I put the list back together using pandas ` concat ` function .
This is not generally applicable , however . It works for me because the function I want to apply to each chunk of the dataframe takes about a minute . And the pulling apart and putting together of my data does not take all that long . So this is clearly a kludge . With that said , here's an example . I'm using Ipython notebook so you'll see ` %%time ` magic in my code : #CODE
write a silly function to apply to our data #CODE
then it only takes a few ms to merge them back into one dataframe #CODE
I will try this with my code , thank you . Can you explain to me why apply does not automatically parallelize operations ? It seems like the whole benefit of having the apply function is to avoid looping , but if it is not doing that with these groups , what gives ?
There's a long story about parallelization being hard in Python because of the GIL . Keep in mind that apply is usually syntactic sugar and underneath it's doing the implied loop . Using parallelization is somewhat tricky because there are runtime costs to parallelization which sometimes negate the benefits of parallelization .
By doing small modification to the function it can be made to return the hierarchical index that the regular apply returns :
By the way : this can not replace any groupby.apply() , but it will cover the typical cases : e.g. it should cover cases 2 and 3 in the documentation , while you should obtain the behaviour of case 1 by giving the argument ` axis=1 ` to the final ` pandas.concat() ` call .

` print ( Boston1.DESCR )` gives the slightly mysterious comment , " Median Value ( attribute 14 ) is usually the target " . But it does not mention how to access the MEDV data .

Yes with the resample , there is no more line between two points . However , there is still a lot of empty space . Is there a way to make it skip that ? In other words , after 16:00 of one day , skip straight 09:30 of the next day ?

And the first append is like :
Append 50M rows . #CODE

I thought the issue might be in using ` join ` , so I tried another route . It was faster but did not solve the problem . #CODE
It is indeed faster to use ` reindex ` instead of ` join ` , but now ` MultiIndex ` is even slower , comparatively !

pandas : how to apply scipy.stats test on a groupby object ?

I wanted to take just the rows with dates from 2014.07.14 to 2014.07.24 . I did this using the loc index in pandas : #CODE

This is my first post in Stack Overflow and I'm trying to be as concise as I can .

How can I truncate a table using pandas ?
However , before entering the above code I want to truncate ( or even drop ) the table . I didn't find any dedicated function in pandas.io.sql . So I tried to create an empty data frame just to call : #CODE
This code does drop the table but then generates exception from sqlalchemy as it tries to re-create an empty table . I can catch and ignore it , and the next call to_sql() with if_exists= ' append ' will create the table correctly , but this is pretty ugly .
This almost works : it truncates the table but then insert a single record with all fields as NULL ...
Thanks @USER , the 2nd case I tested by ` df1 = df.drop ( df.index )` actually create an empty DataFrame with the correct columns but as I wrote it truncate but insert a single record of NULL in all fields . I don't know how to delete it with sqlalchemy ( just used alchemy engine to be able to use pandas to_sql() for mssql ) but I can delete it using pyodbc . The question is whether it's possible to truncate it using pandas api

When i try to insert this into the same data frame , the values are all NAN . i think it is because the aggregation is on Date , but the dataframe is indexed on Date and ID . so doing this does not work : df [ ' w_avg '] = g.wa.sum() . How do i resolve this ?

How to use groupby to apply multiple functions to multiple columns in Pandas ?
Since you are aggregating each grouped column into one value , you can use ` agg ` instead of ` apply ` . The ` agg ` method can take a list of functions as input . The functions will be applied to each column : #CODE

Look at [ ` shift `] ( #URL )
@USER I don't understand how shift will help here .
shift allows you to compare whole arrays against a shifted row , what you are doing could be done very quickly using numpy or pandas , for example you could take a slice of your data ( imported into numpy or pandas ) and then filter them using your criteria as all you are doing is looking for matches between 3 consecutive rows
Good spotting by arhuaco . If you do ' for idx , i in enumerate ( big [: 500000 ]): for jdx , j in enumerate ( **big [ idx : ** ]): ' you will cut the number of iterations .
@USER I've already cut the initial table and dropped all unnecessary rows and columns . The data is actually sorted by date , from present to past . So the first rows are from 2014 and the last are from 1970-s . I need to take each row from the first 4 years ( about 500k lines ) and check it against all rows that are older .

One option is to aggregate the grouped ` B ` column using ` pd.unique ` and the builtin Python ` len ` function : #CODE
There is a built in ` unique ` attribute so it could be re-written like this : ` df.groupby ( ' A ') [ ' B '] .unique() .apply ( lambda x : len ( x ))`
ok , I can combine the function ` value_counts ` and ` groupby ` through ` apply ` function . Thank you very much !
@USER I tried with a 100,000 row dataframe and the difference becomes 11.4ms vs . 8.9ms for my implementation versus yours . There is probably some battle / tradeoff between the vectorised ` value_counts ` and calling ` apply ` , there maybe a better way but I've not figured out a better way yet

Panda's DataFrame dup each row , apply changes to the duplicate and combine back into a dataframe
I need to create a duplicate for each row in a dataframe , apply some basic operations to the duplicate row and then combine these dupped rows along with the originals back into a dataframe .
I'm trying to use apply for it and the print shows that it's working correctly but when I return these 2 rows from the function and the dataframe is assembled I get an error message " cannot copy sequence with size 7 to array axis with dimension 2 " . It is as if it's trying to fit these 2 new rows back into the original 1 row slot . Any insight on how I can achieve it within apply ( and not by iterating over every row in a loop ) ? #CODE
The ` apply ` function of pandas operates along an axis . With ` axis=1 ` , it operates along every row . To do something like what you're trying to do , think of how you would construct a new row from your existing row . Something like this should work : #CODE

Append multiple CSVs with the name of the name of each CSV in all rows - Python
You can use the converters parameter to change or format values on the fly . I am not sure if would append a column if one didn't exist but you can give this a try : #CODE

Pandas concat gives error ValueError : Plan shapes are not aligned
My understanding of concat is that it will join where columns are the same , but for those that it can't find it will fill with NA . This doesn't seem to be the case here .
Heres the concat statement #CODE
I recently had the same error , it turns out that I had a duplicate column name in the dataframe ` df_e ` when joining using an append statement ` df =d f_t.append ( df_e )` . Before the statement worked fine , then I accidentally added a duplicated column and it gave them the same error statement as above .
I don't know whether this answer would have solved the OP's problem ( since they didn't post enough information ) , but for me , this was caused when I tried to ` concat ` dataframe ` df1 ` with columns ` [ ' A ' , ' B ' , ' B ' ' C ']` ( see the duplicate column headings ? ) with dataframe ` df2 ` with columns ` [ ' A ' , ' B ']` . Understandably the duplication caused pandas to throw a wobbly . Change ` df1 ` to ` [ ' A ' , ' B ' , ' C ']` ( i.e. drop one of the duplicate columns ) and everything works fine .

merging an empty pandas Series with inner join does not give null result
When I use the pandas concat function , it mostly gives me results I would expect : #CODE
But when I merge two series , one of which is empty , I do not get a length-zero result : #CODE
Not sure if this is a bug or feature , but what are you trying to accomplish ? Note that axis=1 is sort of a strange concept here since series are essentially 1-dimensional . Also , I wonder if merge might be what you are looking for . But again , it isn't real clear to me what the point would be of intentionally using concat on an empty series or dataset in the first place .
I'm creating a dataframe from the intersection of two Series . For downstream processing I want to have them in one object . I'm not really intentionally using it to handle empty Series objects , it just happens that I sometimes get an empty data set back for one of the Series . I could write a handler outside that just tests if either one is empty before trying to join them . But it didn't seem like I should have to .
As far as merge , it seems like that only handles dataframes , not series . I could use that if I cast both series objects as dataframes . But again , it doesn't seem like I should have to do that . Plus if I have two dataframes , concat works as planned .

But I don't understand how to apply it to my case .
Any idea how to solve this problem in a efficient pandas approach ? ( using apply , map or rolling ? )
What you needed to do from the answer you linked to was to turn the index into a series so you can then call apply on it . The other key thing here is that you also have to index the constructed series the same as your df index as the default is to just create an index from scratch like 0 , 1 , 2 , 3 ... #CODE

Plotting from Pivot Table using Pandas
Let's say that I have the following pivot table ( this is the one created in the documentation ): #CODE

I am new to Python , and not much of a coder . I have 40+ text files that I want to combine together ( in a ' wide ' csv , as opposed to a ' tall ' csv . That is , I don't want to append the files ) and produce a new csv .
Using Pandas ( merge ) I am able to achieve what I want , but I presume there is a simpler way . Here it is on seven of the files :
You can apply ` merge ` to a list of DataFrames using reduce : #CODE

Probably the simplest solution is to use the APPLYMAP or APPLY fucntions which applies the function to every data value in the entire data set .
Apply Docs

To query the results of your ` groupby ` operation , you can use ` loc ` . For example : #CODE

This code should resample your data to 2.5s intervals #CODE

The reason the first element in your return array is 2.67 is that you're using the ` last ` method for ` how ` to resample your data . If you want to change to where the first resampled point will read ` 1.832128 ` , use the ` how= ' first '` kwarg .
now , ` concat ` with the original frame , ` sort_index ` and find cumulative sum : #CODE

` stats = users.groupby ( ' lang ') .agg ( len )`

slightly more readable to use ` notnull ` so ` data_df [ data_df.c.notnull() ]`

Wunderbar ! I was going down the third path and getting tangled , but I had no idea there was an existing metaphor for this in the whole pivot area . Thank you !

So my idea was , I will create an excel-template with these tricky parts and then from python just insert dynamic data to this template .
@USER : That's not really relevant here , and not a productive comment . There isn't any code he could have tried , given the tools that he's mentioned in the question and the tags . If you don't think this question is a good fit for Stack Overflow , then comment on why you believe that , and / or downvote the question .

Can one perform a left join in pandas that selects only the first match on the right ?
Can one perform a left join in pandas that selects only the first match on the right ? Example : #CODE
This would straightforward to add as an option to `` merge `` , but is not implemented at the moment .

To reshape this to the expected output , we can now use ` unstack ` to move the ' year ' index level from the rows to the columns ( and the use ` fillna ` to get 0's ): #CODE
thanks a lot joris , i feel stupid , unstack was not in my scope of known function :-(

` resample ( ' 2.5S ' , how= ' sum ' , label= ' right ')`
Thanks @USER I have updated the post . The problem with ` resample ` is that it only considers the time values where I have data ( i.e. the time window defined by the index of the dataframe ) , but I want to get samples on a different set of timestamps .
try using `` freq= ' 2500L '`` ; there is a bug with fractions in the frequency ( it will make it 5s freq which is wrong ) . You can simply resample and it will work like you want . You reindex if you really want to ( either before or after ) to your full-range , but those will be NaN
Thanks @USER . If I re-index beforehand , will ` resample ` start counting ` freq ` from the first timestamp in the dataframe ? ( if so , this fully answers the problem ) .
yes it will ' snap ' it to the first freq ( you can offset this using `` loffset `` if desired and / or `` label `` , which counts from the biggest stamp )

the final output would be a stacked bar chart , with each segment of the stack ( on the ordinate ) representing the count() of values in each bin , and the abscissa showing time values ( e.g. days ) .
In yesterday's puzzle , the abscissa showed the values and the series showed the times . I tried to transpose the matrix , but that led to nowhere . Groupby by two columns ( at least the way I attempted it ) also did not work .

So I have data that I am outputting to an excel file using pandas ' ExcelWriter . After the entire data is outputted to the Excel file , what is the easiest way to apply conditional formatting to it programmatically using Python ?
I want to be able to do the equivalent ( through Python ) of selecting ( in Excel ) all the filled cells in the Excel sheet and clicking on " Conditional Formatting " > Color Scales . The end result is a gradient of colors based on the values , a " heat map " if you will .
After the data is written , I need a way to apply the conditional formatting using python . To make it simple , I want the colors to be darker shades of blue the more positive ( > 0 ) the values are and to be darker shades of red the more negative the values are ( 0 ) and the cell to be white if the value is 0 .
Here is an example of how to apply a conditional format to the XlsxWriter Excel file created by Pandas : #CODE

pandas intraday 8Min resample bug ?

How to merge Series to DataFrame as columns , broadcasting
You could construct a dataframe from the series and then merge with the dataframe .
I suggest a small edit : ` df.merge ( pd.DataFrame ( data = [ s.values ] * len ( s ) , columns = s.index , index =d f.index ) , left_index=True , right_index=True )` . That way df doesn't have to have index like 0 , 1 , 2 ...
minor correction : ` * len ( df )` not ` * len ( s )`
Next , ` join ` concatenates this new frame with ` df ` : #CODE
your answer is 20x faster , but it's still a difference of ~100ms with df at 1e5 rows . My for loop is horrifically slow . BTW in your answer the ` 2 ` ought to be ` len ( df )` to be generally applicable .

I have data , in which I want to find number of NaN , so that if it is less than some threshold , I will drop this columns . I looked , but didn't able to find any function for this . there is count_values() , but it would be slow for me , because most of values are distinct and I want count of NaN only .
You should time it on your data . For small Series got a 3x speed up in comparison with the ` isnull ` solution .
Indeed , best time it . It will depend on the size of the frame I think , with a larger frame ( 3000 rows ) , using ` isnull ` is already two times faster as this .

Converting panda's dataframe group iteration into groupby with apply
I need to split a dataframe into groups , and for those groups that have odd number of lines , i need to pull in the first line whose column matches a certain condition and then i need to assemble back all such first lines ( so only the first ones in odd numbered groups matching a condition ) . I can do it in a loop like below ( it works ) but can't rework it into a groupby with apply . Could you help ? #CODE
Interesting problem which I would solve by writing a function which you then pass to apply .
Note that when calling passing a function to apply , the fist argument passed is the DataFrame itself and this is done so automatically . That is why you don't need to specify the ' df ' argument when passing the function to apply . In fact if you do you get an error saying you have passed too many arguments . Also somewhat strangely in my view when passing the function the arguments are supplied after commas rather than in parenthese . This I find confusing to look at , but it is what it is ....

Is this expected ? I know I can apply ` dropna() ` on this output , but isn't the above already supposed to filter out the values I ask for ? ( it typically works on dataframes wihtout having to call ` dropna `)

concat pandas DataFrame along timeseries indexes
transpose multiple columns Pandas dataframe
Apparently , there are indeed repeating client-order pairs on later dates , contrary to Note 2 above , which is what's messing my data ( so much for checking only 20,000 rows in a million , pfft ) and the pivot . DSM's answer ( and my initial hunch ) is correct that pivot is the solution .

There is a related question on SO , but it uses placeholders and search / replace functionality to postprocess the HTML , which I would like to avoid :

Pandas : Shift down values by one row within a group
Shift works on the output of the groupby clause : #CODE
default param for shift is 1 so not necessary to set ` shift ( 1 )` although it is clearer code by specifiying it
` df [ ' B_shifted '] = df.groupby ([ ' A ']) [ ' B '] .transform ( ' shift ')` .
The former notation is more flexible , of course ( e.g. if you want to shift by 2 ) .

I use this to get the vote totals and I apply a function to the group to get the number of precincts reporting and the total number . #CODE
This works great , but if there are duplicate precinctIDs , it doesn't count the number of unique precincts . So to handle all of the #ABS duplicate , I just replace them , like so : #CODE
But this doesn't solve the problem of duplicate PrecinctIDs - of which there are many - impossible to do a find and replace on every exception .
EDIT : I found the error happens before I use the .count() function . I am grouping the duplicate precincts before I count them , so I need to replace the duplicate precinct_name before I count them .

I think you can use the select method to apply a filter to the index : ` df_raw.select ( lambda r : r.lower in my_list_of_rows )`

Pandas : DataFrame too long after merge
Say I have to DataFrames , one longer than the other , that I want to join on a specific column , as in the following example : #CODE
Then I join them with : #CODE
However , I have two DataFrames that I'm trying to merge , with 28,011 and 15,676 rows , respectively . Merging them the same way as above , I would expect to get back a DataFrame with 28,011 rows and NaN in those cells where df2 had no observations . What happens instead is this : #CODE
You sound like you want ` how= ' left '` ? The outer join will also include observations in df2 with no much in df1 . Also , can you double check that ` col1 ` doesn't have duplicates ?
Sounds like you want a left join .
` len ( pd.merge ( df1 , df2 , left_on= ' col1 ' , right_on= ' col1 ' , how= ' left '))`
` len ( pd.merge ( df1 , df2 , left_on= ' col1 ' , right_on= ' col1 ' , how= ' inner '))`
My bad , the column I was merging on did actually contain some duplicate values , so for exact identification I needed to merge on two columns , in the end

To me this makes sense but it seems not to be working . Is there an easy way to make this work that I am unaware of ? The map is there because I needed to broadcast the df row into the new dataframe at several rows ( not just once ) .
Took me more time to create the dataframes then to merge them haha

I am working with a data frame that takes up roughly 2 Gb of memory ( according to htop ) with dimensions ( 6287475 , 19 ) . The data frame is heterogeneous in data type , which probably does not matter . Immediately after loading the data frame I drop duplicate rows using the command #CODE
What should * should * do in order to have a lower memory usage is this ( assuming you are starting with a csv ) . chunk-read with a reasonable chunksize ( say 1M ) , then drop-duplicates on that chunk ( depending on where your dups are this may remove a lot or just a little of the dups ) . Then concat the chunks . The drop_duplicates again .

` tshift ` requires a freq argument for this usage ( because the freq is potentially and usually not regular once you group ) , so ` df.groupby ( ' A ') .tshift ( -1 )` returns an empty frame ( it is raising for each group , slowing it as well ) . #CODE
Aside from this , this issue here is waiting for a cythonized implementation of shift ( and tshift ) too . Which would make this on par with sum , which is cythonized . Contributions welcome !
Thanks , Jeff ! See my updates below . My actual code had the freq indicated ; I just erroneously dropped it in the example . It looks like my actual issue is the number of groups ? n_A and n_B are actually 2k-500k and 121 , respectively .

But I wonder if there is a ( more pythonic ? ) way to do it with melt ?
as an FYI . this is a chained assignment and will not in general work ( and will show the `` SettiWithCopyWarning / Error `` , better to do : `` df.ix [ 0 : len ( X ) , ' seris '] = ' X '``
Thanks ! I forgot concat indeed , this answer is very explicit !

I cannot reproduce the error where the index is NOT a DatetimeIndex . Note that this is on 0.15.0 ( releasing today ) . And a lot of fixes went thru for tz types of things .
You are doing a really odd comparison that won't work anyhow . In order to select / reindex these have to be the same tz , otherwise it doesn't make any sense .

I have tried to do a for loop on distinct cat names and then insert the item . That didn't really work but is there a more pandithic ( ? ) way

Thank you , i had a do an insert of column to the new data frame and was able to achieve what i needed thanks
Thank you , i had a do an insert of column to the new data frame and was able to achieve what i needed thanks df2 = pd.DataFrame ( series.apply ( lambda x : pd.Series ( x.split ( ' , '))))

I see so if I do not append the data columns when I store it using ` to_hdf ` it would not work if I use ` store [ ' table2 '] = df_tl ` either . I am surprise pandas does not automate the ` data_columns=True ` .

So in fact ` mean ` , ` median ` , ` sem ` , ` std ` , ` var ` and ` ohlc ` will all raise an exception .
Compare what happens when you call apply with ` mean ` : #CODE

Merge pandas dataframe , with column operation
In that case you may have to merge / concat all the dfs together and then sum all the columns where there are clashes ignoring the ` NaN ` values
You should post as an answer if it works for you , none of merge , join , concat will do what you want because essentially you are not merging data you are performing an operation so it's distinctively different so I can't see how this could be done in a single pass

Pandas report top-n in group and pivot
2 ) Now next problem is that I want to to pivot the top 5 ( ie so I have one row for each element of d1 )
so to pivot I have to create the ranking along d2 ( ie 1 to 5 - this is my columns field ) . This would be easy if I always had 5 entries , but occasionally there are fewer than 5 elements of d2 for a given value of d1 .

So I have initialized an empty pandas DataFrame and I would like to iteratively append lists ( or Series ) as rows in this DataFrame . What is the best way of doing this ?

How to check if the signs of a Series conform to a given string of signs ?

Add calculated column to a pandas pivot table
I have created a pandas data frame and then converted it into pivot table .
My pivot table looks like this : #CODE
I was wondering how to add calculated columns so that I get my pivot table with Autopass% ( ` Autopass ( cb ) / TotalCB*100 `) just like we are able to create them in Excel using calculated field option .
I want my pivot table output to be something like below : #CODE
How do I define the function which calculates the percentage columns and how to apply that function to my two columns namely ` Qd ( cb )` and ` Autopass ( cb )` to give me additional calculated columns
Clearly some code has been written @USER : You don't get to the stage of making a pivot table in pandas without some code .
Thanks Oxinabox . @USER Yes I imported the data using read_csv into a dataframe and the used the pivot_table function to create the pivot table .

` df1 ` be some transform of ` df0 ` such that ` len ( df1 )` equals ` len ( df0 )` , and ` df1.columns ` equals ` df0.columns ` ;

How to apply a custom formula over a group element in a grouped.apply() .unstack() method ?
Looks like when you group the dataframe it returns a bunch of series due to your original only having two columns . ` applymap ` is a Dataframe method that applies a method element wise . It looks like you are looking for apply in this instance . Try the following ... #CODE

Combine Python Pandas Dataframe more Efficiently than List Append Method

The map ( str , row ) step is needed so Orange know that the data contains values of discrete features ( and not the indices of values in the values list ) .

Finding the intersection between two series in Pandas using index
I have two series of different lengths , and I am attempting to find the intersection of the two series based on the index , where the index is a string . The end result is , hopefully , a series that has the elements of the intersection based on the common string indexes .
Pandas indexes have an intersection method which you can use . If you have two Series , ` s1 ` and ` s2 ` , then #CODE
Tried common = s1.index.intersection ( s2.index ); printing len ( common ) gives 0 .

The problem is that the date and time columns are read in as int64 . I would like to merge these two to a single timestamp such as : 2013-06-25 07:15 : 00 .
And then I'll want to merge into a single DatetimeIndex .

I try to find the best way to get the first value ` just before ` and the value ` just after ` ` value_to_find ` in the column ` A ` , and then return column ` A ` and ` B ` . And then interpolate value_to_find to get the ` B ` value .
Is there any better way to do it ? Maybe using query , map or something similar .
I am not sure what you mean with quantities . I guess that's your notion of using also units rather than just numbers . However , I wouldn't mix the concepts here . Keep the units in the name of the column . What prevent's you from using m in your first row and m*m in the second row , etc . If you are using plain numbers you can easily call interpolate and you will have numerous options at hand for that ....

I have a dataframe with a datetime64 column called DT . Is it possible to use groupby to group by financial year from April 1 to March 31 ?
and then apply a method to the groups or whatever you want to do . If you just want these groups separated call ` grouped.groups `
Or if you use Date as an index of DT , it is even simpler : #CODE
But beware that ` shift ( -3 , freq= ' m ')` shifts date to ends of months ; for example , 8 Apr to 31 Jan and so on . Anyway , it fits your problem well .

Pandas creates duplicate index entries on merge
When merging data sets , I would expect pandas to not duplicate the values in the columns in the join . However , this does not appear to be the case .
Also , the multindex created from the join is stated as being unique , but a groupby suggests that it is not .

I think , not ` asfreq ` but ` resample ` fits your needs : #CODE

Do I have to make a new dataframe for each year group and merge them ?
Really it's just the second half of my post that is the point , I've just been trying several ways and can't figure it out . I got all the years into a list of frames but when I do the join no data gets added of course because while the day and month may line up the year does not .
There is no merge to be done . All your data is date-specific , just " stack " the data behind each other , and then make sure you group by month , day to get the yearly correlation ( for the same day )
If you want to apply additional filtering , with this index , you can even select a specific year using ` df [ ' 2013 ']` .

Drop multiple columns pandas
I am trying to drop multiple columns ( column 2 and 70 in my data set , indexed as 1 and 69 respectively ) by index number in a pandas data frame with the following code : #CODE

I'm trying to join two dataframes with dates that don't perfectly match up . For a given group / date in the left dataframe , I want to join the corresponding record from the right dataframe with the a date just before that of the left dataframe . Probably easiest to show with an example .
then a regular join or merge between `' join_date '` on the left and `' date '` on the right will work . You may need to tweak the function to handle Null values or other corner cases .

this gives me an error " cannot join with no level specified and no overlapping names "

It's still quite misleading . Imagine if this happened when you called ` to_dict ` . You could just as well say that you are " leaving Pandas " and going back to " pure Python " , and then apply some type conversion on the values that will be ` dict ` values . Then ` o.head() .datetime .to_dict() [ 0 ]` would be different than ` o.head() .datetime [ 0 ]` . In any of these cases , if you are asking for some iterable thing that has values in it as a sequence ( whether dict , Series , or ndarray ) , you expect the entries to be references to a single value in memory . You don't expect to get a different value .

To do this , you would group the data and use the ` apply ` method to apply a function that does the above .
And then pass that function to ` apply ` using the groupby object as follows : #CODE

In Pandas , how to apply 2 custom formulas in a groupby.agg() method ?

The apply functionality is probably what your are looking for : #CODE

Typically when comparing rows you want to use the shift method #CODE

How to concatenate within ' apply ' on a grouped object
I have a dataframe where I wish to edit the information in columns a and b , within groups defined by columns d and e . The procedure to apply to a and b is : set all rows equal to the row where c is a minimum . Columns c , d and e must remain unchanged .
I am using an apply function on a grouped object . I use ' reindex ' to change a and b . The difficulty comes with concatenating , " cannot concatenate a non-NDFrame object "
You provided strings to the ` concat ` function instead of dataframe objects : In the ` wvmexp ` function do : #CODE

You can use ` loc ` as a boolean mask to assign just to the rows that meet the criteria , even for such a small df it is faster , for a larger df it will be significantly faster : #CODE
Interestingly for a 50,000 row dataframe , the ` loc ` method outperforms the nested ` np.where ` method : I get 4.24 ms versus 12.1 ms .

I wanted to know if there is a way to make the dataframe insert an index column automatically .

@USER yes - it could be generalised by using ` & ` and changing the shift value , e.g. ` df [( df.p ! = df.p.shift ( 1 )) & ( df.p ! = df.p.shift ( 2 ))]`

This has been the bane of my life for the past couple of days . I have numerous Pandas Dataframes that contain time series data with irregular frequencies . I try to align these into a single dataframe .
Why am I getting this error ? Even if this worked , it is completely manual and ugly . How can I align > 2 time series and combine them in a single dataframe ?
Your specific error is due the column names of ` combined_1_n_2 ` having duplicates ( both columns will be named ' price ') . You could rename the columns and the second align would work .
One alternative way would be to chain the ` join ` operator , which merges frames on the index , as below . #CODE
Correct , ` join ` handles the index logic for you .
There aren't too many practical cases I can think of , but if for some reason you need to only align the index , you'd save a little time by using ` align ` .

I need to resample some data with numpys weighted-average-function - and it just doesn't work ... .

Either apply a regex pattern or apply a function that returns the characters you want , even if length did work , it'll just return the length of each row which is no different to doing nothing . You've not clarified the requirement , are you just wanting numbers only ? Is the number length fixed etc ..

docs , so I'd use numpy to get a " windowing " view on the array and apply a ufunc
Strides are number of bytes between two neighbour elements along given axis ,
thus ` strides =( arr.shape [ 1 ] * isize , arr.shape [ 1 ] * isize , isize )` means skip 5
Yep - though really when I see this it reminds me that if I'm going to have to count bytes then why not go the whole way and do C or Fortran . This is a great little intro to the strides though so I thank you , immerr . Sometimes going lower level is so refreshingly rewarding .

How can I keep the intersection of a panel between dates using Pandas ?
I need to keep only the rows where the IDs are the same for consecutive dates . I'm trying to calculate returns for a portfolio that changes every month and the only coherent way to do it is take the intersection of securities for consecutive dates and take the difference of the sum of those prices . I tried to filter the dataframe by iterating through the dates , but it wasn't fruitful . Here's my attempt ( ' hol ' is my original dataframe and ' dates ' is a list of unique dates in ' hol ') : #CODE
Group this new dataframe on ID , and use the ` shift() ` method to get the differences in the stock prices using the ` apply ` method
Thanks a lot for responding . This doesn't work b / c ' date ' and ' ID ' aren't aligned . date is a block vector of the entire date range repeating every len ( date.unique() ) rows and ID is a block vector of the entire range of IDs repeating every len ( ID.unique() ) rows . The list comprehension for one or the other has to be modified . Unless I made a blunder . It looks like something funky happens either when the index for DF2 is set or when the dataframe is declared for DF2 .

An alternative method which is likely to be even faster is to use ` loc ` and your condition to only strip the last character where it contains a trailing s , all other rows are left untouched : #CODE
No , of that you'd have to apply a regex or a lambda to test each word for that character and strip it

Drop all the rows for such dates that are earlier than everything else
` axis=0 ` drops rows , ` axis=1 ` will drop columns , sorry the meaning of axis changes depending the on the operation
NaN , so I assume the change would be to use isnull ( frame.A ) ?

I've been able to create a boxplot using the boxplot() function built into pandas ' DataFrame by doing : #CODE
Beeswarm plot will very nice in this situation , especially when you have a lot of dots and what to show the distributions of those dots . You need to , however , supply the ` position ` parameter to ` beeswarm ` as by default it will started at 0 . The the ` boxplot ` method of ` pandas ` ` DataFrame ` , on the other hand , plots boxes at x = 1 , 2 ...

Python , how to merge 2 pandas DataFrame
How can I merge the two of them to have this similar output ?
If the lengths are the same you can just concat ` pd.concat ([ df1 , df2 ] , axis=1 , ignore_index=True )`
You can just ` concat ` in this case : #CODE
` join ` would also work : #CODE

Then merge the sub-tables back together in a way that replaces NaN values when there is data in one of the tables . I tried : #CODE

Strip time from an object date in pandas
You can apply a post-processing step that first converts the string to a datetime and then applies a lambda to keep just the date portion : #CODE
Create a new column , then just apply simple ` datetime ` functions using ` lambda ` and ` apply ` . #CODE
That error should only occur if you have a date whose year is below 1900 . Otherwise , ` strftime ` is smart enough to convert it as shown above without errors . Are you sure your values are correct ? Try exporting to CSV right before doing the ` apply ` . A date might have been incorrectly converted from ` Created Date ` .
Seems to be working . Is there a command to drop the rows where the date is not a date ? My fear would be that a row would have a different value ( like an EmployeeID ) in the date column which would make it seem like it was not null .

There is no need to transpose your DataFrame ; just use the ` axis=1 ` argument to ` cumsum ` .

I dont want to remove the index or set new indexes . it messes up the format by rearranging everthing in a chronological manner . is there any other way ? perhaps using concat or merge ?
Yes , merge would be the way . I have up-voted the other answer for that reason .
It sounds like you need to merge the two DataFrames on the column ` n ` : #CODE

combine = concat ( df , ignore_index=True ) #I know something like this needs to be created but I am not sure how #CODE
Are you struggling just with the writing out portion ? you don't need to concat if you are just wanting to write out the modified portion . Essentially you just write out the first chunk as normal and then on each subsequent chunk add the params to ` to_csv ` : ` mode= ' a ' , header=None ` this will append to the existing csv and not write the headers out again as you already did this the first time .

Currently , I'm doing this through a ` for i in xrange ( len ( df2 ))` loop , checking if ` df2.A.iloc [ i ]` is present in ` df1.A ` , and if it is , I store ` df1.A , df1.B , df1.C , labels [ i ]` into a dictionary with the first element as the key and the rest of the elements as a list .
I posted some example data above . df2 is basically only being used so I know which relevant element of ``` labels ``` I want to join with a row of ``` df1 ``` . But yeah , you understood what I'm trying to do correctly .

The line of code to replace ` inf ` , etc ., now throws the following error : #CODE

You can pass a function to apply , but it shouldn't return a dict . If you want to add a sum column your way - do it like this : #CODE

I've designed this method so I can apply any function I wish to the table at any point . It needs to stay this flexible , but it just seems horrible ! Is there a more efficient way to do something like this ? Is iterating over groups + appending better ?
Thanks . I was aware of this type of solution . But the problem is : I may need to apply an aggregate function which needs access to more than one column ( table manipulation ) . And I will need to apply more than one of these in batch to different data !

You mean ` NaN ` ? ` df.loc [ len ( df )]= [ 2.0 , ' James ' , NaN , 96 , 87 ]`

But how are you going to ' align ' the data ? Say , if the ` nan ` s are not interleaved by a fixed step ? Since your data appears to a time series , maybe downsample it to every minute ( or other ) ? Of course if the alignment is not an issue , @USER ' s solution will work .
Any chance of a little spoonfeeding ? Not sure how to apply this to my dataframe ...

How to resample a dictionary of dataframes ( Yahoo Data ) in Pandas ?
What makes you say that ?. Doesn't the fact that the resample works for a single dataframe invalidate your point ?
@USER Thanks for the input . But I can run the resample with the same conversion as a dict . And it works fine . But for a single df . It's been used like that forever see this recipe #URL
well when I correct your loop , it works for me . What makes you think it's not working ? You're not reassigning the results back into ` z ` , so the whole process will be very ephemeral . You can replace your last line with ` z [ key ] = df.resample ( ' BQ ' , how=conversion )` to save the results .

Normalize rows of pandas data frame
Any idea how I can normalize rows of this data frame where each value is between 0 and 1 ?
there is an apply function , e.g. frame.apply ( f , axis=1 ) where f is a function that does something with a row ...
You can use the package sklearn and its associated preprocessing utilities to normalize the data . #CODE

Pandas passing wrong dtypes when calling apply
It seems to cast all values to ` bool ` , unless I " touch " the ` DataFrame ` by adding a new column . This happens regardless of whether I use row or column-based ` apply ` ( i.e. ` axis=0 ` or ` axis=1 `) .

You can read them into multiple dataframes and concat them together afterwards . Suppose you have two of those files , containing the data shown . #CODE

Why not just set them to floats and then do a diff ? Do you need them to be decimal ? Can you post your raw data , the csv parser should be able to sniff the dtype if it's stored like so : ` 19.2222 ` but if any values on that column are different then it may have mixed dtypes or if it's stored as a string like `' 19.2222 '` then you need to convert them

Pandas DF Pivot / Transform / Vectorize Operation
Use ` unstack ` might be faster : #CODE

The problem comes from the ` samples = data [ idx ]` line . I suspect I need to be more specific than using ' grouped ' for the data field in bootstrap , but I am unsure how to do this . Do I need to apply this as a lambda function ? Or maybe with a for loop ? Any suggestions would be much appreciated .

You can use ` join ` , telling pandas exactly how you want to do it : #CODE

Python Pandas pivot_table missing column after pivot
After pivot what I want is following data frame : #CODE
So far I have done following to to build the above matrix but I am unable to get the ` expert_level ` column as shown after the pivot .
Then join your pivot and lookup #CODE
If you also want to include ' total_min_length ' , you can do a second pivot : #CODE
and join all three instead of two : #CODE
@USER -Hypothesis that error occurs when you join two frames with some column names that are the same , and haven't specified the ' lsuffix ' and ' rsuffix ' parameters . The offending column is ' expert_level ' , which means you've probably joined the ' lookup ' frame in twice by mistake .

Pandas : apply different functions to different columns

As you can see , loc is not a method but a property , which then in turn has its own get method . So the answer is , it's complicated .
` loc ` is a property that creates returns a name called ` _loc ` if its not ` None ` else it creates a ` pandas.core.indexing._LocIndexer ` on demand . Indexers , by default have access to the DataFrame that created them , so you can modify the DataFrame on a key miss .

The same thing happens when I try to resample : #CODE

Thanks Jeff . I've tried the HDFStore ( append ) method and find it is massively slower even in my small test sample ( pytables < 10secs , HDFStore=60secs ) . I guess I'm going to have to stick with pytables and filter / convert dates before processing just to be efficient . BTW I did group my data in batches before appending but this doesn't help much .
I might have to re-visit that as when profiling I find it spends 99% of the time in the HDFStore append . Using Pytables I split each record ( 2MB ) into multiple tables ( 80-120 bytes each row ) and append to each group / table and it takes around 1 sec per record . For HDFStore I group the data into an array before appending to the HDFStore tables . - there are around 60 tables for each 2MB record and only 10 records in my sample .

This is NOT true for resample however as it requires a monotonic index ( it WILL work with a non-monotonic index , but will sort it first ) .
I meant I couldn't figure out how to pass ` nth() ` as one of the functions sent in the list to ` agg() ` . You can't do ` .agg ([ np.mean , nth ])` , or ` DataFrame.nth() ` or ` lambda x : x.nth ( 2 )` . That's what led my to iloc , though it will throw index errors . The best way is probably to not try to do it all in one step ; first use ` nth() ` then use ` agg() ` , then merge them .

It shifts the Date to the second month of the year . If you want to shift the date for 2 months you have to use : #CODE

At this point , I should be able to use something like an " apply " function on ` df ` using ` months ` , but am a bit lost ...
I think a way should exist to avoid the use of apply but I didn't find it .
Your solutions do not work precisely because you sort by the entire date triples ( Y , m , d ) and * do not * perform the parsing necessary to sort by pairs ( Y , m ) . To do the latter , I think groupby ( or join ) may be best . Also note : on pandas 14.0 and with the setup i mentioned , your edited solution does not work unfortuntaley : TypeError : cannot astype a datetimelike [ datetime64 [ ns ]] to [ int32 ] .
I think ' apply ' is a good way to go .

Is there a solution that works for all three cases ? Or is there some way to apply a condition in string splits , without iterating over each row in the data frame ?
What are you splitting the string on ? Will your longest-string approach break on viruses like A / New York / 107 / 2003 ? Depending on where you access the viruses from , this may or may not be a concern , because some databases offer the option to replace spaces with " _ " .

Element-wise median of a lot of matrices , python pandas
I want to essentially make a list of every i , j component in a dataframe across keys and take the median of all of those . You can think of this as stacking the matrices on top of each other and taking the median value for each i , j element . I hope I explained this clearly enough .
I was wondering if there is a clever way to do this . I would like to avoid making the list of n ( n+1 ) / 2 unique i , jth pairs and then taking the medians , then putting them back in their proper place in the final median matrix ( dataframe ) .
In general , the median requires all of the data in memory , unless you only want an estimate . Therefore , for a large amount of data , you'll have to work in chunks : #CODE
Ah yes , you cannot have a median when some values are NaN ; unless of course you want a median of all the non-NaN values .
@USER that only works for median of a series , right ? I need median of a list of matrices .

I am looking at a group of temporary employees in a DataFrame . I am using Pandas and I need to drop duplicates within the set for each person . So for Greene , I would only want one unique date from the ` apnt_ymd ` column . There are two ` 2012-04-08 ` dates in the set and I would only need one .
How can I use the ` drop_duplicates ` method to remove all the duplicate dates for each persons set of records ? Or is there another way - such as apply ? #CODE

This will pivot your df ( which is in the long format ala ggplot style ) into a frame from which pandas default plot behavior will produce your desired result of one line per network type with index as the x-axis and count as the value .

Median of a list with NaN values removed , in python
Is it possible to calculate the median of a list without explicitly removing the NaN's , but rather , ignoring them ?
I want ` median ([ 1 , 2 , 3 , NaN , NaN , NaN , NaN , NaN , NaN ])` to be 2 , not NaN .
You mention list but tagged this as pandas , for a series by default calling ` median ` on a series will ignore ` NaN ` values : #URL
I would clean the list of all NaN's , and then get the median of the cleaned list . There're two ways that come to mind . If you're using the numpy library , you can do :
where ` x ` is the list you want to get the median of
Then to get the median just use the cleaned list : ` median ( x )``

Python pandas insert list into a cell
I want to insert the list into cell 1B , so I want this result : #CODE
because it tries to insert the list ( that has two elements ) into a row / column but not into a cell .
I want insert the ' abc ' list into ` df2.loc [ 1 , ' B ']` and / or ` df3.loc [ 1 , ' B ']` .
Thank you ! I use Python 2.7 and I tried pandas 0.14.0 and 0.15.0 and it worked with the test data above . But what if I have a ' C ' column as well with some integer values ? ' A ' has strings . Having an integer column and a srting column I get the same error : ValueError : Must have equal len keys and value when setting with an iterable

There's probably a way to avoid the int ( bool ) shenanigan .

Proper way to merge DataFrame columns in Pandas ?
While this works , I wonder if there are other , faster , built-in ways of dealing with this case ? The reason I do this is because in different datasets I have different numbers of columns with related info that I want to merge into one column / row so that I can do frequency / mean calculations on them .
This seems fine to me as you are concatenating all your dfs in one go , if you did a join or merge you end up repeatedly joining / merging and each time you allocate space for the additional rows / columns . I don't know for instance if you could directly assign new columns to a master df something like ` df [ ' new_col '] , df [ ' another_col '] ... = other_df [ ' new_col '] , another_df [ ' another_col '] .... ` etc .. however this approach would require that the indices align which may not be true , in any case I think concat is appropriate

I was also just able to find use of the " loc " command - again , not quite what I want to do but :

Is there a memory efficient way to replace a list of values in a pandas dataframe ?
I am trying to replace all of the unique strings in a large pandas dataframe ( 1.5 million rows , and about 15 columns ) with an integer index . My problem is that my dataframe is 2Gigs and my list of unique strings ends up with around eighty thousand or more entries .
Have you tried ` df.map ( replacedict )` ? Or you could create a series using the unique strings as the index and the integer values as the values and again call map pass this series
Thanks , it looks like the DictVectoriser will add some serious memory overhead to this dataframe , so I am not sure that would be the best way to do it . I will try to use the applymap method tonight , but I am sceptical that it would be any better than replace .
I think your best bet is to just create a Series with the unique string values as the index , then just assign an increasing int value for the series and then use this as the param to the ` map ` call : #URL this uses merge but you could just call map if this was a Series
I made something that works , but there is probably a better way to do this . For each column in my dataframe ` df [ column ] = df [ column ] .map ( mapdict )` . ` apply ` doesn't seem to work on a dataframe . Thank you !

I have data that looks like this in a dataframe . The data ranges from 2010-01-01 up until the year 2020 and beyond . This data is on a daily regime but needs to be converted into monthly intervals . The data corresponds to outages ( how much of a substance is offline per day ) for a given date range . To do this , I need help breaking down the end points of months so that they span the correct months then I will resample the data using resample ( ' M ') in pandas .
After this , my plan is to resample the data using df.resample ( ' M ') by resampling on the ' start_date ' and ' offline_total " columns so that I have an accurate picture of how much of this substance is offline given a month . I want to resample this into all months from 2010-01 to the end of the dataset , throwing out any data that does not fit this range and filling in 0s for months where there is no original daily data .

Very slow single pandas apply / groupby call
I have a small dataframe ( 200 * 19 ) . I want to apply a function to each row . There's no sub-loops . I've tried using groupby and row apply : #CODE
Incidentally , I don't see the use of the groupby version . If you want to apply a function to * every * row , groupby doesn't make sense .
was just using it for clarity / possible future abstraction . at the time , each row is it's own round , so each group is a df with one row . This might not always be the case , so the row apply would only be a temporary fix ( but it's not faster , anyway , so I guess not )

Excellent ! pivot is the key piece I was missing . Thanks !

As a work-around I'm trying to do the same thing filter does with groupby and apply but it doesn't work as expected . Any suggestions ? #CODE
The problem with your example code is that the ` apply ` doesn't know what to do with the ` None ` when putting the dataframe back together . Your ` apply ` function needs to output the same type of object every time . If you return ` pd.DataFrame() ` instead of None you should get what you're looking for .
if len ( x ) > 2 : return x

not sure if I understand your question , but you can search for year startswith 1970 and replace with 2013

I did not play around with that approach long enough to figure out how to " enlarge " batches of indexes all at once . But , if you figure that out , you can just " enlarge " the original data frame with all NaN values ( at index values from ` DatesEOY `) , and then apply the function about to ` YTDSales ` instead of bringing ` output ` into it at all .

Before trying anything clever , I find it helps to spell out the straightforward version . Using only boolean comparisons , ` shift ` , and the fact we can sum boolean columns to get the number of Trues because ` int ( True ) == 1 ` , we can do : #CODE

I know I can do this in separate lines with ix , i.e. : #CODE

I came up with this function , using ` apply ` : #CODE

Excellent , thanks @USER that sorted it out . The pandas site doesn't seem to have any documentation on loc . Busy trying to research more about it .
Thanks again @USER .I was aware of loc but couldn't find any documentation on it to actually learn exactly what it does . Busy going through your link now .

then I apply the multi-indexing and unstacking so I can plot the yearly data on top of each other like this : #CODE

Tableau - blend , join , or modify raw ?
Question : Do I use a blend between the files ( there are about 8 different data sources right now ) , a left join , or do I just use python / pandas to literally add the information to the raw data source ? I want the twbx file to be as fast as possible for the end users .
Data blend ( creating a relationship between different datasources in Tableau ) is the worst performing solution . While it is easy to use , it's highly inefficient , because it will virtually perform the join every single time a calculation is made .
Performing a left join on Tableau when connecting to data ( I'm assuming you're using csv files ) is a very good solution , as is making a single table with the information in Pandas . For the final user , there shouldn't bet any difference in performance ( especially if you extract the data to tde in Tableau ) . Here you need to measure if it's easier for you to maintain a process in python / pandas or in Tableau extracting tool . I believe it's simpler to have Tableau doing the join . But it's your call
So perhaps I will add a file called RosterDatabase which has the daily ID+Date figures and the corresponding information ( manager , location , etc ) . Then I will left join that with my larger raw files which have the ID+Date unique ID . Does that seem like a fair solution ?
Server side joins are in fact more efficient than client side data blends , but data blends can be very useful and reasonably efficient when used well . Blending is one of the few options available for combining data drawn dynamically from different sources , say comparing a spreadsheet to related data in a database . Blending can cause some complex behavior behind the scenes and you have to pay careful attention to the blending fields ( analogous to join keys ) . It's an advanced feature . Not for every situation , but very useful when needed .

Time-series boxplot in pandas
How can I create a boxplot for a pandas time-series where I have a box for each day ?

To be clear : 1 . using fromtimestamp() without specifying timezone + replace ( tzinfo ) as you do is wrong if the local utc offset is not the same as the offset in the input date it does not depend on Python version or anything else 2 . The format is referenced by name in other places that interpret it as HHMM I.e. , it is much more likely ( I'd say , I'm certain ) that it is an error in the docs .

Oh , I see . The problem is due to ` values ` being a list of * strings * instead of a list of floats . A quick patch would be to define ` df_new [ ' values '] = df_new [ ' values '] .str .split ( ' , ') .apply ( lambda x : map ( float , x ))` . But really , this is just a band-aid pasted over a bigger problem . You shouldn't be forming a DataFrame with a column which is a comma separated list of numbers . If that's the way the data is stored in the database , then the database needs to be fixed . It should be storing numbers as numbers , not strings .

Basing it on the CSV you uploaded , the issue here is that your dates are of a format that is not readily convertible to datetime . You need to truncate the first two and the last characters before converting . Following code works and provides the correct target value . #CODE

Combine two Pandas dataframes , resample on one time column , interpolate
I'm trying to merge df1 and df2 , interpolating y2 on df1.t . The desired result is : #CODE
I figured out one possible solution : interpolate the second series first , then append to the first data frame : #CODE

Groupby or pivot in pandas ?
AttributeError : Cannot access callable attribute ' to_csv ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method

I am familiar with the ` df [ column_name ] [ index_name ]` , but I don't think this addresses things . Perhaps I can mix this with the transpose function , but then I lose track of datatypes , right ?

Or you could create a custom function via the map tool :

you can upgrade to 0.15 now with conda ; the hrs is trying to align the data ( in the add ) so u can specify .values ( for each term ) to simply ignore the alignment

I'm trying to replace one record of pandas DataFrame ( df ) with the latest version of that label ( found in a separate DataFrame ( latest_version ) . #CODE

Just do it step-by-step , the only non-trivial part is the ` unstack ` -call .

I can think of two ways , setting or assigning to a new column just the ` date() ` attribute , or calling ` replace ` on the datetime object and passing param ` hour=0 , minute=0 ` : #CODE
The fastest way I have found to strip everything but the date is to use the underlying Numpy structure of pandas Timestamps . #CODE

Resample error : ValueError : month must be in 1 .. 12
I have a .csv file that I would like to resample at 1 minute granularity .
#URL Try using the intialization part of the documentation examples with your resample method . Print their data after it is initialized and compare it to yours for formatting .

@USER -Hypothesis my original answer was incorrect , have corrected it now , I now apply row-wise dividing each item by the row sum , the values seem correct to me now
you can replace the cols with ` result.loc [: , ' 0 ' : ' 13 ']`

I would like to insert a column ' Interval ' which shows these intervals like this . #CODE

If I apply the rule in the df example , the output should be : #CODE
@USER without adding the ` [: : -1 ]` at the end , index of ` mask [: : -1 ] .cumsum() > 0 ` does not align with the dataframe . the warning basically says that pandas will do ` reindex ` to align the indices . see [ here ] ( #URL ) for some examples of alignment .

Thanks for the example . In my case , there are around 30 columns : ' measure1 ' , ' measure2 ' ... ' measure30 ' . However , I only want to apply sum() to ' measure1 ' ... ' measure20 ' . is there a way df.groupby ([ ' dim1 ']) [ ' measure1 ' , ' measure2 '] .sum() can be written without having to write all 20 column names . This is important because some of my column names are generated programatically , and I do not know their names beforehand .

I need to apply the formula above for each chunk . So following this recipe I tried : #CODE
You just need to add the param ` axis=1 ` : apply ( lambda row : func ( row ) , axis=1 )` then whatever your func does access the columns of interest : ` def func ( x ): return x [ ' A '] + x [ ' C ']` as an example

pandas : how to merge 2 dataframes on ' X ' from df1 and f [ ' Y '] from df2
i want to merge 2 dataframes df1 and df2 ,
using right_on : result of function f apply to col ' Y ' from df2 .
Why not just add the column , do the merge and then drop the column ?
You can pass ` left_on ` and ` right_on ` arguments to merge : #CODE

Now we can merge the dfs : #CODE
Obviously replace ` df ` , ` df1 ` with ` frameA ` and ` frameB ` in your case
here's a little trick to normalize the times : df [ ' Date '] + pd.offsets.DateOffset ( minute=0 , second=0 ) .
Then , they will merge on the exact same index , and since they have similar columns ( ' Buy ' , ' Sell ') , we need to specify suffixes for the merger :
merge = frameA.join ( frameB , lsuffix = ' A ' , rsuffix = ' B ')
The advantage of this approach is that if your second data set ( ' Buy B ' , ' Sell B ') is missing times present in the first slot , the merger will still work and you won't have data misassigned to the improper time . Let's say we have an arbitrary numerical index from 1-10000 for both , and the second dataframe is missing 3 values ( index only goes from 1-9997 ) . This will cause a shift , and then we improperly assign values to the wrong index is the one guiding the joining .
Then , you merge the dataframes : #CODE
One way to check is to print the dataframes to file before and after , or to run this in interactive mode . If they are truly moved to the index , they should only show up on the left and should not have an associated column . Then with the merger , they should merge on this index , and there should be only one index .
Another way is you can always just drop the column after everything is done :

then you will prob need to have to drop down to pure numpy / write your own code

I feel like this should be a simple task , but currently I'm thinking of reading it line by line and using some find replace to sanitise the data before importing .

How do I drop these rows ? I can see the ones with errors when I use df.sort ( ' Datetime ') . I just want a way to drop these quickly .
The date column should always have a value ( notnull )
Can someone help think of a way to solve this problem ? ( I think this date thing is the key issue because I have formulas which subtract StartDate from EndDate .. if one of those contains a word then it messes up the entire process . Maybe I can create some error exception or drop error rows ? )
Can I drop specific rows based on the default index ( like 362739 , 781146 ) . The good_rows covers most errors , but I see a couple which are definitely wrong even though the ' TransID ' column is correct.These columns are concatenated with / symbols . Can I say that certain columns should not include a ' /

basically what you're asking can be done either by using 3 ` loc ` statements or a bunch of nested ` np.where ` statements but it's better you show us real input data and desired output data
the elif / else is done with the loc / update , since these cases are distinct .

I figured you can get to the minutes by executing this line : mins = np.array ([ data [ ' duration ']] , dtype = " timedelta64 [ m ]") [ 0 ] . How can I append the values I receive in this array to my original data frame ?

one thing you have to take care with is if a string in searchfor has special regex characters ( you can [ map with re.escape ] ( #URL )) .

And to drop the index names : #CODE
Thanks ! The A -> B value and B -> A values seem to have been interchanged , can pivot fix this ? i.e. A -> B should be 23 and not 19
never mind , I can always transpose again :) Thanks !

Can anyone suggest a way to apply a function sequentially , so not only using the last calculated value of the column being computed but also using the present and past values of other columns in a pandas dataframe .
This is very unclear . In what way do " Start " and " End " columns mark an event ? Spell it out to us . Anyway it sounds like " Event " needs to be a vectorized logical-or of ` diff ( df$Start ) & diff ( df$End )` . But we can't know until you explain Start , End columns .
@USER : Thanks for responding , I have added clarification as asked for . The crux of what I am trying to do is to be able to apply a function such that it has access to current values of all columns and past calculated values of the current column .
The tools you want are ` diff ` and ` cumsum ` . You also need to do some boolean calculations .
` diff() ` will give you both positive and negative diffs ; since you're only interested in the earliest positive change , then compare the output of ` diff ( ... ) == +1 `

So the way I would approach this is to try to convert the columns to an int using a user function with a ` Try ` / ` Catch ` to handle the situation where the value cannot be coerced into an Int , these get set to ` NaN ` values . Drop the row where you have an empty value , for some reason it actually has a length of 1 when I tested this with your data , it may work for you using len 0 . #CODE

How to insert empty excel dates into oracle with Python+Pandas ?
I've got a Python application that is using pandas to grok some excel spreadsheets and insert values into an oracle database .
How do you insert your date value in the DB ? What is the type of the date column ?
Welcome to Stack Overflow . Please see my answer , and if you have any outstanding questions , please comment it on my answer :-)
And , regarding the ` NULL ` values , unless you have ` NOT NULL ` constraint , I don't see any issue with loading . The ` NULL ` values would anyway loaded as NULL . If you want to manipulate the ` NULL ` values , use ` NVL ` function and use the desired value you want to replace the NULL value with .
Anyway , one I was able to come with is to add a ` BEFORE INSERT TRIGGER ` to fix incoming timestamps . For that to work , you will have to manually create the table first . #CODE
After that , from Python , using ` pandas.DataFrame.toSql ( ..., if_exists= ' append ')` : #CODE
Beware that , if you ever need to rewrite an other DataFrame to the same table , you will first need to delete its content -- but not drop it , otherwise you would loose the trigger at the same time . For example : #CODE

I suppose you are using pandas 0.15 . ` PandasSQLAlchemy ` was not yet really public , and was renamed in pandas 0.15 to ` SQLDatabase ` . So if you replace that in your code , it should work ( so ` pdsql = pd.io.sql.SQLDatabase ( engine , meta=meta )`) .

How do I append an existing column to another column , aligning with the indices ?
Currently , when I attempt to append them together , I get NaNs and the same indices are duplicated . I created an empty dataframe so that I can put all three dataframes into by append . Maybe this is wrong ?
DataFrames have a ` join ` method which does exactly this . You'll just have to modify your code a bit so that you're calling the method from the real dataframes rather than the empty one . #CODE

I have to resample my dataset from a 10 minutes interval to a 15 minutes interval to make it in sync with another dataset . Based on my searches at stackoverflow I have some ideas how to proceed , but none of them deliver a clean and clear solution .
Using two forms of resample , with ` close= ' left ' , label= ' left '` and ` close= ' right ' , label= ' right '` . Afterwards I could average both resampled forms . The results will give me some error on the results , but smaller than the first method .
Resample everything to 5 minute data and then apply a rolling average . Something like that is apllied here : Pandas : rolling mean by time interval
Resample and average with a varying number of input : Use numpy.average with weights for resampling a pandas array
Resample everything to 5 minute data and then apply linear interpolation . This method is close to method 3 . Pandas data frame : resample with linear interpolation
if you reindex ( not resample ) your ` 10T ` dataframe to ` 5T ` , you'll have NAs at e.g. , XX : 05 , XX : 15 , ... XX : 55 . Then ` dataframe.interpolate ` should do exactly what you need .
and then linearly interpolate #CODE

It seems a little odd that the ` u ' ... '` actually shows up when you print your dataframe . It makes me suspect that you have strings that literally contain ` u'Ignore '` . If I eval those strings , I just get ` Ignore ` when I print : #CODE

@USER check the UPDATE . What if we can use filtering . We create a filtered list where we append every element in d if it's in the fields , then compare the result to d . If we got the same list , it means that all the elements are in fields , false in the opposite case .

I want to replace the 7th to 5th from last character with a 0 if it's a /: #CODE
Thanks . Is your replace operation basically trying to make sure that the days in each date are zero-padded , so ` 4 / 1 / 2012 -> 4 / 01 / 2012 ` ? What date format are you trying to achieve ?

The simplest way to fix this is to replace #CODE

reorder column in pandas pivot table

Apply the function to the array like this : #CODE

groupby the user and apply a lambda : #CODE

Append string to some dataframe rows with Pandas
Use the vectorised ` str ` method ` contains ` to create a boolean mask and use the negation operator ` ~ ` , pass this to ` loc ` and prepend your string to the current value : #CODE

Any ideas how can I easily parse / strip this into an efficient dataframes ?

The string entry ' nan nan ' cannot be converted using ` to_datetime ` , so replace these with an empty string so that they can now be converted to ` NaT ` : #CODE

After doing calculations , sorting , and data reduction , I've posted the plotting portion of the code below . It runs , and you can cut and paste it into your favorite editor . I want to add error bars to the df plot using the values either in df2 or e1 . I'm not married to the idea of doing this as a Data Frame plot , but it's the only way I was able to get the bar plots to plot in a group format . Can someone who knows Python better than I do help with getting the grouped plot with error bars added from the data included below . I am grateful for any help anyone can provide ! :-) #CODE

Python pandas apply function if a column value is not NULL
I want to apply a simple function for rows that does not contain NULL values in a specific column . My function is as simple as possible : #CODE
And my apply code is the following : #CODE
To fix it , you could simply wrap the isnull statement with ` np.all ` : #CODE

I've also tried playing around with the replace function , but to no avail ( the values to be replaced are all zero , so an implementation of .replace ( 0 , np.nan ) could also work ) .

You'll have to make a choice about what to do with those values , either set them to some int value like -1 or drop them
One common pitfall with ` dtypes ` which I should mention is the ` pd.merge ` operation , which will silently refuse to join when the keys used has different ` dtypes ` , for example ` int ` vs ` object ` even if the ` object ` only contains ` int ` s .

But seriously ... be careful with eval .

Stack pandas data frame using selected columns
I want to stack this into this form : #CODE
The Stack command cannot do this in its normal way .

` pivot ` and ` fillna ` are what you want : #CODE
thanks . I was trying the ` pivot ` thing but somehow couldnt find the suitable example . Thanks again
@USER , glad it helps :) you may read more about ` pivot ` [ here ] ( #URL ) .

Use ` nan_to_num ` to replace a nan with a number :
Just apply this : #CODE
Then you can merge the arrays however you want .
see above solution for .fillna example . another way to do this is to interpolate . another command in the later version of pandas .

Replace values in a dataframe column based on condition
I have a seemingly easy task . Dataframe with 2 columns : A and B . If values in B are larger than values in A - replace those values with values of A . I used to do this by doing ` df.B [ df.B df.A ] = df.A ` , however recent upgrade of pandas started giving a ` SettingWithCopyWarning ` when encountering this chained assignment . Official documentation recommends using ` .loc ` .
Since pandas allows basically anything to be set on the right-hand-side of an expression in loc , there are probably 10+ cases that need to be disambiguated . To give you an idea : #CODE

Personally , I would use eval which is very efficient for large array , something like this : #CODE
i tried your suggestion on a small subset of my data , it takes 13 minutes , while my solution takes 26 seconds . I guess eval _is_ very efficient , but it wastes time by checking the whole df everytime , while I check only a handfuls of intervals at a time .

Looking for a python solution . Need assistance in unpivoting a data frame in ` python ` . The structure is a little funky for a basic pivot function for how I'd like to reshape it .

I've tried using shift but couldn't get the desired result : #CODE

Thought this would be straight forward but had some trouble tracking down an elegant way to search all columns in a dataframe at same time for a partial string match . Basically how would I apply ` df [ ' col1 '] .str .contains ( ' ^ ')` to an entire dataframe at once and filter down to any rows that have records containing the match ?

dt = num2date ( x , self.tz )
return _from_ordinalf ( x , tz )
dt = datetime.datetime.fromordinal ( ix )

Pandas join issue : columns overlap but no suffix specified
When I try to join these 2 dataframes : #CODE
The error is a bit cryptic , the problem here is that you have no values that are common to perform the join on , if you did this it would be fine : ` LS_sgo.merge ( MSU_pi , on= ' mukey ' , how= ' left ')`
You'd have to post your raw data , also did you try merge ?
Merge works ! Thank you so much . Why not join though ? Perhaps you can add this as an answer , and I can accept
You'd have to post your data in order for me to look at this and explain why join didn't work
Your error on the snippet of data you posted is a little cryptic , in that because there are no common values , the join operation fails because the values don't overlap it requires you to supply a suffix for the left and right hand side : #CODE
` merge ` works because it doesn't have this restriction : #CODE

Pandas replace issue
I can use pandas replace to replace values in a dataframe using a dictionary : #CODE
What do I do if I want to replace a set of values in the dataframe with a single number . E.g I want to map all values from 1 to 20 to 1 ; all values from 21 to 40 to 2 and all values from 41 to 100 to 3 . How do I specify this in a dictionary and use it in pandas replace ?
You can do that using ` apply ` to traverse and apply function on every element , and lambda to write a function to replace the key with the value of in your dictionary .
I will create lambda function to map the values .
You can replace the column by replacing it : #CODE

You can use this to go through the dates that you have classified as " year-month " and apply cretiria on it to get related data . #CODE
You can use either resample or TimeGrouper ( which resample uses under the hood ) .
Thanks for the help . I couldn't get TimeGrouper to work , but resample ( " M ") did the trick . However just fyi , it required the argument how= ' sum ' . My only problem now is that the plot is using the full datetime for the tick labels . I need it to just show the month and year for each bar . Thanks again .

I'm not sure what might be the real issue behind this , but will it be possible for you to install the Anaconda distribution instead ? My experience with their distribution is that the SciPy stack is almost always guaranteed to work , regardless of OS .

Python folium GeoJSON map not displaying
I'm trying to use a combination of geopandas , pandas and folium to create a polygon map that I can embed incorporate into a web page .
Although the mapf.create_map() function successfully creates a map , the polygons don't render .

My interpretation of this warning is that by using columns name ( `' column3 '`) and ` loc ` I do not really access ( refer to ) the desired cell of the data frame . Instead , I create an object which is a copy of the " cell " object and then I try to change the value associated with this " copy-object " .

But those have to be ints . Add a separate issue on Github to have the str.slice method take series objects and apply element-wise .

getting the index of a row in a pandas apply function
I can apply it like so : #CODE
Aside : is there a reason you need to use ` apply ` ? It's much slower than performing vectorized ops on the frame itself . ( Sometimes apply * is * the simplest way to do something , and performance considerations are often exaggerated , but for your particular example it's as easy * not * to use it . )

The names of the groups will replace the default numbers as column headings in the returned DataFrame .

I was thinking about using ` cut ` but the bins attribute seems to only take integers .

keep_default_na : bool , default True

I believe ` loc ` may also work instead of ` ix ` here .

If in ipython you can turn on ` pdb ` and start debugging : ` %pdb ` execute the command and then ` %debug ` you will then be able to walk the stack and display the values
Usually you could drop to a debugger when an exception is raised to inspect the problematic value of ` row ` .
@USER , you could arrange for code to [ drop to the debugger when an exception is raised ] ( #URL ) .

` apply ` is too slow .
@USER : I have a script that I ( would like to ) run fairly frequently that spent about 300 seconds in this computation when using ` apply ` ( more like 100 with the awful hack above ) . It goes through tens of thousands of rows , tens of times . Pandas 0.14.1 .
I was not able to find a way without at least using an ` apply ` for setup but assuming that is okay : #CODE
Note that you must use the ` datetime ` from the ` datetime ` module rather than the ` numpy ` one or the ` pandas ` one . Since you are only creating the delta with the apply I would hope you experience a speedup .
Here is a way to do it ( by adding NumPy datetime64s with timedelta64s ) without calling ` apply ` : #CODE

I'm trying to convert pandas dataframe to json in python . but I couldn't get the result format that want in json i'm new to pandas is there a way to pivot and simply use All I can think of is create loop over and over to get key " freq " and state . But I feel like there must be a better way . Any recommendation would be appreciated .

` array_split ` accepts any array-like argument ( including ` pandas.DataFrame ` objects ) , but only returns guarantees that it return a ` numpy.ndarray ` ( which DataFrames are not ) . Of course , ndarrays don't have an ` apply ` method , which is exactly the error you're seeing . I'm actually surprised that this works in any scenario . You'll either need to split the dataframe into sub-frames or apply a function that operations on ndarrays .

If you want the file ( one big string ) split out into smaller strings , don't build up a new string , then split it apart again . Just append each line to a list : #CODE

Since Limit varies on each row , you should use , for example , apply like following : #CODE

If the element in column A is n , I need to insert the n th element from the List in a new column , say ' D ' .
I think the OP knows how to do this already . By my reading the issue is constructing ` D ` from the elements of ` A ` and ` List ` ( " If the element in column A is n , I need to insert the n th element from the List in a new column , say ' D ' . ")

pandas transform dataframe pivot table
by using pandas pivot table : #CODE
Is there a quick way to create the pivot table with percentage of row totals instead of raw sum of counts ?
Does row totals mean ( 27,537,963 + 23,448,904 + 1,213,184 ) for the first row , and so on ? And you want to replace numbers in the row to percentages ?

The below function works well if freq is hourly but doesn't work with below freq .

Since you want to apply the operation generically , to any given ` foo ` function , you have no choice but to call that function ` na ` -times- ` nb ` times . That part is not likely to be further optimizable .

` resample ` is the indeed the function you need , but therefore you need to set the date as the index : ` df.set_index ( ' Date ')` . However , the dates don't look like they are really parsed as dates .
You might want to check ` df.info()` to make sure you successfully parsed your data into a DataFrame with three columns , and that the Date column has dtype datetime64 [ ns ] . Since the ` repr ( df )` you posted prints the date in a different format and the column headers do not align with the data , there is a good chance that the data has not yet been parsed properly . If that's true and you post some sample lines from the csv , we should be able help you parse the data into a DataFrame . #CODE

The ` pandas ` way actively shuns looping in favor of proper indexing and selecting due to the expensive overhead incurred in looping . Are you sure you cannot apply indexing plus , for example , a lambda expression to apply these filters ?
You could also create a mask in a loop and apply it all at once : #CODE

Now both ` dt ` and ` dt2 ` will be dictionaries like this : #CODE

@USER . Thanks ! But , this was the answer I was looking for : #URL Using ` shift ` , and storing the shifted values in another column . The function I was looking for was ` shift ` . I remembered reading about it at some point , but forgot the details .

How do I insert an incremental index in a dataframe , or reassign / realign existing index items .

Either override ` loc ` to return a ` CustomFrame ` , or convert its return value in ` get ` .

Welcome to Stack Overflow ! Can you post some code that you've written ?

Pandas pivot table maximum value
I have the following pivot table ( df ): #CODE

I have a pandas dataframe that contains for multiple positions ( defined by coordinate ` x `) a value for different timesteps . I want to create a ` pandas.Series ` object that contains the value at a given position ` x ` for all timesteps ( so all index-values of the dataframe ) . If ` x ` is not one of the column labels , I want to interpolate between the two nearest ` x ` values .
Another option is to use the ` interpolate ` method , but therefore , you should add a column with NaNs with the label you want .
With the ` interpolate ` method : #CODE
I don't want to adjust the dataframe with a fictive point . Is there a way to use the interpolate option without adding a column ? If no , then I think I will go for the first solution .

I want to append a column s , such that in every row , it contains the sum of ` v ` s within a 1-second lookback time interval , such as #CODE
set ` ts ` as index and then ` groupby ` second , and transform with ` cumsum() ` as a new column s , then apply ` reset_index ` , like this : #CODE
oh you mean the freq from the first record on or I got it misunderstood ?

More efficient pandas python command to drop Nan rows ?
I have a DF called TI . I want to drop rows where BookDate is NaN . So I run : #CODE
@USER , that seems weird that to drop rows I have to create a copy and end up using 2x the RAM .
well , if you think about it you have to create a new array that is the same size as the existing one , then copy over the non-dropped rows . you don't a-priori know that you have rows to drop . Much easier with a large data set to keep it in HDF5 . Then chunk write to a new store , dropping as you go . When you read it back it you are done .
Ohhh so you mean DROP in chunks ? That might work .

df [ ' rank '] = np.arange ( len ( df )) + 1
Apply the ranker function on each group separately :

Normalize each column of the Pandas Dataframe
It might be simplest to normalize the entire DataFrame in one go ( and avoid looping over rows / columns altogether ): #CODE

Agreed with Paul about ' loc ' usage .

You can use a list of datetimes with the desired frequency , use ` searchsorted ` to find the nearest datetimes in your index , and then use it for slicing ( as suggested in question python pandas dataframe slicing by date conditions and Python pandas , how to truncate DatetimeIndex and fill missing data only in certain interval ) .

` pd.concat ` to stack the sub-DataFrames : #CODE
Another option , inspired by HYRY's solution , would be to hide the common columns in the index , and then apply HYRY's ` stack ` ing trick : #CODE

Pandas : map values of categorical variable to a predefined list of dummy columns
I have a categorical variable with known levels ( e.g. ` hour ` that just contains values between 0 and 23 ) , but not all of them are available right now ( say , we have measurements from between 0 and 11 o'clock , while hours from 12 to 23 are not covered ) , though other values are going to be added later . If we naively use ` pandas.get_dummies() ` to map values to indicator variables , we will end up with only 12 of them instead of 24 . Is there a way to map values of the categorical variable to a predefined list of dummy variables ?

I want to apply one single function call on " df " . Sometimes the function will call member functions of " df " . Sometimes it will just print " df " . I dont want a switch case . I am a Python newbie , so I dont really understand how my question is unclear . As I know so little , I tend to skip over crucial details . What are those crucial details you need to know ?
I want to process the dataframe in different ways , in different places in my code . Sometimes I want to do A , other times B , C , D , etc . Instead of having a if else clause in every different places in my code , I insert just one line : " process_df ( ... )" now , thanks to your help . Or am I thinking wrong ?

You could use boolean indexing and ` shift ` . For example : #CODE

Pandas Pivot Table alphabetically sorts categorical data ( incorrectly ) when adding columns parameter
I ran into trouble with the Pandas pivot function . I am trying to pivot sales data by month and year . The dataset is as follows : #CODE
However when I pivot across years and months the months are sorted alphabetically . #CODE
So , the result index is `` object `` dtype , as its converted automatically by the pivot , and it is not reordered according to the category . So that's a bug ; pandas doesn't support a CategoricalIndex yet , as that would be ideal result . here is the bug report : #URL

Is it using ` groupby ` ? ` resample ` ? should I make an ID ` index ` ?

In Pandas version 0.14 and older , you can use ` apply ` to extract the dates from the ` datetime ` values : #CODE
In 0.15 you can use the new ` dt ` accessor , like ` series1.dt.month `

pandas drop duplicates of one column with criteria
If you want to drop any duplicates , this should work . The sort will place all valid entries after NAs , so they will have preference in the ` drop_duplicate ` logic . #CODE
Then drop duplicates w.r.t. column ' A ' : #CODE

pool.join() does nothing in this script pool.map already returns a list in the order of the arguments and pool.close() already closed the pool you are trying to join .

I need to create a pandas series whose elements are each a function of a row from a DataFrame . Specifically the is a ' metadata ' column which is a json string and I want a Series of dicts that are the json plus the rest of the columns . Ideally I would want something equivalent to a map method for a dataframe : #CODE
I would rather a key-value map is saved in the series but this works for my specific case . Thank you

I think you should wrap the dict in a Series , and then this will already expand in the groupby call ( but then using ` apply ` instead of ` agg ` as it is not an aggregated ( scalar ) result anymore ): #CODE
Thank you . I didn't know about this ` unstack ` thing , it seems to be a nice solution .

How to transpose this dataframe ?
Unfortunately , this structure is not really to my liking- I would like to transpose the whole thing so that the I have the Series Codes as Columns , a year column and a country column - while the value is in the rows . I figured that I should use the pivot function , but the result is not what I expected .

bool , DataFrame or Series . The simple way to draw a table is to

it's easy from here figuring out the actual times . you can also resample to make it clearer , but this should do it .
edit : adding how to resample by seconds : #CODE

Using apply in pandas data frame gives ValueError
I have a vector that I want to apply a pearson correlation to all rows of a pandas data frame . I am trying the following : #CODE
Apply func simply takes two ` numpy ` arrays and calculates the correlation #CODE
python pandas : apply a function with arguments to a series . Update
I ensure that args is a tuple which is what the ` apply ` function is expecting and I get the result I was expecting

Python Pandas Pivot - Why Fails
Are you sure you need pivot ? what about ` data.T ` ?
From what you were trying to do , you were trying to pass ' group ' as ` index ` so the pivot fails .
However I'm not entirely sure what expected output you want , if you just want shorter presentation , you can always use ` transpose ` : #CODE

Python pandas to_sql ' append '
I am trying to send monthly data to a MySQL database using Python's pandas to_sql command . My program runs one month of data at a time and I want to append the new data onto the existing database . However , Python gives me an error : #CODE
Any ideas on why I cannot append to a database ?

I'm trying to merge many ( a few thousand one column tsv files ) data frames into a single csv file using pandas . I'm new to pandas ( and python for that matter ) and could use some input or direction .
I've tried to merge the data frames one at a time using pandas : #CODE
Look at the docs for merge , when called from a frame , the first parameter is the ' other ' frame , and the second is which variables you want to merge on ( not actually sure what happens when you pass a DataFrame ) .
I would think the fastest way is to set the column you want to merge on to the index , create a list of the dataframes and then ` pd.concat ` them . Something like this : #CODE

I have a dataframe ` fal_mean ` where the multi-indices are `' lat '` , `' lon '` , and , `' yr '` . Below is just a snippet of the data , but what I need to do is to select data by the number of entries for the index ' yr ' ( years ) for a given lat / lon combindation . For example , I would like to select only the data for each lat and lon combination where there are N or more entries ( years of yearly data ) such that ` len ( fal_mean [ ' yr '] = N )` #CODE
` len ( fal_mean.loc [( 35 , -75 ) , ' 101 ']) N `
e.g. ` fal_mean.loc [: , len ( fal_mean.index.get_level_values ( ' yr ') 32 )]` does not work .

This is easy to do in a for-loop , but it takes quite a long time for large features sets ( like going back to 1960 on " ge " and making each row contain the preceding 256 session values ) . Does anyone see a good way to vectorize this code ? #CODE
>>> timeit.timeit ( ' data.featurize ( data.get ( " ge " , dt.date ( 1960 , 1 , 1 ) ,
It looks like ` shift ` is your friend here and something like this will do : #CODE

EdChum : I'm presuming that you mean the logic in the for loop . Here's what I meant : the col_headers list contains all the column names . If I cycle through them , one at a time ( through the loop ) , and output to a * .csv in each iteration , then I can manually specify which column to print in each iteration . To do that , I set up the loop counter to go from 0 ( first list element ) to range ( 0 , len ( col_headers )) which is the last loop element . Is there something in this counter specification that is blocking it from picking up the last list element ( Date ) ?
the problem is you are generating a range from 0 to the len of the column list and using the index , which is a number , to index back into the df but those columns have names rather than an index so it'll fail . My answer is less verbose and a more intuitive way of achieving the same thing IMO
EdChum : Yes , I see what you mean . Your method works - I just tested it and it answers my question . It definitely is simpler . However , what confuses me is why my method works with the " -1 " ( i.e. picking up column headers A , B , C , D ) but does not work when I drop the " -1 " ( i.e. picking up column headers A , B , C , D , Date ) . This is the part that I just cannot understand ???

Easiest way is to filter the len of the groups ( according to the minimum of the resample period ) #CODE

You can find the data type of a column from the ` dtype.kind ` attribute . Something like ` df [ col ] .dtype .kind ` . See the numpy docs for more details . Transpose the dataframe to go from indices to columns .
You can then drop them : #CODE

@USER replace ` input() ` with ` raw_input() `

s1 and s2 are both of type int32 . The resulting Series is of type float . This doesn't happen when using e.g. concat . I wonder , did I misunderstand something or stumble across a bug ?

How to calculate new vectorized column from bool and float columns in pandas dataframe ?

` df.apply ` is to each value in array , so ` df.apply ( list , axis=1 )` is equivalent to : apply ` list() ` on each value in the array , ie . ` 81 = [ 81 ] , 88 = [ 8 8] , ... ` individually . So it will have no effects .
Actually no . apply works with entire row ( if axis parameter = 1 ) . If you make df.apply ( sum , axis=1 ) you will receive sum of entire row . Additionally , try df.apply ( lambda r : ' , ' .join ([ str ( e ) for e in r ]) , axis=1 ) and you will get one result for each row .
@USER , I can't see why ` sum ` is applicable here . The OP is asking for a method of aggregating values into a list structure , not a numerical operation . From your first comment , it looks like you're arguing that ` list() ` in Python takes more than one parameter . It doesn't . The behavior you're attributing to apply , element-wise operations , is what df.applymap does . Can you clarify what you mean , please ?
I think my presentation skills need improving , I only refer to ` sum ` as @USER mentioned in comment about why ` sum ` works on ` apply ` but ` list ` doesn't . my first comment was trying to make a point how ` list ` doesn't change the element in the row array , I guess I didn't explain it in a clear way .

Just assign another column as a ` cumsum ` of ` indicator ` , then apply ` groupby ` , this should do the trick : #CODE

pandas multiprocessing apply
I'm trying to use multiprocessing with pandas dataframe , that is split the dataframe to 8 parts . apply some function to each part using apply ( with each part processed in different process ) .
there is a space in the ` res = df.apply ( process apply , axis=1 )` , is that right ?
currently apply only saturates one core of the CPU . I want to use multiprocess and use all cores to decrease processing time

So here is the useful stack trace error . #CODE
You didn't get " a stack trace error " . You got an error message including a very useful stack trace that tells you how to find the problem .

If you're using Python 2.x , try changing the line to calculate size to ` size = math.ceil ( float ( len ( df )) / n )`

The SettingWithoutCopy warning is well-known in pandas , but as far as I understand , I am already using a single-level indexing ( ` ix `) and not nesting indexers ( e.g. ` [ ] [ ]` . Why do I get this warning , and how can I avoid it ?

In Pandas 0.12 , if you used the resample method on a DataFrame with a custom resampling function , it would make one call per dataframe row to the custom function , giving access to the values in all columns . In Pandas 0.15 , the resample method calls my custom function once per dataframe entry , and the only available value is that entry ( not the entire row ) .

How do I update the Quantity values in Containers to reflect the number of units in each container based on the information in Inventory ? Note that multiple SKUs can reside in a single ContainerCode , so we need to add to the quantity , rather than just replace it , and there may be multiple entries in Containers for a particular ContainerCode .
I also tried using a regular merge : #CODE
We then perform the join between the two , and each containercode existent in containers would recieve the summed quantity from inventory
@USER This will create a new one . You can do the drop in place , but to the best of my knowledge you can't join in place .

Iam trying to get the row with maximum value based on another column of a groupby , I am trying to follow the solutions given here Python : How can I get the Row which has the max value in goups making groupby ? , however it doesn't work when you apply #CODE

One way would be to use ` transform ` to assign the new flow values back and then drop duplicates : #CODE

You should replace the first line by the following : ' df2 = df.replace ( np.NaN , -1 )' , ' g = df2.groupby ( ... )' . ' groupby ' does not work well ( " as expected here ") with ' NaN ' and will create a separate group for every ' NaN ' value .

I have a huge CSV file with a bunch of metrics across item IDs that I'm trying to compare to one another , and I want to find the quartiles of every item within each metric and replace each actual number with its quartile ranking within the column . An example is shown below for some dummy data . If the metric has ` NaN ` values , I want to completely ignore them . So for ` Metric 2 ` , the quartiles would be based on the column having 10 values instead of 12 . #CODE

R merge two different time series with same date
Paul , I think I did all that you asked . I did mention that " How can I merge TS1 and TS2 either using zoo or Python-Pandas ? " and my tables are copy / pastable examples . Please let me know , if anything is missing . Thank you
In pandas , you just join on the index : #CODE

I'm importing this into pandas because I need to be able to join this data with other DataFrames and I need to be able to query for stuff like : " get all legs of variant 1 for route_id 5 " .
Alternatively , you could use ` apply ` to split each variant on commas : #CODE
Thus , to avoid possibly complicated regex or a relatively slow call to ` apply ` , I think your best bet is to build the DataFrame with one integer variant per row .
Thanks . This is pretty much what I was hoping it wouldn't be :-/ I can avoid the string manipulation by doing that when I'm parsing the data and instead of inserting a string " 1 , 2 , 3 , 5 " into the variant column I'd split it during parsing and insert a list or a tuple instead . That would be much more efficient but still a little borked somehow . I was really hoping that I would be able to have some sort of " multidimensional columns " or whatever you'd call them .

I did some more work and this works more or less , I have yet to figure out how to properly align the labels , I still do to like why I have to have a numeric x axis , when it is really categorical ( again ggplot2 deals with this on its own ) - but ar least this can be done in code #CODE

Everything works fine if I replace ` df [ ' p_hat '] .plot() ` with ` plt.errorbar ( df.index.values , df [ ' p_hat '] .values , yerr =d f [[ ' low_delta ' , ' high_delta ']] .T .values )` , but I would like to make this work in pandas or at least know why it doesn't .

Thanks for the suggestions but NaN , None or '' dont work . NameError : name ' NaN ' is not defined . The json is created using df.to_json ( orient= ' values ') . The json is created correctly . The issue is with trying to insert null's . The resulting json needs to look exactly like the example , ie : the word null with no quotation marks . ( I know the null is not actually a null ,... but it needs to say null in the json with no quotation marks ) . Thanks for trying to help
Thanks for the suggestions but NaN , None or '' dont work . NameError : name ' NaN ' is not defined . The json is created using df.to_json ( orient= ' values ') . The json is created correctly . The issue is with trying to insert null's . The resulting json needs to look exactly like the example , ie : the word null with no quotation marks . Thanks for trying to help .

It feels like you're trying to make Pandas be something it is not . If you always have 3 runtimes , you could make 3 columns . However the more Pandas-esqe approach is to normalize your data ( no matter how many different trials you have ) to something like this : #CODE

do you really intend to be doing a join between frame5fin and frame5 at the end of the loop ? Maybe that should be an append ? It seems like after each iteration frame5fin will get exponentially larger .
does the join do what you expect when you test it with a subset of data ? Trying to read your code ( very hard to do ) I am guessing that ` frame4 ` and ` frame5 ` have a number of columns in common as well as a common index . When you join these Pandas tries to match every value in every column . As ` frame5in ` gets bigger , this join slows down considerably . You are correct that it may be linear decrease in speed . I suspect , however , that you do not want to be joining based on every column . However I find reading your mind to be quite difficult .
If you want to keep missing values you might consider using the append method then after you're done looping you can take the result and reindex it to add in dates with no data .
However that code is a join , not appending to the dataframe ` frame5fin ` .
By different indices I mean that the dataframes I am trying to combine do not have a common index between them . For example , one is a time series from June 13,199 4 to Aug 1 , 2014 while the other is a time series from May , 13 , 1998 to Aug 1 , 2014 . When you join both of these you get a dataframe with an index ranging the union of the two indices ( in this case June 13,199 4 to Aug 1 , 2014 ) and it will put nans in the second column for anything before May 13 , 1998 .

Just use ` head ` and ` tail ` and ` concat ` . You can even adjust the number of rows . #CODE

the header does not automatically align .
Or use a dummy character and replace it at the end : #CODE

You should be able to use ` loc ` and ` np.repeat ` , as done [ here ] ( #URL ) -- could you confirm that I'm reading you goal correctly ? If so , I can close as a dup .

Transform slow pandas iterrows into apply
This is slow , but I am not sure how to translate it into something using apply . Any hints ?
You can use ` apply ` function , but you need to do extra work here ( just to simplify the work ) .

You can then change the boolean values to ' pass ' or ' fail ' using ` replace ` : #CODE

Use ` map ` and ` lambda ` can achieve what you need : #CODE
Note that using ` map ` will become much slower than the ` numpy.in1d ` method , when data dimension gets large : #CODE

While I do know how to do this if I were to simply iterate through all records , I would love to know a Pandas way to do this . While I've read about " shift " and other functions , I'm not entirely sure how to tie it all together . I assume I would start with something like this : #CODE
Insert columns with boolean value to indicate the sequences of data that passes the filter : #CODE

Pandas Replace NaN with blank / empty string
@USER -L I don't want to replace it with an integer value . I want to replace it with an empty string .

Well you can use ` loc ` and change just the values ` 1 ` with mapping ` C ` with the newly mapped dictionary key of ` C ` : #CODE
And apparently using ` loc ` is more efficient in this case .

Also you can use ` map ` and ` lambda ` , like this to achieve the same result , this is more efficient than your attempt : #CODE

What is the trick behind the map function in Python ?
Why is it much more faster then the " straightforward " solution . What trick the map function uses to be so fast ?
The right-hand side of ` df [ ' out_col '] = map ( my_func , row.col1 , row.col2 , row.col3 )` is one value ( dependent on ` row `) . But assigning that to ` df [ ' out_col ']` means the whole column will get the same value . So it will not do the same thing as ` d [ ind ] = my_func ( row.col1 , row.col2 , row.col3 )` followed by ` df [ ' out_col '] = pandas.Series ( d , index = inds )` outside the loop . I don't think you are comparing apples to apples .
It is not ` map ` that is fast , but ` iterrows ` that is very slow . You can use ` itertuples ` and will get something almost as fast as ` map ` , but as @USER says , you should try to see if you can apply this function on the whole columns .

Dear JD , I dont want to create the custom column in the dataframe gain.I want to apply the where condition for the custome column

what does .agg ( len ) do ? is it like aggfunc ?
yep , ` .agg ( len )` uses ` len ` as the ` aggfunc ` and return the length of each group , which essentially is the count .

Series ( for example , columns of a DataFrame ) have a convenient ` map ` method . You just need the encoding in a dictionary form : #CODE
Then go through the ` rno_cd ` column , and apply a function that transform the data . You can use ` apply ` and function ` tranform ` where you can verify whether x is a key so you get the values using your dictionary ` D [ x ]` if it's not the case , you just return `" unknown "` #CODE

How to join two panda series without duplicating keys
How can I merge them to get the following result : #CODE

Now I am getting a Memory Error and it runs more than seven minutes . ( the first " wrong " hist plot needs around one minute ) Any idea , what I can do ?

I need the original dataset intact . Just remove all values ( replace by NaN ) which are not the minimum .
I can also transpose the dataset if the operation is easier per column .
Thanks for the fast response . I was not completely clear in what I need . I need the original dataset intact . Just remove all values ( replace by NaN ) which are not the minimum .

Is there a way that I can apply a function to a pandas dataframe that returns a list for each row that it is applied to , and then take that list and put each item into new columns of that existing dataframe ?
Right now the return of the apply function is a list of lists each of the inner lists is a 9 item list like that shown above . I am fine putting the response into a new dataframe such as the below , but I haven't figured out how to get the apply function to write to each new row for each return or get the list of lists in to the right form . #CODE

You could use the ` diff() ` Series method ( with ` fillna ` to replace the first value in the series ): #CODE

My initial approach was to merge the different kdes by generating one ` ax ` object and passing that along , but the accepted answer inspired me to rather generate one df with the group identifiers as columns : #CODE
Then group by ` expenditure ` , iterate through each expenditure , pivot the data , and plot the kde : #CODE

Then you could just append these to your existing DataFrame to fill the missing , like this : #CODE
Alternatively , if you a DataFrame with the pairs you ' should ' have , ( a 1-5 , b 1-4 ) , you could merge that against the data to fill the missing . For example : #CODE

Apparently you get two figures because ` m ` is the context from which you call every map drawing command , and there after you draw in current active figure using ` plt ` based plotting commands .
replacing my plotting functions withe snippet you sent plots the map without contours nor scatter plot .
Thanks all those who spent their time reading my code ( above ) , and those who helped me make this code to work ( six months ago ) , particularly @USER . Since then I have been wondering if there is any extrapolation in python . I know extrapolation can give weird results , but I have not seen anything in documentation . Is it possible to extrapolate to at least fill the map ?
@USER you could use , for example , ` scypy.rbf ` module to create an RBF interpolator for a set of data . You could then interpolate further away from the original dataset ( that is , extrapolate ) , but check results , because the farther away , the more error you get . And also , check out other interpolators , not only RBF ( there is also kriging , but not natively in ` scipy ` I think .
You're almost there , but Basemap can be temperamental , and you have to manage the z-order of plots / map details . Also , you have to transform your lon / lat coordinates to map projection coordinates before you plot them using basemap .
Thank you very much for your precious time @USER . This will serve me well as learning material as well . I have been working on this for close to a year . I have noted all the suggestions you have made . BTW what does YMMV mean ? I started with R . I was able to get contours on a map in r but I understood that one could not extrapolate ( to fill the domain ) in r.Though am still not extrapolating I think I can be happy living with python . Thanks so much for all kind and helpful people .
@USER YMMV means ' your mileage may vary ' that you might feel differently about the colour map and line width choices I made , and that you should feel free to experiment with these . Good luck !

I like this structure of data before ` apply ` ing , before I am usually able to just ` df [ ' someNewColumn '] = df.apply ( ... )` . But strangely , this time , I'm not able to instantly remerge the results .
@USER , ` level=0 ` will give the same result as above as you have only 1 index by the time you do the ` apply ` . the ** ... ** is exactly from the output as you're trying to apply the ` ... / x.coef.mean() ` on a group level . However , I think JD Long's suggestion is more likely what you're trying to achieve . And my pandas's version is ** 0.14.1 **
It's not obvious to me which version of pandas you're using , but your apply does not work for me at all .
whoops . You are correct . I read his code too quickly and didn't notice the apply was on the grouped data . I'll fix my comments above .
you can group by the index just fine , the issue is you're still returning series object and stuffing them in a series with an index of ` [ ' foo ' , ' bar ']` . If the OP wants to normalize the ` coef ` column based on the mean of the groups , the solution will be a little more nuanced . I think the solution in my previous comment achieves this .

So first the merge . It's important to specify ` how = ' left '` so we'd get the rows from ` tmp ` that are not on ` df ` : #CODE
You see that the rows we want received ` nan ` for their ` items ` column , since they were merged only from ` tmp ` . Now let's drop them , and get rid of the marker column . #CODE

I already know about the np.isfinite and pd.notnull commands from this question but I do not know how to apply them to combinations of columns .
You can use ` apply ` and lambda function where you choose non-Nan value . You can verify if it's Nan value using ` Numpy.isNan ( .. )` . #CODE
This is very helpful . I can use this to make a new dataframe for each combination-of-interest and then join them all together .

What do your data actually look like ? Also , are you sure you want a groupby and apply together ( i.e. not ` agg ` instead ) ? When you say " they all fail " , a specific error message would be helpful , as would a description of what exactly you are trying to do by doing this operation .
So if apply a simple function , ` mean ` , to the grouped data we get the following : #CODE
That gives all NaNs for ` newcol ` . Honestly I have no idea why that doesn't throw an error . I'm not sure why it would produce anything at all . I think what you really want to do is merge ` df2 ` back into ` df `
To merge df and df2 we need to remove the index from df2 , rename the new column , then merge : #CODE

This works just fine ( i.e. ` ctt_ask ( example_data )` yields 2.90 ) for the above example but my real dataset has several stocks and many date times ( it has a ` MultiIndex `) . When I use ` groupby ` and ` agg ` to apply this function to every stock-date time combination ( ` full_book_ask.groupby ( level =[ 0 , 1 ]) .agg ( ctt_ask )`) I get an error : ` KeyError : ' avail_shares '` . This is strange because I do have a column named avail_shares in my actual dataset . I have also tried the same with the ` apply ` functionality but this raises the error message ` Exception : cannot handle a non-unique multi-index !
Thank you for the comments . @USER H , as I state in the description there is a column called avail_shares in my dataset so this can't be the issue . @USER Pride , you are correct , I understand now why I can't use ` agg ` . However , both ` apply ` and ` transform ` give me an error as well : ` Exception : cannot handle a non-unique multi-index ! ` . So there must be something else going on . I guess there is a problem with the use of ` ix ` but I don't know why or what I should do to solve it .

Then I could transfer this into a series and be able to resample it : #CODE
finally resample and plot : #CODE

I hoped their would be some line of code which I could insert at the top .

I have two CSV files that I would like to merge . With pandas I would use :
1 ) One way to perform such a merge / join operation without loading the files into memory

I realize I can do all my operations separately and merge them , and that is fine if I am using python , but when it comes down to choosing a tool , any line of code you do not have to type and check and validate adds up in time
OP asked for a way to apply multiple aggregate functions at the same time . A short answer is still an answer .

There ought to be a metric you can apply that takes a baseline picture of memory usage prior to creating the object under inspection , then another picture afterwards . Comparison of the two memory maps ( assuming nothing else has been created and we can isolate the change is due to the new object ) should provide an idea of whether a view or copy has been produced .

but way faster than stack #CODE
Now I need a way to get the top n . One naive idea is to copy the array , and iteratively replace the top values with ` NaN ` grabbing index as you go . Seems inefficient . Is there a better way to get the top n values of a numpy array ? Fortunately , as shown here there is , through ` argpartition ` , but we have to use flattened indexing . #CODE
what you want to use is ` stack ` #CODE

Map phase I can take in one row and output multiple rows ,

step 3 ) truncate time part in seconds
If you are dealing with a `` DatetimeIndex `` , you can use the ` normalize ` method . This is however not availabe on the individual Timestamp ( a workaround is : ` pd.DatetimeIndex ([ ts ]) .normalize() [ 0 ]`)
Instead of using ` datetime.datetime ` , use ` datetime.date ` and it will automatically truncate the hour / minute / second for you .
I think you are looking for the ` replace ` method ( see docs ): #CODE
If you want to reset the full time part , you specify all parts in ` replace ` : #CODE

You try do a merge on both dataframes , which should return the ( set ) intersection of both . #CODE

I wanna get strings from Series that was drop duplicates .

You saved a string " [ 1.5 , 2.5 , 3.5 ]" . So you get a string back :) . You can use eval ( "" [ 1.5 , 2.5 , 3.5 ]"") , but I hear it's bad practice .
I don't know if CSV has a concept of complex objects such as lists . You can map your lists to strings by using `" , " .join ( your_list )` given that you only use floats .

Next , use the apply function in pandas to apply the function - e.g. #CODE

I have verified no additional whitespaces are in the string with the strip function .

AttributeError with an outer join in pandas 0.15.1

I've been unable to find how to do this either online or in the documentation . Experimented a bit with the stack and unstack methods but never got what I was looking for that way . ANy tips much appreaciated .

You can use ` all() ` ` any() ` ` ix [ ]` operators . Check the official documentation , or this thread for more details #CODE

I think so , it seems to align to the records format shown in the [ pandas documentation ] ( #URL ) , i.e. [ { column -> value} , ... , { column -> value } ]

Python Pandas drop columns based on max value of column

Pandas DataFrame insert column based on values in lists

Drop level from one specific column
Inspired by Pandas : drop a level from a multi-level column index ?

Thanks @USER , this gives me a good view of using a function instead of a lambda , lamdas still confuse the heck out of me . When i ran your code it looked good except it also seemed to Drop US Coke Week 4 which I need to maintain as it is still in sequence to the preceding week .

is it possible to edit this to return the value ? instead of a bool

Thanks , this almost works ! I now have both histograms in a single plot and both kde's in another one . How can I merge all of them in the same figure ? Running all your code , without the second ` fig = ... ` still gives me 2 different plots .

I'm running Win7 64Bit on 4GB RAM . I read a large data file ( 3Mio rows to read ) into a Pandas dataframe , do several isin() -Operations and obtain 2 other dataframes df1 and df2 , each having 300000 lines . Until here everything is fine , total memory consumption is around 40% . However , when I try to merge df1 and df2 , RAM consumption goes directly up to almost 100% and a a system freeze results . Looks like a memory leak . Anybody observing something silimar ? What happens under the roof of pandas.merge() that leads to that ? Any possibility to get the code running ? The merge command : #CODE
You may have to provide some example data on this . I suspect your join is going many to one or something . I tried to set up an example with 6 columns and I was able to do the join with 50,000,000 records on my MBP in about 2 minutes .
then I do the merge : #CODE
Trying to merge then resultied in a memory error . I run on WIn7 with 4 GB . And you ?

` TypeError : cannot astype a datetimelike from [ datetime64 [ ns ]] to [ bool ]`

Specifically I have a date , I want to take the date and run off to a service and return the Day of the Week , and insert it into that row in a new column :
but I don't know how to best iterate over the dates in the DataFrame and insert the function return .

Well , with a transpose on the original frame and the result this actually works
Finally we must replace the obtained Series with ` value ` if ` col == " E "` and ` value == False ` . You can't apply a condition on the index of a Series , thats why you need the ` reset_index ` first .

But when I try to use the asof method with a float , I get a TypeError : #CODE
Although this is not stated clearly in the docs , I think ` asof ` only works with a DatetimeIndex , and not with other index types .
@USER : I think you are right about the ` asof ` being limited to DateTimeIndex . I have edited my code to use the method ( and save the results ) before changing my index from DateTime to float . I have edited the question above .

Pandas ( bug in 0.14 ? ) Int64Index : " intersection " with repeated indexes
So missing is ` -1 ` , but then it is used to extract the last element . Is it a bug in Pandas ? It seems pretty dramatic , and I am relying on this ` intersection ` all over the place . Is it always including the last element for good measure ?

I would like to use the to_datetime method to convert the recognized string date formats into datetimes in the dataframe column , leaving the unrecognized strings in excel format which I can then isolate and correct off line . But unless I apply the method row by row ( way too slow ) , it fails to do this .

Since index positioning in Python is 0-based , there won't actually be an element in ` index ` at the location corresponding to ` len ( DF )` . You need that to be ` last_row = len ( DF ) - 1 ` : #CODE

Pandas - intersection of two data frames based on column entries
You can merge them so : #CODE
To drop NA rows : #CODE
I am not interested in simply merging them , but taking the intersection . That is , if there is a row where ' S ' and ' T ' do not have both prob and knstats , I want to get rid of that row . You'll notice that dfA and dfB do not match up exactly . However , this seems like a good first step . How can I prune the rows with NaN values in either prob or knstats in the output matrix ?
Changed to how= ' inner ' , that will compute the intersection based on ' S ' an ' T '
Also , you can use dropna to drop rows with any NaN's . Edited my answer

How can I do this with Pandas ? Note , even thought this structure may be a bit inefficient , I'd like to stick with the general flow of things . Perhaps it's possible to insert some " post-processing " to fill in the gaps .

I want to truncate this to show only HH : MM : SS ( e.g. 00:00 : 00 to 1:14 : 11 ) . How can I remove the " 0 days " prefix in the x-axis ? Also , is there a simpler pd command to convert data to HMS objects ?

Join / Merge two pandas dataframes and filling
I want merge / join the two frames by time .
Try this . The ` how= ' left '` will have the merge keep all records of df1 , and the ` fillna ` will populate missing values . #CODE
Is the ` fillna ` necessary ? Isn't this the default behavior of a left join ?

creating a pivot or summary of text data
And I am trying to pivot it to look like this ( header row doesnt matter much to me ): #CODE

( Slightly more pythonic , if you wanna keep the order of files : drop the ` count ` variable outside and do ` for count , file in enumerate ( filenames )` )

I am grouping according to your ' description ' . If its really [ 0 , 1 , 0 , 0 , 1 ] then its even easier . IOW . seems you wanted to calculate diff betwee 4 and 2 ( and 3 is in that group ) . Implies that 0 , 1 are in the other group .

My idea was to then apply the rolling mean on this time period .
Sounds like you want to resample everything in a 90s window . #CODE
Great , I think this will work quite a bit better than my approach . However , when I shorten the window length ( ex : 30 seconds ) , I get NaN for certain values , even though the timestamps are continuous ( some are caused by gaps in timestamps , and those are fine ) . How can I fix this - interpolate the values ? Thanks .

Pandas provides a method for DataFrame and Timeseries named ` resample ` .
so we can do ( to resample with 2 hours sampling period ) : #CODE
though you dont ' really need to do this , e.g. resample will call this to convert string inputs .

How to do a left inner join in python / pandas ?
Also , this isn't really a join operation ; it's a selection . I think you should edit the title to reflect that .
What I'm trying to do is what this website , #URL calls " left excluding join " .
But you only want the columns from one of the dataframes , right ? Joins are used to align rows and columns from different tables . All you're doing is selecting the data based on elements that are incidentally stored in a different dataframe . There's a subtle difference .
A true left join would look like this : #CODE
Doing a right join : #CODE
@USER I know -- my point is that since you don't actually want a join operation , you should edit the title of the question to better reflect what you actually want . Also , you can use ` .dropna() ` to remove those rows or use a ` right ` join .

Is there a way to replace the time-stamps in the original data-set with the new group's time-stamp in an efficient way ?

I want to be able to append to a ` .txt ` file each time I run a function .
Is there a way to append the output in a format that can be more easily imported into pandas

First : ` for i in df ` loops through the column names -- not rows -- of the dataframe . So you definitely don't want that . Second , ` ix ` has been more or less deprecated . Use ` loc ` and ` iloc ` instead . Third , you almost never need to loop through pandas objects . See my response for a more efficient way .

How do I apply a lambda function on pandas slices , and return the same format as the input data frame ?
I want to apply a function to row slices of dataframe in pandas for each row and returning a dataframe with for each row the value and number of slices that was calculated .
What I want is to apply lambda function f from column 0 to 5 and from column 5 to 10 .
I was thinking that I could do the same for the second slice ` b = pandas.DataFrame ( f ( df.T.iloc [ 5 :: , :]) ` and then concatenate both frames , and than transpose again . However , ` concat ` takes lists or dictionaries not DataFrames ..
let's see .. I want to apply the function to the slice on columns 0 , 1 , 2 , 3 , 4 and then also on 5 , 6 , 7 , 8 , 9 . However , I want the function to run on the original data where the mean is taken only from the first 5 in the first round and then on the last 5 in the second round . Does that make sense ?
@USER , you're right , I got mixed up with ` loc ` , answer edited , and Yes , you can just reassign that at one go
hmmmm .... it's so weird .. it worked and now it doesn't apply any kind of calculation , even when I take exactly your code .. >>> When I use ` df1 =d f.copy() ` it works , but not with ` df1 =d f ` .. don't know how that makes sense ..

I want to apply a function f to many slices within each row of a pandas DataFrame .
If you're applying the same function to all of the groups , why not just apply it to the whole dataframe ? does the function aggregate the values in some why ? ( all of these questions I have could be avoided if you simply included some example output that you would like to see )
First , I want to say that I've studied basic python and had an intro into pandas , but I'm overwhelmed by pandas a bit .. So , I found it hard for me to breakdown my eventual goals into one question , so I thought it made more sense to go in little steps and build on top of each other to finally get the full picture , but I was worried that it would seem like I'm repeating myself . But yes , one of the things I want to do I guess is transform matrices into same-size output matrices , not so much aggregating- but mostly I need to apply functions to slices of the rows of my input matrix ..
it'll take me a bit to digest your answer , but this is very useful info for me at this point . I do want eventually be fluent in applying these things as such , but didn't know how to step into the whole indexing and grouping with pandas . Delicate balancing between learning code and getting my project done ... I'll be trying to apply your answer to my needs . Thank you !
Read my data from file and trying to apply indexes to the ( 43 , 49 ) df . I put all my indexes as a list of tuples ( idx_tuple ) and then created multi-index by ` index = pd.MultiIndex.from_tuples ( idx_tuple , names =[ ' nr ' , ' date_sample ' , ' month ' , ' conc ' , ' time '])` . Now tried to update my df as such :

Another way is to use the dt accessor ( new in Pandas 0.15 ): #CODE
Other way ( probably was ` resample ` does under the hood ) #CODE

@USER , looping with dataframes of course will be slow . However , looping their indexes which are basically lists and write to file , will be much more efficient and quicker than ` reset_index ` and ` concat ` in OP use case . That said , I do like your approach to do things in ` pandas ` way : 0

@USER here's what I'm doing . I load up a bunch of csv's into dataframes . I merge the dataframes into a single dataframe , including some processing . But in the end I have a dataframe , yes there are NaNs , but it's nice . The problem is that this processing takes some time and I want to save my results to an h5 which I will query in the future . This error is reproduceable , though I will restart my VM . My problem is now posted : #URL

Well , you could insert a ` np.nan ` value between the two data sets , but that's not the standard way to handle this problem . Usually you'd just use a loop . ( Try ` plt.plot ([ 1 , 2 , np.nan , 1 , 2 ] , [ 0 , 1 , 2 , 1 , 2 ])` to see what I mean . )

pls read the extensive docs : #URL sounds like you want append which is not possible with a fixed store - not enough code to tell what you are actually doing
return ( seq [ #URL + size ] for pos in range ( 0 , len ( seq ) , size ))

setting levels apriori when using factorize in Pandas to cover missing cases
I understand how to use factorize to encode levels of a factor , such as " L " and " W " ( for wins and loses ) into numeric values , such as " 0 " and " 1 " : #CODE
I need some way to preemptively declare the fact that there are 3 different levels when I create the dataframes , and map the correct numeric value to the correct level . How can I achieve this ?

I would like to add the first dataframe to the second and name it something like ' values ' . How can I do that ? I tried merge with different options but without success .
and viola , merge is working like a charm : #CODE

Note there are many other vectorized str methods besides startswith .

I am trying to translate the following SQL query to run on a large pandas HDFStore : #CODE

Sorry but how would I apply this for a dataframe ? df.values.apply ( lambda x : round ( x )) ??

In the second column the dates , in the third the hour ( 1-24 ) of an observation . I'd like to get the date and time in a datetime format to merge it with other data . My try : #CODE

This code generates the error ' Int64Index ' object has no attribute ' apply '
Index types don't have an ` apply ` method , but ` Series ` does .
To apply a function to your index , you can convert it to a series first , using its ` to_series ` method : #CODE

You should probably just join the dataframes . It'll be really hard to test possible solutions without a subset of representative data .
I'm confused , why are you setting the ` call / put ` and ` expiration ` columns to integer values ? Can't you make an example with like , 10 rows and construct the output you expect ? That way we'll have a target to shoot for . ` n = 330000 ` is pretty absurd for stack overflow .
Just replace n=330000 by n=11 .

Having said that , using ` for i in range ( len ( total_val_count.index ))` -- or , what amounts to the same thing , ` for i in range ( len ( total_val_count ))` -- is not recommended . Instead of #CODE

I am trying to concat two dataframes :
You could do ` ABFluid.index = AB1.index ` before the concat , to make the second DataFrame have the same index as the first .

Now I need to replace the timestamp for each row of my table with the actual time to yield : #CODE

Inner Join the separate dataframes produced in a . ) , on the elements column ( c_col1 ) in Pandas . This is a little difficult to understand so here is the dataframe what I would like to get from this step :
Join ; if only 1 and 2 occur , then there will be only one Inner Join .
I do not understand part two of your question . You want then to join the data within in s_id ? Could you show what the expected output would be ? If you want to do something within each s_id you might be better off exploring groupby options . Perhaps someone understands what you want , but if you can clarify I might be able to show a better option that skips the first part of the question ...
and then pass it to ` apply ` on data grouped by s_id : #CODE
So ` apply ` passes each chunk of grouped data to the function and the the pieces are glues back together once this has been done for each group of data .

How about using string.find() to locate the ' and the " and then cast and then do your conversion ? All that could be done within a function and passed to an Apply
One possible method without using ` regex ` is to write your own function and just ` apply ` it to the column / Series of your choosing .
You could apply that regular expression to the elements in the data . However , the solution of mapping your own function over the data works well . Thought you might want to see how you could approach this using your original idea .

Pandas : Feeding index values to apply
I'm having problems when trying to use apply on the result of a groupby operation .
And I now would like to use apply , but I need to feed it id1 , which is part of the index , so I get an error when I try to do the following : #CODE
[ BTW , I have also tried to merge df1 and df2 in one table ( so that each row in df2 has a field with the corresponding col1 , col2 and col3 from df1 ) , but when I do the groupby and sum() it aggregates col1 , col2 and col3 values ( which I don't want )]

This should be a pretty simple question , but I'm looking to programmatically insert the name of a ` pandas DataFrame ` into that ` DataFrame `' s column names .

run the replace : #CODE

The ` DataFrame ` object doesn't have ` nunique ` . You have to pick out which column you want to apply ` nunique() ` on . You can do this with a simple dot operator : #CODE
To answer your question about why your recursive lambda prints the ` A ` column as well , it's because when you do a ` groupby ` / ` apply ` operation , you're now iterating through three ` DataFrame ` objects . Each ` DataFrame ` object is a sub- ` DataFrame ` of the original . Applying an operation to that will apply it to each ` Series ` . There are three ` Series ` per ` DataFrame ` you're applying the ` nunique() ` operator to .

I am using groupby and apply , so I am not explicitly pulling the groups , which is why i need to do this .
Apply will break the dataframe into multiple smaller dataframes by the groupby columns . The columns you group by are still inside the smaller dataframes . Is that what you are after ?
so you can see in the resulting printed output that each iteration of the ` apply ` gets all columns of the input dataframe .
I'm not sure how to grab a tuple of keys from an ` apply ` but I can from a loop : #CODE
so are you asking how to write a function which , when you apply it to grouped data , can see the keys ?
I get it now . not sure how to do this from apply , but I added an example of how to do it from a loop

Parallelizing apply function in pandas python . worked on groupby

IIUC , you can build a number-to-colour dictionary and then use the ` replace ` method . For example : #CODE
An all Pandas solution would be to make the color map a pandas dataframe and then join it to the primes dataframe : #CODE

You can avoid ` map ` and ` lambda ` altogether and use ` isin() ` , which ends up working more like the typical Python idiom of ` if item in [ ' a ' , ' b ' , ' c '] : ` #CODE

Still trying to figure out how to not have data paths connect across ( other than interpolate nulls )
I'm sorry but I don't speak Pandas , so I tried to express the concept you should use using generic names ( ` x ` and ` y `) and the generic ` pyplot ` API . If you can't translate this concept in a Pandas frame context , you should ask for someone else's help because I'm not a Pandas man ...

where n = m * len ( list with dictionaries ) ( where length of each list in ' data ' = m )
Finally merge on the original index and get desired DataFrame

Unstack doesn't return a dataframe
` unstack ` works on the index , so you have to first set it as index : ` dt.set_index ( ' var ') .unstack() `

@USER @USER means that you can replace the ` datetime.timedelta ( minutes=1 )` by ` pd.Timedelta ( ' 1 minute ')`

Alternatively , both Series and DataFrame objects have a ` reindex ` method . This allows you more flexibility when sorting the index . For instance , you can insert new values into the index ( and even choose what value it should have ): #CODE
Yet another option for both Series and DataFrame objects is the ever-useful ` loc ` method of accessing index labels : #CODE

The same " apply " pattern works for SFrames as well . You could do : #CODE

The most obvious solution to me is the one you present of adding values . I would do that probably through a join . I'd join the existing dataframe to one with an index that had all the values I wanted on the x axis and do the join with the ` how= ' outer '` option .

I'm guessing that I can't apply a sort method to the returned groupby object .

There may be a pandas way to get around this , but you could also write a quick script to append some ` , ""` for each column that a line is lacking . By lacking , I mean compared to the row with the most columns .

Drop row in pandas dataframe if any value in the row equals zero
How do I drop a row if any of the values in the row equal zero ?

This ` str ` attribute also gives you access variety of very useful vectorised string methods , many of which are instantly recognisable from Python's own assortment of built-in string methods ( ` split ` , ` replace ` , etc . ) .

You ALWAYS want to append to a list , then concat in one go . see the end of the first section : #URL
So there are a few things you're doing here . One is you need a date range where each element is a day . That day then needs to be formatted as yymmdd . Then you pull in the csv into a dataframe . Then add a coumn for the date . Then append that to a main dataframe . Here's an attempt : #CODE
hey JD , see jeff's comment on my post . apparently pandas ` concat ` function is smart enough that it's faster to append to the list ( #URL )

so that fills in all the missing dates with zeros . Now we can apply the rolling sum . #CODE

then produce a chain of these and concat all at once . Will be much more efficient then modifying in-place ( though if you are only modifying a small number of values then my 1st method might work better ) . YMMV .

python - insert intermediate values into Pandas DataFrame
I need to insert intermediate rows for every 1 to -1 and -1 to 1 transition . This row should contain backfilled time and param , and the data value should be zero .
Concat old and new dataframes , sorting by time and counter , and then reset index . #CODE

You can define a function which returns your different states " Full " , " Partial " , " Empty " , etc and then use ` df.apply ` to apply the function to each row . Note that you have to pass the keyword argument ` axis=1 ` to ensure that it applies the function to rows . #CODE
Then using apply : #CODE

unstack " mullti-indexed " column| in pandas
the idea is to first stack the first level of the column to the first level of index , and then swap two indices ( pandas.DataFrame.swaplevel ) #CODE

Pandas Filter function returned a Series , but expected a scalar bool
TypeError : filter function returned a Series , but expected a scalar bool
I am creating the dataframe by concatenating two other frames immediately before trying to apply the filter .
i'm not sure about the issue u mentioned regarding the interactive console . technically speaking in this particular case ( there might be other situations such as the intricate " import " functionality in which diff env may behave differently ) , the console ( such as ipython ) should behave the same as other environment ( orig python env , or some IDE embedded one )
an intuitive way to understand the pandas groupby is to treat the return obj of DataFrame.groupby() as a list of dataframe . so when u try to using filter to apply the lambda function upon x , x is actually one of those dataframes : #CODE

What I'd like to do is : for each day , apply a function that takes the sum of all logvol between 14:40 : 00 and 15:00 : 00 .
I have a feeling it has to do with the resample function but I'm not sure exactly how to use it .

I have a dataframe with multiple columns of timestamps in UTC and a column of the timezone it should be converted to . How would I write a function to map this ? #CODE

1 Or you could just write the default function and pass that as the ` defaut ` keyword argument to ` json.dumps ` . In this scenario , you'd replace the last line with ` raise TypeError ` , but ... meh . The class is more extensible :-)
For real fun , try this with ` np.float64 ` or ` np.bool ` and everything works fine , because they're actually subclasses of ` float ` and ` bool ` . Once you think about it , it makes sense why those two types are subclasses but none of the other numeric types are , but until you do , it can make for some real fun debugging

date_x is in order from the date farthest out to the most recent date . It seems shift doesn't use the order of the dates , instead it uses the index order to shift . #CODE
Your data is not sorted to begin with , so it will be shifted in this unordered order . If you want to shift it in a sorted manner , first sort it before the groupby . Eg : #CODE
OK , you now changed the data . But the data is not sorted to begin with ! If you want to shift it in a sorted manner , first sort it before the groupby . See my updated answer

this post but shows `' TypeError : Join on level between two MultiIndex objects is ambiguous '`

You could use ` diff ` followed by ` cumsum ` to give each group of Trues and Falses its own number : #CODE
This seems to give a difference for those dataframes that are of length one , which sometimes happens in my pipeline . Calling ` diff `` on those results in an empty df , which results in no groups .

I want to replace ' ABC ' and ' AB ' in column BrandName by A .
Probably the easiest way is to use the ` replace ` method on the column : #CODE

Plot multiple boxplot in one graph in pandas or matplotlib ?
Then pass that axes to the second call to ` boxplot ` using ` ax=ax ` . This will cause both boxplots to be drawn on the same axes . #CODE

You can create the 3x3 matrix easily using Pandas . Create a DataFrame ` df ` from the above array and pivot on the third column using ` pivot_table ` .

I have two Pandas DataFrames . I would like to add the rows of the other dataframe as columns in the other . I've tried reading through the Merge , join , and concatenate - documentation , but can't get my head around how to do this in Pandas .

I want to replace the NaN in the the top right with the same values as in the bottom left : #CODE
But that's slow with my actual data , and I'm sure there's a way to do it in one step . I know I can generate the upper right version with " m.T " but I don't know how to replace NaN with non-NaN values to get the complete matrix . There's probably a single-step way to do this in numpy , but I don't know from matrix algebra .
` m [ np.triu_indices_from ( m , k=1 )]` returns the values above the diagonal of ` m ` and assigns them to the values values above the diagonal of the transpose of ` m ` .

Pandas merge giving error " Buffer has wrong number of dimensions ( expected 1 , got 2 )"
I am trying to do a pandas merge and get the above error from the title when I try to run it . I am using 3 columns to match on whereas just before I do similar merge on only 2 columns and it works fine . #CODE
Is it possible to merge on three columns like this ? Is there anything wrong from the merge call here ?

I'm trying to find the latest date before an arbitrary cut off ( say before 2014-10-31 or 2014-09-30 ) in the series for each id . index.asof or Series.asof seems to be what I want but I can't figure out how to use it with multiple indices . For a date of ' 2014-10-30 ' I want this output : #CODE
Thanks . I wasn't totally clear in my question . I'm trying to find the latest date before some arbitrary cut off . So the nearest date at Oct month end for example . edited the question to clarify .
By the way it seems ` .asof ` when applied to the ` groupby ` dataframe evaluates the whole index and not the index of the group , so your version with ` asof ` does not work as expected : #CODE

Python pandas tz_localize throws NonExistentTimeError , then unable to drop erroneous times
Ok I've used the following code which has worked to drop the offending times :
can you post the row for " 2006-04-02 02:00 : 00 " and for some of the rows you are trying to drop ? It seems that those data points are missing , and most probably because of DST as you said .
Your drop command doesn't look like it should work based on the description in the docs . To get rid of the offending times , I would create a mask on the dataframe , ie : #CODE
Probably there's a way to make drop work too . Check this related question :
Thanks ! This works to drop rows . A quick follow-up : I'm not really familiar with masks .. do you know how what syntax I can use to basically make it " and " other time windows that I want to omit ? Is there a way more concise than adding a mask for each time window ( eg . mask1 , mask2 , mask3 ) ? Thanks again !
Should change ix to index

If you want to join them all together after processing ( this might be confusing to others , by the way ) , just use ` concat ` after processing groups . #CODE

Sorry if this is too late to be useful . I work in archiving these files ( and use Python ) , so feel free to drop me a line if you have future questions .

convert to radians and shift everything up 1 row so that we can look ahead #CODE
` apply ` that function to each row , save in the original dataframe #CODE

I can pivot this table like so : #CODE
What is a better way of accomplishing a pivot resulting in a columns with prefixes as opposed to a ` MultiIndex ` ?

Apply multiple functions to multiple groupby columns

You might start by looking for conditional ` apply ` - there are plenty examples on how this can be done . Alternatively ` numpy.where ` can do ` if ... else ` replacement / assignment . And for comparing current value ( s ) with previous ones ` pandas ` has ` .shift ` method , which you could use with any of the first two approaches .

How can I achieve this ? Do I need to use merge , concat , or join ?

Thanks but I am not a fan of this solution for layout reasons . In my code my approach is supposed to look like this : `' - ' *myBoolSerVar ` . It is immediately clear what is happening : the `' - '` leads the expression ! Your solution kind of buries the lead by having `' - '` in the middle somewhere : ` myBoolSerVar.map ( lambda x : ' - ' if x else None )` . Also I think by using map , the operation is no longer vectorized and thus slower than could be ?
Pandas ' inbuilt ` map ` is explicitly for vectorised calculation across a series . I can't speak for your coding style , it's a matter of taste . Again I prefer to use the explicit ` map ` method when transforming one series into another series .
Thanks for letting me know about ` map ` being vectorized . I agree coding style is a matter of preference . Mine may be non-pythonic as it stems from using Matlab . That being said , any idea why my approach only work after casting ` myBoolSerVar ` to ` object ` ?

It's a list ... In [ 81 ]: type ( parsedSeries.ix [ 0 ]) Out [8 1 ]: list . I apply str.split to a df to create that list
The " best " solution probably involves not finding yourself in this situation in the first place . Most of the time when you have non-scalar quantities in a Series or DataFrame you've already taken a step in the wrong direction , because you can't really apply vector ops .

I need to parse the df [ ' DT '] into a Datetime and then a DatetimeIndex . It seems to work , but then keeps the two types of datetimes : #CODE
This tells u that the datetime format of variable { b } is wrong . so two choices here . the first one is to correct the str format ( modify " 24 " to " 00 ") , then apply the { pd.to_datetime } func : #CODE

I know about ` scipy.interpolate ` mentioned in this article ( which is where I got the images from ) , but how can I apply it for Pandas time series ?
Resample my ` tsgroup ` from minutes to seconds . #CODE
Interpolate the data using ` .interpolate ( method= ' cubic ')` . This passes the data to ` scipy.interpolate.interp1d ` and uses the ` cubic ` kind , so you need to have scipy installed ( ` pip install scipy `) 1 . #CODE
1 If you're getting an error from ` .interpolate ( method= ' cubic ')` telling you that Scipy isn't installed even if you do have it installed , open up ` / usr / lib64 / python2.6 / site-packages / scipy / interpolate / polyint.py ` or wherever your file might be and change the second line from ` from scipy import factorial ` to ` from scipy.misc import factorial ` .

To achieve this we can perform a left merge first : #CODE
Nice ! Now what if my number_y can have null values going into the merge ? Is there another way of detecting if the join was successful or not ?
The key to this was to detect when the rhs is not present in lhs , in that case you'd still have a column clash so I guess you could filter where all columns matched in values to your filtered merged df , e.g. after ` merged_null ` step , rename `' number_x '` back to `' number '` drop column ` number_y ` and you could do a reverse merge : ` merged_null.merge ( data1 , on =[ ' id1 ' , ' id2 ' , ' number ' , how= ' left ') this will ensure now that only the valid rows remain

I write this to flatten nested dictionaries . Might help you also . pk becomes a string of previous key , and current key with a ' to join them . a becomes a list of items . #CODE

I am using ` pandas.DataFrame.resample ` to resample random events to 1 hour intervals and am seeing very stochastic results that don't seem to go away if I increase the interval to 2 or 4 hours . It makes me wonder whether Pandas has any type of method for generating a smoothed density kernel like a Gaussian kernel density method with an adjustable bandwidth to control smoothing . I'm not seeing anything in the documentation , but thought I would post here before posting on the developer list server since that is their preference . Scikit-Learn has precisely the Gaussian kernel density function that I want , so I will try to make use of it , but it would be a fantastic addition to Pandas .

Typically for nested data types like this , I merge the inner data with the outer . In your case your inner data is reviews , which is something by itself can be represented nicely with a DataFrame . #CODE

Pandas : Generate a histogram / pivot against timeseries data
ii ) a count of results split by period and a second column value ( i.e. a pivot ? )
The question is , is there a simpler way of doing this ? And how about my part ii ) Perhaps there's a method using groupby and / or pivot ? I've read the docs on these , but I'm missing the point somehow . Any suggestions please ?
And for ( ii ) you can use ` pd.pivot_table ` , once you've created a ` month ` column which holds the year-month for you to pivot with . #CODE

Hmmm . Your first plot example doesn't use ` x= v.PER ` , so it should discard that , which is fine . Perhaps check what kind of index you have ( i.e. make sure it's a ` Float64Index ` or if it's something that would translate to categorical ) ? I feel like ` plot() ` should just work , unless you've hit a bug in pandas . If you post a copy-pasteable piece of the dataframe I can test it .

I get " TypeError : cannot compare a dtyped [ object ] array with a scalar of type [ bool ]"

I think you are a little confused here , the reason you need to reset the index is because if you didn't there would be nothing to match the mask index values against as pairs df is using your currency.pair string as the index value . the length is not the issue in this case , it's the fact that the index values cannot align with your mask index

Your ` apply ` approach would work too if you used ` x ` instead of ` df.radon ` : #CODE
Is it possible for them to differ ? ( Non-rhetorical , BTW -- I can't remember what methods are nan-aware and what ones aren't . I don't * think * ` len ` cares , but I wouldn't bet twenty bucks on it . )

Starting with this dataframe I want to generate 100 random numbers using the hmean column for loc and the hstd column for scale

I think I figured it out . Just do the merge twice , and make one of them based on the sample_id , one based on the sampled_id , and put one column into the other .

How to drop columns from dataframe having all values equal to False ( Boolean ) ?
My dataframe has 5 columns with all False Boolean Values . Is there any way to drop all 5 of them in one command ?

Pandas replace values
I want to replace all rows in the dataframe which do not contain ' pre ' to become ' nonpre ' , so dataframe looks like : #CODE
I can do this using a dictionary and pandas replace , however I want to just select the elements which are not ' pre ' and replace them with ' nonpre ' . is there a better way to do that without listing all possible col values in a dictionary ?

Use ` unstack ( ' cat ')` , followed by ` fillna ( 0 )` to replace the NaNs with zeros : #CODE
` stack ` moves column level values to index level values .
` unstack ` moves index levels values to column level values . So when you see you have a ` cat ` index level and you want ` cat ` values in a column level , you can quickly recognize what's needed is an unstacking operation .

Use concat and pass ` axis=1 ` and ` ignore_index=True ` : #CODE

Is there reason you don't just store the dfs in a list and then concat them all ? Also this line is unnecessary : ` dftemp2 = pd.concat ([ dftemp1 [ " source "] , dftemp1 [ " text "] , dftemp1 [ " timestamp_ms "]] , axis=1 )` you can specify the columns of interest in the params to ` read_csv ` , ` usecols ` : #URL
@USER Thanks , changed the code . Aside from speeding up the concat process from 2 hours to 10 minutes , it also solved the problem with the loaded csv - it runs at normal speed now ( for whatever reason ) :)

A much more dirty solution would be to fetch the string representation of the list object and just replace the u . I would not use that but it might befit your needs in this special case ;-) #CODE

This is a bit tricky due to the presence of the ` NaN ` , one method would be to sort the columns that have no NaNs and then sort the columns with NaN and concat them together , does that sound reasonable ?

My next attempt will be to reset the indexes on both ` df ` and ` sel ` and then use a join . Is that really the best way to do this , or is there a better trick that I am missing ?
Your intuition to use ` join ` is good . That's the Pandas-esque way of doing that : #CODE

hmmm .. what OS are you running ? On my Mac this method results in a big stack of plotting windows all stacked on top of each other

map the values on pandas dataframe using the values on the corresponding index in a list
I want to map this DataFrame #CODE

Is there some kind of " merge " or " join " like : #CODE
Then you could use Merge #URL #CODE
You can do a join the following way :
This does an inner join by default , so only the rows from ` data ` with days which appear in the ` days ` frame will be in the result .
And I get an empty dataframe .. this is probably a stupid mistake but I'm not sure I understand why the merge on ' days ' would return nothing .
Ah I see now . Ok I was able to merge but now I see all my minute data is missing from the index ( as it was there in the original " data " dataframe ) .. any ideas on how to merge but retain this index ?

I struggled with this problem for several hours to little avail . Ultimately , I wound up writing a nested for loop and solved the problem iteratively . Unfortunately , that solution is painfully slow and I'd much prefer something that utilizes nice features in Pandas such as groupby or apply .

This doesn't answer the question , because your comparison between ` m ` and ` mod ` does only compare ` m [ i ]` vs . ` mod [ i ]` . The question asks for comparison of ` m [ i ]` vs . ` mod [ j ]` for all ` i ` in ` range ( len ( m ))` and ` j ` in ` range ( len ( mod ))` .

If you want to have an object in a DataFrame that displays itself a certain way , you will have to write your own class that uses ` __repr__ ` to display itself the way you want , and then insert instances of that class in the DataFrame . The DataFrame just stores objects ; the way the objects are displayed is determined by those objects , not by the DataFrame ( except that the DataFrame will cut off the display if it is too long ) .

take a join of two csv files over a common column in python
the data in these two files is separated by ` , ` . What I want is to take a join of these two files over ` objectID ` . The output should have joined data and the data of file 1 which did not matched with file 2 . I tried this code but it is not giving correct output : #CODE
What am I doing wrong here and how can I do the join correctly
I think you are looking for a left join , try #CODE

` resample ` is grouping times according to the frequency , and then aggregating the associated values according to the ` how ` method , which is by default taking the mean . #CODE

I do not know how can I filter out the two columns from merged output of pandas merge above and form the list of lists form those two columns like above format ?

-> 1286 numticks = len ( self.get_major_locator() ( )) 1287 if len ( self.majorTicks ) numticks : 1288 # update the
in num2date ( x , tz )
343 tz = _get_rc_timezone()
--> 345 return _from_ordinalf ( x , tz )
in _from_ordinalf ( x , tz )
223 tz = _get_rc_timezone()
224 ix = int ( x )
--> 225 dt = datetime.datetime.fromordinal ( ix )
226 remainder = float ( x ) - ix
Length : 744 , Freq : None , Timezone : None

Please include a few rows / columns of the input ` df ` . Is the sample period constant - the dT for each successive sample is the same ?

So basically the ` dt ` attribute allows you to access the components of your datetime to perform the comparisons you desire for filtering

I have found workaround which is extremely slow due to the " in python " apply : #CODE

you put me on track #URL if I create a column dfmatches [ ' date '] = dfmatches.index.day then a mask = dfmatches [ ' date '] .shift() < dfmatches [ ' date '] I got what I want minus the fact it displayed the 1st day and not the last . Dunno why shift on index doesn't work the same way

I want the data separated by year and then the same function ( pivot ) is applied to each data set . Right now to produce a summary I am writing out code like this : #CODE
You could just get the unique values , convert to a list and then enumerate over this to produce your pivots . So something like ` vals = df.Year.unique() ` then you can just iterate over this and append to a list the pivot tables or add to a dict or something similar

The way I used to solve this was to output to a .csv , read it back in ( which makes it time zone naive but keeps the time zone it was in ) , then strip the + ' s .

This is not implemented directly , though you * can * use underlying PyTables methods . However , why are you not writing to a separate file , check that it has the correct / data , then repeat . Then you can easily concat these ( I suppose with the mark / undo ) , if you really really need . The risk of disk corruption is exceedenly low , better to simply try it , if it works ( as it will most of the time ) , you are done , and if their IS a problem . Then just start over .

Using IPython 3.0.0 and Python 3.4 , I found that ` display ( data )` as described by @USER will render as a table with up / down and left / right scroll bars , but the table is still wider than the cell and some columns are off-screen to the right . To see all the data , one must collapse the cell - which adds scroll bars . Consequently you have a scrolling box in a scrolling box , which is not ideal as you have to shift focus between the doubled-up scroll bars to navigate all the way through the data .

I do most of my data work in R , but am trying to do more work in Python / pandas . In R , merging on factors ( analogous to the categorical dtype ) induces type coercion , typically to character . This allows one data frame to have a by-variable ( join column ) specified as a factor ( categorical ) and the other to have its by-variable be a string . Does pandas perform similar coercion of categorical data to string prior to merging / joining ? Should I expect merging on categoricals to be robust ? Where can I find documentation on ( automatic ) type coercion in pandas ?
So in short , in 0.15.1 the merging behavior was changed ( fixed really ) to allow merging of Categoricals that had exactly the same categories . Further if an object array was merged in it is allowed , but the resulting character of the returned merge would now be object ( IIRC ) . I don't recall if we try to infer it back to a Categorical or not .

Apply a function to a specific row using the index value
How can i apply a function to the dataframes index ? I want to round the numbers for every column where the index is " c " . #CODE
I think your title is a bit misleading , what you are saying really is you want to apply a function to a specific row using the index value
For label based indexing use ` loc ` : #CODE

I think what you want to do won't work due to the shape of the returned values and expected return type . Another way would be to apply a lambda and concatenate the result : #CODE
@USER my edited answer ( 2nd ) answer is based on that question , the point being it is not something that works automatically , you have to coerce the return type and then merge / concat afterwards

python pandas : case insensitive drop column
I have a df and I want to drop a column by label but in a case insensitive way . Note : I don't want to change anything in my df so I'd like to avoid ' str.lower ' .
Is there any magic I can apply to the code below ? #CODE

One way I could conceive a solution would be to groupby all duplicated columns and then apply a concatenation operation on unique values : #CODE

This obviously changed the number format from datetime into tz datetime .

Now , I've made a little method that will take an input string and do this , spitting back the value I desire . However , it seems to be horribly inefficient . I'm using pandas for data manipulation and this method gets applied to a whole column of timeseries string data in the above string format . Calling the apply method via interactive shell finished execution in ~2sec , but strangely , letting the code run as compiled / interpreted on the same dataframe takes more like 15-20 seconds . Why is that ? This is how I'm calling it for the dataframe / series :

I know how to drop rows , #CODE
Moreover , if I try to drop ( just as an example , I need to keep the these rows ) multiple rows #CODE

Use ` startswith ` instead of ` contains ` : #CODE
Thank you very much , it work perfectly . Just one tip if you can . What if I had to replace this string with another string ? for instance sth like : dataset [ dataset [ ' Postcode '] .str .startswith ( " WC1 ") .replace ( " center ")] . However , this doesn't work

And I don't want to use ` xs = pd.Series ( data=range ( len ( ts )) , index =p d.to_datetime ( ts ))` instead of ` xs = pd.Series ( data=range ( len ( ts )) , index =p d.to_datetime ( ts ))` since I want to use some operation as #CODE
Thanks for your help . But I don't want to use ` Series ( data=range ( len ( ts )) , index=ts )` .
@USER . Actually your new approach is based on matplotlib rather than pandas . your ` xs.plot ( use_index=False )` can be replace by ` ax.plot ( xs.values )` . I wonder if there is a simple solution only need to change some parameter of pandas .

Now you could loop over the list of criteria , like ` [ df.Quantity [( df [ ' Date '] == a ) & ( df [ ' Delivery Beg '] == b ) & ( and so on )] .sum() for b in df [ ' Delivery Beg ']]` . Btw , if looping takes to long . you can use pivot to use ' Delivery Beg ' as a column index and then take the sum for each column . E.g. ` df = pd.DataFrame ( { ' Quantity ' : [ 1 , 2 , 3 , 4 , ] , ' val2 ' : [ ' a ' , ' f ' , ' c ' , ' r '] , ' date ' : [ dt.time ( 2 , 4 , 5 ) , dt.time ( 2 , 6 , 10 ) , dt.time ( 3 , 7 , 5 ) , dt.time ( 3 , 40 , 5 )] } )` . Then ` df [[ ' Quantity ' , ' date ']] .pivot ( index =d f.index , columns= ' date ') .sum() ` . Though not memory efficient , I found it often much faster than groupby .

As you can see in the first Dataframe there are a bunch coordinates as rownames and colnames . The objective then , is to map the line-grouped coordinates from the file in the big dataframe . For example , the following coordinate : #CODE

May be not the most elegant way , but you can merge grouped result with the initial DataFrame : #CODE

pandas pivot table with same rows and columns
I have a data frame . I would like to create a pivot table from this dataframe with both the rows and the columns of the pivot table equal to ` df [ ' event ']` . #CODE

You're getting NaNs in A because you're telling the constructor you want an index of three strings . But ` a ` has an index of its own , consisting of the integers ` [ 0 , 1 , 2 ]` . Since that doesn't match the index you've said you want , the data doesn't align , and so you get a DataFrame with the index you said you wanted and the NaNs highlight that the data is missing . By contrast , ` B ` is simply a list , and so there's no index to ignore , and accordingly it assumes the data is given in index-appropriate order .
And if you use a partially-matching index , you'll get values where the indices align and NaN where they don't : #CODE

means that you've given ` pd.DataFrame ` a series of length 13064 and an index of length 1 , and asked it to index the series by the index . Indeed , that is what you've done : ` date ` starts off as ` [ ]` , and then you append one value to it , so the index you're passing to the dataframe is just a singleton list .

When I look at the values of df [ ' Diff '] They all appear to be timedeltas . Any idea what is going on here ? It seems like creating an indicator based on the difference between two date fields should be easier than this ...
The values in ` df [ ' Diff ']` are numpy timedelta64s . You can compare them with ` pd.Timedelta ` s ; see below .
Moreover , you do not need to call ` df [ ' Diff '] .apply ( wdw )` , which calls ` wdw ` for each value in the Series ; you can compare whole Series with a ` pd.Timedelta ` : #CODE
Okay , I was wrong about being wrong :) You can't do a vectorized comparison against datetime.timedeltas because of the ` NaT ` s . But instead , you can do a vectorized comparison of ` df [ ' Diff ']` against a ` np.timedelta64 ` . I've edited the post above to show how .
Your ` x / np.timedelta64 ( 1 , ' D ') <= 10 ` is cleaner than my ` ( df [ ' Diff '] .values < np.timedelta64 ( 10 , ' D ') .astype ( ' < m 8[ ns ]')` ; I've changed my answer to use your comparison -- it generalizes to Series nicely .

No luck with several tries . This HDF5 was created directly with pytables , so maybe I will just drop back to pytables and read the arrays there and use pandas to write to SQL . Thanks .

I have tried ` numpy.histrogram ` ( which returned error : ` TypeError : ufunc add cannot use operands with types dtype ( ' m 8[ ns ]') and dtype ( ' float64 ')`) and ` hist ( series )` ( which returned error : ` KeyError : 0 `) .

Alternatively , instead of calling ` df.to_hdf ` , you could append to a HDFStore : #CODE
Once the array is created , you can use its ` append ` method in the expected way .

I want to replace the values in a column by a calulation ( which is a multiplcation of comlun values ) .

Where ' s ' is a single column of data in a datetime-indexed Pandas dataframe . In this case , ' s ' is position in meters , at 15 second time intervals . So , my window size is 40 lines , or 40*15 = 600 sec = 10 min . It is not clear what exactly the std argument refers to , but I assume this is in the frequency domain , and would be some value smaller than the window size , controlling the shape of the Gaussian curve ( regardless , I have experimented with many std values ; if std is very large , then no offset occurs , but this is because the Gaussian curve becomes so wide compared to the window , that you are essentially using a boxcar ) . The ' center ' and ' freq ' arguments do not appear to change output either way . Other optional arguments also seem irrelevant .
So , I am wondering if I need to find a mean value of my data for every window , shift the data to zero by that mean , then run the rolling_window function , and then shift both the original data and the smoothed data back to the original magnitude ..... any other ideas ?

I would like to normalize each measurement in the second dataframe ( df2 ) with its corresponding stock solution from which i took the sample .

since the index of ` pd.Series ( np.repeat ( 1 , len ( df )))` is ` Int64Index ([ 0 , 1 , 2 , 3 , 4 ] , dtype= ' int64 ')` .
without needing to create a Series ( in fact , ` df [ ' new '] = 1 ` will do for this case ) . Using a Series is helpful when you need to align the new column with the existing DataFrame using the index .

You can use the function ` repeat ` on Series to duplicate the ` id ` values in groups of three rows , and then use ` pivot ` to reshape the DataFrame : #CODE

How can I insert a dataframe in Excel using xlwings without pywintypes.com_error ?
The problem I encounter is that when I insert the data in the sheet there is an empty line at 5002 , again at 7503 , 10004 .... I realize there's a mistake in my code but I can't find it .
Currently I open the csv and insert the lines 15000 at a time , but it takes 90 seconds . The workaround seems more efficient . Is there a way to insert an existing sheet containing the data in the workbook I use ?

I've tried to index my Series by the values that aren't part of dt ( which is the variable I assigned to month end range ) but I wasn't able for some reason as I kept AttributeError : ' NotImplementedType ' object has no attribute ' dtype ' when I tried to run : #CODE

While if I replace #CODE

Pandas apply with argument that varies by row
I am attempting to apply a function to each row , where the function takes a ' size ' argument . #CODE
but is there a better way using apply functions ?
Why don't you make ` size ` another column in your data , so it is passed as part of the ` apply ` ?
This takes advantage of the fact that ` scipy.misc.comb ` can accept NumPy arrays as input . So you can call ` comb ( diff , size )` where ` diff ` is an array of shape ( 225 , 50 ) and ` size ` is an array of shape ( 50 , ) . Since ` size ` is only used in the calls to ` comb ` , it is possible to perform all the calculations with just two calls to ` comb ` . No looping per row required .

Python Pandas Linear Interpolate Y over X
Can Pandas interpolate ? And how ?
Pandas interpolate data with units
If you want to interpolate a " dependent " variable over an " independent " variable , make the " independent " variable ; i.e. the Index of a Series , and use the ` method= ' index '` ( or ` method= ' values '` , they're the same )

I am unable to specify the minimum size for the index in a to_hdf append operation . Min_itemsize works for the data columns , so how can I get it to work for the index column ?

You can create function and ` apply ` it to your dataset : #CODE

Pandas interpolate NaNs based on different column
Now I would like to interpolate to fill the NaN based on the wind_speed . For the example above I would expect to get 6.643773541
The idea is to use ` set_index ` to put the values you want to interpolate at in the index . Something like ` data.set_index ( ' wind_speed ') [ ' Velocity '] .interpolate ( method= ' index ')` . interpolate doesn't have a ` levels ` argument . Not sure if you need one here .

This error usually rises when you join / assign to a column when the index has duplicate values . Since you are assigning to a row , I suspect that there is a duplicate value in ` affinity_matrix.columns ` , perhaps not shown in your question .

What is the most efficient way to join multiple tables with the same index / col ?
I want to persist this dataframe on disk , so I am thinking maybe there is a better way using command line tools like sed / awk / cut to get a csv going which I can then just load . Any tips there ?
I would just read in the data from the CSVs , join , dump to a master CSV .
It's hard to say without you posting some concrete input and expected output but it sounds like the UNIX utility ` join ` is what you're looking for .
I was 50 / 50 on that but it seems like comments are best for asking for more information about or suggesting ways to improve a question or answer . I'm doing neither here , I'm suggesting the answer is ` join ` .
I think suggestions are valid comments , especially on open ended questions like this . To be fair the answer is probably join though !
Q . " What is the most efficient way to join multiple tables with the same index / col ? "
Unfortunately merge only takes two tables at a time , so you'd need to nest them or use reduce . Here's a merge that uses the index : #CODE
Here's a merge that uses the column `' a '` : #CODE
If you need to merge an indeterminate number of dataframes , you can use the ` reduce ` function , found in the functools module , with ` partial ` to set the column to join on ( as well as other parameters you desire ): #CODE

` read_csv ` accepts an argument named ` converters ` . This can be used to apply functions to particular columns as a file is read in . ` converters ` should be passed in as a dictionary of the following form : #CODE
You could use this to apply a function to the third column . All you need to do is set the function to get a value from a dictionary ` d ` which maps `" male "` to ` 0 ` and `" female "` to ` 1 ` : #CODE

Can someone point me to a link or provide an explanation of the benefits of indexing in pandas ? I routinely deal with tables and join them based on columns , and this joining / merging process seems to re-index things anyway , so it's a bit cumbersome to apply index criteria considering I don't think I need to .
Sometimes the index plays a role in reshaping the DataFrame . Many functions , such as ` set_index ` , ` stack ` , ` unstack ` , ` pivot ` , ` pivot_table ` , ` melt ` ,
Sometimes we want the DataFrame in a different shape for presentation purposes , or for ` join ` , ` merge ` or ` groupby ` operations . ( As you note joining can also be done based on column values , but joining based on the index is faster . ) Behind the scenes , ` join ` , ` merge ` and ` groupby ` take advantage of fast index lookups when possible .
Time series have ` resample ` , ` asfreq ` and ` interpolate ` methods whose underlying implementations take advantage of fast index lookups too .
So not just lookups , but also merge operations would be faster if index columns are used , right ?
Yes , join calls merge ( in most situations ) . Merging by index is faster than merging by columns because of the fast lookups . So ultimately it all comes back to the fast lookup ability .

where the " B " value can be variable len > = 1 . This says I have valid JSON .

This needs to be done to several million rows of data . Any thoughts on how to speed up the process ? I am using pandas data frame's map function to apply the function ` toTheExp ` to my column of data already . This step is still pretty slow though . Currently I'm trying something like : #CODE

YS-L so this would replace the existing month_date column rather than creating another one , with the dates in date format as a date-time obect ?
You can find more information [ here ] ( #URL ) . Generally , anything else other than ` float ` , ` int ` , ` bool ` , ` datetime64 [ ns ]` and ` timedelta [ ns ]` will be regarded as ` object ` . You need to access the individual element to find out the actual type .

A little unclear what you want , ` applymap ` is for a dataframe it doesn't make much sense to call ` applymap ` on a series when ` apply ` is specifically for this . You can get a df by using double square brackets : ` pd.DataFrame ( df [[ ' col ']]) .applymap ( isnan )`

pandas merge with MultiIndex , when only one level of index is to be used as key
I want to recover the values in the column ' _Cat ' from df2 and merge them into df1 for the appropriate values of ' _ItemId ' . This is almost ( I think ? ) a standard many-to-one merge , except that the appropriate key for the left df is one of MultiIndex levels . I tried this : #CODE
which I suppose makes sense since my ( left ) index is actually made of two keys . How do I select the one index level that I need ? Or is there a better approach to this merge ?

I have tried different ways such as stack / unstack and reindex but different errors occur . I'm guessing there's a clean way to do this . What's the best practice to do this ?

In general , because arbitrary rows get selected , both ` groupby ` and ` df.loc ` return new DataFrames . ` df.loc ` can select rows and columns based on values . ` df.iloc ` can select rows and columns based on ordinal location . Now that Pandas has ` loc ` and ` iloc ` I don't recommend ever using ` ix ` , since its behavior is not immediately apparent from the syntax .
ok thanks unutbu - i'll use iloc and loc from now on for rows and columns . So given that loc and groupby return new dataframes how does one access the January dataframe is it still just df.loc [ January ] as before in your comment ?

Right now , I am trying to replace a stored procedure with a Python service , and the temp tables with Pandas dataframes . But I'm stuck on this : #CODE
Apply FROM_UNIXTIME on column , c

is it possible to filter the columns as well in the same operation where the are sliced like : " df [ df [ ' category '] ! = ' purple '] [[ ' amount ' , ' freq ' == [ 3 , 5 ]]]"
you can do it like this ` df [( df [ ' category '] == ' blue ') & ( df [ ' amount '] == 5 ) & ( df [ ' freq '] == 2 )]`

Python : how to translate complex SQL aggregate statements into pandas ?

hey mwaskom , sorry typo . I find it odd that I can manually replace the ( i ) value with " green " , or " blue " when I copy the line of code and run it separately but it doesn't work in a loop . I've updated the question and added the error I got !

Python : merge lists or data frames and overwrite missing values

If you want sequential index , you can apply ` reset_index ( drop=True )` to the result .
Trying to work through Roman Pekar's solution step-by-step to understand it better , I came up with my own solution , which uses ` melt ` to avoid some of the confusing stacking and index resetting . I can't say that it's obviously a clearer solution though : #CODE
Output ( obviously we can drop the original samples column now ): #CODE

Get Row and Column Names ( argmax ) for max entry in pandas dataframe
Assuming that all your pandas table is numerical , something you can do is transform to its numpy interpretation and extract maximum locations from there . However , numpy's ` argmax ` works on the flattened data , so you will need to work around : #CODE
Transform table to numpy data and calculate argmax : #CODE

I think you want ` diff `
Just call ` diff ` : #CODE

How would I drop all the ` NA ` , ` Nans ` and ` 0 ` in the columns so I would get this : #CODE

Another method is to create a series from the columns and use the vectorised str method ` startswith ` : #CODE
To select only the required rows ( containing a ` 1 `) and the columns , you can use ` loc ` , selecting the columns using ` filter ` ( or any other method ) and the rows using ` any ` : #CODE

If you want to drop the date column , you can use #CODE

Pandas groupby strip timezone in index

pandas merge on index not working
When I try and ` merge ` them using
Hi @USER its the pandas in the ubuntu repository which I think is 0.14.1 . bw.join ( pw ) gives an error ` AttributeError : ' Series ' object has no attribute ' join '` which is why I was going down the path of merging ...
Ok , shoot . I had forgotten that join was a df only thing
Change the series into DataFrame then it is possible to merge #CODE
Or if the merge is to be performed in a parallel manner ( bw and pw have the same index , same number of items ) . #CODE
When you ` reset_index() ` a series , it turns to a DataFrame ( index to column ) . That is why you can merge after that .

So i have 5 columns in a dataframe that i like merge as a list in an existing column . A subset of the dataframe is below : #CODE
the code below perfectly fine , but i need to create a function that will do that for me rather than creating lambda functions for every block of columns that i like to merge . ( I still have other similar columns that i like to apply the same logic on them . ) #CODE

Name : start , dtype : bool
So you should use ` ix ` which allows you to be explicit in indexing by integer value ( or label ) and then specifying the column label you want to set the data : #CODE

The indices are non unique , but will always map to the same value , for example ' a ' always corresponds to ' 1 ' in my sample , b always maps to ' 2 ' etc . So if I want to see which values correspond to each index value I simply need to write #CODE
So long as your indices map directly to the values then you can simply call ` drop_duplicates ` : #CODE
EDIT a roundabout method would be to reset the index so that the index values are a new column , drop duplicates and then set the index back again : #CODE

python string replace conditional
I want to know how to replace values in a column using a condition ( a DataFrame in Pandas ) .

I want to merge several strings in a dataframe based on a groupedby in Pandas .
I don't get how I can use groupby and apply some sort of concatenation of the strings in the column " text " . Any help appreciated !
You can groupby the `' name '` and `' month '` columns , then call ` transform ` which will return data aligned to the original df and apply a lambda where we ` join ` the text entries : #CODE
EDIT actually I can just call ` apply ` and then ` reset_index ` : #CODE

Then slice data2 and append row to data2sub based on selection : #CODE
By implementing a simple ` getnearpos ` function from a previous Stack Overflow answer : #CODE

Is it possible to use percentile or quantile as the aggfunc in a pandas pivot table ? I've tried both numpy.percentile and pandas quantile without success .

there's no need for a ` groupby ` if that is your desired output . Simply do : ` selected_names = np.random.choice ( df.name.unique() , 2 , replace = False )` followed by ` df [ df.name.isin ( selected_names )]`

merge two txt files together by one common column python
how to read in two tab delimited files .txt and map them together by one common column .

How to shift entire groups in pandas groupby
I would now like to shift the whole thing down by n groups , so that their current order is preserved . The desired output for a shift of n=1 would be : #CODE
a shift of n=2 should be : #CODE
I have been messing around with groupby / transform / apply but haven't gotten anything to work so far . If I groupby and then shift , it shifts each group giving the output of : #CODE
This is an interesting operation . I can think of an alternative way to do it with ` replace ` .
To shift by 1 group : #CODE
To shift by more than one group , you just need to shift column ` b ` of ` x ` . If you want to shift by ` n ` groups , you need to shift ` x.b ` an additional ` n-1 ` times . Just insert the line #CODE
` ( * x.values.T )` is tuple unpacking of a NumPy array by column ( see [ this ] ( #URL ) answer for an explanation ) . It essentially uses columns ` a ` and ` b ` for the appropriate find / replace lists .

Ah , yes , I would have assumed that a comparison to NaT should always be False . That looks like a bug . Your way to replace it with a higher date seems OK to me .

EDIT I've even checked by calling ` duplicated ` on the dataframe after the drop , and all rows show ` False ` . As I would expect .
If you cannot show your data , please do at least copy the code you use when trying to drop the duplicates . At the moment it is very hard to help .

I'm trying to merge two dataframes which contain the same key column . Some of the other columns also have identical headers , although not an equal number of rows , and after merging these columns are " duplicated " with the original headers given a postscript _x , _y , etc .
Does anyone know how to get pandas to drop the duplicate columns in the example below ?
would adding more columns to merge on still give you the desired result ? ` merge_df = pd.merge ( holding_df , invest_df , on =[ ' key ' , ' dept_name ' , ' res_name ' , ' year '] , how= ' left ') .fillna ( 0 )`
The ` _x ` and ` _y ` columns originate from the left and right frames in the merge . You'll need to specify more columns to indicate that they're the same ( pandas doesn't know that ) .
You can pass a list of columns to [ ` drop `] ( #URL ) but rename will require passing a dict to [ ` rename `] ( #URL )
that indicates that the values do not agree or are missing from lhs or rhs , you therefore need to rename the ` _x ` columns and drop all the ` _y ` columns , you'll need to use ` drop ` and ` rename ` as suggested I can post a dynamic method to do this
The reason you have additional columns with suffixes ' _x ' and ' _y ' is because the columns you are merging do not have matching values so this clash produces additional columns . In that case you need to drop the additional ' _y ' columns and rename the ' _x ' columns : #CODE
If you added the common columns to your merge then it shouldn't produce the duplicated columns unless the matches on those columns do not match : #CODE
But they DO have matching values ! They have matching keys as well as matching values in exactly the columns which are duplicated , and then two additional columns which are only in the right dataframe and not in the left ( hence the merge ) .

Is there anyway to randomly apply changes of stings by row to a Pandas data frame .

Pandas use groupby to apply a different function for each value of the groupby variable
I'd like to use groupby , but instead of applying the same functions to each group , I want to specify which function to apply to which group value . I'm providing a very simple example here to illustrate the point , but in reality there are many values of my groupby variable , and my functions are all user-defined and fairly complex -- so solutions that involve selecting each group separately or apply the same functions to all groups will not be practical . ( Answers of that sort were provided to this very similar question : how to apply different functions to each group of pandas groupby ? but they don't address my question ) #CODE
This makes sense , but how do I specify function_map so that it contains functions in valid python syntax ? Or , alternatively , if I store the names of the functions as strings , how do I then pass them as functions to apply ?

i.e. to compute the value of C I need the previously computed value of C . This can be done by a simple for loop , but I would like to use map , apply or some other pandas functionality . Can this be done i a simple manner ?

What I need to do is collapse this frame into an Nx2 pandas DataFrame which has the index and column values that have ` True ` at the intersection as the record values . For example : #CODE

` pd.get_dummies ( df.index.month )` should do it . ( Make sure to drop one of the columns so you don't have linear dependence if you have an intercept ) . You can concat the result of that to the original df .

Length : 895 , Freq : None , Timezone : None #CODE
Drop down to NumPy datetime64s and use ` ndarray.__contains__ ` : #CODE

I have a dataset that contains individual observations that I need to aggregate at coarse time intervals , as a function of several indicator variables at each time interval . I assumed the solution here was to do a ` groupby ` operation , followed by a resample : #CODE
I've tried the new ` Grouper ` syntax , but it does not allow me to subsequently change the hierarchical indices to data columns . Applying ` unstack ` to this table :
See here : #URL this gives you the ability to arbitrarily group / resample ' at once ' , also here : #URL ( the groupby enhancements section )

I checked my code and there is nothing more special about it . And the pandas documentation shows the same code ( link inserted in the answer ) . The only crazy idea that I would check : can it be that you copied the command from my post and single quotes were replace by something else ( back quotes , for example ) ?

I'd like to be able to save the pandas datasets after I've downloaded them from the server . I think that will allow me to take better advantage of the limited memory available to me . My impression is that HDF is the best way to do this because it is integrated into the pandas library . However I've run into a few frustrating items . Because I don't have super amazing hardware , I have to chunk the results from the SQL server to save . Unfortunately , I have null values in some of my integer columns . Pandas doesn't support nan's in integer series as part of some design decisions , so it gets upconverted to float64 . The problem is that when chunking , some of the chunks won't have nan's and some will , so the chunks will have differing datatypes . This causes problems when trying to append the chunks into HDFStore dataset . So what is best practice when dealing with data like this in a memory-constrained world ? Should I write code to test for the best datatype for each column , then convert each column within a chunk to that datatype before saving to the HDF store ?
Thanks . In one of the comments , its suggested that you run the chunking twice . First time to detect the right datatype and then again to actually save it . I like this approach . Is there logic built into pandas that I can use that help me manage the promotions efficiently , or would I have to built my own promotion tree , test each chunk for the best datatype , and somehow merge the detected chunks dtype to the prior dtype and promote as needed ? What's best practice for this ?

How to combine pivot with cumulative sum in Pandas
Which i can easily pivot with the dates as columns using the following function : #CODE

Thanks @USER Paulo - Is there a way of specifying the size of the title when it is inside the df.plot ( title=MyTitle ) . if I move the plt.title ( MyTitle , size=20 ) outside of the PLOT brackets then in my for loop it creates a title above every subplot and I don't want that . I want a single title for a group of subplots - but I don't know how to apply the size property when it's in the brackets plot ( title=MyTitle ) .

I am brand new to pandas and working with two dataframes . My goal is to append the non-date values of df_ls ( below ) column-wise to their nearest respective date in df_1 . Is the only way to do this with a traditional for-loop or is their some more effective built-in method / function . I have googled this extensively without any luck and have only found ways to append blocks of dataframes to other dataframes . I haven't found a way to search through a dataframe and append a row in another dataframe at the nearest respective date . See example below :
I can tell you from experience that this will need to be performed in some kind of for loop / apply method . As there are no exact matches you have to find the appropriate index value to set the new column values , I would use ` numpy.searchsorted ` or you could use a filter , in ` 0.15.1 ` I think you can do some fancy filtering if you pass a range but not sure it applies to dates , worth a try though .

I want to apply this function on my data , stored in a data frame . However , the data frame consists many experiment subjects and 4 experiment conditions , while the outlier detection function should be applied on the level and for each subject + trialcode .
Is there a way to apply this function on groups of subject+trialcode ?

I think you want to ` merge ` on ` ID ` column : #CODE
If I have more ` DataFrame `' s similar to ` df1 ` and ` df2 ` , can I merge them directly , or only iteratively ?
Don't understand what you mean , if you mean if you have more than 2 dfs you can just iteratively merge them all
So there is no ` merge ([ df1 , df2 ,..., dfn ])` but only ` df1.merge ( df2.merge ( ... ( dfn )))` . Is that correct ?
Yes , I did look at ` concat ` which does take a list of dfs but the result is not quite what you want as it will introduce a hierarchical index or level in the columns which is not what you want .

Now I need to re-group this by ' trialcode ' and drop thhe ' subject ' column . I tried to do this with #CODE

pandas python : concet / merge / join 2 dfs on index
I have two dfs which I would like to concat on index to make a multiindex df . #CODE

where condition with inner join in pandas
Can you please help me on how to use the where condition with the inner join ? #CODE
If you look at the ` merge ` docstring , there is is option for suffixes on columns that have the same name . The defaults are `' _x '` and `' _y '` . So to filter where the two col4s are not equal , you could do : #CODE

I think your problem at datetime type , you have to normalize before you plot
You can replace month by year , day , etc ..
I think resample might be what you are looking for . In your case , do : #CODE
See this post for more details on the documentation of resample
pandas resample documentation

Convert freq string to DateOffset in pandas
In pandas documentation one can read " Under the hood , these frequency strings are being translated into an instance of pandas DateOffset " when speaking of freq string such as " W " or " W-SUN " .

I have been looking at ` resample ` ( see also this and this thread ) , since it looks like the right tool for the job . However , I don't have a good grasp of the function yet .
Can ` resample ` be used for this ? If so , how ?
If you don't want to give a specific set , but just all dates from the start to end of the original Series , you can use ` resample ` do reach the same : #CODE

Groupby and apply ` .mean ` multiplying by 30 : #CODE

Not sure about pandas , but you could do it in pure python . Tough , I don't know what you mean by " ( resample and fill the timestamp and the mean speed value )" . But without this , you could as follows : #CODE

And this indeed implies that appending a row is more expensive . In general , appending multiple single rows is not a good idea : better to eg preallocate an empty dataframe to fill , or put the new rows / columns in a list and concat them all at once .
See the note at the end of the concat / append docs ( just before the first subsection " Set logic on the other axes ") .
I added a link for concat / append , but for the internals , I don't think there is any good documentation .

switching to ` Continuum Analytics ` ` anaconda ` may be a good idea , too . ` anaconda ` is free and comes with a basic numpy-scipy stack and tons of scientific packages .

My problem is that I'm having trouble getting pandas to create a date column which I can then apply a timedelta to . Here's my offending line : #CODE

and I want to insert a new column into df1 that looks up the corresponding index values in df2 and inserts the value of column ' eins ' that has the same index value as df1 into the new column of df1.The result is supposed to look like this : #CODE

I thought I would use the reindex() function but I am uncertain on how to create the index I need to insert the missing elements . Any suggestions much appreciated .

How to properly insert NULL timestamps to postgres using pandas
I'm using ` pandas ` and ` sqlalchemy ` to insert data to postgresql , using pandas ` to_sql ` method . However , I encounter a DataError when the data has some missing values ( NULLs ) in date columns . #CODE
If i change the date column to ` str ` and map the NaT values to None ( they get transformed to the string ' NaT ' when casting ) , then I can call to_sql and everything works #CODE

I have a dataframe with some data ( names , amount_x , amount_y and " diff " column which subtracts amount X and Y .
I would like to color the cells , where " diff " is a positive number ( and make them green ) and where it is negative , make it red ) ?

I'm fairly certain only has 4 joints : inner , outer , left , and right . The default concat is to use ' outer ' . If you pass a specific index , I'm pretty sure it does a left / right type of join . Meaning if the row is not present in the index you pass , it will insert a blank row . If the index does not contain a row , it will be removed . I think .
You're right . Glanced over that . Replace all of my " rows " with " columns . "
The actual use ` join_axes ` is to replace the indexes of the dataframes that you want to concatenate with a different one ( or actually one per dimension ) .
does that ( with ` join =o uter `) .

The location of the legend is specified using the ` loc ` argument to ` legend ` . You can try using the `' best '` option and see if it avoids the lines . If you want to place the legend box outside your plot area , you can have a look at the technique described in the legend guide .

You don't need to change the index of ` d1 ` . Just make ` d2 ` a Series with ` a ` as the index . Then you can use the ` map ` method of Series : #CODE

Conditional join Pandas.Dataframe
I try to partially join two dataframes : #CODE
Yes , this is the output I want to have . However , I want to apply this procedure several times with different filter criteria to df1 . Using the above code snippet would overwrite older inserted values in ' eins ' everytime with NaN . Therefor I look for joining only those rows of df1 , which fulfill the filter criteria .

When you apply a function on the groupby , in your example ` df.groupby ( ... ) .agg ( ... )` ( but this can also be ` transform ` , ` apply ` , ` mean ` , ... ) , you combine the result of applying the function to the different groups together in one dataframe ( the apply and combine step of the ' split-apply-combine ' paradigm of groupby ) . So the result of this will always be again a DataFrame ( or a Series depending on the applied function ) .

Pandas : resample with an external Series
I want to ` resample ` a ` DataFrame ` containing intraday data on market volume and market prices using an external ` Series ` with a ` datetime ` in it .
Then the ` index ` column is extracted as a series to map ` Datetime ` column in ` df ` . There are ` Datetime ` values which are not a part of ` beginpoints ` , making corresponding ` point ` N / A . But since ` Datetime ` is sorted , we can use ` ffill ` to fill these N / A . #CODE
Thanks , clever solution combining ` reset_index ` , ` set_index ` and ` map ` . Works perfectly !

Now you can apply the condition like this : #CODE

You can append to an open file , as this question shows

What I'd like to do is merge them into a single dataframe with two columns : one ` key ` , the union of both ; and the other ` source ` , a list of which of the two original dataframes contained said key .
and then join them and calculating a new column : #CODE
If adding a column isn't an option you can use ` np.in1d ` after the merge . This gets most of the way there except you have empty strings to remove . #CODE

You need to ` apply ` your logic to each row , like this : #CODE

Imagine for example a year of temperature measurements , and you want to know the warmest 20-day periods that have occurred . The absolute warmest is easy , perform a rolling function and sort the result . But given the nature of how temperature changes , the second , third etc will likely be the same period with just a 1-day shift in window . Which is what i want to avoid , and so only end up with unique , non-intersecting , periods .

the product name is a merge of 2 cells of two rows " no of sales " and " sales value " for each of 1000 or so areas for a given month . Similarly there are separate files for each month for the last 5 years . Further , new products have been added and removed in different months . So a different month file might look like : #CODE
Now join the two rows together with ` . ` and assign that as the column level values : #CODE

Another way would be to use ` resample ` for each group of colors : #CODE

I have a relatively large DataFrame object ( about a million rows , hundreds of columns ) , and I'd like to clip outliers in each column by group . By " clip outliers for each column by group " I mean - compute the 5% and 95% quantiles for each column in a group and clip values outside this quantile range .
This works , except that it's very slow , presumably due to the nested ` apply ` calls : one on each group , and then one for each column in each group . I tried getting rid of the second ` apply ` by computing quantiles for all columns at once , but got stuck trying to threshold each column by a different value . Is there a faster way to accomplish this procedure ?
Thanks , that's a good pointer , I didn't realize scipy had a ` winsorize ` function . However , I presume a more substantial speed up would be achieved if there's a way to do the operation in bulk on the DataFrame without having to operate column by column , similar to how one could standardize or normalize in bulk , e.g. , #URL

I have data stored in a DataFrameGroupBy object . Therefore , I would like to apply the function to the entire column .
Also , the ` apply ` method of a GroupBy object doesnt have the ` axis ` keyword , die ` apply ` method of a DataFrame does .
@USER : I think I read in the linked stackoverflow question that ` axis ` has been added to the ` apply ` method of the ` GroupBy ` object . But this is obviously not the case , given the error message I get back from the Python console .

Note : in newer versions of pandas , use ` index ` / ` columns ` instead of ` rows ` / ` cols ` when creating the pivot table .
I get a 3x3 array after running ` df.groupby ( ' client_id ') .apply ( lambda x : x [ ' items '] .astype ( int ) .diff() ) .values ` with the diff values . Maybe thats why I cannot assign ` df [ ' deltas ']` to an array ? My pandas version is 0.14.1

Drop row if any column value does not a obey a condition in pandas
If I have a dataframe df with " 1 " to " x " columns and " y " number of rows . How do I drop any row where one or more column values are outside a conditional statement like greater than or less than :

Yes , you can use ` df.drop_duplicates() ` ( docs ) You can also pass it a ` subset ` list of column names so as to drop duplicates from those columns .

All you would have to do is merge these DataFrames : #CODE

merge data frames with duplicated indices
Is there any way to merge two data frames while one of them has duplicated indices such as following :
Use join : #CODE

You could set ` files ` as the index in ` df1 ` and then apply a function which uses ` loc ` to look up the ` pkid ` value corresponding to the index : #CODE
i actually meant to compare multiple columns , but failed to include that in my example . so , this solution works best in that case since i can just add to the on parameter of merge . thanks !
A faster method than @USER ' s is to use ` map ` here because you have a unique index then this will be much faster than calling apply which is essentially a for loop : #CODE
So you see that this is already 4 times faster and will scale much better . If you pass a series or dict as a form of lookup to map as a param , if the index is unique ( which in this case it is and keys have to be unique for a dict ) then the lookup will be blisteringly fast .
yep , this is MUCH better than using apply as its fully vectorized
Ah - I must remember to use ` map ` more often !
thank you for the solution ! does there happen to be a way to set and map according to multiple indices ?
Yes that will happen , map performs a lookup , so long as the column value exists in either the index of a series or key in a map as the param then it will set the value for all instances of that value

It is possible that pandas gets confused if your function sometimes returns a list and sometimes a single value , since different dtypes would be used for those two cases . It is probably better not to do it that way . The calling-twice behavior could be related to the issue described [ here ] ( #URL ) for ` apply ` : it calls the function twice on the first group in order to check whether the function mutates the existing data .
The strangest thing is , im reuse this code all the time with no issues . I know apply and transform pass different packets of data such that it is quite hard to ascertain from print statements what is going on , but agh is fairly straightforward . Were you able to recreate the error ?

I would like to insert a Duration column ( integer ) , defined as #CODE

I cannot send the data frame to the function row by row because there is a rank by group aspect to the algorithm . So I have to send at least one group of data at a time to the function . I tried groupby.apply but there were unexpected results due to the apply calling the function twice on the first group . So now I am using a lambda like this . #CODE
In the current implementation apply calls func twice on the first

Add Multiple DataFrames and Drop Mismatched Indices
The pandas DataFrame.add method has the following note : " Mismatched indices will be unioned together " but I'm looking for the most efficient way to drop mismatched indices .
I have multiple DataFrames , each with a DateTime index , but I only want to add together the values on dates that have observations in ANY of the columns in ALL of the DataFrames that I'm adding together . What's the most efficient way to do this ( i.e. drop mismatched indices instead of union them ) ? #CODE
Thanks . I should have been more specific in my question but really what I want to do is drop rows > ' 2014-03-31 ' since I only have data for those periods in one DataFrame .

When rows change , mark their ` activeState ` field as ` False ` and insert a new row with ` activeState=True ` .

You could use ` groupby ` to group by User , ` value_counts ` to compute a histogram , and ` unstack ` to reshape the result : #CODE

pandas resample .csv tick data to OHLC
I want to resample into Daily OHLC using pandas so i can import it into my charting software in the correct format .
Can you help me convert the data in the fomat i have into OHLC with pandas resample .

Yeah you need to replace " column name " with the column you actually want to plot .

I have tried variations of concat etc with no success . Is there a different way to think about this ?
My recommendation would be to concat the list of dataframes using pd.concat . This will allow you to use the standard group-by / apply . In this example , multi_df is a MultiIndex which behaves like a standard data frame , only the indexing and group by is a little different : #CODE

I'm not sure of the most efficient way in pandas to map my timeline of events into something and plot it . I suppose I could convert the timestamps into duration ( e.g. number of seconds ) , but I'd prefer the x-axis to display timestamps as per the first example above , so that may not work .

Whenever you can , try to phrase computations as operations on whole columns rather than rows , or item-by-item . Instead of handling each value in ` reindexed [ ' arrival_time ']` one at a time , you can convert the whole column into ` datetime64 ` s using ` pd.to_datetime ` . A Series of ` datetime64 ` s has a ` dt ` attribute which allows you to access the hour and minutes as integers . So you can express the calculation for the whole column like this : #CODE
So that seems to work for the values that exist , but seems to assign a value of -61 for all empty spaces / null values and doesn't interpolate between the actual values .
Upon further inspection , whenever pd.to_datetime() is called , it's replacing all NaNs with NaTs , which seems to cause interpolate to fail . However , I still can't get interpolate to work .

Finding it hard to strip just the date out since I am working with two columns rather than two values . Any pointers ?
By " Finding it hard " I mean that strptime on the x [ ' datex '] doesn't work because those are series and not values and I can't apply it to the x in " lambda x " or use %Y%M%d instead of %s . An example would be date1 = datetime.datetime ( 2014 , 1 , 1 ) and date2 = datetime.datetime ( 2014 , 1 , 3 ) .
Finding it hard to strip just the date out since I am working with two columns rather than two values
Well , since you're already using ` apply ` , you're dealing with two values ( not columns ) , so you can call the ` date ` method on each : #CODE

Now if you ` unstack ` this reindexed DataFrame and take the transpose , you have a DataFrame where each column is an entry of ` i-j ` and contains the counts per day : #CODE
To finish , you can ` stack ` this frame of rolling means to get back to the shape of the original reindexed ` df2 ` : #CODE

python / numpy / pandas fastest way apply algorithm for expanding calculations

You can group by ` Location_Id ` then pivot on ` date ` and ` Item_Id ` and get the correlations : #CODE

` loc ` uses index label indexing , -1 and ` len ( df ) +1 ` does not exist hence the error , for integer based indexing use ` iloc ` , also if you repeatedly appending rows this will be horribly inefficient
Thanks !. For future reference , I could resolve that using ' concat ' using a new df2 .

Pandas get_dummies to output dtype integer / bool instead of float

You can replace that inner for loop with : #CODE

and I get ' Exception in Tkinter callback ' with a long stack crawl ending in #CODE
Since you didn't specify a tz argument , I'm guessing its trying to parse it out of your datetime . Just a guess . Can you post an example of your datetime format ?
These are datetime.time ' s , so TZ doesn't really make sense to me .

Given a Pandas ` MultiIndex ` instance ` my_index ` , I need to detect whether its list-of-tuples representation ( i.e. , ` my_index.tolist() `) contains any duplicate tuples . While this can be done using ` len ( pandas.unique ( my_index )) len ( my_index )` , I was wondering whether the ` MultiIndex.has_duplicates ` property can be used for this purpose . Although the corresponding unit test in the Pandas source code seems to suggest that this is the case , I'm not sure why the the property is True for the following example which contains no duplicate tuples . ( I'm using Pandas 0.15.2 on Linux with Python 2.7.6 . ) Am I misunderstanding the purpose of the property ? #CODE

In other words I'd like a function ` f ` so that I can apply ` f ` to a series ( or multiple series ) as well as applying ` f ` to a float ( or multiple floats ) , and ideally apply ` f ` to a combination of floats and series .

I'm writing several pivot tables using pandas . For many of them , I need to return unique values . In a two-dimensional pivot table , the below code works as it should . When I add a third dimension , the code returns the count rather than the unique count . I suspect this has something to do with the aggfunc , but can't determine to what it should be changed .
Use a groupby to get at each combination of ` col_1 ` and ` col_3 ` , then unstack to get the ` col_3 ` values as columns : #CODE

I understand that for the rm columns the 1st 4 items aren't filled because there is no data available , but if I shift the column calculation should be made , shouldn't it ?
You can change the order of operations . Now you are first shifting and afterwards taking the mean . Due to your first shift you create your NaN's at the end . #CODE

However I am struggling to figure out how to apply one function ` convertToMeters ` to the first column and ` convertToNewtons ` for the second column .
it will apply the respective function to each column and not just the desired column .
Can you show how ` convertToMeters ` looks like ? Probably you can write this function so that you can just do ` df [ ' col_meters '] = convertToMeters ( df [ ' col '])` without using the apply .

What I need is to drop rows from each group , where the number in column ` B ` is less than maximum value from all rows from group's column ` B ` . Well I have a problem translating and formulating this problem to English so here is the example :
So I want to drop row with index ` 0 ` and keep rows with indexes ` 1 ` and ` 2 `
So I want to drop row with index ` 4 ` and keep row with index ` 3 `
Sorry I was a little bit busy when writting this question . Data are correct now . Well I mean just to delete rows from groups and keep these groups as they are - I need to apply several filters and after each apply is needed new groupby .
You just need to use ` apply ` on the ` groupby ` object . I modified your example data to make this a little more clear : #CODE
` B_maxes ` is a series which identically indexed as the original ` df ` containing the maximum value of ` B ` for each ` A ` group . You can pass lots of functions to the transform method . I think once they have output either as a scalar or vector of the same length . You can even pass some strings as common function names like `' median '` .
So , for example , if you want all rows in above the median B-value in each A-group you call #CODE

works , although I also had to drop the penalty term .

You can use a pivot table #CODE

For just one of these columns , I need to replace the NaN immediately before a non-NaN value with a value from another column .

pandas : iterating over DataFrame index with loc
Similarly if I create a column to store the int day value and then perform the apply then it works also : #CODE
@USER that I don't know , ` ix ` first tries label based and then integer for based , for your info ` iloc ` also shows this error so it's puzzling why ` ix ` works

Works great , thanks a lot . I'm not really sure what to do with the second code block , though . Does this replace the ` s-s [ 0 ]` part ?

I guess you could easily turn this into a function to apply on a dataframe inplace .
Please see edit in the answer to see the fix for this error ( since you added more columns ` transform ` was being applied on a GroupbyDataframe and not on the Series ) - all it takes is to pass name of the column after ` groupby ` to apply ` transform ` on .

It seems I can apply some functions without problems to a DataFrame , but other give a Value Error . #CODE
The first apply works fine , the second one generates a :
I know I can generate the " max ( df , 0 )" in other ways ( e.g. by df [ df 0 ]= 0 ) , so I'm not looking for a solution to this particular problem . Rather , I'm interested in why the apply above doesn't work .

To add process-based parallelism , we bring in the ` Pool ` class from the ` multiprocessing ` stdlib module , and pass the ` Pool ` instance's ` map ` method as a keyword argument to ` compute ` : #CODE

` pd.to_datetime ( dt [ ' Date '])` and ` pd.to_datetime ( str ( dt [ ' Date ']))`
Actually it's quicker to convert the type to string and then convert the entire series to a datetime rather than calling apply on every value : #CODE

Right ! And that is exactly what you wanted . In my solution I have : data [ np.isnan ( data )] = dfrand [ np.isnan ( data )] . Translated , this means : take the randomly-generated value from dfrand that corresponds to the NaN-location within " data " and insert it in " data " where " data " is NaN . An example will help : #CODE

How can i apply do_calcuations without loops like this . Loops like that is discourage in panda because slow , right ?
groupby these event numbers and apply ` do_calculations ` to each group .

Concat float with string in pandas

How do you stack two Pandas Dataframe columns on top of each other ?
You can select the first two and second two columns using ` pandas.DataFrame.iloc ` . Then , change the column name of both parts to ` c ` and ` d ` . Afterwards , you can just join them using ` pandas.concat ` . #CODE

Decision Tree Cut Points
I have been looking at scikit-learn and have been trying to work out how to output an array or a dictionary of the cut points for each level of a decision tree . I can see how to generate an image of the tree but I am looking to use these cut points for further feature engineering .

I have found a way that works but I'm not sure why it works like this , if I convert both series into dataframes then join them it works as expected : #CODE

Apply vs transform on a group object
In other words , I thought that transform is essentially a specific type of apply ( the one that does not aggregate ) . Where am I wrong ?
You asked ` .transform ` to take values from two columns and ' it ' actually does not ' see ' both of them at the same time ( so to speak ) . ` transform ` will look at the dataframe columns one by one and return back a series ( or group of series ) ' made ' of scalars which are repeated ` len ( input_column )` times .

X and Y are actually pairs of coordinates and the function I would like to apply is the vincenty distance from the geopy package .
And another problem : Even if I just want to apply the vincenty formula to the series in my dataframe , I receive an error message : #CODE
The problem is that I have not clue how I get these errors . I can give four single values as coordinates to the vincenty formula or a list or a string and it will work . But the only way I can apply the formula to several entries is using lists .
Gives you a generator with the values , which you can reshape and print as you need . Substitute operator.mul with the pertinent function you need to apply .
Beware that the way in which you add new axes to your data implies a transposition in the result matrix and what you want to do depends eventually in how you like to read the result , as in algebra ( ` x ` is the row and ` y ` is the column ) or just like a map ( ` x ` is longitude and ` y ` is latitude ) #CODE

Try read the whole file and put it in the list , then you can loop the list and strip data . Then you can write back to new file . Look at this : #URL

Use the ` shift ` method : #CODE

pandas data frame indexing using loc

IIUC , your function probably doesn't support sequences of strings as input , only strings . You can use ` apply ` to pass the values individually : #CODE

I'm trying to translate a simple linearised least squares problem to statsmodels , in order to learn how to use it for iterative least squares :
The ( contrived ) data comprise measurements of the time it takes for a ball to drop a given distance . #CODE

You may want to use ' value_counts ' to count the number the instances of a particular time event and then resample the dataframe to fill na , like so , #CODE

How to replace non-ASCII by ASCII in pandas data frame
I would like to replace some non ASCII characters in the pandas Data Frame . #CODE
I would like to create some replace rules : eg . -> ss and -> UE
One way would be to create a dict and iterate over the k , v and use ` replace ` : #CODE

It works for the first row if all the strings are inside one double quotes . But it doesn't apply for the third and second row if there're commas outside the quotes ( single or double )

I could do a left merge , but I would end up with a huge file . Is there any way to add specific rows from df2 to df1 using merge ?
Unclear why you think a left merge would produce a huge file , by performing a left merge on the product id you are stating that you are only interested in matches in the product_id column only
Just perform a left ` merge ` on ' product_id ' column : #CODE

The reason this happens is because ` NaN ` cannot be represented by an integer , once this is present the dtype will be converted to the appropriate type , in this case float . So you have to make a decision whether to drop the ` NaN ` values , replace them with something like 0 or other value that can be represented by an ` int `

... than a pivot table like this : #CODE

Is this a general question about how to return multiple columns using your function or is it just to achieve your result ? For example : ` df [ ' sum '] , df [ ' prod '] = df.sum ( axis=1 ) , df.prod ( axis=1 )` gives you what you desire
One way to do this is to pass the columns of the DataFrame to the function by unpacking the transpose of the array : #CODE
You could write ` df [ ' sum '] , df [ ' prod '] = sumprod ( df [ 0 ] , df [ 1 ] , df [ 2 ])` to get the same result . This is clearer and is preferable if you need to pass the columns to the function in a particular order . On the other hand , it's a lot more verbose if you have a lot of columns to pass to the function .

Merge and fill two dataframes in Pandas with different time intervals
I have two dataframes I'd like to merge in Pandas . They both have a datetime column that I am merging on , however , one has rows every minute and 5 minutes ( depending on the year ) , while the other dataframe has rows every 15 minutes . If I do an outer join , I can merge them , but only the rows every 15 minutes will have data from both dataframes . I'd like to copy the 15 minute data and fill in each of the 1 or 5 minute rows with this data . So , data from 12:00 AM would populate all rows up to and including 12:14 AM . Then , 12:15 AM would be copied and fill in up to 12:29 AM etc . Does that make sense ? How can I accomplish this ? This question seems similar but I'm not sure how to implement for my exact scenario especially given that my one dataframe changes from 5 minute to 1 minute intervals for different years .
You want to resample the two dataseries so that they have the same interval and fillna with method ' ffill ' #CODE

Assuming the len of col1 and col2 would be equal

I have a pandas DataFrame with a mix of numerical ( float ) and text columns . Some of the numerical values seem to be off by a factor 10 and I want to modify them in place . I can do with apply , but I was wondering if there is any way to only indexing instead .
You probably want to replace this with , #CODE
This works of course , it is similar to the apply solution , it just make the loop over the columns explicit . I am still surprised that this is a faster than a solution with no ( apparent ) loop . Thanks for the performance numbers .
I may be doing something wrong , but on my machine I got the following timing information : 1.58 ms for apply on subset of columns , 62.3 ms for fillna , and 2.65 ms for the explicit loop .

You could replace the ` 0 ` values with ` NaN ` , then forward fill them , also it's useful to post data and code , see [ ` fillna `] ( #URL ) , so I think something like ` df [ ' Total_Data '] .replace ( 0 , NaN )` and then ` df [ ' Total_Data '] .fillna ( method= ' ffill ' , inplace=True )` I think should work

Second , we're going to use the dataframe method ` apply ` . What ` apply ` does is it takes a function and runs every row ( axis=1 ) or column ( axis=0 ) through it , and builds a new pandas object with all of the returned values . So we need to set up ` haversine ` totake row of a dataframe and unpack the values . It becomes : #CODE
Now we need to make rows with two sets of coords . For that we'll use the ` shift ` method and join the result to the original dataframe . Doing that all at once looks like this : #CODE
` rsuffix ` and ` lsuffix ` are what append " 1 " and " 2 " to the column names during the join operation .
So now we can ` apply ` the Haversine function : #CODE
I changed up a few things : moving inrad = df.applymap ( radians ) out of the function and into the preprocessing means it isn't performed 4000+ times in my real set . Then , instead of a list comprehension , data = np.empty ( len ( df.index )); data.fill ( df.latitude [ column ]); lat1 = pd.Series ( index =d f.index , data =d ata ) seems to be a bit faster . Thanks for your help !

I'd like to convert a Pandas DataFrame that is derived from a pivot table into a row representation as shown below .
to get rid of the multi-indexes , but this results in this because I pivot now on two columns ( ` [ " goods " , " category "]`) : #CODE
hmmmm , melt ( my answer ) loses month from the index :(
Thanks . It works fine with my example . However , I don't quite understand why using ` stack ` previously worked and I now should use ` unstack ` .
It seems to me that ` melt ` ( aka unpivot ) is very close to what you want to do : #CODE
There's a rogue column ( stock ) , that appears here that column header is constant in piv . If we drop it first the melt works OOTB : #CODE
@USER I wouldn't say it's a bug , but could potentially be a feature / enhancement ( to not drop index ) . Also , you can't pass things like ` col_level =[ 1 , 2 ]` which would be useful here .
Great - thanks a lot . I'll try this out , as I now have to select either ` stack ` or ` unstack ` depending on the number of grouping elements . ` melt ` will be more streamlined . Sorry , I can only accept one answer ( + 1'ed your's ) .

` dropna() ` is the same as ` dropna ( how= ' any ')` be default . This will drop * any * row which has a NaN . ` dropna ( how= ' all ')` will drop a row only if * all * the values in the row are NaN .

although ` .fillna ` is not efficient in here , because it is only the first value which comes out null . so , you just need to replace the first value with cumulative value at ` .iloc [ 0 ]` .

If someone can suggest a better column truncate function that would be welcomed too . FYI , I have a very large HDF dataset that that is appended with new logs . The new logs have some string fields that may exceed the maxsize of the HDF dataset column . So in effect the root problem is merging in many logs ( over time ) into a single format .
If you just want to truncate the col then ` df [ col ] = df [ col ] .str [ 0 : maxsize ]` I think should work and it will be vectorised so should be fast
Tried above . Decided to use it as it seems to be best way . But message still occurs . I fixed code example to align with this . Any tips on how to debug this ?

thats the thing . i have done so much and am confused what to do . i mean i can go the c++ way and iterate through elements of the dataframe and store and append it to a list . i was just wondering if there was a more elegant pythonistic way to do this . i even tried to collapse by dataset but nothing works .
Now you can use ` stack ` : #CODE

I have a pandas series ` series ` . If I want to get the element-wise floor or ceiling , is there a built in method or do I have to write the function and use apply ? I ask because the data is big so I appreciate efficiency . Also this question has not been asked with respect to the Pandas package .

` value_counts ` seems a strange choice in nunique , ` Series.unique() ` doesn't sort , so you could just call len on that ( in O ( n )) ?

You can drop them using ` dropna ` : #CODE
Sorry , forgot to mention the first column ( which I used as label and really do not need the index ) is str . Can I ignore that while apply the dropna or use that column as axis ? take a look at the example file at the link ? #URL
If you read it in as a csv you specify this col as the index : ` df = pd.read_csv ( ' file.csv ' , index_col=0 )` or just set the index after ` df.set_index ( keys= ' col1 ')` then you can drop the rows as per my example

Drop contradicting duplicates from a pandas dataframe

Is it feasible to apply your techniques when no_row=1600000 and no_colors=230000 ?

I want to plot a date agnostic graph of the day wise trends on a time only axis . Towards that end I have resorted to ` shift ` ing the data back by an appropriate number of days as demonstrated in the following code #CODE

It won't do this unless its necessary ( as its a tiny bit computational ) , e.g. when you resample . You can see what it is , by doing `` df.index.inferred_freq `` . However if it is STILL `` None `` . then it is not a regular frequency . You might want to reindex to make it one .
You can recover a regular frequency by doing this . Downsampling to a lower freq ( where you don't have overlapping values ) , forward filling , then reindexing to the desired frequency and end-points ) . #CODE

Please show the full stack trace leading to the ` TypeError ` as otherwise it's impossible to tell which code is erroneously trying to set that ` dtype ` .

The next way would be to keep two separate data structures : one for Items and one for Categories . The Items data structure is a hash map indexed by the item ID . Each item holds a list of category identifiers for the categories that the item belongs to .
The Categories is a hash map that is indexed by category ID . Each Category entry holds a list of the items that belong to that category .

so what is an easy way to go about converting the time from a unix timestamp to a useable time format for the resample code ?
Yon convert a unixtimestamp by using pandas to_datetime . You can read in the timecode from the csv as an integer and then apply pd.to_datetime #CODE
so , thanks for this . I got the time converted into this format 2014-12-19 23:19 : 07 453.3 0.050 but i cant seem to get the resample code to work . It still errors with " TypeError : Only valid with DatetimeIndex , TimedeltaIndex or PeriodIndex " , any idea why ?

Note I had to drop the ` NaN ` row as this will introduce a mixed dtype which cannot be used for ordering e.g. float > str will not work

[( A1 [ i ] , A2 [ i ] , A3 [ i ]) for i in range ( len ( B )) for _ in range ( B [ i ])]
For len ( A )= 1e6 , I find that the itertools solution is about three times slower than the list comprehension . The itertools syntax is also awfully hard to read in comparison . I could live with the ugly syntax if there were a performance benefit , but for this problem there doesn't seem to be .

Some people like using ` stack ` or ` unstack ` , but I prefer good ol ' ` pd.melt ` to " flatten " or " unpivot " a frame : #CODE

Sorry about the ugly looking code , I ' m using the mobile Stack Exchange app . What I would like to do is to convert the data object into a data frame which columns are the first 5 elements of each data object list . Can you help ?

Then ( since False == 0 and True == 1 ) we can apply a cumulative sum to get a number for the groups : #CODE

and replace #CODE

How would I take the dataframe and cut the data into two dataframes ? One for responses and the other for Nos , for each combination of skill and login with bins of 50 increments . Need to look at each segment of 50 COMM_IDs ( 50 , 100 , 150 ) and count how many 1s are in each of those segement . This would be for every skill / login combo . The SUM of the 1s would then go in the dataframe per the bins .
sorry it was late when I wrote this .... I edited the question . " How would I take the dataframe and cut the data into two dataframes ? One for responses and the other for Nos , for each combination of skill and login with bins of 50 increments . "
There are lots of ways to apply this to your problem , but if I understand your approach correctly -- a straightforward application that follows your structure would be something like this : #CODE
Made a ' BINS ' column and used cut to group them : #CODE

Or , You can make DataFrame from Series and concat then #CODE

Pandas - Replace outliers with groupby mean
I have a pandas dataframe which I would like to split into groups , calculate the mean and standard deviation , and then replace all outliers with the mean of the group . Outliers are defined as such if they are more than 3 standard deviations away from the group mean . #CODE
Note : If you want to eliminate the 100 in your last group you can replace ` 3*std ` by just ` 1*std ` . The standard deviation in this group is 48.33 so it would be included in the result .

I would like to ` bfill ` and ` ffill ` a multi-index ` DataFrame ` containing ` NaN ` s ( in this case the ` ImpVol ` field ) using the ` interpolate ` method . A section of the ` DataFrame ` might look like this : #CODE
For those of you not familiar with the domain , I'm interpolating missing ( or bad ) implied option volatilities . These need to be interpolated across strike by expiration and option type combination and cannot be interpolated across the entire population of options . For example , I have to interpolate across the ` 2014-12-26 ` ` call ` options separately than the ` 2014-12-26 ` ` put ` options .
I was previously selecting a slice of the values to interpolate with something like this : #CODE
but the frame can be quite large and I'd like to avoid having to loop through each of the indexes . If I use the ` interpolate ` method to fill without selecting a slice ( i.e. across the entire frame ) , ` interpolate ` will interpolate across all of the sub indexes which is what I do not want . For example : #CODE
I'd try to unstack the data frame at the OptionType level of index . #CODE
If multi index df is the most desirable one for further computations , you can restore the original format using stack method .
you'll need to unstack twice to have the date in the column labels too ; it won't make sense to interpolate across dates if the strikes are in ascending order .

I can replace any occurrence of ' a ' with ' AAA ' as follows : #CODE
You are using two different things in both cases : ` df [ ' Letters ']` vs ` unique_df ` in the iteration / assignment . So in the second case it tries to set the ` i ` th column ( it is ' Letters ' , not ' First ') . If you replace ` unique_df ` with ` unique_df [ ' Letters ']` , it works . But anyway , you should just better do ` df.loc [ df [ ' Letters '] == ' a ' , ' Letters '] = " AAA "` instead of the for loop .
Great . Assuming I want to replace more than one value , is there a way of doing it in one swoop instead of stepwise as below : ` unique_df.loc [ unique_df [ ' Letters '] == ' a ' , ' Letters '] = " AAA "

So I want to change freq of data for of every 15 sec that price at given time ( like 15:30 : 15 ) is last price for every ticker . #CODE
` asfreq() ` is just a wrapper for ` resample() ` . Something like ` resample ( ' 15s ' , fill_method= ' pad ')` should work . Using some abbreviated data from the above : #CODE
If you want to start at 15:30 : 00 , you can drop the first row from the DataFrame .

First , I'm wondering if I can achieve the same result more efficiently using ` merge ` , ` zip ` , ` join ` or the other concatenation functions . Second , if I provide a matching ` else ` to the ` if ` statement , as follows :
The fastest method would be to set ' Letters ' as the index for df_a and then call ` map ` : #CODE
Calling map here is nearly 18 times faster , this is a vectorised function and will scale much better .
Thanks greatly . I see clearly now , esp . with the printouts . In terms of efficiency in this given example , how does the nested loops and map stack ? Perhaps you can talk in terms of big O .
Well your solution is O2 as you loop over 2 loops , this I suspect will be between On and nlogn , ` map ` is a vectorised function so will operate on the whole array rather than a row value at a time
Thanks for the time analysis ! I applied map in my actual dataset and for some reason my target column is not being filled in . I've something like this ` df_1 = df_1.set_index ( " Visit ") ; df_2 [ ' Result '] = df_2 [ ' Visit '] .map ( df_1 [ ' Result '])` . Not that you can glean much from that . Any random thought why the Result column in df_2 might come in as blank ? Before applying ` map ` , I ensured ' Visit ' columns in both data frames contain [ unique values ] ( #URL ) using ` groupby ` .

I think I need to use the apply method to trim the column data . So if there is anything after the period keep the data unchanged but if there is nothing after the period then return just the letters without the period at the end . I know I can probably use a lambda function and maybe a string split or something to do this but have not much of an idea to make it happen .
Otherwise you'd have to do something like ` df.applymap ( lambda x : x.rstrip ( " . "))` , or drop down to numpy ` char ` methods .

But that does not work , is there some other way to do what I need ? I think my issue is that method is for a series and not a column of values . I tried apply and could not seem to get that to work either .

Note that I'm preserving the dates of the " closest prior price , " instead of simply using ` pandas.asfreq ( freq = ' q ' , method = ' ffill ')` , as the preservations of dates that exist within the original ` Series.Index ` is crucial .
This seems like a silly problem that many people have had and must be addressed by all of the ` pandas ` time manipulation functionality , but I can't figure out how to do it with ` resample ` or ` asfreq . `
for the last non-NA value . You can then join these together . As you suggested in your comment : #CODE

this is my first question at Stack Overflow .
You could use ` stack ` to flatten the frame to a series , use boolean indexing to select the terms you want , and finally turn the resulting index into a list : #CODE

I had an idea : do you think I should just manually create the date_range myself and then append the values to the range ? If so , I suppose I will have to worry about DST and other shifts

I would like to normalize my data by dividing every row by the first value of that very row .

Stack Trace of error : #CODE

This is an example of a ` pivot ` operation : #CODE
For example lets say we have added a column to the left hand of the table with date of purchase . How could this be handled ? I guess stack is the answer but again I don't know how ! Either I am dumb or the documentation is terrible !

@USER what I meant there is : imagine a week starting from Sunday to Saturday . I want to be able to map all these timestamps to the corresponding #URL just specific to the week , irregardless of which week they may be . Does this make sense ?

I also tried using the apply function , but very likely I am doing it wrong . My guess is that I am either not applying the functions correctly for a column or the values I am getting arent integers . I have been trying for days so now am breaking down to ask for help ....

Thanks . I can see that I could easily replace the res.params . Do you know how would I create a new res object ? I suspect it would need to have a model as well ? I am trying to avoid using the build-in save() function as I need to use JSON here .

I select a dataframe from mysql . Then I do some calculations and then I want the rows in the mysql table to update to the rows in the dataframe . I do not select the complete table so I cannot just replace the table .
the column order / type remain unchanged so it just needs to replace / update the rows and I have a primary key indexed , auto-increment ' id ' column if this makes any difference .
I don't think that's possible with Pandas . At least not directly from Pandas . I know that you can use to_sql() to append or replace , but that doesn't help you very much .
Thanks for the input . Can you give me an example on how to update the dictionary in mysql ? I only found [ this post ] ( #URL ) but that works with " insert ignore on duplicate " . And is there a maximum number of rows for 1 query ? thanks

Replace a whole dataframe with another ( overwrite ) ( Python 3.4 pandas )
But it doesn't work . I know this because when I check ` len ( Sframe )` after selecting ` 2 ` to eliminate duplicates ( and their unique , original values ) , I get the same number as before dealing with the duplicates .
Which field are you using for ` identifier ` when you merge ` Sframe ` and ` SframeDup ` ? ` StudentID ` ?
That's what I thought about my original code but for some reason when I check len ( Sframe ) at the end in the main code , it still has the duplicate values even if the conditional statement applies option 2 to remove duplicates . Maybe I'm missing your point ; is there something about the duplicate checking code that you propose that should work differently than what I am doing after the conditional statement is applied ? When I check len ( SframeWOdup ) at the end of the code , it shows that the duplicates are removed .

The final goal is to be able to use ` pd.resample() ` to resample from the series above .

You can ` merge ` the two DataFrames : #CODE
I would look at the result from ` merge ` and remove the columns I didn't want . And I would be more specific than " it messed up badly . " :)
I see . By messed up , I realized that what it did was change the ordering around . I'll just apply a sort again and get it in the correct order . This answer worked , thanks a bunch ! I spent 6 hours trying different methods . :(

Unclear if it should work at all but you could apply the function : ` s.apply ( pd.DataFrame.mean )`

index : bool , optional whether to print index ( row ) labels , default

That is : I would like to merge the columns with the same Timestamp ( I have 17 columns ) , resample at 1 min granularity and for those column with no values I would like to have NaN .
Assumed that you have your ` Timestamp ` as index to begin with , you need to do the resample first , and ` reset_index ` before doing a ` groupby ` , here's the working sample : #CODE
As said in comment , your ' Timestamp ' isn't datetime and probably as string so you cannot resample by DatetimeIndex , just reset_index and convert it something like this : #CODE
Now just run the previous code again but replace ' Timestamp ' with ' ts ' and you should be OK .
I tried to do that before and when I resample with df.resample ( ' 1Min ' , how= ' max ') , I get the following : TypeError : Only valid with DatetimeIndex or PeriodIndex and I don't know how to go about this .

I am trying to write Python Pandas code to merge the data in two DataFrames , with the new DataFrame's data replacing the old DataFrame's data if the index and columns are identical . There seems to be a bug in Pandas that sometimes causes the column names to be mixed up .
Now merge them : #CODE
Then merge using the same code as before : #CODE

If you are after performance , you can set index on ' x ' and drop the list comprehension and make it generator , it further reduces the lookup time . #CODE

I found a solution but it seems to be way too convoluted . Basically I make the index into a column , then melt the data frame . #CODE

Pandas concat : ValueError : Shape of passed values is blah , indices imply blah2
I'm trying to merge a ( Pandas 14.1 ) dataframe and a series . The series should form a new column , with some NAs ( since the index values of the series are a subset of the index values of the dataframe ) .
Have you tried ` append ` instead of ` concat ` ? And if I understand the ` ValueError ` correctly it's saying there are 286 rows of data , but the indices of the data frame are expecting 276 rows . Try checking out ` len ( df.index )` and ` len ( h1.index )` .
df.append ( hl ) fails with TypeError : ' NoneType ' object is not iterable . But then I tried join - thanks ! :)
Aus_lacy's post gave me the idea of trying related methods , of which join does work : #CODE
Some insight into why concat works on the example but not this data would be nice though !
I had a similar problem ( ` join ` worked , but ` concat ` failed ) .

How to use python pandas to get intersection of sets from a csv file

The only other option I can think of would be to drop python all together and reimplement FlowCytometryTools in Objective-C to avoid py2app , but that would be pretty labor intensive .

As updated by OP , the blank filed is actually NaN , you can make use of isnull method from pandas , like this : #CODE
I will not downvote but your answer is somehow wrong as ** notnull ** will only filter ** NaN ** whereas OP intentionally uses "" , which isn't ** NaN ** and therefore will not be filtered . That said , I'm still await to OP response on what type ' Discipline ' field really is .

I have to calibrate a distance measuring instrument which gives capacitance as output , I am able to use ` numpy polyfit ` to find a relation and apply it get distance . But I need to include limits of detection 0.0008 m as it is the resolution of the instrument .
For example If I have a capacitance value of 3044 and if you look into calibration data the distance should be between 0.4 m to 1 m and If I do the present method I get distance like 0.8967892678 m ( for example ) , instead something like 0.8008 ( example ) . Because the instrument will only able to differentiate 0.0008 m . I need to apply a correction like if the value is between the two numbers it is rounded and shows the limits of detection
use your raw data of known distances ( your calibration set ) and apply the fit . you can then see how much variation you have from the actual values ( range ) and standard deviation ( 1 sigma )

We can look inside ` pandas / core / frame.py `' s ` DataFrame.corrwith ` to see how the computation is done . Then translate it into corresponding code done on NumPy arrays , making adjustments as necessary for the fact that we want to do the computations on a whole array full of rolling windows instead of just one window at a time , while keeping ` patch ` constant . ( Note : the Pandas ` corrwith ` method handles NaNs . To keep the code a bit simpler I've assumed there are no NaNs in the inputs . ) #CODE

simple pivot table of pandas dataframe
You can do it with ` pivot_table ` , but it will give you NaN instead of 0 for missing combos . You need to specify ` len ` as the aggregating function : #CODE

Using Pandas to shift only a slice of a DatetimeIndex series and put it back in to the file shifted ahead
I have a time series that had a DST issue . Simply put , I am wanting to pull out a slice of the series and shift it ahead an hour ( minutely data ) , and then put the shifted slice back in and ffill any missing values with 0 .
Here is the data I need to shift an hour : #CODE
My approach has been to slice out the time chunk that I need to shift data by an hour , shift the data by an hour , and then replace the original data with the shifted slice .
Notice how the sift has worked , and it has added another couple columns , but I wanted to replace the original columns ( df , on the left ) with shifted ( those 2 on the right ) . To start values around 8:47 not 7:47 , i.e. : #CODE
If you identify where the duplicates exist you could use ` resample ( ' col_with_dups ' , how= ' mean ')` . See here --> #URL
The duplicate indexes ( shifted ) are those I am trying to set back into the original data frame ( df ) . I am wondering if I am wanting to use another method than the one I used above ; perhaps merge or join ? Effectively I want to replace the df indexes with shifted ones , so is there a " replace " method ?
Well you could ` merge ` the two data frames and then specify if you want to keep the indices from either the right hand side data frame or left hand side . ` df.merge ( shifted , left_index=False , right_index=True )` might work for you instead of your last line .
I am having a hard time incorporating the desired changes using merge , but have gotten the data merged in , just not correctly ( i.e. : replacing the original slice ) . Please see the edited question above for examples .
Try ` df.update ( shifted )` rather than your merge .
After many trials and errors I have found a way to remove a slice from a Dataframe indexed by datetime , shift it an hour , and then combine it back into the original dataframe . Thanks to aux_lacy for the help . See below : #CODE

- if length of the gap is lower than 5 NaN , then interpolate
Now we want to interpolate when gap is 10 and put something where gap > 9 #CODE

The ` custom_df ` as ` Freq : C ` , while ` custom_df2 ` has ` Freq : None ` . The ` freq ` is used by certain methods , such as ` snap ` and ` to_period ` . But these methods also allow you to specify the desired frequency as an argument , so in practice I have not found this difference to be a big deal .

You can use ` melt ` to convert a DataFrame from wide format to long format : #CODE
You could use ` stack ` to pivot the DataFrame . First set ` date ` as the index column : #CODE

I am trying to append data to a log where the order of columns isn't in alphabetical order but makes logical sense , ex . #CODE
Seems you have to reorder the columns after the append operation : #CODE
The same alpha sorting of the columns happens with concat also so it looks like you have to reorder after appending .
An alternative is to use ` join ` : #CODE
` join ` does what you want I think except it aligns on index which may or may not be what you want though
@USER I see your problem , join seems to preserve the column name and data order

Pandas : Take the median accross multiple dataframe
Across all the dataframes , I want to take the median of each i , j element .
Now to calculate the median just do : #CODE

How to replace all Negative Numbers in Pandas DataFrame for Zero
@USER I mean the logic behind the fact that I can apply the comparison to the whole DataFrame only if the rhs is a Timedelta , although it works just fine with separate columns and ints . Sorry for being unclear .

How to efficiently apply a function to each DataFrame of a Pandas Panel
I am trying to apply a function to every DataFrame in a Pandas Panel . I can write it as a loop but the indexing seems to take a long time . I am hoping a builtin Pandas function might be faster .
I looked at ` mypanel.apply ( condenser , axis = ' items ' )` but this loops over each column of my DataFrames separately . Is there something which would apply a function to each DataFrame ?
apply is correct , but the usage is :

The code I'd like to replace ( with something that will take an arbitray number of k's ) is : #CODE
Thanks for any helpful suggestions . Searching the web has been frustrating because I'm trying to do a join ( in the pythonic sense ) and most search results relate to joining columns in the database sense ( including as adapted in pandas ) .
@USER : Thanks ! Also good to see a solution using ` apply ` ; it could be handy for implementing more exotic concatenation functions .
try to use str.cat over apply whenever you can . feels a gazillion times faster .

You can use ` pandas.apply ( args )` to apply a function to each row in the ` transdf ` data frame if you know that the rules set in the ` segmentdf ` are static and don't change . Perhaps the following code snippet may help you . I haven't tested this so be wary , but I think it should get you started in the right direction . #CODE

I have a TimeSeries data object to which I append data worst case every 200ms .
Well ` append ` is a wrapper for ` concat ` so if you can use an implementation using ` concat ` it should be marginally faster .
An ` append ` operation returns a new Series . So you are copying the data each time which is quite inefficient ( from speed and memory perspective ) . The usual way to deal with this is to pre-create a larger size than needed then simply fill in the values . Resizing periodcially if needed . ( this is how python lists / dict do it as well ) .

change schema for sql file ( drop quotes in names )
Unfortunately , I do not have access to their code , so I can't fix it ( probably simple strip method would do the trick ) .
It is a bit strange that an sqlite application errors on the quotes ( as sqlite should be case insensitive , quotes or not ) , and I would also be very cautious with some of the special characters you mention in column names . But if you need to insert the data into sqlite with a schema without quotes , you can do the following .

OK - there should be an short error message right at the bottom of the stack trace , e.g. beginning with ` ValueError ` or ` TypeError ` or something similar . What is it and what does say ?

As documented , you can get the number of groups with ` len ( dfgroup )` .

I am attempting to resample tick time series data in Pandas . #CODE
I use resample #CODE
I don't understand why it starts with " 2014-07-01 10:00 : 15 " while I resample slice starting with " 2014-07-01 10:00 : 20 " . Is it a feature or a bug ? How can I make it start with needed time ?
[ resample ] ( #URL ) has many arguments , that control resampling . One of them is base . Using this , you can do ' data [ " 2014-07-01 10:00 : 20 " :] .resample ( " 15s " , base = 5 ) .head() ' which will output ` 2014-07-01 10:00 : 20 ` as first date . But I dont know it works , and how it affects the resampling . Maybe have a look at this parameter , and hopefully it will help .
@USER Thanks , base helps , but unfortunately I don't know what base to use before resampling , with different frequencies first index seems to be unpredictable to me . However I found out that first index doesn't depend on the length of the slice , so a little weird solution is to resample small slice , check first index , get needed base , and then resample whole data .

Now I try all ( is x less than y ) , which I translate to " are all the values of x less than y " , and I get an answer that doesn't make sense . #CODE
Next I try any ( is x less than y ) , which I translate to " is any value of x less than y " , and I get another answer that doesn't make sense . #CODE

I think I need some help with the " melt " function .
I know that I can use the melt function but I have no clue how this function works ... #CODE
How can I use the melt function to fill the new df ?
You are probably looking for ` pivot_table ` , which is like the inverse of ` melt ` . For simplicity , the following code recreates the input DataFrame with the " Uhrzeit " column containing 96 integer values , representing the time quarters : #CODE
Ok I found the problem . Can you show me how to convert this 1.087 , 29 ( on thousand .. ) into this 1087.29 ? I tried the replace function in that way : replace " . " by "" and " , " by " . " but than it throws an empty series ...
thank you perfect !! So the trick was to put the replace together ?!

Pandas pivot gives ValueError
I am trying to pivot it by count : #CODE

Basically , the cumulative sum operation ( with a factor ) is done using ` numpy.convolve ` . The rest is straight forward : just ` groupby ` the data into groups , apply the ` convolve ` and then ` concat ` the resultants together .

This gives me what I want , but it doesn't seem like it could possibly be the best solution . I can't seem to find a good way to complete this all using a data frame . I've tried using stack / unstack and pivoting the data frame ( as some of the other solutions here have suggested ) , but I haven't had any luck . I've tried doing something like this : #CODE
OK , I see your problem you have data spanning across multiple columns , you need to concat the various columns into a single df , I'd first fix the column names and it will be easy

You can use ` apply ` to force all your objects to be immutable . Try #CODE

Let's say I have a DataFrame ( that I sorted by some priority criterion ) with a "` name `" column . Few names are duplicated , and I want to append a simple indicator to the duplicates .
However , I think the ` apply ` function does not allow for ` inplace ` modification , right ? So what I basically ended up doing is : #CODE
Note that your solution only works if there is a maximum of one duplicate . Also , you should be able to replace everything after the ` = ` with ` df.name.duplicated ( take_last=True ) .apply ... `
This appends an indicator to all . If you want to append the indicator to only those after the first , you can do it with a little special casing : #CODE
If you want to use this to replace the original names just do ` d.Name = ... ` where ` ... ` is the expression shown above .
Thanks , that's a pretty good solution ! The problem is that I want to merge and update DataFrames that come from different sources . I thought about taking more " first name letters " , but some sources only have 1 first name letter , so that would not be an option ...

Many operations in Pandas align on the index . The correlation of data points from two Series is not matched by integer index location ( as would be done by NumPy ) . Instead the data points are aligned by index . If the indexes don't match then the data points totally miss each other and the correlation is unknown , hence NaN is returned .

You can apply a ` lambda ` to only the relevant column , instead of the whole row : #CODE

What is the approximate byte size of the data you're trying to insert ? ( per chunk ? )
The TL ;D R answer : While BQ is not strictly enforcing the max rows per request of 500 rows / insert at this time , there are some other limits elsewhere in the API stack related to the overall request size that are preventing the call from succeeding .
We knew about the limits when writing the original GBQ library and the corresponding docs . It just seemed weird that a test that was previously passing would suddenly have issues . I'm particularly concerned about this 500 rows / insert at a time . Interestingly , upon trying again today it worked ( for the first time in a good month at least ) . I wonder if something was corrected .

One thought that I had was to convert to it a string , left strip the space , and then take [ 0 ] , but i can't figure out the code .
I believe that str() or the int() in the 1st line of your function wouldn't like a NaN . You could change your apply to df.new_var = df.ID.dropna() .apply ( checker ) or test for NaN in your function

Failing that , what is the best way to insert a column index into the first row of data ?
I believe you can set you header=None in your parse() that will use some default ' 0 , 1 , 2 ' type of column names . Then you can change names before or after your concat

later we need to merge them .
Just transpose it after you've created the DataFrame : #CODE

What I want to do is to merge them and create single big data frame that looks like this : #CODE

You could shorten to ` df [ ' CountryCount '] = df [ ' PersonId '] .map ( df.groupby ( ' PersonId ') [ ' Visited '] .nunique() )` and avoid the merge
I think using ` nunique ` is better than calling ` apply ( len )` though ;) +1
@USER IMHO , I think you should edit to avoid the merge part and put the nunique() , which looks great .
@USER check out timings , ` nunique() ` is worse than ` apply ( len )` :)

After the for loop and before you define frame , replace the left bracket ( ' [ ') with whatever you want .

Now add another column for the week and year ( one way is to use ` apply ` and generate a string of the week / year numbers ): #CODE

I noticed that it is not a smart thing to drop missing NAN values . I usually do interpolate ( compute mean ) using pandas and fill it up the data which is kind of works and improves the classification accuracy but may not be the best thing to do .
You can either drop the rows that have any missing values but this may reduce performance , or set the missing values to some value that doesn't affect the prediction but this may still skew your model if you have many missing values , it really depends . You can use the mean / median but you will have to measure the performance of all approaches and see what is best , it depends on whether there is any value in those features and what model you selet

Your snippet is very close to working except that it is also including the index in the ` data ` column . Is there a way to drop or ignore the index column ?

The easiest thing is to first make a Series that just uses the tuples as a MultiIndex . Then you can unstack the MultiIndex into columns : #CODE

append item to list in pandas by index
I will eventually want multiple ' tags ' on each index , this is a fall back for the moment . I still would like to know why the append doesn't work .

Replace text with numbers using dictionary in pandas
I'm trying to replace months represented as a character ( e.g. ' NOV ') for their numerical counterparts ( ' -11- ') . I can get the following piece of code to work properly . #CODE
However , to avoid redundancy , I'd like to use a dictionary and .replace to replace the character variable for all months . #CODE
However , if you need / want date strings with 3-letter months like `' NOV '` converted to ` -11- ` , then you can convert the Timestamps with ` strftime ` and ` apply ` : #CODE
To answer your question literally , in order to use ` Series.str.replace ` you need a column with the month string abbreviations all by themselves . You can arrange for that by first calling ` Series.str.extract ` . Then you can join the columns back into one using ` apply ` : #CODE

what column are you calling your ` apply ` on ? A sample of your data would help you get an answer much quicker .
I would like to apply the function to the new column and get the results by referencing the other two columns . The data is a bit messy and also confidentially , i will try and knock together some simple data for the question .
also avoid using ` apply ` where possible as this is just going to loop over the values , ` np.where ` is a vectorised method and will scale much better .
No need to drop down to ` numpy ` -- frames and series have ` where ` methods too ..
@USER no worries , the key thing to take away from this is to look for a vectorised method that will operate on the whole df or series rather than calling apply which loops over the values
You'r lambda is operating on the 0 axis which is columnwise . Simply add ` axis=1 ` to the ` apply ` arg list . This is clearly documented . #CODE

Append pandas DataFrame column to CSV
I'm trying to append a pandas DataFrame ( single column ) to an existing CSV , much like this post , but it's not working ! Instead my column is added at the bottom of the csv , and repeated over and over ( rows in csv >> size of column ) . Here's my code : #CODE
It's appending the ` scores ` data frame at the end of the file because that's how the pandas ` to_csv ` functionality works . If you want to append ` scores ` as a new column on the original csv data frame then you would need to read the csv into a data frame , append the ` scores ` column and then write it back to the csv .
@USER I don't think it's possible to append the column to the original data frame within the csv without opening the file and parsing the data since python has no way of knowing that there is a data frame in the csv to append to .
@USER , then it will be difficult , provided that you still need to open+read each line of your csv to find the linefeed and append your new column . I don't like ** DiskIO**-wise you can have an easy solution

Not knowing much about your data but assuming it is a time series analysis , I would try to create a correlation matrix among all the features you have , and maybe merge features with very high correlation . However , in using that approach , you need to make sure that correlations hold over time , and check serial correlation .

I used the code provided below and was able to drop the time to about 3.5 seconds . Maybe the specificity of that code allows it to run much faster ?

How is a DataFrameGroupBy structured to then apply functions to it ( like a for loop ) ?
I think you want to use ` shift ` : ` goods_data [ ' time_difference '] = goods_data.groupby ( ' id ') .time_retrieved .apply ( lambda x : x - x.shift() )`

The mental image I have for what I'm trying to do is strip the zeroes in each row up to the leftmost non-zero value , and then " left align " that row . Each row would start with a nonzero number and then proceed as before .
Meanwhile , this doesn't even seem like the right approach . From reading other topics it seems like I may want to transpose and then use a mapping function ?
Output would hopefully look something like this . The columns would indicate the 1st nonzero month of data for each row , the 2nd would be the calendar month following it , etc . #CODE
If you " left align " the rows won't that mess up your columns ?

Here , I rename the score column so that when I join it to the to the original dataframe , I know which column is which . This may not be necessary , but I'm not sure . #CODE
Genius ! I'm not sure what ~O ( N log N ) and O ( N ) mean , but thanks so much ! A follow-up question : Will the fact that groupby was used pose problems when I try to append these records to an existing table ?

To do we consider that the first element will be the first . Then we need to specify two values ( in a tuple of two integers as the argument to the parameter ` strides `) . The values specify the steps we need to do in the original array ( the 1-D one ) to fill the second ( the 2-D one ) .
There is only one last thing to note . The ` strides ` argument does not accept the " steps " we used , but instead the bytes in memory . To know them , we can use the ` strides ` method of numpy arrays . It returns a tuple with the strides ( steps in bytes ) , with one element for each dimension . In our case we get a 1 element tuple , and we want it twice , so we have the ` * 2 ` .
Is it possible change it to shift forward by ` N ` observations , as opposed to ` 1 ` ( as implemented in your answer ) ? I played around a bit but could not manage to get it to work .

Why does simply making the data tuple ( or also a list : list ( map ( list , pos )) works just as well ) solve the pandas error ?

How to replace all value in all columns in a Pandas dataframe with condition
What I want to do is to replace all values that is less than 1 with 0 .

So are you wanting to get the size at a specific level ? ` len ( df.columns.levels )` and ` len ( df.index.levels )` both work
@USER you should put that in an answer , as that is a good one . @USER ` len ( df.index.levels )` does not work if you don't have a multi-index
As @USER mentioned above ` len ( df.columns.levels )` will not work in the example above as ` columns ` is not ` MultiIndex ` , giving : #CODE
You probably need something like len ( set ( df.a )) which works on both index or normal column .

Python , use " order by " inside a " group concat " with pandas DataFrame
There's no group concat function in python / pandas , so we'll have to use some groupby . It's a bit longer than SQL , but still relatively short ( main part is 3 lines ) .

However , no combination of groupby or pivot is giving me anything remotely like what I want , even though I know this should be fairly simple . I can't seem to find a standard approach to do this help ?

and the stack trace : #CODE
@USER I've added the ` df.info` . ` User ` is in there , there are no nulls , it is a simple collection of names , and those names don't have any strange characters in them . This df is creating by ` concat ` on a bunch of * .xlsx files .

Assuming I understand what you're after ( setting aside weird duplicated-index cases ) , one way is to use ` loc ` to index into your frame : #CODE
Create an DataFrame with only index =[ ' C '] and concat : #CODE

I have tried to merge the two dataframes and obtained a new dataframe containing df1 and the columns ` 0.0 ` - ` 23.0 ` matched to the correct date . I did this by converting ' time ' to the ` YYYY-mm-dd ` format and applying .merge . However , this dataframe is a bit too messy .

pandas dataframe columns with non-numeric data get deleted on resample
Before the ` resample ` ... #CODE
After the ` resample ` ... #CODE
When I ` resample ` the data , the ` object ` and ` timedelta64 [ ns ]` columns get deleted .

Thanks . Checked and the table was indeed created . Like @USER said , it was a case sensitivity problem in table name : I re-wrote the table : ` i.to_sql ( ' stat_table ' , engine , if_exists= ' replace ')` and then it works : ` a =p d.read_sql_query ( ' select * from stat_table ' , engine )`

Why not just do ` train_df = df.iloc [ 0 : floor ( 2 * len ( df ) / 3 )]` and ` test_df = df.iloc [ floor ( 2 * len ( df ) / 3 ): ]`

The index of DataFrame lost its name after append a row
This looks like a bug and happens whether using ` ix ` or ` loc ` if you append e.g. ` df.append ( pd.Series ([ 35,172 ]) , ignore_index=True )` then the index name is preserved , this also happens on ` 0.15.2 `

How can I pivot this Pandas DataFrame ?
If you use ` pivot_table ` instead of ` pivot ` , it works : #CODE
The problem is that ` pivot ` cannot handle a list of columns for the index / columns argument . The only caveat is that now the default is to take the mean if there are multiple values for one continent / country / year combination .

Pandas DataFrame , replace value of a column by the value of an other column
for each line , if the value of ' sku ' is the same as the value boost1 or boost2 or boost3 then I want to replace the matching value by the value inside boost4
` .loc ` should also be perfectly fine , does this not give the same output as ` ix ` ?

Create another distinct SQLite database ( call it B ) again in memory . With some looping , query and extract information out of database A , do a bit of combinations to them , and insert them into database B . Database B is now the database from which we will take data out from to perform analyses .

The other point here is that effectively you have created a tuple by the way you declared ` Mix ` , you'd be better off making this a list initially so you can append to it like so : ` Mix =[ ' business_unit ' , ' isoname ' , ' planning_channel ' , ' is_tracked ' , ' planning_partner ' , ' week ']
The other point here is that the way you defined Mix will result in a tuple , if instead it was a list then we could append the additional column of interest and all would be fine : #CODE
Thanks EdChum for your complete workings and solution . I guess I thought I could manipulate the Variable list somehow within the groupby . I can work with manipulating the list or at least having two or more lists referencing them when needed or I could append to the list like you have and then remove on other occasions .

Applying a cumulative sum on this DataFrame is easy , just using e.g. ` df.cumsum() ` . But is it possible to apply a cumulative sum every ` X ` days ( or data points ) say , yielding only the cumulative sum of the last ` Y ` days ( data points ) .
If the index is a ` DatetimeIndex ` , you can ` resample ` to a daily frequency , take a ` rolling_sum ` , and then select only the original dates : #CODE

How to replace NaNs by preceding values in pandas DataFrame ?
What I need to do is replace every ` NaN ` with the first non- ` NaN ` value in the same column above it . It is assumed that the first row will never contain a ` NaN ` . So for the previous example the result would be #CODE

I want to perform outer join of two dataframes with the same row index using Pandas 0.14.1 .
` join ` works : #CODE
` concat ` fails : #CODE
Possible cause : one of the dataframes has a duplicate index value . Discovered that downstream from the successful ` join ` . This may cause ` concat ` to fail , though the error thrown isn't obvious .

Back to your idea of raw SQL , that would make a difference , if e.g. the Python code that creates the ` StrategyRow ` instances is slow ( e.g. ` StrategyRow.objects.create() `) , but still I believe the key is to batch insert them instead of running N queries

I want to append the files so that any columns with the same word ( like Bword in this example ) are matched while new words are added in new columns : #CODE
Is there a different approach which could align matching columns while appending ? ie without iterating through each column and checking for matches .
IIUC , you can simply use ` pd.concat ` , which will automatically align on columns : #CODE
Thank you very much for your help . When I use your approach I find the columns concat by index number rather than by column name ( eg by 0 , 1 , 2 , 3 instead of Aword , Bword etc ) . Is there a way around this ?

I suppose you could always define a function ` f ` that accepts the parameters of column ` A ` , column ` C ` , and column ` L ` and then ` apply ` it to your data frame . See --> #URL

I'm trying to fill a column ( signal ) in a dataframe conditional on another column ( diff ) in the dataframe being compared to 2 variables . This column to be filled has 3 possible outcomes , 1 , -1 , 0 for buy , sell , hold ( cover ) . This is the code and output thus far . #CODE

Obviously you would replace the ` 0 ` with the name of the column that your dates are under within your data frame , but I'm sure you get the idea .

Well you've not specified what doesn't work , my guess here is that the result is not what you desired but you've not stated what your desired output should look like . Are you trying to merge based on the index ?
Yes , I'm trying to merge by index . In this case index =d ate.I have tried append , but that seems to concatenate .

INSERT INTO ` jos_users ` VALUES ( ' 4065 ' , ' lel lel ' , ' joel ' ,
INSERT INTO ` jos_users ` VALUES ( ' 4066 ' , ' jame lea ' , ' jamal ' ,
As a first step you need to clean your TXT file , at the moment it's a SQL script not a csv file , you could strip out everything up to and including the left bracket and then remove the trailing bracket and any new lines .
Anyway what the following does is to open you text file and remove the ` insert into ` , open and close braces , and quoting characters ( not necessary but I prefer this style ) and any extraneous spaces . #CODE

Some other remarks : 1 ) you don't need to loop over the values to use ` to_datetime ` , just provide the column at once : ` rev [ ' rev_date '] = pd.to_datetime ( rev [ ' rev_date '])` , and 2 ) you can use the ` .dt ` accessor instead of the ` map ` function : ` rev [ ' rev_date '] .dt .year `

I have built a pytables table and filled it using append like so : #CODE
not a dataframe with columns based on the Hybrid columns and rows for each append . Can anyone help show me what I am doing wrong
You need to explicitly read the data from the table . Table.read will pull in the entire table , and Table.read_where allows you to apply a conditional statement to filter the data that is returned . #CODE

Alright I have an answer ! The trick is to create an empty data frame and append in it . #CODE

Join Series on MultiIndex in pandas
I think not . You can of course extend this with several joins , the join solution detects common indices automatically . These are the relevant discussions [ 3662 ] ( #URL ) , [ 6360 ] ( #URL )

Going around , I have found also this solution based on apply method

or perhaps do I need a merge ? It seems less elegant and I'm also not sure .. I've tried : #CODE
Get rid of the `' days '` column in your ` mlc ` series since it is redundant with the index . Or , specify ` right_suffix ` and / or ` left_suffix ` in the join .

So for the short example I give df.index has shape ( 3 , ) and df.T has shape ( 2 , 3 ) . I think numpy broadcasting ( which I thought should apply here ) goes from last dimension to the first . So the 3's should match and the operation should be successful . That's why df.T.values * df.index.values will always work .

Improve Row Append Performance On Pandas DataFrames

` Series.map ` is quite versatile -- not only does it accept sequences , it can also take a callable ( like the Python built-in , ` map `) , or a Series or a dict .

So I downloaded a number of .csv files containing FX data ( tick-level ) off dukascopy.com and I am keen to convert them into Pandas Series objects and then in turn resample them to OHLC . I have written the following function below ( please also see data-set extract ): #CODE

This can be done easily using the ` diff ` -compare- ` cumsum ` pattern : #CODE

We can add a dummy tick column , pivot , and then use the " it's simply a dot product " observation from this question : #CODE

Note that doing the above will require you to drop the extraneous ' head0 ' column which can be done by calling ` drop ` like so : ` df.drop ( ' head0 ' , axis=1 )`

pandas replace not working for specific values
I've written the following code to replace values in a column : #CODE
Also is there a more efficient way to do conditional replaces in pandas ? I've seen this post Conditional Replace Pandas but I don't use headers in my .csv file . Just wondering if there is a way to do something similar using indices . I tried doing this : ` data.iloc [ data [ 9 ] 10000 , 9 ] = ' low '` but that that results in an error .

This seems to be similar to an old stack thread Pandas compiled from source : default pickle behavior changed

pandas concat DataFrame on different Index
I have an arbitrary list of ` pandas.DataFrame `' s ( let's use 2 to keep the example clear ) , and I want to ` concat ` them on an ` Index ` that :
is neither the ` inner ` nor the ` outer ` join of the existing ` DataFrames `
I would like to join these two ` DataFrame `' s on an intersecting ` Index ` , such ` my_index ` , constructed here : #CODE
Use existing ` pandas ` functionality instead of building my own hack , i.e. ` reduce ( map ( ) )` etc .
Return views of the intersection of the ` DataFrame `' s instead of creating copies of the ` DataFrame `' s
I'm ** really ** surprised there isn't out of the box functionality for this ( because it seems like trivial use case ) . Before I RTD I thought ` concat ( [ df_1 , df_2 ] , join_axes = my_axis )` was the specific functionality I was looking for , however , you most certainly would know !
return pandas.concat ( map ( lambda x : x.reindex ( index ) , df_list ) , axis = 1 )`
Interesting ! Feel free to post that as an answer ( and accept it if you find that's the best solution . ) The reason why I chose to avoid ` concat ` here is because it [ can raise an error ] ( #URL ) if the index contains duplicates while ` join ` does not .

As far as I know there is no direct shortcut for this , but you can do it using ` map ` : #CODE

Python 2.7 & Pandas : How to replace values at 12:00 with values from 11:55 ?
How do I explicitly say ' replace the values at 19:40 : 00 with the values at 19:35 : 00 ?

Think this works . I've used panda's broadcasting capabilities to replace values in 1 column ( actually several columns ) at once . #CODE
It's possible to give pandas ' [ replace ] ( #URL ) a dict of from ( keys ) and to ( values ) pairs . So here , I replaced anything that wasn't a number with '' in each value and , separately , replaced empty strings with ` nan ` . It's ( kinda ) equivalent to ` .replace ( r ' [ ^ 0-9 \ . ]' , '' , regex=True ) .replace ( '' , np.nan )` .
2 ) Replace asterisks and percent signs and convert to decimal fraction : #CODE

Append the rows to your other dataframe using ` df.append ( other_df )` .

This is my first question at Stack Overflow .
Is there any smart way to do this or to apply gensim from pandas data ?

Depending on how you want to have the final dataframe , you can use ` pivot ` for this : #CODE

I want to drop rows from a pandas dataframe when the value of the date column is in a list of dates . The following code doesn't work : #CODE

` iconv -f " UTF-16le " -t " UTF-8 " file1.txt file2.txt ` leads to a very weird behaviour : a row in between lines is cut . All looks fine but only 80K rows are actually converted .

perform some simple maths on the frames then drop NaN and unneeded columns #CODE
then I merge the frames together #CODE

Script to normalize subfields in CSV using pandas dataframes
Until I learn how to use map / apply , looping through a pandas dataframe is good enough .

Is there any way I can add / append new items on the same index ?
Are you trying to append the new item into a new column or as a new row with the same index ?

` pandas ` is built on ` numpy ` . numpy arrays are fixed sized objects . While there are numpy append and insert functions , in practice they construct new arrays from the old and new data .
Appending to a Python list is a common and fast task . There is also a list insert , but it is slower . For sorted inserts there are specialized Python structures ( e.g. ` bisect `) .

Or a faster and shorthanded version using map : #CODE

` df [ ' ids '] .str ` allows us to apply vectorized string methods ( e.g. , ` lower ` , ` contains `) to the Series

I want to append a pandas dataframe , ` df ` , to ` table ` in ` mydb ` .
( 1 ) Append fails : #CODE
Won't the manual ` executemany ` with an ` insert into ` query also fail ? ( as in that case you will try inserting keys already in the table ? )
Yes , you may be right : with ` sqlite3 ` library , ` insert or replace ` in the query solves this problem with ` conn.executemany() ` . If I include ` or replace ` in the query here , I get an error : ` ProgrammingError : ( ProgrammingError ) syntax error at or near " or " . `
As far as I know , postgresql has no such feature to do an insert with update on duplicate values , see eg #URL
Got it . What about performing the ` insert ` for each record in the array and catching the error when the key is present ? That would solve the problem .

It would be faster to replace the comma separator on all the columns of interest first and just call ` convert_objects ` like so : ` df.convert_objects ( convert_numeric=True )`

How to include strings with Pandas resample
I need to resample the data using Pandas in order to create clean half-hour interval data , taking the mean of values where any exist . Of course , this removes the three safety string columns .
I don't think it's possible to do what you want with the resample function , as there's not much customisation you can do . We have to do a TimeGrouper with a groupby operation .

Here comes my problem , I would like to apply a PCA on the table which requires the whole DataFrame to be loaded but I don't have enough memory to do that .
The PCA function takes a numpy array or a pandas DataFrame as input , is there another way to apply a PCA that would directly use an object stored on disk ?

You cannot add a column to your table with data in it all in one step . You must use at least two separate statements to perform the DDL first ( ` ALTER TABLE `) and the DML second ( ` UPDATE ` or ` INSERT ... ON DUPLICATE KEY UPDATE `) .

which we can then pivot to get the state changes : #CODE
Here is a slightly different approach than @USER ' s . I stack the ` start ` and ` end ` columns on top of each other and then filter with a ` groupby ` and an ` aggregate ` function on the ` length ` . Then in order to achieve the desired looking output I ` pivot ` the table . #CODE
And finally the data frame after the pivot : #CODE
I imagine that @USER ' s solution is more efficient than mine in regards to computational time since the ` append ` method is rather costly because it requires construction of an entirely new object upon each call . I haven't timed either method though so I don't know for sure .

You need to pass param ` ignore_index=True ` to ` concat ` #CODE

If you look in the [ documentation ] ( #URL ) , you can see the ` rolling_apply ` function , which allows you to apply any function in a rolling way . Your function must take the data inside the " rolling window " as an argument . It's not clear how your hodgesLehmannMean involves a window . What is ` x ` ? Is it the window or the whole data set ?
Apologies for not responding earlier , but your code is remarkable in that it speeds up the routine by an order of magnitude . Thank you for this . It's worth pointing out that the Hodges Lehman mean is a very useful estimator because of its very high ( 95% ) efficiency in the standard ( Gaussian ) case , along with the ability to reject arbitrarily large outliers so long as they do not exceed 29% of the points . In short , it gives us most of the efficiency of the mean , with much of the robustness and none of the shortcomings ( e.g. locking in to a static central value ) of the median . Thanks again .

Pandas replace values in dataframe timeseries
I want to replace one datapoint , lets day 2.389 in column Close with NaN : #CODE
Replace did not change 2.389 to NaN . Whats wrong ?
Unfortunately in this case it did not work , but if replace would actually find the value , this would be the way to go .
` replace ` might not work with floats because the floating point representation you see in the ` repr ` of the DataFrame might not be the same as the underlying float . For example , the actual Close value might be : #CODE

Pandas apply to dateframe produces ' < built-in method values of ... '
The weirdest part is , when I directly call the function ( i.e. ` make_geojson ( data.loc [ 0 ])` I do in fact get the dictionary I'm expecting . Perhaps even weirder is that , when I call the functions I'm getting from the apply ( e.g. ` data.output [ 0 ] ( )` , ` data.loc [ 0 ] [ ' output '] ( )`) I get the equivalent of the following list :
@USER I am following the example [ here ] ( #URL ) , but my equivalent of ` f() ` is returning a ` dict ` . Same issue as this question . Yet it's possible to store a ` dict ` in a ` DataFrame ` . I don't know quite what you mean by " a branch is taken " -- does that mean : ` apply ` with a returned ` dict ` is not possible at all ? Is there another way to operate on each row while storing the ` dict ` result in a new column ?
@USER The issue is that the values method on the dictionary is overwriting the values method on the NDFrame . Instead of applying whatever function you're calling to a DataFrame , map it to lists of the columns you need

You can pass a function to a ` groupby ` object using ` apply ` : #CODE

Assuming I'm understanding you correctly -- it's weird to have column names being the same as the values of all the elements in those columns -- I think you can simply throw ` groupby ` at it after ` melt ` ing : #CODE
Melt ! Just like R ... thanks much , exactly what was needed .

How to resample dataframe with counts into new column and aggregate column into list
And want to resample it by days , create a new column with counts and aggregate the labels into a list . Such that I have the following result : #CODE
EDIT : If the order of the elements is important then replace ` list ( set ( x ))` with ` list ( OrderedDict.fromkeys ( x ))` .

No problem ! ( that is part of SO answering , learning yourself ) . Actually , the method on the index already existed for a long time , and because this is so handy , it was added to columns / series through the ` dt ` accessor only recently .

* " Questions asking us to recommend or find a book , tool , software library , tutorial or other off-site resource are off-topic for Stack Overflow as they tend to attract opinionated answers and spam . Instead , describe the problem and what has been done so far to solve it . " * - I suggest you continue to look through the existing documentation for what you are interested in .
And I have looked through the docs , and searched using their search function , to no avail . I am now following the instructions at here : #URL " Your first stop should be Stack Overflow . "
@USER In any case ( however you are totally right there are some docs missing ) , for more compley code I would always advice to use ` .iloc [ ] / .loc [ ]` to be very explicit about label-based vs position-based indexing ( as ` [ ]` can be both depending on the situation , just like ` ix ` which is documented a bit more )
@USER yes , I agree that the territory of [ ] extends into or overlaps with .iloc , loc and ix -- a doc regarding the entire territory would , I think , be very helpful to users .

There are additional examples in the documentation Pandas merge join and concatenate documentation

Just call ` map ` and pass the dict , this will perform a lookup of your series values against the values in your dict , this is vectorised and will be much faster than doing this in a loop : #CODE

If the array has an homogeneous numerical ` dtype ` ( typically ` numpy.float64 `) then it should be fine for scikit-learn 0.15.2 and later . You might still need to normalize the data with ` sklearn.preprocessing.StandardScaler ` for instance .
If your data frame is heterogeneously typed , the ` dtype ` of the corresponding numpy array will be ` object ` which is not suitable for scikit-learn . You need to extract a numerical representation for all the relevant features ( for instance by extracting dummy variables for categorical features ) and drop the columns that are not suitable features ( e.g. sample identifiers ) .

Then you can apply a function to each subset . It sounds like you want either ` rolling_mean ` or ` expanding_mean ` , both of which are already available in ` pandas ` : #CODE

I want to mean normalize my data frame , when I implement the first version of code I am getting the normalized values , but when I implement version 2 I am getting an error called ` stop iteration ` . ` [ " 1B " , " 2B " , " 3B " , " HR " , " BB "]` are columns in my data frame .
I'm not sure how you can do things within the function , it's just not really how groupby and apply work . ` frame.groupby ( ' year ') [[ ' gate ' , ' pop ']]` is almost the same as ` frame.groupby ( ' year ')` , it just excludes the state column .

As you can see , the index is the same and there are values . I tried to merge , concat and append but I always have the same issue .
It may also be better to perform a merge in your case : ` df1.merge ( df2 , left_index=True , right_index=True , how= ' outer ')`
Perhaps a simple ` concat ` would work for you . #CODE

You need to use ` pandas.DataFrame.apply ` . The code below will apply the lambda function to each row of ` df ` . You could , of course , define a separate function ( if you need to do more something more complicated ) . #CODE

Using just the ` pandas.DataFrame.plot ( xerr =) ` , the error bars work so long as the ` pandas.Timedelta ` is in the same units as the ` DatetimeIndex ` Freq .

` RainD.loc [ ' 1971 ']` results in ` AttributeError : ' DataFrame ' object has no attribute ' loc '` . Seems like fedora still has not updated pandas . Just checked , I am on 0.10 . so much for fedora being cutting edge and all that

Apply a lambda with a shift function in python pandas were some null elements are to be replaced
for the last part , in row 2 period 3 I was hoping this would also subsequently be filled with the same formula i.e. period 2 ( 0.425 ) x 0.94 . I am guessing I would need to step through each row or repeat the lambda until there are no more NaNs ? I was assuming a lambda function would automatically apply the function in a sort of iterrows fashion .

index = pd.date_range ( start = ' 2014-01-01 ' , end = ' 2014-01-04 ' , freq = ' 5S '))

I want to reshape a Pandas dataframe to have a new multi-index based on a combination of some of the original columns , and at the same time unstack some of the rows . But I don't know how even after reading the tutorial on stacking and pivoting .
` unstack ` can move index level values into column level values .

( I'm not very familiar with Pandas , but this describes a very generic idea - you should be able to apply it . If necessary , adapt the Pandas-specific functions . )
But if we're really trying to squeeze out every drop of performance , we can use the ` searchsorted ` method after dropping down to ` numpy ` : #CODE

The ` apply ` method calls the lambda function once for each row of the Series ,
using Pandas , only use the ` apply ` method if there is no other option .
space-efficient . So ` zip ` , like ` apply ` , should be avoided here if possible .

Yeah , will have to go with the H5 reorg . I wonder if there is a way to query and merge different tables when row numbers between different tables are not the same . With separate tables I will have to use pandas to do the merge ** after ** the H5 store has been queried . Common sense would say this is a fundamental feature to support natively in H5 but I suppose there are technical reasons why this is not available ( to my knowledge ) .

now I can plot it again ... and drop the added columns afterwards .

So when I bring in the new ' evidence_id ' column it only has the string value ( ' 1 ') for the 1st row . I tried backfill and forward fill and replace to get the ' 1 ' to repeat for every row in the ' evidence_id ' column . I cannot hardcode ' 1 ' though ; as the ' evidence_id ' will change . It just will always be the same number for all rows in the final dataframe . I printed the object type of the extract_properties [ ' evidence_number '] and object ' evidence_number ' , in hopes that helps discern my issue . Any ideas are greatly appreciated ... #CODE
You need to assign the result of [ ` fillna `] ( #URL ) so ` df = df.fillna ( ' Null ')` most pandas operations return a copy , some take a param ` inplace=True ` also the [ ` replace `] ( #URL ) needs to be assigned back also
You don't need to call ` replace ` if all you want to do is propagate the last valid value , just do ` df = df.fillna ( metho= ' ffill ')`

possible speed up of pandas apply
Both functions works , but they are very slow if I use them on thousands of csv files . I think the main bottleneck is the apply method . Is there anyway to speed it up ? Thank you
You are right that using ` apply ` here is also a potential bottleneck , since it is calling a Python function once for each row of the dataframe . Instead of parsing the time strings using ` to_timestamp ` , you could instead use ` pd.read_csv `' s built-in date string parsing ability : #CODE

Will you break it and delete or break and replace with a similar feature ?. )) I just need to store a boolean flag together with dataframe on disk . Don't want to make ( un ) pickling more complex

I am new to Pandas for Python and am busy reading a csv file . Unfortunately the Excel file has some cells with #VALUE ! and #DIV / 0 ! in them . I cannot fix this in Excel because the data is pulled from other sheets . Pandas turns these columns into ` objects ` instead of ` numpy64 ` , so I cannot plot from them . I want to replace the #VALUE ! and #DIV / 0 ! strings with NaN entries in Pandas , however i cannot find how to do this . I have tried the following ( my code runs , but it changes nothing ): #CODE

If the ` ` is the only problem , just create a map , e.g. ` {int : " Integer " , float : " Float " , ... } `
But how are you inferring the type ? Are you using ` eval ` or ` ast.literal_eval ` to parse the values of some line to the best-fitting type , or is there another line , or some other list , with the type information ? Could you provide a minimal example of the input CSV file ( just a few rows and columns ) ?

Oh just noticed that you mentioned you're using ` set_context ` , you can also pass that information to the ` rc ` parameter in that function and it will apply to all figures .

is it preferable to append a dict to the list and create the ` df ` only at the end due to superior performance , or just better readability ?
Performance . To quote the [ docs ] ( #URL ): ... ` concat ` ( and therefore ` append `) makes a full copy of the data , and ... constantly reusing this function can create a signifcant performance hit .

Indeed they are , and your suggestions works . Thank you very much ! I now get a message for some of the regressions saying that the array must not contain infs on NaNs . I know that the NaNs are at the beginning ( because I don't have data on the predictors ) or at the end ( because the relevant time period hasn't occured ( e.g. 10 yr forward return starting in 2013 ) . How can I cut out the relevant section of both the predictor and the response so that they both have data ?
@USER about nans : you can use pandas ` dropna ` method as in elyase's example , or use the ` missing= ' drop ' argument for the statsmodels OLS model . In elyase's example rows will also be dropped if there are other variables / columns with nans that are not used in the regression , even if there are no nans in the columns used for x and y .

You can add append to a list from ` for loop ` and then create a ` df ` from it . #CODE

How can I merge the " code " column into a string so the result would look like ... #CODE

Python Join two dataframes on columns meeting a condition
I want to join these two dataframes but not on rows on which the x and y columns are equal across the two dataframes , but on rows where A's x columns is a substring of B's x column and same for y .

Then I group ` POS ` values and mean normalize the OPW columns and then store the normalized values as a seperate column ` [ ' resid ']` .

The main idea behind ` groupby ` and similar functions is " Split - Apply - Combine " whereby , in general , you :
Apply some aggregate function to each of the individual groups ,
Finally in pandas you need to apply some aggregate function to your groups ( the apply stage ) , we're going to use ` count() ` to count the amount of results . This line then becomes : #CODE

I need to filter out data with specific hours . The DataFrame function between_time seems to be the proper way to do that , however , it only works on the index column of the dataframe ; but I need to have the data in the original format ( e.g. pivot tables will expect the datetime column to be with the proper name , not as the index ) .

Pandas merge not giving expected output with datetime
and then I merge using : #CODE
I am getting : MergeError : Must pass right_on or right_index=True . Also how will this merge using DateTimesy .

You can use the ` xlsxwriter ` engine from Pandas to apply a conditional format to data in an Excel worksheet . See this answer to [ Easiest way to create a color gradient on excel using python / pandas ? ] ( #URL ) . That may be close to what you want to do .

Merge Many json strings with python pandas inputs
Which gets me the correct result , the problem is the ` eval ` usage is very costly . For example , if I remove the ` eval ` and just deal with the smattering of ` \\ ` that result from ` panel_no_eval_to_json ` function here : #CODE
So my final goal would be to loop through the ` Panel ` ( or my object , that has different keys / attributes , many of which are ` Panel `' s and ` DataFrame ` s ) , and merge the ` json ` streams created from invoking ` to_json() ` into an aggregated ` json ` stream ( which would actually be the flattening data representation of my data object ) just as is performed by using the ` panel_to_json ` function above ( the one with ` eval `) .
Thanks for your answer @USER . From my goals and objectives , you can see that I'm trying to " merge the json streams created from invoking ` to_json() ` into an aggregated json stream "
I was going to put that that simply using ` replace ( " \\ " , "")` didn't accomplish what I was looking for , but I thought that was clear from my goals and objectives .
Have you tried using json.loads ( tmp ) instead of eval ?
In the end , the fastest way was to write a simple string ` concat ` -er . Here were the two best solutions , ( one provided by @USER )) and their respective ` %timeit ` times in graphical form

I decided to translate some of Python code from Peter Harrington's Machine Learning in Action into Julia , starting with kNN algorithm .
Using array comprehensions like that is not idiomatic Julia ( or Python for that matter ) . The first is also slow because all that slicing makes many copies of the data . I'm not an expert with ` Distances.jl ` , but I think you can replace it with #CODE
More performance could be extracted by doing the transpose once in ` mass_kNN ` , but that required touching a few too many places and this post is long enough . Trying to micro-optimize it led me to using #CODE

Use ` ix ` to perform the index label selection : #CODE

I have tried potential solutions using cumsum , concat series.shift ( 1 ) but had a block . Now I came across using dict zip which seems clean solution but doesn't work for me . Any suggestions .

When i run this on my actual data , i'm getting " IndexingError : Unalignable boolean Series key provided " , and i'm not sure how to investigate this issue ? ever see that ? why do i need the ' astype ( bool ) ?
The ` astype ( bool )` is needed because the ` transform ` method converts the boolean values returned by ` onemax ` to ints . To use the values as a boolean mask , we need to re-convert them to bools . That seems ugly to me , but I haven't investigated why Pandas chooses to do this .

Pandas merge two dataframes with different columns
I'm surely missing something simple here . Trying to merge two dataframes in pandas that have mostly the same column names , but the right dataframe has some columns that the left doesn't have , and vice versa . #CODE
I've tried joining with an outer join : #CODE
I've also specified a single column to join on ( on = " id " , e.g . ) , but that duplicates all columns except " id " like attr_1_x , attr_1_y , which is not ideal . I've also passed the entire list of columns ( there are many ) to " on " : #CODE
I think in this case ` concat ` is what you want : #CODE
Closer . Is there a restriction on the number of columns for a concat ?
It was something simple , a paren inside the column name from a format file was causing duplicate column names . I'm giving @USER the answer as this method is certainly the easiest way to achieve the append .

How can I iterate over Pandas pivot table ? ( A multi-index dataframe ? )
I have a pivot table I want to iterate over , to store in a database . #CODE
I've played with reshape , melt , various for loops , syntax stabs in the dark , chains of stacks , unstacks , reset_indexes , etc .. The closest I have got is the syntax : #CODE
Alternatively , if you want to manually insert data into database , you can use Pandas ' to_csv() , for example :

My goodness that's old :) Are you willing to try the git / virtualenv instructions I [ posted here ] ( #URL ) ? It's a lot to download , but if successful , will allow you to always have the latest version of the entire NumPy --> Pandas stack .

How to melt two data frames in Pandas

` pages.fillna ( value=None , inplace=True )` then I have specified the value but I still obtain the error shown below . In terms of outcome , I want to replace all ` nan ` with ` None ` to then pass it on into JSON
If I read correctly , @USER needs to replace all instances of ` NaN ` in the dataframe with ` None ` . This is actually non-trivial given the implementation of ` .fillna ` , which uses the ` None ` value of ` value ` or ` method ` as a sentinel to determine which option the user wants to use .

stack from Github in a virtualenv using Python2.7 :
I think anaconda and canopy should be mentioned in the answer . That's a very convenient way of installing the scipy stack , especially for mac and windows users .

To find values at particular locations in a DataFrame , you can use ` loc ` : #CODE
So here , ` loc ` picks out all of the rows where column B is equal to its minimum value ( ` df.B == df.B.min() `) and selects the corresponding values in column A .

Sorry are you asking to merge the 2 dfs on ` Asset ` column ? Can you show desired output

How to append a new column to my Pandas DataFrame based on a row-based calculation ?
Your output shows user ids that are not in you orig data but the following does what you want , you will have to replace / fill the ` NaN ` values with 0 : #CODE
Here we generate the desired column by calling ` transform ` on the groupby by object and pass a string which maps to the ` diff ` method which subtracts the previous row value . Transform applies a function and returns a series with an index aligned to the df .

I've been through lots of questions in stack overflow but still can't figure this out . I understand it's returning a Bool etc but basically I want to apply multiple conditionals to a DataFrame ( If And Else , Else if ... ) But continue to get Ambiguous Error asking to you use any() , all()
will evaluate to a ` Series ` of type ` bool ` , not a scalar . Suppose that some of the elements are ` True ` and some are ` False ` - should the next line under your ` if ` statement be executed or not ? It's ambiguous . For that reason , an ` if ` statement must always be followed by an expression that can be safely cast to a scalar boolean value .

` allFiles ` is an empty list , and ` Sframe = pd.concat ( list , ignore_index=False )` then tries to concat an empty list of data frames which gives you the error .

I don't know how to use apply , or whatever else , to figure this out by group and return a dataframe of only those groups .
I just figured out the answer . I was looking at apply but I needed to use filter #CODE

To join the Dataframes : #CODE

Anyway I can collapse them elegantly , without using the reading from dict and then concat to the list as applicable . In the traditional sense I need to perform multiple joins on this - Any way around ?
Create a group by on the column you want to reduce over and then apply a function that returns the results of the group by an a list per group . Note this returns a series .

I'm using the following dictionary and the pandas resample function to convert the dataframe to monthly data : #CODE
Instead of ` M ` you can pass ` MS ` as the resample rule : #CODE
Pass ` custom_start_months ` to the fist parameter of ` resample `
I also found this list of example resample options here : #URL

how to use map function to split character , get min values , and store in a newly created pandas column
Firstly you are missing a trailing parentheses on you line : ` loansData [ ' FICO.Score '] = list ( map (( lambda x : min ( x ) , fico ))` secondly even with this added you get a TypeError : ` TypeError : map() must have at least two arguments . `
Actually you have 1 too many leading parentheses , it works if you do this : ` loansData [ ' FICO.Score1 '] = list ( map ( lambda x : min ( x ) , fico ))`

I want to bin into bins of 10 ( 0 , 10 , 20 ... 3600 ) . Ultimately this is about producing a density function that I want to do further manipulations on . The 10-size group is arbitrary and something I want to be able to shift as occasion permits . OK I understand the quesion now -- so I guess I am not really concerned whether the 10 is inclusive or not . Usually I do this by dividing by 10 , taking the integer , and then multiplying that integer by 10 , so that I think is closed upper , right ? 10 / 10 is 1*10 is 10 , so 10 goes into the next bin .

You can just call ` diff ` , the default period is ` 1 ` : #CODE
You can also use ` shift ` which is a more general purpose operation that returns a series shifted by a period ( again the default is 1 ) , in this case though on this size dataset it is slower : #CODE
So you can see here that ` diff ` is ~4000x faster than looping over each row , as it is vectorised it will scale much better than a non-vectorised approach .
Thanks . Wondering why ` diff ` is not mentioned in this page ? #URL
` diff ` works on all series that are numerical dtypes as such it is a general function , you can always suggest a doc enhancement at [ github ] ( #URL )

This can be done with ` pandas ` using ` map ` to map the data from one column using the data of another . Here's an example with sample data : #CODE
that is exactly the functionality I was looking for . I just played around with some fake data of very large size , and the map routine is lightning fast and quite robust . Pandas is so damn awesome . Thanks !
In ` python2 ` the iteration could use ` map ` : #CODE

Note that the type on the date is now Timestamp , not datetime . Down the other portion of my code it stays datettime ( which is the correct chain of events , yes ? ) and so now they both reference the same date but test as not equal so I can't apply DataFrame.update to push data from one to the other . Unfortunately both paths have strong data-driven reasons why they should be done the way they are .
` df.columns.levels [ 1 ] [ 0 ] .to_datetime() ` will convert the timestamps back to datetime . I can't find away to apply this to the whole level of the index at once . List comprehension will work but i guess it isn't very ` pandas `

I couldn't use ` .dt ` it give me an error : ` AttributeError : ' Series ' object has no attribute ' dt '`

call ` concat ` and pass param ` axis=1 ` to concatenate column-wise : #CODE
For example , as you have no clashing columns you can ` merge ` and use the indices as they have the same number of rows : #CODE
And for the same reasons as above a simple ` join ` works too : #CODE
Not sure why , on my real data , having 1000 rows in each df , ` concat ` gives me 2000 rows but ` join ` works fine . On example data , both work fine thoug

And the line ` Hist.agg ( len ) .dropna() ` looks like : #CODE

Also found it easier to query the HDF to retrieve unique items direct from the store in a list which could then be sorted and iterated through to retrieve data plot by plot . Note that I wanted the final csv file to have plot and then all the D100 data in date order , hence the pivot at the end .

Essentially I want to drop all the other indexes of the multi-index other than level ` first ` . Is there a easy way to do this ?

If you're working under Windows , you could do something similar . ( I wonder if it makes sense to add a native insert option using this approach to help people in this situation , or if we should simply post a recipe . )
Working with an existing excel file , you can drop an anchor in the data block ( Sheet3 ) you want to import to pandas by naming it in excel and do : #CODE

I am retrieving multiple data frames in csv format from a website . I save the data frames in a an empty list and then read one by one . I can not append them into a single data frame since they have different column names and column orders . So I have the following questions :

how to merge two pandas DataFrames and aggregate one specific column

It should be quite straightforward to get time objects into a dataframe ( load them as string , then use an apply to transform into a time object )

You could also try the [ ` eval `] ( #URL ) method so something like ` df.eval ( ' B =val ')` which may be faster
In your case you need to replace these column headers and do what you want to obtain from the data frame . #CODE

inProp will look like a tuple of my input values for activation , num - a tuple of expected two-neuron output ( either ( 0 , 1 ) or ( 1 , 0 )) and num.argmax() will translate it into just 0 or 1 - real output .

You need to access the vectorised [ ` str `] ( #URL ) method so : ` df [ ' Tweet '] .str .findall ( r ' | ' .join ( phrases ) , flags= re.IGNORECASE ) .astype ( bool )` should work
@USER , as a side note , although current method is very quick , it could benefit even more if you apply ** compiled ** regexp .
@USER While that's true for python regexs in general , I just ran some timeit tests using compiled and uncompiled versions and they resulted in similar time durations . What you can do is use match instead of findall to speed things up ( and cut down on memory usage ) .
@USER , interesting , perhaps if the Series gets bigger it will eventually shows the difference , and YES I agree using ** match ** will definitely improve . You may consider rewriting the logic to make it yield and test on each row rather than on all rows ? It will satisfy OP requirements for really large DataFrame . I am not 100% agreed on my ** map ** + ** lambda ** combinations , only because I need to control memory usage not to max out .
A simple ` apply ` can solve this . If you can weather a few seconds of processing , I think this is the simplest method available to you without venturing outside ` pandas ` . #CODE
Another way to achieve this is to use pd.Series.isin() with map and apply , with your sample it will be like : #CODE
Updated : as @USER pointed out , it could be even more efficient to combine both map and regexp , in fact I don't quite like map + lambda neither , here we go : #CODE
@USER , I think speed-wise , using regexp will be faster as my method requires 3 x ** apply ** to transform the string content , efficiency wise , using ** map ** will yield each ** lambda ** whereas ** findall ** may eventually max out memory
This works , but be very wary of applies as they slow things down as your data grows in size ... Regexs are so fast because they are basically just lexical parsing . If you really are that concerned about memory usage ( which shouldn't be the case here as you are already holding a DF in memory that is bigger or equal in size to the resulting df ) and want to use apply , then at least use regexs in your lambda function .

This is apparently how to load the data correctly from the begining . I still want to know why I can't append this data though ...

But how do I know that I can append age to the groupby object like that ?
Sorry , I confusingly used the word " append " . What I mean to ask is , given ` heart.groupby ([ ' censors '])` , how can I know that I can do ` heart.groupby ([ ' censors ']) [ ' age ']` ? How can a beginner know about this kind of information by inspecting the structure ( not just knowing the type ) of an object ?

Yea I gotta force myself to learn the map function . Thanks !

Python , pandas : Cut off filter for spikes in a cumulative series
This can be accomplished with a one line solution using Pandas ' boolean indexing . The one-liner also employs some other tricks : Pandas ' ` map ` and ` diff ` methods and a ` lambda ` function . ` map ` is used to apply the ` lambda ` function to all rows . The ` lambda ` function is needed to create a custom less-then comparison that will evaluate NaN values to True .
There is a built in method for this ` diff ` : #CODE
as pointed out calling ` diff ` here will lose the first row so I'm using a ugly hack where I concatenate the first row with the result of the ` diff ` so I don't lose the first row
Using ` diff ` like this drops the first row .

ResultFile2 is * .csv file . And I have tried also join that doesn't work , morevoer I cannot use join in an iterative fashion since doesen't work well with " \ " . Insteed I think that the issue is in the management of the Drive letter " D: " , but how to solve it ?

Is there also a possibility to write the ` DataFrame ` into a ` .xlsm ` Excel file ? This is actually more or less the same as ` .xlsx ` , but with the possibility enabled to store VBA macros within the file . I need this because I want to insert and run a VBA macro after creating the file .

Error : AttributeError : ' Series ' object has no attribute ' dt '

This is a bit odd , your second df has a different interval for the it's index so your call to ` loc ` will fail because that label doesn't exist , can you explain better what you require , are you trying to assign to ' C ' the value of z for the row in df2 where it's index is up to and including the corresponding index value in df1 ? I can't see how you can avoid iterating here as you have to perform some index operation for each value in df1 index
TIL : Pandas Index instances have map method attributes .

I think I understand what you want , I would append each df to a list and use ` concat ` to make a single df , for each df we can just add a new column for each symbol : #CODE

pandas merge dataframe fill in missing values
One of the first answers I received suggested using merge outter which seems to do some weird things . Here is a code sample : #CODE
I think you want to perform an ` outer ` ` merge ` : #CODE

Please let me know the command , I am trying with apply but it ll only given the boolean expression . I want the entire row with latest year .

What you are doing is called [ chained indexing ] ( #URL ) so you are right you are operating on a copy in this case , you should use the new indexing methods ` ix ` , ` iloc ` and ` loc ` to avoid this

Merge / Join 2 DataFrames by complex criteria
I think it would be ideal if I could merge / join two ` DataFrames ` , but I can't figure it out how to merge on the complex criterion . Also my datasets are not equal in size .
The efficient way to join on nearby locations is to use a tree data structure , as described nicely in the scikit-learn documentation . Both SciPy and scikit-learn have suitable KDTree implementations .
If you want to merge based on nearness for multiple columns , it's as simple as setting ` join_cols = [ ' d ' , ' e ' , ' f ']` .

Call ` transform ` on the ' measurement ' column and pass the method ` diff ` , transform returns a series with an index aligned to the original df : #CODE
If you are intending to apply some sorting on the result of ` transform ` then sort the df first : #CODE

You can use transpose and count to easily achieve what you want , like this : #CODE
Finally transpose back and assign data frame with replaced columns : #CODE

Assuming I understand what you're trying to do , you could replace ` -9999 ` by ` NaN ` : #CODE
when reading your data , normalize , and replace -9999 with n / a .

on your ` diff ` data frame which produces an output of : #CODE

I think that replacing the values in `' b '` and adding the values from `' c '` are two pretty distinct operations . You can do it in one step with ` X [[ ' b ' , ' c ']] = Y [[ ' b ' , ' c ']]` ( assuming you spell out the columns explicitly ) , but I don't think ` merge ` and ` join ` will give you a way to do the two different operations with a single call .
The idea is you want only the columns unique to X , plus all the columns from Y . ( The DataFrame ` columns ` attribute gives a pandas Index , and these objects support set-like operations such as difference , union , and intersection . )
@USER : I'm not sure , but my hunch is that anything involving a join will be slower ( or at least no faster ) .

It adds one month and tries to replace the day with the original day . If it succeeds , it is no special case . If it fails ( ValueError ) , we have to add another month and go to its first day .

I would like to add a new column , ` d ` , where I apply a rolling function , on a fixed window ( 6 here ) , where I somehow , for each row ( or date ) , fix the value ` c ` . One loop in this rolling function should be ( pseudo ): #CODE

I am trying to apply a filter on a series of values stored in a pandas series object . The desired output is the value itself if it meets the criterion otherwise zero . I can only get it to half work : #CODE

Did you look at ` s.resample ( ' 1D ' , how= " ohlc ")` ? This should resample your series to daily values and calcualte the OHLC columns automatically . And look at ` pd.to_datetime ` for converting your strings to real dates .
Agree with @USER . ` to_datetime ` is another option , though for more esoteric formats , ` strptime ` and ` apply ` work wonders .

It looks like what you may want to do is ` map ` the values I can't answer now as I have to go home first but I think something like ` df [ ' D '] = df [ df.A == 0 ] [ ' B '] .map ( df1.set_index ( ' ID ') [ ' value '])` may work , this is some ad hoc code that may not work , I can't test until I get home
Slightly tricky this one , there are 2 steps here , first is to select only the rows in df where ' A ' is 0 , then merge to this the other df where ' B ' and ' ID ' match but perform a ' left ' merge , then select the ' value ' column from this and assign to the df : #CODE
This works as it will align with the left hand side idnex so any missing values will automatically be assigned ` NaN `
Another method and one that seems to work for your real data is to use ` map ` to perform the lookup for you , ` map ` accepts a dict or series as a param and will lookup the corresponding value , in this case you need to set the index to ' ID ' column , this reduces your df to one with just the ' Value ' column : #CODE
So the above performs boolean indexing as before and then calls ` map ` on the ' B ' column and looksup the corresponding ' Value ' in the other df after we set the index on ' ID ' .
What does work is if you apply the same mask to the left hand side like so : #CODE
So this masks the rows on the left hand side correctly and assigns the result of the merge

Maybe you can have a look at the ` chunksize ` argument of ` read_sql ` . See example here : #URL Not sure this will solve it , but that will do the query in chunks , and you can aggregate or merge them in pandas

Pandas dataframe join groupby speed up
I am adding some columns to a dataframe based on the grouping of other columns . I do some grouping , counting , and finally join the results back to the original dataframe .
But a similar join , takes quite some time ~couple hours : #CODE
Can you post sample data and expected output , at the moment I can only guess that the ` join ` operation looks unnecessary but ` groupby ` is an expensive operation generally
We can see that my combined code is marginally faster than yours so there's not much saved by doing this , normally you can apply multiple aggregation functions so that you can return multiple columns , but the problem here is that you are grouping by different columns so we have to perform 2 expensive groupby operations .
Use groupby , then select one column , then count , convert back to dataframe , and finally join . Result : much faster : #CODE

However , I don't see a way to achieve what you want only using parameters passed to hist .
you replace the single ax.hist() call with the 8 lines or so that I showed above ( and further customize the logic / colors as you want ) . Now it's your own method , so if you want the ranges or colors to be dynamic instead of static , you could add your own parameters for that . You have to add a couple imports , and make explicit the namespace on a few variables , but then you'll have much more control .

This obviously means that I wanted to search for words like rigour and rigour s , en demeanour and demeanour s , centre and centre s , h arbour and arbour , and fulfil . So the keywords list I have is a mix of complete and partial strings to find . I would like to apply the search on this DataFrame " df " : #CODE
Thanks a lot . To get the new ' keyword ' column right even when searching through a longer string , I suggest making the ` mask = [ m.group ( 1 ) if m else None for m in map ( r.search , df [ ' name '])]` and then ` df [ ' Keyword '] = mask ` should return the correct keyword found in each string .

As you can see , the data type changed to ` float64 ` . Now let's set all NaNs to 1 , via bool indexing , making use of numpy's ` isnan() ` function : #CODE

@USER .mnky .dshwshr Right ! Sorry , I implicitly assumed that I want to drop the NaNs anyway , so I would just take the difference between the shape before and after .
it does appear to be significantly slower call ` sum() .sum() ` see my update but your ` map ` scales better

How do I drop rows containing all values as zero after column 3 ? The first and second rows ( index ` a ` and ` b `) in this example are to be dropped .
You can subscript the columns , replace ` 0 ` with ` NaN ` , drop any rows that don't have at least 1 non ` NaN ` value and use ` loc ` on the index : #CODE
Thant's true . I forgot the abs !

You can pre-create an axis object using matplotlibs ` pyplot ` package and then append the plots to this axis object : #CODE
Alternatively , you could concat the two series into a m x 2 DataFrame and plot the dataframe instead : #CODE

I want to be able to use all of the built in functionality of pandas , like being able to merge DataFrames together , slicing , indexing , plotting etc . I also want to keep track of the units of the data so that when I merge , plot , etc . I can adjust values so they are congruent .
Then my merge method looks something like this #CODE

So what are you asking ? how to drop all rows after 49th minute or you want to drop just rows 47:49 ?
I want to drop rows 47:49 . I have a loop that looks for at least 3 consecutive flows > 0 before calculating energy , however this data file is huge and I haven't been able to work with the whole file yet without crashing . I've used Pandas in the past for resampling and like how quickly it deals with large data , but I haven't been able to figure out how to do filtering like this .

you can use ` concat ` to concatenate a list of DataFrames #CODE

unfortunately if you want a unique random value for each item there have to be len ( items ) calls . At least a generator expression will avoid having to create it all in memory . See how you go anyway

If the quality flags are the same , then interpolate the value between the two valid values on either side . In the example , the second set of NaNs would be replaced by qf = 1 and v = 6 and 9 .
I have tried to solve this by finding the NA rows and looping through them , to fix the first criteron , then using interpolate to solve the second . However , this is really slow as I am working with a large set .

As you've not posted any data or code I will demonstrate how the following should work for you . You can pass a list to ` isin ` which will return a boolean index which you can use to filter your df , there is no need to loop over and append the rows of interest . It's probably failing for you ( I'm guessing as I don't have your data ) because you've either gone off the end or your index doesn't contain that specific label value . #CODE
Would it be more efficient to create a second pandas dataframe from the " customer_list " list and then perform a SQL type merge ?

I think you've neglected to pass the ` axis=1 ` param to apply , so it's operating column-wise hence the error . Try : ` holdings [ ' wt '] = holdings.groupby ([ ' holdings.portfolio ' , ' holdings.date ']) .apply ( lambda x : x [ ' mv '] / sum ( x [ ' mv ']) , axis=1 )`

How to append a list as a row in pandas.DataFrame() ?
I am iteratively reading a log file and parsing / extracting data and would like to append that to a dataframe . #CODE
Please post data that reproduces your error , basically you need to return either a Series or DataFrame to append to your existing df , also this is horribly inefficient , what is the format of your data ? You can probably use any of the existing methods to read it in and create the df once : #URL

That said , in your case you basically want to broadcast ` average_read_intervals ` over some currently non-existent columns of ` R ` , which you could engineer by changing the shape of ` R ` through duplicating its column ` len ( average_read_intervals )` times . #CODE

You can subscript the cols like so : ` for i in len ( df ): if i + 1 ! = len ( df ): # sm.OLS ( returns [ returns.coloumns [ i ]] , returns [ returns.columns [ i+1 ]]) , fit() ` os similar
You can index dataframe columns by the position using ` ix ` . #CODE
This would be the value at the intersection of row 0 and column 1 : #CODE
A workaround is to transpose the ` DataFrame ` and iterate over the rows . #CODE

does it act differently if you transpose the data ? 10^3 rows and 10^6 seems ... backwards .

Simple pandasql join is failing
Hmm - even the simpler join #CODE
where ` name_map ` is a map created from the ` types ` table above .
It works if you include the ` JOIN ` statement : #CODE

Transpose data in Pandas DataFrame
The ' transposition ' you are looking for is really [ ` unstack `] ( #URL ) . You would do something like ` df.set_index ([ ' ID ' , ' Month ']) .unstack() ` . However , as BrenBarn said , if you don't need to create the extra DF it might be better not to .
I'm looking into using the ` sklearn.linear_model.LinearRegression ` module but am unsure on the syntax to use the ` LinearRegression ` in the ` apply ` function that BrenBarn suggested .

To concatenate , first rename and then join : #CODE

first melt your DataFrame to convert it from wide format to long format , and

I thought about doing an ` apply ` of some sorts to the ` Category ` column after ` groupby ` but I am having trouble figuring out the right function .
you don't need a ` groupby ` here , you just need ` sort ` and ` shift ` . #CODE

Looks like it getting over written instead of increasing the index of the data frame to record the next element . What is the best way to solve this issue and insert row wise as I needed in this case . I looked at two other stackoverflow questions both didn't have the solution .

I'm trying to create a sub class of DataFrame , that extends it with few properties and methods . In addition to the default constructor there are few others like the one below that initialized the DataFrame from SQL table and then add few attributes ( I simplified it and left a dummy just to demonstrate the problem ) . So once I get the initial df , I " convert " it to my class by ` df.__class__ = Cls ` statement . It seems somewhat weired to me , but reading few posts on this issue ( e.g. Reclassing an instance in Python ) it's a valid way to go , and seems to work most of the time . But the problem is when I use a method of the parent class ( in that case DataFrame.append ) that returns a new instance of the object : ` sdf2 = sdf1.append ( item )` - the resulting class of sdf2 is the DataFrame and not SubDataFrame , and consequently ` print ( ' sdf2 : ' , sdf2.name )` fails because ' DataFrame ' has no attribute ' name ' ... the bottom line , trying naively to use a standard DataFrame method , my object was corrupted ... I can solve it by writing the ( virtual ) ' append ' method in my subclass , but in that case I would need to do it for many methods and if I cannot use the inherited methods no sense in subclassing at all ( I can just define the DataFrame as a member variable of my class ) .

In addition , I can't pivot on this either . When I try the following : dfpp =d f.pivot ( index=0 , columns=1 , values=2 )
You can't ` pop ` a Numpy array . You need to use the ` delete ` method from Numpy . If you need help with the pivot then you should include the code that you tried so that others can help to produce a solution .

I'm trying to essentially do the same as a pivot table in Excel would do but using pandas . Here is some of my data : #CODE

I would like to pivot it to get the table arranged as : #CODE
Wow . OK , I just realized that ` pivot ` and ` pivot_table ` are two different methods .

Unstack Pandas DataFrame with MultiIndex
As the error message says , you apparantly have duplicated values in the index , and ` unstack ` cannot handle these ( multiple values to put at the same place ) . So you should first get handle the duplicate values yourself , or you could use ` pivot_table ` and specify how the duplicates should be aggregated .
You can chain a ` .drop_duplicates() ` call before the ` unstack ( ' indicator ')` call if don't need to keep the duplicate indices .

As requested by OP , if you want to implement an ` apply ( lambda ... )` to all the columns then you can either explicitly set each column with a line that looks like the one above replacing `' col1 '` with each of the column names you wish to alter or you can just loop over the columns like this :
but how can I apply this to my entire data frame ?? can you edit your answer to my question , it would be really helpful

Python Pandas inner join
I'm trying to inner join DataFrame A to DataFrame B and am running into an error .
Here's my join statement : #CODE
Do I need to correct my join statement ? Or is there another , better way to get the intersection ( or inner join ) of these two DataFrames ?
use ` merge ` if you are not joining on the index : #CODE

It varies ; always a good idea to use ` timeit ` yourself to find out , if it's really a bottleneck . For longer frames it'll be much faster than ` apply ` , but for smaller ones you won't be able to amortize the startup cost and soit might be a little slower .

Drop range of columns by labels
What's the best way to drop all columns between column `' D '` and `' R '` using the labels of the columns ?
Edited to cut out instead of extract . This should do the trick !

So you open a file , use ` csv.writer ` to insert a line and then into the same opened files you export the DataFrame .

It's quite common to use boolean indexing for this kind of task . With this method , you find out where column ` a ` is equal to ` 1 ` and then sum the corresponding rows of column ` b ` . You can use ` loc ` to handle the indexing : #CODE
Ok thanks , also how do I convert all the values in the column to an ` int ` thereby enabling me to sum them ? I was using ` map ` to convert all of them to ` ints ` but I think there might be an inbuilt function in pandas to do this more efficiently .

dt = datetime.datetime.strptime ( dateStr.split ( " ") [ 0 ] , " %Y-%m-%d %H : %M : %S ")

How to merge pandas string in python ( join sql alike ) ?
I'm trying to merge ( or left join sql alike ) for 2 table ( df1 , df2 ) using string value " name " to merge .. similar to sql : #CODE
in python I can use integer to merge / but when I use " name " which is string I got an error . #CODE
This work fine if I use integer on , but when I try to use merge table with name ( string ) i got this error below , How can I fix this issue ?

You could read the csv in chunks and only append if the subset meets your condition

Better strategy to drop ALL duplicates from pandas.DataFrame ?
Does anyone know a better strategy how to drop ` ALL ` duplicates from ` pandas.DataFrame ` ??
So you only want to drop where both a and b are duplicates , is it true that where there are duplicates in ' a ' there is also a duplicate in ' b ' ?

You can just use the subscript notation with ` loc ` which is label based indexing : #CODE
That ( by which I mean ` iloc `) actually won't work without a tweak for the OP's case , as the OP's index begins with 1 .. could just use ` loc ` , though , no ?
Maybe I didn't make myself clear . I want to find values inside a interval , and not slice it with loc .
@USER please explain ` loc ` selects labels that match those values as my output shpws , it is not simply selecting the 10th row up to 10000th row .
@USER OK , for example , if I do ' loc [ 10:100 ]' I receive a error point to the inexistence of the index=10 , in case it does not exist in my DataFrame . What I wish is a scan inside a given interval . Like the Series.between command .

I am using Pandas and I have a dataframe which is a merge from multiple different SQL tables and looks something like this : #CODE
Then I want to drop the columns Col_2 to Col_4 .
Use ` any ` and pass param ` axis=1 ` which tests row-wise this will produce a boolean array which when converted to int will convert all ` True ` values to ` 1 ` and ` False ` values to ` 0 ` , this will be much faster than calling ` apply ` which is going to iterate row-wise and will be very slow : #CODE
As you are running pandas version ` 0.12.0 ` then you need to call the top level ` notnull ` version as that method is not available at df level : #CODE
AttributeError : ' DataFrame ' object has no attribute ' notnull '
I'm using version 0.12.0 and I guess I can't upgrade because it is the version we have at work . Thanks for the suggestion , i get an attribute error for isnull as well

Using the ` %timeit ` module running in IPython Notebook the ` for ` loop compared to the ` apply . ( lambda ... )` is a little over 3 times as fast . #CODE
@USER is there a reason you don't want to use a loop ? According to the ` %timeit ` module running in IPython Notebook , the for loop implementation is over 3 times as fast as a lambda apply .
I thought the for loop is less efficient as l thought the dataframe apply is optimized for matrix type operation like MATLAB . I probably use your approach and stitch the result back to the main dataframe . What's is the most efficient to stitch that column vector back to the data frame
@USER If by stitch you mean append the column back to the original data frame then you can do something like ` original_df [ ' new_column '] = df_dt ` assuming your ` df_dt ` data frame is a nx1 column vector .

I know that this very possible to do with an apply command but I would like to keep this as vectorized as possible so that is not what I am looking for . So far I haven't found any solutions anywhere else on stack overflow .
I knew there had to be a way to do this without resorting to an apply function . Thank you so much .

I want to create a new DataFrame such that each row is created from the original df but rows with loc counts greater than 2 are excluded . That is , the new df is created by looping through the old df , counting the number of loc rows that have come before , and including / excluding the row based on this count .
The output excludes the 4th row in the original df because its loc count is greater than 2 ( i.e. 3 ) .
Also , be careful with your column names , since ` loc ` clashes with the ` .loc ` method .

If you want to mutate a series ( column ) in pandas , the pattern is to ` apply ` a function to it ( that updates on element in the series at a time ) , and to then assign that series back into into the dataframe #CODE
While this will work , ` apply ` is the method of last resort : it tends to be pretty slow . In this case , we can reach for the vectorized ` dt ` accessor , and get the times via ` df [ " date "] .dt .time ` ( after we've ensured the column is datetimelike , anyway . )

How do I found out tz for both dates ? How do I make them the same so I can calculate the number of days between them ?
Length : 50 , Freq : None , Timezone : UTC

but can't figure out how to apply this to my problem ?
What you are asking for can be achieved by using [ ` masking and where() `] ( #URL ) and [ ` shift `] ( #URL )
PS the original problem is hiding another issue that shows up when taking out the shift portion of the function . The return shape doesn't match , but thats another problem , just mentioning it here for full disclosure

should work . The first method simply asks whether something is equal to the number which came before or 13 , and then patches the start . The second replaces all 13s with ` nan ` , does a forward-fill ( so that the 13s are replaced by the last non-13 number or nan ) , and then does the usual shift check . This gives the right answer : #CODE

You could use ` pd.Int64Index ( np.arange ( len ( df ))) .difference ( index )` to form a new ordinal index . For example , if we want to remove the rows associated with ordinal index [ 1 , 3 , 5 ] , then #CODE

Pandas join : Does not recognize joining column
I have no idea what's happening , the title is just a first-order approximation . I'm trying to join two data frames : #CODE
That's the data frames , I want to join them over the common column ` TUCASEID ` , of which there are intersections : #CODE
Well , that's weird , the only column that appears in both data frames is the one to join over , but well , let's concur [ 1 ]: #CODE
Despite there being a huge intersection . What's going on here ? #CODE
Your index values don't match why not just merge them ` df_sum.merge ( emp , on= ' TUCASEID ' , how= ' outer ')` or you just interested in adding the ' status ' column for each ' TUCASEID ' row ? in which case do ` df_sum [ ' status '] = df [ ' sum [ ' TUCASEID '] .map ( emp.set_index ( ' TUCASEID ')`
Dunno but ` join ` joins on index usually , it is strange the behaviour which I can recreate but the other methods I suggested should work
` df.join ` generally calls ` pd.merge ` ( except in a special case when it calls ` concat `) . Therefore , anything ` join ` can do , ` merge ` can do
Under the hood , ` df_sum.join ` calls merge this way : #CODE
Furthermore , the merge is empty when called this way : #CODE
because the ` left_on= ' X '` needs to be ` on= ' X '` for the merge to succeed as desired : #CODE

have you created a ` matplotlibrc ` file ? did you specify a non-interactive backend ? do you get an image file when you replace ` plt.show() ` with ` plt.savefig ( ' test.png ')` ?

@USER Bradley , I'm including days forward in the running mean ( 2 days forward and 2 days back , rather than 4 days back to present . So I'm looking at a central moving average ) though it's simply a shift of days .
Calculating the 5 day centered mean . Because the dataframe can have missing days or days with more than 1 observation the data needs to be resample a frequency of days , with blank days filled to 0 : #CODE
So I'm trying as above except with min_periods = 1 ( so it doesn't complain for days with missing records ) and how= ' mean ' ( ' sum ' does not average ) . However , the answers are not corresponding to the values I'm calculating by hand . Perhaps I need to do something with the freq or how parameters ?
the ` how ` parameter handles the aggregation of the resample . If you use mean then the rolling mean will be calculated from the mean of the days ratings when upsampled to the frequency of days . Updated question to show how the resampling works .
I'm afraid this still does not answer my question . I'm not looking to resample anything . Instead , I have data like

Many statistical functions contain a parameters to append the computation weights , for example np.average :
Given that I would like to include functions such as ` np.average ` , how would I have to rephrase my command from above ? And are there commands for other aggregation functions such as ` median ` ? Trivially , ` min ` and ` max ` shouldn't change , and computing ` len() ` shouldn't be too difficult either .

Hi @USER thanks for looking at this . The output should be the same as ( in this case ) ` df.stack ( port ) .groupby ( level =[ 1 , 2 ]) .apply ( lambda x : ( x [ ' wts '] *x [ " pchg "]) .sum() ) .unstack ( ' port ')` if that makes sense . In practice I have a number of other columns that stop be taking the stack - unstuck round trip and would like to avoid the processing overhead too . You're right I'm trying to ` groupby ` on two axes at once .
You indeed do not need to unstack to get what you want , using the ` product ` method to do the multiplication you want . Step by step :

Nan values when I merge these data frames
I am want to merge two data frames in pandas but I am getting Nan values . #CODE
Independent of your actual question , you don't need the ` parse_label ` function . ` map ` also works with dictionaries directly , like ` data_org [ ' sex '] .map ( { ' I ' : 0 , ' F ' : 1 , ' M ' : 2} )` .
@USER I am getting an Nan value if I apply this map function directly to my data_org data frame .
It's unclear what you are trying to do here , you performed a conversion of your sex character values to their corresponding int value and then you try to merge this back , why not just convert them in the orig df : #CODE

You can call the vectorised ` str ` methods to split the string on decimal point , join the result of split but discard the last element , this produces for example a list ` [ 2,985 2 ]` which you then join with a decimal point : #CODE

How should I align the text labels against the x tickers in the graph here ? I am using host.set_xticklabels ( labels , rotation= ' vertical ') , but that doesnt seem to work .
I need to align the " black " mass as in the picture .

In addition , I wanted to plot the data for years 2010 , 2011 and 2012 in one plot . Without changing the year in all_weekdays = boxplot_data [ ' pv '] [ 2010 ] , I think I need a subplot in a subplot , or ? So I need seven boxplot particular adjacent for year 2010 and below the respective seven boxplots for 2011 und 2012 .

For column ' 2nd ' and ' CTR ' we can call the vectorised ` str ` methods to replace the thousands separator and remove the ' % ' sign and then ` astype ` to convert : #CODE

@USER Sounds like the apply isn't working . Can you post the exact code you have ? If you're using a full dataframe , you would have to do something like : DF [ ' COL '] = DF [ ' COL '] .apply ( lambda x : .......

A possible solution is based on pandas merge method to create databases on the basis of relations .
2 : reset the indices and merge on the base of the proper labels #CODE
It's clear that one has to merge ` b_reset ` and ` df_reset ` in a relation one-to-many , linking ` level_0 ` of ` b_reset ` and ` index ` of ` df_reset ` : #CODE

I am trying to create a new data frame by indexing just values in the " diff " column . I want to use two comparisons but it is not working .

Well , I have no use for these NaNs . I can easily drop them later , but this is obviously a hackish solution . Anyone has a better one ?

apply to the entire dataframe a user-defined function involving another dataframe in pandas
Also , I am always confused by apply and applymap , what is the difference and when should use one over the other ?
Never used ' where ' before , thank you very much ! is there any other method to do it ? maybe using apply ?
@USER apply won't be as efficient / fast as vectorizing or using where , since basically it has to loop through within python just like you do in your example code ( rather than using much faster numpy / C ) .

@USER : Merge takes two data frame as input . But my problem is the input is * list of dataframes ( 2 or more ) * .
You might find [ this question ] ( #URL ) useful . You could set your Probe and Gene columns as the index and then use ` concat ` as shown there .
Note : this means it may not be as efficient as using concat if you have lots of DataFrames in tmp .

You have to use ` loc ` : #CODE
Without loc , ` df [[ ' QRR ' , ' QTM ' , ' QXM ']]` , pandas is trying to select those columns ( which don't exist , hence the " not in index " message ): #CODE
@USER ah , yes it does precisely that ! ok you just need to use loc :) updated the answer

The ` kind= ' hist '` option was added to ` Series.plot() ` in version ` 0.15.0 ` . Your code example should work with latest version ` 0.15.2 `

merge multiple dataframes into one using pandas
how can I merge all these in to the following data frame : #CODE
should I use merge or join ? what is the difference between them ? Thank you .
` join ` uses ` merge ` underneath , as a rule of thumb if you are going to join on indices then use ` join ` if not then use ` merge `
For your sample data you can achieve what you want by performing ` concat ` twice , this assumes that the last 2 dfs align with the master df . The inner ` concat ` concatenates the 2 supplemnentary dfs into a single df row-wise , the outer ` concat ` concatenates column-wise : #CODE
An approach to your real data problem would be to add a ' fruit ' column to each supplemental df , concatenate all these and then merge back using ' fruit ' and ' date ' columns as the keys : #CODE
@USER you could groupby the fruit and date and then set the price , your problem though is that if you have lots of dfs with no identifier in them ( i.e. a fruit column ) then you can't merge them back without setting all the rows that match the dates with the same price , What you could do is add the fruit column for each supplementary df , concatenate all these supplementary dfs and then you can merge them back using the fruit column and date column , that would be how I would approach this

When I stack the years , the new index level has no name : #CODE
Is there a nice way to tell ` stack ` I'd like the new level to be called `' year '` ? It doesn't mention this in the docs .
I think that is the best solution for now . But you can always open an issue with an enhancement proposal to add this functionality to stack ( #URL )

As you can see in rows 2 , 3 timestamps can be duplicates . At first I tried using pivot ( with timestamp as an index ) , but that didn't work because of those duplicates . I don't want to drop them , since the other data is different and should not be lost .
Since index contains no duplicates , I thought maybe I can pivot over it and after that merge the result into the original DataFrame , but I was wondering if there is an easier more intuitive solution .
As your ` get_dummies ` returns a df this will be aligned already with your existing df so just ` concat ` column-wise : #CODE
You can drop the ' cat ' column by doing ` df.drop ( ' cat ' , axis=1 )`

` iterrows ` gives you a ` ( index , row )` tuple on each iteration . The best way to use it is generally to do ` for ix , row in data.iterrows() : ` , not using ` next() ` .

@USER Hayden this is partial string indexing ; does not work with a scalar only slices ( and loc is strict )

What's the most efficient way to do something like this ( which doesn't work since the argument to ` replace ` is a slice ) ? #CODE

` get_loc_level ` is the similar of loc , i.e. label based rather than by position :
Thanks @USER could I just check if you have any thoughts on the first question ? Are there more efficient ways to pick items from the date index other than the four I've described above ? .. and thanks for explaining the get_loc_level . Do you any use case examples you could share . Where might I apply it ?

Credit to this thread : Append string to the start of each value in a said column of a pandas dataframe ( elegantly )

UPDATE D: But when I access the entry using loc I got something like \u0102\u02d 8\ xe2\x82\u0179\xc2\u015 , \u0102\u02d 8\ xe2\x82\u0179\xe2\x84\u02d8

I'm trying to merge those DFs in the list so that it produces this : #CODE
You also need to specify it as an outer merge ( to get the NaN rows ): #CODE

How to insert dataframe to a data frame in Pandas
What I want to do then is to insert ` pg ` into ` rep1 ` , resulting in : #CODE
Call ` concat ` and pass param ` axis = 1 ` to concatenate column-wise : #CODE
there is no insert point as such with concat , merge or join
how can I specify the insert location ? Namely after ` RP1 ` . Note there are more " RPx RPx.pacall " pairs .

This seems a bug , but not necessarily in the pandas ` read_sql ` function , as this relies on the ` keys ` method of the SQLAlchemy ResultProxy object to determine the column names . And this seems to truncate the column names : #CODE

I want to merge all wheat varieties ( i.e. spring and winter ) into a single , level 0 called Wheat . the out come should be : #CODE

Some timings indicate that both my solution as that of @USER is much faster than the apply ( pd.Series ) approach ( and the difference between both is negligible ): #CODE

please add the full stack trace .

looks like pandas.DataFrame.quantile member func is consistent with the numpy.percentile func . however the pandas.rolling_quantile func returns diff results . reduce the row number to 5 , the problem will be gone ( all three methods return the same results ) . any thoughts ?

Python pandas apply on more columns
How can I generate more columns in a dataframe using apply with more columns ? My df is : #CODE
But what if I want to use more than two columns at apply ? #CODE
Using the Series constructor within the apply usually does the trick : #CODE

4 ] finally , we merge the two dataframes : #CODE
Thank you this is very helpful reference . I had tried similar methods but was too narrow minded on solely using the Datetimeindex where groupby , map , and apply were running into errors .

you can use shift to offset the rows in the dataframe .
shift a copy of the dataframe 12 periods to get the value 12 months ago , minus from the current record , and divide by current record : #CODE
update the sign on the period in the shift function

I think it's easier to use plain ol ' argmax : #CODE
There still is a problem if ' display ' is not only 0 / 1 : ` argmax ` gives the index of the first maximum value , not the first non-zero value . We would need to convert the ` display ` column to Boolean first .
As you wrote , there is another issue : if ` seen ` is always zero for a given user , ` argmax ` will return 0 , just like it would for a user with a non-zero ` seen ` on the first row .

Pandas Merge on Specific Attributes of DateTimeIndex
Is there a way to perform this merge as I have outlined it ?
Then just do a merge on the whole index .

Pandas resample bug ?

What I want is a new column of data ( or really to replace the modifiedAmount column with one ) that contains "" in cases where the original modifiedAmount was EITHER :
I see this option when I want to apply a scalar result , but I couldn't figure out how to put df into lambda like this : #CODE
you can use ` apply ` column-wise on the whole dataframe . #CODE

If you want to form a single string from that you could join together multiple ` p ` texts as lines , eg : #CODE

How to pivot a dataframe in Pandas ?
I have a table in csv format that looks like this . I would like to transpose the table so that the values in the indicator name column are the new columns , #CODE
What does your dataframe look like currently ? What does it do when you call ` pivot ` on it , that you're calling " not much success ? "

they are transposing the sorted zipped list , ` zip ( imp , names )` zip together , then sort ` sorted ( zip ( imp , names ))` finally transpose ` zip ( *sorted ( zip ( imp , names )))`
@USER - No , I know the meaning of transpose . but how does zip of zip perform a transpose , thats where I am confused with . in numpy you can transpose an array using .T command . but zip function is used to create pairs right ?? but how does it perform a transpose here ??

@USER you might be able to use slicing to index a range of columns in a single operation and an ` apply ` . Check out the Pandas documentation as well as the numerous SO questions for better guidance .

Welcome to Stack overflow ! This cam to me to review as it is your first post . Have you tried looking at the [ ` dataframe.groupby `] ( #URL ) object ? Maybe you can edit your question with what you've tried from that if so , if not - I'll answer .
@USER Hello , I have actually came across groupby in the documentation / cookbook / tutorials . I felt like it is what I am looking for , however I was not able to apply it on my problem .
If you really need to replace column ` H_SC ` with ` H_AVSC ` in your ` finalTable ` , rather than add on the averages :

What I mean by ' intersection ' :
That didn't really answer @USER ' s question . You need to define what you mean by intersect . If you mean set intersection then that would imply that you want all the values that occur in both ` SMA_45 ` and ` SMA_15 ` regardless of them occurring on the same day . Or perhaps you only want an intersection of their values on the same day . Your question as it currently stands introduces too much ambiguity to receive a focused answer .

Pandas merge on aggregated columns
Is it possible to merge ` df ` with the newly aggregated table ` gb ` such that I create a new column in df , containing the corresponding values from ` gb ` ? Like this : #CODE
Can I merge ` df ` with ` gb ` on the column names ?
Is it possible to place the result of the ` agg ` into a new column so that the merge doesn't rename columns ? The advantage of using ` reset_index ` , I suppose , is that the number of rows in the aggregate table corresponds to the number of aggregation keys ( i.e. [ ' b ' , ' c ']) , which I may want to keep for further analysis beyond the merge .
@USER - you would have to rename the the column names that clashed in order for the merge to not add a suffix . You can store the groupby object as a variable , it's just an object that describes how the groupby should be performed there is no performance penalty with doing this
There is no need to reset the index or perform an additional merge .

The easiest way ( I see ) to convert the sqlite table into the format Matplotlib's ` pcolor ` requires to draw a heat map is to use the Pandas DataFrame's ` pivot ` method . Since this ` pivot ` will reorder the column and rows , there is no need to fuss over the order generated by the SQL query . Instead , it is easier to fix the order in Pandas : #CODE

I've modified your code based on some assumptions . I think what you are trying to do is use your mask to mask into the original df , print or get the sum , and additionally get the length . To get the length you need to use the mask with the original df , call ` dropna() ` to drop the ` NaN ` rows , and then call ` len ` on the resultant df : #CODE

I would convert the string to a datetime and then use the ` dt ` accessor to access the components of the time and generate your minutes column : #CODE

Is there a way to store the result of ` value_counts() ` and merge it with / add it to the next results ?

Rpy2 and Pandas : join output from predict to pandas dataframe
I am using the ` randomForest ` library in R via ` RPy2 ` . I would like to pass back the values calculated using the ` caret ` ` predict ` method and join them to the original ` pandas ` dataframe . See example below . #CODE
But how can ` join ` this to the ` withheld ` dataframe or compare to the original values ?

loc fails on a DataFrame using the DataFrame's own index ?

Hi , thanks for the answer . Your code works . However , it works the same way as using groupby() .first() . My objective is to have ' first ' and other aggfunc like numpy.mean() so that I don't have to do it twice and merge the tables . Any ideas how to do that ?
You can pass a dictionary to ` aggfunc ` with what functions you want to apply for each column like this :

You could however replace the call to ` cumsum ` with a call to ` argmax ` : #CODE
Note , however , that ` argmax ` returns an index level value , not an ordinal index
location . Therefore , using argmax will not work if ` x.index ` contains duplicate
So while using ` argmax ` is a bit faster in some situations , this alternative is not quite as robust .

The most obvious way to obtain that would be to stack all the daily results into one dataframe , then group it by day and run the stats on the result . However I would like an alternate method because I run into a MemoryError with the amount of data I process .

You can use shift to offset the date and use it to calculate the difference between rows . #CODE
So I tried doing ` timeDelta = df.index - df.index.shift ( 1 )` and that generated ` ValueError : Cannot shift with no offset ` . I then tried ` timeDelta = df.reset_index() [ ' index '] - df.reset_index() [ ' index '] .shift ( 1 )` , which worked brilliantly , so thank you !

Why can't I apply shift from within a pandas function ?
But while directly applying shift works , in a function it doesn't work : #CODE
Why not call shift directly ? I need to embed it into a function to calculate multiple columns at the same time , along axis 1 . See related question How to create interdependent columns in pandas , part 1 : ambiguous truth value with boolean logic
Try passing the frame to the function , rather than using ` apply ` ( I am not sure why ` apply ` doesn't work , even column-wise ): #CODE
have not seen this anywhere in the pandas documentation ! will pursue further , but is this performant on par with apply ?
My assumption is that this is more performant than apply as ( if ? ) the shift and sum are vectorized .

This throws an error when I try to use the dt specification to create the DF #CODE
Clarification question : are you just confused that the datatype reported by the DataFrame is ` object ` and not ` str ` ? Because , ** that's never going to happen . ** From the [ documentation ] ( #URL ): " The main types stored in pandas objects are float , int , bool , datetime64 [ ns ] , timedelta [ ns ] , and object . " String data are always just stored as object datatype . I'm guessing that the * real * problem is that you're having trouble getting your structured array back ?

Apply a function to a DataFrame that is intended to operate
elementwise , i.e. like doing map ( func , series ) for each series in the

The reason I'm asking this is that I can't get shift to work in a formula , but even if the question in the link is answered , I find this question interesting enough by itself .
or ` argmax ` : #CODE

Actually I will be appending 500+ files with various size to append
If you want to drop the rows which have NaNs use dropna ( here , this is the first ten rows ): #CODE

I can see it is possible to multiply col1*Value , then col2*Value and so on ... and make up a new dataframe to replace df1 .

Where is it that I have to apply ` toarray ` or ` todense ` ?

I want to convert those data into a list where the ` i ` th element indicates the position of the nonzero element for the ` i ` th row . For example , the above would be : #CODE
Here is a solution that works for limited use cases including this one . If you know that you will only have a single ` 1 ` in your row , then you can transpose the original data frame so the indices of your columns from the original data frame become the row indices of the transposed data frame . With that you can find the max value in each row and return an array of those values .
Your original data frame is not the best example for this solution because it is symmetrical and its transpose is the same as the original data frame . So for the sake of this solution we'll use a starting data frame that looks like : #CODE

It works , except I can't figure out how to keep the header row while appending . It either disappears or is duplicated with each append .
Unfortunately , that doesn't work . It will add the header , but also replicate it every time you append to the dataframe .

This is one method , I'm trying to figure out a vectorised method , basically you define a function that takes your row and then call apply , passing the function name and param ` axis=1 ` to apply row-wise . The color_cols is just a list of your color column names defined by : ` color_cols = [ col for col in df if ' color ' in col ]` #CODE
there is another method calling ` eval ` : #CODE
The mask method is over 2x faster than the query and eval method for this sample dataset . The ` apply ` method is actually the fastest method but it will not scale as well as the other methods as this essentially loops over each row .
@USER indeed this would get tiresome to type out , I was a little surprised that ` eval ` was a little slower than ` query `

If your data is evenly spaced , creating the ` x-y ` grid using ` np.linspace ` and ` meshgrid ` , then getting the ` z ` values using ` scipy.interpolate.griddata ` should work well . See this post : #URL If not evenly , I believe ` griddata ` will interpolate but I have not tried that so I am not certain .

That is [ utf-16 little endian BOM ] ( #URL ) you should be able to set the encoding to ' utf-16 ' when loading and writing , otherwise just strip it out
Actually My original Data is like this : `" DOEClientID " , " DOEClient " , " ChgClientID " , " ChgClient " , " ChgSystemID "` After using replace and putting data into the text file from original csv ... it looks like above mentioned one which is going in the dataframe .

I have two DataFrames A and B . I want to replace the rows in A with rows in B where a specific column is equal to each other . #CODE
Just call ` update ` : this will overwrite the lhs df with the contents of the rhs df where there is a match in your case replace ` df ` and ` df1 ` with ` A ` and ` B ` respectively : #CODE

python pandas join to one row
I am wanting to join two dateframes together with a left join but want every matched items be be joined to just one row in the resulting dateframe .
Causes the rows in df1 to be duplicated for the merge to work creating what I would call a uni file and not a flat file which I would like .
We can then merge this back to the original df : #CODE
Your method works well . But am struggling to apply it when there are multiple columns in the df1 .

Pivot tables using pandas
So based off the answer below I did a pivot using the following code : #CODE
Which produced a somewhat decent result . When I used ' ethnicity ' or ' veteran ' as a value my results came out really strange and didn't match my value counts numbers . Not sure if the pivot eliminates duplicates or what , but it did not come out correctly . #CODE
To get counts use the ` aggfunc = len `
It is the length of the array generated by the pivot at the intersection of the values in the row and column ( for ethnicity in your example )

As for why you get your result when you ` zip ` the ` transpose ` , it's because the iterable object that is returned are the column names rather than the column contents

I've written a function called muzz that leverages the fuzzywuzzy module to ' merge ' two pandas dataframes . Works great , but the performance is pretty bad on larger frames . Please take a look at my apply() that does the extracting / scoring and let me know if you have any ideas that could speed it up . #CODE
and now the muzz function . EDIT : Added choices= right [ match_col_name ] line and used choices in the apply per Brenbarn suggestion . I also , per Brenbarn suggestion , ran some tests with the extractOne() without the apply and it it appears to be the bottleneck . Maybe there's a faster way to do the fuzzy matching ? #CODE
One possibility is to pull the ` right [ match_col_name ]` outside of the ` apply ` , so that you don't recalculate it every time . You should try profiling your code and testing it with different inputs to see if the bottleneck is really in the apply or in the fuzzy matching itself .
Yes , but that is inside the function you apply , which is called once for each element in the series you apply it on . Your code retrieves ` right [ match_col_name ]` repeatedly , once for each element in the Series . If you extract this once to a variable and then use the variable in the call , you will avoid all those redundant lookups .

If you don't like the post merge names , you can always change them using rename() . For example #CODE
If I include " Date " in columns it creates " Date_away " and " Date_home " . Is there a way to exclude columns ? I know I can drop columns from a dataframe or add them later .

Why the mismatch in second doing grouping one by one in A then C vs doing them together . Is it a bug or feature ? Normally multi grouping should proceed in this fashion . First take out elements satisfying predicate ' A ' and then use those groups to apply ' C ' grouping .
In simple words ==> When we bucket on two columns , I don't want the second column ( or inner column ) cut points two be exactly same for all groups of outer column ( first column ) . The cutting of second column should be done based on the actual data you get in actual groups of first column buckets .
that should be one of the behaviours of groupby . apply qcut to subset of values in column C . Coz already it's been cut into 2 parts via ' A ' cut
is it possible to do grouping via ' A ' , and then for each of those groups apply grouping to column ' C ' via map / lambda's ?

Where did that come from ? Do I misunderstand fillna , and must specify it that it actually should really replace NaN , instead of just pretending to ?

Post raw data and current code , it sounds like you want to filter the 2 dfs separately and merge them on gene only if the condition is satisfied in both dfs

You should be able to ` concat ` , in which case before the call to ` reset_index ` , set the index to ' Code ' for your other data and then call ` pd.concat ([ df , other_df ]) .reset_index() ` this should stack them on top each other , hope this is clear
You could also do it with a merge .

My question is , how can I perform a merge on these two rows taking into account that sometimes there will be a comma separated value ( like an array ) in the Performance DF that needs to have find the two corresponding revenue rows from the Revenue DF together - and the date .
That way you can merge and you're basically there : #CODE

It looks like you want us to write some code for you . While many users are willing to produce code for a coder in distress , they usually only help when the poster has already tried to solve the problem on their own . A good way to demonstrate this effort is to include the code you've written so far , example input ( if there is any ) , the expected output , and the output you actually get ( console output , stack traces , compiler errors - whatever is applicable ) . The more detail you provide , the more answers you are likely to receive .
Thing is , Stack Overflow is not a coding service . We can help people solve problems they are facing * as they themselves program* . You appear to think that Stack Overflow is here to write code for you , but you have underspecified this and haven't shown any efforts to solve this yourself so we can help you fix that code .

open all csv in a folder one by one and append them to a dictionary or pandas dataframe
You should use a list and ` concat ` : #CODE

Thanks shx2 ! That meeds my goal , but turns out : ` assert ( df.column.diff() [ 1 :] <= 0 ) .all() ` ( first value of diff is a NaN ) is slower than just sorting with a mergesort . Also , is_monotonic() doesn't tolerate weakly monotonic columns , so doesn't work in my case . :/ But maybe on a bigger dataset diff would payoff .

But I can't find how to apply this to multiple columns
An alternative is to apply : #CODE

Note : it may be a more efficient to do this while reading in as your data ( i.e. during construction / concat ) ...

Pandas eval with multi-index dataframes
I would like to use ` eval ` to substract ` ( ' bar ' , ' one ')` from ` ( ' flux ' , six ')` . Does the eval syntax support this type of index ?
You can do this without using ` eval ` by using the equivalent standard Python notation : #CODE

Consider the following solution to computing a within-group diff in Pandas : #CODE
Thanks @USER . It's interesting that you sort entries within apply ( e.g. as opposed to sorting them * before * running groupby and apply ) . Is this because ` groupby ` is not guaranteed to preserve the original ordering ?
Also , looking at [ this answer ] ( #URL ) from Jeff , I see that he applies ` transform ( Series.diff )` instead of just ` diff ` as in your code . Do you know when to use one vs the other for within-group differencing ?
@USER -Reina in situations like this ( when the function doesn't " reduce ") then transform and apply are the same . In retrospect , I think that sorting globally may be faster ... I mistakenly thought that was the issue causing the most slow down . I think I have a better solution .
Add the diff column : #CODE

How to pass keyword arguments in reduce over Pandas merge function
What I want to do is to perform outer merge so that it produce the following result : #CODE
One way is to use ` functools.partial ` to partially apply the merge function . #CODE
It reveals some inconsistencies in what you say about your desired result . You can see that there are actually two locations where the outer merge must supply a missing value , not just one .

How to resample / downsample an irregular timestamp list ?

Hi , thanks , but I don't think this answers my question though . Those methods don't create the rank over a window . I have read through the documentation which led me to ` rolling_apply ` . However this appears to simply apply the function a fresh to each window and over a large dataset in can take a long time to iteratively apply that function . Using the pandas roll function was far to slow . argsort was faster , closer to the bottleneck method above , but I still believe it shouldn't be to difficult to implement a much more efficient way , either by using online windows , or a method I am missing ?

Pandas tries to align the index of ` df_ ` with the index of a sub-DataFrame of

You can use the ` pivot ` method specifying which columns should be used as the index , as the column names , and as the values : #CODE

Most return a copy and I get a SettingWithCopyWarning warning . The loc function says the index is incompatible with datetime . Is this something I should be able to do . The result I would like after updating the slice is : #CODE
One way is to use ` loc ` and wrap your conditions in parentheses and use the bitwise oerator ` ` , the bitwise operator is required as you are comparing an array of values and not a single value , the parentheses are required due to operator precedence . We can then use this to perform label selection using ` loc ` and set the ' C ' column like so : #CODE
You can set ` sdf ` to the mask and use this with ` loc ` to set your ' C ' column : #CODE
@USER I've updated my answer basically you just set ` sdf ` to the boolean mask and use this with ' loc '
Thanks again @USER . I don't want a copy , I want a view , but I decided to take a copy as you suggested and then do ` df = df.combine_first ( sdf )` to merge them back together . This seems to have the desired results whilst letting me perform a number of calculations based on the data in the smaller view . Your answer also does solve my question so happy to accept . Thanks again .

I'm trying to replace data in one ` DataFrame ` columns with data from another ` DataFrame ` based on some rule . Please consider following example : #CODE
Replace matched part in ` a.a1 ` with the value of ` b.b2 ` .
Replace entire content of ` a.a2 ` with the value from ` b.b3 ` .

You can combine the 0th and 1st using nth and then concat these : #CODE

Apply will only return more rows than it gets with a groupby , so we're going to use groupby artificially ( i.e. groupby a column of unique values , so each group is one line ) . #CODE
The way I did it was split the list into seperate columns , and then ` melt ` ed it to put each timestamp in a separate row . #CODE
Finally , use ` melt ` to get it into the shape you want : #CODE
If you want to stay in pure pandas you can throw in a tricky ` groupby ` and ` apply ` which ends up boiling down to a one liner if you don't count the column rename . #CODE
We want the date to become the single index for the new rows so we use ` groupby ` which puts the desired row value into an index . Then inside that operation I want to split only this list for this date which is what ` apply ` will do for us .
I'm passing ` apply ` a pandas ` Series ` which consists of a single list but I can access that list via a ` .values [ 0 ]` which pushes the sole row of the ` Series ` to an array with a single entry .
Once this is passed back out I have a multi-index but I can force this into the row format we desire by ` reset_index ` . Then we simply drop the unwanted index .
Speed wise this tends to be pretty good and since it relies on ` apply ` any parallelization tricks that work with ` apply ` work here .

This might seem elementary to experienced users but I was not super clear on this and it was surprisingly unintuitive to search for on stack overflow / google . Some thorough searching yielded this ( Assignment of qcut as new column ) but it didn't quite answer my question because it didn't take the last step and put everything into bins ( i.e. 1 , 2 ,... ) .

Use Stack .

You can use ` ix ` to set multiple columns at the same time without problem : #CODE

You can drop the .keys() , in python 2 this creates a list unnecessarily ( which has to then be converted into a set ) .

Pandas DataFrame : append give index by name to end of df
So you want to prepend those rows and then shift them to end of the df ? Also is this your index rather than your column ?
As it's currently your index , it's probably simpler to reset the index to restore it as a column , then filter the rows and prepend the text , use ` concat ` on the filtered rows to reorder them and then set the index back again : #CODE

Stack Trace is as follows : #CODE

( 3 ) save the header columns for concat later #CODE
( 5 ) output : concat [ header data ] . write output #CODE

Pandas : Map a function using multiple columns
I've checked out map , apply , mapapply , and combine , but can't seem to find a simple way of doing the following :
I want to apply this and create a new column in the dataframe with the result . #CODE

In pandas it would be done by performing a ` groupby ` and then calling ` min() ` on the 1st column , here my df has column names ` 0 ` and ` 1 ` , I then call ` reset_index ` to restore the grouped index back as a column , as the ordering is now a bit messed up I use ` ix ` and ' fancy indexing ' to get the order you desire : #CODE

dtype : integer , but loc returns float
Do you have to use ` loc ` ? What about this : #CODE
You get back a float because each row contains a mix of ` float ` and ` int ` types . Upon selecting a row index with ` loc ` , integers are cast to floats : #CODE
To get around this and return an integer , you could use ` loc ` to select from just the ` age ` column and not the whole DataFrame : #CODE

You can see that the array is masked and that some of the first few rows show examples of ` -- ` in there . So I drop the last field ( ` refGage `) and it works , so I think it's masked values which only appear in that field . #CODE
The ` fill_value ` for each field in ` segArrNew ` is applied in making the output DataFrame . I used df.ix() to replace the filled-in tokens for what was masked out .

` isinstance ` , ` len ` and ` getattr ` are just the built-in functions . There are a huge number of calls to the ` isinstance() ` function here ; it is not that the call itself takes a lot of time , but the function was used 834472 times .
what is isinstance actually doing ? len and getattr have pretty straightforward names , but not clear what " isinstance " is doing , so having trouble figuring out why / where it's being called by pandas .
@USER you not allowing pandas to do anything with your UDF here . You are doing way too much in the apply of the groupby . I am not exactly sure what you are trying to achieve , but using try except blocks , loc , and mutation of the passed in data INSIDE OF A GROUPBY is quite inefficient . A combination of filter and / or indexing will achieve what you want in a much more efficient way . Pls read the docs #URL and possibly provide a self-reproducing example ( in a new question ) if you still have concerns .

so you mean , take my dataframe's columns EF , get a copy dataframe from .astype ( uint64 ) , and then maybe replace EF by the copy , and then to_csv() that

Pandas to_html() : Apply CSS style to < td > tag

Easy way to apply transformation from ` pandas.get_dummies ` to new data ?

What is the easiest way to insert rows ( days ) in the gaps ? Also is there a way to control what is inserted in the columns as data ! Say 0 OR copy the prev day info OR to fill sliding increasing / decreasing values in the range from prev-date toward next-date data-values .
and then resample ... the reason being that importing the .csv with header-col-name Date , is not actually creating date-time-index , but Frozen-list whatever that means .
You'll could resample by day e.g. using mean if there are multiple entries per day : #CODE
You can then ` ffill ` to replace NaNs with the previous days result .
Now you've updated with ohlc , I'm not sure if you can how= ' ohlc ' on an already ohlc DataFrame ( there is an issue about that though ) , you can resample each column individually ( i.e. high with how= ' max ' , low with how= ' min ' , rate with how= ' mean ') . That's if you have multiple entries for a day , if you don't its all good .

If I try the groupby function it seems to output some odd new DataFrame structure with bool in it , and all possible years / dico combinations - my objective is rather to have that simpler new sliced and smaller dataframe I showed above .
I.e. , I want to filter out all bool columns that are False .
` loc ` lets you subset a dateframe by index labels : #CODE

I'm looking for a function something like this fabricated one ( ` pd.get_datetime_limits ( rule , dt )` : #CODE
Then apply the rule to the datatime index , and to a datetime object using ' to_period ` for filtering : #CODE

I think the ` read_csv ` argument ` na_values ` might * replace * missing values with what you choose , and not guarantee that that string is going to be * interpreted * as NaN . Maybe you could try looking into providing a ` converter ` function for that particular column that deals with L cke and missing values in a controlled way explicitly ?
And you workaround works with your testdata , but when I try in on my real data , It ignores the ` L cke ` . It seems like it is not an encoding problem , since when I replace ` L cke ` with ` NaN ` in the original file by hand , it still causes the same error . ( see edits in my question )
If 2 above is a problem , fixing it is difficult because there is no option to ` read_csv() ` to strip trailing space . You can supply ` strip() ` as a function in the ` converter ` dict , but this does not affect the processing of matching ` na_values ` . Perhaps you could remove the whitespace when you pre-process the file with sed .

I've found a solution in R language here , but it's difficult for me to translate it to Python / Pandas code .

Merge the values of two columns in a Pandas Dataframe applying a function to deduplicate and concatenate
What I want as an end result is a merge operation with some logic to clean up separators ( # ) and deduplicate values : #CODE

Pandas on Apply passing wrong value

Can you post some code that product dummy data , and your current code to timeit . I think you can concat all the dataframes , the result dataframe will has MultiIndex . Then you can dot the calculation without for-loop .
Will try to do tonight ( ET ) . I have used concat on dictionaries before .

Apply function to Dataframe GroupBy Object and return dataframe
How do I return a dataframe from a groupby object using Pandas ? The intent here is to read in a CSV and replace each IP address in the IP address column with a value returned by randIP() . I'd like to do this by grouping to maintain consistency throughout the obfuscated dataframe afterwards ( each real IP maps to a new random IP ) .
That's correct . I have a column of just IP addresses that I would like to replace in a group-wise fashion and return the modified ( full ) data frame at the end .
It is better not to try to modify the DataFrame from inside the grouping operation . Instead , use the grouping operation to create the new IPs , then use ` map ` to map the old IPs to the new ones , and then ( if you want ) assign the new ones back into the DataFrame : #CODE
Using ` groupby ` with ` apply ` will give you a Series mapping old to new IPs : #CODE
Using ` map ` on the old IP column and passing this new Series will map old IPs to new IPs : #CODE

then unstack and fill : #CODE

Drop rows if value in a specific column is not an integer in pandas dataframe
If I have a dataframe and want to drop any rows where the value in one column is not an integer how would I do this ?
The alternative is to drop rows if value is not within a range 0-2 but since I am not sure how to do either of them I was hoping someonelse might .

Join multiple grouped dataframes by key in Pandas
I performed the operations I wanted one-by-one . They give me the result I want . So to finish I wanted to join the grouped dataframes by the same key I used to group them . Now I'm getting a " *** KeyError : ' key '" because the grouped dataframes doesn't have a " key " column that would allow me to perform the merge operation .
I'm giving my first steps on Pandas . Is there a way to keep the key column in the groupby so I can perform the merge or a easier way to achieve the same result shown in the desired output ? #CODE

Use pandas asfreq / resample to get the end of the period

@USER you'd have to resample the index to hours or minutes and then pass 12 as the second param to ` np.split `
@USER so basically you'd have to create a new index and set the ` freq ` to ` h ` or ` m ` to get an index that is hourly or as minute intervals e.g. ` pd.date_range ( start = startdate , end = enddate , freq= ' h ')` , however why do you need to do this ? typically you resample data to a different frequency rather than a datetimeindex
In that case you just resample the data and split like I've shown and plot the splits , the other thing you can use is the index method index_slicer but I've never used it so can't advise

Can you show how you were using pd.to_datetime ? That should work fine . If for some reason it won't work , you can apply a strptime function to the str series . #URL

Something like df1 = df [[ ' movie id ' , rating ']] .groupby ( ' movie id ') .agg ( np.mean ) then merge it back in pd.merge ( df , df1 , on= ' movie id ' , how= ' left ')
If all the values are in the columns you are aggregating over are the same for each group then you can avoid the join by putting them into the group .
Note ` len ` is used to count

How would I apply it on a specific slice of the data frame , e.g. I want to run the script for each row , but only using columns 6 through 12 ?

Nice ! To match OP's desired output , you could then ` stack ` , ` reset_index ` on the names and then remove rows where the two names are the same .

for any value p in x.e , and any value u in y.e , abs ( p - u ) 100

BigQuery supports partial success on batched insertions . From the reply , it looks like row 90 failed to insert with reason " timeout " . See " Success HTTP response codes " at #URL for a description of the response .
If you are using an insert id you can simply retry the failed rows , or retry the full request if desired ( though each retried row will count against your table quota ) . Retries should follow an exponential back-off pattern .

Secondly , ` vols [ date ( 2015 , 2 , 12 )]` is actually looking in your DataFrame's column headings , not the index . You can use ` loc ` to fetch row index labels instead . For example you could write ` vols.loc [ ' 2015-02-12 ']`

Pandas DataFrame - how to merge groupby calculations back into DataFrame ?
In pandas you have similar tools . see ` pd.merge ` , ` pd.groupby ` . You just have to break the sql statement down to ` selection ` and ` join ` and ` grouping ` steps and do them manually one after another .
yes , i did a pd.merge and groupby , but the output is a dataframe of groupby keys and the computed value only . but if you look at my sql statement , i have a value tableA.B column that i need to be in the result , i cant find a way to include it . tableA.B column is correlated to tableA.A , but cannot be used in join because tableB doesnt have that value
I really don't understand your problem . You have the ` groupby ` key . So you can use it in an inner join to get the rest of the attributes .

So column ` 0 ` has 2 non-NaNs . Keep that and drop ` 1 ` . #CODE
Read it inside out . ` .isnull ` gives a DataFrame of Bools , flip the Trues / Falses with the ` ~ ` , sum that to get the count of non-null per column , select out the columns where the sum is less than 2 ( column 1 ) and drop that . /

It says ` WITH ` is not supported in conjunction with ` INSERT ` , ` DELETE ` and ` UPDATE ` . See the docs for ` WITH ` under ` SELECT ` : #URL " A ` WITH ` clause is an optional clause that precedes the ` SELECT ` list in a query . "

Both of these methods are 10 times more efficient than the previous one , iterate on rows which is good and work perfectly on my " debug " table ` df ` . But , when I apply it to my " test " table of 18k x 40k , it leads to a ` MemoryError : ` ( I have 60% of my 32GB of RAM occupied after reading the corresponding csv file ) .
You are doing some odd things : ` fimpute_geno = pd.DataFrame ( { ' SampID ' : geno_reader [ ' SampID '] } )` why create a a new df ? could you not just do ` fimpute_geno = geno_reader [ ' SampID ']` ? or do you need a copy ? The reason that the dtype is a float is probably because you have missing data ` NaN ` which cannot be represented as an int , you need to decide whether to replace these values or drop the rows , also what are you trying to achieve , it looks like you are trying to build a string representation of your data

Can you help me understand how to reference the column name in the ` dt =p arseDeviceType ( user_agent_string )` call ? I'd like to also know how to reference it by column number if that is possible in a call to a function .
Should ` user_agent_string ` be a string ? Like : ` dt =p arseDeviceType ( ' user_agent_string ')` .

Or you can slice the columns and pass this to ` drop ` : #CODE

This must be an issue with the timezone . When I replace take off the timezones with ` dates = pd.date_range ( ' 2014-11-01T00 : 00:00 ' , periods=100 , freq= ' D ')` , ` events.plot() ` generates a beautiful graph with no issue .

Simply replace ` fun ` with ` computeErrorForLineGivenPoints ` : #CODE

I don't understand your question if you did this : ` df.groupby ( ' A ') [ ' B ']` then you explicitly select just column ' B ' and then you can still apply your functions to this only

I am writing a script that reads a csv file and uses the pandas library to create a pivot table .

How do i replace ONLY the NaNs with the contents from second csv file without changing any original values . Is there a easy solution with pandas . #CODE

interesting pandas loc behavior
I've attached a screenshot of a pd.DataFrame I am using and I've observed some interesting behavior within loc that to me is counter-intuitive / doesn't make much sense to me , as after reading the pandas API I would have thought are equivalent ( at just for example quicker than loc ) .

pandas : merge / combine / update two dataframes
I need merge or combine these two dataframes and use column ' id ' as a key .

If I apply an operation to the Value column I would then like to recalculate the groupby operation : #CODE
I think the calculated groupby MultiIndex should be re-usable to re-calculate the new agg functions ( np.sum is an example ) , but I can't work out how to apply it . How would one most efficiently reuse a groupby method on a dataframe of the same shape and columns structure multiple times ?
True and I often do , but in this instance I want to be able to use the groupby data / construct to apply the operation on a fresh data frame . In actual fact I would want to save the groupby to a hdf and reload it in separate threads , but it doesn't seem possible to save a groupby object to a file .

Pandas speedup apply on max()

From this I'd like to be able to extract the stats I want and in the order I want e.g. simply display mean , std and median . I get close to this with the following line : #CODE

You will have to create a separate dataframe for each file and then join or merge all the dataframes .

Subsetting DataFrame using ix in Python

I got some fantasy football data and I'm trying to sort it out so I can later apply on it , the full force of scikit-learn .
Here's solution for you test data , I think you can easily apply it to your real data #CODE

What I did is a groupby to identify where the sub-tables should be cut : #CODE

I have a dataframe full of dates and I would like to select all dates where the month == 12 and the day == 25 and add replace the zero in the ` xmas ` column with a 1 .
Basically what you tried won't work as you need to use the ` ` to compare arrays , additionally you need to use parentheses due to operator precedence . On top of this you should use ` loc ` to perform the indexing : #CODE

You don't need to use ` where ` . Just use ` isin ` and apply your condition directly to the columns : #CODE

Ah , in that case you want ` iloc ` ( and not ` loc `) - I've edited .

I want to ignore the hours minutes and seconds and join these two ` data frames ` . No error is thrown but NaN values will appear for columns from ` df2 ` on ` 2004-07-08 20:00 : 00 ` . How can I join these two data frames ?
I think the simplest solution is to reformat the datetime columns of one or both of your dataframes . Perhaps you could remove the time from your first dataframe , assuming the time is not important to you . Here's a post which uses ` normalize ` to achieve this .
Awesome didn't know about ` normalize ` . the solution is simple , ` df.index = df.index.normalize() `

How can I normalize the data in a range of columns in my pandas dataframe
I want to normalize the data in each column by performing : #CODE
This would work fine if my data table only contained the columns I wanted to normalize . However , I have some columns containing string data preceding like : #CODE
I only want to normalize the Age , Income , and Height columns but my above method does not work becuase of the string data in the name state and gender columns .
This will apply it to only the columns you desire and assign the result back to those columns . Alternatively you could set them to new , normalized columns and keep the originals if you want .

You can take the min , max , and mean then use pd.concat to stitch everything together . You'll need to transpose ( T ) then transpose back to get the dataframe to concat the way you want . #CODE

Otherwise you can call ` apply ` like so : #CODE
In the case where the above won't work as it can't generate a Series to align with you df you can apply row-wise passing ` axis=1 ` to the df : #CODE

I tired append function and join but it dint work .. Do we have any specific function this in pandas dataframe ?

pandas apply over a single column
how can I apply this function over a single column of pandas ? In pandas documentation , the structure of the function is given as
but I don't see any ability to apply it to a column of the dataframe .
1 . I'd just convert the columns to datetime and then access the date attribute , so ` df [ ' time '] = pd.to_datetime ( df [ ' time ']) .date ` 2 . You can apply to a series also but if you want to use the axis param then you can force a df with a single column using double sqaure brackets : ` df [[ col_name ]] .apply ( func , axis=0 )` 3 . to check if an element is nan you can use the top-level ` isnull ` method so ` pd.isnull ( x )` will return True or False
It's unclear your 3rd question , are you asking how to replace nan values ? You can use ` fillna ` or ` replace ` to do that . for question 4 . you can do ` df [ ' col '] = df [ ' col '] .astype ( str )` to convert the column to str
You have to decide what to with the nan values , you can either drop them or convert them pre or post conversion , it's not that difficult , also generally on SO it should be 1 question per post

map directly to c-types [ inferred_type- mixed , key- block0_values ] [ items- [ ' operation ' , ' snl_datasource_period ' , ' ticker ' , ' cusip ' , ' end_fisca_perio_date ' , ' fiscal_period ' , ' finan_repor_curre_code ' , ' earni_relea_date ' , ' finan_perio_begin_on ']]`

Python / Pandas : How to quickly pivot datetimeindex on date and time ?
How could I pivot the timeseries so that the index is composed of individual dates and the columns are times ?

I am ultimately trying to merge two dataframes together , but I am running into an issue when I try to specify the column on which they should be merged .

I've been looking around at solutions here : Mass string replace in python ?
If you can write this as a function that takes in a 1d array ( list , numpy array etc ... ) , you can use df.apply to apply it to any column , using ` df.apply() ` .
Because you need to replace letters one at a time , this doesn't sound like a good problem to solve with pandas , since pandas is about doing everything at once ( vectorized operations ) . I would dump out your DataFrame into a plain old list and use list operations : #CODE
Instead of printing the words , you can append them to a list and re-create a DataFrame if that's what you want to end up with .
We have to do those actions for each word ( I assume , the reason is clear ) . We also append the world directly to our just created output-list ( ` newwords `) . #CODE
` c ` as short for ` character ` . So for each character we want to replace ( all keys of ` md `) , we do the following stuff . #CODE
Additional question from OP : Is there an easy way to dictate how many letters in the string I want to replace [( meaning e.g. multiple at a time )] ?
I added ` multipleeee ` to demonstrate , how the replacement works : For ` num = 2 ` it means the first two occurences are replaced , after them , the next two . So there is no intersection of the replaced parts . If you would want to have something like ` [ ' multiplqqee ' , ' multipleqqe ' , ' multipleeqq ']` , you would have to store the position of the " first " occurence of ` char ` . You can then restore ` pos ` to that position in the ` if multiples == num : ` -block .
Hey Dave , solution works ! Would love an explanation of how though :) Also , is there an easy way to dictate how many letters in the string I want to replace ( e.g. instead of one at a time , two at a time , etc )

No , the code runs but places the same value in every row of the two columns . It must record the first or last result and insert it into each row .
Pandas already knows that it must apply the equation to every row and return each value to its proper index . I didn't realize it would be this easy and was looking for more explicit code .

pandas equivalent of R's cbind ( concatenate / stack vectors vertically )

This was the code snippet to read it back ( read it one-column at a time and doing a ` concat ` operation ): #CODE

Now , within each group defined using ` ID ` I want to compare each record with the reference record and I want to compute the number of unique ` A ` and ` B ` values for the combination . For instance , I can compute the value for data record 3 by taking ` len ( set ([ 4 , 4 , 6 , 12 ]))` which gives 3 . The result should look like this : #CODE

join series back to dataframe : #CODE

Usually some combination of ` concat ` or ` merge ` or ` join ` , but you're going to have to let us know what the result should look like .

define a function to replace letters where value 5 with other : #CODE
apply the function to the dataframe : #CODE

I am trying to transpose the following data set #CODE
It has several more years across the top and and about 100 different series names for each country . I would like to transpose the data set so that the years are a column and each seriesname is its own column .

The obvious solution would be convert the column into ` datetime ` which should map to the excel timestamp ( ` 16.02.2015 00:00 : 00 ` -> ` 42051 `) .

I would like to create a function that does this for one imo , and then I can apply it to all of them but I am unfortunately stuck .

I would like to replace the strange characters with ' 0.00 ' but I get an error - #CODE
the code I use to replace the characters : #CODE

Apply a function data frame column

Yes it looks like it . However the input is ` file ` , ` filename ` or ` content ` . I'm not savvy enough with Python to see how I need to map my existing array of texts with this class . My original data is dataframe indexed by ' timestamp ' . How would I pass ' just the texts ' ?
covert the sparse matrix returned by ` CountVectoriser ` to a dense matrix , and pass it and the feature names to the ` dataframe ` constructor . Then transpose the frame and sum along ` axis=1 ` to get the total per word : #CODE
If all you are interested in is the frequency distribution of the words consider using ` Freq Dist ` from ` NLTK ` : #CODE

open a ` .csv ` file on the local computer , append the line of 8 elements , close the file .
So , each price message arrives at your network interface , has to get through the TCP stack ( presumably ) before it hits your CPU . UDP is quicker than TCP but if you're using QuickFIX you'll be using TCP .

You can concat the columns , then convert to a list using numpy's ` tolist() ` : #CODE

Add calculated column to a pandas pivot table doesnt help me

Find where the adjacent difference is nonzero : #CODE

like Bob said , rolling_mean of the diff , and I'd spend some time with the window size for rolling_mean while deciding what I meant by " rapidly " .
But note ! my test data was evenly sampled . Looks like ` rolling_* ` don't apply to irregular time series yet , though there are some workarounds : Pandas : rolling mean by time interval

Pandas resample time serie in equal parts
I am trying to resample a pandas time serie in N equal parts .
How can I resample in N equal parts , N being > 10
Solved . The resample method should use both fill_method= ' pad ' and closed= ' right ' #CODE

This code works for me ( I didn't know you could pass a dict to resample so thanks ) . Can you explain what you mean by ' first and last part ' and provide a sample of the data ?

I kindof disagree with using df as the variable name here , I also think I'd just use len : ` df.groupby ( " Name ") .filter ( lambda x : len ( x ) > 2 )`

should I declare a new list and add header to it and append it ?? res= [ ] firstRow = first.find_all ([ ' td ' , ' th '])

Reshaping & Selecting Pandas from Pivot
I would like to pivot it to show the following format : #CODE

A simple method would be just to use two ` loc ` calls and filter on gender : #CODE
A better way would be to use ` transform ` on the groupby object which will align the returned data : #CODE

I want to loop through and apply a function to the dataframes within ` groups ` that have more than one row in them . My code is below , here each dataframe is the ` value ` in the key , value pair : #CODE
Since I am only interested in applying the function to values where ` len ( value ) 1 ` , is it possible to save time by embedding this condition to filter and loop through only the key-value pairs that satisfy this condition . I can do something like below to ascertain the size of each ` value ` but I am not sure how to marry this aggreagation with the original ` groups ` object . #CODE

As it stands , I have only been able to get it working by bludgeoning the columns of high / low bounds into an Mx2xN numpy array using the rather brutal tools of newaxis and append . This is fraught with error , and unsightly in syntax , so my question is : is there a better way to do what I'm trying to do without having to hammer a DataFrame into an array as I have done here ?

Then you can apply the following logic . #CODE
If you truly want to drop sections of the dataframe , you can do the following : #CODE

Refer to this answer Pandas Pivot tables row subtotals

Brian this works great ! Just two quick questions : What if I want to append to an existing excel file instead of writing to an entirely new file ? Also , what if I just want to write the IndepFlag column , and a column containing the row id into the existing excel file ? Thanks !
To only write the index and IndepFlag , use ` flagged.IndepFlag.to_excel ( " flagged.xlsx ")` . Not sure how to append to an existing excel file : one approach would be to read the existing file into a dataframe , concat the new dataframe using ` pd.concat ` , and write to a new excel file . Or maybe this post can help : #URL

Trying to solve another problem I figured out that one can use the index of a selected part of a dataframe in ` drop ` : #CODE
You have to use loc , iloc or ix when doing more complicated slicing : #CODE
ohhk . I would have to use ` iloc ` ... what is the difference to ` loc ` ? However , if I index with ` df.iloc [ mask , :] ` then I get a completely mixed up " frame " index ( swapping between 0 and 1 ) + the data seems to be strange as well .
@USER .R . hmmm unclear what your seeing , in my examples it works fine . What version of pandas are you using ? Check out the docs I link to for the difference between loc and iloc .

I want to have the dictionary key be in each row for every item in the list its associated with . I've tried making two series and concat them but it didn't work . I must be over thinking this . Thanks in advance for any insight .

This uses ` loc ` to perform label indexing and then 2 conditions which use ` ` as we are comparing arrays and with parentheses becuase of operator precedence .
Yes that is correct , you need to index using ` loc ` , ` iloc ` or ` ix ` to ensure that the assignment is happening on a view , see the docs : #URL

if I add try to append it using the line data [ ' Stars '] .append ( star ) - I get the following error --
What should be done to append it and the rows that dnt have a star should have NA in it .
yes . data frame only takes list of same length . I was trying to declare a list of default value NaN then add contents to others and append . Thanks for looking into the problem .
Your original issue was caused because your individual loops did not contain the same amount of each element you were looping through ( i.e. - 15 stars , v . 20 prices ) . The best way to avoid this type of issue is to firstly have one loop and then to apply a try except value to each item you're scaping . That way if there are any issues with the constant presence of the items you want , you can still collect what is present . #CODE

I'm trying to take an existing data frame and append a new column .
I'm doing this with a 15-element vector , over ~20k rows , and I'm finding that this is super duper slow ( half an hour ) . I suspect that using iterrows and ix is inefficient , but not sure how to correct this .
Is there a way that I can apply this to the entire dataframe at once , rather than looping through rows ? Or other suggestions to speed this up ?

Now , once you insert the above dataframe into python the ordering of the columns gets out of wack . For some reason pandas groups the Price columns together and the Size columns together . This is not the intention . The dataframe should look exactly as I show it . I'm not sure how you can manipulate it back to the way its shown above .

1 ) Copy the dataframe and directly insert it into ipython WITHOUT using read_clipboad() . Call the dataframe df .
4 ) Select what was pasted in #3 and copy to clip board using Control C

python pandas dataframe join two dataframes
I am trying to join to data frames . They look like this #CODE
Other than your ID column , wharf you have should work . What is your merge giving you ?

How to apply rolling functions in a group by object in pandas

What do you want to use that for ? The dtypes really are ` datetime64 [ ns ]` and ` float64 ` . Numpy datatypes are different from builtin datatimes like plain ` float ` , so you'd have to give more information about how you want to map between them . You can get a nicer representation of the datetime type by using ` str ` on it to get ` datetime64 [ ns ]` , though .

Accessing uninform < dt > < / dt > < dd > < / dd > tags
I want to access all the elements but as I populate the table .. since the dl and dt are not uniform in the order they are place in I get actor name in the running time and at times actor name in format .

Pasted in your question , is that the full stack trace , too ?
@USER Yes , it is the full stack . And in my fonts directory under ` / usr / share / matplotlib / mpl-data / ` there are three files : afm pdfcorefonts ttf

Pandas unstack problems : ValueError : Index contains duplicate entries , cannot reshape
I am trying to unstack a multi-index with pandas and I am keep getting : #CODE
Then I try to unstack the location : #CODE
Are there any NaNs in your data ? There was [ an unstack bug related to those ] ( #URL ) .
Another option ( if you don't want to aggregate ) is to append a dummy level , unstack it , then drop the dummy level ...

Apply rolling mean function on data frames with duplicated indices in pandas
Didn't realize the usage of ' freq ' until now , thank you ! it won't work on data frames with irregular space which is the case of my actual data frames .

If you have some object named ` x ` you can inspect what type it is with ` type ( x )` and go from there . If you have a ` list ` , why not write your processing logic as a separate function that takes a single group as an argument , and then use ` map ` to spray it across the list of groups ? If you store your data as a ` pandas ` ` DataFrame ` then you can set the column of ` A ` / ` B ` values as the index , or group by that column ( which will be a bit different than the contiguous-runs grouping logic above , which is why I opted not to use ` pandas ` in my answer ) ... there are many possibilities .

` DataFrame ` has its own ` hist ` method : #CODE
I don't believe ' hist ' was a supported type in 0.14.1 . Try df.hist() instead

pandas uses ` NaN ` . You can test for null values using ` isnull() ` or ` not null() ` , drop them from a data frame using ` dropna() ` etc . The equivalent for ` datetime ` objects is ` NaT `

You can check this question and its answer to see how to apply a function row by row : #URL Otherwise , in order to fully answer the question i.e. be able to have all desired fields , we need to know exactly what's in the data ( not just one line ) . Could you for instance split on " two blanks or more " ( typically no if there are missing values ) ... or do columns have the same position in the string in each row etc .

It will actually use less memory . Native dtypes such as ` np.float ` and ` np.int ` require less space than Python ` float ` s and ` ints ` . And Python ` dict ` s require quite a few bytes as well . A Pandas DataFrame that contains a list of dicts uses an ` object ` dtype to store arbitrary Python objects . If you replace that list of dicts with columns with native NumPy / Pandas dtypes , then you will definitely save space .
Thanks a lot . I really appreciate the advice . I had a thought that I will try soon : To help the memory issue , I'll create a small dataframe that unwinds the nested column ( per your code ) , then merge that on ID with the main dataframe . That way , there is no duplicate ' df , ' just an additional dataframe comprised of ID and rank info .

I am working on data analytic's and wanted to use pandas for the same , i have dataset of size ( appox .. 50million rows ) , i was successful in using pandas from command prompt , wrote scripts / programs to load data into dataframes , used pandas functions such as where , joins , merge etc ..

Python Pandas : Apply function to dataframe in place
Although apply doesn't offer an inplace , you could do something like the following ( which I would argue was more explicit anyway ): #CODE

Pandas : Resample Data Frame with two time columns
How do I manage to resample this dataset to an hourly resolution so that both time formats are resampled and none of them is ` NaN ` ? I tried to read it with one index column as well as with both of them . However , doing the resample with ` df.resample ( ' H ')` in the latter case yields a ` TypeError ` .

drop rows that have duplicated indices
I have a DataFrame where each observation is identified by an ` index ` . However , for some indices the DF contains several observations . One of them has the most updated data . I would like to drop the outdated duplicated rows based on values from some of the columns .
For example , in the following DataFrame , how can I drop the first and third rows with ` index = 122 ` ? #CODE
Based on your new information , the easiest maybe to replace the outdated data with ` NaN ` values and drop these : #CODE
Sorry I made a typo in my question . I want to drop the row that have outdated data . In the example , I want to drop the row with ` col1 = - `
thanks for your edition . But it will drop all rows with ` - ` values . Instead I only need to drop those that have duplicated ` index ` . See my edited example , ` index = 124 ` still needs to be kept even though it has ` - ` values .
Thanks . But this will drop any ` index ` that have ` col1 ` equals to ` - ` . Instead , I only want to drop rows with duplicated indices . Another problem with ` transform ` is it tends to be really slow with large number of observations .
@USER . Thanks for the answer . I accepted it . Just one comment : ` transform ` takes forever for large data set . One work around is to create another reduced dataset of index count and then join it to ` df ` to get the ` count ` column .

When I combine with concat #CODE
Rename the columns after the ` concat ` like so : #CODE

How can I apply a function that transforms it into a dataframe like this : #CODE
you can pivot on the ` date ` and ` time ` components of the index :
pivot : #CODE

Looking at the code for ` geom_boxplot ` it doesn't seem possible to adjust what the axis map to : geom_boxplot.py
Great thanks @USER -e , will use horizontal boxplot for now and have a go at extending the geom_boxplot when I got time . Would upvote your answer , but don't have enough cred

you can use ` map ` and ` strftime ` like this : #CODE

If you were construct a list of data frames and each of those data frames represented one of csv files you could leverage pandas ' concat . A very useful function for combining multiple frames with common headers into one frame

I could use a list comprehension to apply the selected result on the ` get_loc ` function , but perhaps there's some Pandas-built-in function .
Why do you want to get the rows , when you can always say ` pos = df [ ' a '] > =2 ` and then use that later like ` df [ pos ]` ? I guess something like ` np.arange ( len ( df )) [ pos.values ]` is not what you are looking for ?

Thanks , this sounds like it is what I want . One question though is that I still care about another column in the dataframe which contains the ' labels ' ( I do in fact intend to feed this into a regression of sorts ) . Is there a way to have that sparse matrix , but still join it to the labels so I can feed it into an sklearn ML algorithm easily ?

I am accessing a series of Excel files in a for loop . I then read the data in the excel file to a pandas dataframe . I cant figure out how to append these dataframes together to then save the dataframe ( now containing the data from all the files ) as a new Excel file .
This is how I will be doing it with ` pd.concat ` . ` pd.concat ` will merge a list of dataframe into a single big df . #CODE
Great ! Thanks a bunch . If anyone in the future wants to try this just replace the `]` with a non-superscript one :)

5 . Drop NA's

Use ` shift ` to get a series of the dates you want to subtract , filling in your start date at the beginning : #CODE
Then ` concat ` the new Series to your original dataframe . #CODE

I'm not sure of the terminology for this - I did think I could do something with ` cumsum ` and ` diff ` but I think I'm leading myself on a wild goose chase there ...
Your idea with ` cumsum ` and ` diff ` works . It doesn't look too complicated ; not sure if there's an even shorter solution . First , we compute the cumulative sum , operate on that , and then go back ( ` diff ` is kinda sorta the inverse function of ` cumsum `) . #CODE
Following your initial idea of ` cumsum ` and ` diff ` , you could write : #CODE
@USER you just need to append ` .fillna ( 0 )`

Apply permutation matrix to pandas DataFrame
I have two identically sized DataFrames ( call them ` A ` and ` B `) with the same set of index and column names . ` A ` and ` B ` have a different ordering of their ( row / column ) labels and I want them to be identically labeled so I can directly manipulate the matrices in other programs . Mathematically , there is a permutation matrix ` P ` that reshuffles one matrix labels to another , so I can apply this transformation by constructing the matrix . I feel however , that this is overkill and a solution should exist within pandas itself .
You could use ` reindex_like ` to reorder the rows / columns of one DataFrame to conform to another DataFrame . #CODE

Calculate the weights you'll need to apply to achieve your target age / gender distribution : #CODE
Pivot the sample data to get counts of each color , by age and gender : #CODE
Reindex your weights to align with the pivot table index : #CODE
Get the weighted distribution , then normalize it : #CODE

How to drop rows from pandas data frame that contains a particular string in a particular column ?
I have a very large data frame in python and I want to drop all rows that have a particular string inside a particular column .
For example , I want to drop all rows which have the string " XYZ " as a substring in the column C of the data frame .

which would ' cut out ' the weekend and treat Friday's half-day and Sunday's half-day like a single business day .

My first thought was to append the submitting user's email to the group list and then sort the list and then column alphabetically . However , that only works if everyone's group entries are complete .
We can build a dictionary mapping users to groups , merge groups as we go , and remove invalid emails . #CODE

If anyone has experience with this I'd love to see what you wrote . There are examples online for using a .csvreader with python which loops through the rows and adds them as they are being read but I can't find any example of how to take data stored in a dataframe and apply it to a table defined as part of an extract .
I've read all the documentation I'm just not sure how to apply it . There is no clear instruction as to moving data from a dataframe to a virtual table in python and I know this is a problem for other people using different type of target table ( #URL ) . when I try to insert the dataframe to the table as shown in the tutorials , an error returns : " dataframe not callable "
Based on your traceback , the problem is that you are trying to insert a numpy.int64 instead of a " regular " integer .

When I attempt to insert ( overwrite ) a row in the table it seems rather slow , depending on the size of the table I get something like 0.044seconds .
So , why not structure your dataset ( append rows , update records , select columns ) using an indexed , relational , SQL-engine database beforehand , then import into Python data frames for analytical / graphical purposes ? Examples include :
APPEND ROWS #CODE

I then want to apply poisson sampling noise to all data in the abundance frame . #CODE

Pandas Pivot Table - reoganize order of multi-index
I have created a pivot table with a three-level multi-index ( Group , Product , and State ) . The state level is automatically sorted alphabetically , but I need to change the ordering to a custom , non-alphabetical ordering . I may need to also re-order the Group and Product levels in similar fashion . #CODE
A portion of the pivot table looks like this ... #CODE
The method in the post above worked very well for reordering and displaying observations in my DataFrame ; however , when I created a pivot table from the DataFrame , the ordering is changed . The states , which were sorted property in my original DataFrame , are resorted into alphabetical order .
I believe I need a way to specifically re-order the pivot table's multi-index level 2 ( state names ) by providing a list , though I have tried and failed to accomplish this .
Thank you for your reply . I am still unable to prevent the pivot table mutli-index level 2 , which represent the state names , from resorting alphabetically . The DataFrame that the pivot table is based on is reorganized properly by state . Once I create the pivot table , the ordering is changed . It seems that I need to specifically reorder the pivot table multi-index , rather than the DataFrame data it is based on .
What happens when you reset the indexes of pivot ? Does it sort properly ?
I tried pivot.reset_index() but the states are still sorted alphabetically , as they were in the original table . I also tried creating a pivot table using groupby() and unstack() . Still , the states were reordered alphabetically , even though in the original DataFrame , the states were classified as categorical and sorted in the order of the list I provided above .

For a start I'd try to write a reader that takes in 6 lines , splices them together to get the 37 numbers in one line . Then parse that and convert to a list of 37 floats . Finally append it to a master list .
Once I'm done 49 of those , create a ( 49 , 37 ) array , and save it or append it to another list that will hold all the time steps .

I will append True if the row has " Verified page " in it .

you can use melt : #CODE
Melt the frame : #CODE

You have to call ` apply ` and pass the data to ` strftime ` : #CODE

Pandas merge dataframes with different datato achieve specific output
I'm experimenting with pandas , and facing with merge problem , f.e
print df_to merge #CODE
df2 = df.merge ( df_to merge , how= ' outer ' , left_on= " 0 " , right_on=0 ) . I get
merge() will do it . Might need to rename 2 of your columns post merge
You need to do a merge with " outer " mode : #CODE
@USER : Based on your update , it looks like ` df ` has string column names , and ` df_to_merge ` has integer column names . Thus , you were force to use ` left_on= ' 0 '` , ` right_on=0 ` . If you make both dataframes have string column names , e.g. ` df_to_merge.columns = map ( str , df_to_merge.columns )` , then I think ` jeanrjc `' s solution will work .
dear @USER how can I merge this two dataframes and achieve x_val to be in x_val's group ? and not to be in exp's group ( I describe it on your answer )

Iterating a DateTimeIndex in Pandas back and forth with a pivot
Thus allowing me to go back and forth after identifying a pivot row in DF .

Normally we would do ` df.ix [ 2 , ' col2 ' : ' col3 ']` but because your index is ambiguous you get the 2nd rather than the 3rd row as ` 2 ` appears as a value in the index at position 1 so the label selection succeeds as ` ix ` tries label selection first and then position selection .
The simplest thing would be to reset the index and then you can call ` ix ` , we have to drop the index again as this is inserted as a column : #CODE
You could reassign the index that would be less typing e.g. ` df.index = np.arange ( len ( df ))`

@USER . I used k-fold in cross validation that used arr ( len ( arr )) . I wonder if it is correct .

Can you show the output of ` merged.info()` and ` merged1.info()` . Further , do you have NaN values ? Do you have the same problem if you only try to append the first rows of the dataframe ( ` merged.head() .to_sql ( ... )`) ? You can also try to pass ` echo=True ` to ` create_engine ` to get a more verbose output of what is going on .

Sounds like you want [ ` pivot_table `] ( #URL ) . See [ the docs ] ( #URL ) on " reshaping and pivot tables " .

I am running pandas code on several files everyday which are .csv and always just for the one report date ( previous day ) . I download these raw files from our client's website into their designated folders . I then open up ipython notebook and run my pandas modification on each file ( I manually edit the name of the location to match the most recent file ) and append it to my running / consolidated file .
Can I create a code which will run all of my pandas codes daily ? ( example , open latest raw data from file 1 --> modify it as needed --> append it to a consolidate file --> open latest raw data from file 2 --> ... )
pandas has already saved me a ton of time ( I used to manually open up files and paste them below in Excel , then I learned macros and some addins to merge files together ) , but maybe there is more I can learn to automate this further .
There is also two reports which require a login and then there is also a drop down ( we have to select a range of dates and the report type from drop downs and then manually press the Download button .. I have not been able to automate this at all )

Use ` replace ` : #CODE
you can apply a lambda function to the column ` a ` in your dataframe that returns the lowercase of the string contained , if your correction is just making the string lowercase .
the ` apply function ` method can be extended for other more specific replacements .
If you want to replace certain words - #CODE

Rather than passing the dict returned by ` parse_qs ` directly to ` DataFrame.from_dict ` just do a little preprocessing step to search for keys that have list values and replace them with , say , the first value in the list . Or some other scheme if you do want to keep multiple colors .

You can group by all of your columns that you want to remove duplicates from , and use ` FIRST() ` of the others . That is , ` removeDuplicates ([ col1 , col3 ])` would translate to #CODE

I would probably just use map : #CODE
You can use regex , or whatever , provided the function you pass to map takes each item and returns a tuple .

I want to translate the data use by ' date ' , one day in a row .
There is a ` groupby / cumcount / unstack ` trick which converts long-format DataFrames to wide-format DataFrames : #CODE
@USER Sorry , I got a problem again . I tried to expand my data , and use the code you provide to translate the format I want . But I got an error , ValueError : negative dimensions are not allowed . And this is an example CSV file , #URL , can you help again ? Thank you very much .
pd.read_csv ( ... )` . Find the length , ` N = len ( df )` . Now split df into two roughly

Another way , which is faster in some cases -- such as when there are a lot of groups -- is to merge the max values ` m ` into a DataFrame along with the values in ` s ` , and then select rows based on equality between ` m ` and ` s ` : #CODE

How to join two pandas DataFrame when the join condition column has different name
want to join on ( join , Join ) , which represent the same thing with slightly different tokens
how to solve this join cleanly ?
Just to elaborate on Cel's comment try ` df_left.merge ( df_right , left_on= ' join ' , right_on= ' Join ' , how= ' outer ')` see the docs : #URL
Try renaming the columns to the same name - i.e. the second dataframe to ' Join ' to ' join ' .
df2.columns = [ ' c ' , ' d ' , ' join ']
You can then perform your join on ' join ' .

Is pandas / HDF5 the way to go or should I try for other options ? What other options ? Currently feel like I'm trying to cut something with a spoon .
How can I append new clicks while not having duplicates ? I tried with a multiindex and appending , but duplicates ( naturally ? ) do not get overwritten . I could also not find any joining / merging functionality for on-disk data . I also have no idea how to get this to work on batches .
If I index article_id / user_id , How can I insert new article_id / user_ids that have not appeared before ? This will happen frequently with new articles being released and new people appearing on the site . I tried in-memory assignment for a multiindexed table , but I couldn't get this to work .
I searched for a lot of information , but couldn't quite figure out the last two points myself , without going about it in a very convoluted way . For example , when appending a click for a user , query the whole user , if the click is not in the returned set append it and delete the old results . That seems like quite an expensive operation ! Also it does not allow me to assign new values . Deleting every time is expensive , and running ptrepack often should be unnecessary .

` cols ` is depracated , and replace by ` columns ` , and allows you to choose the columns to write .

@USER : dogs-pigs-cats is not a natural ordering , so the sort function won't handle that ( unless you map those values to the ordering you desire ) . You can use the ` ascending ` option to change the order - ` df.sort ([ ' animals ' , ' day '] , ascending =[ False , True ])` , if you want animals to be reverse sorted and day to be incrementally sorted .

Also you can probably generate the new rows by calling ` apply ` which would be much easier to read than what you're doingnow
so I tried replacing the row [ ' J '] and row [ ' K '] with row.loc ( ' J ') and row.loc ( ' K ') but that ended up in some error messages . Am I just going about this all wrong by the for looping through index and row ? Should I just apply a function directly to row.loc ( ' I ') ?
For the email bit we can just use the same regex and call ` findall ` , for the other bit we just pass the func as a param to ` apply : #CODE
@USER OK , I've updated my code , we can use your email regex directly as a param to ` findall ` , for the name bit we can just pass that as the param to ` apply `

So you need to provide a complete path or append the path to where the csv resides to your sys path . Note that backslashes must be escaped e.g. `' c :\ \data\\ my.csv '` but if you use forward slashes then it works fine : e.g. `' c :/ data / my.csv '`

How to prevent from plotting outlier in boxplot in pandas
I have a DataFrame ( called ` result_df `) and want to plot one column with boxplot .

I am trying to apply this function to each row in player_points_position and create a new column ` zscore ` . However , the entire data set is returning the same value . #CODE

This df has a decrease number of cases for each row . After the count drops below a certain threshold , I'd like to drop off the remainder , for example after a 10 case threshold was reached .

Use ` datetime.strftime ` and call ` map ` on the index : #CODE

Then you could convert ` newwords ` into a DataFrame , and use ` pd.merge ` to merge ` newwords ` with ` kw ` by joining on the original word : #CODE

Hi Cel . A similar issue I am facing now again with Notebook . Now when I do shift + Enter it is not printing the result of a command which it was doing earlier .

Cryptic warning pops up when doing pandas assignment with loc and iloc
So as you can see , the statement in my code serves to insert new rows into my DataFrame ` df ` and fill the very last column ( within that newly inserted row ) under ` cycles ` with a ` NaN ` value .
Looking at the Docs , I still don't understand what's the problem or risk that I'm incurring here . I thought that using ` loc ` and ` iloc ` follows the recommendation already ?
Thank you for your suggestion , but I'm not trying to set the entire column to get ` NaN ` . Rather my situation is such that I have missing ` name ` that I need inserted to complete the ` build_number ` and when I insert them in I want to make their corresponding ` cycles ` value at that same row be ` NaN ` . For example , with reference to my df above in my question , I want to insert additional rows of ` 390 jpeg NaN ` and ` 390 blowfish NaN ` to complete the listing of all ` names ` for the given ` build_number 390 ` .
So you just want to append new rows , can you post desired output to your question
Ok this is really funny but now I don't get any errors at all even when I do my own way of double subscripting with ` [ 0 ] [ 0 ]` . This is wierd ... But thank you for your suggestion . Your answer schematic ` df.loc [ i , ' cycles '] = np.nan ` might not work because I can't hardcode ` cycles ` in ; it must be able to be ` fmax ` too . And apart from that , I'm not trying to iterate over each index ; I want to iterate over a list of missing benchmark names and insert each of them into the dataframe .

Yes , that't it , added first condition if index < len ( df1 ) -1 : and error is gone . Thanks @USER !

I've added the code to the append . Dayfirst doesn't work unfortunately .
fair point . It's code I've taken from elsewhere ; you're right , I don't need to create an empty df and the append the new df to it . dayfirst did work , I was applying it to the wrong place . thanks for your help !

I can group by id and apply a function per group to define wich data is the right one . I can unstack the dataframe and put ' group ' as columns and apply a function . Is there a simpler more elegant way to do this ?
One method would be to replace the erroneous data with ` NaN ` , then drop those rows , sort the df by id and group , groupby id and take the first value : #CODE

Pandas apply exponential decay + dataset to function
I'd like to make a function in pandas that calculates the resulting , continuous dataset for my activation function , but I don't know which functions to apply .

Drop values satisfying condition plus arbitrary number of next values in a pandas DataFrame
So my final goal is to drop values in one column of a ` pandas ` ` DataFrame ` according to some condition on another column of the same ` DataFrame ` , plus several next values e.g. : #CODE
So this will drop the records where the condition is satisfied , but how do I drop the next 3 records after the condition was satisfied too ? My desired output would look something like this : #CODE
We can use the boolean condition index to slice the df using ` loc ` and set the following values : #CODE

What I want to do is create a pivot table with the registration weeks as keys . The columns should be the visit_weeks and the values should be the count of unique account ids who have more than 0 weekly visits .

how to iterate rows and append column name
I need to iterate over rows and append column name with ' true ' value in a string with output like : #CODE

pandas - apply time and space functions to groupby
To compute differences between adjacent values in a Series , use the ` diff ` method . So , for example , ` Lat2-Lat1 ` would become ` grp [ ' Lat '] .diff() ` . #CODE

I don't get any errors . The problem is that ` apply() ` takes a very long time and I'm trying to cut it out where I can . I'm trying to find out which entries are problematic , but I can't identify any entries that are type " long " : #CODE

possible duplicate of [ pandas groupby and join lists ] ( #URL )

Any ideas as to why ` concat ` isn't recognizing the loaded , previously pickled dataframes , when they seem to be perfectly good ?

I get an error that says I cannot convert NA to integer . This code works when I drop the check to see if Independent is null . My question is , what is the best way to write an if statement that includes null values in Pandas ?

Also you are performing what is known as chain indexing which may operate on a copy rather than a view , see the #URL basically you should use ` ix ` or ` loc ` instead

Oddly , I'd like the exact opposite functionality . Usually my column names are very meaningful , so in my code I reference them directly . But due to a lot of observation cleaning , the row names in my pandas data frames don't usually correspond to ` range ( len ( df ))` .

pandas merge not using sql
I intend to not use sqldf just use merge but cant figure it out ..
I get some example for filtering bull after the merge but not sure whats next .. #CODE
Carry on with the ` merge ` based solution .

Head and tail of the datasets are the same* , also mean , min , max , median of weight and time .

Loc vs . iloc vs . ix vs . at vs . iat ?
` loc ` is label based indexing so basically looking up a value in a row , ` iloc ` is integer row based indexing , ` ix ` is a general method that first performs label based , if that fails then it falls to integer based . ` at ` is deprecated and it's advised you don't use that anymore . The other thing to consider is what you are trying to do as some of these methods allow slicing , and column assignment , to be honest the docs are pretty clear : #URL
Detail explanation between ` loc ` , ` ix ` and ` iloc ` here : #URL
loc : only work on index
ix : You can get data from dataframe without to be this in the index
at : get scalar values . It's a very fast loc

I am using Folium to create a map which has a set of polygon that need to be colored based on other data . When I try to do so they just appear black and the specified colormap is not applied .

python pandas wildcard ? Replace all values in df with a constant

oops I wasn't putting the brackets in concat ... thanks :)
So even if it didn't complain it wouldn't produce what you want . There are various methods of joining , merging and concatenating multiple dfs , in your case ` concat ` is what you want :

` df.to_csv ( loc , index=False , header=False , sep= ' \t ' , mode= ' a ' , encoding= ' utf8 ')` .
Purpose of loc flag : location of file flag = indicates whether I am writing the 1st row , or 2nd row onwrads I dont need to write headers again if the 1st row has been written already .
I checked this , the length of original list is 155 , the distorted list saved to_csv has 100 datapoints . Try with counting len of both the original & this list ? Also how do I get rid of " ,... " appended at the end of the list while saving . I opened the file in a vi editor , so the issue is not in " csv " reader so to say .
` flag ` and ` loc ` are undefined in the snippet . Makes it hard to test without making up values .
Ignoring the flag and loc which are irrelevant your df goes to csv fine , but reloading it back in is problematic as it's parsing the data in a weird way so I get 1682 as a len as it's parsing each character rather than treating it as a list of strings
@USER I updated the code , with loc & flag . Also I need to keep this as list , if possible . If I store this as string , then I will have to use ast.literal_eval ( list_of_values.ix [ index ]) and so on .
I am not an expert on python garbage collection , that sounds right . [ You can append rows by appending dataframes together ] ( #URL )

This may be an excellent opportunity to highlight the " Split , Apply , Combine " premise and with a simple case use ?
@USER : that is what I meant to say [ here ] ( #URL ): combine all values for the same day of the year ( not counting leap days ) in different years e.g. , ` sum ( data [ ' 12-01 ']) / len ( data [ ' 12-01 '])` will give you average ( assuming each day has all hours otherwise you have to decide whether to find 2005-12-01 average first and then to combine it with 2011-12-01 average or just use the same weight for all hours values .

plot the data as a boxplot

Append Dataframe in Pandas within a function

If you have a DataFrame where all columns are booleans ( like the slice you mention at the end of your question , you could apply ` all ` to it row-wise : #CODE

Use ` set ` ` intersection ` method to get same keys from the dictionaries .

Truncate ` TimeStamp ` column to hour precision in pandas ` DataFrame `
I have a ` pandas.DataFrame ` called ` df ` which has an automatically generated index , with a column ` dt ` : #CODE
( The only route within Pandas I could see was to do something like ` pd.DatetimeIndex ( df [ ' dt ']) .to_period ( ' h ')` which seems heavy in comparison and changes the dtype of the column . )

@USER : ` merge ` is for join-style operations . If you just want to concatenate DataFrames , that's what ` concat ` is for .

Edit : So I guess you can strip the date . save it to a variable in the format you desire and put it back into where you need to in your CSV cell

I'd create a df from your list and then perform a left style merge with your database df this will add the values where they exist and put nan where they don't : #CODE

Usually , you can apply the function in one of the following ways : #CODE
If the dataframe is empty , or has only one entry , these methods no longer work . A Series does not have an ` iterrows() ` method and ` apply ` applies the function to each column ( not rows ) .
Is there a cleaner built in method to iterate / apply functions to DataFrames of variable length ? Otherwise you have to constantly write cumbersome logic . #CODE
I realize there are methods to ensure you form length 1 DataFrames , but what I am asking is for a clean way to apply / iterate on the various pandas data structures when it could be like-formatted dataframes or series .
Instead of doing either of those things , I think it is better to make sure you create the right type of object before calling ` apply ` . For example , instead of using ` df.iloc [ 0 ]` which returns a Series , use ` df.iloc [: 1 ]` to select a DataFrame of length 1 . As long as you pass a slice range instead of a single value to ` df.iloc ` , you'll get back a DataFrame . #CODE

Replace values of specific pattern with ' NaN ' in csv file
and I want to replace any value that has a pattern of a number and then one letter or a number and then two letter with NaN .
` replace =d f.replace ([ r ' [ 0-9 ] [ A-Z ]'] , [ ' NA '])
I was hoping by using the pattern of [ 0-9 ] [ A-Z ] would take care of a number and just one letter and then [ 0-9 ] [ A-Z ] [ A-Z ] would replace any cells with 2 letters but the file stays the exact same even though no errors are returned .
@USER I think @USER is interpreting your data as a string output . pandas does have a [ replace ] ( #URL ) method . I've updated my answer to also include how it would work with ` replace ` as well .
I see how I got off track trying to use replace now , both methods are very helpful though , and again , many thanks .
If you wish to go the route of using ` replace ` , you need to modify your call . #CODE

So Instead of rot I used Transpose . Something like this :
Hi xnxx . Please check the edit . I have used Transpose instead of Rot and it works if done at an individual level and not on the whole matrix . Check the edit for more details .

I would suggest , to separate cut off and conversion to list : #CODE

Python Pandas : Using ' apply ' to apply 1 function to multiple columns
Essentially , I'd like to know if I could apply ` function ` to ` df ` to get the following output : #CODE
you need to apply the function on each row , for this you need to specify axis=1 #CODE

Use apply / map for pandass dataframe column
But id does not work with ` apply ` method : #CODE
What am I doing wrong and why apply method takes this int index as parameter ?

After reading your data and puting in a dataframe , you can groupby values based on one of the columns ` groupby ([ ' month '])` , and then apply a function on these values , Pandas includes a number of common ones such as mean() , max() , median() , etc . : you can use ` sum() ` for example . #CODE

How can I change my original df to append those 2 additional columns ?

How to replace values in pandas with column names

At the first step I used ` df.T ` to transpose the dataframe , and tried something like ` df.value_counts() ` , however I'd

Perform a ` merge ` and pass the list of columns to param ` on ` , the default type of merge is `' inner '` which only matches where values exist in both dfs : #CODE
If your ' id ' column is your index , you'd have to reset the index on both df's so that they become a column in the df's , this is because the inner join will produce an incorrect result if you specify the ` on ` list of columns and also specify ` left_index=True ` and ` right_index=True ` : #CODE

Or , if you know the ` a ` column will be sorted for each ` n ` , you could keep track of the last end-value handled by onebyone() and insert some extra rows to catch up to the next start value you're going to pass to onebyone() .

I only know I could use ` concat() ` to combine columns and use ` apply ( lambda xxx ... )` to set up a suitable function .
yes , I could rewrite it to use the length of the column index using the index , something like ` for i in range ( len ( df.columns ))` : col_1 =d f.columns [ i ] col_2 =d f.columns [ i+1 ] and use these col identifiers to index into the df as usual

I am trying to unstack a column in python but it isn't quite doing what I am expecting . My table looks similar to this : #CODE
I want to unstack by month so that all the days from a months are in one row . The two days from month one would then start first followed by the 2 days from month two then the 2 days from month three and so on . I no longer need the month column after this and I have tried deleting it and unstacking but it won't work .

pandas merge and fill a dataframe with summary data
I'd like to do some kind of merge or join so that I get #CODE
What you want is a left join .
A faster method that doesn't involve renaming / dropping columns is to set the index of frameB to ` title ` and call ` map ` on frameA passing in the other df and passing a series . This will perform a lookup using the title values and return the values that match : #CODE
If we compare the performance of merging against map , we can see that map is much faster nearly 5X faster : #CODE

How to plot pandas notnull values over time ?

append multiple files into one , skip the first row and sort
PROBLEM IS , come March there is an hour from the preceding month in each file from the Daylight Savings shift , like so : #CODE
Some pseudo code , you don't need to skip the rows here as the concatenation will align the columns for you , also it will be much faster to just make a list of dfs and then concatenate them all together rather than appending one df at a time : #CODE

Pivot table with multilevel columns
I would like to create a pivot table as below : #CODE
shows that the ` odd1 ` index needs to become a column index . That operation -- the moving of index labels to column labels -- is the job done by the ` unstack ` method . So it is natural to unstack ` aggd ` : #CODE
Now it is easy to see we just want to select the average columns . That can be done with ` loc ` : #CODE

Error creating pivot tables in pandas
I have searched and cannot find anyone else with this problem . I am trying to create a pivot table summarizing a csv file , and then email that pivot to myself . I have already built out the code to perform this process , but it is not working universally . I keep getting a KeyError on my column name , but if I delete all columns and rows that are not part of the table it miraculously works .

That works perfectly . I was apply to modify the format a bit to help me out on another file as well ! Thanks !

Pandas : Pivot table without sorting index and columns
I'm trying to pivot data in a way so that the index and columns of the resulting table aren't automatically sorted . An example of the data might be : #CODE

I'd use ` melt ` to turn the frame and then ` size ` : #CODE
Thank you ! I had never used melt before , but it worked perfectly !
My take using ` melt ` and ` pivot_table ` . #CODE

I would like to leave the id column but merge in one new file the content , something like this ( i.e. generate a new file ): #CODE
Or merge them ( pd.merge() )

1154 loc : int if unique index , possibly slice or mask if not
it's a small section of a much larger script . I need the rest to remain intact if at all possible . Just trying to replace a manually entered variable which works with an equivalent passed in from a csv file . The line ' end = datetime.datetime ( 2015 , 2 , 6 )' works as intended , just trying to pass it a date from an external file instead .

Pivot Pandas DataFrame into hierarchical columns / change column hierarchy
I want to pivot a dataframe like : #CODE

Ideally , I want to store the output in a new DataFrame , where each row corresponds to the rows in the original DataFrame , and there are 3 columns . I was asked not to use a loop to do this , and to use apply , but I cannot figure out the syntax ( see attempts below ) . What are my options for getting the desired output ? ( I'm new to python and pandas , so please excuse my ignorance and let me know if I've left out any necessary info ) . #CODE

Only problem I'm getting here is that everytime I use csv open it always gives me a IOError Errno 2 " File does not exist " I'm not sure if its suppose to be in a specific location the file or ? I'll look into it . But other than that , the for loop you implemented could you also say column [ 0 ] or would I have to transpose my information ?

if you want to store the results in your data frame , I would define a function and then apply it to the data frame like this : #CODE

Use ix should be able to locate the elements in the data frame , like this : #CODE

Another option for problems like this is to use numexpr , see enhancing performance with eval section of the pandas docs . However I don't think there is ( currently ) support for multiple assignment , so in this case it wouldn't help - nonetheless it is worthwhile reading .

I want to make this list into its own table and populate it with the information in all the other rows . Based on the explanation of concat ( objs ) here , #URL , which says :
unclear what you expect here concat expects a list-like object , you passed a single df this would work : ` odf = pd.concat ([ df ] , ignore_index=True , verify_integrity=False )`

If memory allows , I would suggest building a list with all the adjacent values for comparison to start with ( in my sample using zip ) , and append the result to a new list , re-assign the whole result list back to the DataFrame after completion .
Thank you so much for the help . It solved my problem with a much better performance . I may cut the whole list into parts to fit the memory for large datasets . Your help is really appreciated !

I have a pandas data frame with a few columns . For each column I want to calculate certain percentiles . I then want to replace my data frame with the percentile each observation falls in . #CODE
For example , in row 0 , c0= 24.883 . The largest c0 quantile q_c0 where 24.883 =q_c0 would be 0.5 . In my new data frame I would then want to replace 24.883 with 0.5 .

I'm trying to merge a dataframe ( ` df1 `) with another dataframe ( ` df2 `) for which ` df2 ` can potentially be empty . The merge condition is ` df1.index =d f2.z ` ( ` df1 ` is never empty ) , but I'm getting the following error .
This isn't the same as merging . After a merge , I would expect all columns to be part of ` dfm ` .

How do you merge two Pandas dataframes with different column index levels ?
but a merge or concatenate on the frames will give me a df with a one-dimensional column index like that : #CODE
Now you are able to do the concat without mangling the column names : #CODE

Apply function over relative rows in Pandas
Thoughts / Solutions with ' map ' or ' transform ' are also very welcome .
of course , one alternative is writing a for loop employing df.loc [ i , col ] and df.loc [ i-1 , col ] , but I generally find apply or transform with functions computationally faster
Have you looked at using the ` shift ` method to shift rows up or down ? This works well if you are trying to make use of rows at particular relative positions .
You'll also note that the specific example you have provided can be solved by other , and possibly simpler , means . The solution above is very general in the sense that it is letting you use ` map ` while jailbreaking from the row being mapped but it may also violate assumption about what ` map ` is doing and , for example . deprive you from the opportunity to parallelize easily by assuming independent computation on the rows .
Also , there are some standard processing functions built in like ` diff ` , ` shift ` , ` cumsum ` , ` cumprod ` that do operate on adjacent rows , although the scope is limited .

How can I join these two so that they are in the same dataframe ? The " 668 " etc should be under num_rides .
then convert the Series to a DataFrame , and merge it with ` df ` : #CODE

You could use a mask and apply it to the dataframe #CODE

You almost had it , you just need to drop the columns after assigning the values : #CODE

The above is then groupby'd on customer and then we can apply a filter where the number of unique ( nunique ) customers is equal to 2

Pandas get the index of an element in a map function
I could use the index of the element in the Series to get the node in the sessions DataFrame but I did not found any way to retrieve the index of the element passed to the map .
Merge start and finish timestamps and create another column that has value 1 for start timestamps and -1 for finish timestamps . Then sort timestamps and cumsum the values .

Would it be better to pivot the dataframe and then plot from there ?

I have a grouped DataFrame which i want to aggregate with a dictionary of functions which should map to certain columns . For single-level columns this is straightforward with ` groups.agg ( { ' colname ' : function } )` . I am struggling however to get this working with multi-level columns , from which i only want to refer to a single level .

First , I would like to add 3 extra columns with order numbers , sorting on sum , sum_sq and max , respectively . Next , these 3 columns should be combined into one column - the mean of the order numbers - but I do know how to do that part ( with apply and axis=1 ) .

I am looking for a way to merge categories of one object of ` pandas.Series ` to categories of another . #CODE
Now , all elements with category ' b ' are assigned with category ' a ' . If you now remove category ' b ' , this will end up with a ' merge ' in practice : #CODE

There isn't a built in method to do this , so what I'd do is parse and split the lines and append to a list based on your entire row length : #CODE

Unfortunately this is the behaviour of ` to_datetime ` , for instance if you passed just a time , it will generate a default date component of today . I don't think you can control this , your best bet would to filter these values or just replace them manually
If your date strings are of a common format e.g. year-month-day , you could try ` pd.to_datetime ([ ' 1M '] , format= ' %Y%m%d , coerce=True )` - dates not able to be parsed will be replace with ` NaT ` .

A work around , to load a utf file correctly into excel , is to get the program insert a macro into your excel sheet after you have loaded it which imports the data . I have some code to do this in C# , if that's any help ?

But if I use boxplot style , it doesn't work : #CODE
Is there any way ( maybe through matplotlib ) I can get pandas to plot 2 axes for boxplot ? Using the ` boxplot() ` method would be preferable , but I'll take ` plot() ` . Thanks .

use a list ` allBlue = [ ]` and append to the list ` allBlue.append ( row [ ' imageName '])` then iterate over the list ` for iter in allBlue ` . Is that something you would want to do ?

So instead of using ` test ` , unstack ` test ` first so you get two columns -- one for ` emp ` and one for ` unemp ` : #CODE
Then apply the rolling_mean to ` result ` , so you get two columns of rolling means : #CODE

To drop rows - take first observed , you can also take last #CODE

Can you share your expected result ? Do the want to append the columns in the same position regardless of their label or do you want the table to widen as new labels are added ?
I don't understand the ' slightly different column names part ' . If you concat vertically matching columns will overlap and new columns will widen the table .
Ah yes . You are very correct JAB . I was under the impression that using the join= " outer " argument on the concat function would just append everything straight up and down vertically without regard to column names . It seems this is not the case . I must have been confused when I initially tested it . I will edit my question accordingly .
This works . I was originally under the impression that concat with the join= " outer " argument applied would just append straight up and down without regard to column names . Actually , when the join= " outer " argument is applied it will combine what matching columns it can but then keep all of the non-matched columns off to the side of the DF , which is exactly what is desired . Hope this helps someone else .

` join ( str ( u [ k ] .get ( header , 0 ))` looks like here is an issue here . It complains saying unhashable type : list

Hmmm , I guess I should have transposed the data ... well that was a relatively simple fix . Instead of using groupby and apply , #CODE

Thanks for the reply ! It works when I just pass a single string to that function but not when I apply that function to a column ----> df [ ' Duration '] = df [ ' Avg . Session Duration '] .apply ( convertTime ) . It returned this error = TypeError : expected string or buffer

I can do these by creating 100 list of 1000 dimensions each with a univariate distribution ( mean 0 and 1 ) and can then concat it together in numpy . But I think that would be a very long approach . I am looking for a more pythonic way . Also I would appreciate if a general approach could be given which could help me generate any distribution for a NxD dimensional dataset .

Python Pandas Panel4D resample
To resample by dates the index needs to be a DatetimeIndex , TimedeltaIndex or PeriodIndex , not a MultiIndex . Therefore , we need to move the ` zlevel ` , ` lat ` and ` long ` index levels into the columns : #CODE
and now we can resample the dates : #CODE
This is great ! Thank you very much . Didn't know abot the MultiIndex functionality . Pity that Panel4D resample feature is not sorted yet .

Read large csv file with many duplicate values , drop duplicates while reading
That particualr column of my file contains perhaps 20 values at most ( sample names ) , so it would probably be faster if I could drop the duplicates on the fly instead of storing them and then deleting the duplicates afterwards . Is this possible to delete duplicates as they are found in some way ?

If I am right , you first need to concat three columns in a string ` value = str ( col1 ) +str ( col2 ) +str ( col3 )` and then use the method to convert it in binary .

to check if the freq was 10hz or 100hz just change the units to ` np.timedelta64 ` so for 10hz : ` np.timedelta64 ( 100 , ' ms ')` and for 100hz : ` np.timedelta64 ( 10 , ' ms ')`
convert the diff series which is a timedelta64 to a datetime64 , use the allclose method , convert it back to timedelta64 then check it is how i'm seeing it .

Assign the result of ` df.groupby ( ' User_ID ') [ ' Datetime '] .apply ( lambda g : len ( g ) 1 )` to a variable so you can perform boolean indexing and then use the index from this to call ` isin ` and filter your orig df : #CODE

@USER Oops , I added the ` initializer ` argument to the wrong line . It belongs in the call to ` Pool ( ... )` , not the call to ` map ` . I also made a typo in my spelling of ` initializer ` . That's what I get for writing the code from memory :) . Anyway , fixed now .

I want to group that data set by dist and divide the count column of each group by the sum of the counts for that group ( normalize it to one ) .
This avoids the need for a bespoke Python function and the use of ` apply ` . Testing it for the small example DataFrame in your question showed that it was around 8 times faster .

However I would like to join this information with a series containing the number of unique values . At the moment I compute this value as follows : #CODE
Is there a Pandas-esque way to get unique as a Series so I can join the result with valueCountSeries in a new DataFrame ?
An example DataFrame would really help here , IMO it's unclear how you're hoping to merge these ( what the result should be ) .

I have a log file with timestamps and two columns . I would now like to resample and " pivot " the dateframe created from the logfile .
How would I achieve this ? Resample first , then groupby / pivot ? Or the other way around ? To be more specific I the cells should contain the count for colA / colB combinations for each specific resample timeintervall . In the example seconds , but it could be minutes , hours , etc .

Ii would just say to let them in and then to drop rows that have ` -- ` in the first column . Something like this : #CODE

I'm using the following code of Python using the Pandas library . The purpose of the code is to join 2 CSV files and works as exptected . In the CSV files all the values are within "" . When using the Pandas libray they dissapear . I wonder what I can do to keep them ? I have read the documentation and tried lots of options but can't seem to get it right .

Is this pseudo code or is this really what you are trying to achieve ? do you just want to drop all rows that == 0 ?
a pseudo code sir . I want to delete / drop row whenever it is found on the other df that i have . I have two df having certain columns containing similar values but with different col headers .
I don't know if this is pseudo code or not but you can't delete a row like this , you can ` drop ` it : #CODE

If I replace the last line by ` df = df [ df [ ' A '] .isin ([ ' UBS '])]` I do get the expected error message , ` cannot reindex from a duplicate axis ` .
Pandas tries to align these two Indexes , and as part of that process the values in the indexes are compared . In Python2 strings and ints are comparable , in Python3 the comparison raises an exception .
Now if you insert #CODE

I am really sorry for being that naive , I promise to learn as much as I can if someone could guide me towards the right direction ( regarding the theory and technologies to apply ) .

Interpolate data in pd.DataFrame onto 6-hourly time steps
Pandas : interpolate missing rows and plot multiple series in dataframe

So , effectively we want to unstack and transpose so that 0 / 1 are the index , which we do by : #CODE

Do I really need to ` apply ` and iterate through each row , or is there a more efficient alternative ?
You could use ` map ` here : #CODE
` map ` can take a dictionary , Series or function and return a new Series with the mapped values . It is also very efficiently implemented ( much more so than ` apply ` , for example ) .

Pivot : Most of columns as index
This can be accomplished with ` pivot / transpose ` : #CODE
Per your request , I edited the post to show how to remove the column level name , ` type_string ` . But note that level names are useful . Many pandas functions ( like ` stack ` and ` unstack `) accept level parameters which either accept names or integers . Without the level name you must use integers which is less readable .

I am trying to map the data objects from a Python-Pandas dataframe to a Tableau TDE file compatible datatype . The reason is I want to create a Tableau TDE file out of the pandas dataframe . Can you please give me some directions on the same ?

Does this work ? You replace the series using list comprehension , setting the day to 1 but using the original month and year . I assume you don't need the time information , but you can extend it to include hours , minutes , etc . if needed . #CODE

Finally , create a new data set by calculating the length of each sub group ( which represents the number of requests in that second ) and aggregating the results into a single DataFrame ( something like ` new_df = groups.aggregate ( len )`)

One simple method would be to assign the default value first and then perform 2 ` loc ` calls : #CODE

One method would be to store the result of an inner merge form both dfs , then we can simply select the rows when one column's values are not in this common : #CODE
Another method as you've found is to use ` isin ` which will produce ` NaN ` rows which you can drop : #CODE
If index should be taken into account , set_index has keyword argument append to append columns to existing index . If columns do not line up , list ( df.columns ) can be replaced with column specifications to align the data . #CODE

I think that a merge might be even faster . Usage of ` df.iterrows() ` isn't recommended as it generates a series for every row .
What is a better way to apply the function to each row ?
@USER you don't need to iterate throw the rows . Just do an inner merge and discards all the ` NA `

Replace #CODE

` index = pd.data_range ( start = df.date [ 0 ] , end = ' 2015 / 03 / 06 17:07 : 05 ' , freq = ' S ')` in the pd.Series ( ) function .

Resampling approach doesn't seem to work because I can't resample when indexed by Id and Date ...

All I want is to insert data from the dataframe to a sqlite db and add a column which can act as a primary key
The problem lies in the index column you already have . Pandas tries to insert the index of the dataframe as a column ' index ' , but as there is already an column named ' index ' , it uses a next option , namely ' level_0 ' . But as you append to the table , this column is not present . Possibly this is a bug in pandas , but there are some workarounds for now .
specify to not insert the index of the dataframe : ` dataFrame.to_sql ( ...., index=False )`
or you can let ` to_sql ` create the database table itself ( no append ) , then table definition in python is not needed .

Make the dates datetimes then use the datetime replace method . #CODE

Pandas scalar value getting and setting : ix or iat ?
I'm trying to figure out when to use different selecting methods in pandas DataFrame . In particular , I'm looking for accessing scalar values . I often hear ` ix ` being generally recommended . But in pandas documentation
So , I would assume ` iat ` should be faster for getting and setting individual cells . However , after some tests , we found that ` ix ` would be comparable or faster for reading cells , while ` iat ` much faster for assigning values to cells .
Just to make sure that this behavior is not a bug of this version , I also tested it on 0.11.0 . I do not provide the results , but the trend is exactly the same - ` ix being much faster for getting , and iat for setting individual cells ` . #CODE
Gosh , it's an interesting one ... It seems to make sense that time spent on ``` iat ``` is about the same each way , but then why is ``` ix ``` so much slower when it's on the left hand side than right ? I noticed you are setting float variables equal to integers but that conversion doesn't seem to cost any time .
This information suggests that calls to get values using ` ix ` are limited by ` get_value ` in the best case and calls to set values using ` ix ` would take a core committer to explain .

Try setting the drop inplace parameter to True .

The append gives a diagonal matrix even when ignore index is false : #CODE
Is there a better way of doing this ? and is there a way to fix the append ?

I have looked into ways of creating a function to do this , but confused as to how to map and / or apply it in my case , especially the part returning the result as a new column .

Just trying to find the most elegant way to apply a really simple transformation to values in different columns with each column having it's own condition . So given a dataframe like this : #CODE
I was thinking the where function in pandas would come in handy here but wasn't sure how to apply it . I could do the below but does not seem very efficient and I would have to create a different function for each col : #CODE

I am trying to insert a column , say ' new_col ' , that calculates the percent change in VAL that is grouped by ID . So , for example , I would want the percent change from 45 to 23 , 23 to 54 , and then restart for ID ' B ' . The below code works but it calculates the percent change regardless of ID . #CODE
You can't just just stick your expression in brackets onto the groupby like that . What you need to do is use ` apply ` to apply a function that calculates what you want . What you want can be calculated more simply using the ` diff ` method : #CODE
However , it is good to be aware of how to do it with ` apply ` because you'll need to do things that way if you want to do a more complex operation on the groups ( i.e. , an operation for which there is no predefined one-shot method ) .

I also ran into problems when trying to resample the data at 512Hz ( double the frequency ) . I got this error message :
If I change the sample period from ' 3906U ' to ' 2S ' and resample to

That error is an unrelated error which I have no context for as for proposals I don't think I have time to be honest , if it's a real job you may be better posting something on careers stack exchange

doing multiple operations like this will append to the rhs store .

I used to work with python , csv , and postgres , but not pandas . I know a bit about this . What is the source .csv field ? And what fields that exist in postgres table ? Why not use for in for to insert every field ? But it depends on your RAM .
Change the print line with insert into PostgreSQL . #CODE

( After a long stack trace , the next end ) #CODE

I'd like to align the index so that it's in line with the column headers . Essentially , I'd like to keep the column headers where they are , but shift everything up by one cell .
What worked for me was to reset the index so that ' Date ' becomes an ordinary column , then call the ` dt ` property ` date ` to assign back just the date portion and when writing to excel pass param ` index=False ` : #CODE

Thanks again . Can you please help me understand diff between annotate and text ? Thanks !

Python Pandas - Main DataFrame , want to drop all columns in smaller DataFrame
The columns that make up ' public ' are not consecutive - i.e. they are taken from various points in the larger DataFrame ' main ' . All of the columns have the same Index . [ Not sure if this is important , but ' public ' was created using the ' join ' function ] .

The expensive way to do this is to merge the two DataFrame objects , calculate correlation , and then throw out all the stock to stock and industry to industry correlations . Is there a more efficient way to do this ?

Again we could perform 2 inner merges this would merge on the values that exist in all dfs : #CODE
How about treating each column as a set and then taking the intersection : #CODE

Pandas NameError : name ' merge ' is not defined
Trying to merge two data frames :
dt [: 3 ] gives me :
I need to merge two tables by yearID and teamID . I do : #CODE
Why do I get ? I couldn't find explanation online for this error for ' merge ' . And I have no idea what it wants from me . Ideally I need to merge two tables and sort them by teamID by yearID . #CODE
My goal is to merge these two tables to create a new one , that will show wins and salary for each team for each year .
The error you're receiving is because you're not calling the pandas module along with the merge method .

When I insert pandas Series into dataframe , all values become NaN
This works fine for me too , but my own data doesn't , unless I append the " .values " . It's a mystery to me why .

In what sense does a CSV have limited precision ? You can always write out a CSV that contains the exact same information as a binary file . Generally it's less compact ( at least before zipping ) and almost always slower , but you shouldn't lose any info unless you intentionally round or truncate before writing out the values .

If ** arr ** contains extra elements not in ** frame.index ** then they will be added with NaN values which you will then need to drop from the ** ans ** table .

Dataframe Replace Regex
I am currently trying to replace a set of str values with a int value in python for my Dataframe . The DataFrame has over 200 columns , with columns such as Age_Range , Car_Year , Car_Count , Home_Value , Supermarket_Spend_Per_week , Household_Income etc .
I have read through the wiki and know how to replace with word boundary etc . But I am wanting to replace any occurrence that begins with a with the value 1 , b with the value 2 , etc .
You don't need a regex here , just create a lookup table and apply to your DataFrame's column based on that column's first character , eg : #CODE
To apply this to all columns , then loop over the columns : #CODE
@USER then just apply it to all relevant columns ? Could you [ edit ] ( #URL ) your question to clarify exactly what you do have ... ?

Your output from your starting df is wrong , the last row should be [ 2 , 4 ] , aside from that we can call ` loc ` on the index generated by a boolean filtered df plus drop any ` NaN ` values : #CODE

Pandas efficiently normalize column titles in a dataframe
So it should be more efficient as to function call overhead , but I'd personally view it as more readable by having a ` dict ` of keys , which are the " to " values , with the values being the " from "' s to replace .

You can groupby the user_id column and then call ` apply ` and pass a lambda which filters the results where the start time is equal to the max value , we want to generate a boolean index from this . We can then call ` reset_index ` but due to the way the groupby was filtered we will get an error with duplicate columns so we have to drop this duplicate column : #CODE

thank you , the ` apply ` version actually worked but not the direct ` unique() ` version .

Edit to add : Based on Korem's comment , small negative numbers are in fact causing a problem . I think the best thing to do in this case is to use numpy's ` around ` function and replace the second step above with : #CODE

I've got a DataFrame who's index is just datetime.time and there's no method in DataFrame.Index and datetime.time to shift the time . datetime.time has replace but that'll only work on individual items of the Series ?

Extending on @USER Haffner suggestion , there is also join #URL
Assuming that ' cust ' column is unique in your other df , you can call ` map ` on the sales df after setting the index to be the ' cust ' column , this will map for each ' cust ' in budget df to it's sales value , additionally you will get ` NaN ` where there are missing values so you call ` fillna ( 0 )` to fill those values : #CODE

The solution below uses a lambda function to apply a regex to remove non-digit characters .
Solution is great . but when I apply this to my original data frame I am getting an error " invalid literal for int() with base 10 : ' 16a '"

Recently I received a ` ValueError ` because the data I was trying to append to one of the columns is longer than the declared column size ( 2000 , with ` min_itemsize `) .
Do I append new rows the wrong way that causes the file size to multiple with every append operation ? Googled and searched here but don't think this was discussed before , or maybe I searched with the wrong keywords .
I tried adding ` min_itemsize ` for all data columns in the append line per suggestion in this thread : pandas pytables append : performance and increase in file size : #CODE

There is probably a more concise way , but this works . The main trick is just to normalize the data such that User1 is always the lower number ID . Then you can use ` groupby ` since ` 11 , 12 ` and ` 12 , 11 ` are now recognized as representing the same thing . #CODE
For more concise code that avoids creating new variables , you could replace the middle 3 steps with : #CODE

However , it ( 1 ) returns an empty dataframe and ( 2 ) does not replace the NaNs which result from the merged cells .

What is an apposite function of ` pivot ` in Pandas ?

pandas - show results of apply next to original dataframe
I have a pandas DataFrame , then I apply a function to a bunch of columns and I get a new result column . Then I want to be able to check the results of the new column with the original column values . #CODE

You can call the vectorised ` str ` method on the ' match_up ' column to split the string , map these to int and create a list so we can filter the second df on to create df2 and df3 : #CODE
perfect answer . so I can use merge to merge these 3 data frames right . but what would be the ` on ` value while merging ?

Looks like ` get_dummies ` is what you want . This will take any column and convert it into a pivot of categorical indicators .

pandas merge by coordinates
I am trying to merge two pandas tables where I find all rows in df2 which have coordinates close to each row in df1 . Example follows . #CODE

Is the problem simply that you think ` append ` acts in-place ? It doesn't , see [ this ] ( #URL ) . It's not unreasonable to think that it would , though , because that's how ` list.append ` works .

One way to calculate this is to use ` apply ` on the ` groupby ` object : #CODE

Replace an entry in a pandas DataFrame using a conditional statement

You are going to need to extract the strings and convert the to ints in order to ` merge ` correctly ... #CODE
When I apply this on my real data frame ( problem set ) . I am getting an error ` TypeError : Argument ' values ' has incorrect type ( expected numpy.ndarray , got Series )` what would be the reason ?

Generate ` n=100 ` draws to get ` z ` such that ` len ( z )= 100 `
For ` i ` in ` df.x.unique() ` , compute the product of the output in step ( 2 ) element-wise . I am expecting to get a DataFrame or array of dimension ( len ( df.x.unique() , n=100 )

I have previously this question here , I also got a very good answer . but the problem that I am facing is , when the year value changes ( in data frame ` df ` in ` match_up ` column ) the team value in ` team ` column in ` df2 ` and ` df3 ` keeps on repeating . so if I merge these three data frames on ` team_df3 ` and ` team_df2 ` values I am not getting the desired out put .
It would probably make sense to add the year columns to df2 and df3 so that the year could be used as an additional column to perform the merge on to avoid this ambiguity
@USER but again I have to drop the year column from the final data frame right ?
You can drop after if necessary but I think you need it to avoid this ambiguity , also having a column containing multiple identifiers is a very bad idea
@USER ok then I will go with you . but how do I add it in front of ` df2 ` and ` df3 ` and merge it . I am still confused with the process . It would be great if your could write an answer using this approach .
Now we can merge using year and the team columns to avoid the ambiguity : #CODE
You can then drop the columns you are no longer interested : #CODE
Hi EdChum , I tried to implement your solution . but I get ` ValueError : Length of values does not match length of index ` . when I assign ` df1 [ ' year '] = list ( map ( int , ( df [ ' match_up '] .str .split ( ' _ ') .str [ 0 ])))` . so in that case what should I do ?
If the lengths don't match then you could merge it but I'm guessing that df1 in my code is in fact df2 in yours , this may mean you are missing year information in df2 , what you could do is add the year information by merging it something like ` df1.merge ( df , on= ' team_df2 ' , how= ' left ')`
Ok I am working on it right now . will let you know after I apply it to my code !!

Python Pandas : How to replace a characters in a column of a dataframe ?
and I want to replace the ' , ' comma with ' - ' dash . I am currently using this method but nothing is changed . #CODE
Use the vectorised ` str ` method ` replace ` : #CODE

Pandas : Outer Join on Non-Unique Index
You can't see it here , but ` status ` can take three values : ` emp ` , ` unemp ` , ` NaN ` . This is data at the state-date level . I would like to join new state-date data , which is at a different frequency , and then aggregate / group over time . #CODE
Join without how=inner
join ` foo ` from the other database for every date-time combination , and create 2-year based values .
Join with how=inner
` join ` does ` how=inner ` on default , so this seems to be the issue . However , if I #CODE
Suggested Solution : Append as Column
One suggested solution was to append them as a column instead :
Using ` sort level ` to align the data frames : #CODE
I think I may have a solution . A few questions -- what is the method in the join statement ? Does the joining table have a status value like the first table -- I see test [ ' emp '] is that like status in the first table ? Am I correct in assuming there are not matching state-date combinations since you're trying to get a sum and mean over time ?
@USER no it doesn't . What you observed was a left-over typo . ` test ` originally had a column also called ' emp ' , which referred to something else . For clearness of the question , I renamed it here to ` foo ` - you found the one spot I forgot to change . There are no matching state-date combinations in ` test ` , there is just a time series ` foo ` . For this time series , I need to find means over groups of time . I need to join before aggregation because they have different frequencies : ` dfNew ` starts in ` 2003 ` , ` test ` starts in ` 2004 ` , but I want to match these via the mean values of ` test [ ' foo ']` .
@USER my ` yourtimeseries ` does not contain ` state ` - but I see the approach and am also thinking of a way to make ` append ` work
If you do your append and then classify your buckets of time in another column , you'll be able to easily group and calculate on those buckets .
@USER No . I tried to add them as a column in your spirit , but apparently when the index shapes are not the same , ` append ` is quite inefficient , and my Python instance crashes - see update .
I read another post on bucketing using the cut method . You can read it here -- Python 2.7 PANDAS grouping data by value ranges . I am thinking you could build the 2 year buckets using datetime object manipulation .
Could you show the exact code you used to ` append ` - to create your ` final_df ` ? My use of ` append ` crashed Python on the whole dataset .

How do you apply a function to one of several columns in a DataFrame ?
I would like to apply a function to data in one of the columns and return the same DataFrame but with new values in the column to which the function was applied to .
Sorry it's unclear why you need to apply on several columns when the following does what you want : ` df [ ' Numbers '] = df [ ' Numbers '] .apply ( lambda x : int ( ' 1 ' +str ( x )))` can you explain what you are trying to do ?
I only want to apply on one column . In your example I would get a DataFrame or Series with one column only , which I would further have to merge with the the initial DataFrame . I hoped I could avoid this by applying a function to one column and leave everything else untouched .
Nope I assign back to the original column so there is no need to merge back , also I'm applying on just that column

I saw that it's possible to convert the column into the datetime format by DF = pd.to_datetime ( DF , ' %Y-%m-%d %H : %M : %S ') but when I try to then apply datetime.datetime.year ( DF ) it doesn't work . I will also need to parse the timestamps to months and combinations of years-months and so on ...
No need to apply a function for each row there is a new datetime attribute you can call to access the year attribute : #CODE
You can access the months and other attributes using ` dt ` like the above .
Thanks , @USER , but it doesn't work , giving AttributeError : ' Series ' object has no attribute ' year ' , although I converted the original DF into datetime64 [ ns ] ... I tried to apply DF.year and it is not working ...
No , you do ` df [ ' col_name '] .dt .year ` replace ` col_name ` with whatever your column name is , also what is your pandas version ? I think datetime properties was introduced in ` 0.15.0 `
I get AttributeError : ' Series ' object has no attribute ' dt ' when I use the original DF with additional column and the above error if I define a new DF1 = pd.to_datetime ( DF ) .. Maybe I should use explicit name rather than the dt abbreviation ? :-(

Pandas TimeGrouper : Drop " non full groups "
` rolling_mean ` allows ` center=True ` , which will will put ` NaN ` / drop remainders on the left and right . Is there a similar feature for the Grouper ? I couldn't find any on the manual , but perhaps there is a workaround ?
1 ) Drop enough observations ( at the start or end ) such that you have a multiple of 2 years worth of observations .
3 ) Normalize your data to 2 year sums based on underlying time periods of less than 2 years . This approach could be combined with the other two .
If you take approach ( 1 ) above , you just need to drop some observations . It's very easy to drop the later observations and retype the same command . It's a little trickier to drop the earlier observations because then your first observation doesn't begin on Jan 1 of an even year and you lose the automatic labelling and such . Here's an approach that will drop the first year and keep the last 4 , but you lose the nice labelling ( you can compare with annual data above to verify that this is correct ): #CODE
Alternatively , let's try approach ( 2 ) , extrapolating . To do this , just replace ` sum() ` with ` mean() ` and multiply by 24 . For the last period , this just means we assume that the 60 in 2014 will be equaled by another 60 in 2015 . Whether or not this is reasonable will be a judgement call for you to make and you'd probably want to label with an asterisk and call it an estimate . #CODE

The ` check_validity ` function is the key to the operation . It looks at each column , using the ` isin ` method to test for set membership . It constructs a second ` DataFrame ` , ` valid ` , to record which cells pass or fail the test . It then uses the very handy ` itertools.compress ` to choose just the column names that ` pandas ` awesome selection function ( ` ~ valid.ix [ rownumber ]`) to pull out " items that aren't valid on this row " and join them . Collect that list of not valid items per row across the entire ` DataFrame ` , and we're home .
Thank you ! This solution nicely leverages the tools to cut out complexity .

The problem I am running into is that I am unable to stack these varying data types . I can already plot the number of times a hit falls into a specific location ( bin in the histogram ) but I am unable to do this in a manner that leads to having three colors in the plot one for each " type " of hit that is found .
@USER its close but I cannot even get them to stack and I don't want to plot the entire DF just the value and location and group them by hit type

You can add an index to the ` bedrooms ` column and stack the dataframe .

My problem is that when I try to use the pivot method I get an error . I think this is because my " column " key is repeated ( because I have BMW 0-60 stats for the rain and for the sun conditions ) . #CODE
You can just change the index and unstack it ... #CODE

thanks for the tip . I tried set_index() and I get the following error message ( same one as before ): Series lengths must match to compare . this occurs when i set the conditions for comparison , similar to your bool = (( df1.Channel == df2.Channel ) & ( df1.Quad == df2.Quad )) line

pandas shift time series with missing values
However just applying pandas shift function directly to the column ' value ' would give ' previous_value ' = 10 for ' time ' = 2003 , and ' previous_value ' = 12 for ' time ' = 2007 .
What's the most elegant way to deal with this in pandas ? ( I'm not sure if it's as easy as setting the ' freq ' attribute ) .

Thanks for your effort . I posted another question but I think I am getting close .... using the apply method with a lambda function seems to be headed in the right direction .

I haven't had any luck with pd.join either . I also tried creating a new column with a ( PID , Trial ) tuple , but using merge on that gave similar results as above . Help please !
You want an inner join : #CODE

You can use a ` DataFrame's ` ` shift ` method . Note : this will produce NaN if either of the values being compared is 0 . #CODE

I've figured it out thanks , had to drop the "" at the end

How to merge 2 complicated data frames in python pandas ?

You could add " NA " as an identifier for NaN values and drop them afterwards if you don't have too many NaN values
Not that I can think of but parsing row by row will be slow , it'd be better to just read it all in and drop afterwards unless you expect a lot of NaN values
This should read in all ` ND ` values as NaN's and then you can drop them in-place immediately after . You can also had multiple NaN values if you would like . ex . ` [ ND , nd ]`

Insert 0 in pandas series for timeseries gaps
In order to properly plot data , I need the missing values to be shown as 0 . I do not want to have a 0 value for each missing day , as that bloats the storage . How do I insert 0 value for each ` type ` column for each gap's first and last day ? I do not need 0 inserted before and after the whole sequence . Bonus : what if timeseries is monthly or weekly data ( date set to the first of the month , or to every Monday )
For example , this timeseries contains one gap between 3rd and 10th of January for type ` A ` . I need to insert a 0 value on the 4th and the 9th of January . #CODE

I right now working with 300 float features coming from a preprocessing of item information . Such items are identified by a UUID ( i.e. a string ) . The current file size is around 200MB . So far I have stored them as Pickled ` numpy ` arrays . Sometimes I need to map the UUID for an item to a Numpy row . For that I am using a dictionary ( stored as json ) that maps UUID to row in a ` numpy ` array .
I was tempted to use Pandas and replace that dictionary for a Pandas index . I also discovered the HF5 file format but I would like to know a bit more when to use each of them .

I realise I have to transpose the output once successfully parsed , but I'm having problems in reading the json data contained in the dataframe column . Any help appreciated ! Thanks #CODE

Is there a way to directly load each list into a column to eliminate the transpose and insert the column labels when creating the DataFrame ?

The series here is just an example . The actual series is a limit order book snap shot , which has names ' bid.1.price ' , ' bid.2.price ' ..., ' ask.1.price ' , ' ask.2.price ' ..., and I would like to be able to alter the values of such entries based on a fast selection scheme .

Merge multiple columns into one while repeating two columns

How do I add / merge two multiindex Series / DataFrames which contain lists as elements ( a port-sequence or timestamp-sequence in my case ) . Especially , how to deal with indices , which appear only in one Series / DataFrame ? Unfortunately , the ` .add() ` -method allows only floats for the ` fill_value ` argument , not empty lists .
Oddly enough , ` series1.add ( series1 )` or ` series2.add ( series2 )` does work and appends the lists as expected , however ` series1.add ( series2 )` produces runtime errors . ` series1.combine_first ( series2 )` works , however it does not merge the lists - it simply takes one . Any ideas ?

Further , the encoding must be specified not when opening the store , but on the ` append / put ` calls #CODE

@USER : it's basically because ` or ` and ` and ` can't have their behaviour customized by a class . They do what they do based on the result of bool ( the_object ) , and that's it .

I would like to simultaneously replace the values of multiple columns with corresponding values in other columns , based on the values in the first group of columns ( specifically , where the one of the first columns is blank ) . Here's an example of what I'm trying to do : #CODE
I'd like to replace the '' values in b1 and b2 with the corresponding values in a1 and a2 , where b1 is blank : #CODE

I'd like to apply the model to column ` c ` , but a naive attempt to do so doesn't work : #CODE
If you replace your ` fit ` definition with this line : #CODE

While this example is trivial in my case i have a ton of dropped mappings that might map to one index as an example .

I would like to select certain rows from a DataFrame and apply a result from lambda from it , and I am not able to assign it correctly , either all the other columns become NaN or the DataFrame not changed at all ( I believe this is related to DataFrame returning a copy , read that caveat )

Wait , hang on , now I'm confused ; if you're writing it out to csv , which will translate everything to strings , why do you need to translate to strings in the data ? For the zero-padding ?
Thank you @USER , I asked it now to now what alternatives there were but didn't remember the map lamba solution . :-)

I am using the .to_excel method in pandas to write dataframe in an excel file . However i want to change the default formatting . The answer at Apply styles while exporting to xlsx in pandas with XlsxWriter helps with the basic formatting .

However , this only works with sums , and I can't see an obvious way to make it work with other functions . For example , is there a way to get it to work with median instead of sum ?
Nice , thanks ! That's a clever way of constructing the join table with the condition . I'll have to test it with some real data and see how the performance looks with larger tables .

Apply function on cumulative values of pandas series
Is there an equivalent of ` rolling_apply ` in pandas that applies function to the cumulative values of a series rather than the rolling values ? I realize ` cumsum ` , ` cumprod ` , ` cummax ` , and ` cummin ` exist , but I'd like to apply a custom function .

Ah , sorry , the bracket was in the wrong place ( the stack should be within the get_dummies ) . I am using pandas 0.15.2 . @USER Yes , I wrote that first , but I found it with stack a bit cleaner ( shorter ) , but it gives exactly the same output .
@USER , you started with a different input ( a string that formats as a list , I start from a list ) , but I am not sure what the OP wants . Apart from that , you did the ` get_dummies ` within the apply ( so for each row instead of once on all ) , which made it slower as the approach above .

As you can see , the first interpolation created a copy of the original dataframe . What I wanted is to interpolate and update the original dataframe , so I tried ` inplace ` since the documentation states the follow :
inplace : bool , default False
@USER I made a mistake , it's interpolate , not filter . But still , is it possible to filter out just a view of the subset of the original dataframe ?

Initially , I'm only interested in the cases where the string DOES fit the form of " P-A1-1019-03-C15 " , so it would be nice to be able to drop rows which don't match that specific format .
No it looks for exact matches , it depends on how varied your data is , you could just say slice the strings : ` df [ ' col '] = df [ ' col '] .str . [: -3 ]` which will strip the last 3 characters off ( I think , I may be off by one ) or do this : ` df [ ' col '] = df [ ' col '] .str [: 15 ]` if you want the first 16 characters
Then I could do the filter with ` df [ df [ ' col '] .str .contains ( regex )]` first , then , once all the strings are uniformly formatted , strip the last three ...

For the second column , ` SpeedMedian ` , I want it equal to 1 if the ` Duration ` in row i is less than the median duration for a given ` ID ` , else 0 .

I tried creating an empty ` list ` and using ` append ` : #CODE

I tried ` fdata [ fdata [ ' GEN_TICKER '] == i fdata [ ' date ' j ]]` and I get another type of error : ` TypeError : cannot compare a dtyped [ float64 ] array with a scalar of type [ bool ]` - but individually ` fdata [ fdata [ ' GEN_TICKER '] == i ]` and ` fdata [ fdata [ ' date ' j ]]` both work .

Your main problem was that running hist doesn't return an axis even though it uses one .

AttributeError : ' module ' object has no attribute ' hist '
As an aside you could just call ` plot ` on the df : ` df.plot ( kind= ' hist ')` see the docs : #URL
in order to use ` hist ` function

How can I best format a sample dataset when posting a SO question ? I did try to provide one but couldn't figure out the formatting , so I just included the list of variables in . ( Just starting out on Stack Overflow , I appreciate all the help - thanks for understanding ! )
Shift index by desired number of periods with an optional time freq
Note that we didn't actually need to sort by ` ID ` -- the groupby would have worked equally well without it -- but this way it's easier to see that the shift has done the right thing . Reading up on the split-apply-combine pattern might be helpful .

Is there a clever way to use merge in this scenario ?
So I've now read that the 2 and 7 represent the gov id's you could've used a dict rather than a list with tuples to perform this lookup . It would be easier to have gov be a real column and have a dataframe that had all the values in and then merge on gov to fill in the values you want

I have also tried using ` matplotlib.subplots() ` to create the subplots individually but this lead to the shared x-axis no longer only by at the intersection of all the ` Series ` being plotted .

Python : pivot a pandas DataFrame when the desired index Series has duplicates
It seems that the ` pivot ` method is appropriate but of course when I tried ` my_data.pivot ( index= ' user_id ' , columns= ' event_id ' , values= ' attended ')` I got the error that the index has duplicates .
As written , if there are multiple rows with the same user / event combination ( which probably isn't the case ) the attendance will be summed . It's easy enough to use ` any ` or clip the values instead if you want to guarantee the frame consists only of 0s and 1s .
thanks for the help but one follow-up question if you don't mind : when I look at the resulting pivot tables columns or index I get just the event_id or user_id numbers ... so what are event_id and user_id in the new pivot table ? the resulting thing from calling pivot table is a DataFrame so how would I access these identifiers for example ? Just trying to understand where they are stored in the new DataFrame . Thanks !

I load the files into memory , which shows 9% used . Then I execute a join statement . #CODE
Yes , it's ugly , but it's what I've been asked to do . The moment it hits the join , memory utilization hits 99% instantly and the mouse and all input locks up . I can hear the disk thrashing but can't get control back . After a few hours I power cycled the machine , but lost all the work .
And it does the same thing when I'm just join ng a couple tables together . Tables are indexed . I've also tried using merge() and concat() .
` DataFrame.join ( other , on=None , how= ' left ' , lsuffix= '' , rsuffix= '' , sort=False )` can only join two tables at once . It's interpreting your ` df3 ` as the ` on ` parameter , ` df4 ` as the ` how ` parameter , etc ... at least , it's trying to , but somewhere it's not happy with them being large data frames . Obviously , with correct parameters , pandas should be able to join large tables .
Ok , solved the problem . The problem was on the read_csv() call . I set the index to dtype str , but failed to include engine= ' c ' which resulted in the join to max out ram and eventually fail with a cryptic memory error .

I seemed ` unstack ` could help me but I couldn't understand how exactly .

After dataframe append I have the same amount of the rows
However , this would be terrible slow , since each time ` append ` is called a new DataFrame must be created and all the data from ` data2 ` and ` x ` would need to be copied into the new DataFrame . That's on the order of ` n**2 ` copies being made where ` n ` is the number of calls to ` append ` .

I have a dataframe ' clicks ' created by parsing CSV of size 1.4G . I'm trying to create a new column ' bought ' using apply function . #CODE
` apply ` is essentially just syntactic sugar for a ` for ` loop over the rows of a column . There's also an explicit ` for ` loop over a NumPy array in your function ( the ` for row in boughtSessions ` part ) . Looping in this ( non-vectorised ) way is best avoided whenever possible as it impacts performance heavily .
First use ` groupby ` to group the rows of ` buys ` by the values in ' session ' . ` apply ` is used to join up the strings for each value : #CODE
where ` col_to_join ` is the column from ` buys ` which contains the values you want to join together into a string .
` groupby ` means that only one pass through the DataFrame is needed and is pretty well-optimised in Pandas . The use of ` apply ` to join the strings is unavoidable here , but only one pass through the grouped values is needed .
To match each string in ` boughtSessions ` to the approach value in ` clicks [ ' session ']` you can use ` map ` . Unlike ` apply ` , ` map ` is fully vectorised and should be very fast : #CODE

New to Pandas , need to create a df from 2 others ( not a merge )
One cannot answer your question without merging the lists , and as you don't provide a foreign key linking them , the merge cannot be done . However , counting can be done quite easily with groupby : #CODE

hmmm . Well , two thoughts . 1 ) strip those -9999 out before loading them with bumpy . 2 ) Check out geoPandas . You can load spatial file directly into pandas which might save you the trouble of converting to ascii .

The first part of my question : is this the preferred way to merge the nltk output and my original data ( rating , object_id ) of each item ?
If I treat them separately : what would be the preferred way to merge them with the main dataset containing the ratings ? Do I prefix both vectorized matrices ' column names with tags_ and descr_ ? Or what would a better data structure be ?

Using apply ( or some other vectorisation ) to perform calculation involving two ( or more ) data frames ?
So , I'm not sure how to go about vectorising this . Naively I could split this into two apply statements , each effectively replacing each iterrows call . But is there some other clever approach , as there will still be significant looping overhead with that solution .
Are you just trying to merge on ' time ' ? See docs on ``` merge() ``` , you can do this in one line .

Pandas concat from excel tables with missing data
I am trying to concatenate a number of tables that I have read from excel and pivoted ( stacked ) using pandas . The code below works perfectly except the final dataframe ( GRL ) is missing the last two columns . Those columns correspond to columns in the input excel file that have several empty cells as the first entries . I have tried using concat on subsets of the data ( and test data ) and it seems to work , but not for the whole data set . It seems that the pd.concat is not taking a union on the datetime index where the column starts with missing data .

How to pivot categorical variable in pandas ?
How to pivot this data ? #CODE
I'm using pivot_table / pivot , but it always asking for numerical .
How to pivot categorical variables ? Thanks

Obviously , I can go over all rows of the DF and just merge the values . Which is very SLOW for large tables . I can also use .unique for columns A and B and iterate over all combinations , creating vectors col1_un and col2_un respectively , and then updating the relevant indexes in the table using something like #CODE

python apply function to list and return data frame
I am new to python . I wrote a function that returns a pandas data frame . I am trying to apply this function to a list and I would like to merge all the results to one data frame . For example , if my function looks like : #CODE
I want to apply it to list ` [ 1 , 2 , 3 , 4 , 5 ]` , and get the result as a data frame which looks like : #CODE

This works for me in pandas 0.12 . Can you check which part throws the error ? ( the ` to_datetime ` part , or the ` apply ` part )
Probably it is due to some missing values . If you use ` dropna ` before using ` to_datetime ` and ` apply ( ... strftime() )` , this will work . A small example : #CODE

My current solution is to use stack / unstack in the following way : #CODE
then you could use ` reorder_levels ` to avoid ( most of ) those stack / unstack calls : #CODE

Using append is something that will work for both cases I think : ` b.append ( pd.Series ( { ' new_col ' : 123} ))` . But , in any case , maybe you should also think about if this is necessary : adding an entry into a row ( which is a series ) of a dataframe seems not very efficient .

` import datetime as dt ` ` df [ ' DateTime '] = df [ ' TimeStamp '] .apply ( lambda x : dt.datetime.fromtimestamp ( x ))`
Fine , once you change your index to datetime values just do a resample : df = df.resample ( rule= ' 1M ' , how= ' mean ')

Yes . I want to apply that function in Time column of data frame .
One option is to use 3 ` str ` ` replace ` calls : #CODE
I think your problem maybe that you're not assigning the result of your ` apply ` back : #CODE

Your list l seem like a list of tuple AND list , so using MultiIndex , from_tuples() could maybe not work with the list inside l . Try map ( tuple , l ) , before calling index = pd.MultiIndex.from_tuples ( l )
` reindex ` doesn't just replace the index values ( though that sounds like what it should do , and the documentation isn't especially clear ) . Instead it goes through your new indices , picks the rows or columns that match the new indices , and puts ` NaN ` where no old index matches a new index . That's what's happening to you : when ` reindex ` hits ` [ ' Foo ' , ' B ']` , which doesn't exist in your original dataframe , it fills the column in the new dataframe with ` NaN ` .

Want to show all rows where there is a difference such as ( 3 , 4 , 2 ) vs . ( 3 , 5 , 2 ) from above example . I've tried using the pd.merge() thinking that if I use all columns as the key to join using outer join , I would end up with dataframe that would help me get what I want but it doesn't turn out that way .
Thanks to EdChum I was able to use a mask from a boolean diff as below but first had to make sure indexes were comparable . #CODE

append rows to a Pandas groupby object
I am trying to figure out the best way to insert the means back into a multi-indexed pandas dataframe .
Please note , I know I can do ` df.mean ( level=0 )` to get the level 0 group means as a separate dataframe . This is not exactly what I want -- I want to insert the group means as rows back into the group .
@USER I do not want to ` transform ` the data , just compute some aggregate statistics and insert back into the dataframe .
The main thing you need to do here is append your means to the main dataset . The main trick you need before doing that is just to conform the indexes ( with the ` reset_index() ` and ` set_index() ` so that after you append them they will be more or less lined up and ready to sort based on the same keys . #CODE

and then scale the count array to apply kde() to it ?

Python Pandas ix timestamp
I d like to use ix for slicing the dataframe but I need to enter just hours and minute in the index , not the exact value .

In this case you need to actually insert the string ` i ` into the query expression : #CODE
No . ` i ` is the * name * of a string variable , whose * value * will change from `' Type1 '` to `' Type2 '` etc . as you iterate over the tuple of strings . On each loop iteration you want to insert the * current value * of the variable ` i ` into the query string , hence ` df.query ( ' type == " %s "' % i ) for i in ( ' Type1 ' , ' Type2 ' , ..., )`

I can accomplish this using ` shift ` : #CODE

It is arguably better to map the input data dictionaries to have all lower-case keys before they are pulled into the ` DataFrame ` . This is especially true if the original data had inconsistently capitalized keys ( e.g. some " Mystery " and some " mystery ") . I know no magically simple way to do that . The easiest is probably subclassing ` dict ` along the lines of [ PEP 455 ] ( #URL ) . But this data set doesn't require that much extra .

Append a list of arrays as column to pandas Data Frame with same column indices
Based on your comments , it looks like you want to join your list of lists.I ' m assuming they are in list structure because ` array() ` is not a method in python . To do that you would do the following : #CODE

Lastly interpolate the NaN values linearly to get the final result #CODE

Note that ` np.array ( map ( int , s ))` is sufficient - it isn't required to build a ` list ` first ... Also , it's not exactly intuitive , but ` np.fromstring ( s , ' i1 ') - 48 ` is about 50% faster ...
One pandas method would be to call apply on the df column to perform the conversion : #CODE

Difference between stack / unstack , shape and pivot in Pandas

Try this .i believe you can use the append #CODE

One method , so long as datetime is already a datetime column is to apply ` datetime.strftime ` to get the string for the weekday : #CODE
It will be quicker to define a map of the weekday to String equivalent and call map on the weekday : #CODE
What is ` dt ` ? I get this error = ` AttributeError : ' Series ' object has no attribute ' dt '`
What version of pandas are you using ? the ` dt ` is a datetime attribute accessor that was added in ` 0.15.0 ` , also your series has to be a datetime dtype , if needed you can convert : ` df [ ' datetime '] = pd.to_datetime ( df [ ' datetime '])`
I added another example of how it should work . try : ` import datetime as dt

I think the idea here is to use ` pd.merge ` to join all required columns from the original data back into the grouped data frame .

Thanks ! Wonder why a series has a ` str ` and why calling ` len ` on this ` str ` returns False for empty lists .

Conform the index to another frequency . This makes every index element be the end-of-month here . #CODE
Then its straightforward to resample to another frequency . This gives you the number of business days in the period in this case . #CODE
Thanks Jeff ! I'm a bit confused . In the second chunk you are resampling and the result is a Series of monthly frequency so it would appear that the daily information is lost . Then you resample and somehow the days are there ? I'm sure I'm missing something obvious .

First , drop the ` NaN ` values using :

Or , actually , as the ` Freq : MS ` above shows , just ` freq= ' MS '` will do : #CODE

To resample this time series so as to get one value per day , you could use the ` resample ` method and aggregate the values by taking the mean : #CODE
If we were to resample the original data to daily frequency first and then

I have a dataframe that has columns ` country ` , ` date ` , and ` users ` ( i.e. number of users ) . I want to do a diff along the ` date ` column , but re-start the calculation for each ` country ` . How to do this ?
The diff should give this : #CODE
Sounds like a standard application of groupby and diff . Please refer to documentation on those and ask a more specific question ( including sample data ) if you have problems .
In your example , we want to group by the country column , then do a diff along the users column ( you say along the date column , but that doesn't seem to match your expected output ): #CODE

But my problem is I have so many NaN's and I'm not sure what the correct way to deal with them is as most ` scikit-learn ` methods give me errors because of them . One idea is to replace ' didn't attend ' with -1 and ' not invited ' with 0 but I'm worried this will alter the significance of events .
I actually came across something similar recently , one simple approach you might be interested is using sklearn's ' Imputer ' . As EdChum mentioned earlier , one idea is to replace with the mean on the axis . Other options include replacing with the median for example .
In this case , this will replace the NaN's with the mean across each axis ( for example let's impute by event so axis=1 ) . You then could round the cleaned data to make sure you get 0's and 1's .
I would plot a few histograms for the data , by event , to sanity check whether this preprocessing significantly changes your distribution - as we may be introducing too much of a bias by swapping so many values to the mean / mode / median along each axis .
Then replace your NaN numbers across each event column using random numbers generated from Bernoulli distribution which we fit with the ' p ' you estimated , roughly something like :
` p = 0.25 # estimated probability of each trial ( i.e. replace with what you get for attended / total )`
` # s now contains random a bunch of 1's and 0's you can replace your NaN values on each column with `

It's usually easier to work with these flat formats as much as you can and then pivot only at the end .
Ok , so this gets me most of the way there ... thanks !! It does raise some more questions at the end with how to pivot / filter / group the df_new dataset ( I'd like to group all items that have ' G ' anywhere in them ** as well as ** something else , separately group just the ' G's , etc . ) but hopefully I can do this using another group / agg as you suggest .

python pandas translate DF but got another Error
I want to use pandas to translate my CSV file by DF . Before this time , my CSV files are four files , not one . So this time I combine with four files and delete som columns , after this , I use my code to translate . But got an error .

I suspect there is an easier way than this , but this isn't too hard . Assuming your ' Date ' index is a datetime , you can just create a date range and append that . #CODE
FYI , I changed my method from merge-based to append-based . Your way looks like a simpler way to merge -- essentially doing an outer join implicitly -- but it didn't quite work when I tried it . I got all nans for amount ...

how to convert pair lists ( to matrix ) to heat map ; python
How to convert pair lists ( to matrix ) to heat map ?
I want to generate heat map for this data .

Use fillna to replace the mode only for the NaNs : #CODE

If you wanted just the row cound then perform ` len ( count )` or ` count.shape [ 0 ]`

You're looking for the ` drop() ` function , and there are 2 ways to drop rows , by name and by index . It's probably easiest to show by example : #CODE
Or you can drop rows using numerical indices of the index like this : #CODE
No need to drop just select the rows you're interested in : #CODE

I have a csv file with words and their tf-idf scores . I am writing a method to normalize the values ( to make them between 0 and 1 ) . I am using ` Pandas `

pandas chunksize - concat inside and outside the loops
I have to read massive csv files ( 500 million lines ) , and I tried to read them with pandas using the chunksize method , in order to reduce memory consumption . But I didn't understand the behaviour of the concat method and the option to read all the file and reduce memory . I'm adding some pseudocode in order to explain what I did so far .
Then I have to apply a function to the dataframe to create a new column based on some values : #CODE
So my doubt is whether the first method ( concatenate the dataframe just after the ` read_csv ` function ) is the best one , balancing speed and RAM consumption . And I'm also wondering how may I concat the chunks using the for loop .
Don't you need to concat the df with itself ? ` df = pd.concat ([ df , chunk ])` ?
Yes thanks ! So , should be better to concatenate the dataframe inside the loop instead of build the whole dataframe outside and then apply the function ? The RAM consumption is proportional to the size of the chunk , no matter the final dimension of the whole concatenate DataFrame ?

I'm building the skeleton of a larger application which relies on SQL Server to do much of the heavy lifting and passes data back to Pandas for consumption by the user / insertion into flat files or Excel . Thus far my code is able to insert a stored procedure and function into a database and execute both without issue . However when I try to run the code a second time , in the same database , the drop commands don't seem to work . Here are the various files and flow of code through them .
First , proc_drop.sql , which is to used to store the SQL drop commands . #CODE
I realize there are two kinds of drop statements in the file . Have tested a number of different iterations and none of the drop statements seem to work when executed by Python / SQL Alchemy but all work on their own when executed in SQL Management Studio
Next , the helper.py file in which I am storing helper functions . My drop SQL originally fed into the " deploy_procedures " function as a file and to be execute in the body of the function . I've since isolated the drop SQL reading / executing into another function for testing purposes only . #CODE
So it is the " deploy_procedures " function which is crashing when trying to create the function or stored procedure the second time the code is run . And as far as I can tell , this is because the drop SQL at the top of my explanation is never run .

As of now , to conserve memory I've taken the approach of creating a list of Series ( each column is a series , so ~ 15K-20K Series each containing ~250K rows ) . Then I create a SparseDataFrame containing every index within these series ( because as you notice , this is a large but not very dense dataset ) . The issue is this becomes extremely slow , and appending each column to the dataset takes several minutes . To overcome this I've tried batching the merges as well ( select a subset of the data , merge these to a DataFrame , which is then merged into my main DataFrame ) , but this approach is still too slow . Slow meaning it only processed ~4000 columns in a day , with each append causing subsequent appends to take longer as well .
One part which struck me as odd is why my column count of the main DataFrame affects the append speed . Because my main index already contains all entries it will ever see , I shouldn't have to lose time due to re-indexing .
I ended up using scipy sparse matrices and handling the indexing on my own for the time being . This results in appends at a constant rate of ~ 0.2 seconds which is acceptable ( versus Pandas taking ~150seconds for my full dataset per append ) . I'd love to know how to make Pandas match this speed .

I'd first replace the string values with their boolean equivalents calling ` replace ` , you can then use label indexing to select that row , generate a boolean series where the value equals ` True ` and use this to select the columns : #CODE
If you're worried about calling replace on the whole df then you can call ` replace ` just on that row : #CODE
This does achieve my goal . Didn't think of replace . In my case it seems safe to replace on the whole dataframe , is there a way to only replace the strings of one specific row , i.e. the row called ``' active '`` ?
@USER you can call ` replace ` just on that row by using ` loc ` to perform row label selection

Basically your problem was you had missing / ` NaN ` which are being treated as floats as ` NaN ` numeric values can be represented by float dtype . You could either replace them with an empty string or drop them

Yes , but take care if you use ` setattr ` , since if you call something like ` data.groupby ( ... ) .agg ( ... )` , the returned DataFrame may drop the metadata .

actually , loc and iloc both take a boolean array , in this case the ` mask ` . from now on you can reuse this mask and should be more efficient .

I'm trying to create this widget in Flask , to build a pivot table interface :

Note , I just found these parameters explained from another question on Stack that you can find here : How to increase node spacing for networkx.spring_layout .

What i failed to mention above is that prior to calling the to_hdf method , i was doing a pandas.concat to merge two files . The first file had ids that were all numeric , so the read_csv function must have inferred an int type for that dataframe . the second file had mixed int + string ids , so the read_csv must have inferred string or object . I would have thought that pd.concat would have converted the entire column to string , but i'm guessing not . to solve my issue , i do the following before calling to_hdf : #CODE

How to create predefined color range in Matplotlib heat map from a Pandas Dataframe
What I want to do is to create a heat map with the following condition :

And my data looks like this ( both before melt and after melt ) #CODE
I strip the date and then later did the following : ` df [ ' day '] = pd.to_datetime ( df [ ' day '])` but didn't get the actual dates to show up

Drop values below group means with Pandas DataFrame
How to drop values which are below the group means ( originally 20% of group means ) ? #CODE
The " dumb " way would be to cycle through the frame ( iterrows ) and compare one by one . There must be a smarter , Pandas way like using something like apply / join / whatever .
" drop values which are below the group means " Which group ? The first or second ?
Here's one way of doing it , using ` groupby ` and ` apply ` : #CODE

Also prefer the top level pandas ` isnull ` and ` notnull ` rather than numpy versions ` sum ( dftest.employment_total.isnull() )`

I want to be able to append a suffix to a variable name in pandas , and to have that suffix be able to change . In the line of code below , I want the mask variable name to be appended with var " lastyear " . #CODE
I think this is a little better , since the former computes the median of all the columns in ` df ` and then drops all the columns except ` continuousvar ` .
The latter computes the median only for ` continuousvar ` , so it should be a little faster .

Make a df of all possible combinations then left merge your data to that combination df . #CODE

I'm using pandas and trying to fill in the NewManagerId column on the example table below . I've already done a merge and have columns 1-4 and need to append column 5 , the new manager ID . The OldManagerId = OldId but then use the NewId value to populate . I've tried a few different approaches and haven't gotten this to work , but I would guess there is a way to do it . Any advice appreciated ! #CODE
please include a sample of the two data frames you are looking to merge .
Create a lookup df on a subsection of your df , what we do here is just select the OldId and NewId columns , set the index to OldId . We then call ` map ` on the df OldManagerId values and this will perform a lookup for the corresponding NewId and set this value : #CODE

s , f = list ( next ( zipped )) , map ( float , next ( zipped ))
Do you have header and is your data exactly as you have provided ? Try removing the map float and see if you see any values that would casue an error
I have tested it with 3000 lines of similar data and get no error , try removing the map call and see if you see any strange output
When I removed the map I got this : ` ValueError : too many values to unpack

Trouble passing in lambda to apply for pandas DataFrame
I'm trying to apply a function to all rows of a pandas DataFrame ( actually just one column in that DataFrame )
Can you show ` df.info()` and which columns you are trying to perform the calculations on , you will not be able to pass 2 columns row-wise if you are calling apply on a series

How to merge multiple CSV files using Python Pandas
I'm wondering how to merge multiple CSV files using Pandas , but using two specific criteria :
I don't want values to be merged if they have a common key . As in , I don't want data to be merged as it would via a SQL Join . I want all raw data to show as it does in the original CSV file
I want the CSV file values to be merged as new columns , and not as it does in the append function , where it puts the values underneath the first grouping

I am very new to programming and am working with Python . For a work project I am trying to read several .csv files , convert them to data frames , concatenate some of the fields into one for a column header , and then append all of the dataframes into one big DataFrame . I have searched extensively in StackOverflow as well as in other resources but I have not been able to find an answer . Here is the code I have thus far along with some abbreviated output : #CODE
I am unable to apply the last operation across all of the DataFrames . It seems I can only get it to apply to the last DataFrame in my list . Once I get past this point I will have to append all of the DataFrames to form one large DataFrame .

Unfortunately I am not able to convert the provided ` bool ` values to ` 0 ` and ` 1 ` and sum them . #CODE
You could perform a list comprehension , looping over the columns and then use a boolean condition on that column , drop the values that don't meet the condition and call count : #CODE

I need to create an index on a specific frequency , however I need to apply that frequency only for certain months . This is the frequency : #CODE

Pandas : how to merge 2 dataframes on key1.str.endswith ( key2 )
i want to find the best way to merge 2 dataframes on key1.str.endswith ( key2 ) , an example is sometimes better than words : #CODE
The last bit is to call map on this on the other df after setting the index on the color column , this will perform a lookup on the color value in df and return the corresponding code .
The str method is over 2X faster than using the lambda , this may not be so surprising as the ` str ` methods are vectorised as is calling ` map ` .
So @USER ' s answer is marginally faster than @USER beaveau's but using map here is faster still .
In fact if we combine @USER ' s regex ` str ` method with map we get faster than my original method : #CODE
So using ` map ` here is nearly 2X faster than merging
Yes just noticed ! Will just update the regex . The idea was just to use merge rather than having a very long line ;)
Then ` pd.merge ( df1 , df2 )` will merge on the common column ( s ) which in this case is the ` color ` column : #CODE

Ideally I would have used ` pandas.period_range() ` , but since it doesn't accept multiple in the ` freq ` parameter , I turned to ` pandas.date_range() ` . I'm iterating over the timestamps in the date range like so : #CODE
This seems like the desired result . But in the case where the ` freq ` is longer than the time between start and end , it returns a range with 0 elements . #CODE
The only time I would see the result being empty is if the ` freq ` has a multiplier of 0 ( ie " 0D " , " 0H " , " 0W ") . This already raises an error , so in ` practice date_range() ` should always return at least it's start value . If the end parameter occurs earlier than the end of the frequency , then the range would only contain one timestamp .

As in the title , I have more than 800 data files ( all in .csv ) ( each with size ~ 0-5MB , and each file contains 10 columns with 1st row being the header ) and I want to combine all of them into a single DataFrame . I can append them one by one using Pandas data frame but it is very time consuming .
and what is the merge supposed to accomplish ? Are you just appending new rows ? Not all merges can be replaced by ` pd.concat ` ...

Using trunk pandas , it took me 17 min to write a 10G csv file from a frame of that shape ; it took 8 minutes to write the transpose ( more rows , fewer columns ); and only 30s to write to HD5 . So I'm not seeing the pathology , but I wouldn't use csv for such a big file anyway .

To which I want to join a df that contains VALUE with is only date specific . So I do #CODE
I suppose the issue is that VALUE does not have an upper level . A similar operation is not described in the manual . Isn't there a way to apply ` stack ` only to some columns ?
Just don't unstack in the first place : #CODE

Pivot and plot data
Disclaimer : I had posted a related question previously , where the trick suggested ( don't unstack at all for the join ) was useful for that part , but in the end I actually want to unstack for various reasons ( including plotting ) .
My attempts to get this were partially described in the other question , but one way in particular I was trying was ` pivot ` :
Attempt : Pivot var2 , then merge again #CODE
Here's my attempt to merge afterwards : #CODE
What if you just changed the index from date / status to date / var1 / status ? Or alternatively just do a merge after your pivot to bring back the missing column ?
Yeah , they are just workarounds . I can't really think of a way to make pivot do what you want . I suspect you might have better luck using stack / unstack than pivot though .
@USER I managed the workaround , but it took me some time : I needed to ( i ) recast the column as a data frame , ( ii ) use join , not merge / append .
Having seen your original question , I think you'd be better off doing the join after the unstacking . Take ` df ` from your original question , unstack it along the ` status ` level , then select everything from the top-level column `" var "` , and then do the join after that's done . #CODE

I have a large pickled Sparse DataFrame that I generated , but since it was too big to hold in memory , I had to incrementally append as it was generated , as follows : #CODE
don't concat in a loop ! This is a note in the docs , maybe should be a warning #CODE
The idiom is to append to a list , then do a single concat all at the end .
Thanks , but you can't append to that file , right ?
you cannot append to a single mode , but you could simply write nodes as needed and just append them when u read it out

Essentially I want to find the intersection of a line and a data series in pandas . Is there a best way to do this ?
to interpolate , there are many other choices ( e.g. quadratic or cubic

And then as the index , only ` date ` . Since ` superDuper ` is not specific to the ` txxxx ` columns , this probably implies repeating that column . ` pivot ` doesn't get me there , and since it is not a sublayer , ` unstack() ` probably doesn't do the trick either . What is an approach I could try ?
Pop out ` superDuper ` and save for later . Now you can use unstack . #CODE
Now just join and sort : #CODE

Python Pandas replace NaN in one column with value from corresponding row of second column
I need to replace all NaNs in the ` Temp_Rating ` column with the value from the ` Farheit ` column .
If I do a Boolean selection , I can pick out only one of these columns at a time . The problem is if I then try to join them , I am not able to do this while preserving the correct order .
How can I only find ` Temp_Rating ` rows with the ` NaN ` s and replace them with the value in the same row of the ` Farheit ` column ?
First replace any ` NaN ` values with the corresponding value of ` df.Farheit ` . Delete the `' Farheit '` column . Then rename the columns . Here's the resulting ` DataFrame ` :

how to apply ceiling to pandas DateTime

Resample Length Mismatch Error
I get an error when I try to resample the following time series . It seems it fails only with ' MS ' resampling . If I add one more row to the time series it goes through w / o any problems . Can figure out what's wrong here :( #CODE

I'm trying to set up a Google Compute Engine server to pull options data using Python Pandas . When I make this request from my Mac at home , I only have problems late at night when Yahoo ! is resetting its servers ( the data is being pulled from Yahoo ! Finance ) . But when I try doing the same thing from my Compute Engine server , the request always fails for some of the stocks I'm interested in , although it typically works for options on larger companies , such as ' aapl ' or ' ge ' . On my computer at home , running it at the same time , the same requests succeed for both small and large companies .
I'm beginning to think that this is an Ubuntu issue . Trying it on an Ubuntu box at home , I get the same behavior as I am on Google Compute Engine , whereas it works on my Mac and works for ' aapl ' or ' ge ' on Ubuntu . Just not sure why Ubuntu would be rejecting the smaller companies ? Maybe higher latencies on the Yahoo end ? Maybe Yahoo routes it differently in some relevant way ?

First , you sort it using a MultiIndex , then you unstack it so that each entry in the Name column becomes its own column . Then you select the Rm_Log column and plot it . The argument ` rot=90 ` rotates the xticks . You could also separate this into several lines , but I thought I'd keep it as one to show how it could be done without modifying the DataFrame .

Pandas Join results in keyerror on index column
Join results in keyerror on index column #CODE
Which works , but not in the way I'd like it to . What am I missing on the join statement ? I have played with ' how= ' and other parameters without success .
The code you have working joins different dataframes ( ` df_catagories and ` df_keywordsplus `) . Can you confirm that ` concat ` works on the dataframes that are producing an error when joined ( ` df_all ` and ` df_addresses `) ?
Yes , concat works , join doesn't . No clue why . I can concat one table at a time , but can't join them . Doesn't seem to be a memory issue . I'm not getting above 60% of my 8G of ram .
. I've found that on= doesn't make a difference in his case , but that th the join does occasionally pick up the wrong field , so I try to be explicit .
This works : # Load Authors and Addresses - only 1 index ' ISI_LOC ' to join
# Join Tables

I was considering this as well , but I need to slice several dataframes in the same way , so I would need to do it multiple times . I was looking to have the function take the locationargument and apply it to all dataframes in one call . Thanks !

I will get different groups that do not coincide . Is there some way to get the ` datetime ` related groups from the first grouping , and apply them to the second grouping instead ? Or how could I achieve this else wise ?

EDIT : Also , the ` concat ` call is taking much longer in my real code than my toy example ; in particular the ` get_result ` function takes a lot longer despite the production df having fewer rows and I can't figure out why . Any advice on how to speed this up would be appreciated .
I'm a little confused at exactly what your dataframe should look like , but it's easy to speed this up a lot with a general technique . Basically for pandas / numpy speed you want to avoid ` for ` and any ` concat / merge / join / append ` , if possible .
Two short follow-up questions : 1 ) if I'm starting with a dataframe ` df ` already supplied to me , is it reasonable to construct a numpy array for the columns I want to add , and then concat that to ` df.values ` at the numpy level ? Or should I create an entirely new numpy array , write ` df.values ` into it , and then write my new column into it ?

You can also use the ` dt ` accessor : ` mydf.DateTime.dt.date `

Looking at the docs , the only thing coming close appears to be the parameter ` colormap ` - I would need to ensure that the ` x ` th color in the color map is always black , where ` x ` is the order of ` foobar ` in the data frame . Seems to be more complicated than it should be , also this wouldn't make it bold .

this won't work because you are calling apply on the df , naturally this will iterate over the columns and you are trying to check the probability column it's unclear to me what you are trying to do here , are you checking just the probability column or all columns ?

` datetime64 [ ns ]` is a general dtype , while ` M 8[ ns ]` is a specific dtype . General dtypes map to specific dtypes , but may be different from one installation of NumPy to the next .

So it would be great if there's some option to do the job with a * .csv with two rows . On the left all " Industry Category " items and on the right the desired " Parent Category " I like to apply to the dataset .
@USER would you expect ` replace ` to be faster than ` map ` ? If the second csv was just a lookup and you set the index to be the ' Industry Category ` I would expect ` map ` to be faster
@USER I have no real grasp on the relative time differences between ` map ` and ` replace ` unfortunately .
I think I got the idea . I have to create a csv and apply the the2nd step of this [ link ] ( #URL ) . Once I have the dict , I have to map the df with .map ( category_list.get )
@USER I think you can either create a dict or just create a df but in the latter the index needs to be the ' Industry Category ' , you can use ` map ` only if the keys are unique which is true for dict but for a df this needs to be true for your csv data , I expect ` map ` to be the fastest method based on personal experience
I do this quite a lot . I would create a dictionary and use ` apply ` and ` lambda ` . #CODE

I am trying to read a certain DF from file and add to it two more columns containing , say , the year and the week from other columns in DF . When i apply the code to generate a single new column , all works great . But when there are few columns to be created , the change does not apply . Specifically , new columns are created but their values are not what they are supposed to be .
it has to do with not changing all data values , but i don't understand why the change does not apply - after all , before the second iteration begins , the DF seems to be updated and then ` tbl [ tmp_col_name ] = ' No Week '` for the second iteration " deletes " the changes made in the first iteration , but only partially - it leaves the new column created but filled with ' No Week ' values ...
You are performing [ chained indexing ] ( #URL ) which may or may not work , in your case it doesn't work you need to use the new ` loc ` , ` iloc ` or ` ix ` accessors to set the data .
Many thanks to @USER ! Performing chained indexing may or may not work . In case of creating new multiple columns and then filling in only some of their values , it doesn't work . More precise , it does work but only on the last updated column . Using loc , iloc or ix accessors to set the data works . In case of the above code , to make it work , one needs to cast the tbl_ind into np.array , using ` tbl [ col_name [ j ]] .iloc [ np.array ( tbl_ind )] = tmp.apply ( lambda x : x.year )`

My guess for why this is so slow is at least partly because ' append ' creates a new object every time it's called , and it gets called a LOT . In total , the nested loop from the above code gets run 1595653 times .

Use the ` axis ` parameter in ` diff ` : #CODE

Assign the results to a new dataframe ( temp2 ) , and then insert a new record that sums any remaining items in the list . It also identifies the number of unique items remaining . #CODE

So long as the indices match then this will align correctly .

I've been using the Categorical function to replace categorical values with numerical ones . #CODE

On second thought , instead of chunksize , just read in the first row and count the number of columns , then read and append everything with the correct number of columns . In short : #CODE
@USER R @USER I added some more to the answer , maybe that will help . I have to admit I don't understand your concern about ``` nrows ``` . As you constructed the example , it really doesn't matter , and I can't anticipate what problem you are foreseening on some other dataset . Note that the test based on ``` len ( test.columns )``` is just one way you might test things . I thought it was the simplest way for your example but of course you could use other types of tests as conditions dictate . You could post a new question with a different example dataset if you have something specific in mind .

No . For example , ` Series ` objects have an ` interpolate ` method which isn't available in PySpark ` Column ` objects . There are many many methods and functions that are in the pandas API that are not in the PySpark API .

Please put error messages in code blocks instead of posting an image of the stack trace . Images are not indexed , therefore it is hard for others to find your question .

I'm struggling to find a way to iterate over Df , and for each row , apply a definition that iterates to search for the nearest match in Df1 ( with the aim to add data from Df1 to Df ) . Read and tried a lot of methods found here , but not winning . Would appreciate some pointers , especially if I'm going the wrong route :
It seems like this sort of question comes up a lot and there isn't a really simple solution in pandas . If you search SO on " nearest match merge join " you will find a few workarounds though .
If you know SQL , it might be easiest to merge that way , since the ``` on ``` in ``` inner join ``` will allow for more flexibility than the ``` on ``` in pandas ``` merge ``` .

Yes you can select the columns to be used with the ` x ` and ` y ` keyword arguments . You also select the kind of plot you want , in this case a hist , using ` kind= ' bar '` . #CODE

So I'm trying to offset or shift a part of my dataframe by 2 hours , but I can't seem to get it to work with my following code : #CODE

I don't think so it's easy enough to add comments and append that this seems unnecessary , also you'd have a support issue with some people maybe wanting different comment markers , multi-line support etc ..

I have a pandas.Series object with a hierarchical index consisting of two levels : ( code , date ) . I also have a map { date -> code} . I'd like to get a Series indexed by date only , such that for each date the code is looked up in the provided map and then the pair ( code , date ) is looked up in the original Series . What's the best way to achieve this in Pandas ?

You can use ` apply ` to call a lambda function that splits the string and then joins on the unique values : #CODE

here ' a question on transforming a DataFrame . After creating a DataFrame from a pivot table and applying .unstack() , I came up with a table like this #CODE
Also , it would be good to know how to rename the index " Date " as I have to merge two tables where in one case the time is named " Date " and in the other " Close Date " .

enables a pure label-based slicing paradigm that makes [ ] , ix , loc for

Basically we can drop the ` NaN ` rows first and then call ` apply ` and use ` datetime.strftime ` to apply a new format : #CODE
One way would be to use a regular expression ( ` re ` module ) to get search for a known pattern and replace it with the desired output .

Key error on pandas merge ( left join )
I am trying to join df_logins to df_purchase on age , gender and region with the following pandas code : #CODE
There are no common ages in the two tables . There is therefore no intersection between the tables .

passing the level resulted in this error : TypeError : Join on level between two MultiIndex objects is ambiguous

I'm working in pandas doing pivot tables and when doing the groupby ( to count distinct observations )
` aggfunc={ " person " : {lambda x : len ( x.unique() ) }} ` gives me the following error :
Rather than removing duplicates during the pivot table process , use the ` df.drop_duplicates() ` function to selectively drop duplicates .

( ii ) ` bool `
The main types stored in pandas objects are float , int , bool ,

did you want to append only the value of fourth column ?
pandas syntax in the middle choosing the right rows ; ` join ` all the words-colunn values together , ` split ` them apart into separate words .

Pandas How to Replace ? with NaN - handling non standard missing values
I am new to pandas , I am trying to load the csv in Dataframe . My data has missing values represented as ? , and I am trying to replace it with standard Missing values - NaN
@USER I am using this dataset #URL , I am working for an hw assignment for my Course on Data Mining . Yeah thats my concern that read_csv dint work . I think I need to use regex as there may be spaces around ? -- I got it working by using replace with regex as below .
Numpy - Replace a number with NaN #CODE
You can replace this just for that column using ` replace ` : #CODE

Map an if statement in Python
I'm trying to map the following function over a pandas dataframe ( basically a list ) in python 2.7 : #CODE
But python errors saying using a lambda function like this is a syntax error . Is there some way to map an if statement like this in python ?

How do I use pandas to add a calculated column in a pivot table ?
I did the following to add a calculated field ( column ) in the pivot table
This is because you have a Hierarchical Index ( i.e. MultiIndex ) as columns in your ' pivot table ' dataframe .

With some googling and playing around the data probably i found that this method may normalizing data on Row basis and I want to normalize data with column basis .
Can you post what the final df values should be and give examples of the calculations you are trying to apply , thanks
You could drop the ` NaN ` values and compare the output no ?
Next , in order to normalize your data on a column basis , your method is actually correct : #CODE
By default , sklearn will normalize your variables by feature ( column ) ; to normalize by sample ( row ) you can add ' axis = 1 ' to the arguments of the scale function . However , doubt you would ever want to do that .
i have a small doubt on normalization say i have one class variable " activity " and it has two values " active " and " inactive " what is the best way to normalize 1 ) should i normalize both " active " and " inactive " data separately then combined both for machine learning or 2 ) i can do normalization on combined data . thanks
@USER , unfortunately I think there is no general answer that I know of for which option to opt for . It really depends on both your data and the specific machine learning / statistical algorithm you would like to apply . However ; I would generally suggest opting for 1 ) as base case unless you have specific concerns about correlations between your features and how your ML algorithm deals with that .
With your example , I would imagine " active " and " inactive " data are ( hopefully ) not going to be significantly correlated and so I would just normalize each separately with sklearn's scale function

Also , on another front , you might try to use an ordered map , since you say the prices are all unique . Then you could just look at one end of the map ( depending on how it's sorted ) to pull off the minimum key / value pairs . I'm assuming , of course , that the implementation of that map is well done , but if it's in your standard library , it probably is .

a sequence of values to map onto the current colormap .

How could I transpose table and change index in this way ?

Python pandas : split and append ?
One method is to apply a function to your df to split the ' cc ' column and create a new dict that contains each split country and their associated count , you can then construct a new df from this , groupby the country and perform the sum on the count : #CODE

I tried to use pivot like this : #CODE
The ` item ` index in your dataframe is a red-herring in the sense that the ` item ` s in the input have nothing to do with the ` item ` s in the desired output . The ` item ` values you would need to make pivot work look like this : #CODE
Once you have the right values in the ` item ` column , the ` pivot ` call follows naturally : #CODE
assumes that the ` site_num ` s cycle through ` 0 , 1 , 2 ` and that ` len ( x [ ' site_num '])` is a multiple of 3 . The ` groupby / cumcount / pivot ` method will work even if ` len ( x [ ' site_num '])` is not a multiple of 3 by adding NaNs as necessary .

Drop Rows by Multiple Column Criteria in DataFrame
I have a pandas dataframe that I'm trying to drop rows based on a criteria across select columns . If the values in these select columns are zero , the rows should be dropped . Here is an example . #CODE
This doesn't drop the rows , so I tried : #CODE
What you want to do is generate the boolean mask , then drop the ` NaN ` rows and pass ` thresh=1 ` so that there must be at least a single non- ` NaN ` value in that row , we can then use ` loc ` and use the index of this to get the desired df : #CODE

the example is very similar to yours in my opinion . In the answer below , you're still assigning with a copy -- ` loc ` in on the wrong side in other words . See my comment on that post .

This is the out come of the above code : Now how can i verify wither my data has been normalize by columns and has mean = 0 and variance = 1 or not . #CODE

Python PANDAS : New Column , Apply Unique Value To All Rows
My confusion seems to be about which function to utilize ( apply , mapapply , etc . ) , if I need to reference an index value in the original df to apply a completely unrelated value to the initial df , and the most optimized , pythonic way to accomplish this .
Just append .date() at the end : pd.Timestamp ( dt.datetime.now() ) .to_period ( ' M ') .to_timestamp ( ' M ') .date()
` monthrange ` returns a tuple with the first and last days of the month , so ` [ 1 ]` references the last day of the month . You can also use @USER ' s method for finding the date of the last day , and assign it directly to the column instead of ` apply ` ing it .

The second arg is the list of values to replace with . The value at index=0 replaces where values == 0 , etc .

You can of course implement such a disk-based external sorting using multiple steps : Load a chunk of your dataset , sort it , save the sorted version . Repeat . Load a part of each sorted subset , join them into one DataFrame and sort it You'll have to be careful here on how much t oload from each source . For example , if your 1000 element dataset is already sorted , getting the top 10 results from each of the 10 subsets won't get you the correct top 100 . It will , however , give you the correct top 10 .

How to merge rows in DataFrame according to unique elements and get averages ?

Get subset of pandas DataFrame using dynamic expression including the shift method
I tried using the query method like this ` values = df.query ( condition )` , but I think the ` shift ` is causing it to fail because I get this error : ` NotImplementedError : ' Call ' nodes are not implemented `
Break this up into column + shift ( here the columns are all one-letter so I did the simplest thing possible , which isn't very robust , but switching to something more clever is trivial ): #CODE

I want to use the Google API to get the " location " s latitude and longitude in CSV file , and I can get ' lat ' , ' lng ' with the Google API Module . But I can not save the file back to the original file and insert behind " location "
And I want to insert the ' lat ' and ' lng ' columns after ' location ' , like this : #CODE

Performance : Python pandas DataFrame.to_csv append becomes gradually slower
I'm looping through a couple of thousand pickle files with Python Pandas DataFrames in it which vary in the number of rows ( between aprox . 600 and 1300 ) but not in the number of collumns ( 636 to be exact ) . Then I transform them ( exactly the same tranformations to each ) and append them to a csv file using the ` DataFrame.to_csv() ` method .
Does ` pandas.DataFrames.to_csv() ` append mode load the whole file each time it writes to it ?
2 ) drop the columns you do not need after the normalization . #CODE
But I'm checking , and even with the garbage collection execution time grows , but when I remove the ` to_csv() ` block , keeping the rest of the code intact , it's constant . It must have something to do with the ` pandas.DataFrame.to_csv() ` append method . but I tried reading the code but this python is to advanced for my experience with it .
As for the ` to_csv() ` issue , I have to admit that I am having trouble finding what could cause this , as I use it quite often . At is core it use the [ ` csv `] ( #URL ) module of python's standard library ( with a [ chunk ] ( #URL ) division and a little bit of [ cython ] ( #URL ) to speed things up ) . Maybe there's an issue with big files and windows ? Did you try to save into different files rather than append ?
Columns are fixed I would extract the column calculations and vectorize the real , child and other normalizations . Use apply rather than iterating ( for zfill ) . #CODE
Each time the file is opened before it can append to , it needs to seek to the end before writing , it could be that this is the expensive ( I don't see why this should be that bad , but keeping it open removes the need to do this ) .
Looks like it did in first test @USER . I'm doing some more tests for confirmation so I can apply the bounty with a fair judgement .

You could define a function and call ` apply ` passing the function name , this will create a df with min and max as the index names : #CODE
If you insist on a list of lists we can transpose the df and convert the values to a list : #CODE
Note also that you can use ` describe ` and ` loc ` to get what you want : #CODE

Python pandas pivot not uniqe index
what I would like to do is find the sum for ' numbers ' for every ' category ' and every month . I thought the best option would be to do a pivot table something like #CODE
I believe you want pivot_table instead of pivot

What is the best way for me to get this data into Pandas ? Is there a standard way I could use ` read_table ` or some similar function to read this file directly ? Should I write a script to insert commas where all the column breaks are and then read it in as CSV ? ( I'd just do the latter , but I'm also interested in becoming better with Pandas so if there's an out-of-the-box way I'd like to know it . )

Also I did minMax function to get minmax values of each column and insert them in a list #CODE

Python pandas : merge loses categorical columns
Now you can merge as usual since there is no trouble merging integer valued columns .
I may be missing what you goal is here , but the intent is to have the user convert to category ( or not ) if needed . I think in this particular case this could be done automagically . To be honest the categorical conversions would be done at the end anyhow , so this is not really going to save you anything ( by doing it inside merge ) . #CODE

I've got a loop to merge the files and it works well for the three firsts . Here is my call : #CODE
I tried the solution for a similar issue in Pandas Merge ( pd.merge ) How to set the index and join to no avail .
Well when you merge on keys if any of the other row values don ; t match then this creates a clash and this appends new columns with suffixes ` _x ` and ` _y ` , you need to decide what to do in this case
I would like to merge the ipr files using the first and second columns ( renamed " IPR " and " description " by the script ) . The third entry in these files is the abundance in a particular sample . I would like to obtain something like IPR_abundance3.tsv with abundances from the fourth file added . Ultimately , I should be able to give a list with any number of files and merged them together , inserting NaN if the IPR / description is absent from a number of the files .

Unless I'm missing something , all you need to do is build a mask and shift it : #CODE

@USER No . That way , it doesn't merge any possible duplicates in col5 .

How can I replace the 27 with just 01 without altering anything else ? I have tried things such as : #CODE
Use the datetime attribute to filter on date and then ` apply ` a function to replace just the day component : #CODE

I've found apply and map great for speeding up calculations on particular rows in a DataFrame .
The question I've got is : Is it possible to return a value with apply or map functions which refer to a previous row ?
Apply and map are great at vectorising row by row calculations - is it possible to refer to the previous row so I can make that calculation ?
look at [ ` shift `] ( #URL )

String wildcard in pandas on replace function
I'm sure this problem has an easy answer but I'm having trouble figuring out the correct string to use . I basically want to replace any email address in a data frame with the new domain . For a specific column , replace the substring ' @* ' where * is any set of characters with ' @USER .com ' . I want to keep whatever comes prior to the @ as is .
You can use the vectorised ` str ` method to split on `' @ '` character and then join the left side with the new domain name : #CODE
another method is to call the vectorised ` replace ` which accepts a regex as a pattern on the strings : #CODE
This sounds like a job for regex ! Pandas ' ` replace ` will let you use regular expressions , you just have to set it to true . You're most of the way there , the following should work for you .

Because you are building a model where each feature is forced to be binary , I guess my question is why must this be so in your example ? I.E. , why cant feature ` c_0348 ` map to ` [ 0 , 1 , 2 ]` vs ` c_0348f =[ 1 , 0 ]` , ` c_0348k =[ 1 , 0 ]` , ` c_034 8p =[ 1 , 0 ]` ?

How to get the intersection of two dataframes ?
How do I get this intersection ?
You can just perform a ` merge ` , this will use all columns and the default type of merge is ` inner ` so values must be present in both dfs : #CODE

My second suggestion to specify this in the groupby call with ` as_index=False ` , seems not to work as desired in this case with ` apply ` ( but it does work when using ` aggregate `)

Replace #CODE

Apply function row wise on pandas data frame on columns with numerical values
To apply an arbitrary function , ` func ` , to each row : #CODE

1 ) Is the for loop efficient ? Can I replace with better code even though it is working as is ?
2 ) Try ` frame3 [ " name_len "] = frame3 [ " name "] .map ( lambda x : len ( re.findall ( ' [ a-zA-Z ]' , x )))`

I then normalize the name as such by replacing the space with " # " and add trailing " ? " to standardize the total length of the name entries . I would like to use sequential three-letter substring as my feature to predict ethnicity . #CODE

So you need to replace that duff value and then you can just call map to perform the lookup on the ` NaN ` values : #CODE
So you see here that apply as it is iterating row-wise scales poorly compared to the other two methods which are vectorised but ` map ` is still the fastest .
You'll probably find that calling map like this will be the fastest method

I would like to have the company symbols in their own seperate column instead of inside the Company Name column . Right now I just have it iterate over the company names , and a RE pulls the symbols , puts it into a list , and then I apply it to the new column , but I'm wondering if there is a cleaner / easier way .
I'm new to the whole map reduce lambda stuff . #CODE

I could .. after the concat , reset_index , then drop the ' id ' column then set the index to the pandas generated index ( which should be values 1 .. n ) . then i'm thinking i do a dataframe.apply to find parent by ' date ' and ' level=0 ' and set parent accordingly . Would that be the most efficient way ?

but it will be faster to use the inbuilt ` to_datetime ` rather than call ` apply ` which essentially just loops over your series .

Using apply will be substantially slower than your first method by the way
Not at the moment , cumsum is a vectorised method apply will not beat this .
Here's a one-liner , but with cumsum : ``` ( df.Volume * ( df.High + df.Low + df.Close ) / 3 ) .cumsum() / df.Volume.cumsum() ``` . As @USER notes , cumsum is going to beat apply . I doubt you're going to improve speed by avoiding cumsum in pandas . Why do you want to avoid cumsum ? Beyond this , I'd guess you can improve speed slightly by doing in numpy , and even more by doing in numba .

Are you looking for something like ` df.groupby ( ' col1 ') .apply ( lambda x : pd.Series.count ( x ) / len ( df ))` ?
But isn't there a way to divide by sum from groupby operation as opposed to len of DF ? Also when I tried that statement I get ` unbound method count() must be called with Series instance as first argument ( got DataFrame instance instead )`
that is great but the only issue I believe is that if there were nulls I would have to write something a little verbose like this : ` df.dropna ( subset =[ ' A ']) .groupby ( ' A ') .size() / len ( df.dropna ( subset =[ ' A ']))`

Python Pandas replace values by their opposite sign
I am trying to " clean " some data . I have values which are negative , which they cannot be . And I would like to replace all values that are negative to their corresponding positive values . #CODE
As of now I have just begun writing the replace statement #CODE
Just call ` abs ` : #CODE
Another method would be to create a boolean mask , drop the ` NaN ` rows , call ` loc ` on the index and assign the negative values : #CODE

Are you saying that all row values are the same ? You could load the csv in chunks and then compare ` value_counts() == len ( df )`

But I can confirm this was indeed not working in pandas 0.15.2 . If can upgrade to pandas 0.16 , I would also advice to explicitly use ` loc ` in this case ( so it certainly does not fall back to the positional integer location ) . But note the bug is also in ` loc ` below pandas 0.16

Use Shift .

Why are you just taking the first element of either dataset and comparing each ( presumably ) character for the length of it , eg : in ` for i in range ( 0 , len ( train [ 0 ]))` - surely ` len ( train [ 0 ])` is going to the number of columns ?

Use ` shift ` to compute the inter-row criterion that you want to distinguish .

Once I have the data in the combined_list variable , I want to be able to pass it to a for loop and iterate over each line and insert it into my SQLite db if the date does not exist or update the existing record if the date is found .
I have my separate lists and I have the database insert working . What I'm not sure about is the proper way to combine the lists . Am I looking for a pandas dataframe ? Or something else ?

Yeah that comment was mine . I'm still a little confused . What I have above does pivot the Status column only . I can use pivot because I think you want Trial and Time and the index . Correct ?

You might also replace ``` sep ``` with ``` delim_whitespace=True ``` as Bob did in his answer . I believe that is more efficient and could matter if reading a large file . You could also try using ``` read_fwf ``` or ``` read_table ``` instead of ``` read_csv ``` . I don't know if that would matter , but you could try it . Sometimes with big files you just have to experiment and see what works best for your situation .

Now , suppose I have multiple data frames outside the function , say ` df1 ` , ` df2 ` , ` df3 ` on which I want to apply ` myfunc ` . I want to ensure that the function ` myfunc ` uses a local copy of the dataframe - I want to avoid the situation that the ` df1 ` outside the function doesn't get changed / modified by the operations inside the dataframe .

Apply Across Dynamic Number of Columns
You can do this with a for loop . To demonstrate , I generate a DataFrame with some random values . I then insert a ` lookup_key ` column in the front with some random integers . Finally , I generate ` lkup_dict ` dictionary with some random values . #CODE

I am trying to drop rows where Tenant is missing , however isnull option does not recognize the missing values . #CODE
The column has data type " Object " . What is happening in this case ? How can I drop records where Tenant is missing ?
Now I replace any empty strings in the ` Tenants ` column with ` np.nan ` objects , like so : #CODE
Now I can drop the null values : #CODE

There's no error if I replace both functions above with ` .count() ` .

Python Pandas merge two data frames based on multiple values field
I think a nicer , more general way , to do this is using stack / unstack : #CODE
Now you can either merge , or if you're lucky just switch out the index : #CODE
Note : If you have duplicates ( A , B , Cs ) you're going to need to merge ( which is a little fiddlier , but could be cleaned up ) . Before [ 21 ]: #CODE

You can use ` DataFrame.interpolate ` with the `" time "` method after a ` resample ` . ( It won't give quite the numbers you gave , because there are only 30 days between 2 Nov and 2 Dec , not 31 ): #CODE

Or drop last ` .any() ` to see all columns which have text and which don't : #CODE

I just want to apologies to not thank you Mr JohnE for your response . I have put a similar question in the cross validated stack exchange with the hope that someone will find a solution from the perspective of machine learning way of using such structure in a suggested algorithm . Thank you for trying help , and sorry again .

In order to solve this big problem , I find read_fwf in pandas module and apply it but failed . #CODE
Oh my pandas god !! I understand that read_fwf change the text file to DataFrame after your answer !!! In my Korean book , there is no explain about Ix or lox method , but , after your visualized explain , I can understand about pandas easily !! Thank you so much !!!
May I ask one more question ?. My purpose is , by taking out the data frame ` ( .ix )` from original total data frame , to construct other new data frame via concat or append . [ link ] ( #URL ) . After running this file , ` frame ` in code is empty . Should I find another way ?
Not sure I totally understand the question . But , many pandas operations create copies of the data . In this case , ` df = df.ix [ 2:6 ]` does replace existing data . But you could just as well say ` dnew = df.ix [ 2:6 ]` . Then you would have ` dnew ` with just the desired rows . ` df ` would be exactly as it was . ` .ix ` is not destructive . Some operations ( e.g. ` .shift() `) are , but indexing almost never modifies the existing data .

then we convert Col C to a dict , transpose it and then join it to the original df #CODE

Then I want apply fill_between() on area between A and B series : #CODE

Note that ` stack ` will give you a multi-index which may not be desirable in this case .
` stack ` in combination with ` reset_index ` will do the trick .

You should post a sample of your initial dataframe so we have some data to work with . Regarding your first question , try this instead frame3.loc [: , " space_len "] = frame3 [ " name "] .map ( lambda x : len ( re.findall ( ' [ # ]' , x )))

You can also nest ` where() ` so it's really very powerful and simple method for conditional setting like this . You can also use ` ix / loc / iloc ` ( though you need to create an empty column for ' price ' first ):
Because the result of chained indexing may by a copy rather than reference , which will cause the former to fail but not the latter . You can also get around this by using ` ix / loc / iloc ` .

@USER is probably using version 15 or 16 , and it's quite likely ``` loc ``` has improved in performance since version 12 . You could check the release notes if you are interested . My results ( with version 15.2 ) are certainly much closer to Ed's than to yours .
@USER -- Yes , of course . I get exact opposite results as Eric . Added to my answer below for readability , but loc in 15.2 is 25x faster than chained . Eric -- Time to upgrade !!!

How to append selected columns to pandas dataframe from df with different columns
I want to be able to append df1 df2 , df3 into one df_All , but since each of the dataframe has different column . How could I do this in for loop ( I have others stuff that i have to do in the for loop ) ? #CODE
I want my df_All to only have ( A B ) only , is there a way to this in loop above ? something like append only this two columns ?
You can also use set comprehension to join all common columns from an arbitrary list of DataFrames : #CODE

This is pretty simple to do manually , but when working with 30-50 files it can be a little tedious ( and it's a task I have to do every few months ) . I'm not sure it would be quicker to merge my sheets together ( but some sheets contain additional headers ) , perform this task individually and then merge them after or even structure the data how I want it when scraping the data .

just use ``` pd.concat() ``` . I would put them in a list when reading from excel then just concat that list into a single dataframe ( in one line ) .
And once you have those yo can get the column means etc . and go on to normalize by the mean : #CODE
Thanks ! This seems to work but I'm having trouble getting by the next step now . I want to normalize the data to the first row so I can plot a line graph for each Dx . In other words I want to divide each row of cell_means by the first row of cell_means .

you are appending each row which causes a copy each time of n^2 operations ; append to s list and concat at the end

but in this way I cannot group by the days only . I can have access to the days setting the ' Datetime ' column as index of the df , and then ` df.index.day ` , but in this way again ( working on all year dates ) , I can group only by days ( from 1 to 31 ) , and not in a sequence as ` month.day ` . Then I would like to plot the results as a distribution with ` df.plot ( kind= ' hist ')` .

Sorry no it doesn't , is your output df actually what you're after because that is whay I get after the ` concat ` without any sorting
OK , a way to sort by a custom order is to create a dict that defines how ' name ' column should be order , call ` map ` to add a new column that defines this new order , then call ` sort ` and pass in the new column and the others , plus the param ` ascending ` where you selectively decide whether each column is sorted ascending or not , and then finally drop that column : #CODE

Python find out records in dataframe by column values greater than or equal to their median in each subgroup
I want to subset ` df ` by the ` value2 ` column , the value of which is greater or equal to the median of each sub-group specified by the index of ` group2 ` . In this example , the row of ` group1 ` in ` [ ' 2 ' , ' 4 ' , ' 5 ' , ' 6 ']` should stay in the result . Can anyone help ?
What this does is it gets the ` value2 ` column , and splits it into groups by ` group2 ` . For each group , it finds the median , then replaces and value below the median with ` NaN ` . It then puts this back into the ` value2 ` column , then gets rid of all the rows with ` NaN ` values .

Simple join of two Data Frames in Pandas
For every integer row index there is a corresponding entry in the second data frame . I would like to join them into a resulting data frame where the row indices are equal , such as an inner join in SQL .
I can't seem to get this correct . Append results in 2 x the number of rows I should have . The info is coming in from a CSV . #CODE
I can't seem to use merge because the integer index has no name #CODE
or ` join ` : #CODE
Or ` merge ` and set ` left_index=True ` and ` right_index=True ` : #CODE

` cons_all_rank ` spanning over 4 cells ( like merged cell in excel ) . It is a result of calling ` groupby() ` and ` agg . ([ np.mean , np.std , np.median , len ])` . #CODE
KeyError : ' ( cons_all_rank , median )'
It's probably best to edit the question to highlight that what you were calling a " multiline header " is known in pandas as a " hierarchical index " and then mark it as a duplicate . It's not in the spirit of Stack overflow to add an answer here , as the other one is exactly what you want , comprehensive and accurate as far as I can see . Glad I could help you :)

Stack two pandas data frames
How do I stack the following 2 dataframes : #CODE
Using the pandas merge operations does not work since it just lines the dataframes horizontally ( and not vertically , which is what I want )

I was hoping to drop the columns where all values in the column == 0
Then I transpose , slice and transpose back
Just use a boolean condition to make a mask and then call ` any ` and pass param ` axis=0 ` to drop the columns that are all ` NaN ` , the ` any ` call will create a boolean series which you use to select the columns where all the values are not ` NaN ` : #CODE

if ` dt ` is the ` datetime ` module then use ` utcnow() ` instead ` now() ` here .

I am getting an error when i use your groupby code . This is error "' Series ' object has no attribute ' dt '" . This is sample data on my df [ ' DateTime '] => 2013-06-25
@USER use ` pd.to_datetime ` rather than apply . If you use the apply it may not create a Datetime column . I think you may have to update to 0.15.X for the dt accessor . If you want month-year then use the ` to_period ` part of the answer above ?

You have a couple of options , the easiest IMO is to simply unstack the first level and then ffill . I think this make it much clearer about what's going on than a groupby / resample solution ( I suspect it will also be faster , depending on the data ): #CODE
and stack it back ( Note : you can dropna=False if you want to include the starting NaN ): #CODE
Whether this is more efficient than a groupby / resample apply solution will depend on the data . For very sparse data ( with lots of starting up NaN , assuming you want to drop these ) I suspect it won't be as fast . If the data is dense ( or you want to keep the initial NaN ) I suspect this solution should be faster .

Sounds Like you want a merge . Please show some example scenarios .
` fillna ` has a ` value ` argument which can be used to map missing values by common index , but this expects the argument type to be ` Series ` or ` dict ` , not ` DataFrame ` .

Python Pandas ' apply ' returns series ; can't convert to dataframe
OK , I'm at half-wit ' s end . I'm geocoding a dataframe with geopy . I've written a simple function to take an input - country name - and return the latitude and longitude . I use apply to run the function and it returns a Pandas series object . I can't seem to convert it to a dataframe . I'm sure I'm missing something obvious , but I'm new to python and still RTFMing . BTW , the geocoder function works great . #CODE
The goal is to geocode 166 unique countries , then join it back to the 188K addresses in df_addr . I'm trying to be pandas-y in my code and not write loops if possible . But I haven't found the magic to convert series into dataframes and this is the first time I've tried to use apply .
Your latter method is also incorrect because you drop the duplicate countries and then expect this to assign every country geolocation back but the shape is different .
It is probably quicker for large df's to create the geolocation non-duplicated df's and then merge this back to your larger df like so : #CODE
this will create a df with non duplicated countries with geo location addresses and then we perform a left merge back to the master df .

So this is fine . I'm looking for a function instead of the ` lambda g : len ( g ) 1 ` able to filter Users under different conditions , as I said before . In particular filter Users with with one occurrence / month .
Thanks Ed ! What if I would like to filter Users with occurrences only in some months ? Can I apply some dt.month == june conditions ?

align three time series in python
I would like to align the signals only after time = 800 , based on having the minimum points aligned .
I have tried the following to align two of the signals in pandas : #CODE
To find and align the minima : #CODE

If you actually have strings that look like tuples , you can parse them first and then apply the same pattern as above : #CODE
If it is actually strings , you can first convert it to lists like so , then apply the above operation : #CODE

pandas Normalize dataframe to data ( quotes ) at specific time ( not midnight )
Aside : not only is it easier to insert text ( as opposed to an image ) into a question , but if you copy and paste the text then other people can select those parts and use ` pd.read_clipboard() ` to reproduce your frames .
I ended up doing it another way ( filling down the 3pm quotes during an ordered merge on a matching ' date ' column with the original quotes frame , then dividing the columns across ) and got the same answer . So your answer works as well . Thanks !

Just perform a ` merge ` on ' a ' column and perform a left type merge : #CODE
is because the default type of merge is ' inner ' so values have to exist in both lhs and rhs , see the docs : #URL

Using Unstack in Python
I am trying to unstack a column in python but it isn't quite doing what I am expecting . My table ( called df ) looks similar to this : #CODE
I want to unstack by year so that all the days from a year per stationn are in one row . The two days from 1916 would then start first followed by the 2 days from 1917 for station 210018 and 256700 .
You need to make ` year ` an index before you call unstack : #CODE

Missing values replace by med / mean in conti var , by mode in categorical var in pandas dataframe -after grouping the data by a column )
I have a pandas dataframe , where all missing values are np.nan , now I am trying to replace these missing values . The last column of my data is " class " , I need to group the data based on the class , then get mean / median / mode ( based on data whether data is categorical continuos , normal not ) of that group of a column and replace missing values of the group of the coulmn by respective mean / median / mode .
get median / mode / mean of groups of the cols
replace the missing of those groups
but currently I landed up , finding replacement values ( mean / median / mode ) group wise and storing in dict , then seperating the nan tuples and non-nan tuples .. replacing missing values in nan tuples .. and trying to join them back to dataframe ( which i donno yet how to do ) #CODE
If I understand correctly , this is mostly in the documentation , but probably not where you'd be looking if you're asking the question . See note regarding ` mode ` at the bottom as it is slightly trickier than ` mean ` and ` median ` . #CODE
Note that ` mode() ` may not be unique , unlike ` mean ` and ` median ` and pandas returns it as a ` Series ` for that reason . To deal with that , I just took the simplest route and added ` [ 0 ]` in order to extract the first member of the series .

To create multiple columns when using ` apply ` , I think it's best to return a ` Series ` rather than a list . You can set the column names by setting them as the index for the ` Series ` . So you can do : #CODE

drop : boolean , default False .
Do not try to insert index into dataframe columns . This resets the index to the default integer index

Resample function works under Linux but not Windows
resample
in resample

Insert into MongoDB retuns cannot encode object
I'm doing a rather simple insert into a local MongoDB sourced from of a Python pandas DataFrame . Essentially I'm calling datframe.loc [ n ] .to_dict() and getting my dictionary directly from the df . All is well so far until I attempt the insert , where I'm getting a ' Cannot encode object ' . Looking at the dict directly showed that everything looked good but then ( while writing this question ) it dawned me to check each type in the dict and found that a long ID number had converted to a numpy.int64 instead of a simple int ( which when I created the dict manually as an int would insert fine ) .
This directly from the to_dict output that faulted on insert . I copied this directly into a ' test ' dict and that worked perfectly fine . If I print out values of each of the dicts I get the following ... #CODE
The only difference ( as far as I can tell ) is the Long int , which interestingly enough , when I did the Mongo insert it shows that field as being ' Number Long ' within the document . Hope this help clarify som .

Rolling argmax in pandas
I have a pandas TimeSeries and would like to apply the argmax function to a rolling window . However , due to casting to float from rolling_apply , if I apply ` numpy.argmax() ` , I only obtain the index of the slice of the ndarray . Is there a way to apply a rolling argmax to a Series / DataFrame ?
Here is a work-around , essentially doing the apply ' manually ' , should be pretty efficient actually . #CODE

You can also use the ` apply ` method for a one-liner , which is simpler and clearer but also slower even than your approach : #CODE

How do insert the below data which I was previously plotting sequentially on top of each other in to the created figures above ? #CODE

Sure thing , once you learn how to use ix , loc , and iloc in pandas you can do just about anything

My situation is I have a DataFrame with a MultiIndex including a ` TimeStamp ` and number ( wavelength from 280-4000 nm ) where the wavelength number spacing changes from every 1 nm to 5 nm . I need 1 nm spacing and plan to and plan to linearly interpolate after reindexing my DataFrame .

I think when you specify ` dtype= ' string '` , you are essentially specifying the default ` S64 ` type , which will truncate you string to 64 chars . Just skip that ` dtype= ' string '` part you should be good to go ( and the ` dtype ` will become ` object `) .

But what I actually would like to do is plot the minimum and maximum of my process at every date , and color the area between the two extremas with a color map that would reflect the concentration of the paths in the plot ( so for instance red around the mean and gradually cooler as we go to the extremes ) .

convert to int to drop decimal place and convert int to string with 3 digits .

This is a bug to me , your index is of dtype datetimeindex , when perform loc and trying to divide it produces NaN , if you did this then it would work ` df.iloc [ 3 ] / df.iloc [ 5 ]` and produce the correct result . So you can either reset the index and find the row with that value and perform the division or try to use the integer index value to select that row
The reason why ` df.loc [ df.index.year == 2007 ] / df.loc [ df.index.year == 2009 ]` or ` df.loc [ df.index.year == 2007 ] .divide ( df.loc [ df.index.year == 2009 ])` are not working is that ` pandas ` tries to align the data by their index . In this case , what will happen is data for 2007 will be divided by data with index value of 2007 ( same applies for 2009 ) . That's why you are getting , 2 , not just 1 row of ` Nan ` s .
I've +1 ed everyone's contribution to this , I think you are correct and it makes sense , it's one of those subtle things that doesn't tend to bite you when using unique int indexes and selecting rows using ` loc `

Apply formula to Multi-index column Python
Want to apply formula and label result to multi-index dataframe .
I get into trouble when I concat two dataframes . Want to combine df df1 into single data frame so I can get highest values of ' bar ' ' bat ' , along with additional column identifying if data ( row ) came from from DF or DF1 . #CODE
Gut tells me I need to group-by and then apply formula , I can handle that ( I think ) , but how do I bring label ( ' pattern ') along ?
Aside : this is more personal preference than anything else , but I like to use ` np.sign ` ( 1 for positive , 0 for 0 , and -1 for negative ) and then ` replace ` rather than three lines for your ` pattern ` translation . For example : #CODE

Spliting dataframe in 10 equal parts and merge 9 parts after picking one at a time in loop
I need to split dataframe into 10 parts then use one part as the testset and remaining 9 ( merged to use as training set ) , I have come up to the following code where I am able to split the dataset , and m trying to merge the remaining sets after picking one of those 10 .
test_index = np.random.choice ( df.index , int ( len ( df.index ) / 10 ) , replace=False )

Not sure what you mean here . Do you want to convert strings to timestamps ? Are you converting 2001 Dates to 1990 Dates ? The AAPL values are diff as well . Do you have datetimes already and just want to display zero minutes ?

I would try to replace your ` return ... ` statement with the following ` rows = results.all() \n local_session.close() \n cnt = len ( rows ) \n return rows , cnt ` . My guess is that it has something to do with the session state .
You're doing a join between two classes ( ` T ` , ` User `) - do eager load instead of default lazy load .
If you have 800,000 rows and doing a lazy join , that may be a problem . Add a ` joinedload ` ( ` eagerload ` in earlier versions of SQLAlchemy ) to ` options ` .

Find and replace multiple values in python
I want to find and replace multiple values in an 1D array / list with new ones .
I would like to replace #CODE
What is the fastest way to do this ( for very large lists , i.e. with 50000 values to find and replace ) ?
To replace values in a list using two other lists as #URL pairs there are several approaches . All of them use " list compression " .
In pandas I would create a dict from the 2 lists and then call ` map ` which will perform a lookup and replace the values : #CODE
This approach isn't vectorized , ` map ` uses iteration . For long lists , it is a bit faster doing the ` map ` , but the time necessary to construct the ` Series ` means the iteration-based approach ends up being faster overall .

Basically , since the additional rows , ' total ' , need to be calculated and inserted into the original dataframe , it is not going to be a one-to-one relationship between the original and resultant , neither the relationship is a many-to-one type . So , I think you have to generate the ' total ' dataframe separately and ` concat ` it with the original dataframe .

This looks like a good option ... Is there a way to unstack the table so that only the statistics for one of my columns is being displayed horizontally from ` .describe() `

does not accept the argument ` axis ` . I could unstack the data frame for the second level and then sum by row again , but that seems too complicated .

I'd construct a dict of the values you want to replace and then call ` map ` : #CODE
excellent suggestion . As in : ` map ( lambda v : d [ v ] , df )`
What would you do if you have , say half a million of such distinct values over few millions of data-points ( len [ df ] = ' very big ') ? I wonder regarding the size of the dictionary as well as the way of its creation . Is there a way to create a dictionary fast without actual applying d [ ' fff '] = index for all possible values of keys before applying map ? Thanks .
You could construct just a series of the distinct values and use that : so df [ ' data '] .unique() returns a series of all the unique values the index will be an auto generated int64 index , so you could create a dict from that ` d = dict ( zip ( df [ ' data '] .unique() .values , df [ data '] .unique() .index ))` you could substitute the last param for ` np.arange ( len ( df [ ' data '] .unique() ))`

I think it is due to the ` nan ` in your data , as density can not be estimated for ` nan ` s . However , since you want to visualize density , it should not be a big issue to simply just drop the missing values , assuming the missing / unobserved cells should follow the same distribution as the observed / non-missing cells . Therefore , ` df.dropna() .groupby ( pd.TimeGrouper ( ' AS ')) [ ' INC_RANK '] .plot ( kind= ' density ')` should suffice .

Apply where function [ SQL like ] on datatime Pandas
AttributeError : ' Series ' object has no attribute ' dt '

I am concatenating two data files using Pandas . The concat is working well but when I write the data back to csv the data loses some coherency : #CODE

If you set the index to be the dates for both df's then multiplication will align where the indices match : #CODE
If you have duplicate date values then what you could do is left merge the other df's value column and then multiply the 2 columns so the following should work : #CODE
Where you have duplicate dates are the values also duplicated ? Another method would be to left merge the value column from df1 to df on the dates and then multiply the 2 columns

In Pandas , merge two dataframs with complex multi-indexing
I would like to merge two dataframes on columns Name and Depth . The depth in the left df has a single depth column ( ' depth ') . However , the right df has two depth columns ( ' top_depth ' and ' bottom_depth ') .
Then merge to get this : #CODE
You can join ( on the index ): #CODE
And I think you can attach the rest of df1 using a ` join ` I can't remember .

I'm not sure if you're asking about concat also , but it's pretty straightforward : #CODE

I would like to merge them in one file .
I would like to merge the diferent contents in one id . Thanks ....... @USER

So what does this error mean and how can I multiprocess this map function ?
It is hard to debug errors inside functions used in multiprocessing . You should turn off multiprocessing to debug it , then turn it back on when it is fixed . I usually have a ` mp=True ` argument in my function that runs the function in multiprocessing mode by default but can be set to ` False ` to run it with a regular , non-multiprocessing ` map ` ( using an ` if ` test ) so I can debug these sorts of errors .
So , once the debugging is done , replace : #CODE

pandas - apply datetime functions

Boxplot and groupby : Issue with groups and sharex
The problem I have , is that , in the upper subplot , the boxplot on the right hand side should be shifted of 1 to right to align with the ` ( Yes , False )` label .

Just set the index and unstack . You can use ' rowid ' instead of ' rowname ' or include both in the index if you don't want to leave either one out , but it's redundant as far as the solution . #CODE

use ` merge ` : #CODE

i have large number of values , how can insert all of them in the place of somevalue and how can i save the each iteration result
I'm new to pandas but it seems you want the ` cut ` function . #CODE

I am trying to drop duplicate entries from a Pandas DataFrame in Python . The DataFrame consists of vertically concatentated contents of multiple * .csv files . Here is the DataFrame : #CODE

can you first find the indices of the rows that need to be removed and then use ` drop ` to remove them ?

Python pandas resample montly works weekly doesnot
not even 1 of 7 is getting values because resample ( ' w ') is using sunday as last day . data is stock data which has Friday as last day . I tried to use w-fri but did not get correct results .

How to create a pivot table on extremely large dataframes in Pandas
I need to create a pivot table of 2000 columns by around 30-50 million rows from a dataset of around 60 million rows . I've tried pivoting in chunks of 100,000 rows , and that works , but when I try to recombine the DataFrames by doing a .append() followed by .groupby ( ' someKey ') .sum() , all my memory is taken up and python eventually crashes .
How can I do a pivot on data this large with a limited ammount of RAM ?
Why not use a RDMS to aggregate your dataset ? An SQL engine is designed to store millions of records and handle basic processing like Sum() by groups . And as your pivot indicates , with what I assume are two byte-size fields ( ids ) and one integer ( qty ) field a temp db table should not be too extensive to store and query . Consider aggregating inside SQL Server , Oracle , MySQL , PostgreSQL or any other and pass the result into the Python [ dataframe ] ( #URL ) .
Originally comes from gbq , which sadly doesn't have a pivot function
[ Google BigQuery ] ( #URL ) ? Why not run an equivalent [ query ] ( #URL ) to your pivot : ` SELECT [ catid ] , Sum ( qty ) FROM [ datasource ] GROUP BY [ catid ]` ? Then , import the result into a data frame . Also , Python has an experimental pandas library for [ gbq ] ( #URL ) with query option .
@USER For a groupby sum , that map / reduces , in the sense you can groupby and sum on chunks then add up the results . So chunk through df in the store . Something like : sum ( df.groupby() .sum() for df in store.select ( ' df ' , chunksize=50000 )) ? see #URL

Optimizing pandas filter inside apply function

You can use a merge .

Why didn't you normalize your labels ( i.e. chinese|Chinese -> chinese ) after you've read the dataset before extracting the features ?
To answer your other questions , I didn't normalize the labels because I am doing initial testing , and I think my current way to code it before making the list is fine . I will consider other features but this ( 3-letter substring feature ) should be powerful enough to distinguish the Chinese and non-Chinese names . I will consider NaiveBayes .

Next we filter for everything marked as Control using boolean indexing . Then we merge the ' filtered ' version with our original . #CODE

I now need to apply this function to several million rows and it's impossibly slow so I'm trying to figure out the best way to speed it up . I've heard that Cython can increase the speed of functions but I have no experience with it ( and I'm new to both pandas and python ) . Is it possible to pass two rows of a dataframe as arguments to the function and then use Cython to speed it up or would it be necessary to create new columns with "` diff `" values in them so that the function only reads from and writes to one row of the dataframe at a time , in order to benefit from using Cython ? Any other speed tricks would be greatly appreciated !
I was thinking along the same lines as Andy , just with ` groupby ` added , and I think this is complementary to Andy's answer . Adding groupby is just going to have the effect of putting a NaN in the first row whenever you do a ` diff ` or ` shift ` . ( Note that this is not an attempt at an exact answer , just to sketch out some basic techniques . ) #CODE

At my project I initialize an object with an empty pandas dataframe . Within a method of this object I create another dataframe , with a multilevel column . Also I merge the second dataframe with the empty dataframe and I expect that merged dataframe has also a multilevel column , but the multilevel column is converted to a single level column with a tuple of my levels . So how can I retain the multilevel column in my new dataframe ?
Not sure why do you need to merge an empty dataframe with anything , sounds like an overkill ...
But if you must ... just replace last line with this : #CODE
Thanks for your response , this will work for a single merge , but I want to do the merge every time I call the method . So I think my mwe was wrong ( corrected at the end of my first post ) .

method . I used the equal width binning method to assign labels to certain bin widths . ( example below ) . I stored it in a variable cat_age . Now I want to replace the age attribute in dataset with the categorical age attribute . #CODE
But i am unable to replace the OLD age attribute values in the dataset . I tried using DataFrame.replace() and DataFrame.assign() methods . #CODE
but it is not letting me replace or assign different values to the column in the dataset . #CODE
I am sure I am doing some Naive mistake . Can anyone suggest me a way that I am able to either append a new AGE column with categorical values or replace the old column with these new values . #CODE

What if the series contains a leap date such as ` 2008-02-29 ` ? How is it to map to a date in ` 2007 ` ?
then you could replace all the values in the series with those from 2007 with #CODE
The more robust ( but slower ) way is to use a join : #CODE

I have an excel file ( .xls format ) with 5 sheets , I want to replace the contents of sheet 5 with contents of my pandas data frame .
Thanks for the reply . i went through the documentation of xlwt a couple of days ago and couldn't figure out an easy way to do the task . So , I decided to do this task in VBA and drop python completely . it was a piece of cake in VBA . :) wish it was that easy in python as well

Thanks Ed , ' dt ' works perfectly . And I meant data , not price . It has now been changed in the question .

Geopy error : GeocoderServiceError : HTTP Error 500 : Internal Server Error using pandas apply function with str concat
Working function ( see code Python Pandas apply returns series ; cant convert to dataframe ) has stopped working . Only difference is I'm passing it a string concatenation . #CODE
Ed , I see what you are saying - that I'm passing a series - but am not sure how to fix it . Short of ditching apply and iterating through the table , or passing 5 parameters and then iterating through the table . I thought apply() did this for me - calling the function once for each row . The debug code seems to indicate this as it says that x is a str type , not series . Hmmm ... I believe you , I'm just trying to wrap my head around it and decide what to do next . And I manually confirmed the geocoder is insensitive to white spaces . Any Further advice ?
What you're doing is a little perverse to be honest , you're calling ` apply ` on a series and then trying to construct a str from lots of columns , this is the wrong way to go about this , you can call apply on the df and pass ` axis=1 ` so that the row is passed and either access each column in a lambda func and pass them to ` locate ` or in ` locate ` extract each column value , or just create a series from the concatenation of all the columns and call apply on this : #CODE

Your DataFrame column contains a mixture of strings and tuples . I don't think you can avoid iterating the column . But you can iterate efficiently with the apply method . Example code follows . #CODE

Is is possible to drop ` df2 ` rows if those rows are also present in ` df1 ` ?
One possible solution to your problem would be to use merge . Checking if any row ( all columns ) from another dataframe ( df2 ) are present in df1 is equivalent to determining the intersection of the the two dataframes . This can be accomplished using the following function : #CODE
Then we can look at the common columns using ` common_cols = list ( set ( df1.columns ) set ( df2.columns ))` between the two dataframes then merge : #CODE
EDIT : New question ( comments ) , having identified the rows from df2 that were also present in the first dataframe ( df1 ) , is it possible to take the result of the pd.merge() and to then drop the rows from df2 that are also present in df1
EDIT 2 : How to drop the rows from df2 that are also present in df1 as shown in @USER answer .
This occurs because the value of 1 in column A is found in both the intersection DataFrame ( i.e. ( 1 , 0 , 2 , 3 )) and df2 and thus removes both ( 1 , 0 , 2 , 3 ) and ( 1 , 1 , 1 , 1 ) . This is unintended since the row ( 1 , 1 , 1 , 1 ) is not in df1 and should not be removed .
Oh ofcourse ! An SQL ` INNER JOIN ` . That escaped me . One problem is that I have never used it for ` JOIN ` ing ` ON ` multiple columns . If all the dataframe's columns are to be checked , could you replace your ` on =[ ' A ' , ' B ' , ' C ' , ' D ']` with ` on =d f1.columns ` ?
Andrew , one last question ( I also added it to the original post ) - having identified the rows from ` df2 ` that were also present in the first dataframe ( ` df1 `) , is it possible to take the result of the ` pd.merge() ` and to then drop the rows from ` df2 ` that are also present in ` df1 ` ?
@USER : I believe I found a way to drop the rows of one dataframe that are already present in another ( i.e. to answer my EDIT ) without using loops - let me know if you disagree and / or if my OP + EDIT did not clearly state this :
The columns for both dataframes are always the same - ` A ` , ` B ` , ` C ` and ` D ` . With this in mind , based heavily on Andrew's approach , here is how to drop the rows from ` df2 ` that are also present in ` df1 ` : #CODE

Then I am able to process it using apply , for example : #CODE

Is it possible in pandas to interpolate for missing values in multiindex dataframe . This example below does not work as expected : #CODE
It looks like you want to interpolate based on the MultiIndex . I don't believe there is any way to do that with pandas interpolate , but you can do it based on a simple index ( method= ' linear ' ignores the index btw and is also the default so no need to specify it either ): #CODE
Anyway , this is the kind of 1d interpolation you can do pretty easily with pandas interpolate . If that's not enough , you could look into numpy's interp2d for starters .
@USER -- You should add that to the question then . I'd also add numpy as a tag ( instead of dataframe ) . You may want to remove pandas interpolate from the question too as it doesn't look like that is going to be of any use here . Just suggestions , of course .

Columns get lost on pandas frame join
How do I properly do a group-by and join these two frames ?
The solution to this problem is to apply ` reset_index() ` to " end " the group-by operation .

What is this " freq " argument ? And why is it stopping me from loading the data ?
It looks for me that ' freq ' in index is not exists . Have you check out those links ? #URL #URL

I solve this with map and lambda function ....
y = map ( lambda x : x.year , kk.index )

My decorated DataFrames return new and similarly decorated DataFrames when I use methods such as copy and groupby.agg . How can I have " all " pandas functions like pd.DataFrame() or pd.read_csv return my decorated DataFrames instead of original , undecorated DataFrames without decorating each pandas function individually ? I.e. , how can I have my decorated DataFrames replace the stock DataFrames ?

Just apply a ` filter ` : #CODE

Here's a different look . At a high level , I perform something tantamount to a cross join by ...

The point is that I don't think I've understood it , sorry . Ok , no ANOVA . But then , you will run 24x2 t-tests ?? Maybe I still haven't understood ... Anyway , if you want to apply a value to a multiindex , it would be something like ` df.loc [ ' 0hr '] .loc [ ' 0.01um '] [ ' t '] = xxx ` , assuming that ' t ' is a column . For your dataframe , I've tried it straight in ipython as ` df.loc [ ' 0hr '] .loc [ ' 0.01um '] .loc [ 0 ] [ ' a '] = 3 ` . But if again I haven't understood , just tell me !

How to replace NaN with sum of the row in Pandas DatatFrame
I am trying to replace the NaN in certain columns with the sum of the row in a Pandas DataFrame . See below the example data : #CODE

@USER they are not known but I meant more any other way than normal replace . Is there any ?
No not really depending on how your data is composed it may be quicker to just do 10 calls to replace which will be fast rather than creating some function that is called for each value
@USER I do not agree because if I replace 1 with one for instance what about 51 ? Then it would be fiveone

apply sort to a pandas groupby operation
How do I apply sort to a pandas groupby operation ? The command below returns an error saying that ' bool ' object is not callable #CODE
Normally the sort is performed on the groupby keys and as you've found out you can't call ` sort ` on a groupby object , what you could do is call ` apply ` and pass the ` DataFrame.sort ` function and pass the column as the kwarg param : #CODE
Well you introduce another level of indexing by adding this sort criteria , you'd have to drop the superfluous indices after calling ` reset_index ` I'm afraid

That URL is not properly encoded . Your browser automagically replaces the spaces `' '` by `' %20 '` , the underlying ` urllib ` request from the python standard library doesn't do that . Replace all spaces by `' %20 '` and you are fine .
Regarding your first issue : Somehow you have managed to insert a [ ZERO WIDTH NO-BREAK SPACE ] ( #URL ) character after the `" http : "` . Just retype that part and you are fine , URLs are ASCII-only strings .

It doesn't matter for your mean calculation because you're just generating a boolean that pandas interprets as 0 / 1 . But for the correlation coeffient you need to provide numbers . You also need to use ` apply ` rather than ` agg ` here : #CODE

Just perform an outer ` merge ` : #CODE
The reason you get that result when using ` concat ` is that you are concatenating column-wise and it's aligning on the common index values
Ok , i rewrote the code to make it work . Is it possible to do this within a for-loop so i effectively can merge 10 csv-files together in the same way ?
You'd have to merge each of the files to each other , concat will not produce the csv you're looking for

I haven't found a solution browsing related posts and Stack Overflow questions . Could anyone help , please ?

Also to add this code works perfectly if I replace GradientBoostingClassifier with RandomForestClassifier .

sum of values larger than median of each row in pandas dataframes
very new to pandas . Is there an efficient way to find the sum of values whose absolute value is larger than the median of the row in a pandas data frame ?
How to generate the sum of numbers in each row which are larger than the median of the corresponding row ? what about 25 percentile ? or 75 percentile ? thank you very much !
This uses ` .gt ` which is greater than and uses as the value the ` median ` ( row-wise by passing axis=1 ) .
Thanks Ed and Alex , never used ' .gt ' before . Just out of curiosity , is it possible to use ' apply ' to realize the same function ?
Yes you could but I'd advise against it because apply is slow whilst this will be vectorised , apply should be a last resort always
Since you want to sum values in each row which is greater then median , and if you want to preserve Day values , below approach works fine #CODE
thank you John , I was very curious about how to use apply to realize it .

I want to replace the ' T ' in the preciptotal column with the value .01 . #CODE

I am trying to create a dataframe from the following list of tuples . The first item in the tuple is ` ID ` , second ` values ` is a list of values and the third is the ` lag ` . This ` lag ` defines how many indices I need to shift in either direction of the first tuple to get the dataframe . #CODE
Operations between Series ( + , - , / , * , ** ) align values based on their
... yeah , that's unexpected ; the ` raw ` df is right , but ` map ( newSeries , raw )` fails . Can't see why ! Rearranging ` mytup ` entries isn't informative ...
Okay , map over a df is apparently creaky , but DataFrames have a bunch of useful iter-types and using one of * those * works . ( As did ` map ( fn , roo.index )` . )

Python How to find average of columns using dataframes apply method
Using the dataframe's apply method , create a new Series called ` avg_medal_count ` that indicates the average number of gold , silver , and bronze medals earned amongst countries who earned at least one medal of any kind at the 2014 Sochi Olympics .

I played around with this for a couple of minutes . I think v1 / v2 are empty for some data values , in line with what Andy suggests . Also coef doesn't seem to have any purpose in the code , just fyi . Anyway , your groupby / apply is probably OK , you just have something wrong with the function itself .
And , Now , let's see what was the error that was happening , when you apply np.argsort() after first group on series object . Lets take the second group values . Which is - #CODE

divide the new series by the ' 12:00 ' values . Note that you have to drop the index ( with the .values ) here to broadcast it properly . Reindex back to your original frame . #CODE

How to insert a row in a Pandas multiindex dataframe ?
I want to make sure that every ( Reg , Type , Part ) tuple has True and False for IsExpired . E.g. I'd like to insert a row for ( EMEA , Disk , A , True ) - #CODE
You could ` unstack ` and then ` fillna ` : #CODE
Perhaps it makes sense to keep this as a column ? Otherwise stack it back : #CODE
* Note : when doing stack / unstack there is often an alternative ` pivot ` / ` pivot_table ` method ... *
Thanks Andy ! I have a slight variant of the above question - For some special processing that I am doing , I am processing these records one row at a time . So one tuple of ( Reg , Type , Part , IsExpired ) at a time . So for the row in question , I end up with - [ EMEA Disk A False 22 ] where I can't use unstack / stack method . Is there a way to somehow insert a row for True here ?

then reset the index to drop the second level : #CODE

what you need is pivot table . this blog explains it very clearly : #URL

Then you can reorder the levels and transpose the dataframe .

Note : the groupby from above is actually equivalent to the simpler ` resample ` : #CODE

Ok , it solves the example . The problem is that I do not have a string of data as " data " in your example . I only have csv-file , and i tried to apply your solution but could not make it work . Do I have to convert the csv-file to a string of data first maby ?

I know how to append an existing series / dataframe column . But this is a different situation , because all I need is to add the ' Name ' column and set every row to the same value , in this case ' abc ' .

You could perform an ` inner ` merge :
for a many to one relationship then you can perform a ` left ` merge : #CODE
First off , you would be importing different tables into your database , and not multiple databases . This seems to be the first point of confusion . You should be able to import csv-file tables directly into your database . Once you have the described table relationship you should be able to get the necessary data by doing an ` INNER JOIN ` : #CODE

pandas dataframe drop rows by multiindex
I'd like to drop rows from a pandas dataframe using the MultiIndex value .
I've tried quite a few things but I put below what I think was closer . ( Actually I will explain the full problem since there might be an alternative solutions using a completely different approach ) . From a correlation matrix , I'd like to get the pair of columns that correlate more . I use ` unstack ` and put the results in a dataframe : #CODE
The method ` drop ` is working perfectly when the index is normal ( see drop ) , but , how do I build the ` label ` when I got a ` MultiIndex ` ?

Use a pivot table : #CODE

I need to shift all the individual columns in the following dataframe to extreme end . Is there any switch in ` shift ` function to do that ? #CODE
I am trying to accomplish this by first converting the dataframe into a list of lists , then insert nans , and in the end , convert back into the dataframe .
You could iterate over each col , get the index position of the last valid value and then shift by the length of the df minus this index position , as indexes are 0 based you need to offset by 1 : #CODE

Where I'm stuck is in creating a lag variable that aggregates ( sums ) values over days t-1 through t-3 , or t-1 through t-7 , or t-1 through t-30 . For instance , I would like a column called ' Number of Tweets [ sum of t-1 through t-3 ]' . I have played around with DateOffset and looked into resample but haven't been able to take it further than that . I also can't seem to find any answers in the Cookbook or examples in the documentation that would help . I'm spinning my wheels on this one so would appreciate any help .
Use pd.rolling_sum on the shifted data . To calculate the rolling sum for t-3 through t-1 , use a window length of 3 and shift the data by 1 ( the default if no parameter is specified ) . #CODE

Alternatively , you can represent ` sym ` column as a q generic list containing strings . You can also apply type conversion to other columns : #CODE

I though to first use Sed in a subprocess to remove the " [ " and replace the "]" with a comma , and the use pd.read_csv to get the separate values in a dataframe and then transpose to get it in a column . However then I get the following dataframe : #CODE
replace the ` [ ]` with spaces
replace commas with newline characters
Simple replace with ` awk ` should do : #CODE

How can I apply a search , such that the result would be the index for each value , if it exists , in an efficient way ( since I know the column `' A '` has uniqu values ) to get the following results : #CODE

I've tried to use Deciamal module for this , but when I tried to resample it like this #CODE
@USER hmm .. I will not end up with 133.04 becoming 133.05 or 133.03 , will I ? So I cast it to float64 right before resampling , resample and cast to Decimal again , right ?
That may possibly happen but usually the imprecision occurs at the lower digits but if you cast to Decimal at the end it should clip this

Draw different sized circles on a map
I would like to plot these locations on a map using circles . The radius of the circles needs to correspond to the amount in ` Total ` . In other words , Location 1 needs to have the biggest circle , Location 2 a smaller one , etc .
I managed to draw blue dots on the map , but I don't know how to draw the circles with the corresponding size and color .

pandas boxplot for clustered boxes : how to set multilevel x axis labels

Other than the " hold state " part , the rest is basically a bitwise shift .

I have tried a some join / merge ideas but can't seem to get it to work .
Just ` concat ` them and pass param ` axis=1 ` : #CODE
Or ` merge ` on ' Symbol ' column : #CODE

So you want to reduce ` df2 ` to a single row per hour ? In which case are you wanting the average of the aggregated columns or the sum ? ` df2.groupby ( df2.index.hour ) .mean() .reset_index() ` should squeeze the df to an hourly one , also you could resample
If you want to resample by hour you could just groupby the hour and then call ` reset_index ` : #CODE

I'm running the following function on my json response from some APIs and trying to normalize it into a .csv file using the pandas json_normalize() function . However , my results are coming through without the json key as the column headers and I need them to come through with the keys assigned as column headers . Any ideas how to fix this ? #CODE

Pandas : Merge datasets of different frequency
I have both monthly and bi-annual data , which I want to join .
A straight join would match these only for the firsts of Januarys - what is the proper way to join these ?

I have a large ` DataFrame ` with a few thousand columns and up to hundreds of thousands of rows . I would like to drop duplicate rows but also keep track of which rows of the original ` DataFrame ` were identical before I dropped duplicates .

A better approach than approximation would be to profile the function to get a sense of exactly why it takes too long , followed by using ctypes / Cython / numba to translate the function as-is into a C function that runs without as much overhead . You may need to modify your calling convention to use the numpy array ` values ` of data underlying each pandas ` Series ` column of data , and you can also checkout ` numpy.ctypeslib ` for easy conversion from a numpy array to a ctypes-compatible array . It seems like a lot , but really it's a pretty easy way to access C functions in Python .
Yeah the euclidian approximation will work fine for small enough distances . You shouldn't even need to do an ` apply ` for that , can directly just use the columns in the dataframe .
Purely for the sake of an illustrative example , I took the ` numpy ` version in the answer from @USER and also made a companion C implementation to be called via ` ctypes ` . Since ` numpy ` is such a highly optimized tool , there is little chance that my C code will be as efficient , but it should be somewhat close . The big advantage here is that by running through an example with C types , it can help you see how you can connect up your own personal C functions to Python without too much overhead . This is especially nice when you just want to optimize a small piece of a bigger computation by writing that small piece in some C source rather than Python . Simply using ` numpy ` will solve the problem most of the time , but for those cases when you don't really need all of ` numpy ` and you don't want to add the coupling to require use of ` numpy ` data types throughout some code , it's very handy to know how to drop down to the built-in ` ctypes ` library and do it yourself .
Just to be super clear , I am not at all endorsing this as a better option than just using ` numpy ` . This is precisely the problem that ` numpy ` was built to solve , and so homebrewing your own ` ctypes ` code whenever it both ( a ) makes sense to incorporate ` numpy ` data types in your application and ( b ) there exists an easy way to map your code into a ` numpy ` equivalent , is not very efficient .

Well , this wouldn't occur to me before seeing Result and Result2 side by side , but the pattern seems to be consistent . So really you just need to get all the values and patterns from ``` valuewhen() ``` , and then ( in this example ) map ``` 3 -> 0 ``` , ``` 4 -> 3 ``` , ``` 7 -> 4 ``` , etc . and just do that ``` n ``` times . That's a little tedious , but straightforward . I could be wrong but I think generalizing the function itself would be really hard .

Assuming spreadsheet1 is loaded to pandas df and spreadsheet 2 is loaded to df1 then you can assign the values from the result of a ` merge ` : #CODE
Just to explain a little when we merge we perform a left merge and this will produce a clash of columns as we have column names that clash : #CODE
We are interested in just columns ` A_y ` and ` B_y ` , we perform a left merge because if we did the default merge type which is ' inner ' then this will not align with the original df , i.e. the values will be shifted to the top of the df .

I want to split all the columns that contain tuples . For example I want to replace the column ` LCV ` with the columns ` LCV-a ` and ` LCV-b ` .
You can do this by ` apply ( pd.Series )` on that column : #CODE

Where's the error ( line number / contents of that line ) ? Can you include the entire stack trace ?
That's quite a long lambda function in your apply , I recommend writing as a function . For one thing it'll be easier to debug ( clearer which line is causing the error ) . I could be wrong but it looks like ` search ( Origin_Zip , stop=stop , pause= 5.0 )` doesn't always return a list ( but an int ) .

This looks like a typo ( it's weekday not weekdays ) , either the latter or using loc should work : #CODE

Are you using merge for a SQL-style inner join ? Or could you possible concat instead ?
I see . I'm just trying to understand the type of join you are doing ; if it's inner join , then sticking with merge is good , but if you can do concat , the code would be a lot simpler .

I'd reset the index so that it becomes a column , this allows you to call ` apply ` on it , then for each datetime apply a lambda which calls ` replace ` and null the minute and seconds attributes , then drop the duplicates and set the index back : #CODE
Use resample : #CODE
The reason for using ` .dropna() ` is that an hourly resample will create rows for each hour between the first and last which will be filled with NaNs if there is no data within the hour .

I am creating a Multiindexed pandas Series and each item is a list . First , an empty list , then I append each of these lists individually . However , when I tried to do that in a naive manner , I've faced a problem . It is easily duplicated in this simple example : #CODE
What if you append None instead . #CODE

In the small example you give , the transpose of the DataFrame has columns of a single type so maybe you should be working with that . Note , however , that simply taking the transpose won't change the data types . #CODE

Something like this should work . It will be faster than ` apply ` since it uses vectorized operations . Further , rather than hard-coding the ` apple ` result , it gives you counts and percentages for all purchases , no matter how many you might have . #CODE

How to replace values in pandas DataFrame respecting index alignment
I want to replace some missing values in a dataframe with some other values , keeping the index alignment .
I would like to replace the missing values of ' b ' rows matching them with the corresponding indices of ' c ' rows .
This is great ! One question , though . When the dataframe has more columns and we want to replace nans only on a selection of the columns , this method does not work because subsetting the dataframe creates a copy , not a view . What do you recommend doing , then ?
@USER , right off my head I will say filter the selection of the columns , then apply the same on the values copy

` mm ` here will be the first thing you wanted . Now , to get the counts , value_counts on each column will give you the a series of times and the number of times that time shows up . However , it won't include times that never show up in the max / min , so when you combine the max and min counts into a dataframe , you'll get some NaN values . You can use fillna to remove these and replace them with 0 . #CODE

I have an ` Emmys ` class that uses a 3rd party API . In the class I have a ` create_data ` method that queries a databank and creates a ` Pandas DataFrame ` that I later join to another ` DataFrame ` .

I would like to replace `' a '` with ` a2 ` where ` a2 ` has 5 unique values . In other words I want to define 5 groups and assign to each value of ` a ` one of the group .
simply multiply the number by 100 to scale it in the range ( 1 , 100 ) , and then apply the same algo , however you can play with numbers and find out your own way of doing the same .

If so , insert this between steps 1 and 2 : #CODE
Good spot . I didn't notice that . The difficulty then is how the indices align . Here they contain the same number of elements in each , but it is possible that they don't in which case it isn't clear how they should be aligned .

So this will groupby the fullname and zip as you've stated , we then call ` transform ` on the Amount column and calculate the total amount by passing in the string ' sum ' , this will return a series with the index aligned to the original df , you can then drop the duplicates afterwards .

You can use Counter together with chain from itertools . Note that I first replace periods with commas before parsing . #CODE

The above is basically the pandas data frame . Now when I do this on this data frame it will strip the column headers . #CODE
A sample of the Pandas code might be more useful . Doesn't Pandas provide a way of extracting the data from a frame , and then replace it with a new copy ?

Nice . You can also avoid the ` drop ( ' level_1 ' , axis=1 )` by using ` reset_index ( ' Name ')` .

funny how I noticed ! anyway , I futzed around with setting up a lambda for a while . You can see each facet's data but not alter it in place , and the way heatmap expects its arguments isn't enough like how plot , etc ., expect theirs to map over easily , and in short roll-your-own looked like the way to go . Maybe if you made the title more specific a seaborn expert would notice -- ' plotting correlation heatmaps into a FacetGrid ' , maybe ?
Turns out you can do it pretty conciesely with just seaborn if you use ` map_dataframe ` instead of ` map ` . #CODE

How can I join 2 rows in a dataframe into 1 row in a new one ?
You explicitly set the index for your right df , if it's the same number of rows and you want them to align why not just use the same indices as left : ` right = DataFrame ( bf.values [ 1 :: 2 ] , index= left.index )` ? then the concat would produce what you want no ?
Your attempt to ` concat ` aligned the 2 df's by index thereby producing a disjointed df with 4 rows rather than 2 rows : #CODE
The above creates a new df using the values from your df but you also took the index values also , seeing as the left and right df's have the same number of rows and you're want to concatenate them column-wise so that the indices align then you can just use the same index from the left df : #CODE

I have a time-series data in " stacked " format and would like to compute a rolling function based on two columns . However , as shown in my example below , the ` groupby ` is concatenating my results horizontally instead of vertically . I can apply ` stack ` at the end to get back to tall format . However , I thought the correct behavior should be to concatenate vertically to allow assignment back to the original dataframe ( something like ` x [ ' res '] = df.groupby ( ... ) .apply ( func )`) . Does anyone know why ` groupby ` is not behaving as expected or am I doing something wrong ? #CODE

I can probably just hack through this using some for loops , but would like to do it by selecting or indexing the data , and then setting . I am just getting stuck on " setting with chained indexing " and setting with iloc / loc / ix .
I would like to do ` df [ df [ ' % Beat EPS '] percentile [ 0 ]] = 1 ` , but I know this selection results in a copy being set . I can't figure out how to represent this using iloc , loc and ix . I would like three new columns added to this original dataframe . Thanks !

Carrying on from the same idea , you could set a MultiIndex for ` df2 ` and then stack . For example : #CODE

Upon further research I still haven't found a solution . It seems you can set a flag with pandas to_sql ( if_exists= ' append ') . But there doesn't seem to be anything for if_exists= ' update ' . I find this really strange . What is the suggested way to get a pandas dataframe back into a database and update any rows that have changed ? Surely this is a common task ?
` to_sql ` does not support updates . The best approach I have found so far is to create an ` ON INSERT ` trigger in the database table , that updates all fields if inserting a duplicate primary key .
Are your data large ? A blunt instrument solution could be : read in the database table again into some sort of ` original_table ` dataframe , call ` original_table.update ( modified_table )` where ` modified_table ` is your dataframe , and then ` ... to_sql ( if_exists= ' replace ')` with this new dataframe object .

You could use ` loc ` and a boolean condition to mask your df ( here representing a.csv ) and set the label to 1 if that condition is met : #CODE

Pandas indexing by both boolean ` loc ` and subsequent ` iloc `
Now I would like to set the values of the first two elements returned in the filtered dataframe . Chaining an ` iloc ` onto the ` loc ` call above works to index : #CODE
This does work but is a little ugly , basically we use the index generated from the mask and make an additional call to ` loc ` : #CODE
EDIT : Have to be a little bit careful with this one as it may give unwanted results with a non-unique index , since there could be multiple rows indexed by either of the label in ` ix ` above . If the index is non-unique and you only want the first 2 ( or n ) rows that satisfy the boolean key , it would be safer to use ` .iloc ` with integer indexing with something like #CODE

Pivot table with Pandas float and int values
I must pivot multi values ??.... float and int ... but it doesn't work .
I do see a slightly better way to do it , though . I'll edit above . In short , you are specifying values with ``` value ``` , so you could just just convert that to float before doing the pivot . Those are the only columns that need to be converted and are presumably of a numeric type even if not held as int or float .

" Check length of DataFrame , If length of DataFrame > X , Remove first row , Append new row " ?
Unclear why you need to keep appending but if you want to impose a limit why not just check ` len ( df )` and not append once the len reaches your limit ?
@USER Something like a buffer dataframe of ` len ( df )= =X ` whenever new row of data comes in , the older or first row is removed , like FIFO , to preserve the buffer size .
Instead of appending , why not replace the old row with the new row ? You probably need to sort each time , but I image that's still going to be faster than actions that expand and shrink the dataframe . And as always , a small example dataframe with code makes it easier to get a good answer .
The simplest and most straightforward would be to use a ` collections.deque ` of tuples . You can just append a new tuple to the end , and if it gets too full it will dump the corresponding on from the beginning . At the end , you can just convert them into a ` DataFrame ` . I am just using the ` for ` loop as an example , I gather you get your data in a different way . It wouldn't matter : #CODE
One way would be to pre-allocate the rows , and replace the values cyclically . #CODE
In most of the scenarios that I deal with where truncated append performance matters , neither of the above are true , but your scenario may be different . In that case , @USER has a good solution involving ` .shift() ` .

Have a look at [ ` resample `] ( #URL )
Thanks , Ed . I guess I wasn't explicit enough -- I have been trying to use resample , but the updated DF starts at January 13th rather than January 1st .
Next , pivot the DataFrame , using the month as the index and the ticker as a column level : #CODE
Thanks so much ! That is a really elegant answer . I appreciate your time writing it up . I hadn't even thought of using pivot table , and the DateOffset trick is sweet . Good to have in the toolkit .

Edit : I have defined intersection as the following : #CODE
This will produce a boolean mask of the rows where they match one of the values in ` intersection ` and we invert the mask using ` ~ `
Doesn't this just remove elements from the DataFrame , as opposed to the entire row ? I'm getting a correct solution as ` df.ix [ ~ df.isin ( intersection ) .apply ( lambda x : x.any() , axis=1 )]`
@USER Also calling ` apply ` should be the last resort when working with arrays , it is not vectorised and therefore will not scale well
You could reset the index and drop the rows and set the index again , but the index should still be preserved , my example shows that the index values are preserved , you'll have to post data , code , desired output and errors giving feedback using comments is not that informative

How do I join 4 python lists to create a pandas dataframe ?
I have 4 lists , all of the same length and in the same order . I want to join them all together to create a table . A pandas dataframe will be the easiest way to work with the data so I'm wondering how I join all 4 lists ?
How should the list be joined ? Do you want it as 4 columns , or append them to have one long structure ? Better to show a small example of the desired output
Great - the 1st one is what I need . I'd seen documentation on converting dictionaries to dataframes but I wasn't sure if I could do it direct from lists . The other option was to create 4 dataframes from the individual lists and then join them but this didn't seem very efficient - the dictionary method will work better - thanks .

I guess the best option is to make an inner join and then the ( df2.c3 df1.c1 ) ( df2.c4 df1.c2 ) filtering but the problem is that the inner join would create a huge table , since classid are not indexes and not unique row identifiers . If filtering could be applied concomitantly that might just work . Any ideas ?
You can supply series for comparison but it depends on what you're doing , you should just filter the df first and then you could iterate over the unique classid values and then drop the rows where the other 2 criteria are not satisfied
Iterating should be the last resort , I'd merge the other dfs columns c1 and c2 to df : #CODE
The filtering on groups is not what I need , but the merging is what i intended through " left inner join " . The merged df should be filtered then like this df [( df [ ' c3 '] < df [ ' c1 ']) & ( df [ ' c4 '] < df [ ' c2 ']) ] . And now we arrive at the core of my problem : the merged table is simply too large in terms of RAM , isn't it a way to perform filtering the same time with merging ? This is how databases do , that is why you can have " join " and " where " in the same SQL statement .

I was looking for an answer to a similar question but to produce a mean etc on nonzero items .

You'd have to resample your data to daily , at the moment it doesn't look like your lat and lon change at all over the course of the day , at the moment your question is very broad as you are a long way from the final code so really this question should address the distance measuring by itself and you should then check existing questions about resampling

I am new to pandas . I have dataframe with two columns dt ( date-time stamp ) and value .
So long as dt is a datetime dtype already you can filter using date strings , if not then you can convert doing this : #CODE

apologies if this is a silly question , but I am not quite sure as to why this behavior is the case , and / or whether I am misunderstanding it . I was trying to create a function for the ' apply ' method , and noticed that if you run apply on a series the series is passed as a np.array and if you pass the same series within a dataframe of 1 column , the series is passed as a series to the ( u ) func .

With ` df ` as above , make a pivot table : #CODE
What's wrong with the answer using ` pivot ` ?

I know the ` merge ` would allow me to add the column , but I would have to either overwrite the existing DF or create a new one , like this : #CODE
Is there not a way to join on one field ( maybe an indexed column , maybe not an indexed column ) and update / add the field ?
Perform a left ` merge ` : #CODE
I got that to work . Thanks . In SQL , you can alter a table and add a new column . In ` pandas ` , specifically with this merge , I would need to overwrite DF2 . Is that standard ` pandas ` functionality just to overwrite ?

The ` loc ` call will mask the lhs so that the result of the transform aligns correctly

How did you create the pickle file ( was it created in pandas ? ) , can you post the entire stack trace ?

@USER also to get the total number of groups use ` len ( df [ 1 ] .unique() )` . Ok , not sure why that is , you've been able to do that since forever , perhaps the columns names are strings `' 0 '` ?

mode isn't a groupby method , though it is a Series ( and DataFrame ) method , so you have to pass it to apply : #CODE

For each set_up in set_ups I want to apply ' value ' to ' set_up ' and groupby ( level=0 ) or df.A and df.B . #CODE

First , we can perform a groupby / apply operation to obtain the Protein / Peptide pairs with the two largest Peptide counts for each Protein : #CODE
Now we can merge the original DataFrame , ` df ` with ` counts ` , by joining on the columns of ` df ` and the index of ` counts ` .
Using an inner join guarantees that only those Protein / Peptide pairs which are present in both ` df ` and ` counts ` show up in ` result ` : #CODE

I remember seeing a similar question and someone came up with a great answer to it , but I can't find it . As a start , this will give you every row where the difference is greater than 5000 . data [ abs ( data [ " 2 "] - data [ " 2 "] .shift() ) > 5000 ] . I guess you could iterate through that and slice accordingly
This is yet another example of the compare-cumsum-groupby pattern . Using only rows you showed ( and so changing the diff to 100 instead of 5000 ): #CODE

However , I am unable to merge that result back into the original DataFrame .
set the index of ` df ` to ` idn ` , and then use ` df.merge ` . after the merge , reset the index and rename columns #CODE

so if I have following groups [ g1 , g2 , g3 , g4 , g5 ] , i want to iteratively call them in pairs like [ g1 , g2 ] , [ g2 , g3 ] , [ g3 , g4 ] .... and take the intersection of the 2 groups of series everytime . I am looking for way to call groups [ g1 , g2 ,.. g5 ] by index or some no . so that I can use them for loop operations . Currently only way I know to call groups is through the names of the group , as mentioned above in example ' foo ' and ' bar ' .
so if I have following groups [ g1 , g2 , g3 , g4 , g5 ] , i want to iteratively call them in pairs like [ g1 , g2 ] , [ g2 , g3 ] , [ g3 , g4 ] .... and take the intersection of the 2 groups of series everytime . I am looking for way to call groups [ g1 , g2 ,.. g5 ] by index or some no . so that I can use them for loop operations . Currently only way I know to call groups is through the names of the group , as mentioned above in example ' foo ' and ' bar ' .

pands groupby by diffent key and merge
and also the every user_id , item's amount of distinct type_id , and merge then by the user_id

It creates a heat map as shown below . You can see the numbers are not huge ( max 750 ) , but it's showing them in scientific notation . If I view the table itself this is not the case . Any idea on how I could get it to show the numbers in plain notation ?

Apply function to multilevel columns

I am wondering if there is a way to do something clever ( or basic ) with pandas to make use of vectorization - maybe using shift or an offset function ?

I would like to join / merge them , sort of an outer join so that rows are not dropped .
None of the pandas tools I know worked : ` merge ` , ` join ` , ` concat ` . merge's outer join gives a dot product which is not what I am looking for , while ` concat ` can't handle non unique indexes .
You want to use the ` on= ' outer '` argument for ` join ` ( ` test1.csv ` and ` test2.csv ` are the files you gave ): #CODE
I've managed to sort it out using pandas ' ` concat ` method .
Now we can use ` concat ` : #CODE

I'd look at seeing if you can export it in it's raw form , otherwise this must be a common problem and someone somewhere has probably coded a method to strip the emojis out of the text

where list_1 and list_2 was generated because len ( group_set ) from my current code is equal to 2 .

and then drop the NaN .

I renamed your columns to ' author ' and ' citations ' here , we can groupby the authors and then apply a lambda , here the lambda is comparing the number of citations against the value , this will generate a 1 or 0 if true , we can then sum this : #CODE

Pandas : column of type str converted to tslib.Timestamp after using apply function
Thank you . That worked ! Is this a implementation problem of pandas or the apply function that I was using ?

You can apply multiple functions to multiple fields : #CODE

I realise I could do each field seperately but in the actual problem I am trying to solve I perform a join between the table being updated and an external table within the " updater " ( i.e. square ) and want to be able to grab all the required information at once .
Here you can see the exact details of what I am trying to do . I have a table " to_update " that contains point-in-time information against an item_id . The other table " the_updater " contains date range information against the item_id . For example a particular item_id may sit with customer_1 from DateA to DateB and with customer_2 between DateB and DateC etc . I want to be able to align information from the table containing the date ranges against the point-in-time table .
Please note a merge won't work due to problems with the data ( this is actually being written as part of a dataquality test ) . I really need to be able to replicate the functionality of the update statement above .
So what are you trying to achieve here ? for squaring the values this is trivial to perform and doesn't require the use of ` apply ` in this case
I am trying to create arbitrarily many fields and assign them values simultaneously using the apply method . The simple functions in the example just represent any arbitraryfunction I may want to use for generating a new field . The issue is when trying to create more than 1 field in the above example it errors . Once I can do this I can replace the trivial function with any value .
This will certainly fix the problem . But it will involve having the apply return only a single field at once . What I was curious about was being able to use apply to return arbritrarily many fields . By being able to do this it should allow for cleaner code .

Unlike Numpy , Pandas doesn't seem to like memory strides
That is , it gives me 3 strides of 3 rows each , in a 3d matrix , allowing me to perform computations on a submatrix moving down by one row at a time .
The Numpy array ` mm ` is " C-contiguous " and that's why the stride trick works . If you want to call the exact same code on the array underlying the DataFrame , you can use ` np.ascontiguousarray ` first . Or maybe it would be better to write the data windowing while taking the array ` strides ` and ` itemsize ` into account .
To fix things , you could copy the data in C order and use the same strides as in your question : #CODE
Alternatively , if you want to avoid copying large amounts of data , adjust the strides to acknowledge the column-order layout of the data : #CODE

Pandas : Weighted median of grouped observations
I would like to compute the median income group . What do I mean ?
which visually is what I mean - the median here would be ` 7 ` .
Sorry you want the index of the median value of INCAGG ?
Well the median value of your incomes is the one at row 5 is what pandas tells me
Oh , second column are weights . You just want a weighted median . Maybe something like this #URL But in general , searching for " weighted " whatever should give a lot of results . This is really easy in some other stats packages ( like stata ) , but I don't know if it is as easy in pandas . Maybe scipy or statsmodels has something ?
The second code line above is dividing into rows above and below the median . The median observation will be in the first ` False ` group . #CODE
This will work fine when the median is unique and often when it isn't . The problem can only occur if the median is non-unique AND it falls on the edge of a group . In this case ( with 5 groups and weights in the millions / billions ) , it's really not a concern but nevertheless here's a more general solution : #CODE
You can use chain from itertools . I used list comprehension to get a list of the aggregation group repeated the appropriate number of times , and then used chain to put it into a single list . Finally , I converted it to a Series and calculated the median : #CODE

Pivot Table to Dictionary
I have this pivot table : #CODE
How do I make a dictionary with ' store_nbr ' as the key and ' item_nbr ' as the values from the pivot table shown .
What did you pivot from ? Would a ` groupby ` from there work ? ( is a dictionary to start with . )

As a first step you could load the first csv and then drop the duplicate authors and write this out as the clean csv , this sounds like a messy many to many relationship , it sounds like the publication is the unique thing here so you probably want to repeat the publications for each author
@USER I originally had it insert directly into an sql database . But that was taking even longer . Then someone told me it might be faster to just parse it neatly into a csv file and just use a command like .import on sqlite3 to import it directly into my tables . That's what I'm doing here .

I've since noticed that removing the resample ( " W ") solves the issue . It is still a problem however as the non-resampled data is too noisy to be visible . Being able to plot sampled data with a secondary axis would be hugely helpful .
Can't tell without seeing your data ! How about posting ` head ` of the various dataframes ( incl . the result of resample ( ' W ') , etc . ) . Also , * this * version works * with * your real resampled data ? ( just checking )

And receive ` TypeError : filter function returned a list , but expected a scalar bool ` .
The original approach you suggested is correct , although you have to use a ` transform ` on the groups ( by ` date ` AND ` source `) instead of an ` apply ` . ` transform ` return the group information with the same structure of the original dataframe . #CODE

You could compare the length of ` len ( df ) == len ( df [ ' ID '] .unique() )`

Ok yes you are right , I do want to keep the values in the dictionary . I would like to avoid using an append because my dataframe is has over four million lines , so I think an append might take a little too long . I am going to work on it more over the weekend .
It splits up the DataFrame by store_nbr , calls is_good on each each group ( apply ) to determine the rows you want to keep , puts everything back together in the right order , and then takes a subset of rows from the original frame .

I'm not sure what your level of expertise is based on the question , but I'd start by looking at the docs on pandas ` merge ` .

If you want to apply it on a full column , you can also do : #CODE

I need to do a boxplot by month . There are two ways of how I'm doing it : first to groupby by the index , copy to a new dataframe , and do a groupby by month : #CODE

I can't run your code as ' cut ' is not defined , but what version pandas and numpy are you using ?

However , if you are doing several things here often there is a simpler way . For example , whenever you see " groupby + unstack " you should think " pivot_table " : #CODE

How to insert one column in an existing DataFrame into a new column in a table in existing database
ID is a primary key in an existing table in my database . I created a new column in my table ( which originally has two columns ID and Name ) , labeled " Total " and I want to insert column 1 values into it for each corresponding ID .
I'm currently regenerating the existing table in the DataFrame with the new column Total in the end . Then rewrite the whole table again using df.to_sql ( ..., if_exists= ' replace ') .
@USER I guess I can recreate the whole table , but it seems to be a round about way of doing it as opposed to directly just inserting it . But you're probably right , it probably would be faster than looping and inserting row by row . I will try to recreate the table using a query and replace it with my DataFrame using df.to_sql ( ..., if_exists=replace )

Now I want to do a boxplot by month , so I would imagine that I can group by it : #CODE
How can I create this boxplot without the dummy dataframe ?
this then allows you to groupby by month and generate a boxplot : #CODE

No columns are text : only int , float , bool and dates . I have seen cases where ODBC drivers set nvarchar ( max ) and this slows down the data transfer , but it cannot be the case here .
The only thing I can think of is to export just the structure , i.e. column names and data types but no rows , to SQL , then export the file to CSV and use something like the import / export wizard to append the CSV file to the SQL table . This way I don't have to define all the column types again ; this is important because import tools tend to read the first x rows to guess data types , and if the first rows are all NULLs the guess will be wrong . However , the fact remains that the to_sql method is practically unusable other than for tiny tables . Have you experienced this with other databases , too ?
Or just export the data to a csv and then use bulk insert ( which is very , very fast ) . You will have to build a format file but it might be worth it . [ link ] ( #URL )
The ` DataFrame.to_sql ` method generates insert statements to your ODBC connector which then is treated by the ODBC connector as regular inserts .
The proper way of bulk importing data into a database is to generate a csv file and then use a load command , which in the MS flavour of SQL databases is called ` BULK INSERT `
Those interested in doing a BULK INSERT into SQL Server via Python might also be interested in having a look at [ my answer to a related question ] ( #URL ) .

grouping into time intervals can be done using resample : #URL
Then we resample #CODE

So if you assigned the data to the values rather than the df itself then the df does not try to align to the passed in index

Average of aggregated value by month and boxplot average differs on pandas data frame
So I have a line graph with the mean values per month . Now I want to generate a boxplot , so I use this : #CODE
And with the boxplots , the averages ( the bars in the middle of the boxplots ) are different from the previous graph . I this expected ? ( I can't post graphs right now and it seems that the example dataset is ok , but I have a larger data set that is in the exactly same format that is giving me trouble ) . Boxplots provide the median , not the mean .
Box plots use the median , not the mean .
It's not a Pandas specific feature . Most box plots use the median . There's a description of the plot [ here ] ( #URL ) . The Pandas [ documentation ] ( #URL ) does mention that the median is used , although it doesn't describe that it is the central line .

I'm making use of pandas , and I've iterated through each of the files to build an array that includes all of the unique values I'm trying to replace . I can't individually just use pandas.factorize on each file , because I need ( for example ) ' 3001958145 ' to map to the same value on file1.csv as well as file244.csv . I've created an array of what I'd like to replace these values with just by creating another array of incremented integers . #CODE
With as capable as pandas is with dealing with categories , I assume there has to be a method out there that can accomplish this that I simply just can't find . The function I wrote to do this works ( it relies on a pandas.factorized input rather than the arrangement I described above ) , but it relies on the replace function and iterating through the series so it's quite slow . #CODE
If we are iteratively processing these frames , we use the first ones to start . To get each successive one , we add the symmetric difference with the existing set . This keeps the categories in the same order , so when we factorize we get the same numbering scheme . #CODE
To prove it , we select the codes ( the factorized map ) from each . These codes match ; df2 has an additional code ( as Z is in the 2nd frame but not the first ) . #CODE
Thank you for the incredibly well thought out response . That explanation was crystal clear , and extremely helpful ! I wasn't familiar with the symmetric difference method ( it's a SQL JOIN ! ) , but I see how that will be useful in the future . I'd also completely missed the fact that the categories dtype factorizes " under the hood " , and that the codes are readily accessible . The memory aspect makes perfect sense , too . Thank you for pointing it out .

The source data only records changes in amounts , and I want to insert the missing dates with the amounts pre skip .
I then unstack the DataFrame so that I just have the dates in the index . #CODE
Finally , I fill forward the values to remove the NaNs , and stack the currencies : #CODE

However , you can easily switch rows and columns with the transpose ` .T ` , and then it may be more tractable , and in fact the control mean is a one liner . #CODE
It helps to have the ` Control ` values in their own columns . You can do that using ` unstack ` : #CODE

Python pandas : How do I drop specific levels in a hierarchical index if any column values are NaN ?
I want to drop entire levels ( in this case , countries ) in my hierarchical index if ANY of the data values for that country are NaN . So I want to go from something like this : #CODE
Yes there is a concise and efficient way to solve this . You were on the right track with ` df.dropna() ` , just that you need to ` unstack ` your data before you apply it . #CODE
pivot the ` DataFrame ` to make " Year " the inner most column labels #CODE

I want to cut this one by a specific timerange . My code is : #CODE

How big are we talking here ? you could read all the csv's into a list of df's , then ` concat ` them all set the index to be the time and then filter using ` df.loc [( df.index.time > ' 09:00 : 00 ') & ( df.index.time < ' 09:15 : 00 ')]` , whilst loading them you could sort on index and chuck away the ones that don't have that range anyway

If there is a way to subtract the DateTime as well ? My way would be to only select Order 1 as a dataframe , then only select Order 2 , then merge , then subtract . Is there a way to do it automatically ?

" Am I overthinking this / prematurely optimizing ? " Yes . The implementation of DataFrames is very efficient for this , and doing the agg internally is creating copies just like this . You shouldn't worry * unless * it's a ( performance ) problem . ` .columns ` is more efficient as it just replaces one list with another , rename has to lookup each entry and replace it ( useful sometimes ) . Be warned , using the inplace flag often creates a copy under the hood .

My question : is there some way to do this all in one ` agg ( ... )` step , without having to create a second result ` DataFrame ` and performing the ` merge ` ?
You can drop a level #CODE
This might be a better option . It uses a list comp to concat your column names which I got from here #CODE

Added how to convert the ' datetime ' string column to Timestamps . There are many such techniques available on Stack Overflow .

The last expression would apply to your case if ` res_tmp.fittedvalues ` are the predicted or fitted values of your winsorized model , and ` y_orig ` is your original unchanged response variable . This definition of R squared applies if there is a constant in the model .

Tried it and it really messes up things . Now if you get two rows with 1 match they will have len ( cols ) -1 miss matches , instead of only differing in non-NaN values . Results are way different . Thanks anyway .
By now , you'd have a sense of the pattern . Create a ` distance ` method . Then apply it pairwise to every column using #CODE

To make it clear you only want to assign a copy of the data ( versus a view of the original slice ) you can append .copy() to your request , e.g .

You cannot deceive df.iloc by changing the index the way you propose . df.iloc [ 0:29 ] will give you the first 29 rows , regardless of the row label . Perhaps you meant df.loc [ 0:28 ] , noting loc is an inclusive slice .

We can then iterate over this list of tuples and slice the orig df and append to list : #CODE
Find the ` NewEntry ` rows , add ` [ -1 ]` and ` [ len ( df.index )]` for boundary conditions #CODE

You could ` df.groupby ( level=0 )` but why would you ` df.set_index ( ' date ')` then ? Before ` set_index `' ing the dataframe , you could have ` df.groupby ( ' date ')`' ed ? But , if your looking to daily / monthly aggrgartions the function , you are better off using ` resample ` or ` TimeGrouper ` !
I have used the ` resample ` but it also need the function such as ` sum ` , ` mean ` or etc

How to replace comma with dash using python pandas ?
I am trying to load the data into a dataframe and count the number of keys in in the count_dic . The problem is that the dic items are separated with comma and also some of the keys contain comma . I am looking for a way to be able to replace commas in the key with ' - ' and then be able to separate different key , value pairs in the count_dic.something like this : #CODE
Since ` count_dic ` is actually a ` dict ` , then you can apply ` len ` to get the number of keys , eg : #CODE

But then ` len ( data )` would equal 1 , so ` len ( data ) 0 ` is not the right condition to check to see if ` data ` is an empty list of lists .

the ` [ 3 ]` at the end then selects the third column ( which would return a Series ) . But since you want a list , just append .tolist() to the result .

Python DataFrame - apply different calculations due to a column's value
You could do this using 2 ` loc ` calls : #CODE

I want to replace the NaN values with a ' yes ' or ' no ' depending on which count is greater based on the ' first ' column and if they are equal make it a ' yes ' . For example this is my original dataframe . #CODE
You could use something like ` g = test.groupby ( ' first ') [ ' second '] .value_counts() ` and then use the first row of each of the groups as the ` fillna ` value . However , in your current example ` b ` doesn't conform to your logic as ` b ` has one ` yes ` and one ` no ` .
We then use boolean indexing to filter for the NaNs . Then we map a lambda function that takes the first row of our boolean indexed testCounts #CODE
Boolean index like before and map the dict ( d ) to " first " #CODE

How do I merge two tables and divide numerical instances from a previous value count in each cell in Python ?
I figured the logic to this would be to merge original table with pivoted table and have each cell with a numerical dividing each particular sum of instances by the value.count of the PlayTennis responses , but I just don't know the syntax for it .
The other thing you might have wanted to do is to calculate the mean values for the ' Temperature ' columns , while calculating the proportions under ' Values ' . This would make a bit more sense . To do this , you can use different aggfuncs when you create your initial pivot table , as follows : #CODE

And , then insert it into the query string - #CODE

pandas : read_csv how to force bool data to dtype bool instead of object
When reading the csv the bool column gets typecast as object which prevents saving the data in hdfstore because of serialization error .
To convert the ` NaN ` value use ` fillna ` , it accepts a dict to map desired fill values with columns and produce a homogenous dtype : #CODE

If your data has empty dicss , then you can just test the ` len ` : #CODE

merge multi-indexed column names on pandas dataframe
Is there a simple way to merge the column names , so that I end up with a simple column index with their proper names ? In the end , I would like to have the equivalent of #CODE

then put the pieces back together using concat : #CODE

Here is the code so far ( apologies , tried to cut excess off but leave enough to follow + txt ): #CODE
Ach , that's a lot of code . I notice one shrieker : you're using map as a variable name ? Don't ; it's a reserved word in the language and wierd things will happen .
` data [ ' projected_lon '] , data [ ' projected_lat '] = map ( * ( data.Lon.values , data.Lat.values ))` is just wrong , I think . I don't think you should call ` plt.Figure() ` every time you go through your i-loop . * Really * minimize this example , not just Frankenstein it out of other code , and you'll probably fix it yourself .

Replace negative values in dataframe by NaN , replace NaN with fillna method
I would like to replace negative entries in the column of a dataframe .
I am setting the values to NaN and then apply the fillna method .

replace series data with blanks
Aside from display reasons why do you want to replace with blank , is it just so that when you export it back to xls you get the display you want ?
drop duplicates will complete remove those rows . I need to keep the rest of the data and remove only repetitive cells from one column . This will be used for generating reports in specified format .
Use ` loc ` and ` shift ` to detect when the rows change value , we can then use the boolean mask to set these rows to blank : #CODE

join multiple csv files in one folder using pandas [ MemoryError :]
I have 5 csv files in one folder , In here I want to join all columns from each csv file into one dataframe . When I only join 2 csv files , it works well , but when I want to join 5 csv files , I faced an error . My code is following below : #CODE
Your indices are duplicated so basically when performing a right merge it's exponentially finding matches hence why you run out of memory , What is the first column supposed to be ? You could just ignore it : ` pandas.read_csv ( os.path.join ( file ) , index_col=None , header=0 )`
I've posted an update as I think that seeing as your dfs are equivalent row-wise you should be able to just append to a list and then ` concat ` them all
Also looking at what you're doing I think you don't need to merge , just ` concat ` a list of dfs : #CODE

So you want to set all rows to the URL ? it's tricky with lists , as it's an iterable it's expecting the length of the elements to match so the following would work : ` f [ ' DFA '] = [[ ' Binary ' , ' Html ']] * len ( f )` this creates a list where it's repeated by the length of the df

Re your question about bootstrap_plot doing the Right Thing : well , this is an area where pandas is still improving in general , but there's often going to be a bit of manual labor in this area and it's not generally that hard to do something with fillna or notnull . And honestly , it's often a feature to be forced to do this rather than have missing values handled automatically in a way you might not have liked or even been aware of .

@USER , I too have had trouble with @USER ' s approach ( using ` manager.Namespace() ` to share DataFrames between processes ) , particularly if the df is large ( millions of rows ) . Even if I only have 1 worker child process , doing a simple ` df.loc [ ix ]` can start taking many seconds each time , and the worker does seem to be duplicating the data from the managing process . ( In single process profiling , the same operation is really fast )

Drop observations from the data frame in python
You could build a boolean mask using ` isnull ` : #CODE

And then I'd have to do a truncate table . However , there are inconsistencies between the to_csv and the to_sql methods : to_csv writes boolean fields as the strings ' TRUE ' or ' FALSE ' , whereas to_sql writes them as 0 or 1 . This means that importing files creates with dataframe.to_csv is more complicated than it should be .
... will let you map the columns to their respective SQLAlchemy types .
It seems I can pass a dictionary to dtype if I want to override pandas and SQL alchemy's default data type mapping . This is useful . One question remains : can I control the constraints that Python creates on the table ? E.g. for boolean columns , it adds the constraint that the values must be 0 or 1 ) . I can run a SQL statement to drop or disable the constraints , but I'd like the option to create the table with no constraints at all in the first place

Resample a Pandas dataframe with coefficients
My goal is to resample the data and average it using the coefficiets , ( missing data should be left out ) .
The problem is that the code runs forever , and I'm pretty sure that there's a better way to resample with coefficients .
Could you please help me understand how the correlations translate to the expected output above ? My understanding was that on day 4 , for example , you would want ` ( 0.2 * 19.2 + 0.6 * 20.1 ) / 0.8 ` which is ` 19.875 ` , not ` 19.97 ` . If you could walk through just day 4 or the day 3 calculation , that'd help .

After loading your csv using ` read_csv ` like so : ` df = pd.read_csv ( file_path )` you can then use ` pivot ` and pass the columns Employee and Date as the columns and index respectively : #CODE

Please show the format of ` user_input ` , also you should be able to replace ` frame = pd.DataFrame()
the fact remains your code as it stands does nothing with it as you overwrite it with the result of ` concat ` , but after the concat line you should change ` frame [ ' Date '] = pd.Series() ` to ` frame [ ' Date '] = pd.to_datetime ([ user_input ] , format= ' %m . %d ')`

I see - thanks . So I'd need to specify the option for each boolean column ? Also , the to_sql method creates two such constraints for each boolean column . One possibility is that pandas creates one and sql_alchemy creates another , but I'm not sure yet . Either way - it's wrong ! For now I'm finding it less fiddly to disable or drop the constraints with a SQL statement .
Yes , for now you need to specify it for each column ( unless sqlalchemy provides something for that ) . But you can do this with eg ` {col : Boolean ( create_constraint=False ) for col in df.select_dtypes ([ ' bool ']) .columns } ` . You can always report an issue for such an enhancement if you think of a way to make this easier .

I think the main problem here , is because I'm trying to apply the ` pool.map ` to ` rpy2 ` function and not a Python predefined function . Probably there is some workaround solution for this without using the multiprocessing library , but I can't see any .

Sorry are these strings or integers you're converting , for the % one you could strip the % sign off and then cast it , also can you post raw data to show us what it looks like , thanks
Modify your ` replace ` to include ` regex=True ` : #CODE

If this is just for numerical work you're doing in an interpreter , ` eval ` is alright , but for any code you're releasing , it's very insecure : it will evaluate whatever code is in the string passed into it , thus allowing arbitrary code execution .

There's nothing wrong with the existing answers , but they aren't using any of the built in pandas solutions which can make things easier . If you're starting with things in string format you can convert to datetime and then use pandas ` dt ` to extract hours and such . #CODE
Now you can extract hours / minutes / etc . very easily with ` dt ` methods . #CODE
dt accessor

How to change the arguments of map function to get repeatation of dates in two groups :

I am struggling with the easiest way to do a case insensitive merge in pandas . Is there a way to do it right on the merge ? Do I need to use ( ? i ) or a regex with ignorecase ? In my code snippet below I am joining some Countries where it may be " United States " in one file and " UNITED STATES " in another and I just want to take the case out of the equation . Thank you ! #CODE
Lowercase the values in the two columns that will be used to merge , and then merge on the lowercased columns #CODE

You could use ` merge ` and extract the overlapping keys #CODE

ok.will check on pivot table

So it there a way to " merge " them instead of place side by side ?

maybe the .map() was trying to apply both the mask and the operation : #CODE

I'd either construct a df for each dict and then ` concat ` them all , or flatten all the dicts into a single dict and construct a df from that flattened dict

Thanks . I edited my code as follows , yet I still have the same issue : ` tickersasstrings = map ( str , tickers )

Unstack a MultiIndex pandas DataFrame counter-clockwise instead of clockwise
My objective is to create a horizontal stacked bar chart with the bars , representing Severity , in left-to-right order of : moderate , high , critical . In order to get a stacked bar chart it seems like I need to unstack the Severity index column . When I try to do that , like so , ` df.unstack ( ' Severity ')` , my DataFrame looks like this : #CODE

pandas - unstack with most frequent values in MultiIndex DataFrame
I would like to group the ` User ` field , keeping only the most frequent ` DLang ` code , then unstack and count the numbers of ` User ` in each ` GridCode ` . So far I did : #CODE
Also , I think the example data you provided is cut off by a few rows ( there is only one ` ca ` entry , not 3 ) . I changed this , but you may want to update / check the example data and the resultant data frames .
Merge just the most frequent / max occurrence df with the original input . This will drop rows where users used a ` DLang ` other than the most frequent #CODE

So the boolean condition looks for freq values greater than 5 and also where the word is not in the other df using ` isin ` and invert the boolean mask ` ~ ` .

I can't think of a clever vectorized way to do this , but unless performance is a real bottleneck I tend to use the simplest thing which makes sense . In this case , I might ` set_index ( " Gene ")` and then use ` loc ` to pick out the rows : #CODE
Ah , that is very nice use of loc and quite a bit simpler than mine . I just realized he's only asking for the subset in mygenes so if it's only a few at a time then simplest is definitely the best . FWIW it don't think it would be too hard to vectorize the numpy code I used ( if it was worth the bother ) .

You can use the apply function : #CODE
@USER OK , let me see if I'm understanding you , what you say is that the columns to be compared are dynamically generated ? Well if that's the case you can construct the ` eval ` string , I will put an example on my answer ;)

median of pandas dataframe
I would like to look at the rows that are to within a factor of 10 from the median of the count column .
I tried ` df [ ' count '] .median() ` and got the median . But don't know how to proceed further . Can you suggest how I could use pandas / numpy for this .
I can use any measure as the distance from median ( absolute deviation from median , quantiles etc . ) .
If you're looking for how to calculate the Median Absolute Deviation - #CODE

I have a ground truth dataset ' gt ' ( with 100 entries ) which looks like this : #CODE
Could you also post expected output ? In other words , you want replace ` gt ` ' s ` org_o ` names from ` df `' s ` match ` and then take the counts or .. ?
@USER .carstensen yes and the fact that they belong to the same group in the gt data
Filter out rows with ` gt [ ' org_o ']` values in ` df [ ' org_o ']` using ` df [ ' org_o '] .isin ( gt [ ' org_o '])`
I find @USER ' s answer elegant . A minor addition would be in case , if ` gt [ ' org_o ']` has repetitive values , you can take unique array instead . #CODE
Here we are adding a new column ` tag ` to the ` df ` . We are using numpy's where function to check if the ` org_o ` in ` df ` is present in ` gt ` . If yes , then assign ` TP ` as the value of the ` tag ` to that row , otherwise assign ` FP ` .
As far as efficiency is concerned , this " lookup " is fairly efficient , because when using ` isin ` , pandas will convert the values to compare ( in this case ` gt [ ' org_o ']`) into a set , so the lookup time will be O ( n * log m )

Join & create buckets - Python Pandas
Step 2 - The criteria for the join is - #CODE
tip : join all of the records , then do the filtering later .
You can perform a ` merge ` , it will automatically align on the common columns and the default type is ` inner ` : #CODE
And , the result using ` merge ` #CODE

However , when I try and join the effective coverage series , and print out the dataframe of just one of the groups , it just returns : #CODE
And does not successfully join my newly calculated effective coverage .
And , then ` apply ` ` split_cumsum ` over ` df.groupby ( ' Group ')` #CODE
Thanks John , I didn't quite appreciate what circumstances would make sense to create a function and apply it to a DataFrame , but this solution is definitely a cleaner approach .
Also , you could use ` diff ` in ` groups ` #CODE

For each row , extract string items and join them . #CODE
That is not so surprising but ` apply ` does not scale well , I just did timings on a 600 row df and the timings were 6.24ms vs 33.3ms comparing my method against yours , I expect the performance difference to increase significantly on much larger datasets
@USER Absolutely , ` apply ` doesn't perform well on larger datasets . So , I mentioned * expensive for this smaller data . * =)

I have a datatable of 40000 dates . I got a unique list of the dates and I sorted them . Now what I want to do is for date ( t ) , append date ( t+1 ) to the dataframe . I don't want to write a for-loop since I have 40000 dates
alternatively if you want to replace the date column you can use ` replace ` #CODE
Then call ` map ` on the orig df and pass the ` date_df ` and access the ` date_lookup ` column , ` map ` will use the index to perform a lookup which will return the corresponding next value : #CODE

We don't want to just know the start of each group though -- we want to know what group each row is in . Easy-peasy -- ` Series.cumsum ` adds each row to the previous one . Conveniently , if you try to add ` bool ` s in Python , they get forced to ` int ` s , their superclass . #CODE

you need to unstack so ` type ` are columns , and then use the ` subplots ` parameter : #CODE

Is there some sort of ` apply ` equivalent ( like in ` pandas `) that would make this more efficient ?

Pandas Apply ( axis=1 ): produce more than one row
I have a function I want to apply by row like so : #CODE
As you can tell , this function is meant to take a list of items and create a row for each item that duplicates the rest of the remaining data . Unfortunately , my current method isn't the correct usage of the apply method : #CODE
This question is similar to pandas : apply function to DataFrame that can return multiple rows that Wes McKinney has answered .

I am trying to insert a pandas dataframe in to oracle table with the following code : #CODE
Isn't there any other means to insert other than this ? :( . Or should I go with looping " INSERT " ?

I have a dataframe ' gt ' like this : #CODE
and I would like to add column ' count ' to gt dataframe to counts number member of the groups , expected results like this : #CODE

It looks like the param ` index_col=0 ` is taking precedence over the ` dtype ` param , if you drop the ` index_col ` param then you can call ` set_index ` after : #CODE
An alternative is to drop the ` index_col ` param and just call ` set_index ` on the df returned from ` read_csv ` so it becomes a one-liner : #CODE

Call ` map ` and pass the dict , this will perform a lookup and return the associated value for that key : #CODE

Let , ` df ` the dataframe with two columns , apply conditional absolute minimum over rows using ` axis=1 `
OK , after reading and understanding your question and not being able to find a vectorised approach , we can define a custom function and call ` apply ` and pass each row .
So this will check if either column is null if so return the min value , it then compares the abs value of either column and then returns the column that has smallest abs value but the original value including sign : #CODE

You can try to run * SQL Server Profiler * and take a look at what happens directly on the database level . Alternatively , you could try to use various values for a ` chunksize ` argument , which might have an impact depending on the number of rows you want to return . But * pandas * basically does few queries : check if table exists , create it if needed , and the insert statement itself .

` df.values ` returns a NumPy array containing the values in ` df ` . You could then apply ` np.std ` to that array : #CODE

Pandas DataFrame merge between two values instead of matching one
I have a Dataframe with a date column and I want to merge it with another but not on a match for the column but if the date column is BETWEEN two columns on the second dataframe .
I believe I can achieve this by using apply on the first to filter the second based on these criterion and then combining the results but apply has in practice been a horribly slow way to go about things .
Is there a way to merge with the match being a BETWEEN instead of an Exact match .
instead of merge on date=BeginDate or date=EndDate I would want to match on date BETWEEN ( BeginDate , EndDate )
I ended up realizing I was over thinking this I added a column called merge to both tables which was just all 1's
then I can merge on that column and do regular boolean filters on the resulting merged table . #CODE

Set column name for apply result over groupby
What I'd like to do is assign a name to the result of ` apply ` ( or ` lambda `) . Is there anyway to do this without moving ` lambda ` to a named function or renaming the column after running the last line ?

Pivot table problems : error ' No numeric types to aggregate '
I can't figure out why my pivot table isn't working , here is the pivot table and a sample of the data I'm working with : #CODE

Welcome to Stack Overflow ! It is generally expected here that you either show code you've written or describe research you've done to try to solve your problem . As your question is currently written , it sounds like a request for code . Have you made any attempts you can share with us ?
Also , You can ` groupby ` your data on `' Outlook ' , ' PlayTennis '` get the count and use ` unstack ( ' PlayTennis ')` #CODE

There is data for several variables that I will be grabbing like this , and then I will be combining them into a single DataFrame to be output to a csv . Each variable's data column will be added as a new column with the corresponding name of the variable ( as the file_id should always be the same ) . The time column values might be different ( one DF could be longer than the other , the data wasn't sampled at all of the same times , etc ) , but if I merge the tables on the time ( and file ) column , then any discrepancies are filled in with NaN ( and I will fill them in with DF.fillna ( 0 )) and the DF can be resorted by the time .
You haven't finished simplifying this question yet -- keep going until you have sample inputs and a sample output . Currently I can't even tell if you want to resample when you query , when you concatenate variable results , or when you write to csv .
Yeah , I'm not sure either ! If you had an example or suggestion of how to resample at any of those stages it would be helpful to at least see what that would look like . I already have some code that gets me to this stage ( all of the DFs for the different variables merged into a single table ) , so resampling the full table seems to me like the way to go , but I've been wrong before and will be again .

So are you wanting it to align to the ` df.index ` ? it should work if you convert the series to a list : ` df [ date ] = list ( series_for_date )`
have you tried ` join ` or ` merge ` ?
If i understand you can use ` concat ` : #CODE

Has anyone else dealt with this and found an effective workaround ? I cannot use a straight append because I need the ' column merging ' ( for lack of a better word ) functionality of the ` join= ' outer '` argument in ` pd.concat() ` .
... and so on repeating . I am concatenating on column name with an outer join which means that any columns in ` df2 ` that are not in ` df1 ` will not be discarded but shunted off to the side .
Looks like you are trying to row-wise concat , even though you text indicates that you what column-wise . Specify ` axis=1 ` .
Concat , note using ` axis=1 ` as this is column-wise concat . #CODE

I have some code , reduced to the example included below , which takes some raw data , creates a pivot table from it , then merges it with another dataframe , and finally stores the results in an HDFStore object . If it's stored in fixed format , it can be retrieved just fine . However , if stored in Table format , it produces an error . I need table format so that I can extract chunks at a time ( total dataset is tens of millions of rows ) .

Symbol_list is a list of symbol names , ie [ " SPY " , " GE "]
These can be concatenated into a single DataFrame , which makes it easy to manipulate . We make a pivot table out of it , forward fill the missing values , and re-order the column levels for convenience . #CODE

I am interested in finding a fast method for looking up linearly interpolated colors from a defined lookup table . The purpose if to assign colors to a large number of items at run time based on a given color map . The colormap lookup table is a list of tuples that contains Values ( Ascending ) , Red , Green , Blue , Opacity
You sort of are using numpy if you use pandas since pandas is built on top of numpy ;-) Also , you did mention efficiency and numpy will be much more efficient than standard python lists . As far as lookup + interpolation , I don't think you will find that combo , but it seems your problem can just as well be approached in two steps -- 1 . lookup , 2 . interpolate . FWIW .
If you can pre-process your lookup table ahead of time , you can replace the binary search + interpolation with a simple lookup . That is as long as you're willing to accept the possibility of not fully accurate output , which should be the case for colors - off by one errors are very hard to detect .

which I want to translate it to list of dictionaries per row #CODE
Welcome to Stack Overflow ! I indented your code sample by 4 spaces so that it renders properly - please see the editing help for more information on formatting .
Use ` df.to_dict ( ' records ')` -- gives the output without having to transpose externally . #CODE

Yes , that was what I was planning to do . However , I know realize it is not the correct way to do it since they are not independent . Your answer give me a better understanding how to use pandas groupby and apply . Thank you very much !
What you want to do is ` groupby ` on the index levels and apply a function that calls ` mannwhitneyu ` , passing the two columns ` course1 ` and ` course2 ` . Suppose this is your data : #CODE

You're performing [ chained indexing ] ( #URL ) which may or may not work , the recommended method for indexing with a view to updating / adding values is to use the new ` iloc ` , ` ix ` or ` loc ` , please see the [ docs ] ( #URL )
@USER I understand that ` loc ` is used for label based indexing . How can I use ` loc ` to index the correct value with this MultiIndex in my DataFrame ? Something like ` df.loc [ k ] .loc [ i ] .loc [ j ]` ?

` loc ` supports slicing the beg / end point is included in the range : #CODE

Yeah . this works . I wrongly assumed sum() would not apply to lists . Thanks a lot

I can obviously sort the ` dataframe ` but I am unsure on how to translate the labelling . Row indexes are not affected by ` sort_index ` : #CODE

apply if statement within sort.head()

Pandas pivot and create extra columns for duplicates

I note that the Pandas book presents an example with stock values across multiple companies with time in the major axis , open / close / etc in the minor axis and stock symbol in the items . These data do not map well to my situation .

Am I not allowed to use a series in a pd.apply function ? If so how can I apply a function row by row and assign the output to a new column ?
You don't need to use apply when calling the function . Just use : #CODE
I reworked the formulae to apply to series : #CODE
Because the parameters lon2 and lat2 are Pandas Series , dlon and dlat will both be Series objects as well . You then need to use apply on the series to apply the function to each element in the list .
Okay , I just restarted the kernel in my ipython notebook and now with the above i get this error - ` AttributeError : ' numpy.float64 ' object has no attribute ' apply '` for the a= code that you gave above

I think it needs to be ``` df.cat.code.categories ``` . I think I may have accidentally chosen a very poor column name in ' cat ' ( I'll change that ) . It looks like you need to use ' cat ' like ' str ' or ' dt ' .

Do you just want to print it or actually DO something with it or to it ? E.g. if you want to apply a function to every element , see ``` applymap() ```
Similarly to loc , at provides label based scalar lookups , while , iat

1 ) Merge columns 1 2 : #CODE
You can then drop col1 and col2 : #CODE

I'm trying to find a value of delay which maximises the correlation between two vectors . I have enough data to truncate some values from each vector without compromising the accuracy of the results .
When I apply ` plt.xcorr ( df.Val1 , df.Val2 )`

I want to create a new dataset from this such that , for the first column ` ST.INT.ARVL ` , I basically take the ` log ( ST.INT.ARVL_II ) -log ( ST.INT.ARVL-I )` where ` ST.INT.ARVL_II ` is the value of ` ST.INT.ARVL ` for second obs and ` ST.INT.ARVL_I ` is the value for first . Basically subtract log of second from first and keep the second obs from other variables along with this modified log value . Keep doing this for all rows , such that we take log diff of next with previous and keep the next observation for other features in dataset . But do remember this thing has to be done only within the observations belonging to a country . Like for Australia I do this , then for Other country in the country variable I do the same . It won't be done across the country values . So basically grouped by country and then do this sub .

Pandas : Join two columns in dataframe ( without creating two )
Could you replace all the whitespace with blanks : ` df [ ' Col2 '] = df [ ' Col2 '] .str .replace ( ' ' , '')` and do the same for Col4 and then my previous comment should work
To avoid creating any new temporary variables or dataframes , you could just replace ' Col2 ' with new values and then drop ' Col4 ' . There are a few ways to do that .
Afterwards , just drop ' Col4 ' #CODE

You need to assign the result of ` append ` as it returns the result of the append : #CODE

Something that would probably get you close to the right answer and also be pretty fast would be to ``` resample ``` and then ``` shift ``` .
Now we can shift this DataFrame by 365 dates : #CODE
Stack the SampleLocation for both df2 and df2_lagged : #CODE
Now merge over the lagged data to df2 . The DataFrames have the exact same structure , so you can just copy the values : #CODE
df2_lagged = df2.shift ( 365 ) . Replace 365 with 60 . The sample data wasn't sufficient for a 365 day lag , so I used a 60 day to illustrate the point .

I have some subsequent code that copies , then drops , the first few columns of data in this dataframe ( all the ' header1 ; header2 , header3'leaving just the timestamps . The purpose being to then transpose , and index by the timestamp .

Pandas Pivot Table Subsetting
My pivot table looks like this : #CODE
@USER Yes , you just need to use ix rather than loc . Also , you should consider parsing your dates :)
I tried , ` ix [ 0 ]` and am getting an error : ` KeyError : 0L `

Would importing this in to pandas as a pivot table and iterating them over row by row efficient ? Any suggestions ?
I would suggest you look into Map / Reduce . It is designed for exactly this . Streaming is the key to row by row processing .
Map / Reduce is an algorithm implemented in a very wide variaty of tools . Some of them extremly overbloated for what most people need . For simple stuff in python , I would recommend looking at MRJob #URL If you want more data pipelining integration , maybe pyspark #URL is more for you , but it requires a bit more setup . I hold workshops on how to build map / reduce implementations in bash , but the chance that you are in Berlin is quite slim I guess .

No need to use ` list() ` . ` map ( sum , zip ( *lisolis ))` is good enough .

Look inside your function : you are only calling numpy functions , that is , you leave a small ( if any ) margin for Cython to translate to C code .
Cython is nos just ' type-definign ' variables but you have to make sure that , when calling cython's compiler , it will be able to translate as much of your code to C as possible . ( By the way , you are compiling your code , right ? The speed boost will be only seen if you compile your source and import the resulting module ; running it in pure-python mode is just Python ) .

Replace ` Country ` with ` country_dict ` #CODE
Let's say your dataframe is called ` df ` and you have a nested dictionary of your country codes as shown below . Then you can use ` replace ` #CODE
Call ` df = df.ffill() ` and then call ` factorize ` , ` factorize ` returns a tuple of array values and an index which is composed of your Series values , we only want the array values here : #CODE
output from ` factorize ` : #CODE
Note : Before you apply ` factorize() ` you need to ` fill ` your ` NaNs `

How to replace a string value with None - python , pandas dataframe
In the above example , when I try to replace ' NaT ' the dataframe actually fills the value with preceeding value and not None . This won't help as it needs to be None . In the actual dataframe I'm working with this usually throws up a type error telling me that I can't do replace None with method pad . I'm using a datetime series here but really I'll need this for more than just datetime series . It seems like it should be basic functionality with pandas but I can't find an answer .
In the end , this worked for my needs . I don't think map ( str ) was working in the code I gave for the question but it works below where I assign it to only one field instead of the whole dataframe . #CODE

Currently , to factorize() while enforcing the ordering I want , I first convert ` rate ` to Categorical , then sort the ` df ` and finally factorize so ` Bad ` gets the smallest integer value and ` Good ` gets the highest . In this fashion I create the ` rate_factor ` column , which I can then use as a dimension ( among others ) to calculate similarities / distances . These are given in the code below .
I don't know if this is better but you could remove the sort and do this instead : ` df [ ' factor_rate '] = df [ ' rate '] .map ( dict ( zip ([ " bad " , " neutral " , " good "] , np.arange ( 3 ))))` define your category list and use this to construct a dict and pass this to ` map `
The thing that the above solves is any worries about order and behaviour wrt to factorize as you are now in charge of the categories and their order
If you're concerned or reliant on categorical then another approach is to define your categories in a list and an order , use this to create a dict to map the order to the categories and pass this dict to ` map ` : #CODE

Thanks . Before I accept your answer , how about the case when len ( selected_items )= =1 , i.e. I have only a single item to match to ?

And I want to calculate the following information " How many days was each account active ? " , I understand that I could simply do a count to get this information , but I want to apply the following restriction , " If there are n days between activity dates , only count the days before that gap " .
Thanks , this is really helpful . I think I might have made my condition unclear though so this might be calculating something different . Your snippet seems to sum the days where the timedelta is less than 5 , I'm looking to drop the days after the first occurance of a 5 day delta , not just discount the day that has that delta . eg if you spend from the 1st to the 10th , then again from the 18th to the 30th , it's still just 10 days .

This groupby / merge only builds a dataframe where there are visits , where as I want to be able to aggregate other fields regardless of if there is a visit or not .
add a helper column to aggregate ( needed as all other columns are used in the indices of the pivot table ) #CODE
then create a pivot table so the ` VisitDate ` column is in an index by itself , and the other dimensions are in the other . #CODE
Then resample the index to day . This creates ` NAN ` values for days with no values #CODE
Then unstack the frame , reset the index and fill ` NAN ` with 0 to get the frame you need . #CODE

Agree with @USER here . Even with @USER ' s example you could still drop the non-matching indices right ? ` df1.mul ( df2 [ ' ss '] , axis= ' index ') .dropna() `

I don't think there is going to be an inexpensive way to do this . I guess you could reindex to normalize the windowsize , unclear how expensive that would be ( depends on your data ) .
The proper way to handle a big data problem is to use map / reduce . The problem you are having can easily be solved with map / reduce . Since your data is already sorted on the start_time you say , you could easily just stream it through a reducer to get the cumsum for each start_time .
TLDR ; Pandas is the wrong tool , read up about Map / Reduce
thanks a lot , I will start to learn Map / Reduce .

I'm trying to reorder / swaplevel / pivot / something columns in a pandas dataframe .

I started playing around with pivot but that function doesn't really do what I need it to ... #CODE

When I run this in the shell , it takes quite a bit of time even for one file . I am certain I must be doing something wrong in my approach , as the goal is to read many files and merge them into a pandas DataFrame / Timeseries
It seems more logical to apply the filter to the time column as it is being read in then to operate on it later . Is there a way to do this by telling the read function what function to call as it reads the column , before storing the object in memory ?

Boxplot with pandas and groupby

Many other familiar string methods from Python have been introduced to Pandas . For example , ` lower ` ( for converting to lowercase letters ) , ` count ` for counting occurrences of a particular substring , and ` replace ` for swapping one substring with another .

Could you explain in words how you want to join the dataframes c and d ?
d is a dataframe of devices , with some components ( CPU , Freq and Voltage ) . c is a dataframe of comments and each comment is link to some tags . i.e. if a device has " atmel ' , it will be great !. If it has " intel and 5V " it will get the comment great ! too . There are several ways to get the same comment .

I have taken some large datasets from csv files , stacked them , and am trying to replace missing and negative values in my final column , with 0's . I believe that I have achieved with the missing values , however , I am unsure with negative values . Currently my code is as follows ( I intend to do further plotting later on ): #CODE

Pandas resample by first day in my data
the groupby solution was what I was looking for , but it is still sort of strange to me that resample , doesnt really sample from the existing dates ( like what we did in groupby ) . Thanks a lot Andy

Below , john-galt gives an extremely helpful answer . However , I've found one case where it's not immediately obviously how to apply his solution : using a custom grouping function .

But unlike most Pandas Cython tutorials or examples I am not apply functions so to speak , more manipulating data using slices , sums and division ( etc ) .

@USER in apply / agg the function needs to take the subDataFrame / each group . tbh I'm a little confused at what you're trying to do : s

Is there a built in function in either Pandas or Scikit-learn for resampling according to a specified strategy ? I want to resample my data based on a categorical variable .
However if I understand you correctly , you want to resample to a certain target distribution ( e.g. 50 / 50 ) of one of the categorical features . I guess you would have to come up with your own method to get such a sample ( split the dataset by variable value , then take same number of random samples from each split ) . If your main motivation is to balance the training set for a classifier , a trick could be to adjust the ` sample_weights ` . You can set the weights so that they balance the training set according to the desired variable : #CODE

You could all ` df.filter ( regex= ' HW ')` to return column names like ' HW ' and then apply sum row-wise via ` sum ( axis-1 )` #CODE

Clearly you've got further than this , as you know to use to_datetime ... you should just change your column to that . It sounds like you want to resample ( with aggfunc=len ) ... but it's not clear as you don't include the expected result .
Use resample and sum to get the number of events per time period - examples below

I have a big ` DataFrame ` in pandas with three columns : `' col1 '` is string , `' col2 '` and `' col3 '` are ` numpy.int64 ` . I need to do a ` groupby ` , then apply a custom aggregation function using ` apply ` , as follows : #CODE
I don't mean to replace the sort . It's just that if the testing loop returns true with a small ` i ` , you won't see much difference between the cython loop and the python one . But if the test returns False you have loop through the whole array .

I'd like to insert / append one more row to this 2D array , how to do this ?
[ ` append `] ( #URL ) is designed for this
Append doesn't do anything somehow .

If you want the values themselves , you can ` groupby ` ' Column1 ' and then call ` apply ` and pass the ` list ` method to apply to each group .
You could ` groupby ` on ` Column1 ` and then take ` Column3 ` to ` apply ( list )` and call ` to_dict ` ? #CODE

I'm trying to compute the variance of a time series for many sampling frequencies ( the so called signature plot ) , I used the resample method looping on a set of frequencies but python stops before completing the task ( no errors , just freezed ) .

As @USER said , you can replace this piece of code #CODE

Suggest you are using ` xlwt ` module , first adjust your collection , then do ` merge ` to get your xls file correct . #CODE

@USER this is incredible .. clever use of apply ! what a great principle .. can be applied as a function for any kind of dict look up .

Same code should also apply to a binary confusion matrix like : #CODE

What is the error log ? I guess you have to replace ` series [ ' column_of_ints ']` with ` series ` only in the ` function ( series )` , you're passing a series to the function not a dataframe .
Don't use ` apply ` you can achieve the same result much faster using 3 ` .loc ` calls : #CODE
df [ ' column_of_ints '] is a Series not a DataFrame , there is no ` axis=1 ` for ` apply ` method for a Series , you can force this to a DataFrame using double square brackets : #CODE
yes , it will , you can either use 3 ` loc ` calls to handle each boolean condition to filter the df , or a ` where ` cal that has a nested ` where ` to handle the 3 conditions

Not quite . Two issues here . 1 ) Output will have column names repeated there , rather it should be ` pd.DataFrame ( map ( str.split , a ) [ 1 :] , columns =[ ' date ' , ' name '])` 2 ) Column names should rather taken from list than manually passing it .

How to insert a " Holidays " column into 2Darray with Pandas / Numpy
I would like to insert one more column ( HOLIDAY ) which has a bool value : 1 if the date is holiday , 0 if the date is not holiday .

I tried using pandas merge , but not sure of how to go about it without explicitly iterating using a for loop . Any suggestions ?
Just ` concat ` them as a list and pass param ` ignore_index=true ` , then assign the index values to the 3rd column , convert to str dtype and then append the txt ' .txt : #CODE

what you really need here reshaping with pivot #URL .
Thanks Dyno for pointing me to Reshaping and Pivot Tables - I didn't know the later was supported by Pandas .
Using pivot_table rather than pivot . I'm using Pandas 0.15.2
I've managed to create the pivot table , however , I'm still struggling to show the labels on x-axis correctly . I tried ` pivoted = pd.pivot_table ( data , index= ' yrend ' , columns =[ ' state ' , ' suburb '] , values= ' median_price ')
plt.xticks ( np.arange ( len ( reporting_periods )) , reporting_periods , rotation=90 )`

Then , merge the average snowfall to df1 and then compute the difference : #CODE
Give this a try . Using map to pull directly from your series of averages #CODE

I have a distance matrix in hand where I would like to get its max , min , mean , median , etc . values ; expected describe() to do it for me , but looks like I was wrong . Both ` from scipy.stats import describe ` and ` df.describe() ` work either on columns or rows .

@USER No it gives me the same warning even on using apply instead of map

Well , frankly I'm finding it hard to believe you . After the end of the last ` fp.readline() ` , I did ` fp.tell() ` . That location and the total number of characters in the file match . ( total no . of characters were calculated with ` sum ([ len ( i ) for i in open ( ' .. / test.txt ') .readlines() ])` )

Matplotlib multiple columns hist subplotting

One way to do this apply ` clip_upper() ` on 90 percentile value ` np.percentile ( x , 90 )` for each column #CODE
I had imagined @USER elegant solution would faster than ` apply ` . But ,
Below benchmarks for ` len ( df ) ~ 130K ` #CODE
And for ` len ( df ) ~ 1M ` #CODE
Thanks for the timings ! I had thought the same thing but , having just checked timings on my machine , it appears ` apply ` can be surprising sometimes :-)
Not sure why , but you would notice that in the benchmarks posted , this method seems slower than apply method . +1 for cleaner single-liner .

So , since apparently one cannot control the sorting algorithm using the pandas sort method , just use a lower cased version of that column for the sorting and drop it later on : #CODE

Note that these days you can use ` expand=True ` instead of ` apply ( pd.Series )` : #CODE

You can't pass a Series as a param to a function unless it understands what a pandas Series or the array type is so you can instead call ` apply ` and pass the function as the param which will call that function for every value in the Series as shown above .

You could call ` apply ` and use ` datetime.strptime ` : #CODE

A function can be applied to a ` groupby ` with the ` apply ` function . The passed function in this case ` linregress ` . Please see below : #CODE

My question is : what should I do in a loop to merge ( concatenate ? ) the chunks back in a .csv file after some processing of chunk ( marking the row , dropping or modyfiing the column ) ? Or there is another way ?
Note that ` mode= ' a '` opens the file in append mode , so that the output of each

How do I iterate over each row and column in ` rectangle ` to run my function ` MakeBoolDictOfSearchTermsAndProducts() ` on it and fill in the correct element with the result ? Should I use apply ? or map ? or perhaps apply_map ?

How can I create a pandas pivot table showing the percentage of one of three options ?
I want to create a pivot table for each season , country and league showing the H as percentage of the total matches . #CODE

Pandas-Pivot , stack , melt
You can use groupby and apply : #CODE

I'd stacked to append each file using pandas .
The main issue here is that append returns None : #CODE
and None has no append method , so this will raise an AttributeError : #CODE
I think what you want to do here , to make one frame from several , is concat : #CODE

You're performing [ chain indexing ] ( #URL ) this raises a warning in recent versions . For insertion , updating it is recommended to use the new ` loc ` , ` iloc ` and ` ix ` for indexing
Generally you should be using ` ix ` #CODE
Or ` loc ` #CODE

You could ` apply ` on dataframe and get ` argmax() ` of each row via ` axis=1 ` #CODE
Here's a benchmark to compare how slow ` apply ` method is to ` idxmax() ` for ` len ( df ) ~ 20K ` #CODE

In my naive implementation , I try to use hash map for each combination of col1 and col2 values but it makes the indexing problem really big .
why a hash map and not a vector of vectors ? Is the file particularly big ?

it's strange . the same ` got 1 columns ` happens with ` b = np.genfromtxt ( ' bigfile.csv ' , dtype=float , delimiter= ' \t ' , usecols=range ( 1,401 ))` . It's surely tab separated because i can do ` cut -f ` on unix and surely 401 columns from ` awk ' { print NF ; quit } ' bigfile.csv `

Pandas DataFrame apply function doubling size of DataFrame
I am trying to create this boolean mask using the ` apply ` method , where ` df ` is a DataFrame with numeric data of size a * b , as follows . #CODE
Why is the ` apply ` method doubling the size of the DataFrame ? Unfortunately , the Pandas apply documentation does not offer helpful clues .

specify the end of the year , use quotations , and also use ix to select index values : #CODE

How to insert a large record into Orient DB using pyOrient ?
I am trying to insert roughly 5 to 10mb into the node's data property , however , the database seems to become non responsive when doing so .
Is there a way to insert properties with large properties ? Is the limitation at the database end , at the pyOrient end , or something else altogether ?

In scenario 4 , df.to_msgpack() requires a buffer-like object , whereas memoryview() requires an input parameter . So one would have to create an ' empty ' memory view and then pass this to the to_msgpack() method . Then append the data . Though I wonder if this will lead to artefacts when unpacking the data .

How can I pivot a df with duplicate index values to transform my dataframe ?
There was an answer in [ this ] ( #URL ) post where someone used pivot_table() instead of pivot and it resolved the same error , might be worth trying ?
You can use groupby to aggregate by the common factors , take the max of time to get the most recent dates , and then unstack the msg to view confirmed_settings and sale side by side : #CODE
I suspect there are indeed duplicate ` uid ` - ` msg ` entries / keys ( e.g. ` uid ` 2 has 2 confirmed_settings entries under ` msg `) , which you alluded to in the comments for fixxxer's answer . If there are , you can't use ` pivot ` , because you can't tell it how to treat the different values encountered during aggregation ( count ? max ? mean ? sum ? ) . Note that the Index error is an error on the Index of the resulting pivoted table ` df1 ` , not the original DataFrame ` df ` .
This will help you figure out which user-msg records have duplicate entries ( anything with over 1 ) , and after cleaning them out , you can use ` pivot ` on ` df ` to successfully pivot ` _time ` .

Ability to apply different stats to different columns ( for now just count , sum , mean , weighted mean )

Essentially insert a row into the existing dataframe df1 . I have looked at a ton of examples but haven't figured out how to do this . If my question isn't clear please let me know .

when combining pandas dataframe ( concat or append ) can I set the default value ?
Pandas merge two dataframes with different columns
If I concat two dataframes ( A B ) that have some of the same columns , but also have columns that are not present in both , in the resulting dataframe the entries for the columns that are not common to both A B have a value of NaN . Is there a way to make these entries have another default value ?
I would rather not simply replace NaN after the concat operation as there may be NaN values in the original dataframes that I want to preserve .
Please show what your 2 df's look like you may want to ` merge ` or ` combine_first `
A possible solution is ' conform ' the indices before concatenating both dataframes , and in that step it is possible to define a fill_value : #CODE

I want to plot a histogram for each column . The best result I have achieved is with the hist method of dataframe : #CODE
` data.apply ( math.log10 )` did not work because ` apply ` tries to pass an entire column ( a Series ) of values to ` math.log10 ` . ` math.log10 ` expects a scalar value only .
And can I apply tight_layout to data.hist somehow ?

Avoiding double counting in pandas merge
After I finish the aggregated files I want to again merge the two files at the granular row level . My issue is that File 2 can really have multiple Chats for the same PersonID on the same Date . Is there a way to join / merge this with File 1 which only has one record per PersonID+Date+CustomerID without creating duplicate Visits on the first file ?
+1 for a good question . However , some clarity is needed : Why would you drop duplicates in File 1 to leave only 1 visit per CustomerID per date ? That would mean that the ` Sum of Visits ` column is misleading because it's not going to show you the sum , only ones , yes ? An alternative is pivoting File 1 to show you the actual sum of visits per PersonID . You don't need the CustomerID to do a sum of visit versus sum of chats ratio anyway . If the data is not overly sensitive , maybe you can provide a hundred rows each of the raw data , upload it somewhere , and I'll have a look over it for you .
The File 1 data is not so accurate , users input the data which means that sometimes there are like 10 timestamps for the same CustomerID which is not possible . The only way I could clean it was to drop the duplicates . About 340k out of 17 mil . I do sum Visits after I groupby and it does calculate correctly . Yeah perhaps I'll drop the CustomerID for now .. I wanted it because it includes geographic information which I use to place in maps on tableau . I can add that later perhaps . On phone now but I'll try to upload data later .
So we're pretty much at the start . Remaining step would be to merge it with the second dataframe to get our chats per date per person . #CODE
There are several new approaches here in this answer that I have not tried . Is this better to do on the row based file ( meaning I should not do groupby first ) ? When you do the aggfunction Len , why do you do it that way ? Before I was doing groupby as index = false then .aggregate sum on visits or chats . Thanks for your insight so far
` groupby ` has the downside of introducing iterables . It's easy to set-up , but it takes some time to get used to . Coming from an Excel-reliant environment , I crunch numbers quickly using pivot tables , which ` pandas ` executes in much better ways . ` aggfunc ` is similar to Excel's ` View values as ` option , and feeding it ` len ` is similar to using ` Count ` in Excel's pivot table . Granted , the syntax for ` pivot_table ` takes some getting used to due to its relative strictness compared to the other ` pandas ` functions , but it can manipulate AND show meaningful data at the same time .

The first column is index as you would find in a series . Now I want to basically get all these index names in a list such that only those index should come whose absolute value in the right column is less than 0.5 . To give a context this series is basically a row corresponding to the variable NY.GDP.PCAP.KD.ZG in a correlation matrix and I want to retain this variable along with those variables which have correlation less than 0.5 with this variable . Rest variables I will drop from the dataframe
@USER : Now you seem to be back to the original . If you take all the columns , get rid of the ones in your < 0.5 set , and then drop the resulting ones , isn't that the same as just selecting the ones in the < 0.5 set ? You are dropping the complement of your target set , which is the same as just selecting the target set .

When two DataFrames each have a column with the same name ( like ' PCBD1 ') , and you merge them on the indices and not that column ( left_index=True , right_index=True instead of on= ' PCBD1 ') , then pandas will rename them to avoid having two columns with the same name ( PCBD1_x comes from My_Features and PCBD1_y comes from Drug ) . Can you explain some more what your end goal is ?

concat two dataframe using python
We need to concat both the dataframe like #CODE
If the index is not common then this will stack the rows , you'd need to pass ` ignore_index=True `

to calculate said time difference . Due to prior filtering , the biggest possible diff value should be between 10 and 20s . However , I frequently get diffs slightly under a day ( 1-10s less ) , even though the actual difference is something like 5s .
My workaround would be to copy the index , cast it to int64 , cast t0 to int64 , substract t0 from all rows and then convert the diff column back to timedeltas , but that seems extremely inefficient and ugly .

How are the dfs related ? Are they all subsets of the same larger df ? You could merge , concat them all and write to DB in one go perhaps
( with ` all_data ` as the dataframe ) map dataframe values to string and store each row as a tuple in a list of tuples #CODE

but rather than getting all three columns merged I get a three row dataframe with the values as comma separated values inside ? How can append the three columns onto each other then get the distinct values elegantly ?

I'm not sure I can give you a great explanation for that warning beyond what's in the documentation , but it appears what you did works fine and that warning doesn't always apply even when it appears .

In pandas ' documentation you can find a discussion on area plots , and in particular stacking them . Is there an easy and straightforward way to get a 100% area stack plot like this one

and then use ` pandas.melt() ` to pivot the data : #CODE

What I would like to do is to transpose one of the columns , so that instead of having multiple rows with same medicine and different diseases I have one row for each medicine with several columns for diseases . It is also important to keep index as simple as possible , i.e. 0 , 1 , 2 ... i.e. I don't want to assign ' medicines ' as index column because I will merge it on some other key .
I'm thinking you want a pivot table . Check this link for more information --> #URL
And , finally we can pivot : #CODE
Now , you can ` concat ` -- ` md ` with ` dval ` #CODE

Just ` groupby ` by ` level=0 ` or ' Greek ' if you prefer and then you can call ` diff ` on values : #CODE

And I want to pivot table . #CODE

After you do the ` split ` , you should get the first element ( using [ 0 ]) . And not after the ` map ` . : #CODE

Now we can replace the NaNs with 0 : #CODE

I want to drop the rows which have " _cntrl " in the name column . Can you suggest how to do this ? I've looked into df.drop but could not figure out how to drop based on string based matches .

Now some of the great things about pandas is that it will try to align using existing column names and row labels , this can get in the way of trying to perform a fancier broadcasting like this :

Call ` apply ` on ' B ' and pass a lambda which just accesses the single key in the dict : #CODE
You then have to decide what to do about the missing values , you can replace them by calling ` fillna ` : #CODE

@USER I did try using ` boolean indexing ` , but then since I need to find runlengths , I figured I would need to find those shifting indices and then perform ` diff ` , so ` np.where ` might be the preferred way here . It's good to see those solutions , seems like inherently I am just ending up with solutions that are based off the source codes of all these numpy builtin functions ! :) Appreciate you sharing these links !

As a side note , instead of doing ` for i in range ( len ( jdata.keys() ): ` and then using ` jdata.keys() [ i ]` over and over , just do ` for key in jdata : ` . If you need ` i ` as well as ` key ` , you can do ` for i , key in enumerate ( jdata ): ` , but it doesn't look like you need it anywhere .
You are creating a lot of copies of keys / values ( each call to ` .keys() ` or ` values() ` creates new list . Also ` for i in range ( len ( jdata.keys() )): ` creates unneeded list of integers . Have you considered using ` itertools ` module ? Lazy evaluation is usually better idea than keeping everything in memory multiple times .

Can you post raw input data , as for your first question I think this should work , if ` col_list ` is a list of your variables then ` df [ col_list ] .apply ( sm.tsa.stattools.adfuller , maxlag=None , autolag= ' BIC ' , regression= ' c ')` you can assign these to new columns and then transpose if you want the columns as index values .
OK , I'll knock something up , basically my approach would be to just walk the series / df that is returned from my previous code snippet and then pull the individual components out and append them row by row

That gives you a ` bool ` column . You can use ` astype ` to convert to ` int ` ( because ` bool ` is an integral type , where ` True ` means ` 1 ` and ` False ` means ` 0 ` , which is exactly what you want ): #CODE
To replace the old string column with this new ` int ` column , just assign it : #CODE

One way but not more efficient for long dictionaries is using ` itertools.combinations ` to find the combinations between your dictionaries then loop over the combinations and then the sets and get the intersection between the set items : #CODE
You need to c reate a ` view ` object of your items that tread as ` set ` objects then you can calculate the intersection of items with ` ` operand .

You can make 2 calls to ` dropna ` , ` dropna ` accepts a ` thresh ` param which won't drop the entire axis if there are ` n ` non-Na values so the following drops rows then columns : #CODE

Pandas ` DataFrame ` s support most of the ` dict ` methods , include ` get ` ( which allows you to substitute a value for an empty key ) . So you can do the statistics you want on all columns , then get the values for the column you want , substituting an empty ` Series ` for missing columns , then drop ` NaN ` columns ( I use ` Bad Value ` to demonstrate what happens to missing columns ): #CODE
Wow this works for all cases unless I use ` apply ` and a lambda function

Apply a weighted average function to a dataframe without grouping it , as if it was a single group
I want to apply a function that computes something similar to a weighted average absolute deviation of all the elements of my data frame .
If I don't use groupby , pandas would apply this function to every row of the dataframe , which is not my goal .
Okay , so in your post when you say " If I don't use groupby , pandas would apply this function to every row of the dataframe " , that's not necessarily true . You should try to read up on the way operations on ` numpy ` arrays are " vectorized " . So , like people have pointed out in the comments , your function works fine without having to do the groupby : #CODE

This is gotten from a simple pivot , if you are ok to not using the for-loop : #CODE

After that , I use the ` unstack ` method : #CODE

I'd split the dfs into 2 list the ones that can be combined column-wise and the ones that can be combined row-wise , concat those separately and then concat the 2 concatenated dfs
The ` concat ` way requires you to group things in a specific order , but is more than twice as fast : #CODE
Thanks , nice answer . I'll go with the combine_first because due to some idiosyncrasies there is no easy and predictable way to join the data frames first in one direction , then the other . Fortunately , the combine_first method isn't quite as slow as I had originally anticipated .

Also , depending on the SQL created ( and if the ODBC driver converts ` INSERT ` to a ` COPY `) this might be very slow . If you find that it is doing lots of individual inserts on the database , you'll probably want to switch to a ` COPY ` method where you create strings and ` COPY FROM STDIN ` to make it faster .

Use the older ` openpyxl ` engine to apply formats one cell at a time . This is the approach with which I've had the most success . But it means writing loops to apply formats cell-by-cell , remembering offsets , etc .
This is currently not possible in openpyxl . As you rightly point out applying formats to individual cells is extremely inefficient . This will hopefully improve in forthcoming releases when we add support for named styles you'll still have to apply these individually as resolving all the possible styles for an individual cell ( built-in , row , column , individual ) is an expensive operation which will be much less complex than it currently is .

You do use pivot to get the ` identifiers ` in columns and then plot #CODE

I have also tried to strip the characters with the following : #CODE
A single-liner - you could extract numbers from via regex and ` apply ` on the ` duration ` column like split into multilines for readability #CODE
And , then apply on #CODE

How do I export multiple pivot tables from python using pandas to a single csv document ?
Say I have a function pivots() which aggregates pivot tables #CODE
I know how to export a single pivot table #CODE
You can use ` to_csv ( path , mode= ' a ')` to append files . #CODE

What's REALLY confusing me is , when I try to step through the function ( not using apply ) with just one row , I get the DataFrames that I expect i.e. , not the Series and then Timestamp . Really appreciate any insight into what's going on !

User ` join ` and then ` reset_index ` : #CODE

Counter in Pandas groupby apply
Is there a way to have a counter variable in the function called through a pandas groupby apply ? #CODE
Note : this is an implementation detail , the number of times the function in an apply is called may depend on the return type / whether the apply takes the slow or fast path ...
Isn't the first call to apply the initialisation of the groups though , I thought I saw this as an explanation in a previous answer somewhere ...

I am using Python to insert some columns from a pandas dataframe to MySQL . I'm using pandas 15.2 . and mysql.connector . #CODE
I'm getting the following error message when trying to insert the data in the table : #CODE

Oops didn't see this answer was the first . Great I should have thought of using apply . I ended up having a few issues with integers and special character ( like spanish letters ) . The answer bellow solves my issue with integers but waiting for an answer on how to deal with special characters like ` u ' \xf3 '` .

Nested structures like these are simply not supported . These don't map nicely to pandas structures . Further by creating this file in raw HDF5 you are missing lots of meta data that pandas needs to interpret the data .
if PyTables does not support it , no . Not everything in the spec is actually supported by PyTables . Mainly as were not implemented , or they don't map well to higher level structures .

I'm a Stata user and in Stata , I'd be using replace command conditional on regexm . I'm trying to learn Python and it's been a difficult journey ! I'd appreciate any help on this .
We then apply another function to this that converts the str numbers to ints , puts these in a list and returns the smallest value : #CODE
Thanks ! this is an approach that I hadn't thought about and one that I'm likely to employ down the road . for age , I wanted the series [ 62 , 55 , 67 ] at the end , and the problem I'm having now is that I can't target just row2 when I apply split ( ' ') .
return min ( list ( map ( int , x )))` to ` def highest ( x ):
return max ( list ( map ( int , x )))`
I can't pick the values from the list based on min and max because the expression format from which they came matters ( I think it's not very clear from the examples I gave for sake of simplicity ) . I want to apply df [ ' age '] =d f [ ' e0 '] [( df [ ' e0 '] .str .match ( pattern7 )= =1 )] .apply ( lambda x : str ( x ) .split ( ' ') [ 1 ]) to only rows for which df [ ' e0 '] .str .match ( pattern7 )= =1 ) so as to not overwrite what was already in the age column ...

If you make these a list you can apply loc ( which gets you the desired result ): #CODE

Mark Ransom gave a smarter answer , if you have to do the FFT you can just cut off the noise after the transformation . It won't give a worse result than a filter would .
Yes , I noticed the hidden frequency as well . So , I calculate mean frequency as : ` freqs = fft.fftfreq ( len ( Hn ) , 1 / frate ) ;
ind = np.arange ( 1 , len ( Hn ) / 2+1 ) ;
cut = 0.7 * psd.max() ;
ind1 = np.where ( psd > cut ) ;

I need to merge the two dataframes on the dates , but since they aren't exact , i probably need to convert them first . Any ideas ?
You convert the datetime column to just the date . Save it in a different column and ` merge ` on those columns .
@USER This is going to be much slower than using either dt.date or dt.normalize ( normalize being the fastest ) .
You can normalize a date column / DatetimeIndex index :
Note : At the moment normalize isn't exported to the dt accessor so we need to wrap with DatetimeIndex . #CODE

split string column by " _ " , drop the preceding text , recombine str by " _ " in pandas
2 ) drop the ' NHL ' string
You could use ` apply ` like this : #CODE
Surprisingly , applying ` str ` seem to be taking longer than ` apply ` : #CODE
However , if you need to split and join , you could use a ` string method ` like- #CODE
Alternatively , you could also use ` apply ` #CODE

As commented there is a chunksize argument for ` read_sql ` which means you can work on sql results piecemeal . I would probably use HDF5Store to save the intermediary results ... or you could just append it back to another sql table . #CODE

The thing that I'm wanting to do is take the actual score value and apply a color map to it . So that worse scores are more blue and good scores are more red . Is there any way to do this within a radviz graph ? How would you input the score values into the equation ?

@USER : The code you posted on pastie works for me when I replace the ` custom_distance ` function by something which doesn't have extra dependencies . ( I claimed something wrong before , the Y array actually has to be an 1D array , but you already fixed that with the ` ravel() `)

Summarizing Dataframes with ambiguous columns with apply function
The code works for almost all cases except for ` apply ` functions that count specific cases inside a column : #CODE
Is there a way to incorporate the apply function into the dictionary ` sumdict ` ?

You could join array of elements from ` df [ " city "]` like #CODE

If I drop ` np.nan ` into your example : #CODE

There's a trick here to use pd.Series ( 1 , index= ... ) and let pandas align : #CODE
You can reset the index and then simply join : #CODE
If you want to include the NaN row ( which isn't in your answer ) , then outer join : #CODE
As an alternative , you can create ` gene ` in pure python ( rather than using apply ): #CODE
@USER Note : in your edit , this creates a different Series ( the index is now 0123 whereas in the original it's 0113 ) , so join will give a different result . ?? ( Is that the slow part of the operation ? )

Apply the function .

In that case , you can populate your ` dataframe ` with ` loc ` - #CODE

Is there a way to conditionally drop duplicates ( using drop_duplicates specifically ) in a pandas dataframe w / about 10 columns and 400,000 rows ? That is , I want to keep all rows that have 2 columns meet a condition : if the combination of date ( column ) and store ( column ) # are unique , keep row , other wise , drop .

The most efficient way to calculate the intersection of Product_ID's would be using numpy's ` in1d ` . That gives you a mask .

If x in y does not equal 0 then replace with 1

How to column stack arrays ignoring nan in Python ?

pandas - apply UTM function to dataframe columns
I'm working with this python package called UTM , which converts WGS84 coordinates to UTM and vice versa . I would like to apply this function to a pandas dataframe . The function works as follows : #CODE
You could use ` apply ` method over the columns like

You can do this by ` apply ` ing a ` rolling_sum ` after we ` groupby ` the Type . For example : #CODE

AttributeError : Cannot access attribute ' index ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method

I'm reading Wes McKinney's Python for Data Analysis book . On the topic of using ` DataFrame.resample() ` or ` Series.resample() ` , if I want to resample for Business days , I would use :
How can I explicitly choose to resample based on a particular country's holiday ? Say , US holidays , or European holidays ? Struggling to find some documentation on this ... The doc for ` resample() ` I found here { #URL } is rather short and doesn't really go into the details of the first parameter ` rule ` ...

1 ) Some files contain call drivers or other categorical information which repeats constantly ( for example , Issue Driver 1 with like 20 possibilities and Issue Driver 2 with 100 possibilities ) - these files are about 100+ million records per year so they become pretty large if I consolidate them . Is it better to create a dictionary and map each driver out to an integer ? I read a bit about the category dtype in pandas - does this make output file sizes smaller too or just in-memory ?

Merging will put NaNs in common columns that you merge on , if those values are not present in both indexes . It will not create new data that is not present in the dataframes that are being merged .
And , now we merge it to get something close to the target dataframe : #CODE

The snippet works , but it's not computationally efficient as it loops through the data frame at least three times . I would just like to know if there's a faster and / or more optimal way to do this . For example , using ` zip ` or ` merge ` ? If , for example , I just create one function that returns three elements , how should I implement this ? To illustrate , the function would be : #CODE
Here's on approach to do it using one ` apply `
And , ` apply ` and store the result to ` df [[ ' hour ' , ' weekday ' , ' weeknum ']]` #CODE
Depending on the task that is performed by ` lambdafun ` , you may get some speedup by storing the result of ` apply ` in a new ` DataFrame ` and then joining with the original : #CODE
Even if you do not see a speed improvement , I would recommend using the ` join ` . You will be able to avoid the ( always annoying ) ` SettingWithCopyWarning ` that may pop up when assigning directly on the columns : #CODE

One way is to groupby and apply function to take list , and then convert to dict . #CODE

Pandas dataframe apply function
3 ) How to use apply function on the above dataframe ` temp ` ?
It doesnt seem to work . It throws an error `' Series ' object has no attribute ' dt '`
I had already done that . ` df [ ' Ship Date ']` is a series for which dt attribute is not present .
Then ` apply ` lambda function . #CODE

I have a dataframe ` df ` , with two columns , ` GROUP_I D= {A , B} ` and ` ACTION_DATE ` . What I want to do is to replace the ` ACTION_DATE ` value to ` 03 / 31 / 2006 ` , if the ` GROUP_ID `' s value is ` B ` . Data type of ` ACTION_DATE ` is ` datetime64 [ ns ]` .

Let's say we want to resample this to monthly data . This fails and returns an empty DataFrame : #CODE
The fact that you need to do the ` reset_index() .set_index ( ' dt ')` and then ` groupby ( ' symbol ')` instead of ` groupby ( level= ' symbol ')` seems to defeat the purpose of a multi-index ! What gives ?

It's not the most elegant , but life is short , so I'd apply ` list ` to get the values and then ` pd.Series ` to expand them into columns : #CODE

Thanks . Is there a way I can apply this to all values in a column ?

-Take the first element as the ticker and convert the rest into numbers using translate to undo the string formatting
-Make a DataFrame per row and then concat all at the end , then transpose

Then I want to save that factorization and apply it to other ` DataFrame ` ( look input doesn't have c values in column A ):
As wroted in the question - I know about ` get_dummies ` - but this doesn't resolve my problem to apply the same mapping to the other series object . You don't need to use ` map ` on columns - ` get_dummies ` have an optional parameter ` prefix `

For instance , if you intend to replace missing values by the most frequent value or the median . This knowledge ( median , most frequent value ) must be obtained without having seen the testing set . Otherwise , your missing value imputation will be biased . If some values of feature are unseen in the training data , then you can for instance increasing your overall number of samples or have a missing value imputation strategy robust to outliers .

In particular , if you actually do need to do array-wide work across all 350000 rows , and can't translate that work into SQL queries , you're not going to get much benefit out of sqlite . Ultimately , you're going to be doing a giant ` SELECT ` to pull in all the data and then process it all into one big frame .
( Although this doesn't apply to the question , it may help someone searching later : If you're using Python 2.x , make sure to explicitly use pickle format 2 ; IIRC , NumPy is very bad at the default pickle format 0 . In Python 3.0 + , this isn't relevant , because the default format is at least 3 . )

My goal is to replace certain values in each column from current list and then to save it : #CODE
So , any ideas about it ? Can we implement it in " chunking " mode , or there is another way ( then I need an example ) ? I just want to replace certain values to NaN ... :)
It's possible this could be improved by keeping the file open , rather than opening the file each time in append mode : #CODE

pandas left join and update existing column
I am new to pandas and can't seem to get this to work with merge function : #CODE
With a left join on column a , I would like to update common columns BY THE JOINED KEYS . Note last value in column c is from LEFT table since there is no match . #CODE
How should I do this with Pandas merge function ? Thank you .
Replace ` NaN ` value from ` c_y ` with ` c_x ` value #CODE
Drop unwanted columns , and you've the resilt #CODE
Here's a way to do it with ` join ` : #CODE
Note : ` update ` only does a left join ( not merges ) , so as well as set_index you also need to include the additional columns not present in ` left_a ` . #CODE

I am trying to normalize the missing values in matrix . Here is the code . #CODE
Last line should replace the values in dataset1 by mean values from ` ds2_mean [ 1 ]` . But it does not do . Anything wrong here ?
And after that can I replace NaN with the average value of it's neighbours in dataset1 ?
it does wrong . For any x in dataset2 it has mapped value in col2 . It should replace all values of x in ds1 by mapped value . But this also does not do it
Sorry can you explain clearer , what are you mapping from what to what exactly ? By default fillna will use the index so how do you want the mapping from ` ds2 ` to map to the missing values in ` ds1 ` ? Are you wanting to map using the values in ` ds2 [ 0 ]` as the index lookup ? So use the index from ` ds1 ` find value in ` ds2 [ 0 ]` and return ` ds2 [ 1 ]` ?
yes , I want to use the index from ds1 find value in ds2 [ 0 ] and replace it with ds2 [ 1 ]" sorry for inconvenience

I have a pandas series with MultiIndex and would like to replace one of the levels with the values in a seperate list . I found a workaround how to achieve this , but I was wondering if there is a more direct way of doing this . This question also relates to the working of ` set_levels() ` , which I seemingly do not understand .
One correction to my own comment . I have to write ` res = res.reset_index ( level=1 )` . The ` inplace=True ` does not work here , because apparently one cannot simply replace a Series with a DataFrame , which results by calling ` reset_index ` .

Add the new columns by calling ` shift ` and pass ` -1 ` as the interval : #CODE
Use ` shift ( -1 )` on the dataframe #CODE

Interpolate and fill pandas dataframe with datetime index
Hi I'm trying to interpolate a Dataframe where I have a datetimeIndex index .
First tried to set the index and then interpolate . #CODE
Then use ` interpolate ( method= ' linear ')` on the series to get values . #CODE
The df.reindex call returns a column of NaNs and subsequently the interpolate fails .
Just as an add on to @USER ' s answer , you could also use ` resample ` which is slightly more convenient than ` reindex ` here : #CODE

to get the difference between the price in one row and the next . However if you're actually interested in finding the difference between time periods separated by 3 minutes , not the 2m56s in your data , you're going to need to resample your timeseries using the resample method as documented here :

You might also be be able to use the map directly on the index without resetting , but given that it's a multi-index I generally find it much easier to work ( syntax-wise ) with a specific column rather than a specified level of a multi-index . FWIW .

in all cases above , the function can also be replaced by a string for the most common ones ( eg `' mean '` , `' median '` , `' std '` `' first '` , ... )

Replace blank & null fields at the same time in pandas
I have a dataframe " column " which has blank NaN ( nulls ) at the same time . Now I want to replace the blank NaN field with a string " No Data " . Please give some guidance on the same . I am using Python Pandas .
You could pass the values you want to replace in a dictionary to the ` replace ` function : #CODE

Apply function with pandas dataframe - POS tagger computation time
I'm very confused on the apply function for pandas . I have a big dataframe where one column is a column of strings . I'm then using a function to count part-of-speech occurrences . I'm just not sure the way of setting up my apply statement or my function . #CODE
So basically I have a function similar to the above where I use a POS tagger on a column that outputs a single number ( number of nouns ) . I may possibly rewrite it to output multiple numbers for different parts of speech , but I can't wrap my head around ` apply ` .
I'm pretty sure I don't really have either part arranged correctly . For instance , I can run ` noun_count [ row ]` and get the correct value for any index but I can't figure out how to make it work with apply how I have it set up . Basically I don't know how to pass the row value to the function within the apply statement . #CODE

or , using the ` loc ` label selector , #CODE

You can do this with unstack :

How do I apply a function designed for one number to an entire dataframe in pandas ?

Then I grouped by each pair of dates , adding ids to those pairs that match . This involves breaking the DataFrame into a list of subframes and use ` concat ` to stick back together . #CODE
Now , just append all of these , but drop the ones we don't need ( if we already have an existing row with ' index ' = ' date2 ' , as for id=2 here ): #CODE

Why does Pandas DataFrame Resample change column Order

For time series , there is the massive benefit of handling time series data using a datetime index , which allows you to resample smoothly to different intervals , fill in values and plot your series incredibly easily .

Any function in numpy / pandas / python to search and replace
[[ np.mean ( neighbors ( i , j , a )) for j in range ( len ( a [ 0 ]))] for i in range ( len ( a ))]`

Note that ` CIF / align / aft_port_end / extend_pressure ` is not meant as a path to a group / node / leaf . It is a label , that we use internally which happens to have some internal structure that contains ' / ' as delimiters . We do not want the HDF5 file to know anything about that : it should not care . Clearly , if ' / ' are illegal in any HDF5 name , then we have to change that delimiter to something else .
`" CIF / align / aft_port_end / extend_pressure "` is not a path to a group node / leaf . It is a name in and off itself , just a label with internal structure that HDF5 should not care about . At least , that's the theory .
Just bite the bullet and replace ' / ' with ' | ' before writing your variables to HDF5 . Replace them back when you read them . The time you lose by implementing this , you'll win back x-fold ( for x > 1 ) by avoiding future bugs and user complaints .

I have a dataframe that has an index of percentiles ( 10 , 25 , 50 , 75 , 90 ) and values at each percentile ( see code below ) . I am trying to fill in the percentile numbers to have all values from 10-90 for each section ( 10 , 11 , 12 , etc . ) and interpolate the values using the pandas interpolate function . Is there a way to easily do this in pandas / python ? The interpolating part is easy , but filling the gaps between percentiles is where I am stumped . #CODE
Seems like there might be an easier way but this isn't too bad . Mostly the work is done by ` reindex ` but I had to loop with a groupby rather than apply directly due to the index not being unique . #CODE

It would work if your ` data.data ` was a list of strings , but a float was found there . Try to replace

Restricting column values in Pandas Pivot
If I use Pandas Pivot , I can convert all factors in the ` Flag ` column to separate columns : #CODE
My question is , how can I restrict the factors / classes in ` Flag ` column to say , only include ` Category ` and ` Type ` without having to manually drop columns ? In other words : #CODE

As you can see , this takes time . I have to format the dates on each and then turn them into text , create an object column which adds to the filesize , and ( due to the raw data issues themselves ) drop duplicates so I don't accidentally inflate numbers . Is there a more efficient workflow for me to follow ?
Pandas will correctly merge rows based on the triplet of values from the `' Date ' , ' CustomerID ' , ' AgentID '` columns . This is safer ( see below ) and easier than building the Unique column .

Drop duplicate Pandas and SQLAlchemy
Replace data with your upload table and check will need to be a queried list of phone numbers from your database .

Replace the following line : #CODE
It seems you are trying to filter out an existing dataframe based on indices ( which are stored in your variable called ` comp_rows `) . You can do this without using loops by using ` loc ` , like shown below : #CODE
pandas ` append ` does not happen in-place . You have to store the result into another variable . See here .
Append one row at a time is a slow way to do what you want .
Instead , save each row that you want to add into a list of lists , make a dataframe of it and append it to the target dataframe in one-go . Something like this .. #CODE

What should I use if I want to update ` some_series ` in place from ` other_series ` , but also have the NA values apply ?
To update just the index of s use the intersection : #CODE
@USER updated ( use intersection rather than union )
the following would work , this uses ` loc ` and the other series index to mask the elements we want to overwrite : #CODE
The ` loc ` is acutally unneccessary in this case
Thanks ! What do you mean by " The loc is acutally unneccessary in this case " ?

I would like to know how to make a new row based on the column names row in a python dataframe , and append it to the same dataframe .
I'm assuming this is what you want as you've not responded , we can ` append ` a new row by creating a dict from zipping the df columns and a list comprehension of the middle character ( assuming that column name lengths are 3 ): #CODE
ix --- lets you read the entire row -- you just say which ever row you want .

Using append works , but for my purposes is WAY TOO SLOW #CODE
Create a list of lists and making a dataframe of that will be faster than append . Since you are already creating data frames of small chunks , why not create it in one go : #CODE

Essentially , what I am trying to do is join Table_A to Table_B using a key to do a lookup in Table_B to pull column records for names present in Table_A .
In mySQL , this is easy to do as you can see in my sqlfidde example . I join on real_name and I compress / collapse the result by groupby a.real_name since I don't care if there are multiple records in Table_B for the same real_name .
In the mySQL query result I can then separate the NULL records to be sent for manual data-entry and automatically insert the remaining records into Table_B .
You can drop duplicates based on columns ` raw_name_left ` and also remove the ` raw_name_right ` column using ` drop ` #CODE

The instrument measures at the sub-second level , and I am using pandas to resample the data into 5 minute intervals , instead of messing around with modulo division for time values . After going through the panda s cookbook , I have come up the following code : #CODE

I'm still not sure if this is the best approach , but if you ` .apply ( pd.Series )` to the result of the ` split ` , you get a correctly-indexed frame . After that you can join : #CODE
The reason I'm not sure if this is the best way is because ` apply ` tends to be slow . Something like #CODE

Find a null value and drop from a dataframe in Pandas
What I want to do is that I want to find null in the " data " column and drop the row from the dataframe . I wrote my code for it but I believe it didn't work as expected since the number of rows didn't change . Could someone help me on this ?
You can keep only the notnull values using a boolean mask : #CODE

What I would like to do is replace the column content with just the phone number part of the string above . I tried the suggestions from this thread like so : #CODE

To get to your desired table , you basically just have to unstack ` gpm ` to move Bracket and Win into the columns : #CODE

Finally , just join what you need : #CODE

Combine several bool pandas.Series with uncommon indices with an logical and
If I understand what you want , I'd concat the 2 series column-wise and then call a function row-wise that drops the ` NaN ` values and returns the logical ` and ` of the 2 columns or the lone column value : #CODE

Pandas : Insert column according to index values
Firstly construct a df from the np array and then ` merge ` this . We merge the left side on ' id ' and the right side on column ' 0 ' and perform an outer merge , we have to drop the ' 0 ' column as it's superfluous to what we want : #CODE

why cant you just pivot the first data frame like this #CODE

Now if I drop the column ' Libell _Article ' from df ( which is a string column ) , I don't get the error message anymore .

For fast solution replace this #CODE

Sorry I don't understand how you arrive at the diff column as an output , are you just performing a cumsum on the differences and then indicating the direction of change ?
best explained by example : for 1 , the next value is 2 then 3 so 3-1 = 2 hence +1 value in diff column , for 4 on the other the positive change is only 5-4 =1 so the the next change is negative since 2-4 = 2 so -1 is the value
so for 4 the value change to 5 but since 5-4 =1 is not > = 2 it does not lead to a positive value . iterating further we arrive at 4 ( no change ) 3 ( change of -1 ) and 2 ( change of -2 <= -2 ) therefore diff = -1 for a negative change .
It appears that you are comparing the elements two indices apart . The values in the fourth and sixth rows are both 4 , so why isn't the " diff " 0 for the fourth row ?

Call ` resample ` and pass the rule as ' 10Min ' : #CODE

Interpolate pandas DataFrame to different TimeSeries
Is there a simple way to do this for the entire DataFrame ? I've looked into the ' resample ' and ' interpolate ' methods , but I don't think they do this .

I want to read the ` uk.txt ` file from a uk nga geonames download using python blaze and then odo to insert it into a Postgresql db .
I get the error ` ValueError : cannot safely convert passed user dtype of i8 for object dtyped data in column 0 ` that I think I understand as meaning " a datatype cant be converted to insert into the db "

possible duplicate of [ Map each list value to its corresponding percentile ] ( #URL )

I have a sequence of ` datetime ` objects and a series of data which spans through several years . A can create a ` Series ` object and resample it to group it by months : #CODE
I have tried using ` resample ( " M " , how= ' mean ')` on ` multiMmean ` and list comprehensions but I cannot get it to work . What am I missing ?

Purely count . You can see in the Late column what it is doing . The actual-sum and expected_to_date are equal and the Late is 0 . If they aren't equal , then the last time that expected was > = the current row , the Late column should be the difference in rows between the two . You could imagine a Helper column of df [ ' Helper '] = range ( 0 , len ( df )) and use the difference in the helper column for this difference .
I don't think this will vectorize as you need to check n-back , if it were just reseting each time it would be possible . I think I'd just drop to cython .

Pandas notnull is not working on column in data frame
I have successfully combined a series of ID's into one field and now I need to filter out any rows that did not end up with a Combined ID value . Usually I would use notnull but on this column it is not working . Can anyone fill me in on the problem ? Thanks ! #CODE

Use pandas ' builtin str methods for efficient string operations , and add them on directly to avoid a slow join : #CODE
maybe show a timeit for apply vs using the str ops :)

Call ` map ` and pass the dict : #CODE

Series ( and dictionaries ) can be used just like functions with map and apply : #CODE
I couldn't figure out what the syntax should be using ` .loc ` either to assign the values , hopefully a better pandas person will appear to answer that . To me this should just work so there must be a way of doing this without resorting to ` map `
Oh , I thought you were referring to something else . I figure map is as good of a way to do this as any . Could also do via merge , but I suspect that's a little slower ( but maybe clearer to read ) .

I can easily do this iteratively with loops , but I've read that you're supposed to slice / merge / join data frames holistically , so I'm trying to see if I can find a better way of doing this .
A join will give me all the stuff that matches , but that's not exactly what I'm looking for , since I need a resulting dataframe for each key ( i.e. for every row ) in A .
Ok , from what I understand , the problem at its most simple is that you have a ` pd.Series ` of values ( i.e. ` a [ " key "]` , which let's just call ` keys `) , which correspond to the rows of a ` pd.DataFrame ` ( the df called ` b `) , such that ` set ( b [ " key "]) .issuperset ( set ( keys ))` . You then want to apply some function to each group of rows in ` b ` where the ` b [ " key "]` is one of the values in ` keys ` .
There are a few built in methods on the ` groupby ` object that are useful . For example , check out ` valid_rows.groupby ( " key ") .sum() ` or ` valid_rows.groupby ( " key ") .describe() ` . Under the covers , these are really similar uses of ` apply ` . The shape of the returned ` summary ` is determined by the applied function . The unique grouped-by values -- those of ` b [ " key "]` -- always constitute the index , but if the applied function returns a scalar , ` summary ` is a ` Series ` ; if the applied function returns a ` Series ` , then ` summary ` constituted of the return ` Series ` as rows ; if the applied function returns a ` DataFrame ` , then the result is a multiindex ` DataFrame ` . This is a core pattern in Pandas , and there's a whole , whole lot to explore here .
` loop_iter = len ( A ) / max ( A [ ' SEQ_NUM '])

How do you check a condition of several pandas DataFrame.Series element-wise and apply the result to a new column ?

I have managed to do this , but it's slower than I would like ( takes 2 mins for a single 60mb file ; mostly in the apply part as seen below ) and I'm thinking that there must be a better way of doing it

Or should I calculated / spread the values when I do the merge ? I need help ...

pandas cut with infinite upper / lower bounds
So ` cut ( 250 , bins =[ 10,50,100,200 ])` will produce a ` NaN ` , as will ` cut ( 5 , bins =[ 10,50,100,200 ])` . What I'm trying to do is produce something like ` 200 ` for the first example and ` 10 ` for the second .
I realize I could do ` cut ( weight , bins =[ float ( " inf ") , 10 , 50 , 100 , 200 , float ( " inf ")])` or the equivalent , but the report style I am following doesn't allow things like ` ( 200 , inf ]` . I realize too I could actually specify custom labels via the ` labels ` parameter on ` cut() ` , but that means remembering to adjust them every time I adjust ` bins ` , which could be often .

We can construct a boolean mask that test if the values in the df are greater than ( ` gt ` ) 0 and less than ( ` lt ` ) 1 and then call ` np.all ` and pass ` axis=0 ` to generate a boolean mask to filter the columns and then multiply all values in that column by 100 : #CODE

I'd convert the dates to datetime objects using ` to_datetime ` , this then allows you to use the ` dt ` accessor to access the ` year ` attribute and we can then call ` isin ` and pass a list of years of interest to filter the df : #CODE
AttributeError : ' Series ' object has no attribute ' dt '

There is no ` str ` accessor for datetimes and you can't do ` dates.astype ( str )` either , you can call ` apply ` and use ` datetime.strftime ` : #CODE
Drop them first is one option so ` dates.dropna() .apply ( lambda x : x.strftime ( ' %Y-%m-%d '))`
As of version 17.0 , you can format with the ` dt ` accessor : #CODE

I cannot paste my current output because stack overflow doesn't like the format but what is currently happening is the multiplication is not occurring and the numbers are iteratively being spliced i.e. first element = 240.8856716 , second = 240.8856716240.8856716 and this continues .

To apply the same condition to to dozens of columns I could use ` isin ` , but it seems not to work if I need to substitute `' first '` with a regex , as in ` regex = ' ( ? = . *first ) ( ? = . *second )'` .
Why don't we use ` applymap ` on the entire data frame . This will be different than working the columns but would make it easier for your to apply if-else conditions to ( I hope ): #CODE
The whole point of ` applymap ` is that you can apply a function on every cell of the data frame . So , I think your two drawbacks are covered if you expand the function further . Need help with that ?
All right , after digging into ` pandas.core.strings ` here's the answer using ` applymap ` : ` df.applymap ( lambda v : bool ( re.search ( regex , v ))) .any ( axis=1 )`

Pandas DataFrame Name after pivot
I could rename column b before doing the pivot or unstack the pivot , rename the column and stack again . However , I'm looking for a simple attribute which I can't seem to find .

But how do I apply this function on each element of a pandas data frame ?
Pass the ` hash ` function to ` apply ` on the ` str ` column : #CODE

Merge pandas dataframe where columns don't match
I want to merge the data frames in a way so that data from ` Dataframe1 ` corresponding to 30 Apr 2014 appears against all dates in May 2014 in ` Dataframe2 ` . Is there any simple way to do it ?
My approach would be to add a month column for ` df1 ` that is the current month + 1 ( you'll need to roll December over to January which just means substituting 13 for 1 ) . Then I'd set the index of df1 to this ' month ' column and call ` map ` on df2 against the month of the ' date ' column , this will perform a lookup and assign the ' val ' value : #CODE

Insert rows according to value of columns in pandas dataframes

I then try using apply to run it on a dataframe to create a new column . #CODE
For a start there is a built in ` str.split() ` which is vectorised so you could eliminate that from your code so ` df [ ' word_split '] = df [ ' string '] .str .split() ` and then call apply on this column and change your line in your func to this ` listoflists = st.tag ( x )`

Repair Result to Aaron_schedule 03540.xml Errors were detected in file ' Macintosh H D: Users : AaronsMac :D ocuments : Aaron_schedule.xlsx ' Removed Records : Merge cells from / xl / worksheets / sheet1.xml `

You could iterate through them and apply the ` to_datetime ` function OR

Thanks . This almost worked , but it looks like the time shift was 11 hours forward rather than backward :
you can simply do a vectorized : `` s-pd.Timedelta ( ' 11 hours ')`` , no need for apply

lookback / shift in pandas dataframes calling functions
Is there any other way to do it ? can I somehow use pandas shift function in user defined functions ? thanks .
It's generally expensive to do it this way , as you're losing the vector speed advantage when you ` apply ` a user defined function . Instead , how about using the numpy version of the ternary operator : #CODE

Your question doesn't make much sense , Bob's answer returned you the values as a series , assigning to the values conceptually makes no sense how can you assign to 46 and 47 , they are integer values in a Series ? You can hold the Series as a variable but if you want to assign to those row's then you can use ` loc ` which is what Bob answered

Pandas how to merge two dataframe ( seems easy , ok ;)
I am trying to make a left join over two dataframe in python / pandas . I can't have it done :-(
Why don't you reduce the dfs to a small enough size to post them in the question . What do they look like ? What is the join column . Personally , I have no idea what's happening with your example .
There is no bug on this code : according to the documentation ( actually O'reilly ' python for data analysis ' , p179 / 180 ) a many-to-many left join make a cross-product over the lines .

I have long list of date in dataframe that need to be converted into datetime , the date is in the form " %d%m%Y " , and I apply ` datetime.strptime ( x , ' %d%m%Y ')` , which works fine until meet the date " 3122012 " , which should be datetime.datetime ( 2012 , 12 , 3 , 0 , 0 ) , but instead it throw the error message : #CODE

( FYI if i insert a print print ( vals ) in the middle of that loop , it prints #CODE
Now given the nature of how ' log_lower ' and ' log_upper ' is defined , there's overlap at the end of each row's ' log_upper ' and the subsequent row's ' log_lower ' , with those values being the same - does the append function in the loop or numpy.concatenate later take that into account when making the array and cancel one of the overlaps ?

How to create a new derived column in a pandas dataframe new column using only notnull vaues
Often I want to map a function to a column containing nulls , and I find my self having to write some sort of logic checking for nulls . For example : #CODE
This type of thing always feels somewhat clunky to me . Is there a preferred way to apply a function to only the nonnull rows of a column ?
Use the isnull or notnull methods to filter out non-null elements of a column #CODE
Thank you for this , I had initially tried something like this but didn't realise that you only have to apply the filter on the RHS of the assignment and not the LHS .
Use ` loc ` and ` notnull ` to mask the df and the vectorised ` str ` methods to ` split ` the strings : #CODE

Set index on ' TOTACTVAL ' and unstack to level -1 and reset index #CODE

Pandas : How to use apply to create new dataframe
and appy that function using apply to the created DataFrame using #CODE
Can you reconstruct this problem ? Did I get anything wrong regarding the use of the apply function ?
Because you're passing your ` data ` df as a reference and assigning directly to it each time by calling ` apply ` in your func then it overwrites with the last operation : #CODE

I was encountering the same issue when trying to write a pivot table to Excel . I was able to get this to work by modifying the ` frame.py ` file in ` .. / pandas / core ` . Changing ` if self.columns.nlevels 1 ` to ` if self.columns.nlevels 1 and not index ` got me what I needed .

2 ) Create a map ( is there a way to do this from reading another file ? Like I would create a .csv and load it as another dataframe and then match it ? Or do I literally have to type it out initially ? )
3 ) Basically do a join ( VLOOKUP ) and then del the old column with the long object names #CODE
You want to replace the categorical text with integers hoping it takes up less space ?
Sorry , I should have been more clear . I use the category type for analysis and it works perfectly , but when I store the files as .csvs they are still very large . I also connect these .csvs to Tableau so I was thinking I could do a join in Tableau too .
Map dataframe with int codes to category definitions
Note the drop in memory usage down to ` 976.6 KB `
Now use ` map ` to replace ` int ` coded data with categories descriptions and convert them to ` Categorical ` : #CODE

The index is actually the date , it's just SO formatting . Yes , I want to group by marker and date , and it seems that [ ' marker ' , df.index.date ] doesn't cut it , and without having to do a group by inside a group by just because of the difference in index sizes .

@USER : You'll have to explain what you mean . Each column has only one dtype . You can subset based on any criterion you can apply to each item in the column .
where df is the name of your dataframe . You can then drop those values and recast the remaining values as integers like follows : #CODE
I'm pretty sure using ` map ` is slower than using ` df.year_week.str.len() < 6 ` .

You can use ` df . column .map ` to apply a function to each element in a column : #CODE

but this does not always work ( code from diff columns ) . for example , none of the following will remove \r or ' , ' #CODE

python pandas use map with regular expressions
and I want to map it to a pandas df : #CODE
Is there a way to use regex to match map on " Commerciante " column ? In this way I can rewrite dealer as this : #CODE
Why don't you use ` apply ` and on a modified dictionary lookup : #CODE
And , apply it like this - #CODE
First clean your data , then map it .
And use it with apply #CODE

Replace all your ` df.a ` with your memoryview ( ` df_a_mv `)
As a final small point : ` if ( i len ( df.a ) -1 ): ` is unnecessary - it's guaranteed by the surrounding ` for ` loop .

This is basically the right logic , but the resampling doesn't have a fixed start , so the date ranges for the 3-day periods don't align between categories ( and I get NaN / 0 values ) .

I want to resample flatten dataframe to multi-indexed columns .
I try somethings with groupby or stack but I don't find a good way ... Does anyone know how to do this ?
With ` unstack ` ( to use this you first have to set the multi-index ): #CODE

Yes , the way you put it here , it works , but eg if you change the order of the two ` [ .. ]` it fails . Therefore , in general , always better to use ` loc ` to be sure

If I understand your question correctly you should be able to just call ` merge ` on ` dfBig ` and pass ` dfSmall ` which will look for matches in the aligned columns and only return those rows .
Yes , I'm an idiot . A few minutes after posting this I thought " this sounds like an SQL merge " and google pandas merge . Thanks for the answer !

Will lead to a stack trace ending with the error : #CODE
You're explicitly specifying the dtype so it's expecting all values for that column to conform to that type , if you know the values to treat as ` NaN ` you can pass these to ` na_values ` however it looks like it's better to just let ` read_csv ` guess but this will mean that the dtype will be ` float64 ` as ` NaN ` cannot be represented in ` int64 `

Check out odo . You can do each of these operations ( append and retrieve ) in a single line , even when you have multiple ` DataFrame ` s . Here's an example : #CODE

If I understand your question correctly , your problem here is that your date strings are of an inconsitent format so when you construct a datetimeindex or use ` pd.to_datetime ` then any missing time portion ( or date portion for that matter ) a default value will be supplied , You could replace the ` 00:00 : 00 ` with ` NaT ` afterwards
Hello EdChurn and Ed Smith . Thank you for your comments and sorry for been a bit unclear , I am new to python & pandas . You are right that the date strings are inconsitant and that when constructing the datetimeindex , default values ( missing values ) are given as 00:00 : 00 . I cannot replace these with ' NaT's after , as I also have real time values of 00:00 : 00 . I will also be splitting them to ' Date ' and ' Time ' if that might make a difference . The ' ** ' was only to draw attention to the particular instance's . Sorry for confusion . Thank you again for your help .

You'd be better off using ` df.iloc ` as this is integer based rather than label based as your index values may not be 0 based int64 , so something like ` for i in range ( len ( df )): #doSomething with df.iloc [ i ]`

Should I completely drop any interaction with duplicates ? Should I create a more unique ID based on some type of time interval ( like if the EndDatetime is within X minutes from the Datetime on another file ( which is close to the end of the interaction normally ) ?
I think that is fair enough and worth testing ( total samples if I just drop duplicates on a daily basis versus a ' acceptable time frame ' window . What kind of code could I create to do that ? Like I take the timestamp of the ' ticket ' file and then the actual timestamp of a call . How could I try to match the tickets to a timestamp within 30 minutes or something like that ?

pandas : how to merge multiple indexes together ?
I have two levels of index , and I want to merge them into one level of index . I looked at methods such as reset_index and reindex , but they don't seem to be what I need . Another way I can think of is adding a new column containing the merged indexes , set that column as the new index using pivot_table , and delete the old indexes . But I'm wondering is there a more elegant way to do this . Any input is welcomed . Thank you so much !
will give you tuples in a single level if that's what you mean by ' merge ' .

But I don't get a 2D heat map , when I do : #CODE
Not sure whether this is all that much more efficient , but you could ` pivot ` and then add the frame to its transpose , something like : #CODE
Here is the documentation on ` add ` and ` pivot ` . Here is what is going on . The first line ` df = pd.read_csv ( " input.txt " , header=None , delim_whitespace=True )` returns : #CODE
The third line ` df.add ( df.T , fill_value=0 ) .fillna ( 0 )` just adds the transpose to convert the triangular matrix to a symmetric matrix . It returns : #CODE

datetime dtype values have a ` dt ` accessor where you can access the seconds attribute .

Hi Ann , I tab between the first and second columns , but a space between the second and third columns . I tried using the Unix Unexpand program to replace the space between the second and third column but got two tabs by this method . How can I modify your script to create the tab between second and third column . Thanks .
The tab character is ` \t ` so you can put that where ever you need , or replace it with a space `' '` .

and functions like ` pd.merge() ` where there is an option to merge " on " something did not do what I wanted .
If I understand what you want then you can call ` map ` and pass the series : #CODE
With regards to optimising your code you can replace this line : #CODE

Once this is created , you can create a function to split the categories column by the " , " and count the length of the resulting list . Use lambda and apply .
` business [ ' number of categories '] = business [ ' Categories '] .apply ( lambda x : len ( x.split ( ' , ')))`
num_categories = len ( split_up )
it'd be better to use the vectorised str split method : ` business [ ' Category '] .str .split ( ' , ') .apply ( len )`
---> 31 business [ ' # Categories '] = business [ ' Category '] .apply ( lambda x : len ( x.split ( ' , ')))
Assuming that Category is actually a list , you can use ` apply ` ( per @USER ' s suggestion ): #CODE
Even accounting for the ` len ` function call : #CODE
` business.Category.apply ( len )` would work also
You should use the vectorised str methods : ` df.Category.str.strip() .str .split ( ' , ') .apply ( len )`

Pivot tables can do this : #URL There's also pandas.DataFrame.rename

Python pandas pivot multiindex
I am importing the data frame and I am trying to do a pivot like this : #CODE
` pivot ` looks like this : #CODE
Afterwards I do ` df3 = df2.add ( df2.T , fill_value=0 ) .fillna ( 0 )` to convert the triangular matrix to a symmetric matrix . Is generating new columns really the easiest way to accomplish what I want ? My reason for doing all of this is that I want to generate a heat map with matplotlib and hence need the data to be in matrix form . The final matrix / dataframe looks like this : #CODE

Pandas shift moving discarded elements to the beginning
I can't find a simple way to shift a pandas DataFrame , but instead of discarding the values at the end , moving them to the beginning .

In my case the ' bid ' column has different values for each stock , so I will not know what value to replace it with .

" Continuous values can be discretized using the cut ( bins based on values ) and qcut ( bins based on sample quantiles ) functions "
Sounds very abstract to me ... I can see the differences in the example below but what does qcut ( sample quantile ) actually do / mean ? When would you use qcut versus cut ?
Conversely , for ` cut ` you will see something more uneven : #CODE
That's because ` cut ` will choose the bins to be evenly spaced according to the values themselves and not the frequency of those values . Hence , because you drew from a random normal , you'll see higher frequencies in the inner bins and fewer in the outer . This is essentially going to be a tabular form of a histogram ( which you would expect to be fairly bell shaped with 30 records ) .

pandas - map nested dictionary values to dataframe column
and I want to map on a new column the following nested dictionary : #CODE
I'd flatten the dict to create a new dict and then you can call ` map ` as before : #CODE

Now we can unstack the last index level : #CODE

For the second df , we can call ` pivot ` to rotate the df to create the columns from the ' bid ' values , we need to reset the index and then we can merge the 2 df's together to get your desired result , you can replace the NaN values with blank strings if needed : #CODE
What you want to do is pivot the table .
I have to physically add the header as well as use the ` names ` parameter for it to work , which means I end up with a redundant row header . I have no idea why this is . It seems both unstack and pivot have an issue with ' duplicate entries ' as I've discovered here #URL After that I came across the ` pivot_table ` function which seems to work .
Yeah , which of pivot , pivot table and unstack you want to use will depend on how your data is structured and what you want to achieve .

Also , I assume it would be better to read the roster dump and create separate tables to normalize the data as best as possible . The current roster is like 65 columns and there are 40,000 employees . So many columns are categorical data just repeating over and over again constantly . My method would be to read the file , create dataframes for each Table I want to create , then export it into an SQLite database . I would try to make each dataframe a normalized table . Is that the best method ?

There is also the ` gt ` which means greater than : #CODE
@USER wow thanks for answering also my other question ! Yeah that is arithmetics between series , i did not realise ! I thought that the expression was somehow extracted and evaluated on every row as a lambda . About indentation , i use ipython as well , and commands follow the ` In [ ]: ` line , separated by one space . Just Stack Overflow would not allow to edit such a small thing , it does not matter

You can groupy the ' ITEM ' and ' CATEGORY ' columns and then call ` apply ` on the df groupby object and pass the function ` mode ` . We can then call ` reset_index ` and pass param ` drop=True ` so that the multi-index is not added back as a column as you already have those columns : #CODE

@USER - It helps . But in case the user input is " 1 " , it doesn't select df [ ' POSTING_PERIOD '] = ' CDD01J ' . What is the best way we can use the append prefix 0 logic in case user input is < 10 . Thanks

Without having access to your data , for pandas at least there is the top level ` isnull ` and also this is available for the ` df ` or ` series ` , you can also call ` dropna ` : #CODE
The data frame looks like this : ` df = pd.DataFrame ( { ' a ' :[ ' asdsa ' , np.NaN , 9 8] , ' b ' :[ np.NaN , np.NaN , 99 ] } )` . I then want a unique vector of all elements , excluding ` NaN ` . In this case , ' asdsa ' , 99 , 98 . If I simply replace ` np.isnan ( uniqueID )` by ` pd.notnull ( uniqueID )` in the code of the question it works . The problem with ` df.dropna() ` in that case is that it drops all rows including ` NaN ` s .

should work , depending on whether you'd like to drop the ` id ` level .

How to reference groupby index when using apply , transform , agg - Python Pandas ?
Now I want to groupby date in df1 , and take a sum of value A in each group and then normalize it by the value of B in df2 in the corresponding date . Something like this #CODE
The question is that neither aggregate , apply , nor transform can reference to the index . Any idea how to work around this ?

Liam , I've imported the datetime module as dt , but I'm receiving this new error . Do you know what's going on ? Thanks again !
AttributeError : ' Series ' object has no attribute ' dt '
@USER the ` dt ` accessor was added in ` 0.15.0 ` can you upgrade ?

Here is what i want to do , histogram plots of all columns of a dataframe but without the grid axes . The below code works , but preferably I'd like a more elegant solution ( such as passing an argument to hist ) #CODE

IIUC , all you need is ` pivot ` . If you start from a frame like this : #CODE
and we know the InputID and TargetID are unique , we can simply ` pivot ` : #CODE
FWIW I did the same thing as above with a data set of your size ( with 3330 ids ) which has substantially more rows and the pivot operation still finished inside of 9 seconds . Thumbs up to DSM's pivot solution

How can I join columns in Pandas ?
Now I d like to join the columns YEAR+MONTH+DAY+HOUR+MIN to make a new one , for example #CODE

After some tinkering around , I wrote a function that can be used with the ` apply ` method on a ` groupby ` . #CODE
You don't need to call ` tolist ` , just replace ` straight ` with ` trans ` -- no need to cast as a list . That will save time replicating the data set ( if it's real long , this takes time ) .

It seems like shift is close : I could get the rainfall 60 days ago , 59 days ago , etc . I could possibly create 60 series objects and sum them this way but it feels like there's probably a better way .

How to use join to fill missing values of a column - Python Pandas ?
I want to do kind of a left outer join to fill the missing values in df1 , and generate

How to export DataFrame to_json in append mode - Python Pandas ?
I want to save " df " with to_json to append it to file output.json : #CODE
* There is append mode= ' a ' for to_csv , but not for to_json really .
The existing file output.json can be huge ( say Tetabytes ) , is it possible to append the new dataframe result without loading the file ?
No , you can't append to a json file without re-writing the whole file using ` pandas ` or the ` json ` module . You might be able to modify the file " manually " by opening the file in ` a ` mode and seeking to the correct position and inserting your data . I wouldn't recommend this though . Better to just use a file format other than json if your file is going to be larger than your RAM .

Also , I would like to shift all timestamp by a fix amount of time , such as 1ms ` timedelta ( 0,0,100 0 )` , how can I implement it using method 2 ?
If I want to shift all timestamp by a centain time such 1ms ( timedelta ( 0,0,100 0 )) , how can I do it ?

You can use ` apply ` to extract the numerical values , and do the counting there : #CODE

Pandas create pivot from grouped columns
I have an xls sheet where I want to combine columns ( for each ethnicity ) into an index column ( named ethnicity ) that I can use to partition ( construct a pivot table for ) other numeric variables . The data came as a list of columns like below :

I am able to achieve this using a combination of ` concat ` , ` sort ` and ` ffill ` .
The trick is to use ` append ` which adds a column to the existing index . ` df1.append ( fd2 ) .set_index ( ' id ' , append=True )`

This is what I want to do , for all the rows with same value of column a , I want to drop duplicates , but value of column b should be summed across those rows , and for rest of the columns , I want to keep the first value .
I'd assign to column ' b ' the result of grouping on ' a ' and summing , you can then drop the duplicates : #CODE

If not already you need to convert to datetime , then you can call ` apply ` and use ` datetime.strftime ` to do the formatting : #CODE

Use ` loc ` and a boolean mask to set the values you desire : #CODE
You can use ` apply ` : #CODE

Merge dataframes in a dictionary
And I want to merge them all into one like this : #CODE
But is there a way to automatically zip through the dictionary and merge ?
You can just pass the dict direct and access the ` values ` attribute to ` concat ` : #CODE
This assumes that you are just looking to concatenate all the dfs , if you are going to merge then you need to explain what the merge criteria is

When I try to plot a histogram , I apply : #CODE
it may be a bug , can you upgrade pandas ? also what does ` df.plot ( kind= ' hist ' , bins=50 )` show

In a comment you state that you are recursively passing the data to a function . Why not leave the data alone in a global and pass the index on the stack ?
Since ` index ` is an array of type ` bool ` , you are doing advanced indexing . And the docs say : Advanced indexing always returns a copy of the data .
It has the array ` shape ` , ` strides ` ( default here ) , and pointer to the data buffer . A ` view ` can point to the same data buffer ( possibly further along ) , and have its own ` shape ` and ` strides ` .
But indexing with your boolean can't be summarized in those few numbers . Either it has to carry the ` index ` array all the way through , or copy selected items from the ` x ` data buffer . ` numpy ` chooses to copy . You have choice of when to apply the ` index ` , now or further down the calling stack .

pandas - drop rows under Datetime criteria
with ` User ` - ` Datetime ` information . I would like to drop Users under certain Datetime criteria , for instance when they are present more than , let's say , 3 or more times in the same minute of the same hour of the same day . Under this condition , Users 113 and 115 should be dropped out of the DataFrame . So far I tried to groupby the ` User ` column and to get information about the datatime object , but with no results .

Now we can fill in the missing station number by calling map on the df with the ` NaN ` rows removed , and the station name set as the index , this will lookup the station name and return the station number : #CODE
When I run the 3 section ( to fill NaN's ) on my dataset I get the ` InvalidIndexError : Reindexing only valid with uniquely valued Index objects ` . So my thinking is that there may be a street of eg . ` Paul Rd & Pl NW ` : ` 31202 ` , but also another row with same street bit slightly different : eg . ` paul Rd & Pl Nw ` : ` 31202 ` . Would I need to strip out white space and put to lower case the Start station . hope that makes a little better sence . @USER
Yes you'd have to clean and extract the data before calling ` map `
You'd have to merge in that case something like ` df.merge ( df.dropna() , on= ' Start station ' , how= ' left ')` I think should work
I have marked as answered , but just for clarity for anyone stumbling upon this question . I had to do something slightly different due to the data . the street names in my data were entered inconsistently . i.e. ' Paul Rd & Pl NW ' and ' PI & Paul Rd NW ' . This caused problems with merge and grouping . To overcome this , I was forced to drop some instances and merge with a separate file containing proper station names ( merged on station name after cleaning , white space strip , and putting to lower case . ) Thanks to all for the help .

I also tried with apply and I think it works , but I need to reset the index , which is something I'd rather avoid ( I have a large dataset and I need to do this repeatedly ) #CODE

If you don't have the seaborn package installed , you can just drop color= sns.desaturate ( " indianred " , 1 ) as this was purely for aesthetics .

Trying to create a new column with the groupby calculation . In the code below , I get the correct calculated values for each date ( see group below ) but when I try to create a new column ( df [ ' Data4 ']) with it I get NaN . So I am trying to create a new column in the dataframe with the sum of ' Data3 ' for the all dates and apply that to each date row . For example , 2015-05-08 is in 2 rows ( total is 50+5 = 55 ) and in this new column I would like to have 55 in both of the rows . #CODE

Pandas groupby : apply vs agggregate with missing categories
I'm running into an issue where panda's ` GroupBy.apply ` and ` GroupBy.aggregate ` give different-shaped results when categorical data has missing values . ` aggregate ` retains all " known " categories , but ` apply ` only keeps the categories that are present in the data .
Note that the last data frame is missing the ` NaN ` rows where ` missing = b ` . I understand why ` apply ` might do this ( it chooses not to pass a group full of ` NaN ` s to the reduction function ) . The above snippet is just a toy example : I actually need to use ` apply ` to get the result I want .
Question : What's the best way to use ` apply ` but create an output shape matching the one returned by ` aggregate ` ?

And I need to merge them into a single row with ind_con_dem and ind_con_gop ( so I can create comparison metrics ) .

Nor can I merge the resulting series of a df.groupby.apply ( lambda x : len ( x ))

I'd recommend a dict if you want to quickly access each df , but a list is less typing to concat into a single df

Is there a way to parallelize Pandas ' Append method ?
This issue with using concat is that it lacks the arguments that to_csv affords me : #CODE
It'd be more performant to read them into a list and then call ` concat ` : #CODE

Stack your first level : #CODE

Read the file as you have done and then strip extra whitespace for each column which is a string : #CODE

Appending the function above with ` .values ` will give you the result you desire , but it is then up to you to replace the index and columns . #CODE

Try ` df.groupby ( ' asn ') .filter ( lambda x : len ( x ) 1 )` which will return you a ` DataFrame ` . You could group it again further if necessary .

well I ordered ` desc ` by date , so ` head ` will work :) But you're right , ` tail ` could be more descriptive . I always translate the task to SQL in my head , and in SQL there's only ` top n ` keyword .

i = choice ( range ( len ( df )) , df.value )

the writing . I think it should be possible to append or something similar
for to_hdf check the parameters here : DataFrame.to_hdf you need to ensure mode ' a ' and consider append .

Here's a fairly general solution you can apply to multiple columns . The ' To ' column doesn't need to be rounded , I just included it for the generality of two columns rather than one : #CODE

Then , reset the index of the original ` DataFrame ` , and merge : #CODE
Use this ` func ` and apply over the the ` dff.groupby ( ' Group ')` #CODE

Really can't get to grips with how to plot a summary table of a pandas df . I'm sure this is not a case for a pivot table , or maybe a transposed method of displaying the data . Best I could find was : Plot table and display Pandas Dataframe

With the ` func ` function you wrote in your updated answer ( i.e. your solution ) , you should be able to use the ` DataFrame.apply ` method with parameter ` axis=1 ` . ( I haven't tested it , but perhaps you could try to apply it and report the error message , if any )
The key to this answer is that the ` apply ` method accepts arbitrary positional keyword arguments and passes them to the function .

You can use ` apply ` with option ` axis=1 ` . Then your solution is pretty concise . #CODE
In the above example you start with ` x ` and replace each value with the corresponding value from ` y ` , as long as the new value is not ` NaN ` .

Is there a faster way to append many XLS files into a single CSV file ?
One thing to try immediately is to build a ` datas ` list instead of reappending the frame each time . IOW , add ` datas = [ ]` ; ` datas.append ( data )` and then ` frame = pd.concat ( datas )` or something in the appropriate places , and remove the append . That will help you find out if it's the repeated appends , but you should also time the read_excel itself ( remove everything else and see how long it takes . )
There are several links in google and even in stack overflow that are great starters for profiling : #URL #URL #URL

In Using describe() with weighted data and pandas : groupby and variable weights , the OPs asked very similar questions , but the answers have been insufficient . In my case I want the percentile distribution including the median so multiplying the variable of interest with the weights won't work .

Well can you try ` df_conf [ col_name ] = df_conf [ col_name ] .str .encode ( ' utf-8 ')` replace ` col_name ` with whatever column you want to encode , saying that can you encode your index ? Is your index a str ? could you try ` df_conf.index = df.conf.index.str.encode ( ' utf-8 ')`

` shift ` returns the row at a specific offset , we use this to subtract this from the current row . ` fillna ` fills the ` NaN ` values which will occur prior to the first valid calculation .

Can you post sample df's and desired output , you can use another series and call ` reindex_like ` or add a temp column with the order you want order by that column and drop the temp column

create empty list , add item using append , create series #CODE

df.fillna ( 0 ) command won't replace NaN values with 0
I'm trying to replace the NaN values generated in the code below to 0 . I don't understand what the below won't work . It still keeps the NaN values . #CODE
I tried to save the df as an sql table using : ` df_pubs.to_sql ( ' conferences_pubs ' , db , flavor= ' sqlite ' , if_exists= ' replace ' , index=True )` when ` index=False ` , it works but I need the Conference Name ( ie the index ) to be added as a column , so when I put index=True it gives me an error : ` ProgrammingError : You must not use 8-b it bytestrings unless you use a text_factory that can interpret 8-b it bytestrings ( like text_factory = str )` . It is highly recommended that you instead just switch your application to Unicode strings . Any ideas why it does that ?

Reshape ( stack ) pandas dataframe based on predefined number of rows
I can certainly do it using a loop , but I'm guessing ` ( un ) stack ` and / or ` pivot ` might be able to do the trick , but I couldn't figure it out how ...
Symmetry / filling up blanks - if the data is not integer divisible by the number of rows after unstack - is not important for now .

I've tried various combinations of pivot , shift , and groupby , but still can't coerce this output format . Any suggestions ? I'm interested in solutions that will help me learn to use pandas more effectively . Thanks !
If I understand you correctly and your month column is monthly already then you can ` groupby ` ' category ' column and call ` transform ` on ' var1 ' and pass function ` shift ` and assign this back to your df : #CODE

It's because the indices don't align , if the 2 df's are the same length you could do ` df_if [ ' Conference '] = df_conf [ ' Conference '] .values ` to assign the np array values

How to apply Cython to Pandas DataFrame

I have the following pandas DataFrame dt : #CODE
The sku is now the column so you need to use ` loc ` to perform label selection : #CODE

You can get the row with ` ix ` : #CODE

So replace afterwards .
There is something different about your real use case that I'm not seeing . I've run the code above with ` df = pd.concat ([ df ] *1000000 )` so that ` len ( df )` is 9 million , and ` %timeit df.groupby ([ ' a ']) [ ' b '] .transform ( ave_others )` completes in 1.18 seconds .

After reading the whole Dataframe , I tried to apply function on one Serie : #CODE
Use ` loc ` : #CODE

Ideally , I want to reinject / merge results in other dataframe .

While doing preprocessing of data , I first try to remove all the punctuations and also the most common stop words . After doing that , I want to apply the Porter Stemming algorithm which is readily available in nltk.stem .

why do pandas comparison operators not align on index ?

All I am doing at the moment is loading the .csv as a dataframe and then writing it to the db using ` df.to_sql ( table_name , engine , index=False , if_exists= ' append ' , chunksize=1000 )`

You could ` apply ` and construct the datetime using your desired date values and then copying the time portion to the constructor : #CODE

How to resample time series with pandas and pad dates within a range
I have a pandas ` Series ` indexed by timestamp . I group by the value in the series , so I end up having a number of groups each either their own timestamp indexed ` Series ` . I then want to ` resample() ` the group series each to weekly , but want to align the first date across all groups .

Use the ` apply ` method . #CODE
Assuming the data has been pre-sorted on df [ ' index '] , try using ` loc ` instead : #CODE
An alternative solution is to create a pivot table , forward fill values , and then map them back into the original DataFrame . #CODE

However , when I attempt to normalize on level 1 of the columns as follows : #CODE

You can use a combination of ` loc ` and ` np.diagonal ` to achieve this : #CODE
` loc ` here will perform row label lookup : #CODE

Error when using pandas dataframe map function in ipython notebook
Any idea why I get this error on the 2nd use of the map function but not the first ? There are no NAN values in the Embarked column per value_counts() . I'm guessing it's a noob problem :)
Something doesn't add up you have 891 rows for your gender ` value_counts ` but 889 for your embarked , this means you must have ` NaN ` values , the [ docs ] ( #URL ) state that ` NaN ` value are not counted you can confirm this if you try ` df [ ' Embarked '] .value_counts ( dropna=False )` , this means you need to handle the NaN by filling them first before calling map
The last line of the error message : ValueError : Cannot convert NA to integer . You probably have to drop the NAs from your dataframe .
So you either drop them first ( using ` dropna `) or fill them with some desired value using ` fillna ` .
I was not aware that ` value_counts() ` dropped NaN values by default . added this command ` df2 =d f2.dropna ( subset =[ ' Embarked '])` and that removed the NaN , however I get a different error now when trying to run the map command : `

You'd have to merge in that case to find the common rows and then do some kind of diff between the merged and df2 but it depends on the order and how they differ as it becomes complicated depending on how they differ

Divide the problem up more . E.g , comment out the ticklabel lines , does it still break ? Do it with a dummy ` data ` of four-by-four ones and zeros ; still broken ? Etc . Replace the ' / ' in the last sample line with a ' ( ' . ( I strongly recommend working in a script file instead of the interpreter . Others differ , but while you're learning , don't set yourself up for cut-and-paste errors . )

I am trying to search from one matrix and replace that value on 2nd matrix . #CODE
thanks for your support , still not working . Also one thing , it concerts ` 0 ` to ` nan ` , any way to keep 0 only , I tried wtih ` arr.replace ( np.nan , 0 , inplace=True )` but it says ` AttributeError : ' numpy.ndarray ' object has no attribute ' replace '`

With Pandas it's often a good idea to try and use ` apply ` together with an anonymous function to perform your calculation on every row . Does this work for you ? : #CODE

@USER The problem is that the ` ExcelWriter ` isn't created by ` pd.ExcelWriter() ` , but through ` xlsxwriter.Workbook() ` to just write some arbitrary ( non-pandas ) data . Unfortunately , the approach from your link doesn't apply here .

I edited the code so that you get ids as rows and categories as columns . It does not use sum() because I use a different logic . I create the final dataframe with 0s and populate it : with ` name ` , I iterate over ids and for each id / name : instruction ` df_final.loc [ cat , name ] = 1 ` writes 1 for categories relevant to this id in the column dedicated to this id . Finally I just transpose the dataframe to get ids as rows and categories as columns . It runs in a few seconds on my computer while it's not that recent .

Merge Only When Value is Empty / Null in Pandas
I have two dataframes in Pandas which are being merged together df.A and df.B , df.A is the original , and df.B has the new data I want to bring over . The merge works fine and as expected I get two columns col_x and col_y in the merged df .
Here's what I mean , how can I merge df.A : #CODE
You can then ` drop ` the extraneous columns : #CODE

when you open csv file from excel , it will convert your data to any type it should be in this case your data converted to date type then excel apply default date format to that data . Also , you can't control the date format of excel file since csv files is only a text file , no meta or hidden data to advise excel to proceed .

You could do some hacky stuff to get this to work if you are using a unix system . Like : cat filename | sed ' s / ISODate ( " // g ' | sed ' s / ") // g ' > newfile This will strip the ISODate ( "") but keep what is inside it . Maybe after that you could load the data . Not very clean , but may work for you .

Interestingly , at times , I noticed ` len ( x.unique() )` is much faster than above methods . #CODE

Using ` apply `
You could use ` pandas `' s ` apply ` for this . #CODE

How can I now apply this test on a slice of the dataframe ? #CODE
The quickest method IMO is to just filter the whole df and then drop the resulting ` NaN ` values ( using ` dropna ` setting a threshold on the column axis of at least 1 non- ` NaN ` value : #CODE
Hi @USER ! Thanks for your help . I upvoted your answer because it was helping me getting the job done . Thanks a lot for it ! I am not sure whether it is really the answer to my question , though ( How to apply the test to the dataframe ) . Perhaps someone else can answer that question ?
The boolean test ` df ! =0 ` specifically masks the entire df so will produce a True / False mask when used to mask the df this will put ` NaN ` values where the mask is ` False ` we can then drop these values column-wise ( using ` axis=1 ` but we only drop these if all values in the column are ` NaN ` as your desired

pd.read_csv works fine for me per above . Your exception stack indicates you used this function instead : df = pd.Series.from_csv ( data , sep= ' , ' , index_col=0 )

Pass the dict as an arg to ` map ` : #CODE

I am currently able to do this through some very long-winded method with iterrow rows however for a dataframe of ~16000 , it took 45minutes and I would like to cut it down because I have larger dataframes to run this one .
without going into specific code , is every user_id in dg found in data_conv and data_retargeting ? if so , you can merge ( #URL ) them into a new dataframe first , and then compute the max / min , and extract the desired columns . i suspect that might run a little bit faster .

Also , I suspect this error is because my ` Max . CPC ` column has `' -- '` in a few areas instead of zeros . I want this column datatype to be a float . How do I translate these `' -- '` to ` 0.00 ` and also set this column as a float datatype when reading the CSV ?
You could pass ` na_values =[ ' -- ']` this will convert these values to ` naN ` and the dtype will then float , or replace these values after loading to 0.00 and then the dtype of the column would be float , as for your columns error you'll need to post the raw input in order for us to reproduce your error
The other is to load as before and ` replace ` these values to ` 0.00 ` : #CODE

and so on . Is there any way I can maybe resample ( wrong word ? ) the data so that I can have axes on a minute by minute basis while still maintaining the data points in the right place ?

The problem here is how should ` NaN ` be represented here ? it is a common issue in which you need to decide how to handle them , you can either drop them or substitute them with mean or some other inidcator value
I just picked a strategy to replace missing data with the mean , using the ` Imputer ` class . There are other strategies . #CODE

I'm having a question on python . I'm trying to compare two dataframes and check which elements are different and insert them into another dataframe . So here are my dataframes . df1 : #CODE
So the idea is to add to df1 the data from df2 that is not yet in df1 ; line 2 in df2 does not exist in df1 so I want to insert it .
How should i write code to find not existing line so that I could insert it ?
You can use outer join to achieve this result #CODE
I believe you want to perform an outer ` merge ` : #CODE

Write a function to replace the index : #CODE

One follow-up question : while this works great for " year " , I would like to do a similar thing for " yearmonth " , i.e. a combination of YYYYMM values ( so that each month of each year gets a specific label ) , As there is no attribute to directly extract " yearmonth " from index , what can I do in this case ? Could I define a lambda function and apply this to the index values , for example ?
How is this label going to look as what you're suggesting looks like a string ? you should be able to do ` import datetime as dt df.index.apply ( lambda x : dt.datetime.strftime ( x , ' %Y%m '))` I think should work

Columns : Av_density ; pred2 ; LOC ; Year ;
My aim is to draw a 2x2 figure panel describing the relationship between Av_density and pred2 separately for each LOC (= location ) with years marked with different colours . I call seaborn with : #CODE
sns.lmplot ( " Av_density " , " pred2 " , Data , col= " LOC " , hue= " YEAR " , col_wrap=2 );
This produces the figure panel divided by LOC accurately , with Years in different colours , but the scatter of the data points does not look right . Instead , it looks like lmplot has linearised the data points , and lost the original scatter points that it is supposed to be drawing in addition to the regression lines .

How do I merge columns by two in a table in pandas ?
I want to merge columns in it in this way : #CODE

As pointed out by @USER the column reordering is unnecessary as pandas will naturally align against the columns anyway so : #CODE
If you divide by a Series ( by selecting that one row of the second dataframe ) , pandas will align this series on the columns of the first dataframe , giving the desired result : #CODE

One way to do this is simply by using ` merge ` function to choose which keys you want to join on .

You should use ' environements ' , eg `` virtualenv `` or `` conda `` environemnts . I recommend using ` conda ` ( or the Anaconda python distribution ) when using the scientific python stack .

I originally used the standard Python libraries for this but the result is quite slow . I have tried to replace this by using the POSIX c library functions ` strptime ` and ` mktime ` directly but have not been able to get the right answer for the time conversion .
yep , even with the `` infer_datetime_format=True `` which caches the formatting . And the conversions are already in c , it is most of this . ( Not this is a very general ISO parser , handles tz , odd formats and such ) .

pandas : join two table and populate value when even there is not a match
I want to join the following two table df1 and df2 to get a table like df3 where all IDs , A , B , C , D need to be present for each Time . #CODE
But left join only gets me this ... #CODE
Your merge doesn't make sense you have ID in df1 which has A and B but in df2 it has C and D also so you're going to get NaN values as you've found . Otherwise there is no real relationship here as the other columns are the same values
I don't think the issue is the merge , but rather you need to define what values are missing . I would do this by making an intermediate dataframe that has all the Time ID combos you want to be present in the final dataframe : #CODE

How to avoid return twice the first groupby object after to apply it in other function ? In this case , I'm just printing , but python return the first group twice ( see ' word ' = ' a ') . This occur with other function more elaborated too . Why ? There are any solution for this ? I would continue to use DataFrame() + groupby() + apply() + def f() if possible .

You need to avoid for-loops and use the ' apply ' methods . See #URL

I'd like to append to an existing table , using pandas ` df.to_sql() ` function .
I set ` if_exists= ' append '` , but my table has primary keys .
I'd like to do the equivalent of ` insert ignore ` when trying to ` append ` to the existing table , so I would avoid a duplicate entry error .

but when I try to write a function in pandas to apply this to every cell of a column , it either fails because of an attribute error or I get a warning that a value is trying to be set on a copy of a slice from a DataFrame #CODE
how can I apply this code to each element of a Series ?
The problem seems like you are trying to access and alter ` row [ ' text ']` and return the row itself when doing the apply function , when you do ` apply ` on a ` DataFrame ` , it's applying to each Series , so if changed to this should help : #CODE
Alternatively you might use ` lambda ` as below , and directly apply to only ` text ` column : #CODE

normalize column in pandas dataframe based on value in another column
I want to normalize the values in one column of a pandas dataframe based on the value in another column . It's not a pure normalization in a statistical sense . The second value is a type ; I want to sum all the first values for each type , then , in each row , divide the value by the total for the type of that row . An example should make this clearer . #CODE
Then how do I use this to normalize the value in each row ?
And then replace the column with a new one with these values : #CODE

The second array in C ( array ([ 0 , 7 , 2 , 5 , 3 , 6 , 1 , 3 , 3 , 0 , 8 , 5 , 4 , 6 ]) gives you the positions of the values you want to replace in ds1 .
So you have to replace the values in ds1.values.ravel() with the index of the first array of C with the values in ds2 with the index of the second array of C

pandas dataframe join with where restriction
I want to join these two DataFrames based on the ` name ` column , however as you can see the names are not unique and are repeated each year . Hence a restriction for the join is that the ` year ` of ranks should match the first four characters of ` date ` of tourneys .
I am aware of the ` join ` method in pandas , however , I would also need to restrict the join with some sort of ` WHERE ranks.year == tourneys.date [: 4 ]` .
Create a new ` date4 ` for ` df2 ` and then merge ` df1 ` and ` df2 ` #CODE
Now , merge ` df1 ` and ` df2 ` on ` [ ' year ' , ' name ']` and ` [ ' date4 ' , ' name ']` combinations . #CODE

However , if only this code , not append to ` df ` , I can get a correct series . #CODE
You can apply on ` df.groupby ( ' Sym ') [ ' close ']` using ` pd.rolling_max ( x , 2 )` instead #CODE

I think what happens here is that because you pass the data from ` p2 ` and using one of the columns as the index , the index values no longer align so you end up with 0 values . You can get around this by assigning the index after the df creation : #CODE

Hi I am starting to learn pandas to deal with text files . So far I have been using numpy loadtxt but I am having some issues with some text files generated by a very old program ( which I cannot replace ): #CODE

This will give you the standard deviation for each segment in a DataFrame in a fraction of the time . Another advantage is that you can call many other function on ` grouped ` once the grouping is done , such as mean , median etc .

python pandas : drop a df column if condition
I would like to drop a given column from a pandas dataframe IF all the values in the column is " 0% " .

We do it with an apply on axis=1 :: #CODE
Or with pandas loc querying API ( used to be said slower ): : #CODE
Well this works ` df.assign ( ts4= np.where ( df.a * df.b - df.c > 1 , ' XS ' , ' L '))` , the problem with expression ` ts4=lambda x : ' XS ' if x.a * x.b - x.c > 1 else ' L '` is that it wonly works in the apply because you're using ` axis=1 ` so you're comparing single scalar values , your expression isn't so it's not valid

What I am hoping to achieve , is to apply the logic of ` conditions ` to ` indicators ` in order to produce a new dataframe called ` signals ` . To give you an idea of what I'm looking for , see below . This looks only at the first condition in ` conditions ` and the fifth value in ` indicator ` ( because it evaluates to True ): #CODE
Just define a function that fits your needs then apply it . It can be quite complicated as well : #CODE
In the example given , this would translate to #CODE

Just include the column selector in ` loc ` , using ' : ' to indicate all columns : #CODE

Append to a new csv from other csv columns in python
Now , I want to just take the average of Column [ 1 , 3 , 5 , 7 , 9 , 11 , 13 , 15 ] from new.csv and append it to another file or this file . Basically in the new csv , i want to have the average of colum [8 ] form the input files and each of the column [ 0 ] from the input files . So the final output expecting should have a shape of 9 columns and n rows .
Apologize for the lengthy description , but I can't seem to append the columns in python .
This will join the rows and write them to a new csv : #CODE

In the Python ( 2.7.9 ) code below , I am receiving real time stock data via callbacks through TWS API and IBpy . As the data of interest , bid , ask , last price , and last trade size comes in , it goes into a pandas ( 0.16.1 ) data frame . Also , I have added a column to the data frame ' bidVol ' where the last few lines of code place the running sum of trades or volume done at the bid price . Currently I get the total shares exchanged at the bid for a given stock in the ' bidVol ' column . I would like to replace the single ' bidVol ' column with 78 columns , one for each total generated over the five minute intervals of a trading day .
Insert the following code near the top of your code : #CODE
Then replace your ` contracts ... bidVol ` line with the following : #CODE

Very frequently , regressions will drop some observations because they are missing one or more regressor fields . For example : #CODE
Of course , I could drop the correct rows myself before the regression , like this : #CODE
a comment to you your code for dropping na . it's better to keep the response variable y in the same dataset while dropping observations in case there are also missing values in y . When we want to compare models , then it is better to drop missing values for all variables , so we have a common dataset for the estimation of different models , which means that the user has to do it .

As you can see , it eliminate the index but not in the column header , leading to a data shift . Is there a fix , or a workaround ? In my actual data sets , the actual " index " has repetitive elements and spacers , so it's unsuitable to work as an index .

Pandas DataFrame drop row
I need to drop the 2 row ` ( 2013-07-02 29624840 )` ? How can I do that ?
Use ` drop ` method #CODE

Use one more back slash before " \n " ( \\n ) to consider " \n " as a new line . also for ` lt ; ` and ` gt ; ` check if the pd.DataFrame has any other variable where we can mention the data is of html .

This would apply the desired operation to all the rows and is considerably faster
This gives ` AttributeError : ' list ' object has no attribute ' apply '`

join two dataframe together
having two dataframe , I want to join them together ? the result is like this :
just remove the last entry and then join ?

Use ` apply ` and pass your func to it : #CODE
The correct way as EdChum pointed out is to use ` apply ` on the ' location ' column . You could compress that code in one line : #CODE

I want to get line plotting ( x axis : DT ) as the second chart in #URL
You first have to convert the strings to real datetimes : ` df.index = pd.to_datetime ( p2 [ ' DT '])`
Thanks for yours responses ; I just want to plot the values ( FC_ERROR ) for TAG1 and TAG2 with lines on DT ( dates ) axis . It might be trivial for you but not for me ;-(

I then apply the following filter to ` ORD_ticks ` to get ` ORD_prices ` : #CODE
What is the filter trying to do here ? may be ` resample ` could be of use ?
I have a list of Timestamps and want to get the rows immediately prior to those timestamps , so am using ` asof `

You can ` shift ` column-wise by passing ` axis=1 ` , ` loc ` with row-label selection and a column selection will ensure only those rows and columns are modified : #CODE

You have the right idea in your second code snippet , but you're getting an error because ` axs ` is an array of axes , but ` plot ` expects just a single axis . So it should also work to replace ` next ( axs )` in your example with ` ax = axs.next() ` and change the argument of ` plot ` to ` ax=ax ` .

I can easily plot they separately , but because of the index , it will shift the plots according with the index : #CODE
So it's not the desired result . I've seem some answers here , but you have to concat , create a multiindex and plot . If one of the data frames has NaNs or missing values , it can be very cumbersome . Is there a pandas way to do it ?

possible duplicate of [ Parallelize apply after pandas groupby ] ( #URL )
You are asking ` multiprocessing ` ( or other python parallel modules ) to output to a data structure that they don't directly output to . If you use a ` Pool ` , from any of the parallel packages , the best you are going to get a list ( using ` map `) or an iterator ( using ` imap `) . If you use shared memory from ` multiprocessing ` , you might be able to get the result into a memory block that can be accessed via pointer through ` ctypes ` .

How do I drop rows from a Pandas dataframe based on data in multiple columns ?
I know how to delete rows based on simple criteria like in this stack overflow question , however , I need to delete rows using more complex criteria .
My situation : I have rows of data where each row has four columns containing numeric codes . I need to drop all rows that don't have at least one code with a leading digit of less than 5 . I've currently got a function that I can use with dataframe.apply that creates a new column , ' keep ' , and populates it with 1 if it is a row to keep . I then do a second pass using that simple keep column to delete unwanted rows . What I'm looking for is a way to do this in a single pass without having to create a new column .
No it won't see my edit , when you call ` dropna() ` on a series ( which is what we're doing here when calling ` apply ` on a df ) it drops an entry in the series not an entire row

Transpose Pandas Pivot Table
I created a pandas pivot table using ` pd.pivot_table ` . My table has one dependent variable , and then three independent variables ( 1 for rows , 2 for columns ) . I want to export the table so all the values are in one row . However , when I try to ` unstack() ` the table , instead of moving the row variable to the columns , it moves all the columns variables to the rows - leaving all the values in one column instead .
Is there a way to transpose this so when I export it to excel all the values are in one row ?
You can transpose a table with df.T

For the expanding product , there's ` cumprod() ` . For the rolling version , I think you'll have to use ` rolling_apply ` to apply ` prod() ` to each window .

if i just set timezone to utc with tz_localize ( tz =p ytz.utc ) , timings are : 11.4 sec , whereas the dates didn't change at all from standard datetimes .
Datetimes with a naive tz ( e.g. NO timezone ) ` Series ` are efficiently represented with a dtype of ` datetime64 [ ns ]` . Calculations using int64's and so are pretty fast . tz-aware ` Series ` are represented using ` object ` dtype . These calculations are quite a bit slower .

Haha . I find it very funny that a column-oriented DBMS has an API that only supports adding one row at a time . Hope someone with better knowledge than me comes here to say otherwise , but I only know the insert ( row ) method

That worked quite well . That should be a lot more efficient than storing these columns permanently . Do you have advice on how to track management changes ? I imagine it is EmpID | ManagerID | StartDate | EndDate as a separate table and then join it back somehow ? Like if I look at one employee , his performance in January should be under Jake , but on Feb 15th it switches to Bob ?

Create climatology from pandas dataframe ( append the mean by day-of-year )
I would like to calculate the climatology for this dataframe i.e. find the mean of all values corresponding to Jan 1st ( for the years 1950 to 1953 ) , and then append the mean value to the dataframe for the time period from Jan 1st 1954 to Dec 31st 1960 . How do I do that ?
thanks smci , I am computing the mean by day-of-year , the question is how to append it to the existing dataframe for a new set of year ( 1954 to 1960 )
You could use ` resample ` function over a year period ` AS ` #CODE
And , then append to ` cum_data ` #CODE
thanks , however I want to append this to the original dataframe , and also update the time period from 1954 to 1960
Replace line 12 with ` tmp = tmp.reindex ( pd.date_range ( ' 1954-01-01 ' , ' 1960-12-31 ' , freq= ' D ')) .ffill() `

On a leap year , I insert the entry defined by ` fill_leap ` at the 60th day ( 31 days in January + 29 days in February on a leap year ) #CODE

My end-goal is to have ` df3 ` look like the below table . The key thing to note is that it ` merges ` ` df1 ` and ` df2 ` entire rows where ` [ ' Header 1 ' , ' Header 2 ' , ' Normalized ']` are equal . I've tried the ` merge ` suggestions . It looks exactly like what I need but I see column headers with suffixes ` _x ` , ` _y ` appended . How do I output the below in one swoop ? Do I have to change header labels to match that of the original table and drop a few columns ? Or is there a better approach ? #CODE
This is a perfect example of where you'd want to use ` pandas.merge ` ( basically the Pandas equivalent on SQL JOIN , but where columns equality is the only join condition allowed ): #CODE
I think you want a ` left ` style ` merge ` : #CODE
I added ** Edit 1 ** to my question above . I see that ` merge ` has a ` suffixes =( ' _x ' , ' _y ')` which is applied to overlapping columns . What is the best way to format this ` DataFrame ` so all columns here match the same columns in the original DataFrames so I can then concatenate ` df1 ` with this new one ` df3 ` ? Do I just need to drop columns I don't want ( suffixes with ` _x `) and change the headings on the ones I do want or is there a better way ?
You can either ` drop ` , ` rename ` or just not select them in the merge . The reason they end up in the merge is because the values clash on lhs and rhs so it generates a column for both sides to preserve them
I cannot take credit for answering this . All credit goes to @USER and @USER for steering me in the right direction with ` merge ` . I don't know how efficient this is but posting it just in case anyone stumbles across this similar problem . #CODE

And , then merge on ` dd ` and fill ` NaN's ` with ` 0 ` #CODE

After that you can drop Day , Month , and Year from your dataframe .

Hello . Great : adopted to the " real " dataframe where I have to apply this loop , it worked out perfectly ! To groupby w.r.t the first groupby object in that way is something I should have figured out myself . Apologize for my ignorance ;-)

I normally use ` apply ` for this kind of thing ; it's basically the DataFrame version of map ( the axis parameter lets you decide whether to apply your function to rows or columns ): #CODE
To do that , you can use ` apply ` with ` axis=1 ` . However , instead of being called with three separate arguments ( one for each column ) your specified function will then be called with a single argument for each row , and that argument will be a Series containing the data for that row . You can either account for this in your function : #CODE
Note the double brackets . ( This doesn't really have anything to do with ` apply ` ; indexing with a list is the normal way to access multiple columns from a DataFrame . )
However , it's important to note that in many cases you don't need to use ` apply ` , because you can just use vectorized operations on the columns themselves . The ` combine ` function above can simply be called with the DataFrame columns themselves as the arguments : #CODE
As above , there are two basic ways to do this : a general but non-vectorized way using ` apply ` , and a faster vectorized way . Suppose you have a DataFrame like this : #CODE
You can define a function that returns a Series for each value , and then ` apply ` it to the column : #CODE

I know how to fix the problem , but I can't find what line number it is ! Is there a way to back out the line number ( apart from brute force methods like debug-stepping or putting in multiple prints ) ? The only output I get is the below , which doesn't go up the stack to my code : #CODE
I'd say you use your IDE's debugger , set a breakpoint at the given line and look at the stack trace . If you don't have an IDE , read about Python's internal debugger at #URL
Thanks Klaus - your tip did it . Placing a breakpoint at line 1182 of pandas\core\ generic.py ( where the warning was generated from , which I found by full-text searching the pandas / core directory ) , and then moving up the stack got me to the line number I need .

pandas dataframe replace blanks with NaN
I have a dataframe with empty cells and would like to replace these empty cells with NaN .
This code does not work when the cell is empty . Has anyone a suggestion for a panda code to replace empty cells .
I think the easiest thing here is to do the replace twice : #CODE

Tried doing that and I get an error saying ` Join on level between two MultiIndex objects is ambiguous `
You have to get rid of the ` Inside__Outside ` column from your index , since you are not using it to join the two tables .
You can stack them one on top of the other , group on ` Week_No ` and sum on ` item_Number count ` : #CODE
Just append them together and group them by the first level - #CODE

Trying to divide a column in one data frame with daily time index over many years by a column of a second data frame which is indexed by day of year . For example , make a dataframe indexed daily . Make a second dataframe with the median values by date . #CODE
I would like to divide df1 by df_med so that I generate a dataframe of data normalized by the median value for that day of year . #CODE

Maybe , I should not use " map " and " format " ?
So , I still cannot find a way for dataframe.to_string to output formatted strings . Most interestingly , the " map format " DOES in fact change the length of the strings ( and the spacing ) , but the " justify= ' right ' " did not work on them .
data = ascii.read ( table ) ( and apply the formatters )

I guess we could define ` def idmap ( l ): u = np.unique ( l ) im = { u [ x ]: x for x in range ( u.shape [ 0 ] ) } return map ( lambda x : im [ x ] , l )` to save three lines .
Id assume you want to keep the mapping dictionaries around for later use , which you would loose with function calls .. But that depends on the overall purpose and extent of the code . You can use use ``` enumerate ``` to eliminate the need to have the ``` u_rows ``` and ``` u_cols ``` variables . Also , you can apply the mappings directly in the argument of ``` coo_matrix ``` to save space , but that is a bit messy .

Resample doesn't recognize my index as datetime
Now I try to resample by week and plot the mean : #CODE

Can you post what structure the additional data looks like , essentially you can construct a df from the date data and concat them or add them to this df
use the insert method if you care about column order #URL

this generates the same " AttributeError : ' Series ' object has no attribute ' dt ' "
If you're running a recent-ish version of pandas then you can use the datetime attribute ` dt ` to access the datetime components : #CODE

Then I will append it . #CODE
How can I add the date_name to the file / dataframe ? 1 ) Read the file , 2 ) Add the date column based on the file name , 3 ) Read the next file , 4 ) Add the date column , 5 ) Append , 6 ) Repeat for all files in the path
This seems like a reasonable way to do it . The ` pd.concat ` takes a list of pandas objects and concatenates them . ` append ` adds each ` frame ` to the list as you loop through the files . I see two things to change though .

I would like to get whatever values are common from df1 and df2 and the dt value of df2 must be greater than df1's dt value
One way would be to merge on ' value ' column so this will produce only matching rows , you can then filter the merged df using the ' dt_x ' , ' dt_y ' columns : #CODE

The series produced by ` ( df [ " a "] | df [ " b "])` is of type ` bool ` . This surprised me because ` | ` is a bitwise operator , so I expected the series to be of type ` int ` . Thus , I have to do the ` apply ( lambda ... )` to get the desired ` int ` column .
Ah , so ` __or__() ` is to ` | ` just as ` __eq__() ` is to ` == ` , and ` Series ` implements ` __or__() ` as returning a ` bool ` , correct ? This would explain it . I guess there is no standard object method that is triggered by the ` or ` operator , correct ?
Got it . So the decision for ` Series.__or__() ` to return a ` bool ` instead of an ` int ` was motivated by the fact that there had to be some kind of boolean comparison of ` Series ` , but with no ` object ` method for ` or ` , the ` __or__() ` method and ` | ` operator were the only way to go ? And this explains why you can't ` or ` two ` Series ` instances ? Sorry for all the questions , just trying to understand this .

I would suggest to create a new excel sheet and merge it with the old one by ` ID ` on your choice them using #URL

You can use ` pivot ` #CODE
Cool I didn't know about pivot either ...

I'm sorry but you need to fix this quite a bit before anyone can help you . For example , ` for eachP in perf_array : df_res.loc [ len ( df_res ) +1 ]= [ strategy , ticker , strat , stratROI ]` You don't even use ` eachP ` in the loop , and aside from that , I can sort of guess what you are trying to do there , but can guarantee that the code is not doing what you want it to .
Q1 on line 5 I am using following vars : strategy ( opbtained from the iteration itself , so starting with strategy0 ending on strategy3 , ticker ( same thing from where it is in the iteration from 560 diff company tickers , Return-On-Investment ( ROI ) is the interesting part that where the actual performance of the prior 2 vars ( strategy and ticker combined ) ROI is a percentage .

I have been trying with a function and apply . here is my data set and code #CODE

The easiest method that you can use is to get the year's data with partial string indexing , then ` resample ` annually #CODE
The string slicing is described in the pandas documentation under 17.4.1 DatetimeIndex Partial String Indexing . With this method , you can cut out the creation of the timedelta , the second date_range , and the complex and erroneous slicing . The resample method is standard , using ' A ' as a signifier for " annual " frequency and how= ' first ' to just grab the first matching item . #CODE

From what I read , it seems that HDF-stores should be able to save the data in the required format AND allow me to append rows , so I can convert the strings to GPS-entries bit by bit , thereby not getting problems with my memory limitations .

Replace data from one pandas dataframe to another
The problem with this setup is that the DataFrames won't align on columns D & E .
` update ` will align on the indices of both DataFrames : #CODE
that solves one part of the problem definitely alexander , which is replace values in df1 with values in df2 for the same indexes . However we do not have a track of indexes in df2 in the above case -- 4 which is not present in df1 . The final result should contain that as well
See pandas DataFrame concat / update ( upsert ) ? for the discussion .

An alternative would be to apply strip to the columns to ensure they don't have leading spaces : #CODE
That worked , but the odd thing is that the space does not show up on Jupyter ( visually ) but did when I cut and pasted it to this question . Thank you very much

I'm not sure why ` pd.ols ` is so picky there ( it does appear to me that you followed the example correctly ) . I suspect this is due to changes in how pandas handles or stores datetime indexes but am too lazy to explore this further . Anyway , since your datetime variable differs only in the hour , you could just extract the hour with a ` dt ` accessor : #CODE

Then apply ` maxminbid ` function on ` Auction_id ` grouped objects #CODE
First , set the index equal to the ` Auction_id ` . Then you can use ` loc ` to select the appropriate values for each ` Auction_id ` and use max on their values . Finally , reset your index to return to your initial state . #CODE

My main goal is to match the index value from ` ds2 ` into ` ds1 ` and replace it with corresponding value , so the output would look like #CODE
Now , we'll just use ` ds2 ` to directly map all the elements in ` ds1 ` : #CODE

Honestly , you do not provide enough information . You really need to inspect ` dt ` ( the result of ` read_csv() `) for different pieces of test data . So ** " Can something go wrong while reading the csv ? " ** . Sure , and you have to find out if this is the case !
I did not go through your code in all detail , because it smells . Almost never you need indices in Python , so ` for i in range ( 1 , 4 8) ` as well as the ` for n in range ( len ( transactions ))` make me believe that you first should write your logic with more idiomatic Python . Maybe then the issue is already gone .

Using the traditional SQL " insert into " involves hitting the database millions of times which doesn't seem right .

Now , I wish to search for and extract all matching keywords from each ' post_message ' , in each row of the ' large ' csv file , and append all the matched keywords in a new column ' keywords ' .
I would cut down the number of rows to search by running grep for " Pancrea " first : #CODE

Filling blanks after merge in MultiIndex timeseries
I have tried different groupby / resample expressions and failed . The only thing left is to disassemble everything into Series and perform sampling on them . There has to be a better way
Instead of ` ffill() ` , you could also , interpolate the values , using ` pd.Series.interpolate ` #CODE

Be sure to ` append ` to the lists instead .

So this first constructs a view of your df where the status is ' Infected ' , we then set the index to the address , this creates a lookup table where we can then lookup the address using ` map ` in the ` infected ` index and return the status .
I use ` loc ` here to only select the addresses that are in the infected index , to leave the other rows untouched .

I'd like to use a ` DataFrame ` with a ` DatetimeIndex ` to align data from different sets ( the columns ) on their timestamps . Each data set may have overlapping and non-overlapping timestamps . As a simple example #CODE
Is there a quick way of determining if any ` DataFrame ` columns contain a ` nan ` ( essentially a set intersection ) to determine which are the timestamps or rows that have valid data ( non- ` nan `) in all columns . I can think of the following as a possible solution : #CODE
There's a somewhat more native approach combining ` notnull ` and ` all ` : #CODE

The same is true for the other indexers - ` loc ` , ` ix ` , ` at ` and ` iat ` . These indexers are installed using ` setattr ` in class method pandas.core.generic.NDFrame._create_indexer .

And I apply the following group-by code #CODE

I would like to replace row values in pandas .
Here , I would like to replace the rows with the values ` [ 101 , 3 ]` with ` [ 200 , 10 ]` and the result should therefore be : #CODE
In a more general case I would like to replace multiple rows .
Therefore the old and new row values are represented by nx2 sized matrices ( n is number of row values to replace ) . In example : #CODE
This code instead , allows you to iterate over each row , check its content and replace it with what you want . #CODE
I found this solution better , since when dealing with large amount of data to replace , the time is shorter than by EdChum's solution .

One way is to stack the result : #CODE
The default setting will drop the null values . This will give you a second level in your index which may not want . Just drop it if not . Summing along axis=1 will only work if there is one ` True ` in each row which may not always be the case .
Oh , that's interesting you can do it with stack . Seems fine although it's not obvious to me that it is any better or worse than ` a [ b ] .sum ( axis=1 )` as suggested above .
Yeah I just thought it might be helpful in the slightly different situation where there isn't exactly one ``` True ``` in each row . Also , I think the problem is essentially turning multiple columns into one , which I think naturally falls into the realm of pivot and stack etc . Summing across null values is somehow less intuitive to me . Also , it may be of interest to retain the information of which column the non-null value in each row came from . On the other hand , it would seem that stack is around 2-3 times slower

This is on a Windows 7 Enterprise Service Pack 1 machine and it seems to apply to every CSV file I create . In this particular case the binary from location 55 is 00101001 and location 54 is 01110011 , if that matters .

How to apply group by on data frame with neglecting NaN values in Pandas ?
What you really want to do is to pivot the table so that the values in ` station ` columns become column headers . Try this : #CODE

The trouble is that the implicit ` name ` attribute of the on-the-fly calculated ` Series ` inside of your calls to ` join ` will also be `' Temp '` since it's a derived calculation from that column . Since `' Temp '` already exists in the DataFrame , it's raising an Exception to indicate that it doesn't know what kind of name change ( via suffix ) you want to use to prevent name overlap .
You can provide an ` rsuffix ` argument to ` join ` which will append a given string to the name , for the column coming from the right operand of the join ( in this case , the one inside the function call ) . For example : #CODE
However , note that you are always omitting any ` on ` arguments ( join criteria ) in your use of ` join ` -- meaning you default to " joining " by the index . Really , what you want is to simply write into existence a new column derived from an old column , which gives you the opportunity to declare the name , such as : #CODE
This is preferable because it much more clearly expresses your intention , which is not to join but to create a column . Further , since the default join method is `' left '` , if you happened to have duplicate indices , you could potentially end up joining multiple times for each duplicate in the left-hand index , and because that index is the same as the right-hand index ( and would therefore also have duplicates ) it could mean you would silently and erroneously introduce more duplicates with each join .
You may also choose to use ` map ` instead of ` apply ` since upon accessing a column , you'll be working with a Series object .
works great thanks , but why would I use applymap and apply the function on all the df cells , there is different types of data and all other stuff ?
I expanded a bit in the middle to highlight why simply creating a new column of data makes more sense here than using ` join ` . On the other part , note that I'm not suggesting to use ` applymap ` which is a DataFrame method , rather to use plain ` map ` which is a Series method . ` Series.map ` is meant specifically for element-wise operations , whereas ` apply ` has some extra checking for functions that can vectorially operate on the whole ` .values ` data object in one go .

I'm using odo from the blaze project to merge multiple pandas hdfstore tables following the suggestion in this question : Concatenate two big pandas.HDFStore HDF5 files

I am looking for a way to stack ` results [ 1 ]` , ` results [ 2 ]` etc . on top of each other to yield a DataFrame like this : #CODE

You can use any function within the ` apply ` . More examples here - #URL

I'd profile that code but it's doing a lot of melt and merge operations , can you post your starting and desired df result , you can probably achieve this using a combination of multi-indexing / pivoting / stacking

Another solution if you like map and list #CODE

One clunky method would be to temporarily add the ` abs ` value of column b , then ` sort ` using that column and then ` drop ` it : #CODE
An alternative would be to take a view of the ` abs ` values of ' b ' , call ` sort ` on it and then call ` reindex ` passing the index of the series : #CODE
Thanks to the master @USER for this unknown method ( to me anyways ) , you can call ` order ` on result of ` abs ` which results in cleaner code : #CODE

Map vs applymap when passing a dictionary
I thought I understood map vs applymap pretty well , but am having a problem ( see here for additional background , if interested ) .
That works as expected -- both operate on an elementwise basis , map on a series , applymap on a dataframe ( explained very well here btw ) .
If I use a dictionary rather than a lambda , map still works fine : #CODE
So , my question is why don't map and applymap work in an analogous manner here ? Is it a bug with applymap , or am I doing something wrong ?
df.applymap() dont apply .map() on each Series of the DataFrame , put map .apply() on each Series . See Series .apply() here : [ link ] ( #URL )
Sorry , I really don't understand what you are saying here . I guess that applymap and map are not equivalent , which I don't dispute , but I don't have any better understanding as to the why or how . To quote from the link above ( to a very popular SO answer ): " applymap works element-wise on a DataFrame , and map works element-wise on a Series . " I am hoping for some elaboration on that point .
Doing map ( dct , df [ 0 ]) will give you the same error as df.applymap ( dct ) and df [ 0 ] .apply ( dct ) will also give the same error .
Thanks ! The example showing how map produces a series and apply produces a dataframe also explains some results I'd gotten in the past and not understood . My understanding of all this is still a bit less than 100% , but this helps .

You could just assign / merge this result back to your df , so using the above code you could do ` df1.merge ( total.reset_index() , on= ' full_code ')` I think should work

Pandas : how to merge two dataframes on offset dates ?
I'd like to merge two dataframes , df1 df2 , based on whether rows of df2 fall within a 3-6 month date range after rows of df1 . For example :
I am trying to apply this related topic [ Merge pandas DataFrames based on irregular time intervals ] by adding start_time and end_time columns to df1 denoting 3 months ( start_time ) to 6 months ( end_time ) after DATADATE , then using np.searchsorted() , but this case is a bit trickier because I'd like to merge on a company-by-company basis .

There's some missing data in the time series ( also consecutive ) , plus it's not equally spaced . What I would like to do , is to use Pandas to interpolate the missing values the best I can , and then re-index the time series with 5-minutes intervals .

Self join DF on c , d
Apply condition of opposite values ...

How can I group by a specific column but then append the values of one column of that group to a new column .
The above groups on ' type ' and then we call ` transform ` on ' string ' column and call a lambda function that ` join `' s the string values .

If you want a solution which can include specific phrases ( which you know before hand ) in your count , you could replace the spaces in the the phrases with another character ( say " _ ") . For example : #CODE

Pandas merge dataframes based on closest match
Right now , I am doing a simple merge but that does not do what I want
To join the two DataFrames on ` N0_DWOC ` you could use : #CODE
I didn't include it in the above example , but if it were there , the ` join ` would merge ` df_b [ ' N0_DWOC ']` as a new column called ` N0_DWOC_b ` . If that is not what you want , please update your Q to include the desired output .

You can use the groupby function using the ' Id ' column and then use the resample function with how= ' sum ' .

and I have a map which renames certain columns as such #CODE
( occasionally ` name_map ` might map a column to itself , e.g. ' one ' -> ' one ') . What I want in the end is the object #CODE
@USER , I actually want to map ' one ' to ' one ' , as the map is generated by another function , and this is a possible result of that function . I suppose I can drop items that are mapped to themselves ... and then run your code below
Well , the problem here is identifying the columns uniquely and finding the " potential duplicates " . If you can do that ( and you don't have two columns with the same name ) , you can do drop and rename easily .
I think the easiest way would be to drop the columns which are not present in the ` name_map ` values list ( since you want to remove the first ` two ` column ) #CODE

Pandas : assigning values to a dataframe column based on pivot table values using an if statement
I'm using the Titanic Kaggle data as a means to explore Pandas . I'm trying to figure out how to use an if statement inside of .ix [ ] ( or otherwise ) I have a pivot table I'm using to get a lookup value into my main dataframe . Here's a chunk of the pivot table ( entitled ' data ') : #CODE
However when trying to insert the dynamic portion to include an if statement , things don't work out so great : #CODE
I'm actually trying to assign values to a new column from values calculated in a pivot table . ie . if row 1 under column ' foo ' == ' thing1 ' AND column ' bar ' == ' thing2 ' then I want to get the calculated value held in the pivot table for where those two meet . In my example this would be 97.67 going to any row in the main dataframe where Sex == ' female ' , Embarked == ' C ' , and ' Pclass ' == 1 -- Thanks for the attempt !
So anyone who wants to assign values in the column of one dataframe based on values from another . I used .ix [ ] to drill down to the value , then .apply() to apply a function across each row ( axis=1 ) finding the line's values just as you would a dataframe . ( ' line.element ' / line [ ' element '])

So far we have a DataFrame with a row for each time-name pair and a column containing ` True ` , 1,098,000 rows in total . Now all that needs to be done is pivot the table and fill the null values with ` False ` . #CODE
It was indeed the nan causing the problem . The dataframe has only unqiue date-value pairs , hence we can drop . ` .drop_duplicates() ` in the first line . Unfortunately , the perfomance falls short of the benchmark by a factor of approx . 3 .

Based upon the answers , ` df.apply ( lambda x : len ( x.unique() ))` is the fastest . #CODE
You could do a transpose of the df and then using ` apply ` call ` nunique ` row-wise : #CODE
As pointed out by @USER the transpose is unnecessary : #CODE
Similarly I think ` df.apply ( pd.Series.nunique )` would also work ( and avoid the need to transpose if that's an issue ) .
@USER yes that is better actually , initially I thought that the transpose was necessary to get the columns as the index values

However when I try append ( with ignore index ) I get #CODE
You can create a list of the cols , and call ` squeeze ` to anonymise the data so it doesn't try to align on columns , and then call ` concat ` on this list , passing ` ignore_index=True ` creates a new index , otherwise you'll get the month names as index values repeated : #CODE
Another way to do this is to ` unstack ` the ` DataFrame ` . Then reset the index to the default integer index with ` reset_index ( drop=True )` : #CODE

You ` groupby ` first on the feature , and second on the Iteration variable . On each group you apply the ` mean() ` function , and you get the group whose index is ` 1 ` , which correspond to the ` Feature Active == 1 ` group .

I don't know the SQL method but in pandas you want to ` pivot ` : #CODE
If you also add ` bookmaker ` to the index and then ` unstack ` it : #CODE

Say I want to combine df1 , df2 and df3 and merge on index and column ' id ' : #CODE
but I struggle with the merge function .
Since the result is being aligned on both the date and the ` id ` , it's natural to set ` id ` as an index . Then if we stack the DataFrame we get this Series : #CODE
Now if we turn around and unstack the Series , the values are aligned based on the remaining index -- the date and the ` id ` : #CODE
that there are no duplicate ` ( date , id )` entries . If there are duplicate entries , then its not clear what the desired output should be . One way to address the issue would be to group by the ` date ` and ` id ` and aggregate the values . Another option would be to pick one value and drop the others .

So a span of 60 obviously wouldn't apply here , as Pandas would just interpret that as every 60 datapoints rather than every 60 seconds . Are there any solutions beyond the obvious ? The " obvious " being inserting datapoints for every second in the gaps , and extrapolating the values . I should note that the Date column is a proper Python datetime64 object .
Figured it out . @USER recommended the resample method in Pandas , and that is what I was looking for . #CODE
You could add an ordinal variable for which group-of-60-seconds each row belongs to , then group by that ordinal value and apply the averaging function across the entire group , on a group-by-group basis . An easy way to do this would be to convert the datetime value to a number of seconds since the minimum datetime value , then do integer division by 60 .
I think you want [ ` resample `] ( #URL )
@USER yes , resample is it , thank you !
@USER , I'd like to fill forward . Trying to figure out how to do that with resample .

Pandas : Get grouping-index in apply function
What are you actually trying to do with " mytest " , it looks like what you're looking for is actually a df.groupby and then an agg which can view the index , or a df.resample and then an apply which can also reference the index

You want to [ ` resample `] ( #URL ) there is a [ section ] ( #URL ) on this
Thank you so much @USER : resample is what I need !

Try using ` resample ` : #CODE
You could ` melt ` ( ' unpivot ') the DataFrame to convert it from wide form to long form , remove the null values , then aggregate via ` groupby ` . #CODE

I can manually cut and copy paste these two portions in separate files and read in , but I want to automate it using regex . I think I know how we can regex it , but while reading the whole file as a text , I am seeing the following values .

Pandas dataframe insert rows
I want to insert rows in DF and modify its related values :
The code can only append rows but how to modify its values in a faster way ?

Above , I want to replace all of the ` True ` entries in the mask with the value ` 30 ` .
@USER yes this does seem ambiguous , I read his question as a way to replace ` NaN ` values but it reads more like your answer but the OP accepted my answer so it's unclear to me whether he wanted this result but asked for yours

How can I strip off the hours , minutes and seconds so I am only left with 2014-05-12 ?
I'm getting an error that says my series does not have a ` dt ` attribute
@USER it will only have the ` .dt ` attribute * after * you create it with ` pd.to_datetime ` , only datetime columns have the dt attribute

The Pandas merge is like a SQL INNER JOIN , where by default it will join on any common columns ; in this case the column ' bidder ' : #CODE
Merge DF_B_2 with DF_A using bidder column and how=inner . This will filter out bidders not in DF_A #CODE

- yields either one row by 3 columns ( the way above ) or if I transpose it is 3 rows and 1 column

When you need to append some new user just make a new list of all user details and append it
You could do ` pd_users.T ` to transpose , if you wanted to , and would then see ( via ` info() ` or ` dtypes `) that everything is then stored as a general purpose object because the column contains both strings and numbers .
It's pretty common ( and generally pretty fast ) to change indexes , then change them back , transpose , subset , etc . when working with pandas so it's usually not necessary to think too much about the structure at the beginning . Just change it as you need to on the fly .
I believe @USER is only referring to the case * before * converting to pandas . You already proved they are the same yourself with ` pd_users == pd_users2 ` . But you could do ` pd_users == pd_users2.T ` ( put a transpose on either one ) to further verify . It will raise an exception because the the two dataframes no longer conform . Aside from checking for equality , just printing the dataframe shows how pandas is structuring data in terms of rows and columns .

You can drop ` lower ` column if you want .

print ( len ( upregulated ) , end= ' \n ')

I want to apply the mapping to df1 . The working version I have is this one , but I feel there is potential for improvement , as I find my solution unreadable and I am unsure about how it would generalize to multiindexes #CODE
I could also convert df2 to a dictionary and use the replace method , but it does not convince me either .
There is a ` map ` function for this , which accepts a dict or series , in the latter it uses the index to perform the lookup : #CODE
On your test data ` map ` is nearly 2x faster I would expect this to be true for large data also as it's cython optimised and doesn't need to do as much checking as ` merge ` .
for performing vlookup style assigning you should use ` map ` , it's designed for this , the only caveat is that the index of the series must be unique which is not an issue when you pass a dict . Also it is fast as you can see

If you know the name of the column you want to drop : #CODE
and if you have several columns you want to drop : #CODE
You need to pass ` axis=1 ` for ` drop ` to work : #CODE

How can I both interpolate and regularize the frequency of the observations ? Thank you all for the help .
Then resample the data to have a 5 minute frequency . #CODE
Note that , by default , if two measurements fall within the same 5 minute period , ` resample ` averages the values together .
Finally , you could linearly interpolate the time series according to the time : #CODE
might need to resample at a shorter frequency so cubic or spline interpolation
Thank you it works perfectly ! There's a way I can instead first add the regular 5 minutes timestamps to the series with nan as values , and then interpolate them with an order 3 spline ?
I mean , there would be any difference if instead linearly interpolate the time series with ** resample() ** , we build first a time series with the plain entries in the data , then we add entries of the type
and so on , and finally interpolate them with an order 3 spline ?
The resampling is done * before * and independent of the interpolation . You don't have to interpolate linearly . Just remove the line ` ts.interpolate ( method= ' time ') .plot() ` and the associated ` time ` from ` labels ` . Then the above code interpolates the data with an order-3 spline alone .

.unique() only works for a single column , so I suppose I could concat the columns , or put them in a list / tuple and compare that way , but this seems like something pandas should do in a more native way .

and ` df3 = pd.DataFrame ( range ( len ( ts2 ) , index=ts2 )` .So how do I create a stepwise-dataframe df4 , where for each date T in ts3 the value between [ T , T-1 ) for df4 is df3 [ T ] .

This worked perfectly ! Thanks ! Can the replace method be used to replace anything created by to_html() ?
@USER ` df.to_html() ` returns a ` unicode ` object , so you can use ` find ` , ` replace ` , ` strip ` , ` upper ` , ` lower ` , just like you would a string and replace anything you want .

My purpose is to apply two functions at the same time . Basically , I want to cut my dataset for extreme values by looking for the 5% quantile at the lowest part of the dataset and the top % at the other end . #CODE
works but it's not precise because the 2nd function builds on top of the previous cut . So how can I cut both at once ? #CODE

Interpolate ( or extrapolate ) only small gaps in pandas dataframe
I have a pandas DataFrame with time as index ( 1 min Freq ) and several columns worth of data . Sometimes the data contains NaN . If so , I want to interpolate only if the gap is not longer than 5 Minutes . In this case this would be a maximum of 5 consecutive NaNs . The data may look like this ( several test cases , which show the problems ): #CODE
I understand those reasons , of course I nowhere specified that it should not interpolate longer gaps than 5 minutes . I understand that ` interpolate ` only extrapolates forward in time , but I want it to also extrapolate backward in time . Is there any known methods I can use for my problem , without reinventing the wheel ?
What are you expecting then ? If it has only one value , it cannot interpolate values .
@USER Yep , i just added my awareness of `` limit `` . It almost gets the job done , as it stops interpolating after the 5th value . But it should not interpolate at all , if the gap is longer than the limit of 5 .
So here is a mask that ought to solve the problem . Just ` interpolate ` and then apply the mask to reset appropriate values to NaN . Honestly , this was a bit more work than I realized it would be because I had to loop through each column but then groupby didn't quite work without me providing some dummy columns like ' ones ' .
Hey JohnE , very nice idea to interpolate / bfill for all values and simply just use the results at previously defined positions . Runtime is an issue , there are cases with approx ~1000 columns . I will check for the runtime and see if i can create the mask more efficiently with some hard thinking if needed .

Sorry tabs became spaces when I pasted into Stack Overflow . I've added a link to an image of the table .
To place ` ( Blank )` on an equal footing with the other tiles , let us replace it with a single character , such as an underscore ( ` _ `) . #CODE
Now we can form a list comprehension that iterates over the rows using ` df.iterrows() ` , and for each row , iterate over the letters to form a list of tuples . Each tuple consists of the three values : ` ( row [ ' points '] , row [ ' freq '] , letter )` . Passing this list comprehension to ` pd.DataFrame ` yields the desired result : #CODE

DF1 ( my left join ): #CODE
Here is my thought process . Shift the row to columns to see if they match . Then compare the DF2 TimeStart to be > than the initial TimeStart on DF1 but less than the next row TimeStart ( same Name ) . #CODE
Edit - is it possible to do this with a asof command ? The only trick is that the Name has to match , then we look for the closest timestamp for TimeStart on each file / dataframe .

It looks like your data does not have headers and the first left columns of each file should align ( i.e. , if file A has 10 columns and file B has 15 , then the first 10 of each file align with each other ) . In that case : #CODE

but sometimes there is missing data ( example : see [ 2015 , 2 , ' B ']) . it's not too much of a stretch to insert NaN's into the data using reindexing so that i get this : #CODE
my first idea was to convert the data to a pivot table : #CODE
and then do the forward-fill but i cannot figure how to restrict to year ( or how to unpack the pivot table back to it's original form ) .
though if i edit the pivot table create statement to change ` year ` to the columns from the index , i _seem_ to get what i'm looking for but i'm having a great bit of difficulty in unpacking the new table .
You basically need your data in a table with time along your row indices and everything else in columns . You can use a pivot table or stack / unstack : #CODE
Finally , stack and sort your data , and then reset your index if desired : #CODE

One thing the link doesn't really cover is reading a json into pandas as a dictionary , which might be your case . So above where I just print the json you can append to a list . Here you can use pandas DataFrame.from_dict to read in the json ( a python dictionary at this point ) to a pandas dataframe : #CODE

Let's say you have a list of ` datetime ` objects and you want to group them by the ` .month ` attribute . So , First of all you need to sort them , then you can apply ` itertools.groupby() ` function which returns : a group value and an iterator . #CODE

Notice that I have a 2d list as a items in the position ( ' pos ') and velocity ( ' vel ') column . However when I replace the 2d list with a 2d numpy array : #CODE

As @USER suggested and I always agree with Jeff , for this kind of selection ` ix ` is the typical selection method but it's behaviour differs from ` iloc ` in that it does include the end row selection unlike ` iloc ` : #CODE
this is a use case for ix

Now my idea was , to " resample " the data using the index which contains the value for the length .
Data loks like below . Lenght is usesed as index . Of those Dataframes I have a few so that it woulf be really nice to allign them all to the same " framerate " and then cut them e.g. so that I can compare different datasets .
Seems like this would have an answer already , but maybe not . Standard conversions you could do on a datetime would be ` astype ( int )` or use the ` dt ` accessors . But yeah , sample data and code here will help you get a good answer .

`` hist `` -> `` histogram `` ( `` hist `` is pyplot or something ) .
There is a pandas equivalent to this ` cut ` there is a section describing this here . ` cut ` returns the open closed intervals for each value : #CODE

What I want to do is merge the ` count_watched_yeterday ` column from ` df_two ` to ` df_one ` based on the index of each .
Firstly I think you need to pass dfs not a series , ` df_two [ ' count_watched_yeterday ']` is a Series not a df , by using double square brackets your force a df of a single column , secondly you're supposed to pass a list of columns for the ` on ` parameter , not the indices , so this won't work , if you want to merge on index you have to set the params ` left_index=True ` and ` right_index=True `
This works because it will align on the index values , where you have no matching values a ` NaN ` will be assigned as the value

This avoids apply . Link to docs

This is briefly mentioned in the docs on multi-indexing , although obviously that doesn't quite apply in your case , I'm not sure where to go for a good overview of how slicing works for sorted / unsorted indices .

How to apply multiple formats to one column with XlsxWriter
In the below code I apply number formatting to each of the columns in my excel sheet . However , I can't seem to figure out to apply multiple formattings to a specific column , either the centering or the numbering end up being over written . Is it even possible to apply two types of formatting to one column ? #CODE
Is it even possible to apply two types of formatting to one column ?

Simple pandas pivot table issue
What I would like to do is create a pivot table that counts the instances of each ios_id for each video_id and feed_position . It would look like this : #CODE
You almost had it , just don't include ' video_id ' in columns : columns is just for what's going along the top of the pivot table , and index is for what's going down the left . #CODE

I have a DataFrame ` df ` constructed via ` read_csv ` . I want to compute some statistics on a sampled ` sub_df ` . For ` sub_df ` , I want to drop all rows with missing NaNs and re-check the true types of the columns .

Hey , ok i've changed the csv to be as such . But i'm still getting errors .. I really dont understand what could be wrong .. without ` ix [ 1 ]` it displays the whole column / row perfectly . but its just not able to store the names in the variable .. :(
I was reading the documentation and noticed something . ` ix [ 1 ]` would only return what is at entry number 1 .. meaning to say , based on my csv , it will only return entry if value is ` Alice Suh ` if it is anything else . it will return ` error message ` . I tried testing this out by changing the numbers and typing the corresponding names along with it , and it seems to be correct . But unfortunately , it still doesn't solve my problem . lol ... Thoughts ?

How to expand the data based on the single column in python ( transpose ) ?
I then call ` apply ` on that list to turn it into a Series , this will auto generate the names of the columns 0 .. 4 .
You could also use ` unstack ` after resetting the index of each group : #CODE

Within ' loc ' values are permuted . Each value in a ' loc ' is from a different population ( ' pop1 ' , ' pop2 ' , ' pop3 ') . So far I have have been able to form a massive list of tuples that combines every within ' loc ' rearrangement with every between ' loc ' rearrangement . #CODE
My problem is getting the list of tuples into a dataframe . I need to get every combination possible of ' loc ' . E.g. all possible rearrangements of ' locA ' with rearrangements of ' locB ' with rearrangements of ' locC ' .
Hi @USER . Thanks for the comment . At the moment it's not so much computation power - though you raise a good point and I think this could be an issue in the future . My problem is that I need to make calculations using any given permutation of ' loc ' for a particular ' pop ' . My level of coding has prevented me figuring out how to do this on the fly . Hopefully my edit above has made things clearer .

You can use ` shift ` and ` cumsum ` to create ' id's for each contiguous block : #CODE

I create pivot table with MultiIndex levels [ Defect , Own ] . Then i make " Own " Categorical ( see p.s. part of question ) to sort it as [ ??, ??, ????? ] . But when i prepend levels with " Part " , which is also Categorical based on " Defect " level , and sort index with #CODE
P . S . I convert " Own " level to Categorical with the following code : create new column , replace index level with it . Is it ok ? #CODE

` map ( str.lower , index )` , would work for you .
For versions older than ` 0.16.1 ` ( thanks to @USER for pointing that out ) you can call ` map ` and pass the ` str.lower ` function : #CODE
@USER I'm using version ` 0.16.1 ` so it should work if you upgrade , otherwise try the ` map ` method in my updated answer

` from pandas import Panel , DataFrame , Series , read_csv , concat , to_datetime `

You can use ` clip ` : #CODE

You want to [ ` resample `] ( #URL ) so something like ` df.resample ( ' 30min ')` should work
@USER It's not resample . I want an average of all the cases where time is 00:30 , or 01:00 , or 01:30 for different days . Get it ?
Dataframe has a built in method for this called resample just do ... #CODE
Assuming you want the average value for each 30 min . period averaged over the month , try using resample ( in case your data is not already evenly spaced at 30 minute intervals ) , and then use groupby . #CODE

If you make ` samples ` a DataFrame with columns ` user ` and ` item ` , then you can obtain the desired values with an inner join . By default , ` pd.merge ` merges on all columns of ` samples ` and ` df ` shared in common -- in this case , that would be ` user ` and ` item ` . Hence , #CODE

Are there modules to do something like this hourly without human intervention ? I would read all of the files in a directory , append them into a single file , drop duplicates , apply some changes ( add some columns , perform some calculations on timestamps ) , and then save the consolidated new file in another directory .

join two pandas dataframe using a specific column
I am new with pandas and I am trying to join two dataframes based on the equality of one specific column . For example suppose that I have the followings : #CODE

pandas : setting last N rows of multi-index to Nan for speeding up groupby with shift
I am trying to speed up my groupby.apply + shift and
thanks to this previous question and answer : How to speed up Pandas multilevel dataframe shift by group ? I can prove that it does indeed speed things up when you have many groups .
From that question I now have the following code to set the first entry in each multi-index to Nan . And now I can do my shift globally rather than per group . #CODE
Test setup ( for backwards shift ) if you want to try it : #CODE
ie replace each item X in a list with X-N , X-N +1 , ... X-1 .
I ended up doing it using a groupby apply as follows ( and coded to work forwards or backwards ): #CODE
Note that the requirement to do a sort eats into the effectiveness of this , but its still faster than doing the shift per group . Especially if you need to do multiple calculations with different shifts , as the sort only needs to be done once .

I think what you're after is a sequence of pivot tables ? : #CODE
I would use the DataFrame.groupby function , then Transpose the DataFrame . #CODE

Drop that column from ` df `
Drop that SAME column from ` df2 `
You can use ` DataFrame.apply ` with ` axis=0 ` to apply a function to every column of a dataframe . In your case you want to check whether ` all ( col == 1 )` for each column .
Then you can pick out the columns using a list comprehension , and finally use ` DataFrame.drop ` do drop the columns : #CODE
Nice one marius , never thought of using ` loc ` to index columns .
I would suggest using ` all ` on the boolean condition on the entire df rather than use ` apply ` : #CODE
You can then use this to mask the columns to return which column you wish to drop from ` df2 ` as shown above .
Although maxymoo's answer is correct generally one should avoid using ` apply ` if there is a method that is vectorised and can operate on the entire df which this does

I found this Stack Overflow link that asks a similar question but on a ` DataFrame ` object , not a ` DataFrameGroupBy ` object : How to copy pandas DataFrame values down to fill 0s ?
But when I tried to do something like this on my ` DataFrameGroupBy ` object , it threw an error : ` Cannot access callable attribute ' astype ' of ' SeriesGroupBy ' objects , try using the ' apply ' method `
` ffill ` and ` bfill ` replace ` NaN ` values not 0 values , this is by design , you'd have to replace your ` 0 ` values with ` NaN ` values in order to achieve your desired result

Pandas data frame : replace nan with the mean of that row
To replace ` np.nan ` with the mean of the row in which the ` np.nan ` occurred , I used the fillna method as follows : #CODE
The following works , you can calculate the row-wise mean and pass this as the values to replace the ` NaN ` values , you have to transpose the mean so that the alignment is correctly performed : #CODE

@USER I think he needs to unstack the `` industry `` column in order to apply `` rolling_corr `` on the result .
@USER if you apply df.corr() to the current structure , all you get is a correlation of all the columns , eg : correlation between mean and max . That is not what is required . I need to find the correlation between each industry . In other words the industries need to become columns ( along with the existing columns ) , and the rows will be dates . I haven't had any luck doing that .
@USER - you can use ` pivot ` to get the industries as columns and rows as indexes . I used a simplified version of your dataframe . Please see the answer below .
You could do this by performing an appropriate ` unstack ` followed by a regular ` rolling_corr ` .
Start off by setting ` industry ` as the index ( or part of the index ) . ` unstack ` the appropriate index level using the above link as an example . In the resulting dataframe , just use ` rolling_corr ` on the columns of the industry means .
Thanks to both of you . @USER your solution worked and is what I am using . fixxer I can't get your solution to work with my dataframe structure , it only works with the simplified version you've shown above . But don't worry , using unstack is working correctly . Thanks .
@USER The choice of `` unstack `` vs . `` pivot `` depends on the index structure , in this case . I think that fixxxer's answer is a good one too .
Trying the above solution gives me KeyError : ' mean ' . Using unstack ( level=-1 ) followed by the correlation works . Don't worry it's probably just me not using pivot correctly . Thanks to you both .

It seems to do the job but only in part . It filled ` X `' s of the first row ( the duplicated ) with ` inf ` s . I think I can ` df.replace() ` the values but perhaps there is a better way ? Edit : Oddly it did not replace the ` inf ` value at [ ` 1431183600x96x30 ` ` 76921 ` , ` yellowCard `]

The ugly way would be to regexp replace the string , like this : #CODE

Firstly I'd ` merge ` df1 and df2 on the ' Name ' column to add the weight column to df1 .
Then I'd ` groupby ` df1 on name and apply a ` transform ` to calculate the total weight change for each person . ` transform ` returns a Series aligned to the orig df so you can add an aggregated column back to the df .
Then I'd merge this column to df2 and then it's a simple case of adding this total weight change to the existing weight column : #CODE
So the above groups on ' Name ' , applies a ` shift ` to the column and then ` cumsum ` so we add the incremental differences , we have to call ` fillna ` as this will produce ` NaN ` values where we have only a single weight change per Name .

I have tried using append and join , but neither option has worked so far because the one dataframe has an ' industry ' and the other doesn't .
I have managed to merge them correctly using : #CODE
Check out the [ `` merge ``] ( #URL ) command - it's incredibly versatile , and let's you choose which columns to merge on the left and right , index or not ; also what to do with merged columns .
Thanks @USER I have managed to merge them correctly but have run into a problem . Please see my edit of the original question . Thank you .

@USER , I updated my example to explain better how to use the cut / qcut functions .

Pandas - How can I set rules for selecting which duplicates to drop
What I want to do is drop the values that have the same index ( date time ) , but I want to make a rule like :
I have tried using groupby and apply in several different ways but I cant get it to work .
You could use ` del df [ ' dist ']` to drop the dist column when you no longer need it .
Though I was wondering if you could do it immediately using lambda , apply and groupby .
I am sorry I am trying to insert code into comments I cant do it

You can do this with merge . If you want the full Cartesian product , you can do the following : #CODE
I've checked result and it looks like what you want . More info if you search " Brief primer on merge methods ( relational algebra )" in the pandas docs .

This function is mapped on every row of the ( Pandas ? ) dataframe ( actually , only on filtered columns `' humidity '` and `' workingday '`) and the result is stored in `' sticky '` column . That said , you can translate the same expression in R using an anonymous ` function ` and ` apply ` : #CODE
I have to say this is weird way to apply a function to a pandas df , anyway this is an example which shows what it does : #CODE
The lambda expression is calling ` apply ` and passing ` axis=1 ` which means row-wise and test each named column for whether the expression is True or False , the ` ( 0 , 1 )` casts this to an ` int ` , otherwise you'd get a boolean dtype returned . #CODE

with codecs.open ( filename , encoding = encoding_type , errors = ' replace ') as csvfile :

In the Matplotlib example you can try to replace #CODE

is there a more concise way or a different format to keep dataframes in to make them melt friendly for purposes of breaking data into mean / stdev ?

I then apply this #CODE

To make a weighted random choice of the rows , use the answer in this question ; specifically , make a weighted selection of ` range ( len ( df ))` with the weights given by ` df [ Conversion Rate ]` .
To join the resulting dataframe with the second one , use ` merge `
and use ` groupby / apply ` : #CODE

I'm not sure offhand , probably with a merge but there's a couple of complications : ( 1 ) not unique identifiers so you'd have to come up with a way to fake that ( sort first , then save index values ) , ( 2 ) you need a rule for which ones to drop . For example , you need to drop one occurrence of " 2 " , but which one ?
If this solves most of your problem , you might want to move on and ask a new question regarding the remaining part ( and specifying how exactly you want to deal with issue #2 I mentioned in the prior comment ) . I'm pretty sure it can be solved with a merge / join but just not sure exactly how to do it off the top of my head .

the documentation to concat is impenetrable and its hard to find examples of this relatively simple task in the docs

Or pass ` axis=0 ` to ` loc ` : #CODE

@ BrenBarn .... what do you mean by complete example ? I now understand that .dropna will change the size of the DF . How do I drop an entire row from from the DataFrame when encountering .nan in this feature ?

As to why your method failed , you were calling ` apply ` on a Series ( ` df [ ' ID ']` is a Series and not a df ) and there is no ` axis ` param so the following works : #CODE

Ah I see the problem so did you try ` na_values =[ ' NaN ']` or did you use replace ?
Alternatively you can ` replace ` them using : #CODE

Working on the database of matches I would like to retrive the rating of both players and apply two functions ( i already have them defined ) predicted_result ( rating1 , rating2 ) , and updated_rating ( rating1 , rating2 ) . The first one gives me the expected result of the match given the ratings , the second one gives me the updated ratings . Finally I need to record the updated ratings in the player database .

The input sequence to map is a list of dicts .
Have you cared to read ` eval `' s [ documentation ] ( #URL ) at all ? " If the globals dictionary is present and lacks ` __builtins__ ` , the ** current globals are copied into globals before expression is parsed** . This means that expression normally has full access to the standard builtins module and restricted environments are propagated . "
Thanks @USER , I did not see that in the docs while searching . I am a little confused though - is there a way to avoid what is happening ? I mean , I am able to preserve the input if I pass eval copy ( dat ) instead of just dat , but I am not sure if that is the most efficient way to do what I want ( dat is much bigger in practice so I'd like to avoid copying it ) . Thanks !

@USER I edited the code to reflect the indents and also pasted in the new code that should replace that mess .

Thanks for introducing me to ` itertools ` ! For those wanting to get to the finish line with a ` DataFrame ` , I constructed a ` dict ` to include the group ( ` _ ` in @USER ' s ` for ` loop , ` grp ` in @USER ' s approach in the question comments ) , used ` DataFrame.from_dict ` and ` melt ` to get it into the format I wanted , then ` map ` to pull teams out of the tuple .

It looks like it's binding the function object as the column value rather than unpacking it to a dict , what I'm doing above is to return the ` value_counts ` as a list and then call ` apply ` again to unpack the single element list . This forces the dict to be unpacked into a single element list in the initial ` apply ` call : #CODE

I'm trying to do apply simple functions to mostly numeric data in pandas . the data is a set of matrices indexed by time . I wanted to use hierarchical / multilevel indices to represent this and then use a split-apply-combine like operation to group the data , apply an operation , and summarize the result as a dataframe . I'd like the result of these operations to be dataframes and not Series objects .

But that is too clunky to keep having to change on-the-fly , and anyway , it would only replace the ` .head() ` functionality , but not the ` .tail() ` functionality .

Not sure how robust this is , but you can sort ` df2 ` so it's index is decreasing , and use ` asof ` to find the most recent index label matching each key in ` df `' s index : #CODE

I have first created a Pivot Table using pandas . Then I saved it to a worksheet . Then I wrote some data formulae in this worksheet using xlwt for further processing . Now I am trying to read value from a cell to which I had previously written a formula .

Of course , you can't do it quite this easily if you don't trust the input - if there could be script in your data which eval might pick up , etc .

How to produce & join combinations in a pandas dataframe ?
First join on [ ' a ' , ' b ']
For missing cases join on [ ' a '] and pick first value .
So far I've been able to produce combinations and do first join using the following code : #CODE

I figured there might be a way to "` map `" the read data if I first fed the ` pandas DF ` with all column headers , then I map each file against the columns in the main file .
E.g . Once i used ` read_csv() ` , then ` to_csv() ` will look at the main merged file and "` map `" the available fields to the correct columns in the merged file .
Try using the pandas concat [ 1 ] function , which defaults to an outer join ( all columns will be present , and missing values will be NaN ) . For example : #CODE
Here is a complete example that demonstrates how to load the files and merge them using ` concat ` : #CODE

1 ) ` groupby ` on ' id ' and call ` apply ` on the ' vehicle ' column and pass method ` nunique ` , you have to subtract 1 as you are looking for changes rather than just an overall unique count : #CODE
2 ) ` apply ` a lambda that tests whether the current vehicle does not equal the previous vehicle using ` shift ` , this is more semantically correct as this detects changes rather than just the overall unique count , calling ` sum ` on booleans will convert ` True ` and ` False ` to ` 1 ` and ` 0 ` respectively : #CODE
This works . I knew a lambda had to be used somehow , just didn't think of using the shift function . Thank you !

How to concat two DataFrames by comparing overlapping columns in pandas ?
Is there a way to do this using pandas ? I've tried looking into the ` merge `
, ` join ` and ` concat ` methods , but I didn't find any solutions to my question -- the lack of an absolute time in order to compare the two objects seems to be the problem . A short example ( in general , time and voltage are floats , not ints ):
and then using ` concat ` . This is excruciatingly slow ( especially since I cannot ` break ` after one match is found ; there could be others later ) .
This is fraught with danger , you have no method to uniquely determine which values overlap as your time values clash in both df's , you could try to determine if a range of values match and then concat the remaining values ** if ** you were sure that this is correct but you have a small range in df1 that overlaps with df2 and really there isn't a sane method I can think that would not overwrite or lose values in the merge

I could answer this but really you need to try something first , please take a look at the [ IO ] ( #URL ) section of the docs , also perhaps look at ` read_excel ` or ` read_csv ` , in particular I'd look at ` nrows ` and ` skip_rows ` params for loading the data into 3 separate dfs and thenc calling ` pandas.concat ` pass a list of these dfs and pass param ` axis=1 ` . Alternatively load the entire thing into a single df and then slice the df to get the 3 separate dfs and then ` concat `

I can note that ` df.a ` and ` df.b ` are both sequences . So you could use map for obtain your result .
what are " l = [ 1 , 2 , 3 ]" and " ml = map ( set_negative , l ) # ml = [ -1 , -2 , -3 ]" for ?
Are just for ilustrate the use of map function : ` l ` is a list containing the integer values 1 , 2 and 3 and ` ml ` is a list we have obtained as the result of apply the ` set_negative ` lambda to ` l ` . In others words we have applied a function to each element of the sequence ` l ` without using a ` for ` o ` while ` loop .

Merge two files in Python PANDAS ?
I already know how merge 2 files using Python - I am looking forward to achieve this job in ` PANDAS ` particularly .
Why do you want to merge the files in pandas

It seems like pandas either expects apply to return a scalar for each column , or a vector of the same length as the column . Is there a way to return vectors of different length to the original data ?
Another version that works the same as EdChum's answer , but splits within ` apply ` , instead of within ` np.percentile ` : #CODE

Well here is a way of doing it , first load the list into a df , then add the ' occurrence ' column using ` value_counts ` and then ` merge ` this to your orig df : #CODE
Then you join the grouped dataframe with the original one using the pandas.merge function .

Merge pandas dataframes where one value is between two others
I need to merge two pandas dataframes on an identifier and a condition where a date in one dataframe is between two dates in the other dataframe .
I need to merge this with this dataframe B :
In SQL this would be trivial , but the only way I can se how to do this in pandas is to first merge unconditionally on the identifier , and then filter on the date condition : #CODE
Is this really the best way to do this ? It seems that it would be much better if one could filter within the merge so as to avoid having a potentially very large dataframe after the merge but before the filter has completed .
To be honest that would be the way I'd do it , the merge params allow only exact matches on index or columns not using a special criteria like you want to the best of my knowledge
To some extent one should not expect SQL like functionality when it comes to dfs especially as you can filter the dfs before / after the merge , you could add a request as I'm not a dev but they are very responsive though
I think the problem is always about points in time and timespans and supporting operations with these kinds of objects on join would allow for lots of new solutions .
After that you can fill your DataFrame with these objects , and join on the columns they live in .
Important note : I do not subclass datetime because pandas will consider the dtype of the column of datetime objects to be a datetime dtype , and since the timespan is not , pandas silently refuses to merge on them .
` PointInTime ( 2015 , 3 , 3 )` should also have been included in this join on ` Timespan ( datetime ( 2015 , 2 , 1 ) , datetime ( 2015 , 4 , 1 ))`
I tried to make all PointInTime equal to each other , this changed the behaviour of the join to include the 2015-3-3 , but the 2015-2-2 was only included for the Timespan 2015-2-1 -> 2015-2-5 , so this strengthens my above hypothesis .

You can pass param ` axis=1 ` to ` apply ` so that it process each row rather than the entire column : #CODE

In the current implementation apply calls func twice on the first column / row to decide whether it can take a fast or slow code path .
In the current implementation apply calls func twice on the first group to decide whether it can take a fast or slow code path . This can lead to unexpected behavior if func has side-effects , as they will take effect twice for the first group .

Append column [ 0 ] after usecols =[ 1 ] as an iterator for Pandas
What I would like to do is append the contents in ` column1 ` in ` input.csv ` to ` output.csv ` for each row .
In my opinion you should read the entire csv as a df , then ` apply ` your crawl method on column2 and create the new column and then write the df to your output : #CODE

using apply . #CODE

I am working with spatial data in Python Pandas . I have two dataframes that represent two distinct business entities - transactions and locations ( sizes of 10^6 and 420 ) . I am trying to map transaction to specific location by updating transactions dataframe .

apply pandas qcut function to subgroups

I have several data sets that have a date and time field , however unfortunately the instrument that created them did not consistently label dates , such that all of them are in DD / MM / YYYY format , but the instrument seemingly randomly left off the leading zeroes of month and day for about half of the dates . Pandas has had trouble reading them correctly ( from excel files ) , and since the dataset starts April 10th , it keeps starting at 2014-10-04 , has unconverted dates in between ( when the day goes above 12 ) and then starts reading them as YYYY-MM-DD again when that makes sense considering the input date . Is there a way to force Pandas to read these dates correctly , and concatenate the date and time fields and use that as an index , instead of it assigning numbers ? I tried to create and insert a converter function for the Date field to format the dates correctly , but for some reason it applied after Pandas had already read the date incorrectly , and thus formatted incorrectly .
Since I want to index these data as a time series , what I was doing was simply creating a date / time range and then setting that as the index for the DataFrame , which worked fine . Except , for this data set , there are two days of data where the instrument apparently started taking data at a freq of a sample per minute , instead of a sample every 10 minutes . Is there a way to assign an index and force it to only keep matching records ? Failing that , I've been attempting to try and query the DataFrame for only times where the minute ends with 0 , or specifically delete those records , with no success at all . I really have no idea what to do here .

using " value_counts() " is already a sort of histogram . if you remove it , you will be able to use the hist function .
@USER : Basically what Dror Hilman said . A histogram counts the values . ` value_counts ` also counts the values . If you do both , you count the values twice . However , I find that in many cases for discrete data I want ` value_counts ` rather than ` hist ` , because ` hist ` bins the values , whereas ` value_counts ` actually counts each individual distinct value .
even ` firstperiod.megaball.value_counts() .sort_index() .plot ( kind= ' hist ')` produces that same incorrect chart ...
@USER : Because ` hist ` * bins * the values , it does not necessarily display a separate bar for each individual value .
@USER : You could use ` hist ` . It * is * giving you a histogram . It's just giving you a histogram with multiple values grouped into one bar .

Python Pandas DataFrame If Index Contains Any String Values , Apply Label , Else Apply Different Label
If the ' Search term ' ( index ) CONTAINS `' american brewing '` or `' americanbrewing '` , apply the label `' Brand '` , else apply `' Non-brand '` to a column with the header ` Label ` . #CODE
How do I output the ` Label ` column in the ` result ` ` dataframe ` based on if the ` Search term ` ( index ) contains any of several possible string values ? Where ` True ` , apply ` Brand ` , Else , apply ` Non-brand ` to the ` Label ` column .
You could convert the ` index ` to ` Series ` and apply transformations . #CODE

pandas matplotlib .plot ( kind= ' hist ') vs .plot ( kind= ' bar ') issue
That's because the " hist " plot is not just plotting data , but actually first estimating the empirical distribution of the raw data and then plotting the result . That is , " hist " is going to bin the data , count the instances per bin and plot that , so there is no need of doing the ` value_counts() ` ourselves .

For a large dataframe , it's must faster to use ` map ` rather than a for loop : #CODE

No matter which of the given alternatives I apply - it just doesn't work . Do you have any ideas ? #CODE

Pandas set value in dataframe , multithreading - loc vs . set_value

I can't figure out a way to do this in a single loop , the problem here is that you want some kind of rolling apply that can then look at the previous row , the problem here is that the previous row update will not be observable until the ` apply ` finishes so for instance the following works because we in run the apply 3 times . This isn't great IMO : #CODE

how to drop dataframe in pandas ?
But I want to drop the whole dataframe created in pandas .
like in R : rm ( dataframe ) or in SQL : drop table

Hi Many thanks for all the answers , I have tried to apply the df.ix [: , : ' 4 '] .apply ( lambda x : x.isin ( df [ ' 1 '])) .all ( axis=1 ) .sum() to the last 4 columns of my example ( columns 2 , 3 , 4 and 5 ) by using df.ix [: , : ' 5 '] .apply ( lambda x : x.isin ( df [ ' 2 '])) .all ( axis=1 ) .sum() and have checked by hand ( with a dataset that provides me an answer greater than 0 , but keep getting a different result . Have I applied the logic correctly please ?

Pandas duplicate columns with a shift
There are some ways to append columns to a pandas data frame . One way can be df [ " new_column "] = ts , where df is a dataframe , and ts is a series object of some kind ( list , numpy array ... ) .

I have tried pivot / unstack / stack which im sure is the correct way to accomplish this , however I have been unable to get the desired shape .
im still having no luck with this . I am fairly certain pivot / stack / unstack needs to be used but I just can't get it working to create the correct dataframe shape as per the original question ( with each exchange+symbol having a list of ' close ' values underneath )

After loading your df you can select a specific row using label indexing ` loc ` : #CODE

Sorry are you asking how to simply add a index column ? like ` df [ ' new_col '] = np.arange ( len ( df ))` ?

If you want to combine multiple , I suggest you use the map function : #CODE

Replace NaN in a dataframe with random values
I want to replace all the NaN with some random values like . #CODE

In a Python Pandas ` DataFrame ` , I'm trying to apply a specific label to a row if a ' Search terms ' column contains any possible strings from a joined , pipe-delimited list . How can I do conditional if , elif , else statements with Pandas ?

In the case that data do exist , then , is there a cleaner method than : ` df = pd.DataFrame ( data =( data if len ( data ) else None ) , columns =[ ' a ' , ' b ' , ' c '])` to set it ?

My solution was to bring in just the columns needed to find the duplicates I want to drop and make a bitmask based on that information . Then , by knowing the chunksize and which chunk I'm on I reindex the chunk I'm on so that it matches the correct position it represents on the bitmask . Then I just pass it through the bitmask and the duplicate rows are dropped .

I find , however , that its performance does not appear to stack up against pickle . #CODE

pandas map and join multiple columns
I need to pass 5 columns from my pandas df and join the resulting 5 values into the same row . #CODE
I need to pass each df row into that function and the values in the resulting dict and bool to be inserted in the same row ` ( cleaned_BP , cleaned_Pain , cleaned_Pulse , cleaned_RR , cleaned_Temp , BOOL isCleaned )` and run this in all my df .

You could do everything with your map and lambda : #CODE
Another solution would be convert the messages into their CountVectorizer sparse matrix and join this matrix with the feature values from the posts dataframe ( this skips having to construct a dict and produces a sparse matrix similar to what you would get with DictVectorizer ): #CODE

Rather than doing a very long one-liner , I think 4 separate assignments which are masked using ` loc ` would be more readable : #CODE
It would be possible to rewrite the above into a multi-nested ` np.where ` statement but I find that once you get above 3 conditions it becomes difficult to read and there isn't much to gain performance wise against separate ` loc ` statements

I want to consolidate it into : #CODE

It doesn't look like ` str.contains ` supports multiple patterns , so you may just have to apply over the rows : #CODE
Apply a lambda function on the rows and test if A is in B . #CODE
You can call ` unique ` on column ' A ' and then join with ` | ` to create a pattern for matching using ` contains ` : #CODE

Ok I had moved the ` plt.tight_layout() ` to to top just before I posted my answer - didn't realise it would mess the plot up ! check out my edited version where I do ` plt.tight_layout() ` at the end instead ... For your final version , remove all the ` plt.draw() ` , ` raw_input() ` , ` plt.ion() ` and append a ` plt.show() ` at the end . The latter is just to confirm for you that the plot is working correctly ...

I aggregated on pivot tables some time series information , so each pivot table have columns labeled 2015 , 2014 , etc . I want to compare each pivot table , so I'm plotting them on the same axis : #CODE

Creating tz aware pandas timestamp objects from an integer
If you time is in UTC , but you want it in another tz . #CODE

There are many examples of heat maps using pandas on this website . Which ones did you look at and why did they fall short of solving your problem ? Additionally , seaborn is built with pandas in mind , so I'm surprised that its heat map function doesn't suit your needs . Lastly , if the main purpose of this question is to learn how to generate an image of a heat map , I recommend removing the bits about weasyprint since that seems orthogonal to the heat map question .

If / when i substitute in kind= " hist " , the chart values get all out of whack . Any ideas how to get this to work in histogram form ??
I've tried this as well , to no avail ( gives me a blank hist ): #CODE
Both ` Series.plot ` with ` kind= ' hist '` and ` pyplot.hist ` generate a histogram from raw data , but it looks like you're passing in pre-binned data . Instead , I think #CODE

My goal is to be able to do a map on the a column , but with a knowledge of the related index .
If you're looking to perform some kind of timestamp calculation using your index you can call ` apply ` and access the index using the ` name ` attribute : #CODE

Is there a way to apply to a pandas dataframe while threading state ?
My goal is to be ably to do an apply , but one which maintains some state . Now , I know that I can just have a variable ( and be aware that apply is called twice , I believe ) , but I'm wondering if there is a more idiomatic way to do this ?
" maintains some state " <--- what does this mean and how does ` apply ` not currently achieve this ? post some more code showing what you're trying to accomplish ( and example output )

` shift ( 1 )` compares the previous row , next row would be ` shift ( -1 )`
@USER Shift is the function to do this , but you are not comparing along the y axis of the dataframe , you are copying the data into another column and offsetting it one step , so you can compare it per row . The example I added to my answer should explain this better .

MY advice would be to save yourself a ton of pain and install an environment that comes with the numpy / scipy / pandas stack bundled already such as [ anaconda ] ( #URL ) or my personal choice [ winpython ] ( #URL )

How to apply different aggregation functions to same column by using pandas Groupby
@USER I'd say that question is different , the OP here is asking how to apply multiple different functions at once , not to generate multiple columns from a single function

Thanks , that works . I can do drop [ ' X ' , ' Y '] followed by drop_duplicates() to get it to the desired format .

What I got so far is the code below and it works fine and brings the results it should : It fills ` df [ ' c ']` with the calculation ` previous c * b ` if there is no ` c ` given . The problem is that I have to apply this to a bigger data set ` len ( df.index ) = ca . 10.000 ` , so the function I have so far is inappropriate since I would have to write a couple of thousand times : ` df [ ' c '] = df.apply ( func , axis =1 )` . A ` while ` loop is no option in ` pandas ` for this size of dataset . Any ideas ? #CODE
@USER .klein Your edit does not make this question the least bit more searchable . The tags " python " and " pandas " are already on this post , which is how I and everyone else will find it . Also , Stack Overflow already prepends the tag to every page title so Google will handle it better .

Pivot Tables or Group By for Pandas ?
Alternatively , you ` groupby ` on `' Col X ' , ' Col Y '` and ` unstack ` over ` Col Y ` , then fill ` NaNs ` with zeros . #CODE

I'd load the csv and parse the ' DATECHANGE ' column to a datetime , then ` set_index ` to this column , call ` resample ` and pass param `' fill_method=ffill '` to perform a daily resample and then ` reset_index ` . : #CODE
You have to temporarily set the index to the ' DATECHANGE ' column as resample only works with datetime like indices .

You can ` shift ` the name column and then take a slice using ` iloc ` : #CODE

@USER Cunningham : How would I apply this ?

Map the value counts back to the original indexes . You can pass ` take_last=False ` to ` .drop_duplicates() ` if you want the first unique row ( rather than the last ) #CODE
This is great , many thanks Jeff . This gives me the line number and the amount of occurrences . How do I append the original columns to this data so I can see the original data and not just the line number please ?

Assuming that state and zip are always present and contain valid data , one method to solve this problem is to first split your string . The state and zip are simply the second to last and last columns , respectively . I've used a list comprehension to extract them from ` city_state_zip ` . To extract the city , I've used a nested list comprehension together with ` join ` . The last two elements are the state and zip , so the length of the list minus two tells you how many elements are contained in the city name . You then just need to join them with a space . #CODE
This method uses extraction by regex using multiple named groups , one each for City , State and Zip . The result of the extract method is a dataframe with 3 columns as shown . The syntax for groups is to surround the regex for each group by a bracket . For naming a group insert ` ? P group name ` in the brackets before the group regex . This solution assumes city names contain only upper and lower case letters and spaces and stats abbrev . contain exactly 2 capital letters but you can adjust it if this isn't the case . Note that the spaces between the groups in the regex are important here as they represent the spaces between the city , state and zip .

So I want to join the second frame to the first based on ` Key1 ` and ` Key ` but the first DataFrame is larger and I still want to retain the columns that don't match ( with ESD as Sand ) in the new DataFrame . Eventually I will merge anything with an ESD of Sand too .
but this does not retain columns that don't have a matching key . Would perhaps a join instead of a merge be what I need ?
I don't really care if the duplicate column heading carry over to the join , I will just remove them ( there are two columns named Species ) .
it is better to merge on a list of columns : #CODE
since you can merge on ` [ ' ESD ' , ' Species ']` directly , it is better to avoid the

You can use ` df.drop_duplicates() ` to drop duplicate rows .

Then merge these geometries with the original dataframe on matching ids : #CODE

i needed to drop the time and just do a date comparison , so to extract the date i use .date() #CODE

Re-shape pandas dataframe stack / unstack
I've been trying to re-shape this dataframe for the past day . Tinkering with stack / unstack / melt and shifting columns into indicies etc , but have not been able to achieve my goal .
I want to stack all of ` [ ' HOUR1 ' , ' HOUR2 ' , HOUR3 ']` into a single ` column = HOUR ` .
Similarly , I want to stack all of ` [ ' PRICE1 ' , ' PRICE2 ' , ' PRICE3 ']` in a single ` column = PRICE ` , such that the value in this field is aligned with the corresponding value in the ` HOUR ` column . There is a link between ` HOUR1 ` ` PRICE1 ` , ` HOUR2 ` ` PRICE2 ` , ` HOUR3 ` ` PRICE3 ` .
Then , create two DataFrames for prices and hours , stacking the results . Drop the hours and prices levels from these stacked DataFrames . #CODE

This even occurs if you force it to a bool dtype from the getgo : #CODE

I want to add subtotals to my dataframe : groupby some index level then append new dataframe to the main one . For some unknown reason ` temp [ " ???? "] = " ????? "` does nothing . It doesn't add new column , though ` temp [ " ???? 2 "] = " ????? "` adds one . I think , it's because my `' pvt '` dataframe already has `" ???? "` index level , but what impact does it have on a new `' temp '` dataframe ? #CODE
@USER The goal is to append subtotals ( got with groupby-sum operation ) to the main dataframe . So i recreate multiindex to be the same in both dataframes .

pandas does not append to df as it should of each line of iteration

Then merge back to the original dataframe to have your aggregates displayed against each row : #CODE
Thank you , this looks like it should work . Unfortunately im getting an issue when trying to do the rename . singleton_values is of type therefore I cannot use .rename . Rename is only valid for dataframes , not series . The true / false column does not have a column name , so how would I rename it and then merge it back into the original dataframe ? Thanks .

What is the syntax for pivot tables in Pandas ? Docs don't seem to be right ?

First create an empty dataframe with the timestamp index that you want and then do a left merge with your original dataset : #CODE

I'm trying to append new values to a pandas series inside a while loop .

I tried using a pivot table #CODE
But it's giving me KeyError : ' 0 ' which is strange since I put that in for the input for values . Also , I am not sure it will work since each customer's output is only the type they called in . As in , Company 1 only had equipment error calls so it doesn't list them for User Error or Neither calls . Not sure if a pivot table will account for this .
Look into Pivot Tables : #URL
@USER Yes , this is the most logical way . However I have issues with first making it a dataframe : df = pd.DataFrame ( byqualityissue ) df.pivot ( index= ' CompanyName ' , columns= ' QualityIssue ' , values= ' 0 ') next with KeyError : ' 0 ' , and finally I'm not sure if a pivot table will work since each company only which calls it DID get , but doesn't necessarily label a zero for the types it did not get .
Wanted to comment , but am super busy so I can't elaborate much .. [ unstack ] ( #URL ) should help you in this situation .
Read your CSV file in . Index it by both Company and Quality Issue , then unstack it on Quality Issue . Finally , replace the ` Nan ` values that occur because no matching data were found #CODE
I used unstack to reorganize my data , replaced NaN values with a 0 , added up all the rows and appended a new column with those values , then sorted .

But I'm a bit stumped at how to do this pandas . As far as I can tell , aggregate only applies a function on a given grouped column , and I don't know how to get it to apply a function that involves multiple columns .

How to Pivot in Google BigQuery
How can I pivot the results , from within BQ , to produce output as follows : #CODE
As some additional background , I actually have 2000+ categories that I need to pivot , and the quantity of data is such that I can't do it directly through a Pandas DataFrame in Python ( uses all the memory , then slows to a crawl ) . I tried using a relational database , but ran into a column limit , so I'd like to be able to do it directly in BQ , even if I have to build the query itself through python . Any suggestions ?
After trying out the accepted answer below , I found that trying to use it to create a 2k+ column pivot table was causing " Resources exceeded " errors . My BQ team was able to refactor the query to break it into smaller chunks and allow it to go through . The basic structure of the query is as follows : #CODE
The above pattern can be continued indefinitely by adding ` INNER JOIN EACH ` segments one after the other . For my application , BQ was able to handle about 500 columns per chunk .
See the manual for other pivot table example .
Yep , I went ahead and handed it off to our BQ team . They're going to talk to Google about it . To be fair to Google , it's an enormous amount of data and processing . Tried doing the pivot on my laptop with 32GB of ram and it just chokes .

One of the columns , titled store location , holds the address including latitude and longitude . The purpose of the program below is to take the latitude and longitude out of the store location cell and place each in its own cell . When the file is cut down to ~ 1.04 million rows , my program works properly . #CODE
unclear what you're trying with this line ` sales = pd.concat ( df , ignore_index=True )` you're trying to concat a single df with nothing else , there would be no difference in just using ` df ` by itself or assigning ` sales = df ` , also you can't concat a single df , you need to pass an iterable such as a list ` sales = pd.concat ([ df ] , ignore_index=True )` which is pointless as this is a list of a single df
Honestly , I was not really sure how to use that . That line was a guess implementation , just to see what would happen . So what you are saying , is that I have to have multiple lists in order for concat to be used . If so , would I write it out like this : ` sales = pd.concat (( df , dg , dh ) , ignore_index=True ` ?

This operation will only apply to groups that are larger that 2 in the original dataframe .
I would try a slightly differnt tack : Pivot the table so that you have a column for each value in ` col2 ` containing the dates and the values of ` col1 ` as the index . Then you can use the ` .diff ` method to get the differences between consecutive cells . This might not work if there are duplicate ` col1 ` , ` col2 ` pairs though , which is not clear from the question . #CODE

Given a DataFrame with a hierarchical index containing three levels ( experiment , trial , slot ) and a second DataFrame with a hierarchical index containing two levels ( experiment , trial ) , how do I drop all the rows in the first DataFrame whose ( experiment , trial ) are not contained in the second dataframe ?
One way to do it would by using ` merge ` #CODE
Thanks ! This makes sense ( I used a similar approach using the join method , but was thinking that somehow I could get the reindex method to do what I wanted ) .

Pivot the sample values for each group . aggregate by max ( pValue )
Get the corresponding Status , desc corresponding to the sample with the higher pvalue and replace its value with a pValueString .
For pandas pivot table , you pass the rows you want as ` index ` and the columns you want as ` colums ` : #CODE

How do I calculate the standard deviation with a pivot table in Pandas ?
I have a bunch of data involving certain numbers for certain players of specific sports . I want to use pivot tables in Pandas to have it split up the data by sport , and for the corresponding value for each sport have the mean " number " value for all people who play that sport . ( So if it were basketball , it would average the number of all the players who play basketball , and the number basically represents a preference . )
I can do this pretty easily with pivot tables , but if I wanted to do the same thing for calculating the standard deviation , I cannot figure out how . I can do ` np.mean ` for the mean , but there's no ` np.std ` . I know there's ` std() ` but I'm unsure how I'd use it in this context .
Are pivot tables not advisable for doing this task ? How should I find the standard deviation for the numeric data of all players of a specific sport ?

I tried an alternate solution which involved ` apply ` ing a ` lambda ` function to each element of the ` Series ` but that took longer .

Merge two daily series into one hour series
I have two daily series and I have to merge them into one hour series with the 1st series for the first 12 hours and the 2nd series for the remaining hours .

How do I create a pivot table in Pandas where one column is the mean of some values , and the other column is the sum of others ?
Basically , how would I create a pivot table that consolidates data , where one of the columns of data it represents is calculated , say , by ` likelihood percentage ` ( 0.0 - 1.0 ) by taking the mean , and another is calculated by ` number ordered ` which sums all of them ?

What is the most efficient way to do this in Pandas ? In STATA in which I mostly program , I would either use replace and if ( tedious ) or loop if I have many categories . I want to break out of STATA thinking when using Pandas but sometimes my imagination is limited .
See the docs for ` pandas.cut() ` . Btw , you could do the same thing in stata with ` egen ` and ` cut ` .
Glad that ` cut ` is useful here . Note that it's somewhat new so make sure you are on a recent version of pandas .

Join 2 DataFrames on an index without introducing nans on missing indices
@USER that could work but if df [ ' a '] was nonzero to begin with then this would give the wrong answer .

Now get the distance from points to lines and only save the minimum distance for each point ( see below for a version with apply ) #CODE
( taken from a github issue ) Using ` apply ` is nicer and more consistent with how you'd do it in ` pandas ` : #CODE

Why is concat reformatting my headings ?
I have sorted a ` CSV ` file as I want it and appended a column to sort my data properly . However , in using ` concat ` ( I think this is where the issue is , anyway ) The output ` CSV ` file has been changed to ( 0L , ' HeadingTitle ') . I just want it to be HeadingTitle . #CODE
Then when you call ` unstack ` , #CODE

2 ) How do I apply a function to a set of columns to remove SettingWithCopyWarning when reformatting DATA columns .

That's my approach ( typed on my phone so it's not exact ) . I sort and then shift a unique I'd . If that ID matches the ID , then I shift up date . Then I create a column to measure the time between interactions . Also another column to determine what the reason of the visit was , also just another shift .

If " dt " is your DatetimeIndex object : #CODE

I know I can do it with a custom apply , but I'm wondering if anyone has any fun ideas ? ( Also this is slow when there are many groups . ) Here's one solution : #CODE
@USER all of this soln is completely vectorized or in cython . Using an apply ( even with a fast lambda ) will be orders of magnitude slower on any real dataset . As the apply is essentially a python loop .

The problem is , that I am getting this error form the last line of code , where I try to apply the function with ` df.apply ( flex_relative , axis =1 )`
The only thing I found so far was the link below , but calling a R function won't work for me because I need to apply that to quite big datasets and I may also implement an optimization in this function , so it definitely needs to be built in python . Here is the link anyway : Finance Lib with portfolio optimization method in python

how come its 8 and not 16 ( or 32 ? ) for the first time you go over 8 ? if its always perfectly sequential why couldnt you just do ` range ( len ( df [ col ]))` ? you cant really reverse it since 0 maps to ` 0 , 8 , 16 , 32 .... ` you might be able to fake it if you better describe the problem statement
Why not just simply replace the column , with ` frame [ col_name ] = range ( frame [ col_name ] .shape [ 0 ])` ?

I know this should be really easy but I keep getting errors when trying to pivot . I'm getting my data from psycopg2 #CODE
You don't need the line ` pdres =p dres.set_index ([ ' pricedate ' , ' hour ' , ' node '])` , as ` pivot ` works on the columns .

In the end we append the value of the days played to a list which we can attach to our original dataframe .
The following should be more optimised , basically I'd ` groupby ` on the team , apply a boolean test of whether the difference in the datetime is equal to a timedelta of 1 day .
Then for where this is True then apply a ` cumsum ` on this and add 1 .

then concat and taking the mean : #CODE
Finally concat them and calculate mean #CODE

Pivot each group in Pandas
Then I think you can get what you want by combining ` groupby ` and ` pivot ` : #CODE
Then we ` pivot ` : #CODE

So the problem is with your ` tweet [ ' text ']` ( inside the ` map ` function ) in : #CODE
Also , as a side note , consider list comprehensions as an alternative to ` map ` and ` lambda ` : #CODE
Sorry I didn't know that , please forgive me this time as i am new to stack . From next time I will make a new question for any change . I hope you will understand .

Background- I'm trying to extract unsynchronised dual-doppler measurements from a scanning ` LiDAR ` which is taking PPI scans . I have the data ( from MySQL ) loaded into pandas dataframes , and now need to apply some matching function where the rows are matched if the time of measurement is within some limit ( time 8s apart ) .
This seems like a ` join ` condition that is much much better expressed in the database prior to loading into Pandas . If you have it in MySQL , just do a join based on the condition you want directly , like ` select * from table1 a join table2 b on ABS ( a.dt_stop - b.dt_stop ) <= 8 ` -- or use fancy datetime functions as needed . After that , pivoting so that multiple matches become different columns will be easier in Pandas .

So it seems to have collapsed the groups , but I've now lost data ? Or how is the object now stored ? I realize I haven't done the apply stage , which is probably how I will generate new rows and new columns , but I don't know the next step or if there's a cookbook example for something like this .

Merge two time series with offset ?
and would like to merge them in order to get : #CODE
Finally , concat : #CODE

One method would be to convert the time strings to datetime but only take the time portion and then call ` apply ` and call ` datetime.combine ` to produce your datetime for both columns : #CODE
unfortunately this doesn't work . I get the following message : ' AttributeError : ' Series ' object has no attribute ' dt ''

I am getting an error in my code because I tried to make a dataframe by calling an element from a csv . I have two columns I call from a file : CompanyName and QualityIssue . There are three types of Quality issues : Equipment Quality , User , and Neither . I run into problems trying to make a dataframe df.Equipment Quality , which obviously doesn't work because there is a space there . I want to take Equipment Quality from the original file and replace the space with an underscore .

Btw , I believe ` pylab ` is not recommended anymore with ipython . You probably want to replace with ` import matplotlib.pyplot as plt ` and ` %matplotlib inline `

One way around this is to strip the cells , but there must be something better . #CODE
same problem . Is there any way how I can share an excel input file ? Maybe via google drive or dropbox ? How is this done typically on stack overflow ?

To get the columns containing the coefficients for each combination of date , expiry and symbol you can then merge ` df ` and ` coeffs ` on these columns : #CODE
Oops ! My bad . I forgot to reset the index on coeffs before the merge . It complained because it was trying to merge on ' date ' , ' expiry ' , ' symbol ' but these formed the index of ``` coeffs ``` rather than columns . I've adjusted the answer . Since you're new to pandas you should probably break it down and print the result of each method and function call to see what's going on . The solution is a bit terse , in fairness

I am generating a pivot table report using the pandas Python module . The source data includes a lot of readings measured in milliseconds . If the number of milliseconds exceeds 999 then the value in that CSV file will include commas ( e.g. 1,234 = 1.234 seconds ) .

Return multiple objects from an apply function in Pandas
I'm practicing with using ` apply ` with Pandas dataframes .
So , I'd like to use the 2nd dataframe , ` DFa ` , and get the dates from each row ( using apply ) , and then find and sum up any dates in the original dataframe , that came earlier : #CODE
Obviously I'm new to ` apply ` and I'm eager to get away from loops . I just don't understand how to return values from apply .
I don't think apply is best option for this . If I understand correctly why not DFa [ DF.index ] .sum() ?
I agree , it's a pretty lousy example . My main problem is trying to return from the apply . I would really like to see how I could return 3 different dataframes , and sum them up elsewhere ( but I didn't mention that in the question appropriately ) .
@USER ' Brien : The performance of DF.apply ( func , axis=1 ) is comparable to calling func in a loop . apply is useful when you want to align the output into a single DataFrame . If you need to return 3 disparate DataFrames , go ahead and loop over DF.iterrows() . For better performance you'll have to think of a better way to calculate the result ( such as doing a sorted cumsum for the toy example above ) or perhaps use Cython .
Also , note that ` apply ` returns a ` DataFrame ` . So your current function would return a ` DataFrame ` for each row in ` DFa ` , so you would end up with a ` DataFrame ` of ` DataFrames `
There's a bit of a mixup the way you're using ` apply ` . With ` axis=1 ` , ` foo ` will be applied to each row ( see the docs ) , and yet your code implies ( by the parameter name ) that its first parameter is a DataFrame .
Once you make the changes , as ` foo ` returns a scalar , then ` apply ` will return a series : #CODE

I thought it would be a fun learning exercise to load in a large number of draw results and see how close the posted odds for a number being drawn are . Maybe even do 2 , 3 ... 10 number combinations and see how they stack up against the posted odds . I realize that even with 250,000 draws it is a small sample size but it's more for my python education and not for anything else .
For python , there is something called the pydata stack .
That being said , I see no questions about any other package in the pydata stack that parses CSV , so it's fairly safe to assume , this is how people do it in python .

I've read the docs about slicers a million times , but have never got my head round it , so I'm still trying to figure out how to use ` loc ` to slice a ` DataFrame ` with a ` MultiIndex ` .

First , let's strip out the name ( ' id2 ' in this example ): #CODE
Here is another version using ` pandas ` built-in function ` stack ` . #CODE

It depends , it should be a separate question , you could use ` concat ` potentially
I'm trying to concat the dataframes created by each function but I get NameError . Is this to do with accessing the result from outside of the function itself ?
Well your dfs are local to the functions but you then return them which means you need to append each to a list and then you can concat . If it's not this then it should be a separate question after you accept an answer here

I think that I can obtain the result with the ` pivot ` function but I am not sure of how to do it

In the example below , I load a example data-set from UCI , remove lines with missing data ( thanks to the help from a previous question ) , and now I would like to try to normalize the data .

Here is a cut of the code that I have #CODE

Python Pandas merging two data frames and map one row from one data frame to all rows from the other
For example A , B merge with C gives me all the mapping for A , B , C .
Then merge A and B together on= ' key ' like this : #CODE

Is there a faster / more elegant way to accomplish this ? For example , is there a way to apply ` dateParser ` directly to the index ( perhaps inplace ) so I don't have to ` reset_index ` first ?

I also tried pandas pivot function , groupby , etc ... but I guess my understanding of pandas and python in general is not sufficient for this transformation . Any help ?
It looks like you're doing a " wide-to-narrow " transformation , in ` pandas ` terminology . One way to do this is with ` melt ` .

My current approach is to create an array of the indices where the markers occur , iterating over this array using the values to slice the dataframe , and then appending these slices to a list . I end up with a list of numpy arrays that I can then apply a function to : #CODE
I've looked at using groupby , but that doesn't work because grouping on the markers column only returns the rows where the markers are , and multi-indexes and pivot tables require unique labels . I wouldn't bother asking , except pandas has a tool for just about everything so my expectations are probably unreasonably high .
We can shift and take the cumulative sum to get : #CODE

The tricky part here is that the index of ` count_df ` is the ( unique ) occurrences of the customers . Therefore , I join the index of ` count_df ` ( ` left_index=True `) with the ` CompanyName ` column of ` df ` ( ` right_on= " CompanyName "`) .
You can drop the extraneous column using ` df.drop ` : #CODE

Thanks for your example Ed . I'll study the several uses of loc .
It's really inconsequential , but in your selection you access the column ' Letters ' using the dot notation ; df.loc [ df.Letters == ' C '] . If there are spaces in your column names , you should probably be using converters to strip those out , like you would if importing from a CSV or Excel file .

if not self.is_unique and len ( indexer ):

This SO thread provides a nice transpose technique to remove the duplicate . #CODE
the drop_duplicates was a no go , still encountering the same error . i'm not actually sure I need the .join ( school ); will remove that when i'm back in office with secure connection . the school join is already implicit within the classes .

Transpose ` .T `

I have a series that I split into two since the parts contain terms that need to be processed differently . Afterwards I want to merge the two split series ( actually , after the processing they are now two-column dataframes ) in the order they originally were . I have almost solved it : #CODE
Above I want to merge the annotations column into one . They should be mutually exclusive ( nans in one has a value in another ) .
There is no need to do anything complicated here . The index is preserved so you can simply append them and sort them #CODE

the ` notnull ` will filter out the missing values
I think you have ` NaN ` values you could drop these prior to the filter so after the ` frame = fill_rate() ` line do ` frame = frame [ frame [ ' Media '] .notnull() ]` could you try that

I know how do this by loading up the flat file and then manipulating the Series objects directly , perhaps by using ` append ` or just creating a new DataFrame using a manually-created MultiIndex .

It works , but this can't possibly be the best way to do this , right ? I feel like I'm abusing numpy.where() to get around not knowing how to map values from multiple columns of a dataframe to a lambda function in a non-iterative way . ( Also to avoid writing mildly gnarly lambda functions ) .
How can I non-iteratively map from two dataframe columns to one
FWIW , it took me a while before I could figure out exactly what this was doing . Pretty sure this mostly has to do with my lack of familiarity with df.loc [ row_indexer , column_indexer ] . So , thanks for making me learn that . Now that I'm on board as far as loc is concerned , I agree this seems slightly more readable than maxymoo's answer . +1
For your question about how to map multiple columns , you do it with #CODE

Join each one to the original dataset : #CODE
Apply a value to a ` sold_at_same_place ` column base on the value in ` place ` : #CODE

How to align the pandas series in ipython notebook ?
One annoying thing is that when I print ` pandas.series ` or ` pandas.dataframe.info` , the result in the IPython notebook can't aligned well . as the image " poor " shows . But when I copy the result to the txt editor , it seems to aligned well . So how can I adjust these align in ipython notebook ?

Thanks @USER that is close to what I want , however if I try to merge the means and the data frame I don't get what I'd expect .
I've added code to merge the means into the old data , but see my comment above - it is highly unlikely that you really want derived data together with the old data .

Strangely , when I index my dataframes using ` .ix ` as you do above the result is a dataframe full of ` NaN ` s . If I replace ` .ix ` by ` .iloc ` instead , it seems to work .

You can just perform a left style ` merge ` : #CODE

DatetimeIndex : 2284 entries , 1958-03-29 00:00 : 00 to 2001-12-29 00:00 : 00 Freq : W-SAT Data columns ( total 1 columns ): co2 2284 non-null float64 dtypes : float64 ( 1 ) memory usage : 35.7 KB
** TypeError : Cannot interpolate with all NaNs . ***

How use dataframe as map to change values in another dataframe
I have one large dataframe that acts as a map between integers and names : #CODE
Then I have another dataframe where I want to convert the ` Gene ` column to the ints given in the map ( the names in ` to_convert ` can be overwritten ): #CODE
Like I said , what I'd like to do is replace the names in ` to_convert ` with the integer values from ` gene_int_map ` .
I'm sure this is super-simple , but it seems no permutations of options for merge will do it . I could not get any boolean masks to work either .
Ps . I'd also like to replace the values in a one-column dataframe with the integers in ` gene_int_map ` : #CODE
Call ` set_index ` on the ' Gene ' column in ` gene_int_map ` and pass this as the param to ` map ` and call this on your ' Gene ' column on your other df : #CODE
And then replace values ( using ` map ` as suggested by @USER ): #CODE
` AttributeError : ' DataFrame ' object has no attribute ' map '` Your previous answer seemed to work though .
@USER : I changed it because ` map ` is faster than ` replace ` , while still allows the use of a dictionary , which in my opinion is very clean and straight-forward . You should not get that error with the latest changes anyway . :-)

apply custom function on pandas dataframe on a rolling window
You want to apply a risk calculation function ( let's say VaR ) named compute_var() on last 90 closing prices , on a rolling basis

By setting the index to ' Disease ' the dfs will align on the index values

I have managed to seperate the dataframes into sets of 8 ... but I can't get each dataframe to go beneath each other . They concat to the right , always ! #CODE

Since your Series has a DatetimeIndex , you could use the ` asof ` method : #CODE

How can I merge them in a single CSV interpolating the File1 and File2 with the time series of FileN ?
It's not very clear what you want to achieve . Do you want to concatenate all the csv files into a single data frame ? What is in the ``` Value ``` column ? When you say interpolate , what do you want to interpolate and how ? Maybe give a sample of the output you want , rather than just the headers

3 ) Finally ( so far as I've noticed ) , pandas used to truncate floats to a few decimals but now I'm getting the full representation :

Wait , are both functions meant to apply to individual strings instead of a whole row , or just ` perform_function1 ` ? Maybe it would help to include your functions ( if they're not too complicated ) .
The functions are kind of complicated . They're meant to apply to individual strings .
If you want to apply function to certain columns in a dataframe #CODE
Pandas is quite slow acting row-by-row : you're much better off using the append , concat , merge , or join functionalities on the whole dataframe .

How to apply a function to the elements of a pandas dataframe
I want to apply a lambda function to the elements of a dataframe , in the same way as np.sqrt returns a dataframe with the sqrt of each element . However pd.DataFrame.apply apply the function to an row or an column . Is there a similar comand that apply a lambda function on each element ?

I am using pandas with python and I have a dataframe ` data ` . I have another dataframe ` missing_vals ` . ` missing_vals ` contains a ` field ` column and a ` key ` column . The ` field ` column contains elements that correspond to names of the columns of ` data ` i.e ` data.columns ~= missing_vals [ ' field ']` . The mapping , however , is not one-to-one ( some entries in ` missing_vals [ ' field ']` do not exist in ` data.columns ` . I did a set intersection operation to take care of that and got an output array ` result ` containing all the values that are both in ` missing_vals [ ' field ']` and ` data.columns ` . Now I want to index into ` data ` using each element of ` result ` , check to see if that column contains the value corresponding to the element in ` missing_vals [ ' key ']` and replace it with ` NaN ` . I tried using for-loops , but I know this is not the ideal way to do it . Is there a way to do it with vector / lambda operations or perhaps with other dataframe functions ? I am new to pandas so I would really appreciate some help .
` for i in range ( len ( result )):
The ` replace ` method replace all occurances with the replacement value and if there are none it does nothing . It's unnecessary to loop through the columns yourself to check if the value to be replaced is there . If you post representative examples of the data frames in question I can probably help you more .

Although the above code solve my problem but is there a more simple way to achieve this . I tried Pivot table but it does not solve the problem perhaps it requires to have same number of element in each group . Or may be there is another way which I am not aware of , please share your thoughts about it .
This is really quite similar to what you are doing except that the loop is replaced by ` apply ` . The ` pd.Series ( x.values )` has an index which by default ranges over integers starting at ` 0 ` . The index values become the column names ( above ) . It doesn't matter that the various groups may have different lengths . The ` apply ` method aligns the various indices for you ( and fills missing values with ` NaN `) . What a convenience !

Answers I founded online , such as ` grouped = grouped.filter ( lambda x : len ( x ) 2 )` return a DataFrame where the data are not grouped .

Create a pivot table : employer names vs caller names . #CODE
EDIT2 : A " pivot [: 5 ] .transpose() " will bring you very close to the final table form you suggested , where the top five callers are the table " headers "

I am trying to subtract two weights , male-female . I am able to group the data and select the weights for each sex but am unable to simply create a new variable " wt_diff " and have the " wt_diff " appear on each row regardless of sex so that each city / date / sex group would in fact have , on the same row , the weight diff between the sexes .

To avoid the following error , I would like to replace any integer in my DataFrame with Unix Time :
In a small subset of the Excel files I'm reading in , I know the integers that appear are 0 . However , what if there were multiple distinct integers ? Or what if there are multiple dtypes ? How can I easily replace any non-datetimes with the epoch represented datetime ?

To call ` rebalance ` on a DataFrame other than the one returned by ` setup_df ` , replace ` c = df.colmap ` with #CODE

I'm trying to apply one function ` f1 ` to rows ` [ ' Utah , ' Texas ']` and ` f2 ` to other rows . I don't want to create separate DF for each function .
It works ! I think it would be easier for me either to create separate DFs for each function or to transpose ` frame ` and then ` frame [[ ' Utah ' , ' Texas ']] .apply ( f1 )`

Apply function to column in pandas dataframe that takes two arguments
You can do this with the ` map ` method without writing a function or using ` apply ` at all : #CODE

then the syntax would be ` df.apply ( func , axis = 1 )` to apply the function func to each row .
Apply function to each row of pandas dataframe to create two new columns

A very straightforward way would be to ` concat ` pairs horizontally , concat the results vertically , and write it all out using ` to_csv ` : #CODE

Because NaT is technically a datetime this condition wasn't covered by that function . Since isnull will handle this , I wrote this function to apply to data [ col_name ]: #CODE

It might also be of interest to stack the data frame so that you have the dates and times together in the same index . For example , doing #CODE

to create a sqlalchemy engine to connect pandas and Hive . For some reason , today I started getting KeyError for a particular type_code ( 17 ) after the server I work on was reset . I have only ever installed Thrift via pip , and I don't have a good idea of how it works . From what I understand , thrift will autogenerate some sort of Python file ( TCLIService ? ) that I cannot directly import , and then the pyhive code runs off of that . Is there any way to regenerate this file to fix this problem , or does anyone have experience with a similar problem ? Here is the full stack trace : #CODE

pandas find max value in groupby and apply function
How may I set my maximum H value ( 4 for Dublin and 5 for Madrid ) as a constant / city in order to apply the function all over the DataFrame ? The expected df would appear as : #CODE

Is it possible to specify the order of levels in Pandas factorize method ?
I am using pandas to factorize an array consisting of two types of strings . I want to make sure that one of the strings " XYZ " is always coded as a 0 and the other string " ABC " is always coded as 1 .
AFAICT you can't do that directly with factorize , but it's quite easy to build a ` dict ` ( which you can then use for pandas's ` map ` ) .
Again , you can just use this for pandas's ` map ` .

How to stack data frames on top of each other in Pandas
Finally , I used concat to concatenate the 8 dataframes together ( Stacked on top of each other ) . However , the output it somewhat distorted and it is not in order . #CODE
Seconding the first comment . And further , have a look at ` concat ` , eg ` pd.concat ([ list of frames ])` stacks the list of frames into one dataframe .
The problem is that the column names are all different within each sub dataframe . Thus , when pandas does the ` concat ` , it doesn't just append the dataframes to the bottom , it expands the dataframe to have new colums with the right names and then appends the rows .
Many thanks for your answer :) Just wondering what the sub_df.columns = range ( 12 ) does ? Does it just rename the columns 1 to 12 or apply some sort of indexing ?

How to apply functions with multiple arguments on Pandas selected columns data frame
What I want to do is to apply a function : #CODE
The ` DataFrame.apply ` method takes a parameter ` axis ` which when set to 1 sends the whole row into the apply function . This makes it a lot slower than a normal apply function since it is no longer a proper monoid lambda function . But it does work .
The error message is telling you that you cannot cast a pandas Series to a ` float ` , whilst you could call ` apply ` to call your method row-wise . You should look at rewriting your method so that it can work on the entire ` Series ` , this will be vectorised and be much faster than calling ` apply ` which is essentially a ` for ` loop .

with the whole timestamp as index . I tried to concat the Series #CODE
You can achieve this by a performing the concat you started with and extending it to the other axis . Creating two DataFrames from these series allows you to merge them back together and create the index you want : #CODE

The different data frame don't have the same number of rows . I want to merge the different data frame in a single one like this : #CODE
` concat() ` does not do resample I think . The values have different time sampling they need to be interpolated with a linear method .
I suspect you just want to do an outer merge and then interpolate ? Can't really tell for sure . It seems like all the ??? are just going to be 18.8 if I understand correctly . Can you replace the ??? with actual numbers ( or at least approximations ) and perhaps have some values not equal to 18.8 in the second dataframe . It's hard to check an answer against ???. We are kind of exact about those things here . ;-)
You can ` concat ` the two ` DataFrames ` , ` interpolate ` , then ` reindex ` on the ` DataFrame ` you want .
Next , you need to interpolate to fill in the missing values . I interpolate using `' time '` ` mode ` so it properly handles the time indexes : #CODE
I think generally it would be best to stop here , since you keep all data from all ` csv ` files . But you said you want only the time points from the longest ` csv ` . To get that , you need to find the longest ` DataFrame ` , and then get the rows corresponding to its indexes . Finding the longest ` DataFrame ` is easy , you just find the one with the maximum length . Keeping only the time points in that ` index ` is also easy , you just slice using that ` index ` ( you use the ` loc ` method for slicing in this way ) . #CODE

Now I want to replace TBA in the first dataframe with corresponding TBA in the second dataframe where the dates match . The default value would be 0 . So I am iterating through rows as follows : #CODE
If you call ` set_index ` on ` pdata ` to ` date_2 ` then you can pass this as the param to ` map ` and call this on ` tdata [ ' date_1 ']` column and then ` fillna ` : #CODE

I guess I will have to ` apply ` or map a ` split ( " , ")` to the ` Term ` column , but what do I do after that ? #CODE
You can use ` str.split ` to do the splitting ( instead of apply and split approach , but similar ): #CODE
Now , to obtain your desired output we have to stack these columns , but first merge it with the genes column to have this information in the stacked frame : #CODE
From here , we can reset the index , rename our column with terms , and drop the integer column ( from the automatically generated column names ) we don't need anymore : #CODE

Replace the column index with a hierarchical one : #CODE
use ` stack ` followed by ` reset_index ` to convert ` date ` and ` id ` column labels into columns : #CODE

The left join option is in order to keep rows that don't have matches on crawldf .
You can read the documentation for merge here :

Formatting a Pivot Table in Python

To filter out some rows , we need the ' filter ' function instead of ' apply ' . #CODE

When using panda's ` resample ` function on a DataFrame in order to convert tick data to OHLCV , a resampling error is encountered .

Load it into a new table , then ` DROP ` the old one and ` RENAME ` the new one to the old ones name .

It seems that , by using if_exists= ' replace ' , Pandas drops the table and recreates it , and when it recreates it , it doesn't rebuild the indices .
When I use if_exists= " append " the problem doesn't appear , it is only with if_exists= " replace " that the problem occures .
Yes , this is just a problem of missing features in the pandas ` to_sql ` method . Best solution is only to append , and create the table with the correct primary keys another way .

Does Pandas concat interpolate missing values in multivariate time series ?

resample time series with python
I have several csv files in every folder and I need to merge all the csv file of each folder in to one csv file per folder . This is why i wanted to implement this in python how can i manage that ?

I want to join both dataframes via the ` id ` column . So the result should look like : #CODE
Why is ` join ` empty ?
aah , there seems to be an issue with the ID columns . for df1 they are of type object , and for df2 they are ints . hence , join does not work ...

For now , there is not yet support for specifying primary keys ( it's on the feature wishlist ) . Possible workaround to first create the table , and then use the ' append ' option in ` to_sql ` . To create the table , ` pd.io.sql.get_schema ` could be helpful to create the schema ( that then can be adapted / executed to create the table )
In the end however I found it simpler to just make the table before and append to it .

I have a large mysql table ( name= Table-B ) which I want to do a left join to with my dataframe . I've been reading the pandas documentation but its not clear to me how I can do this without first reading ( via read_sql / read_table ) Table-B into a dataframe . I don't want to load a massive table into pandas just to then left join to df which will return a small fraction of the rows in Table-B . Is there a way to pass the dataframe df as an object / parameter in read_sql .
and then perform the join in the database , and read the result into a new DataFrame using ` read_sql ` : #CODE
I don't think there is a third other option . The join must be performed either by MySQL or Pandas . MySQL works solely on tables . Pandas works solely on NDFrames . Since we want to avoid loading ` Table-B ` into a DataFrame , the only other option is to perform the join on the database side .
Even if Pandas were to provide a function to make it * appear * as though you could join a database table with a DataFrame , under the hood Pandas would have to move the DataFrame into a temporary database table of move a table into a DataFrame .

You need to unstack your results : #CODE
I did not know anything about unstack . The docs boggle my mind . But it works like a charm .

conditional replace based off prior value in same column of pandas dataframe python
Feel like I've looked just about everywhere and I know its probably something very simple . I'm working with a pandas dataframe and looking to fill / replace data in one of the columns based on data from that SAME column . I'm typically more of an excel user and it is sooo simple in excel . If we have : #CODE
in excel what I'm trying to do would be " =IF ( AND ( A2=0 , B1=-1 ) , -1 , A2 ) so that I could then drag down column ' B ' and that would apply . In essence , based on the prior data point of column B , and the current value of column A , I need to update the current value of B .

Pandas interpolate : slinear vs index
In Pandas interpolate what is the difference between using ` method ` of ` slinear ` vs ` index ` ?

Which will change your ` df ` , without raising a warning and there is no need to merge anything back .

ix will keep line 6 to 52 base on your index .

In Pandas , I am creating a dataframe that merges data from two different Beatbox queries . First , I pull all my Opportunity data , then I pull all my Account data , and then I merge .
However I would like to optimize this process by only pulling data for account [ ' ID '] that exists in the oppty [ ' AccountID '] column , as opposed to pulling the entirety of the Account data before a merge . How do I do this ?
This should extract the data using a left inner join , which omits unmatched rows in both tables . It also does the join in SQL , reducing the amount of database traffic ( and therefore also network bandwidth ) by having the database server do the work , thereby reducing the computational load on your desktop client system .

shift index of different dataframe

Transforming pandas data frame using stack function
You could use ` melt ` on ` State ` Column like #CODE

One way would be to create a dict mapping the ` unique ` values in your ID column to your new sequential values and then call ` map ` passing in this dict : #CODE

This seems very wasteful why not just construct a df with the correct number of additional columns and then ` concat ` ?
I think the problem is that on the third iteration the code barfs because although you allow duplicates , you now have 2 columns with empty strings as column names but this then fails the internal checks as it's expecting a unique column to append but it now finds 2 , if your column names were unique then this would work but again , why not just construct a df with the correct dimensions and concat once
Rather than inserting a column at a time , I'd create a df of the dimensions you desired and then call ` concat ` : #CODE

you can create a dataframe based on your list , and merge with your dataframe #CODE

I have to do that for 15 columns though . I have read that lambda is a ' throwaway ' function . Is it better to define a function ( to split and then to turn the values into minutes ) and apply it to each column instead ? Would I loop through the columns ( not all columns , but 15 of about 30 ) ?
I am hoping someone will provide a function I can use to apply / map everything to several columns at once efficiently . I am not too familiar with def and return functions ( I learned basic pandas before I learned basic python , just for practical purposes .. slowly learning though ) . Need to turn those timestamps into minutes
You could just measure this but generally ` apply ` should be the last resort as it doesn't scale as well as its a for loop and if called on a series it executes per row . With respect to turning it into a function so you can apply to 15 columns you put your code for you last method into a function and then call ` apply ` on a df , this will call it for each column but it will try to execute the function on the whole Series
I would imagine the dt accessors are the best / fastest way : #URL
Also , what about .map vs .apply ? And is it better to have a separate line of code for each column I am doing this to , or is it better to define a function and apply that to each column ? It looks a bit messy to have this split lambda function 15 times in a row ( 15 timestamp columns to be converted to minutes )
just wrap it up in a small function , and either apply it across the columns , or since you prob don't have too many of these , just do it per column .

Questions asking us to recommend or find a book , tool , software library , tutorial or other off-site resource are off-topic for Stack Overflow as they tend to attract opinionated answers and spam . Instead , describe the problem and what has been done so far to solve it .

I tried with ` drop ` #CODE

Note : executing the cubic spline interpolation via the apply function takes quite a mount of time ( about 2 minutes in my PC ) . It interpolates from about 100 points to 300 points , row by row ( 2638 in total ) . #CODE

If you file is larger you could also create the dataframe as you go and reset the OrderedDict each time to avoid storing all the data in the dict also , you just have to append the last group outside the main code , we can also use itertools.islice to get all the slices and use itertools.izip to zip if using python2 : #CODE
This answer is going to be a lot like @USER ' s deleted answer ( using ` unstack ` instead of ` pivot_table ` , but they're equivalent here ) with one extra step at the end : #CODE
and then we set the index and unstack : #CODE

Merge multiple pandas columns into new column

Does anyone know what this plot is called , and how I can do this in python ? In R , I use the " cut " command and then plot the x , y of the cut .
For the follow-up questions , we can do something more powerful using boxplot . #CODE

I'm trying to apply a weighted filter on data rather the use raw data before calculating stats , mu , std and covar . But the results clearly need adjusting . #CODE

g [ nongrp_cols ] .apply ( lambda d : d.apply ( lambda s : s.value_counts() ) / len ( d.index ))`
pd.DataFrame.groupby.apply really gives us a lot of flexibility ( unlike agg / filter / transform , it allows you to reshape each subgroup to any shape , in your case , from 538 x 122 to N_categories x 122 ) . But it indeed comes with a cost : apply your flexible function one-by-one and lacks of vectorization .

But I'm not sure how to proceed . How do I apply the date subtraction operation , then combine ?
You can use apply like this : #CODE
FWIW , using ` transform ` can often be simpler ( and usually faster ) than ` apply ` . ` transform ` takes the results of a groupby operation and broadcasts it up to the original index : #CODE

vlookup equivalent to join 2 tables using pandas
using vlookup in Pandas using join ): #CODE
@USER - I referenced the post you suggest as duplicative , and tried to apply the .map() function it recommends . My results from doing so are listed above under " My best effort so far " . I realize I must be missing something ; can you help my understand how to use the .map() function in this example ?
You can use the merge function from the pandas library as follows :
You can read about the merge function here : #URL

getting all corresponding max values in pandas pivot table
Pivot the sample values for each group . aggregate by max ( pValue )
Get the corresponding Status , desc corresponding to the sample with the higher pvalue and replace its value with a pValueString .
Create your pivot table separately : #CODE
Finally merge the two together #CODE
Then get the max pValue per Group / Sample ( in pivot table form ) . #CODE
Finally , set the index of the previous DataFrame to the index of the new one and join . #CODE

Drop a level from the column index , then drop the name : #CODE

If this is possible , is there a way for me to specify a cut off index after which this behavior would take place ? For example if the cut off index would be the key 1 , the result would be : #CODE

This depends ; you will have to take out a sheet of paper and calculate the error your overall statistics will get if you don't interpolate and just zero-fill these NaN .
Just find each NaN , and linearly interpolate to the adjacent four values ( which is , sum up the values at ( y +- 1 , x +- 1 ) ) -- this will seriously limit your error enough ( calculate yourself ! ) , and you don't have interpolate with whatever complex method is used in your case ( you didn't define ` method `) .
You can try to just pre-compute one " averaged " 4800x4800 matrix per z value -- this shouldn't really take long -- by applying a cross-shaped kernel across the matrix ( it's all very image-processing-like , here ) . In case of NaN's , some of the averaged values will be NaN ( every averaged pixel where a NaN was in the neighborliness ) , but you don't care -- unless there are two adjacent NaNs , the NaN cells that you want to replace in the original matrix are all real-valued .
Then you just replace all the NaNs by the value in the averaged matrix .
Sorry , I meant the mean over all 92 values along the z-axis . OItherwise I'm stuck at the initial problem : What's the fastest way to interpolate over missing values along the z-axis .
If I understood your answer correctly you areadvising 2D linear interpolation . I am going for 1D interpolation ( for every NaN interpolate from z+-1 ) . This is also what ` pandas ` wrapper to ` scipy.signal.interp1d ` does . My answer is more of a code optimization one than interpolation selection . Unless I understood your answer wrong and it is more efficient - in that case : Care to explain it with a code sample ?
To sum that up : The basic idea is to precalculate a " mean-array " with the mean value of neighbours along the z-axis . Then find all the indices of of * NaN * and replace them with the according value from the pre-calculated " mean-array " ? I do think this could be very fast but unfortunately doesn't solve the problem of consecutive * NaN * unless I employ a lot of ( slow ) loops .

Ok this seems like it should be easy to do with merge or concatenate operations but I can't crack it . I'm working in pandas .
Some methods I've tried are to select the rows that are in one but not the other ( an XOR ) and then append them , but I can't figure out how to do the selection . The other idea I have is to append them and them delete duplicate rows , but I don't know how to do the latter .
You want an ` outer ` ` merge ` : #CODE
The above works as it naturally finds common columns between both dfs and specifying the merge type results in a df with a union of the combined columns as desired .

Python pandas HDFstore append dataframe with missing column
The problem is how to append dataframe with missing column . Say column ' c ' is missing , I try to append only [ a , b ] , I cannot do it directly because the dataframe ` cannot match existing table structure `
Now I am stucked , how can I append this partial column dataframe into HDFstore ?

The reason for that error is you use ' or ' to ' join ' two boolean vectors instead of boolean scalar . That's why it says it is ambiguous .

The only thing I can think of is to either generate the dirct for each row where you can drop the ` NaN ` values , or to parse the json dict and strip the entries out , I don't think dfs will allow a form where the dimensions are different for each row . Thinking about it you could create a dict for each row that doesn't contain the NaN values and then call ` to_json ` on that column , let me try this
creating a list is necessary here otherwise it will try to align the result with your original df shape and this will reintroduce the ` NaN ` values which is what you want to avoid : #CODE
drop the NaN value when converting DataFrame to dict , and then

Concat Columns produces NAN even though axis is the same for all datasets
I am trying to concat columns from multiple dataframes . #CODE
Alternatively , you can ` unstack ` the ` symbol ` index on the inputs before the ` concat ` to get the ` symbol ` as the column names . Then , after the ` concat ` , you can overwrite the column names with ` [ ' AUD ' , ' CAD ']` .

I have a dataframe that contains for each group the number of observations during a certain period . Some groups don't contain all periods , and for these groups I want to append x rows with the missing periods in it .
You can unstack the results on ` PERIOD ` and then stack them back with the ` dropna ` option set to False . #CODE

comparing 7 with 6 are not current and previous row value . To compare a series with itself shifted by a single row you call ` shift ` so you can replace your code with ` MyFrame [ ' A '] ! = MyFrame [ ' A '] .shift() ` or whatever
It's unclear what you're attempting but you can compare the entire Series or df by calling ` shift ` : #CODE

Use a ` groupby ` and ` value_counts ` to get the initial counts you want . Then unstack the multiindex to get a DataFrame and set null values to 0 to get the final results : #CODE
I did something similar but want to get it as a data frame so that I can join with other existing data frames .
The result should be a dataframe . The intermediate ' grouped ' is a multiindex series . The unstack changes it into a dataframe by making it single index . ( Sorry I can't format this properly , on mobile now . )

How to append data to pandas multi-index dataframe
How can I append data to a Pandas Multi-Index DataFrame ? I currently use the following code to successfully create a dataframe from my data . #CODE
DataFrame to merge #CODE
So , a ` merge ` + ` groupby ` give #CODE
What if I ran a for loop would their be a way to append and create the dataframe as I go ? I think what I could do is append eachTicker to the array list then from there I could build a dataframe from the arrays :-D ( epiphany moment )

I don't understand what the second half of your question has to do with the first half . The error message is pretty clear : " The normalize function assumes floating point values as input , got object " , so you should check what you're reading into your dataset . Is everything in your file a number , that can be properly parsed into a number by ` read_csv ` ?

The following snippet should work after your ` apply ( crawl )` . #CODE
` prices ` is lowercase everywhere in your sample code . Either you didn't copy it correctly into Stack Overflow or you've changed it somewhere else .

KeyError when using melt to restructure Dataframe
The code is as follows , where ` combined_data ` is the dataframe . I used ` melt ` to do this but get the Error ` KeyError : ' years '` and don't know how to handle this : #CODE

pandas dataframe drop columns by number of nan
I have a dataframe with some columns containing nan . I'd like to drop those columns with certain number of nan . For example , in the following code , I'd like to drop any column with 2 or more nan . In this case , column ' C ' will be dropped and only ' A ' and ' B ' will be kept . How can I implement it ? #CODE
So the above will drop any column that does not meet the criteria of the length of the df ( number of rows ) - 2 as the number of non-Na values .
Thanks . A typo in your code ` len ( df )` should be ` len ( dff )`

When I try to insert a portion of this string into a dataframe I get this error : #CODE
On the other hand if I just insert a string literal like " baloney " I do not get this error . I think something funky is going on because the string I am using begins with a number . That's why I tried explicitly casting to ` str() ` but since that didn't work I'm out of ideas . What else should I try ?

From what I understand I am following the steps to apply PCA as they should be . But my results are not similar with the ones in the tutorial ( or maybe they are and I can't interpret them right ? ) . With n_components=4 I obtain the following graph n_components4 . I am probably missing something somewhere , I've also added the code I have so far .
Here is the thing . the data is 17 dimension ( features ) for 4 observations . Then you shouldn't pass use transpose in demo_df.T .

I am trying to drop some rows from my Pandas Dataframe ` df ` . It looks like this and has 180 rows and 2745 columns . I want to get rid of those rows which have a ` curv_typ ` of ` PYC_RT ` and ` YCIF_RT ` . I also want to get rid of the ` geo\time ` column . I am extracting this data from a CSV File and have to realize that ` curv_typ , maturity , bonds , geo\time ` and the characters below it like ` PYC_RT , Y1 , GBAAA , EA ` are all in one column : #CODE
I decided to try and split this Column and then drop the resulting individual columns , but I am getting the error ` KeyError : ' curv_typ , maturity , bonds , geo\time '` in the last line of the code ` df_new = pd.DataFrame ( df [ ' curv_typ , maturity , bonds , geo\time '] .str .split ( ' , ') .tolist() , df [ 1 :]) .stack() ` #CODE
Try to edit the CSV file and replace geo\time with geo_time or something perhaps ?
actually looking at the data in the file , it looks like just really poorly formatted data . The first 4 cols are comma separated , and the rest are tab separated , which is dumb . I'm not sure how to fix that , perhaps use SED to replace all the commas with a \t ( tab ) char and then try to read it in ?
on a * nix machine , can use the ` tr ` command to replace those commas
Thank You . But is there a way to replace the commas with tabs in a script as I need to automate the whole process ?

How to merge two DataFrames of unequal size based on row value
Sorry I am not giving you actual pandas code to create the DataFrames above . But I want to conceptually understand how to join two unequal sized DataFrames with different " indexes " based on a column name . I tried merge and concat and join but dont get the result I want .
A default ` merge ` works fine here , assuming your index actually is your index : #CODE
Here the ` merge ` looks for common columns and performs an ` inner ` merge on those columns .
You can be explicit and specify that you want to merge on the ' Name ' column : #CODE

I'd ` concat ` using a DataFrame ctor : #CODE
You need to assign the result of the concat so ` df =p d.concat ([ df , pd.DataFrame ( columns=list ( ' BCD '))])`
Thanks , that worked . Can I append the columns to the last column ? The new columns are added to the beginning . It seems like concat is doing automatic reordering because my original columns are moved around as well .

I have several pandas data series , and want to train this data to map to an output , df [ " output "] .
OK , first let's prepare your data set , by selecting the relevant columns and removing leading and trailing spaces using ` strip ` : #CODE
If you have too many levels for this to work , or you want to consider the individual words in ` catB ` as well as the bigrams , you could apply your ` CountVectorizer ` separately to each column , and then use and use ` hstack ` to concatenate the resulting output matrices : #CODE

So for instance I cut from a frame : #CODE

It will be faster I believe to use the vectorised ` str ` method to split the string and create the new pixel columns as desired and ` concat ` the new columns to the new df : #CODE

Pandas append list to list of column names
I'm looking for a way to append a list of column names to existing column names in a DataFrame in ` pandas ` and then reorder them by ` col_start ` + ` col_add ` .
To avoid the ` KeyError ` on the capitalised column names , you need to capitalise after calling ` concat ` , the columns have a vectorised ` str ` ` title ` method : #CODE

Using ` apply ` check if value is list ` isinstance ( x , list )` and take the value , and then ` apply ( pd.Series , 1 )` to split as columns #CODE

I would like to take a given row from a DataFrame and prepend or append to the same DataFrame .
Rather than concat I would just assign directly to the df after ` shift ` ing , then use ` iloc ` to reference the position you want to assign the row , you have to call ` squeeze ` so that you assign just the values and lose the original index value otherwise it'll raise a ` ValueError ` : #CODE
To insert at the end : #CODE

This also happens for multiindex columns . Is there a proper way to update the index / columns and drop the empty entries ?
I do not want to drop the level . My example was basic , but I want to filter a level for multiple values , and eliminate others . I would thus like to have the same structure but only have the right values in the filtered dataframe index and / or columns .
@USER , I understand why one would want to keep the same structure . But it also seems natural to me that one might desire the output I am looking for . The same argument could be made when indexing a normal dataframe . It returns only the selected values in the index , and as such it does not conform to the original dataframe anymore . Does it make sense ?
proper way to update the index / columns and drop the empty entries ?
Consideration ( 1 ) is that you want to know the index of what's left . Consideration ( 2 ) is that as mentioned above , if you trim the multiindex you can't merge any data back into your original , and also its a bunch of nonobvious steps that aren't really encouraged .

Thanks , but that doesn't seem ( to me ) to help much . Two adjacent cells might both be merged , but in different merge groups . How do I tell which group a given cell is in ( i.e. which other cells it is merged with ? )

I want the dataframe to be of the form below . I've taken some steps towards this , which included using ` melt ` in the code below , but for some reason that got rid of my ` Date ` Column and resulted in the dataframe above . I am unsure how to get the Date Column back and obtain the Dataframe below : #CODE
Can you not add the currency identifier following the ` melt ` ? #CODE

unstack the new sites to be columns

I'm trying to merge two data frames , testr and testc , but I keep getting a Key Error on " Channel ID " and not sure what the problem is . Do the dataframes have to be the same size or have the same datatype for pd.merge to work ? Here is my code to merge with .info() on each dataframe : #CODE

I currently have a dataframe as follows and all I want to do is just replace the strings in ` Maturity ` with just the number within them . For example , I want to replace ` FZCY0D ` with ` 0 ` and so on . #CODE

` a [ ' Names '] .str .contains ( ' Mel ')` gives you a series of bool values #CODE
` a [ ' Names '] .str .contains ( ' Mel ')` will return an indicator vector of boolean values of size ` len ( BabyDataSet )`

How to replace commas with tabs in TSV File
In my dataframe below I am trying to replace the commas in the column ` curv_typ , maturity , bonds , geo\time ` with tabs and in the strings below it too , so that I can then create new columns from this . #CODE

I would like to change the value of a ` Pandas ` DataFrame based on index and column . I am getting an error ` A value is trying to be set on a copy of a slice from a DataFrame ` . I searched around and found similar questions / answers , but none that I was able to apply . #CODE

Could you explain what you changed ? Did you only add this ` + range_name_list [ ix ] + " | "` or did you also make other changes ?
Yep , so I changed ` df [ ' Range '] = ' | '` so that the default is a pipe rather than an empty string -- this way the string in the range column ends up being ` |condition1|condition2|etc| ` based off the change that you noted , which also included taking out the break statement ( ` + range_name_list [ ix ] + " | "`) . Last change is then filling in the rows whose value is ' | ' rather than '' ( ` df [ ' Range '] [ df [ ' Range '] == ' | '] = ' Not_in_Range '`)

The ` [ 0 ]` indexes the first item of ` ts.value_counts ( ... )` . If nothing is returned by this function , you will get an index error . To avoid this , you only perform the indexing when the length > 0 ( ` if len ( ts.value_counts ( sort=True )` will evaluate to True if it is not None and the length is greater than zero ) . Otherwise , assign a None value ( ` else None `) .

I want to use interpolation to replace some missing values in dataframe . Here is the code to replicate my sample data . #CODE

Is there a chance that you can replace those licensed data with some randomly simulated data and upload an artificial sample file to the Stack Overflow website ? to work out a sample code and test it , I need the real structure of your datafile . Thanks .

I am trying to replace the strings in the ` Years ` column of the Dataframe below with just the numbers in the string . For example , I would like to change ` ZC025YR ` to ` 025 ` . My code is as follows : #CODE

I could get all Wednesdays of a week , using df.resample ( ' W-WED ') , but I cannot merge them back correctly to the original data set so that I can compute cumulative product of returns for a week starting on Wednesday by PERMNO and DATE .
The series of dates falling on Wednesdays is shorter than the original data set . How can I merge back and fill in correctly the dates ?
I was wrong in the beginning thinking that I should create a new data set containing all dates that fall on Wednesday , and then merge it with the original , bigger data set , filling missing values with the appropriate Wednesday dates to create a groupby variable .

I tried using replace , lstring-rstring but I'm not able to replace the extra characters from thr readingtime column #CODE
Tried loc as well but not getting errors
I'm trying following code - I'm trying following code , is there any more efficient way ? for i in range ( len ( da2 )): da2.iloc [ i , 3 ] = da2.iloc [ i , 3 ] .__str__() [ #URL Can you suggest better way ?
You can apply a lambda function to the column of the data frame , extracting the date from the dictionary via ` x [ ' $date ']` , and then just take the date / time portion ( ignoring the time offset ) . As this is a ' datetime naive ' object , Python wouldn't know what to do with any timezone adjustment . Use this stripped date / time string ( e.g. ' 2014-11-04T17 : 27:50 .000 ') as the input to ` strptime ` . #CODE
apply function throws an error = DataFrame ' object has no attribute ' datetime ' Checked Pandas version , its Also dt.datetime should be df.datetime ( just a typo ) right ?
No , I've imported datetime as dt ( see edit above )

This adds the extra column ` name_count ` , which you can drop with ` summary_df.drop ([ ' name_count ] , axis=1 , inplace=True )` . It also strikes me as somewhat inelegant -- I suspect the second and third lines could be combined .

What the ` apply ` function does , is that for each row value of ` df [ ' A ']` , it calls the ` applyFunc ` function with the parameter as the value of that row , and the returned value is put into the same row for ` df [ ' B ']` , what really happens behind the scene is a bit different though , the value is not directly put into ` df [ ' B ']` but rather a new ` Series ` is created and at the end , the new Series is assigned to ` df [ ' B ']` .
You could use ` str.extract ` to search for regex pattern ` BULL|BEAR ` , and then use ` Series.map ` to replace those strings with ` Long ` or ` Short ` : #CODE

Pandas : Conditionally append substrings " y " + " z " to column B if column A contains " x "
If it does not contain ` Xn ` or ` nX ` , then insert `" 1 "` .

You can use ` df.asfreq ( ' 1d ')` to resample your data based on the date column and fill in the missing values automatically .

Pandas : Insert variable substrings into column B from column A with help of dictionary
I would like to identify `" AAPL "` in column ` Name ` , pass that through a dictionary `" AAPL " : " Apple "` and then insert that into a string in new column ` Description ` .
Note : I perhaps need to set up another column to inform whether the leverage is positive or negative and use that to decide whether to append `" - "` in front of the multiplier as ` -10X leverage . ` #CODE
You can define an explicit function to apply to the entire ` Name ` Series . #CODE
If you want to add more words to replace just add them to maps and the same with signs .
@USER Shouldn't I apply ` split ` as : ` df [ ' Description '] = df [ ' Name '] .map ( split )` to run it ? Or similar
Not sure where ac_result comes from but if you have adat like that then just split and strip to get names price then pass the list , try this code and see how it works . #URL

Pandas map to DataFrame from dictionary
and call that for all instances when I need to map company names . How can I make it ` map ` for matches in ` companies ` instead of ` lambda x ` ?
Use Replace :
Then map your replacements in a dictionary : #CODE
Then replace : #CODE
Thanks for the suggestion . This will replace all instances as defined by the dictionary , but it will not exclude the rest of the string as ` .map ( lambda x : " American Express " if " AXP " in x else ""` . Can I make replace discard the rest ?
Applymap is for the whole df , is you ; re doing just a single column ( series ) , then just use apply .
@USER -- yeah , np is numpy . But actually I just replaced that line using ` loc ` instead of ` np.where ` , probably a more standard way to do it . I think ` str.contains ( i )` is the real key there . Beyond that there's a few different ways to do that line including ` loc ` and ` np.where `

Pandas : Difference between pivot and pivot_table . Why is only pivot_table working ?
According to the pivot documentation , I should be able to reshape this on the score_type_name using the pivot function . #CODE
So can anyone tell me how I can get a nice Dataframe like I want using pivot ? Additionally , from the documentation , I can't tell why pivot_table works and pivot doesn't . If I look at the first example of pivot , it looks like exactly what I need .
Not sure , but I think the ` pivot ` v . ` pivot_table ` issue might have to do with non-unique index entries .
I'm not sure I understand , but I'll give it a try . I usually use stack / unstack instead of pivot , is this closer to what you want ? #CODE
I'm not sure why your pivot isn't working ( kinda seems to me like it should , but I could be wrong ) , but it does seem to work ( or at least not give an error ) if I leave off ' struct_id ' . Of course , that's not really a useful solution for the full dataset where you have more than one different values for ' struct_id ' . #CODE
The stacking helps , but I still need it like my desired output , so I can actually join stuff to it . I can't seem to work with the layered dataframes .
Yes , reset_index helps . I will mark this soon if no one can tell me why pivot doesn't work . Thanks !
@USER Sure , no problem . I'm curious too if anyone can explain the issue with pivot !

I wish to have the values of ' Original_Level ' cap to the level where another column ' NS ' hits a trigger ( in this case abs ( NS ) > = 4 ) . This new column , ' Desired_Level ' , created while leaving the ' Original_Level ' column unchanged . The below df shows a cap of 13.122 and 50.887 when abs ( NS ) > = 4 #CODE
I am looking for a generic solution , that works away from the lowest abs ( NS ) level in both directions to hit both the -4 and the +4 trigger . If it not hit ( which it might not be ) then desired level is just original_level
An additional note , it will always be true that the abs ( NS ) continues to grow in size from the min ( abs ( NS )) level . . .in this case i have it as 0.0000 but may be some small number other than zero
You want to use ` clip ` for this , firstly find the indices of your upper and lower clip values using ` idxmax ` and ` idxmin ` and then pass these values as the params : #CODE
Sorry you're trying to apply to a groupby object ? Does it work after calling ` reset_index() ` ? really you should post a new question as chaning the requirements loses context for the changes .

Notice that if you unstack the ` id ` index level of ` df ` then you get : #CODE

You could do a pivot such that the male and female pay for the same job are on the same row . Then you can visually compare , or run other row-based code . #CODE
see also : Pandas Reshaping and Pivot Tables

I'm plotting several histograms using Pandas ` hist ` method . #CODE

Interested to apply multivariate hexagonal binning to this and different color hexagoan for each unique column " ball , mouse ... etc " . scikit offers hexagoanal binning but cant figure out how to render different colors for each hexagon based on the unique data point . Any other visualization technique would also help in this .
interested to apply hexagonal binning to this #URL

Columns name dropped on append in pandas
I need to join dataframes like the ones shown above , but columns name ` hello ` ( it is not a column as it may seem ! ) is dropped on append operation . Why ? I have to force it like this : ` pv.columns.names = df4.columns.names ` #CODE
UP D: ` concat ` / ` append ` drops axis 0 / 1 names when they differ . So , i think , forcing ` .names ` after ` append ` is the best solution now .
i think names should be preserved on append if they are the same , otherwise dropped silently
It is similar to ` df1.update ( df4 )` but using a ' outer ' join manner for unseen records in ` df1 ` .

So the above uses ` rolling_sum ` and ` shift ` to generate the previous 2 years sum and we then divide the citations value by that value .

How can I either drop completely empty columns or delete all columns bar the first 11 ?
To drop all the columns after the 11th one .

Pandas Merge , can't get my dataframes to match up on the key correctly ?
I have two dataframes that I want to merge on a column named Channel ID , this column exists in both dataframes , of course . When I run the code the rows of data are not matched up correctly , not sure what the issue is . Here is my code : #CODE
@USER Good catch . Regarding a fix , matching on a float column is a bad idea 99.99 % of the time ( or more ) . I would suggest using ` astype ( int )` instead of ` convert_objects() ` . Since this is an inner join , rows with NaN for ID can be dropped ( so you don't need the float type to handle NaNs anyway ) .
I'm confused . You printed out ` rc.head() ` and that looks right to me . What is wrong with it ? It appears to contain variables from both datasets , so isn't that a successful merge result ?
Man , this has gotten too hard to follow . I would suggest reposting but being super clear in the following ways : post data before and after merge , show EXACT code you are running , and then show results after running code and EXACTLY what is wrong .

with ` Date ` being a ` pandas.tseries.index.DatetimeIndex ` . I transformed it with the help of @USER -tavory using pandas melt function : #CODE

drop rows with equal mult indexes
Is there a short-cut to drop rows with equal multi indexes ? In other words , is there any way to compare indexes without convert them to columns ? Here goes how I'm doing so far , but I fill there is something better .

merge the dataframe on ID #CODE
The ` merge ` did the trick , but I thought it was more usefull to just do a ` dfMerged.dropna() ` after the merge and that will be the set with the difference .
yes , essentially , the answer was really about the ` merge ` method , which allows you to sql-like joins . The rest of the answer shows one way to compare fields . There are definitely more appropriate ways to achieve certain end results .

I'm not sure I fully understand why that happens ( I guess pandas wants to map unique values to unique values ? ) . But regardless of that , there's an easier way to do it anyway : #CODE

Pandas multilevel concat / group / chunking

loc and iloc helps .
Are you sure you don't mean ` range ( 1 , len ( DF )): ` ?

Python Pandas DataFrame resample daily data to week by Mon-Sun weekly definition ?
Let's say I resample the ` Dataframe ` to try and sum the daily data into weekly rows : #CODE

The preparatory code generates a dataframe with the same structure as yours . Interestingly , I was unable to name the columns ` animal ` and join with ` suffix = ( "" , "")` -- that throws an error ` ValueError : columns overlap but no suffix specified : Index ([ u'animal '] , dtype= ' object ')` . @USER ' s comment to rename the columns works just fine . #CODE

Is there a way to drop columns in a Dataframe with column names having a particular letter as I wasn't able to find any information on this ? I currently have the following code , which creates a dataframe that look as follows : #CODE
I want to drop all column headers having the letter ` F ` in them . I was planning on doing it using ` df.drop ([ df.columns [[ column_names ]]] , axis=1 )` , but there are so many that I was wondering if there is an easier way to do this . #CODE

I have a dataframe of data that I am trying to append to another dataframe . I have tried various ways with .append() and there has been no successful way . When I print the data from iterrows . I provide 2 possible ways I tried to solve the issue below , one creates an error , the other doesn't populate the dataframe with anything .
I think you just want to ` pd.concat ([ df1 , df2 ])` . No need to iteratively append one line at a time .
Have you looked at the [ examples in ` append `] ( #URL ) ?

It performs a sort of merge with the existing values ( according to the indices ) , and those are floats , so it must convert these too .
You should replace it with something else , depending on your problem ( ` fillna ` ? ) .
That got it ! thank you but I still can't merge the dataframes correctly :(
You could drop rows from ` frame ` where ` Channel ID ` is NaN : #CODE
As Ami Tavory explained , you can't drop the NaNs solely from ` frame [ ' Channel ID ']`

I wish to have the values of ' Vol ' cap to the level where another column ' NormStrike ' hits a trigger ( in this case abs ( NormStrike ) > = 2 ) . This new column , ' Desired_Level ' , created while leaving the ' Vol ' column unchanged . The first cap should cause the Vol value at index location 0 to be 16.2 because the cap was triggered at index location 1 when NormStrike hit - 2.576719 .
I am looking for a generic solution , that works away from the lowest abs ( NormStrike ) level in both directions to hit both the -2 and the +2 trigger . If it is not hit ( which it might not be ) then desired level is just original_level
An additional note , it will always be true that the abs ( NormStrike ) continues to grow in size from the min ( abs ( NormStrike )) level as it is a function of abs ( distance from spot to strike )
There are 2 issues , first , it did not work after groupby and second , if a particular group of data does not have a NS value that exceeds the " clip " value then it generates an error . The ideal outcome would be , in this case , nothing is done to the Vol level for the particular Symbol / DTE group in question .
You can try this to see whether it works out . I assume if the clip has been triggered , then NaN will be put . You can replace it by your customized choice . #CODE
thank you much . . .the remaining issue is that I do not know how to replace NaN with the closest Vol level . In essence , the first 2 NaN values in the above output should read 18.0 and 18.0 . The next 2 NaN values ( index level 7&8 ) should read 29.1 and 29.1
It looks like you want to build a cross-Strike curve about desired vol level . The most common way to fill na is to use a cubic interpolation , so the curve is smoothed . Similar to the situation where you have to interpolate yield curve .
new development that I do not think warrants a new question . Using Jianxun's routine i am forces to have the trigger be abs ( 2 ) on both sides . How would I have the trigger be -4 on the downside and +2 on the upside ?

Is it because you're printing ` loc ` and not ` iloc ` ?
No , iloc is used when you are integers indexthis case ' i ' while loc is used when you want to use label index here ' distance '

and then ` unstack ` to move the ` type ` index level into the column index . You don't have to worry about the ` v ` values -- where the indexes go dictate the arrangement of the values .

I get this error for every additional table ( class ) included in my join . The message always refers to the foreign key .
Handling Duplicate Columns in Pandas DataFrame constructor from SQLAlchemy Join
These guys seem to be talking about a related issue , but they use a different pandas method to bring the dataframe in and want to keep duplicates , not drop them . Anyone have thoughts on how to implement a similar styled function , but drop the duplicates as the query comes back ?
It's really telling you that there are columns with duplicate names , even if the columns are in separate tables . In most cases this is innocuous as the columns are simple the join keys . However , I have encountered cases where the tables contain duplicately named by distincted populated columns ( ie a teacher table with name column and student table with name column ) . In these cases , rename the pandas dataframe with an approach like this , or rename the underlying database tables .

To state it again , somewhat differently : ` doc ! [ CDATA [ foo ]] / doc ` is exactly the same as ` doc foo / doc ` . What's different about a ` CDATA ` section is that everything inside it is automatically escaped , meaning that ` ! [ CDATA [ hello ]]` is interpreted as ` lt ; hello gt ; ` . However -- you can't tell from the parsed object tree whether your document contained a ` CDATA ` section with literal ` ` and ` ` or a raw text section with ` lt ; ` and ` gt ; ` . This is by design , and true of any compliant XML DOM implementation .
If you want to read from a file , replace ` doc ` with ` open ( filename , ' r ')` .

Converting multiple columns to categories in Pandas . apply ?
Why does ` apply ` ( ` axis=0 `) return a Series even though it is supposed to act on the columns one by one ?

Panda's Write CSV - Append vs . Write
I would like to use pd.write_csv to write " filename " ( with headers ) if " filename " doesn't exist , otherwise to append to " filename " if it exists . If I simply use command : #CODE
The write or append succeeds , but it seems like the header is written every time an append takes place .
How can I only add the header if the file doesn't exist , and append without header if the file does exist ?

This is done automatically by ` to_sql ` . If you want to overwrite the used column types , see the [ ` dtype `] ( #URL ) keyword argument . And if you want to customize this even more , probably best to make the sql table manually , and then use ` to_sql ( ...., if_exists= ' append ')` to append the data to that created table .

perhaps ` np.arange ( len ( df ))` may be more optimal ?

It looks like all you're doing is just filling / adding / merging the quarters data , why not just do a merge ` df.merge ( df_h , on= ' Country ')` ?
I don't understand what you're saying , but basically you should be able to merge the data , can you post raw input data for both dfs and the desired output

I have tried different things , like Graphs and then taking the degree of the nodes , and pivot tables to express the number , but I want an efficient way that will allow me to continue with the processing of the domain after the if occur > 2 statement .

I'd suggest that you use ` groupby ` / ` apply ` returning a ` Series ` ( see Returning Multiple Values From Apply ) .
The ` concat ` seems to be on the wrong axis
Note the ` axis=1 ` - it means to concat horizontally , which I believe is what you mean here .
It was a typo , the concat is on the axis=1 .

I have the following dataframe ` df ` where I am trying to drop all rows having ` curv_typ ` as ` PYC_RT ` or ` YCIF_RT ` . #CODE

can we use A.shift ( 1 ) in a counter manner that dynamically stores updated results while looping ? as in parallel shift adder ?

I think the way you did it is probably best . I'm not sure ' vectorized ' has any real meaning in your situation . To the extent where you would gain speed , I think you'd have to already have a square dataframe with rows and column in the same order . Given that neither of those conditions apply here , I don't see a reason to change what you have .

You could also do it with apply , but it will be slower than the ` np.where ` approach ( but approximately the same speed as what you are currently doing ) , though much simpler . That's probably a good example of why you should always avoid ` apply ` if possible , when you care about speed . #CODE
You could also do this , which is faster than ` apply ` but slower than ` np.where ` : #CODE

I think you can do with with a melt , a sort , a date parse , and some column shuffling : #CODE

Whats the proper way to append the second DataFrame ?

Merge a csv and txt file , then alphabetize and eliminate duplicates using python and pandas
To sort and drop duplicates : #CODE

Thank you so much . I learned a lot from this . Reset index . Set ` Date ` to ` object ` type . Return ` Date ` part of the Timestamps . Set last row in ` Date ` column to ` Total ` ( I used this instead : ` df.ix [ len ( df [ ' Date ']) -1 , 0 ] = ' Total '`) . Set ` Date ` column as the ` index ` . ` Drop ` ` Date ` column from dataframe since it's now the ` index ` . That's a lot of work to set one index row label ! At least it's a solution . Thank you for your help .

Creating pivot table in Python then exporting to excel
I'd like to know if it's possible to create a pivot table in Python and then export that pivot table with all its sorting properties into excel . Note I don't want just to values to be exported , but the entire pivot table with all its properties .
So when I go to the exported excel pivot table , I can still see its report filter , column label , row labels , and field list . It should be as if I manually created the pivot table .
Since you added a Pandas tag to the question do you mean is is " possible to create a pivot table in ** Pandas ** and then export that pivot table " ? If so that isn't currently possible with the Excel writers supported by Pandas .

Replace wordCnt by a pd.DataSeries : #CODE

The stack and group by sum are just the same .

Actually I'd like to insert JPEG images without decoding ( to save place ) into a data frame ( pandas ) when I got stuck at the phase of loading the images properly to able to read back them with PIL . #CODE
` img = np.asarray ( map ( ord , img ))`
Thanks for the hint , however mahotas open images with decoding , as I wrote above I'd like to read it without decoding and insert it into an array ( actually into pandas DataFrame ) and save like that , and then read it back as array and only then decode it to an image .

Then I would read the new data which is a separate file which is always Month to Date values ( all of June for example , this file is replaced daily with the latest data ) , and concat them to the old dataframe . #CODE
My only solution is to read the entire file ( df_old along with the new data I added ) and then drop duplicates and then overwrite it again . This isn't very efficient .
Have you seen INSERT OR IGNORE ? ( #URL ) Note that rather then keying off of the unique tuple ( Date , EmpID , CustomerID ) , you'll need a unique key column or maybe just an integer primary key in the Sqlite table .
` INSERT OR IGNORE ` to add data , and any duplicates will be ignored .

@USER I think that is a deprecated alternative way to drop duplicates .

I would imagine this should be solved with lambda map , but can't figure out how .
I think you might want to use a merge , something like : #CODE

Also , wouldn't it be simpler to simply append the ` ID ` to the ` Lower File Name ` in order to generate a unique name ? You wouldn't need the solution above and don't even need to check for duplicates , assuming the ID is unique .
Thanks Alexander ! I had to make one modification as it was appending " None " to the non-duplicate file names : ` df.apply ( lambda x : ' { 0}{1} ' .format ( x.ID if x [ ' Lower File Name '] in duplicates else '' , x [ ' File Name ']) , axis=1 )` And yes , it would have been much easier to just append the ID in all cases but unfortunately had to match another file in that format .

Pandas DataFrame casting to timedelta fails with loc
Another strange thing , if I filter for one site , and drop the Region so that I end up with a single-level index , it works ... : #CODE

Your problem is that your data contains ` NaN ` values so you need to drop them using ` dropna ` first : #CODE

You can even replace NaN in different columns with different values .
so my example would is really simple , but I have a another column for Male or Female . I would like to replace NaN with different values based on the gender column .
In this case you will have to explicitly iterate over the df and see if you find ` NaN ` then replace it based on the other column :

What s the best way to create and then merge to an existing Dataframe in a loop ? I have a log file ( say FILENAME1 ) which generates txt files giving me stats of interest . I have a script that loops through and opens each txt file and generates a DataFrame using pd.read_csv . I then paste each Dataframe to Excel using xlsxwriter .
My loop needs work as I am overwriting the Dataframe each time I open a new txt file as the merge function isn t working as I d hoped . But I am looking to merge each iteration generated Dataframe with the previous iteration dataframe ... but on a per txt file basis
In what sense do you want to " merge " the dataframes ? Do you want them to become part of one large dataframe ? That's what merging suggests , but the output you show has separate tables .
Hi ASGM , ideally i'd like to merge each dataframe based on txt file name , so yeah two dataframes if two txt files . However with my loop solution i'm not sure this is possible , and may have to resort to one big dataframe . Unfortunately I went along the loop path to make it more generic and easily expandable if I wanted to add more txt files later
Sorry , I'm still confused - what do you mean by " merge " the dataframes if you still want two separate dataframes at the end of the process ? Two files , two dataframes - where is the merging taking place ?
Simple answer : put each new DataFrame into a dictionary , with the iteration as the key . Then merge them at the end .

you want to declare a list of dfs outside your loop and then concat at the end so ` df_list =[ ]` and then after ` df =p d.read_csv ( url , sep= ' , ' , skiprows=2 )` do ` df_list.append ( df )` and then outside the loop ` frames = pd.concat ( df_list )`
You should declare a list outside your loop and append to this then outside the loop you want to concatenate all the dfs into a single df : #CODE
I must say that for me it is confusing to use append and concat a the same time . I always thought that ` append ` adds new Data Frame as a new columns , and ` concat ` rows or columns if set ` axis=1 ` .... Anyway thank you !

I have the following two tables and I am looking to insert a dataframe ` df ` into ` Table4 ` only if the ` parametrized_fl ` column in ` Table2 ` is equal to 1 . #CODE

Pandas Apply function on Column

How to standardize / normalize a date with pandas / numpy ?
My question is , how can I standardize / normalize data [ ' dates '] to make all the elements lie between -1 and 1 ( linear or gaussian ) ??
UNIX timestamps that we can normalize using a ` MinMaxScaler ` from ` sklearn ` #CODE

You simply need to ` stack ` and then call ` to_dict ` : #CODE

Used the following to unstack the data to prep for plotting #CODE

I was only able to test this now , and it does solve my problem ( I just need to transpose the data frame at the end ) . Thanks a lot !

` ( null ) Codon Freq minmax Codon Freq minmax ... `

So the above uses the vectorised ` str ` methods ` strip ` , ` extract ` and ` isdigit ` to achieve what you want .

Thanks ! ` pv.reset_index() .groupby ( " ??? ") .filter ( lambda g : len ( g )= =1 ) .index .values ` returns numpy array

The pandas grouped function seems to only operate on one column at a time but I want to generate the statistic on all columns in my df . For example , I can use the function grouped [ ' C '] .agg ([ np.mean , len ]) to generate the statistics on column ' C ' but what if I want to generate these statistics on all columns A - F ?
Is there any easy way to do the group by with aggregation in a single command ? If not , is there an easy way to iterate over all columns and merge in new aggregation statistics results for each column ?
You don't have to call ` grouped [ ' B ']` ` grouped [ ' C ']` one by one , simply pass your entire groupby object and pandas will apply the aggregate functions to all columns . #CODE

maybe simpler to replace first line with ` df3 = df1 [[ ' Lat1 ' , ' Lon1 ']]` so you don't need to drop ' tp1 ' at end ?

Then it should be clear why it moans , the index values are repeating and pandas is trying to align the index against your original df , to get around this you can get the raw values as a np array by calling ` .values ` attribute : #CODE

You'd have to expand the ranges to generate a list / array of all index values , you can then pass to ` loc `

I have two data frames that are both multi-indexed on ' Date ' and ' name ' , and want to do a SQL style ` JOIN ` to combine them . I've tried #CODE
After resetting the index , convert the dtype using ` df [ ' date '] = pd.to_datetime ( df [ ' date '])` you should then be able to merge
The reason you cannot join is because you have different dtypes on the indicies . Pandas silently fails if the indicies have different dtypes .
Now you can merge the dataframes on their index : #CODE

python pandas merge with any one value
i could not find any option to do this fuzzy match , i see merge works with an exact match ??
This is a little messed up , firstly split the ' codes ' column on the separator and ` apply ` ` map ` to each element and pass the other df with the index set to ' code ' , this will perform a lookup , we reduce this to just the codes that match .
We then generate an intermediate df from this , we reapply the splitting to the original df to add the ' pcode ' column , this then allows us to ` merge ` : #CODE

You needed to add an additional `' | '` to join your terms : #CODE

` resample ` is your friend . #CODE
The ` TimeGrouper ` method and the ` resample ` method produce identical results . Yours is probably the cooler of the two because it's a proper built-in part of the ` DataFrame ` API . ( I've timed them and the speeds are identical . )

Map function across multi-column DataFrame
I would like to map a row-wise function across its columns #CODE
Is this really what you want to do ? Ideally you want to avoid using ` apply ` if there is a vectorised method , so just ` df [ ' x '] + df [ ' y ']` would work
You can apply a function row-wise by setting ` axis=1 ` #CODE

I think that's the problem , I get same error message for ` np.array ([ 1 . ]) & np.array ([ 0 . ])` but it's fine if I replace those with integers or booleans .

` groupby / apply ` to a columns of ` df ` unless the columns or levels you group by

I'm creating a tkinter gui that will take user input for a variable that then gets passed to SQL and the queried data ( in this case a single column data frame and boxplot ) . However , at this moment I can not find a means of displaying my pandas dataframe in the tk gui . I have not found any module or means of displaying this , and I've spent hours going through possible solutions to no avail . Only thing I need is for the dataframe to display in the gui and be re-rendered each time I change the dataframe through the user input function . My code atm is : #CODE

Select your columns from the DataFrame and then apply your function ( possibly a ` lambda ` expression depending on usage ) . #CODE
@USER Notice the ` abs ` function on the days . I first differenced them , but noticed that more were passing my filter than expected due to negative days ( e.g. -10 < 5 => TRUE ) .

The easiest way I can see to do this is to join them all into a single DataFrame , sort the columns by time , then shift and subtract to get the delta : #CODE

More detailed discussion is in this answer too . Basically , openpyxl version 2 deprecated the style-conversion functions that map dictionary style definitions to the new openpyxl api , but Pandas still uses the old style , and for some reason the deprecation pass-through function errors out .

If you have pandas version > = ` 0.17.0 ` you could try to apply ` pandas.to_numeric ` for each column ( or may be you know suspicious columns ): #CODE
We could see that it's slower then convert_object . Let's pass ` raw=True ` for ` apply ` : #CODE

I am using the below code which gives me the summary of count in the pivot table , #CODE
but what i want is the % of row calculation as in excel pivot when you right click the pivot and select " show value as -> % of Row Total " . Since my Document is a non-numeric value i was not able to get it .
i am trying to manipulate the pivot data which will give me the row total , not the data from the dataframe and what i wanted is " % of row total " . And also most importantly all my data are non-numeric values ...
you can actually just pass ` aggfunc=len ` , since ` len ` is already a function :)
Hi maxymoo in the link you have given they are manipulating one of the column from the dataframe , but my question is different i am trying to manipulate the pivot data which will give me the row total and what i wanted is " % of row total " . And also most importantly all my data are non-numeric values ...
Then you can basically use the solution @USER linked to , but you need to use ` iloc ` or similar b / c the table columns are a little complicated now ( being a multi-indexed result of the pivot table ) . #CODE

can't you not simply just check if the ` ix ` returns a Series or String ?

I am trying to plot a dynamically size able bubble ( scatter map ) . When I try to plot with random data I can very well plot . But when I am trying to parse my input file I am not able to plot .
OK , I am not sure exactly w.r.t. seaborn . You may want to post a followup question focusing specifically on that . You can definitely put multiple aggfuncs in a pivot table , I'm just not sure offhand about translating all of that to a heatmap .

If you only want the ' CREATE TABLE ' sql code ( and not the insert of the data ) , you can use the ` get_schema ` function of the pandas.io.sql module : #CODE
You can do the same way to populate your table with INSERT INTO .

Replace a value in MultiIndex ( pandas )
How can i replace ` [ " x2 " , " Total "]` with ` [ " x2 " , " x2 "]` leaving ` x1 ` as is ? ` .rename ` can replace all `" Total "` values , not just the one I need . #CODE

I am trying to perform a some arithmetic operations in Python Pandas and merge the result in one of the file . #CODE
If the ` col ` with time from ` path_1 file ` is present then look for ` col [ ' Nos ']` and then replace the ` NAN ` with the subtracted values respect to that ` col [ ' Nos ']` .
Merge the dataframes together ( equivalent to a SQL left join or an Excel VLOOKUP

Just run this on the commandline if you are using linux . It will replace all the the spaces with ` , ` .
` apply ` might work well for you here : #CODE
and avoid the need to use the cumbersome ` loc [: , ' val1 ' : ' val3 ']` syntax

Python pandas : Compare rows of dataframe based on some columns and drop row with lowest value

I made a nice small h5 table and it was perfect until I tried to append more data to it ( literally just one day of data since I am receiving daily raw .csv files ) . I received errors which showed that the dtypes were not matching up for each column although I used the same exact ipython notebook .
Is my hdf.put code correct ? If I have append = True does that mean it will create the file if it does not exist , but append the data if it does exist ? I will be appending to this file everyday basically .
I have no idea what blosc compression is . Is that the most efficient one to use ? Any recommendations here ? This file is mainly used to quickly read data into a dataframe to join to other .csv files which Tableau is connected to

I have a list of servers , and each server has a number of patches that apply to that server . The excel looks a bit like this : #CODE
I realise there have been a lot of similar questions on here , but I am new to pandas and can't seem to apply the right syntax to my problem using the documentation - can anyone help ?

I'm trying to figure out how to apply a lambda function to multiple dataframes simultaneously , without first merging the data frames together . I am working with large data sets ( > 60MM records ) and I need to be extra careful with memory management .
My hope is that there is a way to apply lambda to just the underlying dataframes so that I can avoid the cost of stitching them together first , and then dropping that intermediary dataframe from memory before I move on to the next step in the process .
In my example code I start with three dataframes [ df1 , df2 , df3 ] . I then need to create an intermediate dataframe called dfConsolodated that is simply the three underlying dataframes all joined together . This is fine with the toy problem , but when I am operating on > 60 million records per data frame , I can bloat up the memory very quickly with the dfConsolodated table . The real goal , is to conserve system resources by avoiding the concat in the first place .

Merge the two dataframes into one : #CODE
However , it is slower since the ` groupby / apply ` is doing an addition and division once for each group , whereas #CODE
However , it is slower since the ` groupby / apply ` is doing an addition and division once for each group , whereas #CODE

So far I can think of ` apply ` followed by ` itertools.chain ` , but I am wondering if there is a one-step solution .
It is far more typical to normalize your data in Python before you send it to Pandas . If Pandas did do this then it would probably only be able to operate at slow Python speeds rather than fast C speeds .
The introduction of ` NaN ` is because the intermediate object creates a ` MultiIndex ` , but for a lot of things you can just drop that : #CODE

Group by uid and apply ` value_counts ` to the msg column : #CODE
Apply ` groupby ` on both ` id ` and ` msg ` , and then sum the ` count ` of each : #CODE

Perform a ` groupby ` aggregation on the first df , call ` sum ` and then ` merge ` this with the other df : #CODE

The basic key to making stuff using pandas faster is to replace loops done iteratively in python with array operations . But if this usage is at all a surprise , you really need to stop and work through one or more pandas tutorials .

and then pivot #CODE

more over after I write this to csv file how do I transpose all at once ?

Heat Map Seaborn fmt= ' d ' error
I can plot the Heat map with Seaborn very well and with suggestion can get annotation . But I see a new problem now .
Second doubt is now that in ` pivot ` I am giving ` value= ' 00:00 : 00 '` but I need it to dynamically read the last column from the file .
If list of functions passed , the resulting pivot table will have hierarchical columns whose top level are the function names ( inferred from the function objects themselves )

write in chunks as you go and append to the csv , use ` mode = ' a ' , header=False ` after the first write . You can
How did you check if anything was written ? The ` if os.path.isfile() ` is not going to cause much overhead and is easy to apply the logic
Actually the best way would be to create a new file each time the program starts , and then append for each pass in the function .
Another option would be to write to a NamedTemporaryFile then use shutil.move to continually replace a file on your file system every n rows or whatever period you want
The argument " a " will append the new df to an existing file and the file gets closed after the with block is finished , so you should keep all of your intermediary results .
This is a possible solution that will append the data to a new file as it reads the csv in chunks . If the process is interrupted the new file will contain all the information up until the interruption . #CODE

I am not sure what I am doing wrong here , I am simply trying to call a function with a if-then-else filter in it and apply to a dataframe . #CODE
You want to apply it to each row , this will do what you want using apply and a lambda : #CODE
` df.apply ` has performance comparable to a Python ` for-loop ` . Sometimes using apply or a for-loop to compute row-by-row is unavoidable , but in this case a quicker alternative would be express the calculation as one done on whole columns .
thanks for the added color . . . .I just checked , in my application , this is 5.3 times faster than my " def " solution and 2.8 times faster than the df , apply approach . Thank you

When I read an excel file in IPython ( or Jupyter to be exact ) , the dataframe seems to be read ok , but I can't display it or work on textual columns on it ( for instance , to merge with another excel when the key is the textual field ) , because I get a " #CODE

What I need is to look on both sides of min ( dist ) for the first occurrence of a 0 . In the first group ( Chicago on 5 / 25 / 2015 ) the downside trigger occurs at index = 3 and therefore all values with a larger ( abs ( dist )) on the downside will have the same b level as the value just before the trigger ( index =4 ) . The same thing happens on the upside starting from min ( dist ) and going up . The upside trigger is at index level = 11 and further upside new_b values will all be set to the value of b at index = 10

Slicing dataframe , but len is wrong
I am slicing a dataframe along its indices and selecting a response variable , but the result of len is not correct :
Any thoughts ? I thought len should return 1000
Why ? Isn't len ( df.loc [: 1000 , ' a '] taking the first 1000 rows of df ?
to select the first 1000 rows ( assuming ` len ( df ) = 1000 `) .

I am trying to merge the ` Date-time ` column of all the files into a big data file with respect to ` Ids ` #CODE
Fill in empty place with ` NAN ` and if next file has that value then replace the new value with ` NAN `
@USER . I don't quite understand . You can just replace those buffer ` io.StringIO() ` with your ` file path ` and it should work .

Here is my approach . Can someone give me advice on how to translate this basic idea to something clean ? There must be a simpler way to do this !

It is because , if we merge too list with some unique values in each , there will be identical values in each list which will turn into one value instead of two after the set() function .

Apply curve_fit within a loop
Now , I would like to apply a fit to each frequency group ( 400 , 800 and 1200 ) and do this efficiently within a loop . The first attempt is : #CODE
I hope , that there is someone , who could explain , if it is possible to apply the curv_fit routine within a loop and if so - how .

In the above example , how can I select from 100 to 150 and 200:300 without using concat ? Is this even possible ?
The above operation , has a bottleneck when using pd.concat and can eventually been speed up using np.vstack ... but still I'd like to select the two ranges all at once without copy the underling data as concat would do .
concat #CODE
slice and drop #CODE

You can use ` pd.cut() ` to cut your time-series index into chunks , and then use ` groupby ` to perform your customized calculations . #CODE

You should be able to do this using ` apply ` which is for applying a function to every row or every column of a data frame . #CODE

I am completely new to python and Pandas of course . I am trying to run a function " get url " which is function to get the complete extended url from small Url . I have a data frame in python consists all the short URLs . Now I am trying to do with following ways . One is to use " for " loop which loops and apply function on all the elements and will create a another series of extended URL but I am not able to , dont know why , I tried to write it like #CODE
If the short urls are a column in the pandas dataFrame , you can use the ` apply ` function ( though I am not sure if they would resume on error , most probably not ) .
Hi Anand - Thanks for help . but it is working same as apply function . as soon as it faces a error it pass completely . error is like HTTPError : HTTP Error 404 : Not Found ....

Please edit your post with your code and full stack trace , telling me the error without the full context is a waste of my time

" taking each entry in the column and retrieving time-series data from files with the same names as the entries " This sounds like you want to do a " join " operation . In pandas this is done with the ` merge ` function .

@USER I think i may have deleted the post by accident . I thought i was deleting my comment . can you repost it ? I am sorry ... I am only on the stack overflow for a few weeks .

Pivot Tables in Pandas- Unorderable Types
I just want to create a simple pivot table on this to calculate the average score for each manager for each day . As such , I should have a column for each manager name .
Why am I getting this type error ? It should be a simple pivot off of a string field using mean as the automatic aggregate function on a float field ?
Thanks . So pivot and pivot_table do work in creating appropriate output . But I'm still getting the unorderable types error message .

I'm trying to fill a table in SQL with data in the dataframe below . The code functions as expected when I use other dataframes with exactly the same Columns and datatypes , but for this dataframe alone the error ` sqlalchemy.orm.exc.UnmappedInstanceError : Class ' builtins.list ' is not mapped ` results from the line ` session.add ([ yc_node_hist ( fk_yc_update = listofindices3 [ i ] , curve_date = row1 [ ' Date '] , tenor = row1 [ ' Year '] , Abrv = row1 [ ' VALUE '])])` . I am using ` automap() ` to map the table ` yc_node_hist ` to a Class . My main problem is that I can't understand what exactly the Error means . #CODE

In the try , except statement you should replace

There are 2 methods here , first is the assumption that non- ` NaN ` values should be set to ' new ' using ` notnull ` : #CODE

Thanks , Lee . I had to study about cut because I was not familiar with it . This approach won't work because in our database system if an item has only one price break then the 3 columns will have the same values . Sorry for not clarifying that .

` data_q = data.groupby ( pandas.TimeGrouper ( freq = ' 3M '))`

Case insensitive dictionary for Pandas map lambda function
Below , I use ` lambda x : ` in a function to ` map ` value to a ` pandas ` column if they show up in the dictionary ` benchmarks ` .
Ah yes , I did try that also . A regex match is made for the word in ` symbol ` but doesn't translate with the dictionary .

CANCEL_FLAG should also cut the chain because a value of 1 designates that the contract was cancelled .
The copy , merge , fillna , and rename code is repeated for 5 merged dataframes then : #CODE

I have a maybe a hopefully easy question that I wasn't able to find an answer on stack .

Edit to clarify : I don't believe pandas has a direct analog to setting a view in numpy in terms of both speed and syntax . ` iloc ` and ` loc ` are probably the most direct analog in terms of syntax and purpose , but are much slower . This is a fairly common situation with numpy and pandas . Pandas does a lot more than numpy ( labeled columns / indexes , automatic alignment , etc . ) , but is slower to varying degrees . When you need speed and can do things in numpy , then do them in numpy .
I think in a nutshell that the tradeoff here is that ` loc ` and ` iloc ` will be slower but work 100% of the time whereas ` values ` will be fast but not always work ( to be honest , I didn't even realize it would work in the way you got it to work ) .
Edit to add : The following seems to work ( albeit with a warning ) even with column ' g ' as a float . The speed is in between the ` values ` way and ` loc / iloc ` ways . I'm not sure if this can be expected to work all the time though . Just putting it out as a possible middle way . #CODE
@USER " Misses the point " is a little harsh ! ;-) I guess if you want me to explicitly state it , I don't believe it is possible to do what you want without making a copy . Btw you might mean * set * rather than * select * in your question title ? Anyway , you're trying to apply a numpy concept to pandas and it doesn't work like that . My edit was merely an attempt to find something faster . I'll edit the answer to make this more obvious .

Use ` shift ` and ` np.log ` : #CODE

@USER I've modified the code . You can replace ` StringIO ( raw_data_string_buffer )` in ` pd.read_csv ` function with your own file path . Let me know whether this works for you .

You can pass the dict values to ` concat ` , example : #CODE

It's not * totally * clear , but are you looking for e.g. ` kwargs.get ( key , defaults [ key ])` ? Could you cut this down to a simpler example ?

Try to replace #CODE

Pandas DataFrame Pivot Using Dates and Counts
I know that there is a nifty pivot function for dataframes from this documentation #URL for pandas , so I've been trying to use df.pivot ( index= ' recvd_dttm ' , columns= ' CompanyName ' , values= ' NumberCalls ')
You need to aggregate the data before creating the pivot table . If there is no column name , you can either refer it to ` df.iloc [: , 1 ]` ( the 2nd column ) or simply rename the df . #CODE
@USER The ` result ` only has three columns [ Month , CompanyName and NumberCalls ] . ( there is no columns named ` recvd_dttm ` and that's why it throws the error ) I think you can replace the last line of the code with ` result.pivot ( index= ' Month ' , columns= ' CompanyName ' , values= ' NumberCalls ')` and it should work . Let me know if this is not the case .

Darn . So maybe I need to just build out a dict of all the columns and map it all after the data frame is pulled ...

and then drop col and rename col1 .

These lines are causing a problem because I am using ` read_csv ` with ` converters ` , and the functions in the converters will dutifully throw an exception when they encounter invalid data , meaning I don't even arrive at a valid dataframe . I could change the functions to convert invalid data to ` NaN ` and then drop ` NaN ` s from the dataframe , but then I would silently be dropping erroneous data as well as those empty lines .
One way would be to simply clean the file yourself removing the trailing ;;;; , Also ` df.dropna ( how= " all " .. ` would only drop lines that had all nans . What do you want to do with the invalid data ?

I need to merge 2 csv files by one column called ' name ' . Column ' name ' has mixed types .
I've been able to merge these files but only losing all the rows containing special characters ( e.g. my merged file will only contain Aahl , Peter ) . My code looks like this : #CODE
Your problem had nothing to do with encoding , you just used the wrong type of merge . You wanted to perform a ` right ` merge : #CODE

Take a view of the desired left and right side dfs , then ` rename ` the columns and then ` concat ` them and ` sort ` on ' id ' column : #CODE

Boxplot of column_c by column_a ( shows five-number summary and outliers ) #CODE

loc = self.get_loc ( k ) File " / usr / local / lib / python2.7 / dist-packages / pandas / core / index.py " , line

If the value of ` df.y [ i ]` is always greater than any ` df.x [ n ]` value , then append as ` False ` to ` df.new [ i ]` .
The following works but this really is a loop as it uses ` apply ` : #CODE
That's correct ` apply ` will not scale well , one thing you could do is assuming that all values are not unique you could create a dict of the unique values as keys and the row position values as the values and merge this , this should perform slightly better , your issue here is that you are trying to compare a scalar against the entire df for every row value , there isn't a built in method for doing this so you have to call ` apply ` here

In pandas you can use ` loc ` to mask the df and pass the 2 boolean conditions wrapped in parentheses due to operator precedence as you have to use the ` ` operator when comparing arrays : #CODE

pandas : merge on column of collections.Counter ( or even just dict ) objects ?
I need to perform a merge of two pandas DataFrames using columns with ` collections.Counter ` objects ( #URL ) . The merge raises a weird error . See executable code example below . #CODE
I need to merge based on values of two pairs of columns . In the real data they are Start1 , End1 , Start2 , End2 . End2 > Start2 , End1 > Start1 . The example is with a subset of my real values . The problem is that in two datasets may be a situation that ( Start1_1 , End1_1 )= =( Start2_2 , End2_2 ) and ( Start1_2 , End1_2 )= =( Start2_1 , End2_1 ); I want these lines to be merged as well ( the second number denotes the dataset ) . I thought using such counters should be the easiest solution , and I am pretty sure there will be no false positives this way .

Merge with pandas creating new columns ?
I have a dataframe that has multiple references to another ( as if it was foreign keys in SQL ) and so , I want to merge them , in a way that I can have all the info from the second dataframe in the first . Something like the following , remember using pandas in python . #CODE
And what I pretend to merge is in a single table like : #CODE
The merge option ( as I am using it at the moment ) is not giving me any result . Any idea on how to do this ?
OK , so you want to match on ' fk1 ' to ' pk ' AND ' fk2 ' to ' pk ' . It's best to put that in the question itself . Anyway , start with ` df1.merge ( df2 , left_on= ' fk1 ' , right_on= ' pk ')` , but you'll need to change ' fk2 ' column to fix the ` - ` , which may be your problem . Either make it a special valued int like ' 99999 ' for example , or a NaN although then it becomes a float and might have problems matching to ' pk ' . Anyway , try to get pk / fk1 / fk2 to be all the same type and it should be a straightforward merge .
Here it is a little bit easier to read and only filling na's on the columns added through the join : #CODE

@USER is there code not listed above that's altering the data frame ? For example , how are you apply a Month ` Series ` to the table ?

` master_count.csv ` : expected Output ( Append / merge ) #CODE
` master_count.csv ` : expected output after another execution ( Merge / append ) #CODE
When I Wrote a print statement It was same with no error and no answer . Then I had doubt may be I wrote wrong syntax . So I asked John to also include the csv file output . And I was wrong with my print or append syntax . Thank you so much both of you for all the help .
. . . and so on . The idea is that you should make a single column to hold the time information , and then for each preprocess , insert the new data into new rows , and give those rows a value in the time column indicating what time period they come from . You may or may not want to keep the initial rows with " None " in this table ; maybe you just want to start with the " 00:30 : 00 " values and keep the " master ids " in a separate file .

I think you're asking how to apply a function to all columns of a Data Frame : To do this call the ` apply ` method of your dataframe : #CODE

Is there a way for pandas to read from that tar file ? Can I specify a file I know exists within the file , or perhaps concat all of the files into one read ?

Or replace : #CODE

You can first convert the ` df ` to a multi-level index type and then ` unstack ` the level ` Class ` will give you what you want . Suppose df is the original dataframe shown on the very beginning of your post . #CODE

Typically you can process the chunk and add it to a list , and then after this for loop concat all processed chunks in this list together .

Have been trying to just replace the NaN values in my DataFrame with the last valued item however this does not seem to do the job . Just wondering if anyone else has this same issue or what could be causing this problem . #CODE

I am putting my data into a bokeh layout of a heat map , but am getting a KeyError : ' 1 ' . It occurs right at the line ` num_calls = pivot_table [ m ] [ y ]` does anybody know why this would be ?
The pivot table I am using is below : #CODE

This returned ` TypeError : cannot compare a dtyped [ object ] array with a scalar of type [ bool ]
After all the updates I think what the OP is after is a mask like ` mask.groupby ( df.ID ) .transform ( " any ")` after you drop the ID condition from your mask = line . ( BTW , belated happy-new-name ! )

Why not try ` np.where ` . It's column-wise vectorized operation and it is much faster than row-wise apply . #CODE

I've created a function that looks for rows that have a set of column values that are the same . It's ugly though : nested apply functions . Is there a better way to test if ` n ` number columns have the same value , and if so apply a function to them or add them to a dictionary ? #CODE

I am trying to make a heat map like this one from bokeh :

Ah , yes , that was a bug in 0.15 preventing you to use such an instantiated class ( which is the case when providing a keyword ) , see #URL You need 0.16 for that fix ( or you can apply it yourself as it is only a 2 line fix , see the linked PR )

` hist ` takes a 1-dimensional array . Does this work ? #CODE
( see the ` hist ` documentation for more ) .

I am trying to insert data from a dataframe ` df ` into a numpy array ` matrix_of_coupons_and_facevalues ` . However , I basically need to add the value associated with a row of ` df [ ' Coupon ']` into each column of a corresponding row of the array for as many columns as the number ` numberofcoupon_payments = row1 [ ' months_to_maturity '] / 6 ` . I get the error ` ValueError : could not broadcast input array from shape ( 1 , 2 ) into shape ( 1 , 61 )` in the line ` np.insert ( matrix_of_coupons_and_facevalues , row_no+1 , rowtobeadded , 0 )` and I understand why , but I don't know how to proceed .

An easy workaround would be to apply ` .values ` to the series first and then apply ` std ` to these values ; in this case ` numpy's ` ` std ` is used : #CODE

Just access the ` dt ` week attribute : #CODE

By default , when you ` groupby ` - ` sum ` a DataFrame , pandas doesn't assume that you want to do so for all the columns that are not of the classic numeric types . If you'd have a column of strings , it wouldn't try to apply the sum to them too .

Perhaps a workaround is to use transpose ` x.T.fillna ( x.mean ( axis=1 )) .T ` #CODE

` pd.Series ` have a clip method ( defined in pandas / core / generic.py ) . #CODE
` np.clip ` ( defined in numpy / core / fromnumeric.py ) defers to the first argument's ` clip ` method if it has one : #CODE

I did ` print ( len ( df.index ))` at the beginning of this section and got ` 12 ` printed , so I am slightly confused why ` IndexError ` raises .
Must the duplicate rows appear contiguously , or do you want to drop duplicates wherever they appear ?
It seems to me that you are iterating over rows , matching a condition and then drop rows based on the condition that is met . I don't think this is the optimum way of doing what you are trying to do .
To find the matching rows and drop them , I could do , #CODE

I have a bunch of tick data that I can successfully resample into time data using : #CODE
Also - when I choose to resample with smaller timeframes there are occasionally bars where the values are N / A because there was no price changes for that period . When this occurs I would like the previous close to be the value for OHLC on the current bar .

Thanks ! Was the error with my code because datetime.date works on a single row , and with functions working on rows one at a time I hvae to do apply ( fun() ) rather than fun ( apply() ) ?

What do you mean by " doesn't work " ? Does it raise an exception ( if so you should post the stack frame ) or output a result you don't expect ( in which case , you should post the output ) ?
If you try ` type ( pd.isnull ( df [ ' C2y '] [ 0 ]))` , you find that it actually returns ` type ' numpy.bool_ ' ` as opposed to ` type ' bool ' ` . So the identity test ` is True ` is yielding ` False ` , which is why you got ` NaN ` as output of the function .

I haven't quite understood what you mean by " fixture " in this context ( probably something obvious , but my English is not good enough and I'm not much into sports anyway ) . I think I do understand your code ( would be easier if you provided at least a few lines of your csv ) , and maybe you could use ` shift ` like in this [ SO answer ] ( #URL ) . Don't really understand the part after ` if fixture_matches <= 10 : ` though . Hope it helps !

There are several ways to add a new row . Perhaps the easiest one is ( if you want to add the row to the end ) is to use ` loc ` : #CODE
` loc ` expects an index . ` len ( df )` will return the number of rows in the dataframe so the new row will be added to the end of the dataframe .
An exception for this is that if you want all the columns to have the same values you are allowed to have that value as a single element in the list , for example ` df.loc [ len ( df )] = [ ' aa ']` .
Accidentally I found that this solution ( using loc instead of append ) is ** WAY FASTER ** so anyone should consider using it .
You should append Series or DataFrame . ( Series would be more appropriate in your case ) #CODE

I am trying the merge the datetime series with a repository data while grouping by name and summing the values . #CODE

Here is another alternative that might appear simpler ( though learning to apply functions to groups is a great idea ! ) #CODE
For information , Jianxun's answer is faster that this alternative : 1.7ms versus 1.87 ms per loop for mine ( for 1000 loops ) ( 10% diff )

I'd like to replace every value in column ' a ' by the majority of values around ' a ' . For numerical data , I can do this : #CODE
Here is one way to do it by defining your own rolling apply function . #CODE
@USER ` index=index ` forces the returned ` pd.Series ` to have index ` 2 , 3 ,..., 10 ` rather than default integer index ` 0 , 1 ,..., 8 ` . The last ` reindex ` part tries to align the index with original ` df ` and populates unseen index with ` NaN ` .

They lead to ambiguity . Code that you think works fine can suddenly fail on DataFrames with non-unique indexes . ` argmax ` , for instance , can lead to a similar pitfall when DataFrames have duplicates in the index .

not sure of another method other then a loop but to keep track of the values just append them to a new list .
Here is one way of approaching it , using the apply method to subtract the first item from all the other obs . #CODE

Python merge 2 lists / SQL JOIN
If I have 2 lists or data frame ( pandas ) in python how do I merge / match / join them ?
You want an ' outer ' ` merge ` : #CODE

In python 3 you need to use ` list ( map ( ... ))` , I guess .
You need to convert it to a list first . So just change ` map ( lambda ... )` to ` list ( map ( lambda ... ))`

How to resample a df with datetime index to exactly n equally sized periods ?
I've got a large dataframe with a datetime index and need to resample data to exactly 10 equally sized periods .
So far , I've tried finding the first and last dates to determine the total number of days in the data , divide that by 10 to determine the size of each period , then resample using that number of days . eg : #CODE
Another potential shorter way to do this is to specify your sampling freq as the time delta . But the problem , as shown in below , is that it delivers 11 sub-samples instead of 10 . I believe the reason is that the ` resample ` implements a ` left-inclusive / right-exclusive ( or left-exclusive / right-inclusive )` sub-sampling scheme so that the very last obs at ' 2015-02-03 00:00 : 00 ' is considered as a separate group . If we use ` pd.cut ` to do it ourself , we can specify ` include_lowest=True ` so that it gives us exactly 10 sub-samples rather than 11 . #CODE

I want to create a pivot table from a dataframe , but exclude rows which equal a certain value ( in this case , in the column that I'm aggfunc-ing on , but I'd like to have a more general answer ) .
1 ) just create a copy of the original df w / o those values , and pivot on that .
2 ) do that into the pivot table directly , like so : #CODE
[ EDIT ] To clarify , I meant how to set the name of the column where the aggregated values will be in the pivot table , during creation of the pivot , not afterwards , as you would regularly rename columns in any df .
@USER thank John . Just thought there might be a neater way . Anyway , that's just a side note for my question . The real question is how to pivot only rows that have ( or don't have ) certain values .

IIUC , you need to merge ` C ` with ` A ` : #CODE
( this will add a column to it ) and then merge the result with ` B ` : #CODE

@USER Thanks , this gives me a list of pdfs in each cell at least . I guess I can use that as my answer if you want to update the answer . I will work on trying to find a way to insert new rows for each item in the list .

for some reason I am getting the following error : AttributeError : ' numpy.float64 ' object has no attribute ' endswith ' . the trace back points to 860 if name.endswith ( " * ") : any suggestions ?

How to merge Python pandas data frames on multiple keys
I have two data frames A and B . If I want to do an inner join only if the intersection rows has equal relationship from multiple columns in both A and B , i.e , these columns combined are the matching criteria . How should I start ? These two dataframes are large . Thanks
Update : use concat function , with dataframes includes in list as parameter , axis=1 . Hope it can help others

I tried to use ` numpy.concatenate ` but it does not works on these two matrices . So I tried using pandas merge function after converting ` matrix1 ` and ` matrix2 ` into pandas DataFrames . However , it took a lot of time to do so and all the matrices were merged into a single linear array like ` [ 1 , 2 , 3 , 4 , 5 ... ]` and I didn't had any way to distinguish between ` matrix1 ` and ` matrix2 ` in ` mergedMatrix ` .
` mask = np.ones ( len ( merged_matrix ))
mask [ 0 : len ( matrix1 )] = 0 `

Use ` groupby / apply ` to sort each group individually , and pick off just the top three rows : #CODE
At a high-level , this indicates that we would like to look at each country differently . Now our goal is to determine the top 3 metric counts and report the corresponding channel . To do this , we will apply a sort to the resulting data-frame and then only return the top 3 results . We can do this by defining a sort function that returns only the top 3 results and use the apply function in pandas . This indicates to panda that " I want to apply this sort function to each of our groups and return the top 3 results for each group " .

I am using a script to interpolate stop times from the format HH : MM : SS into minute int values . The script is as follows . #CODE

EDIT : I update the answer as per your comments . I don't really understand the ' functional requirement ' , but anyway if you have a more complex operation you can use pandas ' apply
you want to access the index inside the ` apply ` function , and I don't think you can
and then use the apply function , #CODE
for example , for i in range ( len ( df )):
for n in ( range ( len ( df )):

( likely the simplest way is to remove things with less than 2 occurances ) and then just resample til the spit has both on each side ) , but I wonder if there is a clean method that already exists .
Shouldn't it be ` iloc ` instead of ` loc ` so it works right if the index is not a simple range ?

You need ` apply ( your_func , axis=1 )` to work on a row-by-row basis . #CODE
Another way would be to call ` unique ` on the transpose of your df : #CODE

I don't want to drop the column or change it's type just to be able to view the data frame in the variable explorer .

I have a dataframe which has a value of either 0 or 1 in a " column 2 " , and either a 0 or 1 in " column 1 " , I would somehow like to find and append as a column the index value for the last row where Column1 = 1 but only for rows where column 2 = 1 . This might be easier to see than read : #CODE

This looks like a bug in the ` DataFrame ` ctor in that it's not respecting the key order when the orient is ' columns ' a work around is to use ` from_dict ` and transpose the result when you specify the orient as ' index ' : #CODE

This could be done in 2 steps , generate a new column that creates the expanded str values , then ` groupby ` on ' A ' and ` apply ` ` list ` to this new column : #CODE
@USER sorry what do you mean , do you mean the fact I added an intermediate column ? The intermediate step is necessary as I can't figure out how to get the ` apply ` and ` lambda ` to not try to expand the list and remain 1-dimensional
Another way to do it . First reshape the ` df ` using ` pivot_table ` and then ` apply ` ` np.repeat() .tolist() ` . #CODE

Save to csv a pivot table get error " Process finished with exit code 139 "
@USER I see that in pivot mode has 272 row . Added to the question , thanks

Using ` pandas ` and the ` resample ` function could make your life easier .
Using resample function

Note for ` np.savetxt ` you'd have to pass a filehandle that has been created with append mode .
when I use ` np.savetxt ` , I think the text file doesnt open in ` append ` mode . If I use ` np.savetxt ` twice , the previous data is deleted . Is there any way around it , so I can use this function multiple times to output on the same file ?

If you don't already have IPython etc . set up , you can quickly test this out by creating a new notebook at try.jupyter.org , pasting the code into a cell , and hitting ` Shift + Enter ` to run . Since this is running on a free VM it will be slow , running the notebook locally will mean panning / zooming is much smoother .

You can ` apply ` ` round ` : #CODE
If you want to apply to a specific number of places : #CODE

Are you asking about the first row ? you just do : ` returnsDf.iloc [ 0 ] = dayPricesDf.iloc [ 0 ] * np.exp ( rollReturnRandomDf.iloc [ 0 ])` there is no need for ` shift `

You can ` groupby ` on ' Symbol ' and then call ` apply ` passing a lambda and use ` shift ` : #CODE

What follows takes your data and reshapes it appropriately so you can then plot a surface that I believe is what your are looking for . The key is using the ` pivot ` method , which restructures your data by date and time . #CODE

Can you just ` df.groupby ( ' months_to_maturity ') .apply ( len )` before you drop the duplicates ?
@USER Thanks . I am just trying to understand what ` len ` means in your statement above .
@USER It means the Python built-in function . ` len ( df )` returns the number of rows in a DataFrame .

Actually I think it would be better to define your bin values and call ` cut ` , example : #CODE
You can now call ` map ` and add your new column : #CODE

Perfect this exactly what I was looking for , great use of the max since they are all 1 anyway . ( as a side node the pivot example above does not work if Cat_col is not unique )

don't call ` DataFrame.append() ` in a for loop , it will copy all the data every time . Append all the dataframe to a list and use ` concat() ` to get the final result outside the for-loop .
@USER I think the Data was loaded as a DataFrame , saved in a pickle file format ? The basic purpose was to select a loan by index , pull its total payments received and number of payments to construct a cashflow array , join every month's cashflow from the selected loans and sum the cashflows each month to have one giant cashflow array , and then calculate an IRR off that .
@USER Thanks for the tip , it cut my runtime down by quite a bit !

Python Pandas Conditional Statement 2 Dataframes Diff Lengths

Thank you . Could I use ` hist ` to get the same result , without counting by pivot ?
Hmm i'm not sure how you'd achieve this with ` hist ` , I only have used hist to plot a single series .

I also tried using concat , but I can't figure out how to graph the concatenated DataFrame on the same graph to compare the two keys . #CODE
You're one the right track , but you want ` merge ` rather than ` concat ` . Try this : #CODE

would appreciate some assistance or push in the right direction . I have a pandas dataframe , from a txt file , and would like to insert it in an xml doc I'm making . I can set up the xml doc , and convert my dataframe to xml using : How do convert a pandas / dataframe to XML ? But I just can't seem to insert the converted dataframe xml into the xml doc made .
thanks for the time to help . I've tried the insert add suggested , and a few variations , but I can't seem to shake the error : ExpatError : junk after document element : line 2 , column 2 Read into it , but not luck .. still not adding into doc created ... anything I could try ?
Many thanks for your time and helping on this problem . I will look into the string to xml ( also been looking to a merge option ) . But this is perfect ! I did want to loop through the columns and split the timestamp ( I see I had parsed into datetime which was wrong ) . Many thanks again .. I'm enjoying this xml learning :)

To calculate the new ' Skew ' column , you can do a ` groupby ` and define your customized ` apply ` function . To calculate pct_change , you can use the ` .shift() ` operator . #CODE
Thanks Jianxun , creating a " shift " variable did the trick !!! The only comment is that I wanted the pct change in skew to be as a percent of the ATM ( D= 50 ) rather than the " Skew_lag3 " variable . I created a V_lag3 variable and used that . . .worked perfectly !!! thanks again !

I am using scikit learning's StandardScaler() and notice that after I apply a transform ( xtrain ) or fit_transform ( xtrain ) , it also changes my xtrain dataframe . Is this supposed to happen ? How can I avoid the StandardScaler from changing my dataframe ? ( I have tried using copy=False ) #CODE

How to apply line attribute at each group ?

I have two tables : trainSearcStream and SearchInfo . My code is supposed to merge the two tables based on a common column named SearchID . the problem is that the code runs for sometime and then terminates without reporting any error in my eclipse console . the number or rows in each tables is : trainSearchStream| 392,356,948 and
@USER yes it terminates before to_csv . I have a laptop with 16Gb memory and 500Gb flash storage . I also have the tables in .tsv format . in that format they have size of 9.47Gb and 11.02 Gb . My goal is to join several tables according to a common column then export them to a cvs file . I tried pandas merge function . but my code would terminate . When I ran the pandas code in terminal I got a message saying : ' killed : 9 '

` 1 / 2 ID ` is the column head that need to apply UPPERCASE .
How can I apply upper case to the first three letters in the column of the DataFrame ` df ` ?

tweet [ ' text '] does not seem to exist . A key error is generated when you try to access a key in a hash map / dictionary that does not exist . for example #CODE

Hey Joe , that was just an excel column to get an evenly divided x-axis ( Jianxun solved this with ` x = np.linspace ( 0 , 1 , len ( group ))` .
@USER It's due to the current implementation of pandas ` groupby ` . See this Warning Message : ` Warning In the current implementation apply calls func twice on the first group to decide whether it can take a fast or slow code path . This can lead to unexpected behavior if func has side-effects , as they will take effect twice for the first group . ` from #URL

My problem is I have a very large time series ( ~ 5-10 million obs ) that have certain events marked with flags . In this case it is a drop in stock prices that triggers an event that has a dummy variable for that is 1 or 0 if the event is triggered or not . From this time series I would like to extract both the events themselves and the subsequent 29 days of data . Obviously , this involves some type of splicing of arrays .
Nevermind . I found a way to do it using your answer . Just find the date difference in days between each date and the previous date and drop if the difference is less than 30 days . Thanks . Accepting your answer .

python - replace last n columns with sum of all files
I have 8 csv files with 26 columns and 600 rows in each . now I want to take the last 4 column of each csv files ( Column 22 to column 25 ) , read the files and sum them up to replace all the 4 columns in each file . for example ( I am showing some random data here ):
Now , I want to sum each element of " h , i , j , k " of from these 2 files , then replace the files last 4 columns with this new sum .
We need to call the attribute ` .values ` here to return a np array because otherwise it will try to align on the index which in this case do not align .
Now if your file is not comma separated and rather space separated , remove the ` delimiter ` option from all the functions as the delimiter is taken as ` space ` by default . Also join the first column accordingly .

Anyway , the problem is when I run ` pip install pandas ` . Stack trace : #CODE
The error from 225 to 272 appears recursively until at the very end of the stack trace , when it changes slightly to : #CODE

You should be able to figure out from the stack trace ( that massive load of text which appears when the error happens ) where exactly the error is occurring . That would help to work out what's going on .
The KeyError ' 1 ' message suggests that your pivot table doesn't have a column with the label ' 1 ' . Do ` print ( pivot_table.columns )` to see what your column names actually are ( if you have a multiindex columns ) .

One more question - instead of locations , if I want to use the index and a subset of column names to merge values in two dataframes ?

Maybe reserved words is not the right terminology . I meant words that are reserved for special purpose . In pandas ' median ' has a special meaning #URL . However , I was not aware of that and named a data column as ' median ' . One would expect that if there is a conflict , there would be an error or warning message . I just wanted to know how I can catch these errors in the future .
So ` df.ix [ 1 ]` already had a member named ' median ' ? There's no flag afaik to warn every time a name is re-bound .
Thanks for the response . However , according to your example once I have renamed a column ' median ' that should just overwrite the builtin median method / function and I should still get the output I wish right ?
It will depend on how Pandas maps your column names . In your example , you have just overwritten the median property ( if it existed ) to 12 .

That works thank you . I did not realize to use a tuple . How would i append Y with d & e
You can use a tuple notation to indicate a column of the multi-indexed columns ( and you need ` append=True ` to not replace the existing index ): #CODE
I did not realize to use the tuple . How would i append Y with d & e . I tried ( ' Y ' , ( ' d ' , ' e ')) and ( ' Y ' , [ ' d ' , ' e ']) . 2 separate calls will do it but a single operation ?
The DataFrame.set_index method takes an append keyword argument , so you can simply do like this : #CODE

The ` name ` of ` pd.Series ` become the ` column name ` when you concat them together . Vice versa , when you extract a column from ` dataframe ` , it has the ` column name ` as the name of the extracted ` pd.Series ` . #CODE

instead of creating a ` DataFrame ` and then transpose it through ` df = df.T ` .
` df [ i ]` gets each column . ` apply ( Series )` applys ` Series ` function to the column , which creates ` Series ` from the inner ` dict ` . ` join ` append created ` Series ` to the end of column , and ` drop ` deletes the original column .

Python apply a func to two lists of lists , store the result in a Dataframe
My idea to achieve the goal would be constructing a ` pandas.Series ` of length ` len ( AP )` for each ` OP ` , and then append the ` Series ` to the final ` Dataframe ` .

How to replace None only with empty string using pandas ?
I would like to replace all ` None ` ( real ` None ` in python , not str ) inside with `''` ( empty string ) .
OK I can reproduce your issue , it looks like ` None ` is being promoted to a ` NaN ` , I will post a way to only replace ` None `
It looks like ` None ` is being promoted to ` NaN ` and so you cannot use ` replace ` like usual , the following works : #CODE

I am trying to do a dataframe transformation that I cannot solve . I have tried multiple approaches from stackoverflow and the pandas documentation : apply , apply ( lambda : ... ) , pivots , and joins . Too many attempts to list here , but not sure which approach is the best or if maybe I tried the right approach with the wrong syntax .
I came pretty close with the melt() function , and then taking the former column numbers and added the offset to them . However , I've had a lot of problems trying to reform the dataframe using pivot . No luck with apply or apply ( lambda ) !
Thanks for taking a look . I'm trying out your solution , so at least I have something that works . In the meantime , I'm going to try and develop that one path of melting the dataframe and mutating the column numbers ( old column number + offset ) . The only problem is that when I go to " unmelt " the dataframe , the pivot completely messes everything up .
@USER OK , if you prefer the melt / pivot way , you might want to post what you tried and see if anyone can fix or improve it .

My issue is that sometimes I get a new file with old data that I want to append to the existing sqlite table . I am not reading that table into memory so I can't drop_duplicates in pandas . ( For example , one file is always month-to-date data and it is sent to me everyday )
How can I ensure that I am only appending unique values based on my primary key ? Is there a pandas to_sql function to insert or replace when I append the new data ?
If you attempt to insert duplicate data you'll get a ` sqlite3.IntegrityError ` exception . You can catch that and do nothing , for example : #CODE
Will that mean the data will be duplicated in the SQLite table ? Is it easier to just drop duplicates within SQLite ? I have it set up to only accept unique values for the primary key ( the three columns ) .
The above will not insert duplicate data into the SQLite table .

I need to have a 60 day window ( so not the first 60 points but an increase of 60 in the first column ) , and after each average I need to shift down 20 days and retake the average till I get to the end of my data .

I am trying to read all ` csv ` files in a directory and merge a specific column in all files to a new ` DataFrame ` . Basically , the files are of the format :
Try concat :

You can use ` groupby ` on column ' ts ' , and then ` apply ` using ` .any() ` to determine whether any of ` val ` is ` True ` in the cluster / group . #CODE

Rolling median for a large dataset - python
I have a huge file with 200K lines , I need to find out the rolling median by counting distinct words in each line .
I have used numpy to calculate median as below #CODE
I feel that this is not efficient as numpy creates a new array everytime i insert an element . Is there a way to insert an element into a numpy array inplace ?
The rolling median is not trivial as it seems .
I don't understand , you need to compute the median each time you read a line in the file ?
yes , since its rolling median . I need to read line by line , count unique words per line and find the rolling median .
It seems to me that if your file is * that long* , you've got other problems . In particular , you'll want to implement the kind of fast ' running median ' that @USER suggests .

You can perform your groupby in a different way as a workaround . Don't set Instance as the index and use the column for your groupby and drop the Instance column ( last column in this case since it was just added ) . Groupby will will make an Instance index . #CODE

From a couple of other posts , a simple way to concatenate columns in a dataframe is to use the map command , as in the example below . The map function returns a series , so why can't just a regular series be used instead of map ? #CODE
I checked to see if map returns a different type of object under the hood , but it doesn't seem to : #CODE
I'm confused about what map is doing that makes this work and how whatever map does carries over into the subsequent string arithmetic .
` map ` maps the input values against a corresponding value in the passed in type .

is there already a working solution to insert NaN / None values from a pandas dataframe into sybase-ASE tables ?
However , when I try to insert this into sybase-ASE using bulk_copy / bcp , the None gets inserted as ' nan.0 ' for some reason .
As Sybase is supported by sqlalchemy ( #URL ) , this should just work out of the box with ` to_sql ` ( no need to convert to Nones with ` where `) . What version of pandas are you using ? And can you provide a small code sample that you use to insert the data ?
Here is a code sample of how I convert the dataframe to a list and then do the bulk copy to insert this data into sybase ase : inList = df.values.tolist() blk = self.sybase_conn.blkcursor() blk.copy ( table , direction= ' in ') blk.rowxfermany ( inList )
However , I am running into the following error when trying to execute the to_sql : dataframe.to_sql ( ' sybase_table_name ' , engine , if_exists= ' append ' , index=False ) sqlalchemy.exc.DatabaseError : ( DatabaseError ) Msg 102 , Level 15 , State 181 , Line 1
Out [ 9 ]: ' 1.0.6 ' dataframe.to_sql ( ' sybase_table_name ' , engine , if_exists= ' append ' , index=False )

To strip the leading or trailing whitespace off the strings , you could use #CODE

You could also use datetime and replace catching feb 29th returning the year - 1 and feb 28th if it was : #CODE

` m = 0.03 * len ( df )` is the threshold ( it's nice to take the constant out of the complicated expression )

I am trying to make the last two rows of my dataframe ` df ` the first two of my dataframe with the previous first row becoming the 3rd row after the shift . Its because I just added the rows ` [ 3 , 0.3232 , 0 , 0 , 2 , 0.500 ]` , ` [ 6 , 0.3232 , 0 , 0 , 2 , 0.500 ]` . However , these get added to to the end of ` df ` and hence become the last two rows , when I want them to be the first two . I was just wondering how to do this . #CODE
You could take slices and concat them , depends how complicated this gets
I may not be understanding your comment , but the axis=0 is not about column order but rather doing a row shift vs column shift .
I think this is slightly simpler than ` np.roll ` as suggested by EdChum , but numpy is generally faster so I'd use ` np.roll ` if you care about speed . ( And doing some quick tests on 1,000 x3 data suggests it is about 3x to 4x faster than ` append ` . )

I want to pivot the data using : #CODE
The idea is to generate a heat map that shows the count of " Text " by " Location " and " Date " .

None of the Merge , join , and concatenate would work for me since I need to keep the two files separate .
I tried using the Working with missing data and ` .replace() ` but I'm not sure how to properly extract the values needed from ` main_df ` to replace ` NaN ` values in ` data_df ` .

@USER Yes . We can just first divide the full dataset using ` groupby ` , and then apply the above procedure within each group . I've updated the code . The ` NaN ` at the bottom is because the 2nd group just 29 rows whereas 1st group has 71 rows .

I can pivot the table with : #CODE
Still can't figure out how to drop ' q ' from the dataframe

This is a general question about how to apply a function efficiently in pandas . I often encounter situations where I need to apply a function to a ` pd.Series ` and it would be faster to apply the function only to unique values .
But for large data sets , this can take a while . So to speed it up , I'll extract the unique values of ` date ` , apply the function to those , and then merge it back in to the original data : #CODE
And , would it make sense and be feasible to add a feature to pandas that would take this unique / apply / merge approach automatically ? ( It wouldn't work for certain functions , such as those that rely on rolling data , so presumably the user would have to explicitly request this behavior . )
FWIW , here's some sample data for which the merge way is about twice as fast for me : ` mf = pd.DataFrame ( { ' date ' : np.random.choice ( pd.date_range ( ' 1 / 1 / 2011 ' , periods=365 , freq= ' D ') , 900 ) } )`
@USER Doesn't the ` apply ` need to be ` transform ` if you are going to be back to your full size data and not a collapsed version ? I.e. don't you still need to merge here ? Also , when I time this , it's actually slower than either of the original methods and I thought the point of this whole thing was speed ? ( Perhaps my timing was off though , you can check yourself , of course ) But to be clear , I do think the ` groupby ` with ` transform ` is probably the most clear and readable approach . It's just not any sort of speed improvement that I can see .

I have about 7000 homogeneous DataFrames ( same columns but different sizes ) and want to concat them into one big DataFrame for further analysis .
It looks like you just want to append all tables together ( vertically ) . If that's the case , you might want to consider ` append ` in a ` pd.HDFStore ` .
Here is an example to use ` pd.HDFStore ` to append many tables together . #CODE

See my update . I'm interested in using some Pandas functions for this that behave the same ` as to_period ` as there's some postprocessing done ( grouping , pivot table , index ) which I don't want to break .

@USER You can't . ` RFE ` will rank features according to either ` .coef_ ` ( regression ) or ` .feature_importances_ ` ( tree-based ) . But when you normalize your features in ` LinearRegression ` , the ` .coef_ ` gives very close approximation to the variance explained by that particular feature , that is ` R2 ` . Actually , that's why ` RFE ` will make feature selection based on ` .coef_ ` .

error code is ` assert ( len ( items ) == len ( values ))`
There are strict requirements on the shape and form of the data being passed , you can pass just the data and transpose it to get the initial data as a single row and then overwrite the column names : #CODE

use ` shift ` to get the previous row

How to merge two programs with scheduled execution
I am trying to merge two programs or write a third program that will call these two programs as function . They are supposed to run one after the other and after interval of certain time in minutes . something like a make file which will have few more programs included later . I am not able to merge them nor able to put them into some format that will allow me to call them in a new ` main ` program .
I am trying to write a ` main ` program that will call ` master_ids.py ` first and then ` master_count.py ` . Is their a way to merge both in one program or write them as functions and call those functions in a new program ? Please suggest .

How to apply a function on a Series
I would like a apply a function to rename the values . #CODE
Use ` map ` to perform the lookup : #CODE
Your error occurred because you called apply on a single column df : #CODE
So this is different to a Series where ` apply ` iterates over each value which can be hashed but here it's passing the entire ` Series ` which cannot be hashed , it would work if you did this : #CODE
Thx , but I'd rather keep the values if not in ` translate ` .

Pandas append rows to dataframe with two indexes
and then append rows to that with the index in the right spot
the append does not change my T in my 0.16.1 version

I've searched through a few threads and either I'm getting an error or I'm not getting the expected result when trying either the pandas replace method or the regex re.sub method ( python 3.x ) .
I've tried a few different ways of escaping the quotes in the replace function , ( ie using """ and ' to define the find part ) . Any ideas or insights are greatly appreciated !
Your ` .replace() ` method is trying to find an exact string match to replace rather than a subsitution based on existing parameters : Pandas.DataFrame.replace() . The ` .str .replace() ` uses the built in string method in pandas which achieves what you were looking for : Pandas - Working with Text Data .

My expected output is an array of all the simulation values . I also want to append all the results into one pandas dataframe .
That's exactly how you specify the different sizes . Replace N1 , N2 , N3 for the sizes you want for apple , peach , orange .
Ok I think I almost have it . Can you show me how to properly append this ? Thank you so much for your help

but how do I insert my df into my table in the database ?
but it only shows how to insert values .

I am just trying to insert these two rows to the left of the most recent entries , and then save over the previous file . I'm not having any trouble with this , but I am getting " unnamed : # " in the columns next to the dates instead of just an empty cell like this : #CODE
replace the ` NaN ` s with empty strings by calling ` fillna ` , #CODE

how do i insert without inputting all column names with pyodbc execute many ?
I am trying to insert my data into my ms access sql connection . How do I write an insert statement where I do not have to input all the column names and " ? " values .

wondering if the best method is to merge or perform a customizable function on a
In pandas , you can both append column names and reorder the data frame easily . See this article on merging frames .
To append frames and re-order them you could use the following . Re-indexing is as simple as using a list . There are more solutions here . #CODE
It would be more performant to read df from each csv into a list and then concat all the dfs in the list rather than concatenating each one
@USER reading the article will also explain more complex functions but concat should cover everything
@USER Thanks for the reference materials . I have reviewed these thoroughly . However , it appears that one can only merge two data frames at a time . Is there a sub function that you would recommend that allows me to check for the same headers and then append the corresponding value of that header if the check is true ? I want to understand the bowels of Pandas :)
@USER Append them in a list like so pandas.concat ([ df1 , df2 , df3 ]) . The best example off the bat is at this post . I just saw the code . It also goes over append v . concat .

I have two pandas dataframes : one ( ` df1 `) with three columns ( ` StartDate ` , ` EndDate ` , and ` ID `) and a second ( ` df2 `) with a Date . I want to merge the ` df1.ID ` into ` df2 ` based on df2.Date between ` df1.StartDate ` and ` df2.EndDate ` .
You'll have to define a func to perform the lookup , ` merge ` will not perform the match for you as it requires exact matches on value rather than between ranges
When i change the code to use an arbitrary column instead of an index , it gives a ValueError . e.g. ` df2 [ ' ID_matched '] = np.piecewise ( np.zeros ( len ( df2 )) , [( df2 [ ' date '] > = start_date ) & ( df2 [ ' date '] <= end_date ) for start_date , end_date in zip ( df1.StartDate.values , df1.EndDate.values )] , df1.ID.values )`

I first tried to use .groupby ( ' id ') .max but couldnt find a way to use it to drop rows . The resulting dataframe MUST contain the ORIGINAL ROWS and not just the maximum value of each column with each id . My current solution is : #CODE
I tried with ` groupby ( ' id ') .max() ` and it works , and it also drop the rows . Did you remeber to reassign the ` df ` variable ? Because this operation ( and almost all Pandas ' operations ) are not in-place .

You've been around Stack Overflow long enough to know that you should ( a ) ask a question and ( b ) show what you've tried and are stuck with . What are you having problems with ? How have you set up the data frame ?
We can generated the edge list using ` groupby / apply ` and
matrix is symmetric , we can add its transpose to make it symmetric . The

There's a big problem with the " #Creating the list " you're resetting ` list_of_coupons ` to an empty list at the beginning of each iteration . So after you append an item to it , you're erasing it

You don't need to cast your dict as a DataFrame in order to append , you can pass the dictionary directly to ` append ` like this : #CODE
If you want to load them all in one batch , you could clean the responses with a list comprehension to keep only entries with the right keys , and then pass this in to ` append ` ( you won't be able to skip individual exceptions though . ) #CODE
Hey thanks , it's great to know I can append directly - although how would I do this without brining all the fields back from msg ? As I would like to exclude the ' instrument ' field from the dataframe

` len ( df2.index ): 12
print len ( df2.query )

, why does pandas replace the values at index 5 and 6 with 3s , but not leave the values at 0 and 1 as is ?
This ` interpolate ` behaviour in pandas looks strange . You can use ` scipy.interpolate.interp1d ` instead to produce expected result . For linear extrapolation , a simple function can be written to do this task . #CODE

@USER That brings back to the previous question : how do you interpret this ` date ` data ? If you treat it as string , then you need to first use ` sklearn.preprocessing.LabelEncoder ` to convert those text data into numeric form . Since all your dates are distinct , it's equivalent to replace ` date ` with ` np.arange ( len ( date ))` .

Python Pandas - Append data to specific row and column
You may use loc too : #CODE
Thanks ! I'm not sure I completely understand why I can pass two arguments to loc , and I'll need to think through using ' describe ' .

Python Pandas - ( not working ) Extracting from csv into other csvs using dataframes and drop
I am also trying to keep the columns and indexes the same , so when I extract the CSV I can then drop the data from the main dataframe . This means I can continue in processing the remaining data .

And then simply use pandas ' very handy ` melt ` function to unpivot the dataframe : #CODE

When running drop duplicates on python pandas there seems to be a bug which causes the DataFrame to be sorted in the wrong order .
Specifically , I was trying to provide two columns to perform the drop duplicates on . Instead of : #CODE
Whether to drop duplicates in place or to return a copy

I have some sales data grouped by ` ( ' dt ' , ' product_id ')` like this : #CODE

Assuming your first and second dfs are ` df ` and ` df1 ` respectively , you can merge on the matching columns and then mask the ' a ' and ' b ' conditions : #CODE
You can then drop the additional columns : #CODE

Applying a query function to a RDD using map and lambda

I would now like to calculate the maximum within each business quarter for ` test_df ` . One possiblity is to use ` resample ` using ` rule= ' BQ ' , how= ' max '` . However , I'd like to keep the structure of the array and just generate another column with the maximum for each BQ , have you guys got any suggestions on how to do this ?

How do you save to the cvs ? Probably you pass the argument `' w '` ( write wife ) instead of `' a '` ( append to file ) .
Looks like you're looking to append in most programming languages " write " also truncates the file to zero length before writing anything .
will append new lines to the existing file .

More than 6.8 Gb of RAM are used while performing the ` map ` operation .
No , I have simplified a lot . It would be nice to use pandas ( I need to merge that dataframe with an other one , and so on ) .
@USER I tried your previous solution but I get : ValueError : incompatible categories in categorical concat when doing adsInfoDF = adsInfoDF.append ( chunk )

I am trying to choose any day in January and July of every year spanning the period between two datetime objects ( ` row [ ' orig_iss_dt ']` and ` row [ ' maturity_dt ']`) and insert them it into ` my_dict ` . Since , ` row [ ' maturity_dt ']` in my dataframe ` df ` is some multiple of 6 months from today ( July ) , I thought my code below would do this . However , it isn't working as expected as I get some ` date `' s in the months of April , May and June . I've tested the ` monthdelta ` function and it works as expected . #CODE

Basically , I want to apply the same transformation to a huge data set I'm working on now , but I'm getting an error message : #CODE
Why do I get an object type when apply group by with size instead of an integer type ?
and then merge the result back into ` df2 ` : #CODE

As you can see , ' rel ' is just the number of days since the starting day . It's essentially an integer , so all you really need to do is normalize it with respect to the starting date . #CODE

Transpose Pandas Panel to Columns
Thanks so much ! I even tried stack and didn't find unstack when I was looking ! Thanks !

I am fine with deleting these problematic characters altogether . Better yet would be to normalize them to ASCII characters under 127 . How can I do this using JUST PANDAS ? I'm looking for the most panda-like way if it exists .

Format Pandas Pivot Table
I met a problem in formatting pivot table that created by Pandas .

then the concat with the x values from the original frame and the new " y " frame #CODE

pandas two dataframes , some sort of merge
I need some sort of join or merge which will give me dataframe like this : #CODE

Since you want to retrieve ` category ` column as well , a standard ` .agg ` on column ` val ` won't give you what you want . ( also , since there are two values in author3 are 7 , the approach by @USER Cunningham using ` .max() ` will only return one instance instead of both ) You can define a customized ` apply ` function to accomplish your task . #CODE

python dask DataFrame , support for ( trivially parallelizable ) row apply ?
Edit : Thanks @USER for the map function . It seems to be slower than plain pandas apply . Is this related to pandas GIL releasing issue or am I doing it wrong ? #CODE
I am not familiar with ` dask ` module . For mulit-processing , python module ` multiprocessing ` works well for me when I have to process a big dataframe row-by-row . The idea is also very simple : use ` np.array_split ` to split big dataframe into 8 pieces and process them simultaneously using ` multiprocessing ` ; Once it's done , use ` pd.concat ` to concat them back to the original length . For a related post with full code example , see #URL
You can apply your function to all of the partitions of your dataframe with the ` map_partitions ` function . #CODE
Note that func will be given only part of the dataset at a time , not the entire dataset like with ` pandas apply ` ( which presumably you wouldn't want if you want to do parallelism . )
` map `
You can map a function row-wise across a series with #CODE
But avoid ` apply `
However , you should really avoid ` apply ` with custom Python functions , both in Pandas and in Dask . This is often a source of poor performance . It could be that if you find a way to do your operation in a vectorized manner then it could be that your Pandas code will be 100x faster and you won't need dask.dataframe at all .
Thanks ! I tried the map method and it seems to be slower than pandas apply . Could you comment on the edit of original post please ?
@USER Slightly off topic regarding pandas ; i try to use map over apply because I've heard it's faster , but I'm not sure why it's faster . Any clarification or links to clarification would be greatly appreciated .

Using ` len ( chunk )` will only give me how many each one has .

Speeding up Pandas apply function
For a relatively big Pandas DataFrame ( a few 100k rows ) , I'd like to create a series that is a result of an apply function . The problem is that the function is not very fast and I was hoping that it can be sped up somehow . #CODE
No , not for this particular problem . But I think the main problem is the number of calls to the apply function , so ` cython ` , ` numba ` , ` numexpr ` , etc . won't help much to alleviate this .
The trick is not to use ` apply ` , but to do smart selections . #CODE

I get the following error with this stack trace : #CODE
Well , one way to isolate the issue is to cut down the number of things happening in the line . Currently , there's comparisons , bitwise OR , ` __getitem__ ` , accessing the attribute ` index ` and assignment . What if you change the line to ` ( df.A == 0 )` and put it in a for loop that repeats 100 times . Do you still get the error ?

What's the most elegant way to merge several Pandas DataFrames ( DF ) from different dictionaries ?
I have two dictionaries which I use to create several different DFs . The first dictionary I use to create a DF from a list inside each key . The second dictionary I use to outline the characteristics of a DF , for a read_csv approach on different text files . All DFs have a common column called ' FN ' . SO I am looking to merge all combinations of DataFrame between the two dictionaries . #CODE
So ultimately using Pandas I am looking to merge the following combinations below using dictionaries Dict1 and Dict2 to help . #CODE
Meanwhile all this would be done in a loop . So I would merge List1+Filetype1 and perform some stats gathering ( then post stats to Excel ) , then List2+Filetype1 and perform some more stats gathering ( then post stats to Excel ) .
With " efficiently " do you mean elegantly / clean code-wise ? Else Ami's reply says it all , the iteration process itself is irrelevant in comparison to the merge process .

We have to apply the ` map ( func )` to the series in the dataframe : #CODE
DataFrame.applymap ( func ): Apply a function to a DataFrame that is intended to operate elementwise , i.e. like doing map ( func , series ) for each series in the DataFrame

This is a file in Sextractor format . The ` astropy.io.ascii ` reader understands this format natively so this is a snap to read : #CODE

You can convert ` datetime64 ` into whatever string format you like using the ` strftime ` method . In your case you would apply it like this : #CODE

Grouping data on unique column keys and joining ( pivot table ) in Pandas Python
I want to reshape the data so that I'll have a pandas dataframe where each column will be a ticker with its respective closes and if there is no data for the date row key it will have NaN ( ' left ' join ) .
I had to use ` reset_index ` because ` pivot ` requires a column as index ( at least until this bug is fixed ) .
Perfect , that's exactly it .. I thought it had to be pivot , but couldn't get to work ..

@USER Yes , it's possible . You just need to first groupby ` Id ` and then move all the processing into an apply function ` my_func ` . I've updated the code . Please have a look .

Ideally you don't want to iterate row-wise if possible rewrite your filtering and use the ` dt ` accessor to get the ` time ` attribute for your entire series :

pandas.date_range accurate freq parameter
If you try to use ` pandas.date_range() ` , you need to specify the frequency ( parameter ` freq `) as ` str ` or as ` pandas.DateOffset ` . The first one can only handle an accuracy up to 1 ns , the latter has a terrible performance compared to the ` str ` and has even a worse error .

The ` groupby / apply ` above returns a ` pd.Series ` . To make this a DataFrame , we can make the index level values into columns by calling ` reset_index() ` , and then assign column names to the columns : #CODE

Curious question , why does line 4 diff from intended result ?

I think the reason why pandas reads the entire file into memory is unrelated to whether you use ` fixed ` or not . It was designed this way . To do statistics in pandas ( ` sum ` , ` mean `) pandas needs to read the entire dataset . You could drop down to ` PyTables ` which supports queries that won ` t read the entire dataset into memory but only chunk by chunk ( however you won't have the convenient panda functions ) . Alternatively for datasets that don't fit into memory [ Blaze ] ( #URL ) might be a good solution .

apply function to a DataFrame GroupBy and return fewer columns
I want to group my ` DataFrame ` then apply a function of several columns which returns a single result . #CODE
This can be fixed using ` apply ` rather than ` agg ` , as ` apply ` has no constraint on the returned shape . #CODE

How to merge two pandas DataFrames in Python ?
I am trying to join two pandas data frames with an inner join .
I believe joining on dates is valid , however I cannot make this simple join work ?
Your date columns are not datetime dtype , ` df1 ` looks like a ` str ` whilst the other is a ` tuple ` so you need to convert these first and then the merge will work : #CODE
And from there our merge works . I used only row 0 of df1 and row3 to make things simpler in my ide . #CODE

there seems something wrong because my file has 7816361 lines counting the header while my list has an extra element ( len of list 7816361 )
Assuming you just want to join the files based on their line number you could do the following : #CODE
Otherwise you'll need to perform a join or merge on your two dataframes .

I put a condition for one column in pandas dataframe , hopefully it works . But it doesn't work when I apply it for all columns .

You can use ` cumsum ` to compute the cumulative sum ( add 0.001 before that ) , then ` shift ` that column by 1 , finally set the first row to be 0 . #CODE

I am transforming a single dataframe using multiple cpu cores and want to insert the result into MySQL .

Conditional concatenation of python pandas dataframe ( sql join on self )
We have an existing dataframe and wish to extract a series of records and concat ( sql join on self ) given a condition in one command OR in another DataFrame .
Yes , We can retrieve the data from both and then concat them :
But maybe I should have specified that I have a sizeable DataFrame of 10,000 rows+ . I mean technically we could do a for loop but I'm sure there is a neater way to do this with some cool DataFrame functionality ( concat merge join -> i just don't see the conditional join ) . Normally I would join a SQL table on itself given a condition but for some reason I don't see it in this situation
Yes that may work but I find it a bit strange that we don't have a conditional concat , or append or something similar to the merge functionality but then for appending rows : like this ( will return a nasty error ofc ): pd.concat ( out1 , out2 , left_on= ' A ' , right_on= ' A ' , how= ' left ')
Yes it works :) . But with ' pandanic ' i meant something as nice and elegant to use as the merge function but instead of merging on rows we merge columns . Following that , I'd say we concat but I don't have the merge functionality in the concat function :(
Yes that works when we are looking for any in the set that is of value 1 , 2 . But , we are looking for all of the records B = 1 and B = 2 where A = A . In our set we sometimes could have this : B = 2 but no B = 1 . We thus want to exclude these . As such the isin() doesn't seem to apply ?

I am making a heat map that has Company Name on the x axis , months on the y-axis , and shaded regions as the number of calls .
I am taking a slice of data from a database for the past year in order to create the heat map . However , this means that if you hover over the current month , say for example today is July 13 , you will get the calls of July 1-13 of this year , and the calls of July 13-31 from last year added together . In the current month , I only want to show calls from July 1-13 . #CODE

Then I use the pivot table to create an OrderedDict for Plotting #CODE
can you upload you sample data file pls ? what's the original frequency before you make the pivot table ?

How to drop the separator when concatenating two csv files with pandas ?
Recall , how can I fix the above approach and merge both ` text ` columns and tag column in order to get something like this : #CODE
Any how , I read the to_csv documentation but I did not found any " drop separator parameter " . Thanks in advance guys .
Also you'd better make sure that all the ID's line up in your files , otherwise you might want to consider using ` merge ` rather than ` concat `

How to drop rows from a pandas dataframe where any column contains a symbol I don't want

Semantically speaking unless your df just has a single Id then really you need to perform a ` groupby ` or construct a lookup and merge the values back to your df
You can group on ' Id ' column , call ` first ` to return the first value for that group , this returns a Series with ' Id ' as the index , you can then call ` map ` on the orig df ' Id ' column to perform a lookup and assign the corresponding value for each ' Id ' : #CODE

Merge values split in different columns overwriting the 0 values
Then i want to merge all of them so result should be something like
It is not clear if C is the sum of the other columns or the maximum value . Will there only ever be one nonzero column in each row ?
You can use ` .max() ` and apply it row-wise by specifying ` axis=1 ` #CODE
Great but what about if i only want to sum or take the max from specific columns ? To be more precise some of them have the same name ( repeated ) althought the values are split . Then i want merge the columns repeated

I'm not sure exactly what you're expecting , but you could replace your lists with numpy arrays ( I don't think it'll improve your specific code ): #CODE

I have some sales data indexed on ` ( ' dt ' , ' product_id ')` like this : #CODE
just updated my post , note that I've made mistakes in my question , it's not a pandas groupby object but a pandas DataFrame indexed on ` [ ' dt ' , ' product_id ']`

Then perform a left merge on your original df and select the ' H_y ' column , the name comes from the fact that the columns clash and ` ffill ` this : #CODE
Result of ` merge ` to show what it produces : #CODE
OK , IIUC you can group as before but then filter the group where ' Code2 ' equals ' Code ' and then use this to merge against : #CODE

Call ` add ` on itself and ` shift ` , you then need to call ` dropna ` : #CODE

You can then concat this back to get the ' I ' column back : #CODE
Thanks Ed ! Again ! Actually setting index_col= ' I ' when reading allows to avoid the concat !

Just drop the year column and sum the month columns : #CODE

I am trying to join ( merge ) together two dataframe objects on multiple conditions . I know how to do this if the conditions to be met are all ' equals ' operators , however , I need to make use of LESS THAN and MORE THAN .
If I wanted to merge together two dataframes based on multiple ( equals ) conditions , I would do something like the following : #CODE
Note : your example dataframes do not meet your join criteria . Here is an example using modified dataframes : #CODE
I did actually think of using this method - the problem is that the merge operation on the full dataframes creates an enormous output . If I give an example - just for chromosome 1 , there are 3511 entries in gene_df , and 528381 in snp_df . So an inner join on this chromosome alone results in 1855145691 entries !

Then I use the pivot table to create an OrderedDict for Plotting #CODE
Does anyone have ideas on how to approach modifying this code so it can work for shorter time spans ? I am thinking that it will be modification in the OrderedDict section . Possibly making len ( recvd_dttm ) to iterate over ?
And replace `' Month '` with `' Day '` below . You wouldn't have to bother with the OrderedDict etc . in this case as they are just ints . For a week you could do #CODE

Can you verify that ` df ` does indeed have 55 rows ? From the looks of the code , it seems like ` i ` is getting a value of 55 , which could only occur if ` df ` had more than 55 rows . Perhaps you could try something like ` print ( len ( list ( df.iterrows() )))` to see if it is indeed 55 .
@USER ` print ( len ( list ( df.iterrows() )))` results in 55 .

However I want to apply the function to each row in the df and make a new column . I've tried the following #CODE
I appreciate any help . After I get the coordinates I'd like to map them . Any recommended resources for mapping coordinates is greatly appreciated too . thanks
You can call ` apply ` and pass the function you want to execute on every row like the following : #CODE
Or do it in a one liner by calling ` apply ` twice : #CODE
So the above gets all the unique values using ` unique ` , constructs a dict from them and then calls ` map ` to perform the lookup and add the coords , this will be more efficient than trying to geocode row-wise
Based on your data only since the rows seem exact duplicate , and only if you want , drop the extra ones and execute geocoding to one of them . This can be done using ` drop_duplicate `
If you want to keep all your rows , ` group_by ` the city / state combination , apply geocoding to it the first one by calling ` head ( 1 )` , then duplicate to the remainder rows .
That's an interesting point , it's probably better to just get the unique values , geocode them and merge these back , I will update my answer
Thanks for this response . Incredibly useful information ! Although when I did look at [: 5 ] rows of data I received a good dataframe . When I applied the function to all ( 200,000 records ) I received a time out error . I will have to groupby and then apply . Thank you very much .

First , you could transpose each data frame so that the rows are the years and the columns are the countries , then take each respective column from the 3 data frames and join them together . Something like this would give you a data frame for each country : #CODE

I am using pandas.cut in the following manner to map single age years to age groups and then aggregating afterwards . However , the aggregation does not work as I end up with NaN in all columns that are being aggregated . Here is my code : #CODE

I guess it depends on the number of levels for small number of levels this will be faster than calling apply but code wise it doesn't scale to increasing levels well
You can map using the ` bisect ` module : #CODE

pandas merge dataframe and pivot creating new columns
A few steps here . The key is that in order to create columns like ` one_a one_b .... two_c ` , we need add ` Time ` column to ` Sample ` index to build a multi-level index and then ` unstack ` to get the required form . Then , a ` groupby ` on ` Animal ` index is required to aggregate and reduce the number of ` NaN ` s . The rest are just some manipulations on format . #CODE

How to customize the facecolor and hatch pattern of boxplot in seaborn ?
I want to customize the facecolor and hatched pattern of the boxes in a grouped boxplot .
I have examined the document of seaborn and found that seaborn only returns a ` matplotlib.axes._subplots.AxesSubplot ` object but not a ` dict ` object which I can use to set the properties of the boxes in the boxplot .
The closest solution that I have found is to use ` Pandas.Dataframe `' s ` boxplot ` method where it takes an input argument called ` return_type ` . If ` return_type=true ` , the return value of the boxplot method is a ` dict ` object that can be leveraged to fulfill my goal . But it has some problems on the axis label when I choose to group by over more than one column variables . The space between the boxes is not wide enough to show all the words corresponding to each combination of the column variables . I am searching for a way to fix .

` ValueError : Cannot shift with no freq `
I think the error message ( interpreted as ` time index cannot shift with an integer with no time frequency attached `) complains about the last line of your code ` df_percent.index-10 ` . It tries to tell pandas to subtract an integer 10 from ` pd.DatetimeIndex ` , which is not defined .
@USER by the way , I don't think you need to do this in order to product that bar chart . pandas is smart enough to align two bars side-by-side . Can you upload you sample data so that I can write an example to show you how to do it ?

I am now able to read the data successfully , unfortunately I can no longer append data to an HDF5 store that I need to work with . Specifically , Python crashes whenever I attempt to append to this dataset . The append takes place through pandas as :

If I've read your question correctly , your indexes align exactly and you just need to combine columns into a single DataFrame . If that's right then it turns out that copying over a column from one DataFrame to another is the fastest way to go ( ` [ 92 ]` and ` [ 93 ]`) . ` f ` is my DataFrame in the example below : #CODE

The dlret DataFrame is of 600 or so rows and 25000+ columns . I iterate through the columns to look for the first nonnull value ( the delisting return ) and then find the corresponding location in the ret DataFrame to set the value to that of the delisting return . However , the code runs painfully slow using ix to index the corresponding location . Any suggestion on how to efficiently achieve this ?
I think a simple ` combine ` or ` update ` function might solve your problem here . Would you mind uploading a sample of your dataset via dropbox sharelink or google driver ? If data is proprietary , replace them with some random numbers .

You can use apply for that : #CODE
Yes , that is what apply does . df [ df [ ' Q2 '] .apply ( your_custom_function )] . You pass a function into the ** column ** of the data frame and get rows for matching criteria .

Publishing a pandas pivot table via html
Has anyone any experience of publishing a pandas pivot table to a html page ?
Its simple to publish a list as a table but i dont know the syntax for a pandas pivot .
A pivot table is nothing else than a DataFrame , so it has the ` to_html() ` method described in the docs here

Since you only want to change the first element of each group , you can do a customized groupby apply function to do this . #CODE

How to merge pandas calculated series into pandas dataframe

Pivot a dataframe with duplicate values in Index
I tried pivot but it returns an error
Hmm My dataframe had 12 rows but when i tried the unstack operation the resulting dataframe has only 6 rows not exactly what i want.My resulting dataframe should also have 12 rows
@USER .smith I don't think you can keep the same number of rows as your original df because some rows have to mov to columns . For example , say ` Jul-03 ` data , row ` 0 , 6 , 9 ` are all records about the same ` snapDate ` with instance ` XX ` . So doing a pivot would reshape these 3 rows to only one row because those data have been moved to columns .
Hi I went ahead and changed the datatype of AvgWaitInMs to int and the pivot worked

Hi . The 0 is really a 0 , not the Excel representation of = . ( I first found the issue , when I was reading the file back in in Python ; and I changed the cell representation to text , which still showes the 0 in Excel ) . I also tried to replace the " = " with " ' = "

what should the output be , if you add that to your question it will make it lot easier to know exactly what you want , if you wanted unique visits you could simply drop dupes based on id and page ` df.drop_duplicates (( " visitor_id " , " page ")) .groupby (( " visitor_id " , " page "))`

Pandas : Nested merge
I have three dataframes that I need to merge together . Basically , I have one dataframe ` df1 ` that gives me employment for specific sectoral definition ( ` sector `) and year . Then , I have a second dataframe ` df2 ` that gives me a characteristic ` foo ` for each sector and year , but here the sectoral variable is ` sector2 ` .
As these dataframes are pretty large , I'm looking for the most efficient way to merge these dataframes over year and sector , while respecting the sector-sector2 links . I do not need rows where either ` establishments ` or ` foo ` is empty .
You should make an ` reset_index ` on each dataframe and then merge them .
But wouldnt that merge along the ( new integer ) index ? How does that respect my sector , sector2 bridge ?
All right , let say you want to merge ` df2 ` with ` my bridge ` . A single ` sector2 ` can be matched to several ` sector ` . But what about ` foo ` and ` year ` ? How can we and pandas decide to which ` sector ` belong to which ` foo ` ?

My solution ( as changing tz in my DataFrame did not change this behaviour ) was to set ` xticklabel=False ` in ` heatmap() ` and use ` plt.xticks() ` directly .

Now , I am trying to resample this data so that only the business monthend values come back , and if I do this : #CODE
The pattern I use for this problem is to ` pivot ` the table so you only have the date as an index . This will allow the ` resample ` function to work . #CODE
You can then ` stack ` the ` dataframe ` and reset the index to put the field ` id ` back as a column .
After this , I will apply the resample method , and then use the reset_index() method to get back a DataFrame that looks more or less what I have before : #CODE

This worked , but ` len ( a ) 30000000 ` and it is too slow . What can I do to speed up this code ?
Hello and welcome to StackOverflow . Please take some time to read the help page , especially the sections named [ " What topics can I ask about here ? "] ( #URL ) and [ " What types of questions should I avoid asking ? "] ( #URL ) . And more importantly , please read [ the Stack Overflow question checklist ] ( #URL ) . You might also want to learn about [ Minimal , Complete , and Verifiable Examples ] ( #URL ) .

So I would want to replace the id in the first row with 22 because the item is the same as row 1 or 2 .

TypeError : Join on level between two MultiIndex objects is ambiguous
need to unstack the series " ser " as well ?
perfect thanks . I'd probably drop the Beta level like this : df.index = df.index.droplevel ( level= ' Beta ')

Pivot Pandas DataFrame switching Index and Column Names
I want to convert ( pivot ) my table to look like #CODE
How would I be able to pivot this table ? ( with pivot() / pivot_table() im assuming )
You probably want the [ transpose ] ( #URL ) function .

I have a function which I apply it on the rows of a dataframe . This function returns a list of variable length depending on a parameter .

I am trying to drop specific rows from my 3-column dataframe based on values in two of the columns . I have been trying to use Boolean indexing , but have not been seeing the results I expect .

Of course replace accordingly .

So , since the output of the ` str() ` operation cuts off my data ( and might even truncate column values , If I had used longer strings ) , I don't get 1000 , I get 60 .
I'm not entirely sure this is methodically correct insofar as language processing , but using ` join ` will give you the count you need . #CODE
You can get a ` TypeError ` in the first snippet if one of your elements is not of type ` string ` . This is more of ` join ` issue . A simple test can be done using the following snippet : #CODE
A simple workaround is to convert ` x ` in the list comprehension into a ` string ` before using ` join ` , like so : #CODE
Or you can be more explicit and create a list first , map the ` str ` function to it , and then use ` join ` . #CODE
When I try to join the list , I get this error , " TypeError : sequence item 368 : expected string , float found " . I assume that's because that tweet contains a URL and some backslashes . How do I get around that and convert my Series into a string ? When I use the 2nd suggested code and skip the textblob , it's still functioning on the truncated str version .

I have a script that uses FFMPEG and CMD to cut video files based off of an excel document row by row . I would like python to add a timestamp after it is done with a row . Can you guys please help ? #CODE
If you'd like more control over the format of your timestamp , consult the docs for ` datetime.datetime.strftime() ` . Also , I wrote this assuming that your Excel file had other sheets that you would be upset if you lost . If that is not the case , you don't have to do nearly as much . You can ignore changes numbered 1 and 3 , and replace 5 with ` df.to_excel ( datafile )` .

I am trying to merge several different times series from a number of dataloggers into a single pandas dataframe . Each logger records data at a 30 minute interval , but the periods aren't necessarily in-sync or actively logging at any given time .
I've created a new Datetime Index representing the range of min and max datetimes at a 30 min frequency beginning on the hour , but I'm unsure how best to merge the data to this index . Upsampling each of the loggers and performing a merge on the Datetime Index would be simple enough , but I am wondering if there's a more intelligent way to do this by merging datetimes to a PeriodIndex ?

you can create to data frames from two datasets and merge them .

This is equivalent to drop row if both col1 and col2 values are duplicated : #CODE

loc is very slow , will be faster with .ix in my experience . Someone once explained to me why this is , but I cannot remember .

Python . Merge repeated columns
I know that i can merge the columns by : #CODE
Another possibility could be to create dataframes with only the repeated columns , apply either sum or max and then merge everything .

exec ( compile ( scripttext , filename , ' exec ') , glob , loc )

Sorry what's wrong with ` df [ ' group '] .value_counts() [ ' a ']` or ` len ( df [ df [ ' group '] == ' a '])` ?

Here is one way to do it . Basically , you need to first ` shift ` last three columns and then combine with the first 4 columns , and finally calculate the ` min ` . #CODE

I have a column with some ints and some null values in a pandas dataframe . How do I replace the ints with the float values from another column ( by same row ) , but leave all the nulls ?

You can use ` concat ` for this , and the keys of the dictionary will automatically be used for a new index level : #CODE
You can also give this new index level a name using the ` names ` argument of ` concat ` ( eg ` names =[ ' key ']`) .

You can't pass `' reputation '` as a column name to ` x ` while also passing the counts in ` y ` . Passing ' reputation ' for ` x ` will use the values of ` df.reputation ` ( all of them , not just the unique ones ) as the ` x ` values , and seaborn has no way to align these with the counts . So you need to pass the unique values as ` x ` and the counts as ` y ` . But you need to call ` value_counts ` twice ( or do some other sorting on both the unique values and the counts ) to ensure they match up right .

I'm trying to map my directory in a pandas dataframe but the auto index is always 0 . Eventually I want to create a column that will MD5 the file paths . I know there are alternatives but I'm trying to do it in pandas .

When you append ` df.AccName ` you are in fact appending the entire column . So in the end , ` mps ` becomes a list of ` DataFrame ` columns , each element identical and equal to ` df.AccName ` .

What I am trying to do is basically initialize a column and as I iterate over the rows to process some of them , then add a filled list in this new column to replace the initialized value .
You could also knock off ` .index ` in your " Method 1 " when trying to find ` len ` of ` df ` . #CODE
Thanks . Yes , the ` np.empty ` approach does look faster . The ` len ( df.index )` also actually is similarly faster than just ` len ( df )` .

I would like to create df [ ' y '] which will have the first ' cord ' value and shift to get index value . #CODE
being careful to pass the values . Now drop the cord1 column #CODE

BTW , I have used ` loc ` method to select date like this : #CODE

Convert pandas freq string to timedelta
If you start from a ` freqstr ` from a DatetimeIndex , the second approach will always work . But , you can also use ` freq ` instead of ` freqstr ` , which returns a frequency object directly : #CODE

So we filter the series using ` notnull ` to get the first valid index . Then use ` iloc ` to slice the series

I want to create a dataframe in pandas and then pivot it but it needs column headers to work properly . I had previously been importing a completed csv into a pandas DF which included the header so this all worked smoothly but now i need to get this data direct from the DB so I was thinking , that's easy , I just join the two lists and hey presto I have what I am looking for , but when i try to append I actually wind up with this : #CODE
You could insert it into data location 0 as a list #CODE

Complicated datetime merge pandas
Is there a way to merge ` table2 ` onto ` table ` such that the elements of ` table2 ` are joined on the closest but smallest than or equal element of ` table ` , and then fill the table backwards ? This also needs to be done groupwise on the column ` Id ` . As an example , the resulting table would be #CODE

OK , If I understand correctly to test for membership you can use ` intersection ` : #CODE
Hmm , I couldnt find a ` index.complements() ` method . So , to get ` notIn ` , I would first need to ` in = df.index.intersection ( l )` and then ` notIn = set ( l ) - set ( in )` ? Sounds arbitrary that it would support intersection but not complements .

Edit : Prior version used apply / lambda which is not really necessary . This is a simpler version .

Why does order of comparison matter for this apply / lambda inequality ?

Ge the total for all the columns of interest and then add the percentage column : #CODE

Assuming your logic for function ` week ( rawdate )` is correct , You can use the ` series.apply ` function to apply a function to all values in a series ( a pandas column ) , and this returns a new series , which you can assign to your new pandas column .
anyway try the ` apply ` function i specified above .

Try using transpose . #CODE
@USER I guess that's because the column ` country_name ` is string , and after transposing , it has no numeric value . try drop that column and then transpose .

i have multiple PA columns . I want to know if there is a generic python panda code I can apply
With that as a template for the desired output , we can just loop over the relevant columns of the original dataframe and use ` groupby / apply ` with ` join / unique ` to replace the values in the existing columns : #CODE
Hi Thanks for the solution . I tried to apply this on my data set . However it gives an error " T #URL item 0 : expected string , int found " .Upon close inspection .. my data set has dates and integers .. so i converted the data frame to strings using applymap ( str ) .. that is still not helping .. how can i concatenate the values ?

Now I wanna append this group mean as a new column into the original dataframe , in which each mean matches the corresponding Y002 code ( 1 or 2 or 3 ) . Actually I tried this code , but it only shows NAN . #CODE

Do you know how to create a list in Python ? You can append to it using its ` append ` method and get the last two elements as ` l [ -1 ]` and ` l [ -2 ]` .

So there is no variation at all in ` y ( A )` , and that's why ` statsmodels ` returns all 0 coefficients . As a reminder , it's always a good practice to normalize your data first before running regression .
RE : " it's always a good practice to normalize your data first before running regression " #URL

In the link above , look at ` In [ 3 ] / Out [ 3 ]` and you will see that it would convert data where the port is in the row to data where the port is in the column , except it does this with a column called " variable " instead of " port " and there is 1 value reorganized instead of 2 . I don't see a quick exact fit to your problem , which may require ignoring / eliminating some columns before using ` pivot ` . To play with it , it may be helpful to start with ` import pandas as pd ` ( if that doesn't work , install it with your package manager or perhaps ` pip install pandas `) and ` df = pd.read_csv ( filename )`
You can use ` set_index ` together with ` stack ` . #CODE

the diff column may also be a ` datetime ` object or a ` timedelta ` object , but the key point for me is , that I can easily get the Year and Month out of it .
Where row is the dataframe ` row ` . I am assuming your ` start ` and ` end ` columns are ` datetime ` objects . Then you can use ` DataFrame.apply() ` function to apply it to each row . #CODE
Had to use map instead of apply because of pandas ' timedelda64 , which doesn't allow a simple addition to a datetime object .

Pandas : drop column and release memory
To release memory I want to drop the redundant Date and Time columns .
The ` del ` part releases memory but not the ` drop ` part . #CODE
Actually , we store the tick data on a server running MonetDB . This is the code I use to retrieve data from the server and align it to a regular time grid ( e.g. one price every 15 seconds ) . In this particular case a colleague wants to have data sampled every second which breaks the available memory
do the ` drop `
From the main process , ` join ` the child process , and read the reduced DataFrame from the disk .

I want to merge them together based on matching id's in the first column ( SP1 in df1 and SP2 in df2 ) but I want to retain the id's that are only in one of the dataframes still .

Drop missing values from the data before plotting .
Looking at the source code , ` dropna ` appears to only only apply to the values that get assign to ` hue ` . For now , simply drop the NAs yourself via the appropriate pandas methods .

I figured out the problem . My initial dataset in in csv comma separated format . i have to merge them in to the same cell before applying this method . Or is there are way to marge the columns in pandas ?

There might also be a way to do this with a loop or ` apply ` , can't quite think how though .
You could pass a dict to ` SeriesGroupBy.agg ` . The keys of the dict become columns in the result , and the values can be callables or names of common stats ( such as `' min '` , `' max '` , `' mean '` , `' median '` , `' prod '` , `' std '` , `' var '` , `' sum '` , `' size '` , `' first '` , `' last '`) . For example , #CODE

Also , looks like you may have row-oriented data and pandas is going to expect column-oriented , you might want to transpose .

A small note here : Why do you install ` pandas ` via ` setup.py ` instead of using ` conda ` ? The really cool thing about anaconda is that it ships already compiled versions of various packages of the stack of scientific software . So when using ` conda install ` you should never run into these kind of issues .

So I am wondering whether there is a way to accelerate this process ? maybe some vectorized functions on datetime object that I was not aware of ? I think one way to slightly improve the speed is to use ` multiprocessing ` module , and maybe I could expect 4-6 time faster on a 8-core PC . Also , because I invoke python function in the ` apply ` , cython or jit does not help in this case ?

join DataFrames on a partially matching index
I'm trying to find a more elegant way to join two ` DataFrame ` s where the index levels of one DF are a partial subset of the index levels of the other DF . This is a very common operation in SQL and I'm surprised to find it's so difficult to do with ` pandas ` :
So far so good . But now I try to join the latter two together : #CODE
Argh !! Because the index of the second table doesn't exactly match the index of the first table , the join fails . This seems totally contrary to the intuitive behavior of an SQL-like inner join .
Workaround : Truncate the index of the first table
Drop all pretense of using an index , and join without index
If using ` join ` but not ` merge ` ( FML !! ) there is another bizarre side effect : the joined- ` on ` column is reduplicated in the output : #CODE

to drop ` b ` .
prefix : string , list of strings , or dict of strings , default None - String to append DataFrame column names Pass a list with length equal to the number of columns when calling get_dummies on a DataFrame . Alternativly , prefix can be a dictionary mapping column names to prefixes .
This will append some prefix to all of the resulting columns , and you can then erase one of the columns with this prefix ( just make it unique ) .

This answer is a little sparse on the details , but pivot or pivot_table would be another good way to approach the problem .
Instead of using ` unstack ` to swap index levels , you could use ` swaplevel ` : #CODE
And then some switching around , resorting , etc . The main " problem " you have in your data is that you start with the year in the row axis but the month in the column axis . So what you need to do is move the year indexing from the row to the column . That's done with ` unstack ( level= ' year ')` . The rest of it is basically just a matter of cleaning up . #CODE

@USER suggested ` merge ` : #CODE
The merge method in pandas is fairly optimized , so I tried my luck with it and it gave me a significant speed increase . Given my machine is a bit slower than yours , I'm also using pandas 0.15.2 Things may be a bit different . #CODE

Pandas join 2 dataframes
But I try to join them together : #CODE

Is there an elegant way to merge that list with the reset of the data so I can export to JSON more easily ?

and I would like to cut off the ` NaN ` s at the beginning and at the end ONLY ( i.e. only the values incl . ` NaN ` from 1950 to 1954 should remain ) .

Pandas DataFrame : replace all values in a column , based on condition
I want to select all values from the ' First Season ' column and replace those that are over 1990 by 1 . In this example , only Baltimore Ravens would have the 1996 replaced by 1 ( keeping the rest of the data intact ) .
How can I replace just the values from that column ?

Complicated merge based on start and end date pandas
My question is now ; is there a way to append ` Fix ` and ` Performance ` as columns to ` df ` such that the elements of the respective columns are on the rows where ` Start ` and ` End ` are valid , as determined by ` Date ` ? Which means that my table would look like #CODE
Here is one approach where you ` apply ` a function row by row to generate the two wanted columns : #CODE
You can first do a SQL-style ` outer merge ` and then remove those inconsistent records with ` Date ` falling out of ` Start-to-End ` Interval . #CODE
Thanks @USER ! Never occured to me to use the outer merge .

using ` .values [ 0 ]` causes confusions for integer index . Replace it by ` .iloc [ 0 ]` . #CODE

Your data don't have any nulls , so I don't understand how your custom function produces any meaningful output . The size or count of non-nulls of any column in any group will be the same as the length of the group , so it'll just return len ( group ) * 2 , right ?
yes . as stated in the question its a silly example . but its more that dplyr offers me elegant code while also allowing for a flexible syntax . i was wondering how to make this exact code work in python . its more about how to split apply and combine in a similar fashion . not so much that the end result has any meaning .

As you can see , the ' freq ' is None . I am wondering how can I detect the frequency of this series and set the ' freq ' as its frequency .
If there are gaps , is freq set by the smallest difference two timestamps ?
Maybe try taking difference of the timeindex and use the mode ( or smallest difference ) as the freq . #CODE

Under ` File origin ` select ` 65001 : Unicode ( UTF-8 )` from the long drop down list .
Currently I do that and then replace the extra symbol by a space which works but is a bit silly imo .

@USER , seems like that would work , I'll try it . tobias_k , seems like Mauris and you are on the same page . I'll ge back to you .

Apply multiple functions to multiple groupby columns

I have a data set which is actually a occurrence matrix of a feature vector for some numbers of items . In theory , this type of representation helps to apply machine learning algorithms to data set as its normalized . #CODE
How could I apply decision tree algorithms such as CART and Naive Bayes for this type of data sets ? ( I only checked scikit learning library )

Python Pandas Pivot Table Groupby Date Columns Using 7-Day Frequency
Using Python 3.4 and Pandas , my pivot table looks like this : #CODE
What I want to do is group by the ` Day ` COLUMNS using a 7-day ` frequency ` to get a ` summed ` pivot table that looks like this : #CODE
One way is to use ` .dt ` of ` pd.Series ` to get ` weekofyear ` and do pivot based on that column . #CODE
Thank Jianxun Li . In my example , I am trying to find the 7-day frequency because it's the mid-point , not because I'm interested in weekly increments . It looks like the take-away though is to assign two labels like a ' Before ' and ' After ' period before sending to the pivot table . I was hoping for a solution that could group the column ` dates ` data while in the ` pivot table ` without having to pre-process the ` dataframe ` ( assign ' Before ' / ' After ' labels ) . Is there a way to groupby dates in a pivot table if those dates are columns using Pandas ?

why I have to transpose the data in ` f = np.reshape ( kernel ( positions ) .T , X.shape )`
why I have to transpose the data in ` f = np.reshape ( kernel ( positions ) .T , X.shape )`
Thank you so much for your detailed answer . But as you have mentioned that the grid is used in the derivative , 1 ) Does this affect getting the correct zero crossings for the derivative of the original data ?. I plot KDE result in scatter ( x , y , z , c =d ensity ) . However , 2 ) I don't know the best way to show the flow ; plot original data with KDE , show first derivative and also the zero-crossings ( peaks ) , Can you give me any guidance on how to show changes from original data until zero crossing to show the peaks ?. 3 ) If I want to get the number of zero crossing , should it be only len ( Zerocross ) ?.

There are two reasons whiskers length vary from one boxplot to any other boxplot
Are you asking why the top whisker isn't the same length as the bottom ? I think the whiskers are actually the lowest or highest data point within 1.5 IQR . So if there are no data points between Q3 and Q3 + 1.5 IQR , then the top whisker won't show up . For the one boxplot where the are outliers beyond the whiskers on both the top and the bottom , the whiskers do look about the same size .

How does one shift everything up , including entries to replace the column name . Is there a way to do this ?

@USER Uhh , I actually encounter the same problem as you did , and I convert it mannually using ` df = df.fillna ( ' NA ')` to use ` NA ` replace all ` NaNs ` .

If I convert it to a dataframe and call ` df.dtypes ` , it says ` dtype : object ` and I can convert the values of Column A to bool , int , or string , but the dtype is always an object .
Please put any code and values in the question itself , not comments . And indent 4 spaces or enclose in backticks ( key at top left of keyboard ) for readability . The list comprehension looks fine to me but I suggest you try the code I put in my answer below as it should be all you need , with some minor adjustments for however many characters you want to keep / drop .

Using Merge on a column and Index in Pandas
I have two separate dataframe that share a project number . In ` type_df ` , the Project number is the index . In ' time_df ' , the project number is a column . I would like to count the number of rows in ` type_df ` that have a ` Project Type ` of ` 2 ` . I am trying to do this with ` pandas.merge() ` . It works great in both use column , not index . I'm not sure how to reference the index and if merge is even the right way to do this . #CODE
If you want to use an index in your merge you have to specify ` left_index=True ` or ` right_index=True ` , and then use ` left_on ` or ` right_on ` . For you it should look something like this : #CODE
You must have the same column in each dataframe to merge on .
In this case , just make a ' Project ' column for ` type_df ` , then merge on that : #CODE

Applying aggregate function on columns of Pandas pivot table
I generated the following pivot table via taking maximum of values in ` Z ` column : #CODE

Selecting the first n groups is a bit vague , perhaps you mean ** how can you join the first n groups into a single dataframe** .. something along those lines ? And also , how would you like to select the groups ? Randomly , or according to the population of the group , etc ?

I would like to drop the lines in a dataframe ` where ( EndTime CurrentDate )` .

Using pandas to append hdf5 table leads to a pytable exception AttributeError

Merge on key ( unicode column name ) error
There were two pandas dataframe like above which called ' left ' , ' right ' each . and I tried merge like below code . #CODE
But unfortunately , the error occurred . It seems pandas merge left ( right ) _on=key feature couldn't recognize unicode column name . #CODE
Sorry for confusion everyone . It seemed to me but was not unicode issue . It's just because of I tried merge right after groupby . like this .
By default , groupby output has the grouping columns as indicies , not columns , which is why the merge is failing .
Then , your merge should work as expected .
Anyhow , back to the example of my question , the dataframe ' left ' column u ' ? ' was an index not column because I did groupby on ' left ' without as_index=False just before merge .

Inspect a large dataframe for errors arising during merge / combine in python

You want the resample method of a timeseries data-frame . Data gaps get NaN values .

why not just do a many-to-one merge to create a column of constants ` 1.4 1.1 1.5 ` for each state ` CT MA CA ` and do the calculation column-wise . Iterating row-by-row is a bit slower .
I'd construct a dict of your states and desired multiplication factor and just iterate over the dict to get the state and cost factor tuple , use ` loc ` and the boolean mask to selectively multiply only those rows in your df : #CODE

Would it be better for you to simply replace all the ` NaN ` for something like " NorthAm " . You can do this ` df = df.replace ( np.NaN , ' NorthAm ')`
@USER Yes , that would be a good tactic , however there are some actual NaN entries that I want to stay NaN . They are in a different column though , so is there a way to replace NA to NorthAm in only the Region column ?
I don't think you need to worry about ' NA ' once you've read in correctly . Take a look at the output of ` result.columns ` . The error message is just telling you that ` result ` has 4 columns and you're trying to replace with 3 columns . You just need to figure out why you're losing a column you expected to be there .
Here's what you can do to replace the ` NaN ` in one column only : #CODE

Hi ! Jianxun ! Thanks for answering the question ! But I cannot apply this to my code ... I posted another question .. It would be great if you could please check it ..

Replace values in Pandas Series Given Condition
I want to replace all values greater than 1 with 0 . How do I do this ? I've tried

and you can replace the 0s and 1s by whatever you want .

This you should do a groupby on option and apply a sum and retrieve the count ...
The expanded dataframe is basically going to be used in a pivot table like fashion to see how respondents graded various questions .
Yes , this line is just because having 5 digits was making my formatting of table on stack overflow more difficult :) .
For you you just need the function and to apply it to your dataframe

How to replace strings in a DataFrame column using priority based regex ?
Some of the job titles have multiple actual titles like " CEO and Director of Marketing " or " Vice President and Software Engineer " so I would like to replace with the highest ranked bucket . Here's what I've done so far ( which is not working ) . #CODE
The ` replace ` method under ` str ` is easier to use . You mighe have to modify the regex in my code to fit your data , but just remember to match the whole string . #CODE

Suggestion : install genumeric and use the helper function ssconvert to translate the file to csv . In your program change to read_csv . Check the time used by ssconvert and the time taken by read_csv .

how do i get the diff of the Date Index ?
That does it . Actually , I also realised that i needed to format the dates from strings before applying the above ( i.e. replace the first line with df [ " Diff "] =p d.to_datetime ( df.index , dayfirst=True ))

However , no matter how I banged my head , I could not apply ` .str .contains() ` to the object returned by ` df.columns ` - which is an ` Index ` - nor the one returned by ` df.columns.values ` - which is an ` ndarray ` . This works fine for what is returned by the " slicing " operation ` df [ column_name ]` , i.e. a ` Series ` , though .
( one could apply any of the ` str ` functions , of course )
Then , I found the ` map ` function and got it to work with the following code : #CODE
Of course in the first solution I could have performed the same kind of regex checking , because I can apply it to the ` str ` data type returned by the iteration .
I am very new to Python and never really programmed anything so I am not too familiar with speed / timing / efficiency , but I tend to think that the second method - using a map - could potentially be faster , besides looking more elegant to my untrained eye .
EDIT : I just found the ` Index ` method ` Index.to_series() ` , which returns - ehm - a ` Series ` to which I could apply ` .str .contains ( ' whatever ')` .
Your solution using ` map ` is very good . If you really want to use str.contains , it is possible to convert Index objects to Series ( which have the ` str.contains ` method ): #CODE

From this I am finding the min and the max . I want to join this to an pandas dataframe . Below is my desired output : #CODE
Sorry for the late response but if I have the np.random.choice within a loop to produce a bunch of outputs how can I append them all to one dataframe ?
if you get a chance please look at how I can append this from a loop
Simply iterate through simulation and append values into dataframe : #CODE
I believe that is an inefficient approach , though a common one . DataFrames , like arrays , occupy contiguous memory and it is very expensive to append to them . It's always better to append to a list ( which is designed for that ) and convert to a dataframe at the end . Also , you don't need the 0 in range , and you should use vectorized np.max and np.min on the whole record instead of individually on the rows . Just my two cents .
Excellent points @USER ! Indeed , dataframes are intended to load at once and not appended . Only until recently did pandas allow the ` df.loc [ i ]` as a row append . And this [ SO post ] ( #URL ) shows the popularity of the row append . Plus , the OP mentioned running simulations many times . Feel free to downvote , but you'll get the upvote .

` test.max ( columns=1 )` finds the max in each column ( like R's ` apply ( test , 2 , max )`)

but its not very handy . After ' groupby ' , I need to merge the new dataset with the old one .
You still need to join it back to your original DataFrame : #CODE

Excellent thank you . I wasn't aware of shift .

I have 2 questions ( stack ) here .

Hi Anand , yes indeed I want and but if I replace & with and it returns :

Cyclic shift of a pandas series
I am using the shift method for a data series in pandas
Is it possible do a cyclic shift , i.e. the first value become the last value , in one step ? #CODE

You want to ` pivot ` : #CODE

Until we get native support for a Cartesian join ( whistles and looks away .. ) , merging on a dummy column is an easy way to get the same effect . The intermediate frame looks like #CODE

replace NaN with empty list in pandas dataframe
I'm trying to replace some NaN values in my data with an empty list [ ] . However the list is represented as a str and doesn't allow me to properly apply the len() function . is there anyway to replace a NaN value with an actual empty list in pandas ? #CODE
This works using ` isnull ` and ` loc ` to mask the series : #CODE
You have to do this using ` apply ` in order for the list object to not be interpreted as an array to assign back to the df which will try to align the shape back to the original series

But then I try to apply it to DataFrame obtained from a .csv file : #CODE

My index is the result of some selection , and it is composed of not correlative #URL [ 3 , 24 , 34 ] ix [ 0 ] throws an error .

how to change the following example to apply for a column ? say , for any cell in A1 : A15 , Ai , if the cell in column C in the same row , Ci , is less than the cell in A , we put red color in the cell Ai ? I can do a loop but it seems slow for a big file , maybe due to too many I / O in loop #CODE

loc function in pandas
Can anybody explain why is loc used in python pandas with examples like shown below ? #CODE

You can always drop to the lower-level ` matplotlib.hist ` : #CODE

How to transform / interpolate / rebucket columns in Pandas DataFrames based on column names ?

Pandas join two dataframes based on multiple dates
I have two ` pandas ` ` DataFrames ` , where I am trying to preform an advanced join . In the following example I want to join ` df ` and ` df2 ` based on ` my_key ` where the date range ` from_dt ` and ` to_dt ` have an overlap . How can I do this using pandas ?

Ideally I would like something like ` apply_chunk() ` which is the same as apply but only works on a piece of the dataframe . I thought ` dask ` might be an option for this , but ` dask ` dataframes seemed to have other issues when I used them . This has to be a common problem though , is there a design pattern I should be using for adding columns to large pandas dataframes ?
whats about using the apply method ?
However , if your operation is highly custom ( as it appears to be ) and if Python iterators are fast enough ( as they seem to be ) then you might just want to stick with that . Anytime you find yourself using ` apply ` or ` iloc ` in a loop it's likely that Pandas is operating much slower than is optimal .

` unstack ` to move it into a new column index level . I think ` unstacking ` with #CODE
And now undo the ` unstack ` operation , thus moving the column index level back

Since we want to align on the bounds , I dropped down to the underlying bin indices : #CODE
Note that if we have an element which doesn't fit in a bin , say 100 , then the categorical will give NaN and the code -1 , and so it'll ( correctly ) be skipped when we insert into ` df_new ` .

You need to modify the apply ` func ` as below to count consecutive non-zero values . #CODE
@USER No , just replace the ` def func ` with the one shown in ` Edit ` part of my post .

You can take the groupby result , call ` max ` on this and pass param ` level=0 ` or ` level= ' clsa '` if you prefer , this will return you the max count for that level . However this loses the ' clsb ' column so what you can do is ` merge ` this back to your grouped result after calling ` reset_index ` on the grouped object , you can reorder the resulting df columns by using fancy indexing : #CODE

I'm trying to export ` pandas.DataFrame.describe() ` to ` LaTex ` using the ` to_latex() ` -method . This works all fine as long as I don't apply the ` groupby() ` -method beforehand . With a grouped DataFrame , the first row has no values , even though its label is ` count ` . Note that the first row of a grouped dataframe is used to mark down the variable used for grouping in iPython notebook .

pandas dataframe with Date index -> insert into MySQL
It's a long-shot and that's why I'm writing a comment and not an answer , but does it work if instead of ` dtype={ ' date ' : datetime.date } ` you use ` dtype={ ' date ' : datetime.datetime } ` ? If not , does it simply insert ` null ` to the db or does it throw an error ?

You can just filter the df with your boolean condition and then call ` len ` : #CODE

You can define a func which takes the values , sorts them , slices the top 2 values ( ` [: 2 ]`) then calculates the difference and returns the second value ( as the first value is ` NaN `) . You ` apply ` this and pass arg ` axis=1 ` to apply row-wise : #CODE
Thanks perfect ..!!! With " " instead of '' for df [ ' diff '] = df.loc [: , " Time_1 " :] .apply ( func , axis=1 )
This can go wrong depending on the column names . It's safer to use ` .diff() .iloc [ 1 ]` . Also , this is pretty inefficient as it uses sort and apply , neither of which are necessary . See my answer below .
Here's an elegant solution that doesn't involve sorting or defining any functions . It's also fully vectorized as it avoid use of the ` apply ` method . #CODE

I am doing some simulations resp . a system analysis by variing parameters ( in this case ` rpm ` only ) and append every last line of a results dataframe ` results_df ` to a summarizing dataframe ` df ` containing giving the baviour of my system in depencence of the varied ` rpm ` .
In order to get an appropriate index for plotting and data analysis I converted the varied values ( here ` rpm `) from the list into a pandas series ` ser ` and concat this series with the summarizing dataframe ` df ` containing the results I am interested in .

Unfortunately offsets don't support operations using array like objects so you have to ` apply ` the offset for each element : #CODE

and the resulting heatmap should map the ` Code ` column of the dataframe to the grid representation ( so that the cells from 11 to 25 should be colored in white , as no values is there ) .

You can ` groupby ` on ' name ' and ' id ' and just ` apply ` ` len ` function : #CODE

It's because the index values don't match so it will try to align using the indices . Try adding the column as plain np array values : #CODE
However , the above assumes that the order matches across all your csv's , if you have some identifier columns then it's better to join / merge using this so that the row values are aligned .

But it could be an unexpected system difference -- I am using Python 2.7.3 on an Ubuntu machine . An alternative you might try is to replace exit() with os._exit ( os.EX_OK ) .

pandas iloc vs ix vs loc explanation ?
` loc ` works on labels in the index .
` ix ` usually tries to behave like ` loc ` but falls back to behaving like ` iloc ` if the label is not in the index .
It's important to note some subtleties that can make ` ix ` slightly tricky to use :
if the index is of integer type , ` ix ` will only use label-based indexing and not fall back to position-based indexing . If the label is not in the index , an error is raised .
if the index does not contain only integers , then given an integer , ` ix ` will immediately use position-based indexing rather than label-based indexing . If however ` ix ` is given another type ( e.g. a string ) , it can use label-based indexing .
As per the subtleties noted above , ` s.ix [: 6 ]` now raises a KeyError because it tries to work like ` loc ` but can't find a ` 6 ` in the index . Because our index is of integer type it doesn't fall back to behaving like ` iloc ` .
If , however , our index was of mixed type , given an integer ` ix ` would behave like ` iloc ` immediately instead of raising a KeyError : #CODE
Keep in mind that ` ix ` can still accept non-integers and behave like ` loc ` : #CODE
General advice : if you're only indexing using labels , or only indexing using integer positions , stick with ` loc ` or ` iloc ` to avoid unexpected results .
If however you have a DataFrame and you want to mix label and positional index types , ` ix ` lets you do this : #CODE
Using ` ix ` , we can slice the rows by label and the columns by position ( note that for the columns , ` ix ` default to position-based slicing since the label ` 4 ` is not a column name ): #CODE
Great explanation ! One related question I've always had is what relation , if any , loc , iloc and ix have with SettingWithCopy warnings ? There is some documentation but to be honest I'm still a little confused #URL
@USER : ` loc ` , ` iloc ` and ` ix ` might still trigger the warning if they are chained together . Using the example DataFrame in the linked docs ` dfmi.loc [: , ' one '] .loc [: , ' second ']` triggers the warning just like ` dfmi [ ' one '] [ ' second ']` because a copy of data ( rather than a view ) might be returned by the first indexing operation .
and so on . Now , it's probably worth pointing out that the default row and column indices for a DataFrame are integers from 0 and in this case ` iloc ` and ` loc ` would work in the same way . This is why your three examples are equivalent . If you had a non-numeric index such as strings or datetimes , ` df.loc [: 5 ]` would raise an error .
I think it's also worth mentioning that you can pass boolean vectors to the ` loc ` method as well . For example : #CODE

How to append a 1 to the end of each single digit element within an array , list , or dataframe in Python Pandas ?
The items are currently integers , but they can be converted to ` str ` or ` bool ` , whatever necessary to do the task .
You should use a string array and find the length of each string . if len ( variable ) < 2 then variable+ " 1 "
probably easier to multiply by 10 and add one than to cast to string , concat a " 1 " , and re-cast to int :)

This can be changed by setting ` freq = pd.tseries.offsets.DateOffset ( months=6 )` . #CODE

So this works because it will drop all rows that contain a single ` NaN ` so you just use the first index value from this to slice the df : #CODE

I'm trying to write a custom function that essentially extends replace functionality ( with some domain specific concerns ) Essentially I have a series that looks like : #CODE
For the first one you can achieve this using ` map ` so something like ` s.map ( s1 )` assuming ` s ` and ` s1 ` are your respective series , for the second output you can do this using ` merge ` so something like ` pd.DataFrame ( s ) .merge ( df , left_on =[ col_name ] , right_index=True , how= ' left ')`
I don't know how your data really looks like so you may need to modify my code slightly but the following works using ` map ` : #CODE
for the second one you can perform a left ` merge ` but you have to construct a dataframe from your series : #CODE
For the above you can set the index after the merge

Merge 2 data frames in pandas
and I need to merge them according to column Time - to get the coordinates of satellites from only the same time ( I need all GPS coordinates and all Glonass coordinates from particular time ) , the result from above example should look like this : #CODE
Could you please display what happens when you use ` how=inner ` ? Because when you merge those two datasets the first row of Glonass should end up on all the rows of GPS where ` time= 2013-06-01 00:00 : 00 ` . Or am I wrong ?
You have 4 entries for ' 2013-06-01 00:00 : 00 ' in your GPS df and 1 in your glosnass df , if you merge these why would you not expect to see your glosnass values replicated for each of these time entries ?

The only way I can think of to do this is to loop over ` mylist ` and create an new dataframe for each element of it and merge / concat or whatever them afterwards . But that doesn't look very smart for me .
Calling the DataFrame's ` any ` method will perform better than using ` apply ` to call Python's builtin ` any ` function once per row .
You can ` apply ` the function ` any ` along the rows of ` df ` by using ` axis=1 ` . In this case I'll only apply ` any ` to a subset of the columns : #CODE

I have tried playing around with ` input_df.names.str.contains() ` and ` input_df.names.isin() ` , but I can't figure out how to find a name in ` input_df1 ` that matches a name in ` input_df2 ` , compare them for the shortest name , and then replace the longer one with the shorter ( which is what my mind thinks should be done ) .

except that I want to read data from a list of Excel files . I have a list of filenames called ' filenames ' that I want to merge into a single dataframe .
- merge the whole list of dataframes into one dataframe

Another way might be to sort the list , compare and shift successive elements , and then sort back .
@USER Agreed , ` 0 ` was not a good initial value for ` last ` . About Python 3 : ` map ` will return an iterator that is exhausted after the first loop . Edited .

I strongly feel there is a much more efficient approach to solving my problem that utilizes the capabilities of Pandas in a much better way . I was forced to go for the looping-thing since I couldn't figure out how to align the rows ( i.e. games ) from different data frames based on date index . Besides , if I happened to know how to align the rows , I dunno how to sort the columns ( i.e. rank the teams ) as a whole .
Further , a potential problem is that not all teams are playing at each day . But I guess I just resample the data so that there are no ' missing ' dates for teams ?

Yes , just remove copy=False and then it won't be a copy . So just replace #CODE

String replace within index / MultiIndex
I also have a dictionary with values to replace my codes with : #CODE
Clearly the downside here is that the ` replace ` will replace throughout the ` DataFrame ` , not just in the index columns .
On the upside , doing it this way , you get access to all the functionality of ` replace ` like regular expressions .
Then finally change ` codes ` for ` codes_dict ` in the call to ` replace ` . Perfect !

My current methodology is : create an identical " filler " column in both dataframes , merge on this column ( creating an len ( dataframe1 ) *len ( dataframe2 ) length dataframe ) and then filter on the columns I want : #CODE
It feels like naming the columns `" head1 "` , and `" head2 "` is the problem . Maybe if you tell us what you want to do with the columns afterwards , it will be easier to give you a better solution . I have a feeling you actually want to concat the dataframes and then reduce them down to solid second observations .

For each station that I'm working with . I want to be able to apply these using a dictionary like so : #CODE
I just found the ` truncate ` method which will definitely work for this ... Any other approaches are welcome .
Thanks for this . I found out that to do what I've mentioned I needed to replace ` slice ( None )` with simply ` None `

How can I drop columns such that ` number_of_na_values 2000 ` ?
In my tests this seems to be slightly faster than the drop columns method suggested by Jianxun Li in the cases I tested : #CODE

` count ` returns the number of ` non-NaN ` values in your ` Title ` columns , and it contains duplicated values . ` unique ` of course doesn't contain duplicated values , and that's why its ` len ` is less than ` count ` .

You can replace ` mean ` for any function you'd like . You can replace " hour " with " second " , " minute " , " hour " , " day " , " week " , " month " , " year " . Well you can't go from minute to seconds as that would require magic but you can go from micro seconds to seconds anyway .
There's not a built in way to do it that I know about but you could do something like ` dt [ , lapply ( .SD , mean ) , by=minutes ( floor ( as.numeric ( difftime ( ts , ymd ( ' 1970-01-01 ') , units= " mins ")) / 5 ) *5 ) +ymd ( ' 1970-01-01 ')]` where you find the number of minutes since some arbitrary date divide by whatever range you want , take the floor of that , multiply that by the same thing you divided by and add it to the same arbitrary date .

To get distribution / histogram , you can use ` value_counts() ` on ` pd.Series ` object and then normalize by ` .sum() ` to calculate percentage . #CODE

Or use the ` csv ` lib to read and join the columns after zipping with transposing with ` itertools.izip ` : #CODE
If you actually want to keep the other columns just drop after creating the new column : #CODE
@USER , are you trying to join the first two and keep all the rest or just create a df from the first two ?
If the lines really were in csv format , you'd just change the arguments for split and join above from ' ' to ' , ' .

Please take some time to read the [ help page ] ( #URL ) , especially the sections named " What topics can I ask about here ? " and " What types of questions should I avoid asking ? " . And more importantly , please read the [ Stack Overflow question checklist ] ( #URL ) . You might also want to learn about [ Minimal , Complete , and Verifiable Examples ] ( #URL ) .

pandas : create a left join and add rows
I would like to join two Datafame together #CODE
Indeed your result df doesn't make sense are you sure don't just want an outer merge like my answer ?
Perform an outer ` merge ` : #CODE

I want to transform it into a single column data with index being year-month . I try to stack my original data but it becomes a time series , which has the year mix with my values . #CODE
` set_index ` to ` Year ` first , and then ` stack ` . #CODE

You could also run ` python -m pdb / path / to / drugs.py ` . This will drop you into the pdb debugger . Press ` c ` to continue ( i.e. start running drugs.py ) . It will break on the ` TypeError ` . Then ` p key ` to print the value of ` key ` . What slice could this possibly be ? Then press ` u ` to go up a frame . You might press ` u ` 4 times to get to ` self._set_item ( key , value )` . Then try ` p key ` and ` p value ` to see the values of ` key ` and ` value ` . Do they match what you expect ? ` key ` should be the string `' year '` and ` value ` should be an int , the value of ` year ` ...

Looks like a nice usecase for a multi column apply . Just write a function with your mapping dict . Apply this function to slice of your columns . Finish . #URL
Try this out . ` Series.map ` should be a faster way to look up values from the dictionary . ` pandas.get_dummies ` turns a single column of data into columns for each distinct value as 1s / 0s , which I'm converting into a bool , and compare with or ( ` | `) to get whether the service was on either port . #CODE

Raise or translate to caller

Yeah , it would break it but you can replace ` ATC.str ` with ` ATC.astype ( str ) .str ` . It's just a matter of whether that is sufficiently helpful or not with memory to be worth the bother .

You initialize ` all_treatments ` to a DataFrame , and then append to it . This is very inefficient . Try ` all_treatments = list() ` , and add ` all_treatments = pd.concat ( all_treatments , ignore_index=True )` outside the loop just before your ` groupby ` . In addition , it should be ` all_treatments.append ( treatments )` ( vs . ` all_treatments = all_treatments.append ( treatments )`)
` AttributeError : ' NoneType ' object has no attribute ' append '`
It becomes None with the first attempt at append . It is an empty list before , as initiated .
I think you need to find a way to vectorize your solution . Using ` map ` and lambda functions is not efficient and doesn't make use of the speed-ups that make pandas so useful . It's hard to say for sure because you haven't posted sample data , but I think a good starting point would be to do #CODE

Pandas join / merge 2 dataframes using date as index
The output should look like this ( I want merge only data with dates whitch appear in both dataframes ): #CODE
By adding the ` count ` column , you can now merge on both ` Date ` and ` count ` which gets you close to the result you want : #CODE
Another way to restrict the merge to only those dates which are common to both
` df1 ` and ` df2 ` would be find the intersection of ` df1 [ ' Date ']` and
` df2 [ ' Date ']` first , and then apply ` pd.merge ` to sub-DataFrames of ` df1 ` and ` df2 ` which contain only those dates : #CODE

Note : there may be more document numbers in the dataframe than there are .txt files ; for anything that isn't in folder0 or folder1 , I'd like to write NaN values and then just drop those from the final dataframe .

In the workflow I replace the 0 values with NaN values to avoid this issue

How to do a timeseries join in pandas when dates don't match ?

Call ` apply ` on the ' scores ' column on the ` groupby ` object and use the vectorise ` str ` method ` contains ` , use this to filter the ` group ` and call ` count ` : #CODE

Pandas Dataframe resample
I tried to use the set_index ( TS ) , but got instant mermory error . ( Same when i use reset_index() ) . So was hoping that something like resample ( ' 15S ' , COLUMN = TS ) would do the trick . Or might it be that the table i have opened is to big to do any operations on ? If so , is there anyway i can solve this problem ?

Can you describe how you envision the structure will translate to a dataframe ? Do you wish to flatten the nested structure ?
Does not seem to conform to any of the supported formats as it seems to be only a single " record " . Pandas expects some kind of collection .

I'd like to have a dataframe with one row per id . That means I need to consolidate the value2 and value39 boolean value into one row .
The above works . I added an edit to my original question . I want the new dataframe to have one row per id , which means I need to consolidate the boolean values into one row and get rid of the duplicates .

Still not getting the hang of pandas , I am attempting to join two data frames in Pandas using merge . I have read in the CSVs into two data frames ( named dropData and deosData in the code below ) . Both data frames have the column Date_Time , which is a parsed column of Date and Time information to create a unique id for each entry . The deosData file is an entire year s worth of observations that I am trying to match up with corresponding entries in dropData .
I have gone through the documentation for the merge function and have tried the following code in various iterations , so far I have only been able to have a blank data frame with correct header row , or have the two data frames merged on the 0 -- ( N-1 ) indexing that is assigned by default :
After searching on SE and the Doc s I have tried resetting the index , ignoring the index columns , copying the Date_Time column as a separate index and trying to merge on the new column , I have tried using on=None , left_on and right_on as permutations of Date_Time to no avail . I have checked the column data types , Date_Time in both are dtype Objects , I do not know if this is the source of the error , since the only issues I could find searching revolved around matching different dtypes to each other .
What I am looking to do is have the two data frames merge where the two ' Date_Time ' columns intersect . For example : #CODE
and then do your merge .
You can use ` join ` , but you first need to set the index : #CODE

It looks up ` called ` in the result of the function call rather than in the function itself . Just replace it with : #CODE

I tried with pivot table #CODE
any alternative to pivot table to do this ?
You can use ` df = df.T ` to transpose the dataframe - if I'm understanding the question correctly . This switches the dataframe round so that the rows become columns .

Outer join will still apply , but the original answer is correct : #CODE

Pandas color map

I'm looking to translate a dataframe of equipment date ranges and characteristics into their annual total install time by characteristic groupings . I'm looking to translate a dataframe like this : #CODE
Where the ` NaT ` datetime's for ` end ` represent equipment that has not yet been retired . Using this dataframe I'm looking to translate to produce the following samples where the quantities are the install time of units within the given year : #CODE

Efficiently create sparse pivot tables in pandas ?
I'm working turning a list of records with two columns ( A and B ) into a matrix representation . I have been using the pivot function within pandas , but the result ends up being fairly large . Does pandas support pivoting into a sparse format ? I know I can pivot it and then turn it into some kind of sparse representation , but isn't as elegant as I would like . My end goal is to use it as the input for a predictive model .
Alternatively , is there some kind of sparse pivot capability outside of pandas ?
edit : here is an example of a non-sparse pivot #CODE
Pivot tables are just ways to view your original data , which is already sparse ( other than converting ` person ` and ` thing ` to integers )
Here is a method that creates a sparse scipy matrix based on data and indices of person and thing . ` person_u ` and ` thing_u ` are lists representing the unique entries for your rows and columns of pivot you want to create . Note : this assumes that your count column already has the value you want in it . #CODE

` pd.DataFrame.sort() ` takes a colum object as argument , which does not apply here , so how can I achieve this ?

You could use groupby / apply with a custom ( lambda ) function : #CODE
Thank you very much ! If I would like to use the resample function of pandas ( essentially groupby slice ) , could I do that as well ?

Python / Pandas : Drop rows from data frame on string match from list
After importing the data , I'd like to drop rows where one field contains one of several substrings in a list . For example : #CODE
Another method is to join the terms so it becomes a regex and use the vectorised ` str.contains ` : #CODE
Alternatively you can read the csv in chunks , filter out the rows you don't want and append the chunks to your output csv
Minor comment on this , which likely is from the manner in which I worded the question . I'd really like the code to drop anything where a term in to_drop is contained within the text of title . So , for example , if to_drop somehow happened to contain " Bag " it would drop row 1 . How to modify to do loose matching ?

I need to find a way to import my data to a ` .dataframe() ` in such a way that I can get a value from a field from ` ( row [ i ] , col [ j ])` and work to replace it in ` ( row [ k ] , col [ l ])` .

One way to append the row of new data is via a dictionary . #CODE

However , it would be much more efficient to replace ` myfunc ` and ` rolling_df_apply ` with a call to ` pd.rolling_sum ` : #CODE

Using MSSQL ( version 2012 ) , I am using SQLAlchemy and pandas ( on Python 2.7 ) to insert rows into a SQL Server table .
` finaloutput.to_sql ( " MyDB.dbo.Loader_foo " , engine , if_exists= " append " , chunksize= " 10000 ")`

Then , join the two and product two scores : #CODE
Apologies , but when I run >>> ` scores.hist() ` , I get ` TypeError : unorderable types : str() < float() ` The porton of axes.py that gives this error should only do so when ` bin ` is specified as a hist parameter . However , even specifying ` bin ` does not solve the problem . I continue to get the error and a figure with 4 empty subplots . Is this a matter of version ? I'm on pandas version 0.12.0
I checked above code works and my pandas version is 0.16.1 . I need more details to figure out what is the problem . By the way , I'm not sure it is just a typo but there is no ` bin ` option for ` hist ` ; what it has a ` bins ` option .

I want to join the two dataframes into one output . As you can see , the main problem is that the indexes from each table are different from each other . I want the output to follow the second table's format . Also the dates which each table starts from are different .
How would I join these two dataframes ?

AttributeError : ' str ' object has no attribute ' transpose ' I have also tried to move it inside and move all the lines beginning with data but nothing seems to work
it does not run into an error and returns data . However , the result for one url gives me dataframe with 48 columns and I should append each new data row to this dataframe . Yours returns me a dataframe with 383 rows x 345 columns

You can define a list of the cols of interest and pass this to the groupby which will operate on each of these cols via a lambda and ` apply ` : #CODE

Once you make the initial definitions then you can replace `' is_good '` with any other column or subset of columns that you're not interested in so you would only have to do it once .

and I want to plot a histogram together with normal distribution in 1 plot . Is there a plotting function that takes mean and sigma as arguments ? I don't care whether it is matplotplib , seaborn or ggplot . The best would be if I could mark also mode and median of the data all within 1 plot .
You can use matplotlib / pylab with ` scipy.stats.norm.pdf ` and pass the mean and standard deviation as ` loc ` and ` scale ` : #CODE
Either multiply it by a constant ( but then it won't be a normalized distribution ) , or normalize your histogram ( I think there's a ` normed=True ` argument in matplotlib ) .
True ... though I think you could normalize the data , twin the x-axis for the plotted distribution and then set both y-axis limits to be the same .

How to append rows in a pandas dataframe in a for loop ?
I tried append , concat . .. without success . .. thank you !

I have two dataframe and try to join them but failed , can you please help me .
I want to join them to be : #CODE
The reason your code doesn't work is because its trying to align on index but the index values don't match so it's better to just add the column as anonymous np array values like JoeCondron has answered
So just insert a column to the second data frame , since this is the one you want to keep the index of . I just chose `' values '` as an arbitrary name for the column in the result but you can put anything you lie there .
Merging ( Joining ) would need a key to join on . #CODE

have merged 2 dataframes with left join . works as I expected until I attempt to use the generated value in a simple string concatenation . #CODE

Turn nested dataset into dataframes as well , and then ` merge ` them into the parent dataset .

melt and crosstab on a heavy file
Contrary to R , I can melt the entire file ( which makes a loooot of rows ) , but it crashes when I do the crosstab function . #CODE

Write to_csv with Python Pandas : Choose which column index to insert new data

@USER : Yep ! But in your case you'd have to write ` n ` number of times , but here you append the values to a list and then plot them easily by calling just your ` x list ` and ` y list `

Selecting a data value from pandas dataframe based on row and column to append to list
I want to append this value 50 to Numbers which is from column 24 row 50 .

Yeah , seems like a bug . Here is the documentation on " setting with enlargement " and this should work for loc or ix ( but not iloc ) . #URL It's new with version 0.13 . I get the same message you are getting and I'm using 0.16.1 . What version are you using ?

We can do this for each row of ` dates ` by using ` apply ` : #CODE
By making ` lambda ` , above , return a Series , ` apply ` will return a DataFrame ,

How can I accomplish this in pandas so that the dataframe I get contains the statistics of each flow i.e. the columns should contain the ` ip_src ` , ` ip_dst ` , ` sport ` , ` dport ` , ` ip_proto ` , ` service ` , and the mean var values calculated as earlier . I have tried both the ` aggr ` and ` apply ` methods , but haven't been able to do it . Thanks in advance !

I have a bunch of pandas dataframes in a list that I need to convert to html tables . The html code for each individual dataframe looks good , however when I append the html to a list I end up with a bunch of ` \n ` characters showing on my webpage . Can anyone tell me how to get rid of them ?
To display 3 separate tables , join the list of HTML strings into a single string : #CODE

We can now pivot the data by date and location . Your new DataFrame is now indexed on the date : #CODE
Finally , use ` resample ` to get the mean value of your data over ten year periods : #CODE

Do you want to merge the two dataframes then ?

I first tried to use the ` hist ` method for DataFrames , but encountered an error , and a blank 4x4 series of histograms . #CODE
Realizing ` DataFrame.hist() ` " plots the histograms of the columns on multiple subplots " , moved away from this and tried ` outer_df.plot ( kind= ' hist ' , stacked=True )` . Even though I took this directly from the docs , I'm stuck on this error : #CODE
` ValueError : Invalid chart type given hist ` happens when I tried the above . Any ideas ? I'm at a loss ...

Creating large Pandas DataFrames : preallocation vs append
I am confused by the performance in Pandas when building a large dataframe chunk by chunk . In Numpy , we ( almost ) always see better performance by preallocating a large empty array and then filling in the values . As I understand it , this is due to Numpy grabbing all the memory it needs at once instead of having to reallocate memory with every ` append ` operation .
Here is an example with timing . The definition of the ` Timer ` class follows . As you , see I find that preallocating is roughly 10x slower than using ` append ` ! Preallocating a dataframe with ` np.empty ` values of the appropriate dtype helps a great deal , but the ` append ` method is still the fastest . #CODE
1 ) Why is the ` append ` method faster ? ( NOTE : for very small dataframes , i.e. ` n_rows = 40 ` , it is actually slower ) .
@USER See EDIT_2 . It is much faster , but still not faster than append .
Concat #CODE

Just apply a lambda function on the groups like so : #CODE

3for j in range ( len ( b2.index )):
1597 if len ( self ) > 0 and self.inferred_type in [ ' integer ' , ' boolean '] :
This can easily be done by performing an inner join on all of them using the ` pandas.merge() ` function . #CODE
First of all , is there ' right ' argument in join function ?
@USER , sorry , syntax error from my side . It is supposed to be the merge function , not the join function . It should work now .
I believe the reason your code is failing is because you are not resetting the index after dropping . I think if you want to do this manually , you can do something like store the indices first and then drop all at once #CODE

and I need to first map each record as follows ( pseudo code ) ` if df.ix [ row , col ] == 1 : df.ix [ row , col ] = col ` .
Welcome to Stack Overflow . You might take the [ tour ] and visit the [ help ] because your question lacks a few quality attributes we expect from posts . In the links you find guidance that helps you how to improve your question by giving it an [ edit ] .
Now , you can map column 2 : #CODE

Currently my ' ave_data ' DataFrame outputs the following data ( please ignore all dashes , they are to align data only ):
You can apply a function that returns the category : #CODE

How to append a dictionary to a pandas dataframe ?
I have a set of urls containing json files and an empty pandas dataframe with columns representing the attributes of the jsnon files . Not all json files have all the attributes in the pandas dataframe . What I need to do is to create dictionaries out of the json files and then append each dictionary to the pandas dataframe as a new row and , in case the json file doesn't have an attribute matching a column in the dataframe this has to be filled blank .

the column on which the two dataframes were joint does not have unique values . I think this may be the cause of the error . Any idea on how to join them differently ?

How to merge or combine high frequency and low frequency data in python
You probably want [ ` resample `] ( #URL ) or [ ` align `] ( #URL )

Second , I would like the average of the previous 3 IDs to populate the current ID , without the values of the current ID included in the current rolling mean iteration ( maybe with shift ? ) . For example , an output that looks like this ... #CODE
Second part is easy , just another groupby , but this time with shift . #CODE
Then use ` combine_first ` to merge where values are kept from the left-most dataframe if available . #CODE

Drop duplicate row with dicts
I read data from ` json ` , and some of them are duplicates so I want to drop them , please note that there are 2 column ( ` douban_info ` and ` omdb_info `) are still in ` json / dict ` format
So how can I drop these duplicates successfully ??

I can't change the format of my input , and thus have to replace the commas in my DataFrame in order for my code to work , and I want python to do this without the need of doing it manually . Do you have any suggestions ?

could you include the stack trace
@USER I've added the report , not sure if this is the ` stack trace `
Yes that was the stack trace . So rows 119 and 120 exist but you are saying that iterating past 118 causes an error . Is this still a keyError or a different error ?
If you're not sure if the value exists , use ` x.get ( ' imdbRating ' , None )` rather than ` x [ ' imdbRating ']` , and then it'll have ` None ` if there's no value there . ( You can also replace ` None ` with ` np.nan ` if you prefer )
You want to avoid iterating , if possible , and instead find a function to apply , such as this : #CODE
It gives me another error , and I've includede in the update . I don't know what's wrong with that . A possible reason maybe the ` nan ` data ? but I've already drop dulicate and na
@USER : I like this solution , but wouldn't ` .map ` be better ? Or if you're going to use apply on a single series , you need to include an ` axis ` parameter .

I am trying to delete the first two rows from my dataframe ` df ` and am using the answer suggested in this post . However , I get the error ` AttributeError : Cannot access callable attribute ' ix ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method ` and don't know how to do this with the ` apply ` method . I've shown the relevant lines of code below : #CODE
If you just want to drop the rows in ` df ` , just use ` drop ` like this : #CODE

pandas dt accessor error , column made with to_datetime not datelike ?

I know that I can get the values using iloc but I don't care about the values , I just want it so that if we are not on row , col [ 5 , 5 ] then the calculation runs but if we hit row , col [ 5 , 5 ] in the dataframe then the calculation does not run but a different calculation is run only for row , col [ 5 , 5 ] . Does anyone know what function to use for this , I don't think I use loc ?

Then I have code to pull the required data and format it into a pivot table for graphing . Next , I make the figure : #CODE
I'm pretty sure the solution is to take the df.index and put it into a list , then cut out repeats and add it onto the y_range .
Aha ok , sometimes the resample thing can get tricky if your dataframe has data that doesn't aggregate well - you could try constructing a new dataframe using only your index from the original one - maybe something like ` q = pd.DataFrame ( pd.Series ([ n for n in range ( 0 , len ( yourframe.index ))] , index= yourframe.index )) .resample ( ' M ') .index ` which should create a DataFrame q , that you can then iterate through the strftime function ` [ dt.datetime.strftime ( n , ' %b-%Y ') for n in q ]` - is that any better ?

If that's too slow , you can also skip the type checking and just apply the string conversion to columns matching the numeric type .

Python pandas : can I speed up this apply statement ?
I do all of this with apply statements . They work , but seem slow to me : 7 seconds in total , whereas any SQL would take a fraction of a second , even without parallelisation , on the same machine . If this were a one-off I wouldn't invest time in speeding this up , but I must do it multiple times on multiple dataframes of similar size .
Thanks - much appreciated . The data comes from a Microsoft SQL Server . I have a feeling I should learn LINQ and how to apply it to SQL - it might make these things much faster
Your question 3 is easy if you again convert to periods , then adding 1 adds to of the same freq , month in this case . This is also vectorized . #CODE

Replace Nulls in DataFrame with Max in Row
Is there a way ( more efficient than using a for loop ) to replace all the nulls in a Pandas ' DataFrame with the max value in its respective row .
You can use ` apply ` , iterate over all rows and replace 0 by the maximal number of the row by using the ` replace ` function which gives you the expected output : #CODE

Dataframe replace could not replace the data
I want to replace the outlier data by min / max in the sample , but the dataframe.replace doesn't work . #CODE
so my objective is to replace the maxoutlier by the maxresult .
btw , ` replace ` is for replacing strings , not for what you are trying to do .
Thanks for your reply . With result.loc [ result.value > threshold , ' value ' ] = maxresult.value , the value of result is replace by NaN rather than the value I want .

Here is one workaround by defining customized ` apply / map ` function to unpack the list . #CODE

Maybe use ` transpose ` operator to do the trick ? #CODE

Select by column and drop column
It seems a very common activity to select a particular value from a column of a ` DataFrame ` which leaves that column redundant . I always then explicitly drop the column immediately after .
But it sort of feels like there should be a way of doing this that doesn't involve having to explicitly drop ` is_good ` . You can do : #CODE
Here is one way . The idea is to build a multi-level index and use ` .xs ` to select will automatically drop one level . #CODE

Programatically in Python I have 2 x 2D arrays with different lenghts and I want to resample the longest array to the same intervals as the shorter array , interpolating linearly when necessary to create the corresponding data points .
I can't solve your exact problem unless you include demonstration code . You can't apply my example to your problem ?

I think going with ` reset_index() ` is the way , but there is an option to drop the index , not push it back into the dataframe .

I guess diagnoses is a generator and since you drop something in line 2 of your code this changes the generator . I can't test anything right now , but let me know if it works when commenting line 2 of your code .
You can use str.split on the series and apply a function to the result : #CODE
Is apply as efficient as this " vectorized " function definition ? I received this suggestion to avoid mapping , supposedly doing loops of my lambda functions : #URL
` File " / home / seidav / anaconda / lib / python2.7 / site-packages / pandas / core / series.py " , line 2060 , in apply

Whereas ` df.fillna ( 0 )` fills all NA / NaN values with 0 , is there a function to replace all non -NA / NaN values with another value , such as 1 ?
boolean index like ` df [ len ( df ) 0 ] = 1 ` throws ` ValueError : cannot insert True , already exists `

@USER I get an AttributeError : AttributeError : ' DatetimeIndex ' object has no attribute ' apply '
OK the following should work , convert your datetimeindex to a series so you can call ` apply ` and use ` strftime ` to return an array of strings : #CODE

Hello Stack Overflow community ,
I am having an issue where Pandas is not understanding my merge conditions . It works with the other ' keys ' , but breaks as soon as I include the " Date " column as a key . The " Date " columns are string objects in both dataframes ( not timestamps ) .
There will always be many more duplicates of Make / Model Hour in df2 so I only want to left merge matches to df , no matter how many duplicate instances within df . I also do not wish to lose any data in df so any dates from df that is not found in df2 , should remain .
If the ' Date ' merge condition worked , this is the output I am trying to achieve : #CODE
Does anyone have an idea why this is happening ? I have tried even splicing the ' Date ' column into 3 columns ( ' Month ' , ' Day ' , ' Year ') and changing the dtype to int64 , bool , obj and no success there either . So I assume it has something to do with the format .
Thanks ahead of time Stack Overflow community !
Running the below code before the merge should put the dates into a common format so that the merge works properly . #CODE

But , again I want the output of KDE of original X not positions , so I can make first derivative on the output of KDE . That's why I did kernel ( X ) . I'd prefer if you could write an answer with your supported code to plot pdf output vs KDE of original data and save the output of KDE to apply first derivative on it

I have a pivot table that shows customer calls by month . It looks like this #CODE
I would like to reorder the pivot table so that it matches the date_list like so : #CODE
Suppose ` df ` is your pivot table . #CODE
Let's say your pivot table is named ` pt ` . #CODE
The line ` month = months [ date_list [ 0 ] [: 3 ]]` uses the dictionary to get the index value of the first month in your list of dates . If the list changes , your index value will change , and hence your table columns will shift .

I couldn't find an answer to this in the existing ` SettingWithCopy ` warning questions , because the common ` .loc ` solution doesn't seem to apply . I'm loading a table into pandas then trying to create some mask columns based on values in the other columns . For some reason , this returns a ` SettingWithCopy ` warning even when I'm wrapping the test in a ` pd.Series ` constructor .

Note : When I replace ` hist ` in my code below with ` describe ` , it perfectly gives me 100 describe series . Also , the type of the ` grouper.get_group ( days )` object is ` pandas.series ` .

How to drop rows which has elements equal to a specific value
I want to drop the rows which contains elements equal to minus one .
The following code could drop the rows if columns 1 equal to -1 , but each columns could have -1 . #CODE

Data type of pandas column changes to object when it's passed to a function via apply ?
I need to use the ` dtype ` of a pandas column in a function , but for some reason when I call the function using ` apply ` , the ` dtype ` is changed to ` object ` . Does anyone know what is happening here ? #CODE
It appears to be due to an optimization in ` DataFrame._apply_standard ` . The " fast path " in the code of that method creates an output Series whose dtype is the dtype of ` df.values ` , which in your case is ` object ` since the DataFrame is of mixed type . If you pass ` reduce=False ` to your ` apply ` call , the result is correct : #CODE

First , time column is wrong . By the comment you said that the csv file is exactly what you post . Which means the data you get are something like ` 00:22 .9 ` , but you think it's wrong . Since we don't have a corresponding data that could map to the actually time , so you might need to check that if the time ` 00:22 .9 ` is elapsed time or whatever ( ` 00:22 .9 ` might represent ` 07:22 .9 ` ? ) .

How to filter shift + / - 1 day in Pandas ?
If the dataset is larger , I would use the merge operation : #CODE

As you're trying to learn categories from one DataFrame to apply to a different DataFrame , using scikit-learn might provide a more elegant solution : #CODE

@USER it's position 191 in a string that is in your DataFrame . Perhaps you could put the whole stack trace , the DataFrame itself ?

I want to cut the hours / minutes on the following data to keep only the ' YYYY-MM-DD 00:00 : 00 ' .
Note that you can also call normalize on your index : ` index.normalize() `

Thanks for posting an answer to this question ! Code-only answers [ are discouraged ] ( #URL ) on Stack Overflow , because a code dump with no context doesn't explain how or why the solution will work , making it impossible for the original poster ( or any future readers ) to understand the logic behind it . Please , edit your question and include an explanation of your code so that others can benefit from your answer .

See #URL and map the proper column with such encoding function

Representing a SQL join with a merge in python ( pandas ) when one of the join conditions has an increment
I can easily do a merge ( join ) on one condition : #CODE
but I don't seem to find a way to merge / join on the second condition as one of the values has to be incremented by one i.e. alfa.value = beta.value + 1

Append two multiindexed pandas dataframes
Can you please help to append two multiindexed pandas dataframes ? Trying to append df_future to df_current . COMPANY and DATE are the indexes .
Use ` concat ` to concatenate the two DataFrames , and ` sortlevel ` to reorder the first index level : #CODE
Appending in pandas is called concat . And is done with the ` pd.concat ` function .
The ` concat ` function works no matter if you have multiindex or not #CODE

You have to use ` isnull ` for this : #CODE
Or use ` apply ` : #CODE

Try running ` python -m pdb / path / to / script.py ` . This will drop you in the pdb debugger . Press ` c ` to continue . This will break when an exception is reached . Now inspect the value of ` a ` by typing ` p a ` . This will give you a clue what values in the ` data ` dict are causing the problem . Press ` q ` to quit the debugger .

It seems like you want all of the columns in your data except the last two , so use ` df.iloc [: , : -2 ]` to select it . You then want to transpose this data so that the dates are now the row and the countries are the columns ( use ` .T `) . Finally , plot your data . #CODE

I wish I could replace the 2 by string.index ( " | ") , but how do I call the string ?

If you want to take a copy of a row then you can either use ` loc ` for label indexing or ` iloc ` for integer based indexing : #CODE
If you want to remove that row then you can use ` drop ` : #CODE
Similarly you can pass a list to ` drop ` : #CODE
If you wanted to drop a slice then you can slice your index and pass this to ` drop ` : #CODE

Given multiple entries that have the same ` p ` , ` g ` , ` a ` , and ` s ` , I need to drop all but the one with the highest ` v ` . The reason is that the original source of this data is a kind of event log , and each line corresponds to a " new total " . If it matters , the source data is ordered by time and includes a timestamp index , which I removed for brevity . The entry with the latest date would be the same as the entry with the highest ` v ` , as ` v ` only increases .
I need to drop the first two rows and keep the last one .

You can specify the label you want to remove with ` drop ` . In case of a multi-index , you can pass the label as a tuple .
By the way , you can apply your logic of ` df [ df [ ' col_1 '] ! = 754 ]` also on the index . This would give ` df [ df.index ! = 754 ]` , although this would not work with a multi-index
You can pass a tuple of your index labels to ` drop ` #CODE

OK if it's intra log difference then you can do this succinctly using ` diff ` : #CODE

pandas database merge on multiple columns not merging correctly
I am running into the following issue . I want to merge two dataframes on multiple columns ( 11 to be exact ) . Surprisingly the usual methods do not work . Example dataframes are as follows :
Why are you doing an outer join ? As far as I got you need an inner join .

I know that I can simply do one datetime minus the other to find the timedelta - but I don't know how to apply this to make a new column .

Basically , I need to resample the raw data ( every 24 seconds ) into 1 minute bins , and count how many ( valid ) hspeed values are within the bins . So far , the resampling works fine but not the ' avail ' calculation .

You can call ` map ` on ` df2 [ ' c4 ']` after setting the index on ` df2 [ ' c3 ']` , this will perform a lookup : #CODE
Is it actually possible to call " map " on more than one column ? I have updated my example .
To answer your question , no it won't work that way , for your updated example use merge

Note : The ` // ` operator is for floor division to truncate any remainder .

If I understood your problem , the " SQL " way to do it is to merge with another dataset containing the values you want , I don't know if it's faster something like this : ref= pd.Dataframe ( { ' id ' : [ x ] , ' cycle ' : [ y ] } ); pd.merge ( df , ref , on = [ ' id ' , ' cycle ']) . I don't know the general rule for pandas working , probably you are doing twice the work with two different queries

Pandas merge dataframes on same column
I am writing a scraper and I want loop through a list of links and merge all results as columns to a dataframe on same key ( like a left join ) .
I run this code in the Ipython Notebook , the resulting csv that comes from the dataframe does not make sense , however if after running the script I merge df and df2 on the mutual column " questions " , I get the join that I need , but in the script there's something wrong .
I also tried to save every review to .csv and then merge everything with Pandas , but I get a weird , rare undocumented error :

Here is one way using ` np.triu ` to mask out upper triangular matrix and reshaping correlation matrix by ` stack ` . #CODE

I then need to append this data back to the original hdf5 file ( with a column indicating whether the same Customer_ID showed up multiple times ) . The only problem is that there are duplicates . The only solution I have is to read the whole file into memory , drop the duplicates , and then write the file all over again ( replacing it completely ) . Is there a way to avoid appending duplicate data ? Like an ' insert and replace ' command that I can use in pandas ? #CODE
I need to see if the latest data ( from yesterday ) existed previously ( within a 30 day rolling window ) . Below you can see that I append the latest data to the original file , I then read that file into memory , drop duplicates , then write it again ( overwriting the existing file ) #CODE

Python pandas : replace values multiple columns matching multiple columns from another dataframe
I searched a lot for an answer , the closest question was Compare 2 columns of 2 different pandas dataframes , if the same insert 1 into the other in Python , but the answer to this person's particular problem was a simple merge , which doesn't answer the question in a general way .
I have used merge as a workaround : #CODE
If you merge chr , ochr and pos , ostop , then there is no need to update .. maybe you mean to update chr -> CHR and post -> STOP ?
this is how it is in your example , after the merge , you want to update df1.chr -> df2.CHR and df1.pos -> df2.STOP , maybe correct that if its a typo
This can be done in one line of an SQL Update join query . If your dfs derive from a database , consider the power of an SQL relational engine .
Start by renaiming the columns you want to merge in df2 #CODE
Now merge on these columns #CODE

There is an ` apply ` for that : #CODE

Choropleth map from Geopandas GeoDataFame
I'm trying to make a choropleth map from polygons in a Geopandas GeoDataFrame . I want to symbolize the polygons by quantiles of a value in one of the GeoDataFrame columns . I'm trying to figure out different options and see what best fits my needs . Any advice on this would be greatly appreciated .

I have a student table that contains ` student_id ` , ` course_id ` , and ` exam_time ` ( 10k rows ) . I pivot on ` student_id ` and ` exam_time ` to get the number of exams in a session or in a day . I am building a timetabling heuristic that changes the times of the examinations one at a time so I need to update this pivot table a lot of times . A change in one course's examination time affects an average of 50 rows in the original dataframe . Is there a way to update the resulting pivot table without recalculating the whole thing in pandas or should I keep track of the changes on the pivot table myself ( i.e. by adding and subtracting 1 to the changed slots ) ?
Edit : Here's how I construct the pivot table . I added a column of ones to count the numbers by np.sum . I couldn't find another function that works faster . #CODE
Agreee with @USER , I would go simple at first and see if that is OK . 10k rows is really not that big . That said , I'd expect the most efficient way to do this would be to store a pivot table for each exam ( then add them all together ) . Then if you change one exam , you just subtract the old one and add the new one to the combined pivot table .
Thanks for the suggestions . @USER I added the code for the pivot table . Was that what you were asking ?
Here is sample code , the idea is update the total pivot table by subtract the pivot table of old rows and add the pivot table of new rows .

I have the following ` Series ` which is the result of using ` Stack ` on a ` DataFrame ` to result in the desired output : #CODE

Join will return a copy instead of affecting the existing dataframe ( joins the two dataframes on the matching indexes ): #CODE
And , of course , this column actually comes from another dataframe and the label isn't changing , so I can just join it directly . Probably could have done that with ` concat ` too . We should * totally * have a badge for x / y problem .

I've been trying to delete Unnamed : 0 with no success . If I drop it #CODE
Assuming you are talking about the index column of the data frame , you can use the ` index ` argument for ` to_csv() ` method , and send in ` False ` value so that its not written to csv . ( You do not need to drop the index column beforehand )

The problem is with indexing depending on the specific case whether the user requests the data by the reference date or the publish date . I then need to pivot the data and show each item as a column with a multi-level index for reference / publish date .. there can be many duplicate reference dates for a single publish date .
Based on the revised post , I believe a pivot table would do the trick . I also swap the column levels to get your desired format .

Assuming you are using Pandas / NumPy , the standard way to replace an ` if-then ` construction such as the one you are using is to use ` np.where ( mask , A , B )` . The ` mask ` is an array of boolean values . When True , the corresponding value from ` A ` is returned . When False , the corresponding value from ` B ` is returned . The result is an array of the same shape as ` mask ` with values from ` A ` and / or ` B ` . #CODE

The code is not that efficient , Is there a more efficient way to do this ? I would assume there must be some special function ( such as apply , roll_apply ) to iterate through these values , but I couldn't figure that out . Any help would be appreciated .

My goal is to merge two DataFrames by their common column ( gene names ) so I can take a product of each gene score across each gene row . I'd then perform a ` groupby ` on patients and cells and sum all scores from each . The ultimate data frame should look like this : #CODE
That last part should work fine , but I have not been able to perform the first merge on gene names due to a ` MemoryError ` . Below are snippets of each DataFrame .
As a database guy , I always recommend handling large data loads and merging / joining with a SQL relational engine that scales well for such processes . I have written many a comment on dataframe merge Q / As to this effect -even in R . You can use any SQL database including file server dbs ( Access , SQLite ) or client server dbs ( MySQL , MSSQL , or other ) , even where your dfs derive . Python maintains a built-in library for SQLite ( otherwise you use ODBC ); and dataframes can be pushed into databases as tables using pandas to_sql : #CODE
EDIT : I eliminated the ` c = conn.cursor() ` following an example from [ Sebastian Raschka ] ( #URL ) . Seems to work for my merge , but I [ wouldn't be able to query ] ( #URL ) my new database , correct ?

I tried a few alternatives such as only building a map if the names in the first column in sequential rows are different , but this just adds a statement into it and makes the code much slower .

how do I apply normalize function to pandas string series ?
I would like to apply the following function to a dataframe series :
How do I apply the ` normalize ` function to all members of the series ?
If ` c ` is your string column . ` map ` is used to apply a function elementwise ( and of course you wouldn't have to chain it all together like this ) #CODE

However , under the hood , the function is resampling my timeseries with an interval of 1 ms ( which is the ' freq ' that I supplied ) . This causes thousands of additional datapoints to be included in the output . #CODE
You can use ` reindex ` to align the ` ewma ` result with your original series . #CODE

This was written in Python 2 , but the basic idea should work for you . It uses the apply function : #CODE
Note that if you loaded the other keys into the dict , you could do the apply without the swapper function : #CODE

I don't really see anything wrong with your merge approach - I guess it could be a little shorter as ` df.merge ( keys , left_index=True , right_on =[ ' A ' , ' B ' , ' C '] , how= ' right ') .R ` ?

Could you post an array form of the data so I can play with it a bit ? Also , consider creating a new function to use in the apply for now until you can work the problem out .
Simply add axis = 1 to your apply function and it will work : #CODE
` in_group [ " t_wins "] = in_group.apply ( lambda x : len ( temp [ temp [ ' t_date '] < x [ ' date ']]) , axis=1 )`
You add the column to the subgroup inside the apply function , and then when you return that subgroup it replaces the existing subgroup .

The way that I have found to do is by making a copy of the column and operating on it ( I have to do this because a ` DateTimeIndex ` has no ` apply ` method ) . I am sure there must be a way to operate on the index directly though but I could not find it : #CODE

I currently run a loop that for each cid ( I slice the cid rows out of the master df ) , in the loop determine the relevant date range ( min stdt and max enddt for each cid frame , then for each of those newdates ( range mindate-maxdate ) it counts the number of jid where the newdate is between the stdt and enddt of each jid . Then I append each resulting dataset into a new dataframe which looks like the above .
My usual approach for these problems is to pivot and think in terms of events changing an accumulator . Every new " stdt " we see adds +1 to the count ; every " enddt " we see adds -1 . ( Adds -1 the next day , at least if I'm interpreting " between " the way you are . Some days I think we should ban the use of the word as too ambiguous .. )
You could then drop the zeros if you don't want them . I don't think this will be much better than your original solution , however .

The issue is that ` x = pd.np.array ( map ( float , str ( x ) .split ( ' , ')))` in ` split_stat() ` seems to be creating a map object , not a numpy array .
@USER : In Python3 , ` map ` returns a map object , not a list . So use ` x = pd.np.array ( list ( map ( float , str ( x ) .split ( ' , '))))` .

Building a conditional time series pivot table in Pandas
Replace the NaNs with zeros , and then take the cumulative sum down the rows : #CODE
To also strip out the time part of the datetimes , you can use : #CODE
Two problems when I try to do this : ( 1 ) My source data frame contains datetime64s and I'm struggling to strip the time information from them ; and ( 2 ) when I run it the series name is lost so the columns in the final data frame has names 0 , 1 , 2 , ...
Regarding ( 1 ): Do you want to strip off the times * before * the ` value_counts ` is called , or merely off of ` result.index ` ? Regarding ( 2 ): I'm not sure why this is happening to you ; it doesn't happen in the example I posted .
( 1 ) ` df = pd.DataFrame ( df.values.astype ( ' < M 8[ D ]') , columns =d f.columns )` can be used to strip the times off of the datetime64s . ( 2 ) If the list comprehension is changed to a dict comprehension : ` { #URL ( ) for col in df} ` , then ` pd.concat ` will definitely return a DataFrame with the keys as column names . Perhaps a difference in Pandas version explains the difference in behavior we are seeing . I'm using 0.16.2 . I've edited the post above to include these changes .
Okay , I was able to reproduce the problem in my toy example by using ` df.iloc [: : 2 , 0 ] = np.nan ` to add some NaT values to one of the columns . This can be fixed by using ` df.values.astype ( ' < M 8[ D ]') .astype ( ' < M 8[ ns ]')` to strip the times from the dates * and * convert the data back to ` datetime64 [ ns ]` ( instead of ` datetime64 [ D ]`) dtype . I've edited the post to include this change .
Given ` datetime.datetime ` values in the columns , you could strip off the times using

stack trace on small python plotting program snippet
This piece of code works on ubuntu15.04 desktop but on my 14.04 laptop I get the following stack trace which I am having trouble interpreting .
Edit : note - I have tried running the code with both ` python ` and ` python3 ` and the same stack trace occurs . #CODE

Also , ` df.plot ( x= ' age ' , y= ' count ' , kind= ' hist ')` shows
See [ this ] ( #URL ) . ``` dataframe.plot ( x= ' age ' , y= ' count ' , kind= ' hist ')``` ( if it works , add a vote to my answer in that link :)
Try setting the index to the age and then calling the ` hist ` method on the ` count ` column . Something like #CODE

It could be done by calling ` apply ` on the df like so : #CODE
This uses double subscripts ` [[ ]]` to call ` apply ` on a df with a single column , this allows you to pass param ` axis=1 ` so you can call you function row-wise , you then have access to the index attribute , which is ` name ` and the column name attribute , which is ` index ` , this allows you to slice your df to calculate a rolling ` std ` .
It works . what's the difference between ` ix ` and ` iloc `
[ ix ] ( #URL ) is just a way for indexing using label values , ` iloc ` uses integer based indexing , as your index is str based I can't use ` iloc ` to slice this way , it would also work if you use ` loc `

I traced this error to a bug in pandas . I've fixed the error in #URL and opened a pull request to merge in the fix , but feel free to checkout my fork / branch .

If you want to consolidate all the DataFrames should create list containing each of the individual DataFrames and concat them . #CODE
Then you can use ` os.path.join() ` to join it with the other directory and get the resultant path . Example - #CODE
os.path.basename will give you the file name just join it to the new path and save it however you want : #CODE

However , when I apply this method to whole length of data , I couldn't wait until the operation was done .
You should simply drop a level from your nested dict to make life easier . The code below drops the unnecessary part of your dicts and concatenates the dataframes from each of the dicts together . #CODE
Not sure what you mean by your comment @USER . I did use pd.DataFrame() on each of your dict's , however , what I did was apply a dict comprehension using each of them to strip out the nested zeros . pd.concat() just took all three of the DataFrame's from dict's and concatenates them all together .

Now you can map the column names to whatever #CODE

Python pandas : banal apply statements incredibly slow

apply sum calculated using pandas group by to all elements of group
Please suggest how can I apply sum on all rows of one account .

You can use ` itertools.product ` to generate all combinations , then construct a df from this , ` merge ` it with your original df along with ` fillna ` to fill missing count values with ` 0 ` : #CODE

Is there an easy way to basically join them ? I saw the example for the where function but it didn't seem easy to use with a large number of values to look up .
are both datasets the same kind ( columns , datatypes etc . ) ? If yes , do you want to append that table to the one saved as hdf file ? If not , do you just want to save it as hdf5 file ? Please show what you already have .

looks like you want a special case of a pandas pivot table . try that

The above looks weird , but that is exactly how it looks in the output . Note that , contrary to the example presented at the start of this post , there are assumed to be more than 2 ` Values ` for ` A ` ( so that I can illustrate this point ) . When I do this with the actual data , the ` Value ` lists get cut off after the first 4 elements .

How to map a function in pandas which compares each record in a column to previous and next records

You can use the ` map ` method on your json-text column to apply a ` lambda ` function which will parse the json using ` json.loads ` then return the field that you want . #CODE

Drop columns with low standard deviation in Pandas Dataframe
I know I can get the std values by ` std_vals = df.std() ` , which gives the following result , and use these values to drop the columns one by one . #CODE
However , I don't know how to use the Pandas indexing to drop the columns with low std values directly .
To drop columns , You need those column names . #CODE
You can use the ` loc ` method of a dataframe to select certain columns based on a Boolean indexer . Create the indexer like this ( uses Numpy Array broadcasting to apply the condition to each column ): #CODE
Then call ` loc ` with ` : ` in the first position to indicate that you want to return all rows : #CODE

` ids_with_latlong [ " loc "] .str .extract ( " [ -+ ] ? ([ 1-8 ] ? \d ( \ . \d+ ) ? |90 ( \ .0 + ) ? ) , \s* [ -+ ] ? ( 180 ( \ .0 + ) ? | (( 1 [ 0-7 ] \d ) | ([ 1-9 ] ? \d )) ( \ . \d+ ) ? ) $ ")`
1 ) remove extraneous characters with ` replace ` ( or maybe this is where the regex is best )
Being new to Pandas , I'm not sure what to do with that , even though the documentation seems clear , I can't figure out how to use ` loc ` here .
Gosh , I'm not sure why you got warning there . I didn't get it and can't see that it would be a problem here ( and the message doesn't necessarily imply the command didn't work , you just have to double check that it did ) . As a side note , you might want to change the name of the column , since it potentially conflicts with the ` loc ` method . I.e. you'll be OK with ` df [ ' loc ']` but not ` df.loc ` for referring to the ' loc ' column .

This can be done by first construct multi-level index on column names and then reshape the dataframe by ` stack ` . #CODE
I can see from your output that this is exactly what I want . However , I cant recreate it because our initial dataframes are created in different ways . I'm using a groupby to set the initial split and sex index . Then , if I use your code : level_group = np.where ( df.columns.str.contains ( ' 0 ') , 0 , 1 ) on my groupby ( replacing the df ) , I get an error : Cannot access attribute ' columns ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method
The first trick is to gather the columns into a single column using ` stack ` : #CODE
and finally what if I wanted to completely unstack everything so that split and sex become columns as well ?

When we read data from data_frame , we can firstly trunctate it by desired qty and then we need to remove previous indexing and then we can concat that data : #CODE

It makes thing that Counter is not a dictionary as it looks like . Any idea on how to append it to a df ?

I know this is a an assigning to a slice . In pandas 10 this worked , as documented in various pandas upgrades it does not work . without having to refactor the code totally is the a way of changing line 2 so that it can still use the loc method while still using the condition as defined on line 1 .

If I want to add a column ` wgt ` to ` df ` I need to merge this result back to ` df ` on ` name ` and ` index ` . This seems rather clunky .

What I did is to apply this function to get the count of repeating elements in B #CODE
Check this out , first you can make that ` repeat ` column using ` np.where ` , which is not concise . 2nd , to count the number of repeats of particular AB combination , we may want to use ` gourpby ` , and merge the resultant with the original ` DataFrame ` : #CODE

You can pass the ` dtype ` as a param this will map the column to the passed dtype : #CODE

You can also be more explicit by using the ` ix ` indexer : #CODE

Split and Pivot a Data Frame
You can use a simple merge to accomplish this #CODE

How do I replace the content of an entire Pandas ' cell with a string ?
Edit : ` str.replace ` is not what I'm looking for here . I just want to replace the content of that cell ( whatever it is , rather than a regex ) , with some other string ( literal )
Your example uses "` iloc `" rather than "` loc `" , which means you need to access both columns and index by index , not name . Something like ` df.iloc [ 0 , 0 ] = ' Lion '` might work in your example , assuming that you're showing a complete row without the index .

not related to the question : replace your last loop with ` for job in jobs : job.get() `
Apply async usage :

One way would be to introduce a column ranking the subgroups and then pivot .
Then pivot : #CODE

Here is another option that is slightly more elegant . Replace the loop with : #CODE
` stack ` just converts it to a series so you can use ` str ` and then ` unstack ` converts it back to a dataframe .
@USER I'm not sure in this case , you could use ` %timeit ` in ipython to compare . Generally speaking , the advantage of pandas is crunching numbers not strings , so it might not matter too much . But generally speaking , ` apply / lambda ` will be slower than using built in pandas methods .
@USER OK I quickly tested your 2 ways and my 2 ways on 1000 rows of data . From fastest to slowest : split ( 3.28ms ) , applymap ( 3.91ms ) , stack ( 4.25ms ) , extract ( 9.71ms ) .

I'm new to python , and trying to learn how to data analysis with it . I have a data frame in python ( called " data ") . I am looking to recode a variable , GEND , which has three values ( 1 , 2 , 3 ) . Using pandas , I read in a csv file using pd.read_csv() . I am trying to replace all instances of " 3 " in the variable GEND to missing ( NaN ) . However , I can't seem to find out how to do it . So far I've tried a for loop , which doesn't show an error , but doesn't change the variable information : #CODE
You can use loc to replace the 3's : #CODE
That replaces the 3rd ` loc ` with ` NaN ` , print out what ` df.GEND.loc [ 3 ]` is and you should see what it is doing .
@USER loc performs label indexing , so it returns just the row where the index is ` 3 `

Resample so that you have regular intervals , and fill nulls
Left merge .

I don't believe you can ignore them as they are being read , but once they have been read you can easily drop them using ` drop_duplicates ` . #CODE
Depending on the cleanliness of your underlying data , you may first need to pre-process to strip spaces , etc .

ok , that works , Thanks . I need to strip out the date , which I'm doing with :
It would be interested to see if the approach of using ` map ` with ` datetools.parse ` is as scalable as the standard approach given here , here and here .

Pandas : Apply function via " Column A " , simultaneously reading " Column B "
In the ` csv ` , there is also a `" Column B "` that contains values that I want to read into a variable ` x ` within the function . It should not ` apply ` from `" Column B "` this should still be done from `" Column A "` . Is this possible ?
Post-edit : This question has been identified as a possible duplicate of another question . Although the answer may be the same , the question is not the same . For future readers it is probably not apparent that ` apply ` on two columns is interchangeable with ` apply ` on one column and " reading " another column at the same time . The question should therefore remain open .
possible duplicate of [ How to apply a function to two columns of Pandas dataframe ] ( #URL )
The ` axis=1 ` argument passed to the apply method puts the whole row into the apply method as a single tuple argument .

The reason is that ` str.replace ` will be able to handle NaNs , while the custom replace with lambda will error on that : #CODE

This is a really messy way to do this , first use ` first_valid_index ` to get the valid columns , convert the returned series to a dataframe so we can call ` apply ` row-wise and use this to index back to original df : #CODE
The idea here is to use ` stack ` to move the columns into a row index level : #CODE
I'm going to weigh in here as I think this is a good deal faster than any of the proposed methods . ` argmin ` gives the index of the first ` False ` value in each row of the result of ` np.isnan ` in a vectorized way , which is the hard part . It still relies on a Python loop to extract the values but the look up is very quick : #CODE

The test inputs you show , converted to dataframes , have no problem with concat . Have you confirmed that your inputs are actually producing exactly the dataframes you expect ?
I understand that you want to combine the inputs . I'm asking what your inputs actually look like . You are producing dataframes from dbtest2.xlsx and / or dbtest1.xlsx . When you check those dataframes , do they look identical to the " test inputs " you show in your question ? Because the script you show will properly join those " test inputs " , meaning the simplest explanation is that your actual inputs are not the same as those ones .

pandas DataFrame update / combine when indices don't align
Now I would like to merge or combine these two data frames such that the values are collected in a single column , with the ` NaN ` in ` df1 ` replaced by the corresponding observation-year values from ` df2 ` .
i.e. essentially doing a regular merge and then replacing the ` NaN ` s in one column with the values for the other column by hand .
Is there a better / more succinct way of doing this ? I feel like some sort of ` df.combine ` , ` df.combine_first ` , ` df.update ` would do what I'm after , but they all seem to align on indices .
Yes that's correct , I think your best bet is to merge like you have done already , to some extent as you are interested in just the values the index of the other df could be overwritten or reset

I can't take the cross section ( ` xs `) and then assign to a new column , because the cross section will give me a copy . And I can't take a cross section for just the cells I want to set to 99 , because I need to slice by index AND by row , so I need a blend of iloc and loc . One possibility would be to reset the index to drop the Name level and then put it back again afterward , but that seems yucky .

This gives you a dataframe with ` len ( listOfAttributeColumnNames )` ( apparently 6 ) columns . This is not a proper input for the vectorizer as I mentioned . #CODE
Thanks , I missed the join of the columns after refactoring my code .

Context why I'm doing this : I've set up an ORM layer on top of my database and am using it to query data into a ` Pandas DataFrame ` . It works , but it's frequently maxing out my memory . I want to cut my in-memory overhead with some string folding ( pass 3 outlined here : #URL ) . That requires ( and correct me if I'm wrong here ) not using the read_sql string and instead processing the query's return as raw tuples .

No I don't think so , for instance ` df [ ' A '] == float ( ' nan ')` still won't work , bottom line is you have to use ` isnull ` or ` notnull ` to test for ` NaN ` correctly
You could replace ` nan ` with a unique non-NaN value that will not occur in your list , say `' NA '` or `''` . For example : #CODE

You can do the following , this tests each row for membership of ` 1 , 2 ` using ` isin ` and if so this generates a boolean series , you can use this to index into the columns by calling ` apply ` again , we convert this to a list because the dimensions won't align if you don't do this : #CODE
output from inner ` apply ` : #CODE

I have also tried using ` sep=r ' \s* '` , but seeing as how the ` * `' s move around in the file , it makes it to where some rows are shifted , and therefore the columns do not align .
Those asterisks everywhere are really problematic because , as you mention , sometimes they replace the ` | ` delimiter and other times they're in the middle of a field seemingly at random .

To obtain the timedelta in seconds , you can use the ` total_seconds ` method . If I define the negative difference to ` diff = then - now ` : #CODE
Thank you very much . Although it is clear from your answer , I'll leave for clarity , to find the absolute value of a subtraction of timestamp : abs (( ts1-ts2 ) .total_seconds )

I have two dataframes , I wanted to merge them into one single dataframe based on the matching row . My dataframe looks like this
This is what I tried with merge , #CODE
You can also use ` join ` which is basically the same as ` merge ` but automatically uses the index .
Also , your example DataFrames don't have ` Set_1 ` as the index , and if you run your example ` merge ` statement , you raise an exception , so something's odd ....
The pandas merge function takes keyword arguments : ` left_index= ` and ` right_index= ` . When set to ` True ` , the merge function will use the index / indicies of the dataframe ( s ) for merging .

it would help me produce output that was a lot neater and a little more ' human-like ' if I could use pandas and xlsxwriter in a way that would stack two dataframes , one on top of the other , on the same sheet of the Excel spreadsheet I am outputting .
Is there a neat way I can just take my dataframe and my summary dataframe and stack them on the same sheet ?

Is there any ways to achieve this ? I am trying with Pivot tables , but not able to get the desired result .

This is over 100x faster for the example above ( where ` len ( drg )` is close to 4000 ): #CODE

Python merge excel documents with dynamic columns
I have several thousand excel documents . All of these documents are 95% the same in terms of column headings . However , since they are not 100% identical , I cannot simply merge them together and upload it into a database without messing up the data .
If a large proportion of them are similar , and this is a one-off operation it may be worth your while coding the solution for the majority and handling the other documents ( or groups of them if they are similar ) separately . If using Python to do this you could simply build a dynamic query where the columns that are present in a given excel sheet are built into the INSERT statements . Of course , this assumes that your database table allows for NULLs or that a default value is present on the columns that aren't in a given document .
Any recommendations to a db that would allow me to dump a few thousand excel documents and then create join queries to the VIN column ?

I am trying to figure out how to iterate through each value in this pandas DataFrame to see if it's absolute value is higher than some defined threshold ex : .01 > abs ( value ) . #CODE

A quick note on the one liner relative to this longer solution . The ` values ` in the one liner converts from a dataframe to an array , which has the convenient side effect of erasing the row and column indices . Alternatively in this longer solution I explicitly conform the indices . Depending on your situation , either approach could be a better way to go .

That won't work properly , since the ` dict ` constructor will replace the value of each ` state ` as it goes , rather than summing . It's not a one-liner , but : #CODE

How to replace string values in pandas dataframe to integers ?
I want to replace them with integer values in order to calculate similarities .
and I want to replace region == ' Geo RS / SC ' ==> 1 , region == ' Geo PR / SPI ' ==> 2 etc .
You can use the ` .apply() ` function and a dictionary to map all known string values to their corresponding integer values : #CODE

performance degradation when switching from pandas column concatenation to using apply on dataframe
Do you want to keep the ` NaN ` or replace them with blank string values ? personally I'd get the dtype to what you want and then use simple arithmetic operators like my previous comment
Generally using ` apply ` should be avoided it's essentially a for loop and not vectorised , it would be better to write a func that performed vectorised operations rather than using ` apply ` at all

Create pandas dataFrame from two other using append
I still struggle to see the desired result - could you show with a small dataset how you want to combine the two initial dataframes ? Just append them or do some more sophisticated combination ? Your function seems to take some ` indices ` and combines testset with trainset data ?
In other words , it does not change the object at hand . You are calling ` append ` and dropping the result on the bit floor . If you want to append to the frame in the sense of list.append , you need to use something like #CODE

Right now I have the data input , and I have more or less written the function I would like to use to analyze each column separately . However , I can't quite understand how to use a forloop or use the apply function through all of the columns in the dataset . I would prefer not to hardcode the columns because I will have 40,000 ~ 50,000 columns to analyze .
I'm attempting to use the apply function : #CODE
` axis=2 ` is not a valid param for ` apply `
What I gather from the comments sections and your original post , you want to replace each character in each column according to it's frequency of occurrence . This is one approach :
Replace letters in dataframe according to sort result

Each user can go to multiple destinations that are located in different countries . I need to find out the number of unique destinations users go to , median and mean of unique destinations . Same for countries . I don't know how to use groupby to achieve that . I managed to get the stats by placing everything in nested dictionary , but I feel that there may be a much easier way to the approach by using pandas dataframes and groubpy .

My ultimate goal is to delete the time from this column and join it back to the spreadsheet . #CODE

Pandas append perfomance concat / append using " larger " DataFrames
The problem : I have data stored in csv file with the following columns data / id / value . I have 15 files each containing around 10-20mio rows . Each csv file covers a distinct period so the time indexes are non overlapping , but the columns are ( new ids enter from time to time , old ones disappear ) . What I originally did was running the script without the pivot call , but then I run into memory issues on my local machine ( only 8GB ) . Since there is lots of redundancy in each file , pivot seemd at first a nice way out ( roughly 2 / 3 less data ) but now perfomance kicks in . If I run the following script the concat function will run " forever " ( I always interrupted manually so far after some time ( 2h > )) . Concat / append seem to have limitations in terms of size ( I have roughly 10000-20000 columns ) , or do I miss something here ? Any suggestions ? #CODE
EDIT I : To clarify , each csv file has about 10-20mio rows and three columns , after pivot is applied this reduces to about 2000 rows but leads to 10000 columns .
I can solve the memory issue by simply splitting the full-set of ids into subsets and run the needed calculations based on each subset as they are independent for each id . I know it makes me reload the same files n-times , where n is the number of subsets used , but this is still reasonable fast . I still wonder why append is not performing .
If you are appending or concating in a loop ( and the loop is inevitable ) , you should at least append the ` data_tmp ` to a list , and then after the loop concat them all in one time ` pd.concat ( list_of_all_tmp )`
To make it clear , the original csv file has about 10mio rows , and three columns . Then Pivot reduces the redudency by storing date / id information uniquely in the row and column index . However , it ends up with about 10000-20000 columns .
The problem arises actually already when I append only two bigger pivoted tables . If I collect this two dataframes in a list ( as suggested ) , concat them and then only pivot it is rather fast ( cannot do this for the full list of files , due to memory constraints ) , but if I first pivot each table and then append / concat it runs undefinedly .
Can you provide a reproducible example ? ( some random numbers that mimics the dataframe ) Because if I try to concat two dataframes of 2000 rows x 10000 columns , that runs in less than a second .
I tried to simulate the data , have a look please , it should take some to concat this pivoted data-frames , ( it highly depends on how many IDs are choosen to be in each file )
Maybe it makes a difference if I would assign integer column names and map them latter back , I would suspect that string comparison is slower than an int based index
As @USER mentioned , you should append all of the pivot tables to a list and then concatenate them all in one go . Here is a proposed modification to your code : #CODE
Using your reproducible example code , I can indeed confirm that the ` concat ` of only two dataframes takes a very long time . However , if you first align them ( make the column names equal ) , then concatting is very fast : #CODE
Thanks . I figured two things : First , when you replace the id_list with an integer index ( simply add ` id_list = range ( 0 , num_ids )` instead of the simulated ids it has a massiv perfomance impact . Second , I realized I also had some errors codes in the raw data , so the dtypes was of type object , cleaning the data and using only floats increased perfomance also by a larger factor . The aligning as you suggest also increases perfomance but only when columns or of type string not as much anymore when you use integer columns . Maybe someone varifies this findings and it might be worth improving in pandas
3 ) When using string based columns , make sure to use the align method before concat is called as suggested by joris

How do I merge DataFrames using a function ?
I am trying to merge DataFrames of stock prices with varying start dates . My question is , how do I merge the DataFrame with my placeholder DataFrame ? #CODE
You can first create a list of dataframes and then concat them all at once . #CODE
Thank you , this worked perfectly ! One quick question , it worked great in ipython , but not as a script I ran from the command line . It gave me an error saying unstack is not an attribute ... any idea why ?
@USER That error looks quite strange , because the function returns a ` pd.Series ` object and ` unstack ` is a native pandas method , not attribute .

Append to a DataFrame in Pandas as new column
I have two DataFrames with the same indexing and want to append the second to the first . Lets say I have : #CODE
But I want it to append a new column where the indexes match : #CODE
Use ` concat ` and pass param ` axis=1 ` to concatenate the list of dfs column-wise : #CODE
You can also use ` join ` but you have to rename the column first : #CODE
Or ` merge ` specifying that you wish to match on indices : #CODE
In general , the ` concat ` approach is better . If you have different indexes , you can choose to do an ` inner join ` or ` outer join ` . #CODE

I know I can make a partial function with " or " and my vector and apply it to the df , but this is probably unidiomatic and needlessly time-consuming . What is the pandas way ?

where we added the ` strip ` method . This way your column titles do not have white spaces .

I am performing some data analysis with pandas in an ipython notebook and my end goal is to visualize some pivot table data in the notebook .
The pivot table functionality I am implementing is not basic pivot operation but with some advanced drop down selection features , and I have implemented these from this link : #URL
Now , I am looking to cut the step of saving json to local and then reading this json into an HTML file . So I want to display the pandas dataframe data in the JavaScript app . I looked into the documentation for MetricsGraphics.js , but looks like this requires D3 and I am not using D3 in my JavaScript code .

My thought was to merge the dataframe back onto itself on ` user ` , and generate counts based on unique pairs of ` fruit_x ` and ` fruit_y ` . #CODE
Unfortunately the merge yields two types of unwanted results . Instances where a ` fruit ` has been merged back onto itself are easy to fix . #CODE

Am I using the drop function wrong , or do I need to keep track of the index of the row ?

You can first use ` .tolist ` to convert column to a list and then use ` .join ` method to concat all separate words together . #CODE
You could use ` str.cat ` to join the strings in a column with a separator of your choice : #CODE

You can first stack the DataFrame to create a series and then index slice it as required and take the min . For example : #CODE
Here's another method which avoids stacking and is a lot faster on DataFrames of the size you're actually working with ( as a one-off ; slicing the stacked ` DataFrame ` is a lot faster once it's stacked so if you're doing many of these operations you should stack and convert the index ) .
A hacky way , but should be fast , is to concat the shifted DataFrames : #CODE

Apply curve_fit on dataframe columns
I have a ` pandas.DataFrame ` with with multiple columns and I would like to apply a ` curve_fit ` function to each of them . I would like the output to be a dataframe with the optimal values fitting the data in the columns ( for now , I am not interested in their covariance ) .
I can apply the function and get an array in return : #CODE
Can this be done with something similar to ` apply ` ?
I think the issue is that the apply of your fitting function returns an array of dim 3x3 ( the 3 fitparameters as returned by conner ) . But expected is something in the shape of 20x3 as your df .

I have a a file of data and want to select a specific State . From there I need to return this in a list , but there will be years that correspond to the date with missing data , so I need to replace the missing data .
The second issue is in line 62 . Here , you try to append ` np.nan ` to your " list " countsList - but countsList is not a list anymore , but a DataFrame !

I'm sorry , this is not the ` abs ` , you can only + or - 360 degree , ` 50 ` and ` -50 ` is qutie different .

Well all you're doing is creating a df for each column which seems unncessary , semantically they should perform the same , ` apply ` on a df calls the function on each column in turn ( as ` axis=0 ` is the default param value )

Use ` loc ` , place the boolean condition within the square brackets ` [ ]` and the column of interest after the comma so you are not performing chained indexing : #CODE

Time is in seconds . To match right row with certain accuracy , we can have variable diff = 0.3s . #CODE
I updated question . To match right row with certain accuracy , we can have variable diff equal to 0.3s . Yes , in second.csv are only three float point numbers .

Or use the ` dt ` attribute to access the year : #CODE

When I run this code in ` iPython ` , it works perfectly . But when I run the same script from my terminal , I get an error message : ` AttributeError : ' NoneType ' object has no attribute ' unstack '` #CODE

Can probably be improved using ` map ` or ` apply ` . #CODE

So to append two new columns to your ` DataFrame ` you can do : #CODE

You can either concat them to a single dataframe or just find the minimum in each one then the minimum of the minimums .

I've been trying to figure this out for awhile now and haven't been able to apply any of the solutions I've found online for splitting columns in pandas yet . I have to apply the column split to 90+ consistently formatted columns . I feel like the solution should be trivial , I'm just too new to programming and python to figure it out !

I'm trying to then add the lat / long points to a leaflet.js map using d3 using Mike Bostock's tutorial - #URL
I can see how once I can access fields of data I might be able to lever d3 to create a map and a separate bar chart .
I then used this block #URL to get a working map .

Maybe a ` groupby ` in conjunction with ` apply ` ? I'm not familiar with ` apply ` yet .
you can use apply on the groups , which allows you to transform a group . This means that the function inside returns something for each set of entries that has the same ID .
group apply

@USER You can also define your own apply function to achieve the goal by using the ` cumsum ` trick . See the edited part .

Next , I want to separate out group_level and stack those 2 new factors : #CODE
No you just need to do ` stacked.group_level.str.extract ` , that is , call it on the relevant column . It's a series method as you say . I used my own series to show some more examples which is why i didn't put in ` stacked.group_level ` . I removed the pre-edit answer as the strip method wasn't good at all but then I thought of using replace which is in the second approach above . If you do go that way , don't use strip , use replace .
The extract method will create a dataframe with as many columns as groups specified in the pattern you pass , in this case two . Groups are delimited by brackets in the pattern . I've edited the question to show hoe to apply it in your case .

Basically , I'm reading from json log files into a pandas dataframe , but the append function is what is causing the issue . It creates two different objects in memory , causing huge memory usage . In addition , it seems the .to_pickle method of pandas is also a huge memory hog , because the biggest spike in memory is when writing to the pickle . Is there an easy way to reduce memory ? The commented code does the job but takes 100-1000x longer . I'm currently at 45% memory usage at max during the .to_pickle part , 30% during the reading of the logs . But the more logs there are , the higher that number goes . Thanks for your help ,
Searching " inplace panda append " , I found this : #URL
If you need to build a ` DataFrame ` up from pieces , it is generally much more efficient to construct a list of the component frames and combine them all in one step using ` concat ` . See the first approach below . #CODE

Passing a dataframe as an an argument in apply with pandas
Ok , the problem here is the combination of ` func ` and ` apply ` . The ` apply ` method of a dataframe applies the given function to each COLUMN in the data frame and returns the result . So the function you pass to ` apply ` should expect a pandas Series or an array as input , not a dataframe . It should give either a series / array or single value as output .
will apply the ` sum ` function to each column and give a series containing the
Secondly , the ` args ` parameter in ` apply ` is only used when the function you are passing takes additional arguments ( besides the series , which should be the first argument ) . For example , you might have a function that sums an array and then divides by some number ( again a silly example ): #CODE
The you might want to apply this to each column of a dataframe with divisor = 2 .

@USER Sorry that I misunderstood your question . For your case , just apply the same ` groupby.agg ( sum )` logic to hourly price dataframe , and then calculate dot product with daily volume data . Finally sum over ` axis=1 ` . See the edit .
You should use ` resample ` to calculate the mean price for the day , and then apply it to the volume : #CODE
This multiples price times volume for each matching column , sums up the total for all columns in that row ( i.e. hour ) , and then uses resample to sum up the results for each day .

I've tried loading it into a dense matrix first with ` read_csv ` and then calling ` to_sparse ` , but it takes a long time and chokes on text fields . If I call ` pandas.get_dummies ( df )` first to convert the categorical columns to ones zeros , then call ` to_sparse ( fill_value=0 )` it takes an absurd amount of time , much longer than I would expect for a mostly numeric table that has 12 million entries , mostly zero . This happens even if I strip the zeros out of the original file and call ` to_sparse() ` ( so that the fill value is NaN ) . This also happens regardless of whether I pass ` kind= ' block '` or ` kind= ' integer '` .

What I want to do is replace all 0's in the sex column with ' Female ' , and all 1's with ' Male ' , but the values within the dataframe don't seem to change when I use the code above
Also , you can combine the above into a single ` replace ` function call by using ` list ` for both ` to_replace ` argument as well as ` value ` argument , Example - #CODE

This would seem to involved nested group and then counts at one level of the grouping , but I am getting stuck . Maybe there is work around with just using a pivot table with the aggregate function as count
No promises that this is the slickest way , but I think you can get where you want to go with two groupbys , and a ` cut ` to get the levels : #CODE

I think it's the transform which is causing the problem . During the sum itself , memory usage doubles and afterwards returns to only a modest increase . During the transform , for me it quadrupled ( so 3x extra ) and took 2 min . If the OP is getting deep into swap , I could believe things would drop to a crawl . ( Plus it seems a little weird that it takes me 8x longer to do the transform than the sum , which should really be just the sum + a repeat . )

if_exists : append : If table exists , insert data . Create if does not exist .

Here `' columns '` is equivalent to ` df.columns ` and the result of ` eval ` is a Serials whose index is ` df.columns ` , therefore when the columns are named , the result of ` eval ` cannot be used as index of the original DataFrame . #CODE
Actually , it is bad to use `' columns '` in the ` query ` expression like this . In the first example , it is just happened that returned Series can be used as index , but it might not apply to the general case .

I have a dataframe and would like to replace each cell value based the formula
The problem with the ` apply ` / ` map ` / ` applymap ` functions is that they don't
Here is a faster code using ` apply ` , but it will provide wrong result in case there are 2 or more months in the same column with the same value , because ` np.where ` returns an ` np.array ` of the indexes that it found the value of ` x ` in , but there's no way to store it and use the next index the next time we encounter the same ` x ` value : #CODE
Instead , you could unstack ` df ` , and use its index , ` df.stack() .index ` , and ` pd.to_datetime ` to parse the index and column labels into dates : #CODE
and unstack to obtain the desired result : #CODE

Since ` value.Count.values ` and ` np.nan.values ` return arrays with one item , you can instead append the item to ` countsList ` directly : #CODE

Actually , this will go better in [ chat ] ( #URL ) . Care to join me there ?
Sure , let me join now . :)

Sorry , that's my fault and this is not due to date conversion . I was working with a much larger ` DataFrame ` containing other columns . In this context , it was a bad idea to use the ` apply ` function . After this correction , it works fine and I obtain the same result in less time . Thanks for the solution and for the reply .

Below is the code I apply to get the fitted volatility from the regression equation y = ax^2 + x + c and the results are great . . . #CODE
I need to have the regression variables and FitVol apply to all of the original data , not the data that was filtered to have the abs ( Delta ) be between 0.01 and 0.5

I want to merge these two together so that all instruments ( i.e. , A , B , C , D , ... ) can be shown in the same file with all of the measurement time periods . The expected result would look like this : #CODE
Yeah , that seems like a good characterization . With no overlap I think ` append ` or ` concatenate ` would be the more common and straightforward way to do it , but I don't see any problem with using ` combine ` or ` combine_first ` .
Assuming ` first ` is ` df1 ` and ` second ` is ` df2 ` , using ` concat ` appears to solve your problem . #CODE

I am trying to import pandas from a script . I go to command prompt and type ` python add_data.py -- update ` which is a script I have written . Before I was using the csv module to append columns to a csv and re write it , but to be more efficient I want to load it into a pandas data frame . I have used pandas on a prior computer but am having issues on this computer ...

Python Pandas : Transpose or Stack ?
Hello I have an example data frame below . I am having trouble obtain the desired results through transpose .... #CODE
Or , alternatively , you could set ` id ` as the index before calling ` stack ` : #CODE
` stack ` moves the column level values into the index . Since the desired result
the ` id ` column into the index first , and then to call ` stack ` .

You haven't to apply ` for ` loop or ` iterrows() ` at all in pandas : #CODE

Is there a way in pandas to merge two data frames with varying lengths by using a conditional statement ?
You ** cannot ** join on a conditional statement with any pandas functions . You have to create keys in each dataframe that can be joined on .

To align the index and fill the gaps #CODE

So , How can I re-index the data after queries ? Or I should ` drop ` instead ?

Group by then apply function then flatten back to dataframe in Pandas Python
However , to avoid using loop , you can do something like this : get the 1st observation using ` head ` and the delta's using ` diff() ` , then ` concat ` them together . #CODE
Sort your ` DataFrame ` by ` FQTR ` . This will make sure the ` diff ` operation orders by quarter
pick the columns you want to diff , and apply ` pd.DataFrame.diff `
Now apply the logic described above : #CODE
and , if you want , join the differenced columns back to the original ` df ` : #CODE
Clearly there's a little bit of work still to do to tidy up ( i.e. drop the ` NaN ` values if you want ) but that's left as an exercise !

Pandas Pivot Tables- Unexpected keyword ' cols '
I'm trying to make a pivot table using pd.pivot_table . #CODE
Check this documentation of pandas pivot table function . There is no parameter named cols . There used to be ' cols ' in older version of Pandas . Now , ' rows ' is replaced by ' index ' and ' cols ' with ' columns ' .

Merge multiple DataFrame columns into one
A possible solution using Pandas ` concat ` ( #URL ): #CODE
And of course , soring by ` P ` is easily done by appending a ` .sort ( ' P ')` to the end of the melt statement . #CODE
Perfect ! This is exactly what ` melt ` is designed for . In this particular example . you don't even need to specify ` value_vars ` . Nor does ` id_vars ` have to be a list . This is enough : ` pd.melt ( df , id_vars= ' weight ')`

You can call ` map ` on a temporary column and pass your other df ` growth ` with setting the index to column ' year ' , this will perform the lookup : #CODE

And I want an expanding apply function that identifies whether we reach a new maximum value for a given id . The resulting dataframe should look like this : #CODE
I can't seem to pass two columns to the expanding apply function .
Its been a long time since I worked with ` apply ` like a couple releases ago minimum , so my recollection may be bad , or things may have changed . However , as I remember it the grouped data is passed automatically as the first argument .
The temptation when passing your own function to ` apply ` is to do this : #CODE

Python Pandas : Merge or Filter DataFrame by Another . Is there a Better Way ?
One situation I sometimes encounter is , I have two dataframes ( ` df1 ` , ` df2 `) and I want to create a new dataframe ( ` df3 `) based on the intersection of multiple columns between ` df1 ` and ` df2 ` .
I know I can do this with merge ... #CODE
but then I have to figure out how to ` drop ` columns that end with ` _del ` . I guess this : #CODE
Is there a better way to return ` df1 ` without resorting to ` merge ` ?
Is there a way to ` merge ` but NOT return ` df2 `' s columns to the ` merge ` and returning only ` df1 `' s columns ?
Assuming that your ` df1 ` and ` df2 ` have exactly the same columns . You can first set those join-key columns as index and use ` df1.reindex ( df2.index )` and a further ` .dropna() ` to produce the intersection . #CODE
In example data above , this works great but when I apply this concept to my real datasets , I get ` Exception : cannot handle a non-unique multi-index ! ` . I verified that ` df1 ` and ` df2 ` have the exact same columns . Any ideas of the cause and how to fix ?
That did the trick . Really appreciate your help Jianxun Li . This concept seems like a great alternative approach to ` merge ` .

You should be able to replace that whole function using #CODE

In which columns 4 , 5 and 6 are actually the components of a vector . I want to apply a matrix multiplication in these columns , that is to replace columns 4 , 5 and 6 with the vector resulting of a the multiplication of the previous vector with a matrix .

The only reason to use the loop is to cut down on boiler-plate code . However ,
using Jianxun Li answer your modification to his ans . How would loop over the dict and isolate the the ' k ' into their own dataframe ... I am trying to generalize a solution so I dont know the keys ahead of time . I can drop the " df " in ' dfA '

split the columns up into a multiindex with fruit / year / month as the levels , stack it up , group as needed .

I am checking for rows that have missing values , but there's no function for that , so I have to do ( len - count() ) , so I pass a list of functions to agg() . E.G. x.groupby ( lambda x : True ) .agg ([ ' count ' , len ]) . If I leave the column type as a datetime , the len function's output is coerced to a datetime .
` df.isnull() .sum() ` would be great if you were just trying to get the null counts , but it's a common case to want to know the fraction of nulls ( for instance , to decide whether there is enough missing data to throw an exception ) . One ** could ** use ` df.apply ( lambda x : x.isnull() .sum() / float ( len ( x )))` for that case , but it's also useful to see stats at various levels of aggregation , so using groupby makes for a more uniform treatment . If you have several summary stats you want to gather ( e.g. ` len `) , you need to pass in a list of functions to ` groupby() .agg() ` . Like writing your own ` describe() `

When setting values on a Pandas series , the index is used to align values . So for you example , even though the number of values is the same , the indexes are different , hence the null values . If you want to override this behavior , you can use ` values ` to access the underlying array of the Series ( ignoring the index ) . Also you don't need to cast your indexers explicitly with ` pd.Series ` , this will happen implicitly if you past a list : #CODE

I imagine it'd be better to do this either using fillna or loc , but I can't figure out how to do this with either . I have tried the following : #CODE

@USER categoricals are more restrictive than strings in some ways , e.g. you can't just append more rows if those rows contain previously unseen strings . You should only convert to categorical if you know the restrictions don't matter for your current use case , so it has to be done explicitly . I'm not sure if the solution you posted would have the same drawbacks , I suspect it would .

if so i would gest that the apply method is a good hint here but like @USER -sc i would suggest you to restructure your data model or have a look at pytables etc

If you now assume that something / somebody who has no ` Time_in1 / Time_out1 ` did not have a break , you can replace the empty strings ( or ` NaNs `) with a datetime outside your query , for example : #CODE

basically what I want is to append the values of each column ( but B ) to the data frame and add one more column that has as values the name of the column associated with each value . I'll edit the question to make it more clear
You can use the convenient ` pd.melt ` for this , see the answer of @USER . A more general approach is to use ` stack ` , but this requires a bit more manual adaptations : #CODE
Yes , this is indeed the easier way than the manual ` stack `

Drop the date from a matplotlib time series plot

How do I resample a Pandas dataframe containing datetimes without losing tzinfo
I have a dataframe that I want to resample . One of the columns in the dataframe contains timezone-aware datetimes and I want the resampled dataframe to contain the maximum value of the datetime for the sample period . I am expecting the value in the resampled dataframe to still be timezone-aware .

Coerce back to strings , join and parse . #CODE

drop a series of true false in pandas removes first two lines
Also only giving it a list of ` False ` will just drop the first line . #CODE
The pandas drop function takes a list of indicies to drop , not a mask .
How to properly use the drop method
The proper way of using masks to drop data is either to mask , then access the index and hand this to the drop function : #CODE

where you could replace ` [ " D " , " E "]` by ` list_of_columns ` so you wouldn't have to repeat the column names .

You can create a column with the mutation type ( A -> T , G -> C ) with a regular expression substitution then apply pandas groupby to count . #CODE

You can transpose the df and apply a lambda that drops the NaN rows , slices from 4th value onwards and returns the first valid index : #CODE

Pandas Pivot - " TypeError : ' NoneType ' object is not iterable "
Pandas read_SQL is not working , I am trying to access a SQL pivot function ( Example 1 ) . I have tried reading all the data but Pandas does not also process the data .

I am trying to plot a map using basemap of the US and plot Los Angles , Chicago and New York on my map . For some reason everything plots but my labels do not all show up . Chicago is the only one to show up . Code below , appreciate the help : #CODE

The desired DataFrame have the ` D ` s in the index . ` stack ` moves column level values into the index . So it is natural to think about calling ` stack ` : #CODE
The desired DataFrame only has ` D ` s in the index , so let's drop the outer level values : #CODE
and then calling ` unstack ` to move the index level to a column level : #CODE
` stack ` and
` unstack ` .

Concat the same series a bunch of times ?

I want to munge the data , calculate some stats and create a pivot table . Would be quite hard to do , without able to read info in the table .
You'll also find that ` len ( u ' \u043f\u0440\u043 8\ u0432\u0435\u0442 ')` is 6 . So that string has 6 Unicode characters ( codepoints ) in it . In other words , everything's fine .

Join datetimes of DataFrames and forward fill data
I am trying to join the dates and forward fill the values . Right now I am doing it like this : #CODE
sorry are you wanting an outer merge : ` a.merge ( b , how= ' outer ') .ffill() ` ? I don't know if the dates are your index or a column , you need to clarify this
You can't update both dfs in place in a one liner , you can either update 1 or create a new df , not update both , additionally why don't you just assign back the result of the merge , I don't see anything wrong with doing that
This will join and align on indices taking the union of both dfs , this will introduce ` NaN ` values which you can fill using ` ffill `

but with a larger array ( more nonzero elements ) I get a display error . I'm guessing it happens when the display for the ( plain ) series starts to use an ellipsis ( ... ) . I'm running in Py3 , so I get a different error message . #CODE

I have a dataframe that I want to insert into a ` mysql ` database . Before inserting it , I need to fill in NA values in a column containing a non-unique key . The filled in values need to be shared across groups , but cannot overlap with any preexisting values in the sql table .

I experience this a lot in modeling time series . Sometimes you may have data reported at different frequencies , say one daily and one weekly . What I'd like is not to forward fill the weekly data point for every day of the week ( since it is usually a sum of all the values of during the week already ) , but forward fill or replace the data with it's mean . In essence , I'd like to spread out the data .

@USER then may be you can use the idea and apply to panda DataFrame

Maybe show us what you've tried . Pandas has lots of rolling aggregation , resample , and grouping operations .
Note that you can pass ` np.arange ( len ( df )) // 20 ` to groupby in case the index isn't already the standard .

In case you need to convert existing columns in a dataframe here the solution using a helper function ` conv ` and the ` apply ` method . #CODE

To only test your code , replace the `' UTCTIME '` string with a actual date string or use a variable with the string .

If you don't want to keep the index , drop it afterwards : #CODE
So using your method , I would need at least 3 different actions for each time I want to plot ( save a new variable , compute new index column , then drop it ) .. that makes things quite slow :/

I have spent some time looking through the groupby documentation and I have trawled stack overflow and the like but I'm still not sure how to approach the problem . I would be very grateful if anyone would suggest a sensible way of achieving this for a largish dataset .
Yup , I noticed that the moment I posted the comment ... Looks like my evening is going to be spent with the pivot documentation . I have accepted your answer and thanks again !

I am trying to fill the Series ` series ` . ` print ( l.get_results() )` returns the Series's as I expected them to be , however , ` print ( series )` returns an empty series and I don't know how else to append the Series's to ` series ` . #CODE

Merge CSV into HDF5 in pandas leads to crash
I have about 700 ` CSV ` files . They are each typically a few meg and few thousand rows . So , the total folder is ~1gig . I want to merge them into a single ` HDF5 ` file .
I don't know what this error means . I have also seen it pop up a window showing a stack error .

then resample daily : #CODE
Is there a way to resample just one column , and preserve the value in another column at the same index ? So that the final outcome would look like this : #CODE
My first guess is to just resample the ` heat_index ` column ... #CODE
Here's one way - the ` .groupby ( TimeGrouper() )` is essentially what ` resample ` is doing , then the aggregation function filters each group to the max observation . #CODE

This may be some limitation of ` concat ` and probably not a bug but something where you have to decide how the data should be merged / aligned
I found it was referring to the first index my solution was : ( thought not sure how efficient it is but the concat works afterwards ) #CODE

The above uses ` dt.week ` and ` shift ` s by 2 rows and then forward fills the ` NaN ` values .
does the ` -2 ` shift come from Saturday instead of Monday as first day of the week ?
Thanks @USER . I tried your code and it does work , it's indeed a cleaver way to do it . But it won't work in my code because i have duplicate dates in rows , e.g. 3 rows ' 2015-08-09 ' , 10 rows ' 2015-08-10 ' , etc ... then the ' shift ' won't work in this case . Thanks anyway .

I am sure this can be done using pandas merge and then outputting the rows that didn't merge , but I just can't seem to get it right .

I am able to do both tasks separately to find mean and mode for particular columns , but I can't work out how to join the two outputs together , but then I was also wondering if there is a way to calculate mean or mode of selected columns in one hit ?
In my current method , although I am able to do both tasks separetely I am struggling to join the two outputs together .

I believe your data structure isn't appropriate for your problem . Especially the ` list ` in fields of a ` DataFrame ` , they make loops or ` apply ` almost unavoidable . Could you in principle re-structure the data ? ( For example one ` df ` per solar panel with columns ` date ` , ` time ` , ` energy `)
I believe your data structure isn't appropriate for your problem . Especially the list in fields of a DataFrame , they make loops or apply almost unavoidable . Could you in principle re-structure the data ? ( For example one df per solar panel with columns date , time , energy )

Python pandas : retrieve the field associated to the min of another ( cross apply equivalent )
In SQL I was used to doing this with a cross apply .
PS other than calculating the min first , then doing a join on primary key and date
No . I can do this in two steps : 1 ) group by primary key and calculate min ( date ) 2 ) do an inner join between the starting table and the table calculated in the previous step , on primary key and date , to retrieve the amount

Python & Pandas : Unable to drop columns
I try to drop the data , but it reports some column does not exist . #CODE
@USER , that's possible , but I don't know how to deal with it . In my previous experience with pandas , it will automatically turn the second ` Q ` into ` Q.1 ` when reading the data . However , in my case , it failed to do it , and I don't know why . However , This it cannot ` drop ` ` NCDC ` either .
You can then drop your columns : #CODE

@USER yes what you suggested gives the OP what they want , I've updated my answer , I should've realised that doing the ` resample ` after grouping should work
If you set the index to ' date ' then you can ` resample ` quarterly : #CODE
Yeah this gets tricky here you'd have to merge / add back the additional info if you want to retain it

You could strip off the index , join the frames , then add back the index #CODE

type switch from int to float64 after merge creating error
I'm trying to merge two dataframes in Pandas . One of the dataframes has a numerical column whose type is " int64 "
However , after the merge , the type is switched to " float64 " for some reason . Note that this is not my join column

I'm trying to append to a ` Series ` Object within a loop as seen in the code below , but find that the appending is not taking place at all . ` bond.to_cash_flows() .amounts ` results in a Series like the one shown below and I am just trying to append to this Series .
Thanks . I see . So , how could I replace this line ?

I am trying to change the index of a Series . The indices are ` Date ` s . ` cashflow_series ` is the Series , and I am just trying to add a certain number of months to this date by using ` replace ( month = cashflow_series.index [ k ] .month + dict_no_months_between_payments [ count ])` on the index and use this to replace the previous index date . However , I get the error ` SyntaxError : Invalid Syntax ` . #CODE

I am trying to change just a particular index of a Series . The indices are ` Date ` s . ` cashflow_series ` is the Series , and I am just trying to add a certain number of months to this date by using ` replace ( month = cashflow_series.index [ k ] .month + dict_no_months_between_payments [ count ])` on the index and use this to replace the previous index date . I am currently using ` reindex ` but that would change all the indices and hence I'm stuck as to how to change just one index . #CODE
You need to create a copy of the old index values , modify it , and then replace the one in your series with this copy . Something like : #CODE

@USER , the ` snap() ` method could still work with some basic checks : if the snapped date is before the given date , move the given date forward by the given interval and snap again , then go back a day .
Snap to the last day of the week ( for example , Sunday ): #CODE
Snap to the last day of the month : #CODE
Snap to the last day in the quarter : #CODE
Snap to the last day of the year : #CODE
The linked documentation says " nearest . " Does it only snap forwards ?
@USER : Thanks for the correction . Updated answer does " snap forward " .

Bulk Insert A Pandas DataFrame Using SQLAlchemy
I'd like to be able to pass this function a pandas DataFrame which I'm calling ` table ` , a schema name I'm calling ` schema ` , and a table name I'm calling ` name ` . Ideally , the function will 1 . ) delete the table if it already exists . 2 . ) create a new table 3 . ) create a mapper and 4 . ) bulk insert using the mapper and pandas data . I'm stuck on part 3 .
It seems that you are recreating the ` to_sql ` function yourself , and I doubt that this will be faster . The bottleneck writing data to SQL lies mainly in the python drivers ( ` pyobdc ` in your case ) , and this is something you don't avoid with the above implementation . Furthermore , ` to_sql ` does not use the ORM , which is considered to be slower than CORE sqlalchemy even when using bulk insert ( #URL )
@USER Thanks . It seems that the " bulk operations " listed here are a bit of a misnomer then . #URL What I really need to do is output the pandas datafile to a textfile and write the BULK INSERT operation like this ... #URL
yes , but that is to improve the speed of sqlalchemy ORM , which has a lot more functionality than only core sqlalchemy . But pandas ` to_sql ` does not use ORM at all , as I said before , and is in fact already doing a bulk insert .
@USER Well , the reason why I went down this road was I can run a ' BULK INSERT dbo.MyTable FROM \\fileserver\folder\ doc.txt ' on the SQL Server and the performance is great . What I'm thinking is that when the BULK INSERT statement uses " VALUES " instead of " FROM " , that's where the real performance loss is . In other words , the connection from the sql server to file server is better than the connection from my virtual machine to the SQL Server . Thanks .
The Class Current needs to align with the dataframe imported in the CSV and the table in the db1 .

How to change color bar to align with main plot in Matplotlib ?

If you want to apply an arbitrary Python function , you will have to loop it .
Can you write out the formula you want to apply ?

If I don't include the second line ` hist [ ' bins '] = hist.index ` , I still get an ` AttributeError : ' Categorical ' object has no attribute ' flags '` and to the best that I can tell , the traceback is identical .
Can someone explain what the ` flags ` are and why they only seem to work when I set the ` index ` to ` bins ` and then replace the ` bins ` by the version stored in the ` index ` ?

Then drop duplicates : #CODE

Consider using a pandas.merge and then drop any resulting joins . #CODE

I am trying to append to a Dataframe dynamically , but get the error ` ValueError : Incompatible Indexer with Dataframe ` in the line ` df.loc [ count ] = pandas.DataFrame ( amounts ) .T ` . #CODE

AttributeError : ' bool ' object has no attribute ' all '` I'm using ` In [ 12 8] : np.__version__
The ` Ellipsis ` is not producing the scalar boolean . I can replace it with a tuple , and still get the scalar . #CODE

Based on the latest suggestion of Firelynx I have a small update which makes it a bit cleaner . Still , you need to keep a list in order to prevent double counts of the label , because unique apply only to a unique ( label , side ) combination . So I now have #CODE
Is it possible to have unique() apply on the label alone ? Then I could remove the label_list to keep track of which label has been processed already

sorry , n = len ( conditions )

Unfortunately , when you zoom or pan the plot , the labels change and do not align with the data points .

Why is there a mistake in my replace function ?
Why the replace function can't transform 0.01 to 3 in first print statement ?

tom's answer looks good . On a column you could also do ` .map ( lambda x : min ( x , 0 ) )` to apply the standard python ` min ` to each cell , but ` np.minimum ` is probably going to be the fastest way .
OR you can apply over two columns : #CODE

Now I have two columns in my dataframe - old ' col3 ' and new ' c ' and need to drop old columns .
Further , it is possible to select automatically all columns with a certain dtype in a dataframe using ` select_dtypes ` . This way , you can apply above operation on multiple and automatically selected columns .

Pandas : resample timeseries with groupby
I would like resample the data to aggregate it hourly by count while grouping by location to produce a data frame that looks like this : #CODE
use ` unstack ` to move the ` Location ` index level to a column level : #CODE

trouble accessing multi index column after join
However when I join the above table with another one , I can't figure out how to access the table anymore .

I have a pivot table in a data frame and I'd like calculate group percentages .

Append data from file to dataframe with Python ( pandas )
The goal is to loop files , fetch content from those files and append them to the dataframe ` df ` . There is a catch though : content should be fetched per line per file . This question is a Python alternative to a similar question I had for R .
If I understand correctly what you do is , create a dictionary entry that consists of key filename and key sentence , and then add the values that we need . But what I don't get is , how does ` append ` know how to interpret the entry , i.e. how does it know it should merge the entry with the already existing column names ?

Merge two pandas dataframes on not exactly matching timestamps
I need to merge these to dataframes into one on ' time ' column so that for records 3 , 4 , 5 from dataframe 1 indexes 1 , 2 , 3 from dataframe 2 were on the right .
I have troubles because time is not EXACTLY the same ( look at the last 2 microseconds ) . Is there a good way to merge it the way to merge these on time with time not exactly matching , but given some matching threshold maybe ? Also there should only be no more than ONE match for each record .
Assuming your time is a string , one thing you could simply do is just strip out the last two or three digits of the time and then perform the join . e.g. x [ ' time '] =x [ ' time '] [: -3 ]
This was my first idea , but this would produce some duplicated joins .. I need a way to uniquely join those records , meaning find a single closest matching time ( smallest absolute difference ) for each record .
After accumulating the matches , just drop duplicates to keep the more precise matches . #CODE
Then I can simply merge groups1 with groups2 on common index .

And if you want to append the new columns to the old data frame , simply use the same ` DataFrame ` for both input and output

You are calling ` replace ` on the Series object . This is not the string ` replace ` method but a pandas method that replaces entire values . So if any of the values in your column were `" "` ( i.e. , a cell in the DataFrame contained just a single space and that's it ) , it would be replaced with an empty string .

and I am getting the following stack trace : #CODE

What I want to do is append DataFrame2 values to Data Frame 1 ONLY if the ' MedDescription ' matches . When it find the match , I would like to add only certain columns from dataFrame2 [ Min , Max , Days Unused ] which are all integers
I had an iterative solution where I access the dataframe 1 object 1 row at a time and then check for a match with dataframe 2 , once found I append the column numbers from there to the original dataFrame .
This sounds like an opportunity to use Pandas ' built-in functions for joining datasets - you should be able to join on ` MedDescription ` with a the desired columns from DataFrame2 . The ` join ` function in Pandas is very efficient , and should far outperform your method of looping through .
This is the way I used to join the 2 DataFrames , it seems to work , although it deleted one of the Indexes that contained the devices .
It sounds like you want to merge the target columns ( ' MedDescription ' , ' Min ' , ' Max ' , ' Days Unused ') to df1 based on a matching ' MedDescription ' .

Export pandas DataFrame to LaTeX and apply formatters by row
I want to export some DataFrames to LaTeX but these DataFrames have lots of columns , but not that many items . The solution is to display the table transposed . I know about pandas ' transpose , but I want to format my rows and the method to_latex supports only formatters by column .
Is there any way to hack this functionality ? Only thing I thought about was to manually apply the formats converting all my columns to strings before transposing and exporting

The ` interploate ` method in ` pandas ` use the valid data to interpolate the ` nan ` values . However , it keeps the old valid data unchanged as the following codes .

To all the answerers : as the OP mentions pandas , it may be desirable to enclose in apply / lambda . E.g. ` df.apply ( lambda x : your_code )` . It would also be good for the OP to be more explicit : ` df =p d.DataFrame ( [ 0.5 , 4.6 , 7.2 ] )`
` abs ( x * 10 ) % 10 ` works for both positive and negative numbers .

In the end what value will the bar represent ? The sum of values for that street type in that district ? The mean ? Median ?

For this , you can use ` ix ` : #CODE

Again , the index is a DateTimeIndex with dtype = ' datetime64 [ ns ]' , length = 1412 , freq = None , tz = None

For computing median , it should be done according to count of occurrence of every instance ( in column ) or just according to instance ?

What you are doing here is taking each of the lists you created above using the map function and then placing it inside another list . ` [ A ]` for input string `" 1 2 3 "` will become ` [[ 1 , 2 , 3 ]]` which is not what we want . Replace that line with : #CODE

There is one difference though : ` .drop ` throws an error if you try to drop column or columns which don't exist in the DataFrame , while ` .difference ` doesn't . That is good to take into account when you are choosing which one to use .
For completeness , you can also easily use ` drop ` for this : #CODE

Thanks ! one more question , will I manually ` e = re.sub ( ' / ' , ' - ' , c )` and apply ` to_datetime ( e )` that can improve the performance ?
I don't think so . If you can do that , it means you know for sure what the format is , and then it is better to supply a ` format ` string . Supplying an ISO formatted datestring is a bit faster to parse , but that will not outweigh the time to replace the ` / ` with ` - `

Thank you ! The only thing I cannot understand now : in the second one you used ** for i in range ( 1 , 4 ): ** - as far as i've got it , I need to replace it with custom numbers , by , lets say , calculating the max N in df , and then checking : if row.N <= maxN , then proceed . Is that right ?

I thought maybe the problem was that in some entries there was one or more spaces before or after " LARKFIELD " in the incident description , so I did a search / replace to try to strip out any spaces , but I still get only 115 values when searching by " LARKFIELD , " even though I know there are many more incidents in that area .
What does ` len ( larkcounts )` show ? you may have ` NaN ` values in your years which are not being counted in ` value_counts `
Thanks JohnE . I thought whitespaces might be the problem and had tried to strip them out , but being relatively new to Python and pandas I think I may have not done that correctly . I'll try again and see if that changes things .

I think pandas's data reader is pretty limited . I'm sure there's an easy way to do what you're trying to do but pandas may not be much help for the data gathering itself and this is not really a good stack overflow question ( I'm not sure where to suggest you look for the answer though )

I think what your asking ( correct me if I'm wrong ) is best accomplished by putting the two columns in a single dataframe , using ` shift ` to offset one of your columns , then doing an ordinary subtraction . #CODE

This will merge ` df_a ` and ` df_b ` on all columns shared in common .
Yes , that's actually better than my concat :D thanks .

shift by partition using groupby
I have a dataframe with one column I would like to shift , but over partition rather than the whole dataframe .
You can store your intermediate DataFrames in a list and use ` pd.concat ` to join them together : #CODE

Python join - how to join a data in loop ?
How can I join a data below , #CODE
If you want to keep your method of looping through each , then you can simply remove the last ` / ` by doing ` rstrip() ` on it to strip from the right side . Example - #CODE

You're performing [ chain indexing ] ( #URL ) which is why you get the warning , you should use the new indexing methods ` loc ` , ` iloc ` or ` ix ` to ensure you're working on a view rather than a copy
you already accessed the column before calling the ` .apply() ` method on it , notice that you call the apply function as - ` df [ 0 ] .apply ` , which means apply it in 0th column of ` df ` dataframe .

I have numpy DataFrame with 5 columns . It consists of a time series ( Day ) of streamflow ( Discharge ) data measured each 15minutes ( Time ) . A flood event occurs when there is a discharge difference of 1.5 between to consecutive moments ( Flood_Index ) . A last column ( Floods ) indicates when this is the case ` abs ( df [ ' Flood_Index ']) 1.5 )` .

how to transpose dataframe ?
You can think of this as a ` pivot ` . If your DataFrame had an extra column called , say , ` colnum ` : #CODE

and lastly join them back to the original frame with : #CODE

Now I take my dataframe above and replace first digit 8 with digit 9 #CODE

Pandas extrapolating data with resample
And I am trying to resample it to a frequency of , say , 5 minutes . So I am doing ` concs.resample ( ' 5min ' , how= ' mean ')` . However , what I get is data that starts at ` 2013-01-06 ` instead of in the first date of my original data : #CODE

if I apply a groupy say with column col2 and col3 this way #CODE

remove overlay text from pandas boxplot
I am trying to remove the overlay text on my boxplot I created using pandas . The code to generate it is as follows ( minus a few other modifications ):
I just want to remove the " boxplot grouped by 0 ... " etc . and I can't work out what object it is in the plot . I thought it was an overflowing title but I can't find where the text is coming from ! Thanks in advance .

I have the Pandas Series ` s ` , part of which can be seen below . I basically want to insert the indices of those values of ` s ` which are not 0 into a list ` l ` , but don't know how to do this . #CODE

I would like to take the mean of each array per row and replace it with column ' X ' ; #CODE

I'm not that much focused on speed as I'm trying not to lose it where it can be gained . Could you recommend any NumPy data structure to replace the dictionary ? I've been using ` lists ` for this sort of stuff in ` R ` , but that's a different story . I chose the ` dict ` -> ` list ` combo for fast key-search and fast append that doesn't require reallocation of the entire array . I'm not aware of any hash-based NumPy structures and / or dynamic arrays . Or should I ask this in a separate question ?
One option is to use groupby and apply to end with a pandas Series : #CODE

Im using ` Matplotlib ` and ` Pandas ` to create a pretty standard graph of two time-series data points . The problem is ( see below ) , the data I'm interested in is after the large drop in values , I'd like to see a bit more clearly the difference between the green and red lines . Currently its a bit of a mess that is not very readable .
I think making the graph more clear is not what you should aim for . I think you should cut out the data that you want to see and make a graph of that .
Your problem is that you have a very high peak . Just cut away all values that are too high and make a new graph .

I unstacked your dataframe into a longer format , then grouped by the name column . Within each group , I drop the NaNs , but then reindex to the full h1 thought h4 set , thus re-creating your NaNs to the right . #CODE

I want a copy of the ` fruit ` table sorted by ` cost ` from the ` costs ` table , but without the cost column included . What's the best way to do this ? It's fine if there's a join in an intermediate step - I'm mostly worried about long-term memory waste .
I would do a left merge and then argsort : #CODE
Note : that if you used a different index ( for fruits ) , it will be ignored / replaced with ` range ( 0 , len ( fruit ))` . #CODE
Now reorder using iloc ( by position ) rather than loc ( by label ) . #CODE
Note : It's important to left merge as an ordinary merge will change the order ( !! ) . It's also more efficient .
why don't you merge the columns and then drop the unneeded one #CODE

Can I map the ' make-model-variant ' column to a set of unique integers ? I have been coding in python 2.7 for some time , but new to pandas .

` a.sort() ` modifies ` a ` and does not return anything so replace by : ` a.sort() ; print a `

The resulting index might not be exactly what you want as it starts with the earliest datetime correct to the nanosecond . You could replace with datetimes which have 00:00 : 00 in the time by doing #CODE
` resampled = df.resample ( ' D ' , [ sum , len ]); `
` period_range = pd.date_range ( start = ' 2014-01-05 ' , end = ' 2014-12-31 ' , freq= ' 7D ') ; resampled = df.resample ( ' D ' , [ sum , len ]); resampled.reindex ( period_range )`
This will not return weekly sums ! ` df.resample ( ' D ' , [ sum , len ])` gives daily sums . Reindexing with ` period_range ` will simply return a view of of the sum on each Sunday of the year , not the weekly sum to each Sunday . If you want weekly sums to Sunday you have to resample with weekly frequency .
OK . Thanks . So ` resampled = df.resample ( ' W ' , [ sum , len ]); resampled.reindex ( period_range )` seems to work , right ?

As far as numpy is concerned , a ` list ` counts as an arbitrary Python object . numpy can only efficiently deal with arrays that have regular dimensions and contain elements of a constant size in memory ( this all has to do with numpy's [ internal representation ] ( #URL ) of the array ) . This doesn't apply to Python lists , since the length and item size can vary arbitrarily .
I don't really see any reason to use ` iterrow ` . I would probably do something like ` df [ ' num_choices '] = np.array ([ len ( row ) for row in df.options ])`

I want to insert values from one data frame to another data frame , when a particular condition is met . I want to insert it to the top of the dataframe . PL is the source dataframe and sub is the destination Data frame . This is the following code I used , #CODE
Once I do this , one value from pl is insert to the top of the sub data frame . but I am having a problem here . The index value of pl are into sub and the index values are as follows , #CODE

Then I use ` loc ` to get the index values of non-negative values in the series . If this list returns anything ( it could be an empty list ) , then I use ` .ix ` to index the series from the first non-zero index value calculated above through the end of the series .

This is under the circumstance that the csv files are going to be updated monthly thoroughly . If you use the solution 3 ) , you have to replace the data stored in database every month .

Ha , that would be too easy :) . That's the result of the grouping operation that I want to apply . This column is unpopulated until I run the code above .
This is like a resample operation .
Might be simpler just to resample . You can use a custom function for ` how ` . #CODE
assert len ( df.groupby ( pd.Grouper ( freq= ' ms '))) == 1

@USER Which numpy version do you have ? This argument is new in 1.9 ... but there is a workaround , try ` np.linspace ( 0 , len ( pep_list ) , n+1 , endpoint=True ) .astype ( int )`

Another option where you can control the format is using the ` strftime ` method in an apply ( this would actually be equivalent to writing a loop , but shorter ): #CODE

merge few pivot tables in pandas
How I can merge two pandas pivot tables ?

Thanks . I think that I understand what's going on : create a frequency table of ALL words . Then , from that tablemake sums . First the neuter cases , then nonneuter . After each operation , drop all relevant columns , then finally count all remaining columns . Correct ? One question though . What does ` axis=1 ` do ? Also , I quickly tried this in Python 3.4.3 and I got the error that freqDf isn't defined . Should I first create a new table named freqDf ?
There is no KeyError because it is testing whether words in ` precedingWord ` are in a given list . ` df.precedingWord.isin ( neuter )` is just a Series of True or False ( results of the previous test ` isin `) , and pandas will just access True indexes with ` loc `

However , when I apply this to my full dataset , with multiple dates in utctime , the x-axis remains a time - I want it to show the dates in this case .
I have tried converting .to_julian_time and to_py_datetime after following some other stack posts . I've also tried grp.index.date , but this still gives a time in the x-axis , and is worse because it loses the detail . Any ideas ?

Groupby on level 0 ( parameter1 ) and apply ` idxmax() ` and get the values : #CODE

It's a useful and common practice to append predicted values and residuals from running a regression onto a dataframe as distinct columns . I'm new to pandas , and I'm having trouble performing this very simple operation . I know I'm missing something obvious . There was a very similar question asked about a year-and-a-half ago , but it wasn't really answered .

Dont drop non present indexes on group_by - Pandas
You have to pass either a MI or an array of tuples ( I don't think a plain array will cut it ) see ` MultiIndex.from_arrays ` or ` MultiIndex.from_product ` ( you * may * have to splat the array ... ) .

The K-Means clusterer expects a 2D array , each row a data point , which can also be one-dimensional . In your case you have to reshape the pandas column to a matrix having ` len ( data )` rows and 1 column . See below an example that works : #CODE

Go through the matrix line by line , than apply in each element ` ord() - 65 ` if it is an alphabet else use it as it is . ` 64 ` is ` ord ( " A ") -1 ` .

drop the ` NaN ` value when converting DataFrame to dict , and then
The JSON C source code explicitly tests for ` int ` , ` long ` , ` float ` and ` bool ` keys , converting all those to strings . This means you have keys that are not really integers but only * mimic * integers ( their representation is the same , they test equal , but ` isinstance ( int , key )` fails ) .

Juse use ` abs ` : #CODE

With pandas : if , in a row , a word in a column does not occur in string in other column , drop row
How can I drop the rows in which ` one ` doesn't occur in ` two ` ? For instance , on the third row ` banana ` doesn't match ` bananas ` : drop row . In the fourth : ` hello ` doesn't match ` hello-kitty ` : drop . That last one is important : compounds built with a hyphen ` - ` are obstacles .
Another method would be to calculate the list of indexes to drop and store them in a list and then at the end use ` DataFrame.drop() ` . Example / Demo - #CODE
I am not sure if there is a better way to do this , but you can iterate over each row and then split the string in column ` two ` and then check if the string in column ` one ` exists in it or not , and then append the rows that match to a new dataframe .
Hi @USER , Q1 . result is a list , before you append any element to it you have to declare a list instance of it . Q2 . python doesn't know that result is par of df . result is a list of booleans ' False ' and ' True ' when you pass it to df , it will only give you back the rows that match True and let go the others . Q3 . As I mentioned previously , you need to have a list , in order for df to know which lines you don't want to keep , you have to specify which lines are not required ( False ) .

pandas - pivot table to square matrix
I would like to pivot it , and then return a square table ( as a matrix ) . So far I've read the dataframe and build a pivot table with : #CODE

You can use a combination of ` stack ` and ` unstack ` : #CODE

Use ` groupby / agg ` to aggregate the groups . For each group , apply ` set ` to find the unique strings , and `'' .join ` to concatenate the strings : #CODE

[ ' count ' , ' mean ' , ' median ' , ' min ' , ' max ' , ' mad ' , ' std ' , ' var ' , ' sem ' , ' skew ' , ' quantile ']) .sort ([ 1 ])` by tring to sort on the ` column [ 1 ]`

I need to replace all the spaces in a dataframe column with a period . ie :

I append two blocs of dates using something like ` dates.append ( date_range ( df.DateStart.iloc [ i ] , df.DateEnd.iloc [ i ]) )` , looping on both clusters and date intervals , but the appending results in a non-flat list . How to do it properly ?

I would like to get every , let's say , 6 hours of data and independently fit a curve to that data . Since pandas ' ` resample ` function has a ` how ` keyword that is supposed to be any numpy array function , I thought that I could maybe try to use resample to do that with ` polyfit ` , but apparently there is no way ( right ? ) .

Pandas map 0 / 1 data frame entries to column names
convert the dtype of the df to a ` bool ` , then call ` apply ` and use the boolean mask to mask the columns , you need to pass param ` axis=1 ` to ` apply ` the column mask row-wise : #CODE
Your code ` my_df.apply ( lambda x : colnames [ x ])` won't work because firstly when calling ` apply ` on a df without specifying the ` axis ` will call the lambda on each column in turn , secondly the ` 1 / 0 ` will interpret this as an index value rather than a boolean flag .

And I want to pivot the data as follows :
Retrive the data : since you are using ` sqlalchemy ` already , you can simply query the database for only the data you need ( flat , without any CROSSTAB / PIVOT )
Pivot : Call ` pivot = df.crosstab ( ... )` to create a pivot in memory . See pd.crosstab for more information .

You could ` fillna ` to replace the missing values - passing in a ` DataFrame ` with the last value of each group . #CODE

Append with date_range() in Python

The issue occurs because you have three columns with only ` NaT ` values , which is causing those columns to be treated as objects when you do apply your condition on it .
You should put some kind of condition in your ` apply ` part , to default to some timedelta in case of ` NaT ` . Example - #CODE
Or a simpler way to change the ` apply ` part to directly get what you want would be - #CODE

And here's the workaround . It's looks rather long ( it is ) but each individual step is simple and fast . ( The timings are at the bottom . ) The key is simply to avoid using ` groupby ` at all in this case by replacing with ` shift ` and such -- but because of that you also need to make sure your data is sorted by the groupby column . #CODE

Now transpose the df and perform a list comprehension , this will construct your lists and concatenate them using ` pd.concat ` : #CODE

It feels like the data may be in a particularly unhelpful shape , but I don't know how to make it better - I tried a pivot table with the categories as columns , but since utctimes are unique to categories , that didn't work . Should I take utctime out of the index ?
I added an answer . No way to group AND perform this type of calc ( mainly because the grouping zonks the freq ) , but just adding a column solves the problem .
Add in an extra column that we want to diff as well #CODE
add the binary flag with your grouper , or group at a lower freq . not really sure what you are trying to do .

It's hard to tell what you want as you haven't posted desired output but if I understand you correctly you want to count the number of rows in time intervals of certain length . You can do this by combining ` resample ` and ` len ` . To use ` resample ` , first set the index to `' Time and Date ` : #CODE
I must say you understand correctly , but when I try to resample I get and TypeError : Only valid with DatetimeIndex , TimedeltaIndex or PeriodIndex .
This works , but since I need hight granularity on us , resample on us hangs.is There any solutuion for that issue ?

The first one is pretty obvious , you just set things up so that both are series and then things behave as you'd like . And note that this is easier than converting both to dataframes in that you only need the indexes ( axis=0 ) to align when multiplying series , but you need both indexes and columns ( axis=1 ) to align when multiplying dataframes . Though you could get around this here by giving your series the same name as the dataframe column you wish to multiply by .

Join two pandas data frames with the indices of the first ?
Now I want to join ` df1 ` and ` df2 ` , but I don't know how to match the indices . If I simply do ` df1.join ( df2 )` , the indices are aligned to ` df2 ` . That is , it finds the 40th entry of ` df2 ` and that is now the first entry of the dataframe that starts at 40 ( ` df1 `) . How do I tell pandas to align indices to ` df1 ` , meaning that the first entry of ` df2 ` is actually index 40 ? That is , I would like to get : #CODE
You can take a slice of your df that is the same length as ` df1 ` , then you can overwrite the index values and then ` join ` : #CODE

How to normalize by another row in a pandas DataFrame ?
Experiment 0 is my control . I performed this experiment for various colors . I want to normalize all rows by the matching color experiment 0 . #CODE
@USER , isn't that for scaling the values in a single column / array ? When I say normalize , I don't mean the vector ranges ( 0 , 1 ) , but that each is divided by an outside value .
Since we're handling the alignment ourselves , we need to call ` .values ` to drop down to the underlying array -- otherwise pandas will try to outsmart us and align things correctly based on the indices .
and finally join them , renaming the new columns : #CODE

Python Pandas drop columns not found in both dataframes
I've come up with a solution but it's problematic because it will drop ` any ` column that has ` NaN ` . #CODE
I have a feeling there's an easier way to return a dataframe containing only the columns that are found in both two different dataframes ( columns intersection ) . More elegant approach ?
You can use the ` intersection ` of the 2 df columns : #CODE
Here I am asking how to intersect and in ` pd.concat ` , there it is , join by ` inner ` . I can't believe I missed that . I like the fact that this is a simple one-liner .

Have you tried ` resample ` ? E.g. ` df.resample ( ' 1min ' , ' mean ')` What aggregation are you doing
@USER I am applying customized functions with APPLY function . It seems to me that resample or TimeGrouper fills in the gap automatically , even there is a time gap of one year . Is there a way to prevent from this ? Thanks a lot
@USER thanks for the suggestion . I have switched to resample and it almost works . Only resample takes the first column of df , does it apply to multiple columns of df at the same time ? I would reedit my function into the question . thx again

Thank you . To answer your question I didn't think of count ( r ' @ ') ... good point . Then I can append it with .max .mean .min Excellent Point ! It works . I'll also study the applymap method - thanks !

I think you want to ` agg ` ( aggregate ) , not ` apply ` , as for each of your group , you want 1 returning value : #CODE

what do you mean by normalize ? such that the means are zero and standard deviation are 1 ?
The other way is much easier and involves using ` resample ` to convert to daily observations and backfill daily consumption . #CODE
From there , all you have to do is aggregate . ( Note that the first and last months are based on partial data , you may want to either drop them or pro-rate the daily consumption . ) #CODE
Some brief thoughts on an alternate approach : If the above causes memory problems , I think there might be a hybrid approach . Basically , after calculating the daily consumption , do a partial resample by adding the first and last day of each month . From there you can probably aggregate in a similar way although you need to essentially do a weighted sum rather than simple sum .
This looks like a pretty good solution . I will implement it and see how it goes , but can you also explain what ' 1d ' means in the resample method ?
@USER ' 1d ' just means 1 day for the frequency of the resample . It can also just be written as ' d ' , the ' 1 ' is redundant in this case .

This would not replace whatever is inside the ` time_unit ` string and take ` x.week ` , instead it would try to take the ` time_unit ` attribute of x , Which is causing the error you are seeing .

Apply the daily frequency to weekly frequency ( eg . Monday to Sunday )
Apply daily frequency to monthly frequency ( eg . how many times I see " 2012-01 -** " in my column )

Last apply to get annotation #CODE

I'm working on a crawling function that writes continuously to ` output.csv ` . If it's the first pass , it will write the first row with the column ` header ` to a blank file . For the following passes , it will ` append ` with no header .

Using the ` pandas.date_range ( startdate , periods=n , freq=f )` function you can create a range of pandas ` Timestamp ` objects where the ` freq ` optional paramter denotes the frequency ( second , minute , hour , day ... ) in the range .
eg . ` pd.date_range ( start =p d.datetime ( 2000 , 9 , 10 ) , periods=4 , freq =p d.DateOffset ( years=1 ))`

Merge two tables based on intersection in Python
I have to merge them in such a way that the new table looks like this

Approximate Matching for Numeric Values for Pandas Merge
I'm trying to merge two data frames based on multiple columns
I want to merge these two dataframes using the first three columns . The problem is that the Seconds columns are not exact matches . But , they all seem to fall within a range of one another .
I cannot just sort and merge by index because the two dataframes are not of the same length ( there is missing data in one ) .
Is there a way to merge these dataframes where the first two columns must be an exact match and the third column needs to meet some minimum threshold ( e.g. , diff =2 ) ?
Ultimately , to follow this method , I had to combine the columns I wanted to merge on to create a unique string variable for each observation . Then , using the instructions in the post you linked , I was able to match a subset of the data where there was an exact 1-1 match for each record .

Pandas pivot table

Pandas has an efficient ` nlargest ` operation you can use that is faster than a full sort . It will still take awhile to apply across 500,000 columns . #CODE
This should be faster than a temporary sort +1 , the problem here is that the ` apply ` is trying to return a df with the same shape as the original df which is not what can be achieved unless you take the raw values and return some other data structure like in your answer

` A ` here is a very large pandas ` DataFrame ` . Before running this ` while ` loop , the ipython session used 20% of machine memory . I thought that running this ` while ` loop would only cause memory consumption to drop because I am excluding some data from the ` DataFrame ` . However , what I observe is that memory usage keeps increasing and at some point machine memory is exhausted .
That's because you're assigning a new value to ` A ` , so memory is allocated for a new ` DataFrame ` while the old one is being filtered . The new one is almost the same size as the old one because you're selecting almost all of the data points . That eats up enough memory for two copies of ` A ` , and that's without taking account of extra memory for the bookkeeping that the ` loc ` implementation does .
Apparently ` loc ` causes pandas to allocate enough memory for an additional copy of the data . I'm not sure why this is . I presume it's a performance optimization of some sort . This means that you end up consuming , at peak memory usage , four times the size of the ` DataFrame ` . Once ` loc ` is done and the unallocated memory is freed which you can force by calling ` gc.collect() ` the memory load drops down to double the size of the ` DataFrame ` . On the next call to ` loc ` , everything is doubled and you're back up to a quadruple load . Garbage-collect again , and you're back down to double . This will continue as long as you like .
At the start of the first iteration , you'll see a memory usage of ` x ` percent . On the second iteration , after ` loc ` was called for the first time , memory usage doubles to ` 2x ` . Subsequently , you'll see it go up to ` 4x ` during each call to ` loc ` , then go down to ` 2x ` after garbage collection .
@USER Check out my answer as well . I can confirm by using this same gadget with ` top ` that if you use ` drop ` with ` inplace=True ` the memory is not doubling from copies on each pass . Inplace dropping seems more like idiomatic pandas to me than making a copy only to instantly garbage collect the now-defunct original .
The place where the original usage of ` loc ` ultimately bottoms out is in the ` _NDFrameIndexer ` method ` _getitem_iterable ` . ` _LocIndexer ` is a child class of ` _NDFrameIndexer ` and an instance of ` _LocIndexer ` gets created and populates the ` loc ` property of a ` DataFrame ` .
From the code : ` key ` will be your Boolean index ( i.e. the result of the expression ` ( A.coordinate = one_center - exclusion ) | ( A.coordinate = one_center + exclusion )`) and ` self.obj ` will be the parent ` DataFrame ` instance from which ` loc ` was invoked , so ` obj ` here is just ` A ` .
On any reasonable modern machine , using the ` drop ` approach should be problem-free for the size of data you describe , so it is not the case that ` A `' s size is to blame .
@USER I suspect something else is going on then , because when I memory profile this way ( with ` drop `) using the snippet that Michael Laszlo posted , I do not see memory growth . It is unfortunate the the pandas indexer code is so obtuse and hard to understand , because unexpected problems like this seem to pop up a lot .

Pandas : Make function map partial Dict match
How can I make it accept partial matches , and replace the matching word , while keeping the rest of the string intact ? Right now it only accepts literal matches .
Otherwise , a solution might be to first ` replace ` the matching word in the string keeping the rest of the string intact and then match the regex substring with the dictionary . These steps could happen in a temporary column so that the column `" Name "` is still in its original state for future use .

I understand how ` len ( dff ) == 8 `
Pandas will either set all rows to the same value if you pass a scalar , will align your array like object if they are the same length , or align along indices / columns if it's a Series / DataFrame this is expected behaviour , for instance you'd get an error if you tried ` dff [ ' counts '] = np.arange ( 7 )`
could you give a quick example of " align along indices / columns if its a Series / DataFrame " ?

How to apply a condition to a large number of columns in a pandas dataframe
as you've mentioned in your question you may need to drop rows that have a value in a certain range you can do this by the following

I need a single file with many columns (= number of files in the directory ) , from multiple file in the directory .. Each files has unique IDs which will not change for all files and so I need to merge these files based on that id .
the elements of ` files_merge ` are strings - file names . string types do not have a ` merge() ` method . you will need to write some code that reads from the file , creates e.g. a dict mapping the first column to the second , then merge the dicts

I would like to replace ' - ' to ' 0.00 ' using dataframe.replace() but it struggles because of the negative values , ' -1 ' , ' - 45.00 ' .
How can I ignore the negative values and replace only ' - ' to ' 0.00 ' ?
Or you can just use ` replace ` which will only match on exact matches : #CODE

I believe that I need a clever way to replace the ` \\ ` with ` \ ` but for well documented reasons , ` .replace ( ' \\ ' , ' \ ')` doesn't work .

To access the date / day / time component use the ` dt ` accessor : #CODE
This dt accessor is part of pandas right ? I dont have to import datetime module for using dt ?
It you're using a recent-ish version of pandas ( I think [ 0.15.0 ] ( #URL ) or newer ) , in the above code where you see ` dt.datetime ` this implies ` #import datetime as dt ` but if your series is already a datetime dtype then you can perform ` df [ ' col_name '] .dt .date `

use the transpose of the dateframe : DataFrame.T #CODE

You can set error parameter True which will drop the blank or malformed lines . nrows is not suitable as per my view because you have to manually add the row count for each file . #CODE

GOAL : Efficiently join Pandas DataFrames by comparing columns with a string matching algorithm
What I would like to do is join these DataFrames on ` name1 = name2 ` and ` address1 = address2 ` but not based on the exact string . Note , I have conveniently made the indices line up , however this is definitely not the case in reality !
What I would like to do now is apply a function that takes in two strings and produces a score of the similarity between them . For now , I am using the ` difflib ` library .
Based on my reading , the only way to join the strings if we are not looking for exact matches is to iterate over the DataFrame . I wrote the following code to accomplish this : #CODE

You can use ` groupby ` and ` apply ` scheme . #CODE

Generally your idea of trying to apply ` astype ` to each column is fine . #CODE
Note : While using ` pandas.DataFrame ` avoid using iteration using loop as this much slower than performing the same operation using ` apply ` .

I'm not sure that code would produce the answers you say it does . Your ` / ( len ( df ) *100 )` looks like it has the 100 in the wrong place ( you're dividing by an extra factor of 100 , not multiplying a fraction by 100 to get a percentage . )
You're correct DSM - I had the closing bracket in the wrong place , I have edited the code above , it should've been ( len ( df )) *100

And I want to replace ` Band 1 ` with ` LG68 ` and ` Band 10 ` with ` LG69 `
Maybe try using ` map ` function with a ` dict ` to describe the mapping relation . #CODE

Specify an experiment tag so that all the data frames that I insert for that specific experiment can be collected

You can ` groupby ` on ' id ' and then ` apply ` ` list ` to ` value ` column and then call ` reset_index ` : #CODE
I resolved this error by creating list of tuples ( i.e. apply ( tuples )) instead of list . But i am not able to explain why tuples works here & when to use tuples vs list . Please advise .

Pandas dataframe transpose with original row and column values
Does anyone know how to transpose pandas dataframe with original row and column values ? I am looping through each row and column using .iterrows() but I am sure there is a better way to do this . Maybe using pivot ? Thanks !
Need to keep row index too but yes melt seems to be a way . Thank you

Do you want to replace the original files or create new ?

Not exactly sure what you want the final output to look like , but if you don't mind having NAs you can try append your list of dictionaries as a dataframe to the original dataframe . There will be NAns , but this can be removed when you do analysis . #CODE
You can then convert this list to a dataframe of its own , and append it to the dataframe you already have . #CODE
I am not trying to create a dataframe , rather map the values in the list of dictionaries as new columns to specific rows where they are read from .

Can you show what you tried with ` merge ` , for instance it should work if you did ` merged = pd.merge ( df_sensor1 , df_sensor_2 , on= ' timestamp ')` and then repeat for ` df_seonsor3 ` , or if you set the index to timestamp on all the dfs then you could just do ` pd.concat ([ df_sensor_1 , df_seonsor2 , df_sensor3 ])`
Thank you for the quick answer ! I used ` merge ` exactly like you wrote , but that apparently does an inner join , so only data points that have timestamps in all tables are written to the merged table . I've tried an outer join as well , which does include all the data but also doesn't get the ordering right .
I did just try ` concat ` though . I did ` merged = pd.concat ([ df_sensor1 , df_sensor2 , df_sensor3 ] , axis=1 )` and ` merged.to_csv ( ' out.csv ' , sep= ' ; ' , header=True , index=True , na_rep= ' N / A ')` and that seems to have done the job . I'll have to verify it tomorrow .
Use a ` join ` to merge them with the `' outer '` method

You can create another dataframe called ` df1 ` with flipped columns , and then append it to ` df ` . #CODE
Use ` loc ` for creating a new frame : #CODE
Last , use ` pd.concat() ` to merge the two frames : #CODE

My goal is to create ` df2 ` where the first two columns of ` df2 ` are the same as those in ` df1 ` . I use the names of the columns and append the column names of ` col3 ` and ` col4 ` on ( is there a better way to do this ? ) and create the dataframe , ` df2 ` : #CODE

I have a large dataset where multiple columns had NaN values . I used ` python pandas ` to replace the missing values in few columns by mean and the rest by median . I got rid of all the NaN values and wrote the resultant the Dataframe to a new file .

For your reference the ` apply ` method automatically passes the data frame as the first argument .
Also , as you are always going to be reducing each group of data to a single observation you could also use the ` agg ` method ( aggregate ) . ` apply ` is more flexible in terms of the length of the sequences that can be returned whereas ` agg ` must reduce the data to a single value . #CODE

I found also this question related to a similar issues , but I can't figured out how to apply that method in my case .

pandas boxplot : swap box placement for comparison
To get desired output , use ` groupby ` explicitly out of ` boxplot ` , so that we iterate over all subgroups , and plot a ` boxplot ` for each . #CODE

probably because your question is [ off-topic ] ( #URL ): " Questions asking us to recommend or find a book , tool , software library , tutorial or other off-site resource are off-topic for Stack Overflow as they tend to attract opinionated answers and spam . "

If you need to specify more then one character you can combine ` itertools.takewhile ` which will drop lines starting with ` xxx ` : #CODE
@USER , you can still do it when reading from the csv , what is the ` xxx ` in ` startswith ( ' xxx ')`
You can apply the same logic , just keep the rows that have the first elements the start with " AN using a generator expression

The FuzzyWuzzy library has a convenience function that can be used to replace the inner loop by a single function call that extracts the best matches : #CODE

I think a combination of ` to_frame ` , ` unstack ` , and ` swaplevel ` can get you there . See below with some example data . #CODE

Presumably , there is a ` pandas ` string methods that I can drop into the above . But I'm having trouble coming up with a solution . I've been trying ` split() ` and ` extract() ` with a few different patterns , but can't get the desired effect .

that returns ` TypeError : ' bool ' object is not iterable ` . The only files in my input directory are the text files of interest too
To clarify , in your for loop , insert a statement such as ` continue if not os.path.isfile ( os.path.join ( pth , f ))` prior to calling ` df.read_csv ` . In the script you posted it looks like the output directory ` new ` is created in the input directory , is that not the case ?

What I'd like is to take some column names and merge them into one column , storing their data as a list per row , like so : #CODE
You could apply a lambda on each column group : #CODE

I've tried to use ` apply ` like this , but can't figure out the correct syntax : #CODE
Pass param ` axis=1 ` to ` apply ` to iterate row-wise : #CODE

Pandas : Drop leading rows with NaN threshold in dataframe
I'd like to drop leading rows with 6 NaNs or more . Specifically , in this case , I'd only like to drop row with Indices ' 1991-12-31 ' and ' 1992-01-31 ' .
The ' 1992-02-29 ' has 5 NaNs - what's different about it that lets you determine that you don't want to drop it ?
@USER , drop * leading * rows .
Assuming the column named ` Index ` is actually the index , you can count the number of null values in each row and select those that are greater than your threshold . If any are returned , drop the first one ( i.e. the leading row ) . #CODE
I need to drop ALL leading rows with > 5 NaNs , not just first one .
I believe the term ' leading rows ' is a bit misleading . Perhaps just ' all rows ' with 6 or more NaNs ? If so , the edit above should work where I simply changed idx [ 0 ] to idx in the ` drop ` statement .

There are some special characters from some spanish data source names that end up here when attempting to create a pivot table . I would like to be able to handle this in the to_sql call as opposed to having to strip the characters from the headers .

Insert new rows in pandas dataframe
Now , if , for example , multiwordexpr is equal to 1 , I want to duplicate the row and insert it in the database . So , I would like to go from this : #CODE
You can preallocate your df , the required length will be ` len ( df ) + df.multiwordexpr.sum() ` then you can use .ix [ ] to set the correct rows . You still have to iterate your original df and split it though . That might be faster . #CODE
You could iterate your original df and append to a new one while splitting the multiwordexpr rows . No idea if that will perform better though . #CODE
I am still experimenting , but seems that the best way is to append to a vector and the create a new df from a list of lists ( built by appending df.col.values ) .
Why not preallocate your df ( as you can calculate the needed size ) and write to it directly that should give you O ( 1 ) on the insert ?
But anyway I would need to shift the index , right ? So , I have a df of len x and I know that I need , let's say , x+10 . If I need to insert a line at position 2 , I need to shift the index from 2 on . Am I right ?
Yes , you just have to increment your index for each insert .

What does float ' object has no attribute ' replace ' when I try locale.atof in Pandas ?
I'm trying to convert a Pandas dataframe series to float . I do ` locale.setlocale ( locale.LC_NUMERIC , '')` and then ` df.idh.apply ( locale.atof )` , but it gives me the above mentioned error : ` AttributeError : ' float ' object has no attribute ' replace '` . I assume at some point it's getting something like a NaN , maybe or some other string and it does not recognize it . How do I tell ` apply ` to skip those ?

What I'd like is to take some column names and merge them into one column , storing their data as a list per row , like so : #CODE

Then you can add in your missing months with ` resample ( ' MS ')` ( MS stands for " month start " I guess ) , and use ` fillna ( 0 )` to convert null values to zero as in your requirement . #CODE

What is the most efficient way to replace NaNs in multiple columns based on a default value column ?
Instead I could pass a DataFrame to df.fillna() . The documentation says , that this DataFrame specifies the value for each column . Therefore , I have to transpose df , fill the na , and transpose back #CODE

Firstly the conversion to decimal is really ` float ` dtype due to the resampling as this will introduce ` NaN ` values for missing values , you can fix this using ` astype ` , you can then restore your ' timeline ' column which get lost as it can't figure out how to resample a ` str ` so we can apply ` strftime ` to the index : #CODE

You can ` groupby ` on ' col1 ' and then ` apply ` a lambda that joins the values : #CODE

Not clear what you want here . A stacked hist has to share x-values , and you've set your two data frames up so that they don't share any x-values , so there's nothing to stack . If you want to stack them by order , you could reset the indexes , merge them into one data frame , rename the columns , and use " plot ( kind= ' hist ' , stacked=True )
df.plot ( kind = hist )

I have tried a wide variety of merge and concat commands , but do not get the desired result . The strange thing is : what seems the most logical command , according to my understanding of the relevant documentation and the examples there ( i.e. use ` concat ` on the two df with ` axis=1 ` and ` join= " inner "`) , works on toy data but does not work on my real data .
Here is my toy data with the code I used to generate it and to do the merge : #CODE
This is exactly what I would like to accomplish with my real data . And although the two dataframes seem no different from the test data , I get an empty dataframe as the result of the merge .
My merge command : #CODE
I have tried many variants , like using ` merge ` instead or creating extra columns replicating the indexes and then merging on those with ` left_on ` and ` right_on ` , but then I either get the same result or I just get NaN in the " topicwords " column .
You can try casting ` someScores.index = someScores.index.astype ( np.int64 )` and then join
This works ! I get good values for " intersection " and the merged dataframe also looks good . Thank you so much !
Inner join only returns rows whose index is present in both dataframes .
Thanks for this idea ! We are getting closer , there is indeed something wrong here regarding the intersection . On the toy data , I get ` Int64Index ([ 1 , 3 ] , dtype= ' int64 ')` and on the real data ` Index ([ ] , dtype= ' object ')` . I'm wondering why that is so .
EdChum suggested recasting the index of ` someScores ` to get the same dtype . After that , intersection was fine and ` concat ` worked . Thank you !

but this will start the index from ` 0 ` . I want to start it from ` 1 ` . How do I do that without creating any extra columns and by keeping the index / reset_index functionality and options ? I do not want to create a new dataframe , so ` inplace=True ` should still apply .

How can I merge two pandas DataFrames based on a function instead of just where values are equal ?
I have two DataFrames that each have a column for firstname . I'd like to merge the columns on those strings , but on the Levenshtein distance as opposed to just where the strings are equal .
Is it possible to merge DataFrames based on functions like this ?

dataframe append new series column with data
I have a Panda DataFrame structure and I want to add another column to it , but I can't do it with append , add or insert .
` d_data = dict ( zip ( ls_keys , ldf_data ))` is what I want to replicate because it doesn't fetch the data that I want , but I need to figure out a way to append a new column to my dict . Here is my script : #CODE
Thanks , i saw your post too late . Maybe it works , but I made a workaround to actually first design the DataFrame , and then programatically populate the data in all columns and in this way I escaped the need to append the new column .

You can ` apply ` a lambda to your dates and call ` datetime.strftime ` : #CODE
Yes I updated my code sample to add line ` import datetime as dt `

correct . the data is pulled around a central point that happens for all pages at point / hour 24 . I am trying to to align all pages around that point . The full data set is 23 hours prior to and and 48 hours after a certain event takes place .

Sklearn and DictVectorizer This solves the previous issue , as we can make sure that we are applying the very same transformation to the test set . However , the outcome of the transformation is a numpy array instead of a pandas data-frame . If we want to recover the output as a pandas data-frame , we need to ( or at least this is the way I do it ): 1 ) pandas.DataFrame ( data =o utcome of DictVectorizer transformation , index=index of original pandas data frame , columns= DictVectorizer() .get_features_names ) and 2 ) join along the index the resulting data-frame with the original one containing the numerical columns . This works , but it is somewhat cumbersome .

( This is just a simplified example - in real life , my dataframes are larger and I will need to shift by more than one unit )

How to set a value in a pandas DataFrame by mixed iloc and loc
But I wonder if there is a way to use the magic of iloc and loc in one go , and skip the manual conversion .
I don't think you can do this as the indexing methods are specifically for either integer ( iloc ) or label based ( loc ) indexing , if you starting mixing both styles then you have to convert the integer <-> label to do what you want
I don't know what the question is , but here's a great answer regarding ` iloc / loc / ix ` , and is probably relevant to your question #URL Also , a small sample dataset would help here and make the question more concrete .

referring to this link : implementing R scale function in pandas in Python ? I used the function for def scale and want to apply for it , like this fashion : #CODE

Apply a func to generate a new colum based on value of other colums in Pandas
When I ' apply ' that function to both ' Time on Page ' ' Pageviews ' columns , wouldn't it take the value from both columns as the argument and return one value , which is ' AvgTimeOnPage ' , as the output ? I don't quite understand the error msg saying ' 1 ' arg is given , instead of ' 2 '
@USER Ah , I see , apply takes ONE argument ( which is a row / Series ) and you access the column as ` x [ ' Time on Page ']` inside the apply . Passing as ( x , y ) doesn't work - I suspect the second argument is used as some kind of flag .

You should use ` factorplot ` , or if you really want to use ` FacteGrid ` directly , you have to pass the ` hue ` variable in ` map ` .

Using apply on a column
I'd like to somehow apply a function to each column , converting it to a list and placing it in a new DataFrame . However , apply only operates on individual entries .
Managed to figure this out from what you gave me . I simply needed to apply a split function to each string in the dataframe ! :D Thank you !

The only other method I can think of is to write a for loop that'll go through a dictionary of the values and replace them . Is there a more elegant of resolving this ?

So you need to transpose the df ( if necessary ) in order for broadcasting to work , then the series needs to flattened to a 1-D array which in this case can be done by calling ` .values ` attribute
but there is no pandas method that allows you to return the minimum like ` np.miniumum ` , also ` np.minimum ` does not care about aligning columns and indices here . You'd have to define a func yourself and ` apply ` it , also you need to add all this information to your question and to pose representative code to show your desired output including a series with an index in a different order to your df

And then apply it to some function that returns a series like so : #CODE

Essentially this performs a reverse lookup , we iterate over the ingredients series using ` apply ` and then test for membership of this ingredient in the whole df using ` contains ` this will handle plurals in this case .

I have a dataframe with sporadic dates as the index , and columns = ' id ' and ' num ' . I would like to ` pd.groupby ` the ' id ' column , and apply the reindex to each group in the dataframe .
Which returns error : ` AttributeError : Cannot access callable attribute ' reindex ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method `
A slightly different flavor is to write the return part of the function as ` return df.resample ( ' D ') .fillna ( method= ' ffill ') ` . This has the added advantage of changing the ' D ' in the resample part to ' B ' if you only want business days for example ( of course this depends on what you want ) .
Are you sure that works ` Pilik ` because I initially tried ` resample ` but it didn't add in the missing days ? You can achieve different periods like business days in ` pd.date_range ` as well , e.g. ` pd.date_range ( .., offset= ' B ')` for business days .
@USER , you're right , I didn't know ` pd.date_range ` had an offset option . I copied the example using ` pd.read_clipboard() ` and my solution with ` resample ` also yields the desired result .

Merge two tables based on multiple keys in Python pandas
I would kindly ask if in Python Pandas ( or other modules ) has any functions to support merge ( or join ) two tables based on multiple keys .
I don't know if it is really achievable in Python ( or even R ) , because based on my reference to Pandas documentation and past examples in stack overflow nearly all examples only relate to join or merge on one key . I would appreciate if you can show me some ideas or examples to solve it .
To merge by multiple keys , you just need to pass the keys in a list to ` pd.merge ` : #CODE
In fact , the default for ` pd.merge ` is to use the intersection of the two DataFrames ' column labels , so ` pd.merge ( a , b )` would work equally well in this case .
@USER - glad it helped ! If this answers the question to your satisfaction , please consider marking it as an accepted answer . ( Or if you still have questions about ` merge ` , let me know and I'll do my best to answer them . )

How do I append multiple CSV files using Pandas data structures in Python
I have about 10 CSV files that I'd like to append into one file . My thought was to assign the file names to numbered data_files , and then append them in a while loop , but I'm having trouble updating the file to the next numbered date_file in my loop . I keep getting errors related to " data_file does not exist " and " cannot concatenate ' str ' and ' int ' objects " . I'm not even sure if this is a realistic approach to my problem . Any help would be appreciated . #CODE

This appears to be successful . Now I want to apply functions based on metadata criteria : #CODE

To join the data back to the original dataframe as a new column I will need to add call time and a column called employee_id . I'm connecting these rows back to another dataframe which has the customer id
Edited my original comment - I would like to be able to merge the results of my grouped total counts to another dataframe which has 1 record for each customer ID . The total counts should be the number of times the customer ID showed up in the raw data before the timestamp on the original df

You can use apply , like this . #CODE

Well , the CSV does exist after you write to it once , right ? You just want to append instead of over-writing . Specifically , I was looking at the answer with the most votes ( it wasn't the accepted answer though ) using the ` mode= ' a '` option . I have not tested it , just thought it looked like a good approach .
First , put this line ` records.append ( { " URL " : row [ ' URL '] , " Name " :[ ' Name '] , " ID " :[ ' ID '] } )` out of the for loop . You shouldn't append the header for every element of brands . Second , please show a couple of examples of brands ( e.g. what does ` print brands [: 2 ]` looks like ? ) . Also make sure ` brands ` does have elements of shape ( 3 , 0 )
Essentially , I needed to layer the second for loop under the first and create the ' records ' dictionary before the looping began . This causes an append to the dictionary for every line in ' source ' . Seems like a pretty simple concept now !

How can I merge A , B and C correctly with ` numpy.hstack ` , ` scipy.sparse.hstack ` or ` FeatureUnion ` ? . Do you guys think this is a correct pipeline-approach to follow for any machine learning task ?
Using ` FeatureUnion ` , you simply need to append your new transformer to the transformer list : #CODE
Thanks for the help . One thing to comes to my mind is what about the dimesions ?. How can I merge the matrices taking care about the dimensions ?.

I need to apply this calculation to every cell .
But then I am struggling to find a way to apply this function to each cell in the dataframe . I tried ` iterrows ` but it was very slow as the actual dataset is very large .
Or alternatively , if you need to convert from your original dataframe into a numpy array , then you can replace the last line with : #CODE

if you want to drop the original column you can do this #CODE

I need to ` identify ` them , and then do ` interpolate ` from nearby data to replace them . I'm wondering if there is any existing ` algorithm ` for this ?
One way to detect corrupted data or outliers is to first calculate the rolling median ( it's robust to outlier ) of a series , and then compute the distance between the actual observation and rolling median . Filter out those observations with distance larger than threshold . #CODE
If you have trends in your serie , you may rather apply it on time moving window instead of the whole serie .
So about applying custom functions on a window , I usually use ` scipy.ndimage.filters.generic_filter ` wich work also with 1d arrays and return a scalar applying a function on a moving window defined by a footprint . Here is an example about how interpolate mean value for NaN only in 1x3 footprint : #CODE
You can also merge the two functions toghether but for quality analysis , I don't and always keep raw data , valid data , and interpolated data .

I was using " pivot " but that doesn't work as the indices are not unique . So I created another group but pivot seems only allow to have one index as option . #CODE

dtype : bool ` is correct only index row 4 is ` True ` because all rows are equal , so what's wrong this ?
This problem is caused by the fact that ` np.nan == np.nan ` is ` False ` and ` np.nan ! = np.nan ` is ` True ` . A quick workaround would be to replace any ` nan ` in ` df ` and ` df2 ` with something that you know is not in your dataframes eg ` foo ` : #CODE

Pandas - understanding output of pivot table
if I replace my dataframe with #CODE
The output of the same pivot is going to be #CODE

Hello , When I save the plot the legends become stacked on one another though I chose different loc for them . However MATPLOTLIB plot shows the correct locations .

In this case I have some code in the LOCAL file which is not present in the remote . I didn't add that code , so I'm assuming the code has been removed by a merge upstream , while I was working on my local branch . I would therefore opt for removing the code from my LOCAL , in order to have LOCAL , BASE and REMOTE aligned .
To me REMOTE is the upstream version , LOCAL the local file I'm working on and BASE a partial merge git was able to carry over , but requires some manual intervention to complete .

` resample ` is a standard way
As @USER mentioned , ` resample ` is the tool you need here . You can pass ` how= ' ohlc '` to ` resample ` to get the desired output . #CODE

` Panel ` is a bit less developed compared to other ` pandas ` structures , so as far as I'm aware , named axes can't be used for much . That ` transpose ` use-case seems reasonable , may be worth making an issue .

Now we want to replace missing values with ` group ` -wise median values . In ` R ` we can do it using a nested ` ifelse ` call . #CODE
why do you need to do nested if-else statements ? for example , in r this is generally bad practice , and you can simply do ` with ( test , ave ( value , group , FUN = function ( x ) { x [ is.na ( x )] <- median ( x , na.rm = TRUE ); x} ))` which will work for n groups
In this case , you can use a ` groupby ` to fill by the group median : #CODE

Pandas can't join DataFrames after set_index with exception ` IndexError : index 2 is out of bounds for axis 0 with size 2 `
When I run the code below the join operation throws ` IndexError : index 2 is out of bounds for axis 0 with size 2 ` . If I change the index's name before I write the file it works , but in the context where I'm trying to use this the file writing operation is hard to change .

So for your example , you could repeat your sample data ` len ( df ) / 24 ` times and set it into your original dataframe using ` iloc ` , ( i've had to select ` iloc [: -1 , :] ` since your index actually contains one extra time point on the last day ): #CODE

Python pandas merge or concat dataframes
The data is for 2 products ( BBG.XAMS.UL.S_pnl_pos_cost and BBG.XAMS.UNA.S_pnl_pos_cost ) by date , in the future there will be more products . I want to concat or merge ( not sure which ) the list of dataframes into one data frame ( called result ) so they look like : #CODE
where axis is the date . It looks like the data is merged by date , but I am missing the data for the week beginning 2015-03-23 . My current concat result dataframe looks like : #CODE
Try using axis=0 . This should concat column-wise , assuming each dataframe has the same column names .
possible duplicate of [ Pandas join / merge / concat two dataframes ] ( #URL )

@USER you mean to apply the function as follows ? This produced me an error . ` results= ingredients.apply ( lambda x : where ( df [ 0 ] .str .lower() .str .contains ( x.lower() ) , True ))`

You can create a dictionnary , and then map your dataframe with it #CODE
You can try to avoid the ` temp [ i ]` accesses by iterating the temp values instead . You can also append the new values at the end of another list ( fast ) instead of modifying a value in the middle ( not that fast ) . #CODE

Stack Overflow discourages product recommendations . So , I'll do it in a comment . Your problem is quite amenable to databases , and you'll probably find more uses when you start using one . You have a bona fide CSV format , so I would put Postgres near the top of the list.od

Python Data Frame resample on micro second
I am working with resampling data frame and it works on hours , days mins , but doesn't resample less then sec . Program just hangs even on short time span . So am I missing something ?
you can see varable sf represent resampling freq . #CODE
How can I resample data frame on micro seconds granularity ?
If I understand your problem correctly you can't resample microseconds to another frequency that is less than a second , right ? I made a toy example and there doesn't seem to be a problem though : #CODE
( I think your problem is that you are trying to resample data that are in a microsecond format to microseconds itself , which doesn't make any sense . You want to either upsample or downsample ( as in my example ) . )

Now apply a ffill lambda as follows : #CODE

More elegant than the apply function is to use ` result [ xstring ] = tst.bla.str.contains ( xstring )`

Transpose multiple variables in rows to columns depending on a groupby using pandas
This is referred to a questions answered before using SAS . SAS - transpose multiple variables in rows to columns
I don't know what do you think . But I prefer the ` SAS ` ` PROC TRANSPOSE ` syntax for better readability . ` Pandas ` syntax is concise but less readable in this particular case .

You will notice that some of the groups have 3 , 6 or 9 rows as counted by the hours . What I would like to do is slice each group down to 3 hours max and append something to the 6 and 9 length groups to denote that it is the same page like the following : #CODE
So , I truncated my data set in the question to make it easier to read and thinking that whatever solution came would also apply .. In actuality , the groupings are 34 rows long , and replacing the ` 3 ` with ` 34 ` does not seem to work - any thoughts ?

Ideally , for the pages that have multiple groups of 34 , i'd like to add a suffix of _1 , _2 , _3 , etc . so I can groupby for all specific instances of a page . I would need it like this because i need to pivot the data .
You can use ` np.arange ( len ( df ) // 34 )` to do the trick . #CODE
I added an edit - is there any way you can fit this into your answer ? ` Ideally , for the pages that have multiple groups of 34 , i'd like to add a suffix of _1 , _2 , _3 , etc . so I can groupby for all specific instances of a page . I would need it like this because i need to pivot the data . `

I also can't reproduce your output . By the way , it's not a good idea to name your variable ` map ` , since this shadows the built in ` map() ` function
I tried to run through the code and still had the same results . Is there a more efficient way to do calculations and insert them into a dataframe with predefined column names ?
The variable map is not actually something I use , I just changed the variable names from my actual code ! Thanks for the advice though .

My column header objects have methods to return their properties , etc . For example , instead of a column header string ' x5y3z8 ' where I would have to parse out the values , I can simply do col_header.x ( gives 5 ) col_header.y ( gives 3 ) etc . This is very object-oriented and pythonic , instead of using a string to store info and having to parse it every time to retrieve info . How do you suggest I replace my current column header objects in a nice way ( that's also supported ) ?

` color = [ ' b '] * ( len ( df ) - 1 ) + [ ' r ']`
` ax.scatter ( df.ix [ len ( df ) -1 , ' carry '] , df.ix [ len ( df ) -1 , ' vol '] , color = ' r ')`

Using Pandas , I have a Series of 100 equal length Numpy arrays each with 30000 elements . I'd like to quickly transpose them into a series of 30000 arrays with 100 elements .
That is , an element-wise transpose of a Series of arrays into a new Series of arrays . Thanks !

Use ` max ` and check for equality using ` eq ` and cast the boolean df to int using ` astype ` , this will convert ` True ` and ` False ` to ` 1 ` and ` 0 ` : #CODE
Thanks @USER . Did you try my original post ? I would be interested to know how much time this one is taking compared to yours ? ` for i in range ( len ( df )): ... df.loc [ i ] [ df.loc [ i ] .idxmax ( axis=1 )] = 1 ... df.loc [ i ] [ df.loc [ i ] ! = 1 ] = 0 `

Pandas really slow join
I am trying to merge 2 dataframes , where I want to use the most recent date's row . Note that the date is not sorted , so it is not possible to use ` groupby.first() ` or ` groupby.last() ` . #CODE

use ix instead #CODE

If you want to change the display format then you need to parse as a datetime and then ` apply ` ` datetime.strftime : #CODE

Pandas drop index type columns
I want to drop multiple columns from ` pandas.DataFrame ` and can't do it . #CODE

I've tried many many things using both ` pivot ` function or using both ` set_index ` with ` unstack `
use ` .pivot_table ` followed by ` concat ` : #CODE

Dataframe groupby or stack / pivot
I think something like stack or pivot is the answer but I can't get it working .

I have tried append and concat but it just gives me all the columns and all the rows stacked on top of each other .
Just do a default ` merge ` this will perform an inner join on common columns : #CODE

` apply ` a lambda to convert to timedelta and then subtract : #CODE

That now leaves me with $23 to spend on extra presents that birthday ; the same rules as above apply on any additional presents .
And this is the output . Using the median on the 3 numbers is a nice way to work out what granny will contribute .

Python pandas : how to join a Series to a DataFrame ?
Is there any way to join a Series to a DataFrame directly ? The join would be on a field of the dataframe and on the index of the series . The only way I found was to convert the series to a dataframe first , as in the code below .
I want to add the column in the Series to the dataframe , but how do I specify whether it should be a left outer join or an inner join , and how do I specify which column of the dataframe the index of the series should match ? Thanks
For example inner / outer join . #CODE
Does this join on the indices ? In my case , the indices are different , and I want to specify the fields to join on , but concat doesn't seem to have on , left_on , right_on parameters like pd.merge does
Yes , it joins on indexes . Solves only this part >> I want to add the column in the Series to the dataframe , but how do I specify whether it should be a left outer join or an inner join
mmm , so is the only way to convert from series to dataframe with reset_index ? This converts the index to a column , which gets added to the result of the merge function . I'd therefore need to remove it . It all seems needlessly convoluted !

print ( ' Stock : ' , col , ' max diff : ' , sl.max() - sl.min() )`

apply ` df.str.contains() ` to ` s2 ` using the contents of ` s1 ` as the matching pattern
The best I could come up with is to use ` apply ` instead of manual iterations : #CODE
sorry to break this to you but ` apply ` is essentially a ` for ` loop , the code just looks cleaner

I've looked into melt and unstack and I don't think those are correct ...
Is there a simpler , dare I say ' elegant ' , way to map the dataframe rows based on the multi index value directly into a reusable data structure ( right now the dictionary stop_dict ) .

Radar Chart from Pandas Pivot Table
I'm looking for a way to generate a radar chart from a pandas pivot table that looks like this . I have seen the code from #URL , but they seem to be using a totally different format for the data . Is there any direct way this can be done with the data as I have it ? #CODE

Is this simply a table join on item with some filtering , sorting and more filtering ?
Next step was to replace the ` df3.name ` column with a forward-filled groupby series : #CODE

Depending on if your indexers are positions ( ` iloc `) or labels ( ` loc `) .

I have a pandas dataframe with multiple rows . Before I can produce plots I must perform filtering on the time column . Typically the value for time will increase at a 1Hz rate however there will be cases when the value for time will go backward . I need to drop any rows that have those " invalid " values for time .
With pandas use the ` diff ` method to find negative rows ( i.e. rows where the next time value is less ) , which are then filtered out : #CODE
If the dtype of the column is datetime ` diff ` should still work , IMO this is the correct answer

My understanding in pandas is to use a pd.rolling_sum() function but i'm not quite sure how to groupby and apply it while setting a condition . I've also tried using cumcount() to no avail #CODE
Ultimately one of the new columns would look like this . I'm not sure where I would specify ' Right ' in the function above . When I did a pd.rolling_sum I used a shift , but again I couldn't find where to groupby with this . #CODE

Replace data frame values matching given condition
But since we're working with a series of bools anyway , the ` == True ` part doesn't add anything , and we can drop it : #CODE

See comment @USER In short , can I " universalize " this to make it apply to every car ? Thanks .

You could ` groupby ` the ' ID ' column and perform an aggregation : ` f.groupby ( ' ID ' , as_index=False ) [ ' ID '] .agg ([ np.count_nonzero , np.unique , np.max ])` , not sure how to obtain the ` freq ` calculation though

You can apply the recipe from here #CODE

That loop works fine , but I trying to see how I could use map and apply to make this look cleaner . From reading , apply is a loop function underneath , so I'm not sure whether I will get any speed from doing this . But , it could shrink the code by a lot .
Do you know of a more general method ? This works fine with basic addition , but I was trying to find a way to apply a more general function . I've updated the question to show a more complicated example .

You should use ` loc ` to ensure you're working on a view , on your example the following will work and not raise a warning : #CODE

Worked almost exactly as expected . I do have a bit of a shift on my axes though . My argument for 0 forms a small bar at the bottom , where every other integer forms a line going up . I'll post the result in my question .
I do want to say , though , that the scale on the bottom should be reading the dates . From my original dataset , the scatter point should be at the intersection of the column and the index , with the point darkened according to the degree in the data .

Pivot Pandas DataFrame

I cannot understand the error I see when using apply or transform :
1 2 ) ` transform ` expects something " like-indexed " , while ` apply ` is flexible . The two failing functions are adding additional columns .
4 ) The first two functions take a ` DataFrame ` with two parameters and returns data . ` InnerFoo ` actually returns another function , so it needs to be called before being passed into ` apply ` .

But suppose I have many ( hundreds ) of such columns , whose names are in a list ` dummies ` . I'm certain there's some cool pythonic way of doing this with one magical line that will do it all . I've tried using ` map ` with ` dummies ` , but haven't yet been able to figure it out .

I'm currently learning how to use Pandas , and I'm in a situation where I'm attempting to replace missing data ( Horsepower feature ) using a best-fit line generated from linear regression with the Displacement column . What I'm doing is iterating through only the parts of the dataframe that are marked as NaN in the Horsepower column and replacing the data by feeding in the value of Displacement in that same row into the best-fit algorithm . My code looks like this : #CODE
I've looked through the docs , and through other answers on Stack Overflow . All solutions to this seem to use ` .loc ` , but I just can't seem to figure out the correct syntax to get the subset of NaN rows using ` .loc ` Any help is appreciated . If it helps , the dataframe looks like this : #CODE
and then use ` update ` to replace the NaN values : #CODE
Ed Chum's solution shows how to replace the value in arbitrary rows by using a boolean mask and ` auto_data.loc ` .

OK , after looking at what you want it'll be quicker to take the first rows and then ` shift ` the rest of the df after performing a ` groupby ` and then ` concat ` : #CODE
Yes , you're right you need to pass ` -1 ` to ` shift ` so it shifts the other direction

Try ` X_NAN = X [( X > 1000 ) .all() ]` I think your issue is that some cols will have some ` NaN ` and when you call ` dropna ` it will drop any column even if it has a single ` NaN ` value

Hi Tom , it doesn't look like this works . It outputs just one array and is equivalent to df2 [ ' array '] .sum() . But you have given me an idea with apply . Let me see if I can figure something out .

As you can see in comments , you have to join ` rootDir ` , ` dirName ` and ` fname ` . #CODE
Join one or more path components intelligently . The return value is the concatenation of path and any members of ` *paths ` with exactly one directory separator ( ` os.sep `) following each non-empty part except the last , meaning that the result will only end in a separator if the last part is empty . If a component is an absolute path , all previous components are thrown away and joining continues from the absolute path component .
you can create empty df and append one column of data from csv's - #URL but you have to change for adding column ` code ` : ` df = pd.DataFrame() ` and ` df = df.append ( g [ ' code '] .tolist() , ignore_index=True )`

Apply string.format() to row in Pandas DataFrame
can be used to apply the format string using the column data .

ValueError : In safezip , len ( args [ 0 ])= 1 but len ( args [ 1 ])= 21

I need to access the values associated to different points in the cumulative distributions ( percentiles ) . For example I could be interested in the percentiles ` [ .1 ,. 9 ]` I'm finding the location of these percentiles in the DataFrame with the associated values by checking where in the first DataFrame I should insert the percentiles . This gives me a 2-d numpy array where each column has the location of the row for that column .

` df [[ ' pC ' , ' Truth ']] .plot ( kind= ' hist ' , stacked=True )`
If that's what you want , I guess you could use something like ` melt ` to also add to the dataframe those complementary probabilities ` ( 1-pC ) for Truth == 1 ` for each bin . Then plot as a stacked bar-chart ( not as a histogram ) .

Use a ` for loop ` to index each line and use another ` for loop ` nesting it to read the data in each column . use ` lstrip ( '"')` and ` rstrip ( '"')` function to strip off the quotes . then read . It'll work .

Two levels will be loc and S . #CODE

I have 4 different ` df.hist ( columns= , by =) ` that I would like to insert inside of a GridSpec ( 2 , 2 ) .

Let ` df ` be the first dataframe . I would use the pivot method as : #CODE
and then apply the method over it .
The function you're looking for is ` pivot ` . Make sure you read the tutorial here :

I search output [ 3 ] for the string ' station ' and then append the results where this is true to an empty list , results . As per - #CODE
I have tried to create a new dataframe with appropriate index names . Then filtering as above and attempting to append these results to the new dataframe #CODE
You can create a boolean mask by calling ` apply ` on ' type ' column to create your new df : #CODE

Apply a function to translate a column in pandas dataframe with condition on other columns
data is a pandas dataframe in which language and config [ ' TEXT FIELD '] are columns . I want to translate certain reviews in the text column to english and I am using a function dfApply #CODE
Why not just convert the whole column to English , then use a mask of non-english rows to replace only the ones you need to ? That is a bit easier than using apply with your conditionals happening in each step .

After performing the conversion you can use the datetime accessor ` dt ` to access just the ` hour ` or ` time ` component : #CODE

The above function ( an indentation error occurs when copying into Stackoverflow - please simply ignore ) pulls a custom object ( event ) out of a Queue object - if the event is a tick event , it is meant to append the ask variable and the corresponding timestamp ( as an index value ) to a pandas Series object ( see last nested if statement in function ) .

When I try to accsses the data using loc [ ] or lookup() the memory usage increases dramatically to ~16GB , which causes the thread to be very slow due to memory swapping .

Drop NaN cells and move non-null values according to Datetime index
Is it possible to drop ` NaN ` values and leave only 4 values per one time step ? To have something like this : #CODE

and that the function inside ` apply ` should make use of the ` isin ` method .
Another solution might be to merge the two datasets but I think this is not trivial since there is no unique identifier in the ` DataFrame ` .
probably but calling ` apply ` will also be very slow as this is just a ` for ` loop

Then apply your method : #CODE

The rhs will align to the index on the lhs , as we use ` loc ` here it ensures that the correct rows will be assigned

I'm struggling to correctly merge a few datasets in pandas . Let's say I've measured variables A , B , and C , at different times . Sometimes , I've got A and B at the same time , and sometimes not . I have three dataframes , where the dataframe's index is the time of measurement , and a column for the measurement . If I concatenate these dataframes , I get a bunch of NaNs where I have no measurements , maybe something like #CODE
I'm still trying to get my head around the various join / merge / concat operations and how they work , but I'd love a pointer or two .
create a dataframe with index ' timestamp ' and columns ' var_name ' , ' val ' for each of A , B , C , and D . from there , concat , sort , and unstack .
Assuming that your index is a Timestamp , try to ` resample ` it at your desired frequency ( e.g. hourly , daily , weekly , etc ) . You can take the mean measurement in case there are multiple samples observed during the window . #CODE

As suggested by another stackoverflow user , I am using sklearn preprocessing package to normalize columns of a pandas dataframe . I create this dataframe from a file which looks like this : #CODE
It is supposed to create a dataframe from a file , normalize the dataframe and save the results to a new file . But out of no where this error showed up and I do not know how to get rid of it : #CODE

I have a lot of Excel files ( 50+ ) and they each have 15+ sheets within . The sheets each have custom names . I'm trying to find a way to merge all the workbooks and sheets into a single sheet .
I think I can combine single-sheet Excel files ( with glob and append . ) , but when I try it with multi-sheet files the data gets messed up . #CODE

I'm trying to extract the rows that contain the date " 2015-09-01 " . I thought about applying a map , but it works on the row as a whole if I understood it correctly .

I've converted the last step to no longer be a loop and instead save directly to a list . AFAIK , you would have to separate the two parts and append as lists since the columns of interest are different and converting to a dictionary would include the ` NaN ` s otherwise . There may be a way , but I don't know .

apply conditional if loop over groups
Replace ` ` with ` and ` for a logical ` and ` .

One would expect then that `' Campaign X ' in mydf [ ' Campaign ']` would return ` True ` . But it doesn't . And I find I need to append ` get_values() ` to make this membership test behave as I'd like . Why so ?

Replace string in pandas.DataFrame
So I want to replace all the `" . "` with `" - "` in my DataFrame .
@USER - I got this error : ' float ' object has no attribute ' replace ' .
I think it apply the function to the index instead of the value .

You can directly create the DataFrame from the dictionary , but when creating that , the ` keys ` would become the column and ` 0 ` / ` 1 ` , etc would become the indices , if you want it the other way round - ` keys ` as indices and ` 0 ` / ` 1 ` , etc as columns - you can then take its transpose .
One way you can accomplish this is to skip creating a pandas series altogether and just go right to a DataFrame , then transpose the dataframe : #CODE
Read the dictionary into the DataFrame , then transpose it with ` .T ` , or with ` transpose() ` : #CODE

Looping CSV Concat in Python Pandas
I have multiple folders each containing csvs . I am trying to concat the csvs in each subdirectory and then export it . At the end I would have same number of outputs as the folders . At the end I would like to have Folder1.csv , Folder2.csv , ... Folder99.csv etc . This is what #CODE
I tested a treewalk on my pc with basically the same code you have minus the panda stuff ( never worked with panda ) and it worked flawlessly for me . Maybe your performance problem is caused by the panda code you have in your loop . Maybe try to gather the csv records first and concat them after the loops are done . I don't know panda but it seems like ` frame = pd.concat ( _list )` runs over your collection of csv records over and over again as it gets bigger and bigger . Try to do that after your loop .

I've read through and tried all ` merge ` combinations .

and then align this with my df #CODE
and then just count the NAN . This doesn't work . Align error ` unsupported type ` .
OK , I'd ` reindex ` using your time_series , then ` groupby ` on your index and then apply ` isnull ` and call ` sum ` : #CODE

You can use concat ([ list of dict ]) or merge ([ list of dict ]) to get the desired result .
Pandas ` concat ` and ` merge ` functions are not built for working on dicts , they are built for DataFrame and Series etc .

Pandas boxplot covers / overlays matplotlib plot
Following what reported here I tried to create a boxplot with an overlayed scatter plot .

You can perform an inner ` merge ` : #CODE
You can then drop the ' postcode ' column or rename it first : #CODE
that did the job . I still dont understand why they would call this " merge " , should it not be called " converge " ?
You're merging on columns / index , you can specify the criteria for merging on lhs and rhs , if there are column names that match then it will match these and the default type of merge is inner so only values that are present on both sides

You can just replace ` 1 ` with arbitrary value and then call ` idxmax ` and ` reset_index ` as before : #CODE
If you want to add the values then you can call ` apply ` and use the ` new_df ` values to perform a lookup from ` cos ` df : #CODE

I want to plot a heat map of these data with COLS on the horizontal axis and FUNCS on the vertical axis with cells that are scaled according to FLUFF . I don't want to use seaborn . I want to use matplotlib and / or pandas exclusively .
As cel mentioned in the comments , if your data is actually sparse , you might want to do a ` .reindex ` to insert all the rows and columns , filling the NaNs appropriately .

There is also a convenient way to extract information about ` DateTime ` . This can be done by using ` dt ` . For example it is possible to get the year and month numbers simply like this ` lcd.issue_date.dt.year ` ` lcd.issue_date.dt.month ` .

Do I have to transpose the array ( it seems there are many functions for rows in pandas and numpy but relatively few for columns ( I could be wrong , of course ) to get average calculations for a column done ?
As an extension of my question : what is the best way to print Average value of Flavor_Score for all unique beers in the list instead of one chosen beer ? I have created a list of unique beer names using pd.unique ( df.beer_name.ravel() ) as an array and then transferred array in the list with Beer_Name_L= Beer_Name_Arr.tolist() ( not sure if I needed to do that :)) . Now I tried to print using for in in xrange ( len ( Beer_Name_L )): beername=Beer_Name_L ( i ) print df [ df.beer_name == beername ] .Flavor_Score .mean() but got an error message " TypeError : ' list ' object is not callable

and i want to merge them into 1 dataframe . The relation between file ` a ` and ` b ` is ` booking_id ` = ` #URL The relation between file ` b ` and ` c ` is ` #URL #URL = ` Advertentie-ID , Dag `
But the thing is that the ( index ) columns needs some altering before i can use them to merge . But i can't find any index callback functionality so i can alter these columns during import .
( preferably during reading the csv data ) so i can merge the files .
I'd suggest you just read the csv as is , then split / parse the source ( mix values ) into individual fields which can then be used to merge with other dataframes .

There is a large csv file imported . Below is the output , where ` Flavor_Score ` and ` Overall_Score ` are results of applying ` df.groupby ( ' beer_name ') .mean() ` across a multitude of testers . I would like to add a column Std Deviation for each : ` Flavor_Score ` and ` Overall_Score ` to the right of the mean column . The function is clear but how to add a column for display ? Of course , I can generate an array and append it ( right ? ) but it would seem to be a cumbersome way . #CODE
You could do something like yourDataFrame [ ' Flavor_Score_stddev '] = yourDataFrame [ ' Flavor_Score '] .someFunction() to map values of ' Flavor_Score ' to new values using someFunction() and put the new values in a new column named ' Flavor_Score_stddev ' .

I reproduced the issue but could not fix it and it messed up my clipboard so was unable to cut and paste in any app without closing IPython . Perhaps you could just write the df to a file , cvs or excel .

This generates the plot but the Legend B is placed at the upper right hand side corner and Legend A is by default placed at the left hand upper corner . Also I could see that Legend B is cut and not showing all the characters .

` groupby ` ` [ ' Group ' , ' Subgroup ' , ' Normalized ']` , then ` rank ` the ` Max CPC ` s . Next , I want to map the ` Max CPC ` associated to the ` CPC Rank ` to the ` Type Rank ` which is determined based on ` Criterion Type ` and my own custom rank :
This is by far the most efficient solution to this problem . The hard part was realizing that I could ` sort ` ` df ` by the ` Type Rank ` so the ` Criterion Type ` rows were ordered by their rank . This meant I wanted the highest ` Max CPC ` to apply to the first , the second highest ` Max CPC ` to the second , and so on .
` mapping ` contains an index with non-unique values . When defining ` New CPC ` , ` map ( mapping )` seems to cause a ` pandas.core.index.InvalidIndexError : Reindexing only valid with uniquely valued Index objects ` error . Your function works in my example ` df ` but I get ` InvalidIndexError ` on my larger dataset . Also , when I define ` mapping ` not in a function , then do ` df [ ' New CPC '] = df [ ' Type Rank '] .map ( mapping )` , it also raises the error . Does this type of mapping only work in a function ? Either way , I'll need to study this . Thanks for the concept .
This means you have duplicate values of Type rank for each group , is this right ? If so how should you map your values in this case ?

normalize pandas data into one to many relation
You can use ` str.split ` , followed by a ` apply ( pd.Series ) .stack() ` ( the ` apply ( pd.Series )` makes different columns of the elements , ` stack ` is for turning this to rows ): #CODE
Note , with recent pandas , you can replace the ` .apply ( pd.Series )` with ` expand=True ` in str.split : ` df [ ' sports '] .str .split ( ' , ' , expand=True ) .stack() `
Is it possible to merge the columns like index and current_tm for respective country ?
Yes , certainly . You can use merge : ` pd.merge ( res , df , on= ' country ')` ( assuming ` res ` is the result of above , and ` df ` has still the country column )
join method worked as well # res.join ( df )

You could first do a groupby on DF2 , counting one of the columns , and then merge the resulting DataFrame with DF1 , something like this : #CODE
I have tried to apply your answers but I do not get good results as you can see below : I just have the same value 1970 - 01-01 instead of having a column with the same value stored in datetime column . Arrival column is empty instead of having the count of arrivals as needed ( from df1 ) #CODE

Or using @USER approach of multiply , this will do the same . You can also concat them into a single multi-index dataframe : #CODE

Which then allows me to resample the count further , e.g. : #CODE

Trying to merge 3 different dataframes that each have a different number of rows and used these commands #CODE
Note the ` how= ' outer '` , which does a full outer join . More on this in the documentation .

df [ ' unq '] = numpy.arange ( 1 , len ( df ))

I'm not looking to concatenate strings , just shift everything over .
I saw a method using " R " and melt , however I would like to stick with python / pandas if possible .
Excel : Select all , Go To Special ..., Blanks , Delete ..., Shift cells left .

@USER has a pretty good answer . Thinking outside the box , you could groupby school and set indexes on the date columns one at a time . Then you can use the rolling counts because it will be sorted by date . That will be much faster than using the apply method and checking len for each row . Check out cumcount #URL

Assuming that those are the columns in your dataframe ( and none are the index ) , then you want to group by date , time , and id on price . You then unstack the id , which effectively creates a pivot table with dates and times as the rows and ids as the columns . You then need to use ` pct_change ` to achieve your objective . #CODE
Then , just group on the the timestamp and id , and unstack the id . #CODE

Pandas dataframe apply function to entire column
` .apply() ` is the method to apply a function to a ` Series ` on a row-by-row basis . Other than that you haven't given much information to work with .
apply a function that return a list

I know how to split a string , but I could not find a way to apply it to a series , or a Data Frame column .
For all but the last names you can apply `" " .join ( .. )` to all but the last element ( ` [: -1 ]`) of each row : #CODE

Conditional join result count in a large dataframe

I have 8760 hours worth of data and have added a datetime index against it . What I want to do is replace all the values that are on Saturdays with the values from the previous Friday . #CODE
Yes if you have regular intervals then you just need to ` shift ` by the interval amount
You can use ` loc ` and a mask to select the rows you wish to modify and assign the values ` shift ` ed by your regular interval : #CODE

I'm only splitting you data into halves rather than deciles here due to the small example dataset , but everything should work the same if you just increase the number of bins in the initial cut : #CODE

I am wondering if join and merge could be valid use cases for this specific context . I welcome feedback on this .
You definitely want to do an outer join on both frames ?
Don't merge on any columns with null values , you will get an inexact match which will grow your merged df exponentially , I'd either drop them or assign some dummy value
@USER Thanks for bringing that to the fore . From what I understand I think the requirement is to have the union of all of the tables - not solely the intersection from the shared " keys " ( in this case the headers ) . I think that I am over thinking the problem as the source dataframe yields 6 " child " dataframes that have - in some instances these datframes have the same header . Hence , the rationale for this . I am also editing the OP to provide more clarity on the data and the composition of the headers . This will be reflected in the Gist that I originally shared .
This produced the correct ouput . It turns out that I was overthinking the problem . In this context , the row index is not meaningful . The ` ignore_index = False ` parameter allows one to not preserve indexes along the concatenation axis . This is useful as I was not seeking to find the intersection of the data sets ( which , in theory should not be apparent in the data structure that I am wrangling ) .

You need to ` unstack ` your dataframe : #CODE

Without knowing the format of your csv files this question is hard to answer . Yes , you can probably use much less RAM than the 3.8gb text file - No you cannot use the same strategies as you would apply to a file on your disk . On the disk you only have to store the information , in memory you often have to keep this information in a form which is easy to manipulate .

Passing additional arguments to python pandas DataFrame apply
I have a DataFrame df and I tried to iterate each row to map values of two columns to new values , but I have problems passing in the dictionary containing the map to df.apply #CODE
The ` args ` argument given to apply function is passed ` func ` argument ( lambda function given ) . You are getting this error since two arguments are given but lambda function only accepts one argument .

and want to plot a boxplot where the x-axis is ' Av_Temp ' divided into equi-sized bins ( say 2 in this case ) , and the Y-axis shows the corresponding range of values for Tot_Precip . I have the foll . code ( thanks to Find pandas quartiles based on another column ) , however , when I plot the boxplots , they are getting plotted one on top of another . Any suggestions ? #CODE
Since you're using pandas already , why not use the boxplot method on dataframes ? #CODE
If you want a standard boxplot , the above should be fine . My understanding of the separation of boxplot into boxplot_stats and bxp is to allow you to modify or replace the stats generated and fed to the plotting routine . See #URL for some details .
If you need to draw a boxplot with non-standard stats , you can use boxplot_stats on 2D numpy arrays , so you only need to call it once . No loops required . #CODE

Insert a row in a multiindex data
I have a multi-index dataframe ( multindex on the rows ) and I want to insert a row at a certain row number . This question has been aksed before ( How to insert a row in a Pandas multiindex dataframe ? ) but I have a bit different problem I want to solve
Since there is no insert_row function in pandas I thought to insert a row by first copying my data frame up to a certain row to a new frame using a slice , then append the row I want to insert and then append the rest of the original data frame . In fact , this approach work . Here is an example
Both answers are correct . In fact , in my real situation it was a bit more complicated . In the example , all the Side labels are order A-B , but in reality this can change and also I want to have the freedom to impose a different order . If you take that into account , the first anwer with the unstack / stack does not work . After the unstack , I can not impose a different order A / B or B / A anymore . Therefore , I must use the option with dropna .
Now , the unstack method does not allow to insert a row with for instance B first and then A , because after unstack / stack , the order is always A / B . This is not what I want .
I think better is work with ` Index ` instead ` Multiindex ` . So first I unstack df , create new dfs and then concat them . Last I stack df back . #CODE
Thanks for your answer . It works in case the label A / B do not alter in order , but I have modified my question a bit because in my real code I need to be able to change A / B into B / A . I don't think you can still use teh unstack method in that case
If you just want to insert a new set of data at a certain position try the following . With drop you recieve a new object , there will be no KeyError issue anymore . #CODE

Having browsed other threads relating to appending to a pandas dataframe , row by row I have read up on concat and append and setting with enlargement but still have this funny problem where is I create an empty DF with a ` MultiIndex ` and then append to it , the ` MultIndex ` appears to be flattened to a normal ` Index ` . To go back to a ` MultiIndex ` , at the end of all my row appends I must reset the index . Is this meant to work this way or am I going about this in a rather odd fashion ?
So , now I append another row . Now , here , maybe I am doing this in a round about fashion ?? The reason I ask is that Pandas appears to collapse the ` MultiIndex ` into an ` Index ` ... #CODE
Should I have to do this or am I doing this the wrong way ? I've been googling but can't find a solution and don't really see why append isn't happy .

Pandas dataframe Cartesian join
Result ( expected result size = len ( first ) * len ( second )): #CODE
Create a surrogate key to do a cartesian join between them ... #CODE

I've searching for something like this kind operation ` df [ len ( df )= =19 ]` , is it possible ?
Did you try ` df [ df.apply ( lambda x : len ( x [ 1 ]))= =19 ]` ? this seems to do what you want
You could take advantage of the vectorized string operations available under ` .str ` , instead of using ` apply ` : #CODE

I'm trying to join two series with ` pd.concat ([ a , b ] , axis=1 )` , but the result is a dataframe filled with ` NaN ` s , here's what I mean :
so here's two series that won't concat well together : #CODE
I've serialized the head of above series , uploaded to evernote , contains code to load and concat them
@USER they were calculated from a groupby object , grouped by ` [ ' dt ' , ' product_id ']` , does it have anything to do with this situation ?
Also , it appears that your index has changed somewhere . Your ` by_status.groupby ([ ' dt ' , ' product_id '])` operation should result in a MultiIndex , but the results of ` a4.head() ` and ` a5.head() ` pasted above indicate that it changed to tuple pairs somewhere along the line . I suspect that this could could be the ultimate problem .
I do not understand why ` concat ` is not working , but I managed to achieve your objective using ` merge ` .
First , reset your indexes . Then merge the DataFrames on ` dt ` and ` product_id ` : #CODE
even if it's the wrongly formatted multiindex , the concat operation shouldn't result in a all-NaN frame , right ? it should at least contain something , where did all the numbers go ?
Why don't you open a new ticket " Possible Bug : Pandas concat with MultiIndex " . Mention the version of Pandas and Python you are using , and include the sample data .

I am reading a bunch of daily files and using glob to concatenate them all together into separate dataframes.I eventually join them together and basically create a single large file which I use to connect to a dashboard . I am not too familiar with Python but I used pandas and sklearn often .

Two ways to join lists in dataframe : as rows and columns
Now I join them so that each list becomes a column of a data frame : #CODE
If , however , I join lists as rows of a data frame , then Python saves ` 66 ` : #CODE
Or you can use ` map ` with ` None ` . ( but map changed in Python 3.x , so that only works in Python 2.x ): #CODE
You can create a Series for each of your lists and then concatenate them to create your data frame . Here , I use a dictionary comprehension to create the series . ` concat ` requires an NDFrame object , so I first create a DataFrame from each of the Series . #CODE

` pd.read_table ( filename , usecols =[ 0 , 8 , 9 , 11 ] , parse_dates =[ 1 , 2 ] , dtype={ ' LopNr ' : np.uint32 , ' INDATUMA ' : np.uint32 , ' UTDATUMA ' : np.uint32 , ' DIAGNOS ' : np.object } )` , assuming the dtype would apply to the data before it enters the converter , hiccups on a string in some of the rows : ` ValueError : invalid literal for long() with base 10 : ' string '`

Given that all the dataframes have the same columns , you can simply ` concat ` them : #CODE

I'd like to know how I can replace the NANS of my DataFrame by the mean of the others at the same hour and day of the week .

pandas resample MAX-VALUE with corresponding ANGLE-VALUE
I have to resample wind_velocity and wind_angle from the 2 sec period to 2min and receive the maximum wind_velocity (= MAX ) with the corresponding wind_angle ( not MAX ) . The lines below give me the maximum of both columns . #CODE

And one last addition , and this one is likely not trivial : is there a way to have the program detect the same file origin and replace it with the new one ? So if you edited ` dbtest2.xlsx ` and added / subtracted items , the program would remove the old ones and only input this new file ?
Yes , you can specify one or more columns in a list for duplicates . Here ` Item ` is used . Duplicates will keep first instance and drop all instances thereafter . Hence I concatenate ( i.e. , stack ) new on top of old . So always use older dataset as master file and new as updated file . Test it out !

Now I wish to concatenate the two dataframes . When I try to use ` concat ` it flattens the multiindex as follows : #CODE
If instead I append a single column to the multiindex dataframe ` df2 ` as follows : #CODE
How can I instead generate this dataframe from ` df1 ` and ` df2 ` with ` concat ` , ` merge ` , or ` join ` ?

This can be done with ` groupby ` and using ` apply ` to run a simple function on each group : #CODE

and if you want to apply your custom function you can use apply where it takes your custom function as a parameter , and it passes each group to your custom function #CODE

By I am incorrect hist .
This is primitive code , works only on number of hours . I refering to the error here . I can't understand the error . ` len ( hour_list )` seems to work fine .
Thanks ! But please check edit I solved that . I am having a diff . issue though .

Just to compare against using ` apply ` : #CODE

If you had not called ` apply ` on the ` groupby ` object then you could access the ` groups ` : #CODE

I'd construct a ` dict ` of how you want to ` map ` the values and call ` map ` on the column , example : #CODE

` object ` will be the displayed dtype here irrespective of whether you have missing values or not , this means python object for ints , floats , datetimes and bool will be displayed as dtype , object for all others this is correct behaviour

Including the group name in the apply function pandas python
Is there away to specify the groupby call to use the group name in the apply lambda function .
is there away to get the group name in the apply function , such as : #CODE
How can I get the group name as an argument for the apply lambda function ?

I tried to use datetime.replace() function to replace the day . But somehow it doesn't work .
Here how to shift the index to the end of each month . You can choose different frequency by changing the ` freq ` parameter ( see Offset Aliases ) . #CODE

and I would like to replace all the NaN in columns stating with ' dummy ' with previous values ( and only these columns while the rest of the dataframe remain unchanged )

@USER Hey , it doesnt work . the clip function doesnt return me the results of - 0.9999 or 0.9999 . Is my question 1 done correctly ?
I can't say as I don't have your data , you may need to investigate that line as ` clip ` does the correct thing

This is difficult to do in place as pandas will expand the structure after the ` apply `
the dtype probably falls through to ` object ` as it's not categorised as int , float , datetime or bool so if it's behaving correctly then this is just a display artifact

You can use the vectorised ` str ` methods to replace the unwanted characters and then cast the type to int : #CODE

@USER Starting with the whys : this is probably the result of me modifying some code from a groupby.apply function that i found here and used previously . The particulars will always be of the general format : ' if year = x ' or ' if year = x and month = y ' where year and month are columns in the DF . The calculation will be less of a formula and more like small shifts : replace values from 100 columns where year = x and month = y with values from those identical columns but a different year and month .
If you want to apply values from other parts of the df you could put those in a dict and then pass that into your apply function . #CODE

I want to build a dataframe with datetimestamp ( upto minutes ) as index and keep adding columns as I get data for each new column . For example , for Col-A , I aggregate by day , hour and minute from another dataset to a value ' k ' . I want to insert this value ' k ' into a dataframe at the ' right ' row-index . The problem I am facing is the current row-identifier is from a groupby object on date , hour , min . Not sure how to ' concatenate ' these 3 into a nice timeseries type .
can always append more rows later )

do you want to keep the df2 or can we append to that df ?
I would first do an outer merge on the dataframes . I am not sure whether ` DF1 ` refers to the column name or the dataframe varaiable name in your posting , but for simplicity I assume you have two dataframes which have columns with strings : #CODE
Next , make a new dataframe that merges these two ( use an outer merge ) . This takes care of the duplicates #CODE

Using this map of NYC I'd like to change Manhattan to be bright blue . But when I change the individual patch color of Manhattan all the other patch colors change too . This was unexpected to me .

This can be efficiently done by converting a single tz at a time ( but since we have many , groupby already separates these out ) . These are local times ( IOW in the given timezone ) , so ` tz_localize ` makes these tz-aware . Then when we combine them these are auto-magically converted to UTC .

If I'd like to unpack and stack the values in the ' nearest_neighbors " column so that each value would be a row within each ' opponent ' index , how would I best go about this ? Are there pandas methods that are meant for operations like this ? I'm just not aware .
Then you can use ` pd.merge ` to join this back to your original dataframe as @USER suggested in the comment to your original question .

Complex Groupby Pandas Operation to Replace For Loops and If Statements

Pandas dataframes - join on similar timestamps
I want to join them in such a way that every row in ` small_df ` is joined to a row in ` large_df ` that comes immediately after it , so that the desired result looks something like #CODE
Also , assume that these 2 dataframes may have other columns that I would like to maintain in the final result . How do I achieve this ? I know I need some kind of merge , but not exactly sure .
Also , I get a lot of duplicates like this . I think the correct solution involves some sort of filling ( bfill of ffill ) combined with merge , but I am not sure how to do it .

But I recognise that my expressions contain sub-expressions that consist of only scalar values . Do the documented rules apply to sub-expressions as well then ? The code does run , but the result isn't correct .

e.g either have the Python program insert #CODE

Expand and merge Pandas dataframes
I have two dataframes I would like to merge .
You want to do an outer merge and set ` left_index=True , right_index=True ` : #CODE
outer ` join ` would work also : #CODE

The problem with ` apply ` is that you need to return mulitple rows and it expects only one . A possible solution : #CODE
Thanks for the advice hellpanderrr . I think what I needed to know is that it's not possible to reassign different dimensions in an apply function . I also needed a way to generically assign the remaining columns to the new groups . In the end I came up with the technique shown in my answer . Cheers

Below is a trivial example , obviously , but consider any scenario where each row maps to multiple rows . What's the best way to handle that ? In reality , each row can map to a different number of results . This is basically computing a one-to-many relationship .
The ` groupby ` version of ` apply ` supports ` DataFrame ` as return value in the way which you intended : #CODE

Just call ` apply ` and call ` tuple ` : #CODE

You could create three copies of your initial dataframe , then drop two columns in each , i.e. df_1 would have cols =[ SAMPLE_ID , ROW COL , LAB_L ] , df_2 would have cols =[ SAMPLE_ID , ROW COL , LAB_A ] , etc . You could then rename the LAB_* column to Value and then add a Field column with the respective lab name . Finally you can merge these three data frames together .
Simply use the ` melt ` function : #CODE

Panda's merge several csv's with one common column
I have 13 csv files to merge . I wanted to try pandas and python but I am struggling .
You should do an outer merge as follows , making use of the built-in reduce method : #CODE

How do I apply a regex substitution in a string column of a data frame ?

You should assign another DataFrame to hold the index and value of such , and apply to the original DataFrame base on the groupby field ( as index ) .
Then apply and perform the lookup : #CODE

I added a simpler version using numpy array , then just add it to the df before you append .
Try this . You do not need a list . Just append to the original data frame .

If the timestamp column is your index , you can use ` resample ` , for example , to count the number of observations over six hour intervals : #CODE
One way is to resample a pandas DatetimeIndex #CODE

I am wondering if it is possible to use this function to , if in the case of an empty dataframe , return just that empty dataframe's headers and append it to the concatenated dataframe . The output would be a single row for the headers ( and , in the case of a repeating column name , just a single instance of the header ( as in the case of the concatenation function ) . I have two sample data sources , one and two non-empty data sets . Here is an empty dataframe .
For each of the new data frames I then apply this logic : #CODE

A hack is to set the dtype to object before doing the apply : #CODE

Now I want to find the percentage change between the median values of the 2 quarters so I do this : #CODE
You basically want to do a pivot table . An easy way to do this is by setting your index to quarter and status and then unstacking status : #CODE

This solution worked if i apply group by column only on Sex field . Howver another requirement says the below format : Date Sex weight hight Salary

` Out [ 1 ]` shows that the ` data.to_sql() ` function was executed . This function which will drop any existing tables with the same name ( `' data '` in this case ) and recreate a new table named `' data '` with contents corresponding to the ` DataFrame ` ` data ` . However , during this process , if I encounter an error , I want to go back to the state before ` data.to_sql() ` executed , which is where I start my transaction .

At the moment you are searching for a quarter that equals " completed_quarter " rather than the value of the ` completed_quarter ` variable . Using string format method will replace the value in braces with the variable value .

How can I interpolate based on index values when using a pandas MultiIndex ?
I have demographic panel data , where each data point is categorized by a country , sex , year , and age . For a given country , sex , and year , my age-pattern has missing data , and I want to interpolate it based on the value of the age . For example , if 5 year-olds have a value of 5 , and 10 year-olds have a value of 10 , 6.3 year-olds should have a value of 6.3 . I cannot use the default pandas ' linear ' interpolation method because my age groups are not spaced linearly . My data look something like this : #CODE
Surely there must be some way to interpolate based on the index values .
This answer isn't great , because it will only interpolate a single column at a time , while panda's DataFrame interpolate multiple columns all at once , something I'd like to do .

f = open ( path , mode , errors= ' replace ')

How to apply cubic spline interpolation over long Pandas Series ?
I need to replace missing data within pandas Series using cubic spline interpolation . I figured out that I could use the ` pandas.Series.interpolate ( method= ' cubic ')` method , which looks like this : #CODE

pandas - boxplot median color settings issues
I'm running Pandas 0.16.2 and Matplotlib 1.4.3 . I have this issue coloring the median of the boxplot generated by the following code : #CODE
Looking at the code for ` DataFrame.boxplot() ` there is some special code to handle the colors of the different elements that supersedes the ` kws ` passed to matplotlib's ` boxplot ` . In theory , there seem to be a way to pass a ` color= ` argument containing a dictionary with keys being `' boxes ' , ' whiskers ' , ' medians ' , ' caps '` but I can't seem to get it to work when calling ` boxplot() ` directly .
see Pandas Boxplot Examples
Thanks , but for my purposes in plotting a multindexed dataframe I need the boxplot function ( with ' by ' column ) instead of the kind= ' box ' version .
Actually the following workaround works well , returning a ` dict ` from the boxplot command : #CODE

Partly because there's as of yet no convenient cartesian join ( whistles and looks away ) , I tend to drop down to numpy level and use broadcasting when I need to do things like this . IOW , because we can do things like this #CODE

If I understood well your question , I would normalize your data multiplying each value by the current maximum and then divided by the sum of all elements . So that : #CODE

Please note I changed it from ` pd.append() ` to ` df.append() ` , there is no append function directly in pandas module , there is only ` pandas.DataFrame.append() ` ( or ` series.append() ` , but I guess you maybe wanting ` df.append ` for your case ) .

How to join 2 pandas time series
This is not really a ' join ' problem but a ' re-indexing ' problem . pandas supports this and can do this in one line of code . See below , #CODE

Replace NaN in DataFrame index
How do I replace the NaN in the index with a string , say " No label " ?

I loop through each column and perform a ` value_counts ` on each . I then get the index values for each items that occurs at or below the target threshold values . Finally , I use ` .loc ` to locate these elements values in the column and then replace them with ` None ` . #CODE

Does this mean that I can not replace the column with a calculated value ? Then how can I create a new DF where the only difference is a calculated column ?
So you need to do something that that says , " this is the part of the DataFrame I want to assign to . " To use your code again , one way to do this would be to index exactly the part of the DataFrame you want to replace on both sides . E.g. #CODE
1 ) Does this mean that I can not replace the column with a calculated volume ?
You can replace a column with a calculated volume . Try referencing the column you are trying to replace with new values as :
Trust me I tried the whole day . trying different versions , closing and rerunning the script step by step . Nothing worked . Sorry - it is value and not a volume . My bad ! This was driving me crazy :) Just to add : it is difficult to reference with " new values " as you suggest since this a large dataset . Thus it has to be a loc type referencing column's name or something like this .

you also need to align the bars to the

This approach is very slow as it involves iteration and invoking the apply method for each group .
Yes , I do not show that I merge the result of calculate_duration with the result of the aggregation in the code above . I will update the question .

I append the some error message here :

Next , dilate the mask to join together islands of ` True ` s ( rainy days ) separated by 5 or fewer ` False ` s ( non-rainy days ): #CODE

So does icecream demand map to particular ranges in temperature in your data ? If so , can't you just bin your temperatures accordingly and then plot your bar chart ?

@USER - Replace ` big_data ` with the name of your worksheet , it should work . You first load the workbook , then the worksheet , then you can iterate over the rows and cells there .
Yes this works without the ` read_only ` arg . I have a concern though : Documentation says there is no ` read_only ` arg , but a ` use_iterators ( bool )` arg , which is for lazy iteration . When I simply set it to ` True ` , I am having a strange error . I can live with that but this ambiguities with documentation makes me think twice about using this library , sadly . I will be testing this library further to see if it allows me to achieve the performance increase I'm looking for . Then I will add my openpyxl iterator script here and mark your answer as accepted if everything's good . Thank you .

@USER this would work if there are no missing values ( hours , days ) . ` resample ` creates empty rows that I wouldn't like to have . I know I can drop them but I'm looking for a solution that will take date from multiple columns .
Someone else on SO had a similar question , but their solution was to use resample . You can avoid resampling by mapping the tuples in the multi-index to create a new index . This will handle missing rows just fine . #CODE

use [ ` isnull `] ( #URL )
If you're comparing scalars , one way is to use ` assertTrue ` with ` isnull ` . For example , in the DataFrame unit tests ( ` pandas / tests / test_frame.py `) you can find tests such as this : #CODE
Before doing an assert_frame_equal check , you could use the .fillna() method on the dataframes to replace the null values with something else that won't otherwise appear in your values . You may also want to read these examples on how to use the .fillna() method .
@USER : perhaps I've misunderstood exactly what you're trying to do , but ` assert_frame_equal ` already asserts that ` NaN ` is equal to ` NaN ` . Using ` fillna() ` to replace ` NaN ` with some other scalar to be compared for equality is redundant and so isn't used in Pandas ' unit tests .

Re transform vs apply : generally speaking you use transform to keep the same number of rows when the function would otherwise reduce the number of rows . It's not clear to me why it is needed here as expanding_mean should not be reducing anyway ...

drop the temporary key column

You need to apply your function to a data frame , not a series #CODE

So basically , I want to combine two files . I want ALL ROWS to stay in tact . I do not EVER want to combine merge rows . This is important , because sometimes there is repeat data that I need to keep .

@USER ... You are correct . But somehow append or concat is not working
Basically I append all the dataframes created from your csv files into a list .
I then use the pd.concat function to merge all the rows between each element of the list of dataframes dfs .

You could append row ` i ` ( in your case , ` i == 3 `) ` j ` times ( ` j == 8 `) with #CODE
While you can append to the DataFrame , it's a relatively inefficient operation , as each step takes a copy . ` reindex ` provides an easy way to align the data to a new index , then you can forward fill the the values with ` fillna ` method . #CODE

Preserving Column Order - Python Pandas and Column Concat
In Pandas for Python I have 2 datasets , I want to merge them . This works fine using .concat . The issue is , .concat reorders my columns . From a data retrieval point of view , this is trivial . From a " I just want to open the file and quickly see the most important column " point of view , this is annoying . #CODE

Getting an error : NotImplementedError : operator ' / ' not implemented for bool dtypes
Thank you ... I assumed bool would evaluate to 0 or 1 . Clearly it does not .

I would apply your operations to a copy of the ` DataFrame ` and stack back together - something like this : #CODE

You could use ` apply ` to generate the values for each range , then ` melt ` to reshape the data into long form . #CODE

convert my list has unicode string which I ' m not be able replace in python
have strip multiple operation but not able to do it . Please help

@USER Yeah man , I know that . But It's stack overflow rules . Don't mind

How do I remove / strip the row number from a variable in Python
So , how do I either strip or remove \nXXXXX from datex and timex ? Or otherwise match the format ?

I've got 2 pandas dataframes , each of them has an index with dtype ` object ` , and in both of them I can see the value ` 533 ` . However , when I join them the result is empty , as one of them is the number ` 533 ` and the other is a string `" 533 "` .

@USER : thanks for the information . combining row selection with concat is good and I will keep it in my toolchest
To move the third row to the first , you can create an index moving the target row to the first element . I use a conditional list comprehension to join by lists .

I want to resample a pandas dataframe from hourly to annual / daily frequency with the ` how=mean ` method . However , of course some hourly data are missing during the year .
Would be nice if this was integrated in the resample function itself ....

I would like to avoid the ` merge ` because leads me to uncorrect behaviours .

Insert zero to missing data in pandas.DataFrame
You can see records for April and May are missing , and I'd like to insert sales as zero for those missing records : #CODE

Edit : I just noticed that specifying ` right=False ` makes the lowest interval shift to 0 rather than - 0.4 . It seems to take precedence over ` include_lowest ` , as changing the latter does not have any visible effect in combination with ` right=False ` . The following intervals are still specified with one decimal point .

In that case I believe you can use transpose on the original edit ( edited )
A transpose will not help here.Please look at the updated question.Pardon the naive skill .

Do you have a reasonable bound on the key values , e.g. 1 .. n ? Since then this would seem to reduce to some basic linear algebra , which knowing pandas / numpy might be the fastest way to do this . You could have a len ( df1 [ ' s ']) x n matrix to represent the sets in df1 [ ' s '] and then an n-length vector to represent df2 .

And btw , please replace ` index=None ` with ` index_col=None ` in the code above ( in the comments ) to correctly read the CSV file . I couldn't edit that comment .

Apply two operations on the sub ` DataFrame ` obtained by the ` groupby ` ( one for each year )

To get the same result as a pivot table , you can also perform a ` groupby ` operation and then unstack one of the columns : #CODE

Using new_df I would like resample the master_df searching for and binning genes that are similar . I'm looking for genes whose values are within 5% of my interesting gene's values in each the four columns . When I find a match , it gets binned under the correct key in my new_df . Is there a pandas way to accomplish this with hierarchical indexing ? Or perhaps this a task for a new dictionary ?
Alexander , this was extraordinarily helpful ! It may take me some time to evaluate and apply . I will report back . I am proactively calling this question answered . Please let me know if there are other methods for up voting and giving positive feedback !

Next you can simply map : #CODE

Cannot merge CSV file into one dataframe
I am using ipython notebook and struggling to merge 208 CSV files into one dataframe . ( My files names are Customer_1.csv , Customer_2.csv ,,, and Customer_208.csv )

All I want to do is store and access a really long one dimensional list of 16 bit integers . I don't even necessarily need to store an index , as it can be implicitly derived from the order of the samples . But I need a fast append operation .

Put that in your scripts for both your figures and that should align them both nicely .
Well , I tried , but this does not solve the problem . I'm not using plt.subplots to plot those subfigures . I have to generate those figures first and align them using Latex .

You could do it this way , call ` np.min ` on the df as a np array , use this to create a boolean mask and drop the columns that don't have at least a single non ` NaN ` value : #CODE

Wouldn't it make sense to merge the values when creating the columns rather than merge them all into a single df and have to sort it afterwards ?

I created a list with the value I want to use to replace the current index by doing : #CODE
then I tried to replace the old index doing : #CODE

I have tried with resample and groupby but These just seem to work with the index .
Not sure I quite understand what you are trying to do . But you could use ` set_index ( ' Timediff ')` if your only problem is the need to use resample on a non-index column . Groupby can be used on non-index columns .

I am new to python and pandas . How can we apply a groupby and an aggregate on multiple columns ignoring the blank / None / NaN values ?
I am trying to apply a groupby and count agregation function on these values as :

Install pandas from a different source ( using ` pip `) . To do this , create a new environment like above ( make sure to pick a different name to avoid clashes here ) but replace point 4 by ` ( pandas_env ) user @USER : ~$ pip install pandas ` .

a ) in column C , replace the ` np.NaN with 999 ` ,
Those operations return a copy of the data ( most of the pandas ops behave the same ) , they don't operate in place unless you explicitly say so ( the default is ` inplace=False `) , see ` fillna ` and ` replace ` : #CODE

See the following Pandas-XlsxWriter stacked charts with colors example . The example uses brew colors but you can replace those with any Html like RGB color .

But when I try to do the same on the resulting pivot table : #CODE
A describe() of the pivot table gives this : #CODE
The error looks like ' ProductCategory ' is not a column , it looks like you set it as the index , can you show what your pivot table looks like
` ProductCategory ` , ` Currency ` , and ` Producer ` are now part of the index after the ` groupby ` operation . Try to reset the index of your DataFrame named ` pivot.reset_index ( inplace=True )` and then use ` loc ` to make selections as usual . #CODE
Alternatively , you can just use ` loc ` as follows on the original ` pivot ` : #CODE

Thanks , modified the join to be for Type 1

I tried this ( it gives the same error ): validation [ ' intercept '] = pd.Series ([ 0 for x in range ( len ( validation.index ))] , index= validation.index )
validnew [ ' intercept '] = pd.Series ([ 0 for x in range ( len ( validation.index ))] , index= validation.index )

The first argument gives the locations to put the ticks ( ` df.index ` here is equivalent to ` np.arange ( len ( df ))` , but if your index isn't evenly spaced use the arange ) . The second argument gives the tick labels .

It looks like you want to replace a column with the max value in that column , grouped by the values in another column . You should be able to use ` groupby() ` and ` transform ( max )` to get what you want : #CODE

Because your are updating the same set of values in that dataframe at the same time , you can try to use a temp variable to hold the result and apply back to the column , see if this helps ridding of the warning message .

Pandas resample function not working on DateTimeIndex
Then when I try to resample the index into weeks like so
My type check earlier says I have the proper index , but the resample function doesn't thinks so .
axis=1 means it's trying to resample the columns ( which isn't a DatetimeIndex ) . #CODE
Note : this is the default for resample : #CODE

However , I have this data sliced by minute per date . I have files going back a number of days , so I call a certain number of them and concat them into my DataFrame , typically using the last 20 days .
Here is the feedback I get from plugging that in : AttributeError : Cannot access callable attribute ' unstack ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method
You want to pivot your data so that dates are down one axis and time the other .

I don't sure about iloc in this case , but if you want more strictness - you can always use selection by label with loc attribute . You can use it even after you will change ordering of rows , or after adding new rows . Loc selects rows by index ( not integer-location like iloc ) which in default case is just index of row in numpy matrix . #CODE

Are the ' + ' signs necessary ? Why not just strip them out ? There is no error on export after all only in the software you are using to read the output .

I see you can do two conditions on one line , but I don't see how to apply it .
Hi and welcome to Stack Overflow . Its hard for us to tell what is going wrong here , and what you'd expect . Perhaps you could give us a bit more information , specifically : what is the output that you would expect from your code ? What are you getting instead ? Have you tried each of the conditions on their own to make sure that they are working by themselves before combining ? Did they achieve the results you'd expect ? Show us the results ? Note : please edit your question and add this information there - don't put it in the comments as code formatting is awful .
The second line is perfect , but how do you implement the first stage , where location id can be specified for the group by to apply to the subset ?

Approach is to create a DataFrame for each row with color counts , transpose and concatenate . There may be a better method that avoids the overhead with creating a pandas objects for every row . #CODE

and the column ` Col3 ( time )` will be parsed as dates . To perform selection using the time information alone , you can make use of the ` dt ` accessor available to datetime-like columns : #CODE

You can make use of the ` dt ` accessor for datetime-like Series : #CODE

Using Python , I am struggling to merge 208 CSV files into one dataframe . ( My files names are Customer_1.csv , Customer_2.csv ,,, and Customer_208.csv )
Your code works on a small sample of five files that I used for testing ( each file containing two columns and three row ) . ONLY FOR DEBUGGING , try to write this in a for loop . First , before the loop , read all of the files into the list . Then loop again and append each one using a ` try / except ` block to catch the errors . Finally , print the problem files and investigate . #CODE

I am working on Pandas and would like to mimic the result as the PivotTable in Excel 2013 . I use pd.pandas to get the result with sum of values , but I cannot figure out how to get the ' % Diff ' , for example , how can I get the percent value comparing data of Jan . with Feb .

I've tried various permutations of stacking a multi-indexed DataFrame , grouping , pivoting -- I can't seem to figure out how to take the value from the " Car " column and transpose it to a new column with the value " True " , merging people together by , say , their name .

Use the ` map ` method of Series to look up in the dictionary : #CODE
Thanks @USER ! You don't happen to also know how to store that to ` df [ ' real ']` without getting that error ? Do I need to create a separate series and then use ` concat ` to add it to the dataframe , or is there a way to do it all with one line ? ` df [ ' real '] = df [ ' nominal '] * df [ ' year '] .map ( factors )` still errors . Much appreciated .

You want to filter the df first , if you pass a list of the tickers so ` [ ' LT ' , ' HDFC ' , ' ACC ']` instead then you can do ` d [( d [ ' Ticker '] .isin ( ticker )) & ( d [ ' Date '] > start_date ) & ( d [ ' Date '] < end_date )]` and then call pivot

then pivoting the edited data frame , check this pivot if you want to get more information about pivoting #CODE

You need to replace ` 24 ` with ` 0 ` in order for this to parse : #CODE

As you can see , it returns a boolean series matching the original index . Why ? Because Pandas needs to map which index values have an equivalent true value , so it can select the proper outcome . So basically , during your slice opperations , you will always carry this index , because it is a mapping element for the object .

Discretization of continuous attributes using np.histogram - how to apply on a new data point ?
After I " learned " my bins from train data , using ` np.histogram ( A [ ' my_var '])` how do I apply it on my test set ? as in which bin is the my_var attribute of each data point ? Both my train and test data are in pandas data frames , if it matters .

Pandas : How to apply a function to different columns
I want to apply the function to just columns ` B ` and ` D ` . ( Applying it to the full DataFrame isn't the answer as that produces NaN values in the numeric columns ) .
But I cannot fathom how to select distinct columns to apply the function to . I've tried all manner of indexing by numeric position , name , etc .
How to apply a function to two columns of Pandas dataframe
Pandas : How to use apply function to multiple columns
Pandas : apply different functions to different columns
Python Pandas : Using apply to apply 1 function to multiple columns
Apply isn't inplace , it returns a new dataframe , so the question is can you return the complete dataframe in one go .
You can replace ` x.str.lower() ` with ` function ( x )` .

None of them seem to apply to this problem , and all of them say that in order to generate unique values from every column , you should either use a groupby function , or select individual columns . I have a very large number of columns ( over 20 ) , so it doesn't really make sense to group them together just by writing out df.unique [ ' col1 ' , ' col2 ' ... ' col20 ']
I have tried .unique() , .value_counts() , and .count , but I can't figure out how to apply any of those to work across multiple columns , rather than a groupby function or anything that was suggested in the above links .
I have simplified my answer to suppress an unnecessary call to ` transpose ` . Maybe it caused the problem . Try the new version and tell me .

Just call ` apply ` and pass ` pd.Series.value_counts ` : #CODE

I can confirm that this method is one way of making it work . For some reason I struggled to get the merge working properly , so I've had to do it explicitly via :
No idea why , but it was returning an empty merge before and now it returns a merge of the expected proportions .

Use ` insert ` and make use of broadcasting if you want a column of all 0s : #CODE

I had understood ' converters ' specified a function to apply to the column . evidently I was wrong - thank you for pointing this out , it's very useful !
The read_excel() function has a converters argument , where you can apply functions to input in certain columns . You can use this to keep them as strings .

It won't be super-performant , but you should be able to ` apply ( pd.Series )` : #CODE

In python , opening a file with ' a ' means append as write only . Using ' a+ ' will append with read and write access . You must make sure that the code writing the file will only open the file as write-only , and your script that is reading the file must never attempt to write to the file . Otherwise , you will need to implement another solution .

I have also founnd a page describing google_exchange codes indicating the one I'm looking for ( ' EPA ') but I don't how to transpose it in DataReader .

Reshape / pivot pandas dataframe
I am trying to figure out a way to pivot the data so the variables are : ` id , year , a , b `
I am thinking about creating a hierarchical dataframe , but am not sure how to map the ` year ` in the original variable names to a created hierarchical column
oh ... well ... that was exactly what i was looking for ... thanks ! now all i have to do is move the years to a column , but i think that should be easy to do w pivot / reshape

You can use ` factorize ` to encode the different values in the column as integers : #CODE
I think you may have to apply the operation to each column individually since ` factorize ` only takes a 1D array as input .

Accordingly , I followed the guidance provided elsewhere on stack overflow , but pertaining to ` re.sub ` , and attempted to apply it to ` pandas.DataFrame.replace ` ( using replace with ` regex=True , inplace=True ` and with ` to_replace ` set as either a nested dictionary , if specifying a specific column , or otherwise as two lists , per its documentation ) . My code works find without using a function call , but fails if I try to provide a function as one of the replacement values , despite doing so in the same manner as works with ` re.sub ` ( this was tested and worked correctly ) . I realize that the function is expected to accept a match object its only required parameter and return a string .
a function is an object just like anything else in python , so when you replace one object with a function , it makes sense that you achieve your result . Why not use pandas.Series.map ?
@USER That logic is not correct in the operational sense that ``` re.sub ``` allows for my expected notion of function interpolation . I am using apply right now , but this really should work with ``` pandas.DataFrame.replace ``` .
regardless , you should use apply or #URL

You don't need to pass ` df1 ` to the ` hist ` method . You're using a method of the ` df1 ` dataframe , it already knows to use ` df1 ` as the data .

How do I tell python to not count 16:52 : 50-16 : 53:10 as 2 ? I can code for a specific time interval , but how do I translate " at least Y continuous seconds " into python ?

then you can merge both frames on index , and you can do whatever you want #CODE
if you don't want to merge both frames , you can apply the same logic on ` dfV `

Thanks @USER . how can we apply the function to only the rows missing values in C and D ?
check this link indexing-view-versus-copy if you want to know why I've use ` loc `

If you want to use the ` cuts ` string then you can use ` query ` with ` loc ` you need to use ` isin ` with the index and invert the boolean mask using ` ~ ` in order to set the ` False ` rows : #CODE

pandas concat arrays on groupby
After I applied some filtering on ` agg_df ` I want to concat the IDs #CODE

TypeError : dt must be datestring , datetime or datetime64

If I were using arrays I would normally stack them along the third dimension and then average on the third dimension . e.g. #CODE
While this would work , I suspect there is a method of doing this within Pandas . However , for this I do not know where to start . I have played around with groupby and resample , but I have been unable to make those work . Any help would be appreciated .

If the product is not there it creates a new set of rows to insert the values of the product .
What do you think is causing the error ? Can you reproduce the error with less code ? Can you simplify the variable names in the code for readability ? Pasting 20 lines of code out-of-context and a stack trace isn't likely to get a response .

Once you have that , you can apply it to every row using the ` apply ` method on dataframes : #CODE

Then date columns ` min ` ( values `' No '` and last previous `' Yes '`) and column ` date1 ` ( other values `' Yes '`) can be join by columns ` count ` .

how about doing what you've shown above but iterating over each column . then just store the resulting indexes as sets ; finally you can just find the intersection of the sets for your final desired rows .
You can use ` apply ` to make the code more concise . For example , given this DataFrame : #CODE
The ` apply ` function applies the ` contains ` function on each column ( since by default ` axis=0 `) . The ` any ` function returns a Boolean mask , with element True indicating that at least one of the columns met the search criteria . This can then be used to perform selection on the original DataFrame .

Create a boolean mask using ` shift ` that tests whether the previous row contains 1 or not : #CODE
sorry for the million questions but I just want to make sure I understand something before I blindly use it . How does the code know where to shift , it doesn't look intuitive when we have " ... shift() ! = 1 "
I call ` shift ` on the column so ` df [ ' Roll '] .shift() ` will move the rows down by one , you can see this when you try it yourself

that i could create from iterating over the list of ` LOC ` and doing a ` groupby ` and ` get_group `
and as a second part , i also can't seem to figure out ( and unable to phrase the question well to look for answers here in stack overflow ) how to produce the change in measurements .
for instance ` locations = df [ ' LOC '] .unique() ` will return an array of unique locations you can then use this to filter the df like ` df [ df [ ' LOC '] == locations [ 1 ]]` ... etc
Use ` diff ` so ` df [ ' MEAS '] .diff() ` will give you what you want

You can make use of the DataFrame's ` apply ` method . #CODE

New to python and getting up to speed on pandas ( 0.16.2 ) . I have a df ( returned from Quandl ) with the observations in rows . I want to transpose the dataframe and then set a 2-column multi-index : #CODE
However , df.columns is a periodindex object after I transpose . I can't add the two new columns I wanted to use to set a multi-index : #CODE
The periodindex was a huge help with my original list - I was able to group by quarter and then filter out the quarters I didn't want . I can probably send this to a dict and then replace the timestamps with labels , but there has to be a better way that I haven't found in the docs yet .

Cannot resample my Dataframe properly
I want to resample this dataframe following , #CODE
Resample bins the data into the frequency passed , so in this case it's going to group all the times from 1 day together . It looks like you want to group by just the time part of the date , which would be done like this : #CODE

I would like to go through all the columns in a dataframe and rename ( or map ) columns if they contain certain strings .
ooo thanks yes that makes a lot of sense . do you know if i wanted to replace the whole string with ` agri ` rather than just ` agriculture ` ?
The pattern specified in `` replace `` is regex , so you could do something like : `` replace ( ' . *agriculture* ' , ' agri ')`` ; however , that would cause the updated DataFrame to have two columns with the same name .
error : nothing to repeat ` it seems like it does not like the wildcards in the replace function

You are looking for ` DataFrame.pivot_table() ` , pivotting based on the columns - `' Published ' , ' Station '` , taking values from column - ` TypeFuel ` for the new columns in the pivot table and using values from ` Price ` as its values . Example - #CODE

@USER This not a duplicate , the OP does not even use apply here .
@USER Try using ` for i in range ( 1 , len ( df_z )): ` , as range starts with 0 and the i=0 case is already done before the for loop .
@USER That is difficult to say unless you provide a reproducible example . But look at ` len ( df_z )` , the result will have the same number of rows . So if that is wrong , there is something wrong in the logic .
Instead of ` range ( len ( df_z )` , try using : #CODE

Pandas : How to structure row-wise apply which requires previous output as input

pandas - sum objects in DataFrame column and join with DataFrame
And next I need to sum every id's ' buy ' and join the new column ( I named it buy_count ') with my DataFrame . I have smth like this : #CODE
But I can't insert ' buy_count ' to the DataFrame : #CODE
You can call ` map ` against ' id ' column of ` df1 ` and pass ` buys ` to perform a lookup : #CODE

python replace string function throws asterix wildcard error
I am currently using hierarchical columns and would like to only replace ` agri ` for that specific level ( level = 2 ) .
Based on your new and actual requirements , you can use ` str.contains ` to find matches and then use this to build a dict to map the old against new names and then call ` rename ` : #CODE

Recursively read files from sub-folders into a list and merge each sub-folder ' s files into one csv per sub-folder
I am trying to work out how to use ` pandas ` to recursively navigate a folders sub-folders , take each file in the sub-folder and merge it into one CSV file per sub-folder .

Alternatively , you can also apply a function based on the column ` col1 ` : #CODE
The ` apply ` approach should be preferred in this case as it is much faster : #CODE
2 ways , use a couple ` loc ` calls to mask the rows where the conditions are met : #CODE

Sadly Tom I'm at a loss of where to start . I was looking at this example #URL but couldn't apply it to my own

In my main df , I have a column that is combined with two other columns , creating values that look like this : A1_43567_1 . The first number represents a type of assessment taken , the second number being an question ID , and the final number being the question position on an assessment . I plan on creating a pivot table to have each unique value as a column to look across multiple students ' selection per each item . But I want the order of the pivot to be by the Question Position , or the third value in the concatenation . Essentially this output : #CODE
I've tried sorting my data frame by the original column I want it to be sorted by ( Question Position ) and then creating the pivot table , but that doesn't render the above result I'm looking for . Is there a way to sort the original concatenation values by the third value in the column ? Or is it possible to sort a pivot table by the third value in each column ?

I'm fairly new to both Python and Pandas , and trying to figure out the fastest way to execute a mammoth left outer join between a left dataset with roughly 11 million rows and a right dataset with ~160K rows and four columns . It should be a many-to-one situation but I'd like the join to not kick out an error if there's a duplicate row on the right side . I'm using Canopy Express on a Windows 7 64-bit system with 8 Gb RAM , and I'm pretty much stuck with that .
This works with small files but produces a MemoryError on my system with file sizes two orders of magnitude smaller than the size of the files I actually need to merge .
If a system with 8 GB RAM isn't enough to merge two files each 1 / 100th the size of the files I need , how much RAM would be enough ? 1TB ? How many supercomputers would I need to assemble ? There must be a different way to proceed that uses less memory . If not pandas , how ?
AFAIK , all join / merge operations can be done by sort-merges . So it's not necessary to keep things in memory .
And incidentally , the smaller test files are only 14 & 110 MB in size . No idea why 8 GB of system RAM isn't enough to merge those , even with Windows 7 . The full files are 144 MB & 1.1 GB . The final file size shouldn't be more than 2 GB .
Not incredibly familiar with sort-merges . Assuming you mean [ this ] ( #URL ) . I can see how that's a great way to make a long list alpha-sorted . But I don't need to do that . If that method can be used to conduct a left join between two datasets I'd need some more explanation .
You probably have repeated values in the merge ' keys ' which results in a Cartesian product of such rows in the output
In the left dataset there are repeated keys . Just checked and there aren't any in the right dataset - the keys there are unique . Since it's a left join and not a full join , the number of rows in the output should be the same as the left dataset . Right ?
By splitting the left dataset into chunks , turning the right dataset into one dictionary per non-key column , and by adding columns to the left dataset ( filling them using the dictionaries and the key match ) , the script managed to do the whole left join in about four minutes with no memory issues .

You can ` import json ` and apply ` json.loads ` to convert the string data in your ` geojson ` column to ` dict ` . Then , you can extract data from ` dict ` directly , or use one of many Python modules that deal with GIS data . I find ` shapely ` easy to use and helpful in many cases .

Thank you , @USER ! It works good . The only thing is that having cases definition in forms like ` ( -1111111AND2222222 ) NOT ( -3333333 )` I'm using ** regexp ** to convert them to the appropriate form ` case = ' (( presence [ -1111111 ] & presence [ 2222222 ]) &~ ( presence [ -3333333 ]))'` so to perform boolean tests . And since this case has type of ** String** , the only way to evaluate it is to use ` eval ( case )` . Do you have any thoughts or maybe other availavble options to evaluate such cases ?

Pivot table from a pandas dataframe without an apply function
I want to create a pivot table with the following output : #CODE
I thought using the pivot function to the dataframe ( df_pivot = df.pivot ( index= ' ID ' , columns= ..., values= ' count ') but I am missing the columns index list . I thought applying a lambda function to the df to generate an additional column with the missing column names but I have 800M IDs and the apply function to a grouped dataframe is painfully slow . Is there a quick approach you might be aware off ?
Hi , yes , I thought using the following function : def f ( x ): x [ ' Index '] = range ( len ( x )) return x and then do the following : df.groupby ([ ' ID ']) .apply ( f )
Then apply the pivot method setting the new ` subindex ` as columns and fill ` NaN ` values with 0 : #CODE

What would be the Python equivalent ? I cannot think of a way to translate this where statement into pandas syntax .
The only way I can think of is to add an arbitrary field to people_usa ( e.g. ` people_usa [ ' dummy '] =1 `) , do a left join , then take only the records where ' dummy ' is nan , then delete the dummy field - which seems a bit convoluted .
Does this work only on the index of the dataframe ? I'd like the option to specify the field ( s ) to apply this to
Is there any easy way to do this if you have multiple columns to check / join ?
You could do a ` merge ` and then eliminate the rows that exist in the merged df otherwise you'd have to build a boolean condition for all the columns you want to compare but presumably when checking the multiple columns you're stating that it's unique for those columns , correct ? For instance it's not a match if say col1 and col2 match but col3 does not
Yes merge is what I have been doing but it feels like a hassle .

I think you have to do it this way , pandas will try to use existing indices and column labels for alignment without renaming you can't infer how the values should align without renaming
You could try the melt function #CODE

Sorry , I want to simply identify the column that contains the text ' Measure ' in it , which I then apply the filter measure_filter too using .isnin .
Sorry for not being as clear cut , I've attempted to update my question to be more concise . I'm wanting to apply the logic to just identify the word ' measure ' within the following code ` ( df [ ' hereisalltherandomtextmeasure '] .isin ( measure_filter ))`

If list of functions passed , the resulting pivot table will have hierarchical columns whose top level are the function names ( inferred from the function objects themselves )

What is a concise way to split col3 into new , named columns ? ( perhaps using lambda and apply )
You could apply a join to the list elements to make a comma separated string and then call the vectorised ` str.split ` with ` expand=True ` to create the new columns : #CODE
A cleaner method would be to apply the ` pd.Series ` ctor which will turn each list into a Series : #CODE
This might cause difficulties if the " columns " legitimately contain commas ... Maybe something like ` df [[ ' id ' , ' email ' , ' address ']] = df.col3.apply ( pd.Series )` then drop ` col3 ` ?

Pandas Merge 2 data frames by 2 columns each
In each data frame i have column with the same name and values ( Key_Merge1 ) and in each data frame i have 2 different column names with same values ( Key_Merge2 ) . How can i merge 2 data frames by 2 columns :
Can you post an example data and df , your text description is not clear enough but generally you want to merge and pass the list of cols to merge the ; hs and rhs on : ` pd.merge ( df1 , df2 , left_on =[ ' Key_Merge1 ' , ' Key_Merge21 '] , right_on =[ ' Key_Merge1 ' , ' Key_merge22 '])`
OK , you have to rename ' PRODUCT_GROUP ' in DF2 in order for the ` merge ` to work : #CODE
the merge will naturally find the 2 columns that match and perform an inner merge as desired

You could use ` apply ` to remove the nulls and take the integer location like this . #CODE

The error makes sense in that I think ` .transform ` wants to map a DataFrame to a DataFrame . But is there a way to do a groupby operation on a DataFrame , pass each chunk into a function that reduces it to a Series ( with the same index ) , and then combine the resulting Series into something that can be inserted into the original dataframe ?
If you change ' transform ' to ' apply ' , you'll get : #CODE
That's not quite what you want , but if you drop level=0 , you can proceed as desired : #CODE

I started with your suggestion and found a similar method using isin as referenced here : [ link ] ( #URL ) . I am able to groupby a single list using average_df = data_df [ ' Value '] .groupby ([ data_df [ ' Location '] , isin ( locationlist ) .mean() . With either method I haven't been able to pass multiple lists through the groupby ( list of close locations for each location ) . I have tried multiple for loops , but haven't been able to produce a solution that works . I'm also not sure if it will create multiple DFs or if I can append the groupbyobjs .

Join pandas tables , keeping one index
What I want to do is join two dataframes on columns and keep the index of one of them ( but the index is unrelated to whether I join them or not ) .
For example , if ` df1 ` is the dataframe that has certain timestamps as its index that I would like to keep , then to join with ` df2 ` on the ' key ' column , my expected code would be #CODE
What is the proper way to join two tables , keeping one of the indices ?
I am doing an inner join on ' key ' . That is not the issue . The issue is that I want the resulting rows to have the index of one of the dataframes .
Joining the tables really depends on the data , there is not a * single * proper way to join all the tables with * one * universal command . It comes down to what are the two DF you're working with , and what is the output you're wanting . Can you include some data that you're working with , and compare the output you're getting with the expected ?
I think what your looking for is akin to Full Outer Join in SQL , in which case I think the following would work : #CODE
As for keeping just one index , that should be done automatically in this case now that outer join is keeping all keys .

Then just call the function , specifying the name of the columns to shift #CODE

Attempt using SQL : I don t have access to the database as I m writing this , but I had to join two tables to get the Team field , and I think I unsuccessfully tried something along the lines of ( this may not be completely right but should be close to what I tried ): #CODE

Then what you'll need is to index each column using then apply ` df.resample() ` #CODE

You can pass param ` parse_dates =[ 0 ]` for ` read_csv ` so try ` df= pd.read_csv ( " myfile.csv " , names =[ ' DateTime ' , ' Freq '] , parse_dates =[ 0 ])` this will parse the first column as a datetime , it should be significantly faster
return pd.read_csv ( " f20141.csv " , names =[ ' DateTime ' , ' Freq '] , parse_dates =[ ' DateTime '] , skiprows=1 )

Watching my system as it reads the file , the memory usage crawls up until its completely maxed out ( all 6gb used ) , then after the ` del ` command , the memory usage seems to drop down , but not all the way down ; now approximately 1gb of memory is being used . What's going on here ?
Edit : after quiting ipython the memory usage seems to drop back down to the original level

Are you trying to set the value of an existing column by applying a scalar function to each row ? If that's the case , instead of iterating over the rows you can consider apply , map , or applymap methods based on your need . This is a pretty good summary #URL
@USER I have plenty of if statements inside that funcion , else I would have used apply

How to do " ( df1 & not df2 )" dataframe merge in pandas ?
I want to merge do a " ( df1 not df2 )" kind of merge on keys ( x , y ) , meaning I want my code to return a dataframe containing rows with ( x , y ) only in df1 not in df2 .
indicator : Add a column to the output DataFrame called _merge with information on the source of each row . _merge is Categorical-type and takes on a value of left_only for observations whose merge key only appears in ' left ' DataFrame , right_only for observations whose merge key only appears in ' right ' DataFrame , and both if the observation s merge key is found in both .

You could add a dummy variable to your ` useractivity_ids ` then use the pandas ` merge ` to compare and filter . #CODE
In the next version of pandas ( 0.17 ) , ` merge ` has an ` indicator ` keyword that lets you do this without the dummy variable . #CODE
you can do left join between the two data frames #CODE

You could drop the rows that don't contain ' years ' and then remove the word and cast the dtype so something like ` df.loc [ df [ ' Duration '] .str .contains ([ ' years ']) , ' Duration '] .str .rstrip ( ' years ') .astype ( float )` should work
No idea , normally it will try to sniff the form of the csv and guess the number of columns , if it doesn't conform to the sniffed format it should fail so I'm surprised it worked and in fact your previous code worked as it should have borked once it saw the second line didn't match the first line format
And this will strip out ` year ` or ` years ` from the ` emp_length ` column , although you are still left with text categories . #CODE

I have a dataframe of the form above . The 1st row is the header with data for the years 2015 , 2020 and 2025 respectively . Other columns have nan's as misssing data . I want to interpolate by row and am using this : #CODE
@USER : the error you're getting implies that your DataFrame has mixed types - some of the columns are * not * float types ( see [ here ] ( #URL )) . I'd drop the ` inplace ` argument ( it's rarely of benefit anyway ) and then the operation should work .
thanks @USER , this is perfect ! but , I now realize that I should be doing interpolate and not ffill , I am getting an error there . If you could take a look at #URL i will be very grateful

Python interpolate not working on rows
Related to Error in gapfilling by row in Pandas , I would like to interpolate instead of using fillna . Currently , I am doing this : #CODE
However , this does not seem to replace the NaN's . Any suggestion ?

Assuming you have a unique-indexed dataframe ( and if you don't , you can simply do ` .reset_index() ` , apply this , and then ` set_index ` after the fact ) , you could use ` DataFrame.sample ` . [ Actually , you should be able to use ` sample ` even if the frame didn't have a unique index , but you couldn't use the below method to get ` df2 ` . ]

You need to expand your example , I can't grasp it like that . When you do indexing , you insert a boolean vector into the dataframe and it returns rows where the vector has the ` True ` value .

You can do this with groupby and apply : #CODE

Is there some way to map the color of a dot to how many times it occurs in the data set ? What about size ? How can I assign the size of the dot to how many times it occurs in the data set ?

You could do this by adding a new F column and then calling ` pivot ` : #CODE

Pandas Left Merge / Join not Resulting in what is expected for left join
So I may simply not be very informed on what a left join is , because I am getting tripped up ... Here is my definition of a left join :
I don't understand your expected output . The first ` b ` in your first data frame , matches the ` b ` at position 1 and the ` b ` at position 3 in the second data frame . When doing the left join , why do you expect only the first pairing but not the second ?
This requires calculating the full left join first . If speed is critical , this may not be an optimal solution .

I'll assume you've already filtered your DataFrame for the relevant dates . You then want a pivot table where you have unique dates as your index and your symbols as separate columns , with daily returns as the values . Finally , you call ` corr() ` on the result . #CODE

You need to apply ` transform ` to the ` groupby ` , which preserves the shape of your original DataFrame . #CODE

I create two copies of the dataFrame , df1 and df2 . Since , I have only a maximum of one duplicate ( ie . two copies of the same row ) , I drop duplicates by taking the last and the first in df1 and df2 respectively and then attempt to merge the two . #CODE

Your question is very broad here and typically it should be 1 problem per question , you can combine any number of columns to create a datetime column and then [ ` resample `] ( #URL )
See here for [ how to combine date and time ] ( #URL ) then look at resample as suggested

when I print dHaGreen0 , it returns a list of bool , and " dtype : bool " . But when I go to use dHaGreen0 in the if statement , I get the error ValueError : The truth value of a Series is ambiguous . Use a.empty , a.bool() , a.item() , a.any() or a.all() .
The error you're seeing is correct . dHaGreen0 is an array of bool values . When you ask " dHaGreen0 == True " , you need to decide on one of the two following interpretations :
Do you mean if any value is dHaGreen0 is True ? If so , replace " dHaGreen0 == True " with " any ( dHaGreen0 ) == True " .
Or do you mean if all values in dHaGreen0 are True ? If so , replace " dHaGreen0 == True " with " all ( dHaGreen0 ) == True " .

Merge columns and create new column with pandas

pandas resample with function that returns an array
Because that fft function changes the shape of the input you can't just apply it directly . Here would be one way to wrap it . #CODE

Then use a map function followed by a fillna ( method = ' ffill ')

Most time is spent indexing the Pandas dataframe ! Taking the ` argmax ` is only 1.5 % of total execution time .

We want to remove rows whose values show up in all columns , or in other words the values are equal => their minimums and maximums are equal . This is method works on a ` DataFrame ` with any number of columns . If we apply the above , we remove rows 0 and 2 .

I'm trying to generate a cumulative column between any two given datetime ranges using a pandas pivot table , however it's unclear how to actually achieve this . I can create one for all dates as per below .
thanks , this is what I was looking for ! I'm applying this to a DF where dt index isn't necessarily unique anymore , ie . as well as { ' count ' : 0 } there will be { ' Location ' : ' Japan ' } , if I wanted to count the accumulated value between 2 dates ranges ( as per above ) by location , ( Location -> Date -> Accumulated Count ) Do you know how to achieve that ?
wish I had the rep to upvote this . Got a key error on ' loc ' , seems like the ix filter removes this column ( so I added it back in before the grouping ) temp =d f [ ' loc '] then df [ ' loc '] =temp after the filter .

What I need to do is to compute the average temperature for every run , averaging all the temperature measurements belonging to a run . I guess that the first step to do is to join the two tables , but I am not able to express the predicate "` time ` between ` start ` and ` stop `" . Suggestions ?
Yes , it similar , but actually I am not sure I need to merge the two tables . I need only the average . I don't care about measurements witch don't fit into any interval
sure , but it doesn't seem to me to be the optimal solution . For example now I am doing the opposite : I loop over the period table and for every period I find the data which match , and I accumulate the data to compute the mean . No join is necessary .

The above code is what I have written to merge the .dat files into one Dataframe .
That did indeed drop the duplicates :) Thanks a lot . Rocky II is not in the output anymore however ^^ . But i guess that's just random because they all have a rating of 5 . But i will look into it :)
You can drop the duplicate entries by calling ` drop_duplicates ` and pass param ` subset= ' movie_id '` : #CODE

The stack trace references different code version . There you're using hourly and daily . Do they have the same dimensions ? Also , you're plotting ` upp_bd.index ` against ` upp_bd ` and the 4th arg translates to ` where=upp_bd ` . The code works for me with same lib versions .
Yes , you are right . The errors happened because of my poor cut and paste between the real code an the " reproducible " code . The reproducible code does work for my now . The actual code doesn't :|

Pandas apply to multiple rows with missing dates
For a Pandas DataFrame I am looking for a vectorized way to calculate the cumulative sum of the number of views per given group , except the views from more than a week ago . I have tried all kinds apply functions , but I can't seem to go up and down 7 days to collect the data I need .
I looked at using shift for instance , but because not all dates are filled in for all the groups , this does not work . I also tried to use the map function , this also look too long .

Once you create Categorical Data , you can insert only values in category . #CODE

I have a Pandas dataFrame ( called ' pivot ') which is 13843 rows by 40 columns . Most of the data cells are single characters ( A , B , C or D ) .
Turned out to be an issue with the data . There were instances where I was trying to pivot duplicate data rows .

` concat ` your dfs after reading them in , then ` groupby ` on ' lat ' and ' lon ' and then call ` size ` to return the count , ` reset_index ` to restore your grouped columns and finally rename the generated column ` 0 ` to ' time ' : #CODE

I would like to replace some of the values in the foll . dataframe :
I would like to replace the values in the columns with a numeric value derived by multiplying a scalar ( say 0.5 ) by all values in this dataframe :
I saw replace , but not sure how to use it here
Here ` df1 [ df1 ! =0 ]` will produce a df of the same shape with ` NaN ` values where the condition is not met , you can then call ` fillna ` on this and pass the other df which will replace the ` NaN ` values where the index and columns align .
Ref about ` replace() ` : Replace all occurrences of a string in a pandas dataframe ( Python )

I am also not sure how to pass ` .year ` argument after I successfully convert the strings into datetimes . I could write a wrapper function that takes each row as input and then extracts the year , but I think it s useful to know how to apply pandas syntax for future reference . Thanks !

Maybe start with making your ` if ` statement act correctly , did you mean ... ` or len ( block ) == 5 ` ?
so if len ( block )= =6 and block.isdigit() or len ( block )= =5 and block.isdigit() ' ?
More concisely ` if len ( block ) in ( 5 , 6 ) and block.isdigit() `
Your ` except ` block never runs , as nothing you're doing in the ` try ` block will raise an exception as long as all the ` cam ` values are strings . If the " error " you want to detect is there not being a five or six digit number in the campaign string , you should put a ` break ` after the ` append ` call and use an ` else ` after the loop ends . The ` else ` block will be run if the loop ends without the ` break ` being hit : #CODE

Replace values in pandas dataframe based on column names
I would like to replace the values in a pandas dataframe from another series based on the column names . I have the foll . dataframe : #CODE
How do I replace the values in the first dataframe based on the values in the 2nd series ?
Replace data_dir in config_rotations.txt with the path to the input directory i.e. where the files are kept
Replace out_dir in config_rotations.txt with whatever output path you want
Additionally pandas tries to align along index values and column names , if they don't match then you'll get ` NaN ` values so you can get around this by calling ` .values ` to get a np array which just becomes anonymous data that has no index or column labels , so long as the data shape is broadcast-able then it will do what you want : #CODE
That means there is no alignment between either the row values or the column names , if the column names definitely match then try ` df.loc [( df [ ' Country Code '] == replace_cnt ) & ( df [ ' Item '] == crop ) , s.index ] = pd.DataFrame ( columns= s.index , data= s.values.reshape ( 1 , len ( s.index ))) .values ` can you edit your question with what the row / columns are for the lhs and rhs , thanks
The reason that works is because when you assign values , pandas will try to align the lhs and rhs index and columns so if for instance the index values don't match then ` NaN ` will be assigned , the same thing happens if the column names don't match . By calling the attribute ` .values ` you return a numpy array this has no index or column labels so it's anonymous data so as long as the shape is broadcast-able it will assign as desired

I've been playing with Pandas and think I have the fundamentals working , but I can't seem to figure out how to do one last transformation ( from my placeholder value in the pivot to an actual English word ) .
In the code below , the piece I need help with is the comment that says " I need to figure out something I can put here that will replace any non-null value found in the cells of column pivottally [ c ] with the string ' registered ' . "

cut time spells into calendar months in pandas
Basically , it would suffice for me if I could cut spells at turn-of-month datetimes , getting from the data in the first example to the data in the second : #CODE
This is the cut DateTime you can use to compute the portion of the stay attributable to each month . cut - start is the first month ; end - cut is the second .
How does this cut the time spell at end-of-month ?

Classic case of pivot . First , let's introduce a count column , then create a pivot table . Let's ignore your regex , since that is not the issue ; just apply it to the column beforehand . #CODE

What is the Pythonic way to apply a function to multi-index multi-columns dataFrame ?
Given a multi-index multi-column dataframe below , I want to apply LinearRegression to each block of this dataframe , for example , " index ( X , 1 ) , column A " . And compute the predicted dataframe as df_result . #CODE
Some columns of the original data can contain missing value , and we cannot apply regression directly on them . For example , #CODE
Would it be legitimate to use the interpolate method to fill the null values ? #CODE

And I want to drop the ` stockid ` column and get the rest data .
there's also ` new_df = df.ix [: , 1 :] ` . This should eliminate ` stock_id ` . If you want to drop this column then use ` new_df = df.drop ( ' stock_id ' , axis=1 )` .

` df [ df [ ' A '] 1.0 ]` : this works - But I want to apply the filter condition to all columns .
what filter condition do you want to apply , what is an example ` df ` and what are you expecting as output ? When trying it for whole df , there would surely be some rows where only some columns meet the condition ( and vice-versa ) , so for places where the condition is not met , it is substituted with ` NaN ` .

I have a pandas data frame . Some entries are equal to -1 . How to find the number of times -1 exist in every column in the data frame . Based on that count , I am planning to drop the column .

I had been happily using django-tables2 but I decided I wanted to transpose the table so the header row was the first column and then the rows were columns . This is pretty well described all over the place , but not so much how . My approach so far is to use django-pandas to convert the data from queryset to dataframe and then to use pandas to transpose the data . The content of the model doesn't seem to be relevant to so I'll make that simple . #CODE
Then I can get a queryset , convert it to data frame and transpose it . #CODE

the above code results in the only one figure ( with boxplots corresponding to each month in the figure ) , but I want to create 9 such plots each corresponding to a particular area as subplots in the same plot . In other words , I want to first group the data by area , then by month of the year and then plot the result as boxplot . Any thoughts how I can get the desired plots ? Any help is appreciated .
Also , how can I get rid of the " Boxplot grouped by [ 1 1 1 ... 12 12 12 ]" and " 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ,..... " both at the top and bottom of the plot ?

B . Create a function which transforms the " refund " orders into negative values and , then , apply it on the Series : #CODE

How to perform string replace operation using pandas in python ?
I tried using the pandas built in string replace function put its returning the original column without actually performing the string replace operation . #CODE
Why not just use an ` apply ` on the column , and do something like ` lambda lst : ' ' .join ( lst )`
what does ' ' mean in the apply function ?

I think actually it's better to test for each value and apply ` any ` : #CODE

lol .. my apologies I didn't notice you have " 2200 " columns , I jumped too quick to a conclusion . but hey same rules apply you can do : ` df [[ col for col in df.columns if col.endswith ( " _x ")]]` . But hey just follow unutbu's solution , he's GOD of pandas

In pandas , how do I replace label IDs with label strings ?
I'd like to replace the ` labelID ` column with the label names , so that I have something like this : #CODE
Sorry are these values occurring more than once ? Are you just wanting to add the label string or are you looking to replace values ?
The values in ` labelID ` will only occur once . This dataframe is the result of a ` groupby ` so I should only have each one time . I want to replace the IDs with a useful string , because I'll be plotting these for a presentation and the label is much more useful than an integer .
Then you'd have to ` map ` like the other answer but it was a little unclear to me hence my comment , your sample was a bit too basic really
You can use ` map ` to replace values in a series ( or , if you wanted to replace across the entire dataframe you could use ` applymap `) .
You need a dictionary that maps the values to the labels . This dictionary is then passed to the ` map ` function , which is applied on the Series you want to relabel #CODE

I originally tried a concat but if became too difficult to separate the data afterwards and I struggled with using a ` .loc ` function .

@USER : dataframe is an output of a hive SQL query . I then replace the default tab delimiter with " , " . I tried to avoid replacing the tab delimiter and keeping it as is , trying to do ` pd.read_csv ( ...., sep= ' \t ')` but it turns out that some ` body ` fields have tabs in them as well , so it's all just a mess .

Can I make pandas cut / qcut function to return with bin endpoint or bin midpoint instead of a string of bin label ?

Python Pandas Apply Formatting to Each Column in Dataframe Using a Dict Mapping
I want to apply very specific formatting to each column in the dataframe using a dict like the following : #CODE
I know I can use applymap for multiple columns or apply on a single column : #CODE
How can I iterate through each column in a dataframe and apply formatting using a dictionary where the ` dict ` ` key ` is the ` column ` and the ` value ` is the ` string ` formatting ?
The easiest way would be to iterate through the ` format_mapping ` dictionary and then apply on the column ( denoted by the key ) the formatting denoted by the ` value ` . Example - #CODE

I'd like to replace id2 with the value from the array .
With the hopes of x being a series with the values I want that I can column bind to my DF . What actually happens is that it errors out with an IndexError . In that case I made a function to apply , in place of the lambda function , so that it could except the error , but this returned all nulls .

pandas plot replace xticks
It is working well but the ` x ` axis is impossible to read as there are numerous numbers overlapping . I would like the ` x ` axis be a counter of the boxplot .
The ` boxplot ` method doesn't return the axes object like the ` plot ` method of DataFrames and Series . Try this : #CODE
The ` boxplot ` method returns a ` dict ` or ` OrderedDict ` of ` dicts ` of line objects by the look of it .

I don't have mongo installed . Is the 1st box you show the first rows of ` data ` in your code ? If it is , I think it would be more or less easy to solve with ` apply ` , there's many questions around but probably [ this one ] ( #URL ) will help you . If you find issues please post them . Hope it helps .

What you definitely don't want to do is insert one row at a time . You'll end up making a full copy of the dataframe with each insertion . If , for any given row , you will append at most one extra row , you could do the following steps :
2 ) append an uninitialized dataframe to the end of your original dataframe , with the same length
This will have the effective efficiency of approx 4 copies , much better than a copy for each insert .

How to do join of multiindex dataframe with a single index dataframe ?

How to do join of multiindex dataframe with another multiindex dataframe ?
How to do join of multiindex dataframe with a single index dataframe ?
It's not clear from the question which index levels the data frames share . I think you need to revise the set-up code as it gives an error at the definition of ` sngl ` . Anyway , suppose ` mult ` shares the first and second level with ` sngl ` you can just drop the second level from the index of ` mult ` and index in : #CODE

Since version 0.13 , it is possible to append to a dataframe by referring to indices in .loc or .ix which are not yet in the dataframe . See the documentation .

To replace each NaN with a number instead of dropping rows , do this : #CODE

Use ` loc ` for label based indexing : #CODE

Python Pandas Merge Causing Memory Overflow
I'm new to Pandas and am trying to merge a few subsets of data . I'm giving a specific case where this happens , but the question is general : How / why is it happening and how can I work around it ?
Renaming a Column ID to Allow for Merge #CODE
Successful Merge #CODE
Try a second merge . This is where the Memory Overflow Happens #CODE
check the shape of each data frame before and after the merge
One thing that may help is to put the creation of the DataFrames in functions , that way the strings and zipfiles can be closed and garbage collected ( rather than sitting in memory ) . One question is how big do you expect the final merge to be ? If you have a n row merging with an m row it can be n*m rows , which could be VERY big here .

To get exactly what you hoped to see , included the other columns in the group by , and apply sums to the Y variables in the frame : #CODE

I'd use map : #CODE

The issue is that you have some columns that are int , hence when trying to apply the regex on those int values it fails with the error - #CODE
You can convert your columns to ` str ` and then apply the ` DataFrame.filter ` - #CODE
You will need to convert to ` str ` before you can apply regex on the column name , a way ( not sure if the most efficient ) to not convert the column names to ` str ` permanently and still get the required data is - #CODE

If you want shift data , you get ` NaN ` value in last row .
I shift rows , fill ` NaN ` to ` 0 ` .

In the dataframe above , I try to drop the column named ' Item ' , with the foll . command : #CODE
The slice gets me a unique Item , so I cannot drop this column before .
what are you trying to drop ? Do you want a completely new dataframe from the slice and then drop the column from that dataframe ?
Right , I am trying to create a dataframe and then drop that column . I removed inplace=True , but that is still giving same warning
Most probably you received the ` vals_bel_lux ` through slicing , in which case the issue is occuring because you are trying to do ` inplace ` drop ( by passing ` inplace=True ` argument to ` drop ` method ) .

You can drop the columns min , 25% , max , etc . if you don't need them .

yes - however , I should have said that I want to apply the new series the requests.post method , in order to practice passing functions to series items .. for instance , I could have 100 columns that I would not want to write out .

The same principles would apply if you are using excel reader .

Replace the ` * ` with ` ctime ` if you are only using ` ctime ` . If you're fine using ` time.ctime ` , use this instead : #CODE

I just need the prediction , which in this case is 1 . I've tried aresult [ ' result '] , aresult [ 0 ] and aresult [ 1 ] all to no avail . Before I do something awful like converting it to a string and strip it out , I thought I'd ask here .

transforming data frame in ipython a little like transpose
You can use ` pivot ` : #CODE

Pandas - join by time proximity
I have 2 dataframes , ` left_df ` and ` right_df ` , each of which has a column corresponding to datetime . I want to join them in such a way , that for every row ` R ` in ` left_df ` , I find the row in ` right_df ` that is closest in time to ` R ` out of all rows in ` right_df ` , and put them together . I don't have about whether the row from ` left_df ` or from ` right_df ` came first .

Potentially a better data structure ( rather than a Series of lists ) is to stack : #CODE
Note : I thought you used to be able to return a list in an apply ( to create a DataFrame which has list elements ) this no longer appears to be the case .

Why ' hist ' instead of ' bar ' ?

it does require some paradigm shift in thinking ...

How to get the index and column name when apply transform to a Pandas dataframe ?
I think a join across multiindex levels might be better ?

Conditionally replace data in a pandas multi-indexed dataframe
I want to evaluate ' COL1 ' , subframe ' foo ' for values greater than 0 and replace ( in-place not copy ) ' COL4 ' values for the corresponding rows with a new value , COL1 / 1 . Then I would like to do the same thing again for subframe ' bar ' , however evaluate ' COL2 ' values instead .
My actual dataframe is huge , so I've been trying to find a better solution than itterating over rows . I've been able to conditionally replace with a regular dataframe but , something isn't clicking when I try more advanced multi-indexing and setting . I'm probably making things more complicated , but I'm about 45 minutes away from charging a wall .
I don't get what you meand by ' COL1 / 1 ' . What value do you with for COL4 to have at index [ foo ] [ 3 ] ? And when evaluating COL2 , do you wish to replace COL4 values again ?
I didn't quite get what you mean to substitute in your ' target ' column , so I made something a little generic . You can replace bits as needed .
You can replace the parameters to repeat the operation in another slice / columns of the dataframe , as needed . The above operation changes a dataframe like this : #CODE
My version always replaces the positive values with 999 . It does ' apply ' over the rows , but I can't quite get how you could do this without that .

Join Pandas DataFrames Based on Time Comparison Criteria without If / Then For Loops
Serial number 6846 was installed on ' Tire B ' group 2 on ' Hot ' . See Table C for an example of results ( I need to be able to join to either table ) .

How about something like this . Just get the len of each list in the columns , then select where the len is 0 . #CODE

2 ) There is a much better way to apply a function than map to validate urls ?

The filename is illegal . / indicates a directory , not sure about : . Replace all the / and : with either - or _ .
Try to replace the ` / ` with something like ` _ ` .

I might do this with apply rather than eval ( especially if I didn't trust the source ): #CODE

how to do left join using pandas
How can I let these 2 join into 1 dataframe , the result shall be like this : #CODE
What you're asking for isn't a left merge as you skipped the row with ` R3 ` , you just want to perform an inner ` merge ` : #CODE
a left merge would result in this : #CODE

Why not just use the iterator from the ` tab.where ` statement into the constructor of a dataframe ? Then just drop the columns you don't want at the end . Iterating and adding rows to a dataframe one at a time is not the way to go .

Then replace the code you shared with #CODE

how to transpose multiple level pandas dataframe based only on outer index
One way to do this would be to reset the index and then pivot the table indexing on the ` level_1 ` of the index , and using ` level_0 ` as the columns and ` 0 ` as the values . Example - #CODE

Here's the stack trace : #CODE
Since you are using ` pandas ` module , you access elements with ` loc ` or ` iloc ` or ` ix ` . In your case #CODE

How about adding an index name to data dataframe and then use a join ? #CODE

You want a resample at minute ( ' T ') frequency , but you need to specify how the resampling is done . ' first ' , ' last ' , ' mean ' , ' sum ' ...
You first need to set the index to your ` Time ` column in the DataFrame . You then resample as follows : #CODE
Note that you get a ` NaN ` for 07:20 because there were no records during this interval . You can , of course , drop NaNs if desired . #CODE

can give you like 3x read boost . I also suggest you to try defaultdict instead of your ` if k in dict ` create ` [ ]` otherwise ` append ` .

You can use the ` diff ` method for this , together with ` fillna ` to fill the first NaN with a 0 : #CODE

append pandas.DataFrame.GroupBy results into another dataframe
You need to append the intermediate DataFrames to a list and then concatenate the results .

pandas concat ignore_index doesn't work
I am trying to column bind dataframes and having issue with pandas concat , ignore_index = True doesn't seem to work : #CODE
no , it binds the rows . I want to bind the columns ( append ) . I tried append , that doesn't seem to work either .
I think ` ignore_index ` only ignores the labels on the axis you're joining on , so it still does an outer join on the index labels . I agree the names of function arguments aren't the most intuitive here .
` ignore_index=True ` ignores , meaning doesn t align on the joining axis . it simply pastes them together in the order that they are passed , then reassigns a range for the actual index ( e.g. ` range ( len ( index ))`)
so the difference between joining on non-overlapping indexes ( assume ` axis=1 ` in the example ) , is that with ` ignore_index=False ` ( the default ) , you get the concat of the indexes , and with ` ignore_index=True ` you get a range .

This needs a * lot * more info , at least the build log / stack trace . It should brew + pip install just fine . An alternative installer is anaconda which has a really simple / quick way to get started with pydata .

I tried ` merge ` and ` concat ` to join the dataframes together , but the resulting plot is not what I'd like because those functions insert ` numpy.NaN ` in places where the date is not the same , which makes discontinuities in my plots . I can use ` pd.fillna() ` but this is not what I'd like , since I'd rather it just connect the points together rather than drop down to 0 .

pandas pivot on multiple columns gives " The truth value of a DataFrame is ambiguous "
This pivot , with ` SubjectID ` as the index works : #CODE
Function pivot doesn't support multiple columns and indexes , I think it is not implemented yet .

Best possible way to merge two pandas dataframe using a key and divide it
or merge on ' unique ' and then do the calc as normal : #CODE

If you have a container ( e.g. ` list `) of dates that you want to exclude called , say , ` unwanted_dates ` you can just do ` Temp_plot.drop ( unwanted_dates )` . Note this returns a view with the desired dates excluded and doesn't actually alter ` Temp_plot ` . To drop them permanently do ` Temp_plot = Temp_plot.drop ( unwanted_dates )` or ` Temp_plot.drop ( unwanted_dates , inplace=True )`
@USER The data does contain holidays and holidays should be removed from the data . If I have to remove it before resampling then there will be so many rows and it will be difficult . Can't I drop them as suggested by @USER ?
Well , its holidays in a School in UK . But it is not just holidays ' week that I will be deleting , I will be deleting some other weeks as well . So the best idea would be to have a list that contain all those dates and then drop them after resampling . Which I tried to do but got error .

Some results of qcut are NaN . So I decided to use notnull but took error again .

I would like to apply something like ' text to columns ' function in excel .

I would like to merge 2 dataframes based on shared string column , but sometimes that column values differ slightly . Is there a way in pandas to do that #CODE
In above frames I would like to join based on ' street ' column , but treat for example , ' A Street ' and ' A Str ' values as same . It would be good if there is some kind of threshold , like if edit distance between 2 values is say 4 , I would like to treat them as same values .
sorry for being vague , I was thinking if edit distance ( number of changes ( insert , delete , update ) to characters to make 1 string look exactly like other ) is say less than 5 treat them equally . Hope this explanation is clear enough

Pandas - create dataframe manually and insert values

Also you should be able to call ` isnull ` on the series itself , there is no need to use the top-level ` pd.isnull ` so ` ascomb [ ascomb [ ' Discipline '] .isnull() ]` should still work

Hello Thank you for script , but I just edited a little about my sample file in above question , because this script is throwing an error as , Use of uninitialized value in join or string at script.pl line 34
Then ` df1 ` is append to ` df ` and last is set index from list to output ` df ` . #CODE

You compared the result of groupby ` upper_bound ` to ` df [ ' C ']` , but they have different number of elements . Use ` transform ` to have the mean for each line existing witin each group and compare it to ` df [ ' C ']` . Apply this mask with ` loc ` : #CODE

1 ) I'd like to strip the YYYY-MM-DD data and just keep the time data , how can I do this ?
you can strip the date by doing : #CODE

How to speed up append to an existing dataframe
I am trying to append or add rows to the existing dataframe which has around 7 million rows .
You can use np.where , if you need append rows by conditions : #CODE

You could use ` diff / cumsum ` to assign a group number to the boolean values .

If you really must remove the ` microsecond ` part of the datetime , you can use the ` Timestamp.replace ` method along with ` Series.apply ` method to apply it across the series , to replace the ` microsecond ` part with ` 0 ` . Example - #CODE

rbind.fill() which is a way to append data frames with unequal number of columns .
You are looking for the function ` concat ` : #CODE

I have a large groupby dataframe and a smaller list of ids that are a subset of that data frame . The ids are the names of the groups . I would like to drop all groups that are not present in the list of ids .
and got this error : ` AttributeError : Cannot access callable attribute ' drop ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method `

My ultimate goal is to merge the contents of a folder full of .xlsx files into one big file .
Another is that you can then concat these into one DataFrame efficiently in one pass : #CODE
Note : This is significantly faster than append , which has to allocate a temporary DataFrame at each append-call .

Hey EdChum thanks for the reply not sure why it would truncate my data I thought it would just pump out what's in the dataframe as default . Did I mess up somehow and tell it to truncate ? This is a fresh install
The default is to truncate , if you have several million rows you don't want it to crash your machine / pipe out for ages . This is configurable however : #CODE

So if we only group along a subset of the index levels , we can use the ` shift ` method of the dataframe to get a rolling difference within each group : #CODE

Hello and welcome to StackOverflow . Please take some time to read the help page , especially the sections named [ " What topics can I ask about here ? "] ( #URL ) and [ " What types of questions should I avoid asking ? "] ( #URL ) . And more importantly , please read [ the Stack Overflow question checklist\ ] ( #URL ) . You might also want to learn about [ Minimal , Complete , and Verifiable Examples ] ( #URL ) .

but when I apply it to a for loop , it shows integer type values . #CODE

You didn't show the result for individual row access with a datetime and I don't have enough data to verify the issue for which STOXX50E.csv or a small portion of it would help . Anyway , have you tried selection with ix ?
Length : 7396 , Freq : None , Timezone : None
Since ' date ' is the index you can use ` loc ` : #CODE
I find ix indexing using datetime strings works when the DataFrame is indexed with the date column Series . For example , given the following data in test.txt #CODE
@USER : Pandas indexing is a nightmare . I just remember ix is for rows because Wes McKinney emphasizes it . " .ix is the most general indexer and will support any of the inputs in .loc and .iloc . .ix also supports floating point label schemes . .ix is exceptionally useful when dealing with mixed positional and label based hierachical indexes . " - from #URL

exec ( compile ( scripttext , filename , ' exec ') , glob , loc )
exec ( compile ( scripttext , filename , ' exec ') , glob , loc )

You can map the index with a lambda function , and set the result back to the index . #CODE

It is clear that ` table1 column1 ` and ` table2 column 3 ` are the columns in common . I want to join these 2 tables using ` pd.join ` but the problem is that my tables do not have a ` header ` . So how can I do this using ` pandas ` ?
You've not stated what the desired df should look like , even if you did join on first and third column respectively , what do you expect to do with the remaining columns ? Please show desired df
I want to join the 2 tables and export them as a txt file . So basically I want to add all the columns together in 1 table . This is the first time I use pandas . Excuse me if I am not being clear enough . I am not interested in a header for the output table .
I'd set the index to the ordinal columns that you want to merge on , then merge , rename the index name as you need to reset the index afterwards : #CODE

Replace duplicated values with a blank string

Having the data formatted like this will allow me to easily work with pivot tables and carry out analysis .
From there I find the pandas package extremely useful for the types of manipulations you require , Setting multi-indices allow for easy reshaping ( through stack ) . #CODE
You can further call MultiIndex and stack to arrange Price and Quantity to what you want . This isn't exactly clean but it solves your issue . #CODE

You can call ` apply ` with a lambda that calls the vectorise ` str ` methods to slice your strings : #CODE

Looking up values from one csv-file in another csv-file , using a third csv-file as map
I have three files : e_data.csv , t_data.csv and e2d.csv . I want to merge ` e_id ` , ` t_id ` , ` gene_name ` and ` value ` into one file , as represented by desired_result.csv . The naive approach is as follows :
Merge them all to one file .
After you load all the csvs using ` read_csv ` you can just iteratively ` merge ` them so long as the column names are consistent : #CODE
The above works as by default it will try to merge on matching column names and perform an inner merge so the column values must match on lhs and rhs .
Thanks for the quick reply ! I get an empty DataFrame when trying to do t_data.merge ( e2t.merge ( e_data )) . But the column names are the same as in your answer . Merging e2t and e_data works , however . I'll look into the documentation on merge , perhaps I'm doing something wrong .
I'd look at the output of each merge , also you don't necessarily need to merge the entire df to debug this , ie . ` e2t.iloc [: 5 ] .merge ( e_data.iloc [: 5 ])` will merge just the first 5 lines
For some reason , e2t and t_data won't merge in my example , which is identical to the data in the question . I shouldn't matter that the the position of the t_id column is different in the two files , right ? On my real data it works , however , and I have to say , it is lightning fast compared to my pure python solution .
Do all the entries in the DataFrames have to be the same data type ? If I'm changing " Gene1 " etc to integers ( or , conversely , the IDs to strings ) , the merge goes fine .

Python Pandas Group by the same value and replace with the mean

pandas - resample - upsampling before downsampling
My objective is to resample this data frame with a fixed time window ( e.g. : 1 second ) using last for regularization when upsampling and the mean for downsampling .
Is this possible at all using pandas resample function ?
You can't mix upsample / downsample in a single ` resample ` operation . I'm not sure why the order of operations would matter to you as long as you get the desired results .
Thanks for your answer , it was not clear to me that you had to make multiple calls to resample .

Sorry are you looking to ` concat ` : ` merged = pd.concat ([ df , df1 ] , axis=1 )` ?

You can't shift the index , so you first need to reset it . Then use a ` loc ` operation together with testing both up and down one shift . Remember to set your index back to the original . #CODE

which is fair . However , I would like only to replace the values with ` NaN ` if these conditions were met by a maximum of 2 consecutive entries ( so I can interpolate later ) . For example , I wanted the result to be #CODE
Finally a new bool condition is used to assign missing values to those groups with 2 or less values meeting the condition .

Note that when you multiply two DataFrames together , Pandas aligns the rows based on the index . This takes time . If the values are already aligned , then you could drop down to NumPy and use #CODE
This is quicker since there is no need to align rows based on indices first . #CODE

Whilst this answer is correct we should actively discourage using ` apply ` where a vectorised solution exists , of course if the version of pandas is so old that the ` .str ` methods don't exist then this would be a valid answer

Depends on what you're trying to do here ? Do you need to read the whole thing into memory ? For instance you can pass a ` chunksize ` operate on the chunk and write to csv and on each iteration append to the csv

I want to first convert column ` ID ` to a string , since I'm assuming that is how I would want the column to move on to the next step . Next step would be to basically opposite of a pivot table the dataframe ` df ` to look like below .

I wrote the function that will compare those 2 strings and return True or False , the problem is I fail to see how to apply / applymap to the consecutive rows : #CODE

This will result in a number of redundant comparisons , so you should think if there are relationships in your data that can simplify the match . For instance , if each ref matches to at most one id or if you can somehow sort both DataFrames in a meaningful way and merge them recursively .

First ` NaN ` are converted to ` 0 ` , then apply function above and it return NaN instead of problematic values . So you have to find rows with NaN values and return subset of original ` df ` . #CODE

Please show your code . Explain what exactly in your code is not working . Post full error messages that you are receiving from your stack trace . Also , read this : [ How to ask ] ( #URL )

Note that ` copy() ` is required if you wish to later apply changes to that new dataframe ` dfa ` . Otherwise , if I remember correctly , you would be applying changes by pointer , much like when using dictionaries .

Conditionally concat a dataframe in python using pandas
I want to conditionally merge them to a new dataframe
You could ` outer ` merge on ` A ` and ` C ` columns #CODE
Replace column ` A ` nan values from column ` C ` #CODE
then replace ` B ` values where ` C ` values are not null . #CODE

You can use ` ix ` to access by index : #CODE
I want to map all strings to integers , so I want to iterate over rows and columns and replace all strings with integer values
For example there is a ' marital status ' column , its field can have ' single , married , divorced ' status , so if a field contains single , I would map it to 0
I added the updated for loop . Your problem formulation is completely different from the original question . You should use masking to replace desired value .
The problem with masking is that there are too many unique values and columns and I'd have to hard code all of them , this is why I want to use a loop to replace values .

( It is necessary to convert the list ` item.split() [: 2 ]` back to a string using ` join ` , because lists cannot be used as set elements , which is required here to make them unique . )

What this will do is set ` timestamp ` to your index . ` inplace=True ` will prevent you having to do ` df =d f.set_index ( ' timestamp ')` and by default it'll drop the column . #CODE

My objective is to merge row by row every list of words or sentences .
Just change the join to : #CODE

Pandas : merge two dataframes ignoring NaN
If you rename the columns of your second dataframe you can then use the concat and groupby like this : #CODE

Going by the idea in the answer for this question - merge 2 dataframes in Pandas : join on some columns , sum up others

Function np.unique with parameter ` return_index=True ` return unique indexes of array . But I need indexes inverted , so firstly I inverted array and then subtracting them by index . Function apply cannot access to index link , so it is count from length of columns ` colD ` minus 1 . Last values of column ` colB ` are summed by indexes from list of column ` colF ` . #CODE

For starters if you just did this ` df3.loc [ df [ ' field '] == ' a ' , ' field '] = ' 111 '` and so on you wouldn't need to iterate at all just call ` loc ` like above 4 times

Merge two pandas dataframes based on depth range
I would like to merge two dataframe on columns Id and top_depth and bottom_depth .
Then merge to get this : #CODE
In your question you asked for a left join . These rows are more like a right join . Let me know if this is close to what you're after and I can try to help more .

You can see that the ` apply ` approach is not working : #CODE
what version ? your apply code worked for me in ` 0.16.2 `

Append a tuple to a dataframe as a row
I want to extract all the month , year combinations and put that in a new dataframe . Issue : When I iterate over the grouped object , month , row is a tuple , so I converted the tuple into a list and added it to a dataframe using thye append command . Instead of getting added as rows :

If you can use len ( column [ 0 ]) it's trivial for you to iterate over it yourself and perform .join
TypeError : can only join an iterable
You can use ` Series.str.join() ` and give the delimiter to join by as argument . Example -

If you have 2 commas , each row would shift by 2 columns .

Pandas Left Merge with xlsx with CSV producing null value columns in output
The column of interest for the merge in this dataframe is ` REFERENCE_ID ` with #CODE
The columns to merge on are ` ID_y ` and ` REFERENCE_ID ` ( in the data dataframe ) . The columns in Lookup contain unique value counts ( eg . 265926639 etc ) .
To merge Lookup and data on the to key columns ` ID_y ` and ` REFERENCE_ID ` , respectively .
Left merge without reindexing
Conditional Merge
In theory , if there are common numbers in the two " key " columns , the data from the data dataframe to match . My hypothesis is either that there is a data type conflict or because there are repeating numbers in the data dataframe , that the merge is not taking place . I want to avoid dropping the duplicates or multiple instances of keys in the data column as this may have valid data .
I have also tried to rename the ` ID_y ` columnname to ` REFERENCE_ID ` and perform a merge as follows : #CODE
This produces a ` null ` dataframe ( just columns , but no values ) . I have also attempted to set the index as ` ID_y ` and ` REFERENCE_ID ` and then perform the merge on the index : #CODE
Both your join fields are different data types . ` REFERENCE_ID ` as an object type and ` ID_y ` as a numeric ` int64 ` type . Interestingly , the ` object ` dtype in pandas is a NumPy ndarray which holds pointers to variable-length string items . Also , ` object ` is the most general of the data types when imported data contains both strings and numbers .
Consider converting the join fields to same data types using astype . Of course , strings tend to be most permissive allowing various characters like hyphens .

How to drop duplicates in Pandas DataFrame by checking for a condition ?
I want to drop duplicate rows by checking for duplicate entries in the column ' ID ' , and retain the row which has a value of 10 in column a .
Just filter the values in ` a ` before drop duplicates with : #CODE
I want to drop duplicate rows by checking for duplicate entries in the column ' ID ' , and retain the row which has a value of 10 in column a .

Since my real df used strings as column names , I was having an issue with the columns being reordered when using ` append ` . To resolve this I recorded the original column names as ` df_cols ` and then used ` df = df.reindex_axis ( df_cols , axis=1 )` on the appended dataframe .

How can I set the level values of a Series , either by using a dictionary to replace the values , or just with a list of values as long as the series ?
I guess this is probably much more efficient than my dumb method of just replace every value , value for value ...

How to read multiple files and merge them into a single pandas data frame ?
I want to read multiple files located in the same directory and then merge them into a single pandas data frame .
I tried this approach , however I don't know how to apply ` concat ` inside the for loop . #CODE
You can use list comprehension to create the list of DataFrames to concat and then call ` pd.concat() ` on that list . Example - #CODE

You could do something like this , using ` str.split ` and then ` pivot ` : #CODE
thank you really elegant . had never used pivot

Pandas - how to replace specific values in a Series ?
I am looking for the most efficient way to replace that by numerical values ` [ 1 , 2 , 3 ]` .
Use ` factorize ` to encode a new column : #CODE

The axis arguments of max and div are critical . Also this will only work for number-like columns . I think max will drop the others hence the ValueError . You can [ select_dtypes ] ( #URL ) before doing the max / div .

First , apply ` isinstance ` to determine which elements are floats , then slice your series to get the elements back .
Then just apply ` str ` and you're good . #CODE

I can do the proportion using group / apply : #CODE

Firstly you should normalize your column names , at the moment they contain spaces ( this explains the KeyError you saw earlier ): #CODE

If you can import your data into numpy here is a simple solution using matplotlib and should produce a heatmap similar to what you posted . You will just need to replace the dummy data with your data . #CODE

sorting is slow , it's O ( n*log ( n )) . I think this may also be doing an apply , which is also slow . What's the reason you have to do this ?

replace substring in pandas data frame column
What i am trying to do is to replace each value " ABC D; MEAN " with " ABC D; X-BAR " . Sub string " ABCD " may vary but pattern " ; MEAN " is constant i want to replace . Looked into different options using " replace " method but don't know how to replace sub string only and not whole string . Please advise .
use ` str.contains ` to create a boolean index to mask the series and then ` str.replace ` to replace your substring : #CODE
For a 40,000 0 row df using ` str.replace ` is faster than using ` apply ` : #CODE

I'm going to create separate columns for the N , S , E , W and C values to merge on to another dataset .

I am trying to apply something like this .... #CODE
You can use the boolean condition to generate a mask and pass a list of cols of interest using ` loc ` : #CODE

It's not json , you should replace all ' with " , and remove redundant " , "

After that I can easily count the occurrences per column . But currently I am stuck in achieving one of the two given results . I know it should work somehow with ` apply ` or ` transform ` , but I have no precise idea unfortunately .
From the Rank ( column ) you can cut and get_dummies : #CODE
Now you can join / whatever these with your original DataFrames .

Then I intend to look at each column of each group and impute the nans by the group and column specific statistic e.g median #CODE

How to join two pandas data frames
Now I want to merge these two data frames based on the field ` frid ` . In other words , I want to add column ` comment ` to ` df1 ` . How can I do this ? I know about ` join ` command , but it works differently ( ` df1.join ( df2 )`) .
use ` merge ` and pass the column you want to merge on , by default this performs an ' inner ' merge : #CODE

Here's one way , with ` apply ` and ` first_valid_index ` : #CODE
To get these efficiently you could drop to numpy : #CODE

actually , ideally i would need to check before deciding which row to drop . Lets say i know that there are multiple rows with Id 3 and the corresponding values of Col are #CODE
how can i drop multiple rows from my data frame ?

Then merge it back to the original table : #CODE
You can replace the last line with :
df2 [ " diff "] = df2 [ " followers "] - df2 [ " followers_a "]

You can use ` map ` to do this .
As well as a dictionary , ` map ` can also accept another Series as an argument , or a function .

So somehow I need to create an object with which I can compare that xml document and then reference the variable names in the ' freq ' list . Maybe with pandas ...? But I don't even know where to start .

You can create a dictionary of items you'd like to replace : #CODE
Then , apply ` get ` to the column , returning the original value if it is not in the dictionary : #CODE
Use ` map ` for this , it's cyython-ised : ` df [ col ] .map ( d )`

I have a Pandas DataFrame that contains duplicate entries . Some items are also listed twice or three times . I would like to filter it so that it only shows items that are listed at least n times . In the final table all items should only be shown once . The DataFrame contains 3 columns : [ colA , colB , colC ] . It should only consider colB in determining whether the item is listed multiple times . Note : this is not drop_duplicates . It's the opposite , I would like to drop items that are in the dataframe less than n times .
Seems massively over kill - what about ` df.groupby ( ' a ') .filter ( lambda L : len ( L ) > 2 )` ?

I am graphing +-2 standard deviations from the mean on my graph ( the green points ) . However , I would rather have straight vertical lines on top of each x-mean , because currently there is a connecting shade area between states that is misleading . How can I do this ? I've tried everything and can't figure it out . Thanks in advance . ( Since I'm new to Stack Overflow I can't post an image of my graph unfortunately . ) #CODE

I have the jaccard similarity function defined as ` jaccard() ` I only want to know how to apply it to ` df ` so that I can have this type of representation matrix by the end . Thank You !
So you need apply ` jaccard ` function to ` df ` ? What is input and output of ` jaccard() ` ? Can you use function df.apply or df.applymap ? [ src ] ( #URL ) Or you need create ` jaccard_custom() ` ?

use data from two or more columns when using .map to apply a function
I want to calculate the difference between two dates in format YYYY-MM-DD . When I write df [ " diff "] = df [ " date1 "] - df [ " date2 "] I get a value like 75 days which I can't sort .
How can I write it such that I map to function calc_diff ( date1 , date2 ) where #CODE
I.e. I df [ " diff "] = df [ " date1 "] , df [ " date2 "] .map ( calc_diff )

It is quite a generic question . Say we have a dataframe A composed of data from a database and we do some calculation changing some column set C . We then want to update several database servers with the new information . Instead of needing a full python installation along with pandas and all relevant libraries installed in each machine it would be nice to be able to do something like A.gen_sql ( ... ) and generate an sql ( text ) output of the insert / update statements that would update each server .

I'd suggest you to avoid searching for 1-line solution to you problems . Start with ` matplotlib ` ( ` seaborn ` is just set of advanced tools working over the ` matplotlib `) . For your task , allocate array of subplots ( ` plt.subplots ( nrows= ?, ncols= ? )`) , iterate over ` df ` columns and call ` matplotlib `' s ` hist ` for each pair ` subplot ` + ` column ` .
In order to do that , you need first to " melt " your data , in ` R ` parlour , i.e. make it " long " . Then , you can proceed with plotting . #CODE

I tried converting ` date ` to a ` datetime ` object but still received the same error message , even though ` type ( dt )` shows that its ` type ' datetime.datetime ' ` #CODE
No , a Pandas ` Index ` of object dtype is the catch-all index class used when the index values do not fit a more specialized index type . The ` object ` dtype indicates that the values are arbitrary Python objects . When possible use ` pd.DatetimeIndex ` for datetimes . All Pandas methods that rely on time-based indexing ( such as interpolation , and ` asof `) expects the index to be a ` DatetimeIndex ` .

First , you can use ` Order ` column to sign the change in shares . Then , you can group by ` Date ` and ` Symbol ` and aggregate by summing orders . This would give you a ` Series ` of orders for all unique days and ` Symbols ` traded on those days . Finally , use ` unstack ` to convert the ` Series ` to tabular format . #CODE
Thank you . That looks like a good way to convert my indexed_orders to align with the respective columns in my trades data frame . However , I am still stuck on the point of how to update my trade data frame per these rows . Any ideas ?

Pandas merge returning empty dataframe
The merge works after this .

Thanks . Ideally I'd like to add a ` lambda ` in the ` cumsum ` . So to get to ` y ` I take ` x ` and apply some function .

... and then cast the result of " apply " to a list . #CODE

How can Pandas read_csv() be forced not to stack rows with identical datatypes into an array ?

merge two dataframes pandas
What's happening is you have duplicate values of ` key ` in one or both dataframes . So if ` data ` has ` key1 ` in it 5 times , and ` extra ` has ` key1 ` in it 2 times , then you will have 10 entries for ` key1 ` when you merge the two dataframes on the key column .
Just to make it clearer , the column ` key ` is the same in both dfs . And I am using it to merge the 2 dfs . What you are saying is that the ` column key ` has duplicates of a certain ` value ` so when merged they will be entered twice . Right ? How can I fix it ?
You have to state what * fixed * means to you , have you tried passing ` how= ' left '` to ` merge `
The left join shouldn't make a difference . I think what you actually want to do may be to join on the index , in which case you will have the same number of output rows as input . And to do that , you can just write data.join ( extra )
I have to caution you that I'm not sure this is what you want , ` combine_first ` uses the other df to fill in the missing column values , it doesn't merge on the key as such .

The key difference here is that ` values ` attribute returns a np array , this can be useful when you want to assign the values and ignore the index which the former will generate and will mean that the lhs will try to align on the index , in your case it makes no difference as you're adding the result to ` df [ ' start_date ']` which will have an index anyway
In your case there is no difference , generally calling ` values ` attribute returns a np array of your data , typical cases where this is necessary is when you want the array as plain values without the index / column names as pandas will attempt to align on index and column values .

Is there a way to append to an array without this happening ? Is there a way to tell which operations on the dataframe are computed incrementally other than by doing timeit on them ?
Generally np and pandas perform well when the array is not growing , by repeatedly appending to it you will periodically force it to allocate a new memory block and copy the values which may explain why it borks when you append just a single element . does this post help you : #URL

Unless your data is not exactly as shown in your question . Why not drop non integer / float values ?
If ` NaN ` cannot be in column ` columnname3 ` , you can drop rows by this column : #CODE
In this data , you can drop all rows with ` NaN ` values : #CODE

How do I take the highest probability and append it to the data frame just like I did with the class ?

I have the dtaaframe above , where the index is the column datetime . I would like to decrease the values in the column JD ( by 1 ) from 5th Jan 2000 to 8th Jan 2000 , using the dates ( and not merely row numbers ) . Is there a pandas command to do this ? I have been playing around with apply , but not sure how to use it
Nice and simple , no need for apply .

The ` apply ` function returns a new DataFrame instead of modifying the input in-place . Therefore , in ` remove_leap_JD() ` , the code should be changed to something like : #CODE

You could write your logic as a function and then apply the function to your dataframe using applymap() . Also , note that ' 0 pound ' should probably read ' 0 pounds ' .

I am trying to apply a function which returns the latest or maximum date for a stock ( on which I have collected prices for multiple days ) .

Where , ` values ` contain numpy arrays , ` apply ` , ` lambda x : pd.Series ( x )` on ` df [ ' values ']` #CODE
And , you could use join to extend the columns . #CODE

You need something like ` df [ df [ ' b '] ! = df [ ' b '] .shift() ]` - I'm sure I've seen very similar questions on Stack Overflow , so I'll have a look and see if there's a dupe .

Eventually , I think having the distance matrix as a pandas DataFrame may be convenient , since I may apply some ranking and ordering operations per row ( e.g. find the top N closest objects to object ` first `) .

Then merge all 3 tables so that you will have all 3 columns in the main table : #CODE

Use ` apply ` on column to do ` df [ ' B '] .apply ( lambda x : sum ( map ( int , x.split ( ' , '))))` #CODE

Querying Pandas DataFrame with column name that contains a space or using the drop method with a column name that contains a space
I am looking to use ` pandas ` to drop rows based on the column name ( contains a space ) and the cell value . I have tried various ways to achieve this ( drop and query methods ) but it seems I'm failing due to the space in the name . Is there a way to query the data using the name that has a space in it or do I need to clean all spaces first ?
If I understood correctly your issue , maybe you can just apply a filter like : #CODE
That will work , if I dont get an answer using the drop method will happily accept this as a work around . Thank you Fabio
@USER Why do you need a method using ` drop ` ?
` DataFrame.drop() ` takes the ` index ` of the rows to drop , not the condition . Hence you would most probably need something like - #CODE
Thanks for the answer Anand . Can you explain why the answer by Fabio is better than the drop method ? As I understand it the drop method was developed for this exact purpose ? Dont get me wrong you and Fabio are much more experienced than I am . I am only trying to learn from you and understand why you believe it is a better option .
To me the other method looks shorter . The only advantage I see for ` drop ` method is that it has an inplace argument which can be used to change the dataframe inplace .

If you must keep the original values in " results " you can use another list to " shift " the index ( the value 1 in results [ 1 ] is for your first example of [ 5 , 6 ]): #CODE
thanks for your answer . Wasn't clear enough before . What if i need to get the mean for each one of the subgroups in a very big database and not just one of them ? was trying to do a " for i in range ( len ( results ))" before the " for item in results [ i ]" that you did but not working for me ...

How do I fix this ? I want legend to be outside of plot but visible and not partly cut off .
thanks ! is there a way to center align it ? rather than eyeballing bbox_to_anchor parameters ?

You can use the apply method :
Select the columns in the list using ` loc ` and then use ` .prod() ` across the rows ( by specifying ` axis=1 `) . For example : #CODE

Transpose DataFrame in Pandas while preserving Index column
The problem is , when I transpose the DataFrame , the header of the transposed DataFrame becomes the Index numerical values and not the values in the " id " column . See below original data for examples :
Original data that I wanted to transpose ( but keep the 0 , 1 , 2 ,... Index intact and change " id " to " id2 " in final transposed DataFrame ) .
DataFrame after I transpose , notice the headers are the Index values and NOT the " id " values ( which is what I was expecting and needed )
If I understand your example , what seems to happen to you is that you ` transpose ` takes your actual index ( the 0 ... n sequence as column headers . First , if you then want to preserve the numerical index , you can store that as ` id2 ` . #CODE

A dictionary can only have one value associated with a key , so that syntax won't work . There are two other options that come to mind to get a similar output : you could select the column using brackets , and then pass a list of the reduction operations you want to apply : #CODE

I have a dataframe in pandas with the columns ` Year ` ( int ) , ` Loc ` ( ordered pair of ints ) , and ` Rain ` ( boolean ) . There are many data points of ` Rain ` for each ` Year ` . For example , in the graph , you might see : #CODE
Is there a function that will combine these data points into a single data point if ` Year ` AND ` Loc ` are the same , with ` Rain ` as the sum of all the ` Rain ` points of the corresponding ` Year ` AND ` Loc ` points ?
Do you mean to group by " Year " and " Loc " and show SUM of Rain ? something like the following ? #CODE

I guess it's difficult to know until you try which ufuncs already map from string to their np equivalents

Pandas pivot or groupby for dynamically generated columns

@USER , I read docs , but didn't find an appropriate command , I try merge ([ df1 , df2 ] , on= ' id ') , but it was error :
@USER , it's my mistake , I put df1 and df2 in brackets , but the correct variant is : merge ( df1 , df2 , on= ' id ') . Thanks for support !

What I tried until now : I tried a ` groupby() ` and then ` apply() ` or ` transform() ` however I dont know how to merge the information of two columns ( playerA and playerB ) .

how to concat sets when using groupby in pandas dataframe ?

How can I skip ( don't apply ) the filters that are None ? #CODE

Filtering in pandas - how to apply a custom method ( lambda ) ?
How can I apply ` df [ ' column2 '] .apply ( lambda x : ' str2 ' in x.split ( ' , '))` to #CODE
To apply this , simply use this to filter the DataFrame . Example - #CODE
ops , one more thing . What if the key to filter for ( ` str2 `) is an array itself ? For example : ` ..... apply ( lambda x : [ ' str2 ' , ' str4 '] in x.split ( ' , '))]` ? That won't work , but actually that's what I need -- the filter should be an array . And if any element from [ ' str2 ' , ' str4 '] contains in x.split ( ' , ') then that's True condition .
You can use ` any() ` or set intersection . Like - ` any ( y in x.split ( ' , ') for y in [ ' str2 ' , ' str4 '])` . But that should be very slow . Another method - ` len ( { ' str2 ' , ' str4 ' } & set ( x.split ( , ')) > 0 ` .
sorry , ` len ( { ' str2 ' , ' str4 ' } & set ( x.split ( , ')) > 0 ` doesn't work for me because `' str2 ' , ' str4 '` is in a variable ` filter1 = [ ' str2 ' , ' str4 ']` . And ` len ( {filter1 } & ... ` doesn't work .

How to combine ( merge ) an array of identical DataFrames into a single one ?
How do I merge or combine an array of ` DataFrames ` in pandas ? #CODE
did you try ' result = concat ( dfs )' at the end ?
Pandas DataFrame already has an ` append ` method to merge two DataFrames
NameError : name ' concat ' is not defined

My Questions and / or things I've read about on SO but haven't / am unclear on how to apply :
So it looks like ` nextday ` is already vectorized ( i.e. operating on the whole frame at once ) . Why are you calling it via ` apply ` ?
@USER When I don't call nextday via apply , the calculations don't get applied for days 2-last day . I end up with a bunch of NaN values in the output . I tried it as well using for index , row in d.iterrows() , but that's about the same speed as the apply method . Is there another / different way I can apply the calculations without using apply ?
Can you reduce your problem down to some copy paste-able functions / data ? Likely to to get more help . Your ` nextday ` function doesn't seem to use ` row ` at all , which it doesn't make sense to use via ` apply ` ( which is for row-by-row function application ) .
It's somewhat tough to unpack without expected output , but your your function is already vectorized . e.g. when you have ` d [ ' ET_WL '] -d [ ' infilP ']` that subtracts on all the rows in ` d ` , so there isn't any reason to call it via an apply . In essence what you're doing is : #CODE
Not sure depending on your OS , but on Windows you should be able to just download then replace the user name in ' folder ' with your username , then run . # #folder = ' C :\ \Users\\xxxx\\Downloads\\NextDay\\NextDay\\ '

What I am trying to do now is comparing Column1 of df1 and ColumnA of df2 . For each " hit " , where a row in ColumnA in df2 has the same value as a row in Column1 in df1 , I want to append a column to df1 with the vaule ColumnB of df2 has for the row where the " hit " was found , so that my result looks like this : #CODE
I recommend you to use DataFrame API which allows to operate with DF in terms of join , merge , groupby , etc . You can find my solution below : #CODE
4 ) in the end I execute left join for df1 and dfs obtaining needed result .

You could ` apply ` ` tuple ` on ` axis=1 ` #CODE

IIUC so long as the dtype are datetime64 already then you can just use ` diff ` which will create a timedelta and then call the attribute ` dt.seconds ` : #CODE

@USER you could either set to ` NaN ` as you suggested but you then still need to ` concat ` anyway

Im taking a CSV file that looks to contain ` \r\n ` and writing it ` to_html ` with Pandas . The csv data looks fine in the csv file but when it's writing to html the output is not as expected ( data missing from tables , as though it's been cut short ) and it contains various ` \r\n ` entries .
I believe those are most likely the cause of the problem with the output . The suggestions I have read is to replace the \r\n using the ` replace ` method directly on the dataframe .

How can I replace the inner index label of the df below ? #CODE
nope , doesn't replace the NaN . A bit odd .
Looks like I have to replace the entire index which is very annoying . #CODE

You can then construct and index of the previous round and opposing player for each row and map it to the corresponding action : #CODE

Apply the following function over the dataframe to generate a new column : #CODE
Are you sure you are replacing df with the result of calling ` apply ` ? Apply doesn't change the dataframe inplace , rather returns a copy of it , so you need to store it or else the results are vacuous ... that's the only thing that comes to mind
I have tried setting a new df to the results of apply via ` df2 =d f.apply ( sep_yearmonths , axis=1 )` then ` df2 =d f2.groupby ( ' month ') .sum() `

Sorry , I just asked this question : Pythonic Way to have multiple Ors when conditioning in a dataframe but marked it as answered prematurely because it passed my overly simplistic test case , but isn't working more generally . ( If it is possible to merge and reopen the question that would be great ... )

You can add a secondary axis by specifying ' secondary_y=True ' when you apply your plot function directly on your dataframe .

If you want to append each y in y_test to results , you'll need to expand your list comprehension out further to something like this : #CODE

Can you post the full stack trace ? Thanks .

Pandas ' apply method
You should consider a func that is passed to apply that simply makes some calculations and returns either a scalar or array like structure to avoid ambiguous behaviour , using apply to modify a df in place is not going to work in practice as especially if you iterating row-wise yet wanting to mutate the df row-wise
Generally speaking the answer is that ` apply ` is NOT in place but you made this overly complicated . Generally you would use ` iteritems ` OR ` apply ` , not both . In this case , you have no need to use ` iteritems ` in addition to ` apply ` . In fact , just do this : ` tt.iloc [ 1 , :] * 2 ` or ` tt.iloc [ 1 , :] *= 2 `
@USER : above is a simple example to a much more complicated function . My actual function is not a simple ` multiply by 2 ` . It does other things , and uses the ` index ` of the ` Series ` as an input as well . Really I plan to have a second function which acts on every element in the ` iteritems() ` . Perhaps I should use two ` iteritems() ` instead of ` apply ` .
Generally speaking , don't use ` iteritems ` or ` iterrows ` if you can help it ( and it's rare you really need them ) . You're almost certainly better off with ` apply ` than any of the ` iter ` -options , and there are often better options than apply . I realize it's an artificial example , but still ... don't do it ! And consider posting a more realistic ( but still simple ) example if you want more specific advice .
@USER : Just to follow up , maybe I can iterate over columns and use ` map ` on each element . The only issue is how do I feed in the column name to the function ? e.g. , ` for col in df : df [ col ] .map ( func )` .. I need to feed ` func ` the column name as well .

However it's not coming up right as len ( df ) is ` 1350 ` rather than ` 24 ` since the dataframe as a whole is from 1 day worth of data .

I have found some code here at Stack Overflow . However It is not working for float numbers . I am trying to find 5 local max and min points plus the extrema . Any help would be appreciated . #CODE

Python pandas replace string
This is a typical use case of ` loc ` . When selecting multiple criteria for the row selection , an example of a ` cond 1 and ( cond 2 or cond 3 )` pattern would be ` df.loc [( condition 1 ) (( condition 2 ) | ( condition 3 ) , ' selected columns ']` . #CODE

If the type of elements of ` C ` column is ` list ` , then I believe one method to do this would be to use ` set ` intersection between your list and the elements of ` C ` column using ` Series.apply ` method . Example - #CODE

How to apply tz_convert with different timezones to different rows in pandas dataframe

I am a moderator on Code Review and I have flagged this question for migration to Code Review . All we can do is to wait to see if the Stack Overflow moderators will agree with that .
@USER - The code above requires true division in ` dt = max_time / N ` so that's why it doesn't work on Python 2.7 .

Pandas merge not working properly . Duplicating entries
I am trying to merge two large DataFrames . Here is a toy example : #CODE
Indeed they do line up before the merge over at least part of the key : #CODE
It's my impression that a left merge should not change the number of entries ? Can anyone explain what I'm missing ? Why is this happening !?

My updated answer does already add a header to the file . You might need to add another ` translate ` to parse out any characters you don't wont .

Dataframe has method isnull : #CODE

the order of the ` var1 ` in the ` activity_seq ` column follows the ` dt ` column ( i.e. in chronological order ) . Could someone please suggest how this can be achieved using pandas ?
You can ` groupby ` on ' userid ' and then on ' var1 ' col call ` apply ` and pass ` list ` to create a sequence , you can rename / reset if required . #CODE

Call ` diff ` and pass ` -1 ` to shift upwards a single period : #CODE
Also you should almost never need to iterate row-wise so avoid using ` for ` loops , ` apply ` , ` iterrows ` etc ...

The lines below apply to data where ` INDATUMA ` and ` UTDATUMA ` are of the format 20071231 , e.g. Date parsing seems to work for ` indate ` and ` outdate ` , those values make sense .

This works because we can use ` diff ` to compare the jumps in the index ( after converting to a Series ) , and then if we take the cumulative some of the bools where the difference is greater than 1 , we get a growing index for each group : #CODE

You now need to convert the prices to a DataFrame in order to merge it to your original dataframe : #CODE

I want to drop duplicate rows with respect to column ' a ' with the general rule as take_last = true unless some condition say , c = ' Blue ' , in which case I want to make the argument take_last = false .

I want to change the dataframe to numpy.ndarray with datatype float32 , so I want to drop those column which dtype is object or other type which is not number .

Pandas : Dealing with Boolean in Pivot Table
I'm having a heck of a time trying to figure out how to pivot or groupby the values without having to do this individually for each column .
Not sure if I need to convert the boolean to int , or if I am missing a transform or a map step .
IIUC you can just call ` apply ` and pass ` value_counts ` : #CODE
As @USER has pointed out if you have columns with all ` True ` / ` False ` then it will insert ` NaN ` for the non-existing values in which case you can call ` fillna ( 0 )` like so : #CODE
@USER no diff here , I tend to use ` .ix ` when slicing cols personally

When stacking a pandas ` DataFrame ` , a ` Series ` is returned . Normally after I stack a ` DataFrame ` , I convert it back into a ` DataFrame ` . However , the default names coming from the stacked data make renaming the columns a bit hacky . What I'm looking for is an easier / built-in way to give columns sensible names after stacking .
I stack and convert it back to a ` DataFrame ` like so : #CODE
But looking at the docs it doesn't look like ` stack ` takes any such arguments . Is there an easier / built-in way in pandas to deal with this workflow ?

how can i convert such dictionary to one with levels ; i want to create a dictionary for each id and insert the data inside that dictionary where i can reach data for each unique id individually
maybe make your ID your dict key then map it to a list of tuples ? this will only be good if you don't need to have the attribute names . if you do want to have the attribute names , then map each key to a list of dictionaries with Code , q , and color keys .

Python pandas map dict keys to values
I have a csv for input , whose row values I'd like to join into a new field . This new field is a constructed url , which will then be processed by the requests.post() method .
I tried to map values to keys with a dict comprehension , but the assignment of a key like ' FIRST_NAME ' could end up mapping to values from an arbitrary field like test_df [ ' CITY '] .
which will give you output as follows : ` [ { ' FIRST_NAME ' : ..., ' LAST_NAME ' : ... } , { ' FIRST_NAME ' : ..., ' LAST_NAME ' : ... } ]` ( which will give you a list that has equal length as ` test_df `) . This might be one possibility to easily map it to a correct row .

You can use ` melt ` to reshape from wide to long , like this : #CODE
I would drop ` variable ` column
actually ill need that for a join later on

How to merge two columns as per third column ( ID ) in pandas ?
How to merge these two dataframes in pandas so that the resultant dataframe is like : #CODE
` concat ` with ` axis=1 ` concatenates by the index , leaving none where ther is no overlap .

and I want to melt ` SP1_Lower ` and ` SP2_Lower ` based on ` POLY_KEY_I ` , but I want to retain the associated ` SP1_Percent ` and ` SP2_Percent ` . I can melt it with this : #CODE

pandas DataFrame concat / update ( " upsert ") ?
I am looking for an elegant way to append all the rows from one DataFrame to another DataFrame ( both DataFrames having the same index and column structure ) , but in cases where the same index value appears in both DataFrames , use the row from the second data frame .
This is analogous to what I think is called " upsert " in some SQL systems --- a combination of update and insert , in the sense that each row from df2 is either ( a ) used to update an existing row in df1 if the row key already exists in df1 , or ( b ) inserted into df1 at the end if the row key does not already exist .
@USER well YMMV . but this actually is quite efficient , involves one set op ( on the index ) and 1 take ( the indexing ) , and a single copy ( the concat ) . Eg . 1MM rows , takes 150ms on my machine .

Python Pandas Join : If One Column Does Not Work , Try Others
I'd like to join ( merge ) them in pandas as follows :
Try to merge on ID = ID
a . Merge on Last , First , DOB = Last , First , DOB
merge on ID #CODE
merge remaining entries on dob , first and last #CODE
concat both types of merges :
Thanks ! I'm assuming if I want to continue to join on other columns if there were some that did not match in Last , First , DOB , I would continue the pattern above ( i.e. df5 = pd.merge ( df1.drop ( df4.index ) , df3.drop ( df4.index ) , on = [ next column ( s ) to use for joining ]) ? What if next , I wanted to match on ID in df1 and a new column ( not shown above ) called " ID2 " in df2 ?
The merge method accept the following parameters :
inner : keep the intersection ( that is the default behavior )
So if you want the intersection : #CODE
Regarding your point " a " I would strip the first letter out of the " Last " column and then concatenate it with the two others . #CODE
then I would use the merge method : #CODE

How to filter through pandas pivot table .
I have a pivot table created from pandas ( DataFrame object ) .
Currently , I have multiple indexes and I want to be able to filter through some of them . To clarify this is how the pivot tables looks like .
The pivot table is called pt . I've tried pt.index.get_level_values [ ' pv_area_cost1a '] and I have read the entire page in advanced indexing to no avail . Thanks in advanced .
The ` : ` inside ` IndexSlice ` signifies to select all rows of ` i1 ` . The very last ` : ` inside the ` loc ` function signifies to select all columns in the dataframe ( ` v1 ` and ` v2 `) .

I'm trying to left join multiple pandas dataframes on a single ` Id ` column , but when I attempt the merge I get warning :
I think it might be because my dataframes have offset columns resulting from a ` groupby ` statement , but I could very well be wrong . Either way I can't figure out how to " unstack " my dataframe column headers . None of the answers at this question seem to work .

Python Pandas - Using list comprehension to concat data frames
It is worth noting however , that concat ( and therefore append ) makes a
If you have a loop that can't be put into a list comprehension ( like a while loop ) , you can initialize an empty list at the top , then append to it during the while loop . Example : #CODE
Also , if your data comes naturally as a list of dicts or something like that , you may not need to create all the temporary dataframes - just append all of your data into one giant list of dicts , and then convert that to a dataframe in one call at the very end .

You can use a combination of ` apply ` and this [ answer ] ( #URL ) to achieve this but why is this an issue ?

I want to apply following rules :

Now I want to create a pivot table on this data using #CODE
I am able to successfully create the pivot table with a little change . #CODE
Any suggestion on how could i create pivot table having [ ' msisdn ' , ' prob '] in rows , [ ' desc '] in columns and an indicator variable for presence of desc in values filed would be much appreciated .

What I want to do now is to compare Column1 with ColumnA and append the rows of df2 to the rows of df1 that have the same value in Column1 as df2 has in Column A , so that the result looks like this : #CODE
I was thinking of using pandas .groupby() function and set the columns 1 and A as keys , compare them and then merge the grouped objects where the keys are identical , but I could not find an efficient way to compare the keys of grouped objects of 2 dataframes . Does anybody have a good idea how to do this ?
You can specify which columns to ` merge ` for the lhs and rhs dfs : #CODE

Use ` shift ` to perform the comparison and filter the rows out : #CODE

You can then use loc to slice your DataFrame : #CODE

You could call ` apply ` and pass a lambda and call ` squeeze ` to flatten the Series into a 1-D array : #CODE
I'm not sure if this is quicker though , here we're applying the mask column-wise by calling ` apply ` on the df which is why transposing is unnecessary

So to properly fillback by date I can use groupby ( level=0 ) function . The groupby is fast but the fill function apply on the dataframe group by date is really too slow .

So you want to or all the conditions ? In that case replace & with |

Then I'd apply the ` between_time ` pandas function to filter the dataframe by start and end date given by the ` bounds ` dataframe : #CODE

pandas stack with row numbers
And what I would like in the end is to have certain columns stacked with the row number associated above . Using column selection and stack is very close : #CODE

df.plot() correctly shows the labels , as you say . plot ( df ) was what I was doing , and does not . It seems like the ' label= ' part of the plot ( ... ) call ought to take a list or series or something and apply the values according , but I can't figure out how .

You shouldn't need to specify the dtypes yourself ( datetime values are supported ) . Can you try just ` df.to_sql ( name= ' Table ' , con=engine , if_exists= ' append ' , index=False )` ?
@USER Yes , you are right , I had done that anyway but for some reason thought it would insert them all as TEXT . I should have checked . Thank you .

Bug in pandas resample / TimeGrouper
The first price is ` 1965.00 ` . Now let's create 1-minute bars using pandas ` resample ` method : #CODE
I don't believe all pandas sorts are necessarily stable , so I'd guess that resample is doing some non-stable sorts ? If so , I suppose that you would need to do something similar to what you did and just sort manually on some unique column .

That said , this may be better written as a merge .
Thanks . Thought about doing a merge but this seemed liked fewer steps .

Oh yeah , I hadn't had enough coffee yet . Should have been an outer merge to return all rows , my bad

Edit : Something I tried that doesn't work , prepending a category using concat : #CODE
Depending on your taste , this may be a nicer way to do it still using concat : #CODE

My guess is your patsy encoding might require a more complicated StateToRegionGrouping mapping , and will be more difficult to read and understand . I would just use list comprehension with a dict or pandas to map the states into a region factor , and then use patsy with region as a regular factor variable .

You want to resample , with interpolation for non-integer time points . You'll need some signal-processing or statistical interpolation library . You'll need to decide experiment with which interpolating function you pick .

I assume that you are using 4 columns in your input.csv which contains the value of lat1 , lon1 , lat2 and lon2 . So , after going through the operation , the output.csv file is a separate file which contains all the previous 4 columns as well as the 5th column which is the distance . You can use a for loop to do this . The method that I am showing here reads each row and calculates the distance and append it in an empty list which is the new column " Distance " and eventually creates output.csv . Make changes anywhere necessary . Remember that this works on 4 columns csv file with multiple coordinates value . Hope that this helps you . Have a great day . #CODE

Python pandas conditional replace string based on column values
I merge a temporary dataframe ( ` DF2.set_index ([ ' ColX ' , ' ColY ']) [[ ' ColZ ']]`) into DF , which adds all the values from ColZ where its index ( ` ColX ` and ` ColY `) match the values from ` COL1 ` and ` year ` in ` DF ` . All non-matching values are filled with ` NA ` .
One more thing ( hopefully ): What if I wanted to add the condition : if less than all conditions ( 2 ) are met ( found ) , replace the current value with ' n / a ' ?

In a csv file which I read using pandas , there's a column of type bool but in the string format which is ' F ' or ' T ' . How can I convert it to the real Bool when filtering ? No need to change in the source file , only when filtering : #CODE
I probably should use ` apply ` , but how exactly ?
You can convert that column to a column of ` True / False ` values after reading it from the csv . One method to do that would be to use ` Series.map ` , to map `' F '` to ` False ` and `' T '` to ` True ` . Example - #CODE

You could use floor division ` // ` to drop the last two digits and preserve the integer type : #CODE

Performance is drastically improved by increasing the ` arraysize ` attribute of the Cursor - allowing me to get decent performance out of ` fetchall() ` . pandas ` read_sql() ` takes a ` Connection ` object as input and the cursor is created within the function , therefore it's not obvious to me how I can apply that same setting and still take advantage of the ` read_sql() ` function . Have I missed something ?

time series : resample from hour to quarter . Last rows missing
Does it work if you pass param to ` resample ` ` closed= ' right '` ?

I have two pandas DataFrames and I would like to filter out items that are only listed in the second one . It's similar to a merge , then drop_duplicates but with the twist that it should after that also delete all items in the first table .
This should work , although it's admittedly a litte heavy-handed . Still , you can define any number of columns you want in the ` on ` argument to ` merge ` ( provided the columns are in both data sets , of course ) . #CODE

How to merge column data of the same value and sum its specific data
How can I merge column data of the same value and sum its specific data ( in this case based of the DATE column )

Not sure why but this is giving me 20 columns of data when there should be only 3 . the ' find_peaks ' function creates a list called " peaks " with three columns of data . Surely , when I ' append ' this to a new list and convert to a dataframe , there still would only be three columns in the dataframe ?
You need to concat the list ` df = pd.concat ( result , ignore_index=True )`
Sorry try ` for i in range ( 1 , 31 ): result.append ( pd.DataFrame (( find_peaks ( df1 [ ' R '] , df1 [ ' I {} ' .format ( i )])))` and then the concat should work
I suggest making a list of dfs and then ` concat ` them : #CODE

Merge multiple rows with same value into one row in pandas
So basically I want to merge all rows with the same values in all columns into one row . What is the most decent way to do that in python ?

where start and end are ` df.index [ 0 ]` and ` df.index [ -1 ]` respectively , the ` freq ` param accepts a frequency value which here is 90 days which suits your requirements .
Given that your initial dates have weekends and holidays and that you want your intervals to be 90 actual days , you can use ` asof ` to get the most recent value as of the 90th day interval : #CODE

You want to ` groupby ` gender as well as the age bins . Aggregate using ` sum ` and drop the empty rows ( ` dropna `) to get what you want . #CODE

I've been able to construct the following code ( mostly with the help from the StackOverflow contributors ) to calculate the Implied Volatility of an option contract using Newton-Raphson method . The process calculates Vega when determining the Implied Volatility . Although I'm able to create a new DataFrame column for Implied Volatility using the Pandas DataFrame apply method , I'm unable to create a second column for Vega . Is there a way create two separate DataFrame columns when the function to returns IV Vega together ?

I suppose I could create two dataframes first then join them , but I was wondering if there is a better way .

However , the map ( str ) removes trailing zeros from the values in the series .
Obviously as you can see I'm actually working with a data frame ( but as map is a data series func I thought this would simplify the question ) , if there's a way of achieving this with applymap instead I'd be happy to know .
Awesome thanks , that's worked like a charm ( with map ( to_4d ... ) on both series ) . I tried something like that but couldn't get it to work . Is there any way to use map with a format string something like this :
The two tricks I'm using here are 1 ) using %i in the format string to signify that it's an integer ( %f means a float , but it renders w / o trailing zeros ) and 2 ) the apply function on df . Make sure that axis=1 with that one .

Using argmin instead of ` df3.apply ( coalesce , ... )` is significantly quicker if the DataFrame has a lot of rows : #CODE

Dividing by zero results in a NaN value . You could use ` fillna ( 0 )` to replace the NaNs with zeros : #CODE

I am new to python so this may be a very basic question . I am trying to use lambda to remove punctuation for each row in a pandas dataframe . I used the following , but received an error . I am trying to avoid having convert the df into a list then append the cleaned results into new list , then convert it back to a df .
square brackets / list comprehension around the join give you another boost fwiw :)

Note : you can also do this with map ( which is a little cleaner ): #CODE

Replace missing values in list from second list using python / pandas
or if s1 and s2 are columns in a pandas DataFrame df , then we can use similar logic and the apply function : #CODE

To avoid the warning / error use loc and do these in one assignment : #CODE

But it looks like the proposed solution with " merge " does not really fit my needs , as I have many columns ?
I finally was able to make it work using : New.loc [ index , " Time '] = Hist.loc [ Hist [ ' url '] == row [ ' url '] , ' Time '] .values [ 0 ]

I need to do some data manipulations on a large table . What I do is I read the table in chunks , manipulate each chunk , append the chunk to a new table . But before doing these , I need to know in advance the itemsize of the string columns , which I will use to declare the new table .

and in order to obtain start of the window you may either shift the end of window or alternatively shift the series : #CODE

But my code for doing this assemble-all-at-once is horrific , because I have an arbitrary number of measurements ( upper level ) and statistical_vales ( lower level ) on a given run of the function . Here's a horrible hacked version given 4 upper levels and 2 lower levels on the columns . How could I replace this disgusting code ? #CODE

and of course you could replace ` col+row ` with a call to an arbitrary function ` f ` : #CODE

Unstack or Pivot Only Some Columns
I only want to unstack " COL4 " and " Year " by way of COL1 while keeping COL2 and COL3 in tact . The end result should look like this : #CODE
You can set multiindex by first 3 columns and use unstack with ` level=0 ` .

This produces the end result that I want , however , there is a lot of this data , in many iterations , and since the lists can be of N length ( meaning , date / device type will always have ` len ` 2 , but date will always have ` len ` 1 , whereas country , device and date will always have ` len ` 3 ) , I'm looking for a more efficient / programmatic way of doing this . I thought using JSON normalize would help but I get the error ` TypeError : string indices must be integers , not str ` whenever I try this : #CODE

Geo Pandas Data Frame / Matrix - filter / drop NaN / False values
Then I stack the dataframe , give the index levels the desired names , and select only the rows where we have ' True ' values : #CODE

I am trying to learn pandas but i have been puzzled with the following please . I want to replace NaNs is a dataframe with the row average . Hence something like ` df.fillna ( df.mean ( axis=1 ))` should work but for some reason it fails for me . Am I missing anything please , something I'm doing wrong ? Is is because its not implemented ; see link here #CODE
An alternative is to fillna the transpose and then transpose , which may be more efficient ... #CODE

I'm trying to get the dict key to align to columns , and the value to be a row value , then append each record to the dataframe . So [[ k1 , v1 ] , [ k2 , v2 ] ,... ] turns into a record [ v1 , v2 ] with column headings [ k1 , k2 ]
But if you planning to use such construction to create DF with many rows you should join all { #URL in 1 dict for each record , and append them to list : #CODE

python merge panda dataframes keep dates
I'd like to merge two dataframes together but add in a column based on some logic . A simplified example of my dataframes are below : #CODE
What I want to do is merge both of these dataframes into a single data frame that looks like this : #CODE
I can't find a way to merge them and keep the dates . I also want to make sure the dates are in the correct fields . Its important to note that I'm creating the dates from regex pulled from the file names . I will also be running this script continuously over time so the first seen date will only change when something else changes e.g. the domain changes its name server .
The only way I can think of is to merge them with renamed date columns , then loop over the entire dataframe sorting the dates appropriately but this seems inefficient .

replace df with the name of your DataFrame .
resample ( ' M ') change the frequency of the series to monthly . ( #URL )

As the question asks I have a dictionary of pandas dataframes that I want to save so that I don't have to resample the data next time I start the ipython notebook . I tried something simple which has worked in other cases before : #CODE

When I apply #CODE

i=2 ( drop first two in each sequence ) #CODE

Merge two data frames and select specific columns
I want to merge two data frames by a column Number . Also , I want to save the merged data frame in CSV file , but only predefined columns . #CODE

@USER : This is the realistic code . I cannot post real request for security reason . All other things are copy-paste from my code . By the way the code works without " append " within for loop .

How to merge two tables in pandas , KeyError : ' timestamp , city '
I cant join 2 data frames : #CODE

Pandas : keeping dates in order when using groupby or pivot table
and I wish to create a new DataFrame from this which acts as a pivot table and merges the date fields and sums weighted returns for that particular date to get something like this : #CODE
So I tried to use a pivot table function but I get very confused reading the documentation regarding it's use .
What's wrong with the initial groupby ? Then just sort on the index . Don't think you need the pivot at all here . ( Your pivot by the way specified the name ` Weighted Return ` with spaces while your df had no spaces , explaining the key error )
The error with your pivot table is because you entered the column name as " Weighted Return " ( note the space ) instead of " WeightedReturn " . However , ` pivot_table ` will always return a sorted index , which goes back to your original problem .

I have tried your interesting solution but I got ITypeError for df [ ' for_group_by '] = df.PAS_DATE + df [ ' shift '] : ufunc add cannot use operands with types dtype ( ' O ') and dtype ( ' < m 8[ ns ]')
Simply shift the PAS_DATEs by this timedelta .
The iteration is now being done in the pandas package , in order to map the data into an indexed dataframe .

Related to this : Python Pandas Join : If One Column Does Not Work , Try Others
I keep getting the following error when I apply it to real data ( in which case , data frames are of different sizes ):

Your question is confusing . If ' data ' is a pd.DataFrame and you iterate over range ( 0 , len ( data )) and then add data to your list ' all_info ' , you simply add the whole DataFrame ' data ' i times to the list . Your DataFrame df = pd.DataFrame ( all_info ) is a Frame holding i times the DataFrame ' data ' . That exporting does not work seems logical ...

I have a dataframe with different values in column ` x ` . I want to drop values that appear only once in a column .
Do you want to drop the entire row or replace the value with NaN ?
@USER I want to drop the entire row , but replace the value with NaN also help me .

Is there a way to ` apply ` a function to one column of a dataframe while leaving the other columns fixed ?
If ` apply ` is not done ` inplace ` you still have to make an assignment , so what is the difference ?
@USER my point is that if I want to compose functions ` f1 ` , ` f2 ` and ` f3 ` , the syntax would be ` d.assign ( A=f3 ( d.assign ( A=f2 ( d.assign ( A=f1 ))))` ; agree that this is better than in-place , but I would argue that this is less readable than a " forward pipe " style syntax using something like ` apply `

Apply a value to all instances of a number based on conditions
I want to apply a 5 to any ids that have a 1 anywhere in the number column and a zero to those that don't . For example , if the number " 1 " appears anywhere in the Number column for ID 1 , I want to place a 5 in the total column for every instance of that ID .

You can use ` DataFrame.drop_duplicates ` before grouping , to drop duplicates based on ` Department ` and ` ID ` . Then group based on ` ID ` and then take ` sum() ` . Example - #CODE

IIUC then you can use ` loc ` and pass a tuple consisting of a ` slice ` and column label to access the col of interest at that level : #CODE

Okay , relatively new to pandas and Python , so apologies if my question is extremely obvious . Have gone over all of the pandas docs on merge , join , and concatenate , read through all of the similar questions on Stackoverflow and Scriptscoop , and have watched hours of pandas tutorials on YouTube . But have yet to figure out how to do what I want to do , which seems like it would be relatively easy in pandas .
This post would be too long if I went through all the various combinations of join , merge , and concat functions I've tried , and why they didn't work . I know I'm missing something obvious . Any ideas would be appreciated !
Looks like you want an ' outer ' merge ? #CODE

You could ` groupby ` on ' Area ' and ` apply ` ` list ` : #CODE
If you want to split the values out you can call ` apply ` and pass ` pd.Series ` ctor : #CODE

ax.set_xticks ( range ( len ( poverty.index )))

` mrna.iloc [ ' ERBB2 ']` but that only takes a integer , and doesn't map to string
Thanks , I realized soon after that the transpose method didn't also transpose the indices . Now everything is good . Thanks for your response !

and merge : #CODE

Merge two python pandas data frames of different length but keep all rows in output data frame
I know that the merge is doable through ` df1.merge ( df2 , left_on= ' Column1 ' , right_on= ' ColumnA ')` , but this command drops all rows that are not the same in Column1 and ColumnA in both files . Instead of that I want to keep these rows in df1 and just assign NaN to them in the columns where other rows have a value from df2 , as shown above . Is there a smooth way to do this in pandas ?
What you are looking for is a left join . The default option is an inner join . You can change this behavior by passing a different how argument : #CODE
I think he is actually looking for ` left ` join :)

which gives me an outer join . #CODE

I tried using append , but did not get any output .
Why do you need this ? Especially as you can use [ ` loc / .ix `] ( #URL ) to set with enlargement

The method I have doing some housekeeping before saving to a file included dropping duplicates and that is where this error is being spit out . I tested this by saving the original dataFrame and then just loading it , running the truncate function , and then trying drop_duplicates on the result and I get the same error .

` stack ` pivots a DataFrame into a Series with a hierarchical index which then allows us to use the ` value_counts ` function . That's necessary because DataFrames don't support ` value_counts ` .

Is it possible to append Series to rows of DataFrame without making a list first ?
I have some data I'm trying to organize into a ` DataFrame ` in ` Pandas ` . I was trying to make each row a ` Series ` and append it to the ` DataFrame ` . I found a way to do it by appending the ` Series ` to an empty ` list ` and then converting the ` list ` of ` Series ` to a ` DataFrame `
Is it possible to append Series to rows of DataFrame without making a list first ? #CODE
Tried Insert a row to pandas dataframe
I saw that on " Insert a row to pandas dataframe " link above . I'm trying to mess around with it . Maybe there is something that I'm not doing correctly .

I think what you're looking for is a ` groupby ` followed by an ` apply ` which does the correct logic for each user . For example : #CODE

I know word_tokenize can for it for a string , but how to apply it onto the entire dataframe ?
You can use apply method of DataFrame API : #CODE
For finding the length of each text try to use apply and lambda function again : #CODE

nonzero ( self )
716 bool = nonzero

Merge multiple column values into one column in python pandas
You can call ` apply ` pass ` axis=1 ` to ` apply ` row-wise , then convert the dtype to ` str ` and ` join ` : #CODE
actually it seems to append all values from column2 to X five times , not four as I wrote before ..

I'm always confused by the view vs copy thing in pandas . Essentially I want to give it a condition to drop , and drop inplace . The df.loc [ mask ] .index will give the me indexes to drop , correct ?
It would still be a copy and replace , but I'm curious as to why you avoided iloc

@USER The type of Column2 is a series right now ` ` . @USER Column2 was created like this : ` ResData [ ' AR_Genes '] = ResData [ ResData.columns [ 1:10 ]] .apply ( lambda x : ' , ' .join ( x.dropna() .astype ( str ) .astype ( str )) , axis=1 )` and then merged with the already existing Column1 using a left merge .
A method by which you can do this would be to apply a function on the grouped DataFrame .
Strangely the condition that removed all the ` nan ` did not work . But I could resolve it through putting the output of the condition in a new df and apply the code on that , that worked . Thanks again !

Python DataFrame Join on Specific Column Index ?
I have two Dataframes with different column names I want to join on . I have not found an example of a join with two different names or using a column index . #CODE
In fileDF I have index 7 or DATASET_ID . In lakeDF I have TBL_ID or Index 0 I want to join on . How can I do it when there are two different names / indexs ?
So I've tried both but it seems that it is grabbing all the column names but none of the data ? I've double checked and found that they both have the same unique keys to join on .
yes , so you need to decide how to merge here , you can pass ` how= ' left '` as a param
You can merge on different columns for lhs and rhs , additionally you may need to specify the type of merge in your case as you had duplicate values on the lhs then you can elect to perform a ' left ' ` merge ` : #CODE

Setting pandas boxplot y-limit dynamically

I am trying to use DataFrame from Pandas . How can I append calculated values in a for loop to pandas.DataFrame Sequentially ?.

When I try to drop a single row in a pandas dataframe with datetimeindex , it shifts the index
I have a dataframe with a datetimeindex index . When i try and drop a single row by its index value , the number of rows become N-1 correctly , but the times in the index shift . In fact , a large chunk of rows is chopped from the start , and then a chunk of rows with Nan values is added to the end . The size of this ' chunk ' seems to be my timezone offset in hours * my frequency per hour . Here is a reproducible example : #CODE
I just noticed the shift is by the UTC / Timezone offset ... but the timezone shift on the index appears to have not been dropped .
or just drop the tz and localize where its at ( which is what I think you want ) #CODE

The loc function takes an int not an Int64Index .
If you only want to remove certain rows within matching groups , you can write a function and then use ` apply ` : #CODE
( If you really like your current way of doing things , you should know that ` loc ` will accept an index , not just an integer , so you could also do ` hdf.loc [ group.index , ' remove_row '] = 1 `) .

Anyone know how to apply a method to change it ?
what does apply ( floor ) do here ? I don't really know ..

I have list of indices corresponding to rows that I want to drop from my ` DataFrame ` . For simplicity , assume my list is ` idx_to_drop = [ 1560,150 2 ]` which correspond to the 1st row and 4th row in the daraframe above .
How can I drop rows whose ` .ix() ` match ?

What I don't understand is why ` y_imputed ` returns a numpy array that differs in length from the original len of the column in the original data frame ? The non-imputed numbers are included , so its not a matter of showing " imputed only " . What gives ?

The dataframe is a python object , you can map functions to specific columns . If you are planning on importing to a db , you could use the ` to_sql ` command if you do not plan on doing any transformations . You can also join dataframes together if you want to create one huge dataframe ... again , the main issue here , is that you did not give us enough information to go on

Apply unique twice in groupby dataframe
thank you ! I guess I should have mentioned my real df has about 300k rows so it wouldn't be practical this way . Also , what I'm looking for is a " query " or " method " to apply to the whole df and return the desired subset . e.g. df.method_1 = subset_1 , df.method_2 = subset_2
Next , we merge the country count ( grouped on ` key_1 ` and ` key_2 `) to the dataframe .

The name isn't necessary , I can do away with it . How can you drop the index name once you reassign it ?

I am trying to drop anything with NaN .

Using ` loc ` with a simple boolean filter should work : #CODE
Finally , use ` loc ` ( rows , columns ) to locate rows that satisfy a boolean condition . The ` : ` symbol means to select everything , so it selects all columns for rows 0 and 4 .

I define a function to strip the time from the datetime fieldn ( dCount ) and then create a new column ' date ' #CODE

I'd add a new col using ` date_range ` passing the ` min ` and ` max ` date values , then call ` apply ` on a df with a single column passing param ` axis=1 ` to ` apply ` row-wise , you can then count the number of rows that meet your condition using ` sum ` ( as this will convert ` True ` to ` 1 ` and ` False ` to ` 0 `) and add this as a new column : #CODE
So I'd construct a new df with a date range , you can just call apply on this and ` sum ` the number of rows that meet your condition .
This gives us the count for dates less than or equal to the given one , so we need to shift by one day : #CODE

I'm assuming you want the actual index location ( zero-based ) , you can call ` apply ` on your ' date_time ' column and call ` np.searchsorted ` to find the index location of where in ` bounds ` df it falls in : #CODE
@USER has pointed out that ` apply ` is unnecessary here and of course he's right , this will be much faster : #CODE
No need to use apply here : ``` In [ 40 ]: bounds [ ' date_start '] .searchsorted ( df [ ' date_time '])

yeah some logic are same i just want to learn how to make these simple logic work , so that I can in future easily apply different things . Thanks for replying .
You can use ` isnull ` to determine which entries are ` NaN ` : #CODE

Reading csv's into dataframe , change parts of frame , and append all to a single frame
I have a folder with multiple csv files containing stock data . I want to read all the files into dataframes and drop the data I dont need , and then combine the rest into a single dataframe . I have written some code that works , but it does it in a bad way , with some intermediary steps that I want to skip to make it more effective . This is the code I use now . #CODE
DataFrame.drop will accept a list of labels , no need to drop individually . Why are you writing out to csv then reading in again ? you could do df_list.append ( frame ) at the end of your main loop . Your first loop is slightly unnecessary , just do : filelist = glob.glob ( " * .csv ") , or just change your main loop to : for string in glob.glob ( " * .csv ") :

Using ` iteritems ` on a Series ( which is what you get when you take a column from a DataFrame ) iterates over pairs ( index , value ) . So your ` item ` will take the values 0 , 1 , and 2 in the three iterations of the loop , and your ` frame ` will take the values `' hey '` , ` NaN ` , and `' up '` ( so " frame " is probably a bad name for it ) . The error comes from trying to use the method ` notnull ` on ` NaN ` ( which is represented as a floating-point number ) .
Another way would be to use ` notnull ` on the whole Series , and then iterate over those values ( which are now boolean ): #CODE

You should use apply method of DataFrame API : #CODE
You can find more information about apply method here .
@USER what are you talking about ? you can just do ` df [ ' usids '] = df [ ' uids '] .apply ( set )` , there is no ` inplace ` param for ` apply ` anyway you have to assign the result
@USER apply function take so long ( 19 second ) as my list includes 30,000 uids . Isn't there a better way to enhance performance ?
` apply ` is just a ` for ` loop so this will be slow unfortunately , there isn't a ` toset ` method

want to transpose it using pandas , following is the code I tried : #CODE
Once loaded , you can also simpy use ` source.T ` to transpose the dataframe .

How to apply different aggregation functions to different columns and give the results different names ?

I would simply like to plot a graph which plots Total , Female , Male smoker percentages over the years ... How do I do that ? Do I need to use a pivot table ...?

Use ` loc ` : #CODE

I often use pd.Series.resample() , and am wondering if there is a way to resample interpolate monthly gridded data in the form ( time , lat , lon ) to say ' MS ' ( monthly start ) . I understand the feature is not directly implemented in Panel . Is there a workaround ?
For example , you could resample a xray DataArray object ` da ` to a month start frequency like this : #CODE

How about specifying a chunk size , something like ` iterator=True , chunksize=1000 ` and use concat ? We are not storing a great deal in memory at all so not sure why it is causing a memoryerror . Do you see your memory usage spiking ?
might as well use a list comp , passing a generator to join will still get turned into a list

it looks like you want to pivot ? ` df.pivot ( index= ' ix ' , ' columns= ' key ' , values= ' value ')` ? Also can you post raw input data , code to reproduce your df and desired output and not images

So to be clear , you want to do basically an inner join on the ` DATE ` key in df2 such that it is within the Start / End Date range ?
hmm .. works for me . They key step is the second last . First check that the index on both frames is a datetime index . Then check the output of ` map ( df1.index.asof , df2.index )` . This is the array indicating the groups ( ` df1.index.asof ` is a function which is applied to the index of ` df2 `) . For each date in ` df2.index ` the output should be the latest date from ` df1.index ` before that date .

Notice the only one that does not match ` len ( msgs_df )` is ` / all_msgs_idx2 ` which corresponds to the ` append_as_multiple ( dropna=True )` .

I can easily replace the nan in each group by a call to df.fillna . Right now I am concatenating all the group by objects imputed by the respective column medians say . This is creating a new copy which is Ok but it is taking very long .
But the challenge is that in need to do the replace operation in the original dataframe as I need to do more transformation post replace operation .
use the statistic per group and replace the nans in the original dataframe or produce a new copy .

Please post a specific example of data , code or pseudocode . Do you only want to append one result row , or do you want to append arbitrarily many ? Is there a reason you don't simply construct the new rows in a separate dataframe and append them afterwards ? In general , trying to append to an object while iterating over it is a terrible idea , and prone to errors . It's also very slow .
@USER : show us an example which is closer to your use case . Four conditions would be fine . I still say it should be done by a pandas filter expression . No appends . By " preservers the index " do you mean " row-index " ? Using raw row-indices is a bad code smell that you're doing it wrong ; either insert an explicit id column , or start handling the data by filter expressions , and without referencing row-indices .
You have still shown zero reason whatsoever why you need to append to the dataframe1 . In fact you don't need to append anything , you just directly generate your filtered version .
If you really insist on using iterate+append , instead of filter , even knowing all the caveats , then create an empty summary dataframe , then append to that as you iterate . Only after you're finished iterating , append it ( and only if you really need to ) , back to the parent dataframe .
For the loop I use ` iterrows ` , and instead of ` append ` ing to an empty dataframe , I use the index from the iterator to place at the same index position in the empty frame . Notice that I said ` .5 ` instead of ` = 5 ` or else the resulting dataframe would be empty for sure . #CODE

What about a dict with keys equal to the row counts you want to process and values equal to processing functions . The multiple keys could map to the same processing function and you could easily add additional cases .
Essentially I'm using the `` process_map `` dict as an extensible conditional selector . When the `` row_count `` is 2 you the code selects `` func1 `` from the map to process the data . For other row counts you insert that number as a key and define a new function for the value ( i.e. `` func2 ``) .

Fixing the order of the DataFrame columns returned by ` apply ` :

python pandas insert column
I am writing code to insert a new column in a csv file : #CODE

I did change it before I apply the code , and the error come out
Potentially you could unstack these into columns : #CODE

I have create a table in hive by using HQL and need insert a dataframe to it .

If you can set the type of the codes to strings when you read in the data , then you can replace the lambda function with ' len ' . You might have problems if one of the country code lengths is 3 as NaNs will be interpreted as the correct length , so you will need to replace the NaNs .

Thinking about it , you can remove the transpose and just use axis=1 in the apply . Glad I could help .

You could ` apply ` ` value_counts ` : #CODE
` apply ` tends to be slow , and row-wise operations slow as well , but to be honest if your frame isn't very big you might not even notice the difference .

quickly drop dataframe columns with only one distinct value
Is there a faster way to drop columns that only contain one distinct value than the code below ? #CODE
You can use ` Series.unique() ` method to find out all the unique elements in a column , and for columns whose ` .unique() ` returns only ` 1 ` element , you can drop that . Example - #CODE
You can create a mask of your df by calling ` apply ` and call ` value_counts ` , this will produce ` NaN ` for all rows except one , you can then call ` dropna ` column-wise and pass param ` thresh=2 ` so that there must be 2 or more non- ` NaN ` values : #CODE
Drop the columns that are not in ' keep ' #CODE

The problem is i need to map the correct values back to the correct index which is difficult when the ordinary of dictionary changes .
You can simply convert your ` dictA ` to a DataFrame and then take transpose , to make columns into index and index into columns . Example - #CODE

yes , ` isnull ` will create a boolean series , ` all ` returns ` True ` if all are ` True `

I wrote following codes to transpose the datafile containing digit as well as alphabets : #CODE

How to append new columns to a pandas groupby object from a list of values

Now you have a list of fruit , iterate over each fruit by filtering the df and get the last index label using ` iloc [ -1 ]` and ` .name ` attribute and append this to a list .
Now ` drop ` these labels in the list : #CODE
You could use the apply function : #CODE

Try it now , i didn't use ` loc ` for the the ` append ` earlier .

So far I have followed a very inelegant and na ve strategy and used ` shift ` to see which events occurred in the next periods : #CODE
I think your shift command thing is okay , but that's just me . Anyway , from here you limit to `' group ' == 1 ` and populate the transition matrix . In the end , you divide by the columns to get the transition probabilities . #CODE

Note : When using ` to_html() ` recently I had to remove extra ' \n ' characters for some reason . I chose to use ` Atom - Find - ' \n ' - Replace ""` .

My goal is to append to the data list by iterating over the movie labels ( rather than the brute force approach shown above ) and , secondly , create a dataframe that includes all users and that places null values in the elements that do not have movie ratings .

pandas replace ( erase ) different characters from strings
However , I would like to use a list so I can quickly replace ` high ` , ` school ` , ` / ` , etc .

I tried ` df [ ' ColA+ColB '] = df [ ' ColA '] + df [ ' ColB ']` but that creates a nan value if either column is nan . I've also thought about using ` concat ` .

I have an ` xray.DataArray ` comprising daily data spanning multiple years . I want to compute the time tendency of that data for each month in the timeseries . I can get the numerator , i.e. the change in the quantity over each month , using ` resample ` . Supposing ` arr ` is my ` xray.DataArray ` object , with the time coordinate named `' time '` : #CODE
Since `' time '` is a coordinate in the DataArray you provided , for the moment it is not possible 1 preform resample directly upon it . A possible workaround is to create a new DataArray with the time coordinate values as a variable ( still linked with the same coordinate `' time '`)

Any ideas how to translate this pandas dataframe solver linear problem using PuLP or any other solver approach ? The end-result would be what you see in the image below .

Then you can you set your date column as an index and resample at the 30 minutes frequency to get the mean over each interval . #CODE

You can avoid loops by using ` apply ` #CODE

will give me the dataframe indexed with integer from 0 to len ( myDF ) #CODE
As you can see for ` pandas.read_csv() ` the index_col defaults to ` None ` so you get ` 0 ` to ` len ( DataFrame )` as index , whereas in ` pandas.DataFrame.from_csv() ` , ` index_col ` defaults to ` 0 ` , so it takes first column as default .

Can you try ` df.resample ( ' 5min ')` this will resample your df to 5min blocks , by default it should preserve the values

You can reset the index using ` reset_index ` to get back a default index of 1 , 2 , ..., n ( and use ` drop=True ` to indicate you want to drop the existing index instead of adding it as a column to your dataframe ): #CODE

C :\ Users\user\Anaconda\lib\ site-packages \pandas\core\ groupby.pyc in apply ( self , func , * args , ** kwargs )
Now , you can join this back with the original dataframe and replace the 0 with nan . #CODE
Wait , actually it does not work properly .. I something goes wrong in the merge . I guess it has to do with the indexes , since the index of cntr is ` Column1 ` and the other is ` int64 ` . I'll research how to add an ` int64 ` index to the cntr dataframe .

@USER , as requested , answer updated , you only require to enumerate the lists in data array , give it an index , and add it to each dataframe to append to df . Probably not the most efficient way ( using append ) , but will get your job done .

You may use apply with regex : #CODE
@USER Not a problem :) the . * ( dot star ) means zero or more of any characters ( which is anything really ) and \b [ string ] \b means the whole thing must contains the full [ string ] , take it as startswith - endswith

The various Q's on this subject indicate that there should be a way to solve the different length issue , but so far , I have seen no indication on how to use it for specific time periods . I just need to shift by 12 months in increments of 1 , for seeing the time of maximum correlation within one year .
This is what I get when I correlate with pandas and shift one dataset : #CODE
Btw check into [ pandas apply / ufunc object ] ( #URL ) . You've probably found this already though . You can actually put a numpy function into the pandas apply object . So this could do the trick
Didn't know ` series.apply ` , thanks , that might come in handy later . The issue I am having with all the numpy / scipy methods , is that they seem to lack awareness of the timeseries nature of my data . When I correlate a time series that starts in say 1940 with one that starts in 1970 , pandas ` corr ` knows this , whereas ` np.correlate ` just produces a 1020 entries array full of ` nan ` . I just need to shift for seeing the max correlation within one year .

So where is my error ? I tried iloc and loc too , but result is the same .

Stack and Pivot Dataframe in Python
I have a wide dataframe that I want to stack and pivot and can't quite figure out how to do it .
What I would like to do is pivot the table so that the column values for Org are the Column names and the column values for each name are the matching values from D1 , D2 and D3 and finally have Topic as the index . Is this even possible ?
EDIT : As Randy C pointed out , if I use pivot I can get the following ; #CODE

I am having some issue with getting my data onto a map with Basemap and having those points change in color . I have read many different things online about how to do this , but I still get a map with no points . Here is my code : #CODE
My problem is that my shape file generates a map just fine , but I don't get any points on top of it . I want to plot the dataframe columns ` d3data [ ' GrossCounts ']` on top of the map and have the color scale with the ( integer ) value of ` d3data [ ' GrossCounts ']` .
I have checked and confirmed that they are within the bounding box set up in the map .
Sadly , I cannot put the figure in because my rep score is not yet high enough . :( But if you click the flickr link above , you can see it . It is just a map of the roads and what not , but no points on it .
If I look at the link I see a bigger map . I mean try the second block of code in my answer . Just to check that at least that works as it did for me .
And FWIW ... if I just set color= ' k ' , it still does not plot the points on the map itself .
I don't know why I never saw zorder in any of the online examples that I had found , but adding this makes sure that the map itself is sent to the back so the points are brought to the front . Thank you , all for your help !

pandas : pivot table inside multilevel dataframe
I'm trying to pivot table , in order to transform some rows values in columns , so from this dataframe ` df_behave ` #CODE
@USER is right , for MultiIndex , you'd better reset_index and set for the fields he mentioned and perform an unstack . Perhaps you should filter out the unnecessary fields ?
@USER is right , for MultiIndex , you'd better reset_index and set for the fields he mentioned and perform an unstack . Perhaps you should filter out the unnecessary fields ?

You can use ` DataFrame.apply ` with ` axis=1 ` ( to apply the func to each row ) and in that function do your logic . Example - #CODE

Why are you using ` pd.concat ` ? Can you not just select the data directly from ` data ` , like ` data [[ ' UTC ' , ' Lon3 ' , ' Lat3 ' , ' Rain ']]` ? Also I believe ` how= ' any '` is the default so you might as well drop it .

I'd like to convert this ` B ` column to something that I can use more readily . For instance , I'd like to use a ` MultiIndex ` with first the day , and then the time ( so that I can group and aggregate ) . Or I'd like to join the two so that I have a single column that's the full date .
If you just want to join the two , you could join them as strings : #CODE

Python pandas regular expression replace part of the matching pattern
It dropped the words ' Ave ' , ' Rd ' and ' Dr ' from my original street addresses . Is there a way to keep part of the regular expression pattern ( in my case this is ' Ave ' , ' Rd ' , ' Dr ' and replace the rest ?
in default python , i should use ` re.sub ( regex , replace , string )`

Resample function throwing error with Twitter Data
I then try to resample for analysis #CODE

So if we apply this we get a Series with the indices as the c3 keys we want and the values as dictionaries , and that we can turn into a dictionary using ` .to_dict() ` : #CODE

Using Pandas pd.pivot_table to pivot by date
I'm still very new to pandas and python , and I'm afraid I'm doing something foolish here . That said , the closest thing I could find to the problem I'm encountering is here How to create pivot with totals ( margins ) in Pandas ? , so I am asking .
I'd like to pivot this table to get output like this , where I can see things grouped first by account id and then by close date . If an account id has multiple rows with the same close date , I'd like those amounts to be added up in the values column , like this . ( For the record , I'm really only interested in the year , but in trouble shooting I've been trying to simplify as much as possible . ) #CODE
The only problem is that for any account id that has two rows for a date , that data just disappears in the output . Account 10a has 3 rows for 2009-01-01 , but in the pivot table shows 2009-01-01 Nan .
I thought I'd try the same pivot table with margins = True .
It sounds like a group by rather than a pivot table to me - your columns are fixed .
The equivalent with pivot table is : #CODE
Thank you ! I had experimented with both groupby and pivot_table , and it seemed like pivot_table was closer to the end result I wanted . I changed something else earlier in my code , and now the pivot table is producing exactly what I want . Now to figure out why ....

I'm looking for a particular pandas function or idiom for column organization to cut down the code and make this more efficient .
Thanks for this ! I'm getting this error ` KeyError : ' MultiIndex Slicing requires the index to be fully lexsorted tuple len ( 2 ) , lexsort depth ( 0 )'` here ` df = df.loc [: , idx [: , ' Sept ']]` . thoughts ?

How to resample stock tick data to constant value range bars using python ( pandas )
I am relatively new to Python Pandas , I am trying to figure out how to resample stock tick data to constant price range bars ( example 1% change in price should be considered as a bar ) . I have previously used pandas for resampling data to get OHLC data using time with this line #CODE

The following code gives me the plot below , with actual counts , but I could not find a way to convert them into frequencies . I can get the frequencies using ` df.AXLES.value_counts() / len ( df.index )` but I am not sure about how to plug this information into Seaborn's ` countplot() ` .
I tried it using ` vals = ax.get_yticks() ` and ` ax.set_yticks ( vals / len ( df ))` . However , once I do it , all labels end up at the very bottom near the origin , due to the actual y-scale of the plot . Obviously my approach is wrong . How would you do it ?

My ultimate goal is to aggregate the data into 30 minute intervals for each day then take the mean of the actual trip time in each interval . I am under the assumption converting to a datetimeindex i can resample the data under those assumptions . However , if there is a better way to do this please let me know .

Which is the answer I'm looking for . The problem is when I apply this to a ` DataFrame ` with a large dataset it runs slow . Very slow . Is there a better way of achieving this ?

If you are just looking for a well formated string ( your example suggests it ) you can use a label argument to the cut function . #CODE
Then , cut using the ` breaks ` and ` labels ` . #CODE
In your example ` labels ` results in ` [ ]` which leads to the error ` Bin labels must be one fewer than the number of bin edges ` when running your cut method .
ok . In python 3 , zip returns an iterator . Instead of ` joint = zip ( breaks , diff )
` , try ` joint =list ( zip ( breaks , diff ))` and see if that gives you a list of tuples with beginning / end points of each interval .
but you run it with ` joint =list ( zip ( breaks , diff ))` , yes ?

You can apply the ` type ` function to the Series values : #CODE

Given a dataframe with unique rows , where only one row will have a value for any given column , how can I merge the rows into a single row .

Pandas conditional subset for dataframe with bool values and ints

I then iterate through this range of x values which have ` k ` as the step size . At each point , I use ` loc ` to capture ` y ` values located at the selected ` x ` value plus and minus the width . The mean of these selected values are then calculated . These values are used to create the ` result ` dataframe . #CODE

To get back to your desired frame , drop the tracking variable and reset the index . #CODE

but this gives ` NameError : name ' ex ' is not defined ` . These DataFrames can have a lot of rows in them so I'm also concerned that the ` apply ` function might not be very efficient .

` t_df.columns = xrange ( 1 , len ( t_df.columns ) + 1 )` .
When you use ` drop ` on the next line , the field you are dropping is based on the index and not on your df.columns attribute . Not sure that's what you intended .
@USER : you are right . I misunderstand the drop function . It is using the column index , not the column name . I thought when I re-define the columns , then it will go with the column name , not the index . The reason I re-define the column is I want to translate the requirement easier to the code

You want the dates on your left column and all city values as separate columns . One way to do this is set the index on ` date ` and ` city ` , and then unstack . This is equivalent to a pivot table . You can then perform your rolling mean in the usual fashion . #CODE
How can I apply a function for each city or for each column in this " pivot table " ?

I want to replace ` Nan ` with [ ] . How to do that ? Fillna ([ ]) did not work . I even tried ` replace ( np.nan , [ ])` but it gives error : #CODE
You can first use ` loc ` to locate all rows that have a ` nan ` in the ` ids ` column , and then loop through these rows using ` at ` to set their values to an empty list : #CODE

Pivot according to the timezone ; this creates a multi-index with the timezone separated #CODE
Create the groupers that we want to use , namely hours and minutes in the local zone . We are going to populate them according to the mask , IOW . where both sales1 / sales2 are notnull , we will use the hours / minutes for that ( local ) zone #CODE

How to apply a function to each column of a pivot table in pandas ?
You should use ` apply ` function of DataFrame API . Demo is below : #CODE
Put std dev and mean in your table , use dictionary for it : ` some_dict = { ( ' city ' , ' date ') :[ std_dev , mean ] , .. } ` . For putting data in dataframe use apply function .
You have all necessary data for running your check by apply function .
Does the standard deviation and the rolling mean need to be appended to the original data frame for this apply to work ? I think yes bc this is applying the function by row . If yes , then how can I concatenate rolling mean and std dev ? I don't think it's as simple as adding a column because df2 is a stacked table / pivot table .
Figured out concat by pd.concat ([ dfs , you , want , to , concat ] , axis=1 ) .

This works because the iterable returned from a df are the columns which are strings so you can just ` join ` them with your separator .

You can create a pivot table on ` user_id ` and ` category ` , fill ` nan ` values with zero , stack ` category ` ( which makes the dataframe indexed on ` user_id ` and ` category `) , and then reset the index to match the desired output . #CODE

How to join a data frame to each group in a pandas data frame ?
Do I use join / merge / concat / append ?
All I needed to do was replace the print in my code with return . I had originally tried it with no print nor return , which didn't work because then nothing was returned printed ( no output ) . Then I tried it with print , which only prints the dataframe information which isn't in a usable dataframe format . If you replace print with return , it returns a real dataframe .

I am looking at the api and related stack overflow posts but I can't seem to figure it out .

How to pass multiple arguments to the apply function
I have a method called counting that takes 2 arguments . I need to call this method using the apply() method . However when I am passing the two parameters to the apply method it is giving the following error :
I have seen the following thread python pandas : apply a function with arguments to a series . Update and I do not want to use functool.partial as I do not want to import additional classes to be able to pass parameters . #CODE
Modified the question for more clarity . dic is the column value that would come by default via the apply function . The second argument is a new argument that is being passed using logic .
` partial ` is equivalent here , lambda isn't ' better ' in any way : e.g. ` countWord = partial ( counting , strWord= ' word ')` and then ` apply ( countWord )` . And yes , your understanding is correct .

How to apply a concat function to a group by data frame using pandas ?

How to return new data frame when using a apply function on old dataframe ?
How to return new data frame when using a apply function on old dataframe ?
Currently I am not returning anything ! Which is why the output confuses me . Is it a default return with the apply function ? I want to return a new dataframe with only the rows that fulfill the elif statement . However , not even the printing works in the elif so return x in the elif doesn't work either .
This is why you are getting a dataframe of all ` None ` . I do not think you can achieve what you are trying for with ` apply ` , as ` apply() ` with axis ` 1 ` actually runs the function for every row and replaces the row with the returned value ( as you see in your case ) .
Next line does boolean comparison of series ( which does same bool comparison for each of its elements and returns a boolean series back ) , and then it does boolean indexing .

Your comparison func won't work as you've found out , you're trying to compare a scalar with an array . Anyway you can call ` apply ` and pass ` axis=1 ` to process the df row-wise . Convert the dtype to ` str ` so that you can use the vectorised ` str.contains ` with ` any ` to produce a boolean series and use this as the arg for ` np.where ` and return ' yes ' or ' no ' when ` True ` or ` False ` respectively : #CODE

You can groupby on ' Column1 ' and ` apply ` a lambda that calls ` join ` to concatenate all the string values and then if you desire construct a list object from that result : #CODE
not sure why you get the error , note that I'm using a double subscript ` [[ ]]` to create a single column df when calling ` apply ` . You could iterate over each row for that column after the groupby so just take the result of my answer and iterate over ' Column2 '

How to merge two list columns when merging DataFrames ?
When I merge them based on ` date ` as below : #CODE
For merging two lists use ` apply ` function : #CODE
You can add both columns together to get the list you are looking for , and then use ` df.drop() ` with ` axis=1 ` to drop the ` ids_x ` and ` ids_y ` columns . Example - #CODE
Isn't there a way to drop duplicates with ` numpy.unique() ` as it is much more efficient than ` set() ` ?

The values I am using are randomly generate , my question is if there is a method for summing values already to insert as a list instead of random , or do I have to manually go through the list and sum them myself ?

Pandas warning when using map : A value is trying to be set on a copy of a slice from a DataFrame
I did google this ! But no examples seem to be using map , so I'm at a loss ...
If you want to apply some function to some column of dataframe , try to use function ` apply ` function of DataFrame API . Simple demo : #CODE
Aha ... So if I first map , then slice ( so the other way round ) , it works without bitching !

In pandas you can only create a row to the right , unless you do join between two dataframe . Then you can re-arrange however you like . #CODE

If ' 0 ' is a placeholder for a value that was not measured ( i.e. ' NaN ') , then it might make more sense to replace all ' 0 ' occurrences

By passing a dict to aggregate you can apply a different aggregation to the columns of a DataFrame .

replace non Null values in column by 1
I'de like to replace the numerical values that are not NaN by 1 , to have : #CODE
Use ` loc ` for this : #CODE

You could ` apply ` ` pd.Series.nunique ` , and then use that to select : #CODE

The fields are id , start ( date ) , end ( date ) , diff ( the number of days between start and end ) , mindiff ( the min ( diff and the last day x months from start ) .
@ jezreal - Thanks . This looks promising . I will go through it and come back . Would it be more pythonic to use resample for the months ? ( As opposed to dt.month columns
are you saying your way is an improvement over resample ( ' M ') ? Also , this will not work if the dates span multiple years as your assumption to add 1 to month will blow up going from December to January . Any thoughts on that as well ? THanks for the help .

I have a dataframe on which I'm doing a row by row manipulation , I'm currently using iterrows() which I know is slow , and would rather use apply() . However I'm not sure how to go about it with apply ( if at all possible ) .
1 , Shift the ` amp_change ` by one position ( ` shift ( 1 )`)
a quick follow up now that I've looked at it more closely - is the if statement properly being implemented ? Especially the part " abs ( start - index ) < 0.01 "

Inner join to fetch columns from a look up table pandas
I used merge function , to do this : #CODE
first_df has 46 K rows and second_df ahs 56 K rows . but the resultant I got has 18 million records ( so a cross join has happened ) . Any idea where I am going wrong ? I was expecting it will have only 46 K rows with 3 more columns ( destination , businessgroup , business ) having values corresponding to the property value in that row
@USER , I will try to come up with an example . Here , join is not happening the way I want , any reason you can think of as to why this is gng wrong ?

You can apply string methods in a vectorized way using the ` str ` attribute of ` Series ` . To find rows in the `' page_name '` column with some string `' xxx '` you can do #CODE

I have a ` Pandas ` ` dataframe ` , called ` output ` . The basic issue is that I would like to set a certain row , column in the ` dataframe ` to a list using the ` ix ` function and am getting ` ValueError : setting an array element with a sequence . ` My understanding is that a ` dataframe ` element was like a list element , it could hold anything ( string , list , tuple , etc ) . Am I not correct ?

so then I map over this open prices to my frame ? I want that open price to be a value in each row for that date

Now , I would expect pandas to drop the second row ` 3 , 5 ` since it actually is a duplicate . But why is it also dropping ` 7 , 0 ` ?? This makes no sense to me . This phenomenon is causing me to lose thousands of lines of unique data when I scale it up . Am I just not understanding something ? From the documentation it says that " by default use all of the columns " , which in my mind means " only drop a row if all the columns of one row match all the columns of another row " , but clearly this isn't the case .

I recommend using datetime64 , that is first apply ` pd.to_datetime ` on the index . If you set this as an index then you can use resample : #CODE

Attempt 2 : When I try with ` apply ` I almost get what I need : #CODE
Attempt 3 : If I to assign the result of ` apply ` to a new column : #CODE
weirdly , even with ` as_index=False ` the apply version doesn't work ... I kindof think perhaps it should work ( modulo ordering ) ...
Thanks @USER . Sorry , not sure if you saw the second part of my question . I tried using transform and apply without luck .
@USER -Reina seen them thanks , I hope this way works for you . Will have a little think if I can get this with an apply ( but this will always be more efficient ) . :)

I need to get out a numpy matrix with shape ` ( len ( first_index ) , maxlen , num_columns )` .
` maxlen ` is some number ( likely the max of all of the ` len ( second_index )`) or just something simple like ` 1000 ` .
I can do this with ` arr = df.as_matrix ( ... )` and then ` arr.resize (( len ( first_index ) , maxlen , num_columns ))` . Elements in new rows should be ` 0 ` so ` .resize ( ... )` works well .
` df.unstack() ` which would result in shape ` ( len ( index2 ) , maxlen * num_columns )` following your notation ; here ` columns ` will be stored as a ` MultiIndex ` .
Alternatively , you can use ` df.to_panel() ` ; ` Panel ` is a natural Pandas data structure used for 3 dimensions , as in your case . I believe that the shape should be ` ( num_columns , len ( index1 ) , maxlen )` .

How do I drop a table in SQLAlchemy when I don't have a table object ?
I want to drop a table ( if it exists ) before writing some data in a Pandas dataframe : #CODE
The SQLAlchemy documentation all points to a ` Table.drop() ` object - how would I create that object , or equivalently is there an alternative way to drop this table ?
Note : I can't just use ` if_exists = ' replace '` as the input data is actually a dict of DataFrames which I loop over - I've suppressed that code for clarity ( I hope ) .

How to convert recurrent vertical column into rows than stack them together in Python / Pandas ?
I am generating some data vertically at first , but would like to transpose them into row data , then stack them into an array like a Pandas data frame . How do I get a final product of a pandas data frame with 4 columns ( ' fr ' , ' en ' , ' ir ' , ' ab ') and three rows ? #CODE
To implement this I would create a dictionary ( hash ) of the column names each containing an array . Then as I loop through the rows in your file , I'd use the first value to index into the dictionary to get the array and then append the numerical value to that array .

Pandas dataframe column containing list , Get intersection of two consecutive rows
Now I want to use ` len ( filter ( y.__contains__ , x ))` to get no of same elements in two columns which contains ` set ` .
You can do this using ` DataFrame.shift() ` to shift the rows by one column and then rename the ` coulmnis ` column to something else , then reset index and merge the dataframes on ` timestmp ` and then use ` apply() ` on the DataFrame . Example ( in one line ) - #CODE
HI can you explain len ( x [ ' coulmnis '] & x [ ' newcol ']) part ??
It is taking intersection of the set in ` coulmnis ` and set in ` newcol ` for each row and then its length ( set in ` newcol ` is the set from next row ) .
Earlier I was trying len ( filter ( y.__contains__ , x )) for the same purpose but was not able to apply it on df
For ` .apply ` it would become - ` len ( filter ( x [ ' newcol '] .__contains__ , x [ ' coulmnis ']))` . But set intersection would be much faster than this .

One way , is to use ` apply ` and calculate len #CODE

Efficient way to store append rows into dataframe stored in disk .
I need an efficient way to store a dataframe in the disk and append each row ( dic_id and entity freq ) one by one to the dataframe .

I want the ` a ` index to dominate , so if any of the entries under a unique value of ` a ` , in this case ` a == 2 ` , have a corresponding value of ` c ` such that ` c > 1 ` , then drop all entries associated to that value of the ` a ` index .

how to Join 2 columns in numpy when they are list of lists ?
@USER ` concatenate ` is not the solution as it merge the way I don't want to . I want something similar to ` column_stack `

matplotlib boxplot xticks shifting towards y axis
I am beginner to matplotlib and trying to plot a boxplot for my data I extracted using pandas .
I am successful in getting boxplot which looks perfectly like what i want except xtics . These xtics are getting shifted towards yaxis . I dont know why .
And below is my matplotlib boxplot code : #CODE

Transforming DataFrame ( pivot )
After turning a pivot table into a DataFrame , my data looks like this : #CODE
Assuming ` time ` and ` model color ` are forming a hierarchical index ( if they are not , you can create this index easily with ` pd.MultiIndex.from_arrays `) , the simplest solution would be to " unstack " that index : #CODE

When you use that you don't need strip .
You can use ` converters ` to strip the data as you read them in . For this you would need to create a function that does this stripping , and then you would need to pass that into a dict mapping the column to the function .

I have two pandas's dataframes I wish to concat over axis=1 . However , when I try to do something like ` pd.concat ( { ' p_value ' :p _value_df , ' stats ' : stats_df} , axis=1 )` I get an error ( see below ) . What do I do wrong ?
` stats_df.columns ` : ` MultiIndex ( levels =[[ ' gmm ' , ' normal ' , ' regression '] , [ ' covars ' , ' diff ' , ' dof ' , ' expected ' , ' loc ' , ' means ' , ' observed ' , ' scale ' , ' weights ']] ,
` File " C :\ Users\hanans\AppData\Local\Continuum\Anaconda3\lib\ site-packages \pandas\tools\ merge.py " , line 754 , in concat
` pd.concat ` tries to merge ` DataFrame ` s based on its index . Since ` p_values ` has 1 level index and ` stats_df ` has 2 index . Thus index cannot be merged . You could do a reset index and merge based on resulting columns

Typically , you're better off keeping DataFrame columns as simple types rather than lists , dicts , etc . In this particular case , you can pull out specific elements from that list using apply though with something like ` x.apply ( lambda x : x [ 1 ])` to pull the month , but Fabio's answer is better from a data organization perspective .

I want to drop duplicated rows for a dataframe , based on the type of values of a column . For example , my dataframe is : #CODE

However , how do I then replace the original values with those newly rescaled values ?
I would go through unstack to make the multi-index level to be first , and then slice it : #CODE
It's not totally the same output , and only the first version will allow you to change the values ( thanks to ` loc ` and the fact you keep all the index values ): #CODE

I have a dataframe with 5 columns . I printed the count of these 5 columns . 3 of them had 6000 , one column had 4500 rows and one had 4000 rows . I looped through the dataframe to insert rows into a table in database . Code : #CODE

Wel , there might be several rows in ` df2 ` , which satisfy the condition given . If you are sure that there is one and only one row - just do without the ` sum ` , and in this case you also can remove the ` map ` operator

Pandas - Add leading " 0 " to string values so all values are equal len

I want to count and print the number of user names in list b according to list a . For instance : list a len is 700 and list b len is 300 . I want to get a result as below . It means that username1 is listed in list b only once , username2 is not in list b , username699 is listed 5 times in list b , etc .. #CODE
for i in range ( 0 , len ( a ):
b.count ( a [ i ]) I apply this but received this error . AttributeError : ' Int64Index ' object has no attribute ' levels '

you are trying to do an inplace replace . you should do #CODE

This returns a boolean series checking whether the desired value is in each row . Then you can simply replace the relative ` True ` or ` False ` values with 1 and 0 .

I'd like to ` group_by ` this table according to unique combinations of ` id ` and ` timestamp range ` . The grouping operation should ultimately produce a single ` grouped ` object that I can then apply aggregations on . For example :
you might also want to apply integer division to generate time intervals : #CODE
Hi Dima , thanks for your answer . The challenge I'm facing is creating multiple , sometimes overlapping time bins that only apply to specific ` id ` s . If you try to produce the groups from my example you'll see what I mean . As a side-note , pandas has a handy convenience function for grouping on binned values called ` pd.cut ` .
Groupby operates on labels , that is you give each element a label which maps it to a subset . You may not have two labels that map one element to several subsets .
if you really believe it is the only way , you will need to cut your infants into halves , i.e. devise a scheme to duplicate your rows .

I need to upsample a data frame such that there are integer steps ( roughly ) evenly-spaced between the current indices ( which are evenly-spaced to begin with ) . Specifically , I have annual data and I want to resample so that there are 14 rows spaced between each current row . These will later be filled in with interpolation .
What I want is a method to resample so that I get something like this #CODE

Have a look at #URL One issue -- because Pandas Dataframes are a collection of columns , one Series per column , a column can only be converted to numeric or date types if all the rows conform or there is a default value for non-conforming rows .
Also , the conversion problem has been solved before , see [ How to insert pandas dataframe into mysql ] ( #URL )

Insert into Vertica if table does not exist or not a duplicate row
I have written a python script to create a table using a ` create table if not exists ` statement and then insert rows from dataframe into vertica database . For the first time when I run this python script , I want it to create a table and insert the data - it works fine .
But from next time onwards , I want it to create a table only if it does not exist ( works fine ) and insert data only if that row is not contained in the database .
I use both ` insert ` statement and ` COPY ` statement to insert data . How to do this in python ? I am accessing Vertica database from python using pyodbc .
Here in the above code , I am creating table and then inserting into tablename1 and tablename2 using COPY and insert . This works fine when executed the first time ( as there is no data in the table ) . Now by mistake if I run the same script twice , the data will be inserted twice in these tables . What check should I perform to ensure that data does not get inserted if it is already present ?
It isn't clear what you are asking . Specifically what are you trying to figure out how to do in Python ? ` COPY ` ? Or the ` INSERT ` only when data for that row does not exist in the target ? Or both ?
I am looking for both , COPY or Insert only when data for that row does not exist in the target . As @USER asked , I will include the code I am looking for
* " What check should I perform to ensure that data does not get inserted if it is already present ? " * Load into a staging table and use ` NOT EXISTS ` or a ` MERGE `
First I'll mention that ` INSERT VALUES ` is pretty slow if you are doing a lot of rows . If you are using batch sql and the standard vertica drivers , it should convert it to a ` COPY ` but if it doesn't then your inserts might take forever . I don't think this will happen with ` pyodbc ` since they don't implement ` executemany() ` optimally . You might be able to with ` ceodbc ` though , but I haven't tried it . Alternatively , you can use ` vertica_python ` which has a ` .copy ( ' COPY FROM STDIN ... ' , data )` command that is efficient .
Use a control table that somehow uniquely describe the set of data being loaded and insert into it and check before running the script that the data set has not been loaded .
-- Step 2 . If row not found , insert rows #CODE
-- Step 3 . Insert row into control table #CODE
Alternatively you can insert or merge data in based on a key . You can create a temp or other staging table to do it . If you don't want updates and data does not change once inserted , then INSERT will be better as it will not incur a delete vector . I'll do ` INSERT ` based on the way you phrased your question .
-- Step 2 . Insert data . #CODE
-- Step 3 . Insert / select only data that doesn't exist by key value #CODE

Then you can replace actual values as appropriate given the levels of the multi-indices .

Pandas DF : Replace Middle Part of String
I would like to know the most efficient ( fastest computing ) way to replace the " , " between Last and First with " , " ( get rid of just the space before the comma but keep the one after it ) .

One way is to do the plain old merge then throw away the values out of the range : #CODE
Wouldn't it be more efficient to first filter and then merge ?

I'm thinking there must be a smarter or faster way to do this , a mask could have been useful except you can't fill down with this data as ` price2 ` for one row might be thousands of rows after the ` price2 ` for another row , and I can't find a way to turn a merge into a cross apply like one might in TSQL .
df [ ' Exit '] = list ( map ( ATestFcn , df.index , df.Price1 ))

Another problem you will face , is that you reassign ` df [ ' periods ']` at every iteration on ` periods ` ; every changes you make are overwritten by the next iteration . It is much more straightforward to create a column , and assign periods using the ` loc ` method . #CODE

Using ` loc ` like above is recommended . You can also do ` df [ df.RESP == 0 ] [ c ]` , but this chained indexing is not always guaranteed to work if you also want to assign values to that selection .

To make it more generic , compare row values on apply method .
Or , use ` df.iloc [: , 0 ]` for first column values and match it ` eq ` with ` df ` #CODE
if I wanted a ' 0 ' where there is a match and ' 1 ' where is no match , can I just change ` len ( set ( x ))= =1 ` to ` len ( set ( x ))= =0 ` ?
` len ( set ( x )) ! = 1 ` would be more appropriate , I think ?

I have been down the lines of doing a pivot like matt_s suggested below , but didnt come all the way !
You could use a pivot : #CODE

pandas replace zeros with previous non zero value
You can use ` replace ` with ` method= ' ffill '` #CODE

You'd want to first convert the date columns via ` pd.to_datetime() ` and then take the diff ?

You can first merge both dataframes #CODE

Inner Join list of DataFrames on Row Values
I have a list of dataframes in python pandas that have the same rowname and rowvalues . What I would like to do is produce one dataframe with them innerjoined on the rowvalues . I have looked online and found the merge function , but this isn't working because my rows aren't a column . Does anyone know the best way to do this ? Is the solution to take the row values and turn it into a column , and if so how do you do that ? Thanks for the help .
Then to merge : #CODE

will mean you have a DatetimeIndex and can do nifty things like loc by strings . #CODE

I want to compare two pandas dataframes and find out the rows that are only in df1 , by comparing the values in column A and B . I feel like I could somehow perform this by using merge but cannot figure out .. #CODE

Is there a way to get the array to find the years , or a way to shift that first column up by one ?
I have found a way to shift a dataframe column up by one , but I need to shift the grouping , not the dataframe .
I also tried turning the new grouping into a new dataframe so that I can shift the year column up , but haven't been successful .

Use the pivot function : #CODE

it is because of the comma . remove the comma with something like ` replace ( ' , ' , '')` and it should work

This will create a new column with the results . I believe that changing the rows as you pass through them is not a good idea , after that you can simply replace a column and delete the other if you wish .

pandas merge dataframes by closest time
I've got two dataframes ( ` logs ` and ` failures `) , which I would like to merge so that I add in ` logs ` a column which has the value of the closest date found in ' failures ' .

Using Pandas to merge 2 list of dicts with common elements
You should actually use ` groupby ` to group based on ` name ` and ` total_year ` instead of ` apply ` ( as second step ) and in the groupby you can create the list you want . Example - #CODE

To avoid too many tiny slices in a pie chart , I need to merge / sum all elements in a series below a certain threshold . So far this is what I came up with : #CODE
this delivers the correct result , but the use of ` append ` bothers me and does not strike me as particularly pandas-like .

Python Pandas replace string based on format
Please , is there any ways to replace " x-y " by " x , x+1 , x+2 ,..., y " in every row in a data frame ? ( Where x , y are integer ) .
For example , I want to replace every row like this :
In pandas you can use apply to apply any function to either rows or columns in a DataFrame . The function can be passed with a lambda , or defined separately .
( side-remark : your example does not entirely make clear if you actually have a 2-D DataFrame or just a 1-D Series . Either way , ` apply ` can be used )

You can use ` str.cat ` to join the strings in each row . For a Series or column ` s ` , write : #CODE
The ` .str ` accessor only works on a Series or a single column of a DataFrame ( not an entire DataFrame ) . If you want to apply this method to multiple columns of a DataFrame , you'll need to use it on each column individually in turn .
Sure - to apply the method to the ' words ' column , you could write ` df [ ' words '] .str .cat ( sep= ' , ')` ( where ` df ` is the name of your DataFrame ) .

A completely different approach would be to use a dictionary comprehension to map the unique planets : #CODE

okay presently I was trying to use the xlsxwriter engine to open up the workbook and append the dataframe from a particular index which is after the first frame .

right way to use eval statement in pandas dataframe map function
I want to substitute the string with the list which is inside the string . I try to use map , where the function inside map is eval : #CODE
What am I doing wrong ? how can I use eval ( or a vectorized implementation ) instead of the clumsy loop above ?
try to use " apply " instead of " map "
Anzel : thanks a lot for the answer . Unfortunately , I get the same output with " apply " .
Anzel , you are right . but the is_string function should take care of that . after I do map ( is_string ) , the pd.series should only include strings .
That is out of this question scope , but if you have mixed types both non-evaluate and evaluated , then do a map or apply with a function and perform a try / except then you should be good
Using eval is generally frowned upon as it allows arbitrary python code to be run . So you should strongly prefer not to use it if possible .
I agree with ** literal_eval ** for safer measures , although OP's error is probably due to mixed types of both evaluated and non-evaluated strings -- OP's original approach with just " eval " doesn't work neither .
Andy , thanks a lot . literal_eval did solve the problem . I guess that , as a non expert , I still fight a bit with the notion of apply vs map . After filing the nans with strings ( data [ ' organization '] = data [ ' organization '] .fillna ( ' [ ]') , both apply and map on literal_eval did the job . But when is one is one preferable to the other ?
btw , I know that map vs . apply is a complete different question , do not feel the need to reply it . I will some do some research on it .

Fantastic ! Thanks so much . Ironically , I had just been using indexed keys on a standard json import a little earlier , but hadn't thought to apply it to the pandas read :)

I would like to filter ` df1 ` keeping only the values that ARE NOT in ` df2 ` . Values to filter are expected to be as ` ( A , b )` and ` ( C , a )` tuples . So far I tried to apply the ` isin ` method : #CODE
Another option that avoids creating an extra column or doing a merge would be to do a groupby on df2 to get the distinct ( c , l ) pairs and then just filter df1 using that . #CODE

I want to do an inplace replace like this :
replace that value with 1
replace that value with 0
You can also create a function to check your conditions , and apply to the dataframe : #CODE

I want to add a new column which contains values based on df [ ' diff ']
When using ` DataFrame.apply ` if you use ` axis=0 ` it applies the condition through columns , to use ` apply ` to go through each row , you need ` axis=1 ` .
But given that , you can use ` Series.apply ` instead of ` DataFrame.apply ` on the `' diff '` series . Example - #CODE
You can just set all the values that meet your criteria rather than looping over the df by calling ` apply ` so the following should work and as it's vectorised will scale better for larger datasets : #CODE
this will set all rows that meet the criteria , the problem using ` apply ` is that it's just syntactic sugar for a ` for ` loop and where possible this should be avoided where a vectorised solution exists .

You then join this to the original dataframe : #CODE

Inconsistent Nan Key Error using Pandas Apply
I think this may be a bug in apply / map_infer , definitely worth a github issue .

In this case , you need to strip these out first : #CODE

It seems strange to use a lambda that returns a Series in a transform ! ( Rather than use an apply . )
I guess they use the same path , * but * tranform usually means that one value is spread on the group ( e.g. transform ( ' min ')) whereas apply means that the group can return anything . But y'know I'm not sure , that was my understanding .

I have a series of ints , ` s ` , and a list of ints , ` l ` . I would like to construct a new series ` t ` such that ` t [ i ] == l [ s [ i ]]` . ` pandas.Series ` has a method called ` map ` which does this for ` dict ` objects , meaning something like this would work : #CODE
` t = [ l [ s [ i ]] for i in range ( len ( s ))]` ?
@USER no relationship between length of ` l ` and ` s ` , but ` max ( s ) < len ( l )` .
As you already have a list of ints then you can just construct a Series from this and pass this as the data param for ` map ` : #CODE

Change week definition in pandas resample
Currently , when I use the pandas resample function for days to weeks it uses a Sunday to Saturday week , but I'd like it to use a Monday to Sunday week . Is this possible ? I tried using ` loffset ` from the docs , but it doesn't change the data at all .
Can't you just subtract a day and then resample ?
Would it suffice to drop the Saturday rows before the resample ? Edit : ninja'd by EdChum , I think he means exactly the same thing

Test whether the rows are null with notnull : #CODE
You can shift down to get whether the above row was NaN : #CODE
Note : You can shift upwards with ` shift ( -1 )` .
You want to find all the rows that have a np.nan in the next row . Use shift for that : #CODE
Then just drop the columns : #CODE

Then follow the recipe in the comment to your question , and on the columns you want to strip

Standard comment : if you insert an image , no one can copy and paste it -- they'd have to type it in . On the other hand , if you insert * text* , we can use ` pd.read_clipboard() ` to easily reproduce your frame .
Then flatten this with stack ( which adds a MultiIndex ): #CODE

to which I apply pivot_table #CODE

I want to replace all values in ` Late ` that are 46 or 48 with the median of ` Late ` . I believe the command is #CODE
Bear in mind that groupby didn't really apply in your case and that it returns a ` DataFrame ` -ish object
fig.savefig ( " hist " , format= ' pdf ')`
I want to replace all values in Late that are 46 or 48 with the median of Late .

To get a specific cell use iloc ( or loc ): #CODE

UnPivot a Pandas Data Set with Merge
I tried to " stack " the data and reset the index , but this produced an undesired result . #CODE
You're looking for ` melt ` ( aka " unpivot ") : #CODE
Hey Andy , just curious why you prefer melt over setting an index and then stacking ? Is there a particular advantage ? ( my datasets almost always have huge , complex ` MultiIndex ` indexes and columns , so those always feel more intuitive to me .
@USER they do slightly different things . Honestly I rarely use melt , I think it's a bit space-wasteful , I prefer to just leave it as columns . :) That said , I think melt is more efficient than ` df.set_index ([ " N " , " P "]) .stack() ` . For one , melt doesn't drop NaNs ... so slightly different
@USER the melt function worked when i was importing an Excel file but when I tried the same function with a .csv file , the entire values column is " NaN " . Do you know what could have caused that ?

can you resample your dataframe so that it is evenly spaced ?

For the most part , I'm having difficulty figuring out how I can compare a Range1 at row x to all Range2's . I've figured out how to use intersection to determine how much two ranges overlap , and I figure using the length of the intersection output would be a good filter to remove overlapping ranges . However , I've only been able to get ranges compared within the same row . In addition , I'm also wondering what's a good way to create these intersection values for filtering , but keep the rest of the values in the rows .

If you want a string like `' t1t2 '` , in the resulting column , you should use ` str.join ` to join the result and provide that to ` transform ` . Example - #CODE

You mean't to index the cell in the dataframe , with ` irow ` as the index and ` col ` as the column , you cannot use simple subscript for that . You should insteand use ` .iloc ` or such . Example - #CODE
Use ` .iloc ` if you intended for ` irow ` to the the 0-indexed number of the index , if ` irow ` is the exact index of the DataFrame , you can use ` .loc ` instead of ` .iloc ` .

I am trying to add a column to a list where the elements are already being appended . Have seen similar questions on stack , but none seem to solve the issue .

Create an array with numpy that resample values over a year
Can you raw input data , code to recreate your 2 dfs and your attempts that didn't work . For ` resample ` to work the index needs to be a ` DateTimeIndex ` type

I'm looking for method , that iterates over the rows , but apply some method only for every 20th or 30th row values
Actually I try to minimize the number of requests , cause otherwise I have the timeout issue . That's why I tried iterate over the rows , and apply the function of request only for every 20th or 60th row ( cause I have 7000 rows ) and not to speed the process by applying the time.sleep method

then you can apply ` mean() ` to the series : #CODE

I think @USER used ` axis=1 ` because the datetime index is in the * columns* . Can you try to transpose the dataframe ( ` .T `) and then try to resample but without axis=1 ?

Personally , I would like to see that the default behaviour so I wouldn't have to remember to replace all the NaNs in every crosstab calculation .

I am writing a little code to optimize an NFL fantasy draft . Because the database is fairly large , my first task is to cut it down to size by removing entries which I know for a fast will not form part of the optimal draft .
Using ` LEFT JOIN ` you try to match each player with someone better and cheaper .
LEFT JOIN seems to be what I want . It's on the right track , but ends up deleting more rows than it should . I will play with it from there , thanks !

you may want to read the chapters in pandas docs about ` merge ` and ` join ` .
I see . The above method assumes that the tables are aligned ; otherwise , the matching is not well defined ( either there isn't a one-to-one correspondence between records or you get different matches based on sorting order ) . If records are aligned , you can truncate each table by minimal length of the two ; if no alignment is assumed , you should look at ` join ` method .
You want to use ` pandas.merge ` . Read your files into Pandas with ` pandas.read_csv ` and join on the `' gene '` column . Here's the solution to your example . #CODE

I find myself often doing something like the below , starting with a dataframe which has a column of dates in string format which I want to bin by some calendar unit ( days , months , years etc . ) I resort to something like the below because I know ` resample ` only works on a DateTimeIndex series .

Another way of doing this is to put your conversion logic in a function , and to apply this function over the column . #CODE

You're looking for the ` axis ` parameter . Many Pandas functions take this argument to apply an operation across the columns or across the rows . Use ` axis=0 ` to apply row-wise and ` axis=1 ` to apply column-wise . This operation is actually traversing the columns , so you want ` axis=1 ` .

Problem : Given the dataframe below , I'm trying to come up with the code that will apply a function to three distinct columns without having to write three separate function calls .
Then I apply the function to the particular column : #CODE
This works exactly as I want it to for that one column . However , I don't want to have to rewrite this for each of the three different " spend " columns ( 30 , 90 , 365 ) . I want to be able to write code that will generalize and apply this function to multiple columns in one pass .
The ` lambda ` will ensure that only one input parameter of your function is dangling free when it gets ` apply ` d .
@USER sure :) The lambda can only use variables that are explicitly passed to it , so you pass ` col ` and ` day ` to it . It's a lazy thing to name the lambda's parameters this way , probably this would be clearer : ` lambda var1 , var2=col , var3 =d ay : annualize_spend ( var2 , var3 , var1 )` . So you set default values for the * last * two parameters of the lambda , thereby effectively rendering it a single-input function for ` apply ` . Since these are just default values , the lambda could also work in a 2 or 3-input syntax , but ` apply ` only uses a single variable , so it must have at most 1 non-default parameter .
: Wow , thanks so much for the explanation . One more ( general ) question : why pass a lambda into the apply function instead of just the function itself ? That is why df.apply ( lambda row , col=col , day =d ay : annualize_spend ( col , day , row ) instead of just df.apply ( annualize_spend ) ? What efficiency / value is gained from utilizing lambda functionality when the function has already been created ? ( I have seen this approach taken for much simpler functions , and was curious why invoking the lambda was necessary when the function had already been created ) . Thanks again , most helpful !
@USER try it without the lambda :) You should get an error that your function expects 3 arguments , and only 1 is specified , How should your function know what ` col ` and ` day ` are ? The names in function definitions are quite arbitrary , as exemplified by my use of ` lambda row , col=col , day =d ay : ... ` , so the name of the variables in the function's definition can't help in any way ( it only helps the programmer ) . So it's simply because ` apply ` expects a single-input function , which it then applies to the variable .
Okay , I think I understand : the use of a multi-argument ` lambda ` is your way of getting around the single-argument constraint of ` apply ` . And this approach can be generalized when wanting to apply ** any ** multi-argument function with ` apply ` or ` map ` or ` applymap ` . That is , first explicitly create the multi-argument function . Then specify each argument in that function as a variable in the lambda . Finally , complete the lambda by calling the function ( along with each argument ) . Again , many thanks !

Do not use ` read_csv ` in this problem , use some sort of scanner or better streams . First , your data apparently does not conform with a tabular format . Second , even more important , do you have any good reason to read all 2Gb into memory and process later instead of processing this as a stream and redirecting the output somewhere ? Because as soon as you solve the reading problem you will face a memory problem .

The main reason I'm doing this is to be able to join data files and currently , it seems that one table has a key field that is of a different format than the other and I'm guessing that's why the join ( pd.merge ) is failing . I'm going to try stringed integer against scientific notation ( see comment below ) to see if it works .
And yet the join worked , so for my purposes , this solved the problem . Thanks to both of you .

Nice , works ! But I don't understand why the ` columns = len ( group.columns )` is necessary ... Could you explain that ? Thanks !
That line is not necessary . One could as well just have used ` len ( group.columns )` directly . It's just there for better readability in the division .

How to boxplot data after different column values in pandas
What I want to make is a boxplot for each group in ` Country ` , displaying a number of boxes corresponding to the number of unique values in ` Years ` . These boxes should represent the values in ` Column2 ` .
That gives me a boxplot for each Country , but with just one plot in it , representing all the values from that group , not separated by years . How can I get a box for each unique value in ` year ` , representing the values in Column2 in my code ?
Have you seen [ seaborn's boxplot ] ( #URL ) ? It seems like it would do exactly what you want .
I would use the seaborn package , in particular combining the FacetGrid with boxplot .

failed to replace column values in dataframe
I thought about using ` iloc ` ` loc ` but I'm not very strong with this methods , so if you know how better apply them to this case , it could be solution for my problem
I believe the problem is that the iterator returns a copy of the rows , and modifying the copy will not modify the original data . You can use the ` loc ` property to modify a view of the data like this : #CODE
Edit some explanation of ` loc ` :
You can think of ` loc ` as just a way to index rows and columns of dataframes in a flexible way . The syntax is ` df.loc [ row_indicator , col_indicator ]` . The row / col indicator is very flexible : it can be a boolean mask , an index , a slice , a list of indices , etc . It's very similar to the various indexing schemes available in NumPy .
But we don't want the whole row , we just want the `' Direction '` column , so we add this to the ` loc ` call : #CODE
Thank you ! as I supposed the ``` loc ``` is a solution for my problem , but could you explain , please , how it happens that it replaces only non-empty cells of ``` Direction ``` column ( what I needed indeed ) and not all the rows that correspond to ``` df [ ' ID '] == 10 ``` ( what I was afraid of )
I added some more info on how `` loc `` works here . Hope it helps !
The reason the other way works is because the strings are converted to boolean directly : in the standard Python manner , nonempty strings are treated as `` True `` , and empty strings are treated as `` False `` . Check out the output of e.g. `` pd.Series ([ ' ABC ' , '' , ' TRUE ' , '']) .astype ( bool )`` to see this in action .

This is my first time posting on stack overflow , so bear with me . I have been scouring the internet for an entire day and I have not been able to fix this problem .

I'm building a fuzzy search program , using FuzzyWuzzy , to find matching names in a dataset . My data is in a DataFrame of about 10378 rows and ` len ( df [ ' Full name '])` is 10378 , as expected . But ` len ( choices )` is only 1695 .
I'm fairly certain that the issue is in the first line , with the ` to_dict() ` function , as ` len ( df [ ' Full name '] .astype ( str )` results in 10378 and ` len ( df [ ' Full name '] .to_dict() )` results in 1695 .
what is ` len ( df.index.unique() )` ?
@USER using ` choices = dict ( zip ( df [ ' n '] , df [ ' Full name '] .astype ( str )))` , where df [ ' n '] is np.arange ( len ( df )) , worked fine and got what I needed . Had some indexing issues because I was importing the data from different Excel spreadsheets . How do I give you credit for your help ?
This is what is happening in your case , and noted from the comments , since the amount of ` unique ` values for the index are only ` 1695 ` , we can confirm this by testing the value of ` len ( df.index.unique() )` .

How do I use timestamp dates as an index in a pivot table in python ?
I have recently began working with the python.pivot_table and have encountered a challenge using timestamps properly with the pivot tables .
Currently when I use the Date column for ` index =[ ' Date ']` it groups by the day . I would like to option of being able to group by month or year . Is there a way to implement this with pivot tables when the date column are TimeStamp objects ?
Then use those columns to create the pivot table : #CODE

When I create a pivot table from data in Pandas ( python ) , I get an other result than when I create it with Excel . I think this is due to the fact of characters . Someone knows the difference between the pivot table in Pandas and Excel ?
Taking the sum of all these values is not really a pivot table . For the sum you can just use the ` sum() ` method

But when I try and use this function with apply : #CODE
I'm not sure why you have this problem with ` apply ` , but you should not write the function like you did in the first place . Here is a suggestion that avoids dividing two huge numbers one by another : #CODE

To answer the second part , once the dtype is a ` datetime64 ` then you can call the vectorised ` dt ` accessor methods to get just the ` day ` , ` month ` , and ` year ` portions : #CODE

Is conversion in the ` read_csv ` is mandatory ? Otherwise , passing a function which returns ` Series ` to ` apply ` results in ` DataFrame ` . #CODE

You can construct the lists for each continent and ` apply ` a func : #CODE
Or if you have a much larger df then you can just make successive calls using ` isin ` and mask the rows using ` loc ` : #CODE

This looks like a bug in PyTables . The ` UnicodeDecodeError ` exception is raised in ` tables.group._g_check_has_child ` , where it calls ` self._g_get_objinfo ( name )` . When I drop into the debugger at that point ( one frame up from the UnicodeDecodeError ) and decode the UTF-8 string `' df2_\xce\xb2 '` into the Unicode string ` u'df2_ \u03b2 '` , and then call ` _g_get_objinfo ` with _that_ value , it works and returns `' Group '` . You might want to file a bug report to PyTables after checking that the latest dev version has this problem .

if I apply ` .value_counts ` directly to ` groupby ` as #CODE
no , in fact , given the big dataframe I have for ex frequency = 198 instead of 4 for every session , and in some rows of " freq " column I have date values as " 2015-05-22

I want to append 2 dataframes : #CODE
Also tried converting the dataframes into list and then tring to append it . But it gave me the error : #CODE
` merge ` or ` concat ` work on keys . In this case , there are no common columns . However , why not use ` numpy append ` and create the dataframe ? #CODE
You need to change one column name , so ` append ` can detect hat you want to do : #CODE

I have two Dataframes : one with columns " Name " , " Year " and " Type " and the other one with different parameters . There are 4 different types and each one has his specific parameters . Now i need to merge them together .
My problem is now , that the parameters are in a seperate row and not added to row 2 . Is there a chance to merge them together in one row ? Thanks for your answers !
This is by default an inner join .
would merge on column names shared in common by default .
Are you calling ` pd.merge ` in a loop ? ( Otherwise , how do you know it stops after 265 rows ? ) You should be able to merge the two DataFrames with only one call to ` pd.merge ` .... It would help if we could see a minimal runnable example which demonstrates the problem you are seeing .

I have tried creating a new function and using ` groupby ` and ` apply ` , but this works only if rows are sorted . Also it's slow and ugly . #CODE
You could use ` pd.rolling_sum ` with ` window=2 ` , then ` shift ` once and fill ` NaNs ` with ` 0 ` #CODE

Not sure about efficiently but a cleaner method is to call ` apply ` and pass `' , ' , join ` as the func to call : #CODE

IIUC then if you're using pandas version ` 0.17.0 ` then you can use ` merge ` and set ` indicator=True ` : #CODE

no worries , it is indeed simpler than using ix !

This is ok as it is , and with some more work , I can map to results to correct month names , then plot the bar chart . However , I am not sure if this is the correct / best way , and I suspect there might be an easier way to get the results using Pandas .

@USER : I tried it with column name also , but it did not help . My dataframe contains one column which consists of sentences . When i try to apply drop_duplicates() on a column containing 1 or 2 words , if works fine . But not when it come to sentences . Anything that can be done ?
@USER : I tried it with column name also , but it did not help . My dataframe contains one column which consists of sentences . When i try to apply drop_duplicates() on a column containing 1 or 2 words , or on a smaller sample of comments , if works fine . But not when it come to the entire dataset ( about 300 rows ) , it does not work . Anything that can be done ?

OK then you should modify the question to be explicit about what you want to do in case the start / end tags don't align . Your current method ignores repeated start tags , and treats repeated end tags as if they have the same start value . Is this your desired behavior ?

On the append operations pass ` index=False ` ; this will turn off indexing .

How to avoiding skipped months in Python pivot table when all entires in table are 0
I have a pivot table which I sort index by year then month from a pandas dataframe column with timestamp objects . This works great for what I need except for one detail . If there are no entries for a particular month / year that particular row in the pivot table is not populated .
Curent code I have to generate the pivot table is below . Unfortunately I do not know an effective way to share the resulting data , however it's the months in 2016 that are not showing for the index and column #CODE
The link to data @USER is added to the post for reference as well as the code I'm using to generate the pivot table in question . Thanks !

The problem I am running in to is ` df.loc ` is running pretty slow on large DataFrames ( 2-7 million rows ) . Is there a way to speed up this operation ? I've looked into ` eval() ` , but it doesn't seem to apply to hard-coded lists of index values like this . I have also thought about using ` pd.DataFrame.isin ` , but that misses the repeat values ( only returns a row per unique element in ` selection `) .
You can try ` merge ` : #CODE
You can get a decent speedup by using ` reindex ` instead of ` loc ` : #CODE
` loc ` builds the new DataFrame by calling down to

What I would like to do , then , is shift the tract-level population counts on the margins , meeting the following criteria , with the first two being most important ( I realize there are tradeoffs with respect to meeting all of these ):
@USER , I realize there is some conflict / tradeoffs between the requirements but I don't think it is a fundamental one : You can reduce , say , the numbers in age group 4 downwards for a subregion , without fundamentally changing the underlying , within-region map . I.e. tracts with pre-existing concentrations in a certain group should also be those with relative concentrations after the adjustment .

I used ` append_to_multiple ` to append dataframe into 2 tables , and the selector is an datetime column . Just like this . #CODE

A vectorized way to do this would be to normalize the series , and then add ` 12 ` hours to it using ` timedelta ` . Example - #CODE

Pandas TimeSeries resample produces NaNs
I can't post any example data here since it is sensitive info , but I create and resample the series as follows : #CODE
I think [ doc ] ( #URL ) explains it better as me . Instead fillna you can use resample .

I want to use a function from an add-in in excel and apply it to some data i have simulated in python . Is there any modules that can achieve this ?
Unfortunately , this only runs a macro . I need to be able to call the add-in and apply my data indexes there ... something along these lines : = add-in_name ( data_range1 , data_range2 , " GGCV ")

Sadly , if I run this code below I get ` len ( reqsb ) = 21456 ` . But it should be equal to the amount of years which is ` 36 ` .
Where x is length of df.output_gap_pf_sf , a is amount of entries in df.gov_debt_perct_mev under 60 and b is ( len ( df.gov_debt_perct_mev ) - a ) .
To take this further , you could use an [ apply ] ( #URL ) combined with a function to carry out the logic to remove the loop entirely . This would make the code a lot more portable ( and hopefully efficient ) and allow the output to also be a pandas object without any conversion .

When I write values which end with " . " to csv file . The " . " character will be drop automatically . How can I keep the " . " ?

Assuming your dataframe is indexed by ` [ " Sector " , " industry "]` you need first reset_index and then pivot your dataframe and finally make the stacked plot . #CODE

You can then apply ` np.where ` as you did to find the indices where your condition is fulfilled : #CODE

You can try concat this output dataframes to one : #CODE

pandas join data frames on similar but not identical string using lower case only
I need to join data frames on columns that are similar but not identical . Fortunately , the lowercase letters are identical between columns . So I am trying to isolate the lowercase letters from each column , create new columns to join on . #CODE
Note that this assumes collecting all ASCII characters from ` a ` to ` z ` suffices to produce values on which to join .

How to append columns based on other column values to pandas dataframe
I have the following problem : I want to append columns to a dataframe . These columns are the unique values in another row of this dataframe , filled with the occurence of this value in this row . It looks like this : #CODE
When you use ` apply ` , it calls your function once for each column , with that column as an argument . So ` x ` in your NewCols will be set to a single column . When you do ` x [ string ] = list.count ( string )` , you are adding values to that column . Since ` apply ` is called for each column , you wind up appending the values to both columns in this way .
` apply ` is not the right choice when your computation depends only on the values of a single column . Instead , use ` map ` . In this case , what you need to do is write a NewCol function that accepts a single ` Column2 ` value and returns the data for a single row . You can return this as a dict , or , handily , a dict-like object such as a ` collections.Counter ` . Then you need to wrap this new row data into a DataFrame and attach it column-wise to your existing data using ` concat ` . Here is an example : #CODE

When I speak with other fairly novice users like myself , they generally try to extract portions of a " large " dataframe into smaller dfs that are sorted or formatted properly to run applications or plot . This approach certainly has disadvantages in that if you strip out a subset of data into a smaller df and then want to run an analysis against a column of data you left in the bigger df , you have to go back and recut stuff .
My question is - is best practices for more experienced users to leave the large dataframe and try to syntactically pull out the data in such a way that the effect is the same or similar to cutting out a smaller df ? Or is it best to actually cut out smaller dfs to work with ?

Pandas Dataframe - faster apply ?
You should avoid ` apply ` and use ` to_datetime ` : ` df [ ' local_time '] = pd.to_datetime ( df [ ' local_time '])`

How do I add a grand total row at the bottom , as in a pivot table in Excel ?
Only the second question is interesting ;) It's curious how ` groupby ` keeps one of the bins generated by ` cut ` even though the resulting dataframe does not have this particular bin .
To just drop the ` na ` records , you can use the ` .dropna() ` dataframe method . #CODE

I have a dataframe with variables that are coded as integers , which I'd like to replace with their actual value labels .
I could do it using apply and a for loop ( see below ) , but it is pretty clunky . Is there a better way ? #CODE
You can use ` apply ` and use the column's ` name ` attribute to get the key for the outer dictionary : #CODE

I created a dataframe and then used a pivot_table to transpose the data from rows to columns . #CODE

For an irregular time series , ` S ` , and I would like to take points that are at least ` dt ` apart , but without changing their timestamps . For example , consider sampling the following series : #CODE

So to be clear -- Start Date is a potential join field ? You talk about the Start Date as a range ?
Join the two dataframes , convert to datetime and subtract : #CODE
Now , join the ` End Date ` from ` df2 ` to ` df1 ` . #CODE

Essentially I want to merge ` Metrics ` and ` Stage_Name ` :
IIUC correctly then you use ` apply ` with a ` lambda ` : #CODE
Didn't know I can use apply and axis=1 , thanks !

I am able to generate a heatmap with quantity overlaid on the graphic as a visual of a pivot table . I would like to have a column next to the heatmap which shows the summation of rows and I would like to have a row under the heatmap that shows the summation of columns .
Is there a way to incorporate this into the heatmap figure ? ` pv ` is my pivot table which I use to generate the heatmap figure . I would like to have a column on the right of the chart which has the summed values for each row . Likewise , I would like to have a row on the bottom of the chart which has the summed values for each column . #CODE
The pandas pivot table function has a ` margins ` parameter , maybe you want to use that ?

The ` -1 ` on ` searchsorted ` accounts for the shift of ` np.diff ` ; as a minor caveat , this code does not work correctly if there are ` nan ` s before the first ` 1 ` ( although this is easily remedied ) .

Should the DataFrame be used in this way ? I know that dtype object can be ultra slow for sorting and whatnot , but I am really just using the dataframe a convenient container because the column / index notation is quite slick . If DataFrames should not be used in this way is there similar alternative ? I was looking at the Panel class but I am not sure if it is the proper solution for my application . I would hate forge ahead and apply the hack shown above to some code and then have it not supported in future releases of pandas .
This is a strange little corner case of the code . It stems from the fact that if the item being assigned is a DataFrame , ` loc ` and ` ix ` assume that you want to fill the given indices with the content of the DataFrame . For example : #CODE

You can create a list of column names and then iterate through them and apply your logic for them . Example - #CODE
Melt the data frame , then apply the repalce and to lower function . Pivot the data frame to get back

Pandas merge fails on large dataframes
but with large data , the merge fails here my output using large data ( two files , 100mb approx ): #CODE
Using a large file , the merge always set ` hour_y ` and ` phone_2_y ` as NaN , but is not true for all the records .
How does the merge fail exactly ?
You should rewrite your question as : Pandas merge fails on large dataframes

Is there a way to set ' week ' as an index and only calculate the sum on that index ? Or could I resample ' week ' and set sales to zero for all missing rows ?
You can use pivot to create a table which will auto-fill the missing values . This works provided that there is at least one entry for each week in your original data , reindex can be used to ensure that there is a row in the table for every week .
Resample is only valid with ` DatetimeIndex ` , ` TimedeltaIndex ` or ` PeriodIndex ` .
Firstly column ` week ` is set to index . Then df is grouped by column ` product ` and apply reindex by max values of index of each group . Missing values are filled by ` 0 ` . #CODE

I've run into a bit of a sticky problem with pandas merge functionality . Here's a toy example of my issue : #CODE
If I now want to left merge these based on column A in the dataframe and the index in the series , i.e. : #CODE
Do you need to use ` merge ` ? You can assign a series , e.g. ` df1 [ ' E '] = s `
Didn't realise merge needed 2 dataframes . Thanks so much !

I am running a model that outputs data into multiple Pandas frames , and then saves those frames to an HDF5 file . The model is run several hundred times , each time adding new columns ( multi-indexed ) into the existing HDF5 file's frames . This is done with Pandas ` merge ` . Since the frames are different lengths for each run , there ends up being a large number of ` NaN ` values in the frames .

Apply function with args in pandas
You can then apply it straightforwardly : #CODE

I have manage to take out the values I want by doing a simple merge which gives the values I want from table A , given references . However , I would like to use the " isin " -function to make this happened also . I have my syntax according to below , and it gives me duplicate values . The only thing I want is to take out the rows from Table A , given reference from Table B . How can I gear it to do that ?
The strange thing is that when I apply this logic to a bigger table that im working on , I get a " True " for all boolean values , despite me having different " time " -columns and the same ID-number ? Do you know if there are any cases where the boolean expression isnt evaluated over all three included colums , so it only looks on the first column ( " id ")
OK I understand what you're after , you can't use ` isin ` like this because for your data set it returns the rows with values 201511 because this still satisfies the condition that ID is in [ 1 , 2 ] and Time1 and Time2 are also in those row values , you have to use merge to get row matches like you desire : ` df.merge ( df2 )`

I have dataframe with floats as data , and I'd like to normalize the data , so first I convert it into int ( otherwise I have error ` ValueError : Input contains NaN , infinity or a value too large for dtype ( ' float64 ') . `)

And onto that I would like to join #CODE
IIUC then you want to ` pivot ` ` Frame2 ` and ` merge ` this with ` Frame1 ` : #CODE

Yes , thanks . The get_loc returns integer index . I am looking for string index . I can map it back I guess .

use ` str.replace ` to replace a specific character for a column : #CODE

IIUC then you ` groupby ` on ` level=0 ` of your index and ` apply ` a ` lambda ` to ` join ` the values : #CODE

The column name `' Rank '` has a space after it , so a dotted lookup won't work . Use ` nfl_frame [ ' Rank ']` instead , or strip the spaces off the column names .

@USER So I'm new to pandas . What I want to do is store the min_lat , max_lat , etc . variable for each row in roads , and for that road , go through * every * row in events and see if the event's latitude and longitude fit inside that bounding box . If so , I will increment the number of events associated with a road and append it to a new column " num_events " for the roads dataframe .

Issue is with the way you join two data frame , you should do something thing like this , #CODE
Same issue as above describe . In addition , we can use above method to join two data frames .

Firstly , you can create the column ` ID ` from your ` col1 ` , and then drop ` col1 ` .

You could use ` str.contains ` and sum over bool values in list comprehension of ` words ` #CODE
Then use regex operation ` exactmatch = re.compile ( r ' \b ( ? : %s ) \b ' % ' | ' .join ( word_list ))` then ` df.comment.map ( lambda x : re.findall ( exactmatch , x ))` returns lists of words join the list to count .
Regex are usually faster but I'm not sure when map it as function to pandas df ..

One way would be to use ` apply ` by constructing column name for each row based on year like `' w ' + str ( x.year )` . #CODE
you can rename columns names to be the same as year columns using replace #CODE

You can ` groupby ` on ` country ` and resample on week #CODE

sorry to mention that I use python 2.7 , may u translate the code into 2.7 ?

I am facing difficulty with the correct syntax of the if condition . I want to apply the condition to check the equality of a string . But the way i am trying to do it , is giving me an error : #CODE

How to align dfs in pandas
How to join the two so that I have the following : #CODE
So , first I need to align by file column ( and I can do this ) , and then if one line has an id I have to add it to the ID column , if it has more than one , add it to the first token and second one to the second token and so on . I will never have more IDs than token per line .
Note that this will be a left join . You said " I will never have more IDs than token per line . " , but in the example , file_1.3 has 1 ID in df2 and 0 tokens in df . So , I assumed you meant the other way around . Putting how= ' right ' in the join will do it the other way .
I didn't know about the stack function . It is exactly what I was looking for . I really really like this solution . Thank you !

Merge two rows in the same Dataframe if their index is the same ?
I have created a large Dataframe by pulling data from an Azure database . The construction of the dataframe wasn't simple as I had to do it in parts , using the concat function to add new columns to the data set as they were pulled from the database .
This worked fine , however I am indexing by entry date and when concatenating I sometimes get two data rows with the same index . Is it possible for me to merge lines with the same index ? I have searched online for solutions but I always come across examples trying to merge two separate dataframes instead of merging rows within the same dataframe .
It's probably because your other columns are not numeric so cannot be ` sum ` med . You could just merge the result of the groupby sum with your original df but select the missing columns and then call drop_duplicates if required , can you post your actual data or representative data ?

So you just want add hour component , increment it and drop the ` .0 ` correct ?

However , no matter what how = ' xxx ' method in the resample function , the result is not right .
I think the ` freq ` param of ` rolling_mean ` is what you're after but you need a datetimeindex for this to work

I'd recommend using ` pandas ` , specifically the ` resample ` function : #CODE
Then resample over day-long periods , note this defaults to ' mean ' of the samples in each day : #CODE

I am trying to write some code that splits a string in a dataframe column at comma ( so it becomes a list ) and removes a certain string from that list if it is present . after removing the unwanted string I want to join the list elements again at comma . My dataframe looks like this : #CODE
simply you can apply the regex ` b ,? ` , which means replace any value of ` b ` and ` , ` found after the ` b ` if exists #CODE

I don't think you can do an ` agg ` and pass ` list ` as a function , in case you wanted to get count and list in the same op , you'd have to merge both results unfortunately

Merge values split into different columns
I took a look at both pandas functions , ` merge ` and ` join ` , but I could not find the right one for my case .
Is there a way to merge some columns by summing their values and not have to delete the remaining columns afterwards ?
I don't know how to avoid the ` drop ` though .
Great ! What about if the names of the column names differ at all and you want to merge them . Like Ch1 and Ax2 ? Thanks

Padding python pivot tables with 0
I have a pivot table which has an index of dates ranging from 01-01-2014 to 12-31-2015 . I would like the index to range from 01-01-2013 to 12-31-2016 and do not know how without modifying the underlying dataset by inserting a row in my pandas dataframe with those dates in the column I want to use as my index for the pivot table .
Could you provide the code how you generated the pivot table ?
I would try creating a list that has all dates from 01-01-2014 to 12-31-2015 . Let's call this list ` dates ` . I would also create an empty list called ` sales ` ( i.e. ` sales = [ ]`) . At the end of this workflow , ` sales ` should include data from ` dt [ ' Sales ']` AND placeholders for dates that are not within the data frame . In your case , these placeholders will be ` 0 ` . In my answer , the names of the columns in the dataframe are capitalized ; names of lists start with a lower case .
Next , I would iterate through ` dates ` and check to see if each date is in ` dt [ ' Date ']` . Each iteration through the list ` dates ` will be called ` date ` ( i.e. ` date = dates [ i ]`) .
If ` date ` is in ` dt [ ' Date ']` , I would append the ` Sales ` data for that date into ` sales ` . You can find the date in the dataframe through this command : ` df [ ' Date '] == date ` . So , to append the corresponding ` Sales ` data into the list , I would use this command ` sales.append ( df [ df [ ' Date '] == date ] [ ' Sales ']` .
If ` date ` is NOT in ` dt [ ' Date ']` , I would append a placeholder into ` sales ` ( i.e. ` sales.append ( 0 )` .

How to merge two data frames based on nearest date
I want to merge two data frames based on two columns : " Code " and " Date " . It is straightforward to merge data frames based on " Code " , however in case of " Date " it becomes tricky - there is no exact match between Dates in df1 and df2 . So , I want to select closest Dates . How can I do this ? #CODE
call a standard merge on these
Now let's write an ` apply ` function that adds a column of nearest dates to ` df1 ` using scikit-learn : #CODE
Finally , we can merge these together with a straightforward call to ` pd.merge ` : #CODE

this will drop the rows where you have duplicate ID and Value input .

Next I apply data = ` np.asarray() ` on the DataFrame : #CODE

@USER , yes I know I'll have to specify the order manually , I'm just wondering how to apply those specified orders . In reality my dataset is much larger and has several ' questions ' with the same set of responses . For example many Questions that can be ( " Yes " , " No " , " Unsure ") , many questions that can be ( " Not at all " , " A Little " , " A Lot ") , etc . I'd like to specify the orders of these responses and then have them applied to the appropriate questions ( level 0 in the index ) . Does this make sense ?

So question is , how can I join these calculations back to the original dataframe respecting the indexes of df ? #CODE
In the end , I think i figured it out . Had to transpose and then re-assign the original index . Not sure if its the fastest way of doing it but here it goes : #CODE

To track the duplicates I use ` df [ ' duplicate '] = df.duplicated ( ' id ' , keep=False )` However , I would like to keep the ones with the highest ` value ` and either mark or drop the other duplicates . Any suggestions ?

In order to normalize data in a pandas DataFrame I wrote the following functions : #CODE

Why not drop them first ? ` df [ ' name_duplicated '] = df [ ' name '] .dropna() .duplicated() ` ?
@USER I don't want to drop them ( yet ) because I'm also filtering on another column . Only if this is False and another column is True will i want to drop them

How to merge rows with same index on a single data frame ?
You can ` groupby ` on ' A ' and ' C ' seeing as their relationship is the same , cast the ' B ' column to str and ` join ` with a comma : #CODE

Why not just read in a single line , get the list of columns and then drop the last entry ?

If you have multiple conditions besides this example you can use ` apply ` : #CODE

You can use ` loc ` to select all rows that correspond to one label in the first level like this : #CODE

Probably you want to use a merge to bring in the column ` ben ` into your dataframe : #CODE

So I created two dataframes from existing CSV files , both consisting of entirely numbers . The second dataframe consists of an index from 0 to 8783 and one column of numbers and I want to add it on as a new column to the first dataframe which has an index consisting of a month , day and hour . I tried using append , merge and concat and none worked and then tried simply using : #CODE
If the row counts differ , you'll need to use ` df.merge ` and let it know which columns it should be using to join the two frames .
Ok so I tried that and it added it on but it didn't seem to use the index of the first one . I'm new to stack overflow , so how can I post the code in that format so I can show you what it looks like ? I'm using iPython if that makes a difference . If there's no way to do that I'll try to explain what happened . Pretty much , it appeared to add the new column at the very bottom in the form of an array , so once I printed out the dataframe , it looked the same as the original except at the bottom it has the name of the new column , power , followed by [ -47 , -46 , -46 ... ] which are the values

Does color mean anything special or can we treat a combination of item id and color id as a new unique item ? Does the store_min_buy apply to just the one thing or across the sum of all the things you buy at this store ? What does it even mean that there's a min_buy of 9.14 , can I buy 9 and 14% of an item somehow ?
I don't really like this model . The linear phase will abuse that M as much as it can , and probably trivially satisfy the last two constraints by choosing ` u ` tiny but nonzero . That will cause a lot of grief in the integer phase because it means the fractional solution will probably underestimate the cost by a lot .

Python xray - is it possible to append a table ?
Xray doesn't have an append method because its data structures are built on top of NumPy's non-resizable arrays , so we cannot append new elements without copying the entire array . Hence , we don't implement an ` append ` method . Instead , you should use ` xray.concat ` .
For what it's worth , pandas has the same limitation : the ` append ` method does indeed copy entire dataframes each time it is used . This is a perpetual surprise and source of performance issues for new users . So I do think that we made the right design decision not including it in xray .

Thanks , that's a good hint . It works , but I can't pass the first() -function to g.agg ( ... ) , can I ? I would like that better , because I would like to apply many different aggregation functions at once ( amin , amax , first , ... ) . It will be a workaround to use it and then assemble my final dataset manually , I guess .
An alternative is to simply resample and use OHLC ` ( open=first , close=last , high=max , low=min )` #CODE

Pandas drop rare entries

Pandas - How to replace string with zero values in a DataFrame series ?
What's the best way to replace these " $- " strings with zeros ? Or more generally , how can I replace all the strings in a series ( which is predominantly numerical ) , with a numerical value , and convert the series to a floating point type ?

That's not an error , just a representation of the groupby object . You just need to apply an aggregation operation to the object to return a DataFrame or Series . There's more information about this in the docs on groupby .
Group series using mapper ( dict or key function , apply given function

Use ` Series.value_counts ` to count the number of occurrences for each city in ` US [ ' city ']` , and then use ` Series.map ` to apply those counts to corresponding values in ` UK [ ' city ']` : #CODE

Yes and no , I'm working through Canopy . When I try to run the file with %run ( or by just selecting the file from a drop down menu ) , the function has the problem I described .

I posted on stack overflow a few days ago with a similar problem ( which was solved ) , and I'm not sure what the proper etiquette is here , but I'm making a new post .

I would want to append ` 0.813857557031 ` into an array ` res_gamma_init `

replace this #CODE

I have tried using ` apply ` but it is pretty slow : #CODE
` get_dummies ` and other Categorical operations don't apply because they operate on a per row level . Not within the row .
Since Hamming distance doesn't care about magnitude differences , I can get about a 40-60 % speedup just replacing ` df.apply ( lambda x : np.array ([ mapping [ char ] for char in x ]))` with ` df.apply ( lambda x : map ( ord , x ))` on made-up datasets .

This is a bit like a multi-column factorize : #URL

Python Pandas join aggregated tables
I have a data frame with two columns : product_id and rating . I'm trying to join two tables . One obtained from #CODE

How to resample starting from the first element in pandas ?
The timestamps are in milliseconds . I did the following to resample it into 100milliseconds bin time :

Pandas merge pivot_table and a dataframe
how to merge a pandas pivot table and a data frame where the combined column in pivot table is in index and in data frame is in column label
pivot table is #CODE
i had to merge both data frame using pd.merge ( df1 , df2 , on =[ ' bts_name '] , how= ' left ') but i am getting an error

Apply a function to each of the subsequent rows of the dataframe that will give the depreciation relative to the base-year , base-price . This should be put in a set or a list . Used group.apply()
I constructed a separate dataframe with one column as " make " , another as " model " , and a third one " average yearly depreciation " . What this really boiled down to was how to sequentially apply a function to rows of a dataframe . #CODE
Now I can apply a function : #CODE

I'm new to Pandas . I created this pivot table , but I need to figure out how to apply a function within each day on the ' is_match ' values only . See img below for head of data .
could post few records of original data frame before applying pivot .?
Reason is pivot you created made Date as columns so it makes harder to calculate the average for each date . You can deal that with group by .

Using a function with ` apply ` is slower than the list comprehension : #CODE
Thanks ! Really much faster . But maybe it's possible to make it trough pandas apply / map function ?
You can use ` apply ` ( see updated answer above ) . But it is slower due to the function calling overhead .

I don't know of a quick / easy way to convert an h5 file from ` fixed ` to ` table ` format , or to add ` data_columns ` . As far as I know , you would have to read the entire ` h5 ` file into a DataFrame ( or to do so in chunks using the ` chunksize ` parameter ) and then to write it out or append to a different ` h5 ` file in ` table ` format .

Off the top of my head , extract the format of the date , check against the format you want to keep , and drop all records from the dataframe or series that doesn't match your desired format .
Do you want to remove the 00:00 : 00 rows out of the DataFrame or not ? If you do , try the [ ` dt ` accessor ] ( #URL ) .

python pandas - groupby unstack pivot - count len - mixed values and export
i'd tryed with groupby , unstake , pivot but nothing worked ...
Use the ` pivot_table ` method . For the ` aggfunc ` argument , use ` len ` . This will return the count of the items for the provided ` index ` and ` column ` . Finally , just sum the rows along ` axis=1 ` . Use ` .to_csv ` to export .

however I want there to be an easy way to specify that I want to join ` df1.actorName ` and ` df2.directorName ` together , and ` actorID / directorID ` . How can I do this ?
Rename column headers in both data frame and use ` append ` or ` concat ` :)

Yep , added a new option above that will apply the same logic to the entire df .
Take that back , no need for groupby , see my edit though , create dataframe of isnull and can avoid valid_dates part .
This also looks promising . Can you give an example of the function and how to apply to the groupby ?

AttributeError : ' Series ' object has no attribute ' dt '

@USER : that ` replace ` method can only replace entire entries in a Series or DataFrame that match a particular value or pattern with a new value . It doesn't allow the string to be modified in the way that ` str.replace ` does .
I guessed it should work . I used ` replace ` before , though not with regex . Didn't try it myself . Thanks !

I'd use the Pandas ` replace ` function to replace $ and ) by nothing , replace - by 0 , and then finally replace ( by - . Then you can do ` df=astype ( float )` and it should work .

How do I merge two MultiIndexed DataFrames ?
I want to merge ` df1 ` and ` df2 ` to produce : #CODE

Pandas dataframe apply refer to previous row to calculate difference
I want to add a column diff which represents the time difference in days per player . The result should look like this : #CODE
The first row has ` 0 ` for diff , because there is no earlier date . The second row shows ` 8 ` , because the difference between ` 2010-01-01 ` and ` 2010-01-09 ` is eight days .
The problem is not calculating the day-difference between two ` datetime ` objects . I am just not sure on how to add the new column . I know , that I have to make a ` groupby ` first ( ` df.groupby ( ' player ')`) and then use ` apply ` ( or maybe ` transform ` ? ) . However , I am stuck , because for calculating the difference , I need to refer to the previous row in the apply-function , and I don't know how to do that , if possible at all .
( I've used the name " difference " instead of " diff " to distinguish the name from the method ` diff ` . )
this looks good , however , i get this error : ` File " " , line 17 , in diff / ValueError ` . Maybe some problem in my data ? My ' date ' column definitely contains datetime objects . I just double checked .
I suggest that you apply the function on a subset of data for example the first ` 100 ` row if it worked then increase the subset until you get the error and know which rows specifically in your data set causes the issue
can you post the complete error message and stack trace

You can get all the rows where ` 2 ` is in ` a ` using a map , e.g. #CODE
Once you have either of these results , you can use , e.g. ` result [ ' a '] = 2 ` to replace the values in the ` a ` column .

After we used ` resample ` #CODE
@USER the ` resample ` lets you do much more than that . You can do every day , every 2 days , every month ...

use of apply function when you need to pass ' self ' as argument

I need to do a merge on Timestamps columns but the behaviour depends on wether the timezone is set or not .
w / o tz #CODE
with tz #CODE

How to resample pandas series monthy ( not end , not start ) ?
I know how to resample a dataframe but feel frustrated by rule parameter not accepting DateOffset ( months=1 ) .
How can I resample with monthly frequency but from any give date . For example : #CODE

Worked great ! I'm still learning pandas and must have missed the stack / unstack . Also great tip about fillna . And thanks for including the version note , I had to bump my pandas version and then it worked great .

python - pass dataframe column as argument in apply function
You have to ` apply ` over the other axis . #CODE
Another way you could do this is by using a set intersection to calculate the size . In theory this may be faster then iterating over the elements , since ` set ` is sort of designed for this kind of thing : #CODE

does not work . Lets assume I have 3 categories in column a , for each specific on I have 5 categories of b . What I need to do is to find total number of on class of b for each class of a . I tried apply command , but I think I do not know how to use it properly . #CODE
Or you can apply a ` lambda ` function onto the groups : #CODE

Ok , I looked at your data and code . Here is a screen snip when I opened your table in the humble MS Excel . Can you show me where the values for age_code , race_code and los_code are ? Because if there is no data in the columns , Pandas will insert NaN there . Also note the summary statistics produced by Excel in the bottom right corner of the screen snip . Average = 0 , Min = 0 , Max = 0 , Sum = 0 , Count = 410 . There are 410 readings , and all are zeros . I selected all rows in the three columns of interest . The data is missing in the table you are reading .

This is a slightly simpler version , using the ` month ` attribute of the ` datetime ` object . If that is equal to 10 , just map true / false values to your desired 0 / 1 pairs : #CODE

nothing happens . The ' substring ' is not replaced and I get no error . I suspect it might have something to do with the brackets .. Does somebody know what I have to do to replace my ' substring ' ? If you do it would be nice if you could also explain why my code does not work .
I didn't understand the relation between your data and your line that is supposed to replace the substring

` numpy.diff ` may be useful here : calculate the diff per column , and where the diff ! = 0 you'll find the row indices . You can combine the diffs for the two columns using a boolean or , and don't forget to offset your indices by 1 .
( Perhaps there's a ` diff ` function / method in Pandas as well , but I'm more familiar with numpy . )
IIUC you can use ` diff ` : #CODE
So this calls ` diff ` to subtract the rows against the previous rows and filters them where this diff is ` 0 `
The output from ` diff ` : #CODE
This is incredible- thank you . I was unaware of the shift function . I will have to research it .

AttributeError : map object has no attribute ' lower ' #CODE
Also , the ` AttributeError : map object has no attribute ' lower '` tells you that you're dealing with a ` map ` object and not a ` string ` object as you assumed you had .

Edit : Sorry for Unclear Question ... the merge criterion would be that for every ` data_lvl1_TIMESTAMP ` file there is a ` spektrum_TIMESTAMP ` file in the folder . The Timesstamp ist ` %Y%m%d ` . both files have rows for every minute and I want to merge the data rows .
What exactly is the merge criterion , please ? So what do you consider ' corresponding ' ?
You can use merge and if you want remove columns with ` NaN ` , use function dropna .

pandas merge dataframe based on same value in columns
i want to join the 2 dataframe in a way to produce result as below . #CODE
i have try pandas merge which can base on one same name column .
Use ` merge ` and pass the columns to merge on for ` left_param ` and ` right_param ` respectively : #CODE
i am google-ing the pandas merge details , but if u can enlighten me abit more . thank you .

One way could have been to use pd.exanding_apply() , but it doesn't preserve the dataframe to apply the function on , so there is no way to have the correct groupyby index ..

I have a ` pandas.Series ` of datetime and need to replace the tzinfo for every element in it .
I know how to do it using ` apply ` with python function but it's very slow : ~16s for 1M elements on a MacBookPro #CODE

pandas drop row below each row containing an ' na '
However , my growth rate will be invalid not only for rows with missing values in ` [[ ' a ' , ' b ' , ' c ' , ' d ']]` , but also for the following row . How do I drop all these rows ?
IIUC correctly you can use ` notnull ` with ` all ` to mask off any rows with ` NaN ` and any rows that follow ` NaN ` rows : #CODE

Can you not drop the dups before writing to db ? ` df.drop_duplicates() ` you can optionally pass your ID column ` df.drop_duplicates ( ' ID ')` , it unclear what you're asking specifically , are you wanting to insert all rows and ensire ` ID ` is unique or you actually want to drop duplicate rows ?

STEP 5 : convert the spark dataframe into a pandas dataframe and replace any Nulls by 0 ( with the fillna ( 0 )) #CODE

Pandas merge over missing values
merge on ' a ' and ' b ' will result into data frame containing only index 2 . But I want the ' a ' having NA in df C should not be considered for merging . So , final result must have 3 rows . C index 1 merged to both D index 1 and 2.What could be the possible workaround ?
Sorry can't you drop the ` NaN ` values prior to merging ? ` C.dropna() .merge ( D.dropna() )` or am I missing something ?

how to unstack a pandas dataframe with two sets of variables
The neat thing about this is that it only runs over the DataFrame once , picking all the rows under the columns it is looping over . So if you have 5 column pairs , with ' n ' rows under it , the loop will only run 5 times . For each run , it will append all ' n ' rows below the columns to the clean DataFrame to give you a consistent result . You can then eliminate any ` NaN ` values and sort by date , or do whatever you want to do with the clean DF .
The idea is to do two melts , one for each set of columns , then concat them . This assumes that the two kinds of columns are in alternating order , so that the ` columns [ 1 :: 2 ]` and ` columns [ 2 :: 2 ]` select them correctly . If not , you'd have to modify that part of it to choose the columns you want .
@USER : I think ` merge ` is somewhat more flexible and allows different kinds of joins . ` concat ` is just for piling up multiple DFs next to each other .

Preserve None values when using pandas apply
I have a data frame that needs a column , ` c3 ` , added . Each entry in the column depends on entries from the same row in two other columns , ` c1 ` and ` c2 ` . ` c3 ` was originally created by mapping a function over pairs of entries in ` c1 ` and ` c2 ` . I'm trying to speed up the creation of ` c3 ` , since there is a lot of data , by using ` apply ` . Here's what I have now : #CODE
However , when I do this , ' c3 ' becomes a ` float64 ` , while I need it to be of type ` object ` to preserve ` None ` values that I have for further processing of the dataframe ( rather than having them converted to ` NaN ` , which is what happens with the given line of code , since the other values generated by the function are of type ` int `) . I know one can use ` astype ` to change the type of a column , but using it on the already-created column does not work - the ` NaN ` values remain as ` NaN ` values . Is there any way to tell ` apply ` that I want to preserve the ` None ` values ? Do I need to do something special within the lambda expression or within ` my_func ` ?
Your apply function is weird because you don't use ` x ` , instead you extract the two whole columns of your dataframe on each row .

Apply time shift on Pandas DataFrame from another column
This definitely leads in the right direction . Do you have an idea how I can drop NaN values in that conversion ? np.array ( frame [ ' seconds '] .dropna() , dtype= ' timedelta64 [ s ]') does not seem to be the way

How do I use pandas groupby function to apply a formula based on the groupby value
You could also create a special function and pass it to the groupby ` apply ` method : #CODE
Writing a named funtion and using ` apply ` works : #CODE

How do I merge together two columns from two dataframes in Python

I guess it sorts all the dfs indices so that it's easier to align them on index , there is probably not much call for preserving the index order when performing this operation

I want to normalize these values by stripping spaces and uppercasing them . This works great : #CODE

` os.listdir ` lists the filenames relative to the directory ( path ) you're giving as argument . Thus , you need to join the path and filename together to get the absolute path for each file . Thus , in your loop : #CODE

However , I'm sure there is a way to do this without the for loop . Perhaps using applymap or map ? Can someone show me the more efficient way to do this calculation ?

Now i want merge two data frames in such way that the second time stamp is always closer or equal to the first timestamp
Then add these columns / join back to df1 .

Truncate date to last monday or any day of the week in python
I want to truncate the date so that i have only last monday of each timestamp #CODE

The data types in df1 are all integer and the data type for df2 is string . Whenever I merge / concat / join , I get NaN instead of the right data .
string . Whenever I merge / concat / join , I get NaN instead of the right
and after that apply #CODE

Pandas groupby apply performing slow
The bottleneck seems to be the apply function , even when I remove the for loop in the function it remains slow ( ~ 4.25s per loop ) . I am wondering if there is another way to apply the function ( without the apply command ) . I perform some other procedures on the data in this code using the agg command . This works much faster , but I don't know if its possible to perform this check ( full_coverage ) using the agg command .

and that worked on the masked sample . But since I have to apply several filters keeping the original ` H ` column for non-filtered values I'm getting confused .

If I understand correctly he wants to drop columns that have ` NaN ` in them . So you'll need to add ` .dropna ( axis=1 )`

How to change order of plots in pandas hist command

If you have a ` groupby ` object , you should use the ` apply ` , ` agg ` , ` filter ` or ` transform ` methods . In your case ` apply ` is appropriate .
Now , let's ` apply ` that to each group of your real dataframe : #CODE

You apply the function ` fill_seq ` to the H / K sequence columns using the values from H / K sequence as input .

I think what you want is to unstack the multiindex , e.g. #CODE
Here's a full example of ` unstack ` . First we'll create some data : #CODE
Now we unstack the multiindex : we'll show the first 4x4 slice of the data : #CODE
I get ` ValueError : Index contains duplicate entries , cannot reshape ` when trying to unstack . I have a ton of rows with some having the same ` temp_date ` ( but different values ) . Id have to unstack millions of indexes . Is there a way to avoid this ? Reindex the ` temp_date ` or something like this ?
Oh didn't know you had duplicates . In that case , you need to do some sort of aggregation to get the result you want ( and you'll have to decide which aggregation is appropriate for your data ) A pivot table would be a good approach : see my edit above .

You can call ` apply ` on the df pass ` axis=1 ` to apply row-wise and use the column values to slice the str : #CODE

Make a list of list of your desired row values and construct a df and ` concat ` them : #CODE
You'd have to construct a new df and merge on the index to do that and I advise against it for any large dfs , besides adding a row at a time is non-performant . It's better to construct the entire df and concat them as I've shown

I have a feature vector ( dummy ) dataframe for categorical data in Pandas , and I have appended a ' ratings ' column to that dataframe which represents continuous data from 1 to 10 . How do I replace all the 1s in all the columns except the ' ratings ' column with the corresponding ' ratings ' column value ?

The ` anagram_list ` column contains tuples , which is not supported by sqlite to insert in a dataframe . Therefore converting it to strings solves the issue .
The ` anagram_list ` column contains tuples , which is not supported by sqlite to insert in a dataframe . Therefore converting it to strings solves the issue .

All timestamps are in milliseconds . As you can see , the first dataframe is resampled to 100milliseconds . So what I want to do is , to align the two dataframes based on count . Which means based on the count how many events occur during a particular 100milliseconds bin time . For example , from the dataframe 1 , in the first 100millisecond bin time ( 24043950 - 2404049 ) , only one event occur according to the second dataframe which is at 2404030 and so on . The aligned table should look like the following : #CODE
You then merge these counts to your original dataframe and do some formatting operations to get the data in the desired shape . Make sure to fill NaNs with zero . #CODE

So far I have imported the CSV as a dataframe and , all columns have a dtype of ` object ` . I have sorted the data by date and am now trying to group the entries by a time interval which is where i'm struggling . Based around google searching , I have tried to ` resample ` the data using this code ` df.resample ( ' 5min ' , how=sum )` Here I get the error ` TypeError : Only valid with DatetimeIndex , TimedeltaIndex or PeriodIndex ` . I was thinking about trying the ` groupby ` method , perhaps using ` lambda ` as in ` df.groupby ( lambda x : x.minutes + 5 )` which produces the error ` AttributeError : ' str ' object has no attribute ' minutes '` .
You should then be able to resample . #CODE
And your date column should have the datetime type if the format is sane . Then you can set the index and resample as above .

While in this case we have ` notnull ` , ` ~ ` can come in handy in situations where there's no special opposite method . #CODE

How to apply a function on every row on a dataframe ?
And ` ch ` and ` ck ` are float types . Now I want to apply the formula to every row on the dataframe and return it as an extra row ' Q ' . An example ( that does not work ) would be : #CODE
( returns only ' map ' types )

Getting Data of a boxplot - Pandas
I tried the following query to draw the boxplot . #CODE
You are correct I executed without ' _ , ' . I need one more clarification from you . Following are the median values but it repeated . why is that ?

However for my issue I could sort and drop duplicates and reindex , which performs a lot better . Especially on larger data sets this really makes a difference .

the w variable will not surpass len ( seq ) .
Here you will find arrays and methods that are much faster than built-in list . For example instead of looping trough every element in a numpy array to do some processing you can apply a numpy function directly on the array and get the results in seconds rather than hours . as an example : #CODE
Computing ` len ( seq )` inside the loop is not necessary , since its value is not changing .
You don't really need the ` if ` statement , since in your code it always evaluate to true ( ` w in range ( len ( seq ))` means ` w ` maximium value will be ` len ( seq ) -1 `) .

the ` f ` function is more complicated that this one , of course , and I want to apply a sequence of functions to transform the data frame . There are basically string parsers to normalize , transform to unicode , remove characters , split into components , etc , so so far I can t see a way to do it without ` applymap ` .
Can you give more insight in what you want to apply ? ( the simple example you give can of course easily be done without ` applymap ` , but maybe your real function as well ? )

output from the merge : #CODE
you can use the left merge to obtain values that exist in both frames ` + ` values that exist in the first data frame only #CODE
Length of the code . As it stands , one would need to keep adding ` & ` in order to add more columns . It would be better if it could be wrapped into , say , a list comprehension ( or apply function ) since each column is independent

I don't know if this is something possible . I am trying to append 12 files into a single file . One of the files is tab delimited and the rest comma delimitted . I loaded all the 12 files into dataframe and append it into an empty dataframe one by one in a loop . #CODE

This is an LDA Document-Topic Probability List from ` gensim ` in which each list is a document and each tuple is one of five topic probabilities . ( See an earlier question I posted on Stack Overflow here ) . The first element in the tuple represents the topic number , the second element is the probability that the topic probability for the document .
Since you know there will only be five elements you can make a tmp list initialized with 5 zeros and just replace the ones that are non-zero

use ` str.contains ` and join the keys using ` | ` to create a regex pattern and negate the boolean mask ` ~ ` to filter your df : #CODE

In the event that you had unique column and row labels , you could use a combination of ` melt ` with ` pivot ` like so : #CODE
` pivot ` gets us the rest of the way , but will not work in this case because the headers are not unique ( again , the issue with ` Subject 2 ` and ` test C `) . #CODE

Are you need to replace your hours minutes and seconds from original to 13:00 : 00 ?
Use replace method of pandas timestamp objects : #CODE

where is your attempt ? Stack Overflow is not a code writing service

I think you * can * do it if you're willing to do enough shenanigans . You could , for example , for each column : copy the series ; count the ` NaN ` s ; strip the ` NaN ` s keeping the values in order , append the same number of ` NaN ` s at the end . Then , create a new dataframe with these adulterated series as columns . It's not what @USER asked , though , I was just thinking about the possibility

If you also want to extract correlation values from the DataFrame , replace ` .tolist() ` by ` .to_dict() ` . ` .to_dict() ` will generate a dict such that index is key and value is value : ` {index - value} ` . So , ultimately your ` pairs ` will look like ` {column - { index - value}} ` . It will also be guaranteed free of ` nan ` . Note that ` .to_dict() ` will only work if your index contains the row names that you want , else it will return the default index , which is just numbers .
Ps . If your file is huge , I would recommend reading it in chunks . In this case , the piece of code above will be repeated for each chunk . So it should be inside your loop that iterates over chunks . However , then you will have to be careful to append new data coming from the next chunk to ` pairs ` . The following links are for your reference :
You can use the same trick in pandas but with stack to get the index / columns natively : #CODE

thanks that worked . if i want to insert that ' columnF ' from df1 to the column after ' columnA ' in df , would i do something like ` df [ ' columnA ' +1 ] = df [ ' columnF ']` ?
That won't work . You'll need ` insert ` for that . ` df.insert ( 1 , ' columnF ' , df1 [ ' columnF '])`

I have recently upgraded to pandas ` 0.17.0 ` ( numpy : ` 1.10.1 ` , Python : 2.7.10 ) and I receive the warning shown in the block below when i try to reshape a dataframe with an unstack operation . The warning appears only the very first time the code is run on a fresh ipython notebook . A rerun on the same notebook does not throw any messages at all for some reason ( !! ) . A similar unstack exists somewhere in the code of a bigger application written with the Pycharm editor . In that case the warning pops out everytime the application is run .
The result of the unstack call looks to be fine , its just the warning that puzzles me and whether it is expected behaviour . Also passing and argument when unstack is called looks to prevent the warning from appearing .

Pivot your dataframe . Use ` isin() ` in ` apply() ` .

Since you have just mentioned a specific condition , the answer is accordingly framed to align with your requirements . #CODE

If you want in different column I would suggest to do transpose , #CODE

this error indicates that there is no argument with name ` inplace ` for the method ` drop ` which of course is not the case , I'm not sure about this , are you sure you followed the same steps ?

I am creating a new application which uses ZODB and I need to import legacy data mainly from a postgres database but also from some csv files . There is a limited amount of manipulation needed to the data ( sql joins to merge linked tables and create properties , change names of some properties , deal with empty columns etc ) .
If your CSV files aren't very large ( as you say ) then I'd try loading everything into postgres with ` odo ` , then using ` blaze ` to perform the operations , then finally dumping to a format that ZODB can understand . I wouldn't worry about the performance of operations like ` join ` inside the database versus in memory at the scale you're talking about .

For instance , the following doesn't replace elements greater than the mean
I don't understand by what logic the resulting boolean array is like that . I'm able to work around this problem by using transpose : #CODE
Great , this is exactly what I was looking for ! I thought there would be this kind of functions but I couldn't find by googling and I must say the docstrings of ` gt ` and ` lt ` aren't very helpful .. Thanks !

As pointed out by @USER , there a potential workaround , that is to not just append the groupby results to a new csv and read that back in and perform the aggregation again until the df size doesn't change .
Could you not just append the groupby results to a new csv and read that back in and perform the aggregation again until the df size doesn't change ?
Firstly you can choose list of unique constants by read csv with usecols - ` usecols =[ ' id ' , ' col1 ']` . Then read csv by chunks , concat chunks by subset of id and groupby . better explain .
Thanks for your proposition , I don't clearly get the idea behind . But I don't want to drop any of the columns , as the results of the sum aggregation will be altered if I do so .
you drop some columns only for choose constants . And use usecols because it is large df .

Are you wanting to transpose or rename ?
transpose . Thank you !
Just transpose it using ` .T ` : #CODE

Color seaborn boxplot based in DataFrame column name
The column names contain strings that indicate an experimental condition based on which I want the box of the boxplot colored .
This creates a beautiful list of boxplots with correct names . Now I want to give every boxplot that has ' prog + , DMSO+ ' in its name a red color , leaving the rest as blue .
This does make boxplots of different colors but all on top of each other and without the correct labels . If I could somehow put these boxplot nicely in one plot below each other I'd be almost at what I want . Or is there a better way ?

Performance implications of using an outer merge as opposed to concat / append
I have noticed that our system has been using outer merges as opposed to using concat / merge to combine data frames . Are there any performance implications of doing this ? I would assume so because it will try to match on keys ?

However if I uncomment the line ` df.Date = pd.to_datetime ( df.Date )` to do the date type conversion and run it again it will drop me the overflow error mentioned in the title of this post .

Python pandas groupby object apply method adds index
I have this question is an extension after reading the " Python pandas groupby object apply method duplicates first group " .

I have two pandas Dataframes with non-equal elements , but they are indexed . I'd like to divide them , index by index , without have to interpolate .
Actually , I have data for several days , and interpolate one by one is a dificult option . Maybe using ` apply ` but I dont know how .

I realized that given the NaNs in my data , the order in which the operations happen really does matter . If I take the mean across my columns first and resample second , then the columns with values will be weighted . If I resample first and then take the mean across columns , then certain timesteps will be weighted ( for example : if there is only one value for a whole day that will be taken as the mean of that day ) . I can't think of a way around this unfair weighting .

Why can't you insert these two lines above ` df.to_csv() ` ? #CODE
I want nulls to stay nulls . I do not want them to be an arbitrary int . If so , Redshift's copy will insert the arbitrary int as that record's field value .
What is Redshift's null identifier ? If it is like ' \N ' in SQL , you can convert the column to varchar and replace all null values with ' \\N ' ...

For one thing this is very inefficient ( O ( n ) for each append so O ( n^2 ) in total . The preferred way is to concat some objects together in one pass .
Finally , append doesn't act in place , so the variable ` dataf ` is discarded once the callback is finished !! and no changes are made to the parent ` dataf ` .
Now , once the async has finished you have a MultiProcessing.list of DataFrames . You can concat these ( and ` ignore_index `) to get the desired result : #CODE

` .groups ` will return the groups for a ` groupby ` object , you can then find the intersection of these and then get the specific group calling ` .get_group `
You can get the groups from a ` groupby ` object using ` .groups ` this returns a dict , the keys are the group values , you can then find the common columns using ` set ` and ` intersection ` and then get the common groups using ` .get_group ` : #CODE

again thank you . ` for i in range ( 1 , len ( fi.index )):
Still not appending the speeds to DF2 , I've looked at both data frames and exported both to CSV to make sure I haven't missed something . I'm starting to have some success using this code ` for i in range ( 1 , len ( fi.index )):
Maybe ` between_time ` is extracting a view of DF1 , which is not propagating the changes to back to DF1 ... The solution will then be extracting the index , from ` between_time ` and using that to append into DF1 directly .
Performance of iterating is very low . Better is use vectorized solution . I use twice function ` merge ` . Docs .
thanks for the solution , i'm just trying to make it work , when you drop the index the time column reverts to a pandas object rather than ` datetime64 ` , so ` timedelta ` doesn't work . But I understand your logic and am hopeful for the solution .
One issue i've found is using the merge function on ` i ` takes a very long time and lot's of memory . Using ` df.info` I can see the sizes are ~50mb =d f2 and ~1mb =d f1 . Can you suggest an alternate joining method that would create the same join ? I've tried ` concat ` and ` append ` .
#URL This link shows that when merging on a single value the DF size balloons and takes forever . I changed the above code to merge on the ID field and it is a million times faster ... roughly .

How to calculate the count of column values less than 95 on each row on pandas pivot table
I am new to pandas pivot tables , how to get the count of column values less than 95 for a row on pandas pivot table #CODE

Also , ` drop ` can be configured to drop columns which may or may not be there by passing ` errors= ' ignore '`

I can transpose the file with " bash+grep+awk " and then read it as csv but it's not practical for users who has only Python and Windows .

This looks like something you might do with cumulative sums ` w.cumsum() ` might give you something you can just multiply by the range you're trying to interpolate .
Michael gives me the hint i needed . Here's an example of weighted interpolate with only ufuncs ( eg numpy functions ) .
Below a plot of the known points , the linear interpolate and the weighted interpolation

You should accept my answer and ask this as another #URL That said I think unstack that level first , no need for groupby etc .

You can use ` apply ` on the column to generate a boolean mask describing the desired columns , and then filter the DataFrame by this mask : #CODE

What I've got so far is printing out the correct expanded dataframes for each row , but I don't know how to consolidate the results of apply : #CODE
You could , of course , do all this within a function that you apply on a ` groupby ` , but it would be superfluous in this case .
Thanks for the idea . I don't really get stack and unstack yet , so I guess this will be a good opportunity to dig in a grok it . In the meantime - unfortunately this code didn't work for me . I'm using Pandas version 0.15.2 and python 2.7 . I initially got an error that " expand " is an unexpected keyword . Then I removed it and got an error : "' Series ' object has no attribute ' stack '"

python pandas nested loop : to apply a function to each element of e.g. column 2 involving compounding same elements in previous column 1s
This is almost there , but want I need to do is multiply the numbers of the corresponding bins together , so it is e.g. ( port1 , bin0 ) .rebal_wgt * ( port0 , bin0 ) .rebal_wgt , ( port1 , bin1 ) .rebal_wgt * ( port0 , bin1 ) .rebal_wgt , ... ( port_n , bin_n ) .rebal_wgt * ( portn-1 , bin_n ) .rebal_wgt , ... I tried to do this by using shift ( bins ) , e.g. shift ( 10 ) , but the problem is that this is not cumulative , i.e. I need to shift by 10 every result as its a cumulative product . Is there another way , e.g. using groupby [ ' port ' , ' bins '] that might be better / more intuitive ?
In fact , it seems to me , that one could apply a similar method to calculations involving n nested loops . As long as one has pre-calculated each of the n loops , and ordered them using sort , one can apply a function to the result by using groupby on the nth bin directly . In the problem above , the loop was a 2-variable nested loop , with bin being at level 1 , and port being at level 0 . This may relate to indexing ?

I feel like ` df.pct_change() ` would be helpful , but I can't figure out how to apply it in the way I'm trying to describe .

I always use ` ser.plot ( xticks=range ( 0 , len ( ser ) , step ) , .. )` to plot figures with specified xticks . But it doesn't work anymore in pandas 0.17 when the ` Series ` with labels . Here are the codes : #CODE
or apply the patch in the pandas code ( see the link above , it's only a one line change )

You can use the shift function for create new columns and after you can compare them to the initial columns . #CODE

I have seen here a similar question for scatterplots , but it doesn't seem I can apply the same solution to a time series line chart .

Pandas / Scikit - Apply sparse PCA while creating feature vectors
I am planning to generate a huge sparse matrix of 70000 rows and 150000 columns using Pandas get_dummies() , however I get a MemoryError . How do I apply PCA on this sparse matrix to reduce dimensionality ?

Replace values in pandas datatable if in list
How can I replace values in the datatable ` data ` with information in ` filllist ` if a value is in ` varlist ` ? #CODE
Use the ` replace ` method of the dataframe . #CODE
replace method documentation

How can I do this without using groupby and apply function ? Because I need a good performance in computation .
The normal op here is to ` groupby ` on ' item ' and call ` sum ` on the ' grade ' column no need to call ` apply ` here

Note that if you have to perform ` urljoin ` then using ` map ` or ` apply ` would be fine here
This will be much quicker than calling ` map `
For pure string concatenation , this will be vectorised , using ` map ` and ` apply ` this is just a ` for ` loop so this approach will be much faster for large datasets

If I am not mistaken this also takes whatever is the first non missing lag and lead . Can I restrict this ? For instance if I only want to replace if in the last two years and in the next two years they are the same . If the data is from more than two years ago I keep the missing value instead .

You can use function merge on column ` L ` . Then this column is renamed to ` to ` . #CODE

I have a dataframe where the names of the fruits are in integer format . However , I would like to replace them with actual string names . #CODE
You'll need to have the dictionary as ` int ` since ` Fruit ` is ` int ` as well . Then simply use ` map ` . #CODE

Python-pandas Replace NA with the median or mean of a group in dataframe
to replace the NA , we can use df [ " B "] .fillna ( df [ " B "] .median() ) , but it will fill NA with the median of all data in " B "
Is there any way that we can use the median of a certain A to replace the NA ( like below ): #CODE
` with ( dd , ifelse ( is.na ( B ) , ave ( B , A , FUN = function ( x ) median ( x , na.rm = TRUE )) , B ))`
In ` R ` , can use ` na.aggregate / data.table ` to replace the ` NA ` by ` mean ` value of the group . We convert the ' data.frame ' to ' data.table ' ( ` setDT ( df )`) , grouped by ' A ' , apply the ` na.aggregate ` on ' B ' . #CODE

However I am wondering if there is a way I can write my function and apply it to my grouped object such that I can specify when applying it , which column I want to calculate for ( or both ) . Rather than have ' var1 ' written into my function , I'd like to be able to specify when applying the function .
You can apply and return both averages : #CODE

Pandas dataframe : how to apply describe() to each group and add to new columns ?

You could try to merge these two dataframes , then drop DPAvPos and rename DKP / Game as DPAvPos : #CODE

I have asked a question on another thread Link . But I got an incomplete answer . And no one is willing to reply . That is why I am making another modified question . Let me explain the question briefly , I wanted to resample the following data : #CODE
The table goes on and on like that . All the timestamps are in milliseconds . And I wanted to resample it into 100L bin time . #CODE
Note : resample without a DatetimeIndex / PeriodIndex / TimedeltaIndex raises an error in recent pandas , so you should convert to DatetimeIndex before doing this .

I have tried using the cut function in pandas but the results are not promising . Could you help me find a good function to do this .

I'd put these in another dict of ( course , semester , section ) -> DataFrame and then concat and work out where to go from the big MultiIndex dataFrame ...

How do I concat a list of columns to a DataFrame .
Another option is to just concat the entire dict : #CODE
So I figured out a different way to do it but it's definitely less elegant than yours . Using yours , I was able to get frames , but using concat isn't exactly what I want to do . For creating the dataframe from frames , would you just use a for loop to iterate through frames , and then for each item add the column on to a dataframe ?

Probably a convoluted way of doing it , but You could ` groupby ` ID1 and ID2 ; then * iterate over the groups * zipping ( use ` zip `) ` x ` and ` y ` into a ` xy ` column where values are points ( x , y ) . Then , shifting this ` xy ` column by 1 ( use ` shift `) , get a new column ` xyshift ` . Then apply a difference function in the row-axis in this ` xyshift ` column ; and finally merge it iteratively into your dataframe or compose a new one
you can calculate the difference between ` x ` and ` y ` by using the ` diff ` function which will produce ` na ` for the first columns but you can drop it easily using ` dropna ` function .

plot line over boxplot using pandas DateFrame
I'm trying plot a boxplot and a line with 2015 values in same fig in order to compare , using this code : #CODE
In 2015 column I have data from January until September , but I get a shift line plot , from yline until september :
df.plot starts the labelling at 0 and boxplot at 1 I think ( which is annoying especially when the index is the same ) . Try ` ax.plot ( list ( range ( 1 , 13 )) , df [ 2015 ] .values )` for the line plot instead .

I've been trying to replace missing values in a Pandas dataframe , but without success . I tried the ` .fillna ` method and also tried to loop through the entire data set , checking each cell and replacing NaNs with a chosen value . However , in both cases , Python executes the script without throwing up any errors , but the NaN values remain .

If I drop columns using the label , all columns headed by that label are dropped : #CODE
I need the column labels to be unique . For now , I'd like to drop all but the first column in the group ( I stated this in the OP ) . ( That may change , in which case I'll have to adapt the solution , but I'm not going to worry about that for now . )
This does do what I specified ( drop all but the first column in each group ) . And it's quite concise . Thanks , I may end up going with it . Ideally I was hoping for some slight variation on the drop() method , because that seems so close to what I want . It seems there should be some way to drop a column by specifying its ( numeric ) index , and not have all the like-labelled columns be dropped .
actually the drop function doesn't take index as a parameter , I tried to pass ` int ` to ` drop ` but it did nothing , I'm not sure why do you prefer to drop column by it's index
It's not that I prefer to use the index , I just want to be able to say " drop the third column " rather than " drop all the columns with the label on the third column " .

Pandas rename / transpose column using value from another column
Move the index level values into a column index using ` unstack ` .
The ` Id ` column was also placed in the index to " protect " it from getting split up by the ` unstack ` operation . The easiest way to see what I mean is to take a look at

the replicate columns is ' 1 ' until later down the rows , and I am not sure how it's picking which rows to assign the numbers to . There should be a total of 3 block|concentration|name combos that are the same , so I need to assign ' 1 , 2 , 3 ' to each to separate them for later when I use pivot table . I have made the ' concentration ' column as string type so being numeric should not be a problem .

I am trying to connect to a local mariadb database and insert some data into a table .
My code to insert the dataframe into the database is : #CODE
thanks @USER I will look into sql alchemy . my sql table definition has a stock_id which is auto incrementing . How do I tell pandas python to insert into every other field in the table except that one ( because I want it to auto increment ?
You can just have a dataframe without this stock_id columns ( but with all others ) and append this . In such a case , the sql table will autoincrement the stock_id column itself

pandas pivot table descending order python
then i organized everthing with pivot table #CODE
the Pivot table looks like this ( the actual table's format is a little bit different than this , i typed the table in manually here : #CODE
I tried but i got error , i do'nt think i can do that to a pivot table . i want my pivot table output for ' Conc ' in descending order .
@USER : were you able to pivot your table ?
yes , the table above is from my pivot table , i dont know how to copy paste table here , so i typed it in manually , that's why the format looks a bit different

I've recently started learning Pandas and I'm having some trouble on how to plot results after using groupby and agg . Using Pandas , I have created a data frame and grouped it based on two columns ' ID ' and ' x ' . Then I selected one specific column ( ' results ') from the group to calculate the sem and mean .

is there anyway to generate a sparse vector row by row and stack then together ?

pandas map function returning ' NaN '
I have manually added a ' sex ' column onto the DataFrame , and I am trying to replace ' Male ' with 0 and ' Female ' with 1 however it does not seem to work . I just get a ' NaN ' value instead of the ones and zeroes .

Finding the median of a histogram
A have a dataframe like this which represents a histogram , with each bin size being .003 . I want to find the median value in the histogram , but I am unsure how . The median should be where half of the area of the histogram lies to the left , and half of the area to the right . #CODE
and the code I am using to find the median is this : #CODE
and the median value is this :
Aren't you simply calculating the median of the bin edges and discard the information about the histogram ?
Yea I am pretty sure this isn't the correct method , this is just finding the middle ` Value ` and not taking into account the areas covered . I will add an edit in a minute that shows one where it clearly doesn't work . Again , I want to find the median where half the area lies to the right and half to the left .
What you conceptually want to do is : 1 ) Normalize the counts by the total number of observed counts . 2 ) Iteratively sum the normalized counts until you have accumulated more than 0.5 , return the average between the last and this value as approximation to the median .
What you are looking for is the " weighted median " since you have the summarized distribution with frequencies . Try wquantiles package #URL
And the correct way to find the median is to take the cumulative frequencies . The interval where the cumulative frequency exceeds %50 of the total contains the median ( i.e. go from the minimum value to the maximum by summing the frequencies ) . You can just use the starting / end points of the intervals or you can use interpolation .
I understand , I think I understand what you were hinting at , find the cumsum and then write a function to iterate through the value column until .51 of the cum sum is met and that should be approximately the median
np.average has a weighted argument , I don't think there's an equivalent for median #URL

- to replace all values in label column with 0 : num_class index ( a with 0 , b with 1 , c with 2 etc )
For simple conversion you can use map method on your data frame , #CODE

Here's a solution with apply #CODE
You can use ` regex ` and strip first and last spaces by ` strip ` : #CODE
Or regex without strip : #CODE
if you put \s * outside both ends of the group there's no need to strip .
Use the vectorised ` str.split ` this will be much faster than using ` apply ` on a large dataset : #CODE

How to maintain Pandas DataFrame index order when using stack / unstack ?
After using the ` stack ` and ` unstack ` methods on the given ` df ` DataFrame object , the index is automatically sorted lexicographically ( alphabetically ) so that one loses the original order of the rows . #CODE
Is it possible to maintain the original ordering after the ` unstack / stack ` operations above ?
Notice that the stack and unstack methods implicitly sort the index levels involved . Hence a call to stack and then unstack , or viceversa , will result in a sorted copy of the original DataFrame or Series

I'm trying to replace and add some values in pandas dataframe object . I have to following code #CODE
How can I find the cell values in column A which contains ' - ' sign , replace this value with '' and add for these values a trailing ' _0 ' ? The resulting data set should look like this #CODE

Replace empty string with with np.nan but got ' NaN '
I tried to replace empty string with ` np.nan `
If you try to replace `''` with ` np.nan ` in the above example , you get the desired result : #CODE

It looks all you're doing is grouping on email addresses then calculating ` diff ` on those groups and then assigning a count if the diff is larger than ` 10 ` correct ? Also if this is ` pandas ` then tag it so

How to efficiently map entries in a dataframe to dictionary
Ps . I have tried to conform to best practices when it comes to posting a queston on here . My apologies in advance , if I have neglected to provide necessary information .

The above code gives me the correct data but i'm having trouble collecting it in a usable format . I've tried to merge and concatenate them but no joy . Does any body know how to do this ?

I got this answer from here . If anyone could explain what the ` loc ` does , that would be a lot of help as well .
Then glue them up with a concat with axis=1 ( i.e. add as more columns rather add as more rows ) . #CODE
Aside : it would be nice if concat didn't require that the column name be passed in explicitly ( as the key kwarg ) when they exist ...
You can map the columns to det1 , det2 , etc . before doing the concat , for example if you had the mapping as a dictionary . #CODE
@USER If that doesn't help for the final join / concat aspect you might get more activity via asking a new question about that specific part ( otherwise I'll try and answer next time I'm online ) .
@USER but you don't have to do that if you use exclusively crosstab and concat . There's no NaNs , that need filling / astypeing if you use crosstab . RIght ?
how to group and pivot the table ?
how to merge the table ?
what is ` loc ` doing ?
1 . ` concat `
2 . ` merge `
` loc `

first create the df using ` from_dict ` , then call ` stack ` and ` reset_index ` to get the shape you desire , you then need to rename the cols , sort and reset the index : #CODE

I have tried using string functions rsplit and replace in a new column creation statement , but I get an error that the string function is expecting a string and is receiving a Series .
To extract just the month-year piece from the resulting list created by the split , apply ` map ` and a lambda to the result : #CODE
Or as @USER suggests apply ` str ` again instead of the map-lambda : #CODE
Instead of the map / lambda , you could use ` .str [ 0 ]` directly .
I will suggest you create a new column of ' Type ' and then use ` pivot ` : #CODE

I thought TimeGrouper might help . However when I use TimeGrouper and set freq to be " 2D " : #CODE
For simple case like this , ` shift ` will suffice : #CODE
More generally , it is a case of ` rolling ` apply , ` min_periods ` control the minimal window that will be considered as valid . Skipping it in this case will result in having ` nan ` for the 1st cell : #CODE

IIUC you want to ` pivot ` : #CODE
` pivot ` doesn't support multi-index df's which was one method I was considering , what you could do is add a new column which is a composite of the 2 columns and use this as the index to ` pivot ` on : #CODE
You can also use multi Index and unstack like this : #CODE
This where I like to use multi-level indexes and stack / unstack .

I am working on processing an HTML page and translating its data into RDF . I load the html tabular data into a dataframe and am now trying to replace the blank spaces with underscores . I get this error : #CODE
it does return a list of data frames so you just need to iterate through the dataframes and call replace on your iterations

Is unnecessary as diff is unchanged ( since it's already unique via the previous groupby ) .
I would be tempted to unstack here , as the data is really two dimensional : #CODE

Python pandas asof join on groups
I have two pandas data frame X and Y , and each contains intraday price and time data for the past month . I would like to run asof join of Y on X , i.e. we take the prevailing price of Y each time we see an update on X . I would like to run an intraday analysis ( because of the overnight effects )
asof is a Series method , rather than a DataFrame one . It works on the Time column : #CODE
When you do the apply , it's across each row ( which is a Series ) .
Hi , thanks for the help Andy . This works . But how would I get the as of join on the entire data frame . I would like to see the Mid price column as well . Also , why would Y_asof = Y.apply ( lambda x : x.asof ( X.index )) work if asof is method on pd.Series only .
I believe pandas throws an error because ` Y.groupby ( ' Date ')` creates a ` GroupBy ` object which does not have the method ` asof ` . If you're just using ` groupby ` as a way to sort by date , you could do ` Y_asof = Y.sort ( ' Date ') .apply ( lambda x : x.asof ( X.index ))` instead .

I'm guessing the slow part is the apply ( rather than the split , or the stack ) ?

@USER how would you apply the answer in your link to my question ? This is my first question , I wouldn't mind if you undid your down vote .
There is a built-in method for this ` clip ` : #CODE
You can do that but to me this defeats the whole point of using pandas which provides vectorised methods , if you're going to do that then use ` apply ` to process an element

Python Pandas Update / Replace
struggling to get to grips with update / merge / concat etc to achieve what I thought was a simple merge or update procedure . I have df ( recorded values ) and df2 ( times series with 0s ) , and would like to update / replace the values in df2 with the corresponding recorded values of df , matching on date_time .
Tried so far : Re-indexing to Date_time and df2.update ( df ) with variations , multiple merging / join / concat variations , an adapted definition ( below ) with apply ... and now wondering if I need to use iterrows ( see below ? ) .

I have tried simpler approaches , even just making a df [ ' newcol '] and try to strip the left four characters from date . like df [ ' newcol '] = df [ ' sqldate '] [ 0:4 ] but that fails . It just makes the first four rows of newcol = sqldate , and the rest of the rows Nan , because it interprets the [ 0:4 ] as an index selector .

i'm juggling with periodindex and merge method ,
Why Join key is lost if is the same rule i've used to make the merge ?
This looks like a bug with ` reset_index ` if you do this it returns ` False ` for all index values : ` df2.reset_index() [ ' index '] == df2.index ` it would be better to keep the comparisons between the same type e.g. merge both sides on ` Index ` objects or on ` Series ` , note that technically the second merge is still correct as you've reset the index so the index becomes a default ` Int64Index `
I tried to replace ` idx ` values with ` numbers ` and ` strings ` and it worked fine with both , so it turns out that problem is with ` PeriodIndex `
@USER : with reset_index i've " defaulted " an input dataframe , but in all cases the output dataframe must have the index defined in the join rules . ( I expect )
I don't understand what ` but in all cases the output dataframe must have the index defined in the join rules . ( I expect )` means ? you can join on index or column , it unnecessary to specify as the default is ` False ` for left and right indices

Merge dataframes without header
I want to merge them based on Col_A in df_A and 1st column in df_B . Note that df_B does not have any header row . How do I do this ? A simple merge would have been : #CODE
TO CLARIFY : I want to merge two dataframes . One has a header , and the other one does not . How do i specify the join column on a dataframe that does not have a header ? Can I specify it by location / index ? #CODE
Well it looks like you want to merge like so : ` df_A.merge ( df_B , left_on= ' Col_A ' , right_on= ' 23 ')`
Well you've answered my question then , you've either imported the file incorrectly or you'll have to merge using my code
@USER , I should clarify my query . I want to merge two dataframes . One has a header , and the other one does not . How do i specify the join column on a dataframe that does not have a header ? Can I specify it by location / index ?
You can set column names by ` df_B.columns = [ ' Col_B ' , ' Col_A ']` . Then you can merge dataframes . #CODE

If you ensure the ' years ' are different columns ( eg with ` unstack `) , then you can specify ` stacked=True ` to say that the different columns should be stacked : #CODE

How to apply a function to rows of two pandas DataFrame
There are two pandas DataFrame , say ` dfx , dfy ` of the same shape and exactly the same column and row indices . I want to apply a function to the corresponding rows of these two DataFrame .
By the following code , I make a grouped two-level indexed DataFrame . Then I do not know how to apply ` agg ` in the proper way . #CODE

how to replace the header row when reading from Excel- Python

I then try to merge this back to the group ( basically I want to know what each name / number correspond to ) #CODE
@ user3120266 Do you want to concatenate ` Billing ` + ` len ( Billing )` ?
You can find more details in pandas concat API documentation
also you can get the number of characters in any word easily by using the ` len ` function ` df.Group.str.len() ` #CODE

I want to replace the missing values ( indicated by -999 ) with the mean of that column taken over non-missing values .

@USER I updated my answer so it deletes the element at index 6 rather than the label 6 . I don't think you would have to worry anyways though , I believe ` a.index ` converts your dataframe to ` Ints ` so ` drop ( 6 )` should always work . I'm not positive on that though .

However , it occured to me this may not be always correct , since there is no guarantee data was collected everyday . Instead , I tried counting unique days in the timestamp series using ` map ` and ` apply ` , and both take a considerable amount of time for 3,000,000 rows : #CODE
To get the unique dates you should first ` normalize ` ( to get the time at midnight that day , note this is fast ) , then use ` unique ` : #CODE
To get the counts could use ` normalize ` and then use ` value_counts ` : #CODE
but perhaps the cleaner option is to resample ( though I'm not sure if this is less efficient ): #CODE
Thanks ! ` %timeit len ( df [ ' TIMESTAMP '] .dt .normalize() .unique() )` gives me 281 ms , 148 times faster than the map approach !

Join one column then another to same column of a second Pandas Data Frame
I would like to mergeone data frame twice to another data frame , first using one column and then another for the left data frame in the merge . I came up with one way to do this , but I'm looking for a better way . Details below .

If you pass the ` keys ` parameter to ` concat ` , the columns of the resulting dataframe will be comprised of a multi-index which keeps track of the original dataframes : #CODE
If you just want the indices that are different , you can do : ` different_indices = [( i , j ) for i in range ( len (( df1 ! = df2 ) .columns )) for j in range ( len ( df1 ! = df2 )) if ( df1 ! = df2 ) [ i ] [ j ]]`

Note that I'm calling the ` nonzero ` method in order to just output the coordinates of the nonzero entries in the underlying sparse matrix since I don't care about the actual values which are all ` True ` .
@USER do you mean doing the apply ?
This is very surprising , mainly because the accepted answer is O ( n^2 ) . I can see the apply part being slow , but as I say I don't think you really need that part .

MultiIndex pivot table pandas python
Pivot table looks like this ( only a part of the large table ): #CODE
but it doesn't work in my case , it doesn't work with pivot table , im not sure why , i'm confused . Is a pivot table multiindex ??

Pandas can do a number of different SQL-like joins . Assuming each ` Location ` has a unique ` Identifier ` , a join using the `' inner '` strategy should do what you're asking for : #CODE

I came up with a combination of ` resample ` and ` groupby ` : #CODE

I think you need concat chunks to df , because type of output of function : #CODE
I think is necessary add parameter ignore index to function ` concat ` , because avoiding duplicity of indexes .

you've got a couple of problems here . the first involves repopulating your dataframe from a subset . the other , more computational problem involved not ` for ` loops , but ` groupby ` and ` transform ` ( or ` apply ` ? ) operations . I recommend splitting this up into two separate questions .
Consider using groupby apply functions to dataset . The first function averages the values only for ' Print Buffer ' using ` mean() ` , leaving the others in Block zero . And then the second function maximizes the ` meanvalue ` . Finally , simply create ` newvalue ` as arithmetic difference : #CODE

Can't drop NAN with dropna in pandas

You can use ` melt ` from pandas : #CODE
Simply great . Never knew about ' melt ' , works beautifully .

i wanted to count rainfall events in the timeseries data , where we define a rainfall event as a subdataframe of the main dataframe which contains nonzero values between zero rainfall values
i managed to get the start of the dataframe by getting the index of rainfall value before the first nonzero value : #CODE
to go on to the last zero value before the nonzero . can't seem to find the offset thing i was trying to look for .

Series f is large ( 30000 ) so looping over the elements ( with ` map `) is probably not very efficient . What would be a good approach ?

The first column is just a counter between 0-200 - I usually drop it when I load the data since it is the same as the row index . The problem I currently have is in the plot , when I use ` imshow() ` to plot my figure , it uses the row and column index as the axis ticks . This is fine for my Y axis , since it is the same as my values in column 0 ; the problem is with the X-axis which has index values between 1-1000 , whereas I would like to plot it on axis with actual values of X ( i.e. 381.3 , 393.3 , 200.6 etc ) . How could I accomplish that ?

Bug between asfreq and resample in pandas ?
I am trying to understand the difference between resample and asfreq but I can't sem to wrap my head around it , in the example below I took the same dataframe and used resample and asfreq but when I count the values I get different results , how is this possible ? #CODE

just note that I'm importing ` datetime ` module as ` dt ` #CODE

Using pandas loc in hy

UPDATE : Here is a photo to illustrate what I keep geting ! Box 6 is the implementation of apply , and box 7 is what my data looks like .
I think because you strip incorrectly : ` 11 / 14 / 2015 00:00 : 00 ` should be `' %Y / %m / %d %H : %M : %S '` but not with `' - '` symbols between month and day
If you'd like to work with ` df.columns ` you could use ` map ` function : #CODE
You could use apply method for pd.Series of dataframe
So you could convert your ` df.columns ` to ` pd.Series ` with ` pd.Series ( df.columns )` and then use apply method . Look to the last edit

I am a longtime R user trying to make the shift to Python ... It's gone well until I encountered some strange automatic typecasting by Pandas when I perform a ` groupby ` function on my frame . I have a Pandas dataframe , ` df ` , resembling the following : #CODE

For example for ' ind ' = ' la ' and the ' diff ' column : #CODE

I'm thinking I kind of need to do a reverse pivot . Is there an easy way of doing this ?
Use panda's ` melt ` function #CODE
thanks @USER , I did not know ` melt ` , but it seems to be exactly the way to go . I updated the answer accordingly .

How to shift the column headers in pandas

Python Pandas Dataframe Columns of Lists , Get Intersection And Apply Function To Another Column
I'm not used to working with ` lists ` in columns of Pandas and don't know how to get the intersection of ` lists ` from two columns in a ` dataframe ` , then get the index of where the words appear , then apply plus signs to the front of each found index . Or maybe easier would be a string replacement on ` df [ ' Keyword ']` using the words from ` StemmedAG ` ?

So , I think I figured this out . First one needs to stack the data using ; .unstack ( level=-1 )

How to do this ? Do we have any function ? I tried with apply and groupby function , did not work . Please let me know if more information is required for this .
I improved function ` calctot ` - delete columns and then append sum row .

Personally I'd recommend you just use the database system itself instead of loading stuff into memory and crunching with python . If you set up the data properly you can issue many thousands of queries in just seconds . Pandas is actually written to mimic SQL so most of the code you are using can probably be translated directly to SQL . Just recently I had a situation at work where I set up a big join operation essentially to take a couple hundred files ( ~4GB in total , 600k rows per each file ) using pandas . The total execution time ended up being like 72 hours or something and this was an operation that had to be run once an hour . A coworker ended up rewriting the same python / pandas code as a pretty simple SQL query that finished in 5 minutes instead of 72 hours .
Anyways I'd recommend looking into storing your pandas dataframe in an actual database table . Django is built on a database ( usually mySQL or Postgres ) and pandas even has a function to directly insert your dataframe into the database ` dataframe.to_sql ( ' database_connection_str ' )` ! From there you can write the django code such that responses will make a single query to DB , fetch values and return data in timely manner .

What if I want to do re-sampling using more flexible time periods ? E.g. every week , or every month ? Can ` resample ` be used for this ?
You can easily resample weekly or monthly as follows ( weekly shown ): #CODE
@USER -Reina This is more efficient , mainly as it doesn't require the unstack > op > stack ( which creates lots of copies , and from 1d to 2d to 1d ) , but also as there's misdirection by grouping by date then resampling rather than resampling directly . Also / clearly my answer is more concise / easier to reason about . :)

Getting Scalar Value with pandas loc / iloc / at / iat / ix

You can use map and a zip / join over the column levels : #CODE
@USER hmmm , maybe python2 doesn't allow the multi-argument join , try `"" .join ( zip ( *x ))`

I am confused with insert counter ( collections ) into a dataframe :

The thing is , I know that I can set it to the Boolean false using str.contains() . But I don't know what the proper way to apply str.contains() to pd.read_csv ' s na_values . #CODE

Pandas Group By Split , get user input , add column to subset , merge back to df
I have two levels of indexes . I split the data on the appropriate level , then I get the user input . Here is were I can't figure it out . I need each level=0 to make a new column . In this code , this becomes a df were level = 0 is only the appropriate value . Then I want to merge it back to the original df ( I realize that the merge may not be right , I am taking this one step at a time ) .
And then a failure . I am not sure why it says I am making a copy , and I am using .iloc to declare a new df , and merge it to the original .

pandas to_csv comma were replace with a
and I am using ` append ` mode in ` to_csv ` function , but the previous data were cleared , I am confused !
The ` a ` does not refer to append mode ( I assume thats what you're talking about when you say append mode ) . Which means every time your loop runs , ` my_file.csv ` is being overwritten by the latest dataframe going through the loop .
I'm not sure what you want your resulting file ( s ) to look like , but you'll want to make sure you either use a different filename for each dataframe , or join all your dataframes together first and then write the new larger dataframe to a single csv file .

i have a dataframe and a column with integer values ( in my case 0 and 1 ) . The index is time . I need a list when the " areas " with ones start and end . I could do that with diff and followed by loop .
You can use ` diff ` and ` zip ` to get the start and end indexes : #CODE

I'm trying to merge row in a dataframe where I have different inputs for one ID , so I'd like to have a single row for each ID with a weight .
and I would need it to merge the A , B columns for I D= 2 into a weighted average ( 0.3 * 0.35 + 0.6 * 0.55 for A , 3* 0.35 +5* 0.55 for B ) . For column C I'd need to chose the value associated to the highest weight ( C=c for I D= 2 ) , column D the maximum value ( D= 3 in this case ) and the final weight as the sum of all weights ( 0.35 + 0.55 ) . Basically , I need to assign several different rules to each row for duplicate ID's , and I haven't found how to do this .

Firstly as the dtype are ` datetime64 ` then you have the ` dt ` accessors to return just the ` hour ` component you can use this to perform a comparison .
So now it complains that the `' Series ' object has no attribute ' dt '` . I'm using version ' 0.13.1 ' - is this feature available in later versions ?

I'm sure there is still a better way but as I easily fit within memory I just did a SQL type merge . Definitely just a bad day .
Use ` shift ` to shift 7 spots
Drop unneeded data

I'm only interested in the content of the column , but I think that ` for x in xrange ( 0 , len ( tweets_data )):

To do that just use ` read_csv ` , ` concat ` the files , then loop through them . #CODE

How to get just the value of the values column on python pandas pivot table
How to get just the value of the values column on python pandas pivot table #CODE

i want for each block to add 3 new ' 0 ' concentrations for each Name other than the name ' PB ' , and then append the values from ' PB ' to the newly added ' 0 ' concentrations .
The first step is to read the data into a Pandas DataFrame . If the format of the dataframe is consistent with the example you gave above , then you can apply the function to the groupby object . It seems some error ( s ) come out of the reading procedure .

I am working with an irregular df . I am trying to get rid of the initial NaNs and shift all values to the top leaving NaNs at the bottom . I want to perform a realignment of the value at the top which ignores the date .
Is ` STRIP ` a column or index ?
You iterate over the cols and using ` first_valid_index ` and ` get_loc ` ` shift ` the col values : #CODE
Another method using ` apply ` : #CODE
If ' STRIP ' is a column then you don't need ` get_loc ` : #CODE
I think you can just stack the valid numbers and nan's back together : #CODE

Use ` loc ` with a mask to assign : #CODE
I've updated the first solution also , you'd have to call ` fillna ( 0 )` to replace the ` NaN ` values
Then replace the NaN's with zeroes with ` df.fillna ( inplace=True )`

I would like some help to figure out how to pivot a pandas dataframe into a table with a given list of indices and columns ( instead of the default behavior where the indices and columns are picked automatically by pandas ) . Apologies if this is trivial . I am new to python / pandas .

the code below would replace a given column with the same values with another value #CODE

I would not suggest to use ` int ( ma )` as you pointed out in your comment , because that would cut away the decimals .

I found out the problem . I am using a date index ( for both datasets ) , and it doesn't seem to like that . I could merge the datasets together , but then I don't think there's a good way to extract one column and plot it differently than the others . It can be done using the same plot style : #URL

You just need to create your ` MultiIndex ` and replace your column index with the one you created :

I am able to add a new column in Panda by defining user function and then using apply . However , I want to do this using lambda ; is there a way around ?
df [ ' c '] = df.apply ( lambda x : len ( x [ ' a ']) if len ( x [ ' a ']) > len ( x [ ' b ']) else len ( x [ ' b ']))
You can use function map and select by function ` np.where ` more info #CODE
Next solution is with function apply with parameter ` axis=1 ` :
axis = 1 or columns : apply function to each row
Map might works but mainly I am looking for a way to use Lambda with two columns and create a new column if possible

This avoids the apply so will be more efficient .
Thanks Andy , I will post a new question . I know there is a group by rank function but the challenge so far is to combine that with logical operator of a different column and then apply to all rows . I really appreciate your help .

Now I need a new column which will contain the diff ( in days ) between two consecutive date field values ( transaction dates ) .
like thi s a new column conataing the date diff in days for two consecutive date field value for a perticular groupby group.As i have to calculate avg of each groups days_between_transaction .

@USER Can you give a full list of non-numeric characters you'd expect or do you just want to strip any characters that aren't numeric ? In either case , you can still use ` rstrip ` , you just need to pass a string with all the characters to be removed .

call ` apply ` and pass func ` len ` : #CODE

Translate pandas column with TextBlob
I'm trying to read a csv and translate one column that is written in French to English with the TextBlob package in Python ( 2.7.10 Mac OS X Yosemite ) .
On second thought , I actually think I don't need numpy here . But how can I make pandas to read the content field and have textblob translate this to English . Preferably placing this in a column named ' English '

I would like to use the interpolate function , but only between known data values in a pandas DataFrame column . The issue is that the first and last values in the column are often NaN and sometimes it can be many rows before a value is not NaN : #CODE
You can try ` apply ( pandas.Series.interpolate )` instead of ` fillna ` .

@USER first transpose your dataframe , then apply my solution : #URL

Python pandas groupby transform / apply function operating on multiple columns
Trying to use apply-split-combine pandas transform . With the twist that the apply function needs to operate on multiple columns . It seems I can't get it to work using ` pd.transform ` and have to go indirect via ` pd.apply ` . There a way to do #CODE
Note it's important to return a series or it won't align : #CODE

Use join to explicitly add a separator , which you can't do with sum : #CODE

The string join results in the following : #CODE
Which you then use ` eval ` to evaluate , effectively : #CODE

I have a 30GB csv file with 2 columns , 80M rows . One column has 80M unique elements ( emails ) , the other column 5M uniques ( anonymized senders ) to which the 80M map many-to-one . I want to output a csv with only the 5M rows : sender , emails sent by sender
which takes O (8 0M x 5M ) to execute . For every sender , it goes over all 80M messages to match the sender , and join the matched emails .
I get a factor of ~5 speed improvement on a simple N= 10,000 test case by using a pandas groupby / apply and writing the csv from the resulting dataframe : #CODE

I checked the type of b is list type and len ( b ) is 6035980 .
But c = len ( x ) = 68516 , much smaller than 6 millions .
Again , if I did not append , but only print , it worked : #CODE

You can use apply with ` to_json ` : #CODE

As far as the ` drop_duplicates() ` , what you're seeing is a known bug and should be fixed in ` 0.17.1 ` through merge #11403 .

Here is a pretty clunky workaround that I was able to get what I need . I loop through the columns , find those that are made of dicts and then divide it into multiple columns and merge it to the dataframe . I'd appreciate hearing any ways to improve this code . I'd imagine that ideally the dataframe would be constructed from the get-go without having dictionaries as values . #CODE

To apply functions to this object , you can do a few things :
If you want to pass a custom function , you can call ` grouped.apply ( func )` to apply that function to each group .
The last question about the average over a range of columns relies on pandas understanding of how it should apply functions . If you take a dataframe and call ` dataframe.mean() ` , pandas returns the mean of each column . There's a default argument in ` mean() ` that is ` axis=0 ` . If you change that to ` axis=1 ` , pandas will instead take the mean of each row .

However , when I apply this in IPython , it gives me this error : #CODE

I've read this post as well as the pandas 0.17 documentation , but I can't figure out how to use it to selectively replace the column names in a way that doesn't require me to assign new column names manually like this post .

I want to interpolate column B using ` method= ' values '` . But I want NaNs if the delta X between consecutive rows exceeds a certain threshold . For example , if I specify a max delta x of 3 , I'd expect something like this : #CODE

Also , take a look at [ ` ast.literal_eval `] ( #URL ) to handle all Python literals ( without using unsafe ` eval `) . Let's you do a single call to convert to either ` int ` or ` float ` as appropriate .

You need the -1 for the first row in which the ` shift ( 1 )` will return ` NaN `

We use ` str.get_dummies ` and join it to the original dataframe : #CODE
You can easily drop the original column if you wish .

Maybe ` df1 [ map ( lambda x : len ( x ) ! = 1 , df1 )]` but I think it returns a Series . Not sure if someone else can help improve .
Based on your answer i used the following ` dataf [ ' C '] .apply ( lambda x : len ( x ) ! = 1 )` but i get True and False as results . I want to get the rows with their values .

For what it's worth , I made a csv with a lot of columns and got this : ` print ( ' chart1.size {} ' .format ( len ( chart1.axes [ 1 ])))` ` chart1.size 1619 ` . I'm on a new-ish 64 bit laptop ...

I'm running a gradient descent algorithm on some geo data . The goal is to assign different areas to different clusters to minimize some objective function . I am trying to make a short movie showing how the algorithm progresses . Right now my approach is to plot the map at each step , then use some other tools to make a little movie from all the static images ( pretty simple ) . But , I have about 3000 areas to plot and the plot command takes a good 90 seconds or more to run , which kills my algorithm .
There are some obvious shortcuts : save images every Nth iteration , save all the steps in a list and make all the images at the end ( perhaps in parallel ) . That's all fine for now , but ultimately I'm aiming for some interactive functionality where a user can put in some parameters and see their map converge in real time . Seems like updating the map on the fly would be best in that case .
This is great . I call plot_polygon_collection ( ... ) at the start of my algorithm . Then enable interactive plotting with plt.ion() , build my figure and use col.set_array ( ... ) at the end of each iteration . The map updates in real time .

how to shift all the non empty cells all the way to the left in a pandas dataframe
Is it possible to shift all non empty cells as far as possible to the left so that the column and row immediately to their left is a non empty data item .

This is because you are operating a element ` x ` against series ` va2 [ ' pct_vote ']` . What you need is operation on ` va2 [ ' winner ']` and ` va2 [ ' pct_vote ']` element wise . You could use ` apply ` to achieve that .

You could use Pandas ` apply ` function , which allows you to traverse rows or columns and apply your own function to them .

instead of ` x1 = dataf [ ' business_id '] [ newdf ]` ? in that case i got the error ` AttributeError : ' Series ' object has no attribute ' join '` @USER
join is a bit strange . You write connextorstr.join ( thelist ) . So only strings have a join function . Have a look at the python docs to join ( sry , I have no time to Google it and give you a link ) @USER

In this case the desired ` MultiIndex ` has three levels , therefore each tuple in the list of tuples has ` len == 3 ` , just like the ` names ` list .
Yes , but it is an index like ` a ` , ` b ` and so on , instead of being part of the multiindex such as ` loc ` and ` S ` . When I do ` df.plot() ` , It also shows the ` dist ` part .
I'll point out that I was shooting for generality rather than speed . Notice that I used a dictionary to obtain the value of ` dist ` from the values of ` loc ` and ` S ` . In your application perhaps you have a function that calculates ` dist ` , you could just plug in that function instead of the dictionary that I have used .

Somehow a naive way to do it without regex and assuming that the value you want to replace is contained in your desired value or the other way around . I ? not then the original value is retained : #CODE

and apply it column-wise : #CODE
` .T ` at the end for transpose .

How to replace part of a string in Pandas ?

I'm looking at some financial time series data , and am having trouble adding a new column to a pivot table that I created using a dataFrame . The original data frame is called df and has the following format . #CODE
df is indexed by the column " DataDate " . I went ahead and created a pivot table that would give me the values in " DlyReturn " first by indexed by " Factor " and then indexed by " DataDate " using the following #CODE
the new object pivot now has the following format : #CODE
I'm now trying to add a column to this pivot table that gives me the rolling standard deviation for BETA , MOMENTUM , and SIZE across the entire date range contained in " DataDate " . I made the following attempt but kept getting an error #CODE
Does this return inaccurate values : ` pivot [ ' rolling_std '] = pd.rolling_std ( pivot , window=252 )` ?
No errors here . Try converting your pivot_table result into a dataframe type : ` pivot = pd.DataFrame ( pivot )` .

We can group by the User Id , and then for each group apply a function to evaluate the difference between the rows .
Now define a function that will operate on each group , and apply it .
The way to think about this in terms of a vectorised operation is to use ` shift ` to offset the timestamp column , and then just use a subtraction ( this will be broadcast along the array ) #CODE
This is excellent . Thanks for introducing the shift function .

You are looking for the pivot operation , which looks like reshaping the table from long to wide : #CODE

You don't really need to use groupby . You're probably better off making a dict with the mapping and then just using ` map ` to assign it : #CODE
Once you have those you can map them into unique random numbers safely however you like .

How would you do it in SQL ? Can one avoid using a JOIN ?
Why would you avoid join ? Even in background pandas does join :)
@USER Aren't FuzzyTree's queries using a subquery or a window function more efficient than doing a join ?
You can join to a derived table that contains the aggregate value for each grouping of b #CODE
First , I forgot to say that I was trying to avoid a join , sorry . Is it possible ? On another issue , why do you need the ' distinct ' ? I want to considerer all values of A ( unique and not unique ) for each value of B .
@USER mysql doesn't support window functions , so I don't think there's a way to do it w / o a subquery or join

Apply a numeric rank column basis the datetime
Find the rank for desired datetime , and shift ( i.e. , subtract ) the column by that much . ( eg . ` df [ c ] = df [ c ] - 500 `)

I don't think drop duplicates is needed . Doesn't ` df1 [ df1.duplicated() ]` subset correctly ?

To explain a bit , ` str.get_dummies ` method makes a new column for every value it sees in the column specified and then marks a 1 for the values present and a 0 elsewhere . The GroupBy and Aggregate methods make clusters according to the customers and add up the columns . Aggregate will silently drop columns which it can't add , in this case the original ` Genres ` column .

You can call ` apply ` and convert your dict values into a set can convert the ` intersection ` to a list : #CODE

Replace index of a DataFrame with another index from another DataFrame

I am running pandas v17.0.0 on Anaconda stack ( 2.4 ) of Python 3.4.3 .
Running to_numeric via apply on the dataframe iloc selection , ie #CODE
So your approach of using ` apply ` is the correct one if you want to use this functions on several columns at the same time .

Merge dataframe columns that contain a list
and i want to merge the cells like this #CODE

You can apply difference of sets converted from lists of columns and then convert to list . You have to use ` axis=1 ` , because apply function to each row . #CODE

I want to replace the values in column B with a string , if the respective values in column A are equal to ' a ' . I've tried a few things : #CODE

More efficient way to import and apply a function to text data in a Pandas Dataframe
The code runs fine when parsing a short paragraph , but when working on larger text files the code takes a lot longer . I know the key to speed when working with Dataframes is to avoid for-loops and to apply functions to the whole data set .
My question is , is there a quicker way to apply a function to a string when reading in a text file , other than line by line and appending it to a dataframe ?

If you wish to join the lat_lon values , you can do that as follows : #CODE

Now that I have all these lists , I can create a new dataframe ' results ' merge the date back with the original dataframe using the ix as the merge key to make sure everything stays lined up

I need to loop through each line and check the latitude and longitude data I have ( 2 separate columns ) and append a code ( 0 , 1 or 2 ) to the the same line depending on whether the lat , long data falls within a certain range .

Right . The problem is the way you save it . You apply jsonification to it twice . You should do something like ` with open ( myoutfilename , ' w ') as f :\ n f.write ( dataframe.to_json() )`

@USER len ( wordslist ) shows 6672 which means all words are in the wordlist .
You should probably also strip punctuation etc . to get what you really want otherwise this will happen : #CODE
replace #CODE
Yes , I understood from the comments to my questions . But how can I verify that all the words from input file has been considered in the output file . Also how to strip punctuations ?

inner join
I will like to avoid merge if possible

Pandas.apply with dependency on previous value ( not shift )
I am trying to apply a function to each row in a dataframe . The problem is , the function requires output from the previous row as an input .
The issue is from the parameter emaprime as this is computing the current ema value . I am aware I can shift the df to get sampleprime and deltats values .
My data is sparse and irregularly spaced . It is not feasible to resample / interpolate and run over this expanded dataset for each window .
It looks like you want to apply a recursive function . In that case , .rolling_apply won't work . One way would be to use the series values as a list or numpy array . Then loop through the list to use the recursive function .

Join date / time from two columns in dataframe into one

It actually works either way . If you use the transpose of this matrix so the columns sum to 1 instead of the rows , you just have to right multiply then , i.e. above change ` state = np.dot ( start , Tmult )` to ` state = np.dot ( Tmult , start )` .

We could replace the elements in ' fruit ' that are not ' mango ' to ' others ' , then ` groupby ` the variables ( ' customer ' , ' fruit ') , get the ` sum ` and ` unstack ` . #CODE
Try grouping certain columns and then apply sum() like this : #CODE
, Thanks.But is there any to rename those False True to spent_on mango and spent_on others.or how will i get these in two diff columns .
Generally if you see a stack / unstack you should instead try " pivot " :) .

then in order to add your time delta to your date , you need to fulfill two conditions , first you need to create a ` timeseries ` of your date so that later you can add time delta to it , the second thing is that newly created ` timeseries ` must have the same number of elements of your ` timedelta ` , and this can be achieved by ` repeat ( len ( sample_size )` #CODE

Pivot or Transpose a table in Python / Pandas

You can use ` groupby ` after using ` cut ` to bin the index of the DataFrame . For example : #CODE
` np.arange ( 20 , 60 , 10 )` defines the bins that will be used ; you can adjust these according to max / min values in your ' freq ' column .

Replace string values in a dataframe based on regex match
I want to replace the whole text and just put : " Not accredited "
The first argument passed to ` replace ` , e.g. ` r ' ( ? i ) . *not . * '` , can be any regex pattern . The second can be any regex replacement value -- the same kind string as would be accepted by ` re.sub ` . The ` ( ? i )` in the regex pattern makes the pattern case-insensitive so ` not ` , ` Not ` , ` NOt ` , ` NoT ` , etc . would all match .
` Series.str.replace ` Cythonizes the calls to ` re.sub ` ( which makes it faster than what you could achieve using ` apply ` since ` apply ` uses a Python loop . )

I want to perform a " join " on both of the data in a such way that my resulting dataframe would contain : #CODE

I know for this small and fixed ` DataFrame ` I can just use ` ix ` method with the desired position , as follow :
Yeah , don't you want to replace values in col2 with zero values and with index a to be any number ?

The answer may come down to ' calculate some intermediate columns until you can do a simple merge ' or ' just use ` apply() ` , but I could swear that there was a way to do what I've described above .
You can index ` small ` with True and False and just do a straight ` .ix ` lookup on it . Not sure it's all that much tidier than the intermediate column / merge : #CODE
One approach is to use a ` merge ` in which the ` on_left ` is not a column , but a vector of keys . It's made simpler by setting the index of ` small ` to be ` is_even ` : #CODE

To remove them , you can drop the level : #CODE

append multiple pandas data frames to single csv , but only include header on first append
I need to create a .csv file and append subsets of multiple dataframes into it .
All the dataframes are structured identically , however I need to create the output data set with headers , and then append all the subsequent data frames without headers .
I know I could just create the output file using the headers from the first data frame and then do an append loop with no headers from there , but I'd really like to learn how to do this in a more efficient way . #CODE
To do it efficiently , you can use one of the Merge , join , and concatenate so you have two complete dataframe ( ` yankdf ` and ` metsdf `) , then write to csv using ` to_csv ` as you have been doing .
Using append #CODE
Add data to dataframe using ` append ` instead of re-assigning everytime
No worries , you're only looping to read the files then as you're reading you append to dataframe . Once all the files are done being read and you have a single df , then write to csv without loops . How big are all the files combined ?

try using apply function . #CODE

Imagine that my ` store ` ` SFrame ` has 8 columns which I want to append to ` train ` . The code above would be super inefficient .
You could do this using a ` join ` operation .

I am simply trying to replace the index of a particular dataframe based on the index from a another dataframe . To do this i do it like this A.index = B.index
Then i have a second DataFrame which reads from an xlsx file , my aim is to simply replace the index of this file from the index of nai_data using this technique A.index = B.index .
The error message says that the length of both indexes is not equal ( 22 vs 29 elements ) , so you cannot just assign it . So well , is this the case ? Did you check the length of both ? ( ` len ( A.index ) , len ( B.index )`)

Pandas apply but only for rows where a condition is met

pandas concat / merge and sum one column
I have two ` pandas.DataFrame ` objects with ` MultiIndex ` indices . Some of the index values are shared with the two dataframes , but not all . I would like to merge these two data frames and take the sum of one of the columns if the row ( index value ) exists . Otherwise , keep the row and column value as it exists .
What I want to do is merge these two , and sum the ' b ' column , but keep all rows whether they exist in one or the other dataframe : #CODE

I have a dataframe that I created from a text file . Columns B-F should apply to all null fields below them , then once all nulls are filled the next set of periods should be filled by the next values populated in B-F . How would I go about accomplishing this ?

but the problem is this will fail as the indices don't align .
I meant to also comment , when I do that add I get a ` ValueError : cannot join with no level specified and no overlapping names ` . It's strange as I thought add was an outer rather than inner-join .

But what is the common approach to this problem . Is this where people apply normalization ? It would be great if someone could explain how to apply normalization in such a situation .
Group series using mapper ( dict or key function , apply given function to group , return result as series ) or by a series of columns

len ( self.values ) , len ( self.mgr_locs )))
Try this ` result = df2.groupby ( by= ' A ') [ ' B '] .unique() .map ( lambda x : len ( x ))`
As a workaround for now , you can easily use the ` nunique ` Series method through ` apply ` instead of calling it directly on the groupby object : #CODE

and then append this to master dataframe with #CODE

You may ` groupby ` on ` A ` and ` df [ ' C '] 0 ` , and ` unstack ` the result : #CODE
if you want to add these to the original frame corresponding to values of ` groupby ` key , i.e. ` A ` , it would require a left ` join ` : #CODE

I have a pandas.DataFrame object that contains about 100 columns and 200000 rows of data . I am trying to convert it to a bool dataframe where True means that the value is greater than the threshold , False means that it is less , and NaN values are maintained .
Try to replace your ` func ` with this line : ` return x > = threshold if x is not None else x ` , it might be faster . BTW why did you assign two ` lambda x ` ? ` df.apply ( func )` will do the trick .
Combine the output of ` isnull ` with ` df = threshold ` using bitwise or : #CODE
OK I missed the ` NaN values are maintained ` part . This is not pretty and it is still slow ( but faster than apply ):
But that is different from what you will get using the apply method . Here your mask has float dtype containing NaN , 0.0 and 1.0 . In the apply solution you get ` object ` dtype with NaN , False , and True .
Neither are OK to be used as a mask because you might not get what you want . IEEE says that any NaN comparison must yield False and the apply method is implicitly violates that by returning NaN !
In this situation I use an indicator array of floats , coded as : 0=False , 1=True , and NaN=missing . A Pandas DataFrame with ` bool ` dtype cannot have missing values , and a DataFrame with ` object ` dtype holding a mix of Python bool and float objects is not efficient . This leads us to using DataFrames with ` np.float64 ` dtype . ` numpy.sign ( x - threshold )` gives -1 = ( x threshold ) , 0 = ( x == threshold ) and +1 = ( x > threshold ) for your comparison , which might be good enough for your purposes , but if you really need 0 / 1 coding , the conversion can be made in-place . Timings below are on a 200K length array ` x ` : #CODE

but after that one can apply ` .reset_index ( drop=True )` ( mind drop=True here ) and that returns #CODE
Just the usual apply warning , this can be slow ( that's why we special cased groupby head to use cumcount under the hood , at least originally not 100% sure if it still does ) . :)

My aproach is convert ` Datetime ` column to period by months , then shift column ` a0 ` to 2 new columns .

as_type has casting rules , none of which seems to apply ( you'd think it'd be casting= ' safe ' #URL )

What I think I should do is first do the ' count ' operation and then pd.merge that outcome with a part of my original data frame . Trying merge with help of the pandas documentation didn't get me very far , mostly endlessly repeating rows of duplicate data .. Any help would be greatly appreciated !
You group on ` user.id ` , and then use ` agg ` to apply a custom aggregation function to each column . In this case , we use a ` lambda ` expression and then use ` iloc ` to take the last member of each group . We then use ` count ` on the text column . #CODE

Regarding how to convert your data structure to the hierarchically indexed dataframe , first , you need merge your dataframes for each state together to form a horizontal structure , then you can use transpose and stack to rearrange into a vertical structure . #CODE
This is awesome . In your post , you say , " first , you need merge your dataframes for each state together to form a horizontal structure " - how do I do that ? The transpose and stack seems to work all by itself , although accessing elements appears to be really slow .

How to find and replace a single value in a dataframe in python
If the above were to work my next step would have been to find and replace as follows : #CODE

and then make a pivot table based on this column

In the current implementation apply calls func twice on the first group to decide whether it can take a fast or slow code path . This can lead to unexpected behavior if func has side-effects , as they will take effect twice for the first group .

Pythonic / efficient way to strip whitespace from every Pandas Data frame cell that has a stringlike object in it
I'm reading a CSV file into a DataFrame . I need to strip whitespace from all the stringlike cells , leaving the other cells unchanged in Python 2.7 .
I've tried searching for a definitive answer , but most questions on this topic seem to be how to strip whitespace from the column names themselves , or presume the cells are all strings .
What would happen if you were to do x.strip() on an element that is not an instance of a basestring ? If there aren't any downsides maybe you could remove the check and replace it with a try and except block . That might speed things up .
@USER - attempting to call .strip() on a non-stringlike object ( or thing that doesn't have a strip method , such as a numeric data type ) will raise an exception . You're right that handling the exception could be faster than doing the check - depending on the data and frequency of exceptions .

Pandas Apply Function with Multiple ** Kwarg Arguments
If I understand your question , it seems to me that the easiest solution would be to pick the columns from your dataframe first , then apply a function that concatenates all columns . This is just as dynamic , but a lot cleaner , in my opinion .

Specifically , I am interested in how an Index is actually used when slicing via ` loc ` with particular attention to ` MultiIndex ` .
` pandas.Index ` is intended to be subclassed . In particular it looks like there is some intention of a minimal API so that ` loc ` dispatch works , but the docs are a little sparse here .

Probably not ideal , but this can be done using ` groupby ` and apply a function which returns the expanded DataFrame for each row ( here the time difference is assumed to be fixed at 2.0 ): #CODE

So I want to replace that string with None and then drop NA . I am trying this . #CODE
So you could use apply and pd.to_numeric methods : #CODE

Panda Pivot Table : calculate percentage with respect to two columns
But your pivot table has one index , ` ssname ` , and your DataFrame has no duplicates in the ssname column , so there aren't multiple matching rows for the pivot table's index , so in the pivot_table no aggregation into one row will take place . You can see that here : #CODE

I want to append ( merge ) all the csv files in a folder using Python pandas .

Prevent scientific notation in seaborn boxplot
I'm using pandas version 0.17.0 , matplotlib version 1.4.3 and seaborn version 0.6.0 to create a boxplot . I want all values on the x-axis in float notation . Currently the two smallest values ( 0,000 01 and 0,000 05 ) are formatted in scientific notation .
Seaborn Boxplot documentation says , I can pass an Axes object to draw the plot onto . So I tried to create an axis with scientific notation disabled and passed it to sns.boxplot : #CODE

what is your first column name , the one that contains ` name , gender , etc ,.... ` ? try to replace ` x.set_index ([ 0 ]) .T ` with ` x.set_index ([ ' column_name ']) .T `
Your solution is so elegant , it stimulates me to learn deeper in groupby and apply methods .

I have a collection of ` .csv ` files that I want to use to populate a SQL database . My plan is to read the files into a Pandas DataFrame , perform some manipulation , and then write the data into SQL . Across the many files there are going to be many , many , repeated strings that I want to normalise as I insert them into the database .
When it goes to insert the second row it should look at the Experimenters table to realise that John Smith already exists , and has the id 1 . Jane Green doesn't exist yet , so it inserts Jane Green into the Experimenters table , and then takes the id value assigned to her to insert into the second column . Then for the group column it does the same thing , to insert the value 1 .
My best solution so far is to read the Experimenters table and Groups table separate DataFrames , perform merges , insert rows into Experimenters and Groups , and then write to Data .

or you could convert it to a NumPy array and transpose that . #CODE

here ` apply ` will call ` nunique ` on each column

Python 3.4 : Loop and Append : Why Not Working With cx_Oracle and Pandas ?

From the plot , I can see that the distribution is more or less an exponential ( Poisson distribution ) . How can I do the best fitting , taking into account my hist and bins arrays ?
I don't understand too much what you give there . I am trying to use ` param= spy.expon.fit ( distance ); a = np.linspace ( 0 , bins [ -1 ] , 1000 , dtype= ' f ') ; pdf_fitted= spy.expon.pdf ( a , loc =p aram [ 0 ] , scale =p aram [ 1 ]); plt.plot ( a , pdf_fitted , ' r- ')`
That might work , but that's another approach . My linked answer is based on ` spy.optimize.curve_fit ` , and works with the histogram rather than the raw data ( as per your question ) . For this you'd first define a fitting model ` myexp=lambda x , l , A : A* np.exp ( -l*x )` , then use it as ` popt , pcov= spy.optimize.curve_fit ( myexp , bins [ 1 :] +bins [: -1 ]) / 2 , hist )` . Then ` popt ` contains ` ( l , A )` , i.e. the parameter of the exponential distribution and the prefactor for fitting . Does this make more sense ?
Don't ` plt.plot ( stats.expon.pdf ( np.arange ( len ( hist )) , popt ) , ' - ')` , rather ` plt.plot (( x [ 1 :] +x [: -1 ]) / 2 , myexp (( x [ 1 :] +x [: -1 ]) / 2 , *popt ) , ' - ')` ( or with any ` x ` array you prefer ) .
Your ` distance ` will replace ` X ` here .

I've googled and searched through Stack but cannot find an answer to what would seem like this simple question :
All attempts at iloc , loc , slice , sliceindex , and ix have thus far failed . Please help and apologies if this has been posted already .
It's possible to use ` loc ` to slice but the index needs to be ` sorted first ` : #CODE
tuple len ( 2 ) , lexsort depth ( 0 )'

3d bar chart with dataframe in pivot table format
I have a csv with data like the below , which I have had to convert to a pivot table to normalise the data , and ensure that response times where a server does not have a value is filled with the integer 0 #CODE
I am now attempting to draw a 3d bar chart with this information in matplotlib , but I am having issues being able to unravel the pivot data , and see the count as something that could be seen as values of the z axis .

Indexing new column for boxplot of pandas dataframe
Then plot the difference in prices on a boxplot ( based on the time grouping they belong in )
I need to understand how to make sure the ' New column ' is a float and can be indexed for my Boxplot function to plot correctly . #CODE
The Boxplot plots , but gives 0 values for all the subsets #CODE

This function is a merge of StratifiedKFold and ShuffleSplit , which returns stratified randomized folds . The folds are made by preserving the percentage of samples for each class .

There is multiple space in string , so I replace all of them by ` ; ` . Then use function ` read_csv ` with parameter ` skiprows=3 ` , which skip first 3 line of file and ` names ` , which define names of columns . #CODE

I'm having trouble finding how to normalize data in long form in pandas . In R , I would cast the data , normalize , then melt . But I can't work out how to " invert " the pivot_table , here is an example : #CODE

I have 74 relatively large Pandas DataFrames ( About 34,600 rows and 8 columns ) that I am trying to insert into a SQL Server database as quickly as possible . After doing some research , I learned that the good ole ` pandas.to_sql ` function is not good for such large inserts into a SQL Server database , which was the initial approach that I took ( very slow - almost an hour for the application to complete vs about 4 minutes when using mysql database . )
The problem is that insert is not getting any values -- they appear as a bunch of empty parenthesis and I get this error : #CODE
@USER : Using ``` to_sql() ``` yields acceptable performance with MySQL , but not MSSQL . I am using pyodbc . The database is remote , so writing to CSV files and then doing a bulk insert via raw sql code won't really work either in this situation . Additionally , users would need bulk administration privileges to do so , which may not always be possible for users of this application .
I've got some sad news for you , SQLAlchemy actually doesn't implement bulk imports for SQL Server , it's actually just going to do the same slow individual INSERT statements that ` to_sql ` is doing . I would say that your best bet is to try and script something up using the ` bcp ` command line tool . Here is a script that I've used in the past , but no guarantees : #CODE

Apply a value to max values in a groupby
The idea is to write an anonymous function that operates on each of your groups and feed this to your groupby using ` apply ` : #CODE
Assuming that your ' Time ' column is already a ` datetime64 ` then you want to ` groupby ` on ' ID ' column and then call ` transform ` to apply a lambda to create a series with an index aligned with your original df : #CODE
is dt a new feature of pandas -- I get an error when that functionality is called ?

Ipython : 3.0.0 . Enter vs Shift Enter is the culprit for multiple edits . Apologies .

The solution to your question is the ` find_filesets() ` method below . I've included a CSV merge method as well based on MaxNoe's answer . #CODE

Keep getting : KeyError : ' cannot use a single bool to index into setitem ' on this line of code in the second chunk I posted . #CODE

apply a function to a dataframe column ( datetime.date )
I'm trying to apply this formula on a dataframe column ' Days ' ( datetime.date type ): #CODE

From here you can use Datetimelike Properties and call the month by doing ` salesPandas [ ' date '] .dt .month ` , then for day and hour just replace it accordingly .

to replace the indexes 0 , 1 , 2 , 3 with the feature corresponding with your bar-pairs ?

No merge wont work because both of the dataframe are of different length ... Also it print outs all the rows without looking for matching ones
you can then easily drop ` Gene_id ` if you want
Ok try the same solution Replace df1 with Up , this should work
Also Merge wont work on this kind of data frame as the both dataframes Up ( df ) and annon ( df2 ) are of diifferent size , also they are are of different order .. to do a ' merge ' on it
Looks like you didn't copy my answer , actually on Up dataframe the column you want to join on is Gene_Id instead of Gene_id columns are case sensitive , make sure you typed columns correctly
Yes , I correctly checked the spelling .. In fact the question has a typo and in original data Frame its Gene_id and sorry for that .. Because here i think I need some thing which will iterate over each row and check whether Gene_id from df1 ( Up ) is matching to Gene_id from annon ( df2 ) . And if it is matching then I need it to be printed out as I stated in the initial question ... So I think merge is not a right solution is for this issue

You've misinterpreted the function , the passed values are the values to replace , not the index positions

Python Pandas fuzzywuzzy ' join ' of two datasets on string columns
I am following the answer in this question that uses fuzzywuzzy to ' join ' two data sets on string columns .

You could do this using ` map ` : #CODE
This is an alternative solution that is more explicit . First you merge df2 , then add the two columns . Finally , you cleanup by dropping the merged column . #CODE

function won't apply to pandas data frame , getting syntax error
I'm trying to apply this function to a pandas data frame in order to see if a taxi pickup or dropoff time falls within the range that I created using the arrivemin , arrive max variable below .
If the the time does fall into the range , I want to keep the row . If it's outside the range I want to drop it from the dataframe .

If you wanted to translate those list items into strings without the braces / brackets , you could use this function to convert the culprit column into a column that would work : #CODE

Let's say forecast is the function I have created that I want to apply : #CODE
But here we are where my problem is ... How can I apply this function to the dataframes ?
I have tried the apply function as follows : #CODE
The ` apply ` function takes in a function and its args . The documentation is here .
At this point , you can join that result onto ` df2016 ` if you need this to appear in the ` df2016 ` data set : #CODE
If the two DataFrames happen to have compatible indexs already , you can simply write in the result column to ` df2016 ` directly , even if it's a computation on another DataFrame like ` df2015 ` . In general though , you need to be careful about this , and it can be more general to perform the join explicitly ( as I did above by using the ` merge ` function ) . Which way is best will depend on your application and your knowledge of index columns .

How do I apply datetime.date() and datetime.time() to the whole series

Pivot in ID in Python as change column name
Then ` pivot ` and rename columns . #CODE

You could set ' s.no ' as the index first ( if it isn't already ) and unstack to get the columns into a Series . You can then use ` get_dummies ` and sum the level of the multiindex to get the result : #CODE

Python 2.7 groupby and then join
My question is about how to join 2 dataframes , which were created by the ` groupby ` method and the ` sum() ` and ` max() ` functions .
Then I try to join them : #CODE
When you aggregate using ` groupby ` the columns you're grouping by get moved into the index . You should be able to reset the index with ` tr_bin_dep_grouped.reset_index ( inplace=True )` and then merge the two dataframes together .

Whether the speed of this operation is important is unknown -- it probably isn't at all . It's certainly the nice , normal , readable thing to use serieswise operations rather than apply in these sorts of cases , though .

So is the issue that I'm creating lists in my dataframe or that I'm applying a function to adjacent rows ? I just used the list function arbitrarily in this example and am more interested generally in how to apply a function to adjacent rows .
You can write your own function that accepts a subdataframe in the groupby ... what function do you want to apply ? Or is the question how to groupby adjacency ?

Quintessence of the SQL transpose / pivot query . If dataframe originates in a database consider such a solution . Processing is handled in SQL engine not in cpu memory .
check the documentation for ` pivot_table ` , and ` melt `

However , when I want to resample using a multiple of a base frequence , #CODE

I've done some debugging and everytime I get this error is when apply duplicates the first group .
So are there any better ways to do it without using apply ?
Consider simply creating an absolute value column through a defined function , apply the function on a groupby , and then sorting item ascending and absolute value descending . Finally , filter out the newly created , unneeded column : #CODE

Un-pivoting / Stacking a pivot table with Python Pandas

I have a pandas dataframe df and I would like to drop columns which have a mean greater than 10 and less than 2 . How can I do it without a loop ?
You can't use drop , but you can index ... You also need to use ` ` rather than ` and ` : #CODE
your answer is correct but i think , axis=0 since i want to drop column instead . Thanks again
Actually , when i apply it to some dataFrame where some entries are nan , and some with unequal length , I have this error : Unalignable boolean Series key provided

I looked into the error output for index . #URL shown on the image but the fix is not obvious . ( Some other backtraces on similar ( but not identical ) issues on pandas-related github bugs put index.py in the middle of a healthy call stack . )
I might dig deeper into the eventprofiler code if I have time , but I thought I'd ask first . Stack Overflow has almost nothing on QSTK and nothing on pandas that seemed obviously relevant in a quick 5 minute search .
You may download an EventProfiler.py file with the fix from here . Rename the original one in your installation and replace it for this one .

Basically , we use ` np.column_stack ` to stack ` column-1 ` with ` column-2 ` and then again ` column-1 ` with ` column-3 ` to give us a 2D NumPy array ` arr2D ` of shape ` N x 4 ` . Next , we reshape ` arr2D ` to a ` 2*N X 2 ` array and split along the rows with ` np.vsplit ` to give us the expected list of ` 2D ` arrays .

It's not really a bug , and it's just when you use ` apply ` . Did you read the documentation referenced in the github function ?
Can you make a more demonstrative example ? You're not really even using the data frame on which you're calling ` apply ` other than the needless print statement . Seems like omitted that would provide the desired results .
I can imagine a use case would be : generating a column representing a state , whose value changes according to both the previous state and the other current row values . This is typically a case the Pandas doc warns about when using the apply method ( " side-effect ") ...
` apply ` works fine for that .

Merging median from one column by a key column - SFrame / Pandas
To get the median Sales per store , I can do the following to attach a new column for the median sales per store using ` graphlab ` : #CODE
This extracts the median sales per store as per the graphlab code , but when i try to merge it back to the train matrix with : #CODE
How could I merge the median of the " Sales " column using " Store " as the key using ` pandas ` ? The ` graphlab ` code works though .
the merge error simply says that you have duplicated column names across left and right frame , so either you need to provide suffixes to distinguish columns or rename the columns : #CODE

I tried an inner join , but I couldn't find a way to subtract it from df2 .
You can use a list comprehension together with ` join ` to create unique keys of each table consisting of the the first name , last name and the date field ( I assumed date of birth ) . Each field needs to be converted to a string if it is not already .
You can append the two frames and use ` drop_duplicates ` to get the unique rows , then as suggested by @USER you can use ` iloc ` to get the rows you want : #CODE

What is the best way to reshape / collapse / unstack the multilevel DataFrame so we can make use of all the data for our regression ? Other levels may have lesser rows that ` df_Y `

I think you're missing the fundamentals of apply , when passed the Series ` clasif ` , your function should do something with ` clasif ` ( at the moment , the function body makes no mention of it ) .
You have to pass the function to ` apply ` . #CODE

Done . Explicit map solved my issue . The outlook properties accessor gets its values as type ' instance ' and therefore ' str.contains ' did not work . Great help thanks ! End to end solution will be to create a function to map non str columns to strings during extraction . =D

Python - replace exec for dynamic variable creation

for example , i'm looping through a dataset . after the first loop , i get a subset of my dataframe ( rows 1-10 for example ) . for the next loop , i want it to start iterating on ` index = 11 ` , and then apply whatever alg i have . the thing is , i do it by truncating the dataframe . so if the last item in the previous run was at ` index=10 ` i truncate the dataframe to ` df = df [ 11 :] `
is there a way for me to just " point " at the row in the dataframe so that i don't have to truncate it ?

which should be avoided by using ` loc ` : #CODE

Python Pandas pivot table how to handle ' \xc2\xa0 ' ?
Oh I see , you've got some glyphs in that column . Replace those by empty values maybe ( before importing to pandas ) ?
@USER How do I replace ? I don't know what cahracters they are . When I open in text editors like ` gedit ` , they appear as blank spaces .

When I perform the above cuts the code is working fine . Next , I want to add one more cut : I want to select all the targets that have ` 4.0 logg 5.0 ` . However , some of the targets have ` logg = -1 ` ( which stands for the fact that the value is not available ) . Luckily , I can calculate ` logg ` from the other available parameters . So here is my updated cuts : #CODE
The error is from the ` for ` loop . I also tried ` if catalog.loc [ i , ' logg '] == -1 : ` and now the error I get is ` AttributeError : ' DataFrame ' object has no attribute ' loc '`
So I didn't understand what you want .. AFAIU You need to select all ` ( catalog.rp ! = -1 ) & ( catalog.mp ! = -1 )` then replace all rows where ` df.logg == -1 ` to ` df.ix [ i , ' mp '] / df.ix [ i , ' rp ']` and then choose all rows from modified ` df ` where ` ( df.logg > 4 ) & ( df.logg < 5 )` . What exactly do you want ?
What I want is the following : First , I select all ` ( catalog.rp ! = -1 ) & ( catalog.mp ! = -1 ) . Second , IF ` catalog.logg == -1 ` , replace ` -1 ` by ` catalog.mp / catalog.rp ` . Third , select all the entries ( from the newly modified AND original df ) where ` ( catalog.logg > 4 ) & ( catalog.logg < 5 )` .

I presume it has something to do with the fact that the labels are quite long and are getting cut off by the bar chart plotted 2nd ,... but im not sure .

Date ` lastdayfrom ` is used for selecting last 30 days of ` DataFrame ` by function loc . #CODE

I am trying to iterate over groups ( produced by group.by in Pandas ) in order to apply a function ( create a chart in MatPlotLib ) and get a result for each group in the DataFrame . I thought to do something like this , but I know there's a better / functional way : #CODE
The result of the groupby function is a pandas data frame or series . You can use the apply function . See below example : #CODE

You can filter the columns first to get the cols of interest and then call ` apply ` and use the boolean mask to mask the cols : #CODE

Bokeh heat map not displaying correct results
I am trying to make a chloropleth map based off of bokeh's map of US unemployment , and I am not very familiar with this library , although I am somewhat familiar with pandas . I have a dataframe of states and their priority for our system . I cannot get the priorities to display correctly . I think I am using the fill_color attribute incorrectly but I can't find any more information about it other than the one line listed in the bokeh docs .
try to replace your loops with something like that : #CODE
and then replace source by #CODE

But now I don't know how to merge it with the original ` A ` .
You can apply function ` f ` for each group .

I have a sparsely filled Dataframe with continous dates as index . I want to interpolate my data between the known points . #CODE
TypeError : Cannot interpolate with all NaNs .

make a shift by index with a pandas dataframe
I create a new column based on a shift of an other column , but that's not a shift in number of columns . That's a shift based on an index ( here the index is a timestamp )
have you tried ` shift ` ?
the answer was to resample so I won't have any hole , and then apply the answer for this question : How do you shift Pandas DataFrame with a multiindex ?

Apply Number formatting to Pandas HTML CSS Styling

I can't reproduce this , I get ` bool ` as the ` dtype ` when all elements are boolean
You can also forcibly cast the ` dtype ` ` df [ ' b '] .astype ( bool )`
Because you have a mixed ` dtype ` initially even after calling ` dropna ` then you can coerce the dtype , seeing as all you're interested in is preserving numeric and bool types then calling ` convert_objects ` or ` to_numeric ` will correctly convert the ` dtype ` : #CODE

Is the whole row enclosed in quotes ? If yes , you can use the Linux terminal to strip quotes from the ends of the rows quickly . If you are on Windows , or if only a part of your whole row is like this , you can use ` pd.to_csv() ` and write either the whole dataframe or just the problem column back to a csv file , and pass ` quotechar=None ` while you are doing it . Reading this csv file again should solve your problem . I am not able to think of any other better solution at the moment ( without resorting to iteration ) .

below replace ' in the first name with _

I see in the python documentation the ability to resample and synchronize two timeseries . My problem is harder because there is no time regularity in the timeseries . I read three timeseries that have non-deterministic intraday timestamps . However , in order to do most analysis ( covariances , correlations , etc ) on those two timeseries , I need them to be of the same length .
Note that the time series are already read into a pandas DataFrame , so I need to be able to synchronize ( and resample ? ) with already created DataFrames .
Resample timeseries objects using a time vector that is a union of the
Note that they will have ` NaN ` s in all of their new entries ( presumably this is the same behaviour as in Matlab ) , it's up to you if you want to fill these , for example ` fillna ( method= ' pad ')` will fill in null values using the last known value , or you could use ` interpolate ( method= ' time ')` to use linear interpolation based on the timestamps .
It seems like this solution only unions . The Matlab version of union appears to also resample ? " Union Resample timeseries objects using a time vector that is a union of the time vectors of ts1 and ts2 on the time range where the two time vectors overlap . " - Matlab

I am not sure you need a loop here . You can simply create a slice of the data you want , set the index and then unstack as follows : #CODE

Given the following , how can I set the NaN / None value of the B row based on the other rows ? Should I use apply ? #CODE
Apply is the way forward , it seems . ` fillna ` doesn't appear to accept custom functions . [ Reference SO question ] ( #URL )
Then I use apply : #CODE

dropped vectorization and list-comprehension tags - they don't really apply here

I said , let me use a lambda to apply the .hour to every " row " . Thus : #CODE
Using Pandas 0.16.2 , I didn't have a problem getting local US Eastern time from tz aware timestamps . #CODE
I knew about the dt command but it wasn't working . It turns out that when making the time tz-aware by using ' America / New York ' , the dt does not work . If I , on the other hand , use ' EST ' , it works . Is that the expected behavior

I know I can get histogram values with ` np.histogram ` , but I want to use pandas hist method .
As ` xnx ` pointed out in comments , this isn't as easily accessible as if you used ` plt.hist ` . However , if you really want to use the pandas ` hist ` function , you can get this information , from the ` patches ` that are added to the ` hist ` ` AxesSubplot ` when you call ` serie.hist ` .

IIUC then you can combine your boolean conditions and use ` shift ` to test the previous row value : #CODE
So this combines your 2 boolean conditions using ` ` wrapped in parentheses due to operator precedence and tests the previous row using ` shift ` , as you can see entry ` 27 2007-04-10 13:33 : 00 off True ` is set to ` True ` as desired , you can modify the statement to this : #CODE
YES !!! I knew that was the direction but I kept tripping myself up with where the ` shift ` goes . Thanks !!

2.i need to append df1 to df and only columns from df should only remain and it should exclude if any new column from df1 getting appended . But i am getting new column values having NUN values at so many places .
1.is there way that i will read the small sized csv only and append to pickle file ... ( or reading that pickle is mandatory )
2.can it be done like converting the csv to pickle and merge two pickles . by load , dump method ( actually never used that )

I'm trying to create a pivot table that has 3 measures on the same value being aggregated :
It's pretty easy to do that in two stages - standard pivot_table with np.sum and afterwards add two more columns to the pt . What I'm interested is to know if you can do it during the creation of the pivot table , by passing the right aggfuncs .

I've created a DataFrame in my desired date order , however , when I put this into a pivot table the order changes .
I wanted to sort the pivot table base on the newest date of any of the rows within a given level #CODE

call ` set_index ` on ` df2 ` to column ' B ' and then call ` map ` on ` df1 [ ' A ']` : #CODE

This would give me the most recent change in ` a ` and ` b ` namely ` x.loc [ dt ( 2015 , 1 , 16 )]` .
The pandas asof function is meant for this : #CODE

I want to resample the index column ( ` Timestamp `) in 100-millisecond bin time and check in which bin times the signs belong . For instance , between each start time and end time there is 2000 milliseconds difference . So the corresponding sign number will appear repeatedly in around 20 consecutive bin times because each bin time is 100 millisecond . So I need to have two columns only : one with the bin times and the second with the signs . Looks like the following table : ( I am just making up the bin time just for example ) #CODE

IIUC you can use [ ` cut `] ( #URL ) for this
not sure how to apply this suggestion to the problem above . First of all - does your suggestion select the columns in the hypothetical array you suggest ? Or would I do array [: , mask ] which doesn't seem to work ?
many thanks , this is a bit clearer . However , I still need more help - I'd like to use a dataframe to create a stacked bar graph . To do this , I'll need to select the column names corresponding to the ' True ' elements of the array and append ' all others ' which reflect the new column .

` groupby ` on ` transcript_id ` as you do now , and perform your calculations ( say ` agg_df `) . After they are done , merge the two frames together : #CODE
Another solution ( slightly harder ): Merge the columns ` transcript_id ` , ` gene_id ` and ` gene_name ` in another column , say ` merged_id ` and ` groupby ` on ` merged_id ` . Split the column up into the components at the end of your calculations .

In your particular case , this is probably working fine on ` df ` as a whole because ` df ` likely has an integer index ( ` [ 0 , 1 , 2 , ..., len ( df ) -1 ]`) , which is the default row index in a ` DataFrame ` . However , when you select within ` df ` to make ` df0 ` , the result keeps winds up with an index that is a subset of the index of ` df ` ( maybe it winds up ` [ 3 , 6 , 9 , 12 , ... ]`) . So everything works fine on ` df ` ( where the index contains ` 0 `) , but blows chunks on ` df0 ` ( where , ironically , given its name , ` 0 ` does not appear in the index ) .
Glad it helped ! If you're interested , [ here ] ( #URL ) is thread where this issue is being discussed . It would be great if you could include a bit more of the stack trace in your original post so that others can see exactly where the error comes from .

Approximately how many columns do you have ? If there are only a few , it might be easier and efficient to do it by hand , like how you are doing . Otherwise , you can get all the column names , strip the numbers from the end ( ` [ name [: -1 ] for name in df.columns ]`) and then use [ ` sets ` to give you unique names ] ( #URL ) . You can then loop over the unique names , within your function ` f() ` ( after slight modifications ) .

If I print averages right after averages = pd.DataFrame() , I get an empty list . If I change it to averages = pd.DataFrame ( data ) , then print averages , I get the results of each game but no averages . Then if I print averages after averages = averages.concat ( df.groupby ( ' Player_ID ') .mean() .unstack() ) , I get : AttributeError : ' DataFrame ' object has no attribute ' concat '
The purpose of ` averages = pd.DataFrame() ` _is_ to initialize an ** empty ** DataFrame to which the averages will be appended . So when you ` print ( averages )` you are supposed to get nothing . That is the design . Please ** don't change ** it to ` averages = pd.DataFrame ( data )` . About the ` AttributeError ` I actually wanted to write ` append ` , I don't know why I wrote ` concat ` . I have fixed that in the answer .

Pandas merge only if condition is true
I have two DataFrames , I need to merge both , and I need to add a column that specifies if it is accepted or not .
Can I use the apply method to this task ?, can someone help me to do in the right way using pandas .

( Python Pandas ) MemoryError : skiplist failed when using rolling median and apply
This line once computed a rolling median by group but now it produces an error :

Apply group specific function to groups in Pandas
I'm trying to figure out the best way to apply a function to groups within a Pandas dataframe where the function depends on the group .
You can map a function on the index . Each row in the dataframe has a ( Location , Year ) tuple as its index , so you can do : #CODE

What I would like to do is compare them and generate a new dataframe based on their values . For any cell that was False in at least one dataframe , I want to assign it a string ( e.g. " False in Dataframe1 ") and if multiple , append both ( e.g. " False in Dataframe1 , Dataframe2 ") .
Is there some kind of direct dataframe to dataframe comparison I can use ? Or do I need to concat the dataframes so I can compare the columns to each other ?

it seems that you ` x ` should start from 2008-01-30 , after that , you can simply inner join two dataframes : #CODE

This question manages the result for a single column , but I have an arbitrary number of columns , and I want to lag all of them . I can use ` groupby ` and ` apply ` , but ` apply ` runs the ` shift ` function over each column independently , and it doesn't seem to like receiving an ` [ nrow , 2 ]` shaped dataframe in return . Is there perhaps a function like ` apply ` that acts on the whole group sub-frame ? Or is there a better way to do this ?
IIUC , you can simply use ` level= " grp "` and then shift by -1 : #CODE
Great , thanks , I can't remember why I thought I needed to do it with ` apply ` - maybe it'll come to me later .

Part of the solution , because you'll have duplicated rows with slightly different names so you couldn't apply drop_duplicates method of dataframes : #CODE
Pandas is a lot faster and more natural when working with columns . Thus , I would propose to transpose DF first , and then just sum columns

Glad it helped . I removed newline chars initially , but I suppose this is application specific . Sometimes you may want to keep them ( e.g. what if later on you want to know the average length of an email line ) . Also I think you may want to replace them by `' '` rather than `''` since now you merged the last word on every line with the first word on the next line , which doesn't seem a good idea . If it helped , can you also accept the answer please ?

Depends on how you define the last week of the previous month ( final 7 days ?, final 5 trading days , etc ) , it should be easy to compute month-by-month the number of buys in the last week , then just shift this series to align with the subsequent month .

Here's an implementation that uses a MultiIndex and an outer join to handle the cross join . #CODE

You can do this with a group-by , followed by a join , followed by a sort . For example : #CODE
Note that this doesn't match your example output , but it is the correct ordering by total error count . Looking at your example output , perhaps what you want is to replace ` sum() ` above with ` max() ` it's not entirely clear to me from your question .

Python Pandas ( read xlsx and append )
i want to read some columns of an excel-file and finally append them to a csv-Data .

python merge dataframes while choosing the keys conditionally
And I want to merge the lookup table with another table ` table B ` which also has missing values under ` id_B ` : #CODE
I want to merge these two dataframes on ` left_on = ' id_A ' , right_on = ' id_B '` if possible . And for those with missing ` id_A `' s or ` id_B `' s , I want to merge the two dataframes on ` left_on =[ ' first ' , ' last '] , right_on = [ ' fName ' , ' lName ']` . The overall result would be like a left join , so that I only keep the data with keys shown in my lookup table .
Please post what the desired merge df looks like , I sorta understand but you need to post this so that there are no ambiguities

Are you after [ ` interpolate `] ( #URL ) ?
You are right about ` interpolate ` method currently not working with ` Timestamp ` . One solution is to convert it to float , interpolate it and covert it back to ` Timestamp ` : #CODE

eq : #CODE
What I was expecting is to map the unique column values into a dict eg : { " Lip_G D: " 1 , " Jac+Jac " : 2 } and map this int values into the dataframe column .
@USER see my proposed answer below that uses sklearn to map categorical data to numerical values .

Within each iteration , apply the function .

As a general idea , you can use ` apply ` on your grouped data to take compute the ratio for each state : ` sum ( bads ) / sum ( goods )`

Note that you cannot use ' loc ' with the tuple index : #CODE
I believe the hash table is rebuilt when you replace the index . Have a look at the timing comparisons in my edit .

The easiest way would probably be to set ' Date and Time ' as the index and then use ` groupby ` with ` TimeGrouper ` to group the dates . Then you can apply ` cumsum() ` : #CODE

Oh for some reason sort_values() is working now . but how do I drop all the 0 entries in the resulting column ?

The index `' TimeStamp '` for dfEURUSD is different than the dataframe for dfGBPUSD . I am trying to get them to be the same length and resample the missing data so that the two timeseries are the same length . But I run into trouble : #CODE

In reality I need to apply this function on 40K over rows . And currently it runs
very slow using Pandas ' apply ' : #CODE

I loaded 15 sheets from excel sheet and it created dictionary of dataFrames with sheet number as key . Is there a way to actually append 15 sheets while loading or I have to iterate over keys to append 15 DataFrames ?
I mean while loading specifying to append instead of creating dictionary ? #CODE

you can simplify to this : ` df [ ' C '] = df.A.where ( df.B.isnull() , df.B )` as ` isnull ` is available for df and series , also I wouldn't encourage the practice of accessing columns as attributes as it can lead to strange behaviour , better to do this ` df [ ' C '] = df [ ' A '] .where ( df [ ' B '] .isnull() , df [ ' B '])`

how to concat two csv file and then sort by python
I have two .csv files named all_cv.csv and common_cv.csv files . First I have concat this two csv files by pandas , and then save the data into a new file named join_cv_common.csv by pandas . After that I sorted the join_cv_common.csv file by pandas as below , and stored data are stored into a new file named sorted_cv_common.csv . I want to rewrite these two functions of pandas - concat and sort by pure python ( 2.6 and 3.4 ) . Can someone help me in this regards ? Thank you very much .
Pandas concat function #CODE
By my knowledge it can be done by reading both the files using the file i / o after that just join and convert the string to a list after that sort the newly created list and put it in the final output csv by converting the list to a string .

Is there a way to either flatten the header rows of the pivot table into one header row , and to keep the order of the value columns ? Or another way to do this altogether ? I have looked into some other options like ` pd.groupby() ` without finding anything .

Without trying this I can't be entirely sure that the returned value of ` func ` will be acceptable for use with ` apply ` , so you might need to play around with that a little . But this should give you a series with the index being the description and the value being a list or dict of the last five counts .

How do I replace cells in a Panda dataframe column based on a condition
This post consists of two questions , touching on issues I have encountered when trying to replace elements in a Panda dataframe based on a given condition . I am new with Pandas , so any suggestions will be most helpful .
I want to append the integer 0 to each cell that is of length less than four . That is , I want to obtain : #CODE
I want to replace cells in column A that take on the value 5 , with the values on the same row in B .

The table goes on and on like that . The START_TIME and END_TIME are all in milliseconds which are the start and end time of a trial . So what I want to do is , I want to resample the START_TIME into 100milliseconds bin itme and interpolate the variables ( TRIAL_No and itemnr ) between each START_TIME and END_TIME . Outside of these regions , these variables should have the value " NA " . For example , for the first row the START_TIME is 2403950 and the END_TIME is 2413067 . The difference between them is 9117 milliseconds . So " Trial : 1 " stays for 9117msecs which is for aroud 91 bin times since each bin time is 100msec apart . So I want to repeat " Trial_1 " and " P14 " 91 times in the resulting dataframe . The same goes for the rest . Looks like the following : #CODE
After creating new dataframe by ` concat ` dataframes I can group it by row and apply ` resample ` on each of these groups ( with method ` ffill ` to forward fill ) . #CODE

numpy and pandas are not needed here although you need to apply the strip function to every element in each row to remove excess spaces ( ` map ( str.strip , row )`) and also pass ` delimiter= ' | '` into ` csv.reader ` because the default delimiter is a comma . Lastly you need to ` return sum ` at the end of you function .
You can do ` len ( result )` to get the count you're after .

To open the files , parse some data and append it to a dataframe I think pandas is needed . But I am stuck for now , so any tips are welcome .

How can I drop the first value ?

I'd like to change a value in a dataframe by addressing the rows by using integer indexing ( using iloc ) , and addressing the columns using location indexing ( using loc ) .
Mixed integer and label based access is supported by ` ix ` . #CODE

Dask DataFrame : Resample over groupby object with multiple rows
Resample it over a 3-hour period
Thanks for the great solution . Is your use of ` apply ` a recommended method to access pandas functionality that isn't present in dask ( assuming the block can fit into memory ) ? This would be a huge help to by-pass the current limitations of dask dataframes !

There is no direct equivalent in graphlab to ` DataFrame.iloc ` ( previously ` irow `) . One way to achieve the same thing is to add a column of row numbers and use the ` filter_by ` method . Suppose I want to get only the 1st and 3rd rows : #CODE

And i want to merge them so each frame has its column :
I'm trying to merge them together so each of the 10 Dataframes has its own column which I can rename . I searched for a while now and have found some things but nothing works for me , this : #CODE

For the record ` applymap ` is used to apply a lambda function elementwise ( documentation )

IIUC you want filter input csv - select only ` TIME ` columns to datetimeindex . You can use read_csv with parameter ` usecols ` which provide a filter before reading the whole DataFrame into memory . Then all dataframes are joined by concat to one .

I found a similar question on SO for plotting a boxplot here , so I tried modifying it for line plot . I tried using ` df.plot ( column= ' C1 ' , by= ' Type ')` but seems there is no property `' column ' for a plot() ` .
You can add the column " Type " to the index , and unstack it so as to have the values of C1 split in two columns according to the value of Type , and then plot them , e.g. : #CODE

IIUC you need ` resample ` ` df1 ` , because you have an irregular ` frequency ` and you need regular frequency : #CODE
You can use function ` asfreq ` instead of ` resample ` - doc , ` resample vs asfreq ` .
First I think that ` resample ` didn't work , because after resampling the ` Result ` is the same as ` df1 ` . But I try ` print df1.info()` and ` print Result.info() ` gets different results - ` 34857 entries ` vs ` 34920 entries ` .
So I think ` resample ` works well . #CODE
Thanks for the suggestion @USER , I've tried your method , but still have the same issue using ` asfreq ` or ` resample ` . The gaps filled in to make the series regular contain data that shouldn't be there . There are other holes in the index which might have some effect . If it helps , I'm using pandas version 0.14.1 and python 2.7.10
Thanks @USER . You've made it clear for me to see that ` resample ` was working . This made me look closer at my data-set to see that Pandas was interpreting the date differently to how i expected . The data was in _day / month / year #URL - whereas Pandas was seeing this as _month / day / year #URL I've included a ` dayfirst=True ` in the ` pd.read_csv ` and its working exactly I expected it to now . Thanks for all your help !

Expanding a daily roster in pandas to join with other files
You can ` groupby ` by ` EmpID ` and ` resample ` by ` days ` and fill rows by method ` how=first ` .
Then I reset multiindex - first drop first duplicity level and then reset again . #CODE

A slight change to my question , is it possible to normalize this sort in any way ? Like , if A comes 4 times , I want it to be sum() /( no . of occurence of A ) and then sort using this value . How do I do this ?
That would just be the mean . So replace ` sum ` with ` mean ` .

You could use standard method of strings ` isnumeric ` and apply it to each value in your ` id ` column : #CODE

I then would like to apply a function to each of these groups . This function computes two values for each group #CODE
I am having some issues understanding the type of your ` group ` argument to ` compute_thing() ` . Shouldn't apply iterate over the results of the grouping , and the ` group ` argument refers to the current group in consideration ?

The syntax of the operation is incorrect , replace the above split with the following . You also need to wrap each predicate in parens and use ' | ' ( or ) and ' ' ( and ) . This will perform the appropriate splits . #CODE

I'd suggest using ` join ` . First we'll want to set the index of ` df2 ` to be the ` Fruits ` column : #CODE

Pandas merge two dataframes by time column
I have been struggling to merge data frames . I need to have the rows arranged by the time , with both sets of data's columns merged into a new data frame . I'm sorry if this is clearly documented somewhere , but it has been hard for me to find an appropriate method . I tried ` append ` and ` merge ` but I am struggling to find an appropriate solution .
It appears that you want to ` append ` the dataframes to each other . Make sure that your date column has the same name in both dataframes otherwise pandas will treat them as two totally separate columns . #CODE
Also , this blog post is a good resource for ` join ` and ` merge ` in pandas . #URL

Next , you can use a dictionary comprehension together with ` loc ` to select the relevant ` group_no ` dataframe . To get the last group number , I get the last value using ` iat ` for location based indexing . #CODE

Use ` idxmin() ` to get the row index of the earliest shipping date . You can then use ` loc ` to fetch the value at that row from the product ID column : #CODE

I am trying use pandas to resample vessel tracking data from seconds to minutes using how= ' first ' . The dataframe is called hg1s . The unique ID is called MMSI . The datetime index is TX_DTTM . Here is a data sample : #CODE
The code to resample : #CODE

I am familiar with the page . but I dont know how i can apply this to my data and plot , as they are using only one variable with an arithmetic operation , while I have a dataset with different categories .

I know how to loop over the each row to append 1 to matching elements of each columns .

It seems that creating a new ` DataFrame ` may take time memory if your df is large . I tried something like this and it's 500+ times faster than yours on my machine when ` len ( df ) = 100000 ` . If your ` df ` is small , I guess there is no difference . #CODE

iii ) There are no NaNs : notnull ( df.var ) .all() returns True
v ) a period index is used - and that forms the index for pivot table .

The pandas docs give an example of ` concat ` that combines indices ( ` axis=0 `) , by concatenating along the columns ( ` axis=1 `) : #CODE
` concat ` has not duplicated the shared indices , but it has duplicated the columns .
I'm essentially asking for an " upsert " ( insert , update ) operation . So that's an approach that could work :
First , the " insert " , of rows that don't currently exist in ` df1 ` : #CODE

I am trying to merge tsv files using pandas but cannot get pandas to return the file contents correctly . My tsv files contain Italian and pandas fails at accented characters like .

insert rows from Pandas dataframe into mongodb collection as individual documents using Python
I have been attempting to insert the rows of a pandas dataframe into a mongodb collection as individual documents . I am pulling the data from MongoDB using pymongo , performing some transformations , running a scoring algorithm , and adding the score as an additional column to the dataframe . The last step will be to insert the rows into a special collection on the mongoDB database as individual documents but I am completely stuck . My example dataframe df looks like this . #CODE

so you can later apply : #CODE
Looks good ! The performance decreased ( 1M+ rows in my df ) , but I assume that's now because the datatype is storing more data as date versus object . I also forgot to mention that Time_x , Time_y was in result of pd.merge . I parsed the dates on the original and the merge copied the data type automatically . I assume now , I can use the same time difference logic on merge to filter only the time difference in seconds to reduce my df size too .

I can create a basic pivot table from this using : #CODE
Note : I think you have to do this after the pivot as an aggregation function doesn't have access to the other groups .

You will need to use a join operation : #CODE

I receive data from a client in the form of a .csv file . I want to consolidate this information in a postgres database . Ideally , I want to prevent duplicates from being added . I read that you should not use a primary key of multiple columns if you can avoid it .
The way I handle this is I groupby ` call_date ` ( instead of the time ) and select all rows where there was only a single call for the ` person_id ` for that specific ` customer_id ` . It's possible that the same ` person_id ` will speak to the same ` customer_id ` multiple times in a day so I just drop all of those rows from my joins to be safe .
Unless you are building a data warehouse for reporting ( star schema , dimensional data model ) , there is absolutely nothing wrong with having a multi-field primary key . Even then , it's a methodology thing , not a technology thing . As far as setting up your tables -- if your customer data contains repeats , I'd consider piping them into a temp table and then running a function to insert non-duplicates into your actual table . I do this with flat files we receive from FedEx machines . Can you show your table structures ?
Hi wild , that makes sense . I don't need to look at the customer_id until later during my joins . For primary preventing duplicates purposes , the time_measurment and person_id would work . To actually do the joins though , it needs to be at a date level ( since there is a potential for duplicates ) . Join all rows where this person spoke to this customer on this date .
Hi Hambone , I am actually still constructing tables . Right now I do everything purely in python and export to .csv . No databases at all . I read about 3 months of data into memory , groupby at a date+person_id+customer_id level and take unique values and then join the files . I then append the non-unique values ( this prevents Cartesian products duplicating my rows ) . I then export to .csv ( overwrite my file each day ) .

Pandas merge leads to scientific number
Imagine I have two dataframes almost the same and I want to merge them . If I use : #CODE

Although technically correct we should avoid posting answers that use ` apply ` where a vectorised solution exists as this confuses users

` replace ` method of dataframe or series doesn't work in your case because it's trying to find string that match whole pattern which you are set in the ` to_replace ` parameter . If you'd like to match part of your pattern you could specify ` regex=True ` and then set your pattern as regex expression : #CODE
If you want to use the same ` replace ` method for Series for particular column you could use : #CODE
Thank you . The replace method regex=True : TypeError : replace() got an unexpected keyword argument ' regex ' .Pyhton 2.7 My python version is too old .

I would like to split that file into files of len ( index ) = 2 , using linux : #CODE

Figured it out ! Apparently I had some missing values denoted as ' .. ' , so I had to wrangle it out first by dropping those rows - then I can apply .astype

Is there some way i can apply a lambda function to all members of the list columns in order to speed the following up ? Thanks ! #CODE

How to Insert datetime index of pandas DataFrame into mongodb as a field with isoDate type
How to insert df into mongodb with a date.index field in ` isoDate ` format but not datatime64 [ ns ] / timestamps ? .

Adding plot arguments to plots created with a pivot table in matplotlib
I'd like to do the same for a plot created via pivot table : #CODE

All you're doing is skipping rows that have ` ? ` so you can just filter these out using ` apply ` : #CODE

First I append row with max value of column ` odate ` to each group ( grouping by column ` tech `) .
Then you can use custom resample ` how ` method . #CODE

To get pairs , it is a ` combinations ` problem . You can ` concat ` all the rows into one the result ` dataframe ` . #CODE
I did not know about ' combinations ' , but it looks like nice when doing this kind of pair calculation . Also , I learned that making DataFrame from list can be easily don by concat function . Thank you very much !

I could try to save each column as a separate key with index , then when I want to load data I simply load a couple of keys and join them on the index .

Is there a way to apply a ` math ` function to a whole column ?

Here I read the data , strip out the target variables and split the data into testing and training datasets ( this all works fine ): #CODE

Then create a boolean mask to find when the diff is more than 60 seconds : #CODE

You could use apply : #CODE

Output ( after groupby ( ' a ') replace NaN by mean of group ) #CODE

Index levels doubled when using groupby / apply on a multiindexed dataframe
I have a problem when using a groupby / apply chain on multiindexed pandas data frames : The resulting data frame contains the grouped level ( s ) twice !
Is this intended behavior ? How can I avoid that another index level is created ? Do I have to remove it by hand every time I do a groupby / apply operation ?
My actual function that I apply looks different . I just used the sum here to show the effect of a function that takes a dataframe and returns a dataframe ( " case 2 " in the documentation of apply ) .

/ opt / local / lib / python2.7 / numpy-1.10.1 / numpy / lib / function_base . #URL RuntimeWarning : Invalid value encountered in median
I don't know which line in my code throws up this warning . Short of going through every median function call is there a way to print the line number ?

exec ( compile ( scripttext , filename , ' exec ') , glob , loc )

Contact tracing may be easier if I interpolate so that every user has information for every time point ( say ever hour ) though that would increase the amount of data by a huge amount .
How much data ? A good idea would be to sort by time over different files , if that's not the case already . You probably want to calculate at each time increment starting at the earliest time and load files in memory as you go along . Also if the time increment is large for a particular person you probably want to interpolate or the probability of infection will be relatively low :) [ made a mistake and posted this as an answer . sorry ]
First solution with interpolate : #CODE
Second solution without interpolate : #CODE
I have added a version without interpolate . I think it may work

It seems that you would have quite a few columns to aggregate - assume ' date ' is your timestamp , there seem to be seven , ie , data1 - data7 . If you apply three aggregation functions to these seven columns ( mean , min , max ) you'll get 7 x 3 columns with a hierarchical ` MultiIndex ` ( where ` .agg ( dict )` works differently as for ' ordinary ' columns ) . Example follows , including saving to csv at the end . ` GroupBy ` docs and ` to_csv ` docs .
Thank you so much :) it prints the output in the way you've shown . AttributeError : Cannot access callable attribute ' to_csv ' of ' DataFrameGroupBy ' objects , try using the ' apply ' method But I still get this error while printing to csv file

You can use reshape strategies ( ` pivot `) so that you can naturally subtract the result . #CODE

You are looking for the melt functionality , which will allow you to do this in basically one line : #CODE
Then you can use ` apply ` ( or there probably is something better ) , to get the output you have above : #CODE

Interpolate single value from time series
Between data points , I'm assuming people move linearly , so that if we know the location at 8:31 : 10 and 8:31 : 50 , then at 8:31 : 30 they should be exactly halfway between the two locations , and at 8:31 : 11 they should be 1 / 40th of the way between the points ( so interpolating as described here : Pandas data frame : resample with linear interpolation )
interpolate to figure out where they are at that second
After a bit of searching I haven't found any good way to do this for a single value from each group . Other answers suggest using the resample and interpolate functions , but that will take way too long with the size of data I have , and does a lot of unnecessary calculations seeing as I only need one point .
I added more to the question to hopefully explain it a little better . Basically , I want 1 point for each person at a certain time ( e.g. 10:30 : 14 AM ) , but it's unlikely there's any data that corresponds exactly with that time . So I'm thinking I need to get the data just before 10:30 : 14 and just after 10:30 : 14 and interpolate . The data for each person spans for a whole day though .

Then you could drop row with NA values with ` dropna ` or fill them with zeros with ` fillna `
I realised it was the ` infinity ` statement causing the issue in my data . Removing this with a find and replace worked .

How to normalize multiindex dataframe with its sum values in Pandas

I want to normalize this matrix with this formula :

Problems come with the isnull #CODE
Actually , playing with this , ` None ` is working with the ` isnull ` method .

You can call ` unstack ` and pass ` level=1 ` to unstack a specific level : #CODE

Your try with ` np.unique ` didn't work because that returns an array of unique items . The result for ` agg ` must be a scalar . So ` .aggregate ( lambda x : len ( np.unique ( x ))` probably would work too .

The simplest way would be to use DeepSpace answer . However , if you really want to use an anonymous function you can use apply : #CODE

In xlswriter , once a format is defined , how can you apply it to a range and not to the whole column or the whole row ?
In xlswriter , once a format is defined , how can you apply it to a range and not to the whole column or the whole row ?
There isn't a helper function to do this . You will need to loop over the range and apply the data and formatting to each cell .

I am trying to load log files into a dataframe using pandas . I have 2 files I try to merge into 1 . What happens is that the dataframe turns out empty , which is strange because the same code with other log files of the same type .
What's the reason for using dataframes ? Do you want to use them for further data handling or just in order to merge two files ? If you just want to merge two files there might be other ways to do this ...
I believe the problem is in ` pd.concat ` after initialising ` df ` as a dataframe with no rows and then trying to concatenate on a dataframe without rows . Rather try ` df = df.append ( df1 )` . Or if that is not what you want , and you wish to join on the index , initialise df as the first log file .

If you know what the header startswith : #CODE

pandas inner join on data frames without merging data
I would like to perform an inner join like in the following SQL : #CODE
I tried the pandas merge , but it gives me the result from both data frames !
which defaults to inner join according to the docs .
rather than a merge you want to just filter your second df by testing membership of the values against the other df using ` isin ` : #CODE
You need to specify that you want an inner merge ( the default is to do an outer ) . You also need to first restrict ` df1 ` to the merge columns : #CODE
See also the pandas docs for a " brief primer on merge methods " .
@USER My understanding is that a join is going to be a * lot * faster than an isin call ...
That wasn't my point , only that the output seems to me to want a filter rather than merge the op may be after that in reality but there's been no response as yet
Well , an inner join ** is ** a filter , especially in the case where there is no additional columns to merge in .

Normalize values in DataFrame
What I need is normalize the rating column below by the following process :

The code should read the column names from a csv file and I want to append the column names to %%column name . For example : empid , Name , salary so the output should be %%empid Name salary . But the name of the columns will change as different csv will have different column names .

I've written some code to clean up the tweet for some machine learning applications and I would like to apply the cleaning function to the entire Tweet column .
Could also create a set to store ` stopwords.words ( " english ") + [ u'rt ']` ( for faster ` in ` lookups ) and define it outside ` clean_tweet ` to make sure the append operation isn't being run each time .

If this is the reason , you would need to apply ` .fillna ( value )` prior to ` .groupby() ` with a value of your choice - for instance 0 .

How Numpy two-dimensional array ( or pandas DataFrame ) take intersection when there is a column has same value ?
I have tried to pick the intersection of A [ ' time '] and B [ ' time '] , and use np.where to select the rows : #CODE
Sorry are you just asking for the intersection like an ` inner ` ` merge ` ? so ` A.merge ( B , on= ' time ')` ? or ` A [ A [ ' time '] .isin ( B [ ' time '])]` ?
Thanks.I tried merge and isin , but there is a error , .Do you get a error two ?

How to loop through a dataframe , create a new column and append values to it in python
It then counts each value in listi and appends it to Classes if it occures at least three times in the list . The finished list is then ` sorted ` and ` set() ` , so that all objects in the list are unique , and finally joined at comma to a string again . Then I want to append this unique list of value in a new column , at the same index position as the row value the changed value is derived from . As example : #CODE

IIUC you want to pivoting dataframe by ` pivot ` . But with different row is problem - you get ` NaN ` values : #CODE

it just returns a df with all NaN values . With rolling apply is the window size time based ? Is there no other ways to reference the value in a previous row .

Merge on one key after the other
I want to merge on id1 , like this
This is close to what I want . However for the failed lookups ( that is when id1 is not available ) I would like to merge on id2 , so the result looks like #CODE

Are you after [ ` resample `] ( #URL ) ?
it means that ` resample ` only works when the index is a ` datetime ` like index , so you'd need to set the index first#

I concatenate rows as follows ( I do not need to join neither merge rows with respect to some column , just put rows together ): #CODE

You can use a function to map . like the example .
ps . I wanna know how to code fast in stack overflow , are there some one can tell me the tricks ?

I am working on the Walmart Kaggle competition and I'm trying to create a dummy column of of the " FinelineNumber " column . For context , ` df.shape ` returns ` ( 647054 , 7 )` . I am trying to make a dummy column for ` df [ ' FinelineNumber ']` , which has 5,196 unique values . The results should be a dataframe of shape ` ( 647054 , 5196 )` , which I then plan to ` concat ` to the original dataframe .

How can I append an extra row of string to the dataframe , for example : the average is : ( numerical value of the average of third column )
I tried to append a extra row , data.append ([ ' Average ' + w [ n ] *1 8]) .
pandas version 0.13 , you can use loc #CODE
You have to use loc as radar suggested . Here is complete sample code to calculate only third column average #CODE

I get this error : TypeError : unsupported operand type ( s ) for + : ' numpy.int64 ' and ' str ' . This is how I apply your code : featuresA = [ col + ' _x ' for col in group.to_frame() .columns ]

How to merge two data frames with repeated values in pandas
I want to join these two dataframes in pandas . I have performed concatenation but its not allowing me to perform because first dataframe has repeated values .

Join back remaining parts into single column .
I want to keep some of the entries / or delete unwanted in message 3 . Then join with message 1 , like : Desired Message List #CODE

Then I would just use pandas.concat ( list , 0 ) which would merge all the different arrays .

Aggregating functions are ones that reduce the dimension of the returned objects , for example : mean , sum , size , count , std , var , sem , describe , first , last , nth , min , max . This is what happens when you do for example DataFrame.sum() and get back a Series .

Scikit learn's MultiLabelBinarizer creates a binary matrix from labels . You can extract ` feature ` column from pandas dataframe and apply it : #CODE

Another approach which might be cleaner if you have a lot of conditions to apply would to be to chain your filters together with reduce or a loop : #CODE

... and DO NOT merge the cells containing the years across multiple columns . All you will be doing is making everything you do in the future harder .

I read the file and save it in a dataframe . Then i use this code ` A = np.array ( df )` to create a numpy array . My problem is that when i try to convert it to a sparse using the code ` s = sparse.csr_matrix ( A )` i get this error ` TypeError : Cannot cast array data from dtype ( ' O ') to dtype ( ' bool ') according to the rule ' safe '`

and I want to replace ` NaN ` within ` SP2 ` only if there is a non ` NaN ` value in ` SP1 ` . If this happens I want to replace it with the value in ` SP1 ` . My desired output is this : #CODE
Use ` loc ` and a boolean mask to overwrite the values , it's irrelevant if ' SP1 ' is already ` NaN ` as the result is the same : #CODE

I completely change your solution to ` groupby ` with ` apply ` custom function ` f ` . For check string values is better use ` isin ` .

I'm not sure that I'm using the terms correctly as I am quite new to pandas and python . However , I have two dataframes that I would like to join into one . They can be matched on the index column and an other key column ( sample ) . ' sample ' ranges from 001 to 100 and the index from 1 to 729 for each sample .
What if they have same index but different sample or different index but same sample ? How do you want to do it then ? Also , your example will end with two columns with the same name . In this one , the values are the same , but after the joining , you may end up with different values based on how you will join them . And it isn't something you want :)
If you only want to merge on ` index ` , ` pd.concat ` would be the way to go ( docs ) : #CODE
If you want to merge based on multiple column conditions , you should look at ` .merge() ` ( docs ) . Using ` merge ` , you can also combine ` index ` and ` column ` conditions like so : #CODE
You could use ` merge ` for that 2 dataframes on ` id ` and ` sample ` column . Then rename columns if you want them to be q1 , q2 ... : #CODE

First I extract multiple values of column ` L ` to new dataframe ` s ` with duplicity index from original index . Remove unnecessary columns ` L ` and ` Q ` . Then output join to original ` df ` and drop rows with ` NaN ` values . #CODE

I would like to apply a boolean ` mask ` in function of the name of the column . I know that it is easy for values : #CODE
You could transpose your dataframe than compare it with the columns and then transpose back . A bit weird but working example : #CODE
As @USER show in the comment that these two cases not working correctly . So you should use @USER . method . But .. Let's play more with transpose : #CODE
` df.isin ( df.col1 )` works here only by coincidence -- try it with ` df [ " col2 "] = [ 3 , 2 , 8] ` , for example . Actually , come to think of it , your transpose method only works by coincidence too . :-/ Try it with ` df [ " col2 "] = df [ " col1 "]` .

I would like to populate a pandas dataframe from attributes of a list of classes generated via ' append ' . ( not sure this is the right term to describe ' allFoo ' below

Are there other columns that you need to unwrap at the same time , or other columns you could append to the index ?
If you have a lot of columns , it may be easier to take the ' set ' column with a single column that has unique values so you can merge back into the original frame after unwrapping .
Updated to unwrap separately and then merge back .

Drop Elements from Pandas Series by Index
I'd like to drop the dates from 2015-09-11 to 2015-09-15 , so my df would look like : #CODE

I guess you could replace all these ` elif ... ` with a simple ` else `

I am sure that could be do with aggregates ( lambdas func ) or apply .

You can use combine() based on a condition . From the pandas [ docs ] ( #URL ) . Or did you mean join / merge operations ?
IIUC you can use ` merge ` and ` combine_first ` . Last remove columns with suffixes ` _new ` and fill ` 0 ` instead of ` NaN ` . #CODE

How to apply drop_duplicates to grouped dataframe ?
I'm trying to drop the duplicate rows in each chunk of a grouped dataframe . A toy example is #CODE

You could once iterate over the array and get the colours corresponding to each value and store them in an NxMx3 ( image ) array . Then sort the array and the image in the same manner e.g. get the sort indices from the original array and apply them to the image array . Then you can display the image with ` plt.imshow `

Append more than 2 data frames in pandas
I have about 25 data frames with identical column headers that I need to append to one another . I've tried this in the past using 24 .append() calls but it didn't work . Is there a simple way to do this ?
The number of rows was erratic . Unfortunately I don't have the data with me anymore , just sad memories , but imagine 4 data frames with 1000 rows . Merge 1 results in 2000 rows , merge 2 results in 2700 rows , merge 3 results in 3892 rows ... it was weird

What i'm trying to do is where the timestamps in ` times ` fit into the resampled ` Bin_time ` , append the ` Bin_time ` and ` ave_knots ` to the end of the row . This is a paired down data example , ultimately it will have to work on 3 - 5 million rows .
In the top dataframe , why did you merge ` Bin_time ` and ` ave_knots ` with the other columns ? From your desired output , it looks like they have no direct relation ( i.e. the bin time of ` 5 / 6 / 15 19:00 ` doesn't actually belong at index 3 / id 5045496 ) , so wouldn't it be better to keep those columns in a separate DataFrame ? Then you could reindex it to the appropriate " times " with ffill .

I am looking for the best way to join them on their common indexes .

If you transpose your data so that the ` index ` you show below makes up the ` columns ` and your ` xticks ` make up the new ` index ` , you should get what you are looking for .

You are right ! I will actually use a very similar structure to insert such field into a database .
I found an issue about a similar error when using the apply method and that bug was fixed . Since they fixed for apply I used the apply function to do what I want . #CODE
I used the apply method on the grouped data , and the easiest way to get the ' identifier ' was to get the groups keys . If you have a suggestion on how to do this more efficiently let me know !

tz-aware datetime series producing UTC-based .date() output in pandas series apply ( lambda ) operation

Pandas apply function - comparing each row to entire column
The ` groupby ` function makes sense , but I'm having trouble defining the appropriate apply function . I've seen examples using multiple columns as inputs to a function , and others using ` axis=1 ` to look at individual rows , but haven't seen a description of using both the entire column ( to look for matches ) and the entry for the row in question ( to determine the time range ) .
EDIT : I think something along these lines may work , but I'm not sure what the appropriate way to reference the time columns to replace " XXXXXX " should be . #CODE
Not sure if I understand what you looking for . But you can access a column inside an ` apply ` function . For example , this calculates how many rows inside a group have times less than 12 : #CODE
This functionality makes sense - you can use an apply to compare each element to a fixed value ( in your example , ` x < 12 ` and ` 6 < x < 12 `) . What I am looking for would need to , for each element , compare to all the other elements and return the number matching the closeness criterion .
To continue - I think it is O ( n^2 ) , as it needs to compare each value to all other values . I had hoped using a grouping criterion that produces a greater number of smaller segments to apply the function to would speed up the calculation , but this doesn't seem to be the case in practice . Any idea whether this is a fundamental misunderstanding , or just issues with the implementation ?

I am looking to remove the error messages from message_B . These are messages where some of the text changes , but all error messages contain a ' ! ' or ' ? $$ ' in them . I then want to join to message_A for a single column message .
With the final result just been a single column string ( drop Nan ) .
I have split message_B into a ( new ) different dataframe , but how do I iterate over the full DF and delete the unwanted messages ? ( I do not want to drop full rows )

@USER I did check that after I got the variable to insert into the query properly . It is expecting a datetime object so I updated the ` this_week ` variable to make sure it is a datetime object as well .
Seems that pandas is not able to parse the SQL query returned by SQLAlchemy since it is expecting raw SQL ? ( not very familiar with pandas ) so maybe you should find the location of the parameter where you want to insert the value using string.replace() or regex and then pass it to pandas ?
The easiest way I found to insert the variable into the statement was to utilize the read_sql function in pandas and set the variable as a parameter . #CODE
This will insert the variable ( s ) into the string query .
To get around this , I found that you can apply a date function to the column in sqlalchemy , similar to this issue .

This works because we can use a boolean array ( ` row.astype ( bool )`) to select only the elements of ` df.columns ` where the row has True .
@USER , in this solution ` row ` is a numpy array . When you do ` row.astype ( bool )` you get something like ` array ([ False , True , True , False ] , dtype =b ool )` . This array of booleans can be used to selectively index ` df.columns ` , which is a ` pd.Index ` object : ` df.columns == Index ([ ' apple ' , ' banana ' , ' carrot ' , ' dietcoke '] , dtype= ' object ')` .

How to use one date-indexed pandas DataFrame to map over another one ?
I want to map over the prices so that they are all deflated . I need to divide each value by the appropriate deflator for the quarter that the day falls into . I made a function for comparing days to quarters but I'm new to pandas and I have no idea how to use it to map over my DataFrame #CODE
You could resample the quarterly data to daily frequency so that you have corresponding dates for both series . For the missing values that result from the upsampling to higher frequency , you would presumably forward-fill past values , but you could also backfill future values .
In ` pandas ` , using the resample method , this would imply - assuming you have created ` DateTimeIndexes ` for both ` DataFrames ` : #CODE

You then could drop ' Cash ' column or edit your initial table #CODE

Use ` apply ` and pass ` axis=1 ` to call ` describe ` row-wise : #CODE

Based on this post on stack i tried the value counts function like this
You have to remove first and last ` [ ]` from column ` genres ` by function ` str.strip ` and then replace spaces by empty string by function ` str.replace ` #CODE

Then at second level 2 or multiple sub / child nodes can be found based on language filter condition.so after filtering i need to drop the main dataframe at level1 .

How can I convert ` result ` into a row with columns , each time I append ` flights ` ?

Eventually I think I will write a function to replace the hardcoded list assigned to xlsAlpha , so that it can be a virtually unlimited amount of columns .

How to concat a dataframe in pandas ?
count = len ( data_dish ) ## counting dishes ordered
You should be able to get a list of dataframes in a list and then concat them .
Loop and append dataframes #CODE
Then concat the list into the full dataframe : #CODE

But when I apply the code , I get the following Error : #CODE

However , when I try to apply that function in order to write the output to a new column in a pandas data frame , it's returning None . See below : #CODE

You can just create a temporary ` DataFrame ` from the records in ` df.dish ` and join it back to original ` df ` .

You could read the file with ` header=None ` , drop the duplicate rows ( which keeps the first per default ) , and then set the remaining first row as header like so : #CODE

Method #2 : add a dummy column and then pivot : #CODE

Is there a straightforward way to " insert " zero records for missing sample data ( like this question , but I have two columns functioning as indices , and I don't know how to adapt the solution to my problem ) or have Matplotlib plot with holes ?

Python append value to list of unique objects
I would like to append the new value / value-name for each object .
It sounds like you are attempting to merge two dataframes based on a column , though it's not entirely clear . It may clarify the question if you were to include an example of what the output should look like .
I was attempting to join two dataframes on a column ( example from above link ): #CODE

formatter function to apply to columns elements if they are floats , default None . The result of this function must be a unicode string .

Can you insert a ` print os.getcwd() ` into your python script and rerun it using both methods to see what it outputs ?

I want to append a column to df , so that it looks like this : #CODE
One option is to use an apply : #CODE
You can also use ` map ` : #CODE
It's interesting that map doesn't require the ` .get ` , any idea why that is ?
My understanding is that this just uses the keys to perform the lookup , same as if you pass a ` Series ` and similarly it will bork and generate a ` KeyError ` if the label / key doesn't exist , in that case doing ` apply ( lambda x : other_dict.get ( x , other_val ))` would at least not go mental if the key doesn't exist .

Oh , so you want to interpolate for ` NaN ` , rather than ignore them . If you interpolate , using linear interpolation , you will get the same regression , but with as many points as you have . Try : #CODE

I want to apply a function to groups of ` x1 ` based on the columns in ` x2 ` . e.g. : #CODE
Interesting !! Also this ` x1.groupby ( list ( zip ( x2 [ ' e '] , x2 [ ' f ']))) [ " b "] .transform ( " mean ")` gives me a ` GroupByError : len ( index ) ! = len ( labels )` which looks incorrect to me . Surely a pandas bug ...
That said , it may be better to go ahead and do the join ... IMO it's often a good idea to keep your variables independent .

I have a big dataframe , and I'm grouping by one to n columns , and want to apply a function on these groups across two columns ( e.g. foo and bar ) .
But ` transform ` apparently isn't able to combine multiple columns together because it looks at each column separately ( unlike apply ) . What is the next best alternative in terms of speed / elegance ? e.g. I could use ` apply ` and then create ` df [ ' new_col ']` by using ` pd.match ` , but that would necessitate matching over sometimes multiple groupby columns ( col1 and col2 ) which seems really hacky / would take a fair amount of code .

I also want to replace these missing values , I have used ` pandas.filna() ` but it doesn't effect . I think it's due to data type . Could you please give nay hints for this ?

P.S. : Guess need to use the groupby function , but because of lack of experience , do not understand how to apply it to my problem .

I've also done my Google searches but I do not feel found results apply to my issue .
using the ` apply ` method of the DataFrame , with something like :
Actually apply produces the ` < built-in method values of dict object at 0x00 ... ` output . So apply() may not be good for this particular transformation .
I also tend to not use apply but it can be convenient ( for readability or when using certain functions for example ) . Anyway in my test ( on python 3.4 / pandas 0.17 ) the following statment ``` df [ ' word count '] = df.apply ( lambda x : dict ( Counter ( x [ ' test '] .split ( " ")) .items() ) , axis=1 )``` make a new column with the word count ( not sure why you use join * and * split on your text ) . You can also use list comprehension to avoid apply , like ``` df [ ' word count '] = [ dict ( Counter ( i [ 1 ] [ ' text '] .split ( " ")) .items() ) for i in df.iterrows() ]```
You are right , join isn't needed . i see now Counter works fine on split words without needing commas to separate them . List comprehension works perfectly . Apply still outputs ` < built-in method values of dict object at 0x00 ... ` Quick timer test shows apply is bit faster than direct method : ** apply method ** ` 1 loops , best of 3 : 14.6 s per loop ` ** direct method ** ` 1 loops , best of 3 : 18.2 s per loop ` . My data has ~60k rows and mean of 314 words per row .

Then you can append new Series with range from 1 to length of ` NaN ` values in first row .

Type of ` df ` isn't ` dataframe ` , but ` TextFileReader ` . I think you need concat all chunks to dataframe by function ` concat ` and then apply function : #CODE
Not really . The problem is exactly this : if I do a concat I would not have enough memory to do anything . That's why I have to use chunks .
Thanks again , but I don't have a lot of constants on my dataframe and even one column would surpass what I have of RAM ( as I said in the question ) so any concat wouldn't help .

And then we can pivot this in many ways . Using ` get_dummies ` , for example : #CODE

Try defining the fuinction and then applying it using [ ` map `] ( #URL ) or [ ` applymap `] ( #URL ) from pandas .
You can use the ` apply ` and ` applymap ` from pandas .

Function to drop duplicate columns in pandas dataframe

But I don't how to apply that to my code either . Please help .

Many of those values we don't want , but that's okay , because we're only going to insert the ones we need . We should clip to 0 , though : #CODE
( Note : this assumes the rows we need to fill look like the ones in your example . If they're messier we'd have to do a little more work , but the same techniques will apply . )

Haven't benched this , @USER , but I think the numpy approach should be pretty quick too . Especially because Pandas ` .apply() ` now runs through the first apply twice , to find out if it can take a shortcut approach . I don't know the details of under-the-hood working , but it looks similar to Numba's approach at speeding things up : run once to figure out what's happening , speed up , if possible . This is a new thing I found in their [ docs here ] ( #URL ) . Read the warning towards the end of the subsection .
It's been doing that for as long a I can remember ( a few years ( ! ) at least ) :) The apply still happens in python so if you have a lot of small groups the dummy_column + groupby sum will blow apply out of the water . Numba is a game changer however , if it were using numba it might be different ... ( Edit : [ the warning is new though ] ( #URL ) . )
@USER - I am new to pandas.I am guessing from you explanation and needing to apply some some map reduce type of thing is going on ? Is my assumption correct ?

Then you can resample and just need to decide whether you want to forwardfill the value corresponding to the start date or backfill value corresponding to the end date . See resample docs . This would look as follows : #CODE

I wanted to write a loop or a function that would split the data and then add the new column but I couldn't figure out how to do it . I ended up splitting and cleaning the data through a series of dictionaries and now am stuck with how to make the final dictionary into its own dataframe . I think , if I can make this new dataframe , I'll be able to append it to the old one . I also think I am making this harder than it should be and am interested in any solutions that are more elegant .
You can try this approach , which use mainly string methods . Then I ` pivot ` and ` fillna ` dataframe . I lost original column ` most_exhibitions ` , but I hope it is unnecessary . #CODE
I try add all columns as recommended output by function ` merge ` : #CODE

If I use groupby apply , everything works fine : #CODE

My approach don't use ` map ` , but functions ` stack ` , ` groupby ` , ` pivot_table ` , ` merge ` and ` cumsum ` : #CODE

Thanks for the links , @USER , unfortunately this is what I was saying in my intro , those approaches work like a charm when you have non complex data such as int or float , but in the case of vectors inside the dataframe , things get pretty messy . I tried those ideas and failed , so maybe I am just missing how to apply them in my situation :(
Then you can apply the numpy / scipy vectorised methods for computing cosine similarity as in Whats the fastest way in Python to calculate cosine similarity given sparse matrix data ?

All 7 scatter plots look similar to this . I tried changing it to boxplot , but it made no difference , it still returns the same plot . In fact , it doesn't seem to matter what I use for the ` kind ` parameter , I'm always getting the same graph ( tried pie , hist , hexbin , etc . ) .

This calls ` rsplit ` and takes the last element and then calls ` zfill ` on these , we then join it back with the rest of the string to get back what we want . #CODE

Aha , the use of ' shift ' , together with ' cumsum ' . This is BRILLIANT .

I have a pandas dataframe with a series of price values for different types of fruit over a series of unevenly spaced dates . What I would like to do is interpolate prices for the fruit over a consistent time frame for each fruit type .
this looks like it's right in line with what I'm trying to do . Thanks for the help . The only question I have is that rather than extend the price estimates throughout the whole range of dates in the pivot table , I would like to have the first and last dates for each fruit be the first and last dates in the original dataframe . For example , the range of dates interpolated for fruit 2 should be ( 2013-4-11 through 2013-5-20 ) , while the range for fruit 1 should be ( 2014-9-21 through 2014-10-1 ) . Is this possible ?
See revision above . It is not perfect because of the daily to weekly interpolation . For example , the value of the second fruit gets cut as the interpolation breaks the weekly period .

How to create new column and insert row values while iterating through pandas data frame
It's because ` apply ` method works for column by default , change ` axis ` to 1 if you'd like through rows :
0 or index : apply function to each column
1 or columns : apply function to each row
Like indicated by Anton you should execute the apply function with ` axis=1 ` parameter . However it is not necessary to then loop through the rows as you did in the function test , since
the ` apply ` documentation mentions :
How to apply a function to two columns of Pandas dataframe

Pandas pivot table fixing value columns
I pivot it and write in db among otherthings , #CODE
Then my pivot will fail I mean invalid , #CODE
I guess in that case I would not use pivot table for this . Thanks .

How to strip and uppercase a column in a dataframe derived from Excel file -- avoiding unicode / str error message
Now what I want to do is to strip the ` Gene.Symbol ` column and upper case it with the following command : #CODE
Sorry why do you want to strip here ? what's wrong with just ` df [ ' Gene.Symbol '] = df [ ' Gene.Symbol '] .str .upper() ` ?
@USER : I need to strip it for ' normalizing ' . Because later I need to perform join with another table .

So it would be possible to have a unique constraint where I would just overwrite the old data with the new data ( for example , sometimes the roster information is not updated so I occasionally need to read old data and replace it ) . This could be placed on the combination of the 3 columns , right ?
And use ` INSERT INTO ... ON CONFLICT syntax ` instead :

If you're using ` apply ` , the speed difference is minimal ; you should feel free to use ` iteritems ` .
Thanks a lot ! If I am focusing on a single dataframe , and want to use function to do calculations to each column , is iteritems() also better than apply ( lambda x ) ?
@USER - they are very similar from a pandas POV I think - it's just whatever you think is cleaner . For me , ` apply ` is cleaner , but you don't get the name .

to apply various aggregation functions as described in the docs . If you provide some detail on what you'd like to aggregate and how , happy to add an example .

` ( a [ -1 ] -b [ -1 ]) / len ( b )` is one " chunk " and one more of them is added in each " iteration " ( year ) via multiplication with a ` numpy.arange() ` array . I tried a few plots and it doesn't look good unless you tweak it , but it's what you asked for .
Generally speaking , you'd apply an " easing function " over some range .

I want to apply a function to every group in a ` groupby ` object , so that the function operates on multiple columns of each group , and returns a 1 x n " row vector " as result . I want the n entries of these row vectors to form the contents of n new columns in the resulting dataframe .

Thanks for the help with unstack and combining the year field with the quarter row . Thats just what I needed FURTHER : I made an IF construct to convert the FY to calendar year based on the fiscal year end . Is it appropriate to edit your answer or should i add my own ?

I have two separate spreadsheets and using pandas in order to convert them into data frames . I am looking to extract out information from spreadsheet 2 and merge that information into spreadsheet 1 . I know how to extract the information in excel , by utilizing an " index-match-match " function , but I am having trouble figuring this out in Python since it doesn't seem to fit a merge or join method ... or maybe I'm just missing the connection .
What happens is the program will insert a new column and will insert the data based on indexing the column and row to get to the single data point . So for the very first row in my example which is Fresno and Product 1 ... it will look in spreadsheet two across the top row until it gets to Product 1 and then down to until it finds Fresno and the insert 5,060 .
Thank you Alex . I apologize , if someone like to point me in the right direction to research it on my own , I gladly will . I have been unable to figure out any documentation for a merge / join scenario that i am experiencing . Which is merging two spreadsheets where the information is indexed , rather than in one column .
IIUC you can ` stack ` dataframe ` df2 ` created from ` Spreadsheet 2 ` . Then you need ` rename ` column converted from rows - ` Master Product Name ` .
If you need ` merge ` by columns ` Region ` , ` Master Product Name ` and ` Branch ` , you have to lowercase both ` Branch ` columns for matching . #CODE
I was unfamiliar with the stack method and helps solve my dilemma . Thank you ! I'm still curios if there is a way to pull the data point over without manipulating the second data frame .
Maybe it is possible , I don't know . But pandas is optimized for vector working and use high optimized functions like ` stack ` and ` merge ` and this is the best practice .

In the second branch , minimum is less than zero , so adding the abs ( min ) shifts the data up to zero ...
Well , I get that , but your function doesn't check that at all . If you believe that input parameter without checking , then OK , I thought this was a feature to normalize the data . Which would indeed not make much sense , in retrospect , as the resulting discretized signal would be identical . Sorry , I get it now :)

What have you tried ? Considering looking up [ reshaping data ] ( #URL ) , [ ` melt ` function ] ( #URL ) and [ ` merge `] ( #URL )
Thanks . Can you show me how to use merge or melt ?

Because this question is tagged Pandas , I assume we are talking dataframes and series instead of plain lists . You can use ` loc ` to locate the rows and columns that match you criteria ( e.g. whether each element in the ` LOT ` column ` isin ` the series of ` lots of interest `) . #CODE

I want rename the Unnamed Columns with the prefix as the Previous column's name then append ' Count ' to that name . Is there a way to do that ? Can this be done when reading the excel file itself ?

What did you try ? Attach part of your dataframe . You could use ` apply ` with ` rsplit ` .

I could of course write some sort of parser with if / else statements to translate ' > 2 ' into code , but I was wondering if there is a more elegant way ?

I'm generating an overlay for a map using pandas and used : #CODE
this uses ` abs ` to get the absolute value to generate a boolean mask and filter all the duplicate and near duplicate values out .

I'm trying to map ` total_seconds ` for that series hovewer I've got an error : #CODE
The question why it's not working with ` map ` method and why types are different ?
So the ` dtype ` gets converted to ` np.timedelta64 ` using ` map ` but you have no ` totalseconds ` attribute when this happens , but if you access a specific scalar value then it remains a pandas ` Timedelta ` which does
You are probably right . But I thought ` map ` method access directrly to each value instead of ` apply ` which for whole Series . If so it should access as Pandas ` Timedelta ` . Is it correct ?
` apply ` would fail here also , ` apply ` is also a ` for ` loop it just allows you operate either column-wise or row-wise when called on a df , the type conversion is being doing by pandas here

but this will filter the data frame several times , one value at a time , and not apply all filters at the same time . Is there a way to do it programmatically ?

where df2 contains the missing data of df1 . How can I merge the two DataFrames to get #CODE

Another method is do define a dict and call ` map ` : #CODE

It is easy to interpolate values in a ` Pandas.DataFrame ` using ` Series.interpolate ` , how can extrapolation be done ?

But when I do this with pandas ' implementation of hist , as follows : #CODE
Does anyone know a way to get the histogram values off a histogram generated by pandas ' hist method ? Thanks !
The ` pandas ` implementation of ` hist ` does not return the counts , bins and patches that you get from ` matplotlib `' s version . Instead , it returns the ` Axes ` that it plotted the histogram on . If you check the duplicate I linked above , I show a way you can back out the ` counts ` and ` bins ` from the ` patches ` associated with the ` Axes ` returned by ` pandas ` ; but its probably easier to just use ` plt.hist ` .

I want to compare df1 and df2 on column B1 . The column A1 can be used to join . I want to know :
I tried using merge and join but that is not what I am looking for .
When doing your merge , you want to specify an ' outer ' merge so that you can see these items with an A1 key in one dataframe but not the other .
perfect ! seems like just what i needed . slight issue . when i try it on my data i dont see the missing rows . there are a few records that are present in one of the dataframes and they dont show up after the merge . however , your solution works perfectly with the test data that you have provided here . thanks

Merging two pandas data frames in python using outer merge not identifying identical values
I am trying to merge two pandas data frames using a common column ( I imported both data frames from csv files ) . The common column ( key column ) should have some identical values , but the majority are different , so I used an outer merge . I realized after performing the merge that my output was actually just data frame 2 added on to data frame 1 , without the identical values from the key column actually merging into one row .
I second @USER . With your example data , the outer join renders your desired result . And you compare a single value to a series and so returns the False on a string comparison . Loop through series and check for ` name1 ` in ` df1.gene_id `

pandas two dataframe cross join
I can't find anything about cross join include the merge / join or some other .
if used apply i will be how to implement them thx ; ^^
Essentially , you have to do a normal merge but give every row the same key to join on , so that every row is joined to each other across the frames .

Basically I would replace the empty brakets of lists with ` NaN ` and then dropping according to your rule . This works reading the sample file from file , actually I don't know if the dataframe contains empty lists as ' [ ]' or just empty values .
How can it work ? your second line will replace nothing as there are no values ' [ ]' in the dataFrame . The [ ] values in the data frame are empty lists not strings of square brackets . :-)

Python PANDAS : Drop All Rows After First Occurrence of Column Value
I have a PANDAS dataframe with a columns with an open / closed status value and a ranking field value . After I sort by the rank field , what would be the best way to drop / delete all rows after the first occurrence of an " open " value ? I'm just confused if I should take an iterator function or standard index based approach with PANDAS . Any advice would be great !

Using pandas resample / rolling_sum to calculate seconds time intervals
Consider a series sum function and apply it to a ` groupby() ` : #CODE

The basic pivot command is like this : #CODE
I had been looking for answers everywhere , and had been trying methods like `' unstack ' , ' droplevel ' , ' reset_index '` , etc , but wasn't able to make them work myself .
Here's a [ take ] ( #URL ) on pivot sort of operations . Let me know if its at all helpful and I will write an answer . All the source code is at the bottom of the post if you want to try it out .

pandas replace dataframe cell values
I would like to iterate over the entire dataframe , and for any cells that contain the less than sign , I would like to replace the entire cell with 0.0 .

You could try ` iloc ` or ` ix ` methods of dataframe from column : #CODE

Previously ` train_y ` was a Series , now it's numpy array ( it is a column-vector ) . If I apply ` train_y.ravel() ` , then it becomes a row vector and no error message appears , through the prediction step takes very long time ( actually it never finishes ... ) .

Merging Pandas DataFrames ( LEFT join style ) produces strange results
I'm trying to merge two Pandas DataFrames with a many-to-one relationship .
This is normally a rather straightforward SQL-like procedure using a ` LEFT ` join , however I'm getting a rather unexpected result .
To do the merge I simply do the following : #CODE
Now for the interesting part . In an attempt to find out where this might be going wrong , I tried to do the merge on only a subset of both DataFrames using ` head() ` : #CODE
If I then perform the merge I get the desired output without a problem , just only 100 rows . If I keep increasing the amount of rows to take for ` head() ` , even up to the point that it is way more than the number of rows present , the output is still perfect . This , for example , works perfectly : #CODE
Notice that I include the first 100.000 rows ( only 53.000 in df1 ) . This too will give me the expected merge output .
What might be going on here ? Why does the merge produce such strange results , unless I pull the DataFrames through ` head() ` first ?
To summarize : Without using ` head() ` the ` merge() ` output is showing reduced rows ( # rows = # right side rows ) and each ingredient ID is simply " pasted " on there in a serial manner . However , if I do use ` head() ` , even with a great enough number to include all rows on both sides , the join works as expected .

A custom function should apply to a series of the dataframe , a boolean operator for example : #CODE

I'm not familiar with NumPy nor Pandas , but the error is saying that one of the objects in the comparison ` if len ( self ) ! = len ( other )` does not have a ` __len__ ` method and therefore has no length .

I currently want to apply several machine learning models on this data . With some models , it is necessary to do normalization to get better result . For example , converting categorical variable into dummy / indicator variables . Indeed , pandas has a function called get_dummies for that purpose . However , this function returns the result depending on the data . So if I call get_dummies on training data , then call it again on test data , columns achieved in two cases can be different because a categorical column in test data can contains just a sub-set / different set of possible values compared to possible values in training data .

@USER -if you have any idea like list iteration way , then checking first char ( if startswith sub_ or sub_cond1 like this ) .Then also i am fine with that.please suggest .
If you are reading the data into memory from a file then maybe take look at using a chunking technique to read in smaller sections and process / output each section sequentially . The pandas read_csv method has chunks and iterator keyword arguments for sequentially reading in subsets of the main dataset . Since you are not doing any grouping but only filtering , you can process the entire dataset sequentially . Example : divide the dataset into N chunks , read each one in , do your filtering , write csv files , delete dataframes in memory , read next chunk , filter , append output to the csv files , etc .
@USER -so one more thing , can i apply chunksize filtering ( taking a chunk of dataframe like in the case of pandas dataframe , then doing some Op . then merging them -- sending back ) on dask dataframe ???
@USER -while """ import dask.dataframe as dd """ i am getting an error "" AttributeError : type object ' Series ' has no attribute ' dt '""" .Is there any module that should be imported before dask.COZ i have successfully installed dask and successfull in import dask ??

EDIT I might add that the shape of the dendrogram is exactly the same . Do I have to normalize the third column of the resulting ` z ` with the maximum value ?

would give you the mean for the second row anyway , to operate row-wise you can use ` apply ` and pass a ` lambda ` : #CODE

Pandas Melt a dataframe depending on value in final column
However I'd like to melt the dataframe depending on the value in the Output column , i.e. I'll have two crosstabs one where the Output is only zero and the other when the Output is 1 . Need to individually compare each input column with the output column .

Essentially I want len ( x ) slope_x values .

Because that is what you've wrote . ( There is no * i * in ` if abs ( df.START-df.END )= =20 : `)
Thanks , that was it ( the answer to my question ) . I had to do " if abs ( df.ix [ i [ 0 ] , ' START ' - df.ix [ i [ 0 ] , ' END ') . I apparantly have the tendency to not put the ' i ' in a if statement ! Now I will ( try ) to remember to each time to not forget the i ...
is broken , because abs ( df.START-df.END )= =20 returns a Series . For each row , that lambda is asking if a Series is True , hence the error .
this would produce the desired result , because abs ( x.START-x.END )= =20 returns a boolean , not a series .

Well you overwrite ` dish ` on each iteration you should append to a list or similar container and return this , also this is very similar to your previous question , what's wrong with ` ( s / s.sum() ) * 100 ` ?
Use the loop to generate a list of the values you want to return - start with an empty list and append each new value . Then create a series using the list .
That is , you don't overwrite the result in every iteration but append it to the list .

Or use the merge method : #CODE

First I started by using ` pd.rank() ` on the data and then I planned on then using ` pd.cut() ` to cut the data into bins , but it does not seem like this accepts top N% , rather it accepts explicit bin edges . Is there an easy way to do this in pandas , or do I need to create a lambda / apply function which calculates which bin each of the ranked items should be placed in .
Not quite . When I apply this to my data set it says there are 419 posts in the top 0-5 % percentile , when in actuality in my data set of 1674 samples , there should only be 84 samples within the top 5%

Is there any convenient way to normalize each row by row's maximum ( divide by row's max )
You can use ` apply ` and apply a lambda row-wise : #CODE

Bear in mind that this will replace all values in the group with the first value , not just ` NaN ` values ( this seems to be what you're looking for here though ) .

where the index column represents a value , and the ` freq ` column represents the frequency of occurance of that value , as in a frequency table .

I think you can use ` join ` , ` sort_values ` . Aggregation in docs . #CODE

` df.to_sql ( con=cnx , name= ' some_table_name ' , if_exists= ' replace ' , flavor= ' mysql ' , index=False )`

I'm using pandas in python to take a csv file , do some minor transformations on it and then outputting the two columns as a json file . I want two values ` timestamp ` and ` value ` . I only want the two new columns and to drop the rest of the file so that it looks like :

now i need groupby the dfn_grouped by " key1_x " and concat to dict like A_x : A_y #CODE
g_dics = dfg.groupby ([ ' key1_y ' , ' key1_x ']) .apply ( lambda x : dict ( sum ( map ( dict.items , [ d for d in x.dic ]) , [ ])))

Wonder if it would help to pivot / unstack the data ? I'll give that a try . Seems like a pain to have to do that if it works ...
Solution below . Had to pivot the data and create NaNs . #CODE

Pandas Pivot Table List of Aggfunc
Pandas Pivot Table Dictionary of Agg function
If list of functions passed , the resulting pivot table will have hierarchical columns whose top level are the function names ( inferred from the function objects themselves )
Can you please advise how can i rename hierarchical columns , say " len " to " n "

pandas pivot dataframe structure
pivot table
how can I replace industryName with tradeDate and remove that blank row ? I want to make it look like :

I am having trouble getting the dict to append even though this seems to be supported in pandas

Strip the double quotes : #CODE

Merge DataFrames and discard duplicates values
2 ) More generally , is there a faster / more clever way with ` pandas ` to read and merge multiple csv files than doing manually : #CODE

I have a problem , where I'm trying to replace sql-case when statements with pandas

You can use ` apply ` with a ` lambda ` to return the name of the column , here we compare the value row-wise against the max , this produces a boolean mask we can use to mask the columns : #CODE

drop duplicates from a dataframe by a date field in python
2.And i checked with drop.duplicates ([ ' dt ']) , and drop.duplicates ([ ' other columns ']) also . By other columns works fine except dt field
3.my question is , if this is due to datetime.date field i am passing to column dt ..... Then my stand will be , why the case datetime unable to start deletion from 1st duplicate , why from 2nd on-wards .
4.In case of converting to str ( currentdate ) it works fine.The comparision works great and drop duplicate is fine.why such behavior is avoided by datetime ( dt field ) in 1st run and followed from 2nd run onwards .
When you write and then re-read the csv you don't parse the date column as a datetime so it comes in as ` str ` dtype , you need to pass to ` read_csv ` ` parse_dates =[ ' dt ']` to read the strings as datetime

When I run it , my new column contains only zeros . The thought behind this code is to get each string for each row in df , split it at comma into substrings and search the resulting list for the substring I want to remove . After removal , I join the list back together into a string and write that to a new column of df , at the same index position as the corresponding row .
Or just use ` replace ` : #CODE
Hey , thanks for your answer . I already tried replace , but that is not an option as I have to count the list elements later . with df.column.replace ( ' x ,? ' , '') I stll add an element to the list , meaning it is still counted even when ' removed ' from the list .
but here the solution does not use replace but sub ...
Could you explain the difference to me ? I tried your way , but as I said , when I do ` df [ ' columnNewNew '] =d f [ ' new '] .apply ( lambda x : len ( x.split ( ' , ')))` I still get the length of ` col2 ` , not the " new " length
which difference ? sorry but your are asking a lot of questions in a single question ! which is not SO working ... I already provided two different answer solving the original problem ( with ` apply ` and a ` vectorized ` one )
I meant the difference between sub and replace . But I can also look that up . Your second answer works , thank you .

Merge text file with a csv database file using pandas
I'd like to merge the text file with a csv database file based on column E . The column contains integer . #CODE
yes , change to the same column name then using merge to combine two files .

You can pivot your original dataframe ` df ` setting the correct values for index , columns and values . #CODE
The ` fillna ( 0 )` replace ` NaN ` with ` 0 ` values . #CODE

I think this will be easy , but I can't think of the right way . I basically want a column to be sorted from smallest to largest , but I don't actually want to sort these , I want to replace values that are " wrong " in the sort . The DataFrame is already sorted how it needs to be , I just need to replace some values that this ' id ' column has wrong .
You can use ` diff ` to find where the values are not equal or increasing , set these to ` NaN ` and then call ` ffill ` : #CODE
You can use double square brackets to force ` apply ` to be called on a ` df ` , this allows you operate row-wise , then use a user defined func to compare the current row value against all row values prior to current row , this generates a boolean mask to select the invalid rows and assign ` NaN ` to these and then ` ffill ` : #CODE

Ok , I'll fix in a couple hours when back at my mac , the pivot step can be done differently .
Updated & simplified without pivot .

I would like to merge year and month to one column in padas datetime format .

So you need to use ` loc ` method in your case : #CODE
Finally i gave up on ` ix ` and decided to make ` loc ` work , as ` Anton ` recommended i try : #CODE
Got me thinking that loc only accepts strings which finally worked : #CODE

replace each column of pandas dataframe with each value of array
I want to replace non-null value in each column with each value in array , and the result will be , #CODE

What am I getting wrong in my pandas lambda map ?
which I read as for each ` x ` in the series ` df1.var1 ` , apply the function ` np.percentile ( df2.var1 , x )` , which finds the percentile of ` x ` in the series ` df2.var1 ` . For some reason , I'm getting the error #CODE

You could find a lot of information how to work with datetime index from docs . For you case you could try ` loc ` : #CODE
Your loc solution with @USER ' s improvement solved my issue . I think your point about raising an error or a warning is a must . This is quite misleading since it does work with datetimes but ** not always** . In fact I experienced this error just once after having already processed several weeks of data .

Concat values from columns depends on particular condition
You can use ` loc ` : #CODE

This allows me to control the layout , but I can't apply it to bar charts .

I'm assuming you want to use all ` [ ' W ' , ' X ' , ' Y ' , ' Z ']` ` columns ` , and only one of the ` date ` columns . If so , the below should get you there - if you first apply ` set_index ` and then ` unstack ` , ` pandas ` creates the ` MultiIndex ` automatically , which you can then ` swap ` by ` level ` and ` sort ` as you wish : #CODE

How to apply a expanding window formula that restarts with change in date in Pandas dataframe ?
So in your example , do ` df.set_index ( ' Date Time ')` and then ` groupby ` and ` apply ` . You can of course assign the result back to the original ` DataFrame ` .

Pandas loc automatically converting datetime to long
I have two datetime columns in a pandas dataframe : time_col override_col . If override_col isn't null I want to replace the corresponding time_col record with the override_col record . The following code does this but it converts the override_col records to a long and not a datetime ... then the time_col is a mix of datetime and long dtypes . #CODE

Conditionally insert rows into pandas DataFrame
I need to then call the Identity Apple , find its min and max dates and insert rows ie months in order to interpolate between the two points so the end result becomes #CODE
The problem is that although I can loop through a list of identities and get all rows associated I cant seem to find a way to then insert extra rows , especially without a nasty for loop . essentially I need to bridge the date gap and fill the associated Identity values with zeros . #CODE
My suggestion is create the full theoretical DF , merge with data and fillna : #CODE
This is good for one Identity and year , loop over years and Identities , append all created DF's into a list and pd.concat ( list )

pandas - insert values from one data frame into another
You could chose ` inner ` join instead of the default ` outer ` . If you want more fine-grained control ( ` right ` or ` left ` join ) , ` pandas.merge ` is also well documented .

Say for instance you wanted to drop any row that had ' c ' in a column #CODE
You can use ` apply ` together with a lambda expression to check for the target word in each column . Then use ` any ( axis=1 )` to locate any row containing that word . Finally , use boolean indexing with a tilda ( ` ~ `) to locate all rows where income is NOT in the row . #CODE

Wow , this is brilliantly simple and working exactly as I need . I was looking at concat and merge before but it didn't make sense to me because I completely did not realize there can be groupby function used . A bit of comments as suggested above would be good though .
I use ` reindex_like ` for align dataframes and then ` where ` and ` loc ` for filling column ` Value ` of new dataframe ` df ` : #CODE

Interesting ... if you find a subset of your data that reproduced the problem , feel free to edit your question and append it , and I'll take a look .

Without seeing the data and assuming you want the resultant prediction as a column in the second data ( df2 ) frame you can apply the kn.predict() using the .apply() function and specifying the vertical axis . This will give you an additional column with the predicted output .
Heres the info on apply .

Interval-matching PeriodIndex with DatetimeIndex during merge
But if you want ` merge ` : #CODE
I decided to align all ` df_other2.period ` with ` df2.period ` ( and make them ` DatetimeIndex `) and merge as usual .

pick a person to join the team
otherwise let him join the team and update ` met ` variable of the whole team

Append DataFrame to an Index Pandas
I understand all the error messages , and I also have a potential fix which involves pre-creating the DataFrame ` cov [ tpd ]` the way I want it , then use indexing to insert the output from ` cov_var() ` . But that is a few extra lines of code to create the multiindex for ` cov [ tpd ]` and then inserting the data . Does anyone know a better way ?

Essentially you are looking to do a lookup for that we can use ` map ` on the boolean series so the following will add a boolean flag : #CODE
can we have two conditions in map ? ` df_final [ ' dish_name '] .map ( dish_specific_perf > 50 & dish_specific < 70 )`

I'd overwrite the ' b ' column using ` transform ` and then drop the duplicate ' a ' row using ` drop_duplicates ` : #CODE

I have a data frame having 20 years of values of carbon emission and a column named ` average_emission ` . I want to replace all ` NaN ` in a particular row by the corresponding value of ` average_emission ` of the same row . #CODE

` KeyError : ' MultiIndex Slicing requires the index to be fully lexsorted tuple len ( 3 ) , lexsort depth ( 0 )'`

` shifted = data.sign() ! = data.sign() .shift() ` should work rather than use ` apply `
Currently dask.dataframe does not implement the ` shift ` operation . It could though if you raise an issue . In principle this is not so dissimilar from rolling operations that dask.dataframe does support , like ` rolling_mean ` , ` rolling_sum ` , etc ..

Python pandas merge keyerror
Consistently getting a keyerror when I try to merge two data frames . The code : #CODE

Output ( of transpose ) #CODE

HINT : Use the Theano flag ' exception_verbosity=high ' for a debugprint and storage map footprint of this apply node .

how to extract two different pandas series element and map to rows in dataframe's column

python pandas : computing argmax of column in matrix subset
Now lets find argmax of colA for each frame #CODE
I think it's better to use method ` argmax ` instead of ` np.argmax ` : #CODE

How to replace NaN in a row by first element of that row of a data frame using Pandas
I have random NaN values and I want to replace it by the nearest element in the row . How to proceed .
Since you want to do this for a row , you can transpose your data frame first and use the corresponding index ( a column after transposition ) with the ` fillna() ` method .

You have to use apply . Here's a toy example : #CODE

You're going to have to iterate over your list , get copies of them filtered and then concat them all together #CODE
A solution without loop but ` merge ` : #CODE

You can use ` factorize ` : #CODE
Here ` factorize ` returns a tuple containing array pairs : #CODE

I want to add a new column to my dataframe and map it to a dictionary . Mapping should be on the index of my original dataframe and I don't know how to use ` .map() ` to get there . #CODE

I am trying to merge two pandas dataframes together on index but i am getting error ...
When i try to merge , i get this error - `" [ ' TP121 ' ' TP135 ' ' TP283 ' ..., ' TP251072 ' ' TP251178 ' ' TP251355 '] not in index "` when they are clearly in both the indexes . Where am i doing wrong ? #CODE
Well that worked really well . Wonder why the merge didn't work .
For your task ` pd.concat ` is better then merge . But with ` merge ` you could merge two dataframe with ` reset_index ` on column ` rs# ` : #CODE

why I need to apply nth to the whole grouped dataframe
Because you need first apply function ` nth ` for all group and then get first rows of group . I try it in second approach .
It is together ` df.groupby ([ ' a ' , ' b ']) [ ' c ']` and then apply function ` nth ` . Not for all group ` df.groupby ([ ' a ' , ' b '])` .
@USER : Your first block isn't applicable to my question as the group index is not present ( I want to maintain this ) . In your 2nd example , I don't understand why I can't just do ` g [ ' c '] .nth ( 0 )` and why I need to apply ` nth ` to the whole grouped dataframe and _then_ select ` c ` .

How to find and replace specific values in pandas dataframe

Ipython histogram - replace old histogram with new one

How to replace a value in dataframe column with series index

Found it , ` plt.hist ` has a parameter named ` weights ` , to which I can pass an array of weights . Simply passing the ` freq ` column to ` plt.hist ` does the trick . With Seaborn : #CODE

But I don't currently have a way to insert all the ` functionA.__name__ , functionB.__name__ , etc . ` as column names in the dataframe .
IIUC , given your ` concat ` dataframe ` df ` you can : #CODE

Pandas replace nan with mean value for a given grouping
I am trying to replace the NaN values with the gic_industry_id median / mean value for that time period .
However , I am struggling to figure out how to map the NaNs to these means . And , indeed , is this the ' correct ' way of performing this mapping ? Speed actually isn't of paramount importance , but 60 seconds would be nice .

If you want to apply it to all columns , do ` df [ df 0 ]` with ` dropna() ` : #CODE
If you know what columns to apply it to , then do for only those cols with ` df [ df [ cols ] 0 ]` : #CODE

Pivot Pandas Dataframe with a Mix of Numeric and Text Fields
I tried df.pivot_table() but it only works with numeric fields I believe . Here I have a mix of text and numeric that are race related . I am able to use MySQL auto increment functionality to get the race counts as a separate column to pivot on in pandas , but it does not solve the whole problem . Also looking for a pandas only solution anyways .
Then the desired DataFrame can be expressed as the result of a ` set_index / unstack ` operation : #CODE
` set_index ` moves the ` Athlete ` and ` race ` columns into the index . The ` unstack ` operation moves the ` race ` index level into a column level .

using pandas in python to append csv files into one
You can ` concat ` . Let ` df1 ` be your first dataframe and ` df2 ` the second , you can : #CODE
@USER thanks for pointing it out . Actually 0 is the default value for ` axis ` in ` concat ` .
No worries , pandas is a great tool if you actually want to do some computation on the data , it is not the tool to use to concat a few files into one
I created list of all dataframes ` dfs ` , where dataframes are appended by ` dfs.append ( df )` . Then I used function ` concat ` for joining this list to final dataframe .
I add parameter ` header=None ` to ` read_csv ` and ` to_csv ` and add parameter ` ignore_index=True ` to ` append ` . #CODE

How to merge two grouped-by Pandas Dataframes by a common column ( ID ) together ?
You could use ` join ` #CODE
or ` merge ` by using ` reset_index ` of ` df1 ` , ` df2 ` #CODE

access previous rows in python dataframe apply method
You might want to your attempt with ` shift ` or ` rolling_apply ` methods .
Next solution is use ` map ` : #CODE

` periods = map ( lambda i : calc_time_delta ( df , i , i + 1 ) .seconds , range ( 7 ))`

I would like to make a bar plot that shows the yes or no responses aggregated by age . Would it be possible at all ? I have tried ` hist ` and ` kind =b ar ` , but neither was able to sort by age , instead graphing both age and response separately .
To generate a multiple bar plot , you would first need to group by age and response and then unstack the dataframe : #CODE

For this dataset if I delete the first 3 rows the fit is greater than 0.995 , I have tested this but I want this to be a general expression so I can apply it to other datasets .

Merge csv's with some common columns and fill in Nans

Linearly interpolate missing rows in pandas dataframe
You could convert your ` JD ` values to a ` DateTimeIndex ` and ` resample ` to daily frequency ( ( see docs ) . ` pandas.Series.interpolate() ` will then fill in the missing values between existing values in the ` Value ` columns as follows : #CODE

Use pandas merge : #CODE
You can do this in a single operation . ` join ` works on the index , which you don't appear to have set . Just set the index to ` ID ` , join ` df ` after also setting its index to ` ID ` , and then reset your index to return your original dataframe with the new column added . #CODE

You can use groupby size and then unstack : #CODE
However whenever you do a groupby followed by an unstack you should think : pivot_table : #CODE

which comes close , but this seems to make user_id the index which is not what I want ( I think ) . Maybe there's a better way involving ` pivot ` , ` pivot_table ` or even ` get_dummies ` ?
Having ` user_id ` as the index seems appropriate to me , but if you'd like to drop the ` user_id ` , you could use ` reset_index ` : #CODE

Drop rows in pandas dataframe based on columns value
Right now you're getting " if they're all positive " or " any are null " . You want " if they're all ( positive or null )" . ( Replace ` 0 ` with ` =0 ` for nonnegativity . )

The above dataframe has a datetime index . I resample it like so : #CODE
Why did the ' Val ' column disappear ? and all the other columns seem messed up too . See Linearly interpolate missing rows in pandas dataframe for an explanation of where the dataframe is coming from .
Have you tried simply print ( df ) before resample ? Looks like these are different dataframes . Resample shouldn't have this type of effect .
I did , before resample it is fine .
The ` Val ` columns will probably not have a numerical dtype for some reason , and all non-numerical ( eg ` object ` dtype ) columns are removed in ` resample ` .

Use ` merge ` . You can do a self-merge of the DataFrame with itself , then remove the extra pairs ( where features are reversed or paired with themselves ) . #CODE

I think if you don't transpose then maybe an unstack will help ? #CODE

My main question is ... if ` axis=0 ` is thought of as column-wise , why then does ` drop ( 1 , axis=0 )` drop a row ?
So when you use ` df.drop ( 1 , axis=1 )` you are saying drop column number 1 .

Unexpected difference between loc and ix
I've noticed a strange difference between ` loc ` and ` ix ` when subsetting a DataFrame in Pandas . #CODE
Why does ` df.loc [[ 7 ]]` throw an error while ` df.ix [[ 7 ]]` returns a row with NaN ? Is this a bug ? If not , why are ` loc ` and ` ix ` designed this way ?
ix can very subtly give wrong results ( use an index of say even numbers )
you can use whatever function you want ; ix is still there , but it doesn't provide the guarantees that loc provides , namely that it won't interpret a number as a location
In my opinion raising a ` KeyError ` would be ambiguous as whether it it came from index , or integer position . Instead ` ix ` returns ` NaN ` when given a list
As @USER says , this is ( at least for ` loc `) the intended and documented behaviour , and not a bug .
The documentation on ` loc ` / selection by label , gives the rules on this ( #URL ):
This means using ` loc ` with a single label ( eg ` df.loc [[ 7 ]]`) will raise an error if this label is not in the index , but when using it with a list of labels ( eg ` df.loc [[ 7 , 8 , 9 ]]`) will not raise an error if at least one of those labels is in the index .
For ` ix ` I am less sure , and this is not clearly documented I think . But in any case , ` ix ` is much more permissive and has a lot of edge cases ( fallback to integer position etc ) , and is rather a rabbit hole . But in general , ` ix ` will always return a result indexed with the provided labels ( so does not check if the labels are in the index as ` loc ` does ) , unless it falls back to integer position indexing .
In most cases it is advised to use ` loc ` / ` iloc `

Given a vector of cluster assignments and a pandas DataFrame , how can I replicate this using a Python library ( e.g. seaborn ) ? Plotting a DataFrame using seaborn isn't difficult , nor is sorting the rows of the DataFrame to align with the cluster assignments . What I am most interested in is how to display those black dividing lines which delineate each cluster .

I want to drop the all the rows which don't satisfy a given condition , in this case the condition is that the column can't contain the word ` secure `
I want to drop the row at the place , not to have a function that return ` None ` if the condition isn't meet .
Which return a boolean array , but I don't know how to used it to drop the row .
Now , how can I use this array to drop the columns ?
As for your question , you can either use the ` drop ` method or do it yourself : #CODE

` os.getcwd() ` gives you the path C :\ Users\name\folderName and then we join the filename to make the complete path .

I have a Pandas dataframe containing Latitude and Longitude data for an abundance of points , and I would like to clip them to a shapefile ; ie : everything outside the boundaries of the shapefile is dropped .
I have searched on Google , but ' clipping ' seems to refer to data being cut off by map legends / axis rather than what I'm describing .
You can try geopandas . It's a very good library that works basically with pandas dataframes , but geolocalized , i.e. containing a column of Geometry . You can import export shapefiles directly within it , and finally do something like Intersection .

I want to drop the first item in the list so that I am left with the last value without it being in a list . Such as : #CODE
` apply ` a lambda to access the last element : #CODE

But am stack on how to output the data from this loop : x
You may need to strip the filename down to what you really want but this is trivial

pandas pivot table with average time
I have been working with pandas to do analysis on time series data and have become stuck with integrating them into pivot tables . I have a data in a csv as : #CODE
I want to create a pivot table with this time information but when I try : #CODE

Another method would include ` pivot ` . Starting from your dataframe ` df ` , I would set the index to ` Date ` : #CODE
and then pivot the table according to your values : #CODE

I have looked at both ` Transform ` and ` map ` but without much success . Thanks for your help .

Pandas - Merge and Groupby different dataframes and create new columns

I have two pandas tables , ` d ` and ` num_original_introns ` . They are both indexed with the same non-numeric index . I want to apply a step function to transform ` d ` based on values in ` d ` and ` num_original_introns ` , like so : #CODE
I know that this is invalid , and it is not possible to apply a pair of conditionals like this , but I can't seem to find an alternative from googling . How can I do this ?

Individually replace NaN in pandas.dataframe
Why you couldn't use ` apply ` with ` axis=1 ` then ?
IIUC you could use ` apply ` with ` axis=1 ` and ` fillna ` with your custom function : #CODE
If you want to get column names for missing values you could apply or use that function for processing : #CODE

Even after taking out the categorical explanatory variable , you still have other variables that shift the predictions . If you want a plot with respect to one of the variables , then I guess it would be something like the partial regression plot or the CCPR plot #URL Neither of those has confidence intervals and I'm not sure how we would define them .

I am using a mixture of both lists and pandas dataframes to accomplish a clean and merge of csv data . The following is a snippet from my code that runs disgustingly slow ... Generates a csv with about 3MM lines of data . #CODE
I then append this data frame to a list " dummydata " and loop to the next api
The loop is a reasonable solution but I don't understand what it is you're doing with most of the variables in the second loop . Maybe you meant to append to the lists each time like you do to DateList but all the other ones you just rewrite over every time you get a hit in the second loop . If your second loop only has one possible answer , you could add a " break " statement at the end ( of the second loop ) so that it doesn't keep iterating over the rest of the possibilities . That should save loads of runtime .

When looking online , I tend to see examples of ' hardcoded ' variables , but I don't get how to apply this to a dataframe column - I found that I should use strptime to identify what format my date column is , but I don't know if this has any effect ( I get the same error if I comment out the convert_dates apply method ) .

I'd like to avoid searching the whole dataframe as it only contains repetition of the first column , and my value for n can be somewhat long . In this example , what would be the best way to drop the columns that start with ' c ' , ' d ' , ' e ' , and ' f ' , knowing that they will all include an ' f ' somewhere . Later I join all the strings in each row into one value but it seems like it should be easier to manipulate the data at this stage where everything is in different columns . This is with pandas 0.16.0 and must work on python 2.76 and python 3.4 . Thank you !

python pandas- apply function with two arguments to columns

Also , I then want to have another column that tells me how many quarters after the first booking that booking was made . I failed using replace and dictionary , so I used a merge . I create an numeric id for each quarter of booking , and first quarter from above , and then subtract the two : #CODE
Continuing off of part 1 , you can merge the values back on to the original dataframe . At that point , you can write a custom function to subtract your date strings and then apply it to each row .

pandas : concat data frame with different column name
I want to concat x and y into a single column like this keeping their ids #CODE
And how if I want to give a new name for the concat column ? ( e.g. ' x ' to ' xy ')
Updated to hopefully make the steps more transparent - I did in fact set ` id ` as ` index ` before ` concat ` .
If ordering of rows is not important , you can use ` stack ` : #CODE

Next solution is use ` map ` : #CODE

but the next steps of the reshaping elude me . I have tried a few things with pandas pivot and pandas unstack but it seems that I am missing something as none of it worked . I guess I also would need to use pandas cut at one point for the binning but I failed to reach this stage in my analysis .

Of course , the actual datetimes used are only examples used to illustrate the difference from what I want to what I've been getting . I want a something that gives me the same values of ` res ` but without having to create a whole ` Series ` object ( or ` DF `) only to resample it sater . What I've tried so far is #CODE

` len ( df.resample ( ' W-MON ') .index ) = 886 ` and
` len ( df.resample ( ' W-TUE ') .index ) = 885 ` ,

Replace this line : #CODE

The data that I am using has some missing days and I am trying to interpolate that data . Below is some part of the data ; #CODE
Now I have two questions ; while using ` inter_lin_nan ` function , it does interpolate data but it also changes the next day data and the next data is totally different from the one available in the excel file . Is this common or I have missed something ?
Use as frequency to add the required frequency of timestamps to your time series , and uses interpolate() to interpolate nan values only .

` data = data.astype ( bool )` should give what you want .
Perhaps I'm missing something , but doesn't this just bring me back to square one ? 0s and 1s are what I'm trying to replace .
Maybe ` map ` can do it : #CODE

resample gives different final dates in pandas
I'm having trouble to understand whats going on when I resample into weekly data this way , specifically into mondays .
resample defaults to mean . The mean for the week from 12-15 to 12-21 is simply the last row ...
@USER resample does the mean : for values between 12-07 ( inclusive ) up to 12-13 ( not-inclusive ) ... When it's closed= '" left " .
@USER that wouldn't be a resample , resample needs an aggfunc which defaults to mean ( you can also sum etc . ) . You can reindex to just slice out a particular date_range #URL

I am using the pandas.DataFrame.dropna method to drop rows that contain NaN . This function returns a dataframe that excludes the dropped rows , as shown in the documentation .

thanks Padraic , I have an example of the desired output on the question . Nevertheless , any idea why your code give me this error : AttributeError : ' Series ' object has no attribute ' items ' n the " pd.DataFrame ( map ( it ( 0 ) , sorted ( row [ 1 :] .items() , key=it ( 1 ) , reverse=1 ) [: n ])
thank you , it looks a lot better with the explicit loop ... not sure whats going on , probably a version thing ; Just to abuse your will a lottle longer , can you explain what does the " map ( it ( 0 )" is doing in this example ? I see that " row [ 1 :] .order ( ascending=0 ) [: n ] .iteritems() " returns an " itertools.izip " object , but not sure what " map " is doing .
@USER , we sort / order by the value i.e the number but we want the string i , e ` option3 ` etc .. ` map ( itemgetter ( 0 ) .. ` pulls the first element which is each string from the tuples returned from ` .iteritems ` . Each tuple would be ` ( " option_x " , i )`

append to data to from df2 to stock and result of conditional .
I think , i worked it out using - result = pd.concat ([ df3 , df2 ] , axis = 1 , join = ' inner ')
@USER that or a plain old inner join : ` df3.join ( df2 , how= " inner ")` .

You can ` groupby ` dataframe by column ` Name ` , ` apply ` custom function ` f ` and then select dataframes ` df_A ` and ` df_B ` : #CODE

I.e. , all my y-labels are cut off , and margins are too big . I've found how to tweak them here :

If I have a pandas Series composed of strings , say something like [ " ab " , " ac " , " bc " , " ab " , " ab " , " abc "] , is there a built in function to replace each of the strings by some integer corresponding to the string ? In my example , if we index it as " ab " : 0 , " ac " : 1 , " bc " : 2 , " abc " : 3 , then it would be [ 0 , 1 , 2 , 0 , 0 , 3 ] .
Merge seems a little bit overkill there , you can do it base ` python ` with list comprehension in one line : #CODE

@USER Foley Cool ! I went to the link and now I understand the short cut . ( row [ ' xx '] or '') is a short format for ( row [ ' xx '] if row [ ' xx '] or '') , a sort format for the cases when the conditional test and one of the value is the same . Thanks !

I think the drop might be modifying the indexes somehow so that merge can't read ' uid ' but I don't know how to fix it .
Because pandas ' read_csv method will not strip spaces in the first line for you . You can see all the keys in your csv dataframe by printing csv1.keys() , which will like this : #CODE
So you have to use ' uid ' as merge key or change the first lines in your file1.csv or file2.csv .
P.S. You may look at this question to save a little strip work by hands

I have a dataframe and I'd like to apply a function to each 2 columns ( or 3 , it's variable ) .
For example with the following ` DataFrame ` , I'd like to apply the mean function to columns 0-1 , 2-3 , 4-5 , .... 28-29 #CODE

if we traverse the down csv we might likely to have another 2001-02-02 . Instead of creating it , we can append it to initial StartDate . However , the DptCityDptCountry might be different but if another ID matches with the StartDate and DptCityDptCountry , it will be added up i.e. #CODE
Then I push all but 1 column into the index with set_index . This leaves one column which comes back as a Series . Then use apply and return a series indexed on the expanded set of dates for each row ( Series of Series = DataFrame ) . So for each of the 7 rows in the DataFrame , I get a series indexed on the expanded date range . Then its just clever stacking , naming , and reset_index . #CODE

You can select by ` loc ` with ` any ` and specify ` axis ` : #CODE

You can ` apply ` function to ` groupby ` where use another ` apply ` with ` replace ` ` 0 ` to ` NaN ` : #CODE
You can use this apply function : #CODE

IIUC you can use ` stack ` : #CODE

What I don't understand is that if I do ` len ( df [ ' branded ']` I see 8173 , but if I do ` df [ ' branded '] .describe() ` the first line of output is ` count 5158 ` . Is it possible that the evaluation of ` df [ ' branded ']` somehow excludes null / None values , or whatever weird datatype these None values actually are ?
So what does ` len ( df [ ' branded '] .convert_objects ( convert_numeric=True ))` show ? also I'm assuming you are assigning back the return from ` convert_objects `
` print len ( df [ ' branded_items '] .convert_objects ( convert_numeric=True ))` shows 8173 , but then ` print df [ ' branded_items '] .describe() [ ' count ']` shows 5158.0 . The raw data comes from BigQuery so won't be easy to post here , but I'll try to post a simplification that demonstrates the issue . Thanks for the help .
Ah - I see the problem now - ` df.fillna ( 0.0 )` does not replace in-place , I needed to do ` df.replace ( ' None ' , 0.0 , inplace=True )` first , then ` df = df.fillna ( value= 0.0 )` . Odd that the first command replaces the ` None ` strings with ` NaN ` rather than 0 , but that seems to be the case . Thank you so much for your help !

Tried to replace the code with ` loc ` : #CODE

Another option is to use ` df [ ' D '] .apply ` to expand the items in the list into different columns , and then use ` stack ` to expand the rows : #CODE
Although this does not use explicit ` for-loop ` s or a list comprehension , there is an implicit for-loop hidden in the call to ` apply ` . In fact , it is much slower than using a list comprehension : #CODE
Yes and no -- there is a way to do it using ` apply / stack ` which avoids the * explicit * double for-loops , but it is actually much slower than the list comprehension-based solution shown above . So if you are trying to avoid ` for-loop ` s for performance , then I don't think there is a good way . You see , when you put non-native NumPy data types , such as lists , in a DataFrame , ultimately computations on those values require plain Python methods which are relatively slow ( compared to native NumPy methods ) . To break apart the items in the lists require plain Python loops no matter how you phrase it .

Cannot interpolate in pandas dataframe
How can I interpolate the time series like this ? #CODE

Groupby and Pivot Pandas table
This should be quick , but none of the pivot / groupby work I'm doing is coming up with what I need .
I want to pivot it , so that the Index is basically YrMonth and Letter , the Period are the columns , and the Amount are the values .
I understand Pivot in general , but am getting errors when I try to do it with multiple indexes . I made the index a column , and tried this : #CODE

Do I then save that as a function and apply it to the dataframe or could I just run that on it's own and have it append the column to the original df ?
this works but when I start adding in all the exception cases it gets pretty long / unruly . How would I translate that to a standalone function ?

Please , explain , how to extract features in that sample case , how to train a model and so on , or provide a good tutor for that case ( I'm not able to translate sklearn tutor to my case ) .

What is your desired output ? If you simply want to fill the NaNs , append [ ` .fillna ( args )`] ( #URL ) to your concatenation .

Are you sure you want to drop the index ? If these are bootstrap resamples , and you're subtracting predictions from training observations , as it looks like , you want to perform that operation on the same observations . The NaNs may be telling you that you have predictions for observations that don't appear in one of your resample folds .

` apply ( F )` calles ` _python_apply_general ` . As the name implies , it is ageneral propose method . Under the hood , it does not attempt checking if a faster ` cython ` version of aggerate function exists . It applies ` F ` to each group and assembles the results together , which means it would run slower than the optimized ` cython ` version equivalent ( such as ` .sum `) .
Finally , ` apply ( lambda x : F ( x ))` will be slightly slower than ` apply ( F )` due to the additional ` lambda ` function .

Pivot pandas data and add column
I then pivot : #CODE

Replace values in numpy 2D array based on pandas dataframe
In the numpy array above , I would like to replace every value that matches the column ` country_codes ` in the dataframe ( df_A ) with the value from the column ` continent_codes ` in df_A . df_A looks like : #CODE
Right now , I loop through dataframe and replace using numpy indexing notation . Given that iterrows() tends to be slow , is there a more direct / vectorized way to do this ? #CODE
Hmm , one method would be to construct a df from your array and then call ` map ` on each column : ` a = pd.DataFrame ( arr ) a.apply ( lambda x : x.map ( df_A.set_index ( ' country_codes ') [ ' continent_codes '])` or something like this

align on both row and column labels . Can be thought of as a dict-like

You can first create a set of random numbers attached to each of the items in the second level of the index ( ` df.index.levels [ 1 ]`) . Then you can use a list comprehension to cycle through each label of that level and map the random number . #CODE

In Pandas , how to apply a customized function using Group mean on Groupby Object
I want to create groups based on value of column A . So I slice A first . And define a function . Then I use apply method on the Groupby Obj . I am expecting the new column to be the difference between B and C over the group mean of A . #CODE

I want to apply a group by on a pandas dataframe . I want to group by three columns and calculate their count . I used the following code #CODE

I am attempting to map this dictionary over various cases of self reported Twitter location that I have in a pandas dataframe to look for partial matches . For example , if one case read ' anchorage , ak ' it would change the value to Alaska . I could see this being quite simple if it were a list , yet there must be another way to do this without looping . Any help is greatly appreciated !

A simple map on a transposed dictionary should get you what you want . All the values in the dictionary are unique , so transposing it won't result in duplicate keys . #CODE
How about using ` factorize ` ? #CODE
Well done . Never heard of ` factorize ` . ` %timeit labels , uniques = df.A.factorize()

How to merge two dataframes based on the closest ( or most recent ) timestamp
I would like to fuzzy ` merge ` the dataframes with a join on the ` timestamp ` . However , if the timestamps don't match ( which they most likely don't ) , I would like it to merge on the closest entry before the timestamp in ' A ' that it can find in ' C ' .
` numpy.searchsorted() ` finds the appropriate ` index ` positions to ` merge ` on ( see docs ) - hope the below get you closer to what you're looking for : #CODE

I think native apply is the best , but not . I found faster approach : #CODE
You can use apply with lambda with is faster than your solution : #CODE
With len ( df ) = 5 #CODE
With len ( df ) = 10000 #CODE

Try to transform to_keep in a dataframe and then merge it with the original , like in Compare Python Pandas DataFrames for matching rows

I would like to merge two Pandas dataframes together and control the names of the new column values .
I want to merge them together to get a final data frame , joining on the ` org ` and ` name ` values , and then prefixing all other columns with an appropriate prefix . #CODE
I've been reading the documentation on merging and joining . This seems to merge correctly and result in the right number of columns : #CODE
The ` suffixes ` option in the merge function does this . The defaults are ` suffixes =( ' _x ' , ' _y ')` .

Pandas : merge multiple dataframes and control column names ?
I would like to merge nine Pandas dataframes together into a single dataframe , doing a join on two columns , controlling the column names . Is this possible ?
I want to join them into a single dataframe with the following columns : #CODE
I've been reading the documentation on merging and joining . I can currently merge two datasets together like this : #CODE
But how can I do this for nine columns ? ` merge ` only seems to accept two at a time , and if I do it sequentially , my column names are going to end up very messy .
Just found this #URL but I'm not sure it works for my example - I guess I need to concatenate , then merge somehow ? I want to write my output to a BigQuery table , so I don't know if hierarchical dataframes will work for me .
You could use ` functools.reduce ` to iteratively apply ` pd.merge ` to each of the DataFrames : #CODE
To pass the ` on =[ ' org ' , ' name ']` argument , you could use ` functools.partial ` define the merge function : #CODE

I have the dataframe with 9 series ( a date series ( Monthly ) and 8 monthly returns series ) . I would like to append 7 new series formed by taking the difference of each of series3 through 9 and series2 ( i.e. series10 is series3 - series2 , series 11 is series3 - series2 , and so on ) . The new series should have the same labels as the original series with the prefix " Excess " . I am able to do it one series at a time i.e. df [ " series10 "] =d f [ " series3 "] -df [ " series2 "] but how do I do it using a " for " statement and numeric reference to the series ? Also , how does one calculate the correlation of two series within a dataframe . Thanks in advance .

The following code snippet worked fine until I added a couple of lines of code that referenced date but does not append or change it above it . with the simple case of setting #CODE

you don't even need to use ` apply ` : ` df [[ ' AccX ' , ' AccY ']] .values `
But if I want to do some operation on each row , I still need ` apply ` correct ? Also I kind of just want to know why the first three don't work . My whole data frame has 11 columns

when using groupby with multiple columns , how to NOT drop an unobserved combination of columns
I wonder if there is a way to NOT drop this combination and instead give it a count of 0
Minor , but " drop " seems the wrong word , given that the combination isn't there in the first place .

This code gives me an attribute error when I apply the seasonal_decompose method :

Then I use ` melt ` to get all of the quarters in the same column . #CODE
Creating Period objects is expensive , so let's identify the unique quarters and then apply the period mapping . #CODE
Now we can map the ` qtr ` column in the initial dataframe and use a list comprehension to extract each of the four quarters . These values are then zipped with the ` sid ` . #CODE

Python Pandas MatPlotLib Find The Intersection Of Two Lines ( One Curved , One Straight )
I'm trying to determine the intersection of two lines .
The intersection was found near ` ( 0.96 , 37.19 )` : #CODE

If Date were an index / DatetimeIndex , you could resample : #CODE
@USER yes , like I say for the resample you need a DatetimeIndex . i.e. do ` df.set_index ( " Date " , inplace=True ); df.index = pd.to_datetime ( df.index )` .

Consider a groupby apply function with sort : #CODE

pandas dataframes merge with same column names , give priority to one
Any idea how it could be converted with cat or merge or some other function ( without having to manually " drop " the common columns before or after the merge , and merge must be still inner ) into the following output ( assumes " priority " is given to ` df1 [ ' A ']`) ? #CODE
You can transpose your dataframe , drop duplicates , and transform again . #CODE

You can just ` groupby ` a list of length ` len ( df )`

If you have 50 ` Series ` , you can use ` concat ` and ` groupby ` by ` index ` with ` sum ` : #CODE
You can trandform the series to dataframes and then merge them together using the date as key : #CODE

I would like to be able to extract the characteristic parameters from kernel density plots produced using Python's Seaborn . While there is a very nice example on obtaining the median of a distribution , I'd like to see whether this can be generalized for multimodal distributions for 1D data and particularly in the 2D case .

Insert Row in Python Pandas DataFrame
I have searched for a method to insert a Row into a Pandas DataFrame in Python , and I have found this :
How can I make the code insert a row WITHOUT overriding it ?
For example , if I have a 50 row DataFrame , insert a row in position 25 ( Just as an example ) , and make the DataFrame have 51 rows ( Being the 25 a extra NEW row ) .
Is it possible to insert a row at an arbitrary position in a dataframe using pandas ?
Possible duplicate of [ Is it possible to insert a row at an arbitrary position in a dataframe using pandas ? ] ( #URL )
With a small ` DataFrame ` , if you want to insert a line , at position ` 1 ` : #CODE

You can split column ` HISTOGRAM ` to new ` DataFrame ` and ` concat ` it to original . #CODE
or to drop the columns will all nana : #CODE

This works fine for some files but for some other files it raises the error : ` ValueError : could not convert string to float ` . Which naturally makes me think there is something wrong with the file . But , when I try to loop sequentially over the data and apply the same conversion it doesn't give any error . So I cannot figure out where the problem in the file is or what's the problem with the converter .
I discovered that some values are just missing , the non obvious thing is why the conversion gives an error when trying to convert an empty string to float when ` map ( float , [ ])` doesn't .

As you already mentioned , ` by_tz_os.size() ` is a Series . Unlike with , for example , the ` cframe [ " tz "]` Series for which the ` name ` attribute will be set to " tz " , ` by_tz_os.size() ` does not have a name attached to it , most probably due to there being no obvious way to name groupby() results in general . Which is , of course , why ` by_tz_os.size() .name ` is ` None ` here .

Python Pandas Dataframe filter and replace
Very unlikely case because these numbers are random floats . But if it happens it should take two of these highest numbers and replace them with 1

I have a python dataframe with hourly values for Jan 2015 except some hours are missing the index and values both . Ideally the dataframe with columns named " dates " and " values " should have 744 rows in it . However , it has randomly missing 10 hours and hence has only 734 rows . I want to interpolate for missing hours in the month to create the desired dataframe with 744 " dates " and 744 " values " .
Use the panda interpolate funtion
Interpolate will not work in the case were you have datapoints missing at the very start of the time series . One idea is to use pandas.Series.fillna with ' backfill ' after the interpolation . Also , do not set fill_value to 0 whe you call reindex
then use ` df.interpolate ` to replace the NaNs with ( linearly ) interpolated values based on the DatetimeIndex and the nearest non-NaN values : #CODE

Group on the column , get the count of each , and then unstack the results . #CODE

You could probably skip the ` fillna() ` step if you later drop the ` 0 ` values again ( that's why it doesn't say ` df_datafile2 `) .
That is so close to what I'm looking for . Just three more things : 1 ) how do I completely drop the variables that have no values for that participant ? All of the variables are being written to the new file ; 2 ) the .to_csv() transposes the variables so they are now in a column rather than in a row . Is the best solution to just use .transpose() before writing the file ? 3 ) how do I write the V1 variable as the name of each individual .csv file ?

The ' desired values ' as posted are the same as grouping on ' A ' only . I assume the desired result is to group on both A and B , but only select out column B that has ' one ' . If the desired result is to group only on ' A ' and then insert a new column ' B ' that has ' one ' in each entry that can be done also .

In pandas v0.17.1 ( anaconda python v3.4.3 ) the replace function on ` datetime ` is broken .
I am trying to replace a string value in my ` DataFrame ` with new value . This ` DataFrame ` contains multiple columns ( including a datatime column ) .
The replace function fails #CODE
line 594 , in replace
line 3110 , in replace
line 2870 , in replace
return self.apply ( ' replace ' , ** kwargs ) File " / home / xxx / anaconda / envs / py3 / lib / python3.4 / site-packages / pandas / core / internals.py " ,
line 2823 , in apply
line 607 , in replace
As a workaround ( assuming you have no duplicately named columns ) , the Series replace still works fine in 0.17.1 : #CODE

merge every two columns on pandas.DataFrame
Possible duplicate of [ How to merge 2 columns in 1 within same dataframe ( Python , Pandas ) ? ] ( #URL )
Generator - join items of tuples : #CODE

Iterate through ` ArrivalDate ` and append the equivalent rows as follows , then Expected output :
( I need to check under DepartureDate if it matches with ArrivalDate , if it does match , I want to append its ID to the Departure otherwise " Arrival " will only be listed #CODE
The condition to have any ID appended to " Departure " in the JSON format , since i'm considering ArrivalDate , I want to check if the DepartureDate matches with Arrival on a given row and it does match , I want to append it

The problem is that I've got no values to merge it back with . If I bring ` zipcode ` into ` features ` when I create it , the zipcode gets whitened along with ` highclust ` and ` callclust ` , rendering it useless .
Normalize a group of observations on a per feature basis .

` str.contains ` return bool values . You could convert it to integer with simple add 0 : #CODE

Pandas DataFrame stack multiple column values into single column
You can melt your dataframe : #CODE
After trying various ways , I find the following is more or less intuitive , provided ` stack `' s magic is understood : #CODE

use ` factorize ` : #CODE

I can't seem to apply ` to_datetime ` to a pandas dataframe column , although I've done it dozens of times in the past . The following code tells me that any random value in the " Date Time " column is a string , after I try to convert it to a timestamp . The `' errors=coerce '` should convert any parsing errors to `' NaT '` , but instead I still have `' 2015-10-10 12:31 : 04 '` as a string . #CODE

Resample based on a column value
I would like to resample this DataFrame : #CODE
How to do this with pandas , i.e. resample based on a condition on a column ?
I thought this was a common data-esque task : to resample at certain points , each time some amount B increases : here each time B increases of 4 units . Isn't there a panda way ?
I would first remove the ones that are > 4 , then resample with the last : #CODE
If you don't want all the times in the middle , you have to use a groupby rather than resample : #CODE

I have an ` apply ` function that operates on each row in my dataframe . The result of that ` apply ` function is a new value . This new value is intended to go in a new column for that row .
If you need to use other arguments , you can pass them to the ` apply ` function , but sometimes it's easier ( for me ) to just use a lambda : #CODE
I should've mentioned this before : my function has two arguments , a row from the ` dataframe ` , and a global dictionary . I tried incorporating these like so : ` df [ ' new_column '] = df.apply ( my_fxn ( row ) , args =( ) , axis=1 )` but it seems to be breaking the global_dictionary up into individual individual K / V pairs . As a result it tells me that there are too many arguments . How do I pass arguments to the ` apply ` function ?

Another thought was SQLalchemy ; I could shift the whole thing out of Pandas and in to the SQL realm and build functions that make use of the or_ function and SQLalchemy filtering ( like this : Using OR in SQLAlchemy ) . But that means I have to learn SQLalchemy ( which I will do , if that's the best solution here ) .

I'm hoping the solution will detect that there is no existing COL3= ' Y ' for COL1= ' B ' and therefore add the row while setting COL2 to 0 for the new row . The code should get the set of unique values of COL3 , check to see if all exist for all unique values of COL1 , and if not , add the row . It doesn't get more complex than this , I was only trying to get an answer that I can apply to many rows instead of just manually inserting that specific row .

max_len = max ( map ( len , rows ))
@USER weird , can you paste a gist of the entire stack trace #URL ? Are there some empty blocks ( no non-empty lines between +++ lines ) ? cos that might do this .

What if I wanted to keep ` y ` the same ? What if I want it to replace based on ` y ` and not ` x ` . Also , what if there are multiple columns on which I d like to do the replacement ( in the real problem , I have to update a dataset with a new dataset , where there is a match in two or three columns between the two on the values from a fourth column ) .

However , if I replace all the `"` in a.csv to `'` , then the ` f_1 ` could be correctly infered into `' object '` . How can I prevent the wrong inference without modifying `' a.csv '` ? Another question is that why pandas infers strings as `' object '` type rather than `' str '` type ?

can I add a columns this way df [ ' freq '] = df.groupby ([ ' a ' , ' b ']) .size() and delete the duplicates ?

argmax gets the first True . Use argmax on the reversed Series : #CODE
@USER one thing is a little annoying is that reversing creates a copy ( IIUC ) ... you could drop to the values and use the numpy reversed view which may be slightly faster ( but IMO much less readable ) as essentially O ( 1 ) .
@USER I guess that it's not immediately obvious why ` argmax ` works here , still it's quicker which is what usually counts
You can use ` idxmax ` what is the same as argmax of Andy Hayden answer : #CODE

There must be a way to use pandas / numpy array functions but tell it to skip the first row in the calculation . How to do that ? I've tried Boolean indexing but can't get it to work , and maybe there is a way to tell Pandas to skip the NaN results ... but the best approach seems to be a qualifier that says " apply this code , starting at the second row . "
IIUC you can skip first row of column ` A ` of ` df_source ` by selection all rows without first by ` ix ` : #CODE
I think I understand your problem and in these cases , I usually find it easier to make a list and append it to the existing dataframe . You , of course , could make an Series instance first and then do calculations . #CODE
IIRC , iloc is being deprecated in future builds of pandas in favor of ix

Resample pandas dataframe by both name and origin
Now , I would like to resample this time series by listing for each week the number of times a certain rail company departed from this station by origin .
Or you can select column ` Train ` and ` resample ` it by ` count ` : #CODE
Select one column ` Train ` , where is ` BritishRail ` using ` isin ` and resample it with ` count ` instead of ` sum ` : #CODE

After searching thoroughly on the material available and the tutorials , I guess I can replace all the ' ? ' with let's say a scalar value such as 0 . But , won't it effect the model then ( I am sorry , but this is more of a first hands-on project with ML and a proper dataset , so please forgive my ignorance in the matter ) ? Or else , how should I proceed ahead in such a case ?

` map ` iterates over the values in the column to pass them to the ` lambda ` function one at a time . Underneath , columns / Series in pandas are just ( slices of ) NumPy arrays , so pandas defines the following helper function to get the value out of the underlying array for the function . This is called by ` map ` on each iteration : #CODE

I ended up pushing the condition inside the converter , so my solution is very similar to this except that I don't explicitly use a date parser . Btw I avoided using a try catch statement , although it is more intuitive , because for the cases where an exception was raised , probably due to the asynchronous nature of exception handling , it impacted the performance badly . So instead I used a dummy / hackish condition like ` if len ( data.split ( ' : ')) > 1 : ` . Which I hate but it is faster .

Is there a straightforward way to do this ? I've looked into ` groupy ` and ` transpose ` but have not been able to produce anything close to what I'd like .

I am trying to add a column to a pandas dataframe ( df1 ) that has a unique identifier ( ' id ') column from another dataframe ( df2 ) that has the same unique identifier ( ' sameid ') . I have tried merge , but I need to only add one specific column ( ' addthiscolumn ') not all of the columns . What is the best way to do this ? #CODE
If you want to only keep the first occurrence of each ` id ` in ` df1 ` as your sample suggests , you'd have to ` drop_duplicates() ` . Renaming the ` columns ` you are merging on avoids having both in the resulting ` DataFrame ` , but you could of course just as well ` drop ` the redundant ` column ` post merge . #CODE
Because you just want to merge a single column , you can select as follows : #CODE

And want to insert it into an email . #CODE

OneHotEncoder requires integers , so here is one way to map your items to a unique integer . Because the mapping is one-to-one , we can also reverse this dictionary . #CODE
Now create a OneHotEncoder and map your values . #CODE

You can merge ` tf ` with ` df ` using ` tf.merge ( df )` , example with results below : #CODE
I've extened both datasets by one non-unique value . Note how the result above differs from a simple merge , which does an inner join by default . #CODE

pandas : merge several dataframes
You can adapt the join method by setting ` how ` :
left : use only keys from left frame ( SQL : left outer join )
right : use only keys from right frame ( SQL : right outer join )
outer : use union of keys from both frames ( SQL : full outer join )
inner : use intersection of keys from both frames ( SQL : inner join )
Merge do note preserve the index ( as you can see in the above results )

Melt the Upper Triangular Matrix of a Pandas Dataframe
How can I ` melt ` only the upper triangle to get #CODE
First I convert lower values of ` df ` to ` NaN ` by ` where ` and ` numpy.triu ` and then ` stack ` , ` reset_index ` and set column names : #CODE
I the only thing to watch out for is if you have any ` NaN ` values that you want to preserve in the upper triangle ( ` stack ` will drop them all ) . You might have to explicitly construct the multi-index and then reindex if that is the case .

As you asked , you can do this efficiently using ` isin ` ( without resorting to expensive ` merge ` s ) . #CODE
You can use merge if data size is big : #CODE
You can merge them and keep only the lines that have a NaN . #CODE

I am trying to interpolate data for some missing days . The orginal data is ; #CODE
As you can see 2014-12-28 is missing , so I tried to interpolate it using both Numpy and Pandas .
The problem is , both of these method does interpolate for the missing day , but they also change original data for 2014-12-29 . Do you know why this is happening or am I missing something ?

I am trying to merge two geodataframes ( want to see which polygon each point is in ) .

It occurred to me that it might be much faster to identify those groups that have duplicates using count . Then I can apply the max transformation to that grouping , and then recombine the two into one .
I think the problem is that g [ " liq "] .transform ( " max ") resets the index , losing the original index in the process ? certainly df [ df [ " liq "] == g [ " liq "] .transform ( " max ") results in a memory error ... I'm still struggling with this . doing g.size() produces an effective count of the number of duplicates , and is very , very , fast , so I am trying to use this to get the unique_id and period_id pairs where size > 2 , then apply the max idea above to those which should be much faster , and then I'll need to recombine with the original data frame .
Does this do it faster ? If so will append to answer . #URL

Pandas DataFrame apply Specific Function to Each column

I don't know my way around the pandas plotting code to check , but I bet there is a ` if stacked : ` conditional someplace in the code and ` bool ( ' false ') is True `

As I am unable to proceed much , I created the header x first , and then filling the values with unique values . Then saving it with filling all values with 0 first . Then I am trying to next column header v and trying to append with underscore with unique values of v . Here I am stuck as how to proceed . Can you please help .

I don't really see a way beside reading the dataframe row by row and then creating a model instance , and saving it , which is really slow . You might get away with some batch insert operation , but why bother since pandas ' ` to_sql ` already does that for us . And reading Django querysets into a pandas dataframe is just inefficient when pandas can do that faster for us too . #CODE

Oh yes . I am not so familiar with stack exchange ' etiquette ' . done

if you have a list of indices already then you can use ` loc ` to perform label ( row ) selection , you can pass the new column name , where your existing rows are not selected these will have ` NaN ` assigned : #CODE

Pandas / Python : Replace multiple values in multiple columns
I've tried using replace : #CODE
I can use map : #CODE
e.g. Pandas - replacing column values , pandas replace multiple values one column , Python pandas : replace values multiple columns matching multiple columns from another dataframe

You can use the replace method : #CODE

My only solution as of now is to replace the original NaNs with a unique string , concatenate the csv's , replace the new NaNs with a second unique string , replace the first unique string with NaN .
And append #CODE
In case you have a core set of columns , as here represented by ` df1 ` , you could apply ` .fillna() ` to the ` .difference() ` between the core set and any new columns in more recent ` DataFrames ` . #CODE
The issue with this is that it will replace all NaN values in that column . If the 3 is NaN then it would become " predated " and would not be distinguishable .

How to replace infinity in PySpark DataFrame
Or do I have to take the painful route : convert PySpark DataFrame into pandas DataFrame , replace infinity values , and convert it back to PySpark DataFrame
Actually it looks like a Py4J bug not an issue with ` replace ` itself . See Support nan / inf between Python and Java .

How to append on a dataframe with timezone aware timestamp column ?
I have a data frame with a timestamp column and a numeric column . I am able to append a new row to it if the timestamp column is timezone naive . #CODE
But if I set timezone for the timestamp column , and then try to append new rows , I get error . #CODE
Any help on how can I append new rows to a dataframe having timezone aware timespamp column will be greatly appreciated .
what is your pandas version . I can run this example fine in 0.16.1 . As an aside , rather than doing apply ( pd.to_datetime ) , just do pd.to_datetime ( df ) . This line : df [ 0 ]= df [ 0 ] .apply ( pd.to_datetime ) also seems to be wrong it seems you want df [ ' timestamp '] = df [ ' timestamp '] . .

Apply functon with a condition on the first row
For instance , I would like to apply ( lambda x : x+ 273.15 ) on each columns which contain C data .
You don't need to use drop in case you using ` read_csv ` with ` header =[ 0 , 1 ]` because first row already read as header . Just delete that line
It's not a good idea to store units in first row . You could do 2 level header for that and then operate with them as with usual numeric columns . First you need to do 2 level header , then drop 1st row and cast ` reset_index ` to make it from 0 ( or you could omit that if you are fine with index starting from 1 ): #CODE

Thanks , works perfectly fine . just one more thing , what if I've another column ( double or float ) in df1 . Is it possible to get that in the final output by changing the " how " in merge ?
I think ` on ` in function ` merge ` is for matching - better example with pictures is [ here ] ( #URL ) . ` df = df1.merge ( df2 , on= ' Stock ' , how= ' left ')` is same as ` df = pd.merge ( df1 , df2 , on= ' Stock ' , how= ' left ')` .
Thanks , help appreciated . Works similar to merge function in R . Is there a way to add another column ( with values ) present in df1 in the final output() ?
Hmmm , I think if you merge ` df1 ` and ` df2 ` by ` Stock ` , you get all other columns from df1 , so another columns too . Try it .

Replace Numeric Constant in Pandas Panel
I have a panel of data that contains a rolling daily correlation matrix of two identical dataframes with multiple columns . I'd like to calculate some daily descriptive statistics but want to ignore correlation coefficients of 1.0 ( since I'm correlating the dataframe with itself ) and can't figure out the best way to do so . The replace method doesn't seem to be working for me . How best to go about this ? #CODE

If I call map or ` mapPartition ` and my function receives rows from PySpark what is the natural way to create either a local PySpark or Pandas DataFrame ? Something that combines the rows and retains the schema ?
Ah , now I understand . Unfortunately , I can't think of a good solution . Which dataframe operation do you want to apply if you had the rows combined into a dataframe ?
That doesn't answer my question I need it to operate inside a map call on a partition . If there's a map that passes dataframe that would be good too .
Sorry , I couldn't understand ` map that passes dataframe ` . What is expected as output of spark dataframe ? You want to create dataframe for each partition ?
Pandas and Spark DataFrames are not even remotely equivalent . These are different structures , with different properties and in general you cannot replace one with another .

then to resample .
Would I resample the date_index or CEITest first ? Could you give me an example of how to resample these data frames ? Thanks for your help Andy !
@USER this is * instead * of ` date_index ` . Once you have a DatetimeIndex you can do ` df.resample ( " d " , how= " sum ")` or similar . Look up how to resample separately . It's similar to a groupby , you can also do ` df.groupby ([ pd.TimeGrouper ( " d ") , " RegionC "]) .sum() ` etc etc .
And final you can check ` notnull ` values by ` isnull ` with ` any ` : #CODE

One easy-fix method would be to replace the Month string with its equivalent number . #CODE

I'm trying to pivot a data frame with strings , to get some row data to become columns , but not working out so far .
What I would like to pivot to #CODE
What is the way to pivot a data frame with string values ?
You can index the pivot in whatever manner you think matches the level of aggregation you desire . Just set the ` index ` arg to the specification you want .
A higher level question could be , is Pandas the best Python tool to pivot tables with string data ; it seems it might be ...

Edit : I have found a way to do that : I apply ` ast.literal_eval ` to each line .

You can use ` join ` with ` rsuffix ` : #CODE

Apply function to each column that returns the value associated with the ` index ` of the ` min ` ` abs ` value like so : #CODE
It looks like the dates are index values , so the abs and min functions aren't applied to the index .

You can use diff : #CODE

Replace your function with this : #CODE

You could pass an argument to ` apply ` : #CODE

As the first way need ` join ` , so should way 2 be faster than way 1 ?

@USER , inplace parameters expects only a bool while ascending and by are allowing a list . There would be an error if both list were not having the same sizes .

Using apply to go through row by row and test whether the value is numeric of string is the quickest way separate them . #CODE

I have 2 time-series files I wanted to merge both . I can do the merging but the real issue is the Format of timestamp in both files .
When I tried to merge I'm getting below error ( of course it's expected one because of the timestamp issue ): #CODE

I want to normalize my both categorical and numeric values . #CODE
1 . how correctly replace categorical columns ? 2 how normalize numeric columns in this case properly ?

I have tried using a group-by / join as follows : #CODE
No it doesn't . DataFrame aggregations are performed using a logic similar to ` aggregateByKey ` . See DataFrame groupBy behaviour / optimization A slower part is ` join ` which requires sorting / shuffling . But it still doesn't require scan per group .
If this is an exact code you use it is slow because you don't provide a join expression . Because of that it simply performs a Cartesian product . So it is not only inefficient but also incorrect . You want something like this : #CODE
Thanks for the reply . I wasn't aware of the Cartesian product behavior in df.join() ; I had assumed incorrectly that the default behavior was to join on any columns that shares the same name . Adding an explicit equality test with an alias for the category column column from the table of means sped things up massively .
You're welcome . It is always useful to check execution extended execution plan ( ` df.explain ( extended=True )`) . The most common issues ignoring configuration are related to Cartesian products and even if you provide a join expression it may not be optimized .

I have tried expressing it in terms of join or merge but have failed so far . Is there any simple way to express that or will I have to use apply and create a new DataFrame ?
First , create a ` groupby ` object based on column ` A ` . Then create a new dataframe ` df2 ` which uses ` ix ` to index column ` B ` of each group based on the value ` n ` from column ` A ` . Set the index of this dataframe equal to the key values from the ` groupby ` ( i.e. the unique values from column ` A `) .
I'd also recommend using ` groupby ` but I think we can use ` pivot ` to simplify things . First , we create a new C column with the column labels we want to use , and then we call ` pivot ` : #CODE

INSERT SELECT * FROM ON DUPLICATE Results in Error 1136
I am attempting to insert / update a df into a mysql table . I'm receiving this error
This is the schema of the tables I am attempting to insert / update data . #CODE
Thanks , FYI : I just needed to specify the INSERT INTO fields as it still works with the asterisk in the SELECT part of the statement .

You could use ` apply ` with ` axis=1 ` to apply for rows with ` any ` method , if you have only one valid value and all other are ` NaN ` ( using @USER example ): #CODE

Replace the string ` white ` with a STR , HEX or RGB color to set up the background color of your choice .
Depending where you want to insert the table , maybe ` to_html ` , ` to_json ` or ` to_latex ` are better options than plotting with seaborn .

using pandas and numpy to parametrize stack overflow's number of users and reputation
I noticed that Stack Overflow's number of users and their reputation follows an interesting distribution . I created a pandas DF to see if I could create a parametric fit : #CODE
For the moment , let's drop ` pandas ` entirely . There aren't any advantages to using it here , and we'd quickly wind up converting the dataframe to other data structures anyway . ` pandas ` is great , it's just overkill for this situation .
Our data is basically an estimate of the complimentary cumulative distribution function ( CCDF ) , in the same sense that a histogram is an estimate of the probability distribution function ( PDF ) . We'll just need to normalize it by the total number of users in our sample to get an estimate of the CCDF . In this case , we can simply divide by the first element of ` num_users ` . Reputation can never be less than 1 , so 1 on the x-axis corresponds to a probability of 1 by definition . ( In other cases , we'd need to estimate this number . ) As an example : #CODE

So you want to find ` NaN ` in particular column and drop them ? In your example if you'll subset with `' a '` and `' b '` columns only ` 0 ` row will left ..
No I don't want to drop them . I want the resulting dataframe to contain only those rows where column ' a ' and column ' b ' contain NaN .
You could do that with ` isnull ` and ` any ` methods : #CODE
If you want to subset your dataframe you could use mask with your columns and apply it to the whole dataframe : #CODE

Pandas median over grouped by binned data
I'd like to calculate for each user the median of the scores .
and then use groupBy and apply to calculate the median somehow ?
I believe you may be interested in weighted median #URL
I believe you need weighted median . I used function ` weighted_median ` from here , you can also try ` wquantile `' s ` weighted.median ` , but it interpolates in a bit different way so you may achieve nonexpected results ): #CODE

and I need to aggregate this data ( like a pivot table ) .
As suggested , look into the pandas package , put the data in a DataFrame , it can do pivot tables , or just use the groupby function . #URL
and apply a function to the groups .
Ex . 1 Find the number of trips each team went on . ` team ` is the grouper , and we apply the function ` count() ` on column ` [ ' trips ']` . #CODE
Ex . 2 ( multiple columns ) : Find the total time each player on a team spent traveling . We use 2 columns ` [ ' team ' , ' player ']` as the grouper , and apply the function ` sum() ` on column ` [ ' time ']` . #CODE

Python - how select / drop elements in data frames that have multiple ( 2 ) indices
How do I select all non CASH related positions ( there can be many ) - i.e. what is the command to drop the cash positions in this df ?
seems like it was just use the drop statement differently #CODE

Append data in realtime to an empty pandas DataFrame
How to append data to a DataFrame like this ?
Added an example for that . To append a single row using an index .
See update . Still same concept but instead of appending to lists in loop and then a bulk append to df out of loop , each iteration appends to df in loop .

As you can see the two methods that I'm using is " rename " and " insert " . Any help would be much appreciated . Thanks !

You could do ` df.loc [ time_before : time_after ]` using timestamps before and after your line starts so one can see what type of data you have at these points in time . One would assume these are ` np.nan ` which may cause ` matplolib ` to interpolate hence the ` .dropna ` suggestion , but perhaps you are getting a constant for some other reason .
I think you are missing the step in line 3 above to convert the remaining ` timestamps ` to ` strings ` , otherwise ` pandas ` will interpolate to create an even-space series .

Merge pandas DataFrame with MultiIndex
I also tried to merge the frames like so : #CODE

IIUC you need groupby by ` Feed ` from multiindex and apply ` pct_change ` . Then you can use subset of ` df3 ` , where column ` Rate_Return ` is ` notnull ` #CODE

Insert a NumPy rec.array to MongoDB using PyMongo
In an other question some people are trying to insert a Pandas DataFrame into MongoDB using Python internal structures ( ` dict ` , ` list `)
Insert a Pandas Dataframe into mongodb using PyMongo
I wonder if we can't insert instead a NumPy ` rec.array ` ( ` numpy.recarray `) to MongoDB using PyMongo .
but I faced some errors at insert #CODE

But this feels wrong and clunky as I have to drop the column levels ( which removes useful structure from my data ) and I have to manually rename the columns . Is there a better way ?

I tried to use the shift method like this #CODE

` df ` is just a generic term for dataframe . You can replace it with ` main_frame ` in your case . Use ` iloc ` ( index location ) instead of ` input1 ` and ` target_var ` .

You need to pivot your data . Here is an example . #CODE
You need to pivot the data frame . See

I want to find the total number of people in the data set , the number of people who purchased and the total number of orders and total revenue amount . I know how to do it via ` SQL ` using ` left join ` and aggregate functions but I do not know how to replicate this using ` Python ` / ` pandas ` .
So one awful solution is replace ` NaN ` to some string and after ` agg ` replace back to ` NaN ` : #CODE

Replace all commas with an empty space in csv error
I'm trying to replace all the commas with an empty space in a single column in a csv . I tried the following method here : #CODE

We get 3 rows but only 2 columns . In the docs I find that different from standard python , label based slicing in pandas is inclusive . Does this apply here and is it inclusive for rows but not for columns then ?
, ix method is primarily label based with fallback to indexing ... from docs online ...
For guaranteed position based slicing , you can use the ` loc ` indexer .
The ` ix ` indexer you are using , is more flexible ( not strict in type of indexing ) . It is primarily label based , but will fall back to position based ( when the labels are not found and you are using integers ) . This is the case in your example for the columns . For this reason , it is recommended to always use ` loc ` / ` iloc ` instead of ` ix ` ( unless you need mixed label / position based indexing ) .

` seasonal_decompose() ` requires a ` freq ` that is either provided as part of the ` DateTimeIndex ` meta information , can be inferred by ` pandas.Index.inferred_freq ` or else by the user as an ` integer ` that gives the number of periods per cycle . e.g. , 12 for monthly ( from ` docstring ` for ` seasonal_mean `) :

I'm trying to plot a simple line plot and insert a background image to a plot .

Replace values by running maximum values in pandas dataframe
Here , I am grouping by ` id ` and for each ` id ` , the df is sorted by ` time ` . Now , I want to replace the values in ` a ` and ` b ` by the maximum value seen thus far . I guess I can apply a rolling max on each group but is there a better way to do this ?
You can ` apply ` custom function , where find index of first 1 by ` idxmax ` and set rows to the end of group to ` 1 ` : #CODE

Merge / Concat Issue
Is there a way to just concat the comments onto each other with some sort of character ? I know we are out of the realm of regular SQL and Panda .
I think Merge would work well for your case . #CODE
This sounds like you want to do a ' left ' merge ? See updated answer . You can specify to do ` inner ` ( default ) , ` right ` , ` left ` , or ` outer ` join , following ` sql ` logic .

My knowledge isn't that great of Pandas ( yet ) , but I'm guessing it's an " apply " or an agg() function but so far , syntactically , I'm banging my head from the syntax errors , but I appreciate any pointers in the right direction . .. JW
Also a similar approach is to use a pivot table with margins . #CODE

Merge rows with same id and time in pandas

4 . Join the result of above steps with original data frame ! #CODE

Use of loc to update a dataframe python pandas
As you could see from the warning you should use ` loc [ row_index , col_index ]` . When you subsetting your data you get index values . You just need to pass for row_index and then with comma col_name : #CODE

My suggestion is merge the X data to Y data on axis 1 , calculate and rebuild .
This was my original approach , but you run into an issue where , for example , ` name4_Y = NaN ` . Once you unstack , column ' X ' will also be NaN . I believe the end result should have ` name4_Y = NaN ` but should not have ` name4_X = NaN ` .
So what should ` name4_X ` * be * in your scenario ? If you mean it shouldn't exist at all you could always drop the rows where ` X ` is ` Nan ` .

This question is not really suited for Stack Overflow . Maybe you should try to implement your own algorithm ( maybe following [ this ] ( #URL ) blogpost ) and post it for feedback on [ Code Review ] ( #URL ) .

But this takes quite a while given the time complexity , running at around 20s for 500 points and I have a much longer list . This has me looking at vectorization , and I've come across ` numpy.vectorize ` ( ( docs ) , but can't figure out how to apply it in this context .

Dataframe append a row
How do I append only one row to a new dataframe ? I have only seen examples appending a whole dataframe . I am using iterrows to so I have the index of the row I want to append to a new dataframe . #CODE
Inside this if statement I want to append that row .
Do you wan to append one of the rows from your DataFrame to the same DataFrame ?
If you wan to append a row to the DataFrame you have to assign it to a variable first . For example if you want to append the first row in your DataFrame to the same DataFrame : #CODE
then , you can append a specific row in a new DataFrame . #CODE

You could groupby aggregate to list and join the list as below . #CODE

The main part of the solution is the pivot table , where we fill missing values with ` N ` . #CODE
I REALLY like this pivot table solution . Elegant and FAST !

Strip timezone info in pandas
doesn't ` df [ ' datetime '] .dt .tz_localize ( None )` work ? replace ` datetime ` with what ever your column name is
Thanks . It doesn t work . The format of the date is " 2015-12-01 00:00 : 00-06 : 00 " . I used " to_datetime " to convert the original date format to a datetime object , in order to apply " tz_localize " to convert to another time zone . It seems tz_localize adds that offset and I have not found how to get rid of it .
Maybe help strip last 6 chars : #CODE

I am not exactly sure how to go about this . One of the ideas is use itterrows() and apply harvesine() function , if rows ' sequence ' parameter is not 0 and row's ' track_id ' is equal to previous row's ' track_id '
[ EDIT ] I figured there is no need to check if ' track_id ' of row and previous row is the same , since the haversine() function is applied to two rows only , and when sequence = 0 , that row's distance == 0 , which means that the track_id has changed . So , basically , apply haversine() function to all rows whose ' sequence ' ! = 0 , ie haversine ( previous_row.lng , previous_row.lat , current_row.lng , current_row.lat ) . Still need help with that though

I normally populate new columns using " apply , axis = 1 " so I would really appreciate any solution based on that . I found that " apply " works fine when for each row , computation is done across columns using values at the same row level . However , I don't know how an " apply " function can involve different rows , which is what this problem requires . the only exception I have seen so far is " diff " , which is not useful here .
By taking a pivot of the data with dates as your index and stores as your columns , you can simply take a rolling average . You then need to stack the stores to get the data back into the correct shape .
Here is some sample output of the original data prior to the final stack : #CODE
Assuming the above is named ` df ` , you can then merge it back into your original data as follows : #CODE
In our example , the NaN rows of the pivot table do not get merged back into ` df ` because we did a left join and all the rows in ` df ` have one or more Customers .
I just tested it on the 5.66mb training data file . Less than half a second . You then need to merge it back it .
@USER : using min_period to fill in NaN is good if e.g. one wants to remove seasonality in data . I , however , want to use the rolling mean to create a feature to feed into a ML model . I can't use rolling_mean for store-dates not in the initial dataset , which min_period does . Therefore , I loop over the 1115 stores and apply your solution ( without min_periods ) , which is still much faster than my initial attempt . Thanks for your help

It's pretty trivial to truncate the table before I write the data to it , but I feel like there should be a more elegant solution to this problem .

You can try ` loc ` : #CODE

Is this the best way to do this ? Is there a way to do it without having to use the join ?

I have a data frame in pandas which includes number of days since an event occurred . I want to create a new column that calculates the date of the event by subtracting the number of days from the current date . Every time I attempt to apply ` pd.offsets.Day ` or ` pd.Timedelta ` I get an error stating that Series are an unsupported type . This also occurs when I use ` apply ` . When I use ` map ` I receive a runtime error saying " maximum recursion depth exceeded while calling a Python object " .

What is a proper idiom in pandas for creating a dataframes from the output of a apply function on a df ?
One of the operations I need to conduct is grabbing the latest feed entries --- the feed urls exist in a column in a data frame . Once I've done the apply I get feed objects back : #CODE
So , now I'm stuck with the feed entries in the " entries " column , I'd like to create a two new data frames in one apply method , and concatenate the two frames immediately .

How can I create pandas dataframe with column dtype being bool ( or int for that matter ) with support for Nan / missing values .

So ` [ ai , bi , ci ]` moves to a single ` row ` while keeping [ Misc , Year ] . I am working with thousands of 20,000 row datasets so performance is a big issue . I currently am looping per row to separate them , but was hoping there is a better python function for flattening . I have seen panda's ' melt ' function but it seems to only work if there is a single group .
This is not a typical application for reshape / melt type functions , so you're probably going to have to roll your own . Here's a solution that should be relatively performant provided ` ( # groups ) *n ` is not too big :
Build dictionary to rename ` columns ` for proper application of ` concat ` : #CODE
If you want to eke out some more performance , you're going to want to do this concat in numpy and then repeat the index ( though I'm not convinced it's worth the small gain that will give you ) .
Thanks Andy ! What if I want to preserve the ' ith ' column index ? Is it possible to create a brand new column , say ' N_index ' , within the concat function and assign it the value of str ( i ) ?
@USER IIUC I would ( lazily ) do this with ` .sort_index() ` , then ` .reset_index() ` . In some sense the index you wanted wasn't really coming from anywhere and I don't think it can be done during concat .

But I'm not sure how this will help me . I took a look at the pandas Dataframe.to_dict() but I don't think the above code reads into a dataframe ( or , if it does , I don't understand the documentation well enough ) . It looks like it'll only store one value per key at a time . Another thread I was reading says it's possible to store more than one value per key , though ( using .append() ) but I don't know how to apply it to this situation .
eval ( ' ticker_ %s ' % fname = {} )
Yes , it would be a really bad , unexpected and unusual practice , and yes , you can use eval to do this ( I guess ) , and no , it would not affect performance or memory consumption too much

Now we can use numpy's argmax : #CODE

Pandas : apply returns list
I have the following function that I want to apply to each group : #CODE
How do I make the results of the apply operation the values of the " mean_to_date " column ? That is , the mean_to_date for player 200 , season 21999 would be 0 and 10 , then for player 200 , season 21200 it would be 0 , 10 , and 15 , and so forth . Note that the mean_to_date value represents the mean prior to the game , so before the 1st game it is zero , and before the second game it is the total from the first game .
you're getting lists back because your function ` previous_mean ` , when fed a dataframe , returns a list -- it has nothing to do with ` apply ` .
You said in another comment that you want one mean per row , therefore the function you apply should return a single value .
IIUC you can use ` expanding_mean ` , shift data by ` shift ` to ` 1 ` , fill ` NaN ` to ` 0 ` by ` fillna ` and return column ` mean_to_date ` : #CODE

Use ` loc ` instead of ` ix ` : #CODE
For ix it's strange why it's not working because from docs :
But they recommended to use ` loc ` in such cases to be more explicit .
@USER ` .ix ` only works with labels when the index is integer-based . That is indeed confusing and the source of the motivation that created ` loc ` and ` iloc ` .

How can I drop the date from this output ?
You could use ` dropna ` of ` fillna ` methods to drop or fill with values what you like

Apply the function to the groups : #CODE

comparing dtyped [ float64 ] array with a scalar of type [ bool ] in Pandas DataFrame
You can transform either column ' a ' or ' b ' so they are both either float64 or bool . However , an easier solution that preserves the data type of your features is this : #CODE

How to apply a function ( BigramCollocationFinder ) to Pandas DataFrame
I want to adapt this function to my Pandas Dataframe . I am aware of the apply function for Pandas Dataframes , but can't manage to get it work .
If you want to apply ` BigramCollocationFinder.from_words() ` to each ` value ` in the ` Body ` ` column , you'd have to do : #CODE
In essence , ` apply ` allows you to loop through the ` rows ` and provide the corresponding ` value ` of the ` Body ` ` column ` to the applied function .

Pandas I want to drop rows with NaT based on another column's value
You can try ` notnull ` : #CODE
` notnull ` is more readable than a negated ` isnull ` IMO

hmmm , is possible replace ` -- ` to ` AA ` and then compare ?

` Apply ` function over rows and put the result in a new column . #CODE

You could also do apply : #CODE

It relates to a big data analytics course project and demonstrating the use of Spark / Bluemix and map / reduce is a requirement . Even though the file starts out on a local file system - I have to process it in Spark / Ipython .

Tell ` value_counts ` not to drop NaN values by setting ` dropna=False ` ( added in 0.14.1 ): #CODE
No problem ! Yep , that works . In fact , you could just write ` dfv.sum() ` to count all the values . Or even more efficiently , just check ` len ( dfd )` .

You can use ` diff ` , ` astype ` and ` cumsum ` : #CODE
Test ` len ( df ) = 10 ` : #CODE
Test ` len ( df ) = 10000 ` : #CODE

Use ` reset_index() ` to drop the team index column and make the player index column as part of the dataframe . #CODE
When printing we have to map the integers to a string , and use the end parameter of the print function to print a semicolon instead of printing a new line at the end . #CODE

I'm having a DataFrame with a date column . How can I map each date ` d ` to the start day of the week containing ` d ` ?

Do I need to find the difference between each random number generated and store only the sets where the abs diff is 0.5 ? Can someone explain how can I do that in pandas ?
Given the date_range , r , that data frame can be created as : ` DataFrame ( index=r , data=randn ( len ( r )) , columns =[ ' Random Number Generated '])` .
Thanks :) Dithal . Can you please clarify me if you have any idea on the abs different thing
The y-axis values of 0 to 18 are the bin counts . The bins are on the x-axis . To see what's going on , try ` hist ( bins=1 )` , you should get 72 on the y-axis , the length of the data .
Thanks for the explanation stefen :) I tried the same but when I tried to create the bar : df_new = diff [ abs ( diff [ ' Random Number Generated ']) < 0.5 ] , df_new.diff() .hist ( bins =10 ) . I am getting different graph
That's because you are reducing the number of valid observations to around 20 , so with ` bins=10 ` , that is 2 values per bin on average , you can get all sorts of shapes . My chart above was for the 72 values , ie the full series , before dropping the values with ` abs ( diff < 0.5 )` .

Dataframe : shift expanding mean with groupby
Dear @USER thank you for your answer , this works perfectly ! Before I had : ` df2 [ ' homewin_at_home '] = df2.groupby ( ' home ') [ ' hw '] .apply ( pd.expanding_mean ) .shift ( 1 )` which didn't shift the calculation . I know the purpose of lambda so I think my error was putting the ` shift() ` in the wrong place ? Am I correct ?

As long as your index is a timestamp ( as it currently is ) , you can just use resample : #CODE

IIUC you do that with ` interpolate ` method with ` method ` parameter equals to ` linear ` to do linear interpolation or ` nearest ` if you'd like to fill gaps with the closes values for your resampled dataframe : #CODE
For your resampling , ` resample ` method works only valid with DatetimeIndex , TimedeltaIndex or PeriodIndex . So you could convert your column to ` timedelta ` then set it as index , resample , ` reset_index ` to get back to your original dataframe . Also you'll need to call ` dt.total_seconds ` to convert from #URL to only seconds as your original data : #CODE
Maybe you could merge and then fill na's , for instance :
Merge , sort , Forward-fill calculate average : #CODE
I've been thinking the same thing as well . But I think if I do this , it's better do a rolling mean or median ?

Additionally you don't need to name variables , what you want to do is just append each df to a list and then at the end concat all the dfs in the list
Consider using merge with ` right_index ` and ` left_index ` arguments : #CODE
Thank you . This works . But , here are few things that i got hung at . In reality , I wanted to skip columns 2 and 3 from the output . So i wrote : result = pd.read_csv ( " 1isoforms.fpkm_tracking.txt " , sep= " \t " , index_col =[ 0 , 3 ] , usecols =[ 0 , 3 , 7 ]) - However , that gave an error regarding ' list index out of range ' which was surprising to me as when i put usecols 0 , 1 , 2 , 3 , 7 , it works . Second , I assume the {} and format ( i ) means that in every loop , it replaces {} with i , correct ? When you set right and left index as TRUE , does this mean that it will only merge if all the indexes are the same ?

You can use ` loc ` and ` isnull ` : #CODE

If not , what is the closest plot that I can get , to plot the min , max , mean , median , std-dev etc . I know I can plot them using line chart , but I need the boxplots to be grouped / clustered .

Inputting 2 data frames then trying to use data merge or join : #CODE
Is there any suggested method ? I know how to merge to data frames if it has same # of rows and index , but i couldnt do it if i only want to do it using first file as a standard index . I know how to do it in R using the merge function then by.x and by.y , but R messes up all my header names ( the ones up are just an example ) . So it is best to do it in python .
You can use ` join ` to merge on the indices : #CODE
For some reason , join is giving me error : ValueError : columns overlap but no suffix specified : Index ([ u'ID '] , dtype= ' object ') . In regards to this option though , it looks like outer ensures keeping all samples , while left will use the df1 ( i.e. the one on the left ? ) as reference , Is this what's supposed to do ? Do you know how to come around the error ?
@USER if ID is the index then join works , if it's not you need to use merge .
Reading your files with ` sep= ' \t '` didn't parse properly for , but ` sep= ' \s+ '` did for your sample lines , and then the standard ` merge ` gives your desired result : #CODE
You can of course also move ' ID ' to the ` index ` and use ` .join() ` , ` .concat() ` , or ` .merge ( left_index=True , right_index=True )` with the appropriate settings for ` left ` merge for each .
If you are trying to use 1:33 as a ` label ` as opposed to a ` slice ` including ` integer ` position from ` 1 ` to ` 33 ` , than you should probably use it as a ` string ` - `' 1:33 ` . See the documentation which is quite specific as to allowed inputs for the three basic selection methods : ` loc ` , ` iloc ` , and ` ix ` .

For my data set ( with tens of thousands of rows ) , this is somewhat slow , and I understand that loops should be avoided when possible when using pandas dataframes . I feel like the pandas ` apply ` function may be able to do what I need , but I'm at a loss as to how to implement it .

What I would like to do is replace whatever is not a date , with a date based on a variable that represents the last_update + 1 / 2 the update interval , so the items are not filtered out by later functions .
Happy to help , and welcome to Stack Overflow . If this answer or any other one solved your issue , please mark it as accepted . If not please let us know , how to further help resolve your question .
I realized the ' None ' was actually a None object , which just let me do a bit of alteration to your suggestion ( if x ! = None ) , then the other replace solution . Thank you again for the guidance .
` apply ` calls ` tuple_to_timestamp ` for each row of ` df [ ' orig ']` .

The first operation can be done with an ` apply ` function returning a series ( see the accepted answer to this question ) , followed by a horizontal ` concat ` operation ( i.e. , with ` axis=1 `) .
The second operation is simply pandas's own ` stack ` .

You could use ` groupby / cumcount ` to assign column numbers and then call ` pivot ` : #CODE
Using ` reshape ` is quicker than calling ` groupby / cumcount ` and ` pivot ` , but it

bool operator in for Timestamp in Series does not work

I have a large Pandas dataframe in which one column is ( unordered ) datetimes from a known period ( the year 2013 ) . I need an efficient way to convert these datetimes to indices , where each index = # hours since start_time ( ' 2013-1-1 00 )' . There are duplicate times , which should map to duplicate indices .
loc can take a list or array of labels to look up : #CODE

I have a pandas data frame ( result ) df with n ( variable ) columns that I generated using the merge of two other data frames : #CODE

Values missing : Overlaying points on boxplot subplots from a pandas dataframe
I have two dataframes : df1 with 5000 rows and 12 columns and df2 with a single row and identical 12 columns as df1 . I would like to plot each column on a separate subplot as a boxplot with values from df1 , and overlay it with scatter plots ( just one value per boxplot ) from df2 .
UPDATE : I tried using a transposed df2 with a numerical index . The following code does not give me any error but only one value on the second boxplot is visible .. I can't see the rest #CODE
Getting boxplot subplots only is easy using #CODE

use ` diff ` with ` fillna ` : #CODE

This can be done by adapting the ` groupby ` , ` shift ` and ` cumsum ` trick described in the Pandas Cookbook , Grouping like Python s itertools.groupby . The main change is in dividing by the length of the period - 1 and then using the ` ceil ` function to round up to the next integer . #CODE

I have a groupby object I apply expanding mean to . However I want that calculation over another series / group at the same time . Here is my code : #CODE
Next , stack so you can follow each team : #CODE
Then , define what's a ' win ' , calculate overall record and apply ` expanding_mean ` : #CODE
Since you have references for both games and teams , you could ` merge ` and ` filter ` to get your preferred layout .

and you are aiming to have the various ` Distance ` columns form a single ` index ` while the respective ` Force ` ` columns remain in place . You could ` stack ` the frame like so : #CODE

I have already asked a similar question here but couldn't manage to apply the solution for my problem with two columns .
You can extract from columns ` values ` and ` datetime ` new ` Series ` and then merge them with original dataframe ` df ` by ` concat ` : #CODE
You can use a nested list comprehension together with ` concat ` to extract the core part of your dataframe that repeats each row the desired number of times ( based on the length of its ` values ` list .

You could do that with ` isin ` method to subset your original dataframe to smaller one . Then you could do your calculations with that subset dataframe and then using ` loc ` you could assign it to the original dataframe : #CODE
You could use ` dropna ` method to drop all rows which contain ` NaN ` in any of the columns . Or you could use ` fillna ` to fill ` NaN ` value to what you want . You could also check ` NaNs ` with ` notnull() ` and ` isnull ` methods .

Stack columns in pandas dataframe to achieve record format
How would I stack the table such that I have one column for country name , one column for year , and one column for gdp figures ? This is my code so far : #CODE

