
I	O
think	O
that	O
I	O
understand	O
what's	O
going	O
on	O
:	O
create	O
a	O
frequency	O
table	O
of	O
ALL	O
words	O
.	O

Then	O
,	O
from	O
that	O
tablemake	O
sums	O
.	O

First	O
the	O
neuter	O
cases	O
,	O
then	O
nonneuter	O
.	O

After	O
each	O
operation	O
,	O
drop	Y
all	O
relevant	O
columns	O
,	O
then	O
finally	O
count	O
all	O
remaining	O
columns	O
.	O

Correct	O
?	O

One	O
question	O
though	O
.	O

What	O
does	O
`	O
axis=1	O
`	O
do	O
?	O

Also	O
,	O
I	O
quickly	O
tried	O
this	O
in	O
Python	O
3.4.3	O
and	O
I	O
got	O
the	O
error	O
that	O
freqDf	O
isn't	O
defined	O
.	O

Should	O
I	O
first	O
create	O
a	O
new	O
table	O
named	O
freqDf	O
?	O

There	O
is	O
no	O
KeyError	O
because	O
it	O
is	O
testing	O
whether	O
words	O
in	O
`	O
precedingWord	O
`	O
are	O
in	O
a	O
given	O
list	O
.	O

`	O
df.precedingWord.isin	O
(	O
neuter	O
)`	O
is	O
just	O
a	O
Series	O
of	O
True	O
or	O
False	O
(	O
results	O
of	O
the	O
previous	O
test	O
`	O
isin	Y
`)	O
,	O
and	O
pandas	O
will	O
just	O
access	O
True	O
indexes	O
with	O
`	O
loc	Y
`	O

I	O
have	O
tried	O
a	O
some	O
join	Y
/	O
merge	Y
ideas	O
but	O
can't	O
seem	O
to	O
get	O
it	O
to	O
work	O
.	O

Just	O
`	O
concat	Y
`	O
them	O
and	O
pass	O
param	O
`	O
axis=1	O
`	O
:	O
#CODE	O

Or	O
`	O
merge	Y
`	O
on	O
'	O
Symbol	O
'	O
column	O
:	O
#CODE	O

Pandas	O
:	O
join	Y
with	O
outer	O
product	O

How	O
to	O
join	Y
/	O
multiply	O
the	O
DataFrames	O
`	O
areas	O
`	O
and	O
`	O
demand	O
`	O
together	O
in	O
a	O
decent	O
way	O
?	O

Now	O
`	O
apply	Y
`	O
needs	O
to	O
return	O
a	O
`	O
Series	Y
`	O
,	O
not	O
a	O
`	O
DataFrame	Y
`	O
.	O

One	O
way	O
to	O
turn	O
a	O
`	O
DataFrame	Y
`	O
into	O
a	O
`	O
Series	Y
`	O
is	O
to	O
use	O
`	O
stack	Y
`	O
.	O

Look	O
at	O
what	O
happens	O
if	O
we	O

`	O
stack	Y
`	O
this	O
DataFrame	Y
.	O

The	O
columns	O
become	O
a	O
new	O
level	O
of	O
the	O
index	O
:	O
#CODE	O

Now	O
we	O
want	O
the	O
first	O
level	O
of	O
the	O
index	O
to	O
become	O
columns	O
.	O

This	O
can	O
be	O
done	O
with	O
`	O
unstack	Y
`	O
:	O
#CODE	O

`	O
del	O
`	O
+	O
`	O
pivot	Y
`	O
turns	O
out	O
to	O
be	O
faster	O
than	O
`	O
pivot_table	Y
`	O
in	O
this	O
case	O
.	O

Maybe	O
the	O
reason	O
`	O
pivot	Y
`	O
exists	O
is	O
because	O
it	O
is	O
faster	O
than	O
`	O
pivot_table	Y
`	O
for	O
those	O
cases	O
where	O
it	O
is	O
applicable	O
(	O
such	O
as	O
when	O
you	O
don't	O
need	O
aggregation	O
)	O
.	O

Wow	O
,	O
awesome	O
walkthrough	O
!	O

`	O
apply	Y
`	O
is	O
now	O
among	O
my	O
top	O
5	O
functions	O
to	O
always	O
remember	O
.	O

Concerning	O
the	O
`	O
pivot_table	Y
`	O
solution	O
:	O
At	O
which	O
point	O
am	O
I	O
supposed	O
to	O
enter	O
the	O
line	O
?	O

No	O
matter	O
when	O
in	O
my	O
attempt	O
above	O
,	O
I	O
always	O
get	O
`	O
no	O
item	O
named	O
Edge	O
`	O
.	O

Or	O
pass	O
`	O
axis=0	O
`	O
to	O
`	O
loc	Y
`	O
:	O
#CODE	O

I've	O
got	O
2	O
pandas	O
dataframes	O
,	O
each	O
of	O
them	O
has	O
an	O
index	O
with	O
dtype	Y
`	O
object	O
`	O
,	O
and	O
in	O
both	O
of	O
them	O
I	O
can	O
see	O
the	O
value	O
`	O
533	O
`	O
.	O

However	O
,	O
when	O
I	O
join	Y
them	O
the	O
result	O
is	O
empty	O
,	O
as	O
one	O
of	O
them	O
is	O
the	O
number	O
`	O
533	O
`	O
and	O
the	O
other	O
is	O
a	O
string	O
`"	O
533	O
"`	O
.	O

Ideally	O
I	O
would	O
like	O
something	O
like	O
`	O
apply_chunk()	O
`	O
which	O
is	O
the	O
same	O
as	O
apply	Y
but	O
only	O
works	O
on	O
a	O
piece	O
of	O
the	O
dataframe	Y
.	O

I	O
thought	O
`	O
dask	O
`	O
might	O
be	O
an	O
option	O
for	O
this	O
,	O
but	O
`	O
dask	O
`	O
dataframes	O
seemed	O
to	O
have	O
other	O
issues	O
when	O
I	O
used	O
them	O
.	O

This	O
has	O
to	O
be	O
a	O
common	O
problem	O
though	O
,	O
is	O
there	O
a	O
design	O
pattern	O
I	O
should	O
be	O
using	O
for	O
adding	O
columns	O
to	O
large	O
pandas	O
dataframes	O
?	O

whats	O
about	O
using	O
the	O
apply	Y
method	O
?	O

However	O
,	O
if	O
your	O
operation	O
is	O
highly	O
custom	O
(	O
as	O
it	O
appears	O
to	O
be	O
)	O
and	O
if	O
Python	O
iterators	O
are	O
fast	O
enough	O
(	O
as	O
they	O
seem	O
to	O
be	O
)	O
then	O
you	O
might	O
just	O
want	O
to	O
stick	O
with	O
that	O
.	O

Anytime	O
you	O
find	O
yourself	O
using	O
`	O
apply	Y
`	O
or	O
`	O
iloc	Y
`	O
in	O
a	O
loop	O
it's	O
likely	O
that	O
Pandas	O
is	O
operating	O
much	O
slower	O
than	O
is	O
optimal	O
.	O

Convert	O
freq	Y
string	O
to	O
DateOffset	O
in	O
pandas	O

In	O
pandas	O
documentation	O
one	O
can	O
read	O
"	O
Under	O
the	O
hood	O
,	O
these	O
frequency	O
strings	O
are	O
being	O
translated	O
into	O
an	O
instance	O
of	O
pandas	O
DateOffset	O
"	O
when	O
speaking	O
of	O
freq	Y
string	O
such	O
as	O
"	O
W	O
"	O
or	O
"	O
W-SUN	O
"	O
.	O

stack	Y
/	O
unstack	Y
/	O
pivot	Y
dataframe	Y
on	O
python	O
/	O
pandas	O

yes	O
,	O
`	O
isnull	Y
`	O
will	O
create	O
a	O
boolean	O
series	O
,	O
`	O
all	Y
`	O
returns	O
`	O
True	O
`	O
if	O
all	O
are	O
`	O
True	O
`	O

Then	O
merge	Y
the	O
sub-tables	O
back	O
together	O
in	O
a	O
way	O
that	O
replaces	O
NaN	O
values	O
when	O
there	O
is	O
data	O
in	O
one	O
of	O
the	O
tables	O
.	O

I	O
tried	O
:	O
#CODE	O

How	O
can	O
I	O
retrieve	O
specific	O
columns	O
from	O
a	O
pandas	O
HDFStore	O
?	O

I	O
regularly	O
work	O
with	O
very	O
large	O
data	O
sets	O
that	O
are	O
too	O
big	O
to	O
manipulate	O
in	O
memory	O
.	O

I	O
would	O
like	O
to	O
read	O
in	O
a	O
csv	O
file	O
iteratively	O
,	O
append	Y
each	O
chunk	O
into	O
HDFStore	O
object	O
,	O
and	O
then	O
work	O
with	O
subsets	O
of	O
the	O
data	O
.	O

I	O
have	O
read	O
in	O
a	O
simple	O
csv	O
file	O
and	O
loaded	O
it	O
into	O
an	O
HDFStore	O
with	O
the	O
following	O
code	O
:	O
#CODE	O

If	O
you	O
replace	O
that	O
line	O
with	O
:	O

I	O
wanted	O
to	O
merge	Y
these	O
files	O
so	O
that	O
i	O
have	O
something	O
like	O
this	O
#CODE	O

If	O
there	O
are	O
two	O
Bact5	O
rows	O
in	O
file1	O
,	O
three	O
Bact5	O
rows	O
in	O
file2	O
,	O
how	O
many	O
Bact5	O
rows	O
do	O
you	O
want	O
in	O
the	O
output	O
?	O

If	O
it's	O
six	O
,	O
then	O
you	O
can	O
use	O
join	Y
method	O
by	O
@USER	O
Hayden	O
.	O

Then	O
you	O
can	O
simply	O
`	O
join	Y
`	O
them	O
:	O
#CODE	O

@USER	O
when	O
you	O
do	O
a	O
join	Y
with	O
2x2	O
duplicates	O
you	O
get	O
4	O
in	O
the	O
joined	O
DataFrame	Y
.	O

It's	O
unclear	O
how	O
pandas	O
should	O
join	Y
in	O
this	O
case	O
,	O
so	O
you	O
need	O
to	O
be	O
more	O
explicit	O
to	O
it	O
(	O
and	O
tell	O
it	O
what	O
do	O
you	O
want	O
)	O
.	O

On	O
the	O
similar	O
note	O
,	O
is	O
there	O
a	O
way	O
to	O
merge	Y
values	O
based	O
on	O
index	O
.	O

For	O
example	O
,	O
instead	O
of	O
listing	O
Bact5	O
in	O
two	O
rows	O
,	O
can	O
we	O
merge	Y
its	O
value	O
corresponding	O
to	O
file2	O
in	O
one	O
row	O
separated	O
by	O
a	O
delimeter	O
?	O

Pandas	O
dataframe	Y
insert	Y
rows	O

I	O
want	O
to	O
insert	Y
rows	O
in	O
DF	O
and	O
modify	O
its	O
related	O
values	O
:	O

The	O
code	O
can	O
only	O
append	Y
rows	O
but	O
how	O
to	O
modify	O
its	O
values	O
in	O
a	O
faster	O
way	O
?	O

I	O
want	O
to	O
use	O
a	O
function	O
from	O
an	O
add-in	O
in	O
excel	O
and	O
apply	Y
it	O
to	O
some	O
data	O
i	O
have	O
simulated	O
in	O
python	O
.	O

Is	O
there	O
any	O
modules	O
that	O
can	O
achieve	O
this	O
?	O

Unfortunately	O
,	O
this	O
only	O
runs	O
a	O
macro	O
.	O

I	O
need	O
to	O
be	O
able	O
to	O
call	O
the	O
add-in	O
and	O
apply	Y
my	O
data	O
indexes	O
there	O
...	O
something	O
along	O
these	O
lines	O
:	O
=	O
add-in_name	O
(	O
data_range1	O
,	O
data_range2	O
,	O
"	O
GGCV	O
")	O

Usually	O
when	O
I	O
want	O
to	O
put	O
content	O
of	O
a	O
file	O
in	O
a	O
data	O
frame	O
I	O
make	O
a	O
loop	O
over	O
the	O
lines	O
of	O
the	O
file	O
,	O
split	O
the	O
lines	O
into	O
the	O
fields	O
and	O
use	O
this	O
values	O
to	O
specify	O
a	O
dictionary	O
.	O

After	O
reading	O
one	O
line	O
I	O
append	Y
the	O
dictionary	O
to	O
a	O
list	O
(	O
so	O
,	O
the	O
number	O
of	O
dictionaries	O
in	O
the	O
list	O
is	O
equal	O
to	O
the	O
number	O
of	O
lines	O
in	O
the	O
file	O
)	O
.	O

Then	O
I	O
use	O
the	O
list	O
of	O
dictionaries	O
to	O
construct	O
a	O
data	O
frame	O
.	O

I	O
can	O
easily	O
do	O
this	O
iteratively	O
with	O
loops	O
,	O
but	O
I've	O
read	O
that	O
you're	O
supposed	O
to	O
slice	O
/	O
merge	Y
/	O
join	Y
data	O
frames	O
holistically	O
,	O
so	O
I'm	O
trying	O
to	O
see	O
if	O
I	O
can	O
find	O
a	O
better	O
way	O
of	O
doing	O
this	O
.	O

A	O
join	Y
will	O
give	O
me	O
all	O
the	O
stuff	O
that	O
matches	O
,	O
but	O
that's	O
not	O
exactly	O
what	O
I'm	O
looking	O
for	O
,	O
since	O
I	O
need	O
a	O
resulting	O
dataframe	Y
for	O
each	O
key	O
(	O
i.e.	O
for	O
every	O
row	O
)	O
in	O
A	O
.	O

Ok	O
,	O
from	O
what	O
I	O
understand	O
,	O
the	O
problem	O
at	O
its	O
most	O
simple	O
is	O
that	O
you	O
have	O
a	O
`	O
pd.Series	Y
`	O
of	O
values	O
(	O
i.e.	O
`	O
a	O
[	O
"	O
key	O
"]`	O
,	O
which	O
let's	O
just	O
call	O
`	O
keys	O
`)	O
,	O
which	O
correspond	O
to	O
the	O
rows	O
of	O
a	O
`	O
pd.DataFrame	Y
`	O
(	O
the	O
df	O
called	O
`	O
b	O
`)	O
,	O
such	O
that	O
`	O
set	O
(	O
b	O
[	O
"	O
key	O
"])	O
.issuperset	O
(	O
set	O
(	O
keys	O
))`	O
.	O

You	O
then	O
want	O
to	O
apply	Y
some	O
function	O
to	O
each	O
group	O
of	O
rows	O
in	O
`	O
b	O
`	O
where	O
the	O
`	O
b	O
[	O
"	O
key	O
"]`	O
is	O
one	O
of	O
the	O
values	O
in	O
`	O
keys	Y
`	O
.	O

There	O
are	O
a	O
few	O
built	O
in	O
methods	O
on	O
the	O
`	O
groupby	Y
`	O
object	O
that	O
are	O
useful	O
.	O

For	O
example	O
,	O
check	O
out	O
`	O
valid_rows.groupby	O
(	O
"	O
key	O
")	O
.sum()	Y
`	O
or	O
`	O
valid_rows.groupby	O
(	O
"	O
key	O
")	O
.describe()	Y
`	O
.	O

Under	O
the	O
covers	O
,	O
these	O
are	O
really	O
similar	O
uses	O
of	O
`	O
apply	Y
`	O
.	O

The	O
shape	O
of	O
the	O
returned	O
`	O
summary	Y
`	O
is	O
determined	O
by	O
the	O
applied	O
function	O
.	O

The	O
unique	O
grouped-by	O
values	O
--	O
those	O
of	O
`	O
b	O
[	O
"	O
key	O
"]`	O
--	O
always	O
constitute	O
the	O
index	O
,	O
but	O
if	O
the	O
applied	O
function	O
returns	O
a	O
scalar	O
,	O
`	O
summary	Y
`	O
is	O
a	O
`	O
Series	Y
`	O
;	O
if	O
the	O
applied	O
function	O
returns	O
a	O
`	O
Series	Y
`	O
,	O
then	O
`	O
summary	Y
`	O
constituted	O
of	O
the	O
return	O
`	O
Series	Y
`	O
as	O
rows	O
;	O
if	O
the	O
applied	O
function	O
returns	O
a	O
`	O
DataFrame	Y
`	O
,	O
then	O
the	O
result	O
is	O
a	O
multiindex	O
`	O
DataFrame	Y
`	O
.	O

This	O
is	O
a	O
core	O
pattern	O
in	O
Pandas	O
,	O
and	O
there's	O
a	O
whole	O
,	O
whole	O
lot	O
to	O
explore	O
here	O
.	O

`	O
loop_iter	O
=	O
len	Y
(	O
A	O
)	O
/	O
max	O
(	O
A	O
[	O
'	O
SEQ_NUM	O
'])	O

Easy	O
way	O
to	O
apply	Y
transformation	O
from	O
`	O
pandas.get_dummies	Y
`	O
to	O
new	O
data	O
?	O

Hmm	O
.	O

I	O
agree	O
this	O
is	O
unexpected	O
behavior	O
and	O
may	O
be	O
a	O
bug	O
.	O

As	O
an	O
aside	O
that	O
may	O
help	O
you	O
in	O
the	O
meantime	O
,	O
with	O
datetime-indexed	O
data	O
,	O
[	O
resample	Y
]	O
(	O
#URL	O
)	O
is	O
usually	O
a	O
better	O
choice	O
than	O
reindex	Y
.	O

See	O
in	O
particular	O
the	O
keyword	O
arguments	O
``	O
label	O
``	O
and	O
``	O
close	O
``	O
which	O
may	O
be	O
related	O
to	O
your	O
issue	O
.	O

Call	O
`	O
transform	O
`	O
on	O
the	O
'	O
measurement	O
'	O
column	O
and	O
pass	O
the	O
method	O
`	O
diff	Y
`	O
,	O
transform	O
returns	O
a	O
series	O
with	O
an	O
index	O
aligned	O
to	O
the	O
original	O
df	O
:	O
#CODE	O

If	O
you	O
are	O
intending	O
to	O
apply	Y
some	O
sorting	O
on	O
the	O
result	O
of	O
`	O
transform	O
`	O
then	O
sort	O
the	O
df	O
first	O
:	O
#CODE	O

Or	O
you	O
can	O
slice	O
the	O
columns	O
and	O
pass	O
this	O
to	O
`	O
drop	Y
`	O
:	O
#CODE	O

These	O
values	O
are	O
median	Y
values	O
I	O
calculated	O
from	O
elsewhere	O
,	O
and	O
I	O
have	O
also	O
their	O
variance	O
and	O
standard	O
deviation	O
(	O
and	O
standard	O
error	O
,	O
too	O
)	O
.	O

I	O
would	O
like	O
to	O
plot	O
the	O
results	O
as	O
a	O
bar	O
plot	O
with	O
the	O
proper	O
error	O
bars	O
,	O
but	O
specifying	O
more	O
than	O
one	O
error	O
value	O
to	O
`	O
yerr	O
`	O
yields	O
an	O
exception	O
:	O
#CODE	O

Really	O
?	O

So	O
Hash	O
[	O
1	O
-	O
(	O
1	O
/	O
3*3	O
)]	O
!	O

=	O
Hash	O
[	O
0	O
]	O
was	O
my	O
point	O
,	O
but	O
even	O
without	O
arithmetic	O
,	O
there	O
will	O
be	O
a	O
huge	O
range	O
values	O
for	O
the	O
keys	O
that	O
will	O
give	O
potentially	O
unfortunate	O
results	O
.	O

I'd	O
avoid	O
this	O
at	O
all	O
costs	O
personally	O
.	O

if	O
precision	O
is	O
to	O
decimal	O
place	O
,	O
I'd	O
multiply	O
it	O
by	O
10	O
and	O
truncate	Y
maybe	O
.	O

the	O
documentation	O
to	O
concat	Y
is	O
impenetrable	O
and	O
its	O
hard	O
to	O
find	O
examples	O
of	O
this	O
relatively	O
simple	O
task	O
in	O
the	O
docs	O

If	O
you	O
had	O
not	O
called	O
`	O
apply	Y
`	O
on	O
the	O
`	O
groupby	Y
`	O
object	O
then	O
you	O
could	O
access	O
the	O
`	O
groups	O
`	O
:	O
#CODE	O

pandas	O
groupby	Y
X	O
,	O
Y	O
and	O
select	O
last	O
week	O
of	O
X1	O
and	O
X2	O
(	O
which	O
have	O
diff	Y
frequency	O
)	O

Then	O
you	O
can	O
select	O
the	O
rows	O
you	O
want	O
in	O
an	O
apply	Y
call	O
on	O
the	O
grouped	O
object	O
:	O
#CODE	O

If	O
you	O
can't	O
upgrade	O
or	O
don't	O
solve	O
the	O
issue	O
you	O
have	O
with	O
0.14	O
,	O
you	O
can	O
try	O
to	O
use	O
`	O
ix	Y
`	O
instead	O
of	O
`	O
iloc	Y
`	O

How	O
do	O
I	O
export	O
multiple	O
pivot	Y
tables	O
from	O
python	O
using	O
pandas	O
to	O
a	O
single	O
csv	O
document	O
?	O

Say	O
I	O
have	O
a	O
function	O
pivots()	O
which	O
aggregates	O
pivot	Y
tables	O
#CODE	O

I	O
know	O
how	O
to	O
export	O
a	O
single	O
pivot	Y
table	O
#CODE	O

You	O
can	O
use	O
`	O
to_csv	Y
(	O
path	O
,	O
mode=	O
'	O
a	O
')`	O
to	O
append	Y
files	O
.	O

#CODE	O

Use	O
`	O
shift	Y
`	O
and	O
`	O
np.log	O
`	O
:	O
#CODE	O

I'd	O
look	O
at	O
seeing	O
if	O
you	O
can	O
export	O
it	O
in	O
it's	O
raw	O
form	O
,	O
otherwise	O
this	O
must	O
be	O
a	O
common	O
problem	O
and	O
someone	O
somewhere	O
has	O
probably	O
coded	O
a	O
method	O
to	O
strip	Y
the	O
emojis	O
out	O
of	O
the	O
text	O

Python	O
pandas	O
map	Y
dict	O
keys	O
to	O
values	O

I	O
have	O
a	O
csv	O
for	O
input	O
,	O
whose	O
row	O
values	O
I'd	O
like	O
to	O
join	Y
into	O
a	O
new	O
field	O
.	O

This	O
new	O
field	O
is	O
a	O
constructed	O
url	O
,	O
which	O
will	O
then	O
be	O
processed	O
by	O
the	O
requests.post()	O
method	O
.	O

I	O
tried	O
to	O
map	Y
values	O
to	O
keys	O
with	O
a	O
dict	O
comprehension	O
,	O
but	O
the	O
assignment	O
of	O
a	O
key	O
like	O
'	O
FIRST_NAME	O
'	O
could	O
end	O
up	O
mapping	O
to	O
values	O
from	O
an	O
arbitrary	O
field	O
like	O
test_df	O
[	O
'	O
CITY	O
']	O
.	O

which	O
will	O
give	O
you	O
output	O
as	O
follows	O
:	O
`	O
[	O
{	O
'	O
FIRST_NAME	O
'	O
:	O
...,	O
'	O
LAST_NAME	O
'	O
:	O
...	O
}	O
,	O
{	O
'	O
FIRST_NAME	O
'	O
:	O
...,	O
'	O
LAST_NAME	O
'	O
:	O
...	O
}	O
]`	O
(	O
which	O
will	O
give	O
you	O
a	O
list	O
that	O
has	O
equal	O
length	O
as	O
`	O
test_df	O
`)	O
.	O

This	O
might	O
be	O
one	O
possibility	O
to	O
easily	O
map	Y
it	O
to	O
a	O
correct	O
row	O
.	O

Thanks	O
@USER	O
.	O

Do	O
you	O
know	O
if	O
append	Y
returns	O
a	O
copy	O
/	O
view	O
/	O
reference	O
of	O
the	O
original	O
dataframe	Y
?	O

Right	O
now	O
,	O
I	O
am	O
trying	O
to	O
replace	O
a	O
stored	O
procedure	O
with	O
a	O
Python	O
service	O
,	O
and	O
the	O
temp	O
tables	O
with	O
Pandas	O
dataframes	O
.	O

But	O
I'm	O
stuck	O
on	O
this	O
:	O
#CODE	O

Apply	Y
FROM_UNIXTIME	O
on	O
column	O
,	O
c	O

You	O
could	O
pass	O
an	O
argument	O
to	O
`	O
apply	Y
`	O
:	O
#CODE	O

Originally	O
,	O
I	O
used	O
append	Y
api	O
to	O
create	O
a	O
single	O
table	O
'	O
impression	O
'	O
,	O
however	O
that	O
was	O
taking	O
80sec	O
per	O
dataframe	Y
and	O
given	O
that	O
I	O
have	O
almost	O
200	O
of	O
files	O
to	O
be	O
processed	O
,	O
the	O
'	O
append	Y
'	O
appeared	O
to	O
be	O
too	O
slow	O
.	O

Also	O
,	O
why	O
is	O
append	Y
so	O
much	O
slower	O
than	O
put	O
?	O

Can	O
it	O
be	O
sped	O
up	O
?	O

pandas	O
merge	Y
with	O
MultiIndex	O
,	O
when	O
only	O
one	O
level	O
of	O
index	O
is	O
to	O
be	O
used	O
as	O
key	O

I	O
want	O
to	O
recover	O
the	O
values	O
in	O
the	O
column	O
'	O
_Cat	O
'	O
from	O
df2	O
and	O
merge	Y
them	O
into	O
df1	O
for	O
the	O
appropriate	O
values	O
of	O
'	O
_ItemId	O
'	O
.	O

This	O
is	O
almost	O
(	O
I	O
think	O
?	O
)	O
a	O
standard	O
many-to-one	O
merge	Y
,	O
except	O
that	O
the	O
appropriate	O
key	O
for	O
the	O
left	O
df	O
is	O
one	O
of	O
MultiIndex	O
levels	O
.	O

I	O
tried	O
this	O
:	O
#CODE	O

which	O
I	O
suppose	O
makes	O
sense	O
since	O
my	O
(	O
left	O
)	O
index	O
is	O
actually	O
made	O
of	O
two	O
keys	O
.	O

How	O
do	O
I	O
select	O
the	O
one	O
index	O
level	O
that	O
I	O
need	O
?	O

Or	O
is	O
there	O
a	O
better	O
approach	O
to	O
this	O
merge	Y
?	O

loc	Y
will	O
not	O
attempt	O
to	O
use	O
a	O
number	O
(	O
eg	O
1	O
)	O
as	O
a	O
positional	O
argument	O
at	O
all	O
(	O
and	O
will	O
raise	O
instead	O
);	O
see	O
main	O
pandas	O
docs	O
/	O
selecting	O
data	O

I	O
have	O
the	O
following	O
boxplot	Y
:	O
#CODE	O

My	O
question	O
is	O
:	O
how	O
can	O
I	O
change	O
the	O
whiskers	O
/	O
quantiles	O
being	O
plotted	O
in	O
the	O
boxplot	Y
?	O

Assume	O
I	O
have	O
a	O
dataframe	Y
where	O
I	O
can	O
compute	O
the	O
quantiles	O
by	O
rows	O
or	O
columns	O
,	O
as	O
in	O
:	O
#CODE	O

can	O
you	O
explain	O
the	O
final	O
format	O
of	O
dataf3	O
?	O
and	O
what	O
are	O
those	O
backquotes	O
doing	O
?	O

it'll	O
be	O
difficult	O
to	O
translate	Y
those	O
`	O
ddply	O
`	O
calls	O
to	O
pandas	O
.	O

I	O
guess	O
`	O
groupby	Y
`	O
should	O
be	O
used	O
but	O
I	O
find	O
this	O
format	O
very	O
cryptic	O
so	O
it's	O
hard	O
to	O
translate	Y
to	O
python	O

Because	O
"	O
%	O
"	O
is	O
an	O
illegal	O
characters	O
in	O
ggplot	O
I	O
had	O
to	O
enclose	O
the	O
names	O
in	O
backticks	O
.	O

If	O
you	O
drop	Y
the	O
"	O
%	O
"	O
sign	O
,	O
you	O
can	O
make	O
the	O
plot	O
without	O
ticks	O
.	O

I	O
added	O
it	O
.	O

Try	O
to	O
run	O
it	O
in	O
R	O
to	O
see	O
the	O
final	O
format	O
of	O
dataf3	O
.	O

The	O
rows	O
are	O
the	O
species	O
,	O
the	O
calculated	O
quantiles	O
and	O
meanx	O
are	O
the	O
columns	O
,	O
so	O
a	O
dataframe	Y
with	O
3	O
rows	O
and	O
7	O
columns	O
.	O

Append	Y
Two	O
Dataframes	O
Together	O
(	O
Pandas	O
,	O
Python3	O
)	O

I	O
am	O
trying	O
to	O
append	Y
/	O
join	Y
(	O
?	O
)	O
two	O
different	O
dataframes	O
together	O
that	O
don't	O
share	O
any	O
overlapping	O
data	O
.	O

I	O
am	O
trying	O
to	O
append	Y
these	O
together	O
using	O
#CODE	O

EDIT	O
:	O
in	O
regards	O
to	O
Edchum's	O
answers	O
,	O
I	O
have	O
tried	O
merge	Y
and	O
join	Y
but	O
each	O
create	O
somewhat	O
strange	O
tables	O
.	O

Instead	O
of	O
what	O
I	O
am	O
looking	O
for	O
(	O
as	O
listed	O
above	O
)	O
it	O
will	O
return	O
something	O
like	O
this	O
:	O
#CODE	O

OK	O
,	O
what	O
you	O
have	O
to	O
do	O
is	O
reindex	Y
or	O
reset	O
the	O
index	O
so	O
they	O
align	Y

Use	O
`	O
concat	Y
`	O
and	O
pass	O
param	O
`	O
axis=1	O
`	O
:	O
#CODE	O

`	O
join	Y
`	O
also	O
works	O
:	O
#CODE	O

As	O
does	O
`	O
merge	Y
`	O
:	O
#CODE	O

In	O
the	O
case	O
where	O
the	O
indices	O
do	O
not	O
align	Y
where	O
for	O
example	O
your	O
first	O
df	O
has	O
index	O
`	O
[	O
0	O
,	O
1	O
,	O
2	O
,	O
3	O
]`	O
and	O
your	O
second	O
df	O
has	O
index	O
`	O
[	O
0	O
,	O
2	O
]`	O
this	O
will	O
mean	O
that	O
the	O
above	O
operations	O
will	O
naturally	O
align	Y
against	O
the	O
first	O
df's	O
index	O
resulting	O
in	O
a	O
`	O
NaN	O
`	O
row	O
for	O
index	O
row	O
`	O
1	O
`	O
.	O

To	O
fix	O
this	O
you	O
can	O
reindex	Y
the	O
second	O
df	O
either	O
by	O
calling	O
`	O
reset_index()	Y
`	O
or	O
assign	O
directly	O
like	O
so	O
:	O
`	O
df2.index	O
=[	O
0	O
,	O
1	O
]`	O
.	O

So	O
I	O
wouldn't	O
expect	O
there	O
to	O
be	O
any	O
significant	O
deterioration	O
in	O
speed	O
.	O

And	O
you	O
could	O
always	O
drop	Y
back	O
to	O
numpy	O
operations	O
on	O
the	O
numpy	O
array	O
`	O
pan.values	O
`	O
if	O
need	O
be	O
,	O
though	O
,	O
hopefully	O
,	O
that	O
would	O
be	O
unnecessary	O
.	O

@USER	O
Which	O
numpy	O
version	O
do	O
you	O
have	O
?	O

This	O
argument	O
is	O
new	O
in	O
1.9	O
...	O
but	O
there	O
is	O
a	O
workaround	O
,	O
try	O
`	O
np.linspace	O
(	O
0	O
,	O
len	Y
(	O
pep_list	O
)	O
,	O
n+1	O
,	O
endpoint=True	O
)	O
.astype	Y
(	O
int	O
)`	O

Improve	O
Row	O
Append	Y
Performance	O
On	O
Pandas	O
DataFrames	O

Take	O
the	O
time	O
difference	O
(	O
using	O
`	O
shift	Y
`	O
)	O
til	O
the	O
next	O
value	O
,	O
and	O
multiply	O
(	O
value	O
*	O
seconds	O
):	O
#CODE	O

Then	O
do	O
the	O
resample	Y
to	O
seconds	O
(	O
sum	O
the	O
value*seconds	O
):	O
#CODE	O

you	O
can	O
isnull	Y
(	O
df	O
[	O
'	O
difference	O
'])	O
will	O
give	O
True	O
on	O
NaT	O
,	O
so	O
you	O
could	O
subtract	O
then	O
use	O
mask	O
I	O
think	O

`	O
groupby	Y
`	O
on	O
`	O
transcript_id	O
`	O
as	O
you	O
do	O
now	O
,	O
and	O
perform	O
your	O
calculations	O
(	O
say	O
`	O
agg_df	O
`)	O
.	O

After	O
they	O
are	O
done	O
,	O
merge	Y
the	O
two	O
frames	O
together	O
:	O
#CODE	O

Another	O
solution	O
(	O
slightly	O
harder	O
):	O
Merge	Y
the	O
columns	O
`	O
transcript_id	O
`	O
,	O
`	O
gene_id	O
`	O
and	O
`	O
gene_name	O
`	O
in	O
another	O
column	O
,	O
say	O
`	O
merged_id	O
`	O
and	O
`	O
groupby	Y
`	O
on	O
`	O
merged_id	O
`	O
.	O

Split	O
the	O
column	O
up	O
into	O
the	O
components	O
at	O
the	O
end	O
of	O
your	O
calculations	O
.	O

Geo	O
Pandas	O
Data	O
Frame	O
/	O
Matrix	O
-	O
filter	O
/	O
drop	Y
NaN	O
/	O
False	O
values	O

Then	O
I	O
stack	Y
the	O
dataframe	Y
,	O
give	O
the	O
index	O
levels	O
the	O
desired	O
names	O
,	O
and	O
select	O
only	O
the	O
rows	O
where	O
we	O
have	O
'	O
True	O
'	O
values	O
:	O
#CODE	O

Can	O
you	O
enable	O
the	O
debugger	O
to	O
get	O
a	O
stack	Y
trace	O
?	O

Just	O
pass	O
`	O
debug	O
=	O
True	O
`	O
as	O
an	O
argument	O
to	O
`	O
app.run()	O
`	O
.	O

reshape	O
data	O
frame	O
in	O
pandas	O
with	O
pivot	Y
table	O

With	O
pivot	Y
table	O
you	O
can	O
get	O
a	O
matrix	O
showing	O
which	O
`	O
baz	O
`	O
corresponds	O
to	O
which	O
`	O
qux	O
`	O
:	O
#CODE	O

Rolling	O
apply	Y
question	O

For	O
each	O
group	O
in	O
the	O
groupby	Y
object	O
,	O
we	O
will	O
want	O
to	O
apply	Y
a	O
function	O
:	O
#CODE	O

We	O
want	O
to	O
take	O
the	O
Times	O
column	O
,	O
and	O
for	O
each	O
time	O
,	O
apply	Y
a	O
function	O
.	O

That's	O
done	O
with	O
`	O
applymap	Y
`	O
:	O
#CODE	O

Given	O
a	O
time	O
`	O
t	O
`	O
,	O
we	O
can	O
select	O
the	O
`	O
Value	O
`	O
s	O
from	O
`	O
subf	O
`	O
whose	O
times	O
are	O
in	O
the	O
half-open	O
interval	O
`	O
(	O
t-60	O
,	O
t	O
]`	O
using	O
the	O
`	O
ix	Y
`	O
method	O
:	O
#CODE	O

pandas	O
join	Y
data	O
frames	O
on	O
similar	O
but	O
not	O
identical	O
string	O
using	O
lower	O
case	O
only	O

I	O
need	O
to	O
join	Y
data	O
frames	O
on	O
columns	O
that	O
are	O
similar	O
but	O
not	O
identical	O
.	O

Fortunately	O
,	O
the	O
lowercase	O
letters	O
are	O
identical	O
between	O
columns	O
.	O

So	O
I	O
am	O
trying	O
to	O
isolate	O
the	O
lowercase	O
letters	O
from	O
each	O
column	O
,	O
create	O
new	O
columns	O
to	O
join	Y
on	O
.	O

#CODE	O

Note	O
that	O
this	O
assumes	O
collecting	O
all	O
ASCII	O
characters	O
from	O
`	O
a	O
`	O
to	O
`	O
z	O
`	O
suffices	O
to	O
produce	O
values	O
on	O
which	O
to	O
join	Y
.	O

Join	Y
Series	O
on	O
MultiIndex	O
in	O
pandas	O

I	O
think	O
not	O
.	O

You	O
can	O
of	O
course	O
extend	O
this	O
with	O
several	O
joins	O
,	O
the	O
join	Y
solution	O
detects	O
common	O
indices	O
automatically	O
.	O

These	O
are	O
the	O
relevant	O
discussions	O
[	O
3662	O
]	O
(	O
#URL	O
)	O
,	O
[	O
6360	O
]	O
(	O
#URL	O
)	O

I'm	O
building	O
a	O
fuzzy	O
search	O
program	O
,	O
using	O
FuzzyWuzzy	O
,	O
to	O
find	O
matching	O
names	O
in	O
a	O
dataset	O
.	O

My	O
data	O
is	O
in	O
a	O
DataFrame	Y
of	O
about	O
10378	O
rows	O
and	O
`	O
len	Y
(	O
df	O
[	O
'	O
Full	O
name	O
'])`	O
is	O
10378	O
,	O
as	O
expected	O
.	O

But	O
`	O
len	Y
(	O
choices	O
)`	O
is	O
only	O
1695	O
.	O

I'm	O
fairly	O
certain	O
that	O
the	O
issue	O
is	O
in	O
the	O
first	O
line	O
,	O
with	O
the	O
`	O
to_dict()	Y
`	O
function	O
,	O
as	O
`	O
len	Y
(	O
df	O
[	O
'	O
Full	O
name	O
']	O
.astype	Y
(	O
str	O
)`	O
results	O
in	O
10378	O
and	O
`	O
len	Y
(	O
df	O
[	O
'	O
Full	O
name	O
']	O
.to_dict()	Y
)`	O
results	O
in	O
1695	O
.	O

what	O
is	O
`	O
len	Y
(	O
df.index.unique()	O
)`	O
?	O

@USER	O
using	O
`	O
choices	O
=	O
dict	O
(	O
zip	O
(	O
df	O
[	O
'	O
n	O
']	O
,	O
df	O
[	O
'	O
Full	O
name	O
']	O
.astype	Y
(	O
str	O
)))`	O
,	O
where	O
df	O
[	O
'	O
n	O
']	O
is	O
np.arange	O
(	O
len	Y
(	O
df	O
))	O
,	O
worked	O
fine	O
and	O
got	O
what	O
I	O
needed	O
.	O

Had	O
some	O
indexing	O
issues	O
because	O
I	O
was	O
importing	O
the	O
data	O
from	O
different	O
Excel	O
spreadsheets	O
.	O

How	O
do	O
I	O
give	O
you	O
credit	O
for	O
your	O
help	O
?	O

This	O
is	O
what	O
is	O
happening	O
in	O
your	O
case	O
,	O
and	O
noted	O
from	O
the	O
comments	O
,	O
since	O
the	O
amount	O
of	O
`	O
unique	Y
`	O
values	O
for	O
the	O
index	O
are	O
only	O
`	O
1695	O
`	O
,	O
we	O
can	O
confirm	O
this	O
by	O
testing	O
the	O
value	O
of	O
`	O
len	Y
(	O
df.index.unique()	O
)`	O
.	O

what	O
do	O
you	O
mean	O
by	O
normalize	Y
?	O

such	O
that	O
the	O
means	O
are	O
zero	O
and	O
standard	O
deviation	O
are	O
1	O
?	O

The	O
other	O
way	O
is	O
much	O
easier	O
and	O
involves	O
using	O
`	O
resample	Y
`	O
to	O
convert	O
to	O
daily	O
observations	O
and	O
backfill	O
daily	O
consumption	O
.	O

#CODE	O

From	O
there	O
,	O
all	O
you	O
have	O
to	O
do	O
is	O
aggregate	O
.	O

(	O
Note	O
that	O
the	O
first	O
and	O
last	O
months	O
are	O
based	O
on	O
partial	O
data	O
,	O
you	O
may	O
want	O
to	O
either	O
drop	Y
them	O
or	O
pro-rate	O
the	O
daily	O
consumption	O
.	O
)	O
#CODE	O

Some	O
brief	O
thoughts	O
on	O
an	O
alternate	O
approach	O
:	O
If	O
the	O
above	O
causes	O
memory	O
problems	O
,	O
I	O
think	O
there	O
might	O
be	O
a	O
hybrid	O
approach	O
.	O

Basically	O
,	O
after	O
calculating	O
the	O
daily	O
consumption	O
,	O
do	O
a	O
partial	O
resample	Y
by	O
adding	O
the	O
first	O
and	O
last	O
day	O
of	O
each	O
month	O
.	O

From	O
there	O
you	O
can	O
probably	O
aggregate	O
in	O
a	O
similar	O
way	O
although	O
you	O
need	O
to	O
essentially	O
do	O
a	O
weighted	O
sum	O
rather	O
than	O
simple	O
sum	O
.	O

This	O
looks	O
like	O
a	O
pretty	O
good	O
solution	O
.	O

I	O
will	O
implement	O
it	O
and	O
see	O
how	O
it	O
goes	O
,	O
but	O
can	O
you	O
also	O
explain	O
what	O
'	O
1d	O
'	O
means	O
in	O
the	O
resample	Y
method	O
?	O

@USER	O
'	O
1d	O
'	O
just	O
means	O
1	O
day	O
for	O
the	O
frequency	O
of	O
the	O
resample	Y
.	O

It	O
can	O
also	O
just	O
be	O
written	O
as	O
'	O
d	O
'	O
,	O
the	O
'	O
1	O
'	O
is	O
redundant	O
in	O
this	O
case	O
.	O

After	O
removing	O
groups	O
without	O
any	O
ratings	O
,	O
I	O
want	O
to	O
be	O
able	O
to	O
fill	O
in	O
the	O
NaN	O
ratings	O
with	O
the	O
mean	O
rating	O
for	O
each	O
group	O
.	O

However	O
,	O
I	O
need	O
to	O
have	O
at	O
least	O
one	O
rating	O
to	O
be	O
able	O
to	O
do	O
that	O
.	O

So	O
I	O
want	O
something	O
that	O
will	O
drop	Y
the	O
`	O
lob	O
`	O
group	O
,	O
but	O
keep	O
every	O
record	O
of	O
both	O
the	O
`	O
mol	O
`	O
and	O
`	O
thg	O
`	O
group	O
.	O

Pandas	O
Merge	Y
2	O
data	O
frames	O
by	O
2	O
columns	O
each	O

In	O
each	O
data	O
frame	O
i	O
have	O
column	O
with	O
the	O
same	O
name	O
and	O
values	O
(	O
Key_Merge1	O
)	O
and	O
in	O
each	O
data	O
frame	O
i	O
have	O
2	O
different	O
column	O
names	O
with	O
same	O
values	O
(	O
Key_Merge2	O
)	O
.	O

How	O
can	O
i	O
merge	Y
2	O
data	O
frames	O
by	O
2	O
columns	O
:	O

Can	O
you	O
post	O
an	O
example	O
data	O
and	O
df	O
,	O
your	O
text	O
description	O
is	O
not	O
clear	O
enough	O
but	O
generally	O
you	O
want	O
to	O
merge	Y
and	O
pass	O
the	O
list	O
of	O
cols	O
to	O
merge	Y
the	O
;	O
hs	O
and	O
rhs	O
on	O
:	O
`	O
pd.merge	Y
(	O
df1	O
,	O
df2	O
,	O
left_on	O
=[	O
'	O
Key_Merge1	O
'	O
,	O
'	O
Key_Merge21	O
']	O
,	O
right_on	O
=[	O
'	O
Key_Merge1	O
'	O
,	O
'	O
Key_merge22	O
'])`	O

OK	O
,	O
you	O
have	O
to	O
rename	O
'	O
PRODUCT_GROUP	O
'	O
in	O
DF2	O
in	O
order	O
for	O
the	O
`	O
merge	Y
`	O
to	O
work	O
:	O
#CODE	O

the	O
merge	Y
will	O
naturally	O
find	O
the	O
2	O
columns	O
that	O
match	O
and	O
perform	O
an	O
inner	O
merge	Y
as	O
desired	O

I	O
can	O
strip	Y
out	O
the	O
rightmost	O
'	O
.csv	O
'	O
part	O
like	O
this	O
:	O
#CODE	O

How	O
to	O
merge	Y
two	O
DataFrame	Y
columns	O
and	O
apply	Y
pandas.to_datetime	Y
to	O
it	O
?	O

What	O
would	O
be	O
a	O
more	O
pythonic	O
way	O
to	O
merge	Y
two	O
columns	O
,	O
and	O
apply	Y
a	O
function	O
into	O
the	O
result	O
?	O

once	O
sorted	O
I	O
replace	O
the	O
df.index	O
with	O
a	O
numerical	O
index	O
#CODE	O

Python	O
,	O
pandas	O
:	O
Cut	Y
off	O
filter	O
for	O
spikes	O
in	O
a	O
cumulative	O
series	O

This	O
can	O
be	O
accomplished	O
with	O
a	O
one	O
line	O
solution	O
using	O
Pandas	O
'	O
boolean	O
indexing	O
.	O

The	O
one-liner	O
also	O
employs	O
some	O
other	O
tricks	O
:	O
Pandas	O
'	O
`	O
map	Y
`	O
and	O
`	O
diff	Y
`	O
methods	O
and	O
a	O
`	O
lambda	O
`	O
function	O
.	O

`	O
map	Y
`	O
is	O
used	O
to	O
apply	Y
the	O
`	O
lambda	O
`	O
function	O
to	O
all	O
rows	O
.	O

The	O
`	O
lambda	O
`	O
function	O
is	O
needed	O
to	O
create	O
a	O
custom	O
less-then	O
comparison	O
that	O
will	O
evaluate	O
NaN	O
values	O
to	O
True	O
.	O

There	O
is	O
a	O
built	O
in	O
method	O
for	O
this	O
`	O
diff	Y
`	O
:	O
#CODE	O

as	O
pointed	O
out	O
calling	O
`	O
diff	Y
`	O
here	O
will	O
lose	O
the	O
first	O
row	O
so	O
I'm	O
using	O
a	O
ugly	O
hack	O
where	O
I	O
concatenate	O
the	O
first	O
row	O
with	O
the	O
result	O
of	O
the	O
`	O
diff	Y
`	O
so	O
I	O
don't	O
lose	O
the	O
first	O
row	O

Using	O
`	O
diff	Y
`	O
like	O
this	O
drops	O
the	O
first	O
row	O
.	O

Is	O
there	O
a	O
more	O
memory	O
efficient	O
way	O
to	O
do	O
this	O
in	O
HDFStore	O
?	O

Should	O
I	O
set	O
the	O
index	O
to	O
the	O
"	O
sec_id	O
"	O
?	O

(	O
I	O
can	O
also	O
use	O
the	O
chunksize	O
option	O
and	O
concat	Y
myself	O
,	O
but	O
that	O
seems	O
to	O
be	O
a	O
bit	O
of	O
a	O
hack	O
.	O
)	O

Jeff	O
,	O
I	O
updated	O
sec_id	O
and	O
dt	Y
in	O
the	O
dataframe	Y
.	O

Sorry	O
,	O
I	O
had	O
to	O
update	O
"	O
sec_id	O
"	O
and	O
"	O
dt	Y
"	O
to	O
"	O
id	O
"	O
and	O
"	O
date	O
"	O
.	O

This	O
code	O
sample	O
I	O
have	O
is	O
direct	O
from	O
the	O
code	O
.	O

0.12	O
is	O
fine	O
;	O
FYI	O
the	O
format	O
keyword	O
doesn't	O
do	O
anything	O
with	O
append	Y
(	O
and	O
it's	O
for	O
0.13	O
anyhow	O
);	O
append	Y
always	O
is	O
a	O
table	O

I	O
would	O
like	O
to	O
get	O
every	O
,	O
let's	O
say	O
,	O
6	O
hours	O
of	O
data	O
and	O
independently	O
fit	O
a	O
curve	O
to	O
that	O
data	O
.	O

Since	O
pandas	O
'	O
`	O
resample	Y
`	O
function	O
has	O
a	O
`	O
how	O
`	O
keyword	O
that	O
is	O
supposed	O
to	O
be	O
any	O
numpy	O
array	O
function	O
,	O
I	O
thought	O
that	O
I	O
could	O
maybe	O
try	O
to	O
use	O
resample	Y
to	O
do	O
that	O
with	O
`	O
polyfit	O
`	O
,	O
but	O
apparently	O
there	O
is	O
no	O
way	O
(	O
right	O
?	O
)	O
.	O

Why	O
does	O
the	O
second	O
block	O
of	O
code	O
not	O
work	O
?	O

Doesn't	O
DataFrame.apply()	Y
default	O
to	O
inplace	O
?	O

There	O
is	O
no	O
inplace	O
parameter	O
to	O
the	O
apply	Y
function	O
.	O

If	O
it	O
doesn't	O
work	O
in	O
place	O
,	O
doesn't	O
this	O
make	O
pandas	O
a	O
terrible	O
memory	O
handler	O
?	O

Do	O
all	O
pandas	O
data	O
frame	O
operations	O
copy	O
everything	O
in	O
situations	O
like	O
this	O
?	O

Wouldn't	O
it	O
be	O
better	O
to	O
just	O
do	O
it	O
inplace	O
?	O

Even	O
if	O
it	O
doesn't	O
default	O
to	O
inplace	O
,	O
shouldn't	O
it	O
provide	O
an	O
inplace	O
parameter	O
the	O
way	O
replace()	Y
does	O
?	O

No	O
,	O
apply	Y
does	O
not	O
work	O
inplace*	O
.	O

In	O
general	O
apply	Y
is	O
slow	O
(	O
since	O
you	O
are	O
basically	O
iterating	O
through	O
each	O
row	O
in	O
python	O
)	O
,	O
and	O
the	O
"	O
game	O
"	O
is	O
to	O
rewrite	O
that	O
function	O
in	O
terms	O
of	O
pandas	O
/	O
numpy	O
native	O
functions	O
and	O
indexing	O
.	O

If	O
you	O
want	O
to	O
delve	O
into	O
more	O
details	O
about	O
the	O
internals	O
,	O
check	O
out	O
the	O
BlockManager	O
in	O
core	O
/	O
internals.py	O
,	O
this	O
is	O
the	O
object	O
which	O
holds	O
the	O
underlying	O
numpy	O
arrays	O
.	O

But	O
to	O
be	O
honest	O
I	O
think	O
your	O
most	O
useful	O
tool	O
is	O
`	O
%timeit	O
`	O
and	O
looking	O
at	O
the	O
source	O
code	O
for	O
specific	O
functions	O
(	O
`	O
??	O
`	O
in	O
ipython	O
)	O
.	O

*	O
apply	Y
is	O
not	O
usually	O
going	O
to	O
make	O
sense	O
inplace	O
(	O
and	O
IMO	O
this	O
behaviour	O
would	O
rarely	O
be	O
desired	O
)	O
.	O

I	O
use	O
this	O
function	O
with	O
pandas	O
to	O
apply	Y
it	O
to	O
each	O
month	O
of	O
a	O
historical	O
record	O
:	O
#CODE	O

I	O
am	O
trying	O
to	O
merge	Y
tsv	O
files	O
using	O
pandas	O
but	O
cannot	O
get	O
pandas	O
to	O
return	O
the	O
file	O
contents	O
correctly	O
.	O

My	O
tsv	O
files	O
contain	O
Italian	O
and	O
pandas	O
fails	O
at	O
accented	O
characters	O
like	O
.	O

You	O
can	O
use	O
the	O
vectorised	O
`	O
str	Y
`	O
methods	O
to	O
replace	O
the	O
unwanted	O
characters	O
and	O
then	O
cast	O
the	O
type	O
to	O
int	O
:	O
#CODE	O

perhaps	O
`	O
reindex	Y
`	O
creates	O
a	O
new	O
dataframe	Y
,	O
`	O
ix	Y
`	O
returns	O
a	O
view	O

@USER	O
you	O
are	O
,	O
of	O
course	O
,	O
absolutely	O
right	O
.	O
what	O
do	O
`	O
loc	Y
`	O
and	O
`	O
iloc	Y
`	O
do	O
?	O

The	O
reason	O
for	O
the	O
seeming	O
redundancy	O
is	O
that	O
,	O
while	O
using	O
`	O
ix	Y
`	O
is	O
syntacticly	O
limiting	O
(	O
you	O
can	O
only	O
pass	O
a	O
single	O
argument	O
to	O
`	O
__getitem__	O
`)	O
,	O
`	O
reindex	Y
`	O
is	O
a	O
method	O
,	O
which	O
supports	O
taking	O
various	O
optional	O
parameters	O
.	O

(	O
docs	O
)	O

Thanks	O
-	O
What	O
happens	O
if	O
I	O
I	O
want	O
to	O
update	O
`	O
df2	O
`	O
with	O
the	O
output	O
of	O
these	O
commands	O
?	O

I	O
am	O
getting	O
different	O
results	O
when	O
using	O
`	O
reindex	Y
`	O
with	O
`	O
inplace=True	O
`	O
vs	O
using	O
`	O
ix	Y
`	O
(	O
I	O
updated	O
the	O
OP	O
)	O

You	O
say	O
that	O
the	O
best	O
way	O
is	O
to	O
plot	O
each	O
condition	O
(	O
like	O
`	O
subset_a	O
`	O
,	O
`	O
subset_b	O
`)	O
separately	O
.	O

What	O
if	O
you	O
have	O
many	O
conditions	O
,	O
e.g.	O
you	O
want	O
to	O
split	O
up	O
the	O
scatters	O
into	O
4	O
types	O
of	O
points	O
or	O
even	O
more	O
,	O
plotting	O
each	O
in	O
different	O
shape	O
/	O
color	O
.	O

How	O
can	O
you	O
elegantly	O
apply	Y
condition	O
a	O
,	O
b	O
,	O
c	O
,	O
etc	O
.	O
and	O
make	O
sure	O
you	O
then	O
plot	O
"	O
the	O
rest	O
"	O
(	O
things	O
not	O
in	O
any	O
of	O
these	O
conditions	O
)	O
as	O
the	O
last	O
step	O
?	O

From	O
what	O
I	O
can	O
tell	O
,	O
matplotlib	O
simply	O
skips	O
points	O
with	O
NA	O
x	O
/	O
y	O
coordinates	O
or	O
NA	O
style	O
settings	O
(	O
e.g.	O
,	O
color	O
/	O
size	O
)	O
.	O

To	O
find	O
points	O
skipped	O
due	O
to	O
NA	O
,	O
try	O
the	O
`	O
isnull	Y
`	O
method	O
:	O
`	O
df	O
[	O
df.col3.isnull()	O
]`	O

How	O
do	O
I	O
create	O
a	O
pivot	Y
table	O
in	O
Pandas	O
where	O
one	O
column	O
is	O
the	O
mean	O
of	O
some	O
values	O
,	O
and	O
the	O
other	O
column	O
is	O
the	O
sum	O
of	O
others	O
?	O

Basically	O
,	O
how	O
would	O
I	O
create	O
a	O
pivot	Y
table	O
that	O
consolidates	O
data	O
,	O
where	O
one	O
of	O
the	O
columns	O
of	O
data	O
it	O
represents	O
is	O
calculated	O
,	O
say	O
,	O
by	O
`	O
likelihood	O
percentage	O
`	O
(	O
0.0	O
-	O
1.0	O
)	O
by	O
taking	O
the	O
mean	O
,	O
and	O
another	O
is	O
calculated	O
by	O
`	O
number	O
ordered	O
`	O
which	O
sums	O
all	O
of	O
them	O
?	O
